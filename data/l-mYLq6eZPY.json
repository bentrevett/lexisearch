{
  "title": "Pieter Abbeel: Deep Reinforcement Learning | Lex Fridman Podcast #10",
  "id": "l-mYLq6eZPY",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:03.120\n The following is a conversation with Peter Abbeel.\n\n00:03.120 --> 00:04.840\n He's a professor at UC Berkeley\n\n00:04.840 --> 00:07.840\n and the director of the Berkeley Robotics Learning Lab.\n\n00:07.840 --> 00:10.080\n He's one of the top researchers in the world\n\n00:10.080 --> 00:13.080\n working on how we make robots understand\n\n00:13.080 --> 00:15.360\n and interact with the world around them,\n\n00:15.360 --> 00:18.680\n especially using imitation and deep reinforcement learning.\n\n00:19.720 --> 00:22.360\n This conversation is part of the MIT course\n\n00:22.360 --> 00:24.080\n on Artificial General Intelligence\n\n00:24.080 --> 00:26.400\n and the Artificial Intelligence podcast.\n\n00:26.400 --> 00:29.060\n If you enjoy it, please subscribe on YouTube,\n\n00:29.060 --> 00:31.680\n iTunes, or your podcast provider of choice,\n\n00:31.680 --> 00:34.840\n or simply connect with me on Twitter at Lex Friedman,\n\n00:34.840 --> 00:36.920\n spelled F R I D.\n\n00:36.920 --> 00:41.400\n And now, here's my conversation with Peter Abbeel.\n\n00:41.400 --> 00:44.120\n You've mentioned that if there was one person\n\n00:44.120 --> 00:46.200\n you could meet, it would be Roger Federer.\n\n00:46.200 --> 00:50.120\n So let me ask, when do you think we'll have a robot\n\n00:50.120 --> 00:54.760\n that fully autonomously can beat Roger Federer at tennis?\n\n00:54.760 --> 00:57.520\n Roger Federer level player at tennis?\n\n00:57.520 --> 01:00.720\n Well, first, if you can make it happen for me to meet Roger,\n\n01:00.720 --> 01:01.560\n let me know.\n\n01:01.560 --> 01:06.560\n In terms of getting a robot to beat him at tennis,\n\n01:07.440 --> 01:08.920\n it's kind of an interesting question\n\n01:08.920 --> 01:13.920\n because for a lot of the challenges we think about in AI,\n\n01:14.560 --> 01:16.760\n the software is really the missing piece,\n\n01:16.760 --> 01:18.620\n but for something like this,\n\n01:18.620 --> 01:22.720\n the hardware is nowhere near either.\n\n01:22.720 --> 01:26.560\n To really have a robot that can physically run around,\n\n01:26.560 --> 01:28.560\n the Boston Dynamics robots are starting to get there,\n\n01:28.560 --> 01:33.040\n but still not really human level ability to run around\n\n01:33.040 --> 01:34.980\n and then swing a racket.\n\n01:36.920 --> 01:38.400\n So you think that's a hardware problem?\n\n01:38.400 --> 01:39.960\n I don't think it's a hardware problem only.\n\n01:39.960 --> 01:41.640\n I think it's a hardware and a software problem.\n\n01:41.640 --> 01:43.160\n I think it's both.\n\n01:43.160 --> 01:45.680\n And I think they'll have independent progress.\n\n01:45.680 --> 01:50.400\n So I'd say the hardware maybe in 10, 15 years.\n\n01:51.680 --> 01:52.920\n On clay, not grass.\n\n01:52.920 --> 01:53.760\n I mean, grass is probably harder.\n\n01:53.760 --> 01:54.600\n With the sliding?\n\n01:54.600 --> 01:55.420\n Yeah.\n\n01:55.420 --> 01:58.920\n With the clay, I'm not sure what's harder, grass or clay.\n\n01:58.920 --> 02:01.600\n The clay involves sliding,\n\n02:01.600 --> 02:04.560\n which might be harder to master actually, yeah.\n\n02:06.040 --> 02:08.940\n But you're not limited to a bipedal.\n\n02:08.940 --> 02:09.780\n I mean, I'm sure there's no...\n\n02:09.780 --> 02:11.480\n Well, if we can build a machine,\n\n02:11.480 --> 02:13.200\n it's a whole different question, of course.\n\n02:13.200 --> 02:16.300\n If you can say, okay, this robot can be on wheels,\n\n02:16.300 --> 02:19.400\n it can move around on wheels and can be designed differently,\n\n02:19.400 --> 02:23.040\n then I think that can be done sooner probably\n\n02:23.040 --> 02:26.280\n than a full humanoid type of setup.\n\n02:26.280 --> 02:27.760\n What do you think of swing a racket?\n\n02:27.760 --> 02:31.240\n So you've worked at basic manipulation.\n\n02:31.240 --> 02:34.240\n How hard do you think is the task of swinging a racket\n\n02:34.240 --> 02:37.480\n would be able to hit a nice backhand or a forehand?\n\n02:39.480 --> 02:42.720\n Let's say we just set up stationary,\n\n02:42.720 --> 02:46.580\n a nice robot arm, let's say, a standard industrial arm,\n\n02:46.580 --> 02:49.840\n and it can watch the ball come and then swing the racket.\n\n02:50.700 --> 02:51.540\n It's a good question.\n\n02:51.540 --> 02:56.200\n I'm not sure it would be super hard to do.\n\n02:56.200 --> 02:58.240\n I mean, I'm sure it would require a lot,\n\n02:58.240 --> 03:00.000\n if we do it with reinforcement learning,\n\n03:00.000 --> 03:01.520\n it would require a lot of trial and error.\n\n03:01.520 --> 03:03.380\n It's not gonna swing it right the first time around,\n\n03:03.380 --> 03:07.920\n but yeah, I don't see why I couldn't\n\n03:07.920 --> 03:09.480\n swing it the right way.\n\n03:09.480 --> 03:10.340\n I think it's learnable.\n\n03:10.340 --> 03:12.160\n I think if you set up a ball machine,\n\n03:12.160 --> 03:13.800\n let's say on one side,\n\n03:13.800 --> 03:17.780\n and then a robot with a tennis racket on the other side,\n\n03:17.780 --> 03:20.280\n I think it's learnable\n\n03:20.280 --> 03:22.940\n and maybe a little bit of pre training and simulation.\n\n03:22.940 --> 03:25.560\n Yeah, I think that's feasible.\n\n03:25.560 --> 03:27.280\n I think the swing the racket is feasible.\n\n03:27.280 --> 03:28.900\n It'd be very interesting to see how much precision\n\n03:28.900 --> 03:29.740\n it can get.\n\n03:31.840 --> 03:35.400\n Cause I mean, that's where, I mean,\n\n03:35.400 --> 03:37.920\n some of the human players can hit it on the lines,\n\n03:37.920 --> 03:39.240\n which is very high precision.\n\n03:39.240 --> 03:42.840\n With spin, the spin is an interesting,\n\n03:42.840 --> 03:45.760\n whether RL can learn to put a spin on the ball.\n\n03:45.760 --> 03:46.880\n Well, you got me interested.\n\n03:46.880 --> 03:48.400\n Maybe someday we'll set this up.\n\n03:48.400 --> 03:51.120\n Sure, you got me intrigued.\n\n03:51.120 --> 03:52.680\n Your answer is basically, okay,\n\n03:52.680 --> 03:54.160\n for this problem, it sounds fascinating,\n\n03:54.160 --> 03:56.480\n but for the general problem of a tennis player,\n\n03:56.480 --> 03:58.560\n we might be a little bit farther away.\n\n03:58.560 --> 04:01.260\n What's the most impressive thing you've seen a robot do\n\n04:01.260 --> 04:02.500\n in the physical world?\n\n04:04.140 --> 04:06.480\n So physically for me,\n\n04:06.480 --> 04:10.920\n it's the Boston Dynamics videos.\n\n04:10.920 --> 04:14.560\n Always just bring home and just super impressed.\n\n04:15.680 --> 04:17.700\n Recently, the robot running up the stairs,\n\n04:17.700 --> 04:19.440\n doing the parkour type thing.\n\n04:19.440 --> 04:22.280\n I mean, yes, we don't know what's underneath.\n\n04:22.280 --> 04:23.940\n They don't really write a lot of detail,\n\n04:23.940 --> 04:27.040\n but even if it's hard coded underneath,\n\n04:27.040 --> 04:29.800\n which it might or might not be just the physical abilities\n\n04:29.800 --> 04:32.680\n of doing that parkour, that's a very impressive.\n\n04:32.680 --> 04:34.960\n So have you met Spot Mini\n\n04:34.960 --> 04:36.840\n or any of those robots in person?\n\n04:36.840 --> 04:41.040\n Met Spot Mini last year in April at the Mars event\n\n04:41.040 --> 04:42.960\n that Jeff Bezos organizes.\n\n04:42.960 --> 04:44.160\n They brought it out there\n\n04:44.160 --> 04:47.760\n and it was nicely following around Jeff.\n\n04:47.760 --> 04:50.640\n When Jeff left the room, they had it follow him along,\n\n04:50.640 --> 04:52.160\n which is pretty impressive.\n\n04:52.160 --> 04:55.680\n So I think there's some confidence to know\n\n04:55.680 --> 04:58.040\n that there's no learning going on in those robots.\n\n04:58.040 --> 05:00.160\n The psychology of it, so while knowing that,\n\n05:00.160 --> 05:01.140\n while knowing there's not,\n\n05:01.140 --> 05:04.040\n if there's any learning going on, it's very limited.\n\n05:04.040 --> 05:05.800\n I met Spot Mini earlier this year\n\n05:06.840 --> 05:09.520\n and knowing everything that's going on,\n\n05:09.520 --> 05:11.000\n having one on one interaction,\n\n05:11.000 --> 05:15.960\n so I got to spend some time alone and there's immediately\n\n05:15.960 --> 05:18.640\n a deep connection on the psychological level.\n\n05:18.640 --> 05:21.000\n Even though you know the fundamentals, how it works,\n\n05:21.000 --> 05:23.240\n there's something magical.\n\n05:23.240 --> 05:27.560\n So do you think about the psychology of interacting\n\n05:27.560 --> 05:29.080\n with robots in the physical world?\n\n05:29.080 --> 05:32.800\n Even you just showed me the PR2, the robot,\n\n05:33.720 --> 05:36.860\n and there was a little bit something like a face,\n\n05:36.860 --> 05:38.480\n had a little bit something like a face.\n\n05:38.480 --> 05:40.600\n There's something that immediately draws you to it.\n\n05:40.600 --> 05:45.160\n Do you think about that aspect of the robotics problem?\n\n05:45.160 --> 05:48.400\n Well, it's very hard with Brad here.\n\n05:48.400 --> 05:50.680\n We'll give him a name, Berkeley Robot\n\n05:50.680 --> 05:52.200\n for the Elimination of Tedious Tasks.\n\n05:52.200 --> 05:56.560\n It's very hard to not think of the robot as a person\n\n05:56.560 --> 05:58.880\n and it seems like everybody calls him a he\n\n05:58.880 --> 06:01.160\n for whatever reason, but that also makes it more a person\n\n06:01.160 --> 06:06.160\n than if it was a it, and it seems pretty natural\n\n06:06.360 --> 06:07.320\n to think of it that way.\n\n06:07.320 --> 06:08.680\n This past weekend really struck me.\n\n06:08.680 --> 06:13.360\n I've seen Pepper many times on videos,\n\n06:13.360 --> 06:15.360\n but then I was at an event organized by,\n\n06:15.360 --> 06:18.880\n this was by Fidelity, and they had scripted Pepper\n\n06:18.880 --> 06:22.800\n to help moderate some sessions,\n\n06:22.800 --> 06:23.920\n and they had scripted Pepper\n\n06:23.920 --> 06:26.520\n to have the personality of a child a little bit,\n\n06:26.520 --> 06:28.600\n and it was very hard to not think of it\n\n06:28.600 --> 06:31.920\n as its own person in some sense\n\n06:31.920 --> 06:34.560\n because it would just jump in the conversation,\n\n06:34.560 --> 06:35.880\n making it very interactive.\n\n06:35.880 --> 06:37.960\n Moderate would be saying, Pepper would just jump in,\n\n06:37.960 --> 06:40.120\n hold on, how about me?\n\n06:40.120 --> 06:41.360\n Can I participate in this too?\n\n06:41.360 --> 06:43.720\n And you're just like, okay, this is like a person,\n\n06:43.720 --> 06:46.640\n and that was 100% scripted, and even then it was hard\n\n06:46.640 --> 06:50.640\n not to have that sense of somehow there is something there.\n\n06:50.640 --> 06:54.440\n So as we have robots interact in this physical world,\n\n06:54.440 --> 06:56.120\n is that a signal that could be used\n\n06:56.120 --> 06:57.440\n in reinforcement learning?\n\n06:57.440 --> 07:00.240\n You've worked a little bit in this direction,\n\n07:00.240 --> 07:03.300\n but do you think that psychology can be somehow pulled in?\n\n07:04.360 --> 07:07.160\n Yes, that's a question I would say\n\n07:07.160 --> 07:11.320\n a lot of people ask, and I think part of why they ask it\n\n07:11.320 --> 07:14.960\n is they're thinking about how unique\n\n07:14.960 --> 07:16.680\n are we really still as people?\n\n07:16.680 --> 07:18.120\n Like after they see some results,\n\n07:18.120 --> 07:21.440\n they see a computer play Go, they see a computer do this,\n\n07:21.440 --> 07:23.760\n that, they're like, okay, but can it really have emotion?\n\n07:23.760 --> 07:26.760\n Can it really interact with us in that way?\n\n07:26.760 --> 07:29.100\n And then once you're around robots,\n\n07:29.100 --> 07:30.120\n you already start feeling it,\n\n07:30.120 --> 07:33.180\n and I think that kind of maybe mythologically,\n\n07:33.180 --> 07:34.720\n the way that I think of it is\n\n07:34.720 --> 07:37.640\n if you run something like reinforcement learning,\n\n07:37.640 --> 07:39.920\n it's about optimizing some objective,\n\n07:39.920 --> 07:44.920\n and there's no reason that the objective\n\n07:45.360 --> 07:49.380\n couldn't be tied into how much does a person like\n\n07:49.380 --> 07:50.720\n interacting with this system,\n\n07:50.720 --> 07:53.220\n and why could not the reinforcement learning system\n\n07:53.220 --> 07:56.720\n optimize for the robot being fun to be around?\n\n07:56.720 --> 07:58.940\n And why wouldn't it then naturally become\n\n07:58.940 --> 08:01.400\n more and more interactive and more and more\n\n08:01.400 --> 08:03.200\n maybe like a person or like a pet?\n\n08:03.200 --> 08:04.600\n I don't know what it would exactly be,\n\n08:04.600 --> 08:06.640\n but more and more have those features\n\n08:06.640 --> 08:08.320\n and acquire them automatically.\n\n08:08.320 --> 08:10.880\n As long as you can formalize an objective\n\n08:10.880 --> 08:13.440\n of what it means to like something,\n\n08:13.440 --> 08:16.800\n what, how you exhibit, what's the ground truth?\n\n08:16.800 --> 08:19.560\n How do you get the reward from human?\n\n08:19.560 --> 08:20.760\n Because you have to somehow collect\n\n08:20.760 --> 08:22.400\n that information within you, human.\n\n08:22.400 --> 08:26.280\n But you're saying if you can formulate as an objective,\n\n08:26.280 --> 08:27.240\n it can be learned.\n\n08:27.240 --> 08:29.380\n There's no reason it couldn't emerge through learning,\n\n08:29.380 --> 08:31.480\n and maybe one way to formulate as an objective,\n\n08:31.480 --> 08:33.800\n you wouldn't have to necessarily score it explicitly,\n\n08:33.800 --> 08:36.560\n so standard rewards are numbers,\n\n08:36.560 --> 08:38.740\n and numbers are hard to come by.\n\n08:38.740 --> 08:41.320\n This is a 1.5 or a 1.7 on some scale.\n\n08:41.320 --> 08:43.060\n It's very hard to do for a person,\n\n08:43.060 --> 08:45.420\n but much easier is for a person to say,\n\n08:45.420 --> 08:47.800\n okay, what you did the last five minutes\n\n08:47.800 --> 08:51.160\n was much nicer than what you did the previous five minutes,\n\n08:51.160 --> 08:53.080\n and that now gives a comparison.\n\n08:53.080 --> 08:55.320\n And in fact, there have been some results on that.\n\n08:55.320 --> 08:57.880\n For example, Paul Christiano and collaborators at OpenAI\n\n08:57.880 --> 09:02.040\n had the Hopper, Mojoko Hopper, a one legged robot,\n\n09:02.040 --> 09:05.600\n going through backflips purely from feedback.\n\n09:05.600 --> 09:06.920\n I like this better than that.\n\n09:06.920 --> 09:08.640\n That's kind of equally good,\n\n09:08.640 --> 09:10.920\n and after a bunch of interactions,\n\n09:10.920 --> 09:13.080\n it figured out what it was the person was asking for,\n\n09:13.080 --> 09:14.400\n namely a backflip.\n\n09:14.400 --> 09:15.920\n And so I think the same thing.\n\n09:15.920 --> 09:18.640\n Oh, it wasn't trying to do a backflip.\n\n09:18.640 --> 09:20.820\n It was just getting a comparison score\n\n09:20.820 --> 09:22.300\n from the person based on?\n\n09:23.320 --> 09:26.080\n Person having in mind, in their own mind,\n\n09:26.080 --> 09:27.400\n I wanted to do a backflip,\n\n09:27.400 --> 09:30.760\n but the robot didn't know what it was supposed to be doing.\n\n09:30.760 --> 09:32.800\n It just knew that sometimes the person said,\n\n09:32.800 --> 09:34.560\n this is better, this is worse,\n\n09:34.560 --> 09:36.020\n and then the robot figured out\n\n09:36.020 --> 09:38.760\n what the person was actually after was a backflip.\n\n09:38.760 --> 09:40.040\n And I'd imagine the same would be true\n\n09:40.040 --> 09:43.120\n for things like more interactive robots,\n\n09:43.120 --> 09:45.100\n that the robot would figure out over time,\n\n09:45.100 --> 09:48.160\n oh, this kind of thing apparently is appreciated more\n\n09:48.160 --> 09:50.200\n than this other kind of thing.\n\n09:50.200 --> 09:54.000\n So when I first picked up Sutton's,\n\n09:54.000 --> 09:56.200\n Richard Sutton's reinforcement learning book,\n\n09:56.200 --> 10:00.240\n before sort of this deep learning,\n\n10:01.280 --> 10:03.360\n before the reemergence of neural networks\n\n10:03.360 --> 10:05.640\n as a powerful mechanism for machine learning,\n\n10:05.640 --> 10:08.320\n RL seemed to me like magic.\n\n10:08.320 --> 10:10.280\n It was beautiful.\n\n10:10.280 --> 10:13.560\n So that seemed like what intelligence is,\n\n10:13.560 --> 10:15.520\n RL reinforcement learning.\n\n10:15.520 --> 10:20.320\n So how do you think we can possibly learn anything\n\n10:20.320 --> 10:22.980\n about the world when the reward for the actions\n\n10:22.980 --> 10:25.840\n is delayed, is so sparse?\n\n10:25.840 --> 10:29.680\n Like where is, why do you think RL works?\n\n10:30.560 --> 10:32.800\n Why do you think you can learn anything\n\n10:32.800 --> 10:35.040\n under such sparse rewards,\n\n10:35.040 --> 10:36.880\n whether it's regular reinforcement learning\n\n10:36.880 --> 10:38.640\n or deep reinforcement learning?\n\n10:38.640 --> 10:39.740\n What's your intuition?\n\n10:40.580 --> 10:44.480\n The counterpart of that is why is RL,\n\n10:44.480 --> 10:47.240\n why does it need so many samples,\n\n10:47.240 --> 10:49.640\n so many experiences to learn from?\n\n10:49.640 --> 10:50.760\n Because really what's happening is\n\n10:50.760 --> 10:53.040\n when you have a sparse reward,\n\n10:53.040 --> 10:55.200\n you do something maybe for like, I don't know,\n\n10:55.200 --> 10:57.440\n you take 100 actions and then you get a reward.\n\n10:57.440 --> 10:59.760\n And maybe you get like a score of three.\n\n10:59.760 --> 11:03.000\n And I'm like okay, three, not sure what that means.\n\n11:03.000 --> 11:05.040\n You go again and now you get two.\n\n11:05.040 --> 11:07.160\n And now you know that that sequence of 100 actions\n\n11:07.160 --> 11:08.320\n that you did the second time around\n\n11:08.320 --> 11:10.600\n somehow was worse than the sequence of 100 actions\n\n11:10.600 --> 11:11.920\n you did the first time around.\n\n11:11.920 --> 11:14.440\n But that's tough to now know which one of those\n\n11:14.440 --> 11:15.280\n were better or worse.\n\n11:15.280 --> 11:17.480\n Some might have been good and bad in either one.\n\n11:17.480 --> 11:19.840\n And so that's why it needs so many experiences.\n\n11:19.840 --> 11:21.280\n But once you have enough experiences,\n\n11:21.280 --> 11:23.480\n effectively RL is teasing that apart.\n\n11:23.480 --> 11:26.640\n It's trying to say okay, what is consistently there\n\n11:26.640 --> 11:27.840\n when you get a higher reward\n\n11:27.840 --> 11:30.000\n and what's consistently there when you get a lower reward?\n\n11:30.000 --> 11:32.480\n And then kind of the magic of sometimes\n\n11:32.480 --> 11:34.720\n the policy gradient update is to say\n\n11:34.720 --> 11:37.000\n now let's update the neural network\n\n11:37.000 --> 11:39.160\n to make the actions that were kind of present\n\n11:39.160 --> 11:41.460\n when things are good more likely\n\n11:41.460 --> 11:43.080\n and make the actions that are present\n\n11:43.080 --> 11:45.140\n when things are not as good less likely.\n\n11:45.140 --> 11:47.000\n So that is the counterpoint,\n\n11:47.000 --> 11:49.540\n but it seems like you would need to run it\n\n11:49.540 --> 11:50.920\n a lot more than you do.\n\n11:50.920 --> 11:52.760\n Even though right now people could say\n\n11:52.760 --> 11:54.480\n that RL is very inefficient,\n\n11:54.480 --> 11:56.320\n but it seems to be way more efficient\n\n11:56.320 --> 11:58.880\n than one would imagine on paper.\n\n11:58.880 --> 12:02.040\n That the simple updates to the policy,\n\n12:02.040 --> 12:04.960\n the policy gradient, that somehow you can learn,\n\n12:04.960 --> 12:07.740\n exactly you just said, what are the common actions\n\n12:07.740 --> 12:09.820\n that seem to produce some good results?\n\n12:09.820 --> 12:12.800\n That that somehow can learn anything.\n\n12:12.800 --> 12:15.600\n It seems counterintuitive at least.\n\n12:15.600 --> 12:16.920\n Is there some intuition behind it?\n\n12:16.920 --> 12:21.920\n Yeah, so I think there's a few ways to think about this.\n\n12:21.920 --> 12:26.440\n The way I tend to think about it mostly originally,\n\n12:26.440 --> 12:29.080\n so when we started working on deep reinforcement learning\n\n12:29.080 --> 12:32.760\n here at Berkeley, which was maybe 2011, 12, 13,\n\n12:32.760 --> 12:36.160\n around that time, John Schulman was a PhD student\n\n12:36.160 --> 12:38.400\n initially kind of driving it forward here.\n\n12:39.520 --> 12:44.080\n And the way we thought about it at the time was\n\n12:44.080 --> 12:47.000\n if you think about rectified linear units\n\n12:47.000 --> 12:49.340\n or kind of rectifier type neural networks,\n\n12:50.240 --> 12:51.080\n what do you get?\n\n12:51.080 --> 12:55.080\n You get something that's piecewise linear feedback control.\n\n12:55.080 --> 12:57.120\n And if you look at the literature,\n\n12:57.120 --> 12:59.360\n linear feedback control is extremely successful,\n\n12:59.360 --> 13:02.460\n can solve many, many problems surprisingly well.\n\n13:03.720 --> 13:05.700\n I remember, for example, when we did helicopter flight,\n\n13:05.700 --> 13:07.320\n if you're in a stationary flight regime,\n\n13:07.320 --> 13:10.440\n not a non stationary, but a stationary flight regime\n\n13:10.440 --> 13:12.520\n like hover, you can use linear feedback control\n\n13:12.520 --> 13:15.580\n to stabilize a helicopter, very complex dynamical system,\n\n13:15.580 --> 13:18.480\n but the controller is relatively simple.\n\n13:18.480 --> 13:20.660\n And so I think that's a big part of it is that\n\n13:20.660 --> 13:23.220\n if you do feedback control, even though the system\n\n13:23.220 --> 13:25.000\n you control can be very, very complex,\n\n13:25.000 --> 13:28.760\n often relatively simple control architectures\n\n13:28.760 --> 13:30.560\n can already do a lot.\n\n13:30.560 --> 13:32.600\n But then also just linear is not good enough.\n\n13:32.600 --> 13:35.120\n And so one way you can think of these neural networks\n\n13:35.120 --> 13:37.120\n is that sometimes they tile the space,\n\n13:37.120 --> 13:39.480\n which people were already trying to do more by hand\n\n13:39.480 --> 13:41.000\n or with finite state machines,\n\n13:41.000 --> 13:42.520\n say this linear controller here,\n\n13:42.520 --> 13:43.840\n this linear controller here.\n\n13:43.840 --> 13:45.640\n Neural network learns to tile the space\n\n13:45.640 --> 13:46.600\n and say linear controller here,\n\n13:46.600 --> 13:48.320\n another linear controller here,\n\n13:48.320 --> 13:50.080\n but it's more subtle than that.\n\n13:50.080 --> 13:52.000\n And so it's benefiting from this linear control aspect,\n\n13:52.000 --> 13:53.600\n it's benefiting from the tiling,\n\n13:53.600 --> 13:57.440\n but it's somehow tiling it one dimension at a time.\n\n13:57.440 --> 13:59.440\n Because if let's say you have a two layer network,\n\n13:59.440 --> 14:03.360\n if in that hidden layer, you make a transition\n\n14:03.360 --> 14:06.560\n from active to inactive or the other way around,\n\n14:06.560 --> 14:09.520\n that is essentially one axis, but not axis aligned,\n\n14:09.520 --> 14:12.360\n but one direction that you change.\n\n14:12.360 --> 14:14.780\n And so you have this kind of very gradual tiling\n\n14:14.780 --> 14:16.800\n of the space where you have a lot of sharing\n\n14:16.800 --> 14:19.560\n between the linear controllers that tile the space.\n\n14:19.560 --> 14:21.720\n And that was always my intuition as to why\n\n14:21.720 --> 14:24.820\n to expect that this might work pretty well.\n\n14:24.820 --> 14:26.160\n It's essentially leveraging the fact\n\n14:26.160 --> 14:28.560\n that linear feedback control is so good,\n\n14:28.560 --> 14:29.880\n but of course not enough.\n\n14:29.880 --> 14:31.800\n And this is a gradual tiling of the space\n\n14:31.800 --> 14:33.520\n with linear feedback controls\n\n14:33.520 --> 14:36.620\n that share a lot of expertise across them.\n\n14:36.620 --> 14:39.040\n So that's really nice intuition,\n\n14:39.040 --> 14:41.520\n but do you think that scales to the more\n\n14:41.520 --> 14:44.720\n and more general problems of when you start going up\n\n14:44.720 --> 14:49.480\n the number of dimensions when you start\n\n14:49.480 --> 14:52.760\n going down in terms of how often\n\n14:52.760 --> 14:55.400\n you get a clean reward signal?\n\n14:55.400 --> 14:58.800\n Does that intuition carry forward to those crazier,\n\n14:58.800 --> 15:01.580\n weirder worlds that we think of as the real world?\n\n15:03.360 --> 15:08.040\n So I think where things get really tricky\n\n15:08.040 --> 15:09.760\n in the real world compared to the things\n\n15:09.760 --> 15:11.920\n we've looked at so far with great success\n\n15:11.920 --> 15:16.920\n in reinforcement learning is the time scales,\n\n15:17.320 --> 15:18.960\n which takes us to an extreme.\n\n15:18.960 --> 15:21.800\n So when you think about the real world,\n\n15:21.800 --> 15:24.320\n I mean, I don't know, maybe some student\n\n15:24.320 --> 15:26.920\n decided to do a PhD here, right?\n\n15:26.920 --> 15:28.760\n Okay, that's a decision.\n\n15:28.760 --> 15:30.840\n That's a very high level decision.\n\n15:30.840 --> 15:32.680\n But if you think about their lives,\n\n15:32.680 --> 15:34.080\n I mean, any person's life,\n\n15:34.080 --> 15:37.440\n it's a sequence of muscle fiber contractions\n\n15:37.440 --> 15:40.360\n and relaxations, and that's how you interact with the world.\n\n15:40.360 --> 15:42.800\n And that's a very high frequency control thing,\n\n15:42.800 --> 15:44.640\n but it's ultimately what you do\n\n15:44.640 --> 15:46.600\n and how you affect the world,\n\n15:46.600 --> 15:48.320\n until I guess we have brain readings\n\n15:48.320 --> 15:49.800\n and you can maybe do it slightly differently.\n\n15:49.800 --> 15:52.600\n But typically that's how you affect the world.\n\n15:52.600 --> 15:56.360\n And the decision of doing a PhD is so abstract\n\n15:56.360 --> 15:59.320\n relative to what you're actually doing in the world.\n\n15:59.320 --> 16:01.120\n And I think that's where credit assignment\n\n16:01.120 --> 16:04.800\n becomes just completely beyond\n\n16:04.800 --> 16:06.760\n what any current RL algorithm can do.\n\n16:06.760 --> 16:09.000\n And we need hierarchical reasoning\n\n16:09.000 --> 16:12.520\n at a level that is just not available at all yet.\n\n16:12.520 --> 16:14.920\n Where do you think we can pick up hierarchical reasoning?\n\n16:14.920 --> 16:16.960\n By which mechanisms?\n\n16:16.960 --> 16:18.680\n Yeah, so maybe let me highlight\n\n16:18.680 --> 16:20.640\n what I think the limitations are\n\n16:20.640 --> 16:25.640\n of what already was done 20, 30 years ago.\n\n16:26.080 --> 16:27.720\n In fact, you'll find reasoning systems\n\n16:27.720 --> 16:30.960\n that reason over relatively long horizons,\n\n16:30.960 --> 16:32.800\n but the problem is that they were not grounded\n\n16:32.800 --> 16:34.200\n in the real world.\n\n16:34.200 --> 16:37.440\n So people would have to hand design\n\n16:39.160 --> 16:43.920\n some kind of logical, dynamical descriptions of the world\n\n16:43.920 --> 16:46.360\n and that didn't tie into perception.\n\n16:46.360 --> 16:49.280\n And so it didn't tie into real objects and so forth.\n\n16:49.280 --> 16:51.120\n And so that was a big gap.\n\n16:51.120 --> 16:53.960\n Now with deep learning, we start having the ability\n\n16:53.960 --> 16:58.960\n to really see with sensors, process that\n\n16:59.560 --> 17:01.440\n and understand what's in the world.\n\n17:01.440 --> 17:02.840\n And so it's a good time to try\n\n17:02.840 --> 17:04.960\n to bring these things together.\n\n17:04.960 --> 17:06.480\n I see a few ways of getting there.\n\n17:06.480 --> 17:08.160\n One way to get there would be to say\n\n17:08.160 --> 17:10.120\n deep learning can get bolted on somehow\n\n17:10.120 --> 17:12.280\n to some of these more traditional approaches.\n\n17:12.280 --> 17:14.120\n Now bolted on would probably mean\n\n17:14.120 --> 17:16.320\n you need to do some kind of end to end training\n\n17:16.320 --> 17:18.600\n where you say my deep learning processing\n\n17:18.600 --> 17:20.840\n somehow leads to a representation\n\n17:20.840 --> 17:24.640\n that in term uses some kind of traditional\n\n17:24.640 --> 17:29.640\n underlying dynamical systems that can be used for planning.\n\n17:29.840 --> 17:32.280\n And that's, for example, the direction Aviv Tamar\n\n17:32.280 --> 17:34.080\n and Thanard Kuretach here have been pushing\n\n17:34.080 --> 17:36.720\n with causal info again and of course other people too.\n\n17:36.720 --> 17:38.200\n That's one way.\n\n17:38.200 --> 17:41.080\n Can we somehow force it into the form factor\n\n17:41.080 --> 17:43.760\n that is amenable to reasoning?\n\n17:43.760 --> 17:46.520\n Another direction we've been thinking about\n\n17:46.520 --> 17:50.200\n for a long time and didn't make any progress on\n\n17:50.200 --> 17:53.640\n was more information theoretic approaches.\n\n17:53.640 --> 17:56.560\n So the idea there was that what it means\n\n17:56.560 --> 17:59.960\n to take high level action is to take\n\n17:59.960 --> 18:02.560\n and choose a latent variable now\n\n18:02.560 --> 18:04.640\n that tells you a lot about what's gonna be the case\n\n18:04.640 --> 18:05.480\n in the future.\n\n18:05.480 --> 18:09.400\n Because that's what it means to take a high level action.\n\n18:09.400 --> 18:13.040\n I say okay, I decide I'm gonna navigate\n\n18:13.040 --> 18:15.480\n to the gas station because I need to get gas for my car.\n\n18:15.480 --> 18:17.880\n Well, that'll now take five minutes to get there.\n\n18:17.880 --> 18:19.280\n But the fact that I get there,\n\n18:19.280 --> 18:22.320\n I could already tell that from the high level action\n\n18:22.320 --> 18:23.520\n I took much earlier.\n\n18:24.480 --> 18:28.440\n That we had a very hard time getting success with.\n\n18:28.440 --> 18:30.640\n Not saying it's a dead end necessarily,\n\n18:30.640 --> 18:33.120\n but we had a lot of trouble getting that to work.\n\n18:33.120 --> 18:34.720\n And then we started revisiting the notion\n\n18:34.720 --> 18:36.720\n of what are we really trying to achieve?\n\n18:37.800 --> 18:40.680\n What we're trying to achieve is not necessarily hierarchy\n\n18:40.680 --> 18:41.720\n per se, but you could think about\n\n18:41.720 --> 18:43.340\n what does hierarchy give us?\n\n18:44.280 --> 18:47.200\n What we hope it would give us is better credit assignment.\n\n18:49.120 --> 18:51.240\n What is better credit assignment?\n\n18:51.240 --> 18:55.760\n It's giving us, it gives us faster learning, right?\n\n18:55.760 --> 18:59.800\n And so faster learning is ultimately maybe what we're after.\n\n18:59.800 --> 19:03.400\n And so that's where we ended up with the RL squared paper\n\n19:03.400 --> 19:05.160\n on learning to reinforcement learn,\n\n19:06.040 --> 19:07.840\n which at a time Rocky Dwan led.\n\n19:08.840 --> 19:11.080\n And that's exactly the meta learning approach\n\n19:11.080 --> 19:14.240\n where you say, okay, we don't know how to design hierarchy.\n\n19:14.240 --> 19:15.760\n We know what we want to get from it.\n\n19:15.760 --> 19:18.240\n Let's just enter and optimize for what we want to get\n\n19:18.240 --> 19:20.200\n from it and see if it might emerge.\n\n19:20.200 --> 19:21.240\n And we saw things emerge.\n\n19:21.240 --> 19:25.160\n The maze navigation had consistent motion down hallways,\n\n19:26.120 --> 19:27.160\n which is what you want.\n\n19:27.160 --> 19:28.320\n A hierarchical control should say,\n\n19:28.320 --> 19:29.720\n I want to go down this hallway.\n\n19:29.720 --> 19:31.640\n And then when there is an option to take a turn,\n\n19:31.640 --> 19:33.840\n I can decide whether to take a turn or not and repeat.\n\n19:33.840 --> 19:37.280\n Even had the notion of where have you been before or not\n\n19:37.280 --> 19:39.960\n to not revisit places you've been before.\n\n19:39.960 --> 19:42.520\n It still didn't scale yet\n\n19:42.520 --> 19:46.000\n to the real world kind of scenarios I think you had in mind,\n\n19:46.000 --> 19:47.200\n but it was some sign of life\n\n19:47.200 --> 19:51.160\n that maybe you can meta learn these hierarchical concepts.\n\n19:51.160 --> 19:56.160\n I mean, it seems like through these meta learning concepts,\n\n19:56.160 --> 19:59.800\n get at the, what I think is one of the hardest\n\n19:59.800 --> 20:02.360\n and most important problems of AI,\n\n20:02.360 --> 20:04.040\n which is transfer learning.\n\n20:04.040 --> 20:06.280\n So it's generalization.\n\n20:06.280 --> 20:08.480\n How far along this journey\n\n20:08.480 --> 20:11.160\n towards building general systems are we?\n\n20:11.160 --> 20:13.600\n Being able to do transfer learning well.\n\n20:13.600 --> 20:17.520\n So there's some signs that you can generalize a little bit,\n\n20:17.520 --> 20:19.600\n but do you think we're on the right path\n\n20:19.600 --> 20:23.760\n or it's totally different breakthroughs are needed\n\n20:23.760 --> 20:26.800\n to be able to transfer knowledge\n\n20:26.800 --> 20:29.040\n between different learned models?\n\n20:31.240 --> 20:33.840\n Yeah, I'm pretty torn on this in that\n\n20:33.840 --> 20:35.560\n I think there are some very impressive.\n\n20:35.560 --> 20:40.520\n Well, there's just some very impressive results already.\n\n20:40.520 --> 20:44.040\n I mean, I would say when,\n\n20:44.040 --> 20:47.240\n even with the initial kind of big breakthrough in 2012\n\n20:47.240 --> 20:51.240\n with AlexNet, the initial thing is okay, great.\n\n20:52.160 --> 20:55.680\n This does better on ImageNet, hence image recognition.\n\n20:55.680 --> 20:57.840\n But then immediately thereafter,\n\n20:57.840 --> 21:00.520\n there was of course the notion that,\n\n21:00.520 --> 21:03.320\n wow, what was learned on ImageNet\n\n21:03.320 --> 21:05.000\n and you now wanna solve a new task,\n\n21:05.000 --> 21:07.760\n you can fine tune AlexNet for new tasks.\n\n21:09.080 --> 21:12.040\n And that was often found to be the even bigger deal\n\n21:12.040 --> 21:14.320\n that you learn something that was reusable,\n\n21:14.320 --> 21:16.040\n which was not often the case before.\n\n21:16.040 --> 21:17.520\n Usually machine learning, you learn something\n\n21:17.520 --> 21:19.320\n for one scenario and that was it.\n\n21:19.320 --> 21:20.280\n And that's really exciting.\n\n21:20.280 --> 21:22.280\n I mean, that's a huge application.\n\n21:22.280 --> 21:23.680\n That's probably the biggest success\n\n21:23.680 --> 21:27.920\n of transfer learning today in terms of scope and impact.\n\n21:27.920 --> 21:29.040\n That was a huge breakthrough.\n\n21:29.040 --> 21:33.040\n And then recently, I feel like similar kind of,\n\n21:33.040 --> 21:34.760\n by scaling things up, it seems like\n\n21:34.760 --> 21:36.160\n this has been expanded upon.\n\n21:36.160 --> 21:37.960\n Like people training even bigger networks,\n\n21:37.960 --> 21:39.480\n they might transfer even better.\n\n21:39.480 --> 21:41.200\n If you looked at, for example,\n\n21:41.200 --> 21:43.400\n some of the OpenAI results on language models\n\n21:43.400 --> 21:46.600\n and some of the recent Google results on language models,\n\n21:47.560 --> 21:51.040\n they're learned for just prediction\n\n21:51.040 --> 21:54.960\n and then they get reused for other tasks.\n\n21:54.960 --> 21:56.680\n And so I think there is something there\n\n21:56.680 --> 21:58.520\n where somehow if you train a big enough model\n\n21:58.520 --> 22:01.360\n on enough things, it seems to transfer\n\n22:01.360 --> 22:03.600\n some deep mind results that I thought were very impressive,\n\n22:03.600 --> 22:08.600\n the Unreal results, where it was learned to navigate mazes\n\n22:09.240 --> 22:11.240\n in ways where it wasn't just doing reinforcement learning,\n\n22:11.240 --> 22:14.280\n but it had other objectives it was optimizing for.\n\n22:14.280 --> 22:17.240\n So I think there's a lot of interesting results already.\n\n22:17.240 --> 22:22.240\n I think maybe where it's hard to wrap my head around this,\n\n22:22.520 --> 22:26.720\n to which extent or when do we call something generalization?\n\n22:26.720 --> 22:29.760\n Or the levels of generalization in the real world,\n\n22:29.760 --> 22:31.880\n or the levels of generalization involved\n\n22:31.880 --> 22:35.080\n in these different tasks, right?\n\n22:36.240 --> 22:39.280\n You draw this, by the way, just to frame things.\n\n22:39.280 --> 22:41.400\n I've heard you say somewhere, it's the difference\n\n22:41.400 --> 22:44.920\n between learning to master versus learning to generalize,\n\n22:44.920 --> 22:47.880\n that it's a nice line to think about.\n\n22:47.880 --> 22:50.920\n And I guess you're saying that it's a gray area\n\n22:50.920 --> 22:53.680\n of what learning to master and learning to generalize,\n\n22:53.680 --> 22:54.520\n where one starts.\n\n22:54.520 --> 22:56.120\n I think I might have heard this.\n\n22:56.120 --> 22:57.840\n I might have heard it somewhere else.\n\n22:57.840 --> 23:00.480\n And I think it might've been one of your interviews,\n\n23:00.480 --> 23:03.720\n maybe the one with Yoshua Benjamin, I'm not 100% sure.\n\n23:03.720 --> 23:08.440\n But I liked the example, I'm not sure who it was,\n\n23:08.440 --> 23:10.600\n but the example was essentially,\n\n23:10.600 --> 23:13.320\n if you use current deep learning techniques,\n\n23:13.320 --> 23:17.200\n what we're doing to predict, let's say,\n\n23:17.200 --> 23:22.200\n the relative motion of our planets, it would do pretty well.\n\n23:22.200 --> 23:27.200\n But then now if a massive new mass enters our solar system,\n\n23:28.440 --> 23:32.120\n it would probably not predict what will happen, right?\n\n23:32.120 --> 23:33.600\n And that's a different kind of generalization.\n\n23:33.600 --> 23:34.960\n That's a generalization that relies\n\n23:34.960 --> 23:38.560\n on the ultimate simplest, simplest explanation\n\n23:38.560 --> 23:40.240\n that we have available today\n\n23:40.240 --> 23:41.600\n to explain the motion of planets,\n\n23:41.600 --> 23:43.700\n whereas just pattern recognition could predict\n\n23:43.700 --> 23:47.320\n our current solar system motion pretty well, no problem.\n\n23:47.320 --> 23:48.880\n And so I think that's an example\n\n23:48.880 --> 23:52.440\n of a kind of generalization that is a little different\n\n23:52.440 --> 23:54.560\n from what we've achieved so far.\n\n23:54.560 --> 23:59.560\n And it's not clear if just regularizing more\n\n23:59.720 --> 24:01.840\n and forcing it to come up with a simpler, simpler,\n\n24:01.840 --> 24:03.840\n simpler explanation and say, look, this is not simple.\n\n24:03.840 --> 24:05.600\n But that's what physics researchers do, right?\n\n24:05.600 --> 24:08.220\n They say, can I make this even simpler?\n\n24:08.220 --> 24:09.440\n How simple can I get this?\n\n24:09.440 --> 24:12.400\n What's the simplest equation that can explain everything?\n\n24:12.400 --> 24:15.560\n The master equation for the entire dynamics of the universe,\n\n24:15.560 --> 24:17.600\n we haven't really pushed that direction as hard\n\n24:17.600 --> 24:19.480\n in deep learning, I would say.\n\n24:20.740 --> 24:22.040\n Not sure if it should be pushed,\n\n24:22.040 --> 24:24.560\n but it seems a kind of generalization you get from that\n\n24:24.560 --> 24:27.400\n that you don't get in our current methods so far.\n\n24:27.400 --> 24:30.040\n So I just talked to Vladimir Vapnik, for example,\n\n24:30.040 --> 24:34.200\n who's a statistician of statistical learning,\n\n24:34.200 --> 24:36.120\n and he kind of dreams of creating\n\n24:37.000 --> 24:41.080\n the E equals MC squared for learning, right?\n\n24:41.080 --> 24:42.460\n The general theory of learning.\n\n24:42.460 --> 24:44.640\n Do you think that's a fruitless pursuit\n\n24:44.640 --> 24:49.640\n in the near term, within the next several decades?\n\n24:51.800 --> 24:53.560\n I think that's a really interesting pursuit\n\n24:53.560 --> 24:58.040\n in the following sense, in that there is a lot of evidence\n\n24:58.040 --> 25:03.040\n that the brain is pretty modular.\n\n25:03.480 --> 25:05.520\n And so I wouldn't maybe think of it as the theory,\n\n25:05.520 --> 25:09.360\n maybe the underlying theory, but more kind of the principle\n\n25:10.700 --> 25:12.840\n where there have been findings where\n\n25:12.840 --> 25:16.600\n people who are blind will use the part of the brain\n\n25:16.600 --> 25:20.720\n usually used for vision for other functions.\n\n25:21.640 --> 25:24.720\n And even after some kind of,\n\n25:24.720 --> 25:26.440\n if people get rewired in some way,\n\n25:26.440 --> 25:28.700\n they might be able to reuse parts of their brain\n\n25:28.700 --> 25:30.400\n for other functions.\n\n25:30.400 --> 25:35.160\n And so what that suggests is some kind of modularity.\n\n25:35.160 --> 25:39.280\n And I think it is a pretty natural thing to strive for\n\n25:39.280 --> 25:41.720\n to see, can we find that modularity?\n\n25:41.720 --> 25:43.200\n Can we find this thing?\n\n25:43.200 --> 25:45.960\n Of course, every part of the brain is not exactly the same.\n\n25:45.960 --> 25:48.600\n Not everything can be rewired arbitrarily.\n\n25:48.600 --> 25:50.240\n But if you think of things like the neocortex,\n\n25:50.240 --> 25:52.300\n which is a pretty big part of the brain,\n\n25:52.300 --> 25:56.560\n that seems fairly modular from what the findings so far.\n\n25:56.560 --> 25:59.240\n Can you design something equally modular?\n\n25:59.240 --> 26:00.560\n And if you can just grow it,\n\n26:00.560 --> 26:02.520\n it becomes more capable probably.\n\n26:02.520 --> 26:04.940\n I think that would be the kind of interesting\n\n26:04.940 --> 26:09.400\n underlying principle to shoot for that is not unrealistic.\n\n26:09.400 --> 26:14.400\n Do you think you prefer math or empirical trial and error\n\n26:15.200 --> 26:17.560\n for the discovery of the essence of what it means\n\n26:17.560 --> 26:19.000\n to do something intelligent?\n\n26:19.000 --> 26:22.120\n So reinforcement learning embodies both groups, right?\n\n26:22.120 --> 26:26.400\n To prove that something converges, prove the bounds.\n\n26:26.400 --> 26:29.320\n And then at the same time, a lot of those successes are,\n\n26:29.320 --> 26:31.560\n well, let's try this and see if it works.\n\n26:31.560 --> 26:33.400\n So which do you gravitate towards?\n\n26:33.400 --> 26:39.920\n How do you think of those two parts of your brain?\n\n26:39.920 --> 26:44.560\n Maybe I would prefer we could make the progress\n\n26:44.560 --> 26:45.600\n with mathematics.\n\n26:45.600 --> 26:48.040\n And the reason maybe I would prefer that is because often\n\n26:48.040 --> 26:52.840\n if you have something you can mathematically formalize,\n\n26:52.840 --> 26:55.800\n you can leapfrog a lot of experimentation.\n\n26:55.800 --> 26:58.800\n And experimentation takes a long time to get through.\n\n26:58.800 --> 27:01.280\n And a lot of trial and error,\n\n27:01.280 --> 27:04.120\n kind of reinforcement learning, your research process,\n\n27:04.120 --> 27:05.560\n but you need to do a lot of trial and error\n\n27:05.560 --> 27:06.720\n before you get to a success.\n\n27:06.720 --> 27:08.520\n So if you can leapfrog that, to my mind,\n\n27:08.520 --> 27:10.480\n that's what the math is about.\n\n27:10.480 --> 27:13.280\n And hopefully once you do a bunch of experiments,\n\n27:13.280 --> 27:14.440\n you start seeing a pattern.\n\n27:14.440 --> 27:18.320\n You can do some derivations that leapfrog some experiments.\n\n27:18.320 --> 27:19.160\n But I agree with you.\n\n27:19.160 --> 27:21.360\n I mean, in practice, a lot of the progress has been such\n\n27:21.360 --> 27:23.680\n that we have not been able to find the math\n\n27:23.680 --> 27:25.120\n that allows you to leapfrog ahead.\n\n27:25.120 --> 27:28.100\n And we are kind of making gradual progress\n\n27:28.100 --> 27:30.440\n one step at a time, a new experiment here,\n\n27:30.440 --> 27:32.920\n a new experiment there that gives us new insights\n\n27:32.920 --> 27:34.400\n and gradually building up,\n\n27:34.400 --> 27:36.600\n but not getting to something yet where we're just,\n\n27:36.600 --> 27:39.120\n okay, here's an equation that now explains how,\n\n27:39.120 --> 27:40.560\n you know, that would be,\n\n27:40.560 --> 27:42.540\n have been two years of experimentation to get there,\n\n27:42.540 --> 27:45.440\n but this tells us what the result's going to be.\n\n27:45.440 --> 27:47.560\n Unfortunately, not so much yet.\n\n27:47.560 --> 27:50.200\n Not so much yet, but your hope is there.\n\n27:50.200 --> 27:53.680\n In trying to teach robots or systems\n\n27:53.680 --> 27:58.340\n to do everyday tasks or even in simulation,\n\n27:58.340 --> 28:01.860\n what do you think you're more excited about?\n\n28:02.740 --> 28:04.800\n Imitation learning or self play?\n\n28:04.800 --> 28:08.700\n So letting robots learn from humans\n\n28:08.700 --> 28:11.340\n or letting robots plan their own\n\n28:11.340 --> 28:13.880\n to try to figure out in their own way\n\n28:13.880 --> 28:18.320\n and eventually play, eventually interact with humans\n\n28:18.320 --> 28:20.180\n or solve whatever the problem is.\n\n28:20.180 --> 28:21.860\n What's the more exciting to you?\n\n28:21.860 --> 28:24.660\n What's more promising you think as a research direction?\n\n28:24.660 --> 28:29.660\n So when we look at self play,\n\n28:32.300 --> 28:34.900\n what's so beautiful about it is goes back\n\n28:34.900 --> 28:37.260\n to kind of the challenges in reinforcement learning.\n\n28:37.260 --> 28:38.460\n So the challenge of reinforcement learning\n\n28:38.460 --> 28:39.440\n is getting signal.\n\n28:40.580 --> 28:43.300\n And if you don't never succeed, you don't get any signal.\n\n28:43.300 --> 28:46.740\n In self play, you're on both sides.\n\n28:46.740 --> 28:48.020\n So one of you succeeds.\n\n28:48.020 --> 28:49.980\n And the beauty is also one of you fails.\n\n28:49.980 --> 28:51.100\n And so you see the contrast.\n\n28:51.100 --> 28:53.300\n You see the one version of me that did better\n\n28:53.300 --> 28:54.140\n than the other version.\n\n28:54.140 --> 28:57.260\n So every time you play yourself, you get signal.\n\n28:57.260 --> 29:00.100\n And so whenever you can turn something into self play,\n\n29:00.100 --> 29:02.080\n you're in a beautiful situation\n\n29:02.080 --> 29:04.820\n where you can naturally learn much more quickly\n\n29:04.820 --> 29:07.980\n than in most other reinforcement learning environments.\n\n29:07.980 --> 29:12.460\n So I think if somehow we can turn more\n\n29:12.460 --> 29:13.720\n reinforcement learning problems\n\n29:13.720 --> 29:15.500\n into self play formulations,\n\n29:15.500 --> 29:17.180\n that would go really, really far.\n\n29:17.180 --> 29:20.720\n So far, self play has been largely around games\n\n29:20.720 --> 29:22.820\n where there is natural opponents.\n\n29:22.820 --> 29:24.740\n But if we could do self play for other things,\n\n29:24.740 --> 29:25.580\n and let's say, I don't know,\n\n29:25.580 --> 29:26.940\n a robot learns to build a house.\n\n29:26.940 --> 29:28.380\n I mean, that's a pretty advanced thing\n\n29:28.380 --> 29:29.500\n to try to do for a robot,\n\n29:29.500 --> 29:31.900\n but maybe it tries to build a hut or something.\n\n29:31.900 --> 29:34.140\n If that can be done through self play,\n\n29:34.140 --> 29:35.420\n it would learn a lot more quickly\n\n29:35.420 --> 29:36.500\n if somebody can figure that out.\n\n29:36.500 --> 29:37.980\n And I think that would be something\n\n29:37.980 --> 29:41.560\n where it goes closer to kind of the mathematical leapfrogging\n\n29:41.560 --> 29:43.900\n where somebody figures out a formalism to say,\n\n29:43.900 --> 29:47.200\n okay, any RL problem by playing this and this idea,\n\n29:47.200 --> 29:48.700\n you can turn it into a self play problem\n\n29:48.700 --> 29:50.740\n where you get signal a lot more easily.\n\n29:50.740 --> 29:52.780\n Reality is, many problems we don't know\n\n29:52.780 --> 29:53.980\n how to turn into self play.\n\n29:53.980 --> 29:56.980\n And so either we need to provide detailed reward.\n\n29:56.980 --> 29:58.940\n That doesn't just reward for achieving a goal,\n\n29:58.940 --> 30:00.780\n but rewards for making progress,\n\n30:00.780 --> 30:02.660\n and that becomes time consuming.\n\n30:02.660 --> 30:03.900\n And once you're starting to do that,\n\n30:03.900 --> 30:05.060\n let's say you want a robot to do something,\n\n30:05.060 --> 30:07.180\n you need to give all this detailed reward.\n\n30:07.180 --> 30:09.340\n Well, why not just give a demonstration?\n\n30:09.340 --> 30:11.940\n Because why not just show the robot?\n\n30:11.940 --> 30:14.540\n And now the question is, how do you show the robot?\n\n30:14.540 --> 30:16.620\n One way to show is to tally operate the robot,\n\n30:16.620 --> 30:19.020\n and then the robot really experiences things.\n\n30:19.020 --> 30:21.140\n And that's nice, because that's really high signal\n\n30:21.140 --> 30:23.060\n to noise ratio data, and we've done a lot of that.\n\n30:23.060 --> 30:26.020\n And you teach your robot skills in just 10 minutes,\n\n30:26.020 --> 30:27.860\n you can teach your robot a new basic skill,\n\n30:27.860 --> 30:30.300\n like okay, pick up the bottle, place it somewhere else.\n\n30:30.300 --> 30:32.420\n That's a skill, no matter where the bottle starts,\n\n30:32.420 --> 30:34.940\n maybe it always goes onto a target or something.\n\n30:34.940 --> 30:38.100\n That's fairly easy to teach your robot with tally up.\n\n30:38.100 --> 30:40.340\n Now, what's even more interesting\n\n30:40.340 --> 30:41.380\n if you can now teach your robot\n\n30:41.380 --> 30:43.100\n through third person learning,\n\n30:43.100 --> 30:45.700\n where the robot watches you do something\n\n30:45.700 --> 30:48.500\n and doesn't experience it, but just kind of watches you.\n\n30:48.500 --> 30:49.820\n It doesn't experience it, but just watches it\n\n30:49.820 --> 30:52.180\n and says, okay, well, if you're showing me that,\n\n30:52.180 --> 30:53.800\n that means I should be doing this.\n\n30:53.800 --> 30:55.380\n And I'm not gonna be using your hand,\n\n30:55.380 --> 30:57.100\n because I don't get to control your hand,\n\n30:57.100 --> 30:59.540\n but I'm gonna use my hand, I do that mapping.\n\n30:59.540 --> 31:02.140\n And so that's where I think one of the big breakthroughs\n\n31:02.140 --> 31:03.340\n has happened this year.\n\n31:03.340 --> 31:05.540\n This was led by Chelsea Finn here.\n\n31:06.460 --> 31:08.280\n It's almost like learning a machine translation\n\n31:08.280 --> 31:11.340\n for demonstrations, where you have a human demonstration,\n\n31:11.340 --> 31:12.820\n and the robot learns to translate it\n\n31:12.820 --> 31:15.900\n into what it means for the robot to do it.\n\n31:15.900 --> 31:17.560\n And that was a meta learning formulation,\n\n31:17.560 --> 31:20.380\n learn from one to get the other.\n\n31:20.380 --> 31:23.020\n And that, I think, opens up a lot of opportunities\n\n31:23.020 --> 31:24.540\n to learn a lot more quickly.\n\n31:24.540 --> 31:26.580\n So my focus is on autonomous vehicles.\n\n31:26.580 --> 31:29.940\n Do you think this approach of third person watching,\n\n31:29.940 --> 31:31.980\n the autonomous driving is amenable\n\n31:31.980 --> 31:33.860\n to this kind of approach?\n\n31:33.860 --> 31:36.660\n So for autonomous driving,\n\n31:36.660 --> 31:41.580\n I would say third person is slightly easier.\n\n31:41.580 --> 31:43.460\n And the reason I'm gonna say it's slightly easier\n\n31:43.460 --> 31:46.620\n to do with third person is because\n\n31:46.620 --> 31:49.540\n the car dynamics are very well understood.\n\n31:49.540 --> 31:51.020\n So the...\n\n31:51.020 --> 31:53.980\n Easier than first person, you mean?\n\n31:53.980 --> 31:55.700\n Or easier than...\n\n31:55.700 --> 31:57.540\n So I think the distinction between third person\n\n31:57.540 --> 32:00.180\n and first person is not a very important distinction\n\n32:00.180 --> 32:01.840\n for autonomous driving.\n\n32:01.840 --> 32:03.460\n They're very similar.\n\n32:03.460 --> 32:06.100\n Because the distinction is really about\n\n32:06.100 --> 32:08.100\n who turns the steering wheel.\n\n32:09.180 --> 32:12.340\n Or maybe, let me put it differently.\n\n32:12.340 --> 32:14.860\n How to get from a point where you are now\n\n32:14.860 --> 32:17.440\n to a point, let's say, a couple meters in front of you.\n\n32:17.440 --> 32:19.240\n And that's a problem that's very well understood.\n\n32:19.240 --> 32:20.260\n And that's the only distinction\n\n32:20.260 --> 32:21.920\n between third and first person there.\n\n32:21.920 --> 32:23.220\n Whereas with the robot manipulation,\n\n32:23.220 --> 32:25.420\n interaction forces are very complex.\n\n32:25.420 --> 32:27.980\n And it's still a very different thing.\n\n32:27.980 --> 32:29.940\n For autonomous driving,\n\n32:29.940 --> 32:31.420\n I think there is still the question,\n\n32:31.420 --> 32:34.580\n imitation versus RL.\n\n32:34.580 --> 32:36.740\n So imitation gives you a lot more signal.\n\n32:36.740 --> 32:38.900\n I think where imitation is lacking\n\n32:38.900 --> 32:42.380\n and needs some extra machinery is,\n\n32:42.380 --> 32:45.460\n it doesn't, in its normal format,\n\n32:45.460 --> 32:48.580\n doesn't think about goals or objectives.\n\n32:48.580 --> 32:51.060\n And of course, there are versions of imitation learning\n\n32:51.060 --> 32:52.900\n and versus reinforcement learning type imitation learning\n\n32:52.900 --> 32:54.640\n which also thinks about goals.\n\n32:54.640 --> 32:57.100\n I think then we're getting much closer.\n\n32:57.100 --> 32:59.620\n But I think it's very hard to think of a\n\n32:59.620 --> 33:04.060\n fully reactive car, generalizing well.\n\n33:04.060 --> 33:05.960\n If it really doesn't have a notion of objectives\n\n33:05.960 --> 33:08.540\n to generalize well to the kind of general\n\n33:08.540 --> 33:09.500\n that you would want.\n\n33:09.500 --> 33:12.160\n You'd want more than just that reactivity\n\n33:12.160 --> 33:13.660\n that you get from just behavioral cloning\n\n33:13.660 --> 33:15.440\n slash supervised learning.\n\n33:17.100 --> 33:18.700\n So a lot of the work,\n\n33:19.560 --> 33:22.060\n whether it's self play or even imitation learning,\n\n33:22.060 --> 33:24.860\n would benefit significantly from simulation,\n\n33:24.860 --> 33:26.540\n from effective simulation.\n\n33:26.540 --> 33:27.580\n And you're doing a lot of stuff\n\n33:27.580 --> 33:29.660\n in the physical world and in simulation.\n\n33:29.660 --> 33:33.620\n Do you have hope for greater and greater\n\n33:33.620 --> 33:38.380\n power of simulation being boundless eventually\n\n33:38.380 --> 33:40.740\n to where most of what we need to operate\n\n33:40.740 --> 33:43.780\n in the physical world could be simulated\n\n33:43.780 --> 33:46.460\n to a degree that's directly transferable\n\n33:46.460 --> 33:47.580\n to the physical world?\n\n33:47.580 --> 33:49.620\n Or are we still very far away from that?\n\n33:51.660 --> 33:56.660\n So I think we could even rephrase that question\n\n33:57.780 --> 33:58.780\n in some sense.\n\n33:58.780 --> 34:00.360\n Please.\n\n34:00.360 --> 34:04.000\n And so the power of simulation, right?\n\n34:04.940 --> 34:06.580\n As simulators get better and better,\n\n34:06.580 --> 34:08.980\n of course, becomes stronger\n\n34:08.980 --> 34:11.260\n and we can learn more in simulation.\n\n34:11.260 --> 34:12.460\n But there's also another version\n\n34:12.460 --> 34:13.660\n which is where you say the simulator\n\n34:13.660 --> 34:15.900\n doesn't even have to be that precise.\n\n34:15.900 --> 34:18.660\n As long as it's somewhat representative\n\n34:18.660 --> 34:21.060\n and instead of trying to get one simulator\n\n34:21.060 --> 34:23.140\n that is sufficiently precise to learn in\n\n34:23.140 --> 34:25.300\n and transfer really well to the real world,\n\n34:25.300 --> 34:27.100\n I'm gonna build many simulators.\n\n34:27.100 --> 34:28.260\n Ensemble of simulators?\n\n34:28.260 --> 34:29.940\n Ensemble of simulators.\n\n34:29.940 --> 34:33.580\n Not any single one of them is sufficiently representative\n\n34:33.580 --> 34:36.740\n of the real world such that it would work\n\n34:36.740 --> 34:37.900\n if you train in there.\n\n34:37.900 --> 34:39.760\n But if you train in all of them,\n\n34:40.700 --> 34:43.600\n then there is something that's good in all of them.\n\n34:43.600 --> 34:47.620\n The real world will just be another one of them\n\n34:47.620 --> 34:49.700\n that's not identical to any one of them\n\n34:49.700 --> 34:50.940\n but just another one of them.\n\n34:50.940 --> 34:53.180\n Another sample from the distribution of simulators.\n\n34:53.180 --> 34:54.020\n Exactly.\n\n34:54.020 --> 34:54.860\n We do live in a simulation,\n\n34:54.860 --> 34:57.780\n so this is just one other one.\n\n34:57.780 --> 34:59.420\n I'm not sure about that, but yeah.\n\n35:01.580 --> 35:03.580\n It's definitely a very advanced simulator if it is.\n\n35:03.580 --> 35:05.700\n Yeah, it's a pretty good one.\n\n35:05.700 --> 35:07.660\n I've talked to Stuart Russell.\n\n35:07.660 --> 35:09.460\n It's something you think about a little bit too.\n\n35:09.460 --> 35:12.060\n Of course, you're really trying to build these systems,\n\n35:12.060 --> 35:13.780\n but do you think about the future of AI?\n\n35:13.780 --> 35:16.380\n A lot of people have concern about safety.\n\n35:16.380 --> 35:18.240\n How do you think about AI safety?\n\n35:18.240 --> 35:21.460\n As you build robots that are operating in the physical world,\n\n35:21.460 --> 35:25.060\n what is, yeah, how do you approach this problem\n\n35:25.060 --> 35:27.720\n in an engineering kind of way, in a systematic way?\n\n35:29.220 --> 35:32.340\n So when a robot is doing things,\n\n35:32.340 --> 35:36.240\n you kind of have a few notions of safety to worry about.\n\n35:36.240 --> 35:39.380\n One is that the robot is physically strong\n\n35:39.380 --> 35:42.340\n and of course could do a lot of damage.\n\n35:42.340 --> 35:44.840\n Same for cars, which we can think of as robots too\n\n35:44.840 --> 35:45.680\n in some way.\n\n35:46.780 --> 35:48.340\n And this could be completely unintentional.\n\n35:48.340 --> 35:51.780\n So it could be not the kind of longterm AI safety concerns\n\n35:51.780 --> 35:54.380\n that, okay, AI is smarter than us and now what do we do?\n\n35:54.380 --> 35:55.860\n But it could be just very practical.\n\n35:55.860 --> 35:58.920\n Okay, this robot, if it makes a mistake,\n\n35:58.920 --> 36:00.700\n what are the results going to be?\n\n36:00.700 --> 36:02.280\n Of course, simulation comes in a lot there\n\n36:02.280 --> 36:07.280\n to test in simulation. It's a difficult question.\n\n36:07.780 --> 36:09.540\n And I'm always wondering, like, I always wonder,\n\n36:09.540 --> 36:12.020\n let's say you look at, let's go back to driving\n\n36:12.020 --> 36:15.280\n because a lot of people know driving well, of course.\n\n36:15.280 --> 36:18.940\n What do we do to test somebody for driving, right?\n\n36:18.940 --> 36:21.420\n Get a driver's license. What do they really do?\n\n36:21.420 --> 36:26.420\n I mean, you fill out some tests and then you drive.\n\n36:26.660 --> 36:29.500\n And I mean, it's suburban California.\n\n36:29.500 --> 36:32.940\n That driving test is just you drive around the block,\n\n36:32.940 --> 36:36.500\n pull over, you do a stop sign successfully,\n\n36:36.500 --> 36:40.060\n and then you pull over again and you're pretty much done.\n\n36:40.060 --> 36:44.500\n And you're like, okay, if a self driving car did that,\n\n36:44.500 --> 36:46.840\n would you trust it that it can drive?\n\n36:46.840 --> 36:48.900\n And I'd be like, no, that's not enough for me to trust it.\n\n36:48.900 --> 36:51.540\n But somehow for humans, we've figured out\n\n36:51.540 --> 36:55.220\n that somebody being able to do that is representative\n\n36:55.220 --> 36:57.900\n of them being able to do a lot of other things.\n\n36:57.900 --> 36:59.980\n And so I think somehow for humans,\n\n36:59.980 --> 37:02.660\n we figured out representative tests\n\n37:02.660 --> 37:05.860\n of what it means if you can do this, what you can really do.\n\n37:05.860 --> 37:07.380\n Of course, testing humans,\n\n37:07.380 --> 37:09.180\n humans don't wanna be tested at all times.\n\n37:09.180 --> 37:10.300\n Self driving cars or robots\n\n37:10.300 --> 37:11.980\n could be tested more often probably.\n\n37:11.980 --> 37:13.460\n You can have replicas that get tested\n\n37:13.460 --> 37:14.820\n that are known to be identical\n\n37:14.820 --> 37:17.140\n because they use the same neural net and so forth.\n\n37:17.140 --> 37:21.260\n But still, I feel like we don't have this kind of unit tests\n\n37:21.260 --> 37:24.420\n or proper tests for robots.\n\n37:24.420 --> 37:25.520\n And I think there's something very interesting\n\n37:25.520 --> 37:26.780\n to be thought about there,\n\n37:26.780 --> 37:28.540\n especially as you update things.\n\n37:28.540 --> 37:29.580\n Your software improves,\n\n37:29.580 --> 37:32.320\n you have a better self driving car suite, you update it.\n\n37:32.320 --> 37:35.960\n How do you know it's indeed more capable on everything\n\n37:35.960 --> 37:37.280\n than what you had before,\n\n37:37.280 --> 37:41.500\n that you didn't have any bad things creep into it?\n\n37:41.500 --> 37:43.540\n So I think that's a very interesting direction of research\n\n37:43.540 --> 37:46.340\n that there is no real solution yet,\n\n37:46.340 --> 37:47.980\n except that somehow for humans we do.\n\n37:47.980 --> 37:50.820\n Because we say, okay, you have a driving test, you passed,\n\n37:50.820 --> 37:51.940\n you can go on the road now,\n\n37:51.940 --> 37:54.900\n and humans have accidents every like a million\n\n37:54.900 --> 37:57.860\n or 10 million miles, something pretty phenomenal\n\n37:57.860 --> 38:01.660\n compared to that short test that is being done.\n\n38:01.660 --> 38:06.100\n So let me ask, you've mentioned that Andrew Ng by example\n\n38:06.100 --> 38:08.000\n showed you the value of kindness.\n\n38:10.100 --> 38:14.580\n Do you think the space of policies,\n\n38:14.580 --> 38:17.500\n good policies for humans and for AI\n\n38:17.500 --> 38:22.500\n is populated by policies that with kindness\n\n38:22.500 --> 38:27.500\n or ones that are the opposite, exploitation, even evil?\n\n38:28.220 --> 38:30.300\n So if you just look at the sea of policies\n\n38:30.300 --> 38:32.540\n we operate under as human beings,\n\n38:32.540 --> 38:35.300\n or if AI system had to operate in this real world,\n\n38:35.300 --> 38:38.060\n do you think it's really easy to find policies\n\n38:38.060 --> 38:39.580\n that are full of kindness,\n\n38:39.580 --> 38:41.340\n like we naturally fall into them?\n\n38:41.340 --> 38:44.780\n Or is it like a very hard optimization problem?\n\n38:48.100 --> 38:50.300\n I mean, there is kind of two optimizations\n\n38:50.300 --> 38:52.300\n happening for humans, right?\n\n38:52.300 --> 38:54.140\n So for humans, there's kind of the very long term\n\n38:54.140 --> 38:56.900\n optimization which evolution has done for us\n\n38:56.900 --> 39:00.780\n and we're kind of predisposed to like certain things.\n\n39:00.780 --> 39:02.780\n And that's in some sense what makes our learning easier\n\n39:02.780 --> 39:05.420\n because I mean, we know things like pain\n\n39:05.420 --> 39:08.420\n and hunger and thirst.\n\n39:08.420 --> 39:10.100\n And the fact that we know about those\n\n39:10.100 --> 39:12.740\n is not something that we were taught, that's kind of innate.\n\n39:12.740 --> 39:14.060\n When we're hungry, we're unhappy.\n\n39:14.060 --> 39:16.220\n When we're thirsty, we're unhappy.\n\n39:16.220 --> 39:18.420\n When we have pain, we're unhappy.\n\n39:18.420 --> 39:21.760\n And ultimately evolution built that into us\n\n39:21.760 --> 39:22.600\n to think about those things.\n\n39:22.600 --> 39:24.660\n And so I think there is a notion that\n\n39:24.660 --> 39:28.220\n it seems somehow humans evolved in general\n\n39:28.220 --> 39:32.820\n to prefer to get along in some ways,\n\n39:32.820 --> 39:36.940\n but at the same time also to be very territorial\n\n39:36.940 --> 39:40.080\n and kind of centric to their own tribe.\n\n39:41.620 --> 39:43.580\n Like it seems like that's the kind of space\n\n39:43.580 --> 39:44.660\n we converged onto.\n\n39:44.660 --> 39:46.660\n I mean, I'm not an expert in anthropology,\n\n39:46.660 --> 39:49.260\n but it seems like we're very kind of good\n\n39:49.260 --> 39:52.860\n within our own tribe, but need to be taught\n\n39:52.860 --> 39:54.660\n to be nice to other tribes.\n\n39:54.660 --> 39:56.300\n Well, if you look at Steven Pinker,\n\n39:56.300 --> 39:58.100\n he highlights this pretty nicely in\n\n40:00.740 --> 40:02.340\n Better Angels of Our Nature,\n\n40:02.340 --> 40:04.980\n where he talks about violence decreasing over time\n\n40:04.980 --> 40:05.800\n consistently.\n\n40:05.800 --> 40:08.340\n So whatever tension, whatever teams we pick,\n\n40:08.340 --> 40:11.100\n it seems that the long arc of history\n\n40:11.100 --> 40:14.220\n goes towards us getting along more and more.\n\n40:14.220 --> 40:15.420\n So. I hope so.\n\n40:15.420 --> 40:20.420\n So do you think that, do you think it's possible\n\n40:20.620 --> 40:25.620\n to teach RL based robots this kind of kindness,\n\n40:26.180 --> 40:28.380\n this kind of ability to interact with humans,\n\n40:28.380 --> 40:32.860\n this kind of policy, even to, let me ask a fun one.\n\n40:32.860 --> 40:35.140\n Do you think it's possible to teach RL based robot\n\n40:35.140 --> 40:38.580\n to love a human being and to inspire that human\n\n40:38.580 --> 40:40.020\n to love the robot back?\n\n40:40.020 --> 40:45.020\n So to like RL based algorithm that leads to a happy marriage.\n\n40:47.540 --> 40:48.860\n That's an interesting question.\n\n40:48.860 --> 40:52.820\n Maybe I'll answer it with another question, right?\n\n40:52.820 --> 40:56.700\n Because I mean, but I'll come back to it.\n\n40:56.700 --> 40:58.940\n So another question you can have is okay.\n\n40:58.940 --> 41:03.560\n I mean, how close does some people's happiness get\n\n41:03.560 --> 41:07.620\n from interacting with just a really nice dog?\n\n41:07.620 --> 41:09.900\n Like, I mean, dogs, you come home,\n\n41:09.900 --> 41:10.740\n that's what dogs do.\n\n41:10.740 --> 41:12.660\n They greet you, they're excited,\n\n41:12.660 --> 41:14.700\n makes you happy when you come home to your dog.\n\n41:14.700 --> 41:16.460\n You're just like, okay, this is exciting.\n\n41:16.460 --> 41:18.340\n They're always happy when I'm here.\n\n41:18.340 --> 41:21.300\n And if they don't greet you, cause maybe whatever,\n\n41:21.300 --> 41:23.540\n your partner took them on a trip or something,\n\n41:23.540 --> 41:26.100\n you might not be nearly as happy when you get home, right?\n\n41:26.100 --> 41:30.260\n And so the kind of, it seems like the level of reasoning\n\n41:30.260 --> 41:32.200\n a dog has is pretty sophisticated,\n\n41:32.200 --> 41:35.660\n but then it's still not yet at the level of human reasoning.\n\n41:35.660 --> 41:37.840\n And so it seems like we don't even need to achieve\n\n41:37.840 --> 41:40.460\n human level reasoning to get like very strong affection\n\n41:40.460 --> 41:41.700\n with humans.\n\n41:41.700 --> 41:44.220\n And so my thinking is why not, right?\n\n41:44.220 --> 41:47.140\n Why couldn't, with an AI, couldn't we achieve\n\n41:47.140 --> 41:51.460\n the kind of level of affection that humans feel\n\n41:51.460 --> 41:56.080\n among each other or with friendly animals and so forth?\n\n41:57.480 --> 41:59.740\n So question, is it a good thing for us or not?\n\n41:59.740 --> 42:01.380\n That's another thing, right?\n\n42:01.380 --> 42:05.980\n Because I mean, but I don't see why not.\n\n42:05.980 --> 42:09.020\n Why not, yeah, so Elon Musk says love is the answer.\n\n42:09.020 --> 42:12.660\n Maybe he should say love is the objective function\n\n42:12.660 --> 42:14.700\n and then RL is the answer, right?\n\n42:14.700 --> 42:16.500\n Well, maybe.\n\n42:17.660 --> 42:18.880\n Oh, Peter, thank you so much.\n\n42:18.880 --> 42:20.260\n I don't want to take up more of your time.\n\n42:20.260 --> 42:21.900\n Thank you so much for talking today.\n\n42:21.900 --> 42:23.500\n Well, thanks for coming by.\n\n42:23.500 --> 42:44.500\n Great to have you visit.\n\n"
}