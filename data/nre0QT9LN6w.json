{
  "title": "Rodney Brooks: Robotics | Lex Fridman Podcast #217",
  "id": "nre0QT9LN6w",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:06.400\n The following is a conversation with Rodney Brooks, one of the greatest roboticists in history.\n\n00:06.400 --> 00:10.640\n He led the Computer Science and Artificial Intelligence Laboratory at MIT,\n\n00:10.640 --> 00:16.000\n then cofounded iRobot, which is one of the most successful robotics companies ever.\n\n00:16.560 --> 00:22.560\n Then he cofounded Rethink Robotics that created some amazing collaborative robots like Baxter\n\n00:22.560 --> 00:30.640\n and Sawyer. Finally, he cofounded Robust.ai, whose mission is to teach robots common sense,\n\n00:30.640 --> 00:35.120\n which is a lot harder than it sounds. To support this podcast,\n\n00:35.120 --> 00:37.360\n please check out our sponsors in the description.\n\n00:38.160 --> 00:43.920\n As a side note, let me say that Rodney is someone I've looked up to for many years in my now over\n\n00:43.920 --> 00:52.080\n two decade journey in robotics because, one, he's a legit great engineer of real world systems,\n\n00:52.080 --> 00:57.600\n and two, he's not afraid to state controversial opinions that challenge the way we see the AI\n\n00:57.600 --> 01:04.240\n world. But of course, while I agree with him on some of his critical views of AI, I don't agree\n\n01:04.240 --> 01:10.640\n with some others, and he's fully supportive of such disagreement. Nobody ever built anything great\n\n01:10.640 --> 01:16.960\n by being fully agreeable. There's always respect and love behind our interactions, and when a\n\n01:16.960 --> 01:22.560\n conversation is recorded like it was for this podcast, I think a little bit of disagreement is\n\n01:22.560 --> 01:29.520\n fun. This is the Lex Friedman Podcast, and here is my conversation with Rodney Brooks.\n\n01:31.760 --> 01:37.040\n What is the most amazing or beautiful robot that you've ever had the chance to work with?\n\n01:37.600 --> 01:43.760\n I think it was Domo, which was made by one of my grad students, Aaron Edsinger. It now sits in\n\n01:43.760 --> 01:50.720\n Daniela Russo's office, director of CSAIL, and it was just a beautiful robot. Aaron was really\n\n01:50.720 --> 01:56.240\n clever. He didn't give me a budget ahead of time. He didn't tell me what he was going to do.\n\n01:56.240 --> 02:02.960\n He just started spending money. He spent a lot of money. He and Jeff Weber, who is a mechanical\n\n02:02.960 --> 02:08.640\n engineer who Aaron insisted he bring with him when he became a grad student, built this beautiful,\n\n02:08.640 --> 02:17.040\n gorgeous robot, Domo, which is an upper torso humanoid, two arms with fingers, three fingered\n\n02:17.040 --> 02:26.000\n hands, and face eyeballs. Not the eyeballs, but everything else, series elastic actuators.\n\n02:26.880 --> 02:33.760\n You can interact with it. Cable driven. All the motors are inside, and it's just gorgeous.\n\n02:33.760 --> 02:35.680\n The eyeballs are actuated too, or no?\n\n02:35.680 --> 02:40.000\n Oh yeah, the eyeballs are actuated with cameras, so it had a visual attention mechanism,\n\n02:41.280 --> 02:46.240\n looking when people came in and looking in their face and talking with them.\n\n02:46.240 --> 02:47.200\n Wow, was it amazing?\n\n02:48.000 --> 02:48.800\n The beauty of it.\n\n02:49.600 --> 02:51.040\n You said what was the most beautiful?\n\n02:51.040 --> 02:52.160\n What is the most beautiful?\n\n02:52.160 --> 02:55.600\n It's just mechanically gorgeous. As everything Aaron builds,\n\n02:55.600 --> 02:59.520\n there's always been mechanically gorgeous. It's just exquisite in the detail.\n\n03:00.400 --> 03:04.400\n We're talking about mechanically, like literally the amount of actuators.\n\n03:04.400 --> 03:10.080\n The actuators, the cables, he anodizes different parts, different colors,\n\n03:10.080 --> 03:13.200\n and it just looks like a work of art.\n\n03:13.200 --> 03:16.720\n What about the face? Do you find the face beautiful in robots?\n\n03:17.760 --> 03:23.120\n When you make a robot, it's making a promise for how well it will be able to interact,\n\n03:23.120 --> 03:26.800\n so I always encourage my students not to overpromise.\n\n03:27.680 --> 03:31.840\n Even with its essence, like the thing it presents, it should not overpromise.\n\n03:31.840 --> 03:37.200\n Yeah, so the joke I make, which I think you'll get, is if your robot looks like Albert Einstein,\n\n03:37.200 --> 03:39.440\n it should be as smart as Albert Einstein.\n\n03:39.440 --> 03:47.520\n So the only thing in Domo's face is the eyeballs, because that's all it can do.\n\n03:47.520 --> 03:49.200\n It can look at you and pay attention.\n\n03:52.640 --> 03:58.240\n It's not like one of those Japanese robots that looks exactly like a person at all.\n\n03:58.240 --> 04:06.160\n But see, the thing is, us humans and dogs, too, don't just use eyes as attentional mechanisms.\n\n04:06.160 --> 04:09.440\n They also use it to communicate, as part of the communication.\n\n04:09.440 --> 04:12.880\n Like a dog can look at you, look at another thing, and look back at you,\n\n04:12.880 --> 04:15.840\n and that designates that we're going to be looking at that thing together.\n\n04:15.840 --> 04:21.200\n Yeah, or intent, you know, on both Baxter and Sawyer at Rethink Robotics,\n\n04:21.200 --> 04:25.440\n they had a screen with, you know, graphic eyes,\n\n04:25.440 --> 04:31.200\n so it wasn't actually where the cameras were pointing, but the eyes would look in the direction\n\n04:31.200 --> 04:36.160\n it was about to move its arm, so people in the factory nearby were not surprised by its motions,\n\n04:36.160 --> 04:39.040\n because it gave that intent away.\n\n04:39.840 --> 04:45.120\n Before we talk about Baxter, which I think is a beautiful robot, let's go back to the beginning.\n\n04:45.120 --> 04:48.560\n When did you first fall in love with robotics?\n\n04:48.560 --> 04:50.880\n We're talking about beauty and love to open the conversation.\n\n04:50.880 --> 04:51.440\n This is great.\n\n04:51.440 --> 04:56.960\n I was born in the end of 1954, and I grew up in Adelaide, South Australia,\n\n04:57.520 --> 05:05.120\n and I have these two books that are dated 1961, so I'm guessing my mother found them in a store\n\n05:05.120 --> 05:08.560\n in 62 or 63, How and Why Wonder Books.\n\n05:09.600 --> 05:15.680\n How and Why Wonder Book of Electricity, and a How and Why Wonder Book of Giant Brains and Robots.\n\n05:15.680 --> 05:23.200\n And I learned how to build circuits, you know, when I was eight or nine, simple circuits,\n\n05:23.200 --> 05:31.680\n and I read, you know, learned the binary system, and saw all these drawings, mostly, of robots,\n\n05:31.680 --> 05:34.720\n and then I tried to build them for the rest of my childhood.\n\n05:36.080 --> 05:38.400\n Wait, 61, you said?\n\n05:38.400 --> 05:41.200\n This was when the two books, I've still got them at home.\n\n05:41.200 --> 05:43.520\n What does the robot mean in that context?\n\n05:43.520 --> 05:51.600\n Some of the robots that they had were arms, you know, big arms to move nuclear material around,\n\n05:51.600 --> 05:57.600\n but they had pictures of welding robots that looked like humans under the sea, welding stuff\n\n05:57.600 --> 05:58.100\n underwater.\n\n05:59.040 --> 06:04.480\n So they weren't real robots, but they were, you know, what people were thinking about for robots.\n\n06:05.200 --> 06:06.560\n What were you thinking about?\n\n06:06.560 --> 06:07.920\n Were you thinking about humanoids?\n\n06:07.920 --> 06:09.760\n Were you thinking about arms with fingers?\n\n06:09.760 --> 06:12.080\n Were you thinking about faces or colors?\n\n06:12.080 --> 06:14.000\n Were you thinking about faces or cars?\n\n06:14.000 --> 06:19.360\n No, actually, to be honest, I realized my limitation on building mechanical stuff.\n\n06:19.360 --> 06:26.400\n So I just built the brains, mostly, out of different technologies as I got older.\n\n06:28.320 --> 06:35.040\n I built a learning system which was chemical based, and I had this ice cube tray.\n\n06:35.040 --> 06:42.400\n Each well was a cell, and by applying voltage to the two electrodes, it would build up a\n\n06:42.400 --> 06:43.040\n copper bridge.\n\n06:43.040 --> 06:50.000\n So over time, it would learn a simple network so I could teach it stuff.\n\n06:50.000 --> 07:00.160\n And mostly, things were driven by my budget, and nails as electrodes and an ice cube tray\n\n07:00.160 --> 07:02.160\n was about my budget at that stage.\n\n07:02.160 --> 07:07.520\n Later, I managed to buy transistors, and I could build gates and flip flops and stuff.\n\n07:07.520 --> 07:11.040\n So one of your first robots was an ice cube tray?\n\n07:11.040 --> 07:15.120\n Yeah, it was very cerebral because it learned to add.\n\n07:16.720 --> 07:17.220\n Very nice.\n\n07:17.920 --> 07:26.080\n Well, just a decade or so before, in 1950, Alan Turing wrote a paper that formulated\n\n07:26.080 --> 07:32.400\n the Turing Test, and he opened that paper with the question, can machines think?\n\n07:32.400 --> 07:34.160\n So let me ask you this question.\n\n07:34.160 --> 07:36.160\n Can machines think?\n\n07:36.160 --> 07:39.120\n Can your ice cube tray one day think?\n\n07:40.800 --> 07:44.720\n Certainly, machines can think because I believe you're a machine, and I'm a machine, and I\n\n07:44.720 --> 07:45.680\n believe we both think.\n\n07:46.640 --> 07:51.360\n I think any other philosophical position is sort of a little ludicrous.\n\n07:51.360 --> 07:53.760\n What does think mean if it's not something that we do?\n\n07:53.760 --> 07:54.960\n And we are machines.\n\n07:56.160 --> 08:00.880\n So yes, machines can, but do we have a clue how to build such machines?\n\n08:00.880 --> 08:02.480\n That's a very different question.\n\n08:02.480 --> 08:05.680\n Are we capable of building such machines?\n\n08:05.680 --> 08:06.720\n Are we smart enough?\n\n08:06.720 --> 08:10.000\n We think we're smart enough to do anything, but maybe we're not.\n\n08:10.000 --> 08:14.160\n Maybe we're just not smart enough to build stuff like us.\n\n08:14.160 --> 08:18.720\n The kind of computer that Alan Turing was thinking about, do you think there is something\n\n08:18.720 --> 08:25.040\n fundamentally or significantly different between the computer between our ears, the biological\n\n08:25.040 --> 08:31.200\n computer that humans use, and the computer that he was thinking about from a sort of\n\n08:31.200 --> 08:33.280\n high level philosophical?\n\n08:33.280 --> 08:36.480\n Yeah, I believe that it's very wrong.\n\n08:36.480 --> 08:44.160\n In fact, I'm halfway through a, I think it'll be about a 480 page book, the working title\n\n08:44.160 --> 08:45.440\n is Not Even Wrong.\n\n08:45.440 --> 08:48.080\n And if I may, I'll tell you a bit about that book.\n\n08:48.080 --> 08:48.720\n Yes, please.\n\n08:48.720 --> 08:51.360\n So there's two, well, three thrusts to it.\n\n08:52.720 --> 08:56.160\n One is the history of computation, what we call computation.\n\n08:56.160 --> 09:03.760\n It goes all the way back to some manuscripts in Latin from 1614 and 1620 by Napier and\n\n09:03.760 --> 09:06.640\n Kepler through Babbage and Lovelace.\n\n09:06.640 --> 09:16.640\n And then Turing's 1936 paper is what we think of as the invention of modern computation.\n\n09:17.360 --> 09:23.120\n And that paper, by the way, did not set out to invent computation.\n\n09:23.120 --> 09:29.680\n It set out to negatively answer one of Hilbert's three later set of problems.\n\n09:29.680 --> 09:38.560\n He called it an effective way of getting answers.\n\n09:38.560 --> 09:49.360\n And Hilbert really worked with rewriting rules, as did Church, who also, at the same time,\n\n09:49.360 --> 09:54.880\n a month earlier than Turing, disproved Hilbert's one of these three hypotheses.\n\n09:54.880 --> 09:57.360\n The other two had already been disproved by G\u00f6del.\n\n09:57.360 --> 10:01.680\n Turing set out to disprove it, because it's always easier to disprove these things than\n\n10:01.680 --> 10:03.280\n to prove that there is an answer.\n\n10:04.160 --> 10:12.880\n And so he needed, and it really came from his professor while I was an undergrad at\n\n10:12.880 --> 10:16.400\n Cambridge, who turned it into, is there a mechanical process?\n\n10:16.400 --> 10:23.840\n So he wanted to show a mechanical process that could calculate numbers, because that\n\n10:23.840 --> 10:27.760\n was a mechanical process that people used to generate tables.\n\n10:27.760 --> 10:30.800\n They were called computers, the people at the time.\n\n10:30.800 --> 10:35.360\n And they followed a set of rules where they had paper, and they would write numbers down,\n\n10:35.360 --> 10:38.480\n and based on the numbers, they'd keep writing other numbers.\n\n10:39.040 --> 10:46.800\n And they would produce numbers for these tables, engineering tables, that the more iterations\n\n10:46.800 --> 10:48.960\n they did, the more significant digits came out.\n\n10:48.960 --> 10:56.960\n And so Turing, in that paper, set out to define what sort of machine could do that, mechanical\n\n10:56.960 --> 11:04.320\n machine, where it could produce an arbitrary number of digits in the same way a human computer\n\n11:04.320 --> 11:04.880\n did.\n\n11:06.720 --> 11:13.600\n And he came up with a very simple set of constraints where there was an infinite supply\n\n11:13.600 --> 11:14.320\n of paper.\n\n11:14.320 --> 11:22.320\n This is the tape of the Turing machine, and each Turing machine came with a set of instructions\n\n11:22.320 --> 11:27.920\n that, as a person, could do with pencil and paper, write down things on the tape and erase\n\n11:27.920 --> 11:29.200\n them and put new things there.\n\n11:30.000 --> 11:36.560\n And he was able to show that that system was not able to do something that Hilbert had\n\n11:36.560 --> 11:38.800\n hypothesized, so he disproved it.\n\n11:38.800 --> 11:47.120\n But he had to show that this system was good enough to do whatever could be done, but couldn't\n\n11:47.120 --> 11:48.400\n do this other thing.\n\n11:48.400 --> 11:53.840\n And there he said, and he says in the paper, I don't have any real arguments for this,\n\n11:53.840 --> 11:55.200\n but based on intuition.\n\n11:55.840 --> 11:58.080\n So that's how he defined computation.\n\n11:58.080 --> 12:05.440\n And then if you look over the next, from 1936 up until really around 1975, you see people\n\n12:05.440 --> 12:09.200\n struggling with, is this really what computation is?\n\n12:10.000 --> 12:17.200\n And so Marvin Minsky, very well known in AI, but also a fantastic mathematician, in his\n\n12:17.200 --> 12:22.400\n book Finite and Infant Machines from the mid-'60s, which is a beautiful, beautiful mathematical\n\n12:22.400 --> 12:26.720\n book, says at the start of the book, well, what is computation?\n\n12:26.720 --> 12:29.520\n Turing says it's this, and yeah, I sort of think it's that.\n\n12:29.520 --> 12:32.240\n It doesn't really matter whether the stuff's made of wood or plastic.\n\n12:32.240 --> 12:36.320\n It's just that relatively cheap stuff can do this stuff.\n\n12:36.320 --> 12:39.200\n And so yeah, seems like computation.\n\n12:40.160 --> 12:48.880\n And Donald Knuth, in his first volume of his Art of Computer Programming in around 1968,\n\n12:49.440 --> 12:51.600\n says, well, what's computation?\n\n12:52.320 --> 12:57.200\n It's this stuff, like Turing says, that a person could do each step without too much\n\n12:57.200 --> 12:57.600\n trouble.\n\n12:57.600 --> 13:03.600\n And so one of his examples of what would be too much trouble was a step which required\n\n13:03.600 --> 13:08.160\n knowing whether Fermat's Last Theorem was true or not, because it was not known at the\n\n13:08.160 --> 13:08.800\n time.\n\n13:08.800 --> 13:11.120\n And that's too much trouble for a person to do as a step.\n\n13:12.160 --> 13:18.080\n And Hopcroft and Ullman sort of said a similar thing later that year.\n\n13:18.080 --> 13:20.960\n And by 1975, in the A.H.O.\n\n13:20.960 --> 13:24.880\n Hopcroft and Ullman book, they're saying, well, you know, we don't really know what\n\n13:24.880 --> 13:30.400\n computation is, but intuition says this is sort of about right, and this is what it is.\n\n13:31.280 --> 13:32.400\n That's computation.\n\n13:32.400 --> 13:39.280\n It's a sort of agreed upon thing which happens to be really easy to implement in silicon.\n\n13:39.280 --> 13:43.920\n And then we had Moore's Law, which took off, and it's been an incredibly powerful tool.\n\n13:44.640 --> 13:46.080\n I certainly wouldn't argue with that.\n\n13:46.080 --> 13:49.440\n The version we have of computation, incredibly powerful.\n\n13:49.440 --> 13:51.440\n Can we just take a pause?\n\n13:51.440 --> 13:55.440\n So what we're talking about is there's an infinite tape with some simple rules of how\n\n13:55.440 --> 13:59.120\n to write on that tape, and that's what we're kind of thinking about.\n\n13:59.120 --> 14:00.080\n This is computation.\n\n14:00.080 --> 14:03.200\n Yeah, and it's modeled after humans, how humans do stuff.\n\n14:03.200 --> 14:09.040\n And I think it's, Turing says in the 36th paper, one of the critical facts here is that\n\n14:09.040 --> 14:11.120\n a human has a limited amount of memory.\n\n14:11.920 --> 14:15.280\n So that's what we're going to put onto our mechanical computers.\n\n14:15.280 --> 14:19.680\n So, you know, I'm like mass.\n\n14:19.680 --> 14:26.240\n I'm like mass or charge or, you know, it's not given by the universe.\n\n14:26.240 --> 14:28.640\n It was, this is what we're going to call computation.\n\n14:29.200 --> 14:33.600\n And then it has this really, you know, it had this really good implementation, which\n\n14:33.600 --> 14:36.800\n has completely changed our technological world.\n\n14:36.800 --> 14:37.680\n That's computation.\n\n14:40.400 --> 14:48.880\n Second part of the book, or argument in the book, I have this two by two matrix with science.\n\n14:48.880 --> 14:56.080\n In the top row, engineering in the bottom row, left column is intelligence, right column\n\n14:56.080 --> 14:56.640\n is life.\n\n14:58.000 --> 15:02.800\n So in the bottom row, the engineering, there's artificial intelligence and artificial life.\n\n15:03.440 --> 15:07.520\n In the top row, there's neuroscience and abiogenesis.\n\n15:07.520 --> 15:09.920\n How does living matter turn in?\n\n15:09.920 --> 15:12.000\n How does nonliving matter become living matter?\n\n15:12.720 --> 15:14.000\n Four disciplines.\n\n15:14.000 --> 15:22.160\n These four disciplines all came into the current form in the period 1945 to 1965.\n\n15:24.000 --> 15:24.880\n That's interesting.\n\n15:24.880 --> 15:28.480\n There was neuroscience before, but it wasn't effective neuroscience.\n\n15:28.480 --> 15:32.160\n It was, you know, there were these ganglia and there's electrical charges, but no one\n\n15:32.160 --> 15:33.040\n knows what to do with it.\n\n15:33.680 --> 15:37.360\n And furthermore, there are a lot of players who are common across them.\n\n15:38.000 --> 15:43.360\n I've identified common players except for artificial intelligence and abiogenesis.\n\n15:43.360 --> 15:47.200\n I don't have, but for any other pair, I can point to people who work them.\n\n15:47.200 --> 15:51.840\n And a whole bunch of them, by the way, were at the research lab for electronics at MIT\n\n15:53.200 --> 15:56.880\n where Warren McCulloch held forth.\n\n15:58.240 --> 16:06.400\n In fact, McCulloch, Pitts, Letvin, and Maturana wrote the first paper on functional neuroscience\n\n16:06.400 --> 16:10.480\n called What the Frog's Eye Tells the Frog's Brain, where instead of it just being this\n\n16:10.480 --> 16:17.680\n bunch of nerves, they sort of showed what different anatomical components were doing\n\n16:17.680 --> 16:23.920\n and telling other anatomical components and, you know, generating behavior in the frog.\n\n16:23.920 --> 16:29.840\n Would you put them as basically the fathers or one of the early pioneers of what are now\n\n16:29.840 --> 16:31.440\n called artificial neural networks?\n\n16:33.120 --> 16:35.120\n Yeah, I mean, McCulloch and Pitts.\n\n16:36.560 --> 16:38.880\n Pitts was a much younger than him.\n\n16:38.880 --> 16:48.240\n In 1943, had written a paper inspired by Bertrand Russell on a calculus for the ideas eminent\n\n16:48.240 --> 16:56.080\n in neural systems where they had tried to, without any real proof, they had tried to\n\n16:56.080 --> 17:03.280\n give a formalism for neurons basically in terms of logic and gates or gates and not\n\n17:03.280 --> 17:09.120\n gates with no real evidence that that was what was going on, but they talked about it\n\n17:09.120 --> 17:16.160\n and that was picked up by Minsky for his 1953 dissertation on, which was a neural\n\n17:16.160 --> 17:17.760\n network, we call it today.\n\n17:18.400 --> 17:26.640\n It was picked up by John von Neumann when he was designing the Edbeck computer in 1945.\n\n17:26.640 --> 17:31.680\n He talked about its components being neurons based on, and in references, he's only got\n\n17:31.680 --> 17:34.160\n three references and one of them is the McCulloch Pitts paper.\n\n17:35.600 --> 17:40.000\n So all these people and then the AI people and the artificial life people, which was\n\n17:40.000 --> 17:44.560\n John von Neumann originally, there's like overlap between all, they're all going around\n\n17:44.560 --> 17:45.440\n the same time.\n\n17:45.440 --> 17:50.640\n And three of these four disciplines turned to computation as their primary metaphor.\n\n17:51.760 --> 17:54.480\n So I've got a couple of chapters in the book.\n\n17:54.480 --> 17:58.480\n One is titled, wait, computers are people?\n\n17:58.480 --> 18:00.800\n Because that's where our computers came from.\n\n18:00.800 --> 18:01.920\n Yeah.\n\n18:01.920 --> 18:05.280\n And, you know, from people who were computing stuff.\n\n18:05.280 --> 18:08.960\n And then I've got another chapter, wait, people are computers?\n\n18:08.960 --> 18:10.880\n Which is about computational neuroscience.\n\n18:10.880 --> 18:11.360\n Yeah.\n\n18:11.360 --> 18:13.120\n So there's this whole circle here.\n\n18:14.160 --> 18:16.560\n And that computation is it.\n\n18:16.560 --> 18:21.760\n And, you know, I have talked to people about, well, maybe it's not computation that goes\n\n18:21.760 --> 18:22.960\n on in the head.\n\n18:22.960 --> 18:24.160\n Of course it is.\n\n18:24.160 --> 18:24.480\n Yeah.\n\n18:24.480 --> 18:30.800\n Okay, well, when Elon Musk's rocket goes up, is it computing?\n\n18:31.520 --> 18:32.800\n Is that how it gets into orbit?\n\n18:32.800 --> 18:33.520\n By computing?\n\n18:34.080 --> 18:37.920\n But we've got this idea, if you want to build an AI system, you write a computer program.\n\n18:39.840 --> 18:46.480\n Yeah, so the word computation very quickly starts doing a lot of work that it was not\n\n18:46.480 --> 18:48.640\n initially intended to do.\n\n18:48.640 --> 18:53.280\n It's the second and same if you talk about the universe as essentially performing a\n\n18:53.280 --> 18:53.760\n computation.\n\n18:53.760 --> 18:54.320\n Yeah, right.\n\n18:54.320 --> 18:55.280\n Wolfram does this.\n\n18:55.280 --> 18:57.200\n He turns it into computation.\n\n18:57.200 --> 18:59.360\n You don't turn rockets into computation.\n\n18:59.360 --> 18:59.840\n Yeah.\n\n18:59.840 --> 19:04.640\n By the way, when you say computation in our conversation, do you tend to think of computation\n\n19:04.640 --> 19:07.200\n narrowly in the way Turing thought of computation?\n\n19:08.000 --> 19:14.080\n It's gotten very, you know, squishy.\n\n19:14.080 --> 19:14.400\n Yeah.\n\n19:14.400 --> 19:14.880\n Squishy.\n\n19:17.680 --> 19:22.640\n But computation in the way Turing thinks about it and the way most people think about it\n\n19:22.640 --> 19:28.400\n actually fits very well with thinking like a hunter gatherer.\n\n19:29.440 --> 19:34.000\n There are places and there can be stuff in places and the stuff in places can change\n\n19:34.000 --> 19:36.160\n and it stays there until someone changes it.\n\n19:37.120 --> 19:44.880\n And it's this metaphor of place and container, which, you know, is a combination of our place\n\n19:44.880 --> 19:48.160\n cells in our hippocampus and our cortex.\n\n19:48.160 --> 19:52.240\n But this is how we use metaphors for mostly to think about.\n\n19:52.240 --> 19:57.120\n And when we get outside of our metaphor range, we have to invent tools which we can sort\n\n19:57.120 --> 19:58.960\n of switch on to use.\n\n19:58.960 --> 20:01.360\n So calculus is an example of a tool.\n\n20:01.360 --> 20:06.640\n It can do stuff that our raw reasoning can't do, and we've got conventions of when you\n\n20:06.640 --> 20:07.840\n can use it or not.\n\n20:08.480 --> 20:15.280\n But sometimes, you know, people try to all the time, we always try to get physical metaphors\n\n20:15.280 --> 20:21.040\n for things, which is why quantum mechanics has been such a problem for a hundred years.\n\n20:21.040 --> 20:22.080\n Because it's a particle.\n\n20:22.080 --> 20:22.880\n No, it's a wave.\n\n20:22.880 --> 20:24.640\n It's got to be something we understand.\n\n20:24.640 --> 20:29.040\n And I say, no, it's some weird mathematical logic that's different from those, but we\n\n20:29.040 --> 20:30.080\n want that metaphor.\n\n20:30.720 --> 20:35.680\n Well, you know, I suspect that, you know, a hundred years or 200 years from now, neither\n\n20:35.680 --> 20:39.920\n quantum mechanics nor dark matter will be talked about in the same terms, you know,\n\n20:39.920 --> 20:44.320\n in the same way that Flogerson's theory eventually went away.\n\n20:44.320 --> 20:49.440\n Because it just wasn't an adequate explanatory metaphor, you know.\n\n20:49.440 --> 20:56.000\n That metaphor was the stuff, there is stuff in the burning, the burning is in the matter.\n\n20:56.000 --> 20:59.120\n As it turns out, the burning was outside the matter, it was the oxygen.\n\n20:59.840 --> 21:05.440\n So our desire for metaphor and combined with our limited cognitive capabilities gets us\n\n21:05.440 --> 21:06.400\n into trouble.\n\n21:06.400 --> 21:08.320\n That's my argument in this book.\n\n21:08.320 --> 21:10.080\n Now, and people say, well, what is it then?\n\n21:10.080 --> 21:12.720\n And I say, well, I wish I knew that, right, the book about that.\n\n21:12.720 --> 21:14.640\n But I, you know, I give some ideas.\n\n21:14.640 --> 21:17.440\n But so there's the three things.\n\n21:17.440 --> 21:19.920\n Computation is sort of a particular thing we use.\n\n21:22.880 --> 21:26.320\n Oh, can I tell you one beautiful thing, one beautiful thing I found?\n\n21:26.320 --> 21:30.000\n So, you know, I used an example of a thing that's different from computation.\n\n21:30.000 --> 21:35.520\n You hit a drum and it vibrates, and there are some stationary points on the drum surface,\n\n21:35.520 --> 21:37.840\n you know, because the waves are going up and down the stationary points.\n\n21:37.840 --> 21:45.760\n Now, you could compute them to arbitrary precision, but the drum just knows them.\n\n21:45.760 --> 21:46.960\n The drum doesn't have to compute.\n\n21:47.760 --> 21:51.200\n What was the very first computer program ever written by Ada Lovelace?\n\n21:51.920 --> 21:56.240\n To compute Bernoulli numbers, and the Bernoulli numbers are exactly what you need to find those\n\n21:56.240 --> 21:58.320\n stable points in the drum surface.\n\n21:58.320 --> 21:58.820\n Wow.\n\n21:59.520 --> 22:01.120\n And there was a bug in the program.\n\n22:03.280 --> 22:06.400\n The arguments to divide were, I don't know, I don't know.\n\n22:06.400 --> 22:09.120\n The arguments to divide were reversed in one place.\n\n22:10.000 --> 22:11.040\n And it still worked?\n\n22:11.040 --> 22:12.560\n Well, no, she's never got to run it.\n\n22:12.560 --> 22:14.000\n They never built the analytical engine.\n\n22:14.000 --> 22:16.240\n She wrote the program without it, you know.\n\n22:19.040 --> 22:21.040\n So the computation?\n\n22:21.040 --> 22:26.160\n Computation is sort of, you know, a thing that's become dominant as a metaphor, but\n\n22:27.200 --> 22:28.240\n is it the right metaphor?\n\n22:29.680 --> 22:33.360\n All three of these four fields adopted computation.\n\n22:33.360 --> 22:40.480\n And, you know, a lot of it swirls around Warren McCulloch and all his students, and he funded\n\n22:40.480 --> 22:41.120\n a lot of people.\n\n22:45.600 --> 22:49.360\n And our human metaphors, our limitations to human thinking, all play into this.\n\n22:50.000 --> 22:51.680\n Those are the three themes of the book.\n\n22:52.720 --> 22:54.880\n So I have a little to say about computation.\n\n22:54.880 --> 23:05.040\n So you're saying that there is a gap between the computer or the machine that performs\n\n23:05.040 --> 23:13.360\n computation and this machine that appears to have consciousness and intelligence.\n\n23:13.360 --> 23:16.080\n Yeah, that piece of meat in your head.\n\n23:16.080 --> 23:16.800\n Piece of meat.\n\n23:16.800 --> 23:20.720\n And maybe it's not just the meat in your head, it's the rest of you too.\n\n23:20.720 --> 23:24.960\n I mean, you actually have a neural system in your gut.\n\n23:24.960 --> 23:31.040\n I tend to also believe, not believe, but we're now dancing around things we don't know, but\n\n23:31.680 --> 23:35.280\n I tend to believe other humans are important.\n\n23:36.560 --> 23:42.080\n Like, so we're almost like, I just don't think we would ever have achieved the level\n\n23:42.080 --> 23:44.160\n of intelligence we have with other humans.\n\n23:44.880 --> 23:49.680\n I'm not saying so confidently, but I have an intuition that some of the intelligence\n\n23:49.680 --> 23:51.200\n is in the interaction.\n\n23:51.200 --> 24:00.240\n Yeah, and I think it seems to be very likely, again, this is speculation, but we, our species,\n\n24:00.240 --> 24:06.800\n and probably neanderthals to some extent, because you can find old bones where they\n\n24:06.800 --> 24:14.320\n seem to be counting on them by putting notches that were neanderthals, we are able to put\n\n24:15.360 --> 24:18.400\n some of our stuff outside our body into the world.\n\n24:18.400 --> 24:19.840\n And then other people can share it.\n\n24:20.400 --> 24:22.960\n And then we get these tools that become shared tools.\n\n24:22.960 --> 24:30.240\n And so there's a whole coupling that would not occur in the single deep learning network,\n\n24:30.240 --> 24:32.800\n which was fed all of literature or something.\n\n24:33.840 --> 24:38.320\n Yeah, the neural network can't step outside of itself.\n\n24:38.320 --> 24:46.640\n But is there some, can we explore this dark room a little bit and try to get at something?\n\n24:46.640 --> 24:47.840\n What is the magic?\n\n24:47.840 --> 24:51.840\n Where does the magic come from in the human brain that creates the mind?\n\n24:52.480 --> 24:58.880\n What's your sense as scientists that try to understand it and try to build it?\n\n24:58.880 --> 25:04.240\n What are the directions it followed might be productive?\n\n25:04.240 --> 25:06.560\n Is it creative, interactive robots?\n\n25:07.040 --> 25:13.440\n Is it creating large deep neural networks that do like self supervised learning and\n\n25:13.440 --> 25:18.800\n just like we'll discover that when you make something large enough, some interesting things\n\n25:18.800 --> 25:19.840\n will emerge?\n\n25:19.840 --> 25:23.600\n Is it through physics and chemistry, biology, like artificial life angle?\n\n25:23.600 --> 25:28.240\n Like we'll sneak up in this four quadrant matrix that you mentioned.\n\n25:28.240 --> 25:33.440\n Is there anything you're most, if you had to bet all your money, financial?\n\n25:33.440 --> 25:34.160\n I wouldn't.\n\n25:35.040 --> 25:40.960\n So every intelligence we know, animal intelligence, dog intelligence,\n\n25:40.960 --> 25:48.400\n octopus intelligence, which is a very different sort of architecture from us.\n\n25:49.920 --> 25:59.520\n All the intelligences we know perceive the world in some way and then have action in\n\n25:59.520 --> 26:11.520\n the world, but they're able to perceive objects in a way which is actually pretty damn phenomenal\n\n26:11.520 --> 26:12.320\n and surprising.\n\n26:13.200 --> 26:22.000\n We tend to think that the box over here between us, which is a sound box, I think is a blue\n\n26:22.000 --> 26:30.560\n box, but blueness is something that we construct with color constancy.\n\n26:32.560 --> 26:37.120\n The blueness is not a direct function of the photons we're receiving.\n\n26:37.120 --> 26:47.600\n It's actually context, which is why you can turn, maybe seeing the examples where someone\n\n26:47.600 --> 26:53.520\n turns a stop sign into some other sort of sign by just putting a couple of marks on\n\n26:53.520 --> 26:55.280\n them and the deep learning system gets it wrong.\n\n26:55.280 --> 26:57.600\n And everyone says, but the stop sign's red.\n\n26:58.160 --> 26:59.920\n Why is it thinking it's the other sort of sign?\n\n26:59.920 --> 27:02.800\n Because redness is not intrinsic in just the photons.\n\n27:02.800 --> 27:07.120\n It's actually a construction of an understanding of the whole world and the relationship between\n\n27:07.120 --> 27:09.840\n objects to get color constancy.\n\n27:11.040 --> 27:15.760\n But our tendency, in order that we get an archive paper really quickly, is you just\n\n27:15.760 --> 27:18.880\n show a lot of data and give the labels and hope it figures it out.\n\n27:18.880 --> 27:21.040\n But it's not figuring it out in the same way we do.\n\n27:21.040 --> 27:24.720\n We have a very complex perceptual understanding of the world.\n\n27:24.720 --> 27:28.000\n Dogs have a very different perceptual understanding based on smell.\n\n27:28.000 --> 27:34.880\n They go smell a post, they can tell how many different dogs have visited it in the last\n\n27:34.880 --> 27:36.320\n 10 hours and how long ago.\n\n27:36.320 --> 27:39.440\n There's all sorts of stuff that we just don't perceive about the world.\n\n27:39.440 --> 27:42.400\n And just taking a single snapshot is not perceiving about the world.\n\n27:42.400 --> 27:48.400\n It's not seeing the registration between us and the object.\n\n27:48.400 --> 27:52.160\n And registration is a philosophical concept.\n\n27:52.160 --> 27:54.560\n Brian Cantwell Smith talks about it a lot.\n\n27:54.560 --> 27:58.640\n Very difficult, squirmy thing to understand.\n\n27:59.200 --> 28:02.080\n But I think none of our systems do that.\n\n28:02.080 --> 28:06.000\n We've always talked in AI about the symbol grounding problem, how our symbols that we\n\n28:06.000 --> 28:07.440\n talk about are grounded in the world.\n\n28:08.080 --> 28:12.320\n And when deep learning came along and started labeling images, people said, ah, the grounding\n\n28:12.320 --> 28:13.440\n problem has been solved.\n\n28:13.440 --> 28:18.800\n No, the labeling problem was solved with some percentage accuracy, which is different from\n\n28:18.800 --> 28:19.760\n the grounding problem.\n\n28:20.560 --> 28:28.880\n So you agree with Hans Marvick and what's called the Marvick's paradox that highlights\n\n28:28.880 --> 28:38.720\n this counterintuitive notion that reasoning is easy, but perception and mobility are hard.\n\n28:39.440 --> 28:39.840\n Yeah.\n\n28:39.840 --> 28:45.360\n We shared an office when I was working on computer vision and he was working on his\n\n28:45.360 --> 28:46.640\n first mobile robot.\n\n28:46.640 --> 28:48.400\n What were those conversations like?\n\n28:48.400 --> 28:49.040\n They were great.\n\n28:50.160 --> 28:55.440\n So do you still kind of, maybe you can elaborate, do you still believe this kind of notion that\n\n28:56.160 --> 28:59.600\n perception is really hard?\n\n28:59.600 --> 29:04.080\n Like, can you make sense of why we humans have this poor intuition about what's hard\n\n29:04.080 --> 29:04.480\n and not?\n\n29:04.480 --> 29:10.640\n Well, let me give us sort of another story.\n\n29:10.640 --> 29:10.880\n Sure.\n\n29:11.520 --> 29:21.680\n If you go back to the original teams working on AI from the late 50s into the 60s, and\n\n29:21.680 --> 29:27.760\n you go to the AI lab at MIT, who was it that was doing that?\n\n29:27.760 --> 29:32.480\n It was a bunch of really smart kids who got into MIT and they were intelligent.\n\n29:32.480 --> 29:34.160\n So what's intelligence about?\n\n29:34.160 --> 29:39.760\n Well, the stuff they were good at, playing chess, doing integrals, that was hard stuff.\n\n29:40.480 --> 29:45.680\n But, you know, a baby could see stuff, that wasn't intelligent, anyone could do that,\n\n29:45.680 --> 29:46.800\n that's not intelligence.\n\n29:47.280 --> 29:52.480\n And so, you know, there was this intuition that the hard stuff is the things they were\n\n29:52.480 --> 29:56.800\n good at and the easy stuff was the stuff that everyone could do.\n\n29:57.440 --> 29:57.760\n Yeah.\n\n29:57.760 --> 30:00.880\n And maybe I'm overplaying it a little bit, but I think there's an element of that.\n\n30:00.880 --> 30:08.480\n Yeah, I mean, I don't know how much truth there is to, like chess, for example, was\n\n30:08.480 --> 30:14.080\n for the longest time seen as the highest level of intellect, right?\n\n30:14.720 --> 30:17.200\n Until we got computers that were better at it than people.\n\n30:17.200 --> 30:21.120\n And then we realized, you know, if you go back to the 90s, you'll see, you know, the\n\n30:21.120 --> 30:26.320\n stories in the press around when Kasparov was beaten by Deep Blue.\n\n30:26.320 --> 30:28.320\n Oh, this is the end of all sorts of things.\n\n30:28.320 --> 30:30.640\n Computers are going to be able to do anything from now on.\n\n30:30.640 --> 30:35.120\n And we saw exactly the same stories with Alpha Zero, the Go Playing program.\n\n30:36.160 --> 30:36.660\n Yeah.\n\n30:37.280 --> 30:40.640\n But still, to me, reasoning is a special thing.\n\n30:41.200 --> 30:41.920\n And perhaps...\n\n30:41.920 --> 30:44.640\n No, actually, we're really bad at reasoning.\n\n30:44.640 --> 30:48.400\n We just use these analogies based on our hunter gatherer intuitions.\n\n30:48.400 --> 30:53.520\n But why is that not, don't you think the ability to construct metaphor is a really powerful\n\n30:53.520 --> 30:53.920\n thing?\n\n30:53.920 --> 30:54.400\n Oh, yeah, it is.\n\n30:54.400 --> 30:55.200\n Tell stories.\n\n30:55.200 --> 30:55.520\n It is.\n\n30:55.520 --> 31:00.960\n It's the constructing the metaphor and registering that something constant in our brains.\n\n31:00.960 --> 31:04.000\n Like, isn't that what we're doing with vision too?\n\n31:04.000 --> 31:06.080\n And we're telling our stories.\n\n31:06.080 --> 31:07.840\n We're constructing good models of the world.\n\n31:08.560 --> 31:09.760\n Yeah, yeah.\n\n31:09.760 --> 31:16.400\n But I think we jumped between what we're capable of and how we're doing it right there.\n\n31:16.400 --> 31:21.680\n It was a little confusion that went on as we were telling each other stories.\n\n31:21.680 --> 31:22.400\n Yes, exactly.\n\n31:23.440 --> 31:24.800\n Trying to delude each other.\n\n31:24.800 --> 31:27.280\n No, I just think I'm not exactly so.\n\n31:27.280 --> 31:29.200\n I'm trying to pull apart this Moravec's paradox.\n\n31:30.160 --> 31:31.520\n I don't view it as a paradox.\n\n31:33.280 --> 31:36.000\n What did evolution spend its time on?\n\n31:36.000 --> 31:36.320\n Yes.\n\n31:36.320 --> 31:39.360\n It spent its time on getting us to perceive and move in the world.\n\n31:39.360 --> 31:43.600\n That was 600 million years as multi cell creatures doing that.\n\n31:43.600 --> 31:53.120\n And then it was relatively recent that we were able to hunt or gather or even animals hunting.\n\n31:53.120 --> 31:54.960\n That's much more recent.\n\n31:54.960 --> 32:02.960\n And then anything that we, speech, language, those things are a couple of hundred thousand\n\n32:02.960 --> 32:05.760\n years probably, if that long.\n\n32:05.760 --> 32:08.240\n And then agriculture, 10,000 years.\n\n32:09.520 --> 32:13.760\n All that stuff was built on top of those earlier things, which took a long time to develop.\n\n32:14.320 --> 32:20.160\n So if you then look at the engineering of these things, so building it into robots,\n\n32:20.160 --> 32:22.000\n what's the hardest part of robotics?\n\n32:22.000 --> 32:29.920\n Do you think as the decades that you worked on robots in the context of what we're talking\n\n32:29.920 --> 32:37.520\n about, vision, perception, the actual sort of the biomechanics of movement, I'm kind\n\n32:37.520 --> 32:40.160\n of drawing parallels here between humans and machines always.\n\n32:40.800 --> 32:43.360\n Like what do you think is the hardest part of robotics?\n\n32:44.320 --> 32:45.920\n I just want to think all of them.\n\n32:45.920 --> 32:49.360\n I just want to think all of them.\n\n32:49.360 --> 32:51.280\n There are no easy parts to do well.\n\n32:53.040 --> 32:55.600\n We sort of go reductionist and we reduce it.\n\n32:55.600 --> 33:00.320\n If only we had all the location of all the points in 3D, things would be great.\n\n33:02.400 --> 33:07.440\n If only we had labels on the images, things would be great.\n\n33:07.440 --> 33:10.640\n But as we see, that's not good enough.\n\n33:10.640 --> 33:13.040\n Some deeper understanding.\n\n33:13.040 --> 33:20.960\n But if I came to you and I could solve one category of problems in robotics instantly,\n\n33:21.680 --> 33:24.000\n what would give you the greatest pleasure?\n\n33:28.160 --> 33:36.400\n I mean, you look at robots that manipulate objects, what's hard about that?\n\n33:36.400 --> 33:43.040\n You know, is it the perception, is it the reasoning about the world, that common sense\n\n33:43.040 --> 33:48.720\n reasoning, is it the actual building a robot that's able to interact with the world?\n\n33:49.680 --> 33:54.960\n Is it like human aspects of a robot that's interacting with humans in that game theory\n\n33:54.960 --> 33:56.080\n of how they work well together?\n\n33:56.080 --> 34:00.000\n Well, let's talk about manipulation for a second because I had this really blinding\n\n34:00.000 --> 34:05.360\n moment, you know, I'm a grandfather, so grandfathers have blinding moments.\n\n34:05.360 --> 34:15.680\n Just three or four miles from here, last year, my 16 month old grandson was in his new house\n\n34:16.240 --> 34:17.600\n for the first time, right?\n\n34:18.240 --> 34:19.200\n First time in this house.\n\n34:19.760 --> 34:25.040\n And he'd never been able to get to a window before, but this had some low windows.\n\n34:25.040 --> 34:29.360\n And he goes up to this window with a handle on it that he's never seen before.\n\n34:29.360 --> 34:34.800\n And he's got one hand pushing the window and the other hand turning the handle to open\n\n34:34.800 --> 34:35.300\n the window.\n\n34:36.640 --> 34:42.960\n He knew two different hands, two different things he knew how to put together.\n\n34:44.080 --> 34:45.520\n And he's 16 months old.\n\n34:45.520 --> 34:47.040\n And there you are watching in awe.\n\n34:51.840 --> 34:55.200\n In an environment he'd never seen before, a mechanism he'd never seen.\n\n34:55.200 --> 34:56.320\n How did he do that?\n\n34:56.320 --> 34:57.600\n Yes, that's a good question.\n\n34:57.600 --> 34:58.880\n How did he do that?\n\n34:58.880 --> 34:59.380\n That's why.\n\n34:59.380 --> 35:05.700\n It's like, okay, like you could see the leap of genius from using one hand to perform a\n\n35:05.700 --> 35:11.460\n task to combining, doing, I mean, first of all, in manipulation, that's really difficult.\n\n35:11.460 --> 35:15.300\n It's like two hands, both necessary to complete the action.\n\n35:15.940 --> 35:16.820\n And completely different.\n\n35:16.820 --> 35:25.140\n And he'd never seen a window open before, but he inferred somehow handle open something.\n\n35:25.140 --> 35:31.140\n Yeah, there may have been a lot of slightly different failure cases that you didn't see.\n\n35:32.180 --> 35:36.020\n Not with a window, but with other objects of turning and twisting and handles.\n\n35:37.540 --> 35:42.900\n There's a great counter to reinforcement learning.\n\n35:42.900 --> 35:48.740\n We'll just give the robot plenty of time to try everything.\n\n35:50.260 --> 35:52.260\n Can I tell a little side story here?\n\n35:52.260 --> 36:01.940\n Yeah, so I'm in DeepMind in London, this is three, four years ago, where there's a big\n\n36:01.940 --> 36:06.020\n Google building, and then you go inside and you go through this more security, and then\n\n36:06.020 --> 36:09.060\n you get to DeepMind where the other Google employees can't go.\n\n36:09.060 --> 36:15.540\n And I'm in a conference room, a conference room with some of the people, and they tell\n\n36:15.540 --> 36:23.940\n me about their reinforcement learning experiment with robots, which are just trying stuff out.\n\n36:23.940 --> 36:25.380\n And they're my robots.\n\n36:25.380 --> 36:26.900\n They're Sawyer's.\n\n36:26.900 --> 36:27.540\n We sold them.\n\n36:29.060 --> 36:33.300\n And they really like them because Sawyer's are compliant and can sense forces, so they\n\n36:33.300 --> 36:35.620\n don't break when they're bashing into walls.\n\n36:36.180 --> 36:37.700\n They stop and they do all this stuff.\n\n36:38.980 --> 36:42.580\n So you just let the robot do stuff, and eventually it figures stuff out.\n\n36:42.580 --> 36:47.380\n By the way, Sawyer, we're talking about robot manipulation, so robot arms and so on.\n\n36:47.380 --> 36:48.580\n Yeah, Sawyer's a robot.\n\n36:50.180 --> 36:51.220\n What's Sawyer?\n\n36:51.220 --> 36:55.140\n Sawyer's a robot arm that my company Rethink Robotics built.\n\n36:55.140 --> 36:56.580\n Thank you for the context.\n\n36:56.580 --> 36:57.060\n Sorry.\n\n36:57.060 --> 36:57.540\n Okay, cool.\n\n36:57.540 --> 36:58.420\n So we're in DeepMind.\n\n36:59.380 --> 37:04.100\n And it's in the next room, these robots are just bashing around to try and use reinforcement\n\n37:04.100 --> 37:05.300\n learning to learn how to act.\n\n37:05.940 --> 37:06.740\n Can I go see them?\n\n37:06.740 --> 37:07.780\n Oh no, they're secret.\n\n37:08.340 --> 37:09.300\n They were my robots.\n\n37:09.300 --> 37:10.020\n They were secret.\n\n37:10.820 --> 37:11.700\n That's hilarious.\n\n37:11.700 --> 37:12.100\n Okay.\n\n37:12.100 --> 37:17.860\n Anyway, the point is, you know, this idea that you just let reinforcement learning figure\n\n37:17.860 --> 37:21.060\n everything out is so counter to how a kid does stuff.\n\n37:21.780 --> 37:24.740\n So again, story about my grandson.\n\n37:24.740 --> 37:29.220\n I gave him this box that had lots of different lock mechanisms.\n\n37:29.780 --> 37:34.260\n He didn't randomly, you know, and he was 18 months old, he didn't randomly try to touch\n\n37:34.260 --> 37:35.940\n every surface or push everything.\n\n37:35.940 --> 37:42.020\n He found he could see where the mechanism was, and he started exploring the mechanism\n\n37:42.020 --> 37:43.940\n for each of these different lock mechanisms.\n\n37:44.580 --> 37:48.100\n And there was reinforcement, no doubt, of some sort going on there.\n\n37:48.660 --> 37:54.100\n But he applied a pre filter, which cut down the search space dramatically.\n\n37:55.540 --> 37:59.140\n I wonder to what level we're able to introspect what's going on.\n\n37:59.700 --> 38:03.780\n Because what's also possible is you have something like reinforcement learning going\n\n38:03.780 --> 38:05.860\n on in the mind in the space of imagination.\n\n38:05.860 --> 38:10.900\n So like you have a good model of the world you're predicting and you may be running those\n\n38:10.900 --> 38:16.820\n tens of thousands of like loops, but you're like, as a human, you're just looking at yourself\n\n38:16.820 --> 38:18.740\n trying to tell a story of what happened.\n\n38:18.740 --> 38:24.500\n And it might seem simple, but maybe there's a lot of computation going on.\n\n38:24.500 --> 38:28.020\n Whatever it is, but there's also a mechanism that's being built up.\n\n38:28.020 --> 38:30.420\n It's not just random search.\n\n38:30.420 --> 38:33.780\n Yeah, that mechanism prunes it dramatically.\n\n38:33.780 --> 38:40.980\n Yeah, that pruning, that pruning stuff, but it doesn't, it's possible that that's, so\n\n38:40.980 --> 38:45.620\n you don't think that's akin to a neural network inside a reinforcement learning algorithm.\n\n38:46.740 --> 38:47.700\n Is it possible?\n\n38:49.140 --> 38:52.340\n It's, yeah, until it's possible.\n\n38:52.340 --> 39:01.380\n It's possible, but I'll be incredibly surprised if that happens.\n\n39:01.380 --> 39:06.020\n I'll also be incredibly surprised that after all the decades that I've been doing this,\n\n39:06.020 --> 39:09.540\n where every few years someone thinks, now we've got it.\n\n39:10.100 --> 39:10.820\n Now we've got it.\n\n39:12.580 --> 39:15.620\n Four or five years ago, I was saying, I don't think we've got it yet.\n\n39:15.620 --> 39:18.820\n And everyone was saying, you don't understand how powerful AI is.\n\n39:18.820 --> 39:22.420\n I had people tell me, you don't understand how powerful it is.\n\n39:22.420 --> 39:30.420\n I sort of had a track record of what the world had done to think, well, this is no different\n\n39:30.420 --> 39:31.460\n from before.\n\n39:31.460 --> 39:33.060\n Or we have bigger computers.\n\n39:33.060 --> 39:35.940\n We had bigger computers in the 90s and we could do more stuff.\n\n39:37.940 --> 39:43.380\n But okay, so let me push back because I'm generally sort of optimistic and try to find\n\n39:43.380 --> 39:44.260\n the beauty in things.\n\n39:44.260 --> 39:51.860\n I think there's a lot of surprising and beautiful things that neural networks, this new generation\n\n39:51.860 --> 39:57.460\n of deep learning revolution has revealed to me, has continually been very surprising\n\n39:57.460 --> 39:59.300\n the kind of things it's able to do.\n\n39:59.300 --> 40:03.140\n Now, generalizing that over saying like this, we've solved intelligence.\n\n40:03.140 --> 40:05.220\n That's another big leap.\n\n40:05.220 --> 40:10.500\n But is there something surprising and beautiful to you about neural networks that were actually\n\n40:10.500 --> 40:14.500\n you said back and said, I did not expect this?\n\n40:16.100 --> 40:22.260\n Oh, I think their performance on ImageNet was shocking.\n\n40:22.260 --> 40:26.340\n The computer vision in those early days was just very like, wow, okay.\n\n40:26.340 --> 40:32.500\n That doesn't mean that they're solving everything in computer vision we need to solve or in\n\n40:32.500 --> 40:33.700\n vision for robots.\n\n40:33.700 --> 40:37.220\n What about AlphaZero and self play mechanisms and reinforcement learning?\n\n40:37.220 --> 40:39.300\n Yeah, that was all in the 90s.\n\n40:39.300 --> 40:42.740\n Yeah, that was all in Donald Mickey's 1961 paper.\n\n40:44.020 --> 40:47.540\n Everything that was there, which introduced reinforcement learning.\n\n40:48.340 --> 40:49.300\n No, but come on.\n\n40:49.300 --> 40:52.020\n So no, you're talking about the actual techniques.\n\n40:52.020 --> 40:57.140\n But isn't it surprising to you the level it's able to achieve with no human supervision\n\n40:58.740 --> 40:59.700\n of chess play?\n\n40:59.700 --> 41:05.860\n Like, to me, there's a big, big difference between Deep Blue and...\n\n41:05.860 --> 41:11.860\n Maybe what that's saying is how overblown our view of ourselves is.\n\n41:13.140 --> 41:14.740\n You know, the chess is easy.\n\n41:16.740 --> 41:28.340\n Yeah, I mean, I came across this 1946 report that, and I'd seen this as a kid in one of\n\n41:28.340 --> 41:30.340\n those books that my mother had given me actually.\n\n41:30.340 --> 41:39.060\n The 1946 report, which pitted someone with an abacus against an electronic calculator,\n\n41:39.620 --> 41:42.500\n and he beat the electronic calculator.\n\n41:42.500 --> 41:48.980\n You know, so there at that point was, well, humans are still better than machines at calculating.\n\n41:48.980 --> 41:54.420\n Are you surprised today that a machine can, you know, do a billion floating point operations\n\n41:54.420 --> 41:58.500\n a second and, you know, you're puzzling for minutes through one?\n\n41:58.500 --> 42:07.460\n I mean, I don't know, but I am certainly surprised there's something, to me, different about\n\n42:07.460 --> 42:10.420\n learning, so a system that's able to learn.\n\n42:10.420 --> 42:10.980\n Learning.\n\n42:10.980 --> 42:13.700\n See, now you're getting into one of the deadly sins.\n\n42:15.300 --> 42:19.220\n Because of using terms overly broadly.\n\n42:19.220 --> 42:21.700\n Yeah, I mean, there's so many different forms of learning.\n\n42:21.700 --> 42:22.260\n Yeah.\n\n42:22.260 --> 42:23.300\n So many different forms.\n\n42:23.300 --> 42:24.980\n You know, I learned my way around the city.\n\n42:24.980 --> 42:26.500\n I learned to play chess.\n\n42:26.500 --> 42:28.580\n I learned Latin.\n\n42:28.580 --> 42:30.100\n I learned to ride a bicycle.\n\n42:30.100 --> 42:33.700\n All of those are, you know, very different capabilities.\n\n42:33.700 --> 42:34.180\n Yeah.\n\n42:34.180 --> 42:41.860\n And if someone, you know, has a, you know, in the old days, people would write a paper\n\n42:41.860 --> 42:43.220\n about learning something.\n\n42:43.220 --> 42:52.580\n Now the corporate press office puts out a press release about how Company X is leading\n\n42:52.580 --> 42:56.820\n the world because they have a system that can...\n\n42:56.820 --> 42:58.180\n Yeah, but here's the thing.\n\n42:58.180 --> 42:58.500\n Okay.\n\n42:58.500 --> 42:59.460\n So what is learning?\n\n43:00.100 --> 43:00.820\n When I refer to...\n\n43:00.820 --> 43:02.420\n Learning is many things.\n\n43:02.420 --> 43:02.580\n But...\n\n43:02.580 --> 43:04.660\n It's a suitcase word.\n\n43:04.660 --> 43:12.980\n It's a suitcase word, but loosely, there's a dumb system, and over time, it becomes smart.\n\n43:13.700 --> 43:16.340\n Well, it becomes less dumb at the thing that it's doing.\n\n43:16.340 --> 43:19.140\n Smart is a loaded word.\n\n43:19.140 --> 43:21.220\n Yes, less dumb at the thing it's doing.\n\n43:21.220 --> 43:27.060\n It gets better performance under some measure, under some set of conditions at that thing.\n\n43:27.060 --> 43:35.780\n And most of these learning algorithms, learning systems, fail when you change the conditions\n\n43:35.780 --> 43:37.940\n just a little bit in a way that humans don't.\n\n43:37.940 --> 43:45.940\n So I was at DeepMind, the AlphaGo had just come out, and I said, what would have happened\n\n43:45.940 --> 43:49.940\n if you'd given it a 21 by 21 board instead of a 19 by 19 board?\n\n43:49.940 --> 43:51.620\n They said, fail totally.\n\n43:51.620 --> 43:55.540\n But a human player would actually be able to play.\n\n43:55.540 --> 44:02.980\n And actually, funny enough, if you look at DeepMind's work since then, they're presenting\n\n44:02.980 --> 44:07.620\n a lot of algorithms that would do well at the bigger board.\n\n44:07.620 --> 44:10.340\n So they're slowly expanding this generalization.\n\n44:10.340 --> 44:12.580\n I mean, to me, there's a core element there.\n\n44:12.580 --> 44:20.100\n I think it is very surprising to me that even in a constrained game of chess or Go, that\n\n44:20.100 --> 44:27.620\n through self play, by a system playing itself, that it can achieve superhuman level performance\n\n44:28.580 --> 44:29.940\n through learning alone.\n\n44:29.940 --> 44:38.980\n Okay, so you didn't like it when I referred to Donald Mickey's 1961 paper.\n\n44:38.980 --> 44:46.020\n There, in the second part of it, which came a year later, they had self play on an electronic\n\n44:46.020 --> 44:52.180\n computer at tic tac toe, okay, but it learned to play tic tac toe through self play.\n\n44:52.180 --> 44:54.580\n And it learned to play optimally.\n\n44:54.580 --> 45:02.740\n What I'm saying is, okay, I have a little bit of a bias, but I find ideas beautiful,\n\n45:02.740 --> 45:06.660\n but only when they actually realize the promise.\n\n45:06.660 --> 45:08.420\n That's another level of beauty.\n\n45:08.420 --> 45:13.540\n For example, what Bezos and Elon Musk are doing with rockets.\n\n45:13.540 --> 45:18.900\n We had rockets for a long time, but doing reusable cheap rockets, it's very impressive.\n\n45:18.900 --> 45:22.980\n In the same way, I would have not predicted.\n\n45:22.980 --> 45:30.820\n First of all, when I started and fell in love with AI, the game of Go was seen to be impossible\n\n45:30.820 --> 45:31.300\n to solve.\n\n45:31.300 --> 45:38.500\n Okay, so I thought maybe, you know, maybe it'd be possible to maybe have big leaps in\n\n45:38.500 --> 45:42.020\n a Moore's law style of way, in computation, I'll be able to solve it.\n\n45:42.020 --> 45:50.500\n But I would never have guessed that you can learn your way, however, I mean, in the narrow\n\n45:50.500 --> 45:55.620\n sense of learning, learn your way to beat the best people in the world at the game of\n\n45:55.620 --> 45:59.300\n Go without human supervision, not studying the game of experts.\n\n45:59.300 --> 46:08.900\n Okay, so using a different learning technique, Arthur Samuel in the early 60s, and he was\n\n46:08.900 --> 46:14.900\n the first person to use machine learning, had a program that could beat the world champion\n\n46:14.900 --> 46:15.540\n at checkers.\n\n46:16.100 --> 46:19.860\n And that at the time was considered amazing.\n\n46:19.860 --> 46:22.820\n By the way, Arthur Samuel had some fantastic advantages.\n\n46:23.460 --> 46:25.700\n Do you want to hear Arthur Samuel's advantages?\n\n46:25.700 --> 46:26.660\n Two things.\n\n46:26.660 --> 46:30.500\n One, he was at the 1956 AI conference.\n\n46:30.500 --> 46:32.420\n I knew Arthur later in life.\n\n46:32.420 --> 46:34.500\n He was at Stanford when I was a graduate student there.\n\n46:34.500 --> 46:37.060\n He wore a tie and a jacket every day, the rest of us didn't.\n\n46:38.900 --> 46:40.500\n Delightful man, delightful man.\n\n46:42.980 --> 46:51.620\n It turns out Claude Shannon, in a 1950 Scientific American article, on chess playing, outlined\n\n46:51.620 --> 46:57.140\n the learning mechanism that Arthur Samuel used, and they had met in 1956.\n\n46:57.140 --> 47:00.020\n I assume there was some communication, but I don't know that for sure.\n\n47:00.580 --> 47:07.060\n But Arthur Samuel had been a vacuum tube engineer, getting reliability of vacuum tubes, and then\n\n47:07.060 --> 47:11.860\n had overseen the first transistorized computers at IBM.\n\n47:11.860 --> 47:18.180\n And in those days, before you shipped a computer, you ran it for a week to get early failures.\n\n47:18.180 --> 47:28.580\n So he had this whole farm of computers running random code for hours and hours for each computer.\n\n47:28.580 --> 47:29.940\n He had a whole bunch of them.\n\n47:29.940 --> 47:38.820\n So he ran his chess learning program with self play on IBM's production line.\n\n47:38.820 --> 47:43.700\n He had more computation available to him than anyone else in the world, and then he was\n\n47:43.700 --> 47:48.260\n able to produce a chess playing program, I mean a checkers playing program, that could\n\n47:48.260 --> 47:49.220\n beat the world champion.\n\n47:49.940 --> 47:51.540\n So that's amazing.\n\n47:51.540 --> 47:57.060\n The question is, what I mean surprised, I don't just mean it's nice to have that accomplishment,\n\n47:58.020 --> 48:06.180\n is there is a stepping towards something that feels more intelligent than before.\n\n48:06.180 --> 48:08.740\n Yeah, but that's in your view of the world.\n\n48:08.740 --> 48:11.380\n Okay, well let me then, it doesn't mean I'm wrong.\n\n48:11.380 --> 48:13.540\n No, no it doesn't.\n\n48:13.540 --> 48:18.740\n So the question is, if we keep taking steps like that, how far that takes us?\n\n48:18.740 --> 48:21.780\n Are we going to build a better recommender systems?\n\n48:21.780 --> 48:23.860\n Are we going to build a better robot?\n\n48:23.860 --> 48:25.940\n Or will we solve intelligence?\n\n48:25.940 --> 48:33.300\n So, you know, I'm putting my bet on, but still missing a whole lot.\n\n48:33.300 --> 48:33.800\n A lot.\n\n48:34.500 --> 48:36.020\n And why would I say that?\n\n48:36.020 --> 48:43.060\n Well, in these games, they're all, you know, 100% information games, but again, but each\n\n48:43.060 --> 48:50.420\n of these systems is a very short description of the current state, which is different from\n\n48:50.420 --> 48:55.620\n registering and perception in the world, which gets back to Marovec's paradox.\n\n48:55.620 --> 49:05.780\n I'm definitely not saying that chess is somehow harder than perception or any kind of, even\n\n49:05.780 --> 49:10.180\n any kind of robotics in the physical world, I definitely think is way harder than the\n\n49:10.180 --> 49:10.820\n game of chess.\n\n49:10.820 --> 49:15.300\n So I was always much more impressed by the workings of the human mind.\n\n49:15.300 --> 49:15.940\n It's incredible.\n\n49:15.940 --> 49:16.900\n The human mind is incredible.\n\n49:17.700 --> 49:20.340\n I believe that from the very beginning, I wanted to be a psychiatrist for the longest\n\n49:20.340 --> 49:20.740\n time.\n\n49:20.740 --> 49:23.140\n I always thought that's way more incredible in the game of chess.\n\n49:23.140 --> 49:26.740\n I think the game of chess is, I love the Olympics.\n\n49:26.740 --> 49:31.860\n It's just another example of us humans picking a task and then agreeing that a million humans\n\n49:31.860 --> 49:33.860\n will dedicate their whole life to that task.\n\n49:33.860 --> 49:39.860\n And that's the cool thing that the human mind is able to focus on one task and then compete\n\n49:39.860 --> 49:44.500\n against each other and achieve like weirdly incredible levels of performance.\n\n49:44.500 --> 49:46.740\n That's the aspect of chess that's super cool.\n\n49:46.740 --> 49:49.700\n Not that chess in itself is really difficult.\n\n49:49.700 --> 49:53.460\n It's like the Fermat's last theorem is not in itself to me that interesting.\n\n49:53.460 --> 49:57.780\n The fact that thousands of people have been struggling to solve that particular problem\n\n49:57.780 --> 49:58.500\n is fascinating.\n\n49:58.500 --> 50:00.500\n So can I tell you my disease in this way?\n\n50:00.500 --> 50:00.740\n Sure.\n\n50:01.460 --> 50:03.380\n Which actually is closer to what you're saying.\n\n50:03.380 --> 50:07.620\n So as a child, I was building various, I called them computers.\n\n50:07.620 --> 50:09.380\n They weren't general purpose computers.\n\n50:09.380 --> 50:10.180\n Ice cube tray.\n\n50:10.180 --> 50:11.380\n The ice cube tray was one.\n\n50:11.380 --> 50:12.660\n But I built other machines.\n\n50:12.660 --> 50:18.100\n And what I liked to build was machines that could beat adults at a game and the adults\n\n50:18.100 --> 50:19.700\n couldn't beat my machine.\n\n50:19.700 --> 50:19.940\n Yeah.\n\n50:19.940 --> 50:22.660\n So you were like, that's powerful.\n\n50:22.660 --> 50:24.820\n That's a way to rebel.\n\n50:24.820 --> 50:33.220\n Oh, by the way, when was the first time you built something that outperformed you?\n\n50:33.220 --> 50:33.860\n Do you remember?\n\n50:34.660 --> 50:36.340\n Well, I knew how it worked.\n\n50:36.340 --> 50:42.020\n I was probably nine years old and I built a thing that was a game where you take turns\n\n50:42.020 --> 50:47.460\n in taking matches from a pile and either the one who takes the last one or the one who\n\n50:47.460 --> 50:48.660\n doesn't take the last one wins.\n\n50:48.660 --> 50:49.460\n I forget.\n\n50:49.460 --> 50:54.500\n And so it was pretty easy to build that out of wires and nails and little coils that were\n\n50:54.500 --> 50:58.020\n like plugging in the number and a few light bulbs.\n\n50:59.060 --> 51:07.220\n The one I was proud of, I was 12 when I built a thing out of old telephone switchboard switches\n\n51:07.220 --> 51:11.380\n that could always win at tic tac toe.\n\n51:11.380 --> 51:14.500\n And that was a much harder circuit to design.\n\n51:14.500 --> 51:17.620\n But again, it was no active components.\n\n51:17.620 --> 51:23.300\n It was just three position switches, empty, X, zero, O.\n\n51:23.300 --> 51:29.460\n And nine of them and a light bulb on which move it wanted next.\n\n51:29.460 --> 51:31.540\n And then the human would go and move that.\n\n51:31.540 --> 51:33.060\n See, there's magic in that creation.\n\n51:33.060 --> 51:33.860\n There was.\n\n51:33.860 --> 51:34.580\n Yeah, yeah.\n\n51:34.580 --> 51:43.700\n I tend to see magic in robots that like I also think that intelligence is a little bit\n\n51:43.700 --> 51:44.740\n overrated.\n\n51:44.740 --> 51:48.100\n I think we can have deep connections with robots very soon.\n\n51:49.140 --> 51:52.500\n And well, we'll come back to connections for sure.\n\n51:52.500 --> 52:00.100\n But I do want to say, I think too many people make the mistake of seeing that magic and\n\n52:00.100 --> 52:02.020\n thinking, well, we'll just continue.\n\n52:02.820 --> 52:07.300\n But each one of those is a hard fought battle for the next step, the next step.\n\n52:07.300 --> 52:07.540\n Yes.\n\n52:08.180 --> 52:11.940\n The open question here is, and this is why I'm playing devil's advocate, but I often\n\n52:11.940 --> 52:18.420\n do when I read your blog post in my mind because I have like this eternal optimism, is it's\n\n52:18.420 --> 52:19.380\n not clear to me.\n\n52:19.380 --> 52:23.940\n So I don't do what obviously the journalists do or they give into the hype, but it's not\n\n52:23.940 --> 52:34.740\n obvious to me how many steps away we are from a truly transformational understanding of\n\n52:34.740 --> 52:39.780\n what it means to build intelligent systems or how to build intelligent systems.\n\n52:40.580 --> 52:45.140\n I'm also aware of the whole history of artificial intelligence, which is where your deep grounding\n\n52:45.140 --> 52:51.860\n of this is, is there has been an optimism for decades and that optimism, just like reading\n\n52:51.860 --> 52:57.300\n old optimism is absurd because people were like, this is, they were saying things are\n\n52:57.300 --> 53:00.740\n trivial for decades since the sixties, they're saying everything is true.\n\n53:00.740 --> 53:07.700\n Computer vision is trivial, but I think my mind is working crisply enough to where, I\n\n53:07.700 --> 53:09.700\n mean, we can dig into if you want.\n\n53:09.700 --> 53:12.900\n I'm really surprised by the things DeepMind has done.\n\n53:12.900 --> 53:19.300\n I don't think they're so, they're yet close to solving intelligence, but I'm not sure\n\n53:19.300 --> 53:21.220\n it's not 10 to 10 years away.\n\n53:22.500 --> 53:30.100\n What I'm referring to is interesting to see when the engineering, it takes that idea to\n\n53:30.100 --> 53:32.660\n scale and the idea works.\n\n53:32.660 --> 53:34.100\n And no, it fools people.\n\n53:34.900 --> 53:35.300\n Okay.\n\n53:35.300 --> 53:40.420\n Honestly, Rodney, if it was you, me and Demis inside a room, forget the press, forget all\n\n53:40.420 --> 53:47.060\n those things, just as a scientist, as a roboticist, that wasn't surprising to you that at scale.\n\n53:47.060 --> 53:50.180\n So we're talking about very large now, okay, let's pick one.\n\n53:50.180 --> 53:52.340\n That's the most surprising to you.\n\n53:52.340 --> 53:52.820\n Okay.\n\n53:52.820 --> 53:53.940\n Please don't yell at me.\n\n53:53.940 --> 53:56.180\n GPT three, okay.\n\n53:56.180 --> 54:03.300\n Hold on, hold on, I was going to say, okay, alpha zero, alpha go, alpha go, zero, alpha\n\n54:03.300 --> 54:06.340\n zero, and then alpha fold one and two.\n\n54:06.340 --> 54:13.460\n So do any of these kind of have this core of, forget usefulness or application and so\n\n54:13.460 --> 54:19.220\n on, which you could argue for alpha fold, like, as a scientist, was those surprising\n\n54:19.220 --> 54:22.260\n to you that it worked as well as it did?\n\n54:23.140 --> 54:30.820\n Okay, so if we're going to make the distinction between surprise and usefulness, and I have\n\n54:30.820 --> 54:40.580\n to explain this, I would say alpha fold, and one of the problems at the moment with alpha\n\n54:40.580 --> 54:44.820\n fold is, you know, it gets a lot of them right, which is a surprise to me, because they're\n\n54:44.820 --> 54:51.940\n a really complex thing, but you don't know which ones it gets right, which then is a\n\n54:51.940 --> 54:52.500\n bit of a problem.\n\n54:52.500 --> 54:53.620\n Now they've come out with a recent...\n\n54:53.620 --> 54:56.180\n You mean the structure of the proteins, it gets a lot of those right.\n\n54:56.180 --> 55:00.180\n Yeah, it's a surprising number of them right, it's been a really hard problem.\n\n55:00.180 --> 55:02.100\n So that was a surprise how many it gets right.\n\n55:03.220 --> 55:07.460\n So far, the usefulness is limited, because you don't know which ones are right or not,\n\n55:07.460 --> 55:12.900\n and now they've come out with a thing in the last few weeks, which is trying to get a useful\n\n55:12.900 --> 55:14.980\n tool out of it, and they may well do it.\n\n55:15.620 --> 55:20.820\n In that sense, at least alpha fold is different, because your alpha fold tool is different,\n\n55:21.940 --> 55:27.460\n because now it's producing data sets that are actually, you know, potentially revolutionizing\n\n55:27.460 --> 55:31.620\n competition biology, like they will actually help a lot of people, but...\n\n55:31.620 --> 55:36.020\n You would say potentially revolutionizing, we don't know yet, but yeah.\n\n55:36.020 --> 55:36.820\n That's true, yeah.\n\n55:36.820 --> 55:39.220\n But they're, you know, but I got you.\n\n55:39.220 --> 55:40.020\n I mean, this is...\n\n55:40.020 --> 55:45.860\n Okay, so you know what, this is gonna be so fun, so let's go right into it.\n\n55:45.860 --> 55:52.020\n Speaking of robots that operate in the real world, let's talk about self driving cars.\n\n55:52.740 --> 55:54.740\n Oh, okay.\n\n55:54.740 --> 56:00.500\n Okay, because you have built robotics companies, you're one of the greatest roboticists in\n\n56:00.500 --> 56:06.500\n history, and that's not just in the space of ideas, we'll also probably talk about that,\n\n56:06.500 --> 56:13.220\n but in the actual building and execution of businesses that make robots that are useful\n\n56:13.220 --> 56:16.020\n for people and that actually work in the real world and make money.\n\n56:18.660 --> 56:24.420\n You also sometimes are critical of Mr. Elon Musk, or let's more specifically focus on\n\n56:24.420 --> 56:27.620\n this particular technology, which is autopilot inside Teslas.\n\n56:29.380 --> 56:33.780\n What are your thoughts about Tesla autopilot, or more generally vision based machine learning\n\n56:33.780 --> 56:36.740\n approach to semi autonomous driving?\n\n56:38.580 --> 56:43.140\n These are robots, they're being used in the real world by hundreds of thousands of people,\n\n56:43.700 --> 56:49.940\n and if you want to go there, I can go there, but that's not too much, which they're...\n\n56:49.940 --> 56:57.220\n Let's say they're on par safety wise as humans currently, meaning human alone versus human\n\n56:57.220 --> 56:58.500\n plus robot.\n\n56:58.500 --> 57:03.860\n Okay, so first let me say I really like the car I came in here today.\n\n57:03.860 --> 57:04.340\n Which is?\n\n57:06.260 --> 57:11.860\n 2021 model, Mercedes E450.\n\n57:12.740 --> 57:19.620\n I am impressed by the machine vision, sonar, other things.\n\n57:19.620 --> 57:21.700\n I'm impressed by what it can do.\n\n57:21.700 --> 57:27.140\n I'm really impressed with many aspects of it.\n\n57:29.540 --> 57:31.380\n It's able to stay in lane, is it?\n\n57:31.380 --> 57:33.140\n Oh yeah, it does the lane stuff.\n\n57:35.940 --> 57:40.260\n It's looking on either side of me, it's telling me about nearby cars.\n\n57:40.260 --> 57:41.540\n For blind spots and so on.\n\n57:41.540 --> 57:48.100\n Yeah, when I'm going in close to something in the park, I get this beautiful, gorgeous,\n\n57:48.100 --> 57:49.780\n top down view of the world.\n\n57:49.780 --> 57:56.900\n I am impressed up the wazoo of how registered and metrical that is.\n\n57:56.900 --> 58:00.900\n So it's like multiple cameras and it's all ready to go to produce the 360 view kind of\n\n58:00.900 --> 58:00.900\n thing?\n\n58:00.900 --> 58:05.780\n 360 view, it's synthesized so it's above the car, and it is unbelievable.\n\n58:06.580 --> 58:10.740\n I got this car in January, it's the longest I've ever owned a car without digging it.\n\n58:11.460 --> 58:12.420\n So it's better than me.\n\n58:13.540 --> 58:15.940\n Me and it together are better.\n\n58:15.940 --> 58:24.980\n So I'm not saying technology's bad or not useful, but here's my point.\n\n58:24.980 --> 58:30.260\n Yes, it's a replay of the same movie.\n\n58:31.380 --> 58:34.900\n Okay, so maybe you've seen me ask this question before.\n\n58:34.900 --> 58:54.100\n But when did the first car go over 55 miles an hour for over 10 miles on a public freeway\n\n58:54.100 --> 58:56.660\n with other traffic around driving completely autonomously?\n\n58:56.660 --> 58:57.460\n When did that happen?\n\n58:59.060 --> 59:01.380\n Was it CMU in the 80s or something?\n\n59:01.380 --> 59:02.340\n It was a long time ago.\n\n59:02.340 --> 59:07.540\n It was actually in 1987 in Munich at the Bundeswehr.\n\n59:09.380 --> 59:11.540\n So they had it running in 1987.\n\n59:12.500 --> 59:16.660\n When do you think, and Elon has said he's going to do this, when do you think we'll\n\n59:16.660 --> 59:23.780\n have the first car drive coast to coast in the US, hands off the wheel, feet off the\n\n59:23.780 --> 59:25.220\n pedals, coast to coast?\n\n59:25.940 --> 59:28.340\n As far as I know, a few people have claimed to do it.\n\n59:28.340 --> 59:30.660\n 1995, that was Carnegie Mellon.\n\n59:30.660 --> 59:35.700\n I didn't know, but oh, that was the, they didn't claim, did they claim 100%?\n\n59:35.700 --> 59:37.540\n Not 100%, not 100%.\n\n59:37.540 --> 59:41.940\n And then there's a few marketing people who have claimed 100% since then.\n\n59:41.940 --> 59:50.740\n My point is that, you know, what I see happening again is someone sees a demo and they overgeneralize\n\n59:50.740 --> 59:52.340\n and say, we must be almost there.\n\n59:52.340 --> 59:54.900\n But we've been working on it for 35 years.\n\n59:54.900 --> 59:56.180\n So that's demos.\n\n59:56.180 --> 59:59.540\n But this is going to take us back to the same conversation with AlphaZero.\n\n59:59.540 --> 1:00:06.100\n Are you not, okay, I'll just say what I am because I thought, okay, when I first started\n\n1:00:06.100 --> 1:00:12.740\n interacting with the Mobileye implementation of Tesla Autopilot, I've driven a lot of car,\n\n1:00:12.740 --> 1:00:15.940\n you know, I've been in Google self driving car since the beginning.\n\n1:00:18.020 --> 1:00:23.300\n I thought there was no way before I sat and used Mobileye, I thought they're just knowing\n\n1:00:23.300 --> 1:00:24.100\n computer vision.\n\n1:00:24.100 --> 1:00:26.980\n I thought there's no way it could work as well as it was working.\n\n1:00:26.980 --> 1:00:35.300\n So my model of the limits of computer vision was way more limited than the actual implementation\n\n1:00:35.300 --> 1:00:35.940\n of Mobileye.\n\n1:00:35.940 --> 1:00:37.860\n I was so that's one example.\n\n1:00:37.860 --> 1:00:39.380\n I was really surprised.\n\n1:00:39.380 --> 1:00:41.700\n It's like, wow, that was that was incredible.\n\n1:00:41.700 --> 1:00:48.820\n The second surprise came when Tesla threw away Mobileye and started from scratch.\n\n1:00:50.580 --> 1:00:52.740\n I thought there's no way they can catch up to Mobileye.\n\n1:00:52.740 --> 1:00:56.260\n I thought what Mobileye was doing was kind of incredible, like the amount of work and\n\n1:00:56.260 --> 1:00:56.980\n the annotation.\n\n1:00:56.980 --> 1:01:01.620\n Yeah, well, Mobileye was started by Amnon Shashua and used a lot of traditional, you\n\n1:01:01.620 --> 1:01:04.420\n know, hard fought computer vision techniques.\n\n1:01:04.420 --> 1:01:11.620\n But they also did a lot of good sort of like non research stuff, like actual like just\n\n1:01:11.620 --> 1:01:14.420\n good, like what you do to make a successful product, right?\n\n1:01:14.420 --> 1:01:16.020\n Scale, all that kind of stuff.\n\n1:01:16.020 --> 1:01:20.020\n And so I was very surprised when they from scratch were able to catch up to that.\n\n1:01:20.660 --> 1:01:21.620\n That's very impressive.\n\n1:01:21.620 --> 1:01:23.780\n And I've talked to a lot of engineers that was involved.\n\n1:01:23.780 --> 1:01:25.620\n This is that was impressive.\n\n1:01:25.620 --> 1:01:26.260\n That was impressive.\n\n1:01:27.300 --> 1:01:34.900\n And the recent progress, especially under the involvement of Andrej Karpathy, what they\n\n1:01:34.900 --> 1:01:40.340\n were what they're doing with the data engine, which is converting into the driving task\n\n1:01:40.340 --> 1:01:45.140\n into these multiple tasks and then doing this edge case discovery when they're pulling back\n\n1:01:45.140 --> 1:01:49.940\n like the level of engineering made me rethink what's possible.\n\n1:01:49.940 --> 1:01:55.380\n I don't I still, you know, I don't know to that intensity, but I always thought it was\n\n1:01:55.380 --> 1:02:00.260\n very difficult to solve autonomous driving with all the sensors, with all the computation.\n\n1:02:00.260 --> 1:02:01.860\n I just thought it's a very difficult problem.\n\n1:02:02.420 --> 1:02:07.860\n But I've been continuously surprised how much you can engineer.\n\n1:02:07.860 --> 1:02:12.100\n First of all, the data acquisition problem, because I thought, you know, just because\n\n1:02:12.100 --> 1:02:20.180\n I worked with a lot of car companies and they're they're so a little a little bit old school\n\n1:02:20.180 --> 1:02:25.940\n to where I didn't think they could do this at scale like AWS style data collection.\n\n1:02:25.940 --> 1:02:32.180\n So when Tesla was able to do that, I started to think, OK, so what are the limits of this?\n\n1:02:33.140 --> 1:02:40.980\n I still believe that driver like sensing and the interaction with the driver and like studying\n\n1:02:40.980 --> 1:02:43.700\n the human factor psychology problem is essential.\n\n1:02:43.700 --> 1:02:45.460\n It's it's always going to be there.\n\n1:02:45.460 --> 1:02:48.740\n It's always going to be there, even with fully autonomous driving.\n\n1:02:48.740 --> 1:02:55.220\n But I've been surprised what is the limit, especially a vision based alone, how far that\n\n1:02:55.220 --> 1:02:56.020\n can take us.\n\n1:02:57.060 --> 1:02:59.860\n So that's my levels of surprise now.\n\n1:03:00.900 --> 1:03:07.380\n OK, can you explain in the same way you said, like Alpha Zero, that's a homework problem\n\n1:03:07.380 --> 1:03:10.260\n that's scaled large in its chest, like who cares?\n\n1:03:10.260 --> 1:03:15.380\n Go with here's actual people using an actual car and driving.\n\n1:03:15.380 --> 1:03:19.380\n Many of them drive more than half their miles using the system.\n\n1:03:19.380 --> 1:03:19.880\n Right.\n\n1:03:20.420 --> 1:03:24.980\n So, yeah, they're doing well with with pure vision for your vision.\n\n1:03:24.980 --> 1:03:25.480\n Yeah.\n\n1:03:25.480 --> 1:03:30.820\n And, you know, and now no radar, which is I suspect that can't go all the way.\n\n1:03:30.820 --> 1:03:36.340\n And one reason is without without new cameras that have a dynamic range closer to the human\n\n1:03:36.340 --> 1:03:39.300\n eye, because human eye has incredible dynamic range.\n\n1:03:39.300 --> 1:03:46.500\n And we make use of that dynamic range in its 11 orders of magnitude or some crazy number\n\n1:03:46.500 --> 1:03:47.000\n like that.\n\n1:03:47.700 --> 1:03:53.140\n The cameras don't have that, which is why you see the the the bad cases where the sun\n\n1:03:53.140 --> 1:03:57.060\n on a white thing and it blinds it in a way it wouldn't blind the person.\n\n1:03:59.860 --> 1:04:06.020\n I think there's a bunch of things to think about before you say this is so good, it's\n\n1:04:06.020 --> 1:04:06.660\n just going to work.\n\n1:04:06.660 --> 1:04:12.180\n OK, and I'll come at it from multiple angles.\n\n1:04:12.180 --> 1:04:13.700\n And I know you've got a lot of time.\n\n1:04:13.700 --> 1:04:14.200\n Yeah.\n\n1:04:14.420 --> 1:04:17.220\n OK, let's let's I have thought about these things.\n\n1:04:17.220 --> 1:04:18.740\n Yeah, I know.\n\n1:04:18.740 --> 1:04:24.980\n You've been writing a lot of great blog posts about it for a while before Tesla had autopilot.\n\n1:04:24.980 --> 1:04:25.480\n Right.\n\n1:04:25.480 --> 1:04:28.660\n So you've been thinking about autonomous driving for a while from every angle.\n\n1:04:29.220 --> 1:04:36.020\n So so a few things, you know, in the US, I think that the death rate for autonomous driving\n\n1:04:36.020 --> 1:04:42.500\n death rate from motor vehicle accidents is about thirty five thousand a year,\n\n1:04:44.900 --> 1:04:49.140\n which is an outrageous number, not outrageous compared to covid deaths.\n\n1:04:49.140 --> 1:04:50.980\n But, you know, there is no rationality.\n\n1:04:52.100 --> 1:04:54.340\n And that's part of the thing people have said.\n\n1:04:54.340 --> 1:04:58.900\n Engineers say to me, well, if we cut down the number of deaths by 10 percent by having\n\n1:04:58.900 --> 1:05:01.300\n autonomous driving, that's going to be great.\n\n1:05:01.300 --> 1:05:02.100\n Everyone will love it.\n\n1:05:02.100 --> 1:05:09.620\n And my prediction is that if autonomous vehicles kill more than 10 people a year, they'll be\n\n1:05:09.620 --> 1:05:14.260\n screaming and hollering, even though thirty five thousand people a year have been killed\n\n1:05:14.260 --> 1:05:15.300\n by human drivers.\n\n1:05:16.260 --> 1:05:17.300\n It's not rational.\n\n1:05:17.860 --> 1:05:19.620\n It's a different set of expectations.\n\n1:05:20.100 --> 1:05:21.700\n And that will probably continue.\n\n1:05:23.860 --> 1:05:25.300\n So there's that aspect of it.\n\n1:05:25.300 --> 1:05:34.420\n The other aspect of it is that when we introduce new technology, we often change the rules\n\n1:05:34.420 --> 1:05:34.980\n of the game.\n\n1:05:36.020 --> 1:05:45.060\n So when we introduced cars first into our daily lives, we completely rebuilt our cities\n\n1:05:45.060 --> 1:05:46.900\n and we changed all the laws.\n\n1:05:46.900 --> 1:05:52.820\n Yeah, jaywalking was not an offense that was pushed by the car companies so that people\n\n1:05:52.820 --> 1:05:56.820\n would stay off the road so there wouldn't be deaths from pedestrians getting hit.\n\n1:05:57.460 --> 1:06:02.580\n We completely changed the structure of our cities and had these foul smelling things\n\n1:06:02.580 --> 1:06:04.580\n everywhere around us.\n\n1:06:04.580 --> 1:06:11.060\n And now you see pushback in cities like Barcelona is really trying to exclude cars, et cetera.\n\n1:06:11.060 --> 1:06:21.460\n So I think that to get to self driving, we will, large adoption, it's not going to be\n\n1:06:21.460 --> 1:06:27.300\n just take the current situation, take out the driver and put the same car doing the\n\n1:06:27.300 --> 1:06:30.100\n same stuff because the end case is too many.\n\n1:06:31.860 --> 1:06:33.300\n Here's an interesting question.\n\n1:06:33.300 --> 1:06:39.860\n How many fully autonomous train systems do we have in the U.S.?\n\n1:06:41.860 --> 1:06:43.860\n I mean, do you count them as fully autonomous?\n\n1:06:43.860 --> 1:06:47.860\n I don't know because they're usually as a driver, but they're kind of autonomous, right?\n\n1:06:47.860 --> 1:06:49.860\n No, let's get rid of the driver.\n\n1:06:51.380 --> 1:06:51.860\n Okay.\n\n1:06:51.860 --> 1:06:52.820\n I don't know.\n\n1:06:52.820 --> 1:06:54.660\n It's either 15 or 16.\n\n1:06:54.660 --> 1:06:56.180\n Most of them are in airports.\n\n1:06:56.900 --> 1:06:59.860\n There's a few that are fully autonomous.\n\n1:06:59.860 --> 1:07:06.260\n Seven are in airports, there's a few that go about five, two that go about five kilometers\n\n1:07:06.260 --> 1:07:07.060\n out of airports.\n\n1:07:11.460 --> 1:07:17.460\n When is the first fully autonomous train system for mass transit expected to operate fully\n\n1:07:17.460 --> 1:07:22.420\n autonomously with no driver in a U.S.\n\n1:07:22.420 --> 1:07:22.920\n City?\n\n1:07:23.540 --> 1:07:27.940\n It's expected to operate in 2017 in Honolulu.\n\n1:07:27.940 --> 1:07:29.300\n Oh, wow.\n\n1:07:29.300 --> 1:07:32.020\n It's delayed, but they will get there.\n\n1:07:32.020 --> 1:07:35.780\n BART, by the way, was originally going to be autonomous here in the Bay Area.\n\n1:07:35.780 --> 1:07:38.820\n I mean, they're all very close to fully autonomous, right?\n\n1:07:38.820 --> 1:07:41.540\n Yeah, but getting that close is the thing.\n\n1:07:41.540 --> 1:07:48.660\n And I've often gone on a fully autonomous train in Japan, one that goes out to that\n\n1:07:48.660 --> 1:07:50.660\n fake island in the middle of Tokyo Bay.\n\n1:07:50.660 --> 1:07:51.700\n I forget the name of that.\n\n1:07:53.460 --> 1:07:55.540\n And what do you see when you look at that?\n\n1:07:55.540 --> 1:08:02.020\n What do you see when you go to a fully autonomous train in an airport?\n\n1:08:03.380 --> 1:08:05.300\n It's not like regular trains.\n\n1:08:07.060 --> 1:08:12.100\n At every station, there's a double set of doors so that there's a door of the train\n\n1:08:12.100 --> 1:08:16.340\n and there's a door off the platform.\n\n1:08:18.020 --> 1:08:23.540\n And this is really visible in this Japanese one because it goes out in amongst buildings.\n\n1:08:23.540 --> 1:08:27.060\n The whole track is built so that people can't climb onto it.\n\n1:08:27.060 --> 1:08:27.560\n Yeah.\n\n1:08:27.860 --> 1:08:32.260\n So there's an engineering that then makes the system safe and makes them acceptable.\n\n1:08:32.260 --> 1:08:37.620\n I think we'll see similar sorts of things happen in the U.S.\n\n1:08:37.620 --> 1:08:46.180\n What surprised me, I thought, wrongly, that we would have special purpose lanes on 101\n\n1:08:46.180 --> 1:08:55.140\n in the Bay Area, the leftmost lane, so that it would be normal for Teslas or other cars\n\n1:08:55.140 --> 1:09:00.900\n to move into that lane and then say, okay, now it's autonomous and have that dedicated lane.\n\n1:09:00.900 --> 1:09:03.380\n I was expecting movement to that.\n\n1:09:03.380 --> 1:09:06.500\n Five years ago, I was expecting we'd have a lot more movement towards that.\n\n1:09:06.500 --> 1:09:07.460\n We haven't.\n\n1:09:07.460 --> 1:09:12.820\n And it may be because Tesla's been overpromising by saying this, calling their system fully\n\n1:09:12.820 --> 1:09:21.780\n self driving, I think they may have been gotten there quicker by collaborating to change the\n\n1:09:21.780 --> 1:09:22.580\n infrastructure.\n\n1:09:23.460 --> 1:09:30.180\n This is one of the problems with long haul trucking being autonomous.\n\n1:09:30.180 --> 1:09:38.020\n I think it makes sense on freeways at night for the trucks to go autonomously, but then\n\n1:09:38.020 --> 1:09:40.260\n is that how do you get onto and off of the freeway?\n\n1:09:40.260 --> 1:09:42.980\n What sort of infrastructure do you need for that?\n\n1:09:43.780 --> 1:09:48.500\n Do you need to have the human in there to do that or can you get rid of the human?\n\n1:09:48.500 --> 1:09:55.060\n So I think there's ways to get there, but it's an infrastructure argument because the\n\n1:09:55.060 --> 1:10:02.020\n long tail of cases is very long and the acceptance of it will not be at the same level as human\n\n1:10:02.020 --> 1:10:02.580\n drivers.\n\n1:10:02.580 --> 1:10:09.780\n So I'm with you still, and I was with you for a long time, but I am surprised how well\n\n1:10:09.780 --> 1:10:14.820\n how many edge cases of machine learning and vision based methods can cover.\n\n1:10:15.540 --> 1:10:22.260\n This is what I'm trying to get at is I think there's something fundamentally different\n\n1:10:22.260 --> 1:10:27.460\n with vision based methods and Tesla Autopilot and any company that's trying to do the same.\n\n1:10:27.460 --> 1:10:34.260\n Okay, well, I'm not going to argue with you because, you know, we're speculating.\n\n1:10:34.260 --> 1:10:43.620\n Yes, but, you know, my gut feeling tells me it's going to be things will speed up when\n\n1:10:43.620 --> 1:10:48.260\n there is engineering of the environment because that's what happened with every other technology.\n\n1:10:48.260 --> 1:10:53.940\n I'm a bit, I don't know about you, but I'm a bit cynical that infrastructure is going\n\n1:10:53.940 --> 1:10:59.460\n to rely on government to help out in these cases.\n\n1:11:00.340 --> 1:11:05.540\n If you just look at infrastructure in all domains, it's just a government always drags\n\n1:11:05.540 --> 1:11:06.900\n behind on infrastructure.\n\n1:11:07.540 --> 1:11:11.780\n There's like there's so many just well in this country in the future.\n\n1:11:11.780 --> 1:11:12.260\n Sorry.\n\n1:11:12.260 --> 1:11:13.700\n Yes, in this country.\n\n1:11:13.700 --> 1:11:17.780\n And of course, there's many, many countries that are actually much worse on infrastructure.\n\n1:11:17.780 --> 1:11:21.220\n Oh, yes, many of the much worse and there's some that are much worse.\n\n1:11:21.220 --> 1:11:25.940\n You know, like high speed rail, the other countries are much better.\n\n1:11:25.940 --> 1:11:31.220\n I guess my question is, like, which is at the core of what I was trying to think through\n\n1:11:31.220 --> 1:11:37.540\n here and ask is like, how hard is the driving problem as it currently stands?\n\n1:11:37.540 --> 1:11:41.220\n So you mentioned, like, we don't want to just take the human out and duplicate whatever\n\n1:11:41.220 --> 1:11:42.260\n the human was doing.\n\n1:11:42.260 --> 1:11:48.340\n But if we were to try to do that, what, how hard is that problem?\n\n1:11:48.340 --> 1:11:52.420\n Because I used to think is way harder.\n\n1:11:52.420 --> 1:11:59.220\n Like, I used to think it's with vision alone, it would be three decades, four decades.\n\n1:11:59.220 --> 1:12:06.740\n Okay, so I don't know the answer to this thing I'm about to pose, but I do notice that on\n\n1:12:06.740 --> 1:12:13.380\n Highway 280 here in the Bay Area, which largely has concrete surface rather than blacktop\n\n1:12:13.380 --> 1:12:20.900\n surface, the white lines that are painted there now have black boundaries around them.\n\n1:12:20.900 --> 1:12:27.460\n And my lane drift system in my car would not work without those black boundaries.\n\n1:12:27.460 --> 1:12:28.260\n Interesting.\n\n1:12:28.260 --> 1:12:32.420\n So I don't know whether they started doing it to help the lane drift, whether it is an\n\n1:12:32.420 --> 1:12:41.220\n instance of infrastructure following the technology, but my car would not perform as well as the\n\n1:12:41.220 --> 1:12:45.460\n lane, my car would not perform as well without that change in the way they paint the line.\n\n1:12:45.460 --> 1:12:50.340\n Unfortunately, really good lane keeping is not as valuable.\n\n1:12:50.340 --> 1:12:54.900\n Like, it's orders of magnitude more valuable to have a fully autonomous system.\n\n1:12:54.900 --> 1:13:00.900\n Like, yeah, but for me, lane keeping is really helpful because I'm more healthy at it.\n\n1:13:00.900 --> 1:13:03.700\n But you wouldn't pay 10 times.\n\n1:13:03.700 --> 1:13:11.540\n Like, the problem is there's not financial, like, it doesn't make sense to revamp the\n\n1:13:11.540 --> 1:13:14.260\n infrastructure to make lane keeping easier.\n\n1:13:14.820 --> 1:13:17.300\n It does make sense to revamp the infrastructure.\n\n1:13:17.300 --> 1:13:22.260\n If you have a large fleet of autonomous vehicles, now you change what it means to own cars,\n\n1:13:22.260 --> 1:13:23.860\n you change the nature of transportation.\n\n1:13:24.980 --> 1:13:29.620\n But for that, you need autonomous vehicles.\n\n1:13:29.620 --> 1:13:31.540\n Let me ask you about Waymo then.\n\n1:13:31.540 --> 1:13:37.380\n I've gotten a bunch of chances to ride in a Waymo self driving car.\n\n1:13:37.380 --> 1:13:40.980\n And they're, I don't know if you'd call them self driving, but.\n\n1:13:40.980 --> 1:13:45.780\n Well, I mean, I rode in one before they were called Waymo when I was still at X.\n\n1:13:45.780 --> 1:13:50.740\n So there's currently, there's a big leap, another surprising leap I didn't think would\n\n1:13:50.740 --> 1:13:53.780\n happen, which is they have no driver currently.\n\n1:13:53.780 --> 1:13:55.060\n Yeah, in Chandler.\n\n1:13:55.060 --> 1:13:56.100\n In Chandler, Arizona.\n\n1:13:56.100 --> 1:13:58.980\n And I think they're thinking of doing that in Austin as well.\n\n1:13:58.980 --> 1:14:01.540\n But they're expanding.\n\n1:14:01.540 --> 1:14:06.100\n Although, you know, and I do an annual checkup on this.\n\n1:14:06.100 --> 1:14:13.300\n So as of late last year, they were aiming for hundreds of rides a week, not thousands.\n\n1:14:14.020 --> 1:14:22.660\n And there is no one in the car, but there's certainly safety people in the loop.\n\n1:14:22.660 --> 1:14:26.820\n And it's not clear how many, you know, what the ratio of cars to safety people is.\n\n1:14:26.820 --> 1:14:31.620\n It wasn't, obviously, they're not 100% transparent about this.\n\n1:14:31.620 --> 1:14:33.220\n None of them are 100% transparent.\n\n1:14:33.220 --> 1:14:34.420\n They're very untransparent.\n\n1:14:34.420 --> 1:14:39.540\n But at least the way they're, I don't want to make definitively, but they're saying\n\n1:14:39.540 --> 1:14:40.740\n there's no teleoperation.\n\n1:14:42.580 --> 1:14:45.620\n So like, they're, I mean, okay.\n\n1:14:45.620 --> 1:14:51.780\n And that sort of fits with YouTube videos I've seen of people being trapped in the car\n\n1:14:52.820 --> 1:14:55.460\n by a red cone on the street.\n\n1:14:55.460 --> 1:15:01.620\n And they do have rescue vehicles that come, and then a person gets in and drives it.\n\n1:15:01.620 --> 1:15:02.120\n Yeah.\n\n1:15:02.580 --> 1:15:09.700\n But isn't it incredible to you, it was to me, to get in a car with no driver and watch\n\n1:15:09.700 --> 1:15:15.060\n the steering wheel turn, like for somebody who has been studying, at least certainly\n\n1:15:15.060 --> 1:15:18.980\n the human side of autonomous vehicles for many years, and you've been doing it for way\n\n1:15:18.980 --> 1:15:22.420\n longer, like it was incredible to me that this was actually could happen.\n\n1:15:22.420 --> 1:15:24.100\n I don't care if that scale is 100 cars.\n\n1:15:24.100 --> 1:15:25.860\n This is not a demo.\n\n1:15:25.860 --> 1:15:28.820\n This is not, this is me as a regular human.\n\n1:15:28.820 --> 1:15:33.060\n The argument I have is that people make interpolations from that.\n\n1:15:33.060 --> 1:15:33.940\n Interpolations.\n\n1:15:33.940 --> 1:15:35.780\n That, you know, it's here, it's done.\n\n1:15:37.060 --> 1:15:39.380\n You know, it's just, you know, we've solved it.\n\n1:15:39.380 --> 1:15:40.340\n No, we haven't yet.\n\n1:15:40.980 --> 1:15:42.500\n And that's my argument.\n\n1:15:42.500 --> 1:15:42.900\n Okay.\n\n1:15:42.900 --> 1:15:48.420\n So I'd like to go to, you keep a list of predictions on your amazing blog post.\n\n1:15:48.420 --> 1:15:49.700\n It'd be fun to go through them.\n\n1:15:49.700 --> 1:15:51.620\n But before then, let me ask you about this.\n\n1:15:51.620 --> 1:16:03.140\n You have a harshness to you sometimes in your criticisms of what is perceived as hype.\n\n1:16:05.940 --> 1:16:10.980\n And so like, because people extrapolate, like you said, and they kind of buy into the hype\n\n1:16:10.980 --> 1:16:18.900\n and then they kind of start to think that the technology is way better than it is.\n\n1:16:18.900 --> 1:16:21.700\n But let me ask you maybe a difficult question.\n\n1:16:22.260 --> 1:16:22.760\n Sure.\n\n1:16:23.780 --> 1:16:30.740\n Do you think if you look at history of progress, don't you think to achieve the quote impossible,\n\n1:16:30.740 --> 1:16:32.740\n you have to believe that it's possible?\n\n1:16:32.740 --> 1:16:34.260\n Oh, absolutely.\n\n1:16:34.260 --> 1:16:34.820\n Yeah.\n\n1:16:34.820 --> 1:16:46.980\n Look, his two great runs, great, unbelievable, 1903, first human power, human, you know,\n\n1:16:46.980 --> 1:16:49.300\n human, you know, heavier than their flight.\n\n1:16:49.300 --> 1:16:49.800\n Yeah.\n\n1:16:50.580 --> 1:16:52.740\n 1969, we land on the moon.\n\n1:16:52.740 --> 1:16:53.940\n That's 66 years.\n\n1:16:53.940 --> 1:17:00.260\n I'm 66 years old in my lifetime, that span of my lifetime, barely, you know, flying,\n\n1:17:00.260 --> 1:17:05.380\n I don't know what it was, 50 feet, the length of the first flight or something to landing\n\n1:17:05.380 --> 1:17:05.780\n on the moon.\n\n1:17:06.340 --> 1:17:07.380\n Unbelievable.\n\n1:17:08.100 --> 1:17:08.980\n Fantastic.\n\n1:17:08.980 --> 1:17:13.060\n But that requires, by the way, one of the Wright brothers, both of them, but one of\n\n1:17:13.060 --> 1:17:16.180\n them didn't believe it's even possible like a year before.\n\n1:17:16.180 --> 1:17:16.680\n Right.\n\n1:17:16.680 --> 1:17:20.420\n So, like, not just possible soon, but like ever.\n\n1:17:20.420 --> 1:17:21.940\n So, you know.\n\n1:17:21.940 --> 1:17:24.820\n How important is it to believe and be optimistic is what I guess.\n\n1:17:24.820 --> 1:17:26.100\n Oh, yeah, it is important.\n\n1:17:26.100 --> 1:17:32.100\n It's when it goes crazy, when I, you know, you said that, what was the word you used\n\n1:17:32.100 --> 1:17:33.060\n for my bad?\n\n1:17:33.060 --> 1:17:33.780\n Harshness.\n\n1:17:33.780 --> 1:17:34.580\n Harshness.\n\n1:17:34.580 --> 1:17:35.080\n Yes.\n\n1:17:40.180 --> 1:17:41.940\n I just get so frustrated.\n\n1:17:41.940 --> 1:17:42.440\n Yes.\n\n1:17:42.440 --> 1:17:51.260\n When people make these leaps and tell me that I'm, that I don't understand, you know, yeah.\n\n1:17:53.020 --> 1:17:57.420\n There's just from iRobot, which I was co founder of.\n\n1:17:57.420 --> 1:17:57.740\n Yeah.\n\n1:17:57.740 --> 1:18:00.860\n I don't know the exact numbers now because I haven't, it's 10 years since I stepped\n\n1:18:00.860 --> 1:18:06.220\n off the board, but I believe it's well over 30 million robots cleaning houses from that\n\n1:18:06.220 --> 1:18:06.780\n one company.\n\n1:18:06.780 --> 1:18:08.140\n And now there's lots of other companies.\n\n1:18:08.140 --> 1:18:08.140\n Yes.\n\n1:18:08.140 --> 1:18:14.940\n Was that a crazy idea that we had to believe in 2002 when we released it?\n\n1:18:14.940 --> 1:18:20.540\n Yeah, that was, we had, we had to, you know, believe that it could be done.\n\n1:18:20.540 --> 1:18:21.740\n Let me ask you about this.\n\n1:18:21.740 --> 1:18:28.380\n So iRobot, one of the greatest robotics companies ever in terms of creating a robot that actually\n\n1:18:28.380 --> 1:18:31.900\n works in the real world, probably the greatest robotics company ever.\n\n1:18:31.900 --> 1:18:33.660\n You were the co founder of it.\n\n1:18:33.660 --> 1:18:40.860\n If, if the Rodney Brooks of today talked to the Rodney of back then, what would you tell\n\n1:18:40.860 --> 1:18:41.340\n him?\n\n1:18:41.340 --> 1:18:47.100\n Cause I have a sense that would you pat him on the back and say, well, you're doing is\n\n1:18:47.100 --> 1:18:50.780\n going to fail, but go at it anyway.\n\n1:18:50.780 --> 1:18:54.060\n That's what I'm referring to with the harshness.\n\n1:18:54.060 --> 1:18:56.700\n You've accomplished an incredible thing there.\n\n1:18:56.700 --> 1:19:01.500\n One of the several things we'll talk about was, you know, you know, you know, you've\n\n1:19:01.500 --> 1:19:03.820\n done several things we'll talk about.\n\n1:19:03.820 --> 1:19:06.940\n Well, like that's what I'm trying to get at that line.\n\n1:19:06.940 --> 1:19:14.140\n No, it's, it's when my harshness is reserved for people who are not doing it, who claim\n\n1:19:14.140 --> 1:19:16.860\n it's just, well, this shows that it's just going to happen.\n\n1:19:16.860 --> 1:19:18.300\n But here, here's the thing.\n\n1:19:18.300 --> 1:19:19.020\n This shows.\n\n1:19:19.020 --> 1:19:22.700\n But you have that harshness for Elon too.\n\n1:19:24.060 --> 1:19:26.380\n And no, no, it's a different harshness.\n\n1:19:26.380 --> 1:19:30.540\n No, it's, it's a different argument with Elon.\n\n1:19:30.540 --> 1:19:34.780\n I think SpaceX is an amazing company.\n\n1:19:34.780 --> 1:19:40.060\n On the other hand, you know, I, in one of my blog posts, I said, what's easy and what's\n\n1:19:40.060 --> 1:19:40.460\n hard.\n\n1:19:40.460 --> 1:19:44.300\n I said, yeah, space X vertical landing rockets.\n\n1:19:44.300 --> 1:19:45.340\n It had been done before.\n\n1:19:46.380 --> 1:19:48.700\n Grid fins had been done since the sixties.\n\n1:19:48.700 --> 1:19:49.740\n Every Soyuz has them.\n\n1:19:52.780 --> 1:19:58.220\n Reusable space DCX reuse those rockets that landed vertically.\n\n1:19:58.220 --> 1:20:02.780\n There's a whole insurance industry in place for rocket launches.\n\n1:20:02.780 --> 1:20:07.980\n There are all sorts of infrastructure that was doable.\n\n1:20:07.980 --> 1:20:11.980\n It took a great entrepreneur, a great personal expense.\n\n1:20:11.980 --> 1:20:18.220\n He almost drove himself, you know, bankrupt doing it, a great belief to do it.\n\n1:20:18.860 --> 1:20:25.740\n Whereas Hyperloop, there's a whole bunch more stuff that's never been thought about and\n\n1:20:25.740 --> 1:20:28.380\n never been demonstrated.\n\n1:20:28.380 --> 1:20:33.660\n So my estimation is Hyperloop is a long, long, long, a lot further off.\n\n1:20:33.660 --> 1:20:38.940\n But, and if I've got a criticism of, of, of Elon, it's that he doesn't make distinctions\n\n1:20:39.740 --> 1:20:44.780\n between when the technology's coming along and ready.\n\n1:20:44.780 --> 1:20:50.140\n And then he'll go off and mouth off about other things, which then people go and compete\n\n1:20:50.140 --> 1:20:51.100\n about and try and do.\n\n1:20:51.100 --> 1:20:57.580\n And so this is where I, I, I, I understand what you're saying.\n\n1:20:57.580 --> 1:20:59.340\n I tend to draw a different distinction.\n\n1:21:00.060 --> 1:21:06.220\n I, I have a similar kind of harshness towards people who are not telling the truth, who\n\n1:21:06.220 --> 1:21:11.420\n are basically fabricating stuff to make money or to, well, he believes what he says.\n\n1:21:11.420 --> 1:21:18.300\n I just think that's a very important difference because I think in order to fly, in order\n\n1:21:18.300 --> 1:21:24.060\n to get to the moon, you have to believe even when most people tell you you're wrong and\n\n1:21:24.060 --> 1:21:26.940\n most likely you're wrong, but sometimes you're right.\n\n1:21:26.940 --> 1:21:29.900\n I mean, that's the same thing I have with Tesla autopilot.\n\n1:21:29.900 --> 1:21:31.900\n I think that's an interesting one.\n\n1:21:31.900 --> 1:21:38.780\n I was, especially when I was at MIT and just the entire human factors in the robotics community\n\n1:21:38.780 --> 1:21:40.300\n were very negative towards Elon.\n\n1:21:40.300 --> 1:21:43.020\n It was very interesting for me to observe colleagues at MIT.\n\n1:21:45.020 --> 1:21:46.620\n I wasn't sure what to make of that.\n\n1:21:46.620 --> 1:21:51.100\n That was very upsetting to me because I understood where that, where that's coming from.\n\n1:21:51.900 --> 1:21:56.300\n And I agreed with them and I kind of almost felt the same thing in the beginning until\n\n1:21:56.300 --> 1:22:01.660\n I kind of opened my eyes and realized there's a lot of interesting ideas here that might\n\n1:22:01.660 --> 1:22:02.540\n be over hype.\n\n1:22:02.540 --> 1:22:09.740\n You know, if you focus yourself on the idea that you shouldn't call a system full self\n\n1:22:09.740 --> 1:22:16.220\n driving when it's obviously not autonomous, fully autonomous, you're going to miss the\n\n1:22:16.220 --> 1:22:16.860\n magic.\n\n1:22:16.860 --> 1:22:18.940\n Oh, yeah, you are going to miss the magic.\n\n1:22:18.940 --> 1:22:25.340\n But at the same time, there are people who buy it, literally pay money for it and take\n\n1:22:25.340 --> 1:22:27.180\n those words as given.\n\n1:22:27.180 --> 1:22:30.300\n So it's, but I haven't.\n\n1:22:30.300 --> 1:22:33.420\n So that I take words as given is one thing.\n\n1:22:33.420 --> 1:22:38.940\n I haven't actually seen people that use autopilot that believe that the behavior is really important,\n\n1:22:39.500 --> 1:22:40.700\n like the actual action.\n\n1:22:40.700 --> 1:22:45.740\n So like, this is to push back on the very thing that you're frustrated about, which\n\n1:22:45.740 --> 1:22:52.460\n is like journalists and general people buying all the hype and going out in the same way.\n\n1:22:52.460 --> 1:22:57.980\n I think there's a lot of hype about the negatives of this, too, that people are buying without\n\n1:22:57.980 --> 1:23:01.020\n using people use the way this is what this was.\n\n1:23:01.020 --> 1:23:02.060\n This opened my eyes.\n\n1:23:02.060 --> 1:23:07.580\n Actually, the way people use a product is very different than the way they talk about\n\n1:23:07.580 --> 1:23:07.820\n it.\n\n1:23:07.820 --> 1:23:09.500\n This is true with robotics, with everything.\n\n1:23:09.500 --> 1:23:13.660\n Everybody has dreams of how a particular product might be used or so on.\n\n1:23:13.660 --> 1:23:17.980\n And then when it meets reality, there's a lot of fear of robotics, for example, that\n\n1:23:17.980 --> 1:23:20.380\n robots are somehow dangerous and all those kinds of things.\n\n1:23:20.380 --> 1:23:23.980\n But when you actually have robots in your life, whether it's in the factory or in the\n\n1:23:23.980 --> 1:23:28.300\n home, making your life better, that's going to be that's way different.\n\n1:23:28.300 --> 1:23:30.460\n Your perceptions of it are going to be way different.\n\n1:23:30.460 --> 1:23:34.780\n And so my just tension was like, here's an innovator.\n\n1:23:34.780 --> 1:23:41.500\n Supercruise from Cadillac was super interesting, too.\n\n1:23:41.500 --> 1:23:43.020\n That's a really interesting system.\n\n1:23:43.020 --> 1:23:45.580\n We should be excited by those innovations.\n\n1:23:45.580 --> 1:23:49.020\n OK, so can I tell you something that's really annoyed me recently?\n\n1:23:49.020 --> 1:23:56.380\n It's really annoyed me that the press and friends of mine on Facebook are going, these\n\n1:23:56.380 --> 1:23:59.740\n billionaires and their space games, why are they doing that?\n\n1:23:59.740 --> 1:24:02.300\n And that really, really pisses me off.\n\n1:24:02.300 --> 1:24:05.100\n I must say, I applaud that.\n\n1:24:05.100 --> 1:24:06.780\n I applaud it.\n\n1:24:06.780 --> 1:24:13.180\n It's the taking and not necessarily the people who are doing the things, but, you know, that\n\n1:24:13.180 --> 1:24:19.740\n I keep having to push back against unrealistic expectations when these things can become\n\n1:24:19.740 --> 1:24:20.300\n real.\n\n1:24:20.300 --> 1:24:26.220\n Yeah, I this was interesting on because there's been a particular focus for me is autonomous\n\n1:24:26.220 --> 1:24:30.140\n driving, Elon's prediction of when certain milestones will be hit.\n\n1:24:30.140 --> 1:24:37.660\n There's several things to be said there that I always I thought about, because whenever\n\n1:24:37.660 --> 1:24:44.860\n you said them, it was obvious that's not going to me as a person that kind of not inside\n\n1:24:44.860 --> 1:24:46.940\n the system is obvious.\n\n1:24:46.940 --> 1:24:48.700\n It's unlikely to hit those.\n\n1:24:48.700 --> 1:24:50.700\n There's two comments I want to make.\n\n1:24:50.700 --> 1:24:54.220\n One, he legitimately believes it.\n\n1:24:54.220 --> 1:25:04.140\n And two, much more importantly, I think that having ambitious deadlines drives people to\n\n1:25:04.140 --> 1:25:09.420\n do the best work of their life, even when the odds of those deadlines are very low.\n\n1:25:09.420 --> 1:25:12.780\n To a point, and I'm not talking about anyone here, I'm just saying.\n\n1:25:12.780 --> 1:25:14.220\n So there's a line there, right?\n\n1:25:14.220 --> 1:25:20.140\n You have to have a line because you overextend and it's demoralizing.\n\n1:25:20.140 --> 1:25:27.820\n It's demoralizing, but I will say that there's an additional thing here that those words\n\n1:25:28.860 --> 1:25:32.460\n also drive the stock market.\n\n1:25:34.140 --> 1:25:42.060\n And we have because of the way that rich people in the past have manipulated the rubes through\n\n1:25:42.060 --> 1:25:49.260\n investment, we have developed laws about what you're allowed to say.\n\n1:25:49.260 --> 1:25:58.380\n And you know, there's an area here which is I tend to be maybe I'm naive, but I tend to\n\n1:25:58.380 --> 1:26:06.620\n believe that like engineers, innovators, people like that, they're not they're my they don't\n\n1:26:06.620 --> 1:26:09.500\n think like that, like manipulating the price of the stock price.\n\n1:26:09.500 --> 1:26:13.980\n But it's possible that I'm I'm certain it's possible that I'm wrong.\n\n1:26:13.980 --> 1:26:21.820\n It's a very cynical view of the world because I think most people that run companies, especially\n\n1:26:21.820 --> 1:26:27.260\n original founders, they yeah, I'm not saying that's the intent.\n\n1:26:27.260 --> 1:26:33.340\n I'm saying it's eventually it's kind of you you you you fall into that kind of behavior\n\n1:26:33.340 --> 1:26:33.340\n pattern.\n\n1:26:33.340 --> 1:26:33.900\n I don't know.\n\n1:26:33.900 --> 1:26:37.980\n I tend to I wasn't saying I wasn't saying it's falling into that intent.\n\n1:26:37.980 --> 1:26:43.740\n It's just you also have to protect investors in this environment.\n\n1:26:43.740 --> 1:26:44.620\n In this market.\n\n1:26:44.620 --> 1:26:44.860\n Yeah.\n\n1:26:45.580 --> 1:26:50.060\n OK, so you have first of all, you have an amazing blog that people should check out.\n\n1:26:50.060 --> 1:26:54.060\n But you also have this in that blog, a set of predictions.\n\n1:26:54.780 --> 1:26:55.740\n Such a cool idea.\n\n1:26:55.740 --> 1:26:58.220\n I don't know how long ago you started, like three, four years ago.\n\n1:26:58.220 --> 1:27:01.820\n It was January 1st, 2018.\n\n1:27:01.820 --> 1:27:02.220\n 18.\n\n1:27:02.940 --> 1:27:07.740\n And I made these predictions and I said that every January 1st, I was going to check back\n\n1:27:07.740 --> 1:27:09.020\n on how my predictions.\n\n1:27:09.020 --> 1:27:10.220\n That's such a great thought experiment.\n\n1:27:10.220 --> 1:27:11.260\n For 32 years.\n\n1:27:11.900 --> 1:27:13.340\n Oh, you said 32 years.\n\n1:27:13.340 --> 1:27:16.380\n I said 32 years because it's still that'll be January 1st, 2050.\n\n1:27:16.940 --> 1:27:19.980\n I'll be I will just turn ninety.\n\n1:27:21.660 --> 1:27:31.180\n Five, you know, and so people know that your predictions, at least for now, are in the\n\n1:27:31.180 --> 1:27:33.180\n space of artificial intelligence.\n\n1:27:33.180 --> 1:27:34.860\n Yeah, I didn't say I was going to make new predictions.\n\n1:27:34.860 --> 1:27:38.380\n I was just going to measure this set of predictions that I made because I was sort of I was sort\n\n1:27:38.380 --> 1:27:40.620\n of annoyed that everyone could make predictions.\n\n1:27:40.620 --> 1:27:42.460\n They didn't come true and everyone forgot.\n\n1:27:42.460 --> 1:27:44.860\n So I should hold myself to a high standard.\n\n1:27:44.860 --> 1:27:48.700\n Yeah, but also just putting years and like date ranges on things.\n\n1:27:48.700 --> 1:27:50.140\n It's a good thought exercise.\n\n1:27:50.140 --> 1:27:52.940\n Yeah, like and like reasoning your thoughts out.\n\n1:27:52.940 --> 1:27:58.300\n And so the topics are artificial intelligence, autonomous vehicles and space.\n\n1:27:58.300 --> 1:27:58.800\n Yeah.\n\n1:28:00.940 --> 1:28:04.700\n I was wondering if we could just go through some that stand out maybe from memory.\n\n1:28:04.700 --> 1:28:06.140\n I can just mention to you some.\n\n1:28:06.140 --> 1:28:10.780\n Let's talk about self driving cars, like some predictions that you're particularly proud\n\n1:28:10.780 --> 1:28:20.220\n of or are particularly interesting from flying cars to the other element here is like how\n\n1:28:20.220 --> 1:28:24.700\n widespread the location where the deployment of the autonomous vehicles is.\n\n1:28:25.900 --> 1:28:27.580\n And there's also just a few fun ones.\n\n1:28:27.580 --> 1:28:30.300\n Is there something that jumps to mind that you remember from the predictions?\n\n1:28:31.980 --> 1:28:37.500\n Well, I think I did put in there that there would be a dedicated self driving lane on\n\n1:28:37.500 --> 1:28:41.740\n 101 by some year, and I think I was over optimistic on that one.\n\n1:28:42.380 --> 1:28:42.860\n Yeah, actually.\n\n1:28:42.860 --> 1:28:44.140\n Yeah, I actually do remember that.\n\n1:28:44.140 --> 1:28:48.620\n But you I think you were mentioning like difficulties at different cities.\n\n1:28:48.620 --> 1:28:49.120\n Yeah.\n\n1:28:50.460 --> 1:28:52.460\n Cambridge, Massachusetts, I think was an example.\n\n1:28:52.460 --> 1:28:56.860\n Yeah, like in Cambridge Port, you know, I lived in Cambridge Port for a number of years\n\n1:28:56.860 --> 1:29:02.780\n and you know, the roads are narrow and getting getting anywhere as a human driver is incredibly\n\n1:29:02.780 --> 1:29:07.660\n frustrating when you start to put and people drive the wrong way on one way streets there.\n\n1:29:07.660 --> 1:29:14.860\n It's just your prediction was driverless taxi services operating on all streets in\n\n1:29:14.860 --> 1:29:20.300\n Cambridge Port, Massachusetts in 2035.\n\n1:29:21.100 --> 1:29:21.740\n Yeah.\n\n1:29:21.740 --> 1:29:25.020\n And that may have been too optimistic.\n\n1:29:25.020 --> 1:29:26.060\n You think so?\n\n1:29:26.060 --> 1:29:31.020\n You know, I've gotten a little more pessimistic since I made these internally on some of these\n\n1:29:31.020 --> 1:29:31.500\n things.\n\n1:29:31.500 --> 1:29:42.780\n So what can you put a year to a major milestone of deployment of a taxi service in in a few\n\n1:29:42.780 --> 1:29:47.500\n major cities like something where you feel like autonomous vehicles are here.\n\n1:29:47.500 --> 1:29:55.900\n So let's let's take the grid streets of San Francisco north of market.\n\n1:29:55.900 --> 1:29:56.540\n Okay.\n\n1:29:56.540 --> 1:29:57.040\n Okay.\n\n1:29:57.040 --> 1:30:07.040\n Relatively benign environment, the streets are wide, the major problem is delivery trucks\n\n1:30:07.040 --> 1:30:10.880\n stopping everywhere, which made things more complicated.\n\n1:30:12.880 --> 1:30:21.280\n Taxi system there with somewhat designated pickup and drop offs, unlike with Uber and\n\n1:30:21.280 --> 1:30:28.160\n Lyft, where you can sort of get to any place and the drivers will figure out how to get\n\n1:30:28.160 --> 1:30:28.720\n in there.\n\n1:30:30.720 --> 1:30:32.080\n We're still a few years away.\n\n1:30:32.080 --> 1:30:35.200\n I, you know, I live in that area.\n\n1:30:35.200 --> 1:30:42.240\n So I see, you know, the self driving car companies cars, multiple multiple ones every day.\n\n1:30:42.240 --> 1:30:52.480\n Now if they're cruise, Zooks less often, Waymo all the time, different and different ones\n\n1:30:52.480 --> 1:30:53.440\n come and go.\n\n1:30:53.440 --> 1:30:54.960\n And there's always a driver.\n\n1:30:55.520 --> 1:31:02.240\n There's always a driver at the moment, although I have noticed that sometimes the driver does\n\n1:31:02.240 --> 1:31:08.000\n not have the authority to take over without talking to the home office, because they will\n\n1:31:08.000 --> 1:31:14.640\n sit there waiting for a long time, and clearly something's going on where the home office\n\n1:31:14.640 --> 1:31:15.680\n is making a decision.\n\n1:31:16.960 --> 1:31:21.600\n So they're, you know, and, and so you can see whether they've got their hands on the\n\n1:31:21.600 --> 1:31:22.400\n wheel or not.\n\n1:31:22.400 --> 1:31:27.680\n And, and it's the incident resolution time that tells you, gives you some clues.\n\n1:31:28.240 --> 1:31:30.720\n So what year do you think, what's your intuition?\n\n1:31:30.720 --> 1:31:34.880\n What date range are you currently thinking San Francisco would be?\n\n1:31:34.880 --> 1:31:42.960\n Are you currently thinking San Francisco would be autonomous taxi service from any point\n\n1:31:42.960 --> 1:31:45.840\n A to any point B without a driver?\n\n1:31:47.760 --> 1:31:53.040\n Are you still, are you thinking 10 years from now, 20 years from now, 30 years from now?\n\n1:31:53.040 --> 1:31:54.400\n Certainly not 10 years from now.\n\n1:31:55.440 --> 1:31:56.320\n It's going to be longer.\n\n1:31:56.880 --> 1:31:59.520\n If you're allowed to go south of market way longer.\n\n1:31:59.520 --> 1:32:03.440\n And unless it's reengineering of roads.\n\n1:32:03.440 --> 1:32:05.120\n By the way, what's the biggest challenge?\n\n1:32:05.120 --> 1:32:06.080\n You mentioned a few.\n\n1:32:06.080 --> 1:32:09.360\n Is it, is it the delivery trucks?\n\n1:32:09.360 --> 1:32:15.040\n Is it the edge cases, the computer perception, well, here's a case that I saw outside my\n\n1:32:15.040 --> 1:32:20.560\n house a few weeks ago, about 8pm on a Friday night, it was getting dark, it was before\n\n1:32:20.560 --> 1:32:21.120\n the solstice.\n\n1:32:23.520 --> 1:32:32.080\n It was a cruise vehicle come down the hill, turned right and stopped dead, covering the\n\n1:32:32.080 --> 1:32:33.600\n crosswalk.\n\n1:32:33.600 --> 1:32:35.120\n Why did it stop dead?\n\n1:32:35.120 --> 1:32:38.480\n Because there was a human just two feet from it.\n\n1:32:38.480 --> 1:32:41.680\n Now, I just glanced, I knew what was happening.\n\n1:32:41.680 --> 1:32:47.840\n The human was a woman was at the door of her car trying to unlock it with one of those\n\n1:32:47.840 --> 1:32:49.360\n things that, you know, when you don't have a key.\n\n1:32:50.480 --> 1:32:54.720\n That car thought, oh, she could jump out in front of me any second.\n\n1:32:55.520 --> 1:32:57.760\n As a human, I could tell, no, she's not going to jump out.\n\n1:32:57.760 --> 1:32:59.360\n She's busy trying to unlock her.\n\n1:32:59.360 --> 1:33:00.240\n She's lost her keys.\n\n1:33:00.240 --> 1:33:01.200\n She's trying to get in the car.\n\n1:33:01.200 --> 1:33:05.440\n And it stayed there for, until I got bored.\n\n1:33:05.440 --> 1:33:11.600\n And so the human driver in there did not take over.\n\n1:33:11.600 --> 1:33:14.080\n But here's the kicker to me.\n\n1:33:14.080 --> 1:33:22.720\n A guy comes down the hill with a stroller, I assume there's a baby in there, and now\n\n1:33:22.720 --> 1:33:25.760\n the crosswalk's blocked by this cruise vehicle.\n\n1:33:25.760 --> 1:33:27.440\n What's he going to do?\n\n1:33:27.440 --> 1:33:30.800\n Cleverly, I think, he decided not to go in front of the car.\n\n1:33:30.800 --> 1:33:34.960\n But he had to go behind it.\n\n1:33:34.960 --> 1:33:39.360\n He had to get off the crosswalk, out into the intersection, to push his baby around\n\n1:33:39.360 --> 1:33:41.200\n this car, which was stopped there.\n\n1:33:41.200 --> 1:33:44.080\n And no human driver would have stopped there for that length of time.\n\n1:33:44.880 --> 1:33:46.160\n They would have got out and out of the way.\n\n1:33:46.880 --> 1:33:56.000\n And that's another one of my pet peeves, that safety is being compromised for individuals\n\n1:33:56.000 --> 1:33:59.760\n who didn't sign up for having this happen in their neighborhood.\n\n1:33:59.760 --> 1:34:03.200\n Now you can say that's an edge case, but...\n\n1:34:03.200 --> 1:34:13.040\n Yeah, well, I'm in general not a fan of anecdotal evidence for stuff like this is one of my\n\n1:34:13.040 --> 1:34:17.920\n biggest problems with the discussion of autonomous vehicles in general, people that criticize\n\n1:34:17.920 --> 1:34:24.640\n them or support them are using edge cases, are using anecdotal evidence, but I got you.\n\n1:34:24.640 --> 1:34:26.800\n Your question is, when is it going to happen in San Francisco?\n\n1:34:26.800 --> 1:34:29.040\n I say not soon, but it's going to be one of them.\n\n1:34:29.040 --> 1:34:38.640\n But where it is going to happen is in limited domains, campuses of various sorts, gated\n\n1:34:38.640 --> 1:34:45.120\n communities where the other drivers are not arbitrary people.\n\n1:34:46.000 --> 1:34:52.800\n They're people who know about these things, they've been warned about them, and at velocities\n\n1:34:52.800 --> 1:34:55.520\n where it's always safe to stop dead.\n\n1:34:57.120 --> 1:34:58.720\n You can't do that on the freeway.\n\n1:34:58.720 --> 1:35:06.160\n That I think we're going to start to see, and they may not be shaped like current cars,\n\n1:35:06.160 --> 1:35:12.560\n they may be things like May Mobility has those things and various companies have these.\n\n1:35:12.560 --> 1:35:14.400\n Yeah, I wonder if that's a compelling experience.\n\n1:35:14.400 --> 1:35:20.320\n To me, it's not just about automation, it's about creating a product that makes your...\n\n1:35:20.320 --> 1:35:23.680\n It's not just cheaper, but it's fun to ride.\n\n1:35:23.680 --> 1:35:29.600\n One of the least fun things is for a car that stops and waits.\n\n1:35:29.600 --> 1:35:34.400\n There's something deeply frustrating for us humans for the rest of the world to take advantage\n\n1:35:34.400 --> 1:35:35.520\n of us as we wait.\n\n1:35:35.520 --> 1:35:47.520\n But think about not you as the customer, but someone who's in their 80s in a retirement\n\n1:35:47.520 --> 1:35:53.200\n village whose kids have said, you're not driving anymore, and this gives you the freedom to\n\n1:35:53.200 --> 1:35:54.240\n go to the market.\n\n1:35:54.240 --> 1:35:59.840\n That's a hugely beneficial thing, but it's a very few orders of magnitude less impact\n\n1:35:59.840 --> 1:36:00.800\n on the world.\n\n1:36:00.800 --> 1:36:05.760\n It's just a few people in a small community using cars as opposed to the entirety of the\n\n1:36:05.760 --> 1:36:06.080\n world.\n\n1:36:07.920 --> 1:36:13.600\n I like that the first time that a car equipped with some version of a solution to the trolley\n\n1:36:13.600 --> 1:36:14.800\n problem is...\n\n1:36:14.800 --> 1:36:16.400\n What's NIML stand for?\n\n1:36:16.400 --> 1:36:17.040\n Not in my life.\n\n1:36:17.040 --> 1:36:17.680\n Not in my life.\n\n1:36:17.680 --> 1:36:20.080\n I define my lifetime as up to 2050.\n\n1:36:20.080 --> 1:36:28.640\n You know, I ask you, when have you had to decide which person shall I kill?\n\n1:36:29.360 --> 1:36:31.760\n No, you put the brakes on and you break as hard as you can.\n\n1:36:31.760 --> 1:36:35.360\n You're not making that decision.\n\n1:36:35.360 --> 1:36:41.280\n I do think autonomous vehicles or semi autonomous vehicles do need to solve the whole pedestrian\n\n1:36:41.280 --> 1:36:45.520\n problem that has elements of the trolley problem within it, but it's not...\n\n1:36:45.520 --> 1:36:51.760\n Yeah, well, and I talk about it in one of the articles or blog posts that I wrote, and\n\n1:36:51.760 --> 1:36:55.120\n people have told me, one of my coworkers has told me he does this.\n\n1:36:56.480 --> 1:37:01.600\n He tortures autonomously driven vehicles and pedestrians will torture them.\n\n1:37:01.600 --> 1:37:07.360\n Now, once they realize that putting one foot off the curb makes the car think that they\n\n1:37:07.360 --> 1:37:10.800\n might walk into the road, teenagers will be doing that all the time.\n\n1:37:10.800 --> 1:37:15.440\n I, by the way, one of my, and this is a whole nother discussion, because my main interest\n\n1:37:15.440 --> 1:37:18.400\n with robotics is HRI, human robot interaction.\n\n1:37:19.200 --> 1:37:24.080\n I believe that robots that interact with humans will have to push back.\n\n1:37:25.520 --> 1:37:30.480\n Like they can't just be bullied because that creates a very uncompelling experience for\n\n1:37:30.480 --> 1:37:31.280\n the humans.\n\n1:37:31.280 --> 1:37:35.600\n Yeah, well, you know, Waymo, before it was called Waymo, discovered that, you know, they\n\n1:37:35.600 --> 1:37:38.080\n had to do that at four way intersections.\n\n1:37:38.080 --> 1:37:42.800\n They had to nudge forward to give the cue that they were going to go, because otherwise\n\n1:37:42.800 --> 1:37:45.680\n the other drivers would just beat them all the time.\n\n1:37:46.400 --> 1:37:52.320\n So you cofounded iRobot, as we mentioned, one of the most successful robotics companies\n\n1:37:52.320 --> 1:37:52.800\n ever.\n\n1:37:53.520 --> 1:38:00.480\n What are you most proud of with that company and the approach you took to robotics?\n\n1:38:00.480 --> 1:38:07.840\n Well, there's something I'm quite proud of there, which may be a surprise, but, you know,\n\n1:38:07.840 --> 1:38:17.280\n I was still on the board when this happened, it was March 2011, and we sent robots to Japan\n\n1:38:17.280 --> 1:38:27.520\n and they were used to help shut down the Fukushima Daiichi nuclear power plant, which was, everything\n\n1:38:27.520 --> 1:38:32.240\n was, I've been there since, I was there in 2014, and the robots, some of the robots were\n\n1:38:32.240 --> 1:38:33.120\n still there.\n\n1:38:33.120 --> 1:38:35.600\n I was proud that we were able to do that.\n\n1:38:35.600 --> 1:38:37.280\n Why were we able to do that?\n\n1:38:38.000 --> 1:38:42.000\n And, you know, people have said, well, you know, Japan is so good at robotics.\n\n1:38:42.960 --> 1:38:51.600\n It was because we had had about 6,500 robots deployed in Iraq and Afghanistan, teleopt,\n\n1:38:51.600 --> 1:38:55.920\n but with intelligence, dealing with roadside bombs.\n\n1:38:56.480 --> 1:39:03.360\n So we had, it was at that time, nine years of in field experience with the robots in\n\n1:39:03.360 --> 1:39:09.200\n harsh conditions, whereas the Japanese robots, which were, you know, getting, this goes back\n\n1:39:09.200 --> 1:39:14.560\n to what annoys me so much, getting all the hype, look at that, look at that Honda robot,\n\n1:39:14.560 --> 1:39:20.800\n it can walk, wow, the future's here, couldn't do a thing because they weren't deployed,\n\n1:39:20.800 --> 1:39:26.960\n but we had deployed in really harsh conditions for a long time, and so we're able to do\n\n1:39:26.960 --> 1:39:30.400\n something very positive in a very bad situation.\n\n1:39:30.400 --> 1:39:36.640\n What about just the simple, and for people who don't know, one of the things that iRobot\n\n1:39:36.640 --> 1:39:40.400\n has created is the Roomba vacuum cleaner.\n\n1:39:42.320 --> 1:39:47.760\n What about the simple robot that, that is the Roomba, quote unquote, simple, that's\n\n1:39:47.760 --> 1:39:51.760\n deployed in tens of millions of, in tens of millions of homes?\n\n1:39:53.200 --> 1:39:54.240\n What do you think about that?\n\n1:39:54.240 --> 1:39:59.440\n Well, I make the joke that I started out life as a pure mathematician and turned into a\n\n1:39:59.440 --> 1:40:05.440\n vacuum cleaner salesman, so if you're going to be an entrepreneur, be ready for, be ready\n\n1:40:05.440 --> 1:40:15.040\n to do anything, but I was, you know, there was a, there was a wacky lawsuit that I got\n\n1:40:15.040 --> 1:40:20.800\n opposed for not too many years ago, and I was the only one who had emailed from the\n\n1:40:20.800 --> 1:40:27.520\n 1990s, and no one in the company had it, so I went and went through my email, and it\n\n1:40:27.520 --> 1:40:34.880\n reminded me of, you know, the joy of what we were doing, and what was I doing?\n\n1:40:34.880 --> 1:40:41.040\n What was I doing at the time we were building, building the Roomba?\n\n1:40:41.920 --> 1:40:46.160\n One of the things was we had this, you know, incredibly tight budget because we wanted\n\n1:40:46.160 --> 1:40:50.960\n to put it on the shelves at $200.\n\n1:40:50.960 --> 1:40:59.120\n There was another home cleaning robot at the time, it was the Electrolux Trilobite, which\n\n1:40:59.120 --> 1:41:05.360\n sold for 2,000 euros, and to us that was not going to be a consumer product, so we had\n\n1:41:05.360 --> 1:41:10.480\n reason to believe that $200 was a, was a thing that people would buy at.\n\n1:41:10.480 --> 1:41:19.120\n That was our aim, but that meant we had, you know, that's on the shelf making profit.\n\n1:41:19.120 --> 1:41:26.560\n That means the cost of goods has to be minimal, so I find all these emails of me going, you\n\n1:41:26.560 --> 1:41:32.000\n know, I'd be in Taipei for a MIT meeting, and I'd stay a few extra days and go down\n\n1:41:32.000 --> 1:41:38.800\n to Hsinchu and talk to these little tiny companies, lots of little tiny companies outside of TSMC,\n\n1:41:38.800 --> 1:41:45.440\n Taiwan Semiconductor Manufacturing Corporation, which let all these little companies be fabulous.\n\n1:41:45.440 --> 1:41:51.760\n They didn't have to have their own fab so they could innovate, and they were building,\n\n1:41:51.760 --> 1:41:57.840\n their innovations were to build, strip down 6802s, 6802 was what was in an Apple I, get\n\n1:41:57.840 --> 1:42:03.600\n rid of half the silicon and still have it be viable, and I'd previously got some of\n\n1:42:03.600 --> 1:42:11.520\n those for some earlier failed products of iRobot, and that was in Hong Kong going to\n\n1:42:11.520 --> 1:42:16.800\n all these companies that built, you know, they weren't gaming in the current sense,\n\n1:42:16.800 --> 1:42:23.360\n there were these handheld games that you would play, or birthday cards, because we had about\n\n1:42:23.360 --> 1:42:30.640\n a 50 cent budget for computation, so I'm trekking from place to place looking at their chips,\n\n1:42:30.640 --> 1:42:38.320\n looking at what they'd removed, ah, their interrupt handling is too weak for a general\n\n1:42:38.320 --> 1:42:43.440\n purpose, so I was going deep technical detail, and then I found this one from a company called\n\n1:42:43.440 --> 1:42:50.000\n Winbond, which had, and I'd forgotten it had this much RAM, it had 512 bytes of RAM,\n\n1:42:50.000 --> 1:42:54.640\n and it was in our budget, and it had all the capabilities we needed.\n\n1:42:54.640 --> 1:42:57.200\n Yeah, and you were excited.\n\n1:42:57.200 --> 1:43:02.400\n Yeah, and I was reading all these emails, Colin, I found this, so.\n\n1:43:02.400 --> 1:43:05.840\n Did you think, did you ever think that you guys could be so successful?\n\n1:43:07.200 --> 1:43:10.960\n Like, eventually this company would be so successful, could you possibly have imagined?\n\n1:43:12.240 --> 1:43:13.760\n No, we never did think that.\n\n1:43:13.760 --> 1:43:19.200\n We'd had 14 failed business models up to 2002, and then we had two winners the same year.\n\n1:43:19.200 --> 1:43:27.600\n No, and then, you know, we, I remember the board, because by this time we had some venture\n\n1:43:27.600 --> 1:43:36.240\n capital in, the board went along with us building some robots for, you know, aiming at the Christmas\n\n1:43:36.240 --> 1:43:44.640\n 2002 market, and we went three times over what they authorized and built 70,000 of them,\n\n1:43:44.640 --> 1:43:51.200\n and sold them all in that first, because we released on September 18th, and they were\n\n1:43:51.200 --> 1:43:52.560\n all sold by Christmas.\n\n1:43:52.560 --> 1:43:57.040\n So it was, so we were gutsy, but.\n\n1:43:57.040 --> 1:44:00.640\n But yeah, you didn't think this will take over the world.\n\n1:44:00.640 --> 1:44:09.040\n Well, this is, so a lot of amazing robotics companies have gone under over the past few\n\n1:44:09.040 --> 1:44:10.560\n decades.\n\n1:44:10.560 --> 1:44:17.680\n Why do you think it's so damn hard to run a successful robotics company?\n\n1:44:17.680 --> 1:44:18.960\n There's a few things.\n\n1:44:20.960 --> 1:44:28.960\n One is expectations of capabilities by the founders that are off base.\n\n1:44:29.680 --> 1:44:31.600\n The founders, not the consumer, the founders.\n\n1:44:31.600 --> 1:44:34.000\n Yeah, expectations of what can be delivered.\n\n1:44:34.000 --> 1:44:34.500\n Sure.\n\n1:44:34.500 --> 1:44:42.180\n Mispricing, and what a customer thinks is a valid price, is not rational, necessarily.\n\n1:44:42.180 --> 1:44:42.680\n Yeah.\n\n1:44:43.620 --> 1:44:56.100\n And expectations of customers, and just the sheer hardness of getting people to adopt a\n\n1:44:56.100 --> 1:44:57.060\n new technology.\n\n1:44:57.060 --> 1:44:59.700\n And I've suffered from all three of these, you know.\n\n1:44:59.700 --> 1:45:04.820\n I've had more failures than successes, in terms of companies.\n\n1:45:04.820 --> 1:45:06.180\n I've suffered from all three.\n\n1:45:07.860 --> 1:45:18.580\n So, do you think one day there will be a robotics company, and by robotics company, I mean, where\n\n1:45:18.580 --> 1:45:24.740\n your primary source of income is from robots, that will be a trillion plus dollar company?\n\n1:45:24.740 --> 1:45:31.460\n And if so, what would that company do?\n\n1:45:31.460 --> 1:45:35.300\n I can't, you know, because I'm still starting robot companies.\n\n1:45:35.300 --> 1:45:35.800\n Yeah.\n\n1:45:38.180 --> 1:45:41.380\n I'm not making any such predictions in my own mind.\n\n1:45:41.380 --> 1:45:43.140\n I'm not thinking about a trillion dollar company.\n\n1:45:43.140 --> 1:45:47.220\n And by the way, I don't think, you know, in the 90s, anyone was thinking that Apple would\n\n1:45:47.220 --> 1:45:48.580\n ever be a trillion dollar company.\n\n1:45:48.580 --> 1:45:52.580\n So, these are, these are, you know, these are, you know, these are, you know, these\n\n1:45:52.580 --> 1:45:57.220\n would be a trillion dollar company, so these are, these are very hard to predict.\n\n1:45:57.220 --> 1:46:03.460\n But, sorry to interrupt, but don't you, because I kind of have a vision in a small way, and\n\n1:46:03.460 --> 1:46:08.580\n it's a big vision in a small way, that I see that there would be robots in the home,\n\n1:46:10.180 --> 1:46:12.420\n at scale, like Roomba, but more.\n\n1:46:13.540 --> 1:46:14.980\n And that's trillion dollar.\n\n1:46:15.620 --> 1:46:16.120\n Right.\n\n1:46:16.120 --> 1:46:22.100\n And I think there's a real market pull for them because of the demographic inversion,\n\n1:46:22.100 --> 1:46:25.220\n you know, who's going to do all the stuff for the older people?\n\n1:46:26.180 --> 1:46:29.620\n There's too many, you know, I'm leading here.\n\n1:46:31.700 --> 1:46:32.980\n There's going to be too many of us.\n\n1:46:36.420 --> 1:46:41.540\n But we don't have capable enough robots to make that economic argument at this point.\n\n1:46:42.340 --> 1:46:44.180\n Do I expect that that will happen?\n\n1:46:44.180 --> 1:46:45.380\n Yes, I expect it will happen.\n\n1:46:45.380 --> 1:46:50.580\n But I got to tell you, we introduced the Roomba in 2002, and I stayed another\n\n1:46:50.580 --> 1:46:51.780\n nine years.\n\n1:46:51.780 --> 1:46:57.700\n We were always trying to find what the next home robot would be, and still today, the\n\n1:46:57.700 --> 1:47:02.660\n primary product of 20 years late, almost 20 years later, 19 years later, the primary product\n\n1:47:02.660 --> 1:47:03.620\n is still the Roomba.\n\n1:47:03.620 --> 1:47:07.060\n So iRobot hasn't found the next one.\n\n1:47:07.060 --> 1:47:12.580\n Do you think it's possible for one person in the garage to build it versus, like, Google\n\n1:47:12.580 --> 1:47:16.340\n launching Google self driving car that turns into Waymo?\n\n1:47:16.340 --> 1:47:20.980\n Do you think this is almost like what it takes to build a successful robotics company?\n\n1:47:20.980 --> 1:47:24.420\n Do you think it's possible to go from the ground up, or is it just too much capital\n\n1:47:24.420 --> 1:47:24.980\n investment?\n\n1:47:25.540 --> 1:47:31.700\n Yeah, so it's very hard to get there without a lot of capital.\n\n1:47:31.700 --> 1:47:38.100\n And we're starting to see, you know, fair chunks of capital for some robotics companies.\n\n1:47:38.100 --> 1:47:45.540\n You know, Series B's, I saw one yesterday for $80 million, I think it was, for Covariant.\n\n1:47:45.540 --> 1:47:54.740\n But it can take real money to get into these things, and you may fail along the way.\n\n1:47:54.740 --> 1:48:00.900\n I've certainly failed at Rethink Robotics, and we lost $150 million in capital there.\n\n1:48:00.900 --> 1:48:05.700\n So, okay, so Rethink Robotics is another amazing robotics company you cofounded.\n\n1:48:06.580 --> 1:48:08.100\n So what was the vision there?\n\n1:48:09.060 --> 1:48:11.140\n What was the dream?\n\n1:48:11.140 --> 1:48:15.620\n And what are you most proud of with Rethink Robotics?\n\n1:48:15.620 --> 1:48:23.140\n I'm most proud of the fact that we got robots out of the cage in factories that were safe,\n\n1:48:23.140 --> 1:48:26.180\n absolutely safe, for people and robots to be next to each other.\n\n1:48:26.180 --> 1:48:27.700\n So these are robotic arms.\n\n1:48:27.700 --> 1:48:28.500\n Robotic arms.\n\n1:48:28.500 --> 1:48:31.140\n Able to pick up stuff and interact with humans.\n\n1:48:31.140 --> 1:48:35.140\n Yeah, and that humans could retask them without writing code.\n\n1:48:35.140 --> 1:48:40.020\n And now that's sort of become an expectation for a lot of other little companies and big\n\n1:48:40.020 --> 1:48:42.260\n companies, our advertising they're doing.\n\n1:48:42.260 --> 1:48:45.540\n That's both an interface problem and also a safety problem.\n\n1:48:45.540 --> 1:48:46.580\n Yeah, yeah.\n\n1:48:47.620 --> 1:48:49.460\n So I'm most proud of that.\n\n1:48:51.300 --> 1:48:58.580\n I completely, I let myself be talked out of what I wanted to do.\n\n1:48:59.380 --> 1:49:02.260\n And, you know, you always got, you know, I can't replay the tape.\n\n1:49:02.260 --> 1:49:05.460\n I can't replay it.\n\n1:49:05.460 --> 1:49:12.180\n Maybe, you know, if I'd been stronger on, and I remember the day, I remember the exact\n\n1:49:12.180 --> 1:49:12.580\n meeting.\n\n1:49:13.860 --> 1:49:15.380\n Can you take me through that meeting?\n\n1:49:16.260 --> 1:49:16.580\n Yeah.\n\n1:49:18.340 --> 1:49:23.940\n So I'd said that I'd set as a target for the company that we were going to build $3,000\n\n1:49:23.940 --> 1:49:29.060\n robots with force feedback that was safe for people to be around.\n\n1:49:29.700 --> 1:49:30.420\n Wow.\n\n1:49:30.420 --> 1:49:31.380\n That was my goal.\n\n1:49:31.380 --> 1:49:38.980\n And we built, so we started in 2008, and we had prototypes built of plastic, plastic\n\n1:49:38.980 --> 1:49:48.180\n gearboxes, and at a $3,000, you know, lifetime, or $3,000, I was saying, we're going to go\n\n1:49:48.180 --> 1:49:52.500\n after not the people who already have robot arms in factories, the people who would never\n\n1:49:52.500 --> 1:49:53.940\n have a robot arm.\n\n1:49:53.940 --> 1:49:55.940\n We're going to go after a different market.\n\n1:49:55.940 --> 1:49:57.940\n So we don't have to meet their expectations.\n\n1:49:57.940 --> 1:49:59.860\n And so we're going to build it out of plastic.\n\n1:49:59.860 --> 1:50:02.740\n It doesn't have to have a $35,000 lifetime.\n\n1:50:02.740 --> 1:50:05.460\n It's going to be so cheap that it's OpEx, not CapEx.\n\n1:50:09.140 --> 1:50:16.980\n And so we had a prototype that worked reasonably well, but the control engineers were complaining\n\n1:50:16.980 --> 1:50:24.820\n about these plastic gearboxes with a beautiful little planetary gearbox that we could use\n\n1:50:24.820 --> 1:50:29.780\n something called series elastic actuators.\n\n1:50:29.780 --> 1:50:30.980\n We embedded them in there.\n\n1:50:30.980 --> 1:50:32.180\n We could measure forces.\n\n1:50:32.180 --> 1:50:34.500\n We knew when we hit something, et cetera.\n\n1:50:35.060 --> 1:50:40.100\n The control engineers were saying, yeah, but there's this torque ripple because these plastic\n\n1:50:40.100 --> 1:50:44.900\n gears, they're not great gears, and there's this ripple, and trying to do force control\n\n1:50:44.900 --> 1:50:47.220\n around this ripple is so hard.\n\n1:50:47.220 --> 1:50:55.140\n And I'm not going to name names, but I remember one of the mechanical engineers saying, we'll\n\n1:50:55.140 --> 1:50:59.620\n just build a metal gearbox with spur gears, and it'll take six weeks.\n\n1:50:59.620 --> 1:51:00.340\n We'll be done.\n\n1:51:01.140 --> 1:51:02.020\n Problem solved.\n\n1:51:03.700 --> 1:51:06.660\n Two years later, we got the spur gearbox working.\n\n1:51:08.020 --> 1:51:15.540\n We cost reduced it every possible way we could, but now the price went up too.\n\n1:51:15.540 --> 1:51:19.300\n And then the CEO at the time said, well, we have to have two arms, not one arm.\n\n1:51:19.860 --> 1:51:27.460\n So our first robot product, Baxter, now cost $25,000, and the only people who were going\n\n1:51:27.460 --> 1:51:31.460\n to look at that were people who had arms in factories because that was somewhat cheaper\n\n1:51:31.460 --> 1:51:33.460\n for two arms than arms in factories.\n\n1:51:34.180 --> 1:51:43.700\n But they were used to 0.1 millimeter reproducibility of motion and certain velocities, and I kept\n\n1:51:43.700 --> 1:51:45.620\n thinking, but that's not what we're giving you.\n\n1:51:45.620 --> 1:51:47.380\n You don't need position repeatability.\n\n1:51:47.380 --> 1:51:49.700\n Use force control like a human does.\n\n1:51:49.700 --> 1:51:53.060\n No, no, but we want that repeatability.\n\n1:51:53.060 --> 1:51:54.500\n We want that repeatability.\n\n1:51:54.500 --> 1:51:56.340\n All the other robots have that repeatability.\n\n1:51:56.340 --> 1:51:58.500\n Why don't you have that repeatability?\n\n1:51:58.500 --> 1:51:59.780\n So can you clarify?\n\n1:51:59.780 --> 1:52:02.900\n Force control is you can grab the arm and you can move it.\n\n1:52:02.900 --> 1:52:04.660\n You can move it around, but suppose you...\n\n1:52:06.100 --> 1:52:06.900\n Can you see that?\n\n1:52:06.900 --> 1:52:07.540\n Yes.\n\n1:52:07.540 --> 1:52:08.820\n Suppose you want to...\n\n1:52:09.940 --> 1:52:10.440\n Yes.\n\n1:52:10.440 --> 1:52:15.160\n Suppose this thing is a precise thing that's got to fit here in this right angle.\n\n1:52:16.520 --> 1:52:20.520\n Under position control, you have fixtured where this is.\n\n1:52:20.520 --> 1:52:25.320\n You know where this is precisely, and you just move it, and it goes there.\n\n1:52:25.320 --> 1:52:30.120\n In force control, you would do something like slide over here till we feel that and slide\n\n1:52:30.120 --> 1:52:34.040\n it in there, and that's how a human gets precision.\n\n1:52:34.040 --> 1:52:40.600\n They use force feedback and get the things to mate rather than just go straight to it.\n\n1:52:42.440 --> 1:52:48.120\n Couldn't convince our customers who were in factories and were used to thinking about\n\n1:52:48.120 --> 1:52:51.880\n things a certain way, and they wanted it, wanted it, wanted it.\n\n1:52:51.880 --> 1:52:55.560\n So then we said, okay, we're going to build an arm that gives you that.\n\n1:52:56.120 --> 1:52:59.880\n So now we ended up building a $35,000 robot with one arm with...\n\n1:52:59.880 --> 1:53:01.800\n Oh, what are they called?\n\n1:53:04.840 --> 1:53:08.520\n A certain sort of gearbox made by a company whose name I can't remember right now, but\n\n1:53:08.520 --> 1:53:09.480\n it's the name of the gearbox.\n\n1:53:11.880 --> 1:53:14.760\n But it's got torque ripple in it.\n\n1:53:15.560 --> 1:53:19.720\n So now there was an extra two years of solving the problem of doing the force with the torque\n\n1:53:19.720 --> 1:53:20.200\n ripple.\n\n1:53:20.200 --> 1:53:28.440\n So we had to do the thing we had avoided for the plastic gearboxes, which is a little bit\n\n1:53:28.440 --> 1:53:31.240\n for the plastic gearboxes we ended up having to do.\n\n1:53:31.240 --> 1:53:35.240\n The robot was now overpriced and they...\n\n1:53:35.240 --> 1:53:38.680\n And that was your intuition from the very beginning kind of that this is not...\n\n1:53:40.040 --> 1:53:44.760\n You're opening a door to solve a lot of problems that you're eventually going to have to solve\n\n1:53:44.760 --> 1:53:45.800\n this problem anyway.\n\n1:53:45.800 --> 1:53:46.120\n Yeah.\n\n1:53:46.120 --> 1:53:49.240\n And also I was aiming at a low price to go into a different market.\n\n1:53:49.240 --> 1:53:49.720\n Low price.\n\n1:53:50.280 --> 1:53:51.160\n That didn't have robots.\n\n1:53:51.160 --> 1:53:52.600\n $3,000 would be amazing.\n\n1:53:52.600 --> 1:53:52.760\n Yeah.\n\n1:53:52.760 --> 1:53:54.120\n I think we could have done it for five.\n\n1:53:54.120 --> 1:53:58.840\n But, you know, you talked about setting the goal a little too far for the engineers.\n\n1:53:58.840 --> 1:53:59.640\n Yeah, exactly.\n\n1:54:02.280 --> 1:54:07.400\n So why would you say that company not failed, but went under?\n\n1:54:09.000 --> 1:54:15.400\n We had buyers and there's this thing called the Committee on Foreign Investment in the\n\n1:54:15.400 --> 1:54:16.600\n U.S., CFIUS.\n\n1:54:18.120 --> 1:54:21.640\n And that had previously been invoked twice.\n\n1:54:21.640 --> 1:54:27.960\n Around where the government could stop foreign money coming into a U.S. company based on\n\n1:54:29.640 --> 1:54:31.000\n defense requirements.\n\n1:54:32.680 --> 1:54:34.600\n We went through due diligence multiple times.\n\n1:54:34.600 --> 1:54:42.280\n We were going to get acquired, but every consortium had Chinese money in it, and all the bankers\n\n1:54:42.280 --> 1:54:47.080\n would say at the last minute, you know, this isn't going to get past CFIUS, and the investors\n\n1:54:47.080 --> 1:54:47.880\n would go away.\n\n1:54:47.880 --> 1:54:54.280\n And then we had two buyers, once we were about to run out of money, two buyers, and one used\n\n1:54:54.280 --> 1:55:01.960\n heavy handed legal stuff with the other one, said they were going to take it and pay more,\n\n1:55:02.760 --> 1:55:08.040\n dropped out when we were out of cash, and then bought the assets at 1 30th of the price\n\n1:55:08.040 --> 1:55:09.320\n they had offered a week before.\n\n1:55:10.920 --> 1:55:12.280\n It was a tough week.\n\n1:55:12.280 --> 1:55:21.640\n Do you, does it hurt to think about like an amazing company that didn't, you know, like\n\n1:55:21.640 --> 1:55:23.640\n iRobot didn't find a way?\n\n1:55:24.440 --> 1:55:25.400\n Yeah, it was tough.\n\n1:55:25.400 --> 1:55:27.480\n I said I was never going to start another company.\n\n1:55:27.480 --> 1:55:36.360\n I was pleased that everyone liked what we did so much that the team was hired by three\n\n1:55:36.360 --> 1:55:40.040\n companies, and I was very happy that we were able to do that.\n\n1:55:40.040 --> 1:55:42.920\n Three companies within a week.\n\n1:55:42.920 --> 1:55:44.760\n Everyone had a job in one of these three companies.\n\n1:55:44.760 --> 1:55:49.800\n Some stayed in their same desks because another company came in and rented the space.\n\n1:55:50.680 --> 1:55:54.840\n So I felt good about people not being out on the street.\n\n1:55:55.720 --> 1:55:57.880\n So Baxter has a screen with a face.\n\n1:55:59.560 --> 1:56:05.880\n What, that's a revolutionary idea for a robot manipulation, like for a robotic arm.\n\n1:56:07.320 --> 1:56:08.840\n How much opposition did you get?\n\n1:56:08.840 --> 1:56:12.920\n Well, first the screen was also used during codeless programming.\n\n1:56:12.920 --> 1:56:14.440\n We taught by demonstration.\n\n1:56:14.440 --> 1:56:17.000\n It showed you what its understanding of the task was.\n\n1:56:17.640 --> 1:56:18.680\n So it had two roles.\n\n1:56:21.240 --> 1:56:26.520\n Some customers hated it, and so we made it so that when the robot was running it could\n\n1:56:26.520 --> 1:56:30.200\n be showing graphs of what was happening and not show the eyes.\n\n1:56:30.200 --> 1:56:36.600\n Other people, and some of them surprised me who they were, saying well this one doesn't\n\n1:56:36.600 --> 1:56:37.960\n look as human as the old one.\n\n1:56:37.960 --> 1:56:39.640\n We liked the human looking.\n\n1:56:39.640 --> 1:56:40.120\n Yeah.\n\n1:56:40.120 --> 1:56:41.880\n So there was a mixed bag.\n\n1:56:43.240 --> 1:56:48.760\n But do you think that's, I don't know, I'm kind of disappointed whenever I talk to\n\n1:56:50.360 --> 1:56:55.160\n roboticists, like the best robotics people in the world, they seem to not want to do\n\n1:56:55.160 --> 1:56:56.760\n the eyes type of thing.\n\n1:56:56.760 --> 1:57:02.040\n Like they seem to see it as a machine as opposed to a machine that can also have a human connection.\n\n1:57:02.760 --> 1:57:03.960\n I'm not sure what to do with that.\n\n1:57:03.960 --> 1:57:05.480\n It seems like a lost opportunity.\n\n1:57:05.480 --> 1:57:10.440\n I think the trillion dollar company will have to do the human connection very well no matter\n\n1:57:10.440 --> 1:57:11.160\n what it does.\n\n1:57:11.160 --> 1:57:11.960\n Yeah, I agree.\n\n1:57:13.800 --> 1:57:15.560\n Can I ask you a ridiculous question?\n\n1:57:15.560 --> 1:57:15.880\n Sure.\n\n1:57:17.000 --> 1:57:18.280\n I might give a ridiculous answer.\n\n1:57:19.880 --> 1:57:25.640\n Do you think, well maybe by way of asking the question, let me first mention that you're\n\n1:57:25.640 --> 1:57:29.080\n kind of critical of the idea of the Turing test as a test of intelligence.\n\n1:57:32.280 --> 1:57:33.640\n Let me first ask this question.\n\n1:57:33.640 --> 1:57:40.360\n Do you think we'll be able to build an AI system that humans fall in love with and it\n\n1:57:40.360 --> 1:57:43.960\n falls in love with the human, like romantic love?\n\n1:57:46.920 --> 1:57:51.560\n Well, we've had that with humans falling in love with cars even back in the 50s.\n\n1:57:51.560 --> 1:57:52.680\n It's a different love, right?\n\n1:57:52.680 --> 1:57:53.640\n Well, yeah.\n\n1:57:53.640 --> 1:57:58.680\n I think there's a lifelong partnership where you can communicate and grow like...\n\n1:57:59.640 --> 1:58:01.160\n I think we're a long way from that.\n\n1:58:01.160 --> 1:58:03.000\n I think we're a long, long way.\n\n1:58:03.000 --> 1:58:08.440\n I think Blade Runner had the time scale totally wrong.\n\n1:58:10.440 --> 1:58:16.840\n Yeah, but so to me, honestly, the most difficult part is the thing that you said with the Marvex\n\n1:58:16.840 --> 1:58:21.400\n Paradox is to create a human form that interacts and perceives the world.\n\n1:58:21.400 --> 1:58:28.040\n But if we just look at a voice, like the movie Her or just like an Alexa type voice, I tend\n\n1:58:28.040 --> 1:58:29.560\n to think we're not that far away.\n\n1:58:29.560 --> 1:58:43.400\n Well, for some people, maybe not, but as humans, as we think about the future, we always try\n\n1:58:43.400 --> 1:58:44.200\n to...\n\n1:58:44.200 --> 1:58:46.920\n And this is the premise of most science fiction movies.\n\n1:58:46.920 --> 1:58:49.800\n You've got the world just as it is today and you change one thing.\n\n1:58:50.920 --> 1:58:51.960\n But that's not how...\n\n1:58:51.960 --> 1:58:53.960\n And it's the same with a self driving car.\n\n1:58:53.960 --> 1:58:55.000\n You change one thing.\n\n1:58:55.000 --> 1:58:56.840\n No, everything changes.\n\n1:58:56.840 --> 1:58:59.720\n Everything grows together.\n\n1:58:59.720 --> 1:59:04.520\n So surprisingly, it might be surprising to you or might not, I think the best movie about\n\n1:59:04.520 --> 1:59:07.640\n this stuff was Bicentennial Man.\n\n1:59:09.160 --> 1:59:10.440\n And what was happening there?\n\n1:59:11.080 --> 1:59:14.200\n It was schmaltzy and, you know, but what was happening there?\n\n1:59:15.720 --> 1:59:21.160\n As the robot was trying to become more human, the humans were adopting the technology of\n\n1:59:21.160 --> 1:59:23.080\n the robot and changing their bodies.\n\n1:59:23.080 --> 1:59:27.160\n So there was a convergence happening in a sense.\n\n1:59:27.160 --> 1:59:28.760\n So we will not be the same.\n\n1:59:28.760 --> 1:59:32.440\n You know, we're already talking about genetically modifying our babies.\n\n1:59:32.440 --> 1:59:36.680\n You know, there's more and more stuff happening around that.\n\n1:59:36.680 --> 1:59:41.800\n We will want to modify ourselves even more for all sorts of things.\n\n1:59:43.240 --> 1:59:48.440\n We put all sorts of technology in our bodies to improve it.\n\n1:59:48.440 --> 1:59:53.560\n You know, I've got things in my ears so that I can sort of hear you.\n\n1:59:53.560 --> 1:59:54.060\n Yeah.\n\n1:59:56.120 --> 1:59:57.480\n So we're always modifying our bodies.\n\n1:59:57.480 --> 2:00:02.440\n So, you know, I think it's hard to imagine exactly what it will be like in the future.\n\n2:00:03.640 --> 2:00:09.720\n But on the Turing test side, do you think, so forget about love for a second, let's talk\n\n2:00:09.720 --> 2:00:12.280\n about just like the Alexa Prize.\n\n2:00:12.280 --> 2:00:16.200\n Actually, I was invited to be a part of the Alexa Prize.\n\n2:00:16.200 --> 2:00:22.040\n Actually, I was invited to be a, what is the interviewer for the Alexa Prize or whatever\n\n2:00:23.080 --> 2:00:24.120\n that's in two days.\n\n2:00:25.320 --> 2:00:32.440\n Their idea is success looks like a person wanting to talk to an AI system for a prolonged\n\n2:00:32.440 --> 2:00:33.800\n period of time, like 20 minutes.\n\n2:00:35.080 --> 2:00:41.400\n How far away are we and why is it difficult to build an AI system with which you'd want\n\n2:00:41.400 --> 2:00:45.720\n to have a beer and talk for an hour or two hours?\n\n2:00:45.720 --> 2:00:53.160\n Like not for to check the weather or to check music, but just like to talk as friends.\n\n2:00:53.160 --> 2:01:00.840\n Yeah, well, you know, we saw Weizenbaum back in the 60s with his programmer, Elisa, being\n\n2:01:00.840 --> 2:01:03.080\n shocked at how much people would talk to Elisa.\n\n2:01:03.080 --> 2:01:08.360\n And I remember, you know, in the 70s typing, you know, stuff to Elisa to see what it would\n\n2:01:08.360 --> 2:01:09.000\n come back with.\n\n2:01:09.000 --> 2:01:17.960\n You know, I think right now, and this is a thing that Amazon's been trying to improve\n\n2:01:17.960 --> 2:01:22.760\n with Alexa, there is no continuity of topic.\n\n2:01:22.760 --> 2:01:26.680\n There's not, you can't refer to what we talked about yesterday.\n\n2:01:27.880 --> 2:01:32.360\n It's not the same as talking to a person where there seems to be an ongoing existence, which\n\n2:01:32.360 --> 2:01:32.920\n changes.\n\n2:01:33.800 --> 2:01:37.080\n We share moments together and they last in our memory together.\n\n2:01:37.080 --> 2:01:39.000\n Yeah, there's none of that.\n\n2:01:39.000 --> 2:01:46.840\n And there's no sort of intention of these systems that they have any goal in life, even\n\n2:01:46.840 --> 2:01:51.880\n if it's to be happy, you know, they don't even have a semblance of that.\n\n2:01:51.880 --> 2:01:53.720\n Now, I'm not saying this can't be done.\n\n2:01:53.720 --> 2:01:57.960\n I'm just saying, I think this is why we don't feel that way about them.\n\n2:01:57.960 --> 2:02:01.560\n That's a sort of a minimal requirement.\n\n2:02:01.560 --> 2:02:06.840\n If you want the sort of interaction you're talking about, it's a minimal requirement.\n\n2:02:06.840 --> 2:02:10.360\n Whether it's going to be sufficient, I don't know.\n\n2:02:10.360 --> 2:02:11.560\n We haven't seen it yet.\n\n2:02:11.560 --> 2:02:14.120\n We don't know what it feels like.\n\n2:02:14.120 --> 2:02:23.160\n I tend to think it's not as difficult as solving intelligence, for example, and I think it's\n\n2:02:23.160 --> 2:02:24.680\n achievable in the near term.\n\n2:02:26.680 --> 2:02:32.200\n But on the Turing test, why don't you think the Turing test is a good test of intelligence?\n\n2:02:32.200 --> 2:02:39.080\n Oh, because, you know, again, the Turing, if you read the paper, Turing wasn't saying\n\n2:02:39.080 --> 2:02:40.440\n this is a good test.\n\n2:02:40.440 --> 2:02:46.520\n He was using it as a rhetorical device to argue that if you can't tell the difference\n\n2:02:46.520 --> 2:02:52.920\n between a computer and a person, you must say that the computer's thinking because you\n\n2:02:52.920 --> 2:02:56.040\n can't tell the difference, you know, when it's thinking.\n\n2:02:56.600 --> 2:02:58.280\n You can't say something different.\n\n2:02:58.280 --> 2:03:08.920\n What it has become as this sort of weird game of fooling people, so back at the AI Lab in\n\n2:03:08.920 --> 2:03:14.280\n the late 80s, we had this thing that still goes on called the AI Olympics, and one of\n\n2:03:14.280 --> 2:03:21.320\n the events we had one year was the original imitation game, as Turing talked about, because\n\n2:03:21.320 --> 2:03:25.160\n he starts by saying, can you tell whether it's a man or a woman?\n\n2:03:25.160 --> 2:03:28.680\n So we did that at the Lab.\n\n2:03:28.680 --> 2:03:33.720\n You'd go and type, and the thing would come back, and you had to tell whether it was a\n\n2:03:33.720 --> 2:03:50.920\n man or a woman, and one man came up with a question that he could ask, which was always\n\n2:03:50.920 --> 2:03:55.000\n a dead giveaway of whether the other person was really a man or a woman.\n\n2:03:56.520 --> 2:04:01.400\n He would ask them, did you have green plastic toy soldiers as a kid?\n\n2:04:01.400 --> 2:04:01.880\n Yeah.\n\n2:04:01.880 --> 2:04:03.240\n What did you do with them?\n\n2:04:03.240 --> 2:04:07.160\n And a woman trying to be a man would say, oh, I lined them up.\n\n2:04:07.160 --> 2:04:07.800\n We had wars.\n\n2:04:07.800 --> 2:04:08.760\n We had battles.\n\n2:04:08.760 --> 2:04:11.240\n And the man, just being a man, would say, I stomped on them.\n\n2:04:11.240 --> 2:04:11.960\n I burned them.\n\n2:04:11.960 --> 2:04:21.480\n So that's what the Turing test with computers has become.\n\n2:04:21.480 --> 2:04:22.760\n What's the trick question?\n\n2:04:23.560 --> 2:04:28.040\n That's why I say it's sort of devolved into this weirdness.\n\n2:04:29.480 --> 2:04:35.800\n Nevertheless, conversation not formulated as a test is a fascinatingly challenging dance.\n\n2:04:36.680 --> 2:04:38.200\n That's a really hard problem.\n\n2:04:38.200 --> 2:04:45.720\n To me, conversation, when non poses a test, is a more intuitive illustration how far away\n\n2:04:45.720 --> 2:04:48.760\n we are from solving intelligence than computer vision.\n\n2:04:48.760 --> 2:04:49.240\n It's hard.\n\n2:04:49.960 --> 2:04:53.000\n Computer vision is harder for me to pull apart.\n\n2:04:53.000 --> 2:04:55.400\n But with language, with conversation, you could see.\n\n2:04:55.400 --> 2:04:56.840\n Because language is so human.\n\n2:04:56.840 --> 2:04:57.560\n It's so human.\n\n2:04:58.680 --> 2:05:02.440\n We can so clearly see it.\n\n2:05:04.280 --> 2:05:06.920\n Shit, you mentioned something I was going to go off on.\n\n2:05:06.920 --> 2:05:08.920\n OK.\n\n2:05:08.920 --> 2:05:16.120\n I mean, I have to ask you, because you were the head of CSAIL, AI Lab, for a long time.\n\n2:05:17.560 --> 2:05:18.040\n I don't know.\n\n2:05:18.840 --> 2:05:22.840\n To me, when I came to MIT, you were one of the greats at MIT.\n\n2:05:22.840 --> 2:05:24.120\n So what was that time like?\n\n2:05:25.960 --> 2:05:34.760\n And plus, you're friends with, but you knew Minsky and all the folks there, all the legendary\n\n2:05:34.760 --> 2:05:37.400\n AI people of which you're one.\n\n2:05:37.960 --> 2:05:39.560\n So what was that time like?\n\n2:05:39.560 --> 2:05:46.760\n What are memories that stand out to you from that time, from your time at MIT, from the\n\n2:05:46.760 --> 2:05:53.000\n AI Lab, from the dreams that the AI Lab represented, to the actual revolutionary work?\n\n2:05:53.000 --> 2:05:55.640\n Well, let me tell you first the disappointment in myself.\n\n2:05:56.760 --> 2:06:03.960\n As I've been researching this book, and so many of the players were active in the 50s\n\n2:06:03.960 --> 2:06:08.600\n and 60s, I knew many of them when they were older, and I didn't ask them all the questions\n\n2:06:08.600 --> 2:06:10.440\n now I wish I had asked.\n\n2:06:11.320 --> 2:06:16.760\n I'd sit with them at our Thursday lunches, which we had a faculty lunch, and I didn't\n\n2:06:16.760 --> 2:06:19.720\n ask them so many questions that now I wish I had.\n\n2:06:19.720 --> 2:06:20.840\n Can I ask you that question?\n\n2:06:20.840 --> 2:06:21.880\n Because you wrote that.\n\n2:06:22.440 --> 2:06:25.800\n You wrote that you were fortunate to know and rub shoulders with many of the greats,\n\n2:06:26.600 --> 2:06:30.680\n those who founded AI, robotics, and computer science, and the World Wide Web.\n\n2:06:30.680 --> 2:06:34.760\n And you wrote that your big regret nowadays is that often I have questions for those who\n\n2:06:34.760 --> 2:06:41.560\n have passed on, and I didn't think to ask them any of these questions, even as I saw\n\n2:06:41.560 --> 2:06:44.120\n them and said hello to them on a daily basis.\n\n2:06:44.120 --> 2:06:51.160\n So maybe also another question I want to ask, if you could talk to them today, what question\n\n2:06:51.160 --> 2:06:51.960\n would you ask?\n\n2:06:51.960 --> 2:06:53.240\n What questions would you ask?\n\n2:06:53.240 --> 2:06:56.440\n Well, Licklider, I would ask him.\n\n2:06:56.440 --> 2:07:02.600\n You know, he had the vision for humans and computers working together, and he really\n\n2:07:02.600 --> 2:07:10.680\n founded that at DARPA, and he gave the money to MIT, which started Project MAC in 1963.\n\n2:07:12.360 --> 2:07:16.200\n And I would have talked to him about what the successes were, what the failures were,\n\n2:07:16.200 --> 2:07:18.680\n what he saw as progress, etc.\n\n2:07:18.680 --> 2:07:24.680\n I would have asked him more questions about that, because now I could use it in my book,\n\n2:07:24.680 --> 2:07:25.880\n you know, but I think it's lost.\n\n2:07:25.880 --> 2:07:26.920\n It's lost forever.\n\n2:07:26.920 --> 2:07:28.680\n A lot of the motivations are lost.\n\n2:07:33.240 --> 2:07:40.840\n I should have asked Marvin why he and Seymour Pappert came down so hard on neural networks\n\n2:07:40.840 --> 2:07:48.440\n in 1968 in their book Perceptrons, because Marvin's PhD thesis was all about neural networks.\n\n2:07:48.440 --> 2:07:50.280\n And how do you make sense of that?\n\n2:07:50.280 --> 2:07:52.040\n That book destroyed the field.\n\n2:07:52.040 --> 2:07:56.280\n He probably, do you think he knew the effect that book would have?\n\n2:07:59.480 --> 2:08:01.240\n All the theorems are negative theorems.\n\n2:08:02.280 --> 2:08:02.780\n Yeah.\n\n2:08:03.880 --> 2:08:04.380\n Yeah.\n\n2:08:04.920 --> 2:08:05.960\n So, yeah.\n\n2:08:05.960 --> 2:08:09.800\n That's just the way of, that's the way of life.\n\n2:08:10.920 --> 2:08:15.800\n But still, it's kind of tragic that he was both the proponent and the destroyer of neural\n\n2:08:15.800 --> 2:08:16.360\n networks.\n\n2:08:16.360 --> 2:08:19.160\n Yeah.\n\n2:08:19.160 --> 2:08:25.160\n Is there other memories stand out from the robotics and the AI work at MIT?\n\n2:08:28.120 --> 2:08:30.600\n Well, yeah, but you gotta be more specific.\n\n2:08:31.320 --> 2:08:33.160\n Well, I mean, like, it's such a magical place.\n\n2:08:33.160 --> 2:08:40.520\n I mean, to me, it's a little bit also heartbreaking that, you know, with Google and Facebook,\n\n2:08:40.520 --> 2:08:46.280\n like DeepMind and so on, so much of the talent, you know, it doesn't stay necessarily\n\n2:08:46.280 --> 2:08:50.440\n for prolonged periods of time in these universities.\n\n2:08:50.440 --> 2:08:50.940\n Oh, yeah.\n\n2:08:50.940 --> 2:08:57.800\n I mean, some of the companies are more guilty than others of paying fabulous salaries to\n\n2:08:57.800 --> 2:09:00.120\n some of the highest, you know, producers.\n\n2:09:00.120 --> 2:09:02.840\n And then just, you never hear from them again.\n\n2:09:02.840 --> 2:09:04.600\n They're not allowed to give public talks.\n\n2:09:04.600 --> 2:09:05.640\n They're sort of locked away.\n\n2:09:06.600 --> 2:09:12.280\n And it's sort of like collecting, you know, Hollywood stars or something.\n\n2:09:12.280 --> 2:09:13.960\n And they're not allowed to make movies anymore.\n\n2:09:13.960 --> 2:09:14.460\n I own them.\n\n2:09:14.460 --> 2:09:15.660\n Yeah.\n\n2:09:15.660 --> 2:09:20.700\n That's tragic because, I mean, there's an openness to the university setting where you\n\n2:09:20.700 --> 2:09:25.580\n do research to both in the space of ideas and like publication, all those kinds of things.\n\n2:09:25.580 --> 2:09:28.940\n Yeah, you know, and, you know, there's the publication and all that.\n\n2:09:28.940 --> 2:09:31.980\n And often, you know, although these places say they publish.\n\n2:09:32.940 --> 2:09:33.660\n There's pressure.\n\n2:09:33.660 --> 2:09:41.260\n But I think, for instance, you know, on net net, I think Google buying those eight or\n\n2:09:41.260 --> 2:09:45.580\n nine robotics company was bad for the field because it locked those people away.\n\n2:09:46.620 --> 2:09:51.820\n They didn't have to make the company succeed anymore, locked them away for years, and then\n\n2:09:53.660 --> 2:09:55.660\n sort of all frid it away.\n\n2:09:55.660 --> 2:09:56.160\n Yeah.\n\n2:09:56.160 --> 2:10:02.960\n So do you have hope for MIT, for MIT?\n\n2:10:02.960 --> 2:10:03.460\n Yeah.\n\n2:10:03.460 --> 2:10:04.000\n Why shouldn't I?\n\n2:10:04.560 --> 2:10:11.200\n Well, I could be harsh and say that I'm not sure I would say MIT is leading the world\n\n2:10:11.200 --> 2:10:15.440\n in AI or even Stanford or Berkeley.\n\n2:10:15.440 --> 2:10:23.680\n I would say, I would say DeepMind, Google AI, Facebook AI, all of those things.\n\n2:10:23.680 --> 2:10:29.920\n I would take a slightly different approach, a different answer.\n\n2:10:30.560 --> 2:10:32.880\n I'll come back to Facebook in a minute.\n\n2:10:32.880 --> 2:10:41.360\n But I think those other places are following a dream of one of the founders.\n\n2:10:42.880 --> 2:10:46.560\n And I'm not sure that it's well founded, the dream.\n\n2:10:46.560 --> 2:10:52.960\n And I'm not sure that it's going to have the impact that he believes it is.\n\n2:10:54.720 --> 2:10:56.560\n You're talking about Facebook and Google and so on.\n\n2:10:56.560 --> 2:10:57.600\n I'm talking about Google.\n\n2:10:57.600 --> 2:10:58.320\n Google.\n\n2:10:58.320 --> 2:11:03.360\n But the thing is, those research labs aren't, there's the big dream.\n\n2:11:03.920 --> 2:11:08.480\n And I'm usually a fan of no matter what the dream is, a big dream is a unifier.\n\n2:11:08.480 --> 2:11:15.200\n Because what happens is you have a lot of bright minds working together on a dream.\n\n2:11:15.200 --> 2:11:20.000\n What results is a lot of adjacent ideas and how so much progress is made.\n\n2:11:20.000 --> 2:11:20.500\n Yeah.\n\n2:11:21.040 --> 2:11:22.560\n So I'm not saying they're actually leading.\n\n2:11:22.560 --> 2:11:25.280\n I'm not saying that the universities are leading.\n\n2:11:25.280 --> 2:11:25.780\n Yeah.\n\n2:11:25.780 --> 2:11:28.960\n But I don't think those companies are leading in general because they're,\n\n2:11:28.960 --> 2:11:36.160\n we saw this incredible spike in attendees at NeurIPS.\n\n2:11:36.160 --> 2:11:44.800\n And as I said in my January 1st review this year for 2020, 2020 will not be\n\n2:11:44.800 --> 2:11:48.560\n remembered as a watershed year for machine learning or AI.\n\n2:11:48.560 --> 2:11:52.720\n There was nothing surprising happened anyway.\n\n2:11:52.720 --> 2:11:56.720\n Unlike when deep learning hit ImageNet.\n\n2:11:57.440 --> 2:12:00.160\n That was a shake.\n\n2:12:02.080 --> 2:12:06.640\n And there's a lot more people writing papers, but the papers are fundamentally\n\n2:12:06.640 --> 2:12:08.800\n boring and uninteresting.\n\n2:12:08.800 --> 2:12:09.760\n Incremental work.\n\n2:12:09.760 --> 2:12:10.260\n Yeah.\n\n2:12:10.260 --> 2:12:13.140\n Is there a particular memories you have with Minsky or somebody else at\n\n2:12:13.140 --> 2:12:15.620\n MIT that stand out, funny stories?\n\n2:12:16.340 --> 2:12:19.140\n I mean, unfortunately, he's another one that's passed away.\n\n2:12:21.940 --> 2:12:24.020\n You've known some of the biggest minds in AI.\n\n2:12:24.580 --> 2:12:25.080\n Yeah.\n\n2:12:25.080 --> 2:12:29.460\n And you know, they, they did amazing things and sometimes they were grumpy.\n\n2:12:31.460 --> 2:12:35.380\n Well, he was, uh, he was interesting cause he was very grumpy, but that,\n\n2:12:35.380 --> 2:12:41.060\n that was his, uh, I remember him saying in an interview that the key to success\n\n2:12:41.780 --> 2:12:45.940\n or being to keep being productive is to hate everything you've ever done in the past.\n\n2:12:45.940 --> 2:12:48.980\n Maybe that, maybe that explains the Perceptron book.\n\n2:12:49.940 --> 2:12:50.440\n There it was.\n\n2:12:50.440 --> 2:12:51.940\n He told you exactly.\n\n2:12:53.540 --> 2:12:58.100\n But he, meaning like, just like, I mean, maybe that's the way to not\n\n2:12:58.100 --> 2:12:59.380\n treat yourself too seriously.\n\n2:12:59.380 --> 2:13:03.940\n Just, uh, you know, you're not, you're not, you're not, you're not, you're not,\n\n2:13:03.940 --> 2:13:05.620\n you're not treating yourself too seriously.\n\n2:13:05.620 --> 2:13:08.260\n Just, uh, always be moving forward.\n\n2:13:09.220 --> 2:13:10.100\n Uh, that was the idea.\n\n2:13:10.100 --> 2:13:14.980\n I mean, that, that crankiness, I mean, there's a, uh, that's the scary.\n\n2:13:14.980 --> 2:13:21.060\n So let me, let me, let me tell you, uh, you know, what really, um, you know,\n\n2:13:21.060 --> 2:13:27.460\n the joy memories are about having access to technology before anyone else has seen\n\n2:13:27.460 --> 2:13:27.960\n it.\n\n2:13:27.960 --> 2:13:34.860\n You know, I got to Stanford in 1977 and we had, um, you know, we had terminals\n\n2:13:34.860 --> 2:13:37.260\n that could show live video on them.\n\n2:13:37.260 --> 2:13:40.620\n Um, digital, digital sound system.\n\n2:13:40.620 --> 2:13:45.020\n We had a Xerox graphics printer.\n\n2:13:45.020 --> 2:13:50.140\n We could print, um, uh, it wasn't, you know, it wasn't like a typewriter\n\n2:13:50.140 --> 2:13:51.980\n ball hitting in characters.\n\n2:13:51.980 --> 2:13:53.580\n It could print arbitrary things.\n\n2:13:53.580 --> 2:13:58.300\n I mean, you know, one bit, you know, black or white, but you get arbitrary pictures.\n\n2:13:58.300 --> 2:14:00.380\n This was science fiction sort of stuff.\n\n2:14:00.380 --> 2:14:07.260\n Um, um, at, at MIT, the, uh, the list machines, which, you know, they were the\n\n2:14:07.260 --> 2:14:12.060\n first personal computers and, you know, cost a hundred thousand dollars each.\n\n2:14:12.060 --> 2:14:14.620\n And I could, you know, I got there early enough in the day.\n\n2:14:14.620 --> 2:14:15.980\n I got one for the day.\n\n2:14:15.980 --> 2:14:17.420\n Couldn't, couldn't stand up.\n\n2:14:17.420 --> 2:14:18.380\n I had to keep working.\n\n2:14:18.380 --> 2:14:25.340\n Um, um, so they're having that like direct glimpse into the future.\n\n2:14:25.340 --> 2:14:25.580\n Yeah.\n\n2:14:25.580 --> 2:14:29.100\n And, and, you know, I've had email every day since 1977.\n\n2:14:29.100 --> 2:14:36.060\n Um, and, uh, you know, the, the host field was only eight bits, you know, that many\n\n2:14:36.060 --> 2:14:39.980\n places, but I could send the email to other people at a few places.\n\n2:14:39.980 --> 2:14:45.340\n So that was, that was pretty exciting to be in that world so different from what\n\n2:14:45.340 --> 2:14:46.780\n the rest of the world knew.\n\n2:14:46.780 --> 2:14:53.420\n Um, uh, uh, let me ask you probably edit this out, but just in case you have a\n\n2:14:53.420 --> 2:15:00.060\n story, uh, I'm hanging out with Don Knuth, uh, for a while tomorrow.\n\n2:15:00.060 --> 2:15:02.700\n Did you ever get a chance to such a different world than yours?\n\n2:15:03.340 --> 2:15:08.300\n He's a very kind of theoretical computer science, the puzzle of, uh, of, uh, computer\n\n2:15:08.300 --> 2:15:09.500\n science and mathematics.\n\n2:15:09.500 --> 2:15:13.740\n And you're so much about the magic of robotics, like the practice of it.\n\n2:15:13.740 --> 2:15:17.820\n You mentioned him earlier for like, not, you know, about computation.\n\n2:15:17.820 --> 2:15:19.580\n Did your worlds cross?\n\n2:15:19.580 --> 2:15:20.540\n They did enough.\n\n2:15:20.540 --> 2:15:25.100\n You know, I, I know him now we talk, you know, but let me tell you my, my Donald\n\n2:15:25.100 --> 2:15:25.820\n Knuth story.\n\n2:15:26.700 --> 2:15:31.500\n So, um, you know, besides, you know, analysis of algorithms, he's well known for\n\n2:15:32.140 --> 2:15:36.940\n writing tech, which is in LaTeX, which is the academic publishing system.\n\n2:15:37.580 --> 2:15:41.740\n So he did that at the AI lab and he would do it.\n\n2:15:41.740 --> 2:15:44.060\n He would work overnight at the AI lab.\n\n2:15:45.020 --> 2:15:55.660\n And one, one day, one night, the, uh, the mainframe computer went down and, um, uh,\n\n2:15:55.660 --> 2:15:57.340\n a guy named Robert Pore was there.\n\n2:15:57.340 --> 2:16:03.180\n He did his PhD at the Media Lab at MIT and he was, um, you know, an engineer.\n\n2:16:04.300 --> 2:16:08.780\n And so I, he and I, you know, tracked down what were the problem was.\n\n2:16:08.780 --> 2:16:13.100\n It was one of this big refrigerator size or washing machine size disk drives had\n\n2:16:13.100 --> 2:16:13.500\n failed.\n\n2:16:13.500 --> 2:16:15.500\n And that's what brought the whole system down.\n\n2:16:15.500 --> 2:16:20.300\n So we've got panels pulled off and we're pulling, you know, circuit cards out.\n\n2:16:20.300 --> 2:16:25.340\n And Donald Knuth, who's a really tall guy walks in and he's looking down and says,\n\n2:16:25.340 --> 2:16:26.540\n when will it be fixed?\n\n2:16:26.540 --> 2:16:28.940\n You know, cause he wanted to get back to writing his tech system.\n\n2:16:31.340 --> 2:16:37.420\n And so we, we figured out, you know, it was a particular chip, 7,400 series chip,\n\n2:16:37.420 --> 2:16:38.700\n which was socketed.\n\n2:16:38.700 --> 2:16:40.780\n We popped it out.\n\n2:16:40.780 --> 2:16:43.340\n We put a replacement in, put it back in.\n\n2:16:43.340 --> 2:16:45.740\n Smoke comes out cause we put it in backwards.\n\n2:16:45.740 --> 2:16:48.780\n Cause we were so nervous that Donald Knuth was standing over us.\n\n2:16:49.500 --> 2:16:52.940\n Anyway, we eventually got it fixed and got the mainframe running again.\n\n2:16:53.660 --> 2:16:56.220\n So that was your little, when was that again?\n\n2:16:56.220 --> 2:16:58.860\n Well, that must have been before October 79.\n\n2:16:58.860 --> 2:17:00.300\n Cause we moved out of that building then.\n\n2:17:00.300 --> 2:17:03.740\n So sometime probably 78 sometime early 79.\n\n2:17:03.740 --> 2:17:06.140\n Yeah, those, all those figures is just fascinating.\n\n2:17:06.140 --> 2:17:09.420\n All the people with pass, pass through MIT is really fascinating.\n\n2:17:10.220 --> 2:17:17.420\n Is there, let me ask you to put on your big wise man hat.\n\n2:17:18.140 --> 2:17:20.860\n Is there advice that you can give to young people today,\n\n2:17:20.860 --> 2:17:23.980\n whether in high school or college who are thinking about their career\n\n2:17:24.700 --> 2:17:32.060\n or thinking about life, how to live a life they're proud of, a successful life?\n\n2:17:32.060 --> 2:17:36.140\n Yeah. So, so many people ask me for advice and have asked for,\n\n2:17:36.140 --> 2:17:41.020\n and I give, I talk to a lot of people all the time and there is no one way.\n\n2:17:44.060 --> 2:17:48.700\n You know, there's a lot of pressure to produce papers\n\n2:17:51.900 --> 2:17:53.980\n that will be acceptable and be published.\n\n2:17:56.460 --> 2:17:58.620\n Maybe I was, maybe I can't do it.\n\n2:17:58.620 --> 2:18:03.340\n Maybe I was, maybe I come from an age where I would,\n\n2:18:03.340 --> 2:18:07.100\n I could be a rebel against that and still succeed.\n\n2:18:07.100 --> 2:18:13.260\n Maybe it's harder today, but I think it's important not to get too caught up\n\n2:18:14.860 --> 2:18:17.260\n with what everyone else is doing.\n\n2:18:18.380 --> 2:18:22.940\n And if you, if, well, it depends on what you want of life.\n\n2:18:22.940 --> 2:18:31.100\n If you want to have real impact, you have to be ready to fail a lot of times.\n\n2:18:31.100 --> 2:18:33.420\n So you have to make a lot of unsafe decisions.\n\n2:18:34.220 --> 2:18:38.700\n And the only way to make that work is to make, keep doing it for a long time.\n\n2:18:38.700 --> 2:18:40.220\n And then one of them will be work out.\n\n2:18:40.220 --> 2:18:43.740\n And so that, that, that will make something successful.\n\n2:18:43.740 --> 2:18:44.220\n Or not.\n\n2:18:45.500 --> 2:18:48.780\n Or yeah, or you may, or you just may, you know, end up, you know,\n\n2:18:48.780 --> 2:18:50.780\n not having a, you know, having a lousy career.\n\n2:18:50.780 --> 2:18:52.140\n I mean, it's certainly possible.\n\n2:18:52.140 --> 2:18:53.420\n Taking the risk is the thing.\n\n2:18:53.420 --> 2:18:53.580\n Yeah.\n\n2:18:56.220 --> 2:19:04.620\n But there's no way to, to make all safe decisions and actually really contribute.\n\n2:19:06.620 --> 2:19:11.020\n Do you think about your death, about your mortality?\n\n2:19:12.300 --> 2:19:15.660\n I got to say when COVID hit, I did.\n\n2:19:15.660 --> 2:19:18.860\n Because we did, you know, in the early days, we didn't know how bad it was going to be.\n\n2:19:18.860 --> 2:19:22.780\n And I, that, that made me work on my book harder for a while,\n\n2:19:22.780 --> 2:19:25.900\n but then I'd started this company and now I'm doing full time,\n\n2:19:25.900 --> 2:19:27.100\n more than full time of the company.\n\n2:19:27.100 --> 2:19:29.660\n So the book's on hold, but I do want to finish this book.\n\n2:19:30.300 --> 2:19:32.060\n When you think about it, are you afraid of it?\n\n2:19:35.820 --> 2:19:42.220\n I'm afraid of dribbling, you know, of losing it.\n\n2:19:42.220 --> 2:19:43.980\n The details of, okay.\n\n2:19:43.980 --> 2:19:44.220\n Yeah.\n\n2:19:45.180 --> 2:19:45.580\n Yeah.\n\n2:19:45.580 --> 2:19:50.380\n But the fact that the ride ends, I've known that for a long time.\n\n2:19:51.260 --> 2:19:54.780\n So it's, yeah, but there's knowing and knowing.\n\n2:19:55.420 --> 2:19:57.580\n It's such a, yeah.\n\n2:19:57.580 --> 2:19:58.780\n And it really sucks.\n\n2:19:58.780 --> 2:20:00.380\n It feels, it feels a lot closer.\n\n2:20:01.820 --> 2:20:07.900\n So my, in, in my, my blog with my predictions, my sort of push back against that was that I said,\n\n2:20:08.940 --> 2:20:14.860\n I'm going to review these every year for 32 years and that puts me into my mid nineties.\n\n2:20:14.860 --> 2:20:18.780\n So, you know, it's my whole every, every time you write the blog posts,\n\n2:20:18.780 --> 2:20:23.660\n you're getting closer and closer to your own prediction of your death.\n\n2:20:23.660 --> 2:20:23.820\n Yeah.\n\n2:20:24.940 --> 2:20:26.300\n What do you hope your legacy is?\n\n2:20:28.140 --> 2:20:31.900\n You're one of the greatest roboticist AI researchers of all time.\n\n2:20:34.700 --> 2:20:38.140\n What I hope is that I actually finished writing this book\n\n2:20:38.140 --> 2:20:48.220\n and that there's one person who reads it and see something about changing the way they're thinking.\n\n2:20:48.940 --> 2:20:53.340\n And that leads to the next big.\n\n2:20:54.860 --> 2:20:59.340\n And then there'll be on a podcast a hundred years from now saying I once read that book\n\n2:21:01.580 --> 2:21:02.860\n and that changed everything.\n\n2:21:04.460 --> 2:21:06.140\n What do you think is the meaning of life?\n\n2:21:06.140 --> 2:21:10.140\n This whole thing, the existence, the, the, the, all the hurried things we do\n\n2:21:10.140 --> 2:21:13.260\n on this planet, what do you think is the meaning of it all?\n\n2:21:13.260 --> 2:21:15.660\n Yeah. Well, you know, I think we're all really bad at it.\n\n2:21:17.180 --> 2:21:19.020\n Life or finding meaning or both.\n\n2:21:19.020 --> 2:21:24.380\n Yeah. We get caught up in, in, in the, it's easy to get easier to do the stuff that's immediate\n\n2:21:24.940 --> 2:21:26.780\n and not through the stuff. It's not immediate.\n\n2:21:27.820 --> 2:21:29.980\n So the big picture we're bad at.\n\n2:21:29.980 --> 2:21:31.020\n Yeah. Yeah.\n\n2:21:31.020 --> 2:21:33.900\n Do you have a sense of what that big picture is?\n\n2:21:33.900 --> 2:21:37.980\n Like why you ever look up to the stars and ask, why the hell are we here?\n\n2:21:41.580 --> 2:21:50.380\n You know, my, my, my, my atheism tells me it's just random, but you know, I want to understand the,\n\n2:21:50.380 --> 2:21:55.660\n the way random in the, in the, that's what I talk about in this book, how order comes from disorder.\n\n2:21:55.660 --> 2:21:56.220\n Yeah.\n\n2:21:58.220 --> 2:22:02.460\n But it kind of sprung up like most of the whole thing is random, but this, this, this,\n\n2:22:02.460 --> 2:22:06.940\n the whole thing is random, but this little pocket of complexity they will call earth\n\n2:22:07.660 --> 2:22:10.300\n that like, why the hell does that happen?\n\n2:22:10.300 --> 2:22:17.420\n And, and what we don't know is how common that those pockets of complexity are or how often,\n\n2:22:18.060 --> 2:22:21.260\n um, cause they may not last forever.\n\n2:22:22.780 --> 2:22:30.460\n Which is, uh, more exciting slash sad to you if we're alone or if there's infinite number of.\n\n2:22:30.460 --> 2:22:35.420\n Oh, I think, I think it's impossible for me to believe that we're alone.\n\n2:22:36.300 --> 2:22:39.980\n Um, that would just be too horrible, too cruel.\n\n2:22:41.500 --> 2:22:43.180\n It could be like the sad thing.\n\n2:22:43.180 --> 2:22:46.300\n It could be like a graveyard of intelligent civilizations.\n\n2:22:46.300 --> 2:22:46.940\n Oh, everywhere.\n\n2:22:46.940 --> 2:22:47.100\n Yeah.\n\n2:22:47.980 --> 2:22:49.900\n That might be the most likely outcome.\n\n2:22:50.620 --> 2:22:51.660\n And for us too.\n\n2:22:51.660 --> 2:22:52.540\n Yeah, exactly.\n\n2:22:52.540 --> 2:22:52.860\n Yeah.\n\n2:22:52.860 --> 2:22:54.700\n And all of this will be forgotten.\n\n2:22:54.700 --> 2:22:54.940\n Yeah.\n\n2:22:54.940 --> 2:22:59.900\n Yeah, including all the robots you build, everything forgotten.\n\n2:23:01.500 --> 2:23:05.740\n Well, on average, everyone has been forgotten in history.\n\n2:23:05.740 --> 2:23:06.220\n Yeah.\n\n2:23:06.220 --> 2:23:06.940\n Right.\n\n2:23:06.940 --> 2:23:07.500\n Yeah.\n\n2:23:07.500 --> 2:23:10.540\n Most people are not remembered beyond the generation or two.\n\n2:23:11.100 --> 2:23:12.780\n Um, I mean, yeah.\n\n2:23:12.780 --> 2:23:17.900\n Well, not just on average, basically very close to a hundred percent of people who've ever lived\n\n2:23:17.900 --> 2:23:18.780\n are forgotten.\n\n2:23:18.780 --> 2:23:19.020\n Yeah.\n\n2:23:19.020 --> 2:23:24.140\n I mean, you know, long arc of, I don't know anyone alive who remembers my great grandparents\n\n2:23:24.140 --> 2:23:25.260\n because we didn't meet them.\n\n2:23:26.300 --> 2:23:32.460\n So still this fun, this, uh, this, uh, life is pretty fun somehow.\n\n2:23:32.460 --> 2:23:32.620\n Yeah.\n\n2:23:33.660 --> 2:23:39.180\n Even the immense absurdity and, and, uh, at times, meaninglessness of it all.\n\n2:23:39.180 --> 2:23:40.220\n It's pretty fun.\n\n2:23:40.220 --> 2:23:43.740\n And one of the, for me, one of the most fun things is robots.\n\n2:23:43.740 --> 2:23:45.180\n And I've looked up to your work.\n\n2:23:45.180 --> 2:23:46.780\n I've looked up to you for a long time.\n\n2:23:46.780 --> 2:23:47.180\n That's right.\n\n2:23:47.180 --> 2:23:47.740\n God.\n\n2:23:47.740 --> 2:23:53.580\n Rod, it's, it's an honor that, uh, you would spend your valuable time with me today talking.\n\n2:23:53.580 --> 2:23:54.780\n It was an amazing conversation.\n\n2:23:54.780 --> 2:23:55.980\n Thank you so much for being here.\n\n2:23:55.980 --> 2:23:57.820\n Well, thanks for, thanks for talking with me.\n\n2:23:57.820 --> 2:23:58.620\n I've enjoyed it.\n\n2:24:00.060 --> 2:24:02.700\n Thanks for listening to this conversation with Rodney Brooks.\n\n2:24:02.700 --> 2:24:06.300\n To support this podcast, please check out our sponsors in the description.\n\n2:24:06.860 --> 2:24:11.580\n And now let me leave you with the three laws of robotics from Isaac Asimov.\n\n2:24:12.620 --> 2:24:19.020\n One, a robot may not injure a human being or through inaction, allow human being to come to\n\n2:24:19.020 --> 2:24:25.580\n harm. Two, a robot must obey the orders given to it by human beings, except when such orders\n\n2:24:25.580 --> 2:24:32.860\n would conflict with the first law. And three, a robot must protect its own existence as long\n\n2:24:32.860 --> 2:24:37.500\n as such protection does not conflict with the first or the second laws.\n\n2:24:38.620 --> 2:24:39.740\n Thank you for listening.\n\n2:24:39.740 --> 2:24:49.740\n I hope to see you next time.\n\n"
}