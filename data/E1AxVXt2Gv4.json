{
  "title": "Marcus Hutter: Universal Artificial Intelligence, AIXI, and AGI | Lex Fridman Podcast #75",
  "id": "E1AxVXt2Gv4",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:03.480\n The following is a conversation with Marcus Hutter,\n\n00:03.480 --> 00:06.680\n senior research scientist at Google DeepMind.\n\n00:06.680 --> 00:08.360\n Throughout his career of research,\n\n00:08.360 --> 00:11.760\n including with J\u00fcrgen Schmidhuber and Shane Legge,\n\n00:11.760 --> 00:13.960\n he has proposed a lot of interesting ideas\n\n00:13.960 --> 00:16.360\n in and around the field of artificial general\n\n00:16.360 --> 00:20.140\n intelligence, including the development of AICSI,\n\n00:20.140 --> 00:25.360\n spelled AIXI model, which is a mathematical approach to AGI\n\n00:25.360 --> 00:28.880\n that incorporates ideas of Kolmogorov complexity,\n\n00:28.880 --> 00:33.080\n Solomonov induction, and reinforcement learning.\n\n00:33.080 --> 00:38.200\n In 2006, Marcus launched the 50,000 Euro Hutter Prize\n\n00:38.200 --> 00:41.200\n for lossless compression of human knowledge.\n\n00:41.200 --> 00:43.720\n The idea behind this prize is that the ability\n\n00:43.720 --> 00:47.900\n to compress well is closely related to intelligence.\n\n00:47.900 --> 00:51.260\n This, to me, is a profound idea.\n\n00:51.260 --> 00:54.000\n Specifically, if you can compress the first 100\n\n00:54.000 --> 00:56.520\n megabytes or 1 gigabyte of Wikipedia\n\n00:56.520 --> 00:59.000\n better than your predecessors, your compressor\n\n00:59.000 --> 01:02.200\n likely has to also be smarter.\n\n01:02.200 --> 01:04.240\n The intention of this prize is to encourage\n\n01:04.240 --> 01:09.640\n the development of intelligent compressors as a path to AGI.\n\n01:09.640 --> 01:13.280\n In conjunction with his podcast release just a few days ago,\n\n01:13.280 --> 01:16.520\n Marcus announced a 10x increase in several aspects\n\n01:16.520 --> 01:22.680\n of this prize, including the money, to 500,000 Euros.\n\n01:22.680 --> 01:25.240\n The better your compressor works relative to the previous\n\n01:25.240 --> 01:27.680\n winners, the higher fraction of that prize money\n\n01:27.680 --> 01:29.440\n is awarded to you.\n\n01:29.440 --> 01:35.080\n You can learn more about it if you Google simply Hutter Prize.\n\n01:35.080 --> 01:38.240\n I'm a big fan of benchmarks for developing AI systems,\n\n01:38.240 --> 01:39.960\n and the Hutter Prize may indeed be\n\n01:39.960 --> 01:43.240\n one that will spark some good ideas for approaches that\n\n01:43.240 --> 01:47.880\n will make progress on the path of developing AGI systems.\n\n01:47.880 --> 01:50.520\n This is the Artificial Intelligence Podcast.\n\n01:50.520 --> 01:52.720\n If you enjoy it, subscribe on YouTube,\n\n01:52.720 --> 01:54.720\n give it five stars on Apple Podcast,\n\n01:54.720 --> 01:58.040\n support it on Patreon, or simply connect with me on Twitter\n\n01:58.040 --> 02:02.640\n at Lex Friedman, spelled F R I D M A N.\n\n02:02.640 --> 02:04.840\n As usual, I'll do one or two minutes of ads\n\n02:04.840 --> 02:06.960\n now and never any ads in the middle\n\n02:06.960 --> 02:09.240\n that can break the flow of the conversation.\n\n02:09.240 --> 02:11.040\n I hope that works for you and doesn't\n\n02:11.040 --> 02:13.240\n hurt the listening experience.\n\n02:13.240 --> 02:16.400\n This show is presented by Cash App, the number one finance\n\n02:16.400 --> 02:17.800\n app in the App Store.\n\n02:17.800 --> 02:21.240\n When you get it, use code LEX PODCAST.\n\n02:21.240 --> 02:23.520\n Cash App lets you send money to friends,\n\n02:23.520 --> 02:26.040\n buy Bitcoin, and invest in the stock market\n\n02:26.040 --> 02:27.920\n with as little as $1.\n\n02:27.920 --> 02:30.920\n Broker services are provided by Cash App Investing,\n\n02:30.920 --> 02:34.960\n a subsidiary of Square, a member SIPC.\n\n02:34.960 --> 02:37.400\n Since Cash App allows you to send and receive money\n\n02:37.400 --> 02:39.920\n digitally, peer to peer, and security\n\n02:39.920 --> 02:42.800\n in all digital transactions is very important.\n\n02:42.800 --> 02:45.840\n Let me mention the PCI data security standard\n\n02:45.840 --> 02:48.080\n that Cash App is compliant with.\n\n02:48.080 --> 02:52.080\n I'm a big fan of standards for safety and security.\n\n02:52.080 --> 02:55.080\n PCI DSS is a good example of that,\n\n02:55.080 --> 02:57.200\n where a bunch of competitors got together\n\n02:57.200 --> 02:59.000\n and agreed that there needs to be\n\n02:59.000 --> 03:02.520\n a global standard around the security of transactions.\n\n03:02.520 --> 03:06.040\n Now, we just need to do the same for autonomous vehicles\n\n03:06.040 --> 03:08.880\n and AI systems in general.\n\n03:08.880 --> 03:11.920\n So again, if you get Cash App from the App Store or Google\n\n03:11.920 --> 03:16.400\n Play and use the code LEX PODCAST, you'll get $10.\n\n03:16.400 --> 03:19.240\n And Cash App will also donate $10 to FIRST,\n\n03:19.240 --> 03:21.380\n one of my favorite organizations that\n\n03:21.380 --> 03:24.520\n is helping to advance robotics and STEM education\n\n03:24.520 --> 03:26.760\n for young people around the world.\n\n03:27.680 --> 03:31.680\n And now, here's my conversation with Markus Hutter.\n\n03:32.600 --> 03:34.480\n Do you think of the universe as a computer\n\n03:34.480 --> 03:37.020\n or maybe an information processing system?\n\n03:37.020 --> 03:39.080\n Let's go with a big question first.\n\n03:39.080 --> 03:41.560\n Okay, with a big question first.\n\n03:41.560 --> 03:45.240\n I think it's a very interesting hypothesis or idea.\n\n03:45.240 --> 03:47.960\n And I have a background in physics,\n\n03:47.960 --> 03:50.800\n so I know a little bit about physical theories,\n\n03:50.800 --> 03:52.440\n the standard model of particle physics\n\n03:52.440 --> 03:54.440\n and general relativity theory.\n\n03:54.440 --> 03:57.200\n And they are amazing and describe virtually everything\n\n03:57.200 --> 03:58.040\n in the universe.\n\n03:58.040 --> 03:59.780\n And they're all in a sense, computable theories.\n\n03:59.780 --> 04:01.800\n I mean, they're very hard to compute.\n\n04:01.800 --> 04:04.360\n And it's very elegant, simple theories,\n\n04:04.360 --> 04:07.260\n which describe virtually everything in the universe.\n\n04:07.260 --> 04:12.260\n So there's a strong indication that somehow\n\n04:12.400 --> 04:17.400\n the universe is computable, but it's a plausible hypothesis.\n\n04:17.400 --> 04:21.200\n So what do you think, just like you said, general relativity,\n\n04:21.200 --> 04:23.680\n quantum field theory, what do you think that\n\n04:23.680 --> 04:26.560\n the laws of physics are so nice and beautiful\n\n04:26.560 --> 04:29.000\n and simple and compressible?\n\n04:29.000 --> 04:32.800\n Do you think our universe was designed,\n\n04:32.800 --> 04:34.240\n is naturally this way?\n\n04:34.240 --> 04:36.760\n Are we just focusing on the parts\n\n04:36.760 --> 04:39.560\n that are especially compressible?\n\n04:39.560 --> 04:42.780\n Are human minds just enjoy something about that simplicity?\n\n04:42.780 --> 04:44.880\n And in fact, there's other things\n\n04:44.880 --> 04:46.760\n that are not so compressible.\n\n04:46.760 --> 04:49.440\n I strongly believe and I'm pretty convinced\n\n04:49.440 --> 04:52.560\n that the universe is inherently beautiful, elegant\n\n04:52.560 --> 04:55.520\n and simple and described by these equations.\n\n04:55.520 --> 04:57.640\n And we're not just picking that.\n\n04:57.640 --> 05:00.040\n I mean, if there were some phenomena\n\n05:00.040 --> 05:02.680\n which cannot be neatly described,\n\n05:02.680 --> 05:04.640\n scientists would try that.\n\n05:04.640 --> 05:06.720\n And there's biology, which is more messy,\n\n05:06.720 --> 05:09.280\n but we understand that it's an emergent phenomena\n\n05:09.280 --> 05:11.000\n and it's complex systems,\n\n05:11.000 --> 05:12.720\n but they still follow the same rules\n\n05:12.720 --> 05:14.640\n of quantum and electrodynamics.\n\n05:14.640 --> 05:16.560\n All of chemistry follows that and we know that.\n\n05:16.560 --> 05:18.120\n I mean, we cannot compute everything\n\n05:18.120 --> 05:20.280\n because we have limited computational resources.\n\n05:20.280 --> 05:22.040\n No, I think it's not a bias of the humans,\n\n05:22.040 --> 05:23.960\n but it's objectively simple.\n\n05:23.960 --> 05:25.640\n I mean, of course, you never know,\n\n05:25.640 --> 05:28.280\n maybe there's some corners very far out in the universe\n\n05:28.280 --> 05:32.960\n or super, super tiny below the nucleus of atoms\n\n05:32.960 --> 05:37.960\n or parallel universes which are not nice and simple,\n\n05:38.200 --> 05:40.520\n but there's no evidence for that.\n\n05:40.520 --> 05:42.200\n And we should apply Occam's razor\n\n05:42.200 --> 05:45.120\n and choose the simplest three consistent with it.\n\n05:45.120 --> 05:48.000\n But also it's a little bit self referential.\n\n05:48.000 --> 05:49.440\n So maybe a quick pause.\n\n05:49.440 --> 05:50.960\n What is Occam's razor?\n\n05:50.960 --> 05:55.520\n So Occam's razor says that you should not multiply entities\n\n05:55.520 --> 05:58.040\n beyond necessity, which sort of,\n\n05:58.040 --> 06:01.360\n if you translate it to proper English means,\n\n06:01.360 --> 06:03.400\n and in the scientific context means\n\n06:03.400 --> 06:06.400\n that if you have two theories or hypothesis or models,\n\n06:06.400 --> 06:09.760\n which equally well describe the phenomenon,\n\n06:09.760 --> 06:11.520\n your study or the data,\n\n06:11.520 --> 06:13.920\n you should choose the more simple one.\n\n06:13.920 --> 06:16.640\n So that's just the principle or sort of,\n\n06:16.640 --> 06:20.040\n that's not like a provable law, perhaps.\n\n06:20.040 --> 06:23.480\n Perhaps we'll kind of discuss it and think about it,\n\n06:23.480 --> 06:28.080\n but what's the intuition of why the simpler answer\n\n06:28.080 --> 06:33.080\n is the one that is likely to be more correct descriptor\n\n06:33.280 --> 06:35.080\n of whatever we're talking about?\n\n06:35.080 --> 06:36.560\n I believe that Occam's razor\n\n06:36.560 --> 06:40.240\n is probably the most important principle in science.\n\n06:40.240 --> 06:42.040\n I mean, of course we lead logical deduction\n\n06:42.040 --> 06:44.560\n and we do experimental design,\n\n06:44.560 --> 06:49.560\n but science is about finding, understanding the world,\n\n06:49.880 --> 06:51.480\n finding models of the world.\n\n06:51.480 --> 06:53.720\n And we can come up with crazy complex models,\n\n06:53.720 --> 06:56.040\n which explain everything but predict nothing.\n\n06:56.040 --> 07:00.240\n But the simple model seem to have predictive power\n\n07:00.240 --> 07:03.160\n and it's a valid question why?\n\n07:03.160 --> 07:06.000\n And there are two answers to that.\n\n07:06.000 --> 07:07.240\n You can just accept it.\n\n07:07.240 --> 07:10.800\n That is the principle of science and we use this principle\n\n07:10.800 --> 07:12.840\n and it seems to be successful.\n\n07:12.840 --> 07:15.920\n We don't know why, but it just happens to be.\n\n07:15.920 --> 07:18.560\n Or you can try, find another principle\n\n07:18.560 --> 07:21.120\n which explains Occam's razor.\n\n07:21.120 --> 07:24.120\n And if we start with the assumption\n\n07:24.120 --> 07:27.600\n that the world is governed by simple rules,\n\n07:27.600 --> 07:31.400\n then there's a bias towards simplicity\n\n07:31.400 --> 07:36.200\n and applying Occam's razor is the mechanism\n\n07:36.200 --> 07:37.120\n to finding these rules.\n\n07:37.120 --> 07:39.080\n And actually in a more quantitative sense,\n\n07:39.080 --> 07:41.760\n and we come back to that later in terms of somnolent reduction,\n\n07:41.760 --> 07:43.080\n you can rigorously prove that.\n\n07:43.080 --> 07:45.680\n You can assume that the world is simple,\n\n07:45.680 --> 07:47.800\n then Occam's razor is the best you can do\n\n07:47.800 --> 07:49.080\n in a certain sense.\n\n07:49.080 --> 07:51.720\n So I apologize for the romanticized question,\n\n07:51.720 --> 07:56.320\n but why do you think, outside of its effectiveness,\n\n07:56.320 --> 07:58.440\n why do you think we find simplicity\n\n07:58.440 --> 08:00.000\n so appealing as human beings?\n\n08:00.000 --> 08:05.000\n Why does E equals MC squared seem so beautiful to us humans?\n\n08:05.000 --> 08:08.480\n I guess mostly, in general, many things\n\n08:08.480 --> 08:12.000\n can be explained by an evolutionary argument.\n\n08:12.000 --> 08:14.240\n And there's some artifacts in humans\n\n08:14.240 --> 08:18.240\n which are just artifacts and not evolutionary necessary.\n\n08:18.240 --> 08:21.120\n But with this beauty and simplicity,\n\n08:21.120 --> 08:26.120\n it's, I believe, at least the core is about,\n\n08:28.160 --> 08:31.520\n like science, finding regularities in the world,\n\n08:31.520 --> 08:35.120\n understanding the world, which is necessary for survival.\n\n08:35.120 --> 08:39.480\n If I look at a bush and I just see noise,\n\n08:39.480 --> 08:42.080\n and there is a tiger and it eats me, then I'm dead.\n\n08:42.080 --> 08:44.000\n But if I try to find a pattern,\n\n08:44.000 --> 08:49.000\n and we know that humans are prone to find more patterns\n\n08:49.360 --> 08:53.160\n in data than they are, like the Mars face\n\n08:53.160 --> 08:55.680\n and all these things, but these biads\n\n08:55.680 --> 08:58.240\n towards finding patterns, even if they are non,\n\n08:58.240 --> 09:01.360\n but, I mean, it's best, of course, if they are, yeah,\n\n09:01.360 --> 09:02.640\n helps us for survival.\n\n09:04.040 --> 09:04.880\n Yeah, that's fascinating.\n\n09:04.880 --> 09:07.240\n I haven't thought really about the,\n\n09:07.240 --> 09:08.840\n I thought I just loved science,\n\n09:08.840 --> 09:13.600\n but indeed, in terms of just for survival purposes,\n\n09:13.600 --> 09:15.920\n there is an evolutionary argument\n\n09:15.920 --> 09:20.600\n for why we find the work of Einstein so beautiful.\n\n09:21.760 --> 09:24.080\n Maybe a quick small tangent.\n\n09:24.080 --> 09:26.040\n Could you describe what's,\n\n09:26.040 --> 09:28.400\n Salomonov induction is?\n\n09:28.400 --> 09:32.680\n Yeah, so that's a theory which I claim,\n\n09:32.680 --> 09:35.440\n and Mr. Lomanov sort of claimed a long time ago,\n\n09:35.440 --> 09:39.800\n that this solves the big philosophical problem of induction.\n\n09:39.800 --> 09:42.760\n And I believe the claim is essentially true.\n\n09:42.760 --> 09:44.800\n And what it does is the following.\n\n09:44.800 --> 09:49.640\n So, okay, for the picky listener,\n\n09:49.640 --> 09:53.560\n induction can be interpreted narrowly and widely.\n\n09:53.560 --> 09:57.160\n Narrow means inferring models from data.\n\n09:58.800 --> 10:01.240\n And widely means also then using these models\n\n10:01.240 --> 10:02.320\n for doing predictions,\n\n10:02.320 --> 10:04.760\n so predictions also part of the induction.\n\n10:04.760 --> 10:07.680\n So I'm a little bit sloppy sort of with the terminology,\n\n10:07.680 --> 10:10.880\n and maybe that comes from Ray Salomonov, you know,\n\n10:10.880 --> 10:12.800\n being sloppy, maybe I shouldn't say that.\n\n10:12.800 --> 10:15.640\n He can't complain anymore.\n\n10:15.640 --> 10:20.240\n So let me explain a little bit this theory in simple terms.\n\n10:20.240 --> 10:21.960\n So assume you have a data sequence,\n\n10:21.960 --> 10:24.800\n make it very simple, the simplest one say 1, 1, 1, 1, 1,\n\n10:24.800 --> 10:28.840\n and you see if 100 ones, what do you think comes next?\n\n10:28.840 --> 10:30.560\n The natural answer, I'm gonna speed up a little bit,\n\n10:30.560 --> 10:33.640\n the natural answer is of course, you know, one, okay?\n\n10:33.640 --> 10:36.040\n And the question is why, okay?\n\n10:36.040 --> 10:38.920\n Well, we see a pattern there, yeah, okay,\n\n10:38.920 --> 10:40.720\n there's a one and we repeat it.\n\n10:40.720 --> 10:43.440\n And why should it suddenly after 100 ones be different?\n\n10:43.440 --> 10:47.040\n So what we're looking for is simple explanations or models\n\n10:47.040 --> 10:48.640\n for the data we have.\n\n10:48.640 --> 10:49.800\n And now the question is,\n\n10:49.800 --> 10:53.400\n a model has to be presented in a certain language,\n\n10:53.400 --> 10:55.440\n in which language do we use?\n\n10:55.440 --> 10:57.480\n In science, we want formal languages,\n\n10:57.480 --> 10:58.840\n and we can use mathematics,\n\n10:58.840 --> 11:01.920\n or we can use programs on a computer.\n\n11:01.920 --> 11:04.480\n So abstractly on a Turing machine, for instance,\n\n11:04.480 --> 11:06.320\n or it can be a general purpose computer.\n\n11:06.320 --> 11:09.320\n So, and there are of course, lots of models of,\n\n11:09.320 --> 11:11.880\n you can say maybe it's 100 ones and then 100 zeros\n\n11:11.880 --> 11:13.320\n and 100 ones, that's a model, right?\n\n11:13.320 --> 11:17.240\n But there are simpler models, there's a model print one loop,\n\n11:17.240 --> 11:19.840\n and it also explains the data.\n\n11:19.840 --> 11:23.120\n And if you push that to the extreme,\n\n11:23.120 --> 11:25.320\n you are looking for the shortest program,\n\n11:25.320 --> 11:29.400\n which if you run this program reproduces the data you have,\n\n11:29.400 --> 11:32.280\n it will not stop, it will continue naturally.\n\n11:32.280 --> 11:34.600\n And this you take for your prediction.\n\n11:34.600 --> 11:37.040\n And on the sequence of ones, it's very plausible, right?\n\n11:37.040 --> 11:39.400\n That print one loop is the shortest program.\n\n11:39.400 --> 11:41.480\n We can give some more complex examples\n\n11:41.480 --> 11:43.760\n like one, two, three, four, five.\n\n11:43.760 --> 11:44.600\n What comes next?\n\n11:44.600 --> 11:46.240\n The short program is again, you know,\n\n11:46.240 --> 11:50.160\n counter, and so that is roughly speaking\n\n11:50.160 --> 11:51.800\n how solomotive induction works.\n\n11:53.160 --> 11:56.360\n The extra twist is that it can also deal with noisy data.\n\n11:56.360 --> 11:58.680\n So if you have, for instance, a coin flip,\n\n11:58.680 --> 12:02.040\n say a biased coin, which comes up head with 60% probability,\n\n12:03.320 --> 12:06.520\n then it will predict, it will learn and figure this out,\n\n12:06.520 --> 12:09.480\n and after a while it predicts, oh, the next coin flip\n\n12:09.480 --> 12:11.400\n will be head with probability 60%.\n\n12:11.400 --> 12:13.480\n So it's the stochastic version of that.\n\n12:13.480 --> 12:16.440\n But the goal is, the dream is always the search\n\n12:16.440 --> 12:17.520\n for the short program.\n\n12:17.520 --> 12:18.360\n Yes, yeah.\n\n12:18.360 --> 12:21.000\n Well, in solomotive induction, precisely what you do is,\n\n12:21.000 --> 12:24.840\n so you combine, so looking for the shortest program\n\n12:24.840 --> 12:26.520\n is like applying Opaque's razor,\n\n12:26.520 --> 12:28.480\n like looking for the simplest theory.\n\n12:28.480 --> 12:31.160\n There's also Epicorus principle, which says,\n\n12:31.160 --> 12:32.720\n if you have multiple hypotheses,\n\n12:32.720 --> 12:34.440\n which equally well describe your data,\n\n12:34.440 --> 12:36.520\n don't discard any of them, keep all of them around,\n\n12:36.520 --> 12:37.920\n you never know.\n\n12:37.920 --> 12:39.680\n And you can put that together and say,\n\n12:39.680 --> 12:42.080\n okay, I have a bias towards simplicity,\n\n12:42.080 --> 12:44.280\n but it don't rule out the larger models.\n\n12:44.280 --> 12:46.360\n And technically what we do is,\n\n12:46.360 --> 12:49.880\n we weigh the shorter models higher\n\n12:49.880 --> 12:52.040\n and the longer models lower.\n\n12:52.040 --> 12:55.280\n And you use a Bayesian techniques, you have a prior,\n\n12:55.280 --> 12:59.520\n and which is precisely two to the minus\n\n12:59.520 --> 13:01.840\n the complexity of the program.\n\n13:01.840 --> 13:04.440\n And you weigh all this hypothesis and take this mixture,\n\n13:04.440 --> 13:06.840\n and then you get also the stochasticity in.\n\n13:06.840 --> 13:08.200\n Yeah, like many of your ideas,\n\n13:08.200 --> 13:10.560\n that's just a beautiful idea of weighing based\n\n13:10.560 --> 13:12.280\n on the simplicity of the program.\n\n13:12.280 --> 13:15.480\n I love that, that seems to me\n\n13:15.480 --> 13:17.200\n maybe a very human centric concept.\n\n13:17.200 --> 13:19.440\n It seems to be a very appealing way\n\n13:19.440 --> 13:23.560\n of discovering good programs in this world.\n\n13:24.600 --> 13:27.760\n You've used the term compression quite a bit.\n\n13:27.760 --> 13:30.240\n I think it's a beautiful idea.\n\n13:30.240 --> 13:32.600\n Sort of, we just talked about simplicity\n\n13:32.600 --> 13:37.280\n and maybe science or just all of our intellectual pursuits\n\n13:37.280 --> 13:41.040\n is basically the time to compress the complexity\n\n13:41.040 --> 13:43.080\n all around us into something simple.\n\n13:43.080 --> 13:48.080\n So what does this word mean to you, compression?\n\n13:49.920 --> 13:51.560\n I essentially have already explained it.\n\n13:51.560 --> 13:53.960\n So it compression means for me,\n\n13:53.960 --> 13:58.400\n finding short programs for the data\n\n13:58.400 --> 13:59.760\n or the phenomenon at hand.\n\n13:59.760 --> 14:01.640\n You could interpret it more widely,\n\n14:01.640 --> 14:03.960\n finding simple theories,\n\n14:03.960 --> 14:05.440\n which can be mathematical theories\n\n14:05.440 --> 14:09.040\n or maybe even informal, like just in words.\n\n14:09.040 --> 14:11.920\n Compression means finding short descriptions,\n\n14:11.920 --> 14:14.880\n explanations, programs for the data.\n\n14:14.880 --> 14:19.880\n Do you see science as a kind of our human attempt\n\n14:20.320 --> 14:23.040\n at compression, so we're speaking more generally,\n\n14:23.040 --> 14:24.920\n because when you say programs,\n\n14:24.920 --> 14:26.800\n you're kind of zooming in on a particular sort of\n\n14:26.800 --> 14:28.080\n almost like a computer science,\n\n14:28.080 --> 14:30.200\n artificial intelligence focus,\n\n14:30.200 --> 14:31.920\n but do you see all of human endeavor\n\n14:31.920 --> 14:34.360\n as a kind of compression?\n\n14:34.360 --> 14:35.560\n Well, at least all of science,\n\n14:35.560 --> 14:37.600\n I see as an endeavor of compression,\n\n14:37.600 --> 14:39.680\n not all of humanity, maybe.\n\n14:39.680 --> 14:42.160\n And well, there are also some other aspects of science\n\n14:42.160 --> 14:43.600\n like experimental design, right?\n\n14:43.600 --> 14:47.440\n I mean, we create experiments specifically\n\n14:47.440 --> 14:48.720\n to get extra knowledge.\n\n14:48.720 --> 14:52.320\n And that isn't part of the decision making process,\n\n14:53.320 --> 14:55.400\n but once we have the data,\n\n14:55.400 --> 14:58.160\n to understand the data is essentially compression.\n\n14:58.160 --> 15:00.800\n So I don't see any difference between compression,\n\n15:00.800 --> 15:05.040\n compression, understanding, and prediction.\n\n15:05.960 --> 15:07.960\n So we're jumping around topics a little bit,\n\n15:07.960 --> 15:10.480\n but returning back to simplicity,\n\n15:10.480 --> 15:14.320\n a fascinating concept of Kolmogorov complexity.\n\n15:14.320 --> 15:17.120\n So in your sense, do most objects\n\n15:17.120 --> 15:19.680\n in our mathematical universe\n\n15:19.680 --> 15:21.960\n have high Kolmogorov complexity?\n\n15:21.960 --> 15:24.080\n And maybe what is, first of all,\n\n15:24.080 --> 15:25.960\n what is Kolmogorov complexity?\n\n15:25.960 --> 15:28.400\n Okay, Kolmogorov complexity is a notion\n\n15:28.400 --> 15:31.160\n of simplicity or complexity,\n\n15:31.160 --> 15:35.960\n and it takes the compression view to the extreme.\n\n15:35.960 --> 15:39.680\n So I explained before that if you have some data sequence,\n\n15:39.680 --> 15:41.720\n just think about a file in a computer\n\n15:41.720 --> 15:45.120\n and best sort of, you know, just a string of bits.\n\n15:45.120 --> 15:49.440\n And if you, and we have data compressors,\n\n15:49.440 --> 15:52.040\n like we compress big files into zip files\n\n15:52.040 --> 15:53.720\n with certain compressors.\n\n15:53.720 --> 15:56.360\n And you can also produce self extracting ArcaFs.\n\n15:56.360 --> 15:58.000\n That means as an executable,\n\n15:58.000 --> 16:00.760\n if you run it, it reproduces your original file\n\n16:00.760 --> 16:02.880\n without needing an extra decompressor.\n\n16:02.880 --> 16:06.240\n It's just a decompressor plus the ArcaF together in one.\n\n16:06.240 --> 16:08.840\n And now there are better and worse compressors,\n\n16:08.840 --> 16:11.120\n and you can ask, what is the ultimate compressor?\n\n16:11.120 --> 16:14.880\n So what is the shortest possible self extracting ArcaF\n\n16:14.880 --> 16:17.920\n you could produce for a certain data set here,\n\n16:17.920 --> 16:19.560\n which reproduces the data set.\n\n16:19.560 --> 16:23.320\n And the length of this is called the Kolmogorov complexity.\n\n16:23.320 --> 16:26.680\n And arguably that is the information content\n\n16:26.680 --> 16:27.960\n in the data set.\n\n16:27.960 --> 16:30.480\n I mean, if the data set is very redundant or very boring,\n\n16:30.480 --> 16:31.760\n you can compress it very well.\n\n16:31.760 --> 16:34.760\n So the information content should be low\n\n16:34.760 --> 16:36.920\n and you know, it is low according to this definition.\n\n16:36.920 --> 16:39.720\n So it's the length of the shortest program\n\n16:39.720 --> 16:41.040\n that summarizes the data?\n\n16:41.040 --> 16:42.040\n Yes.\n\n16:42.040 --> 16:46.280\n And what's your sense of our sort of universe\n\n16:46.280 --> 16:51.280\n when we think about the different objects in our universe\n\n16:51.360 --> 16:55.440\n that we try, concepts or whatever at every level,\n\n16:55.440 --> 16:58.320\n do they have higher or low Kolmogorov complexity?\n\n16:58.320 --> 16:59.400\n So what's the hope?\n\n17:00.280 --> 17:01.400\n Do we have a lot of hope\n\n17:01.400 --> 17:04.400\n and be able to summarize much of our world?\n\n17:05.680 --> 17:08.520\n That's a tricky and difficult question.\n\n17:08.520 --> 17:13.520\n So as I said before, I believe that the whole universe\n\n17:13.560 --> 17:16.760\n based on the evidence we have is very simple.\n\n17:16.760 --> 17:19.240\n So it has a very short description.\n\n17:19.240 --> 17:23.200\n Sorry, to linger on that, the whole universe,\n\n17:23.200 --> 17:24.040\n what does that mean?\n\n17:24.040 --> 17:26.720\n You mean at the very basic fundamental level\n\n17:26.720 --> 17:28.560\n in order to create the universe?\n\n17:28.560 --> 17:29.400\n Yes, yeah.\n\n17:29.400 --> 17:32.960\n So you need a very short program and you run it.\n\n17:32.960 --> 17:34.040\n To get the thing going.\n\n17:34.040 --> 17:35.040\n To get the thing going\n\n17:35.040 --> 17:37.480\n and then it will reproduce our universe.\n\n17:37.480 --> 17:39.320\n There's a problem with noise.\n\n17:39.320 --> 17:42.080\n We can come back to that later possibly.\n\n17:42.080 --> 17:45.240\n Is noise a problem or is it a bug or a feature?\n\n17:46.240 --> 17:49.440\n I would say it makes our life as a scientist\n\n17:49.440 --> 17:52.160\n really, really much harder.\n\n17:52.160 --> 17:53.480\n I mean, think about without noise,\n\n17:53.480 --> 17:55.920\n we wouldn't need all of the statistics.\n\n17:55.920 --> 17:58.840\n But then maybe we wouldn't feel like there's a free will.\n\n17:58.840 --> 18:01.360\n Maybe we need that for the...\n\n18:01.360 --> 18:04.000\n This is an illusion that noise can give you free will.\n\n18:04.000 --> 18:06.640\n At least in that way, it's a feature.\n\n18:06.640 --> 18:09.000\n But also, if you don't have noise,\n\n18:09.000 --> 18:10.720\n you have chaotic phenomena,\n\n18:10.720 --> 18:12.720\n which are effectively like noise.\n\n18:12.720 --> 18:15.680\n So we can't get away with statistics even then.\n\n18:15.680 --> 18:17.520\n I mean, think about rolling a dice\n\n18:17.520 --> 18:19.200\n and forget about quantum mechanics\n\n18:19.200 --> 18:21.160\n and you know exactly how you throw it.\n\n18:21.160 --> 18:24.000\n But I mean, it's still so hard to compute the trajectory\n\n18:24.000 --> 18:26.400\n that effectively it is best to model it\n\n18:26.400 --> 18:30.080\n as coming out with a number,\n\n18:30.080 --> 18:31.640\n this probability one over six.\n\n18:33.040 --> 18:36.320\n But from this set of philosophical\n\n18:36.320 --> 18:38.080\n Kolmogorov complexity perspective,\n\n18:38.080 --> 18:39.880\n if we didn't have noise,\n\n18:39.880 --> 18:43.160\n then arguably you could describe the whole universe\n\n18:43.160 --> 18:47.400\n as well as a standard model plus generativity.\n\n18:47.400 --> 18:49.600\n I mean, we don't have a theory of everything yet,\n\n18:49.600 --> 18:52.200\n but sort of assuming we are close to it or have it.\n\n18:52.200 --> 18:55.400\n Plus the initial conditions, which may hopefully be simple.\n\n18:55.400 --> 18:56.600\n And then you just run it\n\n18:56.600 --> 18:59.040\n and then you would reproduce the universe.\n\n18:59.040 --> 19:03.520\n But that's spoiled by noise or by chaotic systems\n\n19:03.520 --> 19:06.280\n or by initial conditions, which may be complex.\n\n19:06.280 --> 19:09.680\n So now if we don't take the whole universe,\n\n19:09.680 --> 19:13.720\n but just a subset, just take planet Earth.\n\n19:13.720 --> 19:15.600\n Planet Earth cannot be compressed\n\n19:15.600 --> 19:17.520\n into a couple of equations.\n\n19:17.520 --> 19:19.200\n This is a hugely complex system.\n\n19:19.200 --> 19:20.040\n So interesting.\n\n19:20.040 --> 19:21.640\n So when you look at the window,\n\n19:21.640 --> 19:23.000\n like the whole thing might be simple,\n\n19:23.000 --> 19:26.080\n but when you just take a small window, then...\n\n19:26.080 --> 19:28.760\n It may become complex and that may be counterintuitive,\n\n19:28.760 --> 19:31.720\n but there's a very nice analogy.\n\n19:31.720 --> 19:34.240\n The book, the library of all books.\n\n19:34.240 --> 19:36.960\n So imagine you have a normal library with interesting books\n\n19:36.960 --> 19:39.320\n and you go there, great, lots of information\n\n19:39.320 --> 19:41.960\n and quite complex.\n\n19:41.960 --> 19:45.000\n So now I create a library which contains all possible books,\n\n19:45.000 --> 19:46.800\n say of 500 pages.\n\n19:46.800 --> 19:49.680\n So the first book just has A, A, A, A, A over all the pages.\n\n19:49.680 --> 19:52.240\n The next book A, A, A and ends with B and so on.\n\n19:52.240 --> 19:54.200\n I create this library of all books.\n\n19:54.200 --> 19:57.280\n I can write a super short program which creates this library.\n\n19:57.280 --> 19:59.000\n So this library which has all books\n\n19:59.000 --> 20:01.280\n has zero information content.\n\n20:01.280 --> 20:02.880\n And you take a subset of this library\n\n20:02.880 --> 20:05.320\n and suddenly you have a lot of information in there.\n\n20:05.320 --> 20:06.680\n So that's fascinating.\n\n20:06.680 --> 20:08.320\n I think one of the most beautiful object,\n\n20:08.320 --> 20:10.440\n mathematical objects that at least today\n\n20:10.440 --> 20:12.520\n seems to be understudied or under talked about\n\n20:12.520 --> 20:14.920\n is cellular automata.\n\n20:14.920 --> 20:18.560\n What lessons do you draw from sort of the game of life\n\n20:18.560 --> 20:20.800\n for cellular automata where you start with the simple rules\n\n20:20.800 --> 20:22.840\n just like you're describing with the universe\n\n20:22.840 --> 20:26.280\n and somehow complexity emerges.\n\n20:26.280 --> 20:30.400\n Do you feel like you have an intuitive grasp\n\n20:30.400 --> 20:34.120\n on the fascinating behavior of such systems\n\n20:34.120 --> 20:37.560\n where like you said, some chaotic behavior could happen,\n\n20:37.560 --> 20:39.560\n some complexity could emerge,\n\n20:39.560 --> 20:43.680\n some it could die out and some very rigid structures.\n\n20:43.680 --> 20:46.760\n Do you have a sense about cellular automata\n\n20:46.760 --> 20:48.200\n that somehow transfers maybe\n\n20:48.200 --> 20:50.960\n to the bigger questions of our universe?\n\n20:50.960 --> 20:51.960\n Yeah, the cellular automata\n\n20:51.960 --> 20:54.240\n and especially the Conway's game of life\n\n20:54.240 --> 20:56.240\n is really great because these rules are so simple.\n\n20:56.240 --> 20:57.720\n You can explain it to every child\n\n20:57.720 --> 21:00.280\n and even by hand you can simulate a little bit\n\n21:00.280 --> 21:04.040\n and you see these beautiful patterns emerge\n\n21:04.040 --> 21:06.800\n and people have proven that it's even Turing complete.\n\n21:06.800 --> 21:09.840\n You cannot just use a computer to simulate game of life\n\n21:09.840 --> 21:13.480\n but you can also use game of life to simulate any computer.\n\n21:13.480 --> 21:16.520\n That is truly amazing.\n\n21:16.520 --> 21:21.240\n And it's the prime example probably to demonstrate\n\n21:21.240 --> 21:25.040\n that very simple rules can lead to very rich phenomena.\n\n21:25.040 --> 21:26.840\n And people sometimes,\n\n21:26.840 --> 21:29.720\n how is chemistry and biology so rich?\n\n21:29.720 --> 21:32.400\n I mean, this can't be based on simple rules.\n\n21:32.400 --> 21:34.520\n But no, we know quantum electrodynamics\n\n21:34.520 --> 21:36.360\n describes all of chemistry.\n\n21:36.360 --> 21:38.960\n And we come later back to that.\n\n21:38.960 --> 21:40.960\n I claim intelligence can be explained\n\n21:40.960 --> 21:43.000\n or described in one single equation.\n\n21:43.000 --> 21:44.600\n This very rich phenomenon.\n\n21:45.720 --> 21:49.880\n You asked also about whether I understand this phenomenon\n\n21:49.880 --> 21:53.240\n and it's probably not.\n\n21:54.280 --> 21:55.560\n And there's this saying,\n\n21:55.560 --> 21:56.800\n you never understand really things,\n\n21:56.800 --> 21:58.360\n you just get used to them.\n\n21:58.360 --> 22:03.360\n And I think I got pretty used to cellular automata.\n\n22:03.600 --> 22:05.440\n So you believe that you understand\n\n22:05.440 --> 22:07.120\n now why this phenomenon happens.\n\n22:07.120 --> 22:09.240\n But I give you a different example.\n\n22:09.240 --> 22:11.760\n I didn't play too much with Conway's game of life\n\n22:11.760 --> 22:15.000\n but a little bit more with fractals\n\n22:15.000 --> 22:18.480\n and with the Mandelbrot set and these beautiful patterns,\n\n22:18.480 --> 22:19.960\n just look Mandelbrot set.\n\n22:21.000 --> 22:23.280\n And well, when the computers were really slow\n\n22:23.280 --> 22:25.280\n and I just had a black and white monitor\n\n22:25.280 --> 22:29.040\n and programmed my own programs in assembler too.\n\n22:29.040 --> 22:30.920\n Assembler, wow.\n\n22:30.920 --> 22:32.360\n Wow, you're legit.\n\n22:33.720 --> 22:35.480\n To get these fractals on the screen\n\n22:35.480 --> 22:37.320\n and it was mesmerized and much later.\n\n22:37.320 --> 22:40.080\n So I returned to this every couple of years\n\n22:40.080 --> 22:42.800\n and then I tried to understand what is going on.\n\n22:42.800 --> 22:44.800\n And you can understand a little bit.\n\n22:44.800 --> 22:48.720\n So I tried to derive the locations,\n\n22:48.720 --> 22:53.520\n there are these circles and the apple shape\n\n22:53.520 --> 22:57.360\n and then you have smaller Mandelbrot sets\n\n22:57.360 --> 22:59.000\n recursively in this set.\n\n22:59.000 --> 23:01.720\n And there's a way to mathematically\n\n23:01.720 --> 23:03.480\n by solving high order polynomials\n\n23:03.480 --> 23:05.640\n to figure out where these centers are\n\n23:05.640 --> 23:08.080\n and what size they are approximately.\n\n23:08.080 --> 23:12.560\n And by sort of mathematically approaching this problem,\n\n23:12.560 --> 23:17.560\n you slowly get a feeling of why things are like they are\n\n23:18.080 --> 23:20.760\n and that sort of isn't, you know,\n\n23:21.960 --> 23:24.880\n first step to understanding why this rich phenomena.\n\n23:24.880 --> 23:27.200\n Do you think it's possible, what's your intuition?\n\n23:27.200 --> 23:28.880\n Do you think it's possible to reverse engineer\n\n23:28.880 --> 23:33.320\n and find the short program that generated these fractals\n\n23:33.320 --> 23:36.400\n sort of by looking at the fractals?\n\n23:36.400 --> 23:38.840\n Well, in principle, yes, yeah.\n\n23:38.840 --> 23:42.000\n So, I mean, in principle, what you can do is\n\n23:42.000 --> 23:43.480\n you take, you know, any data set, you know,\n\n23:43.480 --> 23:46.480\n you take these fractals or you take whatever your data set,\n\n23:46.480 --> 23:51.000\n whatever you have, say a picture of Convey's Game of Life\n\n23:51.000 --> 23:53.200\n and you run through all programs.\n\n23:53.200 --> 23:55.280\n You take a program size one, two, three, four\n\n23:55.280 --> 23:57.080\n and all these programs around them all in parallel\n\n23:57.080 --> 23:59.080\n in so called dovetailing fashion,\n\n23:59.080 --> 24:01.320\n give them computational resources,\n\n24:01.320 --> 24:03.880\n first one 50%, second one half resources and so on\n\n24:03.880 --> 24:06.960\n and let them run, wait until they halt,\n\n24:06.960 --> 24:09.120\n give an output, compare it to your data\n\n24:09.120 --> 24:12.360\n and if some of these programs produce the correct data,\n\n24:12.360 --> 24:14.480\n then you stop and then you have already some program.\n\n24:14.480 --> 24:16.680\n It may be a long program because it's faster\n\n24:16.680 --> 24:18.760\n and then you continue and you get shorter\n\n24:18.760 --> 24:20.760\n and shorter programs until you eventually\n\n24:20.760 --> 24:22.520\n find the shortest program.\n\n24:22.520 --> 24:24.040\n The interesting thing, you can never know\n\n24:24.040 --> 24:25.520\n whether it's the shortest program\n\n24:25.520 --> 24:27.440\n because there could be an even shorter program\n\n24:27.440 --> 24:32.200\n which is just even slower and you just have to wait here.\n\n24:32.200 --> 24:35.000\n But asymptotically and actually after a finite time,\n\n24:35.000 --> 24:36.480\n you have the shortest program.\n\n24:36.480 --> 24:40.440\n So this is a theoretical but completely impractical way\n\n24:40.440 --> 24:45.440\n of finding the underlying structure in every data set\n\n24:47.440 --> 24:49.040\n and that is what Solomov induction does\n\n24:49.040 --> 24:50.680\n and Kolmogorov complexity.\n\n24:50.680 --> 24:52.680\n In practice, of course, we have to approach the problem\n\n24:52.680 --> 24:53.760\n more intelligently.\n\n24:53.760 --> 24:58.760\n And then if you take resource limitations into account,\n\n24:58.760 --> 25:01.760\n there's, for instance, a field of pseudo random numbers\n\n25:01.760 --> 25:06.760\n and these are deterministic sequences,\n\n25:06.760 --> 25:09.120\n but no algorithm which is fast,\n\n25:09.120 --> 25:10.800\n fast means runs in polynomial time,\n\n25:10.800 --> 25:13.800\n can detect that it's actually deterministic.\n\n25:13.800 --> 25:16.040\n So we can produce interesting,\n\n25:16.040 --> 25:17.680\n I mean, random numbers maybe not that interesting,\n\n25:17.680 --> 25:18.520\n but just an example.\n\n25:18.520 --> 25:22.480\n We can produce complex looking data\n\n25:22.480 --> 25:25.280\n and we can then prove that no fast algorithm\n\n25:25.280 --> 25:27.440\n can detect the underlying pattern.\n\n25:27.440 --> 25:32.440\n Which is, unfortunately, that's a big challenge\n\n25:34.240 --> 25:35.920\n for our search for simple programs\n\n25:35.920 --> 25:38.440\n in the space of artificial intelligence, perhaps.\n\n25:38.440 --> 25:40.480\n Yes, it definitely is for artificial intelligence\n\n25:40.480 --> 25:44.520\n and it's quite surprising that it's, I can't say easy.\n\n25:44.520 --> 25:48.240\n I mean, physicists worked really hard to find these theories,\n\n25:48.240 --> 25:51.920\n but apparently it was possible for human minds\n\n25:51.920 --> 25:54.040\n to find these simple rules in the universe.\n\n25:54.040 --> 25:59.200\n It could have been different, right?\n\n25:59.200 --> 26:00.200\n It could have been different.\n\n26:00.200 --> 26:04.720\n It's awe inspiring.\n\n26:04.720 --> 26:09.120\n So let me ask another absurdly big question.\n\n26:09.120 --> 26:13.280\n What is intelligence in your view?\n\n26:13.280 --> 26:17.080\n So I have, of course, a definition.\n\n26:17.080 --> 26:18.240\n I wasn't sure what you're going to say\n\n26:18.240 --> 26:20.000\n because you could have just as easily said,\n\n26:20.000 --> 26:21.520\n I have no clue.\n\n26:21.520 --> 26:23.360\n Which many people would say,\n\n26:23.360 --> 26:26.680\n but I'm not modest in this question.\n\n26:26.680 --> 26:31.440\n So the informal version,\n\n26:31.440 --> 26:33.120\n which I worked out together with Shane Lack,\n\n26:33.120 --> 26:35.520\n who cofounded DeepMind,\n\n26:35.520 --> 26:38.720\n is that intelligence measures an agent's ability\n\n26:38.720 --> 26:42.880\n to perform well in a wide range of environments.\n\n26:42.880 --> 26:45.800\n So that doesn't sound very impressive.\n\n26:45.800 --> 26:49.560\n And these words have been very carefully chosen\n\n26:49.560 --> 26:52.960\n and there is a mathematical theory behind that\n\n26:52.960 --> 26:54.920\n and we come back to that later.\n\n26:54.920 --> 26:59.640\n And if you look at this definition by itself,\n\n26:59.640 --> 27:01.160\n it seems like, yeah, okay,\n\n27:01.160 --> 27:03.400\n but it seems a lot of things are missing.\n\n27:03.400 --> 27:05.000\n But if you think it through,\n\n27:05.920 --> 27:08.760\n then you realize that most,\n\n27:08.760 --> 27:10.680\n and I claim all of the other traits,\n\n27:10.680 --> 27:12.600\n at least of rational intelligence,\n\n27:12.600 --> 27:14.440\n which we usually associate with intelligence,\n\n27:14.440 --> 27:17.960\n are emergent phenomena from this definition.\n\n27:17.960 --> 27:22.160\n Like creativity, memorization, planning, knowledge.\n\n27:22.160 --> 27:25.000\n You all need that in order to perform well\n\n27:25.000 --> 27:27.400\n in a wide range of environments.\n\n27:27.400 --> 27:29.000\n So you don't have to explicitly mention\n\n27:29.000 --> 27:29.960\n that in a definition.\n\n27:29.960 --> 27:30.800\n Interesting.\n\n27:30.800 --> 27:34.040\n So yeah, so the consciousness, abstract reasoning,\n\n27:34.040 --> 27:36.200\n all these kinds of things are just emergent phenomena\n\n27:36.200 --> 27:39.640\n that help you in towards,\n\n27:40.640 --> 27:41.880\n can you say the definition again?\n\n27:41.880 --> 27:44.160\n So multiple environments.\n\n27:44.160 --> 27:45.880\n Did you mention the word goals?\n\n27:45.880 --> 27:47.760\n No, but we have an alternative definition.\n\n27:47.760 --> 27:48.800\n Instead of performing well,\n\n27:48.800 --> 27:50.160\n you can just replace it by goals.\n\n27:50.160 --> 27:53.280\n So intelligence measures an agent's ability\n\n27:53.280 --> 27:55.680\n to achieve goals in a wide range of environments.\n\n27:55.680 --> 27:56.520\n That's more or less equal.\n\n27:56.520 --> 27:57.360\n But interesting,\n\n27:57.360 --> 27:59.680\n because in there, there's an injection of the word goals.\n\n27:59.680 --> 28:03.160\n So we want to specify there should be a goal.\n\n28:03.160 --> 28:04.800\n Yeah, but perform well is sort of,\n\n28:04.800 --> 28:05.760\n what does it mean?\n\n28:05.760 --> 28:06.640\n It's the same problem.\n\n28:06.640 --> 28:07.760\n Yeah.\n\n28:07.760 --> 28:09.240\n There's a little bit gray area,\n\n28:09.240 --> 28:12.280\n but it's much closer to something that could be formalized.\n\n28:14.080 --> 28:16.320\n In your view, are humans,\n\n28:16.320 --> 28:18.320\n where do humans fit into that definition?\n\n28:18.320 --> 28:21.920\n Are they general intelligence systems\n\n28:21.920 --> 28:24.120\n that are able to perform in,\n\n28:24.120 --> 28:27.840\n like how good are they at fulfilling that definition\n\n28:27.840 --> 28:31.200\n at performing well in multiple environments?\n\n28:31.200 --> 28:32.760\n Yeah, that's a big question.\n\n28:32.760 --> 28:37.640\n I mean, the humans are performing best among all species.\n\n28:37.640 --> 28:40.680\n We know of, yeah.\n\n28:40.680 --> 28:41.520\n Depends.\n\n28:41.520 --> 28:44.440\n You could say that trees and plants are doing a better job.\n\n28:44.440 --> 28:46.280\n They'll probably outlast us.\n\n28:46.280 --> 28:49.400\n Yeah, but they are in a much more narrow environment, right?\n\n28:49.400 --> 28:51.680\n I mean, you just have a little bit of air pollutions\n\n28:51.680 --> 28:54.040\n and these trees die and we can adapt, right?\n\n28:54.040 --> 28:55.440\n We build houses, we build filters,\n\n28:55.440 --> 28:59.480\n we do geoengineering.\n\n28:59.480 --> 29:01.040\n So the multiple environment part.\n\n29:01.040 --> 29:02.600\n Yeah, that is very important, yeah.\n\n29:02.600 --> 29:04.640\n So that distinguish narrow intelligence\n\n29:04.640 --> 29:07.320\n from wide intelligence, also in the AI research.\n\n29:08.400 --> 29:12.080\n So let me ask the Allentourian question.\n\n29:12.080 --> 29:14.160\n Can machines think?\n\n29:14.160 --> 29:15.880\n Can machines be intelligent?\n\n29:15.880 --> 29:19.560\n So in your view, I have to kind of ask,\n\n29:19.560 --> 29:20.560\n the answer is probably yes,\n\n29:20.560 --> 29:24.360\n but I want to kind of hear what your thoughts on it.\n\n29:24.360 --> 29:27.720\n Can machines be made to fulfill this definition\n\n29:27.720 --> 29:30.760\n of intelligence, to achieve intelligence?\n\n29:30.760 --> 29:33.000\n Well, we are sort of getting there\n\n29:33.000 --> 29:35.840\n and on a small scale, we are already there.\n\n29:36.720 --> 29:38.960\n The wide range of environments are missing,\n\n29:38.960 --> 29:40.320\n but we have self driving cars,\n\n29:40.320 --> 29:42.720\n we have programs which play Go and chess,\n\n29:42.720 --> 29:44.440\n we have speech recognition.\n\n29:44.440 --> 29:45.480\n So that's pretty amazing,\n\n29:45.480 --> 29:48.400\n but these are narrow environments.\n\n29:49.560 --> 29:51.000\n But if you look at AlphaZero,\n\n29:51.000 --> 29:53.720\n that was also developed by DeepMind.\n\n29:53.720 --> 29:55.400\n I mean, got famous with AlphaGo\n\n29:55.400 --> 29:57.720\n and then came AlphaZero a year later.\n\n29:57.720 --> 29:59.280\n That was truly amazing.\n\n29:59.280 --> 30:01.800\n So reinforcement learning algorithm,\n\n30:01.800 --> 30:04.440\n which is able just by self play,\n\n30:04.440 --> 30:08.560\n to play chess and then also Go.\n\n30:08.560 --> 30:10.120\n And I mean, yes, they're both games,\n\n30:10.120 --> 30:11.400\n but they're quite different games.\n\n30:11.400 --> 30:15.120\n And you didn't don't feed them the rules of the game.\n\n30:15.120 --> 30:16.720\n And the most remarkable thing,\n\n30:16.720 --> 30:18.080\n which is still a mystery to me,\n\n30:18.080 --> 30:21.040\n that usually for any decent chess program,\n\n30:21.040 --> 30:22.800\n I don't know much about Go,\n\n30:22.800 --> 30:26.960\n you need opening books and end game tables and so on too.\n\n30:26.960 --> 30:29.680\n And nothing in there, nothing was put in there.\n\n30:29.680 --> 30:31.360\n Especially with AlphaZero,\n\n30:31.360 --> 30:33.520\n the self playing mechanism starting from scratch,\n\n30:33.520 --> 30:38.520\n being able to learn actually new strategies is...\n\n30:39.040 --> 30:43.040\n Yeah, it rediscovered all these famous openings\n\n30:43.040 --> 30:46.280\n within four hours by itself.\n\n30:46.280 --> 30:47.480\n What I was really happy about,\n\n30:47.480 --> 30:50.200\n I'm a terrible chess player, but I like Queen Gumby.\n\n30:50.200 --> 30:53.160\n And AlphaZero figured out that this is the best opening.\n\n30:53.160 --> 30:58.160\n Finally, somebody proved you correct.\n\n30:59.920 --> 31:01.680\n So yes, to answer your question,\n\n31:01.680 --> 31:05.040\n yes, I believe that general intelligence is possible.\n\n31:05.040 --> 31:08.280\n And it also, I mean, it depends how you define it.\n\n31:08.280 --> 31:11.520\n Do you say AGI with general intelligence,\n\n31:11.520 --> 31:13.600\n artificial intelligence,\n\n31:13.600 --> 31:16.120\n only refers to if you achieve human level\n\n31:16.120 --> 31:18.600\n or a subhuman level, but quite broad,\n\n31:18.600 --> 31:19.960\n is it also general intelligence?\n\n31:19.960 --> 31:20.920\n So we have to distinguish,\n\n31:20.920 --> 31:23.360\n or it's only super human intelligence,\n\n31:23.360 --> 31:25.120\n general artificial intelligence.\n\n31:25.120 --> 31:26.680\n Is there a test in your mind,\n\n31:26.680 --> 31:28.680\n like the Turing test for natural language\n\n31:28.680 --> 31:32.000\n or some other test that would impress the heck out of you\n\n31:32.000 --> 31:36.960\n that would kind of cross the line of your sense\n\n31:36.960 --> 31:39.840\n of intelligence within the framework that you said?\n\n31:39.840 --> 31:42.960\n Well, the Turing test has been criticized a lot,\n\n31:42.960 --> 31:45.880\n but I think it's not as bad as some people think.\n\n31:45.880 --> 31:47.680\n And some people think it's too strong.\n\n31:47.680 --> 31:52.120\n So it tests not just for system to be intelligent,\n\n31:52.120 --> 31:56.960\n but it also has to fake human deception,\n\n31:56.960 --> 31:58.960\n which is much harder.\n\n31:58.960 --> 32:01.160\n And on the other hand, they say it's too weak\n\n32:01.160 --> 32:05.640\n because it just maybe fakes emotions\n\n32:05.640 --> 32:07.680\n or intelligent behavior.\n\n32:07.680 --> 32:09.400\n It's not real.\n\n32:09.400 --> 32:11.960\n But I don't think that's the problem or a big problem.\n\n32:11.960 --> 32:14.480\n So if you would pass the Turing test,\n\n32:15.720 --> 32:20.600\n so a conversation over terminal with a bot for an hour,\n\n32:20.600 --> 32:21.760\n or maybe a day or so,\n\n32:21.760 --> 32:25.080\n and you can fool a human into not knowing\n\n32:25.080 --> 32:26.120\n whether this is a human or not,\n\n32:26.120 --> 32:27.720\n so that's the Turing test,\n\n32:27.720 --> 32:30.240\n I would be truly impressed.\n\n32:30.240 --> 32:34.360\n And we have this annual competition, the L\u00fcbner Prize.\n\n32:34.360 --> 32:35.960\n And I mean, it started with ELISA,\n\n32:35.960 --> 32:38.200\n that was the first conversational program.\n\n32:38.200 --> 32:40.200\n And what is it called?\n\n32:40.200 --> 32:41.760\n The Japanese Mitsuko or so.\n\n32:41.760 --> 32:44.680\n That's the winner of the last couple of years.\n\n32:44.680 --> 32:45.520\n And well.\n\n32:45.520 --> 32:46.360\n Quite impressive.\n\n32:46.360 --> 32:47.200\n Yeah, it's quite impressive.\n\n32:47.200 --> 32:50.240\n And then Google has developed Mina, right?\n\n32:50.240 --> 32:55.200\n Just recently, that's an open domain conversational bot,\n\n32:55.200 --> 32:57.560\n just a couple of weeks ago, I think.\n\n32:57.560 --> 32:58.760\n Yeah, I kind of like the metric\n\n32:58.760 --> 33:01.680\n that sort of the Alexa Prize has proposed.\n\n33:01.680 --> 33:02.880\n I mean, maybe it's obvious to you.\n\n33:02.880 --> 33:06.400\n It wasn't to me of setting sort of a length\n\n33:06.400 --> 33:07.720\n of a conversation.\n\n33:07.720 --> 33:10.920\n Like you want the bot to be sufficiently interesting\n\n33:10.920 --> 33:12.360\n that you would want to keep talking to it\n\n33:12.360 --> 33:13.640\n for like 20 minutes.\n\n33:13.640 --> 33:18.640\n And that's a surprisingly effective in aggregate metric,\n\n33:19.520 --> 33:24.520\n because really, like nobody has the patience\n\n33:24.960 --> 33:27.720\n to be able to talk to a bot that's not interesting\n\n33:27.720 --> 33:29.000\n and intelligent and witty,\n\n33:29.000 --> 33:32.960\n and is able to go on to different tangents, jump domains,\n\n33:32.960 --> 33:35.360\n be able to say something interesting\n\n33:35.360 --> 33:36.680\n to maintain your attention.\n\n33:36.680 --> 33:39.040\n And maybe many humans will also fail this test.\n\n33:39.040 --> 33:42.840\n That's the, unfortunately, we set,\n\n33:42.840 --> 33:45.400\n just like with autonomous vehicles, with chatbots,\n\n33:45.400 --> 33:48.200\n we also set a bar that's way too high to reach.\n\n33:48.200 --> 33:50.000\n I said, you know, the Turing test is not as bad\n\n33:50.000 --> 33:51.160\n as some people believe,\n\n33:51.160 --> 33:55.920\n but what is really not useful about the Turing test,\n\n33:55.920 --> 33:58.160\n it gives us no guidance\n\n33:58.160 --> 34:00.560\n how to develop these systems in the first place.\n\n34:00.560 --> 34:02.960\n Of course, you know, we can develop them by trial and error\n\n34:02.960 --> 34:05.400\n and, you know, do whatever and then run the test\n\n34:05.400 --> 34:06.880\n and see whether it works or not.\n\n34:06.880 --> 34:11.880\n But a mathematical definition of intelligence\n\n34:12.320 --> 34:16.200\n gives us, you know, an objective,\n\n34:16.200 --> 34:19.520\n which we can then analyze by theoretical tools\n\n34:19.520 --> 34:22.480\n or computational, and, you know,\n\n34:22.480 --> 34:25.160\n maybe even prove how close we are.\n\n34:25.160 --> 34:28.760\n And we will come back to that later with the iXe model.\n\n34:28.760 --> 34:31.280\n So, I mentioned the compression, right?\n\n34:31.280 --> 34:33.320\n So in natural language processing,\n\n34:33.320 --> 34:36.760\n they have achieved amazing results.\n\n34:36.760 --> 34:38.760\n And one way to test this, of course,\n\n34:38.760 --> 34:40.280\n you know, take the system, you train it,\n\n34:40.280 --> 34:43.200\n and then you see how well it performs on the task.\n\n34:43.200 --> 34:47.520\n But a lot of performance measurement\n\n34:47.520 --> 34:49.040\n is done by so called perplexity,\n\n34:49.040 --> 34:51.920\n which is essentially the same as complexity\n\n34:51.920 --> 34:53.240\n or compression length.\n\n34:53.240 --> 34:55.920\n So the NLP community develops new systems\n\n34:55.920 --> 34:57.520\n and then they measure the compression length\n\n34:57.520 --> 35:01.280\n and then they have ranking and leaks\n\n35:01.280 --> 35:02.800\n because there's a strong correlation\n\n35:02.800 --> 35:04.640\n between compressing well,\n\n35:04.640 --> 35:07.560\n and then the system's performing well at the task at hand.\n\n35:07.560 --> 35:09.840\n It's not perfect, but it's good enough\n\n35:09.840 --> 35:13.680\n for them as an intermediate aim.\n\n35:14.640 --> 35:16.040\n So you mean a measure,\n\n35:16.040 --> 35:18.400\n so this is kind of almost returning\n\n35:18.400 --> 35:19.800\n to the common goal of complexity.\n\n35:19.800 --> 35:22.520\n So you're saying good compression\n\n35:22.520 --> 35:24.960\n usually means good intelligence.\n\n35:24.960 --> 35:25.800\n Yes.\n\n35:27.040 --> 35:31.120\n So you mentioned you're one of the only people\n\n35:31.120 --> 35:36.120\n who dared boldly to try to formalize\n\n35:36.280 --> 35:38.720\n the idea of artificial general intelligence,\n\n35:38.720 --> 35:42.840\n to have a mathematical framework for intelligence,\n\n35:42.840 --> 35:44.120\n just like as we mentioned,\n\n35:45.000 --> 35:49.200\n termed AIXI, A, I, X, I.\n\n35:49.200 --> 35:51.760\n So let me ask the basic question.\n\n35:51.760 --> 35:53.400\n What is AIXI?\n\n35:54.760 --> 35:57.960\n Okay, so let me first say what it stands for because...\n\n35:57.960 --> 35:58.880\n What it stands for, actually,\n\n35:58.880 --> 36:00.360\n that's probably the more basic question.\n\n36:00.360 --> 36:01.640\n What it...\n\n36:01.640 --> 36:04.400\n The first question is usually how it's pronounced,\n\n36:04.400 --> 36:07.240\n but finally I put it on the website how it's pronounced\n\n36:07.240 --> 36:08.400\n and you figured it out.\n\n36:10.520 --> 36:13.280\n The name comes from AI, artificial intelligence,\n\n36:13.280 --> 36:16.400\n and the X, I, is the Greek letter Xi,\n\n36:16.400 --> 36:19.680\n which are used for Solomonov's distribution\n\n36:19.680 --> 36:22.000\n for quite stupid reasons,\n\n36:22.000 --> 36:24.800\n which I'm not willing to repeat here in front of camera.\n\n36:24.800 --> 36:25.640\n Sure.\n\n36:27.040 --> 36:29.840\n So it just happened to be more or less arbitrary.\n\n36:29.840 --> 36:31.600\n I chose the Xi.\n\n36:31.600 --> 36:34.680\n But it also has nice other interpretations.\n\n36:34.680 --> 36:38.360\n So there are actions and perceptions in this model.\n\n36:38.360 --> 36:42.000\n An agent has actions and perceptions over time.\n\n36:42.000 --> 36:44.680\n So this is A index I, X index I.\n\n36:44.680 --> 36:46.120\n So there's the action at time I\n\n36:46.120 --> 36:49.040\n and then followed by perception at time I.\n\n36:49.040 --> 36:50.440\n Yeah, we'll go with that.\n\n36:50.440 --> 36:52.320\n I'll edit out the first part.\n\n36:52.320 --> 36:53.320\n I'm just kidding.\n\n36:53.320 --> 36:55.120\n I have some more interpretations.\n\n36:55.120 --> 36:59.280\n So at some point, maybe five years ago or 10 years ago,\n\n36:59.280 --> 37:04.280\n I discovered in Barcelona, it was on a big church\n\n37:04.720 --> 37:08.480\n that was in stone engraved, some text,\n\n37:08.480 --> 37:11.480\n and the word Aixia appeared there a couple of times.\n\n37:11.480 --> 37:16.480\n I was very surprised and happy about that.\n\n37:16.960 --> 37:17.800\n And I looked it up.\n\n37:17.800 --> 37:19.440\n So it is a Catalan language\n\n37:19.440 --> 37:22.280\n and it means with some interpretation of that's it,\n\n37:22.280 --> 37:23.320\n that's the right thing to do.\n\n37:23.320 --> 37:24.800\n Yeah, Huayrica.\n\n37:24.800 --> 37:27.920\n Oh, so it's almost like destined somehow.\n\n37:27.920 --> 37:32.080\n It came to you in a dream.\n\n37:32.080 --> 37:34.280\n And similar, there's a Chinese word, Aixi,\n\n37:34.280 --> 37:37.480\n also written like Aixi, if you transcribe that to Pinyin.\n\n37:37.480 --> 37:41.120\n And the final one is that it's AI crossed with induction\n\n37:41.120 --> 37:44.680\n because that is, and that's going more to the content now.\n\n37:44.680 --> 37:47.400\n So good old fashioned AI is more about planning\n\n37:47.400 --> 37:48.760\n and known deterministic world\n\n37:48.760 --> 37:51.800\n and induction is more about often IID data\n\n37:51.800 --> 37:53.000\n and inferring models.\n\n37:53.000 --> 37:54.880\n And essentially what this Aixi model does\n\n37:54.880 --> 37:56.160\n is combining these two.\n\n37:56.160 --> 37:59.480\n And I actually also recently, I think heard that\n\n37:59.480 --> 38:02.280\n in Japanese AI means love.\n\n38:02.280 --> 38:06.720\n So if you can combine XI somehow with that,\n\n38:06.720 --> 38:10.320\n I think we can, there might be some interesting ideas there.\n\n38:10.320 --> 38:12.640\n So Aixi, let's then take the next step.\n\n38:12.640 --> 38:16.560\n Can you maybe talk at the big level\n\n38:16.560 --> 38:19.480\n of what is this mathematical framework?\n\n38:19.480 --> 38:22.560\n Yeah, so it consists essentially of two parts.\n\n38:22.560 --> 38:26.520\n One is the learning and induction and prediction part.\n\n38:26.520 --> 38:28.680\n And the other one is the planning part.\n\n38:28.680 --> 38:31.200\n So let's come first to the learning,\n\n38:31.200 --> 38:32.840\n induction, prediction part,\n\n38:32.840 --> 38:35.640\n which essentially I explained already before.\n\n38:35.640 --> 38:40.640\n So what we need for any agent to act well\n\n38:40.680 --> 38:43.480\n is that it can somehow predict what happens.\n\n38:43.480 --> 38:46.040\n I mean, if you have no idea what your actions do,\n\n38:47.080 --> 38:48.920\n how can you decide which actions are good or not?\n\n38:48.920 --> 38:52.840\n So you need to have some model of what your actions effect.\n\n38:52.840 --> 38:56.160\n So what you do is you have some experience,\n\n38:56.160 --> 38:59.360\n you build models like scientists of your experience,\n\n38:59.360 --> 39:01.400\n then you hope these models are roughly correct,\n\n39:01.400 --> 39:03.480\n and then you use these models for prediction.\n\n39:03.480 --> 39:05.200\n And the model is, sorry to interrupt,\n\n39:05.200 --> 39:08.360\n and the model is based on your perception of the world,\n\n39:08.360 --> 39:10.480\n how your actions will affect that world.\n\n39:10.480 --> 39:12.080\n That's not...\n\n39:12.080 --> 39:12.920\n So how do you think about a model?\n\n39:12.920 --> 39:14.280\n That's not the important part,\n\n39:14.280 --> 39:16.000\n but it is technically important,\n\n39:16.000 --> 39:18.240\n but at this stage we can just think about predicting,\n\n39:18.240 --> 39:20.760\n let's say, stock market data, weather data,\n\n39:20.760 --> 39:23.240\n or IQ sequences, one, two, three, four, five,\n\n39:23.240 --> 39:24.520\n what comes next, yeah?\n\n39:24.520 --> 39:28.680\n So of course our actions affect what we're doing,\n\n39:28.680 --> 39:30.240\n but I'll come back to that in a second.\n\n39:30.240 --> 39:32.160\n So, and I'll keep just interrupting.\n\n39:32.160 --> 39:37.000\n So just to draw a line between prediction and planning,\n\n39:37.000 --> 39:40.880\n what do you mean by prediction in this way?\n\n39:40.880 --> 39:43.640\n It's trying to predict the environment\n\n39:43.640 --> 39:47.280\n without your long term action in the environment?\n\n39:47.280 --> 39:48.240\n What is prediction?\n\n39:49.480 --> 39:51.160\n Okay, if you want to put the actions in now,\n\n39:51.160 --> 39:53.680\n okay, then let's put it in now, yeah?\n\n39:53.680 --> 39:54.720\n So...\n\n39:54.720 --> 39:55.560\n We don't have to put them now.\n\n39:55.560 --> 39:56.400\n Yeah, yeah.\n\n39:56.400 --> 39:58.360\n Scratch it, scratch it, dumb question, okay.\n\n39:58.360 --> 40:01.280\n So the simplest form of prediction is\n\n40:01.280 --> 40:04.840\n that you just have data which you passively observe,\n\n40:04.840 --> 40:06.160\n and you want to predict what happens\n\n40:06.160 --> 40:08.960\n without interfering, as I said,\n\n40:08.960 --> 40:12.120\n weather forecasting, stock market, IQ sequences,\n\n40:12.120 --> 40:16.240\n or just anything, okay?\n\n40:16.240 --> 40:18.920\n And Solomonov's theory of induction based on compression,\n\n40:18.920 --> 40:20.400\n so you look for the shortest program\n\n40:20.400 --> 40:22.240\n which describes your data sequence,\n\n40:22.240 --> 40:24.440\n and then you take this program, run it,\n\n40:24.440 --> 40:26.920\n it reproduces your data sequence by definition,\n\n40:26.920 --> 40:29.000\n and then you let it continue running,\n\n40:29.000 --> 40:30.880\n and then it will produce some predictions,\n\n40:30.880 --> 40:35.880\n and you can rigorously prove that for any prediction task,\n\n40:37.160 --> 40:40.040\n this is essentially the best possible predictor.\n\n40:40.040 --> 40:42.040\n Of course, if there's a prediction task,\n\n40:43.680 --> 40:45.080\n or a task which is unpredictable,\n\n40:45.080 --> 40:46.720\n like, you know, you have fair coin flips.\n\n40:46.720 --> 40:48.160\n Yeah, I cannot predict the next fair coin flip.\n\n40:48.160 --> 40:49.160\n What Solomonov does is says,\n\n40:49.160 --> 40:51.640\n okay, next head is probably 50%.\n\n40:51.640 --> 40:52.600\n It's the best you can do.\n\n40:52.600 --> 40:54.080\n So if something is unpredictable,\n\n40:54.080 --> 40:56.600\n Solomonov will also not magically predict it.\n\n40:56.600 --> 40:59.640\n But if there is some pattern and predictability,\n\n40:59.640 --> 41:03.760\n then Solomonov induction will figure that out eventually,\n\n41:03.760 --> 41:06.040\n and not just eventually, but rather quickly,\n\n41:06.040 --> 41:08.400\n and you can have proof convergence rates,\n\n41:10.640 --> 41:11.720\n whatever your data is.\n\n41:11.720 --> 41:14.760\n So there's pure magic in a sense.\n\n41:14.760 --> 41:15.600\n What's the catch?\n\n41:15.600 --> 41:17.040\n Well, the catch is that it's not computable,\n\n41:17.040 --> 41:18.200\n and we come back to that later.\n\n41:18.200 --> 41:19.720\n You cannot just implement it\n\n41:19.720 --> 41:21.160\n even with Google resources here,\n\n41:21.160 --> 41:24.000\n and run it and predict the stock market and become rich.\n\n41:24.000 --> 41:28.160\n I mean, Ray Solomonov already tried it at the time.\n\n41:28.160 --> 41:31.680\n But so the basic task is you're in the environment,\n\n41:31.680 --> 41:33.200\n and you're interacting with the environment\n\n41:33.200 --> 41:35.400\n to try to learn to model that environment,\n\n41:35.400 --> 41:38.760\n and the model is in the space of all these programs,\n\n41:38.760 --> 41:41.360\n and your goal is to get a bunch of programs that are simple.\n\n41:41.360 --> 41:44.040\n Yeah, so let's go to the actions now.\n\n41:44.040 --> 41:45.080\n But actually, good that you asked.\n\n41:45.080 --> 41:46.400\n Usually I skip this part,\n\n41:46.400 --> 41:48.760\n although there is also a minor contribution which I did,\n\n41:48.760 --> 41:49.720\n so the action part,\n\n41:49.720 --> 41:51.800\n but I usually sort of just jump to the decision part.\n\n41:51.800 --> 41:53.400\n So let me explain the action part now.\n\n41:53.400 --> 41:54.320\n Thanks for asking.\n\n41:55.440 --> 41:57.760\n So you have to modify it a little bit\n\n41:58.760 --> 42:01.080\n by now not just predicting a sequence\n\n42:01.080 --> 42:03.240\n which just comes to you,\n\n42:03.240 --> 42:06.760\n but you have an observation, then you act somehow,\n\n42:06.760 --> 42:09.120\n and then you want to predict the next observation\n\n42:09.120 --> 42:11.920\n based on the past observation and your action.\n\n42:11.920 --> 42:14.680\n Then you take the next action.\n\n42:14.680 --> 42:17.240\n You don't care about predicting it because you're doing it.\n\n42:17.240 --> 42:19.040\n Then you get the next observation,\n\n42:19.040 --> 42:20.680\n and you want, well, before you get it,\n\n42:20.680 --> 42:21.880\n you want to predict it, again,\n\n42:21.880 --> 42:24.880\n based on your past action and observation sequence.\n\n42:24.880 --> 42:28.720\n You just condition extra on your actions.\n\n42:28.720 --> 42:30.520\n There's an interesting alternative\n\n42:30.520 --> 42:33.400\n that you also try to predict your own actions.\n\n42:35.600 --> 42:36.600\n If you want.\n\n42:36.600 --> 42:37.960\n In the past or the future?\n\n42:37.960 --> 42:39.720\n In your future actions.\n\n42:39.720 --> 42:40.560\n That's interesting.\n\n42:40.560 --> 42:43.480\n Yeah. Wait, let me wrap.\n\n42:43.480 --> 42:45.800\n I think my brain just broke.\n\n42:45.800 --> 42:47.440\n We should maybe discuss that later\n\n42:47.440 --> 42:48.760\n after I've explained the IXE model.\n\n42:48.760 --> 42:50.160\n That's an interesting variation.\n\n42:50.160 --> 42:52.080\n But that is a really interesting variation,\n\n42:52.080 --> 42:53.080\n and a quick comment.\n\n42:53.080 --> 42:55.440\n I don't know if you want to insert that in here,\n\n42:55.440 --> 42:59.200\n but you're looking at the, in terms of observations,\n\n42:59.200 --> 43:01.640\n you're looking at the entire, the big history,\n\n43:01.640 --> 43:03.320\n the long history of the observations.\n\n43:03.320 --> 43:04.440\n Exactly. That's very important.\n\n43:04.440 --> 43:07.520\n The whole history from birth sort of of the agent,\n\n43:07.520 --> 43:09.080\n and we can come back to that.\n\n43:09.080 --> 43:10.840\n And also why this is important.\n\n43:10.840 --> 43:13.560\n Often, you know, in RL, you have MDPs,\n\n43:13.560 --> 43:15.840\n micro decision processes, which are much more limiting.\n\n43:15.840 --> 43:19.880\n Okay. So now we can predict conditioned on actions.\n\n43:19.880 --> 43:21.600\n So even if you influence environment,\n\n43:21.600 --> 43:24.120\n but prediction is not all we want to do, right?\n\n43:24.120 --> 43:26.960\n We also want to act really in the world.\n\n43:26.960 --> 43:29.120\n And the question is how to choose the actions.\n\n43:29.120 --> 43:32.360\n And we don't want to greedily choose the actions,\n\n43:33.320 --> 43:36.480\n you know, just, you know, what is best in the next time step.\n\n43:36.480 --> 43:38.360\n And we first, I should say, you know, what is, you know,\n\n43:38.360 --> 43:39.960\n how do we measure performance?\n\n43:39.960 --> 43:43.360\n So we measure performance by giving the agent reward.\n\n43:43.360 --> 43:45.640\n That's the so called reinforcement learning framework.\n\n43:45.640 --> 43:48.560\n So every time step, you can give it a positive reward\n\n43:48.560 --> 43:50.320\n or negative reward, or maybe no reward.\n\n43:50.320 --> 43:51.880\n It could be a very scarce, right?\n\n43:51.880 --> 43:54.160\n Like if you play chess, just at the end of the game,\n\n43:54.160 --> 43:56.920\n you give plus one for winning or minus one for losing.\n\n43:56.920 --> 43:59.240\n So in the RxC framework, that's completely sufficient.\n\n43:59.240 --> 44:01.440\n So occasionally you give a reward signal\n\n44:01.440 --> 44:04.040\n and you ask the agent to maximize reward,\n\n44:04.040 --> 44:06.400\n but not greedily sort of, you know, the next one, next one,\n\n44:06.400 --> 44:10.040\n because that's very bad in the long run if you're greedy.\n\n44:10.040 --> 44:12.440\n So, but over the lifetime of the agent.\n\n44:12.440 --> 44:14.600\n So let's assume the agent lives for M time steps,\n\n44:14.600 --> 44:16.920\n or say dies in sort of a hundred years sharp.\n\n44:16.920 --> 44:19.720\n That's just, you know, the simplest model to explain.\n\n44:19.720 --> 44:22.120\n So it looks at the future reward sum\n\n44:22.120 --> 44:24.840\n and ask what is my action sequence,\n\n44:24.840 --> 44:26.920\n or actually more precisely my policy,\n\n44:26.920 --> 44:31.160\n which leads in expectation, because I don't know the world,\n\n44:32.160 --> 44:34.120\n to the maximum reward sum.\n\n44:34.120 --> 44:36.120\n Let me give you an analogy.\n\n44:36.120 --> 44:38.240\n In chess, for instance,\n\n44:38.240 --> 44:40.320\n we know how to play optimally in theory.\n\n44:40.320 --> 44:42.160\n It's just a mini max strategy.\n\n44:42.160 --> 44:44.400\n I play the move which seems best to me\n\n44:44.400 --> 44:46.840\n under the assumption that the opponent plays the move\n\n44:46.840 --> 44:48.600\n which is best for him.\n\n44:48.600 --> 44:52.240\n So best, so worst for me under the assumption that he,\n\n44:52.240 --> 44:54.040\n I play again, the best move.\n\n44:54.040 --> 44:55.960\n And then you have this expecting max three\n\n44:55.960 --> 44:58.880\n to the end of the game, and then you back propagate,\n\n44:58.880 --> 45:00.760\n and then you get the best possible move.\n\n45:00.760 --> 45:02.160\n So that is the optimal strategy,\n\n45:02.160 --> 45:06.200\n which von Neumann already figured out a long time ago,\n\n45:06.200 --> 45:09.000\n for playing adversarial games.\n\n45:09.000 --> 45:11.640\n Luckily, or maybe unluckily for the theory,\n\n45:11.640 --> 45:12.480\n it becomes harder.\n\n45:12.480 --> 45:14.960\n The world is not always adversarial.\n\n45:14.960 --> 45:17.240\n So it can be, if there are other humans,\n\n45:17.240 --> 45:20.120\n even cooperative, or nature is usually,\n\n45:20.120 --> 45:22.720\n I mean, the dead nature is stochastic, you know,\n\n45:22.720 --> 45:26.840\n things just happen randomly, or don't care about you.\n\n45:26.840 --> 45:29.440\n So what you have to take into account is the noise,\n\n45:29.440 --> 45:30.760\n and not necessarily adversarialty.\n\n45:30.760 --> 45:34.040\n So you replace the minimum on the opponent's side\n\n45:34.040 --> 45:36.040\n by an expectation,\n\n45:36.040 --> 45:40.080\n which is general enough to include also adversarial cases.\n\n45:40.080 --> 45:41.600\n So now instead of a mini max strategy,\n\n45:41.600 --> 45:43.840\n you have an expected max strategy.\n\n45:43.840 --> 45:44.680\n So far, so good.\n\n45:44.680 --> 45:45.520\n So that is well known.\n\n45:45.520 --> 45:48.040\n It's called sequential decision theory.\n\n45:48.040 --> 45:49.480\n But the question is,\n\n45:49.480 --> 45:52.480\n on which probability distribution do you base that?\n\n45:52.480 --> 45:55.400\n If I have the true probability distribution,\n\n45:55.400 --> 45:56.960\n like say I play backgammon, right?\n\n45:56.960 --> 45:59.360\n There's dice, and there's certain randomness involved.\n\n45:59.360 --> 46:00.960\n Yeah, I can calculate probabilities\n\n46:00.960 --> 46:02.640\n and feed it in the expected max,\n\n46:02.640 --> 46:04.160\n or the sequential decision tree,\n\n46:04.160 --> 46:07.160\n come up with the optimal decision if I have enough compute.\n\n46:07.160 --> 46:09.760\n But for the real world, we don't know that, you know,\n\n46:09.760 --> 46:13.960\n what is the probability the driver in front of me breaks?\n\n46:13.960 --> 46:14.920\n I don't know.\n\n46:14.920 --> 46:16.920\n So depends on all kinds of things,\n\n46:16.920 --> 46:19.640\n and especially new situations, I don't know.\n\n46:19.640 --> 46:22.520\n So this is this unknown thing about prediction,\n\n46:22.520 --> 46:24.240\n and there's where Solomonov comes in.\n\n46:24.240 --> 46:26.360\n So what you do is in sequential decision tree,\n\n46:26.360 --> 46:28.680\n you just replace the true distribution,\n\n46:28.680 --> 46:32.960\n which we don't know, by this universal distribution.\n\n46:32.960 --> 46:34.640\n I didn't explicitly talk about it,\n\n46:34.640 --> 46:36.800\n but this is used for universal prediction\n\n46:36.800 --> 46:40.280\n and plug it into the sequential decision tree mechanism.\n\n46:40.280 --> 46:42.680\n And then you get the best of both worlds.\n\n46:42.680 --> 46:44.680\n You have a long term planning agent,\n\n46:45.560 --> 46:48.080\n but it doesn't need to know anything about the world\n\n46:48.080 --> 46:51.640\n because the Solomonov induction part learns.\n\n46:51.640 --> 46:54.720\n Can you explicitly try to describe\n\n46:54.720 --> 46:56.080\n the universal distribution\n\n46:56.080 --> 46:59.680\n and how Solomonov induction plays a role here?\n\n46:59.680 --> 47:00.760\n I'm trying to understand.\n\n47:00.760 --> 47:03.840\n So what it does it, so in the simplest case,\n\n47:03.840 --> 47:06.600\n I said, take the shortest program, describing your data,\n\n47:06.600 --> 47:09.040\n run it, have a prediction which would be deterministic.\n\n47:09.040 --> 47:10.760\n Yes. Okay.\n\n47:10.760 --> 47:13.160\n But you should not just take the shortest program,\n\n47:13.160 --> 47:15.320\n but also consider the longer ones,\n\n47:15.320 --> 47:18.480\n but give it lower a priori probability.\n\n47:18.480 --> 47:22.400\n So in the Bayesian framework, you say a priori,\n\n47:22.400 --> 47:27.400\n any distribution, which is a model or a stochastic program,\n\n47:29.360 --> 47:30.760\n has a certain a priori probability,\n\n47:30.760 --> 47:33.320\n which is two to the minus, and why two to the minus length?\n\n47:33.320 --> 47:35.520\n You know, I could explain length of this program.\n\n47:35.520 --> 47:39.760\n So longer programs are punished a priori.\n\n47:39.760 --> 47:41.360\n And then you multiply it\n\n47:41.360 --> 47:43.840\n with the so called likelihood function,\n\n47:43.840 --> 47:46.720\n which is, as the name suggests,\n\n47:46.720 --> 47:51.000\n is how likely is this model given the data at hand.\n\n47:51.000 --> 47:53.240\n So if you have a very wrong model,\n\n47:53.240 --> 47:55.000\n it's very unlikely that this model is true.\n\n47:55.000 --> 47:56.760\n And so it is very small number.\n\n47:56.760 --> 48:00.320\n So even if the model is simple, it gets penalized by that.\n\n48:00.320 --> 48:02.480\n And what you do is then you take just the sum,\n\n48:02.480 --> 48:04.440\n or this is the average over it.\n\n48:04.440 --> 48:07.600\n And this gives you a probability distribution.\n\n48:07.600 --> 48:10.480\n So it's universal distribution or Solomonov distribution.\n\n48:10.480 --> 48:13.160\n So it's weighed by the simplicity of the program\n\n48:13.160 --> 48:14.120\n and the likelihood.\n\n48:14.120 --> 48:15.320\n Yes.\n\n48:15.320 --> 48:17.280\n It's kind of a nice idea.\n\n48:17.280 --> 48:18.120\n Yeah.\n\n48:18.120 --> 48:23.120\n So okay, and then you said there's you're playing N or M\n\n48:23.280 --> 48:25.960\n or forgot the letter steps into the future.\n\n48:25.960 --> 48:28.320\n So how difficult is that problem?\n\n48:28.320 --> 48:29.520\n What's involved there?\n\n48:29.520 --> 48:31.320\n Okay, so basic optimization problem.\n\n48:31.320 --> 48:32.160\n What are we talking about?\n\n48:32.160 --> 48:34.920\n Yeah, so you have a planning problem up to horizon M,\n\n48:34.920 --> 48:38.040\n and that's exponential time in the horizon M,\n\n48:38.040 --> 48:41.760\n which is, I mean, it's computable, but intractable.\n\n48:41.760 --> 48:43.520\n I mean, even for chess, it's already intractable\n\n48:43.520 --> 48:44.360\n to do that exactly.\n\n48:44.360 --> 48:45.440\n And you know, for goal.\n\n48:45.440 --> 48:48.680\n But it could be also discounted kind of framework where.\n\n48:48.680 --> 48:52.960\n Yeah, so having a hard horizon, you know, at 100 years,\n\n48:52.960 --> 48:55.800\n it's just for simplicity of discussing the model\n\n48:55.800 --> 48:57.720\n and also sometimes the math is simple.\n\n48:58.960 --> 49:00.000\n But there are lots of variations,\n\n49:00.000 --> 49:03.360\n actually quite interesting parameter.\n\n49:03.360 --> 49:07.240\n There's nothing really problematic about it,\n\n49:07.240 --> 49:08.240\n but it's very interesting.\n\n49:08.240 --> 49:09.280\n So for instance, you think, no,\n\n49:09.280 --> 49:12.880\n let's let the parameter M tend to infinity, right?\n\n49:12.880 --> 49:15.840\n You want an agent which lives forever, right?\n\n49:15.840 --> 49:17.480\n If you do it normally, you have two problems.\n\n49:17.480 --> 49:19.160\n First, the mathematics breaks down\n\n49:19.160 --> 49:21.360\n because you have an infinite reward sum,\n\n49:21.360 --> 49:22.720\n which may give infinity,\n\n49:22.720 --> 49:25.560\n and getting reward 0.1 every time step is infinity,\n\n49:25.560 --> 49:27.600\n and giving reward one every time step is infinity,\n\n49:27.600 --> 49:28.600\n so equally good.\n\n49:29.480 --> 49:31.080\n Not really what we want.\n\n49:31.080 --> 49:35.760\n Other problem is that if you have an infinite life,\n\n49:35.760 --> 49:38.560\n you can be lazy for as long as you want for 10 years\n\n49:38.560 --> 49:41.400\n and then catch up with the same expected reward.\n\n49:41.400 --> 49:46.400\n And think about yourself or maybe some friends or so.\n\n49:47.240 --> 49:51.440\n If they knew they lived forever, why work hard now?\n\n49:51.440 --> 49:54.240\n Just enjoy your life and then catch up later.\n\n49:54.240 --> 49:56.600\n So that's another problem with infinite horizon.\n\n49:56.600 --> 49:59.760\n And you mentioned, yes, we can go to discounting,\n\n49:59.760 --> 50:01.200\n but then the standard discounting\n\n50:01.200 --> 50:03.080\n is so called geometric discounting.\n\n50:03.080 --> 50:05.400\n So a dollar today is about worth\n\n50:05.400 --> 50:08.320\n as much as $1.05 tomorrow.\n\n50:08.320 --> 50:10.320\n So if you do the so called geometric discounting,\n\n50:10.320 --> 50:12.960\n you have introduced an effective horizon.\n\n50:12.960 --> 50:15.960\n So the agent is now motivated to look ahead\n\n50:15.960 --> 50:18.360\n a certain amount of time effectively.\n\n50:18.360 --> 50:20.600\n It's like a moving horizon.\n\n50:20.600 --> 50:23.840\n And for any fixed effective horizon,\n\n50:23.840 --> 50:26.520\n there is a problem to solve,\n\n50:26.520 --> 50:28.080\n which requires larger horizon.\n\n50:28.080 --> 50:30.440\n So if I look ahead five time steps,\n\n50:30.440 --> 50:32.440\n I'm a terrible chess player, right?\n\n50:32.440 --> 50:34.560\n I'll need to look ahead longer.\n\n50:34.560 --> 50:36.720\n If I play go, I probably have to look ahead even longer.\n\n50:36.720 --> 50:40.280\n So for every problem, for every horizon,\n\n50:40.280 --> 50:43.800\n there is a problem which this horizon cannot solve.\n\n50:43.800 --> 50:46.960\n But I introduced the so called near harmonic horizon,\n\n50:46.960 --> 50:48.360\n which goes down with one over T\n\n50:48.360 --> 50:49.960\n rather than exponential in T,\n\n50:49.960 --> 50:51.600\n which produces an agent,\n\n50:51.600 --> 50:53.880\n which effectively looks into the future\n\n50:53.880 --> 50:55.200\n proportional to each age.\n\n50:55.200 --> 50:57.360\n So if it's five years old, it plans for five years.\n\n50:57.360 --> 51:00.440\n If it's 100 years old, it then plans for 100 years.\n\n51:00.440 --> 51:02.480\n And it's a little bit similar to humans too, right?\n\n51:02.480 --> 51:04.320\n I mean, children don't plan ahead very long,\n\n51:04.320 --> 51:07.080\n but then we get adult, we play ahead more longer.\n\n51:07.080 --> 51:08.560\n Maybe when we get very old,\n\n51:08.560 --> 51:10.360\n I mean, we know that we don't live forever.\n\n51:10.360 --> 51:12.840\n Maybe then our horizon shrinks again.\n\n51:12.840 --> 51:16.040\n So that's really interesting.\n\n51:16.040 --> 51:18.120\n So adjusting the horizon,\n\n51:18.120 --> 51:20.680\n is there some mathematical benefit of that?\n\n51:20.680 --> 51:21.880\n Or is it just a nice,\n\n51:22.960 --> 51:25.560\n I mean, intuitively, empirically,\n\n51:25.560 --> 51:26.560\n it would probably be a good idea\n\n51:26.560 --> 51:27.960\n to sort of push the horizon back,\n\n51:27.960 --> 51:32.960\n extend the horizon as you experience more of the world.\n\n51:33.480 --> 51:35.840\n But is there some mathematical conclusions here\n\n51:35.840 --> 51:37.240\n that are beneficial?\n\n51:37.240 --> 51:38.920\n With solomonic reductions or the prediction part,\n\n51:38.920 --> 51:41.400\n we have extremely strong finite time,\n\n51:42.320 --> 51:44.760\n but not finite data results.\n\n51:44.760 --> 51:46.000\n So you have so and so much data,\n\n51:46.000 --> 51:47.160\n then you lose so and so much.\n\n51:47.160 --> 51:49.400\n So it's a, the theory is really great.\n\n51:49.400 --> 51:51.920\n With the ICSE model, with the planning part,\n\n51:51.920 --> 51:56.800\n many results are only asymptotic, which, well, this is...\n\n51:56.800 --> 51:57.640\n What does asymptotic mean?\n\n51:57.640 --> 51:59.920\n Asymptotic means you can prove, for instance,\n\n51:59.920 --> 52:02.360\n that in the long run, if the agent, you know,\n\n52:02.360 --> 52:04.160\n acts long enough, then, you know,\n\n52:04.160 --> 52:06.400\n it performs optimal or some nice thing happens.\n\n52:06.400 --> 52:09.480\n So, but you don't know how fast it converges.\n\n52:09.480 --> 52:10.880\n So it may converge fast,\n\n52:10.880 --> 52:12.280\n but we're just not able to prove it\n\n52:12.280 --> 52:13.760\n because of a difficult problem.\n\n52:13.760 --> 52:17.320\n Or maybe there's a bug in the model\n\n52:17.320 --> 52:19.520\n so that it's really that slow.\n\n52:19.520 --> 52:21.800\n So that is what asymptotic means,\n\n52:21.800 --> 52:24.680\n sort of eventually, but we don't know how fast.\n\n52:24.680 --> 52:27.920\n And if I give the agent a fixed horizon M,\n\n52:28.920 --> 52:32.240\n then I cannot prove asymptotic results, right?\n\n52:32.240 --> 52:35.040\n So I mean, sort of if it dies in a hundred years,\n\n52:35.040 --> 52:37.840\n then in a hundred years it's over, I cannot say eventually.\n\n52:37.840 --> 52:40.600\n So this is the advantage of the discounting\n\n52:40.600 --> 52:42.760\n that I can prove asymptotic results.\n\n52:42.760 --> 52:46.960\n So just to clarify, so I, okay, I made,\n\n52:46.960 --> 52:51.720\n I've built up a model, we're now in the moment of,\n\n52:51.720 --> 52:55.360\n I have this way of looking several steps ahead.\n\n52:55.360 --> 52:57.840\n How do I pick what action I will take?\n\n52:58.880 --> 53:00.720\n It's like with the playing chess, right?\n\n53:00.720 --> 53:02.320\n You do this minimax.\n\n53:02.320 --> 53:05.240\n In this case here, do expectimax based on the solomonov\n\n53:05.240 --> 53:08.000\n distribution, you propagate back,\n\n53:09.000 --> 53:12.080\n and then while an action falls out,\n\n53:12.080 --> 53:15.480\n the action which maximizes the future expected reward\n\n53:15.480 --> 53:16.800\n on the solomonov distribution,\n\n53:16.800 --> 53:18.240\n and then you just take this action.\n\n53:18.240 --> 53:19.640\n And then repeat.\n\n53:19.640 --> 53:20.960\n And then you get a new observation,\n\n53:20.960 --> 53:22.640\n and you feed it in this action observation,\n\n53:22.640 --> 53:23.480\n then you repeat.\n\n53:23.480 --> 53:24.880\n And the reward, so on.\n\n53:24.880 --> 53:26.760\n Yeah, so you rewrote too, yeah.\n\n53:26.760 --> 53:29.080\n And then maybe you can even predict your own action.\n\n53:29.080 --> 53:29.960\n I love that idea.\n\n53:29.960 --> 53:33.160\n But okay, this big framework,\n\n53:33.160 --> 53:35.600\n what is it, I mean,\n\n53:36.560 --> 53:38.840\n it's kind of a beautiful mathematical framework\n\n53:38.840 --> 53:41.880\n to think about artificial general intelligence.\n\n53:41.880 --> 53:45.800\n What can you, what does it help you into it\n\n53:45.800 --> 53:49.080\n about how to build such systems?\n\n53:49.080 --> 53:51.720\n Or maybe from another perspective,\n\n53:51.720 --> 53:56.720\n what does it help us in understanding AGI?\n\n53:56.720 --> 54:00.440\n So when I started in the field,\n\n54:00.440 --> 54:01.800\n I was always interested in two things.\n\n54:01.800 --> 54:05.800\n One was AGI, the name didn't exist then,\n\n54:05.800 --> 54:09.200\n what's called general AI or strong AI,\n\n54:09.200 --> 54:10.800\n and the physics theory of everything.\n\n54:10.800 --> 54:13.120\n So I switched back and forth between computer science\n\n54:13.120 --> 54:14.680\n and physics quite often.\n\n54:14.680 --> 54:15.960\n You said the theory of everything.\n\n54:15.960 --> 54:17.360\n The theory of everything, yeah.\n\n54:17.360 --> 54:19.240\n Those are basically the two biggest problems\n\n54:19.240 --> 54:21.360\n before all of humanity.\n\n54:21.360 --> 54:26.360\n Yeah, I can explain if you wanted some later time,\n\n54:28.480 --> 54:29.960\n why I'm interested in these two questions.\n\n54:29.960 --> 54:32.080\n Can I ask you in a small tangent,\n\n54:32.080 --> 54:37.080\n if it was one to be solved,\n\n54:37.120 --> 54:38.600\n which one would you,\n\n54:38.600 --> 54:41.800\n if an apple fell on your head\n\n54:41.800 --> 54:43.280\n and there was a brilliant insight\n\n54:43.280 --> 54:46.360\n and you could arrive at the solution to one,\n\n54:46.360 --> 54:49.200\n would it be AGI or the theory of everything?\n\n54:49.200 --> 54:51.800\n Definitely AGI, because once the AGI problem is solved,\n\n54:51.800 --> 54:54.400\n I can ask the AGI to solve the other problem for me.\n\n54:56.520 --> 54:57.720\n Yeah, brilliant input.\n\n54:57.720 --> 55:01.200\n Okay, so as you were saying about it.\n\n55:01.200 --> 55:04.960\n Okay, so, and the reason why I didn't settle,\n\n55:04.960 --> 55:07.400\n I mean, this thought about,\n\n55:07.400 --> 55:09.960\n once you have solved AGI, it solves all kinds of other,\n\n55:09.960 --> 55:11.240\n not just the theory of every problem,\n\n55:11.240 --> 55:14.160\n but all kinds of more useful problems to humanity\n\n55:14.160 --> 55:16.280\n is very appealing to many people.\n\n55:16.280 --> 55:18.240\n And I had this thought also,\n\n55:18.240 --> 55:23.240\n but I was quite disappointed with the state of the art\n\n55:23.960 --> 55:25.440\n of the field of AI.\n\n55:25.440 --> 55:28.160\n There was some theory about logical reasoning,\n\n55:28.160 --> 55:30.600\n but I was never convinced that this will fly.\n\n55:30.600 --> 55:33.320\n And then there was this more heuristic approaches\n\n55:33.320 --> 55:37.480\n with neural networks and I didn't like these heuristics.\n\n55:37.480 --> 55:40.320\n So, and also I didn't have any good idea myself.\n\n55:42.120 --> 55:44.240\n So that's the reason why I toggled back and forth\n\n55:44.240 --> 55:46.360\n quite some while and even worked four and a half years\n\n55:46.360 --> 55:48.240\n in a company developing software,\n\n55:48.240 --> 55:49.680\n something completely unrelated.\n\n55:49.680 --> 55:52.800\n But then I had this idea about the ICSE model.\n\n55:52.800 --> 55:57.760\n And so what it gives you, it gives you a gold standard.\n\n55:57.760 --> 56:02.360\n So I have proven that this is the most intelligent agents\n\n56:02.360 --> 56:06.840\n which anybody could build in quotation mark,\n\n56:06.840 --> 56:08.200\n because it's just mathematical\n\n56:08.200 --> 56:11.160\n and you need infinite compute.\n\n56:11.160 --> 56:14.920\n But this is the limit and this is completely specified.\n\n56:14.920 --> 56:19.280\n It's not just a framework and every year,\n\n56:19.280 --> 56:21.200\n tens of frameworks are developed,\n\n56:21.200 --> 56:23.920\n which are just skeletons and then pieces are missing.\n\n56:23.920 --> 56:25.360\n And usually these missing pieces,\n\n56:25.360 --> 56:27.360\n turn out to be really, really difficult.\n\n56:27.360 --> 56:31.080\n And so this is completely and uniquely defined\n\n56:31.080 --> 56:33.480\n and we can analyze that mathematically.\n\n56:33.480 --> 56:37.320\n And we've also developed some approximations.\n\n56:37.320 --> 56:40.280\n I can talk about that a little bit later.\n\n56:40.280 --> 56:41.800\n That would be sort of the top down approach,\n\n56:41.800 --> 56:44.240\n like say for Neumann's minimax theory,\n\n56:44.240 --> 56:47.240\n that's the theoretical optimal play of games.\n\n56:47.240 --> 56:48.800\n And now we need to approximate it,\n\n56:48.800 --> 56:51.040\n put heuristics in, prune the tree, blah, blah, blah,\n\n56:51.040 --> 56:51.880\n and so on.\n\n56:51.880 --> 56:53.200\n So we can do that also with the ICSE model,\n\n56:53.200 --> 56:54.280\n but for general AI.\n\n56:55.440 --> 56:57.640\n It can also inspire those,\n\n56:57.640 --> 57:00.840\n and most researchers go bottom up, right?\n\n57:00.840 --> 57:01.680\n They have the systems,\n\n57:01.680 --> 57:04.160\n they try to make it more general, more intelligent.\n\n57:04.160 --> 57:07.040\n It can inspire in which direction to go.\n\n57:08.120 --> 57:09.120\n What do you mean by that?\n\n57:09.120 --> 57:11.200\n So if you have some choice to make, right?\n\n57:11.200 --> 57:13.120\n So how should I evaluate my system\n\n57:13.120 --> 57:15.400\n if I can't do cross validation?\n\n57:15.400 --> 57:18.040\n How should I do my learning\n\n57:18.040 --> 57:21.480\n if my standard regularization doesn't work well?\n\n57:21.480 --> 57:22.520\n So the answer is always this,\n\n57:22.520 --> 57:25.000\n we have a system which does everything, that's ICSE.\n\n57:25.000 --> 57:27.760\n It's just completely in the ivory tower,\n\n57:27.760 --> 57:30.600\n completely useless from a practical point of view.\n\n57:30.600 --> 57:31.920\n But you can look at it and see,\n\n57:31.920 --> 57:34.920\n ah, yeah, maybe I can take some aspects.\n\n57:34.920 --> 57:36.520\n And instead of Kolmogorov complexity,\n\n57:36.520 --> 57:38.160\n that just takes some compressors,\n\n57:38.160 --> 57:39.960\n which has been developed so far.\n\n57:39.960 --> 57:42.120\n And for the planning, well, we have UCT,\n\n57:42.120 --> 57:44.360\n which has also been used in Go.\n\n57:45.240 --> 57:50.040\n And at least it's inspired me a lot\n\n57:50.040 --> 57:54.160\n to have this formal definition.\n\n57:54.160 --> 57:55.800\n And if you look at other fields,\n\n57:55.800 --> 57:57.720\n like I always come back to physics\n\n57:57.720 --> 57:58.960\n because I have a physics background,\n\n57:58.960 --> 58:00.680\n think about the phenomenon of energy.\n\n58:00.680 --> 58:03.160\n That was long time a mysterious concept.\n\n58:03.160 --> 58:05.880\n And at some point it was completely formalized.\n\n58:05.880 --> 58:08.160\n And that really helped a lot.\n\n58:08.160 --> 58:10.720\n And you can point out a lot of these things\n\n58:10.720 --> 58:12.960\n which were first mysterious and vague,\n\n58:12.960 --> 58:15.160\n and then they have been rigorously formalized.\n\n58:15.160 --> 58:18.240\n Speed and acceleration has been confused, right?\n\n58:18.240 --> 58:19.680\n Until it was formally defined,\n\n58:19.680 --> 58:21.040\n yeah, there was a time like this.\n\n58:21.040 --> 58:25.080\n And people often who don't have any background,\n\n58:25.080 --> 58:26.200\n still confuse it.\n\n58:28.280 --> 58:31.920\n And this ICSE model or the intelligence definitions,\n\n58:31.920 --> 58:33.160\n which is sort of the dual to it,\n\n58:33.160 --> 58:34.640\n we come back to that later,\n\n58:34.640 --> 58:37.160\n formalizes the notion of intelligence\n\n58:37.160 --> 58:38.880\n uniquely and rigorously.\n\n58:38.880 --> 58:41.640\n So in a sense, it serves as kind of the light\n\n58:41.640 --> 58:43.000\n at the end of the tunnel.\n\n58:43.000 --> 58:46.800\n So for, I mean, there's a million questions\n\n58:46.800 --> 58:47.720\n I could ask her.\n\n58:47.720 --> 58:50.280\n So maybe kind of, okay,\n\n58:50.280 --> 58:52.080\n let's feel around in the dark a little bit.\n\n58:52.080 --> 58:54.720\n So there's been here a deep mind,\n\n58:54.720 --> 58:56.960\n but in general, been a lot of breakthrough ideas,\n\n58:56.960 --> 58:59.480\n just like we've been saying around reinforcement learning.\n\n58:59.480 --> 59:02.080\n So how do you see the progress\n\n59:02.080 --> 59:04.440\n in reinforcement learning is different?\n\n59:04.440 --> 59:08.080\n Like which subset of ICSE does it occupy?\n\n59:08.080 --> 59:10.600\n The current, like you said,\n\n59:10.600 --> 59:14.520\n maybe the Markov assumption is made quite often\n\n59:14.520 --> 59:16.280\n in reinforcement learning.\n\n59:16.280 --> 59:20.240\n There's other assumptions made\n\n59:20.240 --> 59:21.560\n in order to make the system work.\n\n59:21.560 --> 59:24.200\n What do you see as the difference connection\n\n59:24.200 --> 59:26.800\n between reinforcement learning and ICSE?\n\n59:26.800 --> 59:29.000\n And so the major difference is that\n\n59:30.560 --> 59:33.280\n essentially all other approaches,\n\n59:33.280 --> 59:35.600\n they make stronger assumptions.\n\n59:35.600 --> 59:38.320\n So in reinforcement learning, the Markov assumption\n\n59:38.320 --> 59:41.520\n is that the next state or next observation\n\n59:41.520 --> 59:43.360\n only depends on the previous observation\n\n59:43.360 --> 59:45.240\n and not the whole history,\n\n59:45.240 --> 59:47.560\n which makes, of course, the mathematics much easier\n\n59:47.560 --> 59:49.800\n rather than dealing with histories.\n\n59:49.800 --> 59:51.600\n Of course, they profit from it also,\n\n59:51.600 --> 59:53.080\n because then you have algorithms\n\n59:53.080 --> 59:54.320\n that run on current computers\n\n59:54.320 --> 59:56.640\n and do something practically useful.\n\n59:56.640 --> 59:59.680\n But for general AI, all the assumptions\n\n59:59.680 --> 1:00:01.720\n which are made by other approaches,\n\n1:00:01.720 --> 1:00:04.040\n we know already now they are limiting.\n\n1:00:04.040 --> 1:00:07.760\n So, for instance, usually you need\n\n1:00:07.760 --> 1:00:09.840\n a goddessity assumption in the MDP frameworks\n\n1:00:09.840 --> 1:00:10.680\n in order to learn.\n\n1:00:10.680 --> 1:00:13.800\n A goddessity essentially means that you can recover\n\n1:00:13.800 --> 1:00:15.800\n from your mistakes and that there are no traps\n\n1:00:15.800 --> 1:00:17.400\n in the environment.\n\n1:00:17.400 --> 1:00:19.040\n And if you make this assumption,\n\n1:00:19.040 --> 1:00:22.040\n then essentially you can go back to a previous state,\n\n1:00:22.040 --> 1:00:24.320\n go there a couple of times and then learn\n\n1:00:24.320 --> 1:00:29.040\n what statistics and what the state is like,\n\n1:00:29.040 --> 1:00:32.520\n and then in the long run perform well in this state.\n\n1:00:32.520 --> 1:00:35.200\n But there are no fundamental problems.\n\n1:00:35.200 --> 1:00:38.480\n But in real life, we know there can be one single action.\n\n1:00:38.480 --> 1:00:43.480\n One second of being inattentive while driving a car fast\n\n1:00:43.920 --> 1:00:45.240\n can ruin the rest of my life.\n\n1:00:45.240 --> 1:00:47.800\n I can become quadriplegic or whatever.\n\n1:00:47.800 --> 1:00:49.680\n So, and there's no recovery anymore.\n\n1:00:49.680 --> 1:00:52.160\n So, the real world is not ergodic, I always say.\n\n1:00:52.160 --> 1:00:53.920\n There are traps and there are situations\n\n1:00:53.920 --> 1:00:55.760\n where you are not recover from.\n\n1:00:55.760 --> 1:01:00.760\n And very little theory has been developed for this case.\n\n1:01:00.760 --> 1:01:05.760\n What about, what do you see in the context of IECSIA\n\n1:01:05.760 --> 1:01:07.960\n as the role of exploration?\n\n1:01:07.960 --> 1:01:12.960\n Sort of, you mentioned in the real world\n\n1:01:13.440 --> 1:01:16.120\n you can get into trouble when we make the wrong decisions\n\n1:01:16.120 --> 1:01:17.480\n and really pay for it.\n\n1:01:17.480 --> 1:01:20.480\n But exploration seems to be fundamentally important\n\n1:01:20.480 --> 1:01:23.760\n for learning about this world, for gaining new knowledge.\n\n1:01:23.760 --> 1:01:27.360\n So, is exploration baked in?\n\n1:01:27.360 --> 1:01:29.680\n Another way to ask it, what are the potential\n\n1:01:29.680 --> 1:01:34.360\n to ask it, what are the parameters of IECSIA\n\n1:01:34.360 --> 1:01:36.200\n that can be controlled?\n\n1:01:36.200 --> 1:01:38.880\n Yeah, I say the good thing is that there are no parameters\n\n1:01:38.880 --> 1:01:40.200\n to control.\n\n1:01:40.200 --> 1:01:43.120\n Some other people track knobs to control.\n\n1:01:43.120 --> 1:01:44.120\n And you can do that.\n\n1:01:44.120 --> 1:01:46.880\n I mean, you can modify IECSIA so that you have some knobs\n\n1:01:46.880 --> 1:01:48.800\n to play with if you want to.\n\n1:01:48.800 --> 1:01:53.640\n But the exploration is directly baked in.\n\n1:01:53.640 --> 1:01:56.960\n And that comes from the Bayesian learning\n\n1:01:56.960 --> 1:01:58.680\n and the longterm planning.\n\n1:01:58.680 --> 1:02:03.680\n So these together already imply exploration.\n\n1:02:04.200 --> 1:02:08.280\n You can nicely and explicitly prove that\n\n1:02:08.280 --> 1:02:13.280\n for simple problems like so called bandit problems,\n\n1:02:13.560 --> 1:02:18.000\n where you say, to give a real world example,\n\n1:02:18.000 --> 1:02:20.200\n say you have two medical treatments, A and B,\n\n1:02:20.200 --> 1:02:21.560\n you don't know the effectiveness,\n\n1:02:21.560 --> 1:02:23.360\n you try A a little bit, B a little bit,\n\n1:02:23.360 --> 1:02:25.760\n but you don't want to harm too many patients.\n\n1:02:25.760 --> 1:02:29.800\n So you have to sort of trade off exploring.\n\n1:02:29.800 --> 1:02:31.720\n And at some point you want to explore\n\n1:02:31.720 --> 1:02:34.080\n and you can do the mathematics\n\n1:02:34.080 --> 1:02:36.080\n and figure out the optimal strategy.\n\n1:02:38.040 --> 1:02:39.120\n They talk about Bayesian agents,\n\n1:02:39.120 --> 1:02:41.120\n they're also non Bayesian agents,\n\n1:02:41.120 --> 1:02:44.240\n but it shows that this Bayesian framework\n\n1:02:44.240 --> 1:02:47.400\n by taking a prior or possible worlds,\n\n1:02:47.400 --> 1:02:48.440\n doing the Bayesian mixture,\n\n1:02:48.440 --> 1:02:50.640\n then the Bayes optimal decision with longterm planning\n\n1:02:50.640 --> 1:02:52.320\n that is important,\n\n1:02:52.320 --> 1:02:55.880\n automatically implies exploration,\n\n1:02:55.880 --> 1:02:57.600\n also to the proper extent,\n\n1:02:57.600 --> 1:02:59.680\n not too much exploration and not too little.\n\n1:02:59.680 --> 1:03:01.520\n It is very simple settings.\n\n1:03:01.520 --> 1:03:04.400\n In the IXE model, I was also able to prove\n\n1:03:04.400 --> 1:03:06.160\n that it is a self optimizing theorem\n\n1:03:06.160 --> 1:03:07.720\n or asymptotic optimality theorems,\n\n1:03:07.720 --> 1:03:10.480\n although they're only asymptotic, not finite time bounds.\n\n1:03:10.480 --> 1:03:13.120\n So it seems like the longterm planning is really important,\n\n1:03:13.120 --> 1:03:15.720\n but the longterm part of the planning is really important.\n\n1:03:15.720 --> 1:03:18.920\n And also, I mean, maybe a quick tangent,\n\n1:03:18.920 --> 1:03:21.360\n how important do you think is removing\n\n1:03:21.360 --> 1:03:25.320\n the Markov assumption and looking at the full history?\n\n1:03:25.320 --> 1:03:28.040\n Sort of intuitively, of course, it's important,\n\n1:03:28.040 --> 1:03:30.960\n but is it like fundamentally transformative\n\n1:03:30.960 --> 1:03:33.400\n to the entirety of the problem?\n\n1:03:33.400 --> 1:03:34.320\n What's your sense of it?\n\n1:03:34.320 --> 1:03:37.800\n Like, cause we all, we make that assumption quite often.\n\n1:03:37.800 --> 1:03:40.000\n It's just throwing away the past.\n\n1:03:40.000 --> 1:03:41.800\n No, I think it's absolutely crucial.\n\n1:03:42.960 --> 1:03:47.240\n The question is whether there's a way to deal with it\n\n1:03:47.240 --> 1:03:52.240\n in a more heuristic and still sufficiently well way.\n\n1:03:52.360 --> 1:03:55.480\n So I have to come up with an example and fly,\n\n1:03:55.480 --> 1:03:59.360\n but you have some key event in your life,\n\n1:03:59.360 --> 1:04:02.080\n long time ago in some city or something,\n\n1:04:02.080 --> 1:04:05.360\n you realized that's a really dangerous street or whatever.\n\n1:04:05.360 --> 1:04:08.000\n And you want to remember that forever,\n\n1:04:08.000 --> 1:04:09.760\n in case you come back there.\n\n1:04:09.760 --> 1:04:11.520\n Kind of a selective kind of memory.\n\n1:04:11.520 --> 1:04:15.160\n So you remember all the important events in the past,\n\n1:04:15.160 --> 1:04:17.480\n but somehow selecting the important is.\n\n1:04:17.480 --> 1:04:18.600\n That's very hard.\n\n1:04:18.600 --> 1:04:21.720\n And I'm not concerned about just storing the whole history.\n\n1:04:21.720 --> 1:04:26.640\n Just, you can calculate, human life says 30 or 100 years,\n\n1:04:26.640 --> 1:04:28.600\n doesn't matter, right?\n\n1:04:28.600 --> 1:04:31.800\n How much data comes in through the vision system\n\n1:04:31.800 --> 1:04:35.200\n and the auditory system, you compress it a little bit,\n\n1:04:35.200 --> 1:04:37.560\n in this case, lossily and store it.\n\n1:04:37.560 --> 1:04:40.520\n We are soon in the means of just storing it.\n\n1:04:40.520 --> 1:04:44.920\n But you still need to the selection for the planning part\n\n1:04:44.920 --> 1:04:47.280\n and the compression for the understanding part.\n\n1:04:47.280 --> 1:04:50.000\n The raw storage I'm really not concerned about.\n\n1:04:50.000 --> 1:04:52.240\n And I think we should just store,\n\n1:04:52.240 --> 1:04:53.640\n if you develop an agent,\n\n1:04:54.600 --> 1:04:59.400\n preferably just store all the interaction history.\n\n1:04:59.400 --> 1:05:02.240\n And then you build of course models on top of it\n\n1:05:02.240 --> 1:05:04.960\n and you compress it and you are selective,\n\n1:05:04.960 --> 1:05:08.120\n but occasionally you go back to the old data\n\n1:05:08.120 --> 1:05:12.000\n and reanalyze it based on your new experience you have.\n\n1:05:12.000 --> 1:05:13.840\n Sometimes you are in school,\n\n1:05:13.840 --> 1:05:16.800\n you learn all these things you think is totally useless\n\n1:05:16.800 --> 1:05:18.200\n and much later you realize,\n\n1:05:18.200 --> 1:05:21.600\n oh, they were not so useless as you thought.\n\n1:05:21.600 --> 1:05:24.080\n I'm looking at you, linear algebra.\n\n1:05:24.080 --> 1:05:25.160\n Right.\n\n1:05:25.160 --> 1:05:27.720\n So maybe let me ask about objective functions\n\n1:05:27.720 --> 1:05:32.720\n because that rewards, it seems to be an important part.\n\n1:05:33.440 --> 1:05:36.640\n The rewards are kind of given to the system.\n\n1:05:38.200 --> 1:05:39.560\n For a lot of people,\n\n1:05:39.560 --> 1:05:44.560\n the specification of the objective function\n\n1:05:46.600 --> 1:05:48.440\n is a key part of intelligence.\n\n1:05:48.440 --> 1:05:52.920\n The agent itself figuring out what is important.\n\n1:05:52.920 --> 1:05:54.640\n What do you think about that?\n\n1:05:54.640 --> 1:05:58.560\n Is it possible within the IXE framework\n\n1:05:58.560 --> 1:06:01.880\n to yourself discover the reward\n\n1:06:01.880 --> 1:06:03.700\n based on which you should operate?\n\n1:06:05.440 --> 1:06:07.080\n Okay, that will be a long answer.\n\n1:06:07.080 --> 1:06:10.800\n So, and that is a very interesting question.\n\n1:06:10.800 --> 1:06:13.360\n And I'm asked a lot about this question,\n\n1:06:13.360 --> 1:06:15.600\n where do the rewards come from?\n\n1:06:15.600 --> 1:06:17.760\n And that depends.\n\n1:06:17.760 --> 1:06:21.320\n So, and then I give you now a couple of answers.\n\n1:06:21.320 --> 1:06:26.320\n So if you want to build agents, now let's start simple.\n\n1:06:26.320 --> 1:06:28.680\n So let's assume we want to build an agent\n\n1:06:28.680 --> 1:06:33.200\n based on the IXE model, which performs a particular task.\n\n1:06:33.200 --> 1:06:34.720\n Let's start with something super simple,\n\n1:06:34.720 --> 1:06:37.320\n like, I mean, super simple, like playing chess,\n\n1:06:37.320 --> 1:06:38.840\n or go or something, yeah.\n\n1:06:38.840 --> 1:06:42.480\n Then you just, the reward is winning the game is plus one,\n\n1:06:42.480 --> 1:06:45.280\n losing the game is minus one, done.\n\n1:06:45.280 --> 1:06:46.360\n You apply this agent.\n\n1:06:46.360 --> 1:06:49.080\n If you have enough compute, you let it self play\n\n1:06:49.080 --> 1:06:50.840\n and it will learn the rules of the game,\n\n1:06:50.840 --> 1:06:54.320\n will play perfect chess after some while, problem solved.\n\n1:06:54.320 --> 1:06:58.520\n Okay, so if you have more complicated problems,\n\n1:06:59.520 --> 1:07:03.640\n then you may believe that you have the right reward,\n\n1:07:03.640 --> 1:07:04.840\n but it's not.\n\n1:07:04.840 --> 1:07:08.400\n So a nice, cute example is the elevator control\n\n1:07:08.400 --> 1:07:10.400\n that is also in Rich Sutton's book,\n\n1:07:10.400 --> 1:07:12.200\n which is a great book, by the way.\n\n1:07:13.600 --> 1:07:15.640\n So you control the elevator and you think,\n\n1:07:15.640 --> 1:07:17.760\n well, maybe the reward should be coupled\n\n1:07:17.760 --> 1:07:20.200\n to how long people wait in front of the elevator.\n\n1:07:20.200 --> 1:07:21.840\n Long wait is bad.\n\n1:07:21.840 --> 1:07:23.680\n You program it and you do it.\n\n1:07:23.680 --> 1:07:25.840\n And what happens is the elevator eagerly picks up\n\n1:07:25.840 --> 1:07:28.040\n all the people, but never drops them off.\n\n1:07:28.040 --> 1:07:33.120\n So then you realize, oh, maybe the time in the elevator\n\n1:07:33.120 --> 1:07:36.280\n also counts, so you minimize the sum, yeah?\n\n1:07:36.280 --> 1:07:39.000\n And the elevator does that, but never picks up the people\n\n1:07:39.000 --> 1:07:40.400\n in the 10th floor and the top floor\n\n1:07:40.400 --> 1:07:42.320\n because in expectation, it's not worth it.\n\n1:07:42.320 --> 1:07:43.240\n Just let them stay.\n\n1:07:43.240 --> 1:07:44.080\n Yeah.\n\n1:07:44.080 --> 1:07:44.920\n Yeah.\n\n1:07:44.920 --> 1:07:45.760\n Yeah.\n\n1:07:45.760 --> 1:07:49.600\n So even in apparently simple problems,\n\n1:07:49.600 --> 1:07:51.240\n you can make mistakes, yeah?\n\n1:07:51.240 --> 1:07:55.240\n And that's what in more serious contexts\n\n1:07:55.240 --> 1:07:58.000\n AGI safety researchers consider.\n\n1:07:58.000 --> 1:08:00.640\n So now let's go back to general agents.\n\n1:08:00.640 --> 1:08:02.360\n So assume you want to build an agent,\n\n1:08:02.360 --> 1:08:05.080\n which is generally useful to humans, yeah?\n\n1:08:05.080 --> 1:08:07.440\n So you have a household robot, yeah?\n\n1:08:07.440 --> 1:08:09.840\n And it should do all kinds of tasks.\n\n1:08:09.840 --> 1:08:13.440\n So in this case, the human should give the reward\n\n1:08:13.440 --> 1:08:14.440\n on the fly.\n\n1:08:14.440 --> 1:08:16.200\n I mean, maybe it's pre trained in the factory\n\n1:08:16.200 --> 1:08:18.040\n and that there's some sort of internal reward\n\n1:08:18.040 --> 1:08:19.920\n for the battery level or whatever, yeah?\n\n1:08:19.920 --> 1:08:24.160\n But so it does the dishes badly, you punish the robot,\n\n1:08:24.160 --> 1:08:25.680\n it does it good, you reward the robot\n\n1:08:25.680 --> 1:08:28.440\n and then train it to a new task, yeah, like a child, right?\n\n1:08:28.440 --> 1:08:31.160\n So you need the human in the loop.\n\n1:08:31.160 --> 1:08:34.520\n If you want a system, which is useful to the human.\n\n1:08:34.520 --> 1:08:37.960\n And as long as these agents stay subhuman level,\n\n1:08:39.360 --> 1:08:41.080\n that should work reasonably well,\n\n1:08:41.080 --> 1:08:43.040\n apart from these examples.\n\n1:08:43.040 --> 1:08:45.840\n It becomes critical if they become on a human level.\n\n1:08:45.840 --> 1:08:47.200\n It's like with children, small children,\n\n1:08:47.200 --> 1:08:48.800\n you have reasonably well under control,\n\n1:08:48.800 --> 1:08:51.400\n they become older, the reward technique\n\n1:08:51.400 --> 1:08:52.800\n doesn't work so well anymore.\n\n1:08:54.160 --> 1:08:58.600\n So then finally, so this would be agents,\n\n1:08:58.600 --> 1:09:01.800\n which are just, you could say slaves to the humans, yeah?\n\n1:09:01.800 --> 1:09:03.960\n So if you are more ambitious and just say,\n\n1:09:03.960 --> 1:09:08.080\n we want to build a new species of intelligent beings,\n\n1:09:08.080 --> 1:09:09.360\n we put them on a new planet\n\n1:09:09.360 --> 1:09:12.080\n and we want them to develop this planet or whatever.\n\n1:09:12.080 --> 1:09:15.360\n So we don't give them any reward.\n\n1:09:15.360 --> 1:09:16.920\n So what could we do?\n\n1:09:16.920 --> 1:09:21.080\n And you could try to come up with some reward functions\n\n1:09:21.080 --> 1:09:23.400\n like it should maintain itself, the robot,\n\n1:09:23.400 --> 1:09:28.000\n it should maybe multiply, build more robots, right?\n\n1:09:28.000 --> 1:09:33.000\n And maybe all kinds of things which you find useful,\n\n1:09:33.000 --> 1:09:34.800\n but that's pretty hard, right?\n\n1:09:34.800 --> 1:09:36.640\n What does self maintenance mean?\n\n1:09:36.640 --> 1:09:38.120\n What does it mean to build a copy?\n\n1:09:38.120 --> 1:09:40.680\n Should it be exact copy, an approximate copy?\n\n1:09:40.680 --> 1:09:42.040\n And so that's really hard,\n\n1:09:42.040 --> 1:09:47.040\n but Laurent also at DeepMind developed a beautiful model.\n\n1:09:48.800 --> 1:09:50.560\n So it just took the ICSE model\n\n1:09:50.560 --> 1:09:54.960\n and coupled the rewards to information gain.\n\n1:09:54.960 --> 1:09:57.840\n So he said the reward is proportional\n\n1:09:57.840 --> 1:10:00.720\n to how much the agent had learned about the world.\n\n1:10:00.720 --> 1:10:03.320\n And you can rigorously, formally, uniquely define that\n\n1:10:03.320 --> 1:10:05.840\n in terms of archival versions, okay?\n\n1:10:05.840 --> 1:10:09.880\n So if you put that in, you get a completely autonomous agent.\n\n1:10:09.880 --> 1:10:11.680\n And actually, interestingly, for this agent,\n\n1:10:11.680 --> 1:10:13.120\n we can prove much stronger result\n\n1:10:13.120 --> 1:10:16.000\n than for the general agent, which is also nice.\n\n1:10:16.000 --> 1:10:18.080\n And if you let this agent loose,\n\n1:10:18.080 --> 1:10:20.000\n it will be in a sense, the optimal scientist.\n\n1:10:20.000 --> 1:10:22.920\n It is absolutely curious to learn as much as possible\n\n1:10:22.920 --> 1:10:24.120\n about the world.\n\n1:10:24.120 --> 1:10:25.720\n And of course, it will also have\n\n1:10:25.720 --> 1:10:27.160\n a lot of instrumental goals, right?\n\n1:10:27.160 --> 1:10:29.560\n In order to learn, it needs to at least survive, right?\n\n1:10:29.560 --> 1:10:31.520\n A dead agent is not good for anything.\n\n1:10:31.520 --> 1:10:33.960\n So it needs to have self preservation.\n\n1:10:33.960 --> 1:10:38.000\n And if it builds small helpers, acquiring more information,\n\n1:10:38.000 --> 1:10:39.120\n it will do that, yeah?\n\n1:10:39.120 --> 1:10:43.680\n If exploration, space exploration or whatever is necessary,\n\n1:10:43.680 --> 1:10:45.920\n right, to gathering information and develop it.\n\n1:10:45.920 --> 1:10:48.200\n So it has a lot of instrumental goals\n\n1:10:48.200 --> 1:10:51.000\n falling on this information gain.\n\n1:10:51.000 --> 1:10:53.760\n And this agent is completely autonomous of us.\n\n1:10:53.760 --> 1:10:55.640\n No rewards necessary anymore.\n\n1:10:55.640 --> 1:10:57.560\n Yeah, of course, it could find a way\n\n1:10:57.560 --> 1:10:59.600\n to game the concept of information\n\n1:10:59.600 --> 1:11:04.080\n and get stuck in that library\n\n1:11:04.080 --> 1:11:05.720\n that you mentioned beforehand\n\n1:11:05.720 --> 1:11:08.600\n with a very large number of books.\n\n1:11:08.600 --> 1:11:10.680\n The first agent had this problem.\n\n1:11:10.680 --> 1:11:13.640\n It would get stuck in front of an old TV screen,\n\n1:11:13.640 --> 1:11:14.960\n which has just had white noise.\n\n1:11:14.960 --> 1:11:16.480\n Yeah, white noise, yeah.\n\n1:11:16.480 --> 1:11:21.360\n But the second version can deal with at least stochasticity.\n\n1:11:21.360 --> 1:11:22.200\n Well.\n\n1:11:22.200 --> 1:11:23.680\n Yeah, what about curiosity?\n\n1:11:23.680 --> 1:11:27.920\n This kind of word, curiosity, creativity,\n\n1:11:27.920 --> 1:11:30.880\n is that kind of the reward function being\n\n1:11:30.880 --> 1:11:31.920\n of getting new information?\n\n1:11:31.920 --> 1:11:36.920\n Is that similar to idea of kind of injecting exploration\n\n1:11:39.000 --> 1:11:41.880\n for its own sake inside the reward function?\n\n1:11:41.880 --> 1:11:44.880\n Do you find this at all appealing, interesting?\n\n1:11:44.880 --> 1:11:46.320\n I think that's a nice definition.\n\n1:11:46.320 --> 1:11:48.600\n Curiosity is rewards.\n\n1:11:48.600 --> 1:11:53.600\n Sorry, curiosity is exploration for its own sake.\n\n1:11:54.800 --> 1:11:56.200\n Yeah, I would accept that.\n\n1:11:57.120 --> 1:11:59.920\n But most curiosity, well, in humans,\n\n1:11:59.920 --> 1:12:01.240\n and especially in children,\n\n1:12:01.240 --> 1:12:03.040\n is not just for its own sake,\n\n1:12:03.040 --> 1:12:05.960\n but for actually learning about the environment\n\n1:12:05.960 --> 1:12:08.440\n and for behaving better.\n\n1:12:08.440 --> 1:12:13.120\n So I think most curiosity is tied in the end\n\n1:12:13.120 --> 1:12:14.840\n towards performing better.\n\n1:12:14.840 --> 1:12:17.680\n Well, okay, so if intelligence systems\n\n1:12:17.680 --> 1:12:19.760\n need to have this reward function,\n\n1:12:19.760 --> 1:12:22.360\n let me, you're an intelligence system,\n\n1:12:23.680 --> 1:12:26.600\n currently passing the torrent test quite effectively.\n\n1:12:26.600 --> 1:12:30.240\n What's the reward function\n\n1:12:30.240 --> 1:12:33.920\n of our human intelligence existence?\n\n1:12:33.920 --> 1:12:35.160\n What's the reward function\n\n1:12:35.160 --> 1:12:37.720\n that Marcus Hutter is operating under?\n\n1:12:37.720 --> 1:12:39.760\n Okay, to the first question,\n\n1:12:39.760 --> 1:12:44.480\n the biological reward function is to survive and to spread,\n\n1:12:44.480 --> 1:12:48.200\n and very few humans sort of are able to overcome\n\n1:12:48.200 --> 1:12:49.920\n this biological reward function.\n\n1:12:50.920 --> 1:12:54.200\n But we live in a very nice world\n\n1:12:54.200 --> 1:12:56.240\n where we have lots of spare time\n\n1:12:56.240 --> 1:12:57.640\n and can still survive and spread,\n\n1:12:57.640 --> 1:13:01.920\n so we can develop arbitrary other interests,\n\n1:13:01.920 --> 1:13:03.280\n which is quite interesting.\n\n1:13:03.280 --> 1:13:04.400\n On top of that.\n\n1:13:04.400 --> 1:13:06.160\n On top of that, yeah.\n\n1:13:06.160 --> 1:13:09.120\n But the survival and spreading sort of is,\n\n1:13:09.120 --> 1:13:13.160\n I would say, the goal or the reward function of humans,\n\n1:13:13.160 --> 1:13:15.360\n so that the core one.\n\n1:13:15.360 --> 1:13:17.480\n I like how you avoided answering the second question,\n\n1:13:17.480 --> 1:13:19.760\n which a good intelligence system would.\n\n1:13:19.760 --> 1:13:20.880\n So my.\n\n1:13:20.880 --> 1:13:24.320\n That your own meaning of life and the reward function.\n\n1:13:24.320 --> 1:13:26.960\n My own meaning of life and reward function\n\n1:13:26.960 --> 1:13:29.560\n is to find an AGI to build it.\n\n1:13:31.200 --> 1:13:32.040\n Beautifully put.\n\n1:13:32.040 --> 1:13:34.280\n Okay, let's dissect the X even further.\n\n1:13:34.280 --> 1:13:37.960\n So one of the assumptions is kind of infinity\n\n1:13:37.960 --> 1:13:39.680\n keeps creeping up everywhere,\n\n1:13:39.680 --> 1:13:44.680\n which, what are your thoughts\n\n1:13:44.960 --> 1:13:46.920\n on kind of bounded rationality\n\n1:13:46.920 --> 1:13:50.040\n and sort of the nature of our existence\n\n1:13:50.040 --> 1:13:52.000\n and intelligence systems is that we're operating\n\n1:13:52.000 --> 1:13:55.680\n always under constraints, under limited time,\n\n1:13:55.680 --> 1:13:57.640\n limited resources.\n\n1:13:57.640 --> 1:13:59.480\n How does that, how do you think about that\n\n1:13:59.480 --> 1:14:01.600\n within the IXE framework,\n\n1:14:01.600 --> 1:14:04.480\n within trying to create an AGI system\n\n1:14:04.480 --> 1:14:06.760\n that operates under these constraints?\n\n1:14:06.760 --> 1:14:09.200\n Yeah, that is one of the criticisms about IXE,\n\n1:14:09.200 --> 1:14:11.320\n that it ignores computation and completely.\n\n1:14:11.320 --> 1:14:13.800\n And some people believe that intelligence\n\n1:14:13.800 --> 1:14:18.800\n is inherently tied to what's bounded resources.\n\n1:14:19.520 --> 1:14:21.160\n What do you think on this one point?\n\n1:14:21.160 --> 1:14:22.480\n Do you think it's,\n\n1:14:22.480 --> 1:14:23.920\n do you think the bounded resources\n\n1:14:23.920 --> 1:14:25.640\n are fundamental to intelligence?\n\n1:14:27.840 --> 1:14:31.160\n I would say that an intelligence notion,\n\n1:14:31.160 --> 1:14:35.520\n which ignores computational limits is extremely useful.\n\n1:14:35.520 --> 1:14:37.120\n A good intelligence notion,\n\n1:14:37.120 --> 1:14:40.720\n which includes these resources would be even more useful,\n\n1:14:40.720 --> 1:14:42.160\n but we don't have that yet.\n\n1:14:43.280 --> 1:14:48.280\n And so look at other fields outside of computer science,\n\n1:14:48.480 --> 1:14:52.240\n computational aspects never play a fundamental role.\n\n1:14:52.240 --> 1:14:54.880\n You develop biological models for cells,\n\n1:14:54.880 --> 1:14:56.680\n something in physics, these theories,\n\n1:14:56.680 --> 1:14:58.160\n I mean, become more and more crazy\n\n1:14:58.160 --> 1:15:00.320\n and harder and harder to compute.\n\n1:15:00.320 --> 1:15:01.440\n Well, in the end, of course,\n\n1:15:01.440 --> 1:15:02.960\n we need to do something with this model,\n\n1:15:02.960 --> 1:15:05.520\n but this is more a nuisance than a feature.\n\n1:15:05.520 --> 1:15:10.040\n And I'm sometimes wondering if artificial intelligence\n\n1:15:10.040 --> 1:15:12.080\n would not sit in a computer science department,\n\n1:15:12.080 --> 1:15:14.040\n but in a philosophy department,\n\n1:15:14.040 --> 1:15:16.120\n then this computational focus\n\n1:15:16.120 --> 1:15:18.400\n would be probably significantly less.\n\n1:15:18.400 --> 1:15:19.720\n I mean, think about the induction problem\n\n1:15:19.720 --> 1:15:22.080\n is more in the philosophy department.\n\n1:15:22.080 --> 1:15:24.480\n There's virtually no paper who cares about,\n\n1:15:24.480 --> 1:15:26.440\n how long it takes to compute the answer.\n\n1:15:26.440 --> 1:15:28.320\n That is completely secondary.\n\n1:15:28.320 --> 1:15:31.680\n Of course, once we have figured out the first problem,\n\n1:15:31.680 --> 1:15:35.840\n so intelligence without computational resources,\n\n1:15:35.840 --> 1:15:39.400\n then the next and very good question is,\n\n1:15:39.400 --> 1:15:42.480\n could we improve it by including computational resources,\n\n1:15:42.480 --> 1:15:45.520\n but nobody was able to do that so far\n\n1:15:45.520 --> 1:15:47.800\n in an even halfway satisfactory manner.\n\n1:15:49.240 --> 1:15:51.600\n I like that, that in the long run,\n\n1:15:51.600 --> 1:15:54.000\n the right department to belong to is philosophy.\n\n1:15:55.160 --> 1:15:58.680\n That's actually quite a deep idea,\n\n1:15:58.680 --> 1:16:01.440\n or even to at least to think about\n\n1:16:01.440 --> 1:16:03.680\n big picture philosophical questions,\n\n1:16:03.680 --> 1:16:05.280\n big picture questions,\n\n1:16:05.280 --> 1:16:07.400\n even in the computer science department.\n\n1:16:07.400 --> 1:16:10.000\n But you've mentioned approximation.\n\n1:16:10.000 --> 1:16:12.160\n Sort of, there's a lot of infinity,\n\n1:16:12.160 --> 1:16:13.920\n a lot of huge resources needed.\n\n1:16:13.920 --> 1:16:16.280\n Are there approximations to IXE\n\n1:16:16.280 --> 1:16:19.800\n that within the IXE framework that are useful?\n\n1:16:19.800 --> 1:16:23.120\n Yeah, we have developed a couple of approximations.\n\n1:16:23.120 --> 1:16:27.280\n And what we do there is that\n\n1:16:27.280 --> 1:16:29.840\n the Solomov induction part,\n\n1:16:29.840 --> 1:16:33.640\n which was find the shortest program describing your data,\n\n1:16:33.640 --> 1:16:36.640\n we just replace it by standard data compressors.\n\n1:16:36.640 --> 1:16:39.240\n And the better compressors get,\n\n1:16:39.240 --> 1:16:41.680\n the better this part will become.\n\n1:16:41.680 --> 1:16:43.400\n We focus on a particular compressor\n\n1:16:43.400 --> 1:16:44.560\n called context tree weighting,\n\n1:16:44.560 --> 1:16:48.520\n which is pretty amazing, not so well known.\n\n1:16:48.520 --> 1:16:50.120\n It has beautiful theoretical properties,\n\n1:16:50.120 --> 1:16:52.240\n also works reasonably well in practice.\n\n1:16:52.240 --> 1:16:55.160\n So we use that for the approximation of the induction\n\n1:16:55.160 --> 1:16:58.160\n and the learning and the prediction part.\n\n1:16:58.160 --> 1:17:01.680\n And for the planning part,\n\n1:17:01.680 --> 1:17:05.560\n we essentially just took the ideas from a computer go\n\n1:17:05.560 --> 1:17:07.320\n from 2006.\n\n1:17:07.320 --> 1:17:10.440\n It was Java Zipes Bari, also now at DeepMind,\n\n1:17:11.320 --> 1:17:14.600\n who developed the so called UCT algorithm,\n\n1:17:14.600 --> 1:17:17.440\n upper confidence bound for trees algorithm\n\n1:17:17.440 --> 1:17:19.040\n on top of the Monte Carlo tree search.\n\n1:17:19.040 --> 1:17:23.200\n So we approximate this planning part by sampling.\n\n1:17:23.200 --> 1:17:28.200\n And it's successful on some small toy problems.\n\n1:17:29.280 --> 1:17:33.480\n We don't want to lose the generality, right?\n\n1:17:33.480 --> 1:17:34.920\n And that's sort of the handicap, right?\n\n1:17:34.920 --> 1:17:38.840\n If you want to be general, you have to give up something.\n\n1:17:38.840 --> 1:17:41.960\n So, but this single agent was able to play small games\n\n1:17:41.960 --> 1:17:46.960\n like Coon poker and Tic Tac Toe and even Pacman\n\n1:17:49.160 --> 1:17:52.040\n in the same architecture, no change.\n\n1:17:52.040 --> 1:17:54.880\n The agent doesn't know the rules of the game,\n\n1:17:54.880 --> 1:17:57.640\n really nothing and all by self or by a player\n\n1:17:57.640 --> 1:17:58.840\n with these environments.\n\n1:17:59.920 --> 1:18:03.800\n So J\u00fcrgen Schmidhuber proposed something called\n\n1:18:03.800 --> 1:18:06.920\n Ghetto Machines, which is a self improving program\n\n1:18:06.920 --> 1:18:08.400\n that rewrites its own code.\n\n1:18:10.800 --> 1:18:12.800\n Sort of mathematically, philosophically,\n\n1:18:12.800 --> 1:18:15.080\n what's the relationship in your eyes,\n\n1:18:15.080 --> 1:18:16.160\n if you're familiar with it,\n\n1:18:16.160 --> 1:18:18.400\n between AXI and the Ghetto Machines?\n\n1:18:18.400 --> 1:18:19.720\n Yeah, familiar with it.\n\n1:18:19.720 --> 1:18:22.320\n He developed it while I was in his lab.\n\n1:18:22.320 --> 1:18:26.200\n Yeah, so the Ghetto Machine, to explain it briefly,\n\n1:18:27.080 --> 1:18:28.920\n you give it a task.\n\n1:18:28.920 --> 1:18:30.400\n It could be a simple task as, you know,\n\n1:18:30.400 --> 1:18:32.480\n finding prime factors in numbers, right?\n\n1:18:32.480 --> 1:18:33.840\n You can formally write it down.\n\n1:18:33.840 --> 1:18:35.280\n There's a very slow algorithm to do that.\n\n1:18:35.280 --> 1:18:37.520\n Just try all the factors, yeah.\n\n1:18:37.520 --> 1:18:39.240\n Or play chess, right?\n\n1:18:39.240 --> 1:18:41.200\n Optimally, you write the algorithm to minimax\n\n1:18:41.200 --> 1:18:42.080\n to the end of the game.\n\n1:18:42.080 --> 1:18:45.360\n So you write down what the Ghetto Machine should do.\n\n1:18:45.360 --> 1:18:50.360\n Then it will take part of its resources to run this program\n\n1:18:50.720 --> 1:18:54.000\n and other part of its resources to improve this program.\n\n1:18:54.000 --> 1:18:56.880\n And when it finds an improved version,\n\n1:18:56.880 --> 1:19:00.680\n which provably computes the same answer.\n\n1:19:00.680 --> 1:19:02.320\n So that's the key part, yeah.\n\n1:19:02.320 --> 1:19:05.680\n It needs to prove by itself that this change of program\n\n1:19:05.680 --> 1:19:08.960\n still satisfies the original specification.\n\n1:19:08.960 --> 1:19:11.680\n And if it does so, then it replaces the original program\n\n1:19:11.680 --> 1:19:13.120\n by the improved program.\n\n1:19:13.120 --> 1:19:15.120\n And by definition, it does the same job,\n\n1:19:15.120 --> 1:19:17.080\n but just faster, okay?\n\n1:19:17.080 --> 1:19:19.160\n And then, you know, it proves over it and over it.\n\n1:19:19.160 --> 1:19:24.160\n And it's developed in a way that all parts\n\n1:19:24.560 --> 1:19:26.720\n of this Ghetto Machine can self improve,\n\n1:19:26.720 --> 1:19:29.160\n but it stays provably consistent\n\n1:19:29.160 --> 1:19:31.760\n with the original specification.\n\n1:19:31.760 --> 1:19:36.080\n So from this perspective, it has nothing to do with iXe.\n\n1:19:36.080 --> 1:19:40.520\n But if you would now put iXe as the starting axioms in,\n\n1:19:40.520 --> 1:19:44.800\n it would run iXe, but you know, that takes forever.\n\n1:19:44.800 --> 1:19:48.480\n But then if it finds a provable speed up of iXe,\n\n1:19:48.480 --> 1:19:50.960\n it would replace it by this and this and this.\n\n1:19:50.960 --> 1:19:52.840\n And maybe eventually it comes up with a model\n\n1:19:52.840 --> 1:19:54.480\n which is still the iXe model.\n\n1:19:54.480 --> 1:19:59.480\n It cannot be, I mean, just for the knowledgeable reader,\n\n1:19:59.600 --> 1:20:03.200\n iXe is incomputable and that can prove that therefore\n\n1:20:03.200 --> 1:20:08.200\n there cannot be a computable exact algorithm computers.\n\n1:20:08.640 --> 1:20:10.360\n There needs to be some approximations\n\n1:20:10.360 --> 1:20:11.960\n and this is not dealt with the Ghetto Machine.\n\n1:20:11.960 --> 1:20:13.200\n So you have to do something about it.\n\n1:20:13.200 --> 1:20:15.680\n But there's the iXe TL model, which is finitely computable,\n\n1:20:15.680 --> 1:20:16.520\n which we could put in.\n\n1:20:16.520 --> 1:20:19.240\n Which part of iXe is noncomputable?\n\n1:20:19.240 --> 1:20:20.760\n The Solomonov induction part.\n\n1:20:20.760 --> 1:20:22.240\n The induction, okay, so.\n\n1:20:22.240 --> 1:20:26.320\n But there is ways of getting computable approximations\n\n1:20:26.320 --> 1:20:30.000\n of the iXe model, so then it's at least computable.\n\n1:20:30.000 --> 1:20:33.680\n It is still way beyond any resources anybody will ever have,\n\n1:20:33.680 --> 1:20:35.840\n but then the Ghetto Machine could sort of improve it\n\n1:20:35.840 --> 1:20:37.720\n further and further in an exact way.\n\n1:20:37.720 --> 1:20:41.160\n So is it theoretically possible\n\n1:20:41.160 --> 1:20:45.120\n that the Ghetto Machine process could improve?\n\n1:20:45.120 --> 1:20:50.120\n Isn't iXe already optimal?\n\n1:20:51.800 --> 1:20:56.760\n It is optimal in terms of the reward collected\n\n1:20:56.760 --> 1:20:59.360\n over its interaction cycles,\n\n1:20:59.360 --> 1:21:03.440\n but it takes infinite time to produce one action.\n\n1:21:03.440 --> 1:21:07.120\n And the world continues whether you want it or not.\n\n1:21:07.120 --> 1:21:09.720\n So the model is assuming you had an oracle,\n\n1:21:09.720 --> 1:21:11.200\n which solved this problem,\n\n1:21:11.200 --> 1:21:12.920\n and then in the next 100 milliseconds\n\n1:21:12.920 --> 1:21:15.360\n or the reaction time you need gives the answer,\n\n1:21:15.360 --> 1:21:16.640\n then iXe is optimal.\n\n1:21:18.200 --> 1:21:21.440\n It's optimal in sense of also from learning efficiency\n\n1:21:21.440 --> 1:21:25.600\n and data efficiency, but not in terms of computation time.\n\n1:21:25.600 --> 1:21:27.560\n And then the Ghetto Machine in theory,\n\n1:21:27.560 --> 1:21:31.000\n but probably not provably could make it go faster.\n\n1:21:31.000 --> 1:21:31.840\n Yes.\n\n1:21:31.840 --> 1:21:34.520\n Okay, interesting.\n\n1:21:34.520 --> 1:21:36.640\n Those two components are super interesting.\n\n1:21:36.640 --> 1:21:39.960\n The sort of the perfect intelligence combined\n\n1:21:39.960 --> 1:21:42.920\n with self improvement,\n\n1:21:44.120 --> 1:21:45.600\n sort of provable self improvement\n\n1:21:45.600 --> 1:21:48.760\n since you're always getting the correct answer\n\n1:21:48.760 --> 1:21:50.360\n and you're improving.\n\n1:21:50.360 --> 1:21:51.400\n Beautiful ideas.\n\n1:21:51.400 --> 1:21:55.120\n Okay, so you've also mentioned that different kinds\n\n1:21:55.120 --> 1:21:59.840\n of things in the chase of solving this reward,\n\n1:21:59.840 --> 1:22:01.740\n sort of optimizing for the goal,\n\n1:22:02.960 --> 1:22:04.960\n interesting human things could emerge.\n\n1:22:04.960 --> 1:22:08.820\n So is there a place for consciousness within iXe?\n\n1:22:10.880 --> 1:22:13.480\n Where does, maybe you can comment,\n\n1:22:13.480 --> 1:22:17.440\n because I suppose we humans are just another instantiation\n\n1:22:17.440 --> 1:22:20.880\n of iXe agents and we seem to have consciousness.\n\n1:22:20.880 --> 1:22:23.400\n You say humans are an instantiation of an iXe agent?\n\n1:22:23.400 --> 1:22:24.240\n Yes.\n\n1:22:24.240 --> 1:22:25.280\n Well, that would be amazing,\n\n1:22:25.280 --> 1:22:27.880\n but I think that's not true even for the smartest\n\n1:22:27.880 --> 1:22:29.000\n and most rational humans.\n\n1:22:29.000 --> 1:22:32.920\n I think maybe we are very crude approximations.\n\n1:22:32.920 --> 1:22:33.760\n Interesting.\n\n1:22:33.760 --> 1:22:35.720\n I mean, I tend to believe, again, I'm Russian,\n\n1:22:35.720 --> 1:22:40.720\n so I tend to believe our flaws are part of the optimal.\n\n1:22:41.160 --> 1:22:45.640\n So we tend to laugh off and criticize our flaws\n\n1:22:45.640 --> 1:22:49.240\n and I tend to think that that's actually close\n\n1:22:49.240 --> 1:22:50.680\n to an optimal behavior.\n\n1:22:50.680 --> 1:22:53.760\n Well, some flaws, if you think more carefully about it,\n\n1:22:53.760 --> 1:22:54.960\n are actually not flaws, yeah,\n\n1:22:54.960 --> 1:22:57.760\n but I think there are still enough flaws.\n\n1:22:58.920 --> 1:23:00.000\n I don't know.\n\n1:23:00.000 --> 1:23:00.840\n It's unclear.\n\n1:23:00.840 --> 1:23:01.880\n As a student of history,\n\n1:23:01.880 --> 1:23:05.240\n I think all the suffering that we've endured\n\n1:23:05.240 --> 1:23:06.760\n as a civilization,\n\n1:23:06.760 --> 1:23:10.200\n it's possible that that's the optimal amount of suffering\n\n1:23:10.200 --> 1:23:13.800\n we need to endure to minimize longterm suffering.\n\n1:23:15.000 --> 1:23:17.280\n That's your Russian background, I think.\n\n1:23:17.280 --> 1:23:18.120\n That's the Russian.\n\n1:23:18.120 --> 1:23:21.840\n Whether humans are or not instantiations of an iXe agent,\n\n1:23:21.840 --> 1:23:23.920\n do you think there's a consciousness\n\n1:23:23.920 --> 1:23:25.640\n of something that could emerge\n\n1:23:25.640 --> 1:23:29.720\n in a computational form or framework like iXe?\n\n1:23:29.720 --> 1:23:31.720\n Let me also ask you a question.\n\n1:23:31.720 --> 1:23:33.040\n Do you think I'm conscious?\n\n1:23:36.800 --> 1:23:38.200\n Yeah, that's a good question.\n\n1:23:38.200 --> 1:23:43.200\n That tie is confusing me, but I think so.\n\n1:23:44.360 --> 1:23:45.720\n You think that makes me unconscious\n\n1:23:45.720 --> 1:23:47.160\n because it strangles me or?\n\n1:23:47.160 --> 1:23:49.720\n If an agent were to solve the imitation game\n\n1:23:49.720 --> 1:23:50.600\n posed by Turing,\n\n1:23:50.600 --> 1:23:53.400\n I think that would be dressed similarly to you.\n\n1:23:53.400 --> 1:23:56.800\n That because there's a kind of flamboyant,\n\n1:23:56.800 --> 1:24:01.040\n interesting, complex behavior pattern\n\n1:24:01.040 --> 1:24:04.440\n that sells that you're human and you're conscious.\n\n1:24:04.440 --> 1:24:06.080\n But why do you ask?\n\n1:24:06.080 --> 1:24:07.880\n Was it a yes or was it a no?\n\n1:24:07.880 --> 1:24:12.640\n Yes, I think you're conscious, yes.\n\n1:24:12.640 --> 1:24:16.080\n So, and you explained sort of somehow why,\n\n1:24:16.080 --> 1:24:18.760\n but you infer that from my behavior, right?\n\n1:24:18.760 --> 1:24:20.680\n You can never be sure about that.\n\n1:24:20.680 --> 1:24:23.280\n And I think the same thing will happen\n\n1:24:23.280 --> 1:24:26.760\n with any intelligent agent we develop\n\n1:24:26.760 --> 1:24:31.000\n if it behaves in a way sufficiently close to humans\n\n1:24:31.000 --> 1:24:32.080\n or maybe even not humans.\n\n1:24:32.080 --> 1:24:34.240\n I mean, maybe a dog is also sometimes\n\n1:24:34.240 --> 1:24:35.720\n a little bit self conscious, right?\n\n1:24:35.720 --> 1:24:38.800\n So if it behaves in a way\n\n1:24:38.800 --> 1:24:41.160\n where we attribute typically consciousness,\n\n1:24:41.160 --> 1:24:42.720\n we would attribute consciousness\n\n1:24:42.720 --> 1:24:44.320\n to these intelligent systems.\n\n1:24:44.320 --> 1:24:47.240\n And I see probably in particular\n\n1:24:47.240 --> 1:24:48.800\n that of course doesn't answer the question\n\n1:24:48.800 --> 1:24:50.800\n whether it's really conscious.\n\n1:24:50.800 --> 1:24:53.680\n And that's the big hard problem of consciousness.\n\n1:24:53.680 --> 1:24:55.680\n Maybe I'm a zombie.\n\n1:24:55.680 --> 1:24:59.320\n I mean, not the movie zombie, but the philosophical zombie.\n\n1:24:59.320 --> 1:25:02.600\n Is to you the display of consciousness\n\n1:25:02.600 --> 1:25:05.000\n close enough to consciousness\n\n1:25:05.000 --> 1:25:06.720\n from a perspective of AGI\n\n1:25:06.720 --> 1:25:09.800\n that the distinction of the hard problem of consciousness\n\n1:25:09.800 --> 1:25:11.320\n is not an interesting one?\n\n1:25:11.320 --> 1:25:12.480\n I think we don't have to worry\n\n1:25:12.480 --> 1:25:13.920\n about the consciousness problem,\n\n1:25:13.920 --> 1:25:16.840\n especially the hard problem for developing AGI.\n\n1:25:16.840 --> 1:25:20.200\n I think, you know, we progress.\n\n1:25:20.200 --> 1:25:23.120\n At some point we have solved all the technical problems\n\n1:25:23.120 --> 1:25:25.440\n and this system will behave intelligent\n\n1:25:25.440 --> 1:25:26.520\n and then super intelligent.\n\n1:25:26.520 --> 1:25:30.160\n And this consciousness will emerge.\n\n1:25:30.160 --> 1:25:32.480\n I mean, definitely it will display behavior\n\n1:25:32.480 --> 1:25:35.040\n which we will interpret as conscious.\n\n1:25:35.040 --> 1:25:38.120\n And then it's a philosophical question.\n\n1:25:38.120 --> 1:25:39.840\n Did this consciousness really emerge\n\n1:25:39.840 --> 1:25:43.680\n or is it a zombie which just, you know, fakes everything?\n\n1:25:43.680 --> 1:25:45.200\n We still don't have to figure that out.\n\n1:25:45.200 --> 1:25:47.480\n Although it may be interesting,\n\n1:25:47.480 --> 1:25:48.920\n at least from a philosophical point of view,\n\n1:25:48.920 --> 1:25:49.840\n it's very interesting,\n\n1:25:49.840 --> 1:25:53.160\n but it may also be sort of practically interesting.\n\n1:25:53.160 --> 1:25:54.280\n You know, there's some people saying,\n\n1:25:54.280 --> 1:25:56.200\n if it's just faking consciousness and feelings,\n\n1:25:56.200 --> 1:25:58.280\n you know, then we don't need to be concerned about,\n\n1:25:58.280 --> 1:25:59.160\n you know, rights.\n\n1:25:59.160 --> 1:26:01.600\n But if it's real conscious and has feelings,\n\n1:26:01.600 --> 1:26:03.400\n then we need to be concerned, yeah.\n\n1:26:05.840 --> 1:26:07.560\n I can't wait till the day\n\n1:26:07.560 --> 1:26:10.640\n where AI systems exhibit consciousness\n\n1:26:10.640 --> 1:26:14.520\n because it'll truly be some of the hardest ethical questions\n\n1:26:14.520 --> 1:26:15.640\n of what we do with that.\n\n1:26:15.640 --> 1:26:18.880\n It is rather easy to build systems\n\n1:26:18.880 --> 1:26:21.120\n which people ascribe consciousness.\n\n1:26:21.120 --> 1:26:22.600\n And I give you an analogy.\n\n1:26:22.600 --> 1:26:25.320\n I mean, remember, maybe it was before you were born,\n\n1:26:25.320 --> 1:26:26.760\n the Tamagotchi?\n\n1:26:26.760 --> 1:26:27.880\n Yeah.\n\n1:26:27.880 --> 1:26:28.760\n Freaking born.\n\n1:26:28.760 --> 1:26:29.800\n How dare you, sir?\n\n1:26:30.960 --> 1:26:33.240\n Why, that's the, you're young, right?\n\n1:26:33.240 --> 1:26:34.080\n Yes, that's good.\n\n1:26:34.080 --> 1:26:36.200\n Thank you, thank you very much.\n\n1:26:36.200 --> 1:26:37.560\n But I was also in the Soviet Union.\n\n1:26:37.560 --> 1:26:41.240\n We didn't have any of those fun things.\n\n1:26:41.240 --> 1:26:42.680\n But you have heard about this Tamagotchi,\n\n1:26:42.680 --> 1:26:44.600\n which was, you know, really, really primitive,\n\n1:26:44.600 --> 1:26:46.920\n actually, for the time it was,\n\n1:26:46.920 --> 1:26:48.840\n and, you know, you could raise, you know, this,\n\n1:26:48.840 --> 1:26:51.640\n and kids got so attached to it\n\n1:26:51.640 --> 1:26:53.600\n and, you know, didn't want to let it die\n\n1:26:53.600 --> 1:26:56.920\n and probably, if we would have asked, you know,\n\n1:26:56.920 --> 1:26:59.520\n the children, do you think this Tamagotchi is conscious?\n\n1:26:59.520 --> 1:27:00.360\n They would have said yes.\n\n1:27:00.360 --> 1:27:01.600\n Half of them would have said yes, I would guess.\n\n1:27:01.600 --> 1:27:04.720\n I think that's kind of a beautiful thing, actually,\n\n1:27:04.720 --> 1:27:08.640\n because that consciousness, ascribing consciousness,\n\n1:27:08.640 --> 1:27:10.440\n seems to create a deeper connection.\n\n1:27:10.440 --> 1:27:11.280\n Yeah.\n\n1:27:11.280 --> 1:27:12.600\n Which is a powerful thing.\n\n1:27:12.600 --> 1:27:15.880\n But we'll have to be careful on the ethics side of that.\n\n1:27:15.880 --> 1:27:18.440\n Well, let me ask about the AGI community broadly.\n\n1:27:18.440 --> 1:27:22.600\n You kind of represent some of the most serious work on AGI,\n\n1:27:22.600 --> 1:27:24.280\n as of at least earlier,\n\n1:27:24.280 --> 1:27:29.280\n and DeepMind represents serious work on AGI these days.\n\n1:27:29.280 --> 1:27:34.080\n But why, in your sense, is the AGI community so small\n\n1:27:34.080 --> 1:27:38.120\n or has been so small until maybe DeepMind came along?\n\n1:27:38.120 --> 1:27:41.680\n Like, why aren't more people seriously working\n\n1:27:41.680 --> 1:27:45.840\n on human level and superhuman level intelligence\n\n1:27:45.840 --> 1:27:47.360\n from a formal perspective?\n\n1:27:48.240 --> 1:27:49.680\n Okay, from a formal perspective,\n\n1:27:49.680 --> 1:27:52.560\n that's sort of an extra point.\n\n1:27:53.640 --> 1:27:54.960\n So I think there are a couple of reasons.\n\n1:27:54.960 --> 1:27:56.680\n I mean, AI came in waves, right?\n\n1:27:56.680 --> 1:27:58.520\n You know, AI winters and AI summers,\n\n1:27:58.520 --> 1:28:01.520\n and then there were big promises which were not fulfilled,\n\n1:28:01.520 --> 1:28:05.760\n and people got disappointed.\n\n1:28:05.760 --> 1:28:10.760\n And that narrow AI solving particular problems,\n\n1:28:11.480 --> 1:28:14.040\n which seemed to require intelligence,\n\n1:28:14.040 --> 1:28:17.000\n was always to some extent successful,\n\n1:28:17.000 --> 1:28:19.480\n and there were improvements, small steps.\n\n1:28:19.480 --> 1:28:24.240\n And if you build something which is useful for society\n\n1:28:24.240 --> 1:28:26.600\n or industrial useful, then there's a lot of funding.\n\n1:28:26.600 --> 1:28:28.520\n So I guess it was in parts the money,\n\n1:28:29.960 --> 1:28:34.200\n which drives people to develop a specific system\n\n1:28:34.200 --> 1:28:36.240\n solving specific tasks.\n\n1:28:36.240 --> 1:28:38.760\n But you would think that, at least in university,\n\n1:28:39.680 --> 1:28:42.800\n you should be able to do ivory tower research.\n\n1:28:43.680 --> 1:28:46.000\n And that was probably better a long time ago,\n\n1:28:46.000 --> 1:28:48.280\n but even nowadays, there's quite some pressure\n\n1:28:48.280 --> 1:28:52.240\n of doing applied research or translational research,\n\n1:28:52.240 --> 1:28:56.640\n and it's harder to get grants as a theorist.\n\n1:28:56.640 --> 1:28:59.920\n So that also drives people away.\n\n1:28:59.920 --> 1:29:01.520\n It's maybe also harder\n\n1:29:01.520 --> 1:29:03.120\n attacking the general intelligence problem.\n\n1:29:03.120 --> 1:29:05.880\n So I think enough people, I mean, maybe a small number\n\n1:29:05.880 --> 1:29:09.560\n were still interested in formalizing intelligence\n\n1:29:09.560 --> 1:29:12.880\n and thinking of general intelligence,\n\n1:29:12.880 --> 1:29:17.560\n but not much came up, right?\n\n1:29:17.560 --> 1:29:19.880\n Well, not much great stuff came up.\n\n1:29:19.880 --> 1:29:21.360\n So what do you think,\n\n1:29:21.360 --> 1:29:24.840\n we talked about the formal big light\n\n1:29:24.840 --> 1:29:26.160\n at the end of the tunnel,\n\n1:29:26.160 --> 1:29:27.600\n but from the engineering perspective,\n\n1:29:27.600 --> 1:29:30.360\n what do you think it takes to build an AGI system?\n\n1:29:30.360 --> 1:29:33.920\n Is that, and I don't know if that's a stupid question\n\n1:29:33.920 --> 1:29:35.120\n or a distinct question\n\n1:29:35.120 --> 1:29:37.160\n from everything we've been talking about at AICSI,\n\n1:29:37.160 --> 1:29:41.040\n but what do you see as the steps that are necessary to take\n\n1:29:41.040 --> 1:29:43.040\n to start to try to build something?\n\n1:29:43.040 --> 1:29:44.360\n So you want a blueprint now,\n\n1:29:44.360 --> 1:29:46.360\n and then you go off and do it?\n\n1:29:46.360 --> 1:29:48.040\n That's the whole point of this conversation,\n\n1:29:48.040 --> 1:29:49.800\n trying to squeeze that in there.\n\n1:29:49.800 --> 1:29:51.560\n Now, is there, I mean, what's your intuition?\n\n1:29:51.560 --> 1:29:53.960\n Is it in the robotics space\n\n1:29:53.960 --> 1:29:56.800\n or something that has a body and tries to explore the world?\n\n1:29:56.800 --> 1:29:58.960\n Is it in the reinforcement learning space,\n\n1:29:58.960 --> 1:30:01.000\n like the efforts with AlphaZero and AlphaStar\n\n1:30:01.000 --> 1:30:04.360\n that are kind of exploring how you can solve it through\n\n1:30:04.360 --> 1:30:06.720\n in the simulation in the gaming world?\n\n1:30:06.720 --> 1:30:11.440\n Is there stuff in sort of all the transformer work\n\n1:30:11.440 --> 1:30:13.200\n and natural English processing,\n\n1:30:13.200 --> 1:30:15.800\n sort of maybe attacking the open domain dialogue?\n\n1:30:15.800 --> 1:30:18.720\n Like what, where do you see a promising pathways?\n\n1:30:21.560 --> 1:30:24.520\n Let me pick the embodiment maybe.\n\n1:30:24.520 --> 1:30:29.520\n So embodiment is important, yes and no.\n\n1:30:33.160 --> 1:30:38.160\n I don't believe that we need a physical robot\n\n1:30:38.600 --> 1:30:42.080\n walking or rolling around, interacting with the real world\n\n1:30:42.960 --> 1:30:45.080\n in order to achieve AGI.\n\n1:30:45.080 --> 1:30:50.080\n And I think it's more of a distraction probably\n\n1:30:50.600 --> 1:30:54.560\n than helpful, it's sort of confusing the body with the mind.\n\n1:30:54.560 --> 1:30:58.920\n For industrial applications or near term applications,\n\n1:30:58.920 --> 1:31:01.200\n of course we need robots for all kinds of things,\n\n1:31:01.200 --> 1:31:06.200\n but for solving the big problem, at least at this stage,\n\n1:31:06.240 --> 1:31:08.120\n I think it's not necessary.\n\n1:31:08.120 --> 1:31:10.080\n But the answer is also yes,\n\n1:31:10.080 --> 1:31:13.240\n that I think the most promising approach\n\n1:31:13.240 --> 1:31:15.280\n is that you have an agent\n\n1:31:15.280 --> 1:31:18.480\n and that can be a virtual agent in a computer\n\n1:31:18.480 --> 1:31:20.120\n interacting with an environment,\n\n1:31:20.120 --> 1:31:22.560\n possibly a 3D simulated environment\n\n1:31:22.560 --> 1:31:24.120\n like in many computer games.\n\n1:31:25.320 --> 1:31:28.880\n And you train and learn the agent,\n\n1:31:29.760 --> 1:31:33.120\n even if you don't intend to later put it sort of,\n\n1:31:33.120 --> 1:31:35.560\n this algorithm in a robot brain\n\n1:31:35.560 --> 1:31:38.560\n and leave it forever in the virtual reality,\n\n1:31:38.560 --> 1:31:40.520\n getting experience in a,\n\n1:31:40.520 --> 1:31:43.680\n although it's just simulated 3D world,\n\n1:31:45.400 --> 1:31:47.960\n is possibly, and I say possibly,\n\n1:31:47.960 --> 1:31:51.600\n important to understand things\n\n1:31:51.600 --> 1:31:53.880\n on a similar level as humans do,\n\n1:31:55.120 --> 1:31:58.560\n especially if the agent or primarily if the agent\n\n1:31:58.560 --> 1:32:00.320\n needs to interact with the humans.\n\n1:32:00.320 --> 1:32:02.960\n If you talk about objects on top of each other in space\n\n1:32:02.960 --> 1:32:04.760\n and flying and cars and so on,\n\n1:32:04.760 --> 1:32:06.400\n and the agent has no experience\n\n1:32:06.400 --> 1:32:09.560\n with even virtual 3D worlds,\n\n1:32:09.560 --> 1:32:11.120\n it's probably hard to grasp.\n\n1:32:12.320 --> 1:32:14.520\n So if you develop an abstract agent,\n\n1:32:14.520 --> 1:32:16.720\n say we take the mathematical path\n\n1:32:16.720 --> 1:32:18.320\n and we just want to build an agent\n\n1:32:18.320 --> 1:32:19.480\n which can prove theorems\n\n1:32:19.480 --> 1:32:21.760\n and becomes a better and better mathematician,\n\n1:32:21.760 --> 1:32:24.520\n then this agent needs to be able to reason\n\n1:32:24.520 --> 1:32:25.960\n in very abstract spaces\n\n1:32:25.960 --> 1:32:28.920\n and then maybe sort of putting it into 3D environments,\n\n1:32:28.920 --> 1:32:30.480\n simulated or not is even harmful.\n\n1:32:30.480 --> 1:32:33.400\n It should sort of, you put it in, I don't know,\n\n1:32:33.400 --> 1:32:35.840\n an environment which it creates itself or so.\n\n1:32:36.680 --> 1:32:38.760\n It seems like you have a interesting, rich,\n\n1:32:38.760 --> 1:32:40.680\n complex trajectory through life\n\n1:32:40.680 --> 1:32:42.680\n in terms of your journey of ideas.\n\n1:32:42.680 --> 1:32:45.760\n So it's interesting to ask what books,\n\n1:32:45.760 --> 1:32:48.120\n technical, fiction, philosophical,\n\n1:32:49.080 --> 1:32:52.680\n books, ideas, people had a transformative effect.\n\n1:32:52.680 --> 1:32:53.800\n Books are most interesting\n\n1:32:53.800 --> 1:32:57.280\n because maybe people could also read those books\n\n1:32:57.280 --> 1:33:00.120\n and see if they could be inspired as well.\n\n1:33:00.120 --> 1:33:03.520\n Yeah, luckily I asked books and not singular book.\n\n1:33:03.520 --> 1:33:08.120\n It's very hard and I try to pin down one book.\n\n1:33:08.120 --> 1:33:10.520\n And I can do that at the end.\n\n1:33:10.520 --> 1:33:14.200\n So the most,\n\n1:33:14.200 --> 1:33:16.360\n the books which were most transformative for me\n\n1:33:16.360 --> 1:33:19.600\n or which I can most highly recommend\n\n1:33:19.600 --> 1:33:21.920\n to people interested in AI.\n\n1:33:21.920 --> 1:33:22.880\n Both perhaps.\n\n1:33:22.880 --> 1:33:25.440\n Yeah, yeah, both, both, yeah, yeah.\n\n1:33:25.440 --> 1:33:28.560\n I would always start with Russell and Norvig,\n\n1:33:28.560 --> 1:33:30.880\n Artificial Intelligence, A Modern Approach.\n\n1:33:30.880 --> 1:33:33.400\n That's the AI Bible.\n\n1:33:33.400 --> 1:33:35.000\n It's an amazing book.\n\n1:33:35.000 --> 1:33:36.320\n It's very broad.\n\n1:33:36.320 --> 1:33:38.800\n It covers all approaches to AI.\n\n1:33:38.800 --> 1:33:40.840\n And even if you focused on one approach,\n\n1:33:40.840 --> 1:33:42.520\n I think that is the minimum you should know\n\n1:33:42.520 --> 1:33:44.600\n about the other approaches out there.\n\n1:33:44.600 --> 1:33:46.200\n So that should be your first book.\n\n1:33:46.200 --> 1:33:48.320\n Fourth edition should be coming out soon.\n\n1:33:48.320 --> 1:33:50.040\n Oh, okay, interesting.\n\n1:33:50.040 --> 1:33:51.480\n There's a deep learning chapter now,\n\n1:33:51.480 --> 1:33:53.080\n so there must be.\n\n1:33:53.080 --> 1:33:55.560\n Written by Ian Goodfellow, okay.\n\n1:33:55.560 --> 1:33:59.680\n And then the next book I would recommend,\n\n1:33:59.680 --> 1:34:02.920\n The Reinforcement Learning Book by Satneen Barto.\n\n1:34:02.920 --> 1:34:04.440\n That's a beautiful book.\n\n1:34:04.440 --> 1:34:06.920\n If there's any problem with the book,\n\n1:34:06.920 --> 1:34:11.920\n it makes RL feel and look much easier than it actually is.\n\n1:34:12.920 --> 1:34:14.800\n It's very gentle book.\n\n1:34:14.800 --> 1:34:16.760\n It's very nice to read, the exercises to do.\n\n1:34:16.760 --> 1:34:19.520\n You can very quickly get some RL systems to run.\n\n1:34:19.520 --> 1:34:22.520\n You know, very toy problems, but it's a lot of fun.\n\n1:34:22.520 --> 1:34:27.520\n And in a couple of days you feel you know what RL is about,\n\n1:34:28.120 --> 1:34:30.560\n but it's much harder than the book.\n\n1:34:30.560 --> 1:34:31.400\n Yeah.\n\n1:34:31.400 --> 1:34:34.840\n Oh, come on now, it's an awesome book.\n\n1:34:34.840 --> 1:34:36.240\n Yeah, it is, yeah.\n\n1:34:36.240 --> 1:34:41.240\n And maybe, I mean, there's so many books out there.\n\n1:34:41.480 --> 1:34:43.440\n If you like the information theoretic approach,\n\n1:34:43.440 --> 1:34:46.760\n then there's Kolmogorov Complexity by Alin Vitani,\n\n1:34:46.760 --> 1:34:50.800\n but probably, you know, some short article is enough.\n\n1:34:50.800 --> 1:34:52.120\n You don't need to read a whole book,\n\n1:34:52.120 --> 1:34:54.440\n but it's a great book.\n\n1:34:54.440 --> 1:34:59.440\n And if you have to mention one all time favorite book,\n\n1:34:59.440 --> 1:35:01.880\n it's of different flavor, that's a book\n\n1:35:01.880 --> 1:35:04.800\n which is used in the International Baccalaureate\n\n1:35:04.800 --> 1:35:08.560\n for high school students in several countries.\n\n1:35:08.560 --> 1:35:12.520\n That's from Nicholas Alchin, Theory of Knowledge,\n\n1:35:12.520 --> 1:35:16.120\n second edition or first, not the third, please.\n\n1:35:16.120 --> 1:35:18.480\n The third one, they took out all the fun.\n\n1:35:18.480 --> 1:35:20.240\n Okay.\n\n1:35:20.240 --> 1:35:25.240\n So this asks all the interesting,\n\n1:35:25.240 --> 1:35:27.200\n or to me, interesting philosophical questions\n\n1:35:27.200 --> 1:35:30.040\n about how we acquire knowledge from all perspectives,\n\n1:35:30.040 --> 1:35:31.840\n from math, from art, from physics,\n\n1:35:33.400 --> 1:35:36.240\n and ask how can we know anything?\n\n1:35:36.240 --> 1:35:38.040\n And the book is called Theory of Knowledge.\n\n1:35:38.040 --> 1:35:40.720\n From which, is this almost like a philosophical exploration\n\n1:35:40.720 --> 1:35:43.160\n of how we get knowledge from anything?\n\n1:35:43.160 --> 1:35:45.160\n Yes, yeah, I mean, can religion tell us, you know,\n\n1:35:45.160 --> 1:35:46.200\n about something about the world?\n\n1:35:46.200 --> 1:35:48.080\n Can science tell us something about the world?\n\n1:35:48.080 --> 1:35:50.680\n Can mathematics, or is it just playing with symbols?\n\n1:35:51.920 --> 1:35:54.400\n And, you know, it's open ended questions.\n\n1:35:54.400 --> 1:35:56.240\n And, I mean, it's for high school students,\n\n1:35:56.240 --> 1:35:58.320\n so they have then resources from Hitchhiker's Guide\n\n1:35:58.320 --> 1:35:59.960\n to the Galaxy and from Star Wars\n\n1:35:59.960 --> 1:36:01.800\n and The Chicken Crossed the Road, yeah.\n\n1:36:01.800 --> 1:36:05.960\n And it's fun to read, but it's also quite deep.\n\n1:36:07.600 --> 1:36:11.480\n If you could live one day of your life over again,\n\n1:36:11.480 --> 1:36:12.840\n has it made you truly happy?\n\n1:36:12.840 --> 1:36:14.440\n Or maybe like we said with the books,\n\n1:36:14.440 --> 1:36:16.240\n it was truly transformative.\n\n1:36:16.240 --> 1:36:19.120\n What day, what moment would you choose\n\n1:36:19.120 --> 1:36:20.800\n that something pop into your mind?\n\n1:36:22.080 --> 1:36:23.480\n Does it need to be a day in the past,\n\n1:36:23.480 --> 1:36:25.920\n or can it be a day in the future?\n\n1:36:25.920 --> 1:36:27.960\n Well, space time is an emergent phenomena,\n\n1:36:27.960 --> 1:36:30.400\n so it's all the same anyway.\n\n1:36:30.400 --> 1:36:32.040\n Okay.\n\n1:36:32.040 --> 1:36:33.360\n Okay, from the past.\n\n1:36:34.280 --> 1:36:36.800\n You're really good at saying from the future, I love it.\n\n1:36:36.800 --> 1:36:39.120\n No, I will tell you from the future, okay.\n\n1:36:39.120 --> 1:36:41.480\n So from the past, I would say\n\n1:36:41.480 --> 1:36:43.800\n when I discovered my Axie model.\n\n1:36:43.800 --> 1:36:45.160\n I mean, it was not in one day,\n\n1:36:45.160 --> 1:36:48.880\n but it was one moment where I realized\n\n1:36:48.880 --> 1:36:53.200\n Kolmogorov complexity and didn't even know that it existed,\n\n1:36:53.200 --> 1:36:55.800\n but I discovered sort of this compression idea\n\n1:36:55.800 --> 1:36:58.120\n myself, but immediately I knew I can't be the first one,\n\n1:36:58.120 --> 1:36:59.360\n but I had this idea.\n\n1:37:00.240 --> 1:37:02.200\n And then I knew about sequential decisionry,\n\n1:37:02.200 --> 1:37:06.360\n and I knew if I put it together, this is the right thing.\n\n1:37:06.360 --> 1:37:09.680\n And yeah, still when I think back about this moment,\n\n1:37:09.680 --> 1:37:12.400\n I'm super excited about it.\n\n1:37:12.400 --> 1:37:16.320\n Was there any more details and context that moment?\n\n1:37:16.320 --> 1:37:18.040\n Did an apple fall on your head?\n\n1:37:20.120 --> 1:37:21.960\n So it was like, if you look at Ian Goodfellow\n\n1:37:21.960 --> 1:37:25.920\n talking about GANs, there was beer involved.\n\n1:37:25.920 --> 1:37:30.200\n Is there some more context of what sparked your thought,\n\n1:37:30.200 --> 1:37:31.200\n or was it just?\n\n1:37:31.200 --> 1:37:32.960\n No, it was much more mundane.\n\n1:37:32.960 --> 1:37:34.560\n So I worked in this company.\n\n1:37:34.560 --> 1:37:36.160\n So in this sense, the four and a half years\n\n1:37:36.160 --> 1:37:37.640\n was not completely wasted.\n\n1:37:39.320 --> 1:37:43.720\n And I worked on an image interpolation problem,\n\n1:37:43.720 --> 1:37:48.480\n and I developed a quite neat new interpolation techniques\n\n1:37:48.480 --> 1:37:52.240\n and they got patented, which happens quite often.\n\n1:37:52.240 --> 1:37:54.360\n I got sort of overboard and thought about,\n\n1:37:54.360 --> 1:37:56.240\n yeah, that's pretty good, but it's not the best.\n\n1:37:56.240 --> 1:37:59.800\n So what is the best possible way of doing interpolation?\n\n1:37:59.800 --> 1:38:03.200\n And then I thought, yeah, you want the simplest picture,\n\n1:38:03.200 --> 1:38:04.760\n which is if you coarse grain it,\n\n1:38:04.760 --> 1:38:06.560\n recovers your original picture.\n\n1:38:06.560 --> 1:38:08.880\n And then I thought about the simplicity concept\n\n1:38:08.880 --> 1:38:11.280\n more in quantitative terms,\n\n1:38:11.280 --> 1:38:14.080\n and then everything developed.\n\n1:38:15.040 --> 1:38:17.120\n And somehow the four beautiful mix\n\n1:38:17.120 --> 1:38:18.920\n of also being a physicist\n\n1:38:18.920 --> 1:38:20.600\n and thinking about the big picture of it,\n\n1:38:20.600 --> 1:38:24.120\n then led you to probably think big with AIX.\n\n1:38:24.120 --> 1:38:26.200\n So as a physicist, I was probably trained\n\n1:38:26.200 --> 1:38:28.440\n not to always think in computational terms,\n\n1:38:28.440 --> 1:38:30.840\n just ignore that and think about\n\n1:38:30.840 --> 1:38:34.000\n the fundamental properties, which you want to have.\n\n1:38:34.000 --> 1:38:36.920\n So what about if you could really one day in the future?\n\n1:38:36.920 --> 1:38:39.880\n What would that be?\n\n1:38:39.880 --> 1:38:41.520\n When I solve the AGI problem.\n\n1:38:43.320 --> 1:38:45.120\n In practice, so in theory,\n\n1:38:45.120 --> 1:38:48.680\n I have solved it with the AIX model, but in practice.\n\n1:38:48.680 --> 1:38:50.720\n And then I ask the first question.\n\n1:38:50.720 --> 1:38:53.200\n What would be the first question?\n\n1:38:53.200 --> 1:38:54.600\n What's the meaning of life?\n\n1:38:55.680 --> 1:38:58.400\n I don't think there's a better way to end it.\n\n1:38:58.400 --> 1:38:59.240\n Thank you so much for talking today.\n\n1:38:59.240 --> 1:39:01.360\n It's a huge honor to finally meet you.\n\n1:39:01.360 --> 1:39:02.200\n Yeah, thank you too.\n\n1:39:02.200 --> 1:39:31.880\n It was a pleasure of mine too.\n\n1:39:33.160 --> 1:39:35.760\n And now let me leave you with some words of wisdom\n\n1:39:35.760 --> 1:39:38.000\n from Albert Einstein.\n\n1:39:38.000 --> 1:39:42.040\n The measure of intelligence is the ability to change.\n\n1:39:42.040 --> 1:40:01.600\n Thank you for listening and hope to see you next time.\n\n"
}