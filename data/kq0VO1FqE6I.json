{
  "title": "Rosalind Picard: Affective Computing, Emotion, Privacy, and Health | Lex Fridman Podcast #24",
  "id": "kq0VO1FqE6I",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:02.880\n The following is a conversation with Rosalind Picard.\n\n00:02.880 --> 00:04.540\n She's a professor at MIT,\n\n00:04.540 --> 00:06.880\n director of the Effective Computing Research Group\n\n00:06.880 --> 00:08.360\n at the MIT Media Lab,\n\n00:08.360 --> 00:12.440\n and cofounder of two companies, Affectiva and Empatica.\n\n00:12.440 --> 00:13.560\n Over two decades ago,\n\n00:13.560 --> 00:15.420\n she launched a field of effective computing\n\n00:15.420 --> 00:17.560\n with her book of the same name.\n\n00:17.560 --> 00:20.040\n This book described the importance of emotion\n\n00:20.040 --> 00:23.040\n in artificial and natural intelligence.\n\n00:23.040 --> 00:25.320\n The vital role of emotional communication\n\n00:25.320 --> 00:28.520\n has to the relationship between people in general\n\n00:28.520 --> 00:30.880\n and human robot interaction.\n\n00:30.880 --> 00:34.000\n I really enjoy talking with Ros over so many topics,\n\n00:34.000 --> 00:37.440\n including emotion, ethics, privacy, wearable computing,\n\n00:37.440 --> 00:39.680\n and her recent research in epilepsy,\n\n00:39.680 --> 00:42.600\n and even love and meaning.\n\n00:42.600 --> 00:43.960\n This conversation is part\n\n00:43.960 --> 00:46.000\n of the Artificial Intelligence Podcast.\n\n00:46.000 --> 00:48.720\n If you enjoy it, subscribe on YouTube, iTunes,\n\n00:48.720 --> 00:51.920\n or simply connect with me on Twitter at Lex Friedman,\n\n00:51.920 --> 00:53.960\n spelled F R I D.\n\n00:53.960 --> 00:58.960\n And now, here's my conversation with Rosalind Picard.\n\n00:59.480 --> 01:00.720\n More than 20 years ago,\n\n01:00.720 --> 01:03.320\n you've coined the term effective computing\n\n01:03.320 --> 01:06.680\n and led a lot of research in this area since then.\n\n01:06.680 --> 01:09.220\n As I understand, the goal is to make the machine detect\n\n01:09.220 --> 01:12.380\n and interpret the emotional state of a human being\n\n01:12.380 --> 01:14.200\n and adapt the behavior of the machine\n\n01:14.200 --> 01:16.120\n based on the emotional state.\n\n01:16.120 --> 01:19.920\n So how is your understanding of the problem space\n\n01:19.920 --> 01:24.920\n defined by effective computing changed in the past 24 years?\n\n01:25.360 --> 01:28.880\n So it's the scope, the applications, the challenges,\n\n01:28.880 --> 01:32.120\n what's involved, how has that evolved over the years?\n\n01:32.120 --> 01:33.400\n Yeah, actually, originally,\n\n01:33.400 --> 01:36.880\n when I defined the term affective computing,\n\n01:36.880 --> 01:40.120\n it was a bit broader than just recognizing\n\n01:40.120 --> 01:42.260\n and responding intelligently to human emotion,\n\n01:42.260 --> 01:44.520\n although those are probably the two pieces\n\n01:44.520 --> 01:47.120\n that we've worked on the hardest.\n\n01:47.120 --> 01:50.680\n The original concept also encompassed machines\n\n01:50.680 --> 01:52.480\n that would have mechanisms\n\n01:52.480 --> 01:55.680\n that functioned like human emotion does inside them.\n\n01:55.680 --> 01:59.000\n It would be any computing that relates to arises from\n\n01:59.000 --> 02:01.600\n or deliberately influences human emotion.\n\n02:02.560 --> 02:05.160\n So the human computer interaction part\n\n02:05.160 --> 02:07.880\n is the part that people tend to see,\n\n02:07.880 --> 02:11.000\n like if I'm really ticked off at my computer\n\n02:11.000 --> 02:13.480\n and I'm scowling at it and I'm cursing at it\n\n02:13.480 --> 02:15.720\n and it just keeps acting smiling and happy\n\n02:15.720 --> 02:17.880\n like that little paperclip used to do,\n\n02:17.880 --> 02:22.200\n dancing, winking, that kind of thing\n\n02:22.200 --> 02:24.640\n just makes you even more frustrated, right?\n\n02:24.640 --> 02:29.120\n And I thought that stupid thing needs to see my affect.\n\n02:29.120 --> 02:30.640\n And if it's gonna be intelligent,\n\n02:30.640 --> 02:33.000\n which Microsoft researchers had worked really hard on,\n\n02:33.000 --> 02:34.920\n it actually had some of the most sophisticated AI\n\n02:34.920 --> 02:36.240\n in it at the time,\n\n02:36.240 --> 02:38.000\n that thing's gonna actually be smart.\n\n02:38.000 --> 02:41.600\n It needs to respond to me and you,\n\n02:41.600 --> 02:45.360\n and we can send it very different signals.\n\n02:45.360 --> 02:47.160\n So by the way, just a quick interruption,\n\n02:47.160 --> 02:52.160\n the Clippy, maybe it's in Word 95, 98,\n\n02:52.600 --> 02:54.360\n I don't remember when it was born,\n\n02:54.360 --> 02:58.320\n but many people, do you find yourself with that reference\n\n02:58.320 --> 03:00.320\n that people recognize what you're talking about\n\n03:00.320 --> 03:01.680\n still to this point?\n\n03:01.680 --> 03:05.200\n I don't expect the newest students to these days,\n\n03:05.200 --> 03:07.200\n but I've mentioned it to a lot of audiences,\n\n03:07.200 --> 03:09.240\n like how many of you know this Clippy thing?\n\n03:09.240 --> 03:11.720\n And still the majority of people seem to know it.\n\n03:11.720 --> 03:15.340\n So Clippy kind of looks at maybe natural language processing\n\n03:15.340 --> 03:18.200\n where you were typing and tries to help you complete,\n\n03:18.200 --> 03:19.280\n I think.\n\n03:19.280 --> 03:22.520\n I don't even remember what Clippy was, except annoying.\n\n03:22.520 --> 03:25.840\n Yeah, some people actually liked it.\n\n03:25.840 --> 03:27.520\n I would hear those stories.\n\n03:27.520 --> 03:28.480\n You miss it?\n\n03:28.480 --> 03:31.300\n Well, I miss the annoyance.\n\n03:31.300 --> 03:34.080\n They felt like there's an element.\n\n03:34.080 --> 03:34.920\n Someone was there.\n\n03:34.920 --> 03:36.960\n Somebody was there and we were in it together\n\n03:36.960 --> 03:37.800\n and they were annoying.\n\n03:37.800 --> 03:40.880\n It's like a puppy that just doesn't get it.\n\n03:40.880 --> 03:42.200\n They keep stripping up the couch kind of thing.\n\n03:42.200 --> 03:44.960\n And in fact, they could have done it smarter like a puppy.\n\n03:44.960 --> 03:48.000\n If they had done, like if when you yelled at it\n\n03:48.000 --> 03:49.040\n or cursed at it,\n\n03:49.040 --> 03:51.800\n if it had put its little ears back in its tail down\n\n03:51.800 --> 03:52.960\n and shrugged off,\n\n03:52.960 --> 03:55.900\n probably people would have wanted it back, right?\n\n03:55.900 --> 03:58.600\n But instead, when you yelled at it, what did it do?\n\n03:58.600 --> 04:01.260\n It smiled, it winked, it danced, right?\n\n04:01.260 --> 04:03.200\n If somebody comes to my office and I yell at them,\n\n04:03.200 --> 04:04.760\n they start smiling, winking and dancing.\n\n04:04.760 --> 04:06.760\n I'm like, I never want to see you again.\n\n04:06.760 --> 04:08.520\n So Bill Gates got a standing ovation\n\n04:08.520 --> 04:10.160\n when he said it was going away\n\n04:10.160 --> 04:12.360\n because people were so ticked.\n\n04:12.360 --> 04:15.040\n It was so emotionally unintelligent, right?\n\n04:15.040 --> 04:18.160\n It was intelligent about whether you were writing a letter,\n\n04:18.160 --> 04:20.880\n what kind of help you needed for that context.\n\n04:20.880 --> 04:23.440\n It was completely unintelligent about,\n\n04:23.440 --> 04:25.760\n hey, if you're annoying your customer,\n\n04:25.760 --> 04:28.400\n don't smile in their face when you do it.\n\n04:28.400 --> 04:32.360\n So that kind of mismatch was something\n\n04:32.360 --> 04:35.080\n the developers just didn't think about.\n\n04:35.080 --> 04:39.520\n And intelligence at the time was really all about math\n\n04:39.520 --> 04:44.520\n and language and chess and games,\n\n04:44.960 --> 04:47.920\n problems that could be pretty well defined.\n\n04:47.920 --> 04:50.880\n Social emotional interaction is much more complex\n\n04:50.880 --> 04:53.640\n than chess or Go or any of the games\n\n04:53.640 --> 04:56.060\n that people are trying to solve.\n\n04:56.060 --> 04:58.720\n And in order to understand that required skills\n\n04:58.720 --> 05:00.320\n that most people in computer science\n\n05:00.320 --> 05:02.600\n actually were lacking personally.\n\n05:02.600 --> 05:03.800\n Well, let's talk about computer science.\n\n05:03.800 --> 05:06.400\n Have things gotten better since the work,\n\n05:06.400 --> 05:07.920\n since the message,\n\n05:07.920 --> 05:09.520\n since you've really launched the field\n\n05:09.520 --> 05:11.320\n with a lot of research work in this space?\n\n05:11.320 --> 05:14.080\n I still find as a person like yourself,\n\n05:14.080 --> 05:16.680\n who's deeply passionate about human beings\n\n05:16.680 --> 05:18.860\n and yet am in computer science,\n\n05:18.860 --> 05:20.720\n there still seems to be a lack of,\n\n05:22.440 --> 05:26.800\n sorry to say empathy in as computer scientists.\n\n05:26.800 --> 05:27.800\n Yeah, well.\n\n05:27.800 --> 05:28.880\n Or hasn't gotten better.\n\n05:28.880 --> 05:30.720\n Let's just say there's a lot more variety\n\n05:30.720 --> 05:32.400\n among computer scientists these days.\n\n05:32.400 --> 05:35.000\n Computer scientists are a much more diverse group today\n\n05:35.000 --> 05:37.600\n than they were 25 years ago.\n\n05:37.600 --> 05:39.000\n And that's good.\n\n05:39.000 --> 05:41.760\n We need all kinds of people to become computer scientists\n\n05:41.760 --> 05:45.580\n so that computer science reflects more what society needs.\n\n05:45.580 --> 05:49.080\n And there's brilliance among every personality type.\n\n05:49.080 --> 05:52.000\n So it need not be limited to people\n\n05:52.000 --> 05:54.080\n who prefer computers to other people.\n\n05:54.080 --> 05:55.800\n How hard do you think it is?\n\n05:55.800 --> 05:58.580\n Your view of how difficult it is to recognize emotion\n\n05:58.580 --> 06:03.580\n or to create a deeply emotionally intelligent interaction.\n\n06:03.920 --> 06:06.000\n Has it gotten easier or harder\n\n06:06.000 --> 06:07.440\n as you've explored it further?\n\n06:07.440 --> 06:10.020\n And how far away are we from cracking this?\n\n06:12.400 --> 06:16.040\n If you think of the Turing test solving the intelligence,\n\n06:16.040 --> 06:18.740\n looking at the Turing test for emotional intelligence.\n\n06:20.720 --> 06:25.560\n I think it is as difficult as I thought it was gonna be.\n\n06:25.560 --> 06:29.240\n I think my prediction of its difficulty is spot on.\n\n06:29.240 --> 06:33.120\n I think the time estimates are always hard\n\n06:33.120 --> 06:37.280\n because they're always a function of society's love\n\n06:37.280 --> 06:39.440\n and hate of a particular topic.\n\n06:39.440 --> 06:44.440\n If society gets excited and you get thousands of researchers\n\n06:45.200 --> 06:49.000\n working on it for a certain application,\n\n06:49.000 --> 06:52.000\n that application gets solved really quickly.\n\n06:52.000 --> 06:54.320\n The general intelligence,\n\n06:54.320 --> 06:58.120\n the computer's complete lack of ability\n\n06:58.120 --> 07:03.120\n to have awareness of what it's doing,\n\n07:03.480 --> 07:05.480\n the fact that it's not conscious,\n\n07:05.480 --> 07:08.580\n the fact that there's no signs of it becoming conscious,\n\n07:08.580 --> 07:11.800\n the fact that it doesn't read between the lines,\n\n07:11.800 --> 07:15.000\n those kinds of things that we have to teach it explicitly,\n\n07:15.000 --> 07:17.440\n what other people pick up implicitly.\n\n07:17.440 --> 07:20.360\n We don't see that changing yet.\n\n07:20.360 --> 07:23.540\n There aren't breakthroughs yet that lead us to believe\n\n07:23.540 --> 07:25.280\n that that's gonna go any faster,\n\n07:25.280 --> 07:28.640\n which means that it's still gonna be kind of stuck\n\n07:28.640 --> 07:31.240\n with a lot of limitations\n\n07:31.240 --> 07:34.000\n where it's probably only gonna do the right thing\n\n07:34.000 --> 07:37.120\n in very limited, narrow, prespecified contexts\n\n07:37.120 --> 07:40.880\n where we can prescribe pretty much\n\n07:40.880 --> 07:42.800\n what's gonna happen there.\n\n07:42.800 --> 07:44.820\n So I don't see the,\n\n07:46.920 --> 07:47.960\n it's hard to predict a date\n\n07:47.960 --> 07:51.720\n because when people don't work on it, it's infinite.\n\n07:51.720 --> 07:54.520\n When everybody works on it, you get a nice piece of it\n\n07:56.000 --> 07:58.560\n well solved in a short amount of time.\n\n07:58.560 --> 08:01.520\n I actually think there's a more important issue right now\n\n08:01.520 --> 08:04.480\n than the difficulty of it.\n\n08:04.480 --> 08:05.760\n And that's causing some of us\n\n08:05.760 --> 08:07.360\n to put the brakes on a little bit.\n\n08:07.360 --> 08:09.320\n Usually we're all just like step on the gas,\n\n08:09.320 --> 08:11.120\n let's go faster.\n\n08:11.120 --> 08:14.160\n This is causing us to pull back and put the brakes on.\n\n08:14.160 --> 08:18.640\n And that's the way that some of this technology\n\n08:18.640 --> 08:21.160\n is being used in places like China right now.\n\n08:21.160 --> 08:24.480\n And that worries me so deeply\n\n08:24.480 --> 08:27.760\n that it's causing me to pull back myself\n\n08:27.760 --> 08:30.040\n on a lot of the things that we could be doing.\n\n08:30.040 --> 08:33.640\n And try to get the community to think a little bit more\n\n08:33.640 --> 08:36.000\n about, okay, if we're gonna go forward with that,\n\n08:36.000 --> 08:39.240\n how can we do it in a way that puts in place safeguards\n\n08:39.240 --> 08:41.080\n that protects people?\n\n08:41.080 --> 08:43.480\n So the technology we're referring to is\n\n08:43.480 --> 08:46.360\n just when a computer senses the human being,\n\n08:46.360 --> 08:48.560\n like the human face, right?\n\n08:48.560 --> 08:51.800\n So there's a lot of exciting things there,\n\n08:51.800 --> 08:53.880\n like forming a deep connection with the human being.\n\n08:53.880 --> 08:56.800\n So what are your worries, how that could go wrong?\n\n08:57.920 --> 08:59.400\n Is it in terms of privacy?\n\n08:59.400 --> 09:02.880\n Is it in terms of other kinds of more subtle things?\n\n09:02.880 --> 09:04.200\n But let's dig into privacy.\n\n09:04.200 --> 09:07.680\n So here in the US, if I'm watching a video\n\n09:07.680 --> 09:09.760\n of say a political leader,\n\n09:09.760 --> 09:13.520\n and in the US we're quite free as we all know\n\n09:13.520 --> 09:17.800\n to even criticize the president of the United States, right?\n\n09:17.800 --> 09:19.320\n Here that's not a shocking thing.\n\n09:19.320 --> 09:22.600\n It happens about every five seconds, right?\n\n09:22.600 --> 09:27.600\n But in China, what happens if you criticize\n\n09:27.600 --> 09:30.800\n the leader of the government, right?\n\n09:30.800 --> 09:34.080\n And so people are very careful not to do that.\n\n09:34.080 --> 09:37.600\n However, what happens if you're simply watching a video\n\n09:37.600 --> 09:40.760\n and you make a facial expression\n\n09:40.760 --> 09:45.000\n that shows a little bit of skepticism, right?\n\n09:45.000 --> 09:47.920\n Well, and here we're completely free to do that.\n\n09:47.920 --> 09:50.440\n In fact, we're free to fly off the handle\n\n09:50.440 --> 09:54.440\n and say anything we want, usually.\n\n09:54.440 --> 09:56.280\n I mean, there are some restrictions\n\n09:56.280 --> 09:58.800\n when the athlete does this\n\n09:58.800 --> 10:00.800\n as part of the national broadcast.\n\n10:00.800 --> 10:03.800\n Maybe the teams get a little unhappy\n\n10:03.800 --> 10:05.840\n about picking that forum to do it, right?\n\n10:05.840 --> 10:08.680\n But that's more a question of judgment.\n\n10:08.680 --> 10:11.520\n We have these freedoms,\n\n10:11.520 --> 10:14.120\n and in places that don't have those freedoms,\n\n10:14.120 --> 10:17.040\n what if our technology can read\n\n10:17.040 --> 10:19.560\n your underlying affective state?\n\n10:19.560 --> 10:22.400\n What if our technology can read it even noncontact?\n\n10:22.400 --> 10:24.400\n What if our technology can read it\n\n10:24.400 --> 10:28.800\n without your prior consent?\n\n10:28.800 --> 10:30.360\n And here in the US,\n\n10:30.360 --> 10:32.920\n in my first company we started, Affectiva,\n\n10:32.920 --> 10:35.560\n we have worked super hard to turn away money\n\n10:35.560 --> 10:38.400\n and opportunities that try to read people's affect\n\n10:38.400 --> 10:41.320\n without their prior informed consent.\n\n10:41.320 --> 10:45.120\n And even the software that is licensable,\n\n10:45.120 --> 10:46.680\n you have to sign things saying\n\n10:46.680 --> 10:48.360\n you will only use it in certain ways,\n\n10:48.360 --> 10:52.080\n which essentially is get people's buy in, right?\n\n10:52.080 --> 10:55.360\n Don't do this without people agreeing to it.\n\n10:56.760 --> 10:58.560\n There are other countries where they're not interested\n\n10:58.560 --> 10:59.520\n in people's buy in.\n\n10:59.520 --> 11:01.400\n They're just gonna use it.\n\n11:01.400 --> 11:03.000\n They're gonna inflict it on you.\n\n11:03.000 --> 11:04.400\n And if you don't like it,\n\n11:04.400 --> 11:08.440\n you better not scowl in the direction of any censors.\n\n11:08.440 --> 11:11.400\n So one, let me just comment on a small tangent.\n\n11:11.400 --> 11:15.920\n Do you know with the idea of adversarial examples\n\n11:15.920 --> 11:17.360\n and deep fakes and so on,\n\n11:18.760 --> 11:20.760\n what you bring up is actually,\n\n11:20.760 --> 11:23.680\n in that one sense, deep fakes provide\n\n11:23.680 --> 11:28.680\n a comforting protection that you can no longer really trust\n\n11:30.640 --> 11:34.560\n that the video of your face was legitimate.\n\n11:34.560 --> 11:37.040\n And therefore you always have an escape clause\n\n11:37.040 --> 11:38.440\n if a government is trying,\n\n11:38.440 --> 11:43.440\n if a stable, balanced, ethical government\n\n11:44.800 --> 11:46.200\n is trying to accuse you of something,\n\n11:46.200 --> 11:47.080\n at least you have protection.\n\n11:47.080 --> 11:50.600\n You can say it was fake news, as is a popular term now.\n\n11:50.600 --> 11:52.360\n Yeah, that's the general thinking of it.\n\n11:52.360 --> 11:54.360\n We know how to go into the video\n\n11:54.360 --> 11:58.360\n and see, for example, your heart rate and respiration\n\n11:58.360 --> 12:02.200\n and whether or not they've been tampered with.\n\n12:02.200 --> 12:05.520\n And we also can put like fake heart rate and respiration\n\n12:05.520 --> 12:06.680\n in your video now too.\n\n12:06.680 --> 12:08.560\n We decided we needed to do that.\n\n12:10.440 --> 12:12.640\n After we developed a way to extract it,\n\n12:12.640 --> 12:15.920\n we decided we also needed a way to jam it.\n\n12:15.920 --> 12:20.880\n And so the fact that we took time to do that other step too,\n\n12:20.880 --> 12:22.520\n that was time that I wasn't spending\n\n12:22.520 --> 12:25.240\n making the machine more affectively intelligent.\n\n12:25.240 --> 12:28.480\n And there's a choice in how we spend our time,\n\n12:28.480 --> 12:32.400\n which is now being swayed a little bit less by this goal\n\n12:32.400 --> 12:34.320\n and a little bit more like by concern\n\n12:34.320 --> 12:36.560\n about what's happening in society\n\n12:36.560 --> 12:38.840\n and what kind of future do we wanna build.\n\n12:38.840 --> 12:41.640\n And as we step back and say,\n\n12:41.640 --> 12:44.560\n okay, we don't just build AI to build AI\n\n12:44.560 --> 12:46.480\n to make Elon Musk more money\n\n12:46.480 --> 12:48.760\n or to make Amazon Jeff Bezos more money.\n\n12:48.760 --> 12:52.840\n Good gosh, you know, that's the wrong ethic.\n\n12:52.840 --> 12:54.080\n Why are we building it?\n\n12:54.080 --> 12:57.160\n What is the point of building AI?\n\n12:57.160 --> 13:01.520\n It used to be, it was driven by researchers in academia\n\n13:01.520 --> 13:04.120\n to get papers published and to make a career for themselves\n\n13:04.120 --> 13:05.760\n and to do something cool, right?\n\n13:05.760 --> 13:07.600\n Like, cause maybe it could be done.\n\n13:08.480 --> 13:12.440\n Now we realize that this is enabling rich people\n\n13:12.440 --> 13:17.200\n to get vastly richer, the poor are,\n\n13:17.200 --> 13:19.760\n the divide is even larger.\n\n13:19.760 --> 13:22.840\n And is that the kind of future that we want?\n\n13:22.840 --> 13:25.880\n Maybe we wanna think about, maybe we wanna rethink AI.\n\n13:25.880 --> 13:29.080\n Maybe we wanna rethink the problems in society\n\n13:29.080 --> 13:32.720\n that are causing the greatest inequity\n\n13:32.720 --> 13:35.000\n and rethink how to build AI\n\n13:35.000 --> 13:36.720\n that's not about a general intelligence,\n\n13:36.720 --> 13:39.280\n but that's about extending the intelligence\n\n13:39.280 --> 13:41.200\n and capability of the have nots\n\n13:41.200 --> 13:43.760\n so that we close these gaps in society.\n\n13:43.760 --> 13:46.600\n Do you hope that kind of stepping on the brake\n\n13:46.600 --> 13:47.920\n happens organically?\n\n13:47.920 --> 13:51.160\n Because I think still majority of the force behind AI\n\n13:51.160 --> 13:52.720\n is the desire to publish papers,\n\n13:52.720 --> 13:55.480\n is to make money without thinking about the why.\n\n13:55.480 --> 13:57.200\n Do you hope it happens organically?\n\n13:57.200 --> 13:58.920\n Is there room for regulation?\n\n14:01.040 --> 14:02.920\n Yeah, yeah, yeah, great questions.\n\n14:02.920 --> 14:05.920\n I prefer the, you know,\n\n14:05.920 --> 14:07.320\n they talk about the carrot versus the stick.\n\n14:07.320 --> 14:09.120\n I definitely prefer the carrot to the stick.\n\n14:09.120 --> 14:12.360\n And, you know, in our free world,\n\n14:12.360 --> 14:14.880\n we, there's only so much stick, right?\n\n14:14.880 --> 14:17.240\n You're gonna find a way around it.\n\n14:17.240 --> 14:21.160\n I generally think less regulation is better.\n\n14:21.160 --> 14:24.400\n That said, even though my position is classically carrot,\n\n14:24.400 --> 14:26.240\n no stick, no regulation,\n\n14:26.240 --> 14:29.040\n I think we do need some regulations in this space.\n\n14:29.040 --> 14:30.680\n I do think we need regulations\n\n14:30.680 --> 14:33.560\n around protecting people with their data,\n\n14:33.560 --> 14:38.160\n that you own your data, not Amazon, not Google.\n\n14:38.160 --> 14:40.760\n I would like to see people own their own data.\n\n14:40.760 --> 14:42.440\n I would also like to see the regulations\n\n14:42.440 --> 14:44.480\n that we have right now around lie detection\n\n14:44.480 --> 14:48.120\n being extended to emotion recognition in general,\n\n14:48.120 --> 14:50.960\n that right now you can't use a lie detector on an employee\n\n14:50.960 --> 14:52.680\n when you're, on a candidate\n\n14:52.680 --> 14:54.640\n when you're interviewing them for a job.\n\n14:54.640 --> 14:57.720\n I think similarly, we need to put in place protection\n\n14:57.720 --> 15:00.520\n around reading people's emotions without their consent\n\n15:00.520 --> 15:02.120\n and in certain cases,\n\n15:02.120 --> 15:06.080\n like characterizing them for a job and other opportunities.\n\n15:06.080 --> 15:09.120\n So I'm also, I also think that when we're reading emotion\n\n15:09.120 --> 15:11.640\n that's predictive around mental health,\n\n15:11.640 --> 15:14.120\n that that should, even though it's not medical data,\n\n15:14.120 --> 15:16.040\n that that should get the kinds of protections\n\n15:16.040 --> 15:18.440\n that our medical data gets.\n\n15:18.440 --> 15:19.960\n What most people don't know yet\n\n15:19.960 --> 15:22.560\n is right now with your smartphone use,\n\n15:22.560 --> 15:25.160\n and if you're wearing a sensor\n\n15:25.160 --> 15:27.680\n and you wanna learn about your stress and your sleep\n\n15:27.680 --> 15:28.960\n and your physical activity\n\n15:28.960 --> 15:30.760\n and how much you're using your phone\n\n15:30.760 --> 15:32.560\n and your social interaction,\n\n15:32.560 --> 15:34.880\n all of that nonmedical data,\n\n15:34.880 --> 15:37.880\n when we put it together with machine learning,\n\n15:37.880 --> 15:40.080\n now called AI, even though the founders of AI\n\n15:40.080 --> 15:41.640\n wouldn't have called it that,\n\n15:42.840 --> 15:47.840\n that capability can not only tell that you're calm right now\n\n15:48.360 --> 15:50.760\n or that you're getting a little stressed,\n\n15:50.760 --> 15:53.840\n but it can also predict how you're likely to be tomorrow.\n\n15:53.840 --> 15:55.760\n If you're likely to be sick or healthy,\n\n15:55.760 --> 15:58.640\n happy or sad, stressed or calm.\n\n15:58.640 --> 16:00.560\n Especially when you're tracking data over time.\n\n16:00.560 --> 16:03.680\n Especially when we're tracking a week of your data or more.\n\n16:03.680 --> 16:05.600\n Do you have an optimism towards,\n\n16:05.600 --> 16:07.720\n you know, a lot of people on our phones\n\n16:07.720 --> 16:10.280\n are worried about this camera that's looking at us.\n\n16:10.280 --> 16:12.480\n For the most part, on balance,\n\n16:12.480 --> 16:16.000\n are you optimistic about the benefits\n\n16:16.000 --> 16:17.400\n that can be brought from that camera\n\n16:17.400 --> 16:19.560\n that's looking at billions of us?\n\n16:19.560 --> 16:22.000\n Or should we be more worried?\n\n16:24.520 --> 16:28.840\n I think we should be a little bit more worried\n\n16:28.840 --> 16:32.480\n about who's looking at us and listening to us.\n\n16:32.480 --> 16:36.680\n The device sitting on your countertop in your kitchen,\n\n16:36.680 --> 16:41.680\n whether it's, you know, Alexa or Google Home or Apple, Siri,\n\n16:42.160 --> 16:46.360\n these devices want to listen\n\n16:47.520 --> 16:49.680\n while they say ostensibly to help us.\n\n16:49.680 --> 16:52.080\n And I think there are great people in these companies\n\n16:52.080 --> 16:54.360\n who do want to help people.\n\n16:54.360 --> 16:56.160\n Let me not brand them all bad.\n\n16:56.160 --> 16:59.320\n I'm a user of products from all of these companies\n\n16:59.320 --> 17:04.320\n I'm naming all the A companies, Alphabet, Apple, Amazon.\n\n17:04.360 --> 17:09.120\n They are awfully big companies, right?\n\n17:09.120 --> 17:11.520\n They have incredible power.\n\n17:11.520 --> 17:16.520\n And you know, what if China were to buy them, right?\n\n17:17.200 --> 17:19.880\n And suddenly all of that data\n\n17:19.880 --> 17:22.440\n were not part of free America,\n\n17:22.440 --> 17:24.400\n but all of that data were part of somebody\n\n17:24.400 --> 17:26.640\n who just wants to take over the world\n\n17:26.640 --> 17:27.920\n and you submit to them.\n\n17:27.920 --> 17:32.120\n And guess what happens if you so much as smirk the wrong way\n\n17:32.120 --> 17:34.560\n when they say something that you don't like?\n\n17:34.560 --> 17:37.440\n Well, they have reeducation camps, right?\n\n17:37.440 --> 17:39.000\n That's a nice word for them.\n\n17:39.000 --> 17:41.440\n By the way, they have a surplus of organs\n\n17:41.440 --> 17:43.320\n for people who have surgery these days.\n\n17:43.320 --> 17:45.040\n They don't have an organ donation problem\n\n17:45.040 --> 17:48.040\n because they take your blood and they know you're a match.\n\n17:48.040 --> 17:51.800\n And the doctors are on record of taking organs\n\n17:51.800 --> 17:55.360\n from people who are perfectly healthy and not prisoners.\n\n17:55.360 --> 17:58.600\n They're just simply not the favored ones of the government.\n\n17:59.600 --> 18:04.480\n And you know, that's a pretty freaky evil society.\n\n18:04.480 --> 18:06.480\n And we can use the word evil there.\n\n18:06.480 --> 18:07.840\n I was born in the Soviet Union.\n\n18:07.840 --> 18:12.840\n I can certainly connect to the worry that you're expressing.\n\n18:13.080 --> 18:15.440\n At the same time, probably both you and I\n\n18:15.440 --> 18:17.040\n and you very much so,\n\n18:19.120 --> 18:23.160\n you know, there's an exciting possibility\n\n18:23.160 --> 18:27.720\n that you can have a deep connection with a machine.\n\n18:27.720 --> 18:28.640\n Yeah, yeah.\n\n18:28.640 --> 18:29.480\n Right, so.\n\n18:30.920 --> 18:35.440\n Those of us, I've admitted students who say that they,\n\n18:35.440 --> 18:36.760\n you know, when you list like,\n\n18:36.760 --> 18:39.400\n who do you most wish you could have lunch with\n\n18:39.400 --> 18:40.520\n or dinner with, right?\n\n18:41.400 --> 18:43.360\n And they'll write like, I don't like people.\n\n18:43.360 --> 18:44.800\n I just like computers.\n\n18:44.800 --> 18:46.360\n And one of them said to me once\n\n18:46.360 --> 18:48.280\n when I had this party at my house,\n\n18:49.520 --> 18:51.160\n I want you to know,\n\n18:51.160 --> 18:53.160\n this is my only social event of the year,\n\n18:53.160 --> 18:55.560\n my one social event of the year.\n\n18:55.560 --> 18:57.680\n Like, okay, now this is a brilliant\n\n18:57.680 --> 18:59.280\n machine learning person, right?\n\n18:59.280 --> 19:01.920\n And we need that kind of brilliance in machine learning.\n\n19:01.920 --> 19:04.760\n And I love that computer science welcomes people\n\n19:04.760 --> 19:07.200\n who love people and people who are very awkward\n\n19:07.200 --> 19:08.040\n around people.\n\n19:08.040 --> 19:12.720\n I love that this is a field that anybody could join.\n\n19:12.720 --> 19:14.960\n We need all kinds of people\n\n19:14.960 --> 19:16.720\n and you don't need to be a social person.\n\n19:16.720 --> 19:19.000\n I'm not trying to force people who don't like people\n\n19:19.000 --> 19:21.720\n to suddenly become social.\n\n19:21.720 --> 19:23.880\n At the same time,\n\n19:23.880 --> 19:26.480\n if most of the people building the AIs of the future\n\n19:26.480 --> 19:29.400\n are the kind of people who don't like people,\n\n19:29.400 --> 19:31.040\n we've got a little bit of a problem.\n\n19:31.040 --> 19:31.920\n Well, hold on a second.\n\n19:31.920 --> 19:33.400\n So let me push back on that.\n\n19:33.400 --> 19:37.680\n So don't you think a large percentage of the world\n\n19:38.640 --> 19:40.880\n can, you know, there's loneliness.\n\n19:40.880 --> 19:44.400\n There is a huge problem with loneliness that's growing.\n\n19:44.400 --> 19:47.560\n And so there's a longing for connection.\n\n19:47.560 --> 19:49.080\n Do you...\n\n19:49.080 --> 19:51.400\n If you're lonely, you're part of a big and growing group.\n\n19:51.400 --> 19:52.240\n Yes.\n\n19:52.240 --> 19:54.320\n So we're in it together, I guess.\n\n19:54.320 --> 19:56.120\n If you're lonely, join the group.\n\n19:56.120 --> 19:56.960\n You're not alone.\n\n19:56.960 --> 19:57.960\n You're not alone.\n\n19:57.960 --> 19:58.920\n That's a good line.\n\n20:00.160 --> 20:02.120\n But do you think there's...\n\n20:03.160 --> 20:04.600\n You talked about some worry,\n\n20:04.600 --> 20:07.600\n but do you think there's an exciting possibility\n\n20:07.600 --> 20:11.560\n that something like Alexa and these kinds of tools\n\n20:11.560 --> 20:14.240\n can alleviate that loneliness\n\n20:14.240 --> 20:16.640\n in a way that other humans can't?\n\n20:16.640 --> 20:18.920\n Yeah, yeah, definitely.\n\n20:18.920 --> 20:22.120\n I mean, a great book can kind of alleviate loneliness\n\n20:22.120 --> 20:25.000\n because you just get sucked into this amazing story\n\n20:25.000 --> 20:27.760\n and you can't wait to go spend time with that character.\n\n20:27.760 --> 20:30.360\n And they're not a human character.\n\n20:30.360 --> 20:32.240\n There is a human behind it.\n\n20:33.200 --> 20:35.400\n But yeah, it can be an incredibly delightful way\n\n20:35.400 --> 20:39.480\n to pass the hours and it can meet needs.\n\n20:39.480 --> 20:43.440\n Even, you know, I don't read those trashy romance books,\n\n20:43.440 --> 20:44.760\n but somebody does, right?\n\n20:44.760 --> 20:46.200\n And what are they getting from this?\n\n20:46.200 --> 20:50.720\n Well, probably some of that feeling of being there, right?\n\n20:50.720 --> 20:52.920\n Being there in that social moment,\n\n20:52.920 --> 20:56.240\n that romantic moment or connecting with somebody.\n\n20:56.240 --> 20:57.560\n I've had a similar experience\n\n20:57.560 --> 20:59.400\n reading some science fiction books, right?\n\n20:59.400 --> 21:00.560\n And connecting with the character.\n\n21:00.560 --> 21:04.160\n Orson Scott Card, you know, just amazing writing\n\n21:04.160 --> 21:07.560\n and Ender's Game and Speaker for the Dead, terrible title.\n\n21:07.560 --> 21:11.000\n But those kind of books that pull you into a character\n\n21:11.000 --> 21:13.880\n and you feel like you're, you feel very social.\n\n21:13.880 --> 21:17.280\n It's very connected, even though it's not responding to you.\n\n21:17.280 --> 21:19.720\n And a computer, of course, can respond to you.\n\n21:19.720 --> 21:21.440\n So it can deepen it, right?\n\n21:21.440 --> 21:25.480\n You can have a very deep connection,\n\n21:25.480 --> 21:29.400\n much more than the movie Her, you know, plays up, right?\n\n21:29.400 --> 21:30.640\n Well, much more.\n\n21:30.640 --> 21:34.760\n I mean, movie Her is already a pretty deep connection, right?\n\n21:34.760 --> 21:36.760\n Well, but it's just a movie, right?\n\n21:36.760 --> 21:37.600\n It's scripted.\n\n21:37.600 --> 21:39.560\n It's just, you know, but I mean,\n\n21:39.560 --> 21:42.680\n like there can be a real interaction\n\n21:42.680 --> 21:46.600\n where the character can learn and you can learn.\n\n21:46.600 --> 21:49.560\n You could imagine it not just being you and one character.\n\n21:49.560 --> 21:51.600\n You could imagine a group of characters.\n\n21:51.600 --> 21:53.600\n You can imagine a group of people and characters,\n\n21:53.600 --> 21:56.440\n human and AI connecting,\n\n21:56.440 --> 22:00.800\n where maybe a few people can't sort of be friends\n\n22:00.800 --> 22:02.880\n with everybody, but the few people\n\n22:02.880 --> 22:07.000\n and their AIs can befriend more people.\n\n22:07.000 --> 22:10.320\n There can be an extended human intelligence in there\n\n22:10.320 --> 22:14.880\n where each human can connect with more people that way.\n\n22:14.880 --> 22:19.480\n But it's still very limited, but there are just,\n\n22:19.480 --> 22:21.560\n what I mean is there are many more possibilities\n\n22:21.560 --> 22:22.760\n than what's in that movie.\n\n22:22.760 --> 22:24.680\n So there's a tension here.\n\n22:24.680 --> 22:27.360\n So one, you expressed a really serious concern\n\n22:27.360 --> 22:29.120\n about privacy, about how governments\n\n22:29.120 --> 22:31.120\n can misuse the information,\n\n22:31.120 --> 22:34.080\n and there's the possibility of this connection.\n\n22:34.080 --> 22:36.200\n So let's look at Alexa.\n\n22:36.200 --> 22:37.760\n So personal assistance.\n\n22:37.760 --> 22:40.840\n For the most part, as far as I'm aware,\n\n22:40.840 --> 22:42.840\n they ignore your emotion.\n\n22:42.840 --> 22:47.400\n They ignore even the context or the existence of you,\n\n22:47.400 --> 22:52.200\n the intricate, beautiful, complex aspects of who you are,\n\n22:52.200 --> 22:54.160\n except maybe aspects of your voice\n\n22:54.160 --> 22:58.360\n that help it recognize for speech recognition.\n\n22:58.360 --> 23:00.600\n Do you think they should move towards\n\n23:00.600 --> 23:03.160\n trying to understand your emotion?\n\n23:03.160 --> 23:04.960\n All of these companies are very interested\n\n23:04.960 --> 23:07.440\n in understanding human emotion.\n\n23:07.440 --> 23:11.400\n They want, more people are telling Siri every day\n\n23:11.400 --> 23:13.720\n they want to kill themselves.\n\n23:13.720 --> 23:15.640\n Apple wants to know the difference between\n\n23:15.640 --> 23:18.480\n if a person is really suicidal versus if a person\n\n23:18.480 --> 23:21.400\n is just kind of fooling around with Siri, right?\n\n23:21.400 --> 23:25.560\n The words may be the same, the tone of voice\n\n23:25.560 --> 23:30.560\n and what surrounds those words is pivotal to understand\n\n23:31.360 --> 23:34.200\n if they should respond in a very serious way,\n\n23:34.200 --> 23:35.920\n bring help to that person,\n\n23:35.920 --> 23:40.640\n or if they should kind of jokingly tease back,\n\n23:40.640 --> 23:44.960\n ah, you just want to sell me for something else, right?\n\n23:44.960 --> 23:47.920\n Like, how do you respond when somebody says that?\n\n23:47.920 --> 23:51.440\n Well, you do want to err on the side of being careful\n\n23:51.440 --> 23:52.600\n and taking it seriously.\n\n23:53.640 --> 23:58.640\n People want to know if the person is happy or stressed\n\n23:59.120 --> 24:03.160\n in part, well, so let me give you an altruistic reason\n\n24:03.160 --> 24:08.160\n and a business profit motivated reason.\n\n24:08.320 --> 24:11.000\n And there are people in companies that operate\n\n24:11.000 --> 24:12.720\n on both principles.\n\n24:12.720 --> 24:16.920\n The altruistic people really care about their customers\n\n24:16.920 --> 24:19.320\n and really care about helping you feel a little better\n\n24:19.320 --> 24:20.240\n at the end of the day.\n\n24:20.240 --> 24:22.680\n And it would just make those people happy\n\n24:22.680 --> 24:24.320\n if they knew that they made your life better.\n\n24:24.320 --> 24:27.000\n If you came home stressed and after talking\n\n24:27.000 --> 24:29.920\n with their product, you felt better.\n\n24:29.920 --> 24:32.960\n There are other people who maybe have studied\n\n24:32.960 --> 24:35.120\n the way affect affects decision making\n\n24:35.120 --> 24:36.440\n and prices people pay.\n\n24:36.440 --> 24:38.760\n And they know, I don't know if I should tell you,\n\n24:38.760 --> 24:43.760\n like the work of Jen Lerner on heartstrings and purse strings,\n\n24:43.960 --> 24:47.960\n you know, if we manipulate you into a slightly sadder mood,\n\n24:47.960 --> 24:50.800\n you'll pay more, right?\n\n24:50.800 --> 24:53.800\n You'll pay more to change your situation.\n\n24:53.800 --> 24:55.800\n You'll pay more for something you don't even need\n\n24:55.800 --> 24:58.040\n to make yourself feel better.\n\n24:58.040 --> 25:00.120\n So, you know, if they sound a little sad,\n\n25:00.120 --> 25:01.240\n maybe I don't want to cheer them up.\n\n25:01.240 --> 25:04.800\n Maybe first I want to help them get something,\n\n25:04.800 --> 25:07.400\n a little shopping therapy, right?\n\n25:07.400 --> 25:08.480\n That helps them.\n\n25:08.480 --> 25:09.880\n Which is really difficult for a company\n\n25:09.880 --> 25:12.120\n that's primarily funded on advertisement.\n\n25:12.120 --> 25:16.160\n So they're encouraged to get you to offer you products\n\n25:16.160 --> 25:17.840\n or Amazon that's primarily funded\n\n25:17.840 --> 25:20.040\n on you buying things from their store.\n\n25:20.040 --> 25:22.120\n So I think we should be, you know,\n\n25:22.120 --> 25:24.120\n maybe we need regulation in the future\n\n25:24.120 --> 25:27.240\n to put a little bit of a wall between these agents\n\n25:27.240 --> 25:29.120\n that have access to our emotion\n\n25:29.120 --> 25:32.280\n and agents that want to sell us stuff.\n\n25:32.280 --> 25:35.560\n Maybe there needs to be a little bit more\n\n25:35.560 --> 25:37.480\n of a firewall in between those.\n\n25:38.400 --> 25:40.480\n So maybe digging in a little bit\n\n25:40.480 --> 25:42.200\n on the interaction with Alexa,\n\n25:42.200 --> 25:44.880\n you mentioned, of course, a really serious concern\n\n25:44.880 --> 25:46.680\n about like recognizing emotion,\n\n25:46.680 --> 25:49.680\n if somebody is speaking of suicide or depression and so on,\n\n25:49.680 --> 25:53.520\n but what about the actual interaction itself?\n\n25:55.000 --> 25:57.840\n Do you think, so if I, you know,\n\n25:57.840 --> 26:00.360\n you mentioned Clippy and being annoying,\n\n26:01.480 --> 26:04.200\n what is the objective function we're trying to optimize?\n\n26:04.200 --> 26:09.200\n Is it minimize annoyingness or minimize or maximize happiness?\n\n26:09.480 --> 26:12.440\n Or if we look at human to human relations,\n\n26:12.440 --> 26:15.480\n I think that push and pull, the tension, the dance,\n\n26:15.480 --> 26:19.840\n you know, the annoying, the flaws, that's what makes it fun.\n\n26:19.840 --> 26:24.160\n So is there a room for, like what is the objective function?\n\n26:24.160 --> 26:26.720\n There are times when you want to have a little push and pull,\n\n26:26.720 --> 26:29.120\n I think of kids sparring, right?\n\n26:29.120 --> 26:31.200\n You know, I see my sons and they,\n\n26:31.200 --> 26:33.720\n one of them wants to provoke the other to be upset\n\n26:33.720 --> 26:34.720\n and that's fun.\n\n26:34.720 --> 26:38.520\n And it's actually healthy to learn where your limits are,\n\n26:38.520 --> 26:40.080\n to learn how to self regulate.\n\n26:40.080 --> 26:43.000\n You can imagine a game where it's trying to make you mad\n\n26:43.000 --> 26:45.080\n and you're trying to show self control.\n\n26:45.080 --> 26:48.640\n And so if we're doing a AI human interaction\n\n26:48.640 --> 26:51.240\n that's helping build resilience and self control,\n\n26:51.240 --> 26:54.040\n whether it's to learn how to not be a bully\n\n26:54.040 --> 26:55.560\n or how to turn the other cheek\n\n26:55.560 --> 26:58.920\n or how to deal with an abusive person in your life,\n\n26:58.920 --> 27:03.360\n then you might need an AI that pushes your buttons, right?\n\n27:04.480 --> 27:07.800\n But in general, do you want an AI that pushes your buttons?\n\n27:10.440 --> 27:12.080\n Probably depends on your personality.\n\n27:12.080 --> 27:15.320\n I don't, I want one that's respectful,\n\n27:15.320 --> 27:18.160\n that is there to serve me\n\n27:18.160 --> 27:23.160\n and that is there to extend my ability to do things.\n\n27:23.200 --> 27:25.160\n I'm not looking for a rival,\n\n27:25.160 --> 27:27.240\n I'm looking for a helper.\n\n27:27.240 --> 27:30.200\n And that's the kind of AI I'd put my money on.\n\n27:30.200 --> 27:33.680\n Your sense is for the majority of people in the world,\n\n27:33.680 --> 27:35.120\n in order to have a rich experience,\n\n27:35.120 --> 27:37.120\n that's what they're looking for as well.\n\n27:37.120 --> 27:37.960\n So they're not looking,\n\n27:37.960 --> 27:40.800\n if you look at the movie Her, spoiler alert,\n\n27:40.800 --> 27:45.800\n I believe the program that the woman in the movie Her\n\n27:46.280 --> 27:51.280\n leaves the person for somebody else,\n\n27:51.320 --> 27:54.360\n says they don't wanna be dating anymore, right?\n\n27:54.360 --> 27:58.280\n Like, do you, your sense is if Alexa said,\n\n27:58.280 --> 28:02.840\n you know what, I'm actually had enough of you for a while,\n\n28:02.840 --> 28:04.880\n so I'm gonna shut myself off.\n\n28:04.880 --> 28:07.120\n You don't see that as...\n\n28:07.120 --> 28:10.120\n I'd say you're trash, cause I paid for you, right?\n\n28:10.120 --> 28:14.000\n You, we've got to remember,\n\n28:14.000 --> 28:18.160\n and this is where this blending human AI\n\n28:18.160 --> 28:22.520\n as if we're equals is really deceptive\n\n28:22.520 --> 28:26.400\n because AI is something at the end of the day\n\n28:26.400 --> 28:28.840\n that my students and I are making in the lab.\n\n28:28.840 --> 28:33.120\n And we're choosing what it's allowed to say,\n\n28:33.120 --> 28:36.600\n when it's allowed to speak, what it's allowed to listen to,\n\n28:36.600 --> 28:40.760\n what it's allowed to act on given the inputs\n\n28:40.760 --> 28:43.400\n that we choose to expose it to,\n\n28:43.400 --> 28:45.880\n what outputs it's allowed to have.\n\n28:45.880 --> 28:49.320\n It's all something made by a human.\n\n28:49.320 --> 28:50.560\n And if we wanna make something\n\n28:50.560 --> 28:52.920\n that makes our lives miserable, fine.\n\n28:52.920 --> 28:55.400\n I wouldn't invest in it as a business,\n\n28:56.640 --> 28:59.520\n unless it's just there for self regulation training.\n\n28:59.520 --> 29:01.960\n But I think we need to think about\n\n29:01.960 --> 29:02.800\n what kind of future we want.\n\n29:02.800 --> 29:05.560\n And actually your question, I really like the,\n\n29:05.560 --> 29:06.760\n what is the objective function?\n\n29:06.760 --> 29:08.480\n Is it to calm people down?\n\n29:09.320 --> 29:10.560\n Sometimes.\n\n29:10.560 --> 29:13.360\n Is it to always make people happy and calm them down?\n\n29:14.400 --> 29:16.080\n Well, there was a book about that, right?\n\n29:16.080 --> 29:18.840\n The brave new world, make everybody happy,\n\n29:18.840 --> 29:22.520\n take your Soma if you're unhappy, take your happy pill.\n\n29:22.520 --> 29:24.360\n And if you refuse to take your happy pill,\n\n29:24.360 --> 29:27.560\n well, we'll threaten you by sending you to Iceland\n\n29:28.600 --> 29:29.600\n to live there.\n\n29:29.600 --> 29:30.840\n I lived in Iceland three years.\n\n29:30.840 --> 29:31.800\n It's a great place.\n\n29:31.800 --> 29:33.920\n Don't take your Soma, then go to Iceland.\n\n29:35.240 --> 29:37.520\n A little TV commercial there.\n\n29:37.520 --> 29:39.240\n Now I was a child there for a few years.\n\n29:39.240 --> 29:40.600\n It's a wonderful place.\n\n29:40.600 --> 29:43.240\n So that part of the book never scared me.\n\n29:43.240 --> 29:46.720\n But really like, do we want AI to manipulate us\n\n29:46.720 --> 29:49.080\n into submission, into making us happy?\n\n29:49.080 --> 29:52.640\n Well, if you are a, you know,\n\n29:52.640 --> 29:56.080\n like a power obsessed sick dictator individual\n\n29:56.080 --> 29:57.640\n who only wants to control other people\n\n29:57.640 --> 29:59.640\n to get your jollies in life, then yeah,\n\n29:59.640 --> 30:03.720\n you wanna use AI to extend your power and your scale\n\n30:03.720 --> 30:07.080\n to force people into submission.\n\n30:07.080 --> 30:10.080\n If you believe that the human race is better off\n\n30:10.080 --> 30:12.120\n being given freedom and the opportunity\n\n30:12.120 --> 30:15.360\n to do things that might surprise you,\n\n30:15.360 --> 30:20.200\n then you wanna use AI to extend people's ability to build,\n\n30:20.200 --> 30:22.960\n you wanna build AI that extends human intelligence,\n\n30:22.960 --> 30:27.320\n that empowers the weak and helps balance the power\n\n30:27.320 --> 30:28.840\n between the weak and the strong,\n\n30:28.840 --> 30:31.000\n not that gives more power to the strong.\n\n30:32.440 --> 30:37.440\n So in this process of empowering people and sensing people,\n\n30:39.280 --> 30:41.280\n what is your sense on emotion\n\n30:41.280 --> 30:42.680\n in terms of recognizing emotion?\n\n30:42.680 --> 30:44.680\n The difference between emotion that is shown\n\n30:44.680 --> 30:46.640\n and emotion that is felt.\n\n30:46.640 --> 30:51.640\n So yeah, emotion that is expressed on the surface\n\n30:52.640 --> 30:56.560\n through your face, your body, and various other things,\n\n30:56.560 --> 30:58.840\n and what's actually going on deep inside\n\n30:58.840 --> 31:01.760\n on the biological level, on the neuroscience level,\n\n31:01.760 --> 31:03.680\n or some kind of cognitive level.\n\n31:03.680 --> 31:04.840\n Yeah, yeah.\n\n31:05.720 --> 31:07.840\n Whoa, no easy questions here.\n\n31:07.840 --> 31:11.280\n Well, yeah, I'm sure there's no definitive answer,\n\n31:11.280 --> 31:12.360\n but what's your sense?\n\n31:12.360 --> 31:15.240\n How far can we get by just looking at the face?\n\n31:16.160 --> 31:18.480\n We're very limited when we just look at the face,\n\n31:18.480 --> 31:21.920\n but we can get further than most people think we can get.\n\n31:21.920 --> 31:25.920\n People think, hey, I have a great poker face,\n\n31:25.920 --> 31:28.280\n therefore all you're ever gonna get from me is neutral.\n\n31:28.280 --> 31:30.160\n Well, that's naive.\n\n31:30.160 --> 31:32.680\n We can read with the ordinary camera\n\n31:32.680 --> 31:34.920\n on your laptop or on your phone.\n\n31:34.920 --> 31:39.320\n We can read from a neutral face if your heart is racing.\n\n31:39.320 --> 31:41.280\n We can read from a neutral face\n\n31:41.280 --> 31:44.760\n if your breathing is becoming irregular\n\n31:44.760 --> 31:46.920\n and showing signs of stress.\n\n31:46.920 --> 31:50.720\n We can read under some conditions\n\n31:50.720 --> 31:53.200\n that maybe I won't give you details on,\n\n31:53.200 --> 31:57.080\n how your heart rate variability power is changing.\n\n31:57.080 --> 31:58.640\n That could be a sign of stress,\n\n31:58.640 --> 32:02.920\n even when your heart rate is not necessarily accelerating.\n\n32:02.920 --> 32:03.760\n So...\n\n32:03.760 --> 32:06.080\n Sorry, from physio sensors or from the face?\n\n32:06.080 --> 32:09.160\n From the color changes that you cannot even see,\n\n32:09.160 --> 32:11.680\n but the camera can see.\n\n32:11.680 --> 32:12.520\n That's amazing.\n\n32:12.520 --> 32:15.320\n So you can get a lot of signal, but...\n\n32:15.320 --> 32:18.680\n So we get things people can't see using a regular camera.\n\n32:18.680 --> 32:21.920\n And from that, we can tell things about your stress.\n\n32:21.920 --> 32:25.600\n So if you were just sitting there with a blank face\n\n32:25.600 --> 32:28.400\n thinking nobody can read my emotion, well, you're wrong.\n\n32:30.120 --> 32:31.840\n Right, so that's really interesting,\n\n32:31.840 --> 32:34.520\n but that's from sort of visual information from the face.\n\n32:34.520 --> 32:37.120\n That's almost like cheating your way\n\n32:37.120 --> 32:39.120\n to the physiological state of the body,\n\n32:39.120 --> 32:42.600\n by being very clever with what you can do with vision.\n\n32:42.600 --> 32:43.440\n With signal processing.\n\n32:43.440 --> 32:44.360\n With signal processing.\n\n32:44.360 --> 32:45.320\n So that's really impressive.\n\n32:45.320 --> 32:49.320\n But if you just look at the stuff we humans can see,\n\n32:49.320 --> 32:52.240\n the poker, the smile, the smirks,\n\n32:52.240 --> 32:54.320\n the subtle, all the facial actions.\n\n32:54.320 --> 32:55.960\n So then you can hide that on your face\n\n32:55.960 --> 32:57.240\n for a limited amount of time.\n\n32:57.240 --> 33:00.600\n Now, if you're just going in for a brief interview\n\n33:00.600 --> 33:03.880\n and you're hiding it, that's pretty easy for most people.\n\n33:03.880 --> 33:08.800\n If you are, however, surveilled constantly everywhere you go,\n\n33:08.800 --> 33:13.280\n then it's gonna say, gee, you know, Lex used to smile a lot\n\n33:13.280 --> 33:15.920\n and now I'm not seeing so many smiles.\n\n33:15.920 --> 33:20.240\n And Roz used to laugh a lot\n\n33:20.240 --> 33:22.280\n and smile a lot very spontaneously.\n\n33:22.280 --> 33:23.480\n And now I'm only seeing\n\n33:23.480 --> 33:26.440\n these not so spontaneous looking smiles.\n\n33:26.440 --> 33:28.920\n And only when she's asked these questions.\n\n33:28.920 --> 33:31.720\n You know, that's something's changed here.\n\n33:31.720 --> 33:33.720\n Probably not getting enough sleep.\n\n33:33.720 --> 33:35.200\n We could look at that too.\n\n33:35.200 --> 33:37.000\n So now I have to be a little careful too.\n\n33:37.000 --> 33:40.920\n When I say we, you think we can't read your emotion\n\n33:40.920 --> 33:42.760\n and we can, it's not that binary.\n\n33:42.760 --> 33:45.960\n What we're reading is more some physiological changes\n\n33:45.960 --> 33:48.760\n that relate to your activation.\n\n33:48.760 --> 33:51.800\n Now, that doesn't mean that we know everything\n\n33:51.800 --> 33:52.640\n about how you feel.\n\n33:52.640 --> 33:54.880\n In fact, we still know very little about how you feel.\n\n33:54.880 --> 33:56.880\n Your thoughts are still private.\n\n33:56.880 --> 34:01.120\n Your nuanced feelings are still completely private.\n\n34:01.120 --> 34:02.920\n We can't read any of that.\n\n34:02.920 --> 34:07.000\n So there's some relief that we can't read that.\n\n34:07.000 --> 34:09.800\n Even brain imaging can't read that.\n\n34:09.800 --> 34:12.280\n Wearables can't read that.\n\n34:12.280 --> 34:16.000\n However, as we read your body state changes\n\n34:16.000 --> 34:18.520\n and we know what's going on in your environment\n\n34:18.520 --> 34:21.480\n and we look at patterns of those over time,\n\n34:21.480 --> 34:24.960\n we can start to make some inferences\n\n34:24.960 --> 34:26.960\n about what you might be feeling.\n\n34:26.960 --> 34:31.400\n And that is where it's not just the momentary feeling\n\n34:31.400 --> 34:34.120\n but it's more your stance toward things.\n\n34:34.120 --> 34:37.040\n And that could actually be a little bit more scary\n\n34:37.040 --> 34:42.040\n with certain kinds of governmental control freak people\n\n34:42.840 --> 34:46.800\n who want to know more about are you on their team\n\n34:46.800 --> 34:48.320\n or are you not?\n\n34:48.320 --> 34:50.320\n And getting that information through over time.\n\n34:50.320 --> 34:51.640\n So you're saying there's a lot of signal\n\n34:51.640 --> 34:53.680\n by looking at the change over time.\n\n34:53.680 --> 34:54.520\n Yeah.\n\n34:54.520 --> 34:56.600\n So you've done a lot of exciting work\n\n34:56.600 --> 34:57.800\n both in computer vision\n\n34:57.800 --> 35:00.560\n and physiological sense like wearables.\n\n35:00.560 --> 35:03.480\n What do you think is the best modality for,\n\n35:03.480 --> 35:08.360\n what's the best window into the emotional soul?\n\n35:08.360 --> 35:09.200\n Is it the face?\n\n35:09.200 --> 35:10.160\n Is it the voice?\n\n35:10.160 --> 35:11.920\n Depends what you want to know.\n\n35:11.920 --> 35:13.120\n It depends what you want to know.\n\n35:13.120 --> 35:13.960\n It depends what you want to know.\n\n35:13.960 --> 35:15.600\n Everything is informative.\n\n35:15.600 --> 35:17.440\n Everything we do is informative.\n\n35:17.440 --> 35:20.160\n So for health and wellbeing and things like that,\n\n35:20.160 --> 35:22.680\n do you find the wearable physiotechnical,\n\n35:22.680 --> 35:24.840\n measuring physiological signals\n\n35:24.840 --> 35:29.320\n is the best for health based stuff?\n\n35:29.320 --> 35:31.880\n So here I'm going to answer empirically\n\n35:31.880 --> 35:34.680\n with data and studies we've been doing.\n\n35:34.680 --> 35:36.040\n We've been doing studies.\n\n35:36.040 --> 35:38.280\n Now these are currently running\n\n35:38.280 --> 35:39.600\n with lots of different kinds of people\n\n35:39.600 --> 35:41.880\n but where we've published data\n\n35:41.880 --> 35:44.080\n and I can speak publicly to it,\n\n35:44.080 --> 35:45.520\n the data are limited right now\n\n35:45.520 --> 35:47.680\n to New England college students.\n\n35:47.680 --> 35:49.200\n So that's a small group.\n\n35:50.320 --> 35:52.440\n Among New England college students,\n\n35:52.440 --> 35:55.880\n when they are wearing a wearable\n\n35:55.880 --> 35:57.640\n like the empathic embrace here\n\n35:57.640 --> 36:01.760\n that's measuring skin conductance, movement, temperature.\n\n36:01.760 --> 36:05.800\n And when they are using a smartphone\n\n36:05.800 --> 36:09.400\n that is collecting their time of day\n\n36:09.400 --> 36:12.120\n of when they're texting, who they're texting,\n\n36:12.120 --> 36:14.200\n their movement around it, their GPS,\n\n36:14.200 --> 36:18.040\n the weather information based upon their location.\n\n36:18.040 --> 36:19.360\n And when it's using machine learning\n\n36:19.360 --> 36:20.920\n and putting all of that together\n\n36:20.920 --> 36:22.920\n and looking not just at right now\n\n36:22.920 --> 36:26.960\n but looking at your rhythm of behaviors\n\n36:26.960 --> 36:28.560\n over about a week.\n\n36:28.560 --> 36:29.920\n When we look at that,\n\n36:29.920 --> 36:33.560\n we are very accurate at forecasting tomorrow's stress,\n\n36:33.560 --> 36:38.560\n mood and happy, sad mood and health.\n\n36:38.560 --> 36:42.640\n And when we look at which pieces of that are most useful,\n\n36:43.680 --> 36:45.560\n first of all, if you have all the pieces,\n\n36:45.560 --> 36:47.040\n you get the best results.\n\n36:48.320 --> 36:50.600\n If you have only the wearable,\n\n36:50.600 --> 36:52.680\n you get the next best results.\n\n36:52.680 --> 36:56.520\n And that's still better than 80% accurate\n\n36:56.520 --> 36:58.560\n at forecasting tomorrow's levels.\n\n37:00.080 --> 37:02.800\n Isn't that exciting because the wearable stuff\n\n37:02.800 --> 37:05.320\n with physiological information,\n\n37:05.320 --> 37:08.000\n it feels like it violates privacy less\n\n37:08.000 --> 37:12.720\n than the noncontact face based methods.\n\n37:12.720 --> 37:14.040\n Yeah, it's interesting.\n\n37:14.040 --> 37:16.560\n I think what people sometimes don't,\n\n37:16.560 --> 37:18.880\n it's funny in the early days people would say,\n\n37:18.880 --> 37:22.560\n oh, wearing something or giving blood is invasive, right?\n\n37:22.560 --> 37:24.400\n Whereas a camera is less invasive\n\n37:24.400 --> 37:26.920\n because it's not touching you.\n\n37:26.920 --> 37:28.320\n I think on the contrary,\n\n37:28.320 --> 37:31.200\n the things that are not touching you are maybe the scariest\n\n37:31.200 --> 37:33.880\n because you don't know when they're on or off.\n\n37:33.880 --> 37:38.880\n And you don't know who's behind it, right?\n\n37:39.920 --> 37:43.480\n A wearable, depending upon what's happening\n\n37:43.480 --> 37:46.760\n to the data on it, if it's just stored locally\n\n37:46.760 --> 37:52.640\n or if it's streaming and what it is being attached to,\n\n37:52.640 --> 37:54.960\n in a sense, you have the most control over it\n\n37:54.960 --> 37:59.400\n because it's also very easy to just take it off, right?\n\n37:59.400 --> 38:01.440\n Now it's not sensing me.\n\n38:01.440 --> 38:05.320\n So if I'm uncomfortable with what it's sensing,\n\n38:05.320 --> 38:07.240\n now I'm free, right?\n\n38:07.240 --> 38:09.960\n If I'm comfortable with what it's sensing,\n\n38:09.960 --> 38:12.800\n then, and I happen to know everything about this one\n\n38:12.800 --> 38:13.720\n and what it's doing with it,\n\n38:13.720 --> 38:15.600\n so I'm quite comfortable with it,\n\n38:15.600 --> 38:20.240\n then I have control, I'm comfortable.\n\n38:20.240 --> 38:24.720\n Control is one of the biggest factors for an individual\n\n38:24.720 --> 38:26.600\n in reducing their stress.\n\n38:26.600 --> 38:28.160\n If I have control over it,\n\n38:28.160 --> 38:30.200\n if I know all there is to know about it,\n\n38:30.200 --> 38:32.480\n then my stress is a lot lower\n\n38:32.480 --> 38:34.960\n and I'm making an informed choice\n\n38:34.960 --> 38:36.640\n about whether to wear it or not,\n\n38:36.640 --> 38:38.040\n or when to wear it or not.\n\n38:38.040 --> 38:40.320\n I wanna wear it sometimes, maybe not others.\n\n38:40.320 --> 38:42.760\n Right, so that control, yeah, I'm with you.\n\n38:42.760 --> 38:47.520\n That control, even if, yeah, the ability to turn it off,\n\n38:47.520 --> 38:49.000\n that is a really important thing.\n\n38:49.000 --> 38:49.840\n It's huge.\n\n38:49.840 --> 38:53.440\n And we need to, maybe, if there's regulations,\n\n38:53.440 --> 38:55.080\n maybe that's number one to protect\n\n38:55.080 --> 38:59.960\n is people's ability to, it's easy to opt out as to opt in.\n\n38:59.960 --> 39:04.480\n Right, so you've studied a bit of neuroscience as well.\n\n39:04.480 --> 39:08.240\n How have looking at our own minds,\n\n39:08.240 --> 39:12.840\n sort of the biological stuff or the neurobiological,\n\n39:12.840 --> 39:15.440\n the neuroscience to get the signals in our brain,\n\n39:17.400 --> 39:18.760\n helped you understand the problem\n\n39:18.760 --> 39:20.960\n and the approach of effective computing, so?\n\n39:21.880 --> 39:23.720\n Originally, I was a computer architect\n\n39:23.720 --> 39:26.320\n and I was building hardware and computer designs\n\n39:26.320 --> 39:28.240\n and I wanted to build ones that worked like the brain.\n\n39:28.240 --> 39:29.880\n So I've been studying the brain\n\n39:29.880 --> 39:32.520\n as long as I've been studying how to build computers.\n\n39:33.920 --> 39:36.160\n Have you figured out anything yet?\n\n39:36.160 --> 39:37.000\n Very little.\n\n39:37.000 --> 39:39.400\n It's so amazing.\n\n39:39.400 --> 39:40.720\n You know, they used to think like,\n\n39:40.720 --> 39:42.560\n oh, if you remove this chunk of the brain\n\n39:42.560 --> 39:44.040\n and you find this function goes away,\n\n39:44.040 --> 39:45.760\n well, that's the part of the brain that did it.\n\n39:45.760 --> 39:46.880\n And then later they realized\n\n39:46.880 --> 39:48.320\n if you remove this other chunk of the brain,\n\n39:48.320 --> 39:50.120\n that function comes back and,\n\n39:50.120 --> 39:52.800\n oh no, we really don't understand it.\n\n39:52.800 --> 39:56.200\n Brains are so interesting and changing all the time\n\n39:56.200 --> 39:58.080\n and able to change in ways\n\n39:58.080 --> 40:00.680\n that will probably continue to surprise us.\n\n40:02.120 --> 40:04.520\n When we were measuring stress,\n\n40:04.520 --> 40:07.160\n you may know the story where we found\n\n40:07.160 --> 40:10.680\n an unusual big skin conductance pattern on one wrist\n\n40:10.680 --> 40:12.720\n in one of our kids with autism.\n\n40:14.120 --> 40:15.480\n And in trying to figure out how on earth\n\n40:15.480 --> 40:17.760\n you could be stressed on one wrist and not the other,\n\n40:17.760 --> 40:20.160\n like how can you get sweaty on one wrist, right?\n\n40:20.160 --> 40:21.520\n When you get stressed\n\n40:21.520 --> 40:23.280\n with that sympathetic fight or flight response,\n\n40:23.280 --> 40:25.120\n like you kind of should like sweat more\n\n40:25.120 --> 40:26.240\n in some places than others,\n\n40:26.240 --> 40:27.920\n but not more on one wrist than the other.\n\n40:27.920 --> 40:29.320\n That didn't make any sense.\n\n40:30.840 --> 40:33.120\n We learned that what had actually happened\n\n40:33.120 --> 40:37.080\n was a part of his brain had unusual electrical activity\n\n40:37.080 --> 40:41.240\n and that caused an unusually large sweat response\n\n40:41.240 --> 40:42.960\n on one wrist and not the other.\n\n40:44.040 --> 40:45.480\n And since then we've learned\n\n40:45.480 --> 40:49.360\n that seizures cause this unusual electrical activity.\n\n40:49.360 --> 40:51.240\n And depending where the seizure is,\n\n40:51.240 --> 40:53.360\n if it's in one place and it's staying there,\n\n40:53.360 --> 40:55.520\n you can have a big electrical response\n\n40:55.520 --> 40:58.480\n we can pick up with a wearable at one part of the body.\n\n40:58.480 --> 40:59.400\n You can also have a seizure\n\n40:59.400 --> 41:00.480\n that spreads over the whole brain,\n\n41:00.480 --> 41:02.400\n generalized grand mal seizure.\n\n41:02.400 --> 41:04.040\n And that response spreads\n\n41:04.040 --> 41:06.240\n and we can pick it up pretty much anywhere.\n\n41:07.160 --> 41:10.200\n As we learned this and then later built Embrace\n\n41:10.200 --> 41:13.120\n that's now FDA cleared for seizure detection,\n\n41:13.120 --> 41:15.760\n we have also built relationships\n\n41:15.760 --> 41:18.480\n with some of the most amazing doctors in the world\n\n41:18.480 --> 41:20.400\n who not only help people\n\n41:20.400 --> 41:23.040\n with unusual brain activity or epilepsy,\n\n41:23.040 --> 41:24.440\n but some of them are also surgeons\n\n41:24.440 --> 41:27.160\n and they're going in and they're implanting electrodes,\n\n41:27.160 --> 41:31.200\n not just to momentarily read the strange patterns\n\n41:31.200 --> 41:35.080\n of brain activity that we'd like to see return to normal,\n\n41:35.080 --> 41:37.320\n but also to read out continuously what's happening\n\n41:37.320 --> 41:39.120\n in some of these deep regions of the brain\n\n41:39.120 --> 41:41.640\n during most of life when these patients are not seizing.\n\n41:41.640 --> 41:42.960\n Most of the time they're not seizing,\n\n41:42.960 --> 41:44.960\n most of the time they're fine.\n\n41:44.960 --> 41:47.920\n And so we are now working on mapping\n\n41:47.920 --> 41:49.960\n those deep brain regions\n\n41:49.960 --> 41:53.680\n that you can't even usually get with EEG scalp electrodes\n\n41:53.680 --> 41:57.760\n because the changes deep inside don't reach the surface.\n\n41:58.640 --> 42:00.480\n But interesting when some of those regions\n\n42:00.480 --> 42:04.280\n are activated, we see a big skin conductance response.\n\n42:04.280 --> 42:05.800\n Who would have thunk it, right?\n\n42:05.800 --> 42:07.840\n Like nothing here, but something here.\n\n42:07.840 --> 42:10.400\n In fact, right after seizures\n\n42:10.400 --> 42:12.600\n that we think are the most dangerous ones\n\n42:12.600 --> 42:14.120\n that precede what's called SUDEP,\n\n42:14.120 --> 42:16.040\n Sudden Unexpected Death and Epilepsy,\n\n42:16.960 --> 42:19.520\n there's a period where the brainwaves go flat\n\n42:19.520 --> 42:21.640\n and it looks like the person's brain has stopped,\n\n42:21.640 --> 42:23.120\n but it hasn't.\n\n42:23.120 --> 42:26.400\n The activity has gone deep into a region\n\n42:26.400 --> 42:29.120\n that can make the cortical activity look flat,\n\n42:29.120 --> 42:31.480\n like a quick shutdown signal here.\n\n42:32.600 --> 42:35.560\n It can unfortunately cause breathing to stop\n\n42:35.560 --> 42:37.320\n if it progresses long enough.\n\n42:38.360 --> 42:42.080\n Before that happens, we see a big skin conductance response\n\n42:42.080 --> 42:43.760\n in the data that we have.\n\n42:43.760 --> 42:46.880\n The longer this flattening, the bigger our response here.\n\n42:46.880 --> 42:49.480\n So we have been trying to learn, you know, initially,\n\n42:49.480 --> 42:51.720\n like why are we getting a big response here\n\n42:51.720 --> 42:52.560\n when there's nothing here?\n\n42:52.560 --> 42:55.600\n Well, it turns out there's something much deeper.\n\n42:55.600 --> 42:57.760\n So we can now go inside the brains\n\n42:57.760 --> 43:01.280\n of some of these individuals, fabulous people\n\n43:01.280 --> 43:03.480\n who usually aren't seizing,\n\n43:03.480 --> 43:05.600\n and get this data and start to map it.\n\n43:05.600 --> 43:07.600\n So that's the active research that we're doing right now\n\n43:07.600 --> 43:09.360\n with top medical partners.\n\n43:09.360 --> 43:12.960\n So this wearable sensor that's looking at skin conductance\n\n43:12.960 --> 43:17.160\n can capture sort of the ripples of the complexity\n\n43:17.160 --> 43:18.880\n of what's going on in our brain.\n\n43:18.880 --> 43:22.240\n So this little device, you have a hope\n\n43:22.240 --> 43:24.920\n that you can start to get the signal\n\n43:24.920 --> 43:27.880\n from the interesting things happening in the brain.\n\n43:27.880 --> 43:30.600\n Yeah, we've already published the strong correlations\n\n43:30.600 --> 43:32.200\n between the size of this response\n\n43:32.200 --> 43:35.360\n and the flattening that happens afterwards.\n\n43:35.360 --> 43:38.320\n And unfortunately, also in a real SUDEP case\n\n43:38.320 --> 43:42.360\n where the patient died because the, well, we don't know why.\n\n43:42.360 --> 43:43.600\n We don't know if somebody was there,\n\n43:43.600 --> 43:45.280\n it would have definitely prevented it.\n\n43:45.280 --> 43:47.040\n But we know that most SUDEPs happen\n\n43:47.040 --> 43:48.600\n when the person's alone.\n\n43:48.600 --> 43:53.600\n And in this case, a SUDEP is an acronym, S U D E P.\n\n43:53.600 --> 43:56.680\n And it stands for the number two cause\n\n43:56.680 --> 43:58.960\n of years of life lost actually\n\n43:58.960 --> 44:01.040\n among all neurological disorders.\n\n44:01.040 --> 44:03.480\n Stroke is number one, SUDEP is number two,\n\n44:03.480 --> 44:05.640\n but most people haven't heard of it.\n\n44:05.640 --> 44:07.240\n Actually, I'll plug my TED talk,\n\n44:07.240 --> 44:09.280\n it's on the front page of TED right now\n\n44:09.280 --> 44:11.160\n that talks about this.\n\n44:11.160 --> 44:13.560\n And we hope to change that.\n\n44:13.560 --> 44:17.160\n I hope everybody who's heard of SIDS and stroke\n\n44:17.160 --> 44:18.760\n will now hear of SUDEP\n\n44:18.760 --> 44:21.280\n because we think in most cases it's preventable\n\n44:21.280 --> 44:24.720\n if people take their meds and aren't alone\n\n44:24.720 --> 44:26.200\n when they have a seizure.\n\n44:26.200 --> 44:27.800\n Not guaranteed to be preventable.\n\n44:27.800 --> 44:29.680\n There are some exceptions,\n\n44:29.680 --> 44:31.560\n but we think most cases probably are.\n\n44:31.560 --> 44:35.000\n So you had this embrace now in the version two wristband,\n\n44:35.000 --> 44:37.800\n right, for epilepsy management.\n\n44:39.280 --> 44:41.440\n That's the one that's FDA approved?\n\n44:41.440 --> 44:42.640\n Yes.\n\n44:42.640 --> 44:43.480\n Which is kind of a clear.\n\n44:43.480 --> 44:45.200\n FDA cleared, they say.\n\n44:45.200 --> 44:46.040\n Sorry.\n\n44:46.040 --> 44:46.880\n No, it's okay.\n\n44:46.880 --> 44:49.400\n It essentially means it's approved for marketing.\n\n44:49.400 --> 44:50.240\n Got it.\n\n44:50.240 --> 44:52.960\n Just a side note, how difficult is that to do?\n\n44:52.960 --> 44:54.880\n It's essentially getting FDA approval\n\n44:54.880 --> 44:57.000\n for computer science technology.\n\n44:57.000 --> 44:58.000\n It's so agonizing.\n\n44:58.000 --> 45:01.920\n It's much harder than publishing multiple papers\n\n45:01.920 --> 45:04.120\n in top medical journals.\n\n45:04.120 --> 45:05.920\n Yeah, we've published peer reviewed\n\n45:05.920 --> 45:08.840\n top medical journal neurology, best results,\n\n45:08.840 --> 45:10.800\n and that's not good enough for the FDA.\n\n45:10.800 --> 45:12.520\n Is that system,\n\n45:12.520 --> 45:14.880\n so if we look at the peer review of medical journals,\n\n45:14.880 --> 45:16.800\n there's flaws, there's strengths,\n\n45:16.800 --> 45:19.560\n is the FDA approval process,\n\n45:19.560 --> 45:21.560\n how does it compare to the peer review process?\n\n45:21.560 --> 45:23.160\n Does it have the strength?\n\n45:23.160 --> 45:25.720\n I'll take peer review over FDA any day.\n\n45:25.720 --> 45:26.600\n But is that a good thing?\n\n45:26.600 --> 45:28.040\n Is that a good thing for FDA?\n\n45:28.040 --> 45:31.160\n You're saying, does it stop some amazing technology\n\n45:31.160 --> 45:32.320\n from getting through?\n\n45:32.320 --> 45:33.400\n Yeah, it does.\n\n45:33.400 --> 45:36.240\n The FDA performs a very important good role\n\n45:36.240 --> 45:37.600\n in keeping people safe.\n\n45:37.600 --> 45:39.480\n They keep things,\n\n45:39.480 --> 45:41.800\n they put you through tons of safety testing\n\n45:41.800 --> 45:44.240\n and that's wonderful and that's great.\n\n45:44.240 --> 45:46.240\n I'm all in favor of the safety testing.\n\n45:46.240 --> 45:51.000\n But sometimes they put you through additional testing\n\n45:51.000 --> 45:54.080\n that they don't have to explain why they put you through it\n\n45:54.080 --> 45:56.680\n and you don't understand why you're going through it\n\n45:56.680 --> 45:58.440\n and it doesn't make sense.\n\n45:58.440 --> 46:00.680\n And that's very frustrating.\n\n46:00.680 --> 46:03.080\n And maybe they have really good reasons\n\n46:04.400 --> 46:05.480\n and they just would,\n\n46:05.480 --> 46:09.720\n it would do people a service to articulate those reasons.\n\n46:09.720 --> 46:10.640\n Be more transparent.\n\n46:10.640 --> 46:12.120\n Be more transparent.\n\n46:12.120 --> 46:15.760\n So as part of Empatica, you have sensors.\n\n46:15.760 --> 46:17.800\n So what kind of problems can we crack?\n\n46:17.800 --> 46:22.800\n What kind of things from seizures to autism\n\n46:24.480 --> 46:28.000\n to I think I've heard you mentioned depression.\n\n46:28.000 --> 46:29.760\n What kind of things can we alleviate?\n\n46:29.760 --> 46:30.720\n Can we detect?\n\n46:30.720 --> 46:32.240\n What's your hope of what,\n\n46:32.240 --> 46:33.960\n how we can make the world a better place\n\n46:33.960 --> 46:35.760\n with this wearable tech?\n\n46:35.760 --> 46:40.760\n I would really like to see my fellow brilliant researchers\n\n46:40.760 --> 46:45.760\n step back and say, what are the really hard problems\n\n46:46.200 --> 46:47.760\n that we don't know how to solve\n\n46:47.760 --> 46:50.440\n that come from people maybe we don't even see\n\n46:50.440 --> 46:52.800\n in our normal life because they're living\n\n46:52.800 --> 46:54.240\n in the poor places.\n\n46:54.240 --> 46:56.160\n They're stuck on the bus.\n\n46:56.160 --> 46:58.440\n They can't even afford the Uber or the Lyft\n\n46:58.440 --> 47:02.200\n or the data plan or all these other wonderful things\n\n47:02.200 --> 47:04.360\n we have that we keep improving on.\n\n47:04.360 --> 47:07.080\n Meanwhile, there's all these folks left behind in the world\n\n47:07.080 --> 47:09.520\n and they're struggling with horrible diseases\n\n47:09.520 --> 47:12.960\n with depression, with epilepsy, with diabetes,\n\n47:12.960 --> 47:17.960\n with just awful stuff that maybe a little more time\n\n47:19.200 --> 47:20.440\n and attention hanging out with them\n\n47:20.440 --> 47:22.800\n and learning what are their challenges in life?\n\n47:22.800 --> 47:24.000\n What are their needs?\n\n47:24.000 --> 47:25.640\n How do we help them have job skills?\n\n47:25.640 --> 47:28.520\n How do we help them have a hope and a future\n\n47:28.520 --> 47:31.240\n and a chance to have the great life\n\n47:31.240 --> 47:34.960\n that so many of us building technology have?\n\n47:34.960 --> 47:37.920\n And then how would that reshape the kinds of AI\n\n47:37.920 --> 47:41.400\n that we build? How would that reshape the new apps\n\n47:41.400 --> 47:44.040\n that we build or the maybe we need to focus\n\n47:44.040 --> 47:46.880\n on how to make things more low cost and green\n\n47:46.880 --> 47:49.320\n instead of thousand dollar phones?\n\n47:49.320 --> 47:52.840\n I mean, come on, why can't we be thinking more\n\n47:52.840 --> 47:56.840\n about things that do more with less for these folks?\n\n47:56.840 --> 48:00.200\n Quality of life is not related to the cost of your phone.\n\n48:00.200 --> 48:03.960\n It's not something that, it's been shown that what about\n\n48:03.960 --> 48:08.440\n $75,000 of income and happiness is the same, okay?\n\n48:08.440 --> 48:10.720\n However, I can tell you, you get a lot of happiness\n\n48:10.720 --> 48:12.320\n from helping other people.\n\n48:12.320 --> 48:15.200\n You get a lot more than $75,000 buys.\n\n48:15.200 --> 48:19.320\n So how do we connect up the people who have real needs\n\n48:19.320 --> 48:21.920\n with the people who have the ability to build the future\n\n48:21.920 --> 48:25.840\n and build the kind of future that truly improves the lives\n\n48:25.840 --> 48:28.720\n of all the people that are currently being left behind?\n\n48:28.720 --> 48:32.720\n So let me return just briefly on a point,\n\n48:32.720 --> 48:34.320\n maybe in the movie, Her.\n\n48:35.280 --> 48:37.720\n So do you think if we look farther into the future,\n\n48:37.720 --> 48:41.240\n you said so much of the benefit from making our technology\n\n48:41.240 --> 48:46.240\n more empathetic to us human beings would make them\n\n48:46.240 --> 48:50.320\n better tools, empower us, make our lives better.\n\n48:50.320 --> 48:51.960\n Well, if we look farther into the future,\n\n48:51.960 --> 48:54.560\n do you think we'll ever create an AI system\n\n48:54.560 --> 48:56.920\n that we can fall in love with?\n\n48:56.920 --> 49:00.280\n That we can fall in love with and loves us back\n\n49:00.280 --> 49:04.920\n on a level that is similar to human to human interaction,\n\n49:04.920 --> 49:07.680\n like in the movie Her or beyond?\n\n49:07.680 --> 49:12.680\n I think we can simulate it in ways that could,\n\n49:13.920 --> 49:16.200\n you know, sustain engagement for a while.\n\n49:17.280 --> 49:20.160\n Would it be as good as another person?\n\n49:20.160 --> 49:24.560\n I don't think so, if you're used to like good people.\n\n49:24.560 --> 49:27.120\n Now, if you've just grown up with nothing but abuse\n\n49:27.120 --> 49:29.080\n and you can't stand human beings,\n\n49:29.080 --> 49:32.160\n can we do something that helps you there\n\n49:32.160 --> 49:34.000\n that gives you something through a machine?\n\n49:34.000 --> 49:36.160\n Yeah, but that's pretty low bar, right?\n\n49:36.160 --> 49:39.120\n If you've only encountered pretty awful people.\n\n49:39.120 --> 49:41.680\n If you've encountered wonderful, amazing people,\n\n49:41.680 --> 49:44.800\n we're nowhere near building anything like that.\n\n49:44.800 --> 49:49.480\n And I would not bet on building it.\n\n49:49.480 --> 49:53.160\n I would bet instead on building the kinds of AI\n\n49:53.160 --> 49:56.880\n that helps kind of raise all boats,\n\n49:56.880 --> 49:59.480\n that helps all people be better people,\n\n49:59.480 --> 50:02.280\n helps all people figure out if they're getting sick tomorrow\n\n50:02.280 --> 50:05.400\n and helps give them what they need to stay well tomorrow.\n\n50:05.400 --> 50:07.000\n That's the kind of AI I wanna build\n\n50:07.000 --> 50:09.040\n that improves human lives,\n\n50:09.040 --> 50:11.640\n not the kind of AI that just walks on The Tonight Show\n\n50:11.640 --> 50:14.600\n and people go, wow, look how smart that is.\n\n50:14.600 --> 50:15.800\n Really?\n\n50:15.800 --> 50:18.640\n And then it goes back in a box, you know?\n\n50:18.640 --> 50:19.960\n So on that point,\n\n50:19.960 --> 50:23.440\n if we continue looking a little bit into the future,\n\n50:23.440 --> 50:25.200\n do you think an AI that's empathetic\n\n50:25.200 --> 50:28.960\n and does improve our lives\n\n50:28.960 --> 50:31.840\n need to have a physical presence, a body?\n\n50:31.840 --> 50:36.840\n And even let me cautiously say the C word consciousness\n\n50:38.720 --> 50:40.760\n and even fear of mortality.\n\n50:40.760 --> 50:42.760\n So some of those human characteristics,\n\n50:42.760 --> 50:45.920\n do you think it needs to have those aspects\n\n50:45.920 --> 50:50.880\n or can it remain simply a machine learning tool\n\n50:50.880 --> 50:53.400\n that learns from data of behavior\n\n50:53.400 --> 50:56.640\n that learns to make us,\n\n50:56.640 --> 51:00.080\n based on previous patterns, feel better?\n\n51:00.080 --> 51:02.560\n Or does it need those elements of consciousness?\n\n51:02.560 --> 51:03.760\n It depends on your goals.\n\n51:03.760 --> 51:06.720\n If you're making a movie, it needs a body.\n\n51:06.720 --> 51:08.000\n It needs a gorgeous body.\n\n51:08.000 --> 51:10.040\n It needs to act like it has consciousness.\n\n51:10.040 --> 51:11.680\n It needs to act like it has emotion, right?\n\n51:11.680 --> 51:13.360\n Because that's what sells.\n\n51:13.360 --> 51:16.280\n That's what's gonna get me to show up and enjoy the movie.\n\n51:16.280 --> 51:17.800\n Okay.\n\n51:17.800 --> 51:19.800\n In real life, does it need all that?\n\n51:19.800 --> 51:21.840\n Well, if you've read Orson Scott Card,\n\n51:21.840 --> 51:23.520\n Ender's Game, Speaker of the Dead,\n\n51:23.520 --> 51:26.720\n it could just be like a little voice in your earring, right?\n\n51:26.720 --> 51:28.360\n And you could have an intimate relationship\n\n51:28.360 --> 51:29.520\n and it could get to know you.\n\n51:29.520 --> 51:31.640\n And it doesn't need to be a robot.\n\n51:34.160 --> 51:37.160\n But that doesn't make this compelling of a movie, right?\n\n51:37.160 --> 51:38.440\n I mean, we already think it's kind of weird\n\n51:38.440 --> 51:41.600\n when a guy looks like he's talking to himself on the train,\n\n51:41.600 --> 51:43.760\n even though it's earbuds.\n\n51:43.760 --> 51:48.760\n So we have these, embodied is more powerful.\n\n51:49.680 --> 51:51.760\n Embodied, when you compare interactions\n\n51:51.760 --> 51:55.280\n with an embodied robot versus a video of a robot\n\n51:55.280 --> 52:00.160\n versus no robot, the robot is more engaging.\n\n52:00.160 --> 52:01.720\n The robot gets our attention more.\n\n52:01.720 --> 52:03.080\n The robot, when you walk in your house,\n\n52:03.080 --> 52:05.440\n is more likely to get you to remember to do the things\n\n52:05.440 --> 52:06.480\n that you asked it to do,\n\n52:06.480 --> 52:09.000\n because it's kind of got a physical presence.\n\n52:09.000 --> 52:10.840\n You can avoid it if you don't like it.\n\n52:10.840 --> 52:12.440\n It could see you're avoiding it.\n\n52:12.440 --> 52:14.760\n There's a lot of power to being embodied.\n\n52:14.760 --> 52:17.160\n There will be embodied AIs.\n\n52:17.160 --> 52:22.040\n They have great power and opportunity and potential.\n\n52:22.040 --> 52:24.600\n There will also be AIs that aren't embodied,\n\n52:24.600 --> 52:28.520\n that just are little software assistants\n\n52:28.520 --> 52:30.240\n that help us with different things\n\n52:30.240 --> 52:32.600\n that may get to know things about us.\n\n52:33.480 --> 52:34.880\n Will they be conscious?\n\n52:34.880 --> 52:36.640\n There will be attempts to program them\n\n52:36.640 --> 52:39.280\n to make them appear to be conscious.\n\n52:39.280 --> 52:41.760\n We can already write programs that make it look like,\n\n52:41.760 --> 52:42.600\n oh, what do you mean?\n\n52:42.600 --> 52:43.800\n Of course I'm aware that you're there, right?\n\n52:43.800 --> 52:45.720\n I mean, it's trivial to say stuff like that.\n\n52:45.720 --> 52:47.800\n It's easy to fool people,\n\n52:48.720 --> 52:52.160\n but does it actually have conscious experience like we do?\n\n52:53.400 --> 52:55.920\n Nobody has a clue how to do that yet.\n\n52:55.920 --> 52:58.720\n That seems to be something that is beyond\n\n52:58.720 --> 53:01.480\n what any of us knows how to build now.\n\n53:01.480 --> 53:02.880\n Will it have to have that?\n\n53:03.720 --> 53:05.520\n I think you can get pretty far\n\n53:05.520 --> 53:07.280\n with a lot of stuff without it.\n\n53:07.280 --> 53:10.960\n But will we accord it rights?\n\n53:10.960 --> 53:13.320\n Well, that's more a political game\n\n53:13.320 --> 53:16.480\n than it is a question of real consciousness.\n\n53:16.480 --> 53:18.720\n Yeah, can you go to jail for turning off Alexa\n\n53:18.720 --> 53:23.720\n is the question for an election maybe a few decades from now.\n\n53:24.960 --> 53:27.640\n Well, Sophia Robot's already been given rights\n\n53:27.640 --> 53:30.040\n as a citizen in Saudi Arabia, right?\n\n53:30.040 --> 53:33.680\n Even before women have full rights.\n\n53:33.680 --> 53:36.960\n Then the robot was still put back in the box\n\n53:36.960 --> 53:39.600\n to be shipped to the next place\n\n53:39.600 --> 53:41.760\n where it would get a paid appearance, right?\n\n53:42.760 --> 53:47.760\n Yeah, it's dark and almost comedic, if not absurd.\n\n53:50.160 --> 53:54.640\n So I've heard you speak about your journey in finding faith.\n\n53:54.640 --> 53:55.960\n Sure.\n\n53:55.960 --> 54:00.040\n And how you discovered some wisdoms about life\n\n54:00.040 --> 54:03.240\n and beyond from reading the Bible.\n\n54:03.240 --> 54:05.440\n And I've also heard you say that,\n\n54:05.440 --> 54:07.040\n you said scientists too often assume\n\n54:07.040 --> 54:10.360\n that nothing exists beyond what can be currently measured.\n\n54:11.360 --> 54:12.640\n Yeah, materialism.\n\n54:12.640 --> 54:13.480\n Materialism.\n\n54:13.480 --> 54:14.880\n And scientism, yeah.\n\n54:14.880 --> 54:17.360\n So in some sense, this assumption enables\n\n54:17.360 --> 54:20.280\n the near term scientific method,\n\n54:20.280 --> 54:25.280\n assuming that we can uncover the mysteries of this world\n\n54:25.360 --> 54:28.280\n by the mechanisms of measurement that we currently have.\n\n54:28.280 --> 54:32.400\n But we easily forget that we've made this assumption.\n\n54:33.960 --> 54:35.640\n So what do you think we miss out on\n\n54:35.640 --> 54:37.120\n by making that assumption?\n\n54:38.760 --> 54:42.640\n It's fine to limit the scientific method\n\n54:42.640 --> 54:46.840\n to things we can measure and reason about and reproduce.\n\n54:47.720 --> 54:48.640\n That's fine.\n\n54:49.640 --> 54:51.160\n I think we have to recognize\n\n54:51.160 --> 54:53.160\n that sometimes we scientists also believe\n\n54:53.160 --> 54:55.560\n in things that happen historically.\n\n54:55.560 --> 54:57.920\n Like I believe the Holocaust happened.\n\n54:57.920 --> 55:02.920\n I can't prove events from past history scientifically.\n\n55:03.880 --> 55:06.480\n You prove them with historical evidence, right?\n\n55:06.480 --> 55:08.200\n With the impact they had on people,\n\n55:08.200 --> 55:11.640\n with eyewitness testimony and things like that.\n\n55:11.640 --> 55:15.680\n So a good thinker recognizes that science\n\n55:15.680 --> 55:19.440\n is one of many ways to get knowledge.\n\n55:19.440 --> 55:21.640\n It's not the only way.\n\n55:21.640 --> 55:24.280\n And there's been some really bad philosophy\n\n55:24.280 --> 55:27.840\n and bad thinking recently, you can call it scientism,\n\n55:27.840 --> 55:31.200\n where people say science is the only way to get to truth.\n\n55:31.200 --> 55:33.360\n And it's not, it just isn't.\n\n55:33.360 --> 55:35.960\n There are other ways that work also.\n\n55:35.960 --> 55:38.520\n Like knowledge of love with someone.\n\n55:38.520 --> 55:42.720\n You don't prove your love through science, right?\n\n55:43.600 --> 55:48.160\n So history, philosophy, love,\n\n55:48.160 --> 55:50.840\n a lot of other things in life show us\n\n55:50.840 --> 55:55.840\n that there's more ways to gain knowledge and truth\n\n55:55.840 --> 55:57.800\n if you're willing to believe there is such a thing,\n\n55:57.800 --> 56:01.040\n and I believe there is, than science.\n\n56:01.040 --> 56:03.080\n I do, I am a scientist, however.\n\n56:03.080 --> 56:05.880\n And in my science, I do limit my science\n\n56:05.880 --> 56:09.600\n to the things that the scientific method can do.\n\n56:09.600 --> 56:11.800\n But I recognize that it's myopic\n\n56:11.800 --> 56:13.720\n to say that that's all there is.\n\n56:13.720 --> 56:15.680\n Right, there's, just like you listed,\n\n56:15.680 --> 56:17.120\n there's all the why questions.\n\n56:17.120 --> 56:20.520\n And really we know, if we're being honest with ourselves,\n\n56:20.520 --> 56:25.520\n the percent of what we really know is basically zero\n\n56:25.520 --> 56:28.120\n relative to the full mystery of the...\n\n56:28.120 --> 56:30.360\n Measure theory, a set of measure zero,\n\n56:30.360 --> 56:33.120\n if I have a finite amount of knowledge, which I do.\n\n56:34.520 --> 56:36.880\n So you said that you believe in truth.\n\n56:37.960 --> 56:40.600\n So let me ask that old question.\n\n56:40.600 --> 56:42.800\n What do you think this thing is all about?\n\n56:42.800 --> 56:44.800\n What's the life on earth?\n\n56:44.800 --> 56:46.200\n Life, the universe, and everything?\n\n56:46.200 --> 56:47.040\n And everything, what's the meaning?\n\n56:47.040 --> 56:49.680\n I can't quote Douglas Adams 42.\n\n56:49.680 --> 56:51.240\n It's my favorite number.\n\n56:51.240 --> 56:52.640\n By the way, that's my street address.\n\n56:52.640 --> 56:54.880\n My husband and I guessed the exact same number\n\n56:54.880 --> 56:56.720\n for our house, we got to pick it.\n\n56:57.760 --> 57:00.120\n And there's a reason we picked 42, yeah.\n\n57:00.120 --> 57:02.320\n So is it just 42 or is there,\n\n57:02.320 --> 57:05.360\n do you have other words that you can put around it?\n\n57:05.360 --> 57:07.760\n Well, I think there's a grand adventure\n\n57:07.760 --> 57:09.840\n and I think this life is a part of it.\n\n57:09.840 --> 57:12.200\n I think there's a lot more to it than meets the eye\n\n57:12.200 --> 57:14.440\n and the heart and the mind and the soul here.\n\n57:14.440 --> 57:18.000\n I think we see but through a glass dimly in this life.\n\n57:18.000 --> 57:21.720\n We see only a part of all there is to know.\n\n57:22.720 --> 57:25.800\n If people haven't read the Bible, they should,\n\n57:25.800 --> 57:27.920\n if they consider themselves educated\n\n57:27.920 --> 57:30.400\n and you could read Proverbs\n\n57:30.400 --> 57:33.320\n and find tremendous wisdom in there\n\n57:33.320 --> 57:35.680\n that cannot be scientifically proven.\n\n57:35.680 --> 57:38.200\n But when you read it, there's something in you,\n\n57:38.200 --> 57:41.160\n like a musician knows when the instruments played right\n\n57:41.160 --> 57:42.680\n and it's beautiful.\n\n57:42.680 --> 57:45.200\n There's something in you that comes alive\n\n57:45.200 --> 57:47.240\n and knows that there's a truth there\n\n57:47.240 --> 57:50.160\n that it's like your strings are being plucked by the master\n\n57:50.160 --> 57:54.240\n instead of by me, right, playing when I pluck it.\n\n57:54.240 --> 57:57.400\n But probably when you play, it sounds spectacular, right?\n\n57:57.400 --> 58:01.520\n And when you encounter those truths,\n\n58:01.520 --> 58:03.240\n there's something in you that sings\n\n58:03.240 --> 58:06.320\n and knows that there is more\n\n58:06.320 --> 58:09.200\n than what I can prove mathematically\n\n58:09.200 --> 58:11.280\n or program a computer to do.\n\n58:11.280 --> 58:13.360\n Don't get me wrong, the math is gorgeous.\n\n58:13.360 --> 58:16.040\n The computer programming can be brilliant.\n\n58:16.040 --> 58:17.200\n It's inspiring, right?\n\n58:17.200 --> 58:19.200\n We wanna do more.\n\n58:19.200 --> 58:21.840\n None of this squashes my desire to do science\n\n58:21.840 --> 58:23.640\n or to get knowledge through science.\n\n58:23.640 --> 58:26.040\n I'm not dissing the science at all.\n\n58:26.040 --> 58:29.560\n I grow even more in awe of what the science can do\n\n58:29.560 --> 58:33.000\n because I'm more in awe of all there is we don't know.\n\n58:33.000 --> 58:36.040\n And really at the heart of science,\n\n58:36.040 --> 58:38.200\n you have to have a belief that there's truth,\n\n58:38.200 --> 58:41.080\n that there's something greater to be discovered.\n\n58:41.080 --> 58:44.480\n And some scientists may not wanna use the faith word,\n\n58:44.480 --> 58:47.800\n but it's faith that drives us to do science.\n\n58:47.800 --> 58:49.920\n It's faith that there is truth,\n\n58:49.920 --> 58:52.520\n that there's something to know that we don't know,\n\n58:52.520 --> 58:56.000\n that it's worth knowing, that it's worth working hard,\n\n58:56.000 --> 58:58.320\n and that there is meaning,\n\n58:58.320 --> 58:59.880\n that there is such a thing as meaning,\n\n58:59.880 --> 59:02.720\n which by the way, science can't prove either.\n\n59:02.720 --> 59:04.760\n We have to kind of start with some assumptions\n\n59:04.760 --> 59:06.880\n that there's things like truth and meaning.\n\n59:06.880 --> 59:10.680\n And these are really questions philosophers own, right?\n\n59:10.680 --> 59:11.760\n This is their space,\n\n59:11.760 --> 59:14.560\n of philosophers and theologians at some level.\n\n59:14.560 --> 59:17.040\n So these are things science,\n\n59:19.160 --> 59:23.000\n when people claim that science will tell you all truth,\n\n59:23.000 --> 59:23.920\n there's a name for that.\n\n59:23.920 --> 59:25.600\n It's its own kind of faith.\n\n59:25.600 --> 59:29.000\n It's scientism and it's very myopic.\n\n59:29.000 --> 59:32.360\n Yeah, there's a much bigger world out there to be explored\n\n59:32.360 --> 59:34.800\n in ways that science may not,\n\n59:34.800 --> 59:37.120\n at least for now, allow us to explore.\n\n59:37.120 --> 59:40.080\n Yeah, and there's meaning and purpose and hope\n\n59:40.080 --> 59:43.240\n and joy and love and all these awesome things\n\n59:43.240 --> 59:45.480\n that make it all worthwhile too.\n\n59:45.480 --> 59:47.680\n I don't think there's a better way to end it, Roz.\n\n59:47.680 --> 59:49.040\n Thank you so much for talking today.\n\n59:49.040 --> 59:50.360\n Thanks Lex, what a pleasure.\n\n59:50.360 --> 1:00:11.080\n Great questions.\n\n"
}