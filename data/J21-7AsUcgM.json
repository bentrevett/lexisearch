{
  "title": "Ayanna Howard: Human-Robot Interaction & Ethics of Safety-Critical Systems | Lex Fridman Podcast #66",
  "id": "J21-7AsUcgM",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:03.380\n The following is a conversation with Ayana Howard.\n\n00:03.380 --> 00:06.180\n She's a roboticist, professor Georgia Tech,\n\n00:06.180 --> 00:09.820\n and director of the Human Automation Systems Lab,\n\n00:09.820 --> 00:12.780\n with research interests in human robot interaction,\n\n00:12.780 --> 00:15.980\n assisted robots in the home, therapy gaming apps,\n\n00:15.980 --> 00:20.260\n and remote robotic exploration of extreme environments.\n\n00:20.260 --> 00:23.420\n Like me, in her work, she cares a lot\n\n00:23.420 --> 00:26.340\n about both robots and human beings,\n\n00:26.340 --> 00:29.540\n and so I really enjoyed this conversation.\n\n00:29.540 --> 00:32.580\n This is the Artificial Intelligence Podcast.\n\n00:32.580 --> 00:34.940\n If you enjoy it, subscribe on YouTube,\n\n00:34.940 --> 00:36.940\n give it five stars on Apple Podcast,\n\n00:36.940 --> 00:39.580\n follow on Spotify, support it on Patreon,\n\n00:39.580 --> 00:41.700\n or simply connect with me on Twitter\n\n00:41.700 --> 00:45.640\n at Lex Friedman, spelled F R I D M A N.\n\n00:45.640 --> 00:47.140\n I recently started doing ads\n\n00:47.140 --> 00:48.700\n at the end of the introduction.\n\n00:48.700 --> 00:51.660\n I'll do one or two minutes after introducing the episode,\n\n00:51.660 --> 00:53.180\n and never any ads in the middle\n\n00:53.180 --> 00:55.500\n that can break the flow of the conversation.\n\n00:55.500 --> 00:56.860\n I hope that works for you\n\n00:56.860 --> 01:00.140\n and doesn't hurt the listening experience.\n\n01:00.140 --> 01:02.260\n This show is presented by Cash App,\n\n01:02.260 --> 01:04.740\n the number one finance app in the App Store.\n\n01:04.740 --> 01:07.540\n I personally use Cash App to send money to friends,\n\n01:07.540 --> 01:09.300\n but you can also use it to buy, sell,\n\n01:09.300 --> 01:11.700\n and deposit Bitcoin in just seconds.\n\n01:11.700 --> 01:14.580\n Cash App also has a new investing feature.\n\n01:14.580 --> 01:17.520\n You can buy fractions of a stock, say $1 worth,\n\n01:17.520 --> 01:19.640\n no matter what the stock price is.\n\n01:19.640 --> 01:22.560\n Broker services are provided by Cash App Investing,\n\n01:22.560 --> 01:25.840\n a subsidiary of Square and Member SIPC.\n\n01:25.840 --> 01:28.140\n I'm excited to be working with Cash App\n\n01:28.140 --> 01:31.540\n to support one of my favorite organizations called First,\n\n01:31.540 --> 01:35.060\n best known for their FIRST Robotics and Lego competitions.\n\n01:35.060 --> 01:38.340\n They educate and inspire hundreds of thousands of students\n\n01:38.340 --> 01:40.060\n in over 110 countries,\n\n01:40.060 --> 01:42.820\n and have a perfect rating at Charity Navigator,\n\n01:42.820 --> 01:44.100\n which means that donated money\n\n01:44.100 --> 01:46.860\n is used to maximum effectiveness.\n\n01:46.860 --> 01:49.580\n When you get Cash App from the App Store or Google Play\n\n01:49.580 --> 01:53.500\n and use code LEXPODCAST, you'll get $10,\n\n01:53.500 --> 01:56.420\n and Cash App will also donate $10 to FIRST,\n\n01:56.420 --> 01:58.260\n which again, is an organization\n\n01:58.260 --> 02:01.060\n that I've personally seen inspire girls and boys\n\n02:01.060 --> 02:04.260\n to dream of engineering a better world.\n\n02:04.260 --> 02:08.300\n And now, here's my conversation with Ayanna Howard.\n\n02:09.420 --> 02:13.620\n What or who is the most amazing robot you've ever met,\n\n02:13.620 --> 02:16.700\n or perhaps had the biggest impact on your career?\n\n02:16.700 --> 02:21.060\n I haven't met her, but I grew up with her,\n\n02:21.060 --> 02:22.740\n but of course, Rosie.\n\n02:22.740 --> 02:25.220\n So, and I think it's because also.\n\n02:25.220 --> 02:26.120\n Who's Rosie?\n\n02:26.120 --> 02:27.780\n Rosie from the Jetsons.\n\n02:27.780 --> 02:30.940\n She is all things to all people, right?\n\n02:30.940 --> 02:31.780\n Think about it.\n\n02:31.780 --> 02:35.060\n Like anything you wanted, it was like magic, it happened.\n\n02:35.060 --> 02:37.860\n So people not only anthropomorphize,\n\n02:37.860 --> 02:41.940\n but project whatever they wish for the robot to be onto.\n\n02:41.940 --> 02:42.920\n Onto Rosie.\n\n02:42.920 --> 02:44.580\n But also, I mean, think about it.\n\n02:44.580 --> 02:46.780\n She was socially engaging.\n\n02:46.780 --> 02:50.020\n She every so often had an attitude, right?\n\n02:50.020 --> 02:51.940\n She kept us honest.\n\n02:51.940 --> 02:53.740\n She would push back sometimes\n\n02:53.740 --> 02:56.980\n when George was doing some weird stuff.\n\n02:56.980 --> 02:59.800\n But she cared about people, especially the kids.\n\n03:01.180 --> 03:03.980\n She was like the perfect robot.\n\n03:03.980 --> 03:06.460\n And you've said that people don't want\n\n03:06.460 --> 03:08.320\n their robots to be perfect.\n\n03:09.740 --> 03:11.140\n Can you elaborate that?\n\n03:11.140 --> 03:11.980\n What do you think that is?\n\n03:11.980 --> 03:14.780\n Just like you said, Rosie pushed back a little bit\n\n03:14.780 --> 03:15.720\n every once in a while.\n\n03:15.720 --> 03:18.260\n Yeah, so I think it's that.\n\n03:18.260 --> 03:19.860\n So if you think about robotics in general,\n\n03:19.860 --> 03:23.900\n we want them because they enhance our quality of life.\n\n03:23.900 --> 03:27.000\n And usually that's linked to something that's functional.\n\n03:27.000 --> 03:28.640\n Even if you think of self driving cars,\n\n03:28.640 --> 03:29.980\n why is there a fascination?\n\n03:29.980 --> 03:31.500\n Because people really do hate to drive.\n\n03:31.500 --> 03:34.140\n Like there's the like Saturday driving\n\n03:34.140 --> 03:35.300\n where I can just speed,\n\n03:35.300 --> 03:37.500\n but then there's the I have to go to work every day\n\n03:37.500 --> 03:38.980\n and I'm in traffic for an hour.\n\n03:38.980 --> 03:40.380\n I mean, people really hate that.\n\n03:40.380 --> 03:45.380\n And so robots are designed to basically enhance\n\n03:45.380 --> 03:49.740\n our ability to increase our quality of life.\n\n03:49.740 --> 03:54.300\n And so the perfection comes from this aspect of interaction.\n\n03:55.460 --> 04:00.020\n If I think about how we drive, if we drove perfectly,\n\n04:00.020 --> 04:02.140\n we would never get anywhere, right?\n\n04:02.140 --> 04:07.140\n So think about how many times you had to run past the light\n\n04:07.140 --> 04:09.020\n because you see the car behind you\n\n04:09.020 --> 04:10.380\n is about to crash into you.\n\n04:10.380 --> 04:15.320\n Or that little kid kind of runs into the street\n\n04:15.320 --> 04:17.300\n and so you have to cross on the other side\n\n04:17.300 --> 04:18.460\n because there's no cars, right?\n\n04:18.460 --> 04:21.220\n Like if you think about it, we are not perfect drivers.\n\n04:21.220 --> 04:23.580\n Some of it is because it's our world.\n\n04:23.580 --> 04:26.780\n And so if you have a robot that is perfect\n\n04:26.780 --> 04:28.740\n in that sense of the word,\n\n04:28.740 --> 04:31.180\n they wouldn't really be able to function with us.\n\n04:31.180 --> 04:34.520\n Can you linger a little bit on the word perfection?\n\n04:34.520 --> 04:37.380\n So from the robotics perspective,\n\n04:37.380 --> 04:39.460\n what does that word mean\n\n04:39.460 --> 04:42.900\n and how is sort of the optimal behavior\n\n04:42.900 --> 04:44.460\n as you're describing different\n\n04:44.460 --> 04:46.620\n than what we think is perfection?\n\n04:46.620 --> 04:49.460\n Yeah, so perfection, if you think about it\n\n04:49.460 --> 04:51.980\n in the more theoretical point of view,\n\n04:51.980 --> 04:54.060\n it's really tied to accuracy, right?\n\n04:54.060 --> 04:55.620\n So if I have a function,\n\n04:55.620 --> 04:59.480\n can I complete it at 100% accuracy with zero errors?\n\n05:00.660 --> 05:04.180\n And so that's kind of, if you think about perfection\n\n05:04.180 --> 05:05.220\n in the sense of the word.\n\n05:05.220 --> 05:07.500\n And in the self driving car realm,\n\n05:07.500 --> 05:10.460\n do you think from a robotics perspective,\n\n05:10.460 --> 05:13.940\n we kind of think that perfection means\n\n05:13.940 --> 05:15.580\n following the rules perfectly,\n\n05:15.580 --> 05:19.580\n sort of defining, staying in the lane, changing lanes.\n\n05:19.580 --> 05:20.900\n When there's a green light, you go.\n\n05:20.900 --> 05:22.300\n When there's a red light, you stop.\n\n05:22.300 --> 05:26.660\n And that's the, and be able to perfectly see\n\n05:26.660 --> 05:29.140\n all the entities in the scene.\n\n05:29.140 --> 05:31.980\n That's the limit of what we think of as perfection.\n\n05:31.980 --> 05:33.740\n And I think that's where the problem comes\n\n05:33.740 --> 05:38.340\n is that when people think about perfection for robotics,\n\n05:38.340 --> 05:40.820\n the ones that are the most successful\n\n05:40.820 --> 05:43.260\n are the ones that are quote unquote perfect.\n\n05:43.260 --> 05:44.660\n Like I said, Rosie is perfect,\n\n05:44.660 --> 05:47.380\n but she actually wasn't perfect in terms of accuracy,\n\n05:47.380 --> 05:50.380\n but she was perfect in terms of how she interacted\n\n05:50.380 --> 05:51.540\n and how she adapted.\n\n05:51.540 --> 05:53.300\n And I think that's some of the disconnect\n\n05:53.300 --> 05:56.460\n is that we really want perfection\n\n05:56.460 --> 05:59.980\n with respect to its ability to adapt to us.\n\n05:59.980 --> 06:03.500\n We don't really want perfection with respect to 100% accuracy\n\n06:03.500 --> 06:06.780\n with respect to the rules that we just made up anyway, right?\n\n06:06.780 --> 06:09.500\n And so I think there's this disconnect sometimes\n\n06:09.500 --> 06:13.260\n between what we really want and what happens.\n\n06:13.260 --> 06:15.940\n And we see this all the time, like in my research, right?\n\n06:15.940 --> 06:20.340\n Like the optimal, quote unquote optimal interactions\n\n06:20.340 --> 06:24.300\n are when the robot is adapting based on the person,\n\n06:24.300 --> 06:29.300\n not 100% following what's optimal based on the rules.\n\n06:29.540 --> 06:32.580\n Just to link on autonomous vehicles for a second,\n\n06:32.580 --> 06:35.020\n just your thoughts, maybe off the top of the head,\n\n06:36.180 --> 06:37.940\n how hard is that problem do you think\n\n06:37.940 --> 06:40.100\n based on what we just talked about?\n\n06:40.100 --> 06:42.900\n There's a lot of folks in the automotive industry,\n\n06:42.900 --> 06:45.900\n they're very confident from Elon Musk to Waymo\n\n06:45.900 --> 06:47.620\n to all these companies.\n\n06:47.620 --> 06:50.420\n How hard is it to solve that last piece?\n\n06:50.420 --> 06:51.340\n The last mile.\n\n06:51.340 --> 06:56.340\n The gap between the perfection and the human definition\n\n06:57.500 --> 06:59.460\n of how you actually function in this world.\n\n06:59.460 --> 07:00.580\n Yeah, so this is a moving target.\n\n07:00.580 --> 07:04.460\n So I remember when all the big companies\n\n07:04.460 --> 07:06.780\n started to heavily invest in this\n\n07:06.780 --> 07:09.860\n and there was a number of even roboticists\n\n07:09.860 --> 07:13.180\n as well as folks who were putting in the VCs\n\n07:13.180 --> 07:16.660\n and corporations, Elon Musk being one of them that said,\n\n07:16.660 --> 07:19.460\n self driving cars on the road with people\n\n07:19.460 --> 07:24.180\n within five years, that was a little while ago.\n\n07:24.180 --> 07:29.180\n And now people are saying five years, 10 years, 20 years,\n\n07:29.780 --> 07:31.500\n some are saying never, right?\n\n07:31.500 --> 07:33.700\n I think if you look at some of the things\n\n07:33.700 --> 07:37.620\n that are being successful is these\n\n07:39.420 --> 07:41.140\n basically fixed environments\n\n07:41.140 --> 07:43.980\n where you still have some anomalies, right?\n\n07:43.980 --> 07:46.460\n You still have people walking, you still have stores,\n\n07:46.460 --> 07:50.060\n but you don't have other drivers, right?\n\n07:50.060 --> 07:51.700\n Like other human drivers are,\n\n07:51.700 --> 07:55.580\n it's a dedicated space for the cars.\n\n07:55.580 --> 07:57.140\n Because if you think about robotics in general,\n\n07:57.140 --> 07:59.020\n where has always been successful?\n\n07:59.020 --> 08:00.580\n I mean, you can say manufacturing,\n\n08:00.580 --> 08:02.260\n like way back in the day, right?\n\n08:02.260 --> 08:04.340\n It was a fixed environment, humans were not part\n\n08:04.340 --> 08:07.180\n of the equation, we're a lot better than that.\n\n08:07.180 --> 08:10.940\n But like when we can carve out scenarios\n\n08:10.940 --> 08:13.780\n that are closer to that space,\n\n08:13.780 --> 08:16.660\n then I think that it's where we are.\n\n08:16.660 --> 08:20.540\n So a closed campus where you don't have self driving cars\n\n08:20.540 --> 08:23.780\n and maybe some protection so that the students\n\n08:23.780 --> 08:27.220\n don't jet in front just because they wanna see what happens.\n\n08:27.220 --> 08:29.940\n Like having a little bit, I think that's where\n\n08:29.940 --> 08:32.300\n we're gonna see the most success in the near future.\n\n08:32.300 --> 08:33.660\n And be slow moving.\n\n08:33.660 --> 08:37.900\n Right, not 55, 60, 70 miles an hour,\n\n08:37.900 --> 08:42.100\n but the speed of a golf cart, right?\n\n08:42.100 --> 08:45.220\n So that said, the most successful\n\n08:45.220 --> 08:47.900\n in the automotive industry robots operating today\n\n08:47.900 --> 08:51.600\n in the hands of real people are ones that are traveling\n\n08:51.600 --> 08:55.540\n over 55 miles an hour and in unconstrained environments,\n\n08:55.540 --> 08:58.880\n which is Tesla vehicles, so Tesla autopilot.\n\n08:58.880 --> 09:01.720\n So I would love to hear sort of your,\n\n09:01.720 --> 09:04.300\n just thoughts of two things.\n\n09:04.300 --> 09:07.020\n So one, I don't know if you've gotten to see,\n\n09:07.020 --> 09:10.020\n you've heard about something called smart summon\n\n09:10.020 --> 09:13.520\n where Tesla system, autopilot system,\n\n09:13.520 --> 09:17.140\n where the car drives zero occupancy, no driver\n\n09:17.140 --> 09:19.980\n in the parking lot slowly sort of tries to navigate\n\n09:19.980 --> 09:22.720\n the parking lot to find itself to you.\n\n09:22.720 --> 09:25.900\n And there's some incredible amounts of videos\n\n09:25.900 --> 09:28.860\n and just hilarity that happens as it awkwardly tries\n\n09:28.860 --> 09:32.340\n to navigate this environment, but it's a beautiful\n\n09:32.340 --> 09:35.180\n nonverbal communication between machine and human\n\n09:35.180 --> 09:38.780\n that I think is a, it's like, it's some of the work\n\n09:38.780 --> 09:40.660\n that you do in this kind of interesting\n\n09:40.660 --> 09:42.060\n human robot interaction space.\n\n09:42.060 --> 09:43.780\n So what are your thoughts in general about it?\n\n09:43.780 --> 09:46.120\n So I do have that feature.\n\n09:46.980 --> 09:47.820\n Do you drive a Tesla?\n\n09:47.820 --> 09:52.100\n I do, mainly because I'm a gadget freak, right?\n\n09:52.100 --> 09:55.620\n So I say it's a gadget that happens to have some wheels.\n\n09:55.620 --> 09:58.220\n And yeah, I've seen some of the videos.\n\n09:58.220 --> 09:59.420\n But what's your experience like?\n\n09:59.420 --> 10:02.700\n I mean, you're a human robot interaction roboticist,\n\n10:02.700 --> 10:05.580\n you're a legit sort of expert in the field.\n\n10:05.580 --> 10:08.260\n So what does it feel for a machine to come to you?\n\n10:08.260 --> 10:11.900\n It's one of these very fascinating things,\n\n10:11.900 --> 10:16.100\n but also I am hyper, hyper alert, right?\n\n10:16.100 --> 10:20.540\n Like I'm hyper alert, like my butt, my thumb is like,\n\n10:20.540 --> 10:23.220\n oh, okay, I'm ready to take over.\n\n10:23.220 --> 10:27.080\n Even when I'm in my car or I'm doing things like automated\n\n10:27.080 --> 10:30.420\n backing into, so there's like a feature where you can do\n\n10:30.420 --> 10:33.140\n this automating backing into a parking space,\n\n10:33.140 --> 10:35.660\n or bring the car out of your garage,\n\n10:35.660 --> 10:40.260\n or even, you know, pseudo autopilot on the freeway, right?\n\n10:40.260 --> 10:42.220\n I am hypersensitive.\n\n10:42.220 --> 10:44.720\n I can feel like as I'm navigating,\n\n10:44.720 --> 10:46.900\n like, yeah, that's an error right there.\n\n10:46.900 --> 10:51.900\n Like I am very aware of it, but I'm also fascinated by it.\n\n10:52.260 --> 10:54.300\n And it does get better.\n\n10:54.300 --> 10:58.980\n Like I look and see it's learning from all of these people\n\n10:58.980 --> 11:02.700\n who are cutting it on, like every time I cut it on,\n\n11:02.700 --> 11:04.120\n it's getting better, right?\n\n11:04.120 --> 11:07.100\n And so I think that's what's amazing about it is that.\n\n11:07.100 --> 11:10.340\n This nice dance of you're still hyper vigilant.\n\n11:10.340 --> 11:12.780\n So you're still not trusting it at all.\n\n11:12.780 --> 11:13.600\n Yeah.\n\n11:13.600 --> 11:14.580\n And yet you're using it.\n\n11:14.580 --> 11:17.580\n On the highway, if I were to, like what,\n\n11:17.580 --> 11:20.260\n as a roboticist, we'll talk about trust a little bit.\n\n11:22.640 --> 11:23.640\n How do you explain that?\n\n11:23.640 --> 11:25.020\n You still use it.\n\n11:25.020 --> 11:26.460\n Is it the gadget freak part?\n\n11:26.460 --> 11:30.700\n Like where you just enjoy exploring technology?\n\n11:30.700 --> 11:33.680\n Or is that the right actually balance\n\n11:33.680 --> 11:36.860\n between robotics and humans is where you use it,\n\n11:36.860 --> 11:38.340\n but don't trust it.\n\n11:38.340 --> 11:40.100\n And somehow there's this dance\n\n11:40.100 --> 11:42.100\n that ultimately is a positive.\n\n11:42.100 --> 11:44.620\n Yeah, so I think I'm,\n\n11:44.620 --> 11:48.080\n I just don't necessarily trust technology,\n\n11:48.080 --> 11:50.140\n but I'm an early adopter, right?\n\n11:50.140 --> 11:51.960\n So when it first comes out,\n\n11:51.960 --> 11:54.260\n I will use everything,\n\n11:54.260 --> 11:57.420\n but I will be very, very cautious of how I use it.\n\n11:57.420 --> 12:01.020\n Do you read about it or do you explore it by just try it?\n\n12:01.020 --> 12:04.980\n Do you like crudely, to put it crudely,\n\n12:04.980 --> 12:07.960\n do you read the manual or do you learn through exploration?\n\n12:07.960 --> 12:08.800\n I'm an explorer.\n\n12:08.800 --> 12:12.320\n If I have to read the manual, then I do design.\n\n12:12.320 --> 12:14.180\n Then it's a bad user interface.\n\n12:14.180 --> 12:15.060\n It's a failure.\n\n12:16.460 --> 12:19.540\n Elon Musk is very confident that you kind of take it\n\n12:19.540 --> 12:21.780\n from where it is now to full autonomy.\n\n12:21.780 --> 12:24.500\n So from this human robot interaction,\n\n12:24.500 --> 12:26.700\n where you don't really trust and then you try\n\n12:26.700 --> 12:29.180\n and then you catch it when it fails to,\n\n12:29.180 --> 12:32.300\n it's going to incrementally improve itself\n\n12:32.300 --> 12:36.500\n into full where you don't need to participate.\n\n12:36.500 --> 12:39.860\n What's your sense of that trajectory?\n\n12:39.860 --> 12:41.040\n Is it feasible?\n\n12:41.040 --> 12:44.580\n So the promise there is by the end of next year,\n\n12:44.580 --> 12:47.180\n by the end of 2020 is the current promise.\n\n12:47.180 --> 12:52.180\n What's your sense about that journey that Tesla's on?\n\n12:53.620 --> 12:56.580\n So there's kind of three things going on though.\n\n12:56.580 --> 13:01.580\n I think in terms of will people go like as a user,\n\n13:03.260 --> 13:08.260\n as a adopter, will you trust going to that point?\n\n13:08.460 --> 13:10.080\n I think so, right?\n\n13:10.080 --> 13:13.020\n Like there are some users and it's because what happens is\n\n13:13.020 --> 13:16.700\n when you're hypersensitive at the beginning\n\n13:16.700 --> 13:19.300\n and then the technology tends to work,\n\n13:19.300 --> 13:23.820\n your apprehension slowly goes away.\n\n13:23.820 --> 13:28.260\n And as people, we tend to swing to the other extreme, right?\n\n13:28.260 --> 13:30.900\n Because it's like, oh, I was like hyper, hyper fearful\n\n13:30.900 --> 13:33.940\n or hypersensitive and it was awesome.\n\n13:33.940 --> 13:35.600\n And we just tend to swing.\n\n13:35.600 --> 13:37.380\n That's just human nature.\n\n13:37.380 --> 13:38.860\n And so you will have, I mean, and I...\n\n13:38.860 --> 13:41.520\n That's a scary notion because most people\n\n13:41.520 --> 13:44.980\n are now extremely untrusting of autopilot.\n\n13:44.980 --> 13:46.460\n They use it, but they don't trust it.\n\n13:46.460 --> 13:48.900\n And it's a scary notion that there's a certain point\n\n13:48.900 --> 13:51.340\n where you allow yourself to look at the smartphone\n\n13:51.340 --> 13:53.100\n for like 20 seconds.\n\n13:53.100 --> 13:55.300\n And then there'll be this phase shift\n\n13:55.300 --> 13:57.580\n where it'll be like 20 seconds, 30 seconds,\n\n13:57.580 --> 13:58.940\n one minute, two minutes.\n\n13:59.980 --> 14:02.020\n It's a scary proposition.\n\n14:02.020 --> 14:03.460\n But that's people, right?\n\n14:03.460 --> 14:05.560\n That's just, that's humans.\n\n14:05.560 --> 14:09.980\n I mean, I think of even our use of,\n\n14:09.980 --> 14:12.380\n I mean, just everything on the internet, right?\n\n14:12.380 --> 14:16.860\n Like think about how reliant we are on certain apps\n\n14:16.860 --> 14:19.380\n and certain engines, right?\n\n14:20.260 --> 14:22.680\n 20 years ago, people have been like, oh yeah, that's stupid.\n\n14:22.680 --> 14:23.940\n Like that makes no sense.\n\n14:23.940 --> 14:25.900\n Like, of course that's false.\n\n14:25.900 --> 14:29.100\n Like now it's just like, oh, of course I've been using it.\n\n14:29.100 --> 14:30.740\n It's been correct all this time.\n\n14:30.740 --> 14:34.340\n Of course aliens, I didn't think they existed,\n\n14:34.340 --> 14:37.620\n but now it says they do, obviously.\n\n14:37.620 --> 14:39.500\n 100%, earth is flat.\n\n14:39.500 --> 14:43.860\n So, okay, but you said three things.\n\n14:43.860 --> 14:44.700\n So one is the human.\n\n14:44.700 --> 14:45.820\n Okay, so one is the human.\n\n14:45.820 --> 14:47.820\n And I think there will be a group of individuals\n\n14:47.820 --> 14:49.580\n that will swing, right?\n\n14:49.580 --> 14:50.420\n I just.\n\n14:50.420 --> 14:51.260\n Teenagers.\n\n14:51.260 --> 14:54.380\n Teenage, I mean, it'll be, it'll be adults.\n\n14:54.380 --> 14:56.400\n There's actually an age demographic\n\n14:56.400 --> 15:00.140\n that's optimal for technology adoption.\n\n15:00.140 --> 15:02.260\n And you can actually find them.\n\n15:02.260 --> 15:03.940\n And they're actually pretty easy to find.\n\n15:03.940 --> 15:06.100\n Just based on their habits, based on,\n\n15:06.100 --> 15:10.420\n so if someone like me who wasn't a roboticist\n\n15:10.420 --> 15:13.580\n would probably be the optimal kind of person, right?\n\n15:13.580 --> 15:15.660\n Early adopter, okay with technology,\n\n15:15.660 --> 15:20.020\n very comfortable and not hypersensitive, right?\n\n15:20.020 --> 15:22.700\n I'm just hypersensitive cause I designed this stuff.\n\n15:23.580 --> 15:25.940\n So there is a target demographic that will swing.\n\n15:25.940 --> 15:26.820\n The other one though,\n\n15:26.820 --> 15:31.380\n is you still have these humans that are on the road.\n\n15:31.380 --> 15:35.100\n That one is a harder, harder thing to do.\n\n15:35.100 --> 15:40.100\n And as long as we have people that are on the same streets,\n\n15:40.660 --> 15:42.480\n that's gonna be the big issue.\n\n15:42.480 --> 15:45.260\n And it's just because you can't possibly,\n\n15:45.260 --> 15:48.020\n I wanna say you can't possibly map the,\n\n15:48.020 --> 15:51.380\n some of the silliness of human drivers, right?\n\n15:51.380 --> 15:56.240\n Like as an example, when you're next to that car\n\n15:56.240 --> 15:59.780\n that has that big sticker called student driver, right?\n\n15:59.780 --> 16:04.580\n Like you are like, oh, either I'm going to like go around.\n\n16:04.580 --> 16:06.740\n Like we are, we know that that person\n\n16:06.740 --> 16:09.260\n is just gonna make mistakes that make no sense, right?\n\n16:09.260 --> 16:10.960\n How do you map that information?\n\n16:11.860 --> 16:14.300\n Or if I am in a car and I look over\n\n16:14.300 --> 16:19.220\n and I see two fairly young looking individuals\n\n16:19.220 --> 16:21.100\n and there's no student driver bumper\n\n16:21.100 --> 16:22.820\n and I see them chit chatting to each other,\n\n16:22.820 --> 16:26.140\n I'm like, oh, that's an issue, right?\n\n16:26.140 --> 16:28.420\n So how do you get that kind of information\n\n16:28.420 --> 16:33.420\n and that experience into basically an autopilot?\n\n16:35.660 --> 16:37.260\n And there's millions of cases like that\n\n16:37.260 --> 16:41.220\n where we take little hints to establish context.\n\n16:41.220 --> 16:44.360\n I mean, you said kind of beautifully poetic human things,\n\n16:44.360 --> 16:47.120\n but there's probably subtle things about the environment\n\n16:47.120 --> 16:52.120\n about it being maybe time for commuters\n\n16:52.900 --> 16:55.220\n to start going home from work\n\n16:55.220 --> 16:57.140\n and therefore you can make some kind of judgment\n\n16:57.140 --> 17:00.060\n about the group behavior of pedestrians, blah, blah, blah,\n\n17:00.060 --> 17:01.180\n and so on and so on.\n\n17:01.180 --> 17:02.660\n Or even cities, right?\n\n17:02.660 --> 17:07.100\n Like if you're in Boston, how people cross the street,\n\n17:07.100 --> 17:10.660\n like lights are not an issue versus other places\n\n17:10.660 --> 17:15.580\n where people will actually wait for the crosswalk.\n\n17:15.580 --> 17:18.940\n Seattle or somewhere peaceful.\n\n17:18.940 --> 17:22.540\n But what I've also seen sort of just even in Boston\n\n17:22.540 --> 17:25.500\n that intersection to intersection is different.\n\n17:25.500 --> 17:28.940\n So every intersection has a personality of its own.\n\n17:28.940 --> 17:30.860\n So certain neighborhoods of Boston are different.\n\n17:30.860 --> 17:35.220\n So we kind of, and based on different timing of day,\n\n17:35.220 --> 17:40.220\n at night, it's all, there's a dynamic to human behavior\n\n17:40.320 --> 17:42.420\n that we kind of figure out ourselves.\n\n17:42.420 --> 17:46.100\n We're not able to introspect and figure it out,\n\n17:46.100 --> 17:49.340\n but somehow our brain learns it.\n\n17:49.340 --> 17:50.340\n We do.\n\n17:50.340 --> 17:54.860\n And so you're saying, is there a shortcut?\n\n17:54.860 --> 17:56.420\n Is there a shortcut, though, for a robot?\n\n17:56.420 --> 17:59.060\n Is there something that could be done, you think,\n\n17:59.060 --> 18:02.660\n that, you know, that's what we humans do.\n\n18:02.660 --> 18:04.660\n It's just like bird flight, right?\n\n18:04.660 --> 18:06.500\n That's the example they give for flight.\n\n18:06.500 --> 18:09.260\n Do you necessarily need to build a bird that flies\n\n18:09.260 --> 18:10.700\n or can you do an airplane?\n\n18:11.860 --> 18:13.020\n Is there a shortcut to it?\n\n18:13.020 --> 18:16.700\n So I think the shortcut is, and I kind of,\n\n18:16.700 --> 18:19.340\n I talk about it as a fixed space,\n\n18:19.340 --> 18:23.280\n where, so imagine that there's a neighborhood\n\n18:23.280 --> 18:26.500\n that's a new smart city or a new neighborhood\n\n18:26.500 --> 18:27.540\n that says, you know what?\n\n18:27.540 --> 18:31.460\n We are going to design this new city\n\n18:31.460 --> 18:33.660\n based on supporting self driving cars.\n\n18:33.660 --> 18:37.660\n And then doing things, knowing that there's anomalies,\n\n18:37.660 --> 18:39.620\n knowing that people are like this, right?\n\n18:39.620 --> 18:42.080\n And designing it based on that assumption\n\n18:42.080 --> 18:43.940\n that like, we're gonna have this.\n\n18:43.940 --> 18:45.540\n That would be an example of a shortcut.\n\n18:45.540 --> 18:47.140\n So you still have people,\n\n18:47.140 --> 18:49.260\n but you do very specific things\n\n18:49.260 --> 18:51.740\n to try to minimize the noise a little bit\n\n18:51.740 --> 18:53.820\n as an example.\n\n18:53.820 --> 18:56.180\n And the people themselves become accepting of the notion\n\n18:56.180 --> 18:57.740\n that there's autonomous cars, right?\n\n18:57.740 --> 18:59.700\n Right, like they move into,\n\n18:59.700 --> 19:01.420\n so right now you have like a,\n\n19:01.420 --> 19:03.580\n you will have a self selection bias, right?\n\n19:03.580 --> 19:06.180\n Like individuals will move into this neighborhood\n\n19:06.180 --> 19:09.420\n knowing like this is part of like the real estate pitch,\n\n19:09.420 --> 19:10.620\n right?\n\n19:10.620 --> 19:14.140\n And so I think that's a way to do a shortcut.\n\n19:14.140 --> 19:17.540\n One, it allows you to deploy.\n\n19:17.540 --> 19:21.900\n It allows you to collect then data with these variances\n\n19:21.900 --> 19:24.020\n and anomalies, cause people are still people,\n\n19:24.020 --> 19:28.820\n but it's a safer space and it's more of an accepting space.\n\n19:28.820 --> 19:31.900\n I.e. when something in that space might happen\n\n19:31.900 --> 19:34.100\n because things do,\n\n19:34.100 --> 19:36.060\n because you already have the self selection,\n\n19:36.060 --> 19:39.220\n like people would be, I think a little more forgiving\n\n19:39.220 --> 19:40.700\n than other places.\n\n19:40.700 --> 19:43.100\n And you said three things, did we cover all of them?\n\n19:43.100 --> 19:46.340\n The third is legal law, liability,\n\n19:46.340 --> 19:47.820\n which I don't really want to touch,\n\n19:47.820 --> 19:50.900\n but it's still of concern.\n\n19:50.900 --> 19:53.260\n And the mishmash with like with policy as well,\n\n19:53.260 --> 19:55.740\n sort of government, all that whole.\n\n19:55.740 --> 19:57.740\n That big ball of stuff.\n\n19:57.740 --> 19:59.100\n Yeah, gotcha.\n\n19:59.100 --> 20:01.740\n So that's, so we're out of time now.\n\n20:03.540 --> 20:06.020\n Do you think from a robotics perspective,\n\n20:07.180 --> 20:09.820\n you know, if you're kind of honest of what cars do,\n\n20:09.820 --> 20:14.860\n they kind of threaten each other's life all the time.\n\n20:14.860 --> 20:17.340\n So cars are various.\n\n20:17.340 --> 20:19.300\n I mean, in order to navigate intersections,\n\n20:19.300 --> 20:22.300\n there's an assertiveness, there's a risk taking.\n\n20:22.300 --> 20:25.300\n And if you were to reduce it to an objective function,\n\n20:25.300 --> 20:28.740\n there's a probability of murder in that function,\n\n20:28.740 --> 20:31.900\n meaning you killing another human being\n\n20:31.900 --> 20:33.580\n and you're using that.\n\n20:33.580 --> 20:35.700\n First of all, it has to be low enough\n\n20:36.940 --> 20:39.700\n to be acceptable to you on an ethical level\n\n20:39.700 --> 20:41.300\n as an individual human being,\n\n20:41.300 --> 20:45.300\n but it has to be high enough for people to respect you\n\n20:45.300 --> 20:47.540\n to not sort of take advantage of you completely\n\n20:47.540 --> 20:49.620\n and jaywalk in front of you and so on.\n\n20:49.620 --> 20:53.100\n So, I mean, I don't think there's a right answer here,\n\n20:53.100 --> 20:56.100\n but what's, how do we solve that?\n\n20:56.100 --> 20:57.940\n How do we solve that from a robotics perspective\n\n20:57.940 --> 21:00.140\n when danger and human life is at stake?\n\n21:00.140 --> 21:01.980\n Yeah, as they say, cars don't kill people,\n\n21:01.980 --> 21:02.940\n people kill people.\n\n21:02.940 --> 21:05.100\n People kill people.\n\n21:05.100 --> 21:05.940\n Right.\n\n21:07.100 --> 21:08.620\n So I think.\n\n21:08.620 --> 21:10.780\n And now robotic algorithms would be killing people.\n\n21:10.780 --> 21:14.380\n Right, so it will be robotics algorithms that are pro,\n\n21:14.380 --> 21:16.980\n no, it will be robotic algorithms don't kill people.\n\n21:16.980 --> 21:19.740\n Developers of robotic algorithms kill people, right?\n\n21:19.740 --> 21:22.940\n I mean, one of the things is people are still in the loop\n\n21:22.940 --> 21:26.540\n and at least in the near and midterm,\n\n21:26.540 --> 21:29.420\n I think people will still be in the loop at some point,\n\n21:29.420 --> 21:30.300\n even if it's a developer.\n\n21:30.300 --> 21:31.860\n Like we're not necessarily at the stage\n\n21:31.860 --> 21:36.740\n where robots are programming autonomous robots\n\n21:36.740 --> 21:39.980\n with different behaviors quite yet.\n\n21:39.980 --> 21:42.260\n It's a scary notion, sorry to interrupt,\n\n21:42.260 --> 21:47.260\n that a developer has some responsibility\n\n21:47.420 --> 21:49.700\n in the death of a human being.\n\n21:49.700 --> 21:50.620\n That's a heavy burden.\n\n21:50.620 --> 21:55.460\n I mean, I think that's why the whole aspect of ethics\n\n21:55.460 --> 21:58.500\n in our community is so, so important, right?\n\n21:58.500 --> 22:00.060\n Like, because it's true.\n\n22:00.060 --> 22:04.820\n If you think about it, you can basically say,\n\n22:04.820 --> 22:07.460\n I'm not going to work on weaponized AI, right?\n\n22:07.460 --> 22:09.860\n Like people can say, that's not what I'm gonna do.\n\n22:09.860 --> 22:12.740\n But yet you are programming algorithms\n\n22:12.740 --> 22:15.620\n that might be used in healthcare algorithms\n\n22:15.620 --> 22:17.260\n that might decide whether this person\n\n22:17.260 --> 22:18.980\n should get this medication or not.\n\n22:18.980 --> 22:21.420\n And they don't and they die.\n\n22:21.420 --> 22:25.100\n Okay, so that is your responsibility, right?\n\n22:25.100 --> 22:27.340\n And if you're not conscious and aware\n\n22:27.340 --> 22:30.020\n that you do have that power when you're coding\n\n22:30.020 --> 22:35.020\n and things like that, I think that's just not a good thing.\n\n22:35.020 --> 22:38.020\n Like we need to think about this responsibility\n\n22:38.020 --> 22:41.820\n as we program robots and computing devices\n\n22:41.820 --> 22:43.500\n much more than we are.\n\n22:44.340 --> 22:46.980\n Yeah, so it's not an option to not think about ethics.\n\n22:46.980 --> 22:51.340\n I think it's a majority, I would say, of computer science.\n\n22:51.340 --> 22:53.860\n Sort of, it's kind of a hot topic now,\n\n22:53.860 --> 22:56.620\n I think about bias and so on, but it's,\n\n22:56.620 --> 22:59.140\n and we'll talk about it, but usually it's kind of,\n\n23:00.380 --> 23:02.700\n it's like a very particular group of people\n\n23:02.700 --> 23:04.260\n that work on that.\n\n23:04.260 --> 23:06.940\n And then people who do like robotics are like,\n\n23:06.940 --> 23:09.380\n well, I don't have to think about that.\n\n23:09.380 --> 23:11.180\n There's other smart people thinking about it.\n\n23:11.180 --> 23:14.580\n It seems that everybody has to think about it.\n\n23:14.580 --> 23:17.060\n It's not, you can't escape the ethics,\n\n23:17.060 --> 23:21.140\n whether it's bias or just every aspect of ethics\n\n23:21.140 --> 23:22.700\n that has to do with human beings.\n\n23:22.700 --> 23:23.540\n Everyone.\n\n23:23.540 --> 23:25.700\n So think about, I'm gonna age myself,\n\n23:25.700 --> 23:30.140\n but I remember when we didn't have like testers, right?\n\n23:30.140 --> 23:31.100\n And so what did you do?\n\n23:31.100 --> 23:33.580\n As a developer, you had to test your own code, right?\n\n23:33.580 --> 23:36.140\n Like you had to go through all the cases and figure it out\n\n23:36.140 --> 23:37.820\n and then they realized that,\n\n23:39.140 --> 23:40.620\n we probably need to have testing\n\n23:40.620 --> 23:42.460\n because we're not getting all the things.\n\n23:42.460 --> 23:45.540\n And so from there, what happens is like most developers,\n\n23:45.540 --> 23:48.100\n they do a little bit of testing, but it's usually like,\n\n23:48.100 --> 23:49.780\n okay, did my compiler bug out?\n\n23:49.780 --> 23:51.140\n Let me look at the warnings.\n\n23:51.140 --> 23:53.260\n Okay, is that acceptable or not, right?\n\n23:53.260 --> 23:55.820\n Like that's how you typically think about as a developer\n\n23:55.820 --> 23:58.220\n and you'll just assume that it's going to go\n\n23:58.220 --> 24:01.100\n to another process and they're gonna test it out.\n\n24:01.100 --> 24:04.340\n But I think we need to go back to those early days\n\n24:04.340 --> 24:07.540\n when you're a developer, you're developing,\n\n24:07.540 --> 24:09.500\n there should be like the say,\n\n24:09.500 --> 24:12.180\n okay, let me look at the ethical outcomes of this\n\n24:12.180 --> 24:16.020\n because there isn't a second like testing ethical testers,\n\n24:16.020 --> 24:17.100\n right, it's you.\n\n24:18.060 --> 24:21.180\n We did it back in the early coding days.\n\n24:21.180 --> 24:23.300\n I think that's where we are with respect to ethics.\n\n24:23.300 --> 24:26.300\n Like let's go back to what was good practices\n\n24:26.300 --> 24:30.060\n and only because we were just developing the field.\n\n24:30.060 --> 24:33.980\n Yeah, and it's a really heavy burden.\n\n24:33.980 --> 24:37.500\n I've had to feel it recently in the last few months,\n\n24:37.500 --> 24:39.420\n but I think it's a good one to feel like\n\n24:39.420 --> 24:43.380\n I've gotten a message, more than one from people.\n\n24:43.380 --> 24:47.420\n You know, I've unfortunately gotten some attention recently\n\n24:47.420 --> 24:50.380\n and I've gotten messages that say that\n\n24:50.380 --> 24:52.300\n I have blood on my hands\n\n24:52.300 --> 24:56.260\n because of working on semi autonomous vehicles.\n\n24:56.260 --> 24:59.220\n So the idea that you have semi autonomy means\n\n24:59.220 --> 25:02.020\n people will become, will lose vigilance and so on.\n\n25:02.020 --> 25:05.140\n That's actually be humans, as we described.\n\n25:05.140 --> 25:08.100\n And because of that, because of this idea\n\n25:08.100 --> 25:10.060\n that we're creating automation,\n\n25:10.060 --> 25:12.780\n there'll be people be hurt because of it.\n\n25:12.780 --> 25:14.540\n And I think that's a beautiful thing.\n\n25:14.540 --> 25:16.220\n I mean, it's, you know, there's many nights\n\n25:16.220 --> 25:18.820\n where I wasn't able to sleep because of this notion.\n\n25:18.820 --> 25:22.380\n You know, you really do think about people that might die\n\n25:22.380 --> 25:23.860\n because of this technology.\n\n25:23.860 --> 25:26.580\n Of course, you can then start rationalizing saying,\n\n25:26.580 --> 25:29.100\n well, you know what, 40,000 people die in the United States\n\n25:29.100 --> 25:32.380\n every year and we're trying to ultimately try to save lives.\n\n25:32.380 --> 25:35.780\n But the reality is your code you've written\n\n25:35.780 --> 25:36.700\n might kill somebody.\n\n25:36.700 --> 25:38.900\n And that's an important burden to carry with you\n\n25:38.900 --> 25:40.180\n as you design the code.\n\n25:41.180 --> 25:43.820\n I don't even think of it as a burden\n\n25:43.820 --> 25:47.540\n if we train this concept correctly from the beginning.\n\n25:47.540 --> 25:50.300\n And I use, and not to say that coding is like\n\n25:50.300 --> 25:52.420\n being a medical doctor, but think about it.\n\n25:52.420 --> 25:56.100\n Medical doctors, if they've been in situations\n\n25:56.100 --> 25:58.300\n where their patient didn't survive, right?\n\n25:58.300 --> 26:00.820\n Do they give up and go away?\n\n26:00.820 --> 26:02.540\n No, every time they come in,\n\n26:02.540 --> 26:05.460\n they know that there might be a possibility\n\n26:05.460 --> 26:07.260\n that this patient might not survive.\n\n26:07.260 --> 26:10.140\n And so when they approach every decision,\n\n26:10.140 --> 26:11.980\n like that's in the back of their head.\n\n26:11.980 --> 26:15.860\n And so why isn't that we aren't teaching,\n\n26:15.860 --> 26:17.220\n and those are tools though, right?\n\n26:17.220 --> 26:19.740\n They are given some of the tools to address that\n\n26:19.740 --> 26:21.500\n so that they don't go crazy.\n\n26:21.500 --> 26:24.220\n But we don't give those tools\n\n26:24.220 --> 26:26.180\n so that it does feel like a burden\n\n26:26.180 --> 26:28.700\n versus something of I have a great gift\n\n26:28.700 --> 26:31.100\n and I can do great, awesome good,\n\n26:31.100 --> 26:33.340\n but with it comes great responsibility.\n\n26:33.340 --> 26:35.820\n I mean, that's what we teach in terms of\n\n26:35.820 --> 26:37.420\n if you think about the medical schools, right?\n\n26:37.420 --> 26:39.540\n Great gift, great responsibility.\n\n26:39.540 --> 26:42.140\n I think if we just change the messaging a little,\n\n26:42.140 --> 26:45.580\n great gift, being a developer, great responsibility.\n\n26:45.580 --> 26:48.340\n And this is how you combine those.\n\n26:48.340 --> 26:51.100\n But do you think, I mean, this is really interesting.\n\n26:52.180 --> 26:54.300\n It's outside, I actually have no friends\n\n26:54.300 --> 26:57.300\n who are sort of surgeons or doctors.\n\n26:58.260 --> 27:00.020\n I mean, what does it feel like\n\n27:00.020 --> 27:03.780\n to make a mistake in a surgery and somebody to die\n\n27:03.780 --> 27:04.780\n because of that?\n\n27:04.780 --> 27:07.020\n Like, is that something you could be taught\n\n27:07.020 --> 27:10.580\n in medical school, sort of how to be accepting of that risk?\n\n27:10.580 --> 27:14.940\n So, because I do a lot of work with healthcare robotics,\n\n27:14.940 --> 27:18.460\n I have not lost a patient, for example.\n\n27:18.460 --> 27:20.900\n The first one's always the hardest, right?\n\n27:20.900 --> 27:25.900\n But they really teach the value, right?\n\n27:27.300 --> 27:28.740\n So, they teach responsibility,\n\n27:28.740 --> 27:30.780\n but they also teach the value.\n\n27:30.780 --> 27:34.700\n Like, you're saving 40,000,\n\n27:34.700 --> 27:38.260\n but in order to really feel good about that,\n\n27:38.260 --> 27:40.100\n when you come to a decision,\n\n27:40.100 --> 27:42.220\n you have to be able to say at the end,\n\n27:42.220 --> 27:45.300\n I did all that I could possibly do, right?\n\n27:45.300 --> 27:49.100\n Versus a, well, I just picked the first widget, right?\n\n27:49.100 --> 27:52.220\n Like, so every decision is actually thought through.\n\n27:52.220 --> 27:53.780\n It's not a habit, it's not a,\n\n27:53.780 --> 27:55.340\n let me just take the best algorithm\n\n27:55.340 --> 27:57.060\n that my friend gave me, right?\n\n27:57.060 --> 27:59.540\n It's a, is this it, is this the best?\n\n27:59.540 --> 28:03.100\n Have I done my best to do good, right?\n\n28:03.100 --> 28:03.940\n And so...\n\n28:03.940 --> 28:06.500\n You're right, and I think burden is the wrong word.\n\n28:06.500 --> 28:10.740\n It's a gift, but you have to treat it extremely seriously.\n\n28:10.740 --> 28:11.580\n Correct.\n\n28:13.260 --> 28:15.500\n So, on a slightly related note,\n\n28:15.500 --> 28:16.420\n in a recent paper,\n\n28:16.420 --> 28:20.140\n The Ugly Truth About Ourselves and Our Robot Creations,\n\n28:20.140 --> 28:24.300\n you discuss, you highlight some biases\n\n28:24.300 --> 28:27.100\n that may affect the function of various robotic systems.\n\n28:27.100 --> 28:30.100\n Can you talk through, if you remember, examples of some?\n\n28:30.100 --> 28:31.300\n There's a lot of examples.\n\n28:31.300 --> 28:33.060\n I usually... What is bias, first of all?\n\n28:33.060 --> 28:37.060\n Yeah, so bias is this,\n\n28:37.060 --> 28:38.820\n and so bias, which is different than prejudice.\n\n28:38.820 --> 28:41.860\n So, bias is that we all have these preconceived notions\n\n28:41.860 --> 28:45.940\n about particular, everything from particular groups\n\n28:45.940 --> 28:49.700\n to habits to identity, right?\n\n28:49.700 --> 28:51.420\n So, we have these predispositions,\n\n28:51.420 --> 28:54.100\n and so when we address a problem,\n\n28:54.100 --> 28:56.020\n we look at a problem and make a decision,\n\n28:56.020 --> 29:01.020\n those preconceived notions might affect our outputs,\n\n29:01.340 --> 29:02.220\n our outcomes.\n\n29:02.220 --> 29:04.700\n So, there the bias can be positive and negative,\n\n29:04.700 --> 29:07.980\n and then is prejudice the negative kind of bias?\n\n29:07.980 --> 29:09.180\n Prejudice is the negative, right?\n\n29:09.180 --> 29:13.540\n So, prejudice is that not only are you aware of your bias,\n\n29:13.540 --> 29:18.540\n but you are then take it and have a negative outcome,\n\n29:18.820 --> 29:20.660\n even though you're aware, like...\n\n29:20.660 --> 29:22.980\n And there could be gray areas too.\n\n29:22.980 --> 29:24.620\n There's always gray areas.\n\n29:24.620 --> 29:27.580\n That's the challenging aspect of all ethical questions.\n\n29:27.580 --> 29:28.620\n So, I always like...\n\n29:28.620 --> 29:30.020\n So, there's a funny one,\n\n29:30.020 --> 29:31.740\n and in fact, I think it might be in the paper,\n\n29:31.740 --> 29:34.180\n because I think I talk about self driving cars,\n\n29:34.180 --> 29:35.460\n but think about this.\n\n29:35.460 --> 29:39.500\n We, for teenagers, right?\n\n29:39.500 --> 29:44.500\n Typically, insurance companies charge quite a bit of money\n\n29:44.540 --> 29:46.740\n if you have a teenage driver.\n\n29:46.740 --> 29:50.860\n So, you could say that's an age bias, right?\n\n29:50.860 --> 29:52.380\n But no one will claim...\n\n29:52.380 --> 29:54.060\n I mean, parents will be grumpy,\n\n29:54.060 --> 29:58.660\n but no one really says that that's not fair.\n\n29:58.660 --> 29:59.500\n That's interesting.\n\n29:59.500 --> 30:00.340\n We don't...\n\n30:00.340 --> 30:01.580\n That's right, that's right.\n\n30:01.580 --> 30:06.580\n It's everybody in human factors and safety research almost...\n\n30:06.580 --> 30:11.580\n I mean, it's quite ruthlessly critical of teenagers.\n\n30:12.780 --> 30:15.020\n And we don't question, is that okay?\n\n30:15.020 --> 30:17.140\n Is that okay to be ageist in this kind of way?\n\n30:17.140 --> 30:18.580\n It is, and it is ageist, right?\n\n30:18.580 --> 30:20.780\n It's definitely ageist, there's no question about it.\n\n30:20.780 --> 30:24.940\n And so, this is the gray area, right?\n\n30:24.940 --> 30:29.820\n Because you know that teenagers are more likely\n\n30:29.820 --> 30:30.860\n to be in accidents,\n\n30:30.860 --> 30:33.060\n and so, there's actually some data to it.\n\n30:33.060 --> 30:34.980\n But then, if you take that same example,\n\n30:34.980 --> 30:39.380\n and you say, well, I'm going to make the insurance higher\n\n30:39.380 --> 30:43.380\n for an area of Boston,\n\n30:43.380 --> 30:45.020\n because there's a lot of accidents.\n\n30:45.020 --> 30:48.260\n And then, they find out that that's correlated\n\n30:48.260 --> 30:50.220\n with socioeconomics.\n\n30:50.220 --> 30:52.420\n Well, then it becomes a problem, right?\n\n30:52.420 --> 30:55.180\n Like, that is not acceptable,\n\n30:55.180 --> 30:58.940\n but yet, the teenager, which is age...\n\n30:58.940 --> 31:01.820\n It's against age, is, right?\n\n31:01.820 --> 31:05.260\n We figure that out as a society by having conversations,\n\n31:05.260 --> 31:06.180\n by having discourse.\n\n31:06.180 --> 31:07.540\n I mean, throughout history,\n\n31:07.540 --> 31:11.340\n the definition of what is ethical or not has changed,\n\n31:11.340 --> 31:14.300\n and hopefully, always for the better.\n\n31:14.300 --> 31:15.420\n Correct, correct.\n\n31:15.420 --> 31:20.420\n So, in terms of bias or prejudice in algorithms,\n\n31:22.300 --> 31:25.540\n what examples do you sometimes think about?\n\n31:25.540 --> 31:28.940\n So, I think about quite a bit the medical domain,\n\n31:28.940 --> 31:31.260\n just because historically, right?\n\n31:31.260 --> 31:34.500\n The healthcare domain has had these biases,\n\n31:34.500 --> 31:39.500\n typically based on gender and ethnicity, primarily.\n\n31:40.220 --> 31:42.300\n A little in age, but not so much.\n\n31:43.660 --> 31:48.660\n Historically, if you think about FDA and drug trials,\n\n31:49.260 --> 31:54.260\n it's harder to find a woman that aren't childbearing,\n\n31:54.540 --> 31:56.900\n and so you may not test on drugs at the same level.\n\n31:56.900 --> 31:58.940\n Right, so there's these things.\n\n31:58.940 --> 32:02.900\n And so, if you think about robotics, right?\n\n32:02.900 --> 32:04.860\n Something as simple as,\n\n32:04.860 --> 32:07.740\n I'd like to design an exoskeleton, right?\n\n32:07.740 --> 32:09.180\n What should the material be?\n\n32:09.180 --> 32:10.140\n What should the weight be?\n\n32:10.140 --> 32:12.500\n What should the form factor be?\n\n32:14.260 --> 32:16.940\n Who are you gonna design it around?\n\n32:16.940 --> 32:19.620\n I will say that in the US,\n\n32:19.620 --> 32:21.620\n women average height and weight\n\n32:21.620 --> 32:23.380\n is slightly different than guys.\n\n32:23.380 --> 32:25.820\n So, who are you gonna choose?\n\n32:25.820 --> 32:28.900\n Like, if you're not thinking about it from the beginning,\n\n32:28.900 --> 32:33.420\n as, okay, when I design this and I look at the algorithms\n\n32:33.420 --> 32:35.540\n and I design the control system and the forces\n\n32:35.540 --> 32:38.060\n and the torques, if you're not thinking about,\n\n32:38.060 --> 32:41.500\n well, you have different types of body structure,\n\n32:41.500 --> 32:44.380\n you're gonna design to what you're used to.\n\n32:44.380 --> 32:48.060\n Oh, this fits all the folks in my lab, right?\n\n32:48.060 --> 32:51.300\n So, think about it from the very beginning is important.\n\n32:51.300 --> 32:54.500\n What about sort of algorithms that train on data\n\n32:54.500 --> 32:55.940\n kind of thing?\n\n32:55.940 --> 33:00.940\n Sadly, our society already has a lot of negative bias.\n\n33:01.140 --> 33:03.100\n And so, if we collect a lot of data,\n\n33:04.540 --> 33:06.100\n even if it's a balanced way,\n\n33:06.100 --> 33:07.620\n that's going to contain the same bias\n\n33:07.620 --> 33:08.820\n that our society contains.\n\n33:08.820 --> 33:13.540\n And so, yeah, is there things there that bother you?\n\n33:13.540 --> 33:15.420\n Yeah, so you actually said something.\n\n33:15.420 --> 33:19.740\n You had said how we have biases,\n\n33:19.740 --> 33:22.940\n but hopefully we learn from them and we become better, right?\n\n33:22.940 --> 33:24.940\n And so, that's where we are now, right?\n\n33:24.940 --> 33:28.420\n So, the data that we're collecting is historic.\n\n33:28.420 --> 33:29.940\n So, it's based on these things\n\n33:29.940 --> 33:32.420\n when we knew it was bad to discriminate,\n\n33:32.420 --> 33:35.900\n but that's the data we have and we're trying to fix it now,\n\n33:35.900 --> 33:37.660\n but we're fixing it based on the data\n\n33:37.660 --> 33:39.260\n that was used in the first place.\n\n33:39.260 --> 33:40.460\n Fix it in post.\n\n33:40.460 --> 33:43.580\n Right, and so the decisions,\n\n33:43.580 --> 33:46.700\n and you can look at everything from the whole aspect\n\n33:46.700 --> 33:51.220\n of predictive policing, criminal recidivism.\n\n33:51.220 --> 33:54.100\n There was a recent paper that had the healthcare algorithms,\n\n33:54.100 --> 33:58.020\n which had a kind of a sensational titles.\n\n33:58.020 --> 34:00.980\n I'm not pro sensationalism in titles,\n\n34:00.980 --> 34:03.540\n but again, you read it, right?\n\n34:03.540 --> 34:05.540\n So, it makes you read it,\n\n34:05.540 --> 34:06.780\n but I'm like, really?\n\n34:06.780 --> 34:08.740\n Like, ugh, you could have.\n\n34:08.740 --> 34:10.580\n What's the topic of the sensationalism?\n\n34:10.580 --> 34:13.100\n I mean, what's underneath it?\n\n34:13.100 --> 34:16.100\n What's, if you could sort of educate me\n\n34:16.100 --> 34:18.940\n on what kind of bias creeps into the healthcare space.\n\n34:18.940 --> 34:19.780\n Yeah, so.\n\n34:19.780 --> 34:21.260\n I mean, you already kind of mentioned.\n\n34:21.260 --> 34:24.820\n Yeah, so this one was the headline was\n\n34:24.820 --> 34:27.300\n racist AI algorithms.\n\n34:27.300 --> 34:30.700\n Okay, like, okay, that's totally a clickbait title.\n\n34:30.700 --> 34:34.060\n And so you looked at it and so there was data\n\n34:34.060 --> 34:36.460\n that these researchers had collected.\n\n34:36.460 --> 34:39.220\n I believe, I wanna say it was either Science or Nature.\n\n34:39.220 --> 34:40.460\n It just was just published,\n\n34:40.460 --> 34:42.420\n but they didn't have a sensational title.\n\n34:42.420 --> 34:44.700\n It was like the media.\n\n34:44.700 --> 34:47.300\n And so they had looked at demographics,\n\n34:47.300 --> 34:51.940\n I believe, between black and white women, right?\n\n34:51.940 --> 34:56.660\n And they showed that there was a discrepancy\n\n34:56.660 --> 34:58.980\n in the outcomes, right?\n\n34:58.980 --> 35:02.220\n And so, and it was tied to ethnicity, tied to race.\n\n35:02.220 --> 35:04.620\n The piece that the researchers did\n\n35:04.620 --> 35:08.620\n actually went through the whole analysis, but of course.\n\n35:08.620 --> 35:11.900\n I mean, the journalists with AI are problematic\n\n35:11.900 --> 35:14.140\n across the board, let's say.\n\n35:14.140 --> 35:15.980\n And so this is a problem, right?\n\n35:15.980 --> 35:18.100\n And so there's this thing about,\n\n35:18.100 --> 35:20.420\n oh, AI, it has all these problems.\n\n35:20.420 --> 35:22.740\n We're doing it on historical data\n\n35:22.740 --> 35:25.900\n and the outcomes are uneven based on gender\n\n35:25.900 --> 35:27.940\n or ethnicity or age.\n\n35:27.940 --> 35:30.660\n But I am always saying is like, yes,\n\n35:30.660 --> 35:32.340\n we need to do better, right?\n\n35:32.340 --> 35:33.460\n We need to do better.\n\n35:33.460 --> 35:35.220\n It is our duty to do better.\n\n35:36.620 --> 35:39.700\n But the worst AI is still better than us.\n\n35:39.700 --> 35:41.820\n Like, you take the best of us\n\n35:41.820 --> 35:44.020\n and we're still worse than the worst AI,\n\n35:44.020 --> 35:45.500\n at least in terms of these things.\n\n35:45.500 --> 35:47.820\n And that's actually not discussed, right?\n\n35:47.820 --> 35:51.780\n And so I think, and that's why the sensational title, right?\n\n35:51.780 --> 35:54.180\n And so it's like, so then you can have individuals go like,\n\n35:54.180 --> 35:55.340\n oh, we don't need to use this AI.\n\n35:55.340 --> 35:56.620\n I'm like, oh, no, no, no, no.\n\n35:56.620 --> 36:00.780\n I want the AI instead of the doctors\n\n36:00.780 --> 36:01.860\n that provided that data,\n\n36:01.860 --> 36:04.060\n because it's still better than that, right?\n\n36:04.060 --> 36:06.660\n I think that's really important to linger on,\n\n36:06.660 --> 36:09.420\n is the idea that this AI is racist.\n\n36:10.300 --> 36:14.020\n It's like, well, compared to what?\n\n36:14.020 --> 36:19.020\n Sort of, I think we set, unfortunately,\n\n36:20.100 --> 36:23.220\n way too high of a bar for AI algorithms.\n\n36:23.220 --> 36:25.940\n And in the ethical space where perfect is,\n\n36:25.940 --> 36:28.060\n I would argue, probably impossible.\n\n36:28.940 --> 36:33.020\n Then if we set the bar of perfection, essentially,\n\n36:33.020 --> 36:36.140\n of it has to be perfectly fair, whatever that means,\n\n36:37.500 --> 36:39.580\n it means we're setting it up for failure.\n\n36:39.580 --> 36:41.940\n But that's really important to say what you just said,\n\n36:41.940 --> 36:44.900\n which is, well, it's still better than it is.\n\n36:44.900 --> 36:46.860\n And one of the things I think\n\n36:46.860 --> 36:49.380\n that we don't get enough credit for,\n\n36:50.260 --> 36:52.140\n just in terms of as developers,\n\n36:52.140 --> 36:55.820\n is that you can now poke at it, right?\n\n36:55.820 --> 36:58.820\n So it's harder to say, is this hospital,\n\n36:58.820 --> 37:01.020\n is this city doing something, right?\n\n37:01.020 --> 37:04.380\n Until someone brings in a civil case, right?\n\n37:04.380 --> 37:07.100\n Well, with AI, it can process through all this data\n\n37:07.100 --> 37:12.100\n and say, hey, yes, there was an issue here,\n\n37:12.500 --> 37:14.460\n but here it is, we've identified it,\n\n37:14.460 --> 37:16.140\n and then the next step is to fix it.\n\n37:16.140 --> 37:18.060\n I mean, that's a nice feedback loop\n\n37:18.060 --> 37:21.300\n versus waiting for someone to sue someone else\n\n37:21.300 --> 37:22.740\n before it's fixed, right?\n\n37:22.740 --> 37:25.060\n And so I think that power,\n\n37:25.060 --> 37:27.580\n we need to capitalize on a little bit more, right?\n\n37:27.580 --> 37:29.660\n Instead of having the sensational titles,\n\n37:29.660 --> 37:33.300\n have the, okay, this is a problem,\n\n37:33.300 --> 37:34.540\n and this is how we're fixing it,\n\n37:34.540 --> 37:36.500\n and people are putting money to fix it\n\n37:36.500 --> 37:38.580\n because we can make it better.\n\n37:38.580 --> 37:40.340\n I look at like facial recognition,\n\n37:40.340 --> 37:45.340\n how Joy, she basically called out a couple of companies\n\n37:45.460 --> 37:48.220\n and said, hey, and most of them were like,\n\n37:48.220 --> 37:53.020\n oh, embarrassment, and the next time it had been fixed,\n\n37:53.020 --> 37:54.860\n right, it had been fixed better, right?\n\n37:54.860 --> 37:56.740\n And then it was like, oh, here's some more issues.\n\n37:56.740 --> 38:01.740\n And I think that conversation then moves that needle\n\n38:01.740 --> 38:06.740\n to having much more fair and unbiased and ethical aspects,\n\n38:07.540 --> 38:10.580\n as long as both sides, the developers are willing to say,\n\n38:10.580 --> 38:14.020\n okay, I hear you, yes, we are going to improve,\n\n38:14.020 --> 38:16.100\n and you have other developers who are like,\n\n38:16.100 --> 38:19.620\n hey, AI, it's wrong, but I love it, right?\n\n38:19.620 --> 38:23.020\n Yes, so speaking of this really nice notion\n\n38:23.020 --> 38:26.980\n that AI is maybe flawed but better than humans,\n\n38:26.980 --> 38:29.140\n so just made me think of it,\n\n38:29.140 --> 38:34.100\n one example of flawed humans is our political system.\n\n38:34.100 --> 38:38.700\n Do you think, or you said judicial as well,\n\n38:38.700 --> 38:43.700\n do you have a hope for AI sort of being elected\n\n38:46.140 --> 38:49.780\n for president or running our Congress\n\n38:49.780 --> 38:53.940\n or being able to be a powerful representative of the people?\n\n38:53.940 --> 38:58.940\n So I mentioned, and I truly believe that this whole world\n\n38:58.940 --> 39:01.340\n of AI is in partnerships with people.\n\n39:01.340 --> 39:02.420\n And so what does that mean?\n\n39:02.420 --> 39:07.420\n I don't believe, or maybe I just don't,\n\n39:07.620 --> 39:11.420\n I don't believe that we should have an AI for president,\n\n39:11.420 --> 39:13.540\n but I do believe that a president\n\n39:13.540 --> 39:15.900\n should use AI as an advisor, right?\n\n39:15.900 --> 39:17.420\n Like, if you think about it,\n\n39:17.420 --> 39:21.900\n every president has a cabinet of individuals\n\n39:21.900 --> 39:23.660\n that have different expertise\n\n39:23.660 --> 39:26.060\n that they should listen to, right?\n\n39:26.060 --> 39:27.980\n Like, that's kind of what we do.\n\n39:27.980 --> 39:31.100\n And you put smart people with smart expertise\n\n39:31.100 --> 39:33.420\n around certain issues, and you listen.\n\n39:33.420 --> 39:35.700\n I don't see why AI can't function\n\n39:35.700 --> 39:39.260\n as one of those smart individuals giving input.\n\n39:39.260 --> 39:41.020\n So maybe there's an AI on healthcare,\n\n39:41.020 --> 39:43.820\n maybe there's an AI on education and right,\n\n39:43.820 --> 39:48.780\n like all of these things that a human is processing, right?\n\n39:48.780 --> 39:50.420\n Because at the end of the day,\n\n39:51.380 --> 39:53.540\n there's people that are human\n\n39:53.540 --> 39:55.500\n that are going to be at the end of the decision.\n\n39:55.500 --> 39:59.260\n And I don't think as a world, as a culture, as a society,\n\n39:59.260 --> 40:02.980\n that we would totally, and this is us,\n\n40:02.980 --> 40:05.260\n like this is some fallacy about us,\n\n40:05.260 --> 40:10.260\n but we need to see that leader, that person as human.\n\n40:11.780 --> 40:13.180\n And most people don't realize\n\n40:13.180 --> 40:16.940\n that like leaders have a whole lot of advice, right?\n\n40:16.940 --> 40:19.500\n Like when they say something, it's not that they woke up,\n\n40:19.500 --> 40:21.780\n well, usually they don't wake up in the morning\n\n40:21.780 --> 40:24.340\n and be like, I have a brilliant idea, right?\n\n40:24.340 --> 40:26.620\n It's usually a, okay, let me listen.\n\n40:26.620 --> 40:27.460\n I have a brilliant idea,\n\n40:27.460 --> 40:29.780\n but let me get a little bit of feedback on this.\n\n40:29.780 --> 40:30.900\n Like, okay.\n\n40:30.900 --> 40:33.020\n And then it's a, yeah, that was an awesome idea\n\n40:33.020 --> 40:35.780\n or it's like, yeah, let me go back.\n\n40:35.780 --> 40:37.300\n We already talked through a bunch of them,\n\n40:37.300 --> 40:41.380\n but are there some possible solutions\n\n40:41.380 --> 40:45.100\n to the bias that's present in our algorithms\n\n40:45.100 --> 40:46.540\n beyond what we just talked about?\n\n40:46.540 --> 40:49.180\n So I think there's two paths.\n\n40:49.180 --> 40:53.620\n One is to figure out how to systematically\n\n40:53.620 --> 40:56.380\n do the feedback and corrections.\n\n40:56.380 --> 40:57.980\n So right now it's ad hoc, right?\n\n40:57.980 --> 41:02.300\n It's a researcher identify some outcomes\n\n41:02.300 --> 41:05.260\n that are not, don't seem to be fair, right?\n\n41:05.260 --> 41:07.780\n They publish it, they write about it.\n\n41:07.780 --> 41:11.260\n And the, either the developer or the companies\n\n41:11.260 --> 41:14.100\n that have adopted the algorithms may try to fix it, right?\n\n41:14.100 --> 41:18.700\n And so it's really ad hoc and it's not systematic.\n\n41:18.700 --> 41:21.260\n There's, it's just, it's kind of like,\n\n41:21.260 --> 41:24.460\n I'm a researcher, that seems like an interesting problem,\n\n41:24.460 --> 41:26.340\n which means that there's a whole lot out there\n\n41:26.340 --> 41:28.900\n that's not being looked at, right?\n\n41:28.900 --> 41:30.820\n Cause it's kind of researcher driven.\n\n41:32.740 --> 41:35.460\n And I don't necessarily have a solution,\n\n41:35.460 --> 41:40.460\n but that process I think could be done a little bit better.\n\n41:41.020 --> 41:44.820\n One way is I'm going to poke a little bit\n\n41:44.820 --> 41:48.060\n at some of the corporations, right?\n\n41:48.060 --> 41:50.660\n Like maybe the corporations when they think\n\n41:50.660 --> 41:53.660\n about a product, they should, instead of,\n\n41:53.660 --> 41:57.780\n in addition to hiring these, you know, bug,\n\n41:57.780 --> 41:58.820\n they give these.\n\n41:59.660 --> 42:01.420\n Oh yeah, yeah, yeah.\n\n42:01.420 --> 42:02.780\n Like awards when you find a bug.\n\n42:02.780 --> 42:06.620\n Yeah, security bug, you know, let's put it\n\n42:06.620 --> 42:09.580\n like we will give the, whatever the award is\n\n42:09.580 --> 42:12.460\n that we give for the people who find these security holes,\n\n42:12.460 --> 42:13.820\n find an ethics hole, right?\n\n42:13.820 --> 42:15.220\n Like find an unfairness hole\n\n42:15.220 --> 42:17.660\n and we will pay you X for each one you find.\n\n42:17.660 --> 42:19.620\n I mean, why can't they do that?\n\n42:19.620 --> 42:20.900\n One is a win win.\n\n42:20.900 --> 42:22.940\n They show that they're concerned about it,\n\n42:22.940 --> 42:24.980\n that this is important and they don't have\n\n42:24.980 --> 42:28.660\n to necessarily dedicate it their own like internal resources.\n\n42:28.660 --> 42:30.780\n And it also means that everyone who has\n\n42:30.780 --> 42:34.460\n like their own bias lens, like I'm interested in age.\n\n42:34.460 --> 42:36.420\n And so I'll find the ones based on age\n\n42:36.420 --> 42:38.260\n and I'm interested in gender and right,\n\n42:38.260 --> 42:39.860\n which means that you get like all\n\n42:39.860 --> 42:41.420\n of these different perspectives.\n\n42:41.420 --> 42:43.220\n But you think of it in a data driven way.\n\n42:43.220 --> 42:48.220\n So like sort of, if we look at a company like Twitter,\n\n42:48.220 --> 42:51.660\n it gets, it's under a lot of fire\n\n42:51.660 --> 42:54.820\n for discriminating against certain political beliefs.\n\n42:54.820 --> 42:55.880\n Correct.\n\n42:55.880 --> 42:58.060\n And sort of, there's a lot of people,\n\n42:58.060 --> 42:59.260\n this is the sad thing,\n\n42:59.260 --> 43:00.700\n cause I know how hard the problem is\n\n43:00.700 --> 43:03.060\n and I know the Twitter folks are working really hard at it.\n\n43:03.060 --> 43:04.980\n Even Facebook that everyone seems to hate\n\n43:04.980 --> 43:06.860\n are working really hard at this.\n\n43:06.860 --> 43:09.320\n You know, the kind of evidence that people bring\n\n43:09.320 --> 43:11.240\n is basically anecdotal evidence.\n\n43:11.240 --> 43:15.020\n Well, me or my friend, all we said is X\n\n43:15.020 --> 43:17.100\n and for that we got banned.\n\n43:17.100 --> 43:20.980\n And that's kind of a discussion of saying,\n\n43:20.980 --> 43:23.260\n well, look, that's usually, first of all,\n\n43:23.260 --> 43:25.500\n the whole thing is taken out of context.\n\n43:25.500 --> 43:28.660\n So they present sort of anecdotal evidence.\n\n43:28.660 --> 43:31.140\n And how are you supposed to, as a company,\n\n43:31.140 --> 43:33.080\n in a healthy way, have a discourse\n\n43:33.080 --> 43:35.980\n about what is and isn't ethical?\n\n43:35.980 --> 43:38.060\n How do we make algorithms ethical\n\n43:38.060 --> 43:40.780\n when people are just blowing everything?\n\n43:40.780 --> 43:45.140\n Like they're outraged about a particular\n\n43:45.140 --> 43:48.220\n anecdotal piece of evidence that's very difficult\n\n43:48.220 --> 43:51.660\n to sort of contextualize in the big data driven way.\n\n43:52.660 --> 43:55.900\n Do you have a hope for companies like Twitter and Facebook?\n\n43:55.900 --> 43:59.820\n Yeah, so I think there's a couple of things going on, right?\n\n43:59.820 --> 44:04.820\n First off, remember this whole aspect\n\n44:04.860 --> 44:09.420\n of we are becoming reliant on technology.\n\n44:09.420 --> 44:14.380\n We're also becoming reliant on a lot of these,\n\n44:14.380 --> 44:17.980\n the apps and the resources that are provided, right?\n\n44:17.980 --> 44:21.660\n So some of it is kind of anger, like I need you, right?\n\n44:21.660 --> 44:23.220\n And you're not working for me, right?\n\n44:23.220 --> 44:24.660\n Not working for me, all right.\n\n44:24.660 --> 44:27.300\n But I think, and so some of it,\n\n44:27.300 --> 44:31.380\n and I wish that there was a little bit\n\n44:31.380 --> 44:32.860\n of change of rethinking.\n\n44:32.860 --> 44:35.560\n So some of it is like, oh, we'll fix it in house.\n\n44:35.560 --> 44:38.980\n No, that's like, okay, I'm a fox\n\n44:38.980 --> 44:40.940\n and I'm going to watch these hens\n\n44:40.940 --> 44:44.060\n because I think it's a problem that foxes eat hens.\n\n44:44.060 --> 44:45.180\n No, right?\n\n44:45.180 --> 44:49.880\n Like be good citizens and say, look, we have a problem.\n\n44:50.860 --> 44:54.820\n And we are willing to open ourselves up\n\n44:54.820 --> 44:57.060\n for others to come in and look at it\n\n44:57.060 --> 44:58.740\n and not try to fix it in house.\n\n44:58.740 --> 45:00.460\n Because if you fix it in house,\n\n45:00.460 --> 45:01.940\n there's conflict of interest.\n\n45:01.940 --> 45:04.440\n If I find something, I'm probably going to want to fix it\n\n45:04.440 --> 45:07.300\n and hopefully the media won't pick it up, right?\n\n45:07.300 --> 45:09.320\n And that then causes distrust\n\n45:09.320 --> 45:11.880\n because someone inside is going to be mad at you\n\n45:11.880 --> 45:13.580\n and go out and talk about how,\n\n45:13.580 --> 45:17.780\n yeah, they canned the resume survey because it, right?\n\n45:17.780 --> 45:19.320\n Like be nice people.\n\n45:19.320 --> 45:22.760\n Like just say, look, we have this issue.\n\n45:22.760 --> 45:24.420\n Community, help us fix it.\n\n45:24.420 --> 45:25.780\n And we will give you like, you know,\n\n45:25.780 --> 45:28.100\n the bug finder fee if you do.\n\n45:28.100 --> 45:31.260\n Did you ever hope that the community,\n\n45:31.260 --> 45:35.340\n us as a human civilization on the whole is good\n\n45:35.340 --> 45:39.500\n and can be trusted to guide the future of our civilization\n\n45:39.500 --> 45:40.940\n into a positive direction?\n\n45:40.940 --> 45:41.880\n I think so.\n\n45:41.880 --> 45:44.100\n So I'm an optimist, right?\n\n45:44.100 --> 45:49.100\n And, you know, there were some dark times in history always.\n\n45:49.980 --> 45:52.900\n I think now we're in one of those dark times.\n\n45:52.900 --> 45:53.740\n I truly do.\n\n45:53.740 --> 45:54.620\n In which aspect?\n\n45:54.620 --> 45:56.260\n The polarization.\n\n45:56.260 --> 45:57.560\n And it's not just US, right?\n\n45:57.560 --> 46:00.020\n So if it was just US, I'd be like, yeah, it's a US thing,\n\n46:00.020 --> 46:03.480\n but we're seeing it like worldwide, this polarization.\n\n46:04.380 --> 46:06.540\n And so I worry about that.\n\n46:06.540 --> 46:11.540\n But I do fundamentally believe that at the end of the day,\n\n46:11.980 --> 46:13.420\n people are good, right?\n\n46:13.420 --> 46:14.780\n And why do I say that?\n\n46:14.780 --> 46:17.700\n Because anytime there's a scenario\n\n46:17.700 --> 46:20.820\n where people are in danger and I will use,\n\n46:20.820 --> 46:24.260\n so Atlanta, we had a snowmageddon\n\n46:24.260 --> 46:26.620\n and people can laugh about that.\n\n46:26.620 --> 46:30.460\n People at the time, so the city closed for, you know,\n\n46:30.460 --> 46:33.420\n little snow, but it was ice and the city closed down.\n\n46:33.420 --> 46:35.720\n But you had people opening up their homes and saying,\n\n46:35.720 --> 46:39.060\n hey, you have nowhere to go, come to my house, right?\n\n46:39.060 --> 46:41.820\n Hotels were just saying like, sleep on the floor.\n\n46:41.820 --> 46:44.420\n Like places like, you know, the grocery stores were like,\n\n46:44.420 --> 46:45.940\n hey, here's food.\n\n46:45.940 --> 46:47.940\n There was no like, oh, how much are you gonna pay me?\n\n46:47.940 --> 46:50.500\n It was like this, such a community.\n\n46:50.500 --> 46:52.140\n And like people who didn't know each other,\n\n46:52.140 --> 46:55.540\n strangers were just like, can I give you a ride home?\n\n46:55.540 --> 46:58.420\n And that was a point I was like, you know what, like.\n\n46:59.420 --> 47:03.100\n That reveals that the deeper thing is,\n\n47:03.100 --> 47:06.940\n there's a compassionate love that we all have within us.\n\n47:06.940 --> 47:09.500\n It's just that when all of that is taken care of\n\n47:09.500 --> 47:11.300\n and get bored, we love drama.\n\n47:11.300 --> 47:14.820\n And that's, I think almost like the division\n\n47:14.820 --> 47:17.100\n is a sign of the times being good,\n\n47:17.100 --> 47:19.060\n is that it's just entertaining\n\n47:19.060 --> 47:24.060\n on some unpleasant mammalian level to watch,\n\n47:24.220 --> 47:26.140\n to disagree with others.\n\n47:26.140 --> 47:30.260\n And Twitter and Facebook are actually taking advantage\n\n47:30.260 --> 47:33.220\n of that in a sense because it brings you back\n\n47:33.220 --> 47:36.180\n to the platform and they're advertiser driven,\n\n47:36.180 --> 47:37.620\n so they make a lot of money.\n\n47:37.620 --> 47:39.300\n So you go back and you click.\n\n47:39.300 --> 47:42.700\n Love doesn't sell quite as well in terms of advertisement.\n\n47:43.700 --> 47:44.940\n It doesn't.\n\n47:44.940 --> 47:46.980\n So you've started your career\n\n47:46.980 --> 47:49.100\n at NASA Jet Propulsion Laboratory,\n\n47:49.100 --> 47:51.980\n but before I ask a few questions there,\n\n47:51.980 --> 47:54.460\n have you happened to have ever seen Space Odyssey,\n\n47:54.460 --> 47:55.900\n 2001 Space Odyssey?\n\n47:57.220 --> 47:58.060\n Yes.\n\n47:58.060 --> 48:01.420\n Okay, do you think HAL 9000,\n\n48:01.420 --> 48:03.420\n so we're talking about ethics.\n\n48:03.420 --> 48:06.700\n Do you think HAL did the right thing\n\n48:06.700 --> 48:08.580\n by taking the priority of the mission\n\n48:08.580 --> 48:10.260\n over the lives of the astronauts?\n\n48:10.260 --> 48:12.400\n Do you think HAL is good or evil?\n\n48:15.900 --> 48:16.900\n Easy questions.\n\n48:16.900 --> 48:17.740\n Yeah.\n\n48:19.420 --> 48:21.380\n HAL was misguided.\n\n48:21.380 --> 48:24.060\n You're one of the people that would be in charge\n\n48:24.060 --> 48:26.140\n of an algorithm like HAL.\n\n48:26.140 --> 48:26.980\n Yeah.\n\n48:26.980 --> 48:28.340\n What would you do better?\n\n48:28.340 --> 48:31.180\n If you think about what happened\n\n48:31.180 --> 48:35.380\n was there was no fail safe, right?\n\n48:35.380 --> 48:37.780\n So perfection, right?\n\n48:37.780 --> 48:38.620\n Like what is that?\n\n48:38.620 --> 48:40.840\n I'm gonna make something that I think is perfect,\n\n48:40.840 --> 48:44.620\n but if my assumptions are wrong,\n\n48:44.620 --> 48:47.560\n it'll be perfect based on the wrong assumptions, right?\n\n48:47.560 --> 48:51.700\n That's something that you don't know until you deploy\n\n48:51.700 --> 48:53.820\n and then you're like, oh yeah, messed up.\n\n48:53.820 --> 48:58.340\n But what that means is that when we design software,\n\n48:58.340 --> 49:00.300\n such as in Space Odyssey,\n\n49:00.300 --> 49:02.100\n when we put things out,\n\n49:02.100 --> 49:04.000\n that there has to be a fail safe.\n\n49:04.000 --> 49:07.700\n There has to be the ability that once it's out there,\n\n49:07.700 --> 49:11.360\n we can grade it as an F and it fails\n\n49:11.360 --> 49:13.060\n and it doesn't continue, right?\n\n49:13.060 --> 49:16.020\n There's some way that it can be brought in\n\n49:16.020 --> 49:19.620\n and removed in that aspect.\n\n49:19.620 --> 49:21.060\n Because that's what happened with HAL.\n\n49:21.060 --> 49:23.740\n It was like assumptions were wrong.\n\n49:23.740 --> 49:27.820\n It was perfectly correct based on those assumptions\n\n49:27.820 --> 49:31.020\n and there was no way to change it,\n\n49:31.020 --> 49:34.020\n change the assumptions at all.\n\n49:34.020 --> 49:37.020\n And the change to fall back would be to a human.\n\n49:37.020 --> 49:40.040\n So you ultimately think like human should be,\n\n49:42.340 --> 49:45.580\n it's not turtles or AI all the way down.\n\n49:45.580 --> 49:47.820\n It's at some point, there's a human that actually.\n\n49:47.820 --> 49:48.860\n I still think that,\n\n49:48.860 --> 49:51.420\n and again, because I do human robot interaction,\n\n49:51.420 --> 49:54.980\n I still think the human needs to be part of the equation\n\n49:54.980 --> 49:56.440\n at some point.\n\n49:56.440 --> 49:58.460\n So what, just looking back,\n\n49:58.460 --> 50:01.900\n what are some fascinating things in robotic space\n\n50:01.900 --> 50:03.460\n that NASA was working at the time?\n\n50:03.460 --> 50:07.700\n Or just in general, what have you gotten to play with\n\n50:07.700 --> 50:10.060\n and what are your memories from working at NASA?\n\n50:10.060 --> 50:12.540\n Yeah, so one of my first memories\n\n50:13.580 --> 50:18.580\n was they were working on a surgical robot system\n\n50:18.580 --> 50:21.880\n that could do eye surgery, right?\n\n50:21.880 --> 50:25.700\n And this was back in, oh my gosh, it must've been,\n\n50:25.700 --> 50:30.580\n oh, maybe 92, 93, 94.\n\n50:30.580 --> 50:32.880\n So it's like almost like a remote operation.\n\n50:32.880 --> 50:34.720\n Yeah, it was remote operation.\n\n50:34.720 --> 50:38.400\n In fact, you can even find some old tech reports on it.\n\n50:38.400 --> 50:41.620\n So think of it, like now we have DaVinci, right?\n\n50:41.620 --> 50:45.880\n Like think of it, but these were like the late 90s, right?\n\n50:45.880 --> 50:48.240\n And I remember going into the lab one day\n\n50:48.240 --> 50:51.000\n and I was like, what's that, right?\n\n50:51.000 --> 50:53.960\n And of course it wasn't pretty, right?\n\n50:53.960 --> 50:56.640\n Because the technology, but it was like functional\n\n50:56.640 --> 50:59.240\n and you had this individual that could use\n\n50:59.240 --> 51:01.960\n a version of haptics to actually do the surgery\n\n51:01.960 --> 51:04.360\n and they had this mockup of a human face\n\n51:04.360 --> 51:08.480\n and like the eyeballs and you can see this little drill.\n\n51:08.480 --> 51:11.680\n And I was like, oh, that is so cool.\n\n51:11.680 --> 51:13.720\n That one I vividly remember\n\n51:13.720 --> 51:18.640\n because it was so outside of my like possible thoughts\n\n51:18.640 --> 51:20.040\n of what could be done.\n\n51:20.040 --> 51:21.360\n It's the kind of precision\n\n51:21.360 --> 51:26.120\n and I mean, what's the most amazing of a thing like that?\n\n51:26.120 --> 51:28.240\n I think it was the precision.\n\n51:28.240 --> 51:31.960\n It was the kind of first time\n\n51:31.960 --> 51:34.880\n that I had physically seen\n\n51:34.880 --> 51:39.640\n this robot machine human interface, right?\n\n51:39.640 --> 51:42.400\n Versus, cause manufacturing had been,\n\n51:42.400 --> 51:44.520\n you saw those kind of big robots, right?\n\n51:44.520 --> 51:48.040\n But this was like, oh, this is in a person.\n\n51:48.040 --> 51:51.400\n There's a person and a robot like in the same space.\n\n51:51.400 --> 51:53.000\n I'm meeting them in person.\n\n51:53.000 --> 51:55.440\n Like for me, it was a magical moment\n\n51:55.440 --> 51:57.900\n that I can't, it was life transforming\n\n51:57.900 --> 52:00.560\n that I recently met Spot Mini from Boston Dynamics.\n\n52:00.560 --> 52:01.400\n Oh, see.\n\n52:01.400 --> 52:04.680\n I don't know why, but on the human robot interaction\n\n52:04.680 --> 52:09.680\n for some reason I realized how easy it is to anthropomorphize\n\n52:09.680 --> 52:12.580\n and it was, I don't know, it was almost\n\n52:12.580 --> 52:14.700\n like falling in love, this feeling of meeting.\n\n52:14.700 --> 52:17.300\n And I've obviously seen these robots a lot\n\n52:17.300 --> 52:19.180\n on video and so on, but meeting in person,\n\n52:19.180 --> 52:22.340\n just having that one on one time is different.\n\n52:22.340 --> 52:25.020\n So have you had a robot like that in your life\n\n52:25.020 --> 52:28.300\n that made you maybe fall in love with robotics?\n\n52:28.300 --> 52:30.480\n Sort of like meeting in person.\n\n52:32.140 --> 52:35.860\n I mean, I loved robotics since, yeah.\n\n52:35.860 --> 52:37.900\n So I was a 12 year old.\n\n52:37.900 --> 52:40.020\n Like I'm gonna be a roboticist, actually was,\n\n52:40.020 --> 52:41.180\n I called it cybernetics.\n\n52:41.180 --> 52:44.700\n But so my motivation was Bionic Woman.\n\n52:44.700 --> 52:46.260\n I don't know if you know that.\n\n52:46.260 --> 52:49.500\n And so, I mean, that was like a seminal moment,\n\n52:49.500 --> 52:52.340\n but I didn't meet, like that was TV, right?\n\n52:52.340 --> 52:54.500\n Like it wasn't like I was in the same space and I met\n\n52:54.500 --> 52:56.540\n and I was like, oh my gosh, you're like real.\n\n52:56.540 --> 52:58.820\n Just linking on Bionic Woman, which by the way,\n\n52:58.820 --> 53:01.100\n because I read that about you.\n\n53:01.100 --> 53:04.340\n I watched bits of it and it's just so,\n\n53:04.340 --> 53:05.520\n no offense, terrible.\n\n53:05.520 --> 53:08.500\n It's cheesy if you look at it now.\n\n53:08.500 --> 53:09.340\n It's cheesy, no.\n\n53:09.340 --> 53:10.900\n I've seen a couple of reruns lately.\n\n53:10.900 --> 53:15.100\n But it's, but of course at the time it's probably\n\n53:15.100 --> 53:16.740\n captured the imagination.\n\n53:16.740 --> 53:18.100\n But the sound effects.\n\n53:18.100 --> 53:23.100\n Especially when you're younger, it just catch you.\n\n53:23.100 --> 53:24.720\n But which aspect, did you think of it,\n\n53:24.720 --> 53:27.700\n you mentioned cybernetics, did you think of it as robotics\n\n53:27.700 --> 53:30.140\n or did you think of it as almost constructing\n\n53:30.140 --> 53:31.620\n artificial beings?\n\n53:31.620 --> 53:36.200\n Like, is it the intelligent part that captured\n\n53:36.200 --> 53:38.060\n your fascination or was it the whole thing?\n\n53:38.060 --> 53:39.820\n Like even just the limbs and just the.\n\n53:39.820 --> 53:42.900\n So for me, it would have, in another world,\n\n53:42.900 --> 53:46.820\n I probably would have been more of a biomedical engineer\n\n53:46.820 --> 53:50.040\n because what fascinated me was the parts,\n\n53:50.040 --> 53:55.040\n like the bionic parts, the limbs, those aspects of it.\n\n53:55.060 --> 53:59.620\n Are you especially drawn to humanoid or humanlike robots?\n\n53:59.620 --> 54:03.060\n I would say humanlike, not humanoid, right?\n\n54:03.060 --> 54:05.900\n And when I say humanlike, I think it's this aspect\n\n54:05.900 --> 54:09.140\n of that interaction, whether it's social\n\n54:09.140 --> 54:10.660\n and it's like a dog, right?\n\n54:10.660 --> 54:14.100\n Like that's humanlike because it understand us,\n\n54:14.100 --> 54:17.620\n it interacts with us at that very social level\n\n54:18.500 --> 54:21.860\n to, you know, humanoids are part of that,\n\n54:21.860 --> 54:26.860\n but only if they interact with us as if we are human.\n\n54:26.860 --> 54:30.080\n Okay, but just to linger on NASA for a little bit,\n\n54:30.980 --> 54:34.100\n what do you think, maybe if you have other memories,\n\n54:34.100 --> 54:38.580\n but also what do you think is the future of robots in space?\n\n54:38.580 --> 54:41.900\n We'll mention how, but there's incredible robots\n\n54:41.900 --> 54:44.100\n that NASA's working on in general thinking about\n\n54:44.100 --> 54:49.100\n in our, as we venture out, human civilization ventures out\n\n54:49.820 --> 54:52.260\n into space, what do you think the future of robots is there?\n\n54:52.260 --> 54:53.700\n Yeah, so I mean, there's the near term.\n\n54:53.700 --> 54:57.300\n For example, they just announced the rover\n\n54:57.300 --> 55:00.780\n that's going to the moon, which, you know,\n\n55:00.780 --> 55:05.780\n that's kind of exciting, but that's like near term.\n\n55:06.100 --> 55:11.100\n You know, my favorite, favorite, favorite series\n\n55:11.180 --> 55:13.340\n is Star Trek, right?\n\n55:13.340 --> 55:17.200\n You know, I really hope, and even Star Trek,\n\n55:17.200 --> 55:20.100\n like if I calculate the years, I wouldn't be alive,\n\n55:20.100 --> 55:25.100\n but I would really, really love to be in that world.\n\n55:26.700 --> 55:28.460\n Like, even if it's just at the beginning,\n\n55:28.460 --> 55:33.180\n like, you know, like voyage, like adventure one.\n\n55:33.180 --> 55:35.740\n So basically living in space.\n\n55:35.740 --> 55:36.580\n Yeah.\n\n55:36.580 --> 55:39.740\n With, what robots, what are robots?\n\n55:39.740 --> 55:40.580\n With data.\n\n55:40.580 --> 55:41.400\n What role?\n\n55:41.400 --> 55:42.820\n The data would have to be, even though that wasn't,\n\n55:42.820 --> 55:44.740\n you know, that was like later, but.\n\n55:44.740 --> 55:49.160\n So data is a robot that has human like qualities.\n\n55:49.160 --> 55:50.500\n Right, without the emotion chip.\n\n55:50.500 --> 55:51.340\n Yeah.\n\n55:51.340 --> 55:52.220\n You don't like emotion.\n\n55:52.220 --> 55:54.220\n Well, so data with the emotion chip\n\n55:54.220 --> 55:58.580\n was kind of a mess, right?\n\n55:58.580 --> 56:03.580\n It took a while for that thing to adapt,\n\n56:04.660 --> 56:08.580\n but, and so why was that an issue?\n\n56:08.580 --> 56:13.580\n The issue is that emotions make us irrational agents.\n\n56:14.240 --> 56:15.240\n That's the problem.\n\n56:15.240 --> 56:20.040\n And yet he could think through things,\n\n56:20.040 --> 56:23.440\n even if it was based on an emotional scenario, right?\n\n56:23.440 --> 56:25.080\n Based on pros and cons.\n\n56:25.080 --> 56:28.520\n But as soon as you made him emotional,\n\n56:28.520 --> 56:31.160\n one of the metrics he used for evaluation\n\n56:31.160 --> 56:35.480\n was his own emotions, not people around him, right?\n\n56:35.480 --> 56:37.280\n Like, and so.\n\n56:37.280 --> 56:39.000\n We do that as children, right?\n\n56:39.000 --> 56:40.920\n So we're very egocentric when we're young.\n\n56:40.920 --> 56:42.320\n We are very egocentric.\n\n56:42.320 --> 56:45.800\n And so isn't that just an early version of the emotion chip\n\n56:45.800 --> 56:48.280\n then, I haven't watched much Star Trek.\n\n56:48.280 --> 56:52.460\n Except I have also met adults, right?\n\n56:52.460 --> 56:54.600\n And so that is a developmental process.\n\n56:54.600 --> 56:57.600\n And I'm sure there's a bunch of psychologists\n\n56:57.600 --> 57:00.640\n that can go through, like you can have a 60 year old adult\n\n57:00.640 --> 57:04.640\n who has the emotional maturity of a 10 year old, right?\n\n57:04.640 --> 57:08.880\n And so there's various phases that people should go through\n\n57:08.880 --> 57:11.480\n in order to evolve and sometimes you don't.\n\n57:11.480 --> 57:14.840\n So how much psychology do you think,\n\n57:14.840 --> 57:17.600\n a topic that's rarely mentioned in robotics,\n\n57:17.600 --> 57:19.700\n but how much does psychology come to play\n\n57:19.700 --> 57:23.600\n when you're talking about HRI, human robot interaction?\n\n57:23.600 --> 57:25.000\n When you have to have robots\n\n57:25.000 --> 57:26.120\n that actually interact with humans.\n\n57:26.120 --> 57:26.960\n Tons.\n\n57:26.960 --> 57:31.360\n So we, like my group, as well as I read a lot\n\n57:31.360 --> 57:33.280\n in the cognitive science literature,\n\n57:33.280 --> 57:36.160\n as well as the psychology literature.\n\n57:36.160 --> 57:41.160\n Because they understand a lot about human, human relations\n\n57:42.720 --> 57:45.920\n and developmental milestones and things like that.\n\n57:45.920 --> 57:50.920\n And so we tend to look to see what's been done out there.\n\n57:53.120 --> 57:56.500\n Sometimes what we'll do is we'll try to match that to see,\n\n57:56.500 --> 58:00.980\n is that human, human relationship the same as human robot?\n\n58:00.980 --> 58:03.080\n Sometimes it is, and sometimes it's different.\n\n58:03.080 --> 58:04.740\n And then when it's different, we have to,\n\n58:04.740 --> 58:06.440\n we try to figure out, okay,\n\n58:06.440 --> 58:09.040\n why is it different in this scenario?\n\n58:09.040 --> 58:11.900\n But it's the same in the other scenario, right?\n\n58:11.900 --> 58:15.320\n And so we try to do that quite a bit.\n\n58:15.320 --> 58:17.800\n Would you say that's, if we're looking at the future\n\n58:17.800 --> 58:19.140\n of human robot interaction,\n\n58:19.140 --> 58:22.040\n would you say the psychology piece is the hardest?\n\n58:22.040 --> 58:25.640\n Like if, I mean, it's a funny notion for you as,\n\n58:25.640 --> 58:27.360\n I don't know if you consider, yeah.\n\n58:27.360 --> 58:28.400\n I mean, one way to ask it,\n\n58:28.400 --> 58:32.000\n do you consider yourself a roboticist or a psychologist?\n\n58:32.000 --> 58:33.600\n Oh, I consider myself a roboticist\n\n58:33.600 --> 58:36.240\n that plays the act of a psychologist.\n\n58:36.240 --> 58:38.980\n But if you were to look at yourself sort of,\n\n58:40.120 --> 58:42.360\n 20, 30 years from now,\n\n58:42.360 --> 58:43.880\n do you see yourself more and more\n\n58:43.880 --> 58:45.300\n wearing the psychology hat?\n\n58:47.560 --> 58:49.000\n Another way to put it is,\n\n58:49.000 --> 58:51.600\n are the hard problems in human robot interactions\n\n58:51.600 --> 58:55.800\n fundamentally psychology, or is it still robotics,\n\n58:55.800 --> 58:57.720\n the perception manipulation, planning,\n\n58:57.720 --> 58:59.460\n all that kind of stuff?\n\n58:59.460 --> 59:01.680\n It's actually neither.\n\n59:01.680 --> 59:05.160\n The hardest part is the adaptation and the interaction.\n\n59:06.120 --> 59:08.840\n So it's the interface, it's the learning.\n\n59:08.840 --> 59:11.600\n And so if I think of,\n\n59:11.600 --> 59:16.600\n like I've become much more of a roboticist slash AI person\n\n59:17.180 --> 59:19.040\n than when I, like originally, again,\n\n59:19.040 --> 59:20.160\n I was about the bionics.\n\n59:20.160 --> 59:24.040\n I was electrical engineer, I was control theory, right?\n\n59:24.040 --> 59:28.780\n And then I started realizing that my algorithms\n\n59:28.780 --> 59:30.600\n needed like human data, right?\n\n59:30.600 --> 59:32.760\n And so then I was like, okay, what is this human thing?\n\n59:32.760 --> 59:34.360\n How do I incorporate human data?\n\n59:34.360 --> 59:38.440\n And then I realized that human perception had,\n\n59:38.440 --> 59:41.040\n like there was a lot in terms of how we perceive the world.\n\n59:41.040 --> 59:41.940\n And so trying to figure out\n\n59:41.940 --> 59:44.400\n how do I model human perception for my,\n\n59:44.400 --> 59:47.600\n and so I became a HRI person,\n\n59:47.600 --> 59:49.320\n human robot interaction person,\n\n59:49.320 --> 59:51.760\n from being a control theory and realizing\n\n59:51.760 --> 59:54.320\n that humans actually offered quite a bit.\n\n59:55.220 --> 59:56.060\n And then when you do that,\n\n59:56.060 --> 59:59.280\n you become more of an artificial intelligence, AI.\n\n59:59.280 --> 1:00:04.280\n And so I see myself evolving more in this AI world\n\n1:00:05.680 --> 1:00:09.560\n under the lens of robotics,\n\n1:00:09.560 --> 1:00:12.100\n having hardware, interacting with people.\n\n1:00:12.100 --> 1:00:17.100\n So you're a world class expert researcher in robotics,\n\n1:00:17.840 --> 1:00:21.120\n and yet others, you know, there's a few,\n\n1:00:21.120 --> 1:00:24.160\n it's a small but fierce community of people,\n\n1:00:24.160 --> 1:00:26.600\n but most of them don't take the journey\n\n1:00:26.600 --> 1:00:29.440\n into the H of HRI, into the human.\n\n1:00:29.440 --> 1:00:34.440\n So why did you brave into the interaction with humans?\n\n1:00:34.440 --> 1:00:36.880\n It seems like a really hard problem.\n\n1:00:36.880 --> 1:00:39.880\n It's a hard problem, and it's very risky as an academic.\n\n1:00:41.080 --> 1:00:45.320\n And I knew that when I started down that journey,\n\n1:00:46.200 --> 1:00:49.880\n that it was very risky as an academic\n\n1:00:49.880 --> 1:00:53.440\n in this world that was nuance, it was just developing.\n\n1:00:53.440 --> 1:00:56.720\n We didn't even have a conference, right, at the time.\n\n1:00:56.720 --> 1:01:00.120\n Because it was the interesting problems.\n\n1:01:00.120 --> 1:01:01.560\n That was what drove me.\n\n1:01:01.560 --> 1:01:06.560\n It was the fact that I looked at what interests me\n\n1:01:06.920 --> 1:01:10.400\n in terms of the application space and the problems.\n\n1:01:10.400 --> 1:01:14.900\n And that pushed me into trying to figure out\n\n1:01:14.900 --> 1:01:16.840\n what people were and what humans were\n\n1:01:16.840 --> 1:01:18.160\n and how to adapt to them.\n\n1:01:19.040 --> 1:01:21.280\n If those problems weren't so interesting,\n\n1:01:21.280 --> 1:01:26.280\n I'd probably still be sending rovers to glaciers, right?\n\n1:01:26.280 --> 1:01:28.080\n But the problems were interesting.\n\n1:01:28.080 --> 1:01:30.600\n And the other thing was that they were hard, right?\n\n1:01:30.600 --> 1:01:34.560\n So it's, I like having to go into a room\n\n1:01:34.560 --> 1:01:37.000\n and being like, I don't know what to do.\n\n1:01:37.000 --> 1:01:38.280\n And then going back and saying, okay,\n\n1:01:38.280 --> 1:01:39.800\n I'm gonna figure this out.\n\n1:01:39.800 --> 1:01:42.320\n I do not, I'm not driven when I go in like,\n\n1:01:42.320 --> 1:01:44.040\n oh, there are no surprises.\n\n1:01:44.040 --> 1:01:47.320\n Like, I don't find that satisfying.\n\n1:01:47.320 --> 1:01:48.160\n If that was the case,\n\n1:01:48.160 --> 1:01:51.020\n I'd go someplace and make a lot more money, right?\n\n1:01:51.020 --> 1:01:55.000\n I think I stay in academic because and choose to do this\n\n1:01:55.000 --> 1:01:58.280\n because I can go into a room and like, that's hard.\n\n1:01:58.280 --> 1:02:01.720\n Yeah, I think just from my perspective,\n\n1:02:01.720 --> 1:02:03.200\n maybe you can correct me on it,\n\n1:02:03.200 --> 1:02:06.720\n but if I just look at the field of AI broadly,\n\n1:02:06.720 --> 1:02:11.720\n it seems that human robot interaction has the most,\n\n1:02:12.020 --> 1:02:16.540\n one of the most number of open problems.\n\n1:02:16.540 --> 1:02:20.280\n Like people, especially relative to how many people\n\n1:02:20.280 --> 1:02:23.920\n are willing to acknowledge that there are this,\n\n1:02:23.920 --> 1:02:26.160\n because most people are just afraid of the humans\n\n1:02:26.160 --> 1:02:27.240\n so they don't even acknowledge\n\n1:02:27.240 --> 1:02:28.200\n how many open problems there are.\n\n1:02:28.200 --> 1:02:30.440\n But it's in terms of difficult problems\n\n1:02:30.440 --> 1:02:32.400\n to solve exciting spaces,\n\n1:02:32.400 --> 1:02:35.840\n it seems to be incredible for that.\n\n1:02:35.840 --> 1:02:38.680\n It is, and it's exciting.\n\n1:02:38.680 --> 1:02:40.040\n You've mentioned trust before.\n\n1:02:40.040 --> 1:02:45.040\n What role does trust from interacting with autopilot\n\n1:02:46.860 --> 1:02:48.480\n to in the medical context,\n\n1:02:48.480 --> 1:02:51.320\n what role does trust play in the human robot interactions?\n\n1:02:51.320 --> 1:02:53.920\n So some of the things I study in this domain\n\n1:02:53.920 --> 1:02:56.920\n is not just trust, but it really is over trust.\n\n1:02:56.920 --> 1:02:58.160\n How do you think about over trust?\n\n1:02:58.160 --> 1:03:02.280\n Like what is, first of all, what is trust\n\n1:03:02.280 --> 1:03:03.360\n and what is over trust?\n\n1:03:03.360 --> 1:03:05.780\n Basically, the way I look at it is,\n\n1:03:05.780 --> 1:03:08.040\n trust is not what you click on a survey,\n\n1:03:08.040 --> 1:03:09.560\n trust is about your behavior.\n\n1:03:09.560 --> 1:03:11.880\n So if you interact with the technology\n\n1:03:13.460 --> 1:03:17.280\n based on the decision or the actions of the technology\n\n1:03:17.280 --> 1:03:20.700\n as if you trust that decision, then you're trusting.\n\n1:03:22.360 --> 1:03:25.560\n And even in my group, we've done surveys\n\n1:03:25.560 --> 1:03:28.240\n that on the thing, do you trust robots?\n\n1:03:28.240 --> 1:03:29.080\n Of course not.\n\n1:03:29.080 --> 1:03:31.640\n Would you follow this robot in a burdening building?\n\n1:03:31.640 --> 1:03:32.920\n Of course not.\n\n1:03:32.920 --> 1:03:35.480\n And then you look at their actions and you're like,\n\n1:03:35.480 --> 1:03:39.640\n clearly your behavior does not match what you think\n\n1:03:39.640 --> 1:03:42.000\n or what you think you would like to think.\n\n1:03:42.000 --> 1:03:44.040\n And so I'm really concerned about the behavior\n\n1:03:44.040 --> 1:03:45.800\n because that's really at the end of the day,\n\n1:03:45.800 --> 1:03:47.340\n when you're in the world,\n\n1:03:47.340 --> 1:03:50.500\n that's what will impact others around you.\n\n1:03:50.500 --> 1:03:52.920\n It's not whether before you went onto the street,\n\n1:03:52.920 --> 1:03:55.640\n you clicked on like, I don't trust self driving cars.\n\n1:03:55.640 --> 1:03:58.680\n Yeah, that from an outsider perspective,\n\n1:03:58.680 --> 1:04:00.600\n it's always frustrating to me.\n\n1:04:00.600 --> 1:04:02.480\n Well, I read a lot, so I'm insider\n\n1:04:02.480 --> 1:04:04.180\n in a certain philosophical sense.\n\n1:04:06.040 --> 1:04:10.680\n It's frustrating to me how often trust is used in surveys\n\n1:04:10.680 --> 1:04:15.680\n and how people say, make claims out of any kind of finding\n\n1:04:15.680 --> 1:04:18.680\n they make while somebody clicking on answer.\n\n1:04:18.680 --> 1:04:23.680\n You just trust is a, yeah, behavior just,\n\n1:04:23.700 --> 1:04:24.580\n you said it beautifully.\n\n1:04:24.580 --> 1:04:28.080\n I mean, the action, your own behavior is what trust is.\n\n1:04:28.080 --> 1:04:30.740\n I mean, that everything else is not even close.\n\n1:04:30.740 --> 1:04:34.740\n It's almost like absurd comedic poetry\n\n1:04:36.040 --> 1:04:38.500\n that you weave around your actual behavior.\n\n1:04:38.500 --> 1:04:41.780\n So some people can say their trust,\n\n1:04:41.780 --> 1:04:45.620\n you know, I trust my wife, husband or not,\n\n1:04:45.620 --> 1:04:48.260\n whatever, but the actions is what speaks volumes.\n\n1:04:48.260 --> 1:04:52.260\n You bug their car, you probably don't trust them.\n\n1:04:52.260 --> 1:04:53.820\n I trust them, I'm just making sure.\n\n1:04:53.820 --> 1:04:55.620\n No, no, that's, yeah.\n\n1:04:55.620 --> 1:04:57.260\n Like even if you think about cars,\n\n1:04:57.260 --> 1:04:58.580\n I think it's a beautiful case.\n\n1:04:58.580 --> 1:05:01.260\n I came here at some point, I'm sure,\n\n1:05:01.260 --> 1:05:03.580\n on either Uber or Lyft, right?\n\n1:05:03.580 --> 1:05:06.020\n I remember when it first came out, right?\n\n1:05:06.020 --> 1:05:08.020\n I bet if they had had a survey,\n\n1:05:08.020 --> 1:05:11.420\n would you get in the car with a stranger and pay them?\n\n1:05:11.420 --> 1:05:12.660\n Yes.\n\n1:05:12.660 --> 1:05:15.300\n How many people do you think would have said,\n\n1:05:15.300 --> 1:05:16.620\n like, really?\n\n1:05:16.620 --> 1:05:18.660\n Wait, even worse, would you get in the car\n\n1:05:18.660 --> 1:05:21.900\n with a stranger at 1 a.m. in the morning\n\n1:05:21.900 --> 1:05:24.780\n to have them drop you home as a single female?\n\n1:05:24.780 --> 1:05:25.620\n Yeah.\n\n1:05:25.620 --> 1:05:29.280\n Like how many people would say, that's stupid.\n\n1:05:29.280 --> 1:05:30.120\n Yeah.\n\n1:05:30.120 --> 1:05:31.540\n And now look at where we are.\n\n1:05:31.540 --> 1:05:33.940\n I mean, people put kids, right?\n\n1:05:33.940 --> 1:05:37.660\n Like, oh yeah, my child has to go to school\n\n1:05:37.660 --> 1:05:42.300\n and yeah, I'm gonna put my kid in this car with a stranger.\n\n1:05:42.300 --> 1:05:45.580\n I mean, it's just fascinating how, like,\n\n1:05:45.580 --> 1:05:48.260\n what we think we think is not necessarily\n\n1:05:48.260 --> 1:05:49.620\n matching our behavior.\n\n1:05:49.620 --> 1:05:52.260\n Yeah, and certainly with robots, with autonomous vehicles\n\n1:05:52.260 --> 1:05:54.620\n and all the kinds of robots you work with,\n\n1:05:54.620 --> 1:05:59.620\n that's, it's, yeah, it's, the way you answer it,\n\n1:06:00.340 --> 1:06:03.340\n especially if you've never interacted with that robot before,\n\n1:06:04.300 --> 1:06:05.620\n if you haven't had the experience,\n\n1:06:05.620 --> 1:06:09.540\n you being able to respond correctly on a survey is impossible.\n\n1:06:09.540 --> 1:06:12.460\n But what do you, what role does trust play\n\n1:06:12.460 --> 1:06:14.220\n in the interaction, do you think?\n\n1:06:14.220 --> 1:06:19.220\n Like, is it good to, is it good to trust a robot?\n\n1:06:19.380 --> 1:06:21.620\n What does over trust mean?\n\n1:06:21.620 --> 1:06:23.980\n Or is it, is it good to kind of how you feel\n\n1:06:23.980 --> 1:06:26.460\n about autopilot currently, which is like,\n\n1:06:26.460 --> 1:06:29.380\n from a roboticist's perspective, is like,\n\n1:06:29.380 --> 1:06:31.460\n oh, still very cautious?\n\n1:06:31.460 --> 1:06:34.860\n Yeah, so this is still an open area of research,\n\n1:06:34.860 --> 1:06:39.860\n but basically what I would like in a perfect world\n\n1:06:40.700 --> 1:06:44.900\n is that people trust the technology when it's working 100%,\n\n1:06:44.900 --> 1:06:47.260\n and people will be hypersensitive\n\n1:06:47.260 --> 1:06:49.060\n and identify when it's not.\n\n1:06:49.060 --> 1:06:50.940\n But of course we're not there.\n\n1:06:50.940 --> 1:06:52.700\n That's the ideal world.\n\n1:06:53.620 --> 1:06:56.460\n And, but we find is that people swing, right?\n\n1:06:56.460 --> 1:07:01.300\n They tend to swing, which means that if my first,\n\n1:07:01.300 --> 1:07:02.900\n and like, we have some papers,\n\n1:07:02.900 --> 1:07:05.260\n like first impressions is everything, right?\n\n1:07:05.260 --> 1:07:07.620\n If my first instance with technology,\n\n1:07:07.620 --> 1:07:12.620\n with robotics is positive, it mitigates any risk,\n\n1:07:12.700 --> 1:07:16.860\n it correlates with like best outcomes,\n\n1:07:16.860 --> 1:07:21.460\n it means that I'm more likely to either not see it\n\n1:07:21.460 --> 1:07:24.180\n when it makes some mistakes or faults,\n\n1:07:24.180 --> 1:07:27.300\n or I'm more likely to forgive it.\n\n1:07:28.660 --> 1:07:30.340\n And so this is a problem\n\n1:07:30.340 --> 1:07:32.620\n because technology is not 100% accurate, right?\n\n1:07:32.620 --> 1:07:35.100\n It's not 100% accurate, although it may be perfect.\n\n1:07:35.100 --> 1:07:37.700\n How do you get that first moment right, do you think?\n\n1:07:37.700 --> 1:07:40.740\n There's also an education about the capabilities\n\n1:07:40.740 --> 1:07:42.500\n and limitations of the system.\n\n1:07:42.500 --> 1:07:45.740\n Do you have a sense of how do you educate people correctly\n\n1:07:45.740 --> 1:07:47.140\n in that first interaction?\n\n1:07:47.140 --> 1:07:50.260\n Again, this is an open ended problem.\n\n1:07:50.260 --> 1:07:55.020\n So one of the study that actually has given me some hope\n\n1:07:55.020 --> 1:07:57.660\n that I were trying to figure out how to put in robotics.\n\n1:07:57.660 --> 1:08:01.300\n So there was a research study\n\n1:08:01.300 --> 1:08:03.460\n that it showed for medical AI systems,\n\n1:08:03.460 --> 1:08:07.820\n giving information to radiologists about,\n\n1:08:07.820 --> 1:08:12.820\n here you need to look at these areas on the X ray.\n\n1:08:13.980 --> 1:08:18.900\n What they found was that when the system provided\n\n1:08:18.900 --> 1:08:23.900\n one choice, there was this aspect of either no trust\n\n1:08:25.340 --> 1:08:26.860\n or over trust, right?\n\n1:08:26.860 --> 1:08:29.820\n Like I don't believe it at all,\n\n1:08:29.820 --> 1:08:33.580\n or a yes, yes, yes, yes.\n\n1:08:33.580 --> 1:08:36.380\n And they would miss things, right?\n\n1:08:36.380 --> 1:08:40.580\n Instead, when the system gave them multiple choices,\n\n1:08:40.580 --> 1:08:43.260\n like here are the three, even if it knew like,\n\n1:08:43.260 --> 1:08:45.940\n it had estimated that the top area you need to look at\n\n1:08:45.940 --> 1:08:49.780\n was some place on the X ray.\n\n1:08:49.780 --> 1:08:54.060\n If it gave like one plus others,\n\n1:08:54.060 --> 1:08:59.060\n the trust was maintained and the accuracy of the entire\n\n1:09:00.420 --> 1:09:03.580\n population increased, right?\n\n1:09:03.580 --> 1:09:07.500\n So basically it was a, you're still trusting the system,\n\n1:09:07.500 --> 1:09:09.580\n but you're also putting in a little bit of like,\n\n1:09:09.580 --> 1:09:13.660\n your human expertise, like your human decision processing\n\n1:09:13.660 --> 1:09:15.540\n into the equation.\n\n1:09:15.540 --> 1:09:18.540\n So it helps to mitigate that over trust risk.\n\n1:09:18.540 --> 1:09:21.580\n Yeah, so there's a fascinating balance that the strike.\n\n1:09:21.580 --> 1:09:24.420\n Haven't figured out again, robotics is still an open research.\n\n1:09:24.420 --> 1:09:26.740\n This is exciting open area research, exactly.\n\n1:09:26.740 --> 1:09:28.940\n So what are some exciting applications\n\n1:09:28.940 --> 1:09:30.180\n of human robot interaction?\n\n1:09:30.180 --> 1:09:33.060\n You started a company, maybe you can talk about\n\n1:09:33.060 --> 1:09:36.740\n the exciting efforts there, but in general also\n\n1:09:36.740 --> 1:09:41.020\n what other space can robots interact with humans and help?\n\n1:09:41.020 --> 1:09:42.340\n Yeah, so besides healthcare,\n\n1:09:42.340 --> 1:09:44.540\n cause you know, that's my bias lens.\n\n1:09:44.540 --> 1:09:47.100\n My other bias lens is education.\n\n1:09:47.100 --> 1:09:51.260\n I think that, well, one, we definitely,\n\n1:09:51.260 --> 1:09:54.780\n we in the US, you know, we're doing okay with teachers,\n\n1:09:54.780 --> 1:09:56.860\n but there's a lot of school districts\n\n1:09:56.860 --> 1:09:58.300\n that don't have enough teachers.\n\n1:09:58.300 --> 1:10:01.940\n If you think about the teacher student ratio\n\n1:10:01.940 --> 1:10:06.700\n for at least public education in some districts, it's crazy.\n\n1:10:06.700 --> 1:10:10.020\n It's like, how can you have learning in that classroom,\n\n1:10:10.020 --> 1:10:10.860\n right?\n\n1:10:10.860 --> 1:10:12.980\n Because you just don't have the human capital.\n\n1:10:12.980 --> 1:10:15.500\n And so if you think about robotics,\n\n1:10:15.500 --> 1:10:18.460\n bringing that in to classrooms,\n\n1:10:18.460 --> 1:10:20.340\n as well as the afterschool space,\n\n1:10:20.340 --> 1:10:25.100\n where they offset some of this lack of resources\n\n1:10:25.100 --> 1:10:28.460\n in certain communities, I think that's a good place.\n\n1:10:28.460 --> 1:10:30.900\n And then turning on the other end\n\n1:10:30.900 --> 1:10:35.260\n is using these systems then for workforce retraining\n\n1:10:35.260 --> 1:10:38.940\n and dealing with some of the things\n\n1:10:38.940 --> 1:10:43.020\n that are going to come out later on of job loss,\n\n1:10:43.020 --> 1:10:45.900\n like thinking about robots and in AI systems\n\n1:10:45.900 --> 1:10:48.340\n for retraining and workforce development.\n\n1:10:48.340 --> 1:10:53.220\n I think that's exciting areas that can be pushed even more,\n\n1:10:53.220 --> 1:10:56.780\n and it would have a huge, huge impact.\n\n1:10:56.780 --> 1:10:59.620\n What would you say are some of the open problems\n\n1:10:59.620 --> 1:11:03.220\n in education, sort of, it's exciting.\n\n1:11:03.220 --> 1:11:08.220\n So young kids and the older folks\n\n1:11:08.740 --> 1:11:12.580\n or just folks of all ages who need to be retrained,\n\n1:11:12.580 --> 1:11:14.260\n who need to sort of open themselves up\n\n1:11:14.260 --> 1:11:17.700\n to a whole nother area of work.\n\n1:11:17.700 --> 1:11:20.060\n What are the problems to be solved there?\n\n1:11:20.060 --> 1:11:22.460\n How do you think robots can help?\n\n1:11:22.460 --> 1:11:24.820\n We have the engagement aspect, right?\n\n1:11:24.820 --> 1:11:26.460\n So we can figure out the engagement.\n\n1:11:26.460 --> 1:11:27.300\n That's not a...\n\n1:11:27.300 --> 1:11:28.900\n What do you mean by engagement?\n\n1:11:28.900 --> 1:11:33.900\n So identifying whether a person is focused,\n\n1:11:34.940 --> 1:11:38.740\n is like that we can figure out.\n\n1:11:38.740 --> 1:11:43.740\n What we can figure out and there's some positive results\n\n1:11:43.900 --> 1:11:47.180\n in this is that personalized adaptation\n\n1:11:47.180 --> 1:11:49.660\n based on any concepts, right?\n\n1:11:49.660 --> 1:11:54.580\n So imagine I think about, I have an agent\n\n1:11:54.580 --> 1:11:59.580\n and I'm working with a kid learning, I don't know,\n\n1:11:59.620 --> 1:12:03.820\n algebra two, can that same agent then switch\n\n1:12:03.820 --> 1:12:07.980\n and teach some type of new coding skill\n\n1:12:07.980 --> 1:12:11.420\n to a displaced mechanic?\n\n1:12:11.420 --> 1:12:14.500\n Like, what does that actually look like, right?\n\n1:12:14.500 --> 1:12:19.500\n Like hardware might be the same, content is different,\n\n1:12:19.540 --> 1:12:22.700\n two different target demographics of engagement.\n\n1:12:22.700 --> 1:12:24.580\n Like how do you do that?\n\n1:12:24.580 --> 1:12:26.820\n How important do you think personalization\n\n1:12:26.820 --> 1:12:28.580\n is in human robot interaction?\n\n1:12:28.580 --> 1:12:31.980\n And not just a mechanic or student,\n\n1:12:31.980 --> 1:12:35.340\n but like literally to the individual human being.\n\n1:12:35.340 --> 1:12:37.540\n I think personalization is really important,\n\n1:12:37.540 --> 1:12:42.140\n but a caveat is that I think we'd be okay\n\n1:12:42.140 --> 1:12:44.700\n if we can personalize to the group, right?\n\n1:12:44.700 --> 1:12:49.700\n And so if I can label you\n\n1:12:49.700 --> 1:12:52.780\n as along some certain dimensions,\n\n1:12:52.780 --> 1:12:56.500\n then even though it may not be you specifically,\n\n1:12:56.500 --> 1:12:58.220\n I can put you in this group.\n\n1:12:58.220 --> 1:13:00.500\n So the sample size, this is how they best learn,\n\n1:13:00.500 --> 1:13:02.020\n this is how they best engage.\n\n1:13:03.220 --> 1:13:06.780\n Even at that level, it's really important.\n\n1:13:06.780 --> 1:13:09.620\n And it's because, I mean, it's one of the reasons\n\n1:13:09.620 --> 1:13:13.340\n why educating in large classrooms is so hard, right?\n\n1:13:13.340 --> 1:13:15.980\n You teach to the median,\n\n1:13:15.980 --> 1:13:19.780\n but there's these individuals that are struggling\n\n1:13:19.780 --> 1:13:22.340\n and then you have highly intelligent individuals\n\n1:13:22.340 --> 1:13:26.340\n and those are the ones that are usually kind of left out.\n\n1:13:26.340 --> 1:13:28.900\n So highly intelligent individuals may be disruptive\n\n1:13:28.900 --> 1:13:30.860\n and those who are struggling might be disruptive\n\n1:13:30.860 --> 1:13:32.980\n because they're both bored.\n\n1:13:32.980 --> 1:13:35.580\n Yeah, and if you narrow the definition of the group\n\n1:13:35.580 --> 1:13:37.900\n or in the size of the group enough,\n\n1:13:37.900 --> 1:13:40.380\n you'll be able to address their individual,\n\n1:13:40.380 --> 1:13:44.580\n it's not individual needs, but really the most important\n\n1:13:44.580 --> 1:13:45.980\n group needs, right?\n\n1:13:45.980 --> 1:13:47.780\n And that's kind of what a lot of successful\n\n1:13:47.780 --> 1:13:50.980\n recommender systems do with Spotify and so on.\n\n1:13:50.980 --> 1:13:53.820\n So it's sad to believe, but as a music listener,\n\n1:13:53.820 --> 1:13:55.940\n probably in some sort of large group,\n\n1:13:55.940 --> 1:13:58.300\n it's very sadly predictable.\n\n1:13:58.300 --> 1:13:59.260\n You have been labeled.\n\n1:13:59.260 --> 1:14:02.100\n Yeah, I've been labeled and successfully so\n\n1:14:02.100 --> 1:14:04.820\n because they're able to recommend stuff that I like.\n\n1:14:04.820 --> 1:14:07.740\n Yeah, but applying that to education, right?\n\n1:14:07.740 --> 1:14:09.780\n There's no reason why it can't be done.\n\n1:14:09.780 --> 1:14:13.060\n Do you have a hope for our education system?\n\n1:14:13.060 --> 1:14:16.180\n I have more hope for workforce development.\n\n1:14:16.180 --> 1:14:19.660\n And that's because I'm seeing investments.\n\n1:14:19.660 --> 1:14:23.300\n Even if you look at VC investments in education,\n\n1:14:23.300 --> 1:14:26.140\n the majority of it has lately been going\n\n1:14:26.140 --> 1:14:28.540\n to workforce retraining, right?\n\n1:14:28.540 --> 1:14:32.860\n And so I think that government investments is increasing.\n\n1:14:32.860 --> 1:14:36.060\n There's like a claim and some of it's based on fear, right?\n\n1:14:36.060 --> 1:14:37.980\n Like AI is gonna come and take over all these jobs.\n\n1:14:37.980 --> 1:14:41.500\n What are we gonna do with all these nonpaying taxes\n\n1:14:41.500 --> 1:14:44.340\n that aren't coming to us by our citizens?\n\n1:14:44.340 --> 1:14:47.140\n And so I think I'm more hopeful for that.\n\n1:14:48.060 --> 1:14:51.780\n Not so hopeful for early education\n\n1:14:51.780 --> 1:14:56.380\n because it's still a who's gonna pay for it.\n\n1:14:56.380 --> 1:15:01.380\n And you won't see the results for like 16 to 18 years.\n\n1:15:01.380 --> 1:15:05.940\n It's hard for people to wrap their heads around that.\n\n1:15:07.180 --> 1:15:10.580\n But on the retraining part, what are your thoughts?\n\n1:15:10.580 --> 1:15:13.860\n There's a candidate, Andrew Yang running for president\n\n1:15:13.860 --> 1:15:18.860\n and saying that sort of AI, automation, robots.\n\n1:15:18.940 --> 1:15:20.940\n Universal basic income.\n\n1:15:20.940 --> 1:15:23.900\n Universal basic income in order to support us\n\n1:15:23.900 --> 1:15:26.740\n as we kind of automation takes people's jobs\n\n1:15:26.740 --> 1:15:30.180\n and allows you to explore and find other means.\n\n1:15:30.180 --> 1:15:35.180\n Like do you have a concern of society\n\n1:15:35.660 --> 1:15:40.500\n transforming effects of automation and robots and so on?\n\n1:15:40.500 --> 1:15:41.340\n I do.\n\n1:15:41.340 --> 1:15:46.180\n I do know that AI robotics will displace workers.\n\n1:15:46.180 --> 1:15:47.980\n Like we do know that.\n\n1:15:47.980 --> 1:15:49.500\n But there'll be other workers\n\n1:15:49.500 --> 1:15:54.500\n that will be defined new jobs.\n\n1:15:54.980 --> 1:15:57.460\n What I worry about is, that's not what I worry about.\n\n1:15:57.460 --> 1:15:59.500\n Like will all the jobs go away?\n\n1:15:59.500 --> 1:16:02.460\n What I worry about is the type of jobs that will come out.\n\n1:16:02.460 --> 1:16:06.340\n Like people who graduate from Georgia Tech will be okay.\n\n1:16:06.340 --> 1:16:07.660\n We give them the skills,\n\n1:16:07.660 --> 1:16:10.660\n they will adapt even if their current job goes away.\n\n1:16:10.660 --> 1:16:12.620\n I do worry about those\n\n1:16:12.620 --> 1:16:15.460\n that don't have that quality of an education.\n\n1:16:15.460 --> 1:16:18.300\n Will they have the ability,\n\n1:16:18.300 --> 1:16:21.700\n the background to adapt to those new jobs?\n\n1:16:21.700 --> 1:16:22.980\n That I don't know.\n\n1:16:22.980 --> 1:16:24.220\n That I worry about,\n\n1:16:24.220 --> 1:16:27.220\n which will create even more polarization\n\n1:16:27.220 --> 1:16:31.220\n in our society, internationally and everywhere.\n\n1:16:31.220 --> 1:16:32.940\n I worry about that.\n\n1:16:32.940 --> 1:16:36.820\n I also worry about not having equal access\n\n1:16:36.820 --> 1:16:39.540\n to all these wonderful things that AI can do\n\n1:16:39.540 --> 1:16:41.100\n and robotics can do.\n\n1:16:41.100 --> 1:16:42.540\n I worry about that.\n\n1:16:43.620 --> 1:16:48.620\n People like me from Georgia Tech from say MIT\n\n1:16:48.860 --> 1:16:50.340\n will be okay, right?\n\n1:16:50.340 --> 1:16:53.340\n But that's such a small part of the population\n\n1:16:53.340 --> 1:16:55.940\n that we need to think much more globally\n\n1:16:55.940 --> 1:16:58.500\n of having access to the beautiful things,\n\n1:16:58.500 --> 1:17:01.580\n whether it's AI in healthcare, AI in education,\n\n1:17:01.580 --> 1:17:05.140\n AI in politics, right?\n\n1:17:05.140 --> 1:17:05.980\n I worry about that.\n\n1:17:05.980 --> 1:17:08.140\n And that's part of the thing that you were talking about\n\n1:17:08.140 --> 1:17:09.660\n is people that build the technology\n\n1:17:09.660 --> 1:17:12.420\n have to be thinking about ethics,\n\n1:17:12.420 --> 1:17:15.220\n have to be thinking about access and all those things.\n\n1:17:15.220 --> 1:17:17.900\n And not just a small subset.\n\n1:17:17.900 --> 1:17:20.300\n Let me ask some philosophical,\n\n1:17:20.300 --> 1:17:22.460\n slightly romantic questions.\n\n1:17:22.460 --> 1:17:24.900\n People that listen to this will be like,\n\n1:17:24.900 --> 1:17:26.180\n here he goes again.\n\n1:17:26.180 --> 1:17:31.180\n Okay, do you think one day we'll build an AI system\n\n1:17:31.940 --> 1:17:35.500\n that a person can fall in love with\n\n1:17:35.500 --> 1:17:37.900\n and it would love them back?\n\n1:17:37.900 --> 1:17:39.780\n Like in the movie, Her, for example.\n\n1:17:39.780 --> 1:17:43.260\n Yeah, although she kind of didn't fall in love with him\n\n1:17:43.260 --> 1:17:45.500\n or she fell in love with like a million other people,\n\n1:17:45.500 --> 1:17:47.060\n something like that.\n\n1:17:47.060 --> 1:17:48.460\n You're the jealous type, I see.\n\n1:17:48.460 --> 1:17:50.820\n We humans are the jealous type.\n\n1:17:50.820 --> 1:17:55.060\n Yes, so I do believe that we can design systems\n\n1:17:55.060 --> 1:17:59.420\n where people would fall in love with their robot,\n\n1:17:59.420 --> 1:18:03.220\n with their AI partner.\n\n1:18:03.220 --> 1:18:05.100\n That I do believe.\n\n1:18:05.100 --> 1:18:06.300\n Because it's actually,\n\n1:18:06.300 --> 1:18:08.900\n and I don't like to use the word manipulate,\n\n1:18:08.900 --> 1:18:12.300\n but as we see, there are certain individuals\n\n1:18:12.300 --> 1:18:13.340\n that can be manipulated\n\n1:18:13.340 --> 1:18:16.260\n if you understand the cognitive science about it, right?\n\n1:18:16.260 --> 1:18:19.620\n Right, so I mean, if you could think of all close\n\n1:18:19.620 --> 1:18:21.380\n relationship and love in general\n\n1:18:21.380 --> 1:18:24.700\n as a kind of mutual manipulation,\n\n1:18:24.700 --> 1:18:27.100\n that dance, the human dance.\n\n1:18:27.100 --> 1:18:30.180\n I mean, manipulation is a negative connotation.\n\n1:18:30.180 --> 1:18:32.820\n And that's why I don't like to use that word particularly.\n\n1:18:32.820 --> 1:18:34.220\n I guess another way to phrase it is,\n\n1:18:34.220 --> 1:18:36.900\n you're getting at is it could be algorithmatized\n\n1:18:36.900 --> 1:18:38.380\n or something, it could be a.\n\n1:18:38.380 --> 1:18:40.620\n The relationship building part can be.\n\n1:18:40.620 --> 1:18:41.820\n I mean, just think about it.\n\n1:18:41.820 --> 1:18:44.780\n We have, and I don't use dating sites,\n\n1:18:44.780 --> 1:18:48.940\n but from what I heard, there are some individuals\n\n1:18:48.940 --> 1:18:52.780\n that have been dating that have never saw each other, right?\n\n1:18:52.780 --> 1:18:54.100\n In fact, there's a show I think\n\n1:18:54.100 --> 1:18:57.540\n that tries to like weed out fake people.\n\n1:18:57.540 --> 1:18:59.460\n Like there's a show that comes out, right?\n\n1:18:59.460 --> 1:19:01.940\n Because like people start faking.\n\n1:19:01.940 --> 1:19:05.140\n Like, what's the difference of that person\n\n1:19:05.140 --> 1:19:08.020\n on the other end being an AI agent, right?\n\n1:19:08.020 --> 1:19:09.340\n And having a communication\n\n1:19:09.340 --> 1:19:12.180\n and you building a relationship remotely,\n\n1:19:12.180 --> 1:19:15.660\n like there's no reason why that can't happen.\n\n1:19:15.660 --> 1:19:17.620\n In terms of human robot interaction,\n\n1:19:17.620 --> 1:19:19.660\n so what role, you've kind of mentioned\n\n1:19:19.660 --> 1:19:23.940\n with data emotion being, can be problematic\n\n1:19:23.940 --> 1:19:26.220\n if not implemented well, I suppose.\n\n1:19:26.220 --> 1:19:30.500\n What role does emotion and some other human like things,\n\n1:19:30.500 --> 1:19:32.820\n the imperfect things come into play here\n\n1:19:32.820 --> 1:19:37.300\n for good human robot interaction and something like love?\n\n1:19:37.300 --> 1:19:39.780\n Yeah, so in this case, and you had asked,\n\n1:19:39.780 --> 1:19:43.700\n can an AI agent love a human back?\n\n1:19:43.700 --> 1:19:47.340\n I think they can emulate love back, right?\n\n1:19:47.340 --> 1:19:48.980\n And so what does that actually mean?\n\n1:19:48.980 --> 1:19:52.260\n It just means that if you think about their programming,\n\n1:19:52.260 --> 1:19:55.220\n they might put the other person's needs\n\n1:19:55.220 --> 1:19:57.980\n in front of theirs in certain situations, right?\n\n1:19:57.980 --> 1:20:00.380\n You look at, think about it as a return on investment.\n\n1:20:00.380 --> 1:20:01.740\n Like, what's my return on investment?\n\n1:20:01.740 --> 1:20:04.540\n As part of that equation, that person's happiness,\n\n1:20:04.540 --> 1:20:07.940\n has some type of algorithm waiting to it.\n\n1:20:07.940 --> 1:20:11.380\n And the reason why is because I care about them, right?\n\n1:20:11.380 --> 1:20:13.700\n That's the only reason, right?\n\n1:20:13.700 --> 1:20:15.540\n But if I care about them and I show that,\n\n1:20:15.540 --> 1:20:18.300\n then my final objective function\n\n1:20:18.300 --> 1:20:20.580\n is length of time of the engagement, right?\n\n1:20:20.580 --> 1:20:24.020\n So you can think of how to do this actually quite easily.\n\n1:20:24.020 --> 1:20:24.860\n And so.\n\n1:20:24.860 --> 1:20:26.500\n But that's not love?\n\n1:20:27.420 --> 1:20:28.820\n Well, so that's the thing.\n\n1:20:29.940 --> 1:20:32.580\n I think it emulates love\n\n1:20:32.580 --> 1:20:37.580\n because we don't have a classical definition of love.\n\n1:20:38.540 --> 1:20:41.660\n Right, but, and we don't have the ability\n\n1:20:41.660 --> 1:20:45.500\n to look into each other's minds to see the algorithm.\n\n1:20:45.500 --> 1:20:48.740\n And I mean, I guess what I'm getting at is,\n\n1:20:48.740 --> 1:20:51.020\n is it possible that, especially if that's learned,\n\n1:20:51.020 --> 1:20:52.580\n especially if there's some mystery\n\n1:20:52.580 --> 1:20:55.220\n and black box nature to the system,\n\n1:20:55.220 --> 1:20:57.660\n how is that, you know?\n\n1:20:57.660 --> 1:20:58.580\n How is it any different?\n\n1:20:58.580 --> 1:21:00.660\n How is it any different in terms of sort of\n\n1:21:00.660 --> 1:21:04.060\n if the system says, I'm conscious, I'm afraid of death,\n\n1:21:05.180 --> 1:21:10.180\n and it does indicate that it loves you.\n\n1:21:10.860 --> 1:21:12.780\n Another way to sort of phrase it,\n\n1:21:12.780 --> 1:21:14.180\n be curious to see what you think.\n\n1:21:14.180 --> 1:21:15.700\n Do you think there'll be a time\n\n1:21:16.700 --> 1:21:20.140\n when robots should have rights?\n\n1:21:20.140 --> 1:21:23.420\n You've kind of phrased the robot in a very roboticist way\n\n1:21:23.420 --> 1:21:25.940\n and just a really good way, but saying, okay,\n\n1:21:25.940 --> 1:21:27.940\n well, there's an objective function\n\n1:21:27.940 --> 1:21:30.620\n and I could see how you can create\n\n1:21:30.620 --> 1:21:33.380\n a compelling human robot interaction experience\n\n1:21:33.380 --> 1:21:36.300\n that makes you believe that the robot cares for your needs\n\n1:21:36.300 --> 1:21:38.940\n and even something like loves you.\n\n1:21:38.940 --> 1:21:43.740\n But what if the robot says, please don't turn me off?\n\n1:21:43.740 --> 1:21:46.460\n What if the robot starts making you feel\n\n1:21:46.460 --> 1:21:50.060\n like there's an entity, a being, a soul there, right?\n\n1:21:50.060 --> 1:21:52.100\n Do you think there'll be a future,\n\n1:21:53.420 --> 1:21:55.700\n hopefully you won't laugh too much at this,\n\n1:21:55.700 --> 1:22:00.020\n but where they do ask for rights?\n\n1:22:00.020 --> 1:22:03.980\n So I can see a future\n\n1:22:03.980 --> 1:22:08.500\n if we don't address it in the near term\n\n1:22:08.500 --> 1:22:11.820\n where these agents, as they adapt and learn,\n\n1:22:11.820 --> 1:22:15.820\n could say, hey, this should be something that's fundamental.\n\n1:22:15.820 --> 1:22:18.860\n I hopefully think that we would address it\n\n1:22:18.860 --> 1:22:20.580\n before it gets to that point.\n\n1:22:20.580 --> 1:22:22.140\n So you think that's a bad future?\n\n1:22:22.140 --> 1:22:25.340\n Is that a negative thing where they ask\n\n1:22:25.340 --> 1:22:27.740\n we're being discriminated against?\n\n1:22:27.740 --> 1:22:31.100\n I guess it depends on what role\n\n1:22:31.100 --> 1:22:34.340\n have they attained at that point, right?\n\n1:22:34.340 --> 1:22:35.820\n And so if I think about now.\n\n1:22:35.820 --> 1:22:39.220\n Careful what you say because the robots 50 years from now\n\n1:22:39.220 --> 1:22:42.180\n I'll be listening to this and you'll be on TV saying,\n\n1:22:42.180 --> 1:22:44.420\n this is what roboticists used to believe.\n\n1:22:44.420 --> 1:22:45.260\n Well, right?\n\n1:22:45.260 --> 1:22:48.700\n And so this is my, and as I said, I have a bias lens\n\n1:22:48.700 --> 1:22:50.780\n and my robot friends will understand that.\n\n1:22:52.780 --> 1:22:55.180\n So if you think about it, and I actually put this\n\n1:22:55.180 --> 1:22:59.660\n in kind of the, as a roboticist,\n\n1:22:59.660 --> 1:23:02.500\n you don't necessarily think of robots as human\n\n1:23:02.500 --> 1:23:05.020\n with human rights, but you could think of them\n\n1:23:05.020 --> 1:23:09.180\n either in the category of property,\n\n1:23:09.180 --> 1:23:14.180\n or you can think of them in the category of animals, right?\n\n1:23:14.340 --> 1:23:18.340\n And so both of those have different types of rights.\n\n1:23:18.340 --> 1:23:22.740\n So animals have their own rights as a living being,\n\n1:23:22.740 --> 1:23:25.060\n but they can't vote, they can't write,\n\n1:23:25.060 --> 1:23:29.700\n they can be euthanized, but as humans,\n\n1:23:29.700 --> 1:23:32.980\n if we abuse them, we go to jail, right?\n\n1:23:32.980 --> 1:23:35.980\n So they do have some rights that protect them,\n\n1:23:35.980 --> 1:23:39.180\n but don't give them the rights of like citizenship.\n\n1:23:40.140 --> 1:23:42.260\n And then if you think about property,\n\n1:23:42.260 --> 1:23:45.700\n property, the rights are associated with the person, right?\n\n1:23:45.700 --> 1:23:49.500\n So if someone vandalizes your property\n\n1:23:49.500 --> 1:23:53.820\n or steals your property, like there are some rights,\n\n1:23:53.820 --> 1:23:57.700\n but it's associated with the person who owns that.\n\n1:23:58.660 --> 1:24:01.500\n If you think about it back in the day,\n\n1:24:01.500 --> 1:24:03.380\n and if you remember, we talked about\n\n1:24:03.380 --> 1:24:08.180\n how society has changed, women were property, right?\n\n1:24:08.180 --> 1:24:11.860\n They were not thought of as having rights.\n\n1:24:11.860 --> 1:24:15.740\n They were thought of as property of, like their...\n\n1:24:15.740 --> 1:24:17.620\n Yeah, assaulting a woman meant\n\n1:24:17.620 --> 1:24:20.060\n assaulting the property of somebody else.\n\n1:24:20.060 --> 1:24:22.900\n Exactly, and so what I envision is,\n\n1:24:22.900 --> 1:24:27.820\n is that we will establish some type of norm at some point,\n\n1:24:27.820 --> 1:24:29.580\n but that it might evolve, right?\n\n1:24:29.580 --> 1:24:31.460\n Like if you look at women's rights now,\n\n1:24:31.460 --> 1:24:35.380\n like there are still some countries that don't have,\n\n1:24:35.380 --> 1:24:36.700\n and the rest of the world is like,\n\n1:24:36.700 --> 1:24:39.260\n why that makes no sense, right?\n\n1:24:39.260 --> 1:24:42.100\n And so I do see a world where we do establish\n\n1:24:42.100 --> 1:24:44.140\n some type of grounding.\n\n1:24:44.140 --> 1:24:45.700\n It might be based on property rights,\n\n1:24:45.700 --> 1:24:47.620\n it might be based on animal rights.\n\n1:24:47.620 --> 1:24:50.700\n And if it evolves that way,\n\n1:24:50.700 --> 1:24:54.460\n I think we will have this conversation at that time,\n\n1:24:54.460 --> 1:24:58.500\n because that's the way our society traditionally has evolved.\n\n1:24:58.500 --> 1:25:01.860\n Beautifully put, just out of curiosity,\n\n1:25:01.860 --> 1:25:05.460\n Anki, Jibo, Mayflower Robotics,\n\n1:25:05.460 --> 1:25:08.380\n with their robot Curie, SciFiWorks, WeThink Robotics,\n\n1:25:08.380 --> 1:25:10.580\n were all these amazing robotics companies\n\n1:25:10.580 --> 1:25:14.300\n led, created by incredible roboticists,\n\n1:25:14.300 --> 1:25:19.300\n and they've all went out of business recently.\n\n1:25:19.580 --> 1:25:21.660\n Why do you think they didn't last long?\n\n1:25:21.660 --> 1:25:25.140\n Why is it so hard to run a robotics company,\n\n1:25:25.140 --> 1:25:29.300\n especially one like these, which are fundamentally\n\n1:25:29.300 --> 1:25:34.300\n HRI human robot interaction robots?\n\n1:25:34.380 --> 1:25:35.700\n Or personal robots?\n\n1:25:35.700 --> 1:25:37.100\n Each one has a story,\n\n1:25:37.100 --> 1:25:41.180\n only one of them I don't understand, and that was Anki.\n\n1:25:41.180 --> 1:25:43.340\n That's actually the only one I don't understand.\n\n1:25:43.340 --> 1:25:44.660\n I don't understand it either.\n\n1:25:44.660 --> 1:25:47.020\n No, no, I mean, I look like from the outside,\n\n1:25:47.020 --> 1:25:50.500\n I've looked at their sheets, I've looked at the data that's.\n\n1:25:50.500 --> 1:25:51.740\n Oh, you mean like business wise,\n\n1:25:51.740 --> 1:25:52.900\n you don't understand, I got you.\n\n1:25:52.900 --> 1:25:53.740\n Yeah.\n\n1:25:53.740 --> 1:25:58.740\n Yeah, and like I look at all, I look at that data,\n\n1:25:59.180 --> 1:26:02.660\n and I'm like, they seem to have like product market fit.\n\n1:26:02.660 --> 1:26:05.660\n Like, so that's the only one I don't understand.\n\n1:26:05.660 --> 1:26:08.260\n The rest of it was product market fit.\n\n1:26:08.260 --> 1:26:09.860\n What's product market fit?\n\n1:26:09.860 --> 1:26:11.940\n Just that of, like how do you think about it?\n\n1:26:11.940 --> 1:26:15.620\n Yeah, so although WeThink Robotics was getting there, right?\n\n1:26:15.620 --> 1:26:17.420\n But I think it's just the timing,\n\n1:26:17.420 --> 1:26:20.340\n it just, their clock just timed out.\n\n1:26:20.340 --> 1:26:23.100\n I think if they'd been given a couple more years,\n\n1:26:23.100 --> 1:26:25.060\n they would have been okay.\n\n1:26:25.060 --> 1:26:28.620\n But the other ones were still fairly early\n\n1:26:28.620 --> 1:26:30.100\n by the time they got into the market.\n\n1:26:30.100 --> 1:26:32.740\n And so product market fit is,\n\n1:26:32.740 --> 1:26:37.140\n I have a product that I wanna sell at a certain price.\n\n1:26:37.140 --> 1:26:40.060\n Are there enough people out there, the market,\n\n1:26:40.060 --> 1:26:42.780\n that are willing to buy the product at that market price\n\n1:26:42.780 --> 1:26:47.780\n for me to be a functional viable profit bearing company?\n\n1:26:47.820 --> 1:26:48.940\n Right?\n\n1:26:48.940 --> 1:26:50.420\n So product market fit.\n\n1:26:50.420 --> 1:26:53.300\n If it costs you a thousand dollars\n\n1:26:53.300 --> 1:26:57.340\n and everyone wants it and only is willing to pay a dollar,\n\n1:26:57.340 --> 1:26:59.260\n you have no product market fit.\n\n1:26:59.260 --> 1:27:01.940\n Even if you could sell it for, you know,\n\n1:27:01.940 --> 1:27:03.660\n it's enough for a dollar, cause you can't.\n\n1:27:03.660 --> 1:27:05.380\n So how hard is it for robots?\n\n1:27:05.380 --> 1:27:07.580\n Sort of maybe if you look at iRobot,\n\n1:27:07.580 --> 1:27:10.740\n the company that makes Roombas, vacuum cleaners,\n\n1:27:10.740 --> 1:27:14.100\n can you comment on, did they find the right product,\n\n1:27:14.100 --> 1:27:15.100\n market product fit?\n\n1:27:15.940 --> 1:27:18.540\n Like, are people willing to pay for robots\n\n1:27:18.540 --> 1:27:20.540\n is also another kind of question underlying all this.\n\n1:27:20.540 --> 1:27:23.700\n So if you think about iRobot and their story, right?\n\n1:27:23.700 --> 1:27:28.660\n Like when they first, they had enough of a runway, right?\n\n1:27:28.660 --> 1:27:29.780\n When they first started,\n\n1:27:29.780 --> 1:27:31.340\n they weren't doing vacuum cleaners, right?\n\n1:27:31.340 --> 1:27:36.340\n They were contracts primarily, government contracts,\n\n1:27:36.540 --> 1:27:37.380\n designing robots.\n\n1:27:37.380 --> 1:27:38.220\n Or military robots.\n\n1:27:38.220 --> 1:27:39.380\n Yeah, I mean, that's what they were.\n\n1:27:39.380 --> 1:27:40.820\n That's how they started, right?\n\n1:27:40.820 --> 1:27:41.660\n And then.\n\n1:27:41.660 --> 1:27:42.740\n They still do a lot of incredible work there.\n\n1:27:42.740 --> 1:27:44.660\n But yeah, that was the initial thing\n\n1:27:44.660 --> 1:27:46.620\n that gave them enough funding to.\n\n1:27:46.620 --> 1:27:50.740\n To then try to, the vacuum cleaner is what I've been told\n\n1:27:50.740 --> 1:27:53.900\n was not like their first rendezvous\n\n1:27:53.900 --> 1:27:56.500\n in terms of designing a product, right?\n\n1:27:56.500 --> 1:27:59.300\n And so they were able to survive\n\n1:27:59.300 --> 1:28:00.620\n until they got to the point\n\n1:28:00.620 --> 1:28:05.540\n that they found a product price market, right?\n\n1:28:05.540 --> 1:28:09.100\n And even with, if you look at the Roomba,\n\n1:28:09.100 --> 1:28:10.540\n the price point now is different\n\n1:28:10.540 --> 1:28:12.260\n than when it was first released, right?\n\n1:28:12.260 --> 1:28:13.460\n It was an early adopter price,\n\n1:28:13.460 --> 1:28:14.540\n but they found enough people\n\n1:28:14.540 --> 1:28:16.700\n who were willing to fund it.\n\n1:28:16.700 --> 1:28:20.340\n And I mean, I forgot what their loss profile was\n\n1:28:20.340 --> 1:28:22.180\n for the first couple of years,\n\n1:28:22.180 --> 1:28:25.860\n but they became profitable in sufficient time\n\n1:28:25.860 --> 1:28:28.140\n that they didn't have to close their doors.\n\n1:28:28.140 --> 1:28:29.140\n So they found the right,\n\n1:28:29.140 --> 1:28:31.860\n there's still people willing to pay\n\n1:28:31.860 --> 1:28:32.700\n a large amount of money,\n\n1:28:32.700 --> 1:28:35.940\n so over $1,000 for a vacuum cleaner.\n\n1:28:35.940 --> 1:28:37.780\n Unfortunately for them,\n\n1:28:37.780 --> 1:28:39.180\n now that they've proved everything out,\n\n1:28:39.180 --> 1:28:40.020\n figured it all out,\n\n1:28:40.020 --> 1:28:40.860\n now there's competitors.\n\n1:28:40.860 --> 1:28:43.500\n Yeah, and so that's the next thing, right?\n\n1:28:43.500 --> 1:28:44.660\n The competition,\n\n1:28:44.660 --> 1:28:47.500\n and they have quite a number, even internationally.\n\n1:28:47.500 --> 1:28:50.180\n Like there's some products out there,\n\n1:28:50.180 --> 1:28:52.420\n you can go to Europe and be like,\n\n1:28:52.420 --> 1:28:55.020\n oh, I didn't even know this one existed.\n\n1:28:55.020 --> 1:28:56.780\n So this is the thing though,\n\n1:28:56.780 --> 1:28:58.340\n like with any market,\n\n1:28:59.300 --> 1:29:03.580\n I would, this is not a bad time,\n\n1:29:03.580 --> 1:29:06.500\n although as a roboticist, it's kind of depressing,\n\n1:29:06.500 --> 1:29:11.340\n but I actually think about things like with,\n\n1:29:11.340 --> 1:29:13.060\n I would say that all of the companies\n\n1:29:13.060 --> 1:29:15.780\n that are now in the top five or six,\n\n1:29:15.780 --> 1:29:19.620\n they weren't the first to the stage, right?\n\n1:29:19.620 --> 1:29:22.780\n Like Google was not the first search engine,\n\n1:29:22.780 --> 1:29:24.780\n sorry, Altavista, right?\n\n1:29:24.780 --> 1:29:28.340\n Facebook was not the first, sorry, MySpace, right?\n\n1:29:28.340 --> 1:29:29.180\n Like think about it,\n\n1:29:29.180 --> 1:29:31.100\n they were not the first players.\n\n1:29:31.100 --> 1:29:32.980\n Those first players,\n\n1:29:32.980 --> 1:29:37.980\n like they're not in the top five, 10 of Fortune 500 companies,\n\n1:29:38.580 --> 1:29:39.420\n right?\n\n1:29:39.420 --> 1:29:43.940\n They proved, they started to prove out the market,\n\n1:29:43.940 --> 1:29:46.340\n they started to get people interested,\n\n1:29:46.340 --> 1:29:48.300\n they started the buzz,\n\n1:29:48.300 --> 1:29:50.060\n but they didn't make it to that next level.\n\n1:29:50.060 --> 1:29:52.300\n But the second batch, right?\n\n1:29:52.300 --> 1:29:57.300\n The second batch, I think might make it to the next level.\n\n1:29:57.540 --> 1:30:02.380\n When do you think the Facebook of robotics?\n\n1:30:02.380 --> 1:30:03.660\n The Facebook of robotics.\n\n1:30:04.740 --> 1:30:08.500\n Sorry, I take that phrase back because people deeply,\n\n1:30:08.500 --> 1:30:10.340\n for some reason, well, I know why,\n\n1:30:10.340 --> 1:30:13.700\n but it's, I think, exaggerated distrust Facebook\n\n1:30:13.700 --> 1:30:15.500\n because of the privacy concerns and so on.\n\n1:30:15.500 --> 1:30:18.420\n And with robotics, one of the things you have to make sure\n\n1:30:18.420 --> 1:30:21.340\n is all the things we talked about is to be transparent\n\n1:30:21.340 --> 1:30:22.980\n and have people deeply trust you\n\n1:30:22.980 --> 1:30:25.780\n to let a robot into their lives, into their home.\n\n1:30:25.780 --> 1:30:28.620\n When do you think the second batch of robots will come?\n\n1:30:28.620 --> 1:30:32.140\n Is it five, 10 years, 20 years\n\n1:30:32.140 --> 1:30:34.700\n that we'll have robots in our homes\n\n1:30:34.700 --> 1:30:36.540\n and robots in our hearts?\n\n1:30:36.540 --> 1:30:38.900\n So if I think about, and because I try to follow\n\n1:30:38.900 --> 1:30:43.180\n the VC kind of space in terms of robotic investments,\n\n1:30:43.180 --> 1:30:44.900\n and right now, and I don't know\n\n1:30:44.900 --> 1:30:45.900\n if they're gonna be successful,\n\n1:30:45.900 --> 1:30:47.900\n I don't know if this is the second batch,\n\n1:30:49.220 --> 1:30:50.980\n but there's only one batch that's focused\n\n1:30:50.980 --> 1:30:52.900\n on like the first batch, right?\n\n1:30:52.900 --> 1:30:56.260\n And then there's all these self driving Xs, right?\n\n1:30:56.260 --> 1:30:59.540\n And so I don't know if they're a first batch of something\n\n1:30:59.540 --> 1:31:03.060\n or if like, I don't know quite where they fit in,\n\n1:31:03.060 --> 1:31:05.540\n but there's a number of companies,\n\n1:31:05.540 --> 1:31:08.500\n the co robot, I call them co robots\n\n1:31:08.500 --> 1:31:11.340\n that are still getting VC investments.\n\n1:31:13.060 --> 1:31:14.500\n Some of them have some of the flavor\n\n1:31:14.500 --> 1:31:15.740\n of like Rethink Robotics.\n\n1:31:15.740 --> 1:31:18.980\n Some of them have some of the flavor of like Curie.\n\n1:31:18.980 --> 1:31:20.740\n What's a co robot?\n\n1:31:20.740 --> 1:31:25.740\n So basically a robot and human working in the same space.\n\n1:31:26.060 --> 1:31:30.500\n So some of the companies are focused on manufacturing.\n\n1:31:30.500 --> 1:31:34.220\n So having a robot and human working together\n\n1:31:34.220 --> 1:31:37.580\n in a factory, some of these co robots\n\n1:31:37.580 --> 1:31:41.220\n are robots and humans working in the home,\n\n1:31:41.220 --> 1:31:43.180\n working in clinics, like there's different versions\n\n1:31:43.180 --> 1:31:45.380\n of these companies in terms of their products,\n\n1:31:45.380 --> 1:31:48.660\n but they're all, so we think robotics would be\n\n1:31:48.660 --> 1:31:52.660\n like one of the first, at least well known companies\n\n1:31:52.660 --> 1:31:54.580\n focused on this space.\n\n1:31:54.580 --> 1:31:56.700\n So I don't know if this is a second batch\n\n1:31:56.700 --> 1:32:00.940\n or if this is still part of the first batch,\n\n1:32:00.940 --> 1:32:01.980\n that I don't know.\n\n1:32:01.980 --> 1:32:03.740\n And then you have all these other companies\n\n1:32:03.740 --> 1:32:06.860\n in this self driving space.\n\n1:32:06.860 --> 1:32:09.380\n And I don't know if that's a first batch\n\n1:32:09.380 --> 1:32:11.140\n or again, a second batch.\n\n1:32:11.140 --> 1:32:11.980\n Yeah.\n\n1:32:11.980 --> 1:32:13.860\n So there's a lot of mystery about this now.\n\n1:32:13.860 --> 1:32:16.380\n Of course, it's hard to say that this is the second batch\n\n1:32:16.380 --> 1:32:18.460\n until it proves out, right?\n\n1:32:18.460 --> 1:32:19.300\n Correct.\n\n1:32:19.300 --> 1:32:20.540\n Yeah, we need a unicorn.\n\n1:32:20.540 --> 1:32:21.660\n Yeah, exactly.\n\n1:32:23.460 --> 1:32:26.740\n Why do you think people are so afraid,\n\n1:32:27.700 --> 1:32:30.460\n at least in popular culture of legged robots\n\n1:32:30.460 --> 1:32:32.340\n like those worked in Boston Dynamics\n\n1:32:32.340 --> 1:32:34.140\n or just robotics in general,\n\n1:32:34.140 --> 1:32:36.300\n if you were to psychoanalyze that fear,\n\n1:32:36.300 --> 1:32:37.980\n what do you make of it?\n\n1:32:37.980 --> 1:32:39.780\n And should they be afraid, sorry?\n\n1:32:39.780 --> 1:32:41.420\n So should people be afraid?\n\n1:32:41.420 --> 1:32:43.860\n I don't think people should be afraid.\n\n1:32:43.860 --> 1:32:47.060\n But with a caveat, I don't think people should be afraid\n\n1:32:47.060 --> 1:32:51.460\n given that most of us in this world\n\n1:32:51.460 --> 1:32:55.660\n understand that we need to change something, right?\n\n1:32:55.660 --> 1:32:58.100\n So given that.\n\n1:32:58.100 --> 1:33:01.500\n Now, if things don't change, be very afraid.\n\n1:33:01.500 --> 1:33:04.380\n Which is the dimension of change that's needed?\n\n1:33:04.380 --> 1:33:07.740\n So changing, thinking about the ramifications,\n\n1:33:07.740 --> 1:33:09.420\n thinking about like the ethics,\n\n1:33:09.420 --> 1:33:12.740\n thinking about like the conversation is going on, right?\n\n1:33:12.740 --> 1:33:15.860\n It's no longer a we're gonna deploy it\n\n1:33:15.860 --> 1:33:20.300\n and forget that this is a car that can kill pedestrians\n\n1:33:20.300 --> 1:33:22.500\n that are walking across the street, right?\n\n1:33:22.500 --> 1:33:23.340\n We're not in that stage.\n\n1:33:23.340 --> 1:33:25.820\n We're putting these roads out.\n\n1:33:25.820 --> 1:33:27.500\n There are people out there.\n\n1:33:27.500 --> 1:33:28.700\n A car could be a weapon.\n\n1:33:28.700 --> 1:33:33.140\n Like people are now, solutions aren't there yet,\n\n1:33:33.140 --> 1:33:35.300\n but people are thinking about this\n\n1:33:35.300 --> 1:33:38.460\n as we need to be ethically responsible\n\n1:33:38.460 --> 1:33:40.820\n as we send these systems out,\n\n1:33:40.820 --> 1:33:43.060\n robotics, medical, self driving.\n\n1:33:43.060 --> 1:33:43.940\n And military too.\n\n1:33:43.940 --> 1:33:45.260\n And military.\n\n1:33:45.260 --> 1:33:46.980\n Which is not as often talked about,\n\n1:33:46.980 --> 1:33:50.260\n but it's really where probably these robots\n\n1:33:50.260 --> 1:33:51.900\n will have a significant impact as well.\n\n1:33:51.900 --> 1:33:52.820\n Correct, correct.\n\n1:33:52.820 --> 1:33:57.340\n Right, making sure that they can think rationally,\n\n1:33:57.340 --> 1:33:58.700\n even having the conversations,\n\n1:33:58.700 --> 1:34:01.260\n who should pull the trigger, right?\n\n1:34:01.260 --> 1:34:03.380\n But overall you're saying if we start to think\n\n1:34:03.380 --> 1:34:05.740\n more and more as a community about these ethical issues,\n\n1:34:05.740 --> 1:34:06.980\n people should not be afraid.\n\n1:34:06.980 --> 1:34:08.660\n Yeah, I don't think people should be afraid.\n\n1:34:08.660 --> 1:34:10.540\n I think that the return on investment,\n\n1:34:10.540 --> 1:34:14.060\n the impact, positive impact will outweigh\n\n1:34:14.060 --> 1:34:17.500\n any of the potentially negative impacts.\n\n1:34:17.500 --> 1:34:20.540\n Do you have worries of existential threats\n\n1:34:20.540 --> 1:34:25.540\n of robots or AI that some people kind of talk about\n\n1:34:25.540 --> 1:34:28.620\n and romanticize about in the next decade,\n\n1:34:28.620 --> 1:34:29.980\n the next few decades?\n\n1:34:29.980 --> 1:34:31.340\n No, I don't.\n\n1:34:31.340 --> 1:34:33.700\n Singularity would be an example.\n\n1:34:33.700 --> 1:34:36.380\n So my concept is that, so remember,\n\n1:34:36.380 --> 1:34:39.580\n robots, AI, is designed by people.\n\n1:34:39.580 --> 1:34:41.260\n It has our values.\n\n1:34:41.260 --> 1:34:45.060\n And I always correlate this with a parent and a child.\n\n1:34:45.060 --> 1:34:47.100\n So think about it, as a parent, what do we want?\n\n1:34:47.100 --> 1:34:49.860\n We want our kids to have a better life than us.\n\n1:34:49.860 --> 1:34:52.300\n We want them to expand.\n\n1:34:52.300 --> 1:34:55.780\n We want them to experience the world.\n\n1:34:55.780 --> 1:34:59.740\n And then as we grow older, our kids think and know\n\n1:34:59.740 --> 1:35:03.020\n they're smarter and better and more intelligent\n\n1:35:03.020 --> 1:35:04.780\n and have better opportunities.\n\n1:35:04.780 --> 1:35:08.220\n And they may even stop listening to us.\n\n1:35:08.220 --> 1:35:10.500\n They don't go out and then kill us, right?\n\n1:35:10.500 --> 1:35:11.340\n Like, think about it.\n\n1:35:11.340 --> 1:35:14.180\n It's because we, it's instilled in them values.\n\n1:35:14.180 --> 1:35:17.420\n We instilled in them this whole aspect of community.\n\n1:35:17.420 --> 1:35:19.780\n And yes, even though you're maybe smarter\n\n1:35:19.780 --> 1:35:22.460\n and have more money and dah, dah, dah,\n\n1:35:22.460 --> 1:35:26.780\n it's still about this love, caring relationship.\n\n1:35:26.780 --> 1:35:27.740\n And so that's what I believe.\n\n1:35:27.740 --> 1:35:29.020\n So even if like, you know,\n\n1:35:29.020 --> 1:35:32.140\n we've created the singularity in some archaic system\n\n1:35:32.140 --> 1:35:35.340\n back in like 1980 that suddenly evolves,\n\n1:35:35.340 --> 1:35:40.180\n the fact is it might say, I am smarter, I am sentient.\n\n1:35:40.180 --> 1:35:43.220\n These humans are really stupid,\n\n1:35:43.220 --> 1:35:46.060\n but I think it'll be like, yeah,\n\n1:35:46.060 --> 1:35:47.620\n but I just can't destroy them.\n\n1:35:47.620 --> 1:35:49.660\n Yeah, for sentimental value.\n\n1:35:49.660 --> 1:35:53.140\n It's still just to come back for Thanksgiving dinner\n\n1:35:53.140 --> 1:35:53.980\n every once in a while.\n\n1:35:53.980 --> 1:35:54.820\n Exactly.\n\n1:35:54.820 --> 1:35:57.460\n That's such, that's so beautifully put.\n\n1:35:57.460 --> 1:36:00.580\n You've also said that The Matrix may be\n\n1:36:00.580 --> 1:36:03.660\n one of your more favorite AI related movies.\n\n1:36:03.660 --> 1:36:05.580\n Can you elaborate why?\n\n1:36:05.580 --> 1:36:07.860\n Yeah, it is one of my favorite movies.\n\n1:36:07.860 --> 1:36:11.180\n And it's because it represents\n\n1:36:11.180 --> 1:36:14.060\n kind of all the things I think about.\n\n1:36:14.060 --> 1:36:16.100\n So there's a symbiotic relationship\n\n1:36:16.100 --> 1:36:20.140\n between robots and humans, right?\n\n1:36:20.140 --> 1:36:22.500\n That symbiotic relationship is that they don't destroy us,\n\n1:36:22.500 --> 1:36:24.620\n they enslave us, right?\n\n1:36:24.620 --> 1:36:25.580\n But think about it,\n\n1:36:28.060 --> 1:36:30.260\n even though they enslaved us,\n\n1:36:30.260 --> 1:36:32.820\n they needed us to be happy, right?\n\n1:36:32.820 --> 1:36:33.860\n And in order to be happy,\n\n1:36:33.860 --> 1:36:35.420\n they had to create this cruddy world\n\n1:36:35.420 --> 1:36:36.980\n that they then had to live in, right?\n\n1:36:36.980 --> 1:36:39.460\n That's the whole premise.\n\n1:36:39.460 --> 1:36:44.380\n But then there were humans that had a choice, right?\n\n1:36:44.380 --> 1:36:47.660\n Like you had a choice to stay in this horrific,\n\n1:36:47.660 --> 1:36:51.220\n horrific world where it was your fantasy life\n\n1:36:51.220 --> 1:36:54.740\n with all of the anomalies, perfection, but not accurate.\n\n1:36:54.740 --> 1:36:57.940\n Or you can choose to be on your own\n\n1:36:57.940 --> 1:37:02.500\n and like have maybe no food for a couple of days,\n\n1:37:02.500 --> 1:37:05.180\n but you were totally autonomous.\n\n1:37:05.180 --> 1:37:07.980\n And so I think of that as, and that's why.\n\n1:37:07.980 --> 1:37:09.700\n So it's not necessarily us being enslaved,\n\n1:37:09.700 --> 1:37:13.060\n but I think about us having the symbiotic relationship.\n\n1:37:13.060 --> 1:37:15.780\n Robots and AI, even if they become sentient,\n\n1:37:15.780 --> 1:37:17.100\n they're still part of our society\n\n1:37:17.100 --> 1:37:20.700\n and they will suffer just as much as we.\n\n1:37:20.700 --> 1:37:23.820\n And there will be some kind of equilibrium\n\n1:37:23.820 --> 1:37:26.700\n that we'll have to find some symbiotic relationship.\n\n1:37:26.700 --> 1:37:28.220\n Right, and then you have the ethicists,\n\n1:37:28.220 --> 1:37:30.180\n the robotics folks that are like,\n\n1:37:30.180 --> 1:37:34.500\n no, this has got to stop, I will take the other pill\n\n1:37:34.500 --> 1:37:36.300\n in order to make a difference.\n\n1:37:36.300 --> 1:37:40.380\n So if you could hang out for a day with a robot,\n\n1:37:40.380 --> 1:37:44.740\n real or from science fiction, movies, books, safely,\n\n1:37:44.740 --> 1:37:48.780\n and get to pick his or her, their brain,\n\n1:37:48.780 --> 1:37:49.740\n who would you pick?\n\n1:37:55.980 --> 1:37:57.620\n Gotta say it's Data.\n\n1:37:57.620 --> 1:37:58.740\n Data.\n\n1:37:58.740 --> 1:38:00.460\n I was gonna say Rosie,\n\n1:38:00.460 --> 1:38:03.660\n but I'm not really interested in her brain.\n\n1:38:03.660 --> 1:38:05.820\n I'm interested in Data's brain.\n\n1:38:05.820 --> 1:38:08.460\n Data pre or post emotion chip?\n\n1:38:08.460 --> 1:38:10.460\n Pre.\n\n1:38:10.460 --> 1:38:15.100\n But don't you think it'd be a more interesting conversation\n\n1:38:15.100 --> 1:38:16.180\n post emotion chip?\n\n1:38:16.180 --> 1:38:17.740\n Yeah, it would be drama.\n\n1:38:17.740 --> 1:38:22.740\n And I'm human, I deal with drama all the time.\n\n1:38:22.860 --> 1:38:24.860\n But the reason why I wanna pick Data's brain\n\n1:38:24.860 --> 1:38:29.540\n is because I could have a conversation with him\n\n1:38:29.540 --> 1:38:34.540\n and ask, for example, how can we fix this ethics problem?\n\n1:38:34.540 --> 1:38:38.300\n And he could go through like the rational thinking\n\n1:38:38.300 --> 1:38:40.780\n and through that, he could also help me\n\n1:38:40.780 --> 1:38:42.220\n think through it as well.\n\n1:38:42.220 --> 1:38:44.860\n And so there's like these fundamental questions\n\n1:38:44.860 --> 1:38:46.420\n I think I could ask him\n\n1:38:46.420 --> 1:38:49.980\n that he would help me also learn from.\n\n1:38:49.980 --> 1:38:51.660\n And that fascinates me.\n\n1:38:52.860 --> 1:38:55.140\n I don't think there's a better place to end it.\n\n1:38:55.140 --> 1:38:57.300\n Ayana, thank you so much for talking to us, it was an honor.\n\n1:38:57.300 --> 1:38:58.140\n Thank you, thank you.\n\n1:38:58.140 --> 1:38:58.980\n This was fun.\n\n1:39:00.300 --> 1:39:02.420\n Thanks for listening to this conversation\n\n1:39:02.420 --> 1:39:05.900\n and thank you to our presenting sponsor, Cash App.\n\n1:39:05.900 --> 1:39:08.540\n Download it, use code LexPodcast,\n\n1:39:08.540 --> 1:39:11.340\n you'll get $10 and $10 will go to FIRST,\n\n1:39:11.340 --> 1:39:13.620\n a STEM education nonprofit that inspires\n\n1:39:13.620 --> 1:39:15.820\n hundreds of thousands of young minds\n\n1:39:15.820 --> 1:39:18.740\n to become future leaders and innovators.\n\n1:39:18.740 --> 1:39:21.540\n If you enjoy this podcast, subscribe on YouTube,\n\n1:39:21.540 --> 1:39:23.540\n give it five stars on Apple Podcast,\n\n1:39:23.540 --> 1:39:26.220\n follow on Spotify, support on Patreon\n\n1:39:26.220 --> 1:39:28.260\n or simply connect with me on Twitter.\n\n1:39:29.300 --> 1:39:31.860\n And now let me leave you with some words of wisdom\n\n1:39:31.860 --> 1:39:33.980\n from Arthur C. Clarke.\n\n1:39:35.180 --> 1:39:38.580\n Whether we are based on carbon or on silicon\n\n1:39:38.580 --> 1:39:40.620\n makes no fundamental difference.\n\n1:39:40.620 --> 1:39:43.660\n We should each be treated with appropriate respect.\n\n1:39:43.660 --> 1:40:02.660\n Thank you for listening and hope to see you next time.\n\n"
}