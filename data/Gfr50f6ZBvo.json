{
  "title": "Demis Hassabis: DeepMind - AI, Superintelligence & the Future of Humanity | Lex Fridman Podcast #299",
  "id": "Gfr50f6ZBvo",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:03.480\n The following is a conversation with Demis Hassabis,\n\n00:03.480 --> 00:06.720\n CEO and co founder of DeepMind,\n\n00:06.720 --> 00:08.600\n a company that has published and built\n\n00:08.600 --> 00:12.200\n some of the most incredible artificial intelligence systems\n\n00:12.200 --> 00:14.120\n in the history of computing,\n\n00:14.120 --> 00:18.040\n including AlphaZero that learned all by itself\n\n00:18.040 --> 00:21.000\n to play the game of go better than any human in the world\n\n00:21.000 --> 00:25.760\n and AlphaFold2 that solved protein folding.\n\n00:25.760 --> 00:28.800\n Both tasks considered nearly impossible\n\n00:28.800 --> 00:30.360\n for a very long time.\n\n00:31.240 --> 00:33.040\n Demis is widely considered to be\n\n00:33.040 --> 00:35.720\n one of the most brilliant and impactful humans\n\n00:35.720 --> 00:38.160\n in the history of artificial intelligence\n\n00:38.160 --> 00:41.280\n and science and engineering in general.\n\n00:41.280 --> 00:44.640\n This was truly an honor and a pleasure for me\n\n00:44.640 --> 00:47.320\n to finally sit down with him for this conversation.\n\n00:47.320 --> 00:50.580\n And I'm sure we will talk many times again in the future.\n\n00:51.520 --> 00:53.320\n This is the Lux Readman podcast.\n\n00:53.320 --> 00:55.500\n To support it, please check out our sponsors\n\n00:55.500 --> 00:56.780\n in the description.\n\n00:56.780 --> 01:00.560\n And now, dear friends, here's Demis Hassabis.\n\n01:01.580 --> 01:04.040\n Let's start with a bit of a personal question.\n\n01:04.040 --> 01:07.840\n Am I an AI program you wrote to interview people\n\n01:07.840 --> 01:10.100\n until I get good enough to interview you?\n\n01:11.100 --> 01:13.120\n Well, I'd be impressed if you were.\n\n01:13.120 --> 01:14.880\n I'd be impressed by myself if you were.\n\n01:14.880 --> 01:16.520\n I don't think we're quite up to that yet,\n\n01:16.520 --> 01:18.800\n but maybe you're from the future, Lex.\n\n01:18.800 --> 01:20.400\n If you did, would you tell me?\n\n01:20.400 --> 01:23.080\n Is that a good thing to tell a language model\n\n01:23.080 --> 01:25.080\n that's tasked with interviewing\n\n01:25.080 --> 01:27.440\n that it is, in fact, AI?\n\n01:27.440 --> 01:29.680\n Maybe we're in a kind of meta Turing test.\n\n01:29.680 --> 01:32.440\n Probably it would be a good idea not to tell you,\n\n01:32.440 --> 01:33.920\n so it doesn't change your behavior, right?\n\n01:33.920 --> 01:35.080\n This is a kind of link.\n\n01:35.080 --> 01:37.100\n Heisenberg uncertainty principle situation.\n\n01:37.100 --> 01:39.080\n If I told you, you'd behave differently.\n\n01:39.080 --> 01:40.980\n Maybe that's what's happening with us, of course.\n\n01:40.980 --> 01:42.800\n This is a benchmark from the future\n\n01:42.800 --> 01:46.560\n where they replay 2022 as a year\n\n01:46.560 --> 01:49.440\n before AIs were good enough yet,\n\n01:49.440 --> 01:52.080\n and now we want to see, is it gonna pass?\n\n01:52.080 --> 01:52.920\n Exactly.\n\n01:52.920 --> 01:56.000\n If I was such a program,\n\n01:56.000 --> 01:57.960\n would you be able to tell, do you think?\n\n01:57.960 --> 01:59.960\n So to the Turing test question,\n\n01:59.960 --> 02:04.960\n you've talked about the benchmark for solving intelligence.\n\n02:05.840 --> 02:07.320\n What would be the impressive thing?\n\n02:07.320 --> 02:09.120\n You've talked about winning a Nobel Prize\n\n02:09.120 --> 02:11.440\n and AIS system winning a Nobel Prize,\n\n02:11.440 --> 02:14.400\n but I still return to the Turing test as a compelling test,\n\n02:14.400 --> 02:17.280\n the spirit of the Turing test as a compelling test.\n\n02:17.280 --> 02:18.520\n Yeah, the Turing test, of course,\n\n02:18.520 --> 02:20.200\n it's been unbelievably influential,\n\n02:20.200 --> 02:22.120\n and Turing's one of my all time heroes,\n\n02:22.120 --> 02:24.920\n but I think if you look back at the 1950 paper,\n\n02:24.920 --> 02:27.000\n his original paper and read the original,\n\n02:27.000 --> 02:28.840\n you'll see, I don't think he meant it\n\n02:28.840 --> 02:30.880\n to be a rigorous formal test.\n\n02:30.880 --> 02:32.880\n I think it was more like a thought experiment,\n\n02:32.880 --> 02:34.640\n almost a bit of philosophy he was writing\n\n02:34.640 --> 02:36.440\n if you look at the style of the paper,\n\n02:36.440 --> 02:38.640\n and you can see he didn't specify it very rigorously.\n\n02:38.640 --> 02:41.840\n So for example, he didn't specify the knowledge\n\n02:41.840 --> 02:44.040\n that the expert or judge would have.\n\n02:45.440 --> 02:48.320\n How much time would they have to investigate this?\n\n02:48.320 --> 02:49.500\n So these are important parameters\n\n02:49.500 --> 02:53.220\n if you were gonna make it a true sort of formal test.\n\n02:54.320 --> 02:58.400\n And by some measures, people claim the Turing test passed\n\n02:58.400 --> 03:00.920\n several, a decade ago, I remember someone claiming that\n\n03:00.920 --> 03:05.920\n with a kind of very bog standard, normal logic model,\n\n03:06.000 --> 03:08.440\n because they pretended it was a kid.\n\n03:08.440 --> 03:13.280\n So the judges thought that the machine was a child.\n\n03:13.280 --> 03:15.360\n So that would be very different\n\n03:15.360 --> 03:18.680\n from an expert AI person interrogating a machine\n\n03:18.680 --> 03:20.720\n and knowing how it was built and so on.\n\n03:20.720 --> 03:24.600\n So I think we should probably move away from that\n\n03:24.600 --> 03:28.800\n as a formal test and move more towards a general test\n\n03:28.800 --> 03:32.040\n where we test the AI capabilities on a range of tasks\n\n03:32.040 --> 03:35.360\n and see if it reaches human level or above performance\n\n03:35.360 --> 03:38.400\n on maybe thousands, perhaps even millions of tasks\n\n03:38.400 --> 03:41.940\n eventually and cover the entire sort of cognitive space.\n\n03:41.940 --> 03:44.140\n So I think for its time,\n\n03:44.140 --> 03:45.520\n it was an amazing thought experiment.\n\n03:45.520 --> 03:48.240\n And also 1950s, obviously there's barely the dawn\n\n03:48.240 --> 03:49.440\n of the computer age.\n\n03:49.440 --> 03:51.480\n So of course he only thought about text\n\n03:51.480 --> 03:54.580\n and now we have a lot more different inputs.\n\n03:54.580 --> 03:57.080\n So yeah, maybe the better thing to test\n\n03:57.080 --> 03:59.660\n is the generalizability, so across multiple tasks.\n\n03:59.660 --> 04:04.540\n But I think it's also possible as systems like Gato show\n\n04:04.540 --> 04:08.320\n that eventually that might map right back to language.\n\n04:08.320 --> 04:10.800\n So you might be able to demonstrate your ability\n\n04:10.800 --> 04:14.760\n to generalize across tasks by then communicating\n\n04:14.760 --> 04:17.080\n your ability to generalize across tasks,\n\n04:17.080 --> 04:19.200\n which is kind of what we do through conversation anyway\n\n04:19.200 --> 04:20.800\n when we jump around.\n\n04:20.800 --> 04:23.720\n Ultimately what's in there in that conversation\n\n04:23.720 --> 04:27.000\n is not just you moving around knowledge,\n\n04:27.000 --> 04:30.360\n it's you moving around like these entirely different\n\n04:30.360 --> 04:34.920\n modalities of understanding that ultimately map\n\n04:34.920 --> 04:38.920\n to your ability to operate successfully\n\n04:38.920 --> 04:42.600\n in all of these domains, which you can think of as tasks.\n\n04:42.600 --> 04:45.600\n Yeah, I think certainly we as humans use language\n\n04:45.600 --> 04:48.440\n as our main generalization communication tool.\n\n04:48.440 --> 04:51.280\n So I think we end up thinking in language\n\n04:51.280 --> 04:54.440\n and expressing our solutions in language.\n\n04:54.440 --> 04:58.320\n So it's going to be a very powerful mode in which\n\n04:58.320 --> 05:03.080\n to explain the system, to explain what it's doing.\n\n05:03.080 --> 05:07.600\n But I don't think it's the only modality that matters.\n\n05:07.600 --> 05:10.920\n So I think there's going to be a lot of different ways\n\n05:10.920 --> 05:15.600\n to express capabilities other than just language.\n\n05:15.600 --> 05:19.000\n Yeah, visual, robotics, body language,\n\n05:21.120 --> 05:23.760\n yeah, actions, the interactive aspect of all that.\n\n05:23.760 --> 05:24.760\n That's all part of it.\n\n05:24.760 --> 05:27.600\n But what's interesting with Gato is that\n\n05:27.600 --> 05:30.200\n it's sort of pushing prediction to the maximum\n\n05:30.200 --> 05:33.400\n in terms of like mapping arbitrary sequences\n\n05:33.400 --> 05:35.480\n to other sequences and sort of just predicting\n\n05:35.480 --> 05:36.400\n what's going to happen next.\n\n05:36.400 --> 05:41.040\n So prediction seems to be fundamental to intelligence.\n\n05:41.040 --> 05:44.160\n And what you're predicting doesn't so much matter.\n\n05:44.160 --> 05:46.840\n Yeah, it seems like you can generalize that quite well.\n\n05:46.840 --> 05:49.640\n So obviously language models predict the next word,\n\n05:49.640 --> 05:53.880\n Gato predicts potentially any action or any token.\n\n05:53.880 --> 05:55.320\n And it's just the beginning really.\n\n05:55.320 --> 05:58.120\n It's our most general agent one could call it so far,\n\n05:58.120 --> 06:01.240\n but that itself can be scaled up massively more\n\n06:01.240 --> 06:02.120\n than we've done so far.\n\n06:02.120 --> 06:04.280\n And obviously we're in the middle of doing that.\n\n06:04.280 --> 06:08.240\n But the big part of solving AGI is creating benchmarks\n\n06:08.240 --> 06:11.080\n that help us get closer and closer,\n\n06:11.080 --> 06:14.920\n sort of creating benchmarks that test the generalizability.\n\n06:14.920 --> 06:17.440\n And it's just still interesting that this fella,\n\n06:17.440 --> 06:20.520\n Alan Turing, was one of the first\n\n06:20.520 --> 06:22.600\n and probably still one of the only people\n\n06:22.600 --> 06:25.040\n that was trying, maybe philosophically,\n\n06:25.040 --> 06:26.840\n but was trying to formulate a benchmark\n\n06:26.840 --> 06:27.880\n that could be followed.\n\n06:27.880 --> 06:30.960\n It is, even though it's fuzzy,\n\n06:30.960 --> 06:32.520\n it's still sufficiently rigorous\n\n06:32.520 --> 06:33.960\n to where you can run that test.\n\n06:33.960 --> 06:36.640\n And I still think something like the Turing test\n\n06:36.640 --> 06:38.720\n will, at the end of the day,\n\n06:38.720 --> 06:42.560\n be the thing that truly impresses other humans\n\n06:42.560 --> 06:46.400\n so that you can have a close friend who's an AI system.\n\n06:46.400 --> 06:48.320\n And for that friend to be a good friend,\n\n06:48.320 --> 06:53.160\n they're going to have to be able to play StarCraft\n\n06:53.160 --> 06:56.600\n and they're gonna have to do all of these tasks,\n\n06:56.600 --> 06:59.560\n get you a beer, so the robotics tasks,\n\n07:00.440 --> 07:03.160\n play games with you, use language,\n\n07:03.160 --> 07:04.800\n humor, all of those kinds of things.\n\n07:04.800 --> 07:08.000\n But that ultimately can boil down to language.\n\n07:08.000 --> 07:11.200\n It feels like, not in terms of the AI community,\n\n07:11.200 --> 07:13.120\n but in terms of the actual impact\n\n07:13.120 --> 07:14.760\n of general intelligence on the world,\n\n07:14.760 --> 07:16.640\n it feels like language will be the place\n\n07:16.640 --> 07:18.480\n where it truly shines.\n\n07:18.480 --> 07:20.640\n I think so, because it's such an important\n\n07:20.640 --> 07:22.480\n kind of input output for us.\n\n07:22.480 --> 07:23.320\n I think you're right.\n\n07:23.320 --> 07:24.680\n I think the Turing test,\n\n07:24.680 --> 07:27.440\n the kind of the philosophy behind it,\n\n07:27.440 --> 07:31.120\n which is the idea of can a machine mimic\n\n07:31.120 --> 07:34.960\n the behaviors of a human?\n\n07:34.960 --> 07:38.680\n And I would say wider than just language and text,\n\n07:38.680 --> 07:41.320\n then in terms of actions and everything else,\n\n07:41.320 --> 07:42.680\n creativity, all these things,\n\n07:42.680 --> 07:45.320\n then if it can sort of match or exceed\n\n07:45.320 --> 07:47.320\n human cognitive capabilities,\n\n07:47.320 --> 07:49.880\n then I think we have a true intelligence.\n\n07:49.880 --> 07:51.640\n So from that perspective, you're right.\n\n07:51.640 --> 07:55.960\n I think he did formulate the right kind of setup.\n\n07:55.960 --> 07:59.280\n I just, I think there'll be a kind of humor\n\n07:59.280 --> 08:01.440\n in the AI systems of the future looking back\n\n08:01.440 --> 08:06.040\n to this conversation and thinking about the Turing test\n\n08:06.040 --> 08:09.360\n and also thinking about by that time,\n\n08:09.360 --> 08:11.880\n they would know which year they were finally able\n\n08:11.880 --> 08:15.560\n to sort of cross the threshold of human level intelligence\n\n08:15.560 --> 08:18.800\n and think how funny it is that we humans\n\n08:18.800 --> 08:22.400\n were still confused about this whole problem\n\n08:22.400 --> 08:24.720\n that has been long ago been solved.\n\n08:24.720 --> 08:27.240\n Well, hello to the AI agents of the future.\n\n08:27.240 --> 08:30.320\n Anyway, so going back to your journey,\n\n08:30.320 --> 08:33.800\n when did you fall in love with programming first?\n\n08:33.800 --> 08:35.960\n Well, I was pretty young age actually.\n\n08:35.960 --> 08:40.840\n So, I started off, actually games was my first love.\n\n08:40.840 --> 08:43.680\n So starting to play chess when I was around four years old\n\n08:43.680 --> 08:46.160\n and then it was actually with winnings\n\n08:46.160 --> 08:48.400\n from a chess competition that I managed\n\n08:48.400 --> 08:49.800\n to buy my first chess computer\n\n08:49.800 --> 08:50.840\n when I was about eight years old.\n\n08:50.840 --> 08:53.160\n It was a ZX Spectrum, which was hugely popular\n\n08:53.160 --> 08:54.720\n in the UK at the time.\n\n08:54.720 --> 08:58.520\n And it was amazing machine because I think it trained\n\n08:58.520 --> 09:00.440\n a whole generation of programmers in the UK\n\n09:00.440 --> 09:02.520\n because it was so accessible.\n\n09:02.520 --> 09:03.800\n You know, you literally switched it on\n\n09:03.800 --> 09:05.120\n and there was the basic prompt\n\n09:05.120 --> 09:06.680\n and you could just get going.\n\n09:06.680 --> 09:09.920\n And my parents didn't really know anything about computers.\n\n09:09.920 --> 09:12.600\n So, but because it was my money from a chess competition,\n\n09:12.600 --> 09:15.760\n I could say I wanted to buy it.\n\n09:15.760 --> 09:17.960\n And then, you know, I just went to bookstores,\n\n09:17.960 --> 09:22.040\n got books on programming and started typing in,\n\n09:22.040 --> 09:23.520\n you know, the programming code.\n\n09:23.520 --> 09:26.480\n And then of course, once you start doing that,\n\n09:26.480 --> 09:29.120\n you start adjusting it and then making your own games.\n\n09:29.120 --> 09:30.840\n And that's when I fell in love with computers\n\n09:30.840 --> 09:33.880\n and realized that they were a very magical device.\n\n09:34.840 --> 09:36.440\n In a way, I kind of, I wouldn't have been able\n\n09:36.440 --> 09:37.440\n to explain this at the time,\n\n09:37.440 --> 09:38.960\n but I felt that they were sort of almost\n\n09:38.960 --> 09:40.920\n a magical extension of your mind.\n\n09:40.920 --> 09:43.080\n I always had this feeling and I've always loved this\n\n09:43.080 --> 09:46.160\n about computers that you can set them off doing something,\n\n09:46.160 --> 09:48.520\n some task for you, you can go to sleep,\n\n09:48.520 --> 09:51.240\n come back the next day and it's solved.\n\n09:51.240 --> 09:53.080\n You know, that feels magical to me.\n\n09:53.080 --> 09:55.280\n So, I mean, all machines do that to some extent.\n\n09:55.280 --> 09:57.640\n They all enhance our natural capabilities.\n\n09:57.640 --> 10:00.080\n Obviously cars make us, allow us to move faster\n\n10:00.080 --> 10:04.520\n than we can run, but this was a machine to extend the mind.\n\n10:04.520 --> 10:08.480\n And then of course, AI is the ultimate expression\n\n10:08.480 --> 10:11.360\n of what a machine may be able to do or learn.\n\n10:11.360 --> 10:14.440\n So very naturally for me, that thought extended\n\n10:14.440 --> 10:16.040\n into AI quite quickly.\n\n10:16.040 --> 10:18.560\n Do you remember the programming language\n\n10:18.560 --> 10:22.080\n that was first started and was it special to the machine?\n\n10:22.080 --> 10:25.880\n No, I think it was just basic on the ZX Spectrum.\n\n10:25.880 --> 10:27.480\n I don't know what specific form it was.\n\n10:27.480 --> 10:29.600\n And then later on I got a Commodore Amiga,\n\n10:29.600 --> 10:32.760\n which was a fantastic machine.\n\n10:32.760 --> 10:33.800\n Now you're just showing off.\n\n10:33.800 --> 10:36.520\n So yeah, well, lots of my friends had Atari STs\n\n10:36.520 --> 10:38.880\n and I managed to get Amigas, it was a bit more powerful\n\n10:38.880 --> 10:42.800\n and that was incredible and used to do programming\n\n10:42.800 --> 10:46.240\n in assembler and also Amos basic,\n\n10:46.240 --> 10:49.280\n this specific form of basic, it was incredible actually.\n\n10:49.280 --> 10:51.000\n So I learned all my coding skills.\n\n10:51.000 --> 10:53.040\n And when did you fall in love with AI?\n\n10:53.040 --> 10:56.880\n So when did you first start to gain an understanding\n\n10:56.880 --> 10:58.880\n that you can not just write programs\n\n10:58.880 --> 11:01.640\n that do some mathematical operations for you\n\n11:01.640 --> 11:05.400\n while you sleep, but something that's akin\n\n11:05.400 --> 11:08.800\n to bringing an entity to life,\n\n11:08.800 --> 11:11.840\n sort of a thing that can figure out something\n\n11:11.840 --> 11:15.920\n more complicated than a simple mathematical operation.\n\n11:15.920 --> 11:17.600\n Yeah, so there was a few stages for me\n\n11:17.600 --> 11:18.920\n all while I was very young.\n\n11:18.920 --> 11:21.680\n So first of all, as I was trying to improve\n\n11:21.680 --> 11:23.160\n at playing chess, I was captaining\n\n11:23.160 --> 11:24.680\n various England junior chess teams.\n\n11:24.680 --> 11:27.480\n And at the time when I was about maybe 10, 11 years old,\n\n11:27.480 --> 11:29.360\n I was gonna become a professional chess player.\n\n11:29.360 --> 11:32.000\n That was my first thought.\n\n11:32.000 --> 11:34.680\n So that dream was there to try to get\n\n11:34.680 --> 11:35.520\n to the highest levels of chess.\n\n11:35.520 --> 11:39.160\n Yeah, so when I was about 12 years old,\n\n11:39.160 --> 11:41.320\n I got to master standard and I was second highest rated\n\n11:41.320 --> 11:42.680\n player in the world to Judith Polgar,\n\n11:42.680 --> 11:45.760\n who obviously ended up being an amazing chess player\n\n11:45.760 --> 11:48.560\n and a world women's champion.\n\n11:48.560 --> 11:50.760\n And when I was trying to improve at chess,\n\n11:50.760 --> 11:52.760\n where what you do is you obviously, first of all,\n\n11:52.760 --> 11:55.120\n you're trying to improve your own thinking processes.\n\n11:55.120 --> 11:58.080\n So that leads you to thinking about thinking,\n\n11:58.080 --> 12:00.400\n how is your brain coming up with these ideas?\n\n12:00.400 --> 12:01.880\n Why is it making mistakes?\n\n12:01.880 --> 12:04.520\n How can you improve that thought process?\n\n12:04.520 --> 12:06.360\n But the second thing is that you,\n\n12:06.360 --> 12:09.640\n it was just the beginning, this was like in the early 80s,\n\n12:09.640 --> 12:11.240\n mid 80s of chess computers.\n\n12:11.240 --> 12:12.760\n If you remember, they were physical balls\n\n12:12.760 --> 12:14.000\n like the one we have in front of us.\n\n12:14.000 --> 12:17.000\n And you press down the squares.\n\n12:17.000 --> 12:19.600\n And I think Kasparov had a branded version of it\n\n12:19.600 --> 12:21.000\n that I got.\n\n12:21.000 --> 12:24.640\n And you used to, they're not as strong as they are today,\n\n12:24.640 --> 12:27.320\n but they were pretty strong and you used to practice\n\n12:27.320 --> 12:30.560\n against them to try and improve your openings\n\n12:30.560 --> 12:31.480\n and other things.\n\n12:31.480 --> 12:33.440\n And so I remember, I think I probably got my first one,\n\n12:33.440 --> 12:34.920\n I was around 11 or 12.\n\n12:34.920 --> 12:37.760\n And I remember thinking, this is amazing,\n\n12:37.760 --> 12:42.760\n how has someone programmed this chess board to play chess?\n\n12:42.920 --> 12:45.600\n And it was very formative book I bought,\n\n12:45.600 --> 12:47.760\n which was called The Chess Computer Handbook\n\n12:47.760 --> 12:49.000\n by David Levy.\n\n12:49.000 --> 12:50.680\n This thing came out in 1984 or something.\n\n12:50.680 --> 12:52.360\n So I must've got it when I was about 11, 12.\n\n12:52.360 --> 12:56.120\n And it explained fully how these chess programs were made.\n\n12:56.120 --> 12:57.680\n And I remember my first AI program\n\n12:57.680 --> 13:00.440\n being programming my Amiga.\n\n13:00.440 --> 13:02.920\n It couldn't, it wasn't powerful enough to play chess.\n\n13:02.920 --> 13:04.200\n I couldn't write a whole chess program,\n\n13:04.200 --> 13:07.480\n but I wrote a program for it to play Othello or reverse it,\n\n13:07.480 --> 13:09.320\n sometimes called I think in the US.\n\n13:09.320 --> 13:11.760\n And so a slightly simpler game than chess,\n\n13:11.760 --> 13:14.360\n but I used all of the principles that chess programs had,\n\n13:14.360 --> 13:16.000\n alpha, beta, search, all of that.\n\n13:16.000 --> 13:17.440\n And that was my first AI program.\n\n13:17.440 --> 13:19.440\n I remember that very well, I was around 12 years old.\n\n13:19.440 --> 13:21.680\n So that brought me into AI.\n\n13:21.680 --> 13:24.120\n And then the second part was later on,\n\n13:24.120 --> 13:25.560\n when I was around 16, 17,\n\n13:25.560 --> 13:28.840\n and I was writing games professionally, designing games,\n\n13:28.840 --> 13:30.640\n writing a game called Theme Park,\n\n13:30.640 --> 13:34.040\n which had AI as a core gameplay component\n\n13:34.040 --> 13:35.680\n as part of the simulation.\n\n13:35.680 --> 13:38.480\n And it sold millions of copies around the world.\n\n13:38.480 --> 13:41.000\n And people loved the way that the AI,\n\n13:41.000 --> 13:42.280\n even though it was relatively simple\n\n13:42.280 --> 13:44.440\n by today's AI standards,\n\n13:44.440 --> 13:47.720\n was reacting to the way you as the player played it.\n\n13:47.720 --> 13:49.240\n So it was called a sandbox game.\n\n13:49.240 --> 13:51.320\n So it was one of the first types of games like that,\n\n13:51.320 --> 13:52.680\n along with SimCity.\n\n13:52.680 --> 13:55.720\n And it meant that every game you played was unique.\n\n13:55.720 --> 13:58.840\n Is there something you could say just on a small tangent\n\n13:58.840 --> 14:02.160\n about really impressive AI\n\n14:02.160 --> 14:06.560\n from a game design, human enjoyment perspective,\n\n14:06.560 --> 14:09.680\n really impressive AI that you've seen in games\n\n14:09.680 --> 14:12.480\n and maybe what does it take to create an AI system?\n\n14:12.480 --> 14:14.240\n And how hard of a problem is that?\n\n14:14.240 --> 14:18.360\n So a million questions just as a brief tangent.\n\n14:18.360 --> 14:23.000\n Well, look, I think games have been significant in my life\n\n14:23.000 --> 14:23.840\n for three reasons.\n\n14:23.840 --> 14:26.080\n So first of all, I was playing them\n\n14:26.080 --> 14:28.800\n and training myself on games when I was a kid.\n\n14:28.800 --> 14:31.480\n Then I went through a phase of designing games\n\n14:31.480 --> 14:33.000\n and writing AI for games.\n\n14:33.000 --> 14:35.960\n So all the games I professionally wrote\n\n14:35.960 --> 14:37.680\n had AI as a core component.\n\n14:37.680 --> 14:40.040\n And that was mostly in the 90s.\n\n14:40.040 --> 14:42.960\n And the reason I was doing that in games industry\n\n14:42.960 --> 14:45.080\n was at the time the games industry,\n\n14:45.080 --> 14:47.160\n I think was the cutting edge of technology.\n\n14:47.160 --> 14:49.800\n So whether it was graphics with people like John Carmack\n\n14:49.800 --> 14:53.040\n and Quake and those kinds of things or AI,\n\n14:53.040 --> 14:56.160\n I think actually all the action was going on in games.\n\n14:56.160 --> 14:58.440\n And we're still reaping the benefits of that\n\n14:58.440 --> 15:01.480\n even with things like GPUs, which I find ironic\n\n15:01.480 --> 15:03.680\n was obviously invented for graphics, computer graphics,\n\n15:03.680 --> 15:06.320\n but then turns out to be amazingly useful for AI.\n\n15:06.320 --> 15:08.760\n It just turns out everything's a matrix multiplication\n\n15:08.760 --> 15:11.080\n it appears in the whole world.\n\n15:11.080 --> 15:15.800\n So I think games at the time had the most cutting edge AI.\n\n15:15.800 --> 15:19.800\n And a lot of the games, I was involved in writing.\n\n15:19.800 --> 15:21.280\n So there was a game called Black and White,\n\n15:21.280 --> 15:22.760\n which was one game I was involved with\n\n15:22.760 --> 15:24.000\n in the early stages of,\n\n15:24.000 --> 15:28.400\n which I still think is the most impressive example\n\n15:28.400 --> 15:30.560\n of reinforcement learning in a computer game.\n\n15:30.560 --> 15:34.600\n So in that game, you trained a little pet animal and...\n\n15:34.600 --> 15:35.440\n It's a brilliant game.\n\n15:35.440 --> 15:37.640\n And it sort of learned from how you were treating it.\n\n15:37.640 --> 15:40.680\n So if you treated it badly, then it became mean.\n\n15:40.680 --> 15:42.920\n And then it would be mean to your villagers\n\n15:42.920 --> 15:45.760\n and your population, the sort of the little tribe\n\n15:45.760 --> 15:47.240\n that you were running.\n\n15:47.240 --> 15:49.400\n But if you were kind to it, then it would be kind.\n\n15:49.400 --> 15:51.080\n And people were fascinated by how that works.\n\n15:51.080 --> 15:54.160\n And so was I to be honest with the way it kind of developed.\n\n15:54.160 --> 15:55.120\n And...\n\n15:55.120 --> 15:57.240\n Especially the mapping to good and evil.\n\n15:57.240 --> 15:58.080\n Yeah.\n\n15:58.080 --> 16:01.640\n Made you realize, made me realize that you can sort of\n\n16:01.640 --> 16:06.640\n in the choices you make can define where you end up.\n\n16:07.440 --> 16:12.440\n And that means all of us are capable of the good, evil.\n\n16:12.640 --> 16:15.240\n It all matters in the different choices\n\n16:15.240 --> 16:18.240\n along the trajectory to those places that you make.\n\n16:18.240 --> 16:19.080\n It's fascinating.\n\n16:19.080 --> 16:21.360\n I mean, games can do that philosophically to you.\n\n16:21.360 --> 16:22.200\n And it's rare.\n\n16:22.200 --> 16:23.040\n It seems rare.\n\n16:23.040 --> 16:23.880\n Yeah.\n\n16:23.880 --> 16:24.720\n Well, games are, I think, a unique medium\n\n16:24.720 --> 16:26.600\n because you as the player,\n\n16:26.600 --> 16:30.080\n you're not just passively consuming the entertainment,\n\n16:30.080 --> 16:30.920\n right?\n\n16:30.920 --> 16:34.280\n You're actually actively involved as an agent.\n\n16:34.280 --> 16:36.160\n So I think that's what makes it in some ways\n\n16:36.160 --> 16:38.400\n can be more visceral than other mediums\n\n16:38.400 --> 16:40.000\n like films and books.\n\n16:40.000 --> 16:42.640\n So the second, so that was designing AI in games.\n\n16:42.640 --> 16:46.440\n And then the third use we've used of AI\n\n16:46.440 --> 16:48.440\n is in DeepMind from the beginning,\n\n16:48.440 --> 16:50.920\n which is using games as a testing ground\n\n16:50.920 --> 16:55.000\n for proving out AI algorithms and developing AI algorithms.\n\n16:55.000 --> 16:58.480\n And that was a sort of a core component\n\n16:58.480 --> 17:00.320\n of our vision at the start of DeepMind\n\n17:00.320 --> 17:03.200\n was that we would use games very heavily\n\n17:03.200 --> 17:06.360\n as our main testing ground, certainly to begin with,\n\n17:06.360 --> 17:08.560\n because it's super efficient to use games.\n\n17:08.560 --> 17:11.480\n And also, it's very easy to have metrics\n\n17:11.480 --> 17:14.080\n to see how well your systems are improving\n\n17:14.080 --> 17:15.840\n and what direction your ideas are going in\n\n17:15.840 --> 17:18.400\n and whether you're making incremental improvements.\n\n17:18.400 --> 17:20.400\n And because those games are often rooted\n\n17:20.400 --> 17:23.360\n in something that humans did for a long time beforehand,\n\n17:23.360 --> 17:26.520\n there's already a strong set of rules.\n\n17:26.520 --> 17:28.240\n Like it's already a damn good benchmark.\n\n17:28.240 --> 17:30.200\n Yes, it's really good for so many reasons\n\n17:30.200 --> 17:32.800\n because you've got clear measures\n\n17:32.800 --> 17:35.520\n of how good humans can be at these things.\n\n17:35.520 --> 17:36.840\n And in some cases like Go,\n\n17:36.840 --> 17:39.720\n we've been playing it for thousands of years\n\n17:39.720 --> 17:43.280\n and often they have scores or at least win conditions.\n\n17:43.280 --> 17:45.640\n So it's very easy for reward learning systems\n\n17:45.640 --> 17:46.480\n to get a reward.\n\n17:46.480 --> 17:49.320\n It's very easy to specify what that reward is.\n\n17:49.320 --> 17:54.320\n And also at the end, it's easy to test externally\n\n17:54.320 --> 17:56.920\n at how strong is your system by of course,\n\n17:56.920 --> 18:00.240\n playing against the world's strongest players at those games.\n\n18:00.240 --> 18:02.680\n So it's so good for so many reasons\n\n18:02.680 --> 18:05.520\n and it's also very efficient to run potentially millions\n\n18:05.520 --> 18:08.280\n of simulations in parallel on the cloud.\n\n18:08.280 --> 18:12.800\n So I think there's a huge reason why we were so successful\n\n18:12.800 --> 18:14.760\n back in starting out 2010,\n\n18:14.760 --> 18:16.680\n how come we were able to progress so quickly\n\n18:16.680 --> 18:18.880\n because we've utilized games.\n\n18:18.880 --> 18:21.320\n And at the beginning of DeepMind,\n\n18:21.320 --> 18:24.600\n we also hired some amazing game engineers\n\n18:24.600 --> 18:28.000\n who I knew from my previous lives in the games industry.\n\n18:28.000 --> 18:30.920\n And that helped to bootstrap us very quickly.\n\n18:30.920 --> 18:33.880\n And plus it's somehow super compelling\n\n18:33.880 --> 18:38.080\n almost at a philosophical level of man versus machine\n\n18:38.080 --> 18:41.200\n over a chess board or a Go board.\n\n18:41.200 --> 18:43.600\n And especially given that the entire history of AI\n\n18:43.600 --> 18:45.960\n is defined by people saying it's gonna be impossible\n\n18:45.960 --> 18:50.960\n to make a machine that beats a human being in chess.\n\n18:50.960 --> 18:53.200\n And then once that happened,\n\n18:53.200 --> 18:55.880\n people were certain when I was coming up in AI\n\n18:55.880 --> 18:58.760\n that Go is not a game that can be solved\n\n18:58.760 --> 19:02.000\n because of the combinatorial complexity is just too,\n\n19:02.000 --> 19:06.640\n it's no matter how much Moore's law you have,\n\n19:06.640 --> 19:08.560\n compute is just never going to be able\n\n19:08.560 --> 19:10.200\n to crack the game of Go.\n\n19:10.200 --> 19:14.920\n And so then there's something compelling about facing,\n\n19:14.920 --> 19:18.160\n sort of taking on the impossibility of that task\n\n19:18.160 --> 19:22.480\n from the AI researcher perspective,\n\n19:22.480 --> 19:24.520\n engineer perspective, and then as a human being,\n\n19:24.520 --> 19:26.120\n just observing this whole thing.\n\n19:27.040 --> 19:31.560\n Your beliefs about what you thought was impossible\n\n19:32.520 --> 19:34.120\n being broken apart,\n\n19:35.920 --> 19:40.480\n it's humbling to realize we're not as smart as we thought.\n\n19:40.480 --> 19:43.160\n It's humbling to realize that the things we think\n\n19:43.160 --> 19:47.000\n are impossible now perhaps will be done in the future.\n\n19:47.000 --> 19:50.800\n There's something really powerful about a game,\n\n19:50.800 --> 19:52.880\n AI system beating human being in a game\n\n19:52.880 --> 19:55.680\n that drives that message home\n\n19:55.680 --> 19:58.000\n for like millions, billions of people,\n\n19:58.000 --> 19:59.320\n especially in the case of Go.\n\n19:59.320 --> 20:00.520\n Sure.\n\n20:00.520 --> 20:01.640\n Well, look, I think it's,\n\n20:01.640 --> 20:03.720\n I mean, it has been a fascinating journey\n\n20:03.720 --> 20:06.880\n and especially as I think about it from,\n\n20:06.880 --> 20:08.760\n I can understand it from both sides,\n\n20:08.760 --> 20:13.080\n both as the AI, creators of the AI,\n\n20:13.080 --> 20:15.640\n but also as a games player originally.\n\n20:15.640 --> 20:17.960\n So, it was a really interesting,\n\n20:17.960 --> 20:21.160\n I mean, it was a fantastic, but also somewhat\n\n20:21.160 --> 20:23.520\n bittersweet moment, the AlphaGo match for me,\n\n20:24.680 --> 20:28.520\n seeing that and being obviously heavily involved in that.\n\n20:29.440 --> 20:32.480\n But as you say, chess has been the,\n\n20:32.480 --> 20:34.360\n I mean, Kasparov, I think rightly called it\n\n20:34.360 --> 20:37.280\n the Drosophila of intelligence, right?\n\n20:37.280 --> 20:39.520\n So, it's sort of, I love that phrase\n\n20:39.520 --> 20:42.960\n and I think he's right because chess has been\n\n20:42.960 --> 20:45.360\n hand in hand with AI from the beginning\n\n20:45.360 --> 20:47.440\n of the whole field, right?\n\n20:47.440 --> 20:49.640\n So, I think every AI practitioner,\n\n20:49.640 --> 20:52.480\n starting with Turing and Claude Shannon and all those,\n\n20:52.480 --> 20:55.400\n the sort of forefathers of the field,\n\n20:56.320 --> 20:58.840\n tried their hand at writing a chess program.\n\n20:58.840 --> 21:01.160\n I've got original edition of Claude Shannon's\n\n21:01.160 --> 21:04.000\n first chess program, I think it was 1949,\n\n21:04.000 --> 21:06.760\n the original sort of paper.\n\n21:06.760 --> 21:09.960\n And they all did that and Turing famously wrote\n\n21:09.960 --> 21:12.480\n a chess program, but all the computers around them\n\n21:12.480 --> 21:13.760\n were obviously too slow to run it.\n\n21:13.760 --> 21:16.040\n So, he had to run, he had to be the computer, right?\n\n21:16.040 --> 21:18.920\n So, he literally, I think spent two or three days\n\n21:18.920 --> 21:21.360\n running his own program by hand with pencil and paper\n\n21:21.360 --> 21:24.960\n and playing a friend of his with his chess program.\n\n21:24.960 --> 21:28.560\n So, of course, Deep Blue was a huge moment,\n\n21:28.560 --> 21:31.880\n beating Kasparov, but actually when that happened,\n\n21:31.880 --> 21:34.120\n I remember that very vividly, of course,\n\n21:34.120 --> 21:36.640\n because it was chess and computers and AI,\n\n21:36.640 --> 21:39.240\n all the things I loved and I was at college at the time.\n\n21:39.240 --> 21:40.800\n But I remember coming away from that,\n\n21:40.800 --> 21:43.080\n being more impressed by Kasparov's mind\n\n21:43.080 --> 21:44.480\n than I was by Deep Blue.\n\n21:44.480 --> 21:47.680\n Because here was Kasparov with his human mind,\n\n21:47.680 --> 21:49.400\n not only could he play chess more or less\n\n21:49.400 --> 21:53.160\n to the same level as this brute of a calculation machine,\n\n21:53.160 --> 21:55.160\n but of course, Kasparov can do everything else\n\n21:55.160 --> 21:57.480\n humans can do, ride a bike, talk many languages,\n\n21:57.480 --> 21:59.400\n do politics, all the rest of the amazing things\n\n21:59.400 --> 22:00.880\n that Kasparov does.\n\n22:00.880 --> 22:03.160\n And so, with the same brain.\n\n22:03.160 --> 22:07.040\n And yet Deep Blue, brilliant as it was at chess,\n\n22:07.040 --> 22:12.040\n it'd been hand coded for chess and actually had distilled\n\n22:12.040 --> 22:16.400\n the knowledge of chess grandmasters into a cool program,\n\n22:16.400 --> 22:18.000\n but it couldn't do anything else.\n\n22:18.000 --> 22:20.080\n It couldn't even play a strictly simpler game\n\n22:20.080 --> 22:21.280\n like tic tac toe.\n\n22:21.280 --> 22:25.880\n So, something to me was missing from intelligence\n\n22:25.880 --> 22:28.480\n from that system that we would regard as intelligence.\n\n22:28.480 --> 22:30.880\n And I think it was this idea of generality\n\n22:30.880 --> 22:32.160\n and also learning.\n\n22:33.000 --> 22:36.120\n So, and that's obviously what we tried to do with AlphaGo.\n\n22:36.120 --> 22:38.600\n Yeah, with AlphaGo and AlphaZero, MuZero,\n\n22:38.600 --> 22:42.040\n and then God and all the things that we'll get into\n\n22:42.040 --> 22:45.640\n some parts of, there's just a fascinating trajectory here.\n\n22:45.640 --> 22:48.520\n But let's just stick on chess briefly.\n\n22:48.520 --> 22:51.960\n On the human side of chess, you've proposed that\n\n22:51.960 --> 22:53.400\n from a game design perspective,\n\n22:53.400 --> 22:56.440\n the thing that makes chess compelling as a game\n\n22:57.760 --> 23:01.000\n is that there's a creative tension between a bishop\n\n23:01.000 --> 23:02.960\n and the knight.\n\n23:02.960 --> 23:04.040\n Can you explain this?\n\n23:04.040 --> 23:06.440\n First of all, it's really interesting to think about\n\n23:06.440 --> 23:08.640\n what makes a game compelling,\n\n23:08.640 --> 23:11.000\n makes it stick across centuries.\n\n23:12.000 --> 23:13.480\n Yeah, I was sort of thinking about this,\n\n23:13.480 --> 23:15.440\n and actually a lot of even amazing chess players\n\n23:15.440 --> 23:16.840\n don't think about it necessarily\n\n23:16.840 --> 23:18.280\n from a game's designer point of view.\n\n23:18.280 --> 23:20.240\n So, it's with my game design hat on\n\n23:20.240 --> 23:23.080\n that I was thinking about this, why is chess so compelling?\n\n23:23.080 --> 23:27.560\n And I think a critical reason is the dynamicness\n\n23:27.560 --> 23:30.000\n of the different kind of chess positions you can have,\n\n23:30.000 --> 23:32.200\n whether they're closed or open and other things,\n\n23:32.200 --> 23:33.520\n comes from the bishop and the knight.\n\n23:33.520 --> 23:36.480\n So, if you think about how different\n\n23:36.480 --> 23:39.240\n the capabilities of the bishop and knight are\n\n23:39.240 --> 23:40.880\n in terms of the way they move,\n\n23:40.880 --> 23:43.080\n and then somehow chess has evolved\n\n23:43.080 --> 23:46.080\n to balance those two capabilities more or less equally.\n\n23:46.080 --> 23:48.720\n So, they're both roughly worth three points each.\n\n23:48.720 --> 23:50.560\n So, you think that dynamics is always there\n\n23:50.560 --> 23:51.640\n and then the rest of the rules\n\n23:51.640 --> 23:53.760\n are kind of trying to stabilize the game.\n\n23:53.760 --> 23:55.080\n Well, maybe, I mean, it's sort of,\n\n23:55.080 --> 23:56.560\n I don't know, it's chicken and egg situation,\n\n23:56.560 --> 23:57.680\n probably both came together.\n\n23:57.680 --> 24:00.520\n But the fact that it's got to this beautiful equilibrium\n\n24:00.520 --> 24:02.360\n where you can have the bishop and knight\n\n24:02.360 --> 24:04.400\n that are so different in power,\n\n24:04.400 --> 24:06.920\n but so equal in value across the set\n\n24:06.920 --> 24:09.480\n of the universe of all positions, right?\n\n24:09.480 --> 24:11.560\n Somehow they've been balanced by humanity\n\n24:11.560 --> 24:13.480\n over hundreds of years,\n\n24:13.480 --> 24:16.880\n I think gives the game the creative tension\n\n24:16.880 --> 24:19.000\n that you can swap the bishop and knights\n\n24:19.000 --> 24:20.160\n for a bishop for a knight,\n\n24:20.160 --> 24:22.080\n and they're more or less worth the same,\n\n24:22.080 --> 24:24.040\n but now you aim for a different type of position.\n\n24:24.040 --> 24:26.040\n If you have the knight, you want a closed position.\n\n24:26.040 --> 24:28.160\n If you have the bishop, you want an open position.\n\n24:28.160 --> 24:29.000\n So, I think that creates\n\n24:29.000 --> 24:30.920\n a lot of the creative tension in chess.\n\n24:30.920 --> 24:34.040\n So, some kind of controlled creative tension.\n\n24:34.040 --> 24:35.960\n From an AI perspective,\n\n24:35.960 --> 24:38.840\n do you think AI systems could eventually design games\n\n24:38.840 --> 24:41.640\n that are optimally compelling to humans?\n\n24:41.640 --> 24:42.920\n Well, that's an interesting question.\n\n24:42.920 --> 24:46.000\n Sometimes I get asked about AI and creativity,\n\n24:46.000 --> 24:48.880\n and the way I answered that is relevant to that question,\n\n24:48.880 --> 24:51.240\n which is that I think there are different levels\n\n24:51.240 --> 24:52.920\n of creativity, one could say.\n\n24:52.920 --> 24:55.320\n So, I think if we define creativity\n\n24:55.320 --> 24:57.280\n as coming up with something original, right,\n\n24:57.280 --> 24:59.320\n that's useful for a purpose,\n\n24:59.320 --> 25:02.240\n then I think the kind of lowest level of creativity\n\n25:02.240 --> 25:03.720\n is like an interpolation.\n\n25:03.720 --> 25:06.280\n So, an averaging of all the examples you see.\n\n25:06.280 --> 25:08.320\n So, maybe a very basic AI system could say\n\n25:08.320 --> 25:09.160\n you could have that.\n\n25:09.160 --> 25:11.400\n So, you show it millions of pictures of cats,\n\n25:11.400 --> 25:13.920\n and then you say, give me an average looking cat, right?\n\n25:13.920 --> 25:15.480\n Generate me an average looking cat.\n\n25:15.480 --> 25:17.200\n I would call that interpolation.\n\n25:17.200 --> 25:18.720\n Then there's extrapolation,\n\n25:18.720 --> 25:20.440\n which something like AlphaGo showed.\n\n25:20.440 --> 25:24.320\n So, AlphaGo played millions of games of Go against itself,\n\n25:24.320 --> 25:26.600\n and then it came up with brilliant new ideas\n\n25:26.600 --> 25:30.760\n like Move 37 in game two, brilliant motif strategies in Go\n\n25:30.760 --> 25:32.840\n that no humans had ever thought of,\n\n25:32.840 --> 25:34.800\n even though we've played it for thousands of years\n\n25:34.800 --> 25:36.600\n and professionally for hundreds of years.\n\n25:36.600 --> 25:38.840\n So, that I call that extrapolation,\n\n25:38.840 --> 25:41.080\n but then there's still a level above that,\n\n25:41.080 --> 25:44.000\n which is, you could call out of the box thinking\n\n25:44.000 --> 25:47.520\n or true innovation, which is, could you invent Go, right?\n\n25:47.520 --> 25:49.200\n Could you invent chess and not just come up\n\n25:49.200 --> 25:51.320\n with a brilliant chess move or brilliant Go move,\n\n25:51.320 --> 25:53.680\n but can you actually invent chess\n\n25:53.680 --> 25:55.880\n or something as good as chess or Go?\n\n25:55.880 --> 26:00.080\n And I think one day AI could, but then what's missing\n\n26:00.080 --> 26:02.240\n is how would you even specify that task\n\n26:02.240 --> 26:04.440\n to a program right now?\n\n26:04.440 --> 26:07.560\n And the way I would do it if I was telling a human to do it\n\n26:07.560 --> 26:10.800\n or a human games designer to do it is I would say,\n\n26:10.800 --> 26:14.120\n something like Go, I would say, come up with a game\n\n26:14.120 --> 26:16.080\n that only takes five minutes to learn,\n\n26:16.080 --> 26:17.880\n which Go does because it's got simple rules,\n\n26:17.880 --> 26:20.280\n but many lifetimes to master, right?\n\n26:20.280 --> 26:22.080\n Or impossible to master in one lifetime\n\n26:22.080 --> 26:23.920\n because it's so deep and so complex.\n\n26:23.920 --> 26:26.520\n And then it's aesthetically beautiful.\n\n26:26.520 --> 26:30.080\n And also it can be completed in three or four hours\n\n26:30.080 --> 26:35.080\n of gameplay time, which is useful for us in a human day.\n\n26:35.280 --> 26:38.560\n And so you might specify these sort of high level concepts\n\n26:38.560 --> 26:40.640\n like that, and then with that\n\n26:40.640 --> 26:42.800\n and then maybe a few other things,\n\n26:42.800 --> 26:47.560\n one could imagine that Go satisfies those constraints.\n\n26:47.560 --> 26:49.600\n But the problem is that we're not able\n\n26:49.600 --> 26:53.040\n to specify abstract notions like that,\n\n26:53.040 --> 26:57.040\n high level abstract notions like that yet to our AI systems.\n\n26:57.040 --> 26:58.840\n And I think there's still something missing there\n\n26:58.840 --> 27:01.840\n in terms of high level concepts or abstractions\n\n27:01.840 --> 27:03.080\n that they truly understand\n\n27:03.080 --> 27:06.560\n and they're combinable and compositional.\n\n27:06.560 --> 27:09.760\n So for the moment, I think AI is capable\n\n27:09.760 --> 27:11.760\n of doing interpolation and extrapolation,\n\n27:11.760 --> 27:13.520\n but not true invention.\n\n27:13.520 --> 27:18.040\n So coming up with rule sets and optimizing\n\n27:18.040 --> 27:20.640\n with complicated objectives around those rule sets,\n\n27:20.640 --> 27:22.280\n we can't currently do.\n\n27:22.280 --> 27:25.480\n But you could take a specific rule set\n\n27:25.480 --> 27:28.360\n and then run a kind of self play experiment\n\n27:28.360 --> 27:32.040\n to see how long, just observe how an AI system\n\n27:32.040 --> 27:35.960\n from scratch learns, how long is that journey of learning?\n\n27:35.960 --> 27:39.160\n And maybe if it satisfies some of those other things\n\n27:39.160 --> 27:41.680\n you mentioned in terms of quickness to learn and so on,\n\n27:41.680 --> 27:44.200\n and you could see a long journey to master\n\n27:44.200 --> 27:46.920\n for even an AI system, then you could say\n\n27:46.920 --> 27:49.280\n that this is a promising game.\n\n27:49.280 --> 27:51.720\n But it would be nice to do almost like AlphaCode\n\n27:51.720 --> 27:53.960\n so programming rules.\n\n27:53.960 --> 27:58.960\n So generating rules that automate even that part\n\n27:59.000 --> 28:00.440\n of the generation of rules.\n\n28:00.440 --> 28:02.960\n So I have thought about systems actually\n\n28:02.960 --> 28:05.680\n that I think would be amazing for a games designer.\n\n28:05.680 --> 28:09.200\n If you could have a system that takes your game,\n\n28:09.200 --> 28:11.960\n plays it tens of millions of times, maybe overnight,\n\n28:11.960 --> 28:13.840\n and then self balances the rules better.\n\n28:13.840 --> 28:18.080\n So it tweaks the rules and maybe the equations\n\n28:18.080 --> 28:22.680\n and the parameters so that the game is more balanced,\n\n28:22.680 --> 28:26.280\n the units in the game or some of the rules could be tweaked.\n\n28:26.280 --> 28:28.320\n So it's a bit of like giving a base set\n\n28:28.320 --> 28:30.800\n and then allowing Monte Carlo Tree Search\n\n28:30.800 --> 28:33.360\n or something like that to sort of explore it.\n\n28:33.360 --> 28:37.080\n And I think that would be super powerful tool actually\n\n28:37.080 --> 28:39.720\n for balancing, auto balancing a game,\n\n28:39.720 --> 28:42.120\n which usually takes thousands of hours\n\n28:42.120 --> 28:44.520\n from hundreds of human games testers normally\n\n28:44.520 --> 28:47.480\n to balance a game like StarCraft,\n\n28:47.480 --> 28:50.640\n which is Blizzard are amazing at balancing their games,\n\n28:50.640 --> 28:52.600\n but it takes them years and years and years.\n\n28:52.600 --> 28:54.120\n So one could imagine at some point\n\n28:54.120 --> 28:57.080\n when this stuff becomes efficient enough\n\n28:57.080 --> 28:59.560\n to you might be able to do that like overnight.\n\n28:59.560 --> 29:04.560\n Do you think a game that is optimal designed by an AI system\n\n29:05.000 --> 29:08.320\n would look very much like a planet earth?\n\n29:09.640 --> 29:11.640\n Maybe, maybe it's only the sort of game\n\n29:11.640 --> 29:16.040\n I would love to make is, and I've tried in my games career,\n\n29:16.040 --> 29:18.560\n the games design career, my first big game\n\n29:18.560 --> 29:21.440\n was designing a theme park, an amusement park.\n\n29:21.440 --> 29:25.200\n Then with games like Republic, I tried to have games\n\n29:25.200 --> 29:28.480\n where we designed whole cities and allowed you to play in.\n\n29:28.480 --> 29:30.320\n So, and of course people like Will Wright\n\n29:30.320 --> 29:32.640\n have written games like SimEarth,\n\n29:32.640 --> 29:35.200\n trying to simulate the whole of earth, pretty tricky,\n\n29:35.200 --> 29:36.040\n but I think.\n\n29:36.040 --> 29:37.600\n SimEarth, I haven't actually played that one.\n\n29:37.600 --> 29:38.440\n So what is it?\n\n29:38.440 --> 29:40.320\n Does it incorporate of evolution or?\n\n29:40.320 --> 29:43.280\n Yeah, it has evolution and it sort of tries to,\n\n29:43.280 --> 29:45.320\n it sort of treats it as an entire biosphere,\n\n29:45.320 --> 29:47.240\n but from quite high level.\n\n29:47.240 --> 29:48.080\n So.\n\n29:48.080 --> 29:50.280\n It'd be nice to be able to sort of zoom in,\n\n29:50.280 --> 29:51.320\n zoom out and zoom in.\n\n29:51.320 --> 29:52.160\n Exactly, exactly.\n\n29:52.160 --> 29:53.440\n So obviously it couldn't do, that was in the 90s.\n\n29:53.440 --> 29:54.920\n I think he wrote that in the 90s.\n\n29:54.920 --> 29:57.560\n So it couldn't, it wasn't able to do that,\n\n29:57.560 --> 30:00.520\n but that would be obviously the ultimate sandbox game.\n\n30:00.520 --> 30:01.480\n Of course.\n\n30:01.480 --> 30:04.760\n On that topic, do you think we're living in a simulation?\n\n30:04.760 --> 30:06.160\n Yes, well, so, okay.\n\n30:06.160 --> 30:07.000\n So I.\n\n30:07.000 --> 30:09.280\n We're gonna jump around from the absurdly philosophical\n\n30:09.280 --> 30:10.120\n to the technical.\n\n30:10.120 --> 30:11.880\n Sure, sure, very, very happy to.\n\n30:11.880 --> 30:13.800\n So I think my answer to that question\n\n30:13.800 --> 30:17.640\n is a little bit complex because there is simulation theory,\n\n30:17.640 --> 30:18.800\n which obviously Nick Bostrom,\n\n30:18.800 --> 30:20.600\n I think famously first proposed.\n\n30:21.680 --> 30:24.720\n And I don't quite believe it in that sense.\n\n30:24.720 --> 30:29.600\n So in the sense that are we in some sort of computer game\n\n30:29.600 --> 30:34.000\n or have our descendants somehow recreated earth\n\n30:34.000 --> 30:36.520\n in the 21st century and some,\n\n30:36.520 --> 30:38.480\n for some kind of experimental reason.\n\n30:38.480 --> 30:41.880\n I think that, but I do think that we,\n\n30:41.880 --> 30:45.600\n that we might be, that the best way to understand physics\n\n30:45.600 --> 30:49.320\n and the universe is from a computational perspective.\n\n30:49.320 --> 30:52.440\n So understanding it as an information universe\n\n30:52.440 --> 30:56.200\n and actually information being the most fundamental unit\n\n30:56.200 --> 30:59.920\n of reality rather than matter or energy.\n\n30:59.920 --> 31:02.400\n So a physicist would say, you know, matter or energy,\n\n31:02.400 --> 31:03.760\n you know, E equals MC squared.\n\n31:03.760 --> 31:06.440\n These are the things that are the fundamentals\n\n31:06.440 --> 31:07.400\n of the universe.\n\n31:07.400 --> 31:09.880\n I'd actually say information,\n\n31:09.880 --> 31:11.760\n which of course itself can be,\n\n31:11.760 --> 31:13.560\n can specify energy or matter, right?\n\n31:13.560 --> 31:14.920\n Matter is actually just, you know,\n\n31:14.920 --> 31:16.880\n we're just out the way our bodies\n\n31:16.880 --> 31:19.720\n and the molecules in our body are arranged as information.\n\n31:19.720 --> 31:23.080\n So I think information may be the most fundamental way\n\n31:23.080 --> 31:24.960\n to describe the universe.\n\n31:24.960 --> 31:28.280\n And therefore you could say we're in some sort of simulation\n\n31:28.280 --> 31:29.880\n because of that.\n\n31:29.880 --> 31:31.040\n But I don't, I do, I'm not,\n\n31:31.040 --> 31:34.200\n I'm not really a subscriber to the idea that, you know,\n\n31:34.200 --> 31:36.920\n these are sort of throw away billions of simulations around.\n\n31:36.920 --> 31:40.640\n I think this is actually very critical and possibly unique,\n\n31:40.640 --> 31:41.760\n this simulation.\n\n31:41.760 --> 31:42.600\n This particular one.\n\n31:42.600 --> 31:43.440\n Yes.\n\n31:43.440 --> 31:48.440\n And you just mean treating the universe as a computer\n\n31:48.760 --> 31:52.240\n that's processing and modifying information\n\n31:52.240 --> 31:54.880\n is a good way to solve the problems of physics,\n\n31:54.880 --> 31:57.160\n of chemistry, of biology,\n\n31:57.160 --> 31:59.720\n and perhaps of humanity and so on.\n\n31:59.720 --> 32:02.240\n Yes, I think understanding physics\n\n32:02.240 --> 32:04.840\n in terms of information theory\n\n32:04.840 --> 32:07.880\n might be the best way to really understand\n\n32:07.880 --> 32:09.360\n what's going on here.\n\n32:09.360 --> 32:13.560\n From our understanding of a universal Turing machine,\n\n32:13.560 --> 32:15.280\n from our understanding of a computer,\n\n32:15.280 --> 32:17.400\n do you think there's something outside\n\n32:17.400 --> 32:19.440\n of the capabilities of a computer\n\n32:19.440 --> 32:21.000\n that is present in our universe?\n\n32:21.000 --> 32:23.560\n You have a disagreement with Roger Penrose\n\n32:23.560 --> 32:25.920\n about the nature of consciousness.\n\n32:25.920 --> 32:27.760\n He thinks that consciousness is more\n\n32:27.760 --> 32:29.040\n than just a computation.\n\n32:30.080 --> 32:32.680\n Do you think all of it, the whole shebangs,\n\n32:32.680 --> 32:34.000\n can be a computation?\n\n32:34.000 --> 32:35.840\n Yeah, I've had many fascinating debates\n\n32:35.840 --> 32:37.680\n with Sir Roger Penrose,\n\n32:37.680 --> 32:39.680\n and obviously he's famously,\n\n32:39.680 --> 32:41.520\n and I read, you know, Emperors of the New Mind\n\n32:41.520 --> 32:45.400\n and his books, his classical books,\n\n32:45.400 --> 32:47.800\n and they were pretty influential in the 90s.\n\n32:47.800 --> 32:50.960\n And he believes that there's something more,\n\n32:50.960 --> 32:53.040\n something quantum that is needed\n\n32:53.040 --> 32:55.840\n to explain consciousness in the brain.\n\n32:55.840 --> 32:58.320\n I think about what we're doing actually at DeepMind\n\n32:58.320 --> 32:59.920\n and what my career is being,\n\n32:59.920 --> 33:01.920\n we're almost like Turing's champion.\n\n33:01.920 --> 33:05.360\n So we are pushing Turing machines or classical computation\n\n33:05.360 --> 33:06.200\n to the limits.\n\n33:06.200 --> 33:09.440\n What are the limits of what classical computing can do?\n\n33:09.440 --> 33:11.760\n Now, and at the same time,\n\n33:11.760 --> 33:14.240\n I've also studied neuroscience to see,\n\n33:14.240 --> 33:15.520\n and that's why I did my PhD in,\n\n33:15.520 --> 33:17.720\n was to see, also to look at, you know,\n\n33:17.720 --> 33:19.240\n is there anything quantum in the brain\n\n33:19.240 --> 33:21.360\n from a neuroscience or biological perspective?\n\n33:21.360 --> 33:24.560\n And so far, I think most neuroscientists\n\n33:24.560 --> 33:26.440\n and most mainstream biologists and neuroscientists\n\n33:26.440 --> 33:29.480\n would say there's no evidence of any quantum systems\n\n33:29.480 --> 33:30.800\n or effects in the brain.\n\n33:30.800 --> 33:33.000\n As far as we can see, it can be mostly explained\n\n33:33.000 --> 33:35.880\n by classical theories.\n\n33:35.880 --> 33:39.280\n So, and then, so there's sort of the search\n\n33:39.280 --> 33:40.600\n from the biology side.\n\n33:40.600 --> 33:42.120\n And then at the same time,\n\n33:42.120 --> 33:44.960\n there's the raising of the water, the bar,\n\n33:44.960 --> 33:47.240\n from what classical Turing machines can do.\n\n33:48.240 --> 33:51.680\n And, you know, including our new AI systems.\n\n33:51.680 --> 33:55.040\n And as you alluded to earlier, you know,\n\n33:55.040 --> 33:57.760\n I think AI, especially in the last decade plus,\n\n33:57.760 --> 34:02.360\n has been a continual story now of surprising events\n\n34:02.360 --> 34:03.920\n and surprising successes,\n\n34:03.920 --> 34:05.800\n knocking over one theory after another\n\n34:05.800 --> 34:07.760\n of what was thought to be impossible, you know,\n\n34:07.760 --> 34:10.080\n from Go to protein folding and so on.\n\n34:10.080 --> 34:14.760\n And so I think I would be very hesitant\n\n34:14.760 --> 34:19.520\n to bet against how far the universal Turing machine\n\n34:19.520 --> 34:23.400\n and classical computation paradigm can go.\n\n34:23.400 --> 34:26.720\n And my betting would be that all of,\n\n34:26.720 --> 34:29.080\n certainly what's going on in our brain,\n\n34:29.080 --> 34:32.160\n can probably be mimicked or approximated\n\n34:32.160 --> 34:34.720\n on a classical machine,\n\n34:34.720 --> 34:38.400\n not requiring something metaphysical or quantum.\n\n34:38.400 --> 34:41.720\n And we'll get there with some of the work with AlphaFold,\n\n34:41.720 --> 34:45.080\n which I think begins the journey of modeling\n\n34:45.080 --> 34:48.160\n this beautiful and complex world of biology.\n\n34:48.160 --> 34:50.160\n So you think all the magic of the human mind\n\n34:50.160 --> 34:54.280\n comes from this, just a few pounds of mush,\n\n34:54.280 --> 34:57.480\n of biological computational mush,\n\n34:57.480 --> 35:00.560\n that's akin to some of the neural networks,\n\n35:01.560 --> 35:03.800\n not directly, but in spirit\n\n35:03.800 --> 35:06.200\n that DeepMind has been working with.\n\n35:06.200 --> 35:08.680\n Well, look, I think it's, you say it's a few, you know,\n\n35:08.680 --> 35:09.680\n of course it's, this is the,\n\n35:09.680 --> 35:11.520\n I think the biggest miracle of the universe\n\n35:11.520 --> 35:15.000\n is that it is just a few pounds of mush in our skulls.\n\n35:15.000 --> 35:18.560\n And yet it's also our brains are the most complex objects\n\n35:18.560 --> 35:20.240\n that we know of in the universe.\n\n35:20.240 --> 35:22.360\n So there's something profoundly beautiful\n\n35:22.360 --> 35:23.920\n and amazing about our brains.\n\n35:23.920 --> 35:28.640\n And I think that it's an incredibly,\n\n35:28.640 --> 35:30.720\n incredible efficient machine.\n\n35:30.720 --> 35:35.520\n And it's, you know, phenomenon basically.\n\n35:35.520 --> 35:37.480\n And I think that building AI,\n\n35:37.480 --> 35:38.920\n one of the reasons I wanna build AI,\n\n35:38.920 --> 35:40.440\n and I've always wanted to is,\n\n35:40.440 --> 35:43.800\n I think by building an intelligent artifact like AI,\n\n35:43.800 --> 35:46.480\n and then comparing it to the human mind,\n\n35:46.480 --> 35:49.560\n that will help us unlock the uniqueness\n\n35:49.560 --> 35:50.960\n and the true secrets of the mind\n\n35:50.960 --> 35:53.480\n that we've always wondered about since the dawn of history,\n\n35:53.480 --> 35:58.480\n like consciousness, dreaming, creativity, emotions,\n\n35:59.160 --> 36:00.760\n what are all these things, right?\n\n36:00.760 --> 36:04.200\n We've wondered about them since the dawn of humanity.\n\n36:04.200 --> 36:05.920\n And I think one of the reasons,\n\n36:05.920 --> 36:08.760\n and, you know, I love philosophy and philosophy of mind is,\n\n36:08.760 --> 36:11.200\n we found it difficult is there haven't been the tools\n\n36:11.200 --> 36:13.680\n for us to really, other than introspection,\n\n36:13.680 --> 36:15.880\n from very clever people in history,\n\n36:15.880 --> 36:17.200\n very clever philosophers,\n\n36:17.200 --> 36:19.360\n to really investigate this scientifically.\n\n36:19.360 --> 36:21.720\n But now suddenly we have a plethora of tools.\n\n36:21.720 --> 36:23.240\n Firstly, we have all of the neuroscience tools,\n\n36:23.240 --> 36:25.920\n fMRI machines, single cell recording, all of this stuff,\n\n36:25.920 --> 36:29.000\n but we also have the ability, computers and AI,\n\n36:29.000 --> 36:31.640\n to build intelligent systems.\n\n36:31.640 --> 36:34.720\n So I think that, you know,\n\n36:34.720 --> 36:37.320\n I think it is amazing what the human mind does.\n\n36:37.320 --> 36:41.120\n And I'm kind of in awe of it really.\n\n36:41.120 --> 36:44.440\n And I think it's amazing that with our human minds,\n\n36:44.440 --> 36:46.760\n we're able to build things like computers\n\n36:46.760 --> 36:48.280\n and actually even, you know,\n\n36:48.280 --> 36:49.880\n think and investigate about these questions.\n\n36:49.880 --> 36:52.720\n I think that's also a testament to the human mind.\n\n36:52.720 --> 36:53.560\n Yeah.\n\n36:53.560 --> 36:56.200\n The universe built the human mind\n\n36:56.200 --> 36:59.600\n that now is building computers that help us understand\n\n36:59.600 --> 37:01.480\n both the universe and our own human mind.\n\n37:01.480 --> 37:02.320\n That's right.\n\n37:02.320 --> 37:03.140\n This is actually it.\n\n37:03.140 --> 37:03.980\n I mean, I think that's one, you know,\n\n37:03.980 --> 37:05.760\n one could say we are,\n\n37:05.760 --> 37:08.160\n maybe we're the mechanism by which the universe\n\n37:08.160 --> 37:09.840\n is going to try and understand itself.\n\n37:09.840 --> 37:10.680\n Yeah.\n\n37:10.680 --> 37:13.160\n It's beautiful.\n\n37:13.160 --> 37:16.960\n So let's go to the basic building blocks of biology\n\n37:16.960 --> 37:20.200\n that I think is another angle at which you can start\n\n37:20.200 --> 37:22.280\n to understand the human mind, the human body,\n\n37:22.280 --> 37:23.400\n which is quite fascinating,\n\n37:23.400 --> 37:26.640\n which is from the basic building blocks,\n\n37:26.640 --> 37:28.960\n start to simulate, start to model\n\n37:28.960 --> 37:30.480\n how from those building blocks,\n\n37:30.480 --> 37:33.080\n you can construct bigger and bigger, more complex systems,\n\n37:33.080 --> 37:35.820\n maybe one day the entirety of the human biology.\n\n37:35.820 --> 37:39.680\n So here's another problem that thought\n\n37:39.680 --> 37:42.720\n to be impossible to solve, which is protein folding.\n\n37:42.720 --> 37:47.720\n And Alpha Fold or specifically Alpha Fold 2 did just that.\n\n37:48.840 --> 37:50.320\n It solved protein folding.\n\n37:50.320 --> 37:53.400\n I think it's one of the biggest breakthroughs,\n\n37:53.400 --> 37:55.140\n certainly in the history of structural biology,\n\n37:55.140 --> 37:58.200\n but in general in science,\n\n38:00.240 --> 38:04.840\n maybe from a high level, what is it and how does it work?\n\n38:04.840 --> 38:08.700\n And then we can ask some fascinating questions after.\n\n38:08.700 --> 38:09.980\n Sure.\n\n38:09.980 --> 38:12.880\n So maybe to explain it to people not familiar\n\n38:12.880 --> 38:14.400\n with protein folding is, you know,\n\n38:14.400 --> 38:16.980\n first of all, explain proteins, which is, you know,\n\n38:16.980 --> 38:18.840\n proteins are essential to all life.\n\n38:18.840 --> 38:21.520\n Every function in your body depends on proteins.\n\n38:21.520 --> 38:23.920\n Sometimes they're called the workhorses of biology.\n\n38:23.920 --> 38:25.340\n And if you look into them and I've, you know,\n\n38:25.340 --> 38:26.660\n obviously as part of Alpha Fold,\n\n38:26.660 --> 38:30.200\n I've been researching proteins and structural biology\n\n38:30.200 --> 38:31.760\n for the last few years, you know,\n\n38:31.760 --> 38:34.760\n they're amazing little bio nano machines proteins.\n\n38:34.760 --> 38:36.460\n They're incredible if you actually watch little videos\n\n38:36.460 --> 38:39.000\n of how they work, animations of how they work.\n\n38:39.000 --> 38:42.600\n And proteins are specified by their genetic sequence\n\n38:42.600 --> 38:44.280\n called the amino acid sequence.\n\n38:44.280 --> 38:47.040\n So you can think of it as their genetic makeup.\n\n38:47.040 --> 38:50.080\n And then in the body in nature,\n\n38:50.080 --> 38:53.360\n they fold up into a 3D structure.\n\n38:53.360 --> 38:55.320\n So you can think of it as a string of beads\n\n38:55.320 --> 38:57.160\n and then they fold up into a ball.\n\n38:57.160 --> 38:59.100\n Now, the key thing is you want to know\n\n38:59.100 --> 39:02.480\n what that 3D structure is because the structure,\n\n39:02.480 --> 39:06.120\n the 3D structure of a protein is what helps to determine\n\n39:06.120 --> 39:08.580\n what does it do, the function it does in your body.\n\n39:08.580 --> 39:12.320\n And also if you're interested in drugs or disease,\n\n39:12.320 --> 39:13.980\n you need to understand that 3D structure\n\n39:13.980 --> 39:15.840\n because if you want to target something\n\n39:15.840 --> 39:18.640\n with a drug compound about to block something\n\n39:18.640 --> 39:21.120\n the protein's doing, you need to understand\n\n39:21.120 --> 39:23.440\n where it's gonna bind on the surface of the protein.\n\n39:23.440 --> 39:24.940\n So obviously in order to do that,\n\n39:24.940 --> 39:26.720\n you need to understand the 3D structure.\n\n39:26.720 --> 39:28.640\n So the structure is mapped to the function.\n\n39:28.640 --> 39:29.880\n The structure is mapped to the function\n\n39:29.880 --> 39:32.560\n and the structure is obviously somehow specified\n\n39:32.560 --> 39:34.840\n by the amino acid sequence.\n\n39:34.840 --> 39:37.420\n And that's the, in essence, the protein folding problem is,\n\n39:37.420 --> 39:39.620\n can you just from the amino acid sequence,\n\n39:39.620 --> 39:42.560\n the one dimensional string of letters,\n\n39:42.560 --> 39:45.600\n can you immediately computationally predict\n\n39:45.600 --> 39:47.120\n the 3D structure?\n\n39:47.120 --> 39:50.020\n And this has been a grand challenge in biology\n\n39:50.020 --> 39:51.500\n for over 50 years.\n\n39:51.500 --> 39:54.360\n So I think it was first articulated by Christian Anfinsen,\n\n39:54.360 --> 39:57.040\n a Nobel prize winner in 1972,\n\n39:57.040 --> 39:59.240\n as part of his Nobel prize winning lecture.\n\n39:59.240 --> 40:01.860\n And he just speculated this should be possible\n\n40:01.860 --> 40:04.960\n to go from the amino acid sequence to the 3D structure,\n\n40:04.960 --> 40:06.060\n but he didn't say how.\n\n40:06.060 --> 40:09.440\n So it's been described to me as equivalent\n\n40:09.440 --> 40:12.320\n to Fermat's last theorem, but for biology.\n\n40:12.320 --> 40:15.120\n You should, as somebody that very well might win\n\n40:15.120 --> 40:16.560\n the Nobel prize in the future.\n\n40:16.560 --> 40:19.240\n But outside of that, you should do more\n\n40:19.240 --> 40:20.080\n of that kind of thing.\n\n40:20.080 --> 40:22.160\n In the margin, just put random things\n\n40:22.160 --> 40:24.440\n that will take like 200 years to solve.\n\n40:24.440 --> 40:26.000\n Set people off for 200 years.\n\n40:26.000 --> 40:27.720\n It should be possible.\n\n40:27.720 --> 40:29.040\n And just don't give any details.\n\n40:29.040 --> 40:29.880\n Exactly.\n\n40:29.880 --> 40:31.500\n I think everyone exactly should be,\n\n40:31.500 --> 40:33.520\n I'll have to remember that for future.\n\n40:33.520 --> 40:34.800\n So yeah, so he set off, you know,\n\n40:34.800 --> 40:37.040\n with this one throwaway remark, just like Fermat,\n\n40:37.040 --> 40:42.040\n you know, he set off this whole 50 year field really\n\n40:42.640 --> 40:44.400\n of computational biology.\n\n40:44.400 --> 40:46.240\n And they had, you know, they got stuck.\n\n40:46.240 --> 40:48.520\n They hadn't really got very far with doing this.\n\n40:48.520 --> 40:52.500\n And until now, until AlphaFold came along,\n\n40:52.500 --> 40:54.320\n this is done experimentally, right?\n\n40:54.320 --> 40:55.500\n Very painstakingly.\n\n40:55.500 --> 40:57.440\n So the rule of thumb is, and you have to like\n\n40:57.440 --> 40:59.820\n crystallize the protein, which is really difficult.\n\n40:59.820 --> 41:03.060\n Some proteins can't be crystallized like membrane proteins.\n\n41:03.060 --> 41:05.940\n And then you have to use very expensive electron microscopes\n\n41:05.940 --> 41:08.200\n or X ray crystallography machines.\n\n41:08.200 --> 41:10.680\n Really painstaking work to get the 3D structure\n\n41:10.680 --> 41:12.400\n and visualize the 3D structure.\n\n41:12.400 --> 41:14.840\n So the rule of thumb in experimental biology\n\n41:14.840 --> 41:16.860\n is that it takes one PhD student,\n\n41:16.860 --> 41:19.400\n their entire PhD to do one protein.\n\n41:20.320 --> 41:23.440\n And with AlphaFold 2, we were able to predict\n\n41:23.440 --> 41:26.400\n the 3D structure in a matter of seconds.\n\n41:26.400 --> 41:28.700\n And so we were, you know, over Christmas,\n\n41:28.700 --> 41:30.240\n we did the whole human proteome\n\n41:30.240 --> 41:33.280\n or every protein in the human body or 20,000 proteins.\n\n41:33.280 --> 41:34.760\n So the human proteomes like the equivalent\n\n41:34.760 --> 41:37.560\n of the human genome, but on protein space.\n\n41:37.560 --> 41:40.240\n And sort of revolutionized really\n\n41:40.240 --> 41:43.300\n what a structural biologist can do.\n\n41:43.300 --> 41:45.720\n Because now they don't have to worry\n\n41:45.720 --> 41:47.960\n about these painstaking experimental,\n\n41:47.960 --> 41:49.560\n should they put all of that effort in or not?\n\n41:49.560 --> 41:51.120\n They can almost just look up the structure\n\n41:51.120 --> 41:53.280\n of their proteins like a Google search.\n\n41:53.280 --> 41:56.880\n And so there's a data set on which it's trained\n\n41:56.880 --> 41:58.800\n and how to map this amino acid sequence.\n\n41:58.800 --> 42:00.760\n First of all, it's incredible that a protein,\n\n42:00.760 --> 42:02.480\n this little chemical computer is able to do\n\n42:02.480 --> 42:05.720\n that computation itself in some kind of distributed way\n\n42:05.720 --> 42:07.800\n and do it very quickly.\n\n42:07.800 --> 42:08.840\n That's a weird thing.\n\n42:08.840 --> 42:10.480\n And they evolve that way because, you know,\n\n42:10.480 --> 42:13.200\n in the beginning, I mean, that's a great invention,\n\n42:13.200 --> 42:14.760\n just the protein itself.\n\n42:14.760 --> 42:18.240\n And then there's, I think, probably a history\n\n42:18.240 --> 42:22.740\n of like they evolved to have many of these proteins\n\n42:22.740 --> 42:26.600\n and those proteins figure out how to be computers themselves\n\n42:26.600 --> 42:28.560\n in such a way that you can create structures\n\n42:28.560 --> 42:30.540\n that can interact in complexes with each other\n\n42:30.540 --> 42:32.660\n in order to form high level functions.\n\n42:32.660 --> 42:35.520\n I mean, it's a weird system that they figured it out.\n\n42:35.520 --> 42:36.360\n Well, for sure.\n\n42:36.360 --> 42:37.640\n I mean, you know, maybe we should talk\n\n42:37.640 --> 42:39.000\n about the origins of life too,\n\n42:39.000 --> 42:41.180\n but proteins themselves, I think are magical\n\n42:41.180 --> 42:45.760\n and incredible, as I said, little bio nano machines.\n\n42:45.760 --> 42:50.280\n And actually Leventhal, who was another scientist,\n\n42:50.280 --> 42:55.120\n a contemporary of Amphinson, he coined this Leventhal,\n\n42:55.120 --> 42:56.820\n what became known as Leventhal's paradox,\n\n42:56.820 --> 42:58.320\n which is exactly what you're saying.\n\n42:58.320 --> 43:01.580\n He calculated roughly an average protein,\n\n43:01.580 --> 43:05.080\n which is maybe 2000 amino acids base as long,\n\n43:05.080 --> 43:09.960\n is can fold in maybe 10 to the power 300\n\n43:09.960 --> 43:11.480\n different confirmations.\n\n43:11.480 --> 43:13.320\n So there's 10 to the power 300 different ways\n\n43:13.320 --> 43:14.800\n that protein could fold up.\n\n43:14.800 --> 43:18.160\n And yet somehow in nature, physics solves this,\n\n43:18.160 --> 43:20.520\n solves this in a matter of milliseconds.\n\n43:20.520 --> 43:23.080\n So proteins fold up in your body in, you know,\n\n43:23.080 --> 43:25.600\n sometimes in fractions of a second.\n\n43:25.600 --> 43:29.080\n So physics is somehow solving that search problem.\n\n43:29.080 --> 43:31.200\n And just to be clear, in many of these cases,\n\n43:31.200 --> 43:33.040\n maybe you can correct me if I'm wrong,\n\n43:33.040 --> 43:37.680\n there's often a unique way for that sequence to form itself.\n\n43:37.680 --> 43:41.240\n So among that huge number of possibilities,\n\n43:41.240 --> 43:43.540\n it figures out a way how to stably,\n\n43:45.320 --> 43:47.800\n in some cases there might be a misfunction, so on,\n\n43:47.800 --> 43:50.040\n which leads to a lot of the disorders and stuff like that.\n\n43:50.040 --> 43:52.720\n But most of the time it's a unique mapping\n\n43:52.720 --> 43:54.820\n and that unique mapping is not obvious.\n\n43:54.820 --> 43:55.660\n No, exactly.\n\n43:55.660 --> 43:57.120\n Which is what the problem is.\n\n43:57.120 --> 44:00.720\n Exactly, so there's a unique mapping usually in a healthy,\n\n44:00.720 --> 44:04.040\n if it's healthy, and as you say in disease,\n\n44:04.040 --> 44:05.400\n so for example, Alzheimer's,\n\n44:05.400 --> 44:09.000\n one conjecture is that it's because of misfolded protein,\n\n44:09.000 --> 44:12.040\n a protein that folds in the wrong way, amyloid beta protein.\n\n44:12.040 --> 44:14.560\n So, and then because it folds in the wrong way,\n\n44:14.560 --> 44:17.600\n it gets tangled up, right, in your neurons.\n\n44:17.600 --> 44:20.560\n So it's super important to understand\n\n44:20.560 --> 44:23.600\n both healthy functioning and also disease\n\n44:23.600 --> 44:26.480\n is to understand, you know, what these things are doing\n\n44:26.480 --> 44:27.600\n and how they're structuring.\n\n44:27.600 --> 44:30.540\n Of course, the next step is sometimes proteins change shape\n\n44:30.540 --> 44:32.160\n when they interact with something.\n\n44:32.160 --> 44:35.960\n So they're not just static necessarily in biology.\n\n44:37.200 --> 44:39.780\n Maybe you can give some interesting,\n\n44:39.780 --> 44:43.260\n so beautiful things to you about these early days\n\n44:43.260 --> 44:46.160\n of AlphaFold, of solving this problem,\n\n44:46.160 --> 44:51.160\n because unlike games, this is real physical systems\n\n44:51.280 --> 44:55.640\n that are less amenable to self play type of mechanisms.\n\n44:55.640 --> 44:56.460\n Sure.\n\n44:56.460 --> 44:58.440\n The size of the data set is smaller\n\n44:58.440 --> 44:59.760\n than you might otherwise like,\n\n44:59.760 --> 45:01.800\n so you have to be very clever about certain things.\n\n45:01.800 --> 45:03.600\n Is there something you could speak to\n\n45:04.800 --> 45:06.680\n what was very hard to solve\n\n45:06.680 --> 45:09.920\n and what are some beautiful aspects about the solution?\n\n45:09.920 --> 45:12.800\n Yeah, I would say AlphaFold is the most complex\n\n45:12.800 --> 45:14.600\n and also probably most meaningful system\n\n45:14.600 --> 45:15.860\n we've built so far.\n\n45:15.860 --> 45:18.400\n So it's been an amazing time actually in the last,\n\n45:18.400 --> 45:20.520\n you know, two, three years to see that come through\n\n45:20.520 --> 45:23.200\n because as we talked about earlier, you know,\n\n45:23.200 --> 45:25.480\n games is what we started on\n\n45:25.480 --> 45:27.900\n building things like AlphaGo and AlphaZero,\n\n45:27.900 --> 45:30.400\n but really the ultimate goal was to,\n\n45:30.400 --> 45:31.520\n not just to crack games,\n\n45:31.520 --> 45:33.120\n it was just to build,\n\n45:33.120 --> 45:35.320\n use them to bootstrap general learning systems\n\n45:35.320 --> 45:37.440\n we could then apply to real world challenges.\n\n45:37.440 --> 45:40.640\n Specifically, my passion is scientific challenges\n\n45:40.640 --> 45:41.920\n like protein folding.\n\n45:41.920 --> 45:43.280\n And then AlphaFold of course\n\n45:43.280 --> 45:45.360\n is our first big proof point of that.\n\n45:45.360 --> 45:49.040\n And so, you know, in terms of the data\n\n45:49.040 --> 45:50.920\n and the amount of innovations that had to go into it,\n\n45:50.920 --> 45:52.280\n we, you know, it was like\n\n45:52.280 --> 45:54.480\n more than 30 different component algorithms\n\n45:54.480 --> 45:57.960\n needed to be put together to crack the protein folding.\n\n45:57.960 --> 46:00.800\n I think some of the big innovations were that\n\n46:00.800 --> 46:04.220\n kind of building in some hard coded constraints\n\n46:04.220 --> 46:07.760\n around physics and evolutionary biology\n\n46:07.760 --> 46:10.400\n to constrain sort of things like the bond angles\n\n46:11.640 --> 46:14.240\n in the protein and things like that,\n\n46:15.400 --> 46:18.040\n a lot, but not to impact the learning system.\n\n46:18.040 --> 46:21.000\n So still allowing the system to be able to learn\n\n46:21.000 --> 46:25.540\n the physics itself from the examples that we had.\n\n46:25.540 --> 46:26.640\n And the examples, as you say,\n\n46:26.640 --> 46:28.840\n there are only about 150,000 proteins,\n\n46:28.840 --> 46:31.240\n even after 40 years of experimental biology,\n\n46:31.240 --> 46:33.880\n only around 150,000 proteins have been,\n\n46:33.880 --> 46:35.920\n the structures have been found out about.\n\n46:35.920 --> 46:37.120\n So that was our training set,\n\n46:37.120 --> 46:41.120\n which is much less than normally we would like to use,\n\n46:41.120 --> 46:43.840\n but using various tricks, things like self distillation.\n\n46:43.840 --> 46:48.280\n So actually using AlphaFold predictions,\n\n46:48.280 --> 46:49.480\n some of the best predictions\n\n46:49.480 --> 46:51.000\n that it thought was highly confident in,\n\n46:51.000 --> 46:53.320\n we put them back into the training set, right?\n\n46:53.320 --> 46:55.440\n To make the training set bigger,\n\n46:55.440 --> 46:58.400\n that was critical to AlphaFold working.\n\n46:58.400 --> 47:00.160\n So there was actually a huge number\n\n47:00.160 --> 47:02.720\n of different innovations like that,\n\n47:02.720 --> 47:06.080\n that were required to ultimately crack the problem.\n\n47:06.080 --> 47:09.720\n AlphaFold one, what it produced was a distrogram.\n\n47:09.720 --> 47:13.600\n So a kind of a matrix of the pairwise distances\n\n47:13.600 --> 47:17.880\n between all of the molecules in the protein.\n\n47:17.880 --> 47:20.440\n And then there had to be a separate optimization process\n\n47:20.440 --> 47:23.640\n to create the 3D structure.\n\n47:23.640 --> 47:25.120\n And what we did for AlphaFold two\n\n47:25.120 --> 47:26.920\n is make it truly end to end.\n\n47:26.920 --> 47:31.720\n So we went straight from the amino acid sequence of bases\n\n47:31.720 --> 47:33.860\n to the 3D structure directly\n\n47:33.860 --> 47:36.080\n without going through this intermediate step.\n\n47:36.080 --> 47:38.600\n And in machine learning, what we've always found is\n\n47:38.600 --> 47:40.920\n that the more end to end you can make it,\n\n47:40.920 --> 47:42.160\n the better the system.\n\n47:42.160 --> 47:46.160\n And it's probably because in the end,\n\n47:46.160 --> 47:48.560\n the system's better at learning what the constraints are\n\n47:48.560 --> 47:51.920\n than we are as the human designers of specifying it.\n\n47:51.920 --> 47:54.040\n So anytime you can let it flow end to end\n\n47:54.040 --> 47:55.400\n and actually just generate what it is\n\n47:55.400 --> 47:58.440\n you're really looking for, in this case, the 3D structure,\n\n47:58.440 --> 48:00.560\n you're better off than having this intermediate step,\n\n48:00.560 --> 48:03.360\n which you then have to handcraft the next step for.\n\n48:03.360 --> 48:06.160\n So it's better to let the gradients and the learning\n\n48:06.160 --> 48:09.000\n flow all the way through the system from the end point,\n\n48:09.000 --> 48:10.880\n the end output you want to the inputs.\n\n48:10.880 --> 48:13.040\n So that's a good way to start on a new problem.\n\n48:13.040 --> 48:14.360\n Handcraft a bunch of stuff,\n\n48:14.360 --> 48:16.640\n add a bunch of manual constraints\n\n48:16.640 --> 48:18.640\n with a small end to end learning piece\n\n48:18.640 --> 48:21.560\n or a small learning piece and grow that learning piece\n\n48:21.560 --> 48:22.840\n until it consumes the whole thing.\n\n48:22.840 --> 48:23.680\n That's right.\n\n48:23.680 --> 48:25.320\n And so you can also see,\n\n48:25.320 --> 48:26.960\n this is a bit of a method we've developed\n\n48:26.960 --> 48:29.640\n over doing many sort of successful alpha,\n\n48:29.640 --> 48:32.200\n we call them alpha X projects, right?\n\n48:32.200 --> 48:34.600\n And the easiest way to see that is the evolution\n\n48:34.600 --> 48:36.720\n of alpha go to alpha zero.\n\n48:36.720 --> 48:39.640\n So alpha go was a learning system,\n\n48:39.640 --> 48:42.280\n but it was specifically trained to only play go, right?\n\n48:42.280 --> 48:45.360\n So, and what we wanted to do with first version of alpha go\n\n48:45.360 --> 48:47.520\n is just get to world champion performance\n\n48:47.520 --> 48:49.200\n no matter how we did it, right?\n\n48:49.200 --> 48:51.400\n And then of course, alpha go zero,\n\n48:51.400 --> 48:55.280\n we remove the need to use human games as a starting point,\n\n48:55.280 --> 48:56.120\n right?\n\n48:56.120 --> 48:57.960\n So it could just play against itself\n\n48:57.960 --> 49:00.280\n from random starting point from the beginning.\n\n49:00.280 --> 49:03.720\n So that removed the need for human knowledge about go.\n\n49:03.720 --> 49:05.960\n And then finally alpha zero then generalized it\n\n49:05.960 --> 49:08.920\n so that any things we had in there, the system,\n\n49:08.920 --> 49:12.240\n including things like symmetry of the go board were removed.\n\n49:12.240 --> 49:14.600\n So the alpha zero could play from scratch\n\n49:14.600 --> 49:16.440\n any two player game and then mu zero,\n\n49:16.440 --> 49:18.360\n which is the final, our latest version\n\n49:18.360 --> 49:20.680\n of that set of things was then extending it\n\n49:20.680 --> 49:22.120\n so that you didn't even have to give it\n\n49:22.120 --> 49:23.200\n the rules of the game.\n\n49:23.200 --> 49:24.880\n It would learn that for itself.\n\n49:24.880 --> 49:26.600\n So it could also deal with computer games\n\n49:26.600 --> 49:27.760\n as well as board games.\n\n49:27.760 --> 49:30.400\n So that line of alpha go, alpha go zero, alpha zero,\n\n49:30.400 --> 49:33.480\n mu zero, that's the full trajectory\n\n49:33.480 --> 49:37.200\n of what you can take from imitation learning\n\n49:37.200 --> 49:40.440\n to full self supervised learning.\n\n49:40.440 --> 49:41.640\n Yeah, exactly.\n\n49:41.640 --> 49:44.720\n And learning the entire structure\n\n49:44.720 --> 49:47.640\n of the environment you're put in from scratch, right?\n\n49:47.640 --> 49:51.840\n And bootstrapping it through self play yourself.\n\n49:51.840 --> 49:53.720\n But the thing is it would have been impossible, I think,\n\n49:53.720 --> 49:55.960\n or very hard for us to build alpha zero\n\n49:55.960 --> 49:58.600\n or mu zero first out of the box.\n\n49:58.600 --> 50:01.400\n Even psychologically, because you have to believe\n\n50:01.400 --> 50:03.040\n in yourself for a very long time.\n\n50:03.040 --> 50:04.640\n You're constantly dealing with doubt\n\n50:04.640 --> 50:06.680\n because a lot of people say that it's impossible.\n\n50:06.680 --> 50:08.640\n Exactly, so it's hard enough just to do go.\n\n50:08.640 --> 50:10.920\n As you were saying, everyone thought that was impossible\n\n50:10.920 --> 50:14.160\n or at least a decade away from when we did it\n\n50:14.160 --> 50:17.320\n back in 2015, 2016.\n\n50:17.320 --> 50:20.960\n And so yes, it would have been psychologically\n\n50:20.960 --> 50:22.960\n probably very difficult as well as the fact\n\n50:22.960 --> 50:26.400\n that of course we learn a lot by building alpha go first.\n\n50:26.400 --> 50:28.520\n Right, so I think this is why I call AI\n\n50:28.520 --> 50:29.880\n an engineering science.\n\n50:29.880 --> 50:32.280\n It's one of the most fascinating science disciplines,\n\n50:32.280 --> 50:34.200\n but it's also an engineering science in the sense\n\n50:34.200 --> 50:38.200\n that unlike natural sciences, the phenomenon you're studying\n\n50:38.200 --> 50:39.440\n doesn't exist out in nature.\n\n50:39.440 --> 50:40.880\n You have to build it first.\n\n50:40.880 --> 50:42.480\n So you have to build the artifact first,\n\n50:42.480 --> 50:46.480\n and then you can study and pull it apart and how it works.\n\n50:46.480 --> 50:50.000\n This is tough to ask you this question\n\n50:50.000 --> 50:51.480\n because you probably will say it's everything,\n\n50:51.480 --> 50:54.360\n but let's try to think through this\n\n50:54.360 --> 50:56.480\n because you're in a very interesting position\n\n50:56.480 --> 50:59.520\n where DeepMind is a place of some of the most brilliant\n\n50:59.520 --> 51:01.760\n ideas in the history of AI,\n\n51:01.760 --> 51:04.600\n but it's also a place of brilliant engineering.\n\n51:05.880 --> 51:08.040\n So how much of solving intelligence,\n\n51:08.040 --> 51:09.880\n this big goal for DeepMind,\n\n51:09.880 --> 51:12.120\n how much of it is science?\n\n51:12.120 --> 51:13.320\n How much is engineering?\n\n51:13.320 --> 51:14.720\n So how much is the algorithms?\n\n51:14.720 --> 51:16.160\n How much is the data?\n\n51:16.160 --> 51:19.840\n How much is the hardware compute infrastructure?\n\n51:19.840 --> 51:22.800\n How much is it the software compute infrastructure?\n\n51:23.960 --> 51:24.800\n What else is there?\n\n51:24.800 --> 51:27.200\n How much is the human infrastructure?\n\n51:27.200 --> 51:30.280\n And like just the humans interacting certain kinds of ways\n\n51:30.280 --> 51:31.720\n in all the space of all those ideas.\n\n51:31.720 --> 51:33.640\n And how much is maybe like philosophy?\n\n51:33.640 --> 51:35.160\n How much, what's the key?\n\n51:35.160 --> 51:40.160\n If you were to sort of look back,\n\n51:40.680 --> 51:43.200\n like if we go forward 200 years and look back,\n\n51:43.200 --> 51:46.320\n what was the key thing that solved intelligence?\n\n51:46.320 --> 51:47.800\n Is it the ideas or the engineering?\n\n51:47.800 --> 51:49.040\n I think it's a combination.\n\n51:49.040 --> 51:49.880\n First of all, of course,\n\n51:49.880 --> 51:51.360\n it's a combination of all those things,\n\n51:51.360 --> 51:54.760\n but the ratios of them changed over time.\n\n51:54.760 --> 51:57.480\n So even in the last 12 years,\n\n51:57.480 --> 51:59.400\n so we started DeepMind in 2010,\n\n51:59.400 --> 52:01.920\n which is hard to imagine now because 2010,\n\n52:01.920 --> 52:03.400\n it's only 12 short years ago,\n\n52:03.400 --> 52:05.600\n but nobody was talking about AI.\n\n52:05.600 --> 52:07.600\n I don't know if you remember back to your MIT days,\n\n52:07.600 --> 52:08.720\n no one was talking about it.\n\n52:08.720 --> 52:11.080\n I did a postdoc at MIT back around then.\n\n52:11.080 --> 52:12.880\n And it was sort of thought of as a,\n\n52:12.880 --> 52:14.200\n well, look, we know AI doesn't work.\n\n52:14.200 --> 52:17.040\n We tried this hard in the 90s at places like MIT,\n\n52:17.040 --> 52:19.880\n mostly using logic systems and old fashioned,\n\n52:19.880 --> 52:22.600\n sort of good old fashioned AI, we would call it now.\n\n52:22.600 --> 52:25.320\n People like Minsky and Patrick Winston,\n\n52:25.320 --> 52:26.720\n and you know all these characters, right?\n\n52:26.720 --> 52:28.280\n And used to debate a few of them.\n\n52:28.280 --> 52:30.120\n And they used to think I was mad thinking about\n\n52:30.120 --> 52:32.360\n that some new advance could be done with learning systems.\n\n52:32.360 --> 52:34.720\n And I was actually pleased to hear that\n\n52:34.720 --> 52:36.960\n because at least you know you're on a unique track\n\n52:36.960 --> 52:37.840\n at that point, right?\n\n52:37.840 --> 52:41.880\n Even if all of your professors are telling you you're mad.\n\n52:41.880 --> 52:43.840\n And of course in industry,\n\n52:43.840 --> 52:47.680\n we couldn't get, it was difficult to get two cents together,\n\n52:47.680 --> 52:48.920\n which is hard to imagine now as well,\n\n52:48.920 --> 52:51.560\n given that it's the biggest sort of buzzword in VCs\n\n52:51.560 --> 52:54.720\n and fundraisings easy and all these kinds of things today.\n\n52:54.720 --> 52:57.720\n So back in 2010, it was very difficult.\n\n52:57.720 --> 52:59.360\n And the reason we started then,\n\n52:59.360 --> 53:02.480\n and Shane and I used to discuss\n\n53:02.480 --> 53:04.920\n what were the sort of founding tenants of DeepMind.\n\n53:04.920 --> 53:06.120\n And it was various things.\n\n53:06.120 --> 53:08.680\n One was algorithmic advances.\n\n53:08.680 --> 53:09.760\n So deep learning, you know,\n\n53:09.760 --> 53:12.360\n Jeff Hinton and Co had just sort of invented that\n\n53:12.360 --> 53:15.200\n in academia, but no one in industry knew about it.\n\n53:15.200 --> 53:16.640\n We love reinforcement learning.\n\n53:16.640 --> 53:18.240\n We thought that could be scaled up.\n\n53:18.240 --> 53:20.160\n But also understanding about the human brain\n\n53:20.160 --> 53:23.920\n had advanced quite a lot in the decade prior\n\n53:23.920 --> 53:25.440\n with fMRI machines and other things.\n\n53:25.440 --> 53:28.840\n So we could get some good hints about architectures\n\n53:28.840 --> 53:32.480\n and algorithms and sort of representations maybe\n\n53:32.480 --> 53:33.400\n that the brain uses.\n\n53:33.400 --> 53:36.920\n So at a systems level, not at a implementation level.\n\n53:37.760 --> 53:41.040\n And then the other big things were compute and GPUs, right?\n\n53:41.040 --> 53:44.160\n So we could see a compute was going to be really useful\n\n53:44.160 --> 53:46.960\n and had got to a place where it become commoditized\n\n53:46.960 --> 53:48.560\n mostly through the games industry\n\n53:48.560 --> 53:50.760\n and that could be taken advantage of.\n\n53:50.760 --> 53:52.800\n And then the final thing was also mathematical\n\n53:52.800 --> 53:54.960\n and theoretical definitions of intelligence.\n\n53:54.960 --> 53:57.560\n So things like AIXI, AIXE,\n\n53:57.560 --> 54:00.160\n which Shane worked on with his supervisor, Marcus Hutter,\n\n54:00.160 --> 54:03.360\n which is this sort of theoretical proof really\n\n54:03.360 --> 54:05.280\n of universal intelligence,\n\n54:05.280 --> 54:08.000\n which is actually a reinforcement learning system\n\n54:08.000 --> 54:08.840\n in the limit.\n\n54:08.840 --> 54:10.640\n I mean, it assumes infinite compute and infinite memory\n\n54:10.640 --> 54:12.920\n in the way, you know, like a Turing machine proves.\n\n54:12.920 --> 54:15.840\n But I was also waiting to see something like that too,\n\n54:15.840 --> 54:19.440\n to, you know, like Turing machines and computation theory\n\n54:19.440 --> 54:21.520\n that people like Turing and Shannon came up with\n\n54:21.520 --> 54:23.680\n underpins modern computer science.\n\n54:24.800 --> 54:26.400\n You know, I was waiting for a theory like that\n\n54:26.400 --> 54:28.880\n to sort of underpin AGI research.\n\n54:28.880 --> 54:30.120\n So when I, you know, met Shane\n\n54:30.120 --> 54:32.000\n and saw he was working on something like that,\n\n54:32.000 --> 54:33.680\n you know, that to me was a sort of final piece\n\n54:33.680 --> 54:34.560\n of the jigsaw.\n\n54:34.560 --> 54:38.320\n So in the early days, I would say that ideas\n\n54:38.320 --> 54:40.040\n were the most important.\n\n54:40.040 --> 54:42.440\n You know, for us, it was deep reinforcement learning,\n\n54:42.440 --> 54:44.600\n scaling up deep learning.\n\n54:44.600 --> 54:46.240\n Of course, we've seen transformers.\n\n54:46.240 --> 54:48.920\n So huge leaps, I would say, you know, three or four\n\n54:48.920 --> 54:51.520\n from, if you think from 2010 till now,\n\n54:51.520 --> 54:53.680\n huge evolutions, things like AlphaGo.\n\n54:53.680 --> 54:57.920\n And maybe there's a few more still needed.\n\n54:57.920 --> 55:01.120\n But as we get closer to AI, AGI,\n\n55:02.000 --> 55:04.600\n I think engineering becomes more and more important\n\n55:04.600 --> 55:07.800\n and data because scale and of course the recent,\n\n55:07.800 --> 55:10.440\n you know, results of GPT3 and all the big language models\n\n55:10.440 --> 55:12.800\n and large models, including our ones,\n\n55:12.800 --> 55:16.000\n has shown that scale and large models\n\n55:16.000 --> 55:18.080\n are clearly gonna be a necessary,\n\n55:18.080 --> 55:21.960\n but perhaps not sufficient part of an AGI solution.\n\n55:21.960 --> 55:24.560\n And throughout that, like you said,\n\n55:24.560 --> 55:26.720\n and I'd like to give you a big thank you.\n\n55:26.720 --> 55:30.640\n You're one of the pioneers in this is sticking by ideas\n\n55:30.640 --> 55:33.480\n like reinforcement learning, that this can actually work\n\n55:34.560 --> 55:38.480\n given actually limited success in the past.\n\n55:38.480 --> 55:41.480\n And also, which we still don't know,\n\n55:41.480 --> 55:46.480\n but proudly having the best researchers in the world\n\n55:46.760 --> 55:49.400\n and talking about solving intelligence.\n\n55:49.400 --> 55:50.920\n So talking about whatever you call it,\n\n55:50.920 --> 55:54.720\n AGI or something like this, speaking of MIT,\n\n55:54.720 --> 55:57.240\n that's just something you wouldn't bring up.\n\n55:57.240 --> 56:02.240\n Not maybe you did in like 40, 50 years ago,\n\n56:03.560 --> 56:08.560\n but that was, AI was a place where you do tinkering,\n\n56:09.320 --> 56:12.560\n very small scale, not very ambitious projects.\n\n56:12.560 --> 56:16.160\n And maybe the biggest ambitious projects\n\n56:16.160 --> 56:17.480\n were in the space of robotics\n\n56:17.480 --> 56:19.200\n and doing like the DARPA challenge.\n\n56:19.200 --> 56:23.400\n But the task of solving intelligence and believing you can,\n\n56:23.400 --> 56:24.560\n that's really, really powerful.\n\n56:24.560 --> 56:27.680\n So in order for engineering to do its work,\n\n56:27.680 --> 56:30.960\n to have great engineers, build great systems,\n\n56:30.960 --> 56:32.360\n you have to have that belief,\n\n56:32.360 --> 56:33.920\n that threads throughout the whole thing\n\n56:33.920 --> 56:35.040\n that you can actually solve\n\n56:35.040 --> 56:36.640\n some of these impossible challenges.\n\n56:36.640 --> 56:37.480\n Yeah, that's right.\n\n56:37.480 --> 56:42.280\n And back in 2010, our mission statement and still is today,\n\n56:42.280 --> 56:45.600\n it was used to be solving step one, solve intelligence,\n\n56:45.600 --> 56:47.520\n step two, use it to solve everything else.\n\n56:47.520 --> 56:51.120\n So if you can imagine pitching that to a VC in 2010,\n\n56:51.120 --> 56:52.680\n the kind of looks we got,\n\n56:52.680 --> 56:55.880\n we managed to find a few kooky people to back us,\n\n56:55.880 --> 56:57.680\n but it was tricky.\n\n56:57.680 --> 57:00.160\n And it got to the point where we wouldn't mention it\n\n57:00.160 --> 57:03.120\n to any of our professors because they would just eye roll\n\n57:03.120 --> 57:05.760\n and think we committed career suicide.\n\n57:05.760 --> 57:10.040\n And so it was, there's a lot of things that we had to do,\n\n57:10.040 --> 57:11.560\n but we always believed it.\n\n57:11.560 --> 57:13.240\n And one reason, by the way,\n\n57:13.240 --> 57:16.160\n one reason I've always believed in reinforcement learning\n\n57:16.160 --> 57:19.120\n is that if you look at neuroscience,\n\n57:19.120 --> 57:22.720\n that is the way that the primate brain learns.\n\n57:22.720 --> 57:24.880\n One of the main mechanisms is the dopamine system\n\n57:24.880 --> 57:26.440\n implements some form of TD learning.\n\n57:26.440 --> 57:28.600\n It was a very famous result in the late 90s\n\n57:29.680 --> 57:31.320\n where they saw this in monkeys\n\n57:31.320 --> 57:34.520\n and as a propagating prediction error.\n\n57:34.520 --> 57:36.800\n So again, in the limit,\n\n57:36.800 --> 57:39.480\n this is what I think you can use neuroscience for is,\n\n57:39.480 --> 57:43.160\n at mathematics, when you're doing something as ambitious\n\n57:43.160 --> 57:44.560\n as trying to solve intelligence\n\n57:44.560 --> 57:47.760\n and it's blue sky research, no one knows how to do it,\n\n57:47.760 --> 57:50.160\n you need to use any evidence\n\n57:50.160 --> 57:52.120\n or any source of information you can\n\n57:52.120 --> 57:54.280\n to help guide you in the right direction\n\n57:54.280 --> 57:56.680\n or give you confidence you're going in the right direction.\n\n57:56.680 --> 57:59.840\n So that was one reason we pushed so hard on that.\n\n57:59.840 --> 58:01.840\n And just going back to your earlier question\n\n58:01.840 --> 58:04.280\n about organization, the other big thing\n\n58:04.280 --> 58:06.000\n that I think we innovated with at DeepMind\n\n58:06.000 --> 58:10.320\n to encourage invention and innovation\n\n58:10.320 --> 58:12.920\n was the multidisciplinary organization we built\n\n58:12.920 --> 58:14.160\n and we still have today.\n\n58:14.160 --> 58:16.680\n So DeepMind originally was a confluence\n\n58:16.680 --> 58:19.400\n of the most cutting edge knowledge in neuroscience\n\n58:19.400 --> 58:22.840\n with machine learning, engineering and mathematics, right?\n\n58:22.840 --> 58:24.400\n And gaming.\n\n58:24.400 --> 58:26.760\n And then since then we've built that out even further.\n\n58:26.760 --> 58:30.280\n So we have philosophers here and ethicists,\n\n58:30.280 --> 58:33.160\n but also other types of scientists, physicists and so on.\n\n58:33.160 --> 58:35.160\n And that's what brings together,\n\n58:35.160 --> 58:38.760\n I tried to build a sort of new type of Bell Labs,\n\n58:38.760 --> 58:41.200\n but in its golden era, right?\n\n58:41.200 --> 58:45.680\n And a new expression of that to try and foster\n\n58:45.680 --> 58:48.480\n this incredible sort of innovation machine.\n\n58:48.480 --> 58:50.600\n So talking about the humans in the machine,\n\n58:50.600 --> 58:53.080\n DeepMind itself is a learning machine\n\n58:53.080 --> 58:55.600\n with lots of amazing human minds in it\n\n58:55.600 --> 58:58.920\n coming together to try and build these learning systems.\n\n59:00.360 --> 59:04.960\n If we return to the big ambitious dream of AlphaFold,\n\n59:04.960 --> 59:08.400\n that may be the early steps on a very long journey\n\n59:08.400 --> 59:13.400\n in biology, do you think the same kind of approach\n\n59:14.200 --> 59:16.400\n can use to predict the structure and function\n\n59:16.400 --> 59:18.720\n of more complex biological systems?\n\n59:18.720 --> 59:21.480\n So multi protein interaction,\n\n59:21.480 --> 59:24.400\n and then, I mean, you can go out from there,\n\n59:24.400 --> 59:26.920\n just simulating bigger and bigger systems\n\n59:26.920 --> 59:29.560\n that eventually simulate something like the human brain\n\n59:29.560 --> 59:32.560\n or the human body, just the big mush,\n\n59:32.560 --> 59:36.480\n the mess of the beautiful, resilient mess of biology.\n\n59:36.480 --> 59:39.600\n Do you see that as a long term vision?\n\n59:39.600 --> 59:42.560\n I do, and I think, if you think about\n\n59:42.560 --> 59:45.680\n what are the top things I wanted to apply AI to\n\n59:45.680 --> 59:47.680\n once we had powerful enough systems,\n\n59:47.680 --> 59:52.240\n biology and curing diseases and understanding biology\n\n59:52.240 --> 59:54.120\n was right up there, top of my list.\n\n59:54.120 --> 59:56.760\n That's one of the reasons I personally pushed that myself\n\n59:56.760 --> 59:59.240\n and with AlphaFold, but I think AlphaFold,\n\n1:00:00.200 --> 1:00:03.000\n amazing as it is, is just the beginning.\n\n1:00:03.000 --> 1:00:07.160\n And I hope it's evidence of what could be done\n\n1:00:07.160 --> 1:00:08.800\n with computational methods.\n\n1:00:08.800 --> 1:00:12.200\n So AlphaFold solved this huge problem\n\n1:00:12.200 --> 1:00:15.240\n of the structure of proteins, but biology is dynamic.\n\n1:00:15.240 --> 1:00:16.880\n So really what I imagine from here,\n\n1:00:16.880 --> 1:00:18.640\n and we're working on all these things now,\n\n1:00:18.640 --> 1:00:23.160\n is protein, protein interaction, protein ligand binding,\n\n1:00:23.160 --> 1:00:25.400\n so reacting with molecules,\n\n1:00:25.400 --> 1:00:27.640\n then you wanna build up to pathways,\n\n1:00:27.640 --> 1:00:30.000\n and then eventually a virtual cell.\n\n1:00:30.000 --> 1:00:32.680\n That's my dream, maybe in the next 10 years.\n\n1:00:32.680 --> 1:00:33.600\n And I've been talking actually\n\n1:00:33.600 --> 1:00:35.000\n to a lot of biologists, friends of mine,\n\n1:00:35.000 --> 1:00:36.760\n Paul Nurse, who runs the Crick Institute,\n\n1:00:36.760 --> 1:00:39.080\n amazing biologists, Nobel Prize winning biologists.\n\n1:00:39.080 --> 1:00:42.080\n We've been discussing for 20 years now, virtual cells.\n\n1:00:42.080 --> 1:00:44.720\n Could you build a virtual simulation of a cell?\n\n1:00:44.720 --> 1:00:46.240\n And if you could, that would be incredible\n\n1:00:46.240 --> 1:00:48.120\n for biology and disease discovery,\n\n1:00:48.120 --> 1:00:49.520\n because you could do loads of experiments\n\n1:00:49.520 --> 1:00:52.400\n on the virtual cell, and then only at the last stage,\n\n1:00:52.400 --> 1:00:53.920\n validate it in the wet lab.\n\n1:00:53.920 --> 1:00:56.400\n So you could, in terms of the search space\n\n1:00:56.400 --> 1:00:59.200\n of discovering new drugs, it takes 10 years roughly\n\n1:00:59.200 --> 1:01:03.360\n to go from identifying a target,\n\n1:01:03.360 --> 1:01:06.480\n to having a drug candidate.\n\n1:01:06.480 --> 1:01:09.760\n Maybe that could be shortened by an order of magnitude,\n\n1:01:09.760 --> 1:01:13.120\n if you could do most of that work in silico.\n\n1:01:13.120 --> 1:01:15.720\n So in order to get to a virtual cell,\n\n1:01:15.720 --> 1:01:18.320\n we have to build up understanding\n\n1:01:18.320 --> 1:01:20.760\n of different parts of biology and the interactions.\n\n1:01:20.760 --> 1:01:24.560\n And so every few years we talk about this,\n\n1:01:24.560 --> 1:01:25.600\n I talked about this with Paul.\n\n1:01:25.600 --> 1:01:27.840\n And then finally, last year after AlphaFold,\n\n1:01:27.840 --> 1:01:30.600\n I said, now's the time we can finally go for it.\n\n1:01:30.600 --> 1:01:32.360\n And AlphaFold is the first proof point\n\n1:01:32.360 --> 1:01:33.800\n that this might be possible.\n\n1:01:33.800 --> 1:01:35.920\n And he's very excited, and we have some collaborations\n\n1:01:35.920 --> 1:01:38.480\n with his lab, they're just across the road actually\n\n1:01:38.480 --> 1:01:40.960\n from us, it's wonderful being here in King's Cross\n\n1:01:40.960 --> 1:01:42.880\n with the Crick Institute across the road.\n\n1:01:42.880 --> 1:01:45.960\n And I think the next steps,\n\n1:01:45.960 --> 1:01:48.040\n I think there's gonna be some amazing advances\n\n1:01:48.040 --> 1:01:50.960\n in biology built on top of things like AlphaFold.\n\n1:01:50.960 --> 1:01:53.160\n We're already seeing that with the community doing that\n\n1:01:53.160 --> 1:01:56.000\n after we've open sourced it and released it.\n\n1:01:56.000 --> 1:02:01.000\n And I often say that I think if you think of mathematics\n\n1:02:02.360 --> 1:02:05.080\n is the perfect description language for physics,\n\n1:02:05.080 --> 1:02:06.920\n I think AI might be end up being\n\n1:02:06.920 --> 1:02:09.280\n the perfect description language for biology\n\n1:02:09.280 --> 1:02:13.040\n because biology is so messy, it's so emergent,\n\n1:02:13.040 --> 1:02:15.320\n so dynamic and complex.\n\n1:02:15.320 --> 1:02:16.920\n I think I find it very hard to believe\n\n1:02:16.920 --> 1:02:18.600\n we'll ever get to something as elegant\n\n1:02:18.600 --> 1:02:21.760\n as Newton's laws of motions to describe a cell, right?\n\n1:02:21.760 --> 1:02:23.600\n It's just too complicated.\n\n1:02:23.600 --> 1:02:26.160\n So I think AI is the right tool for that.\n\n1:02:26.160 --> 1:02:29.480\n So you have to start at the basic building blocks\n\n1:02:29.480 --> 1:02:31.680\n and use AI to run the simulation\n\n1:02:31.680 --> 1:02:32.880\n for all those building blocks.\n\n1:02:32.880 --> 1:02:36.040\n So have a very strong way to do prediction\n\n1:02:36.040 --> 1:02:37.800\n of what given these building blocks,\n\n1:02:37.800 --> 1:02:40.880\n what kind of biology, how the function\n\n1:02:40.880 --> 1:02:43.640\n and the evolution of that biological system.\n\n1:02:43.640 --> 1:02:45.280\n It's almost like a cellular automata,\n\n1:02:45.280 --> 1:02:47.880\n you have to run it, you can't analyze it from a high level.\n\n1:02:47.880 --> 1:02:49.840\n You have to take the basic ingredients,\n\n1:02:49.840 --> 1:02:51.960\n figure out the rules and let it run.\n\n1:02:51.960 --> 1:02:53.960\n But in this case, the rules are very difficult\n\n1:02:53.960 --> 1:02:56.200\n to figure out, you have to learn them.\n\n1:02:56.200 --> 1:02:57.040\n That's exactly it.\n\n1:02:57.040 --> 1:03:00.800\n So the biology is too complicated to figure out the rules.\n\n1:03:00.800 --> 1:03:03.600\n It's too emergent, too dynamic,\n\n1:03:03.600 --> 1:03:05.080\n say compared to a physics system,\n\n1:03:05.080 --> 1:03:07.040\n like the motion of a planet, right?\n\n1:03:07.040 --> 1:03:09.200\n And so you have to learn the rules\n\n1:03:09.200 --> 1:03:11.920\n and that's exactly the type of systems that we're building.\n\n1:03:11.920 --> 1:03:14.800\n So you mentioned you've open sourced AlphaFold\n\n1:03:14.800 --> 1:03:16.640\n and even the data involved.\n\n1:03:16.640 --> 1:03:20.040\n To me personally, also really happy\n\n1:03:20.040 --> 1:03:22.640\n and a big thank you for open sourcing Mojoko,\n\n1:03:23.520 --> 1:03:27.080\n the physics simulation engine that's often used\n\n1:03:27.080 --> 1:03:29.080\n for robotics research and so on.\n\n1:03:29.080 --> 1:03:31.120\n So I think that's a pretty gangster move.\n\n1:03:31.120 --> 1:03:36.120\n So what's the, I mean, very few companies\n\n1:03:37.200 --> 1:03:39.080\n or people do that kind of thing.\n\n1:03:39.080 --> 1:03:41.240\n What's the philosophy behind that?\n\n1:03:41.240 --> 1:03:42.920\n You know, it's a case by case basis.\n\n1:03:42.920 --> 1:03:44.040\n And in both of those cases,\n\n1:03:44.040 --> 1:03:47.360\n we felt that was the maximum benefit to humanity to do that.\n\n1:03:47.360 --> 1:03:50.040\n And the scientific community, in one case,\n\n1:03:50.040 --> 1:03:53.360\n the robotics physics community with Mojoko, so.\n\n1:03:53.360 --> 1:03:54.200\n We purchased it.\n\n1:03:54.200 --> 1:03:55.840\n We purchased it for, yes,\n\n1:03:55.840 --> 1:03:58.520\n we purchased it for the express principle to open source it.\n\n1:03:58.520 --> 1:04:02.440\n So, you know, I hope people appreciate that.\n\n1:04:02.440 --> 1:04:04.040\n It's great to hear that you do.\n\n1:04:04.040 --> 1:04:05.800\n And then the second thing was,\n\n1:04:05.800 --> 1:04:08.040\n and mostly we did it because the person building it\n\n1:04:08.040 --> 1:04:11.920\n was not able to cope with supporting it anymore\n\n1:04:11.920 --> 1:04:13.600\n because it got too big for him.\n\n1:04:13.600 --> 1:04:16.720\n He's an amazing professor who built it in the first place.\n\n1:04:16.720 --> 1:04:18.240\n So we helped him out with that.\n\n1:04:18.240 --> 1:04:20.520\n And then with AlphaFold is even bigger, I would say.\n\n1:04:20.520 --> 1:04:21.960\n And I think in that case,\n\n1:04:21.960 --> 1:04:25.520\n we decided that there were so many downstream applications\n\n1:04:25.520 --> 1:04:29.400\n of AlphaFold that we couldn't possibly even imagine\n\n1:04:29.400 --> 1:04:30.480\n what they all were.\n\n1:04:30.480 --> 1:04:34.360\n So the best way to accelerate drug discovery\n\n1:04:34.360 --> 1:04:38.680\n and also fundamental research would be to give all\n\n1:04:38.680 --> 1:04:43.240\n that data away and the system itself.\n\n1:04:43.240 --> 1:04:45.280\n You know, it's been so gratifying to see\n\n1:04:45.280 --> 1:04:47.040\n what people have done that within just one year,\n\n1:04:47.040 --> 1:04:49.240\n which is a short amount of time in science.\n\n1:04:49.240 --> 1:04:54.160\n And it's been used by over 500,000 researchers have used it.\n\n1:04:54.160 --> 1:04:56.560\n We think that's almost every biologist in the world.\n\n1:04:56.560 --> 1:04:58.840\n I think there's roughly 500,000 biologists in the world,\n\n1:04:58.840 --> 1:05:00.000\n professional biologists,\n\n1:05:00.000 --> 1:05:03.320\n have used it to look at their proteins of interest.\n\n1:05:04.480 --> 1:05:06.520\n We've seen amazing fundamental research done.\n\n1:05:06.520 --> 1:05:09.040\n So a couple of weeks ago, front cover,\n\n1:05:09.040 --> 1:05:10.840\n there was a whole special issue of science,\n\n1:05:10.840 --> 1:05:12.040\n including the front cover,\n\n1:05:12.040 --> 1:05:14.000\n which had the nuclear pore complex on it,\n\n1:05:14.000 --> 1:05:15.800\n which is one of the biggest proteins in the body.\n\n1:05:15.800 --> 1:05:18.960\n The nuclear pore complex is a protein that governs\n\n1:05:18.960 --> 1:05:21.680\n all the nutrients going in and out of your cell nucleus.\n\n1:05:21.680 --> 1:05:24.760\n So they're like little gateways that open and close\n\n1:05:24.760 --> 1:05:27.320\n to let things go in and out of your cell nucleus.\n\n1:05:27.320 --> 1:05:29.400\n So they're really important, but they're huge\n\n1:05:29.400 --> 1:05:31.680\n because they're massive donut ring shaped things.\n\n1:05:31.680 --> 1:05:33.440\n And they've been looking to try and figure out\n\n1:05:33.440 --> 1:05:34.960\n that structure for decades.\n\n1:05:34.960 --> 1:05:37.160\n And they have lots of experimental data,\n\n1:05:37.160 --> 1:05:39.600\n but it's too low resolution, there's bits missing.\n\n1:05:39.600 --> 1:05:43.080\n And they were able to, like a giant Lego jigsaw puzzle,\n\n1:05:43.080 --> 1:05:46.200\n use alpha fold predictions plus experimental data\n\n1:05:46.200 --> 1:05:49.760\n and combined those two independent sources of information,\n\n1:05:49.760 --> 1:05:51.240\n actually four different groups around the world\n\n1:05:51.240 --> 1:05:54.600\n were able to put it together more or less simultaneously\n\n1:05:54.600 --> 1:05:56.280\n using alpha fold predictions.\n\n1:05:56.280 --> 1:05:57.720\n So that's been amazing to see.\n\n1:05:57.720 --> 1:05:59.400\n And pretty much every pharma company,\n\n1:05:59.400 --> 1:06:01.440\n every drug company executive I've spoken to\n\n1:06:01.440 --> 1:06:03.760\n has said that their teams are using alpha fold\n\n1:06:03.760 --> 1:06:08.040\n to accelerate whatever drugs they're trying to discover.\n\n1:06:08.040 --> 1:06:11.440\n So I think the knock on effect has been enormous\n\n1:06:11.440 --> 1:06:15.240\n in terms of the impact that alpha fold has made.\n\n1:06:15.240 --> 1:06:17.840\n And it's probably bringing in, it's creating biologists,\n\n1:06:17.840 --> 1:06:20.800\n it's bringing more people into the field,\n\n1:06:20.800 --> 1:06:23.320\n both on the excitement and both on the technical skills\n\n1:06:23.320 --> 1:06:28.320\n involved in, it's almost like a gateway drug to biology.\n\n1:06:28.760 --> 1:06:29.600\n Yes, it is.\n\n1:06:29.600 --> 1:06:32.640\n And to get more computational people involved too, hopefully.\n\n1:06:32.640 --> 1:06:35.920\n And I think for us, the next stage, as I said,\n\n1:06:35.920 --> 1:06:37.960\n in future we have to have other considerations too.\n\n1:06:37.960 --> 1:06:39.640\n We're building on top of alpha fold\n\n1:06:39.640 --> 1:06:41.200\n and these other ideas I discussed with you\n\n1:06:41.200 --> 1:06:44.800\n about protein interactions and genomics and other things.\n\n1:06:44.800 --> 1:06:46.200\n And not everything will be open source.\n\n1:06:46.200 --> 1:06:48.000\n Some of it we'll do commercially\n\n1:06:48.000 --> 1:06:49.000\n because that will be the best way\n\n1:06:49.000 --> 1:06:51.720\n to actually get the most resources and impact behind it.\n\n1:06:51.720 --> 1:06:53.480\n In other ways, some other projects\n\n1:06:53.480 --> 1:06:55.280\n we'll do nonprofit style.\n\n1:06:55.280 --> 1:06:58.520\n And also we have to consider for future things as well,\n\n1:06:58.520 --> 1:06:59.720\n safety and ethics as well.\n\n1:06:59.720 --> 1:07:03.600\n Like synthetic biology, there is dual use.\n\n1:07:03.600 --> 1:07:05.080\n And we have to think about that as well.\n\n1:07:05.080 --> 1:07:08.600\n With alpha fold, we consulted with 30 different bioethicists\n\n1:07:08.600 --> 1:07:10.240\n and other people expert in this field\n\n1:07:10.240 --> 1:07:13.280\n to make sure it was safe before we released it.\n\n1:07:13.280 --> 1:07:15.280\n So there'll be other considerations in future.\n\n1:07:15.280 --> 1:07:17.120\n But for right now, I think alpha fold\n\n1:07:17.120 --> 1:07:20.840\n is a kind of a gift from us to the scientific community.\n\n1:07:20.840 --> 1:07:24.200\n So I'm pretty sure that something like alpha fold\n\n1:07:25.600 --> 1:07:29.080\n will be part of Nobel prizes in the future.\n\n1:07:29.080 --> 1:07:30.840\n But us humans, of course,\n\n1:07:30.840 --> 1:07:32.480\n are horrible with credit assignment.\n\n1:07:32.480 --> 1:07:34.520\n So we'll of course give it to the humans.\n\n1:07:35.560 --> 1:07:37.440\n Do you think there will be a day\n\n1:07:37.440 --> 1:07:42.440\n when AI system can't be denied\n\n1:07:42.520 --> 1:07:45.120\n that it earned that Nobel prize?\n\n1:07:45.120 --> 1:07:47.400\n Do you think we will see that in 21st century?\n\n1:07:47.400 --> 1:07:50.200\n It depends what type of AIs we end up building, right?\n\n1:07:50.200 --> 1:07:53.600\n Whether they're goal seeking agents\n\n1:07:53.600 --> 1:07:57.800\n who specifies the goals, who comes up with the hypotheses,\n\n1:07:57.800 --> 1:08:00.320\n who determines which problems to tackle, right?\n\n1:08:00.320 --> 1:08:01.160\n So I think...\n\n1:08:01.160 --> 1:08:02.440\n And tweets about it, announcement of the results.\n\n1:08:02.440 --> 1:08:05.440\n Yes, and tweets about results exactly as part of it.\n\n1:08:05.440 --> 1:08:07.760\n So I think right now, of course,\n\n1:08:07.760 --> 1:08:12.200\n it's amazing human ingenuity that's behind these systems.\n\n1:08:12.200 --> 1:08:15.120\n And then the system, in my opinion, is just a tool.\n\n1:08:15.120 --> 1:08:18.400\n Be a bit like saying with Galileo and his telescope,\n\n1:08:18.400 --> 1:08:21.160\n the ingenuity that the credit should go to the telescope.\n\n1:08:21.160 --> 1:08:23.560\n I mean, it's clearly Galileo building the tool\n\n1:08:23.560 --> 1:08:25.160\n which he then uses.\n\n1:08:25.160 --> 1:08:27.320\n So I still see that in the same way today,\n\n1:08:27.320 --> 1:08:30.440\n even though these tools learn for themselves.\n\n1:08:30.440 --> 1:08:32.960\n There, I think of things like alpha fold\n\n1:08:32.960 --> 1:08:35.840\n and the things we're building as the ultimate tools\n\n1:08:35.840 --> 1:08:38.560\n for science and for acquiring new knowledge\n\n1:08:38.560 --> 1:08:41.160\n to help us as scientists acquire new knowledge.\n\n1:08:41.160 --> 1:08:43.200\n I think one day there will come a point\n\n1:08:43.200 --> 1:08:46.360\n where an AI system may solve\n\n1:08:46.360 --> 1:08:48.800\n or come up with something like general relativity\n\n1:08:48.800 --> 1:08:52.040\n of its own bat, not just by averaging everything\n\n1:08:52.040 --> 1:08:55.240\n on the internet or averaging everything on PubMed,\n\n1:08:55.240 --> 1:08:56.320\n although that would be interesting to see\n\n1:08:56.320 --> 1:08:58.520\n what that would come up with.\n\n1:08:58.520 --> 1:09:00.400\n So that to me is a bit like our earlier debate\n\n1:09:00.400 --> 1:09:03.240\n about creativity, you know, inventing go\n\n1:09:03.240 --> 1:09:06.280\n rather than just coming up with a good go move.\n\n1:09:06.280 --> 1:09:10.400\n And so I think solving, I think to, you know,\n\n1:09:10.400 --> 1:09:11.800\n if we wanted to give it the credit\n\n1:09:11.800 --> 1:09:13.520\n of like a Nobel type of thing,\n\n1:09:13.520 --> 1:09:15.800\n then it would need to invent go\n\n1:09:15.800 --> 1:09:19.280\n and sort of invent that new conjecture out of the blue\n\n1:09:19.280 --> 1:09:22.720\n rather than being specified by the human scientists\n\n1:09:22.720 --> 1:09:23.560\n or the human creators.\n\n1:09:23.560 --> 1:09:26.280\n So I think right now it's definitely just a tool.\n\n1:09:26.280 --> 1:09:27.880\n Although it is interesting how far you get\n\n1:09:27.880 --> 1:09:29.960\n by averaging everything on the internet, like you said,\n\n1:09:29.960 --> 1:09:33.160\n because, you know, a lot of people do see science\n\n1:09:33.160 --> 1:09:35.640\n as you're always standing on the shoulders of giants.\n\n1:09:35.640 --> 1:09:40.040\n And the question is how much are you really reaching\n\n1:09:40.040 --> 1:09:42.000\n up above the shoulders of giants?\n\n1:09:42.000 --> 1:09:44.700\n Maybe it's just simulating different kinds\n\n1:09:44.700 --> 1:09:49.360\n of results of the past with ultimately this new perspective\n\n1:09:49.360 --> 1:09:51.120\n that gives you this breakthrough idea.\n\n1:09:51.120 --> 1:09:54.860\n But that idea may not be novel in the way\n\n1:09:54.860 --> 1:09:56.740\n that it can't be already discovered on the internet.\n\n1:09:56.740 --> 1:10:00.080\n Maybe the Nobel prizes of the next 100 years\n\n1:10:00.080 --> 1:10:03.040\n are already all there on the internet to be discovered.\n\n1:10:03.040 --> 1:10:04.560\n They could be, they could be.\n\n1:10:04.560 --> 1:10:08.560\n I mean, I think this is one of the big mysteries,\n\n1:10:08.560 --> 1:10:11.720\n I think is that I, first of all,\n\n1:10:11.720 --> 1:10:13.760\n I believe a lot of the big new breakthroughs\n\n1:10:13.760 --> 1:10:15.280\n that are gonna come in the next few decades\n\n1:10:15.280 --> 1:10:17.400\n and even in the last decade are gonna come\n\n1:10:17.400 --> 1:10:20.200\n at the intersection between different subject areas\n\n1:10:20.200 --> 1:10:23.480\n where there'll be some new connection that's found\n\n1:10:23.480 --> 1:10:26.180\n between what seemingly were disparate areas.\n\n1:10:26.180 --> 1:10:28.840\n And one can even think of DeepMind, as I said earlier,\n\n1:10:28.840 --> 1:10:31.720\n as a sort of interdisciplinary between neuroscience ideas\n\n1:10:31.720 --> 1:10:35.040\n and AI engineering ideas originally.\n\n1:10:35.040 --> 1:10:37.960\n And so I think there's that.\n\n1:10:37.960 --> 1:10:40.380\n And then one of the things we can't imagine today is,\n\n1:10:40.380 --> 1:10:41.720\n and one of the reasons I think people,\n\n1:10:41.720 --> 1:10:44.440\n we were so surprised by how well large models worked\n\n1:10:44.440 --> 1:10:47.900\n is that actually it's very hard for our human minds,\n\n1:10:47.900 --> 1:10:49.440\n our limited human minds to understand\n\n1:10:49.440 --> 1:10:52.020\n what it would be like to read the whole internet, right?\n\n1:10:52.020 --> 1:10:53.520\n I think we can do a thought experiment\n\n1:10:53.520 --> 1:10:54.680\n and I used to do this of like,\n\n1:10:54.680 --> 1:10:57.600\n well, what if I read the whole of Wikipedia?\n\n1:10:57.600 --> 1:10:58.440\n What would I know?\n\n1:10:58.440 --> 1:11:00.480\n And I think our minds can just about comprehend\n\n1:11:00.480 --> 1:11:01.920\n maybe what that would be like,\n\n1:11:01.920 --> 1:11:04.440\n but the whole internet is beyond comprehension.\n\n1:11:04.440 --> 1:11:07.420\n So I think we just don't understand what it would be like\n\n1:11:07.420 --> 1:11:10.320\n to be able to hold all of that in mind potentially, right?\n\n1:11:10.320 --> 1:11:12.920\n And then active at once,\n\n1:11:12.920 --> 1:11:14.520\n and then maybe what are the connections\n\n1:11:14.520 --> 1:11:15.780\n that are available there?\n\n1:11:15.780 --> 1:11:17.520\n So I think no doubt there are huge things\n\n1:11:17.520 --> 1:11:19.280\n to be discovered just like that.\n\n1:11:19.280 --> 1:11:22.280\n But I do think there is this other type of creativity\n\n1:11:22.280 --> 1:11:25.400\n of true spark of new knowledge, new idea,\n\n1:11:25.400 --> 1:11:26.680\n never thought before about,\n\n1:11:26.680 --> 1:11:29.320\n can't be averaged from things that are known,\n\n1:11:29.320 --> 1:11:32.000\n that really, of course, everything come,\n\n1:11:32.000 --> 1:11:33.680\n nobody creates in a vacuum,\n\n1:11:33.680 --> 1:11:35.420\n so there must be clues somewhere,\n\n1:11:35.420 --> 1:11:38.280\n but just a unique way of putting those things together.\n\n1:11:38.280 --> 1:11:40.480\n I think some of the greatest scientists in history\n\n1:11:40.480 --> 1:11:42.240\n have displayed that I would say,\n\n1:11:42.240 --> 1:11:45.120\n although it's very hard to know going back to their time,\n\n1:11:45.120 --> 1:11:48.120\n what was exactly known when they came up with those things.\n\n1:11:48.120 --> 1:11:52.440\n Although you're making me really think because just a thought\n\n1:11:52.440 --> 1:11:57.360\n experiment of deeply knowing a hundred Wikipedia pages.\n\n1:11:57.360 --> 1:11:59.200\n I don't think I can,\n\n1:11:59.200 --> 1:12:03.400\n I've been really impressed by Wikipedia for technical topics.\n\n1:12:03.400 --> 1:12:07.040\n So if you know a hundred pages or a thousand pages,\n\n1:12:07.040 --> 1:12:10.120\n I don't think we can truly comprehend\n\n1:12:10.120 --> 1:12:13.400\n what kind of intelligence that is.\n\n1:12:13.400 --> 1:12:14.760\n That's a pretty powerful intelligence.\n\n1:12:14.760 --> 1:12:16.120\n If you know how to use that\n\n1:12:16.120 --> 1:12:18.320\n and integrate that information correctly,\n\n1:12:18.320 --> 1:12:20.000\n I think you can go really far.\n\n1:12:20.000 --> 1:12:22.080\n You can probably construct thought experiments\n\n1:12:22.080 --> 1:12:25.840\n based on that, like simulate different ideas.\n\n1:12:25.840 --> 1:12:28.840\n So if this is true, let me run this thought experiment\n\n1:12:28.840 --> 1:12:30.160\n that maybe this is true.\n\n1:12:30.160 --> 1:12:31.360\n It's not really invention.\n\n1:12:31.360 --> 1:12:34.640\n It's like just taking literally the knowledge\n\n1:12:34.640 --> 1:12:37.240\n and using it to construct the very basic simulation\n\n1:12:37.240 --> 1:12:38.080\n of the world.\n\n1:12:38.080 --> 1:12:40.080\n I mean, some argue it's romantic in part,\n\n1:12:40.080 --> 1:12:42.400\n but Einstein would do the same kind of things\n\n1:12:42.400 --> 1:12:43.720\n with a thought experiment.\n\n1:12:43.720 --> 1:12:46.320\n Yeah, one could imagine doing that systematically\n\n1:12:46.320 --> 1:12:48.440\n across millions of Wikipedia pages,\n\n1:12:48.440 --> 1:12:50.400\n plus PubMed, all these things.\n\n1:12:50.400 --> 1:12:53.680\n I think there are many, many things to be discovered\n\n1:12:53.680 --> 1:12:55.280\n like that that are hugely useful.\n\n1:12:55.280 --> 1:12:56.200\n You could imagine,\n\n1:12:56.200 --> 1:12:58.520\n and I want us to do some of these things in material science\n\n1:12:58.520 --> 1:13:00.000\n like room temperature superconductors\n\n1:13:00.000 --> 1:13:01.560\n is something on my list one day.\n\n1:13:01.560 --> 1:13:05.000\n I'd like to have an AI system to help build\n\n1:13:05.000 --> 1:13:06.640\n better optimized batteries,\n\n1:13:06.640 --> 1:13:09.000\n all of these sort of mechanical things.\n\n1:13:09.000 --> 1:13:11.600\n I think a systematic sort of search\n\n1:13:11.600 --> 1:13:14.360\n could be guided by a model,\n\n1:13:14.360 --> 1:13:17.120\n could be extremely powerful.\n\n1:13:17.120 --> 1:13:18.160\n So speaking of which,\n\n1:13:18.160 --> 1:13:20.160\n you have a paper on nuclear fusion,\n\n1:13:21.320 --> 1:13:23.120\n magnetic control of tachymic plasmas\n\n1:13:23.120 --> 1:13:24.720\n through deep reinforcement learning.\n\n1:13:24.720 --> 1:13:29.720\n So you're seeking to solve nuclear fusion with deep RL.\n\n1:13:29.800 --> 1:13:31.840\n So it's doing control of high temperature plasmas.\n\n1:13:31.840 --> 1:13:33.520\n Can you explain this work\n\n1:13:33.520 --> 1:13:37.240\n and can AI eventually solve nuclear fusion?\n\n1:13:37.240 --> 1:13:40.200\n It's been very fun last year or two and very productive\n\n1:13:40.200 --> 1:13:43.360\n because we've been taking off a lot of my dream projects,\n\n1:13:43.360 --> 1:13:44.960\n if you like, of things that I've collected\n\n1:13:44.960 --> 1:13:46.960\n over the years of areas of science\n\n1:13:46.960 --> 1:13:48.200\n that I would like to,\n\n1:13:48.200 --> 1:13:51.200\n I think could be very transformative if we helped accelerate\n\n1:13:51.200 --> 1:13:53.600\n and really interesting problems,\n\n1:13:53.600 --> 1:13:55.760\n scientific challenges in of themselves.\n\n1:13:55.760 --> 1:13:57.040\n So this is energy.\n\n1:13:57.040 --> 1:13:58.520\n So energy, yes, exactly.\n\n1:13:58.520 --> 1:13:59.960\n So energy and climate.\n\n1:13:59.960 --> 1:14:01.760\n So we talked about disease and biology\n\n1:14:01.760 --> 1:14:04.520\n as being one of the biggest places I think AI can help with.\n\n1:14:04.520 --> 1:14:07.120\n I think energy and climate is another one.\n\n1:14:07.120 --> 1:14:09.240\n So maybe they would be my top two.\n\n1:14:09.240 --> 1:14:12.520\n And fusion is one area I think AI can help with.\n\n1:14:12.520 --> 1:14:15.360\n Now, fusion has many challenges,\n\n1:14:15.360 --> 1:14:17.240\n mostly physics and material science\n\n1:14:17.240 --> 1:14:18.600\n and engineering challenges as well\n\n1:14:18.600 --> 1:14:20.520\n to build these massive fusion reactors\n\n1:14:20.520 --> 1:14:21.920\n and contain the plasma.\n\n1:14:21.920 --> 1:14:22.760\n And what we try to do,\n\n1:14:22.760 --> 1:14:26.280\n and whenever we go into a new field to apply our systems,\n\n1:14:26.280 --> 1:14:29.240\n is we look for, we talk to domain experts.\n\n1:14:29.240 --> 1:14:30.680\n We try and find the best people in the world\n\n1:14:30.680 --> 1:14:31.680\n to collaborate with.\n\n1:14:33.000 --> 1:14:34.120\n In this case, in fusion,\n\n1:14:34.120 --> 1:14:36.400\n we collaborated with EPFL in Switzerland,\n\n1:14:36.400 --> 1:14:38.280\n the Swiss Technical Institute, who are amazing.\n\n1:14:38.280 --> 1:14:39.640\n They have a test reactor.\n\n1:14:39.640 --> 1:14:41.360\n They were willing to let us use,\n\n1:14:41.360 --> 1:14:43.400\n which I double checked with the team\n\n1:14:43.400 --> 1:14:46.120\n we were gonna use carefully and safely.\n\n1:14:46.120 --> 1:14:47.760\n I was impressed they managed to persuade them\n\n1:14:47.760 --> 1:14:49.160\n to let us use it.\n\n1:14:49.160 --> 1:14:53.440\n And it's an amazing test reactor they have there.\n\n1:14:53.440 --> 1:14:57.000\n And they try all sorts of pretty crazy experiments on it.\n\n1:14:57.000 --> 1:14:59.720\n And what we tend to look at is,\n\n1:14:59.720 --> 1:15:01.760\n if we go into a new domain like fusion,\n\n1:15:01.760 --> 1:15:04.160\n what are all the bottleneck problems?\n\n1:15:04.160 --> 1:15:05.960\n Like thinking from first principles,\n\n1:15:05.960 --> 1:15:07.000\n what are all the bottleneck problems\n\n1:15:07.000 --> 1:15:09.280\n that are still stopping fusion working today?\n\n1:15:09.280 --> 1:15:12.080\n And then we look at, we get a fusion expert to tell us,\n\n1:15:12.080 --> 1:15:13.760\n and then we look at those bottlenecks\n\n1:15:13.760 --> 1:15:14.600\n and we look at the ones,\n\n1:15:14.600 --> 1:15:18.920\n which ones are amenable to our AI methods today, right?\n\n1:15:18.920 --> 1:15:22.200\n And would be interesting from a research perspective,\n\n1:15:22.200 --> 1:15:24.400\n from our point of view, from an AI point of view,\n\n1:15:24.400 --> 1:15:26.760\n and that would address one of their bottlenecks.\n\n1:15:26.760 --> 1:15:29.720\n And in this case, plasma control was perfect.\n\n1:15:29.720 --> 1:15:32.480\n So, the plasma, it's a million degrees Celsius,\n\n1:15:32.480 --> 1:15:34.640\n something like that, it's hotter than the sun.\n\n1:15:34.640 --> 1:15:37.640\n And there's obviously no material that can contain it.\n\n1:15:37.640 --> 1:15:39.440\n So, they have to be containing these magnetic,\n\n1:15:39.440 --> 1:15:42.520\n very powerful and superconducting magnetic fields.\n\n1:15:42.520 --> 1:15:43.960\n But the problem is plasma,\n\n1:15:43.960 --> 1:15:45.360\n it's pretty unstable as you imagine,\n\n1:15:45.360 --> 1:15:49.320\n you're kind of holding a mini sun, mini star in a reactor.\n\n1:15:49.320 --> 1:15:52.520\n So, you kind of want to predict ahead of time,\n\n1:15:52.520 --> 1:15:54.040\n what the plasma is gonna do.\n\n1:15:54.040 --> 1:15:56.240\n So, you can move the magnetic field\n\n1:15:56.240 --> 1:15:58.440\n within a few milliseconds,\n\n1:15:58.440 --> 1:16:00.960\n to basically contain what it's gonna do next.\n\n1:16:00.960 --> 1:16:03.160\n So, it seems like a perfect problem if you think of it\n\n1:16:03.160 --> 1:16:06.280\n for like a reinforcement learning prediction problem.\n\n1:16:06.280 --> 1:16:09.720\n So, you got controller, you're gonna move the magnetic field.\n\n1:16:09.720 --> 1:16:12.560\n And until we came along, they were doing it\n\n1:16:12.560 --> 1:16:16.720\n with traditional operational research type of controllers,\n\n1:16:16.720 --> 1:16:18.320\n which are kind of handcrafted.\n\n1:16:18.320 --> 1:16:19.160\n And the problem is, of course,\n\n1:16:19.160 --> 1:16:20.480\n they can't react in the moment\n\n1:16:20.480 --> 1:16:21.640\n to something the plasma is doing,\n\n1:16:21.640 --> 1:16:23.040\n they have to be hard coded.\n\n1:16:23.040 --> 1:16:26.040\n And again, knowing that that's normally our go to solution\n\n1:16:26.040 --> 1:16:27.960\n is we would like to learn that instead.\n\n1:16:27.960 --> 1:16:30.320\n And they also had a simulator of these plasma.\n\n1:16:30.320 --> 1:16:31.480\n So, there were lots of criteria\n\n1:16:31.480 --> 1:16:34.760\n that matched what we like to use.\n\n1:16:34.760 --> 1:16:38.440\n So, can AI eventually solve nuclear fusion?\n\n1:16:38.440 --> 1:16:39.760\n Well, so with this problem,\n\n1:16:39.760 --> 1:16:42.040\n and we published it in a nature paper last year,\n\n1:16:42.040 --> 1:16:46.160\n we held the fusion, we held the plasma in a specific shapes.\n\n1:16:46.160 --> 1:16:48.360\n So, actually, it's almost like carving the plasma\n\n1:16:48.360 --> 1:16:51.000\n into different shapes and hold it there\n\n1:16:51.000 --> 1:16:52.880\n for a record amount of time.\n\n1:16:52.880 --> 1:16:57.600\n So, that's one of the problems of fusion sort of solved.\n\n1:16:57.600 --> 1:16:59.840\n So, have a controller that's able to,\n\n1:16:59.840 --> 1:17:01.480\n no matter the shape.\n\n1:17:01.480 --> 1:17:02.360\n Contain it. Contain it.\n\n1:17:02.360 --> 1:17:04.160\n Yeah, contain it and hold it in structure.\n\n1:17:04.160 --> 1:17:05.760\n And there's different shapes that are better\n\n1:17:05.760 --> 1:17:10.080\n for the energy productions called droplets and so on.\n\n1:17:10.080 --> 1:17:11.880\n So, that was huge.\n\n1:17:11.880 --> 1:17:12.720\n And now we're looking,\n\n1:17:12.720 --> 1:17:14.400\n we're talking to lots of fusion startups\n\n1:17:14.400 --> 1:17:17.400\n to see what's the next problem we can tackle\n\n1:17:17.400 --> 1:17:19.360\n in the fusion area.\n\n1:17:19.360 --> 1:17:23.080\n So, another fascinating place in a paper titled,\n\n1:17:23.080 --> 1:17:25.120\n Pushing the Frontiers of Density Functionals\n\n1:17:25.120 --> 1:17:27.520\n by Solving the Fractional Electron Problem.\n\n1:17:27.520 --> 1:17:30.880\n So, you're taking on modeling and simulating\n\n1:17:30.880 --> 1:17:33.320\n the quantum mechanical behavior of electrons.\n\n1:17:33.320 --> 1:17:34.160\n Yes.\n\n1:17:36.040 --> 1:17:39.240\n Can you explain this work and can AI model\n\n1:17:39.240 --> 1:17:41.560\n and simulate arbitrary quantum mechanical systems\n\n1:17:41.560 --> 1:17:42.400\n in the future?\n\n1:17:42.400 --> 1:17:44.240\n Yeah, so this is another problem I've had my eye on\n\n1:17:44.240 --> 1:17:47.160\n for a decade or more,\n\n1:17:47.160 --> 1:17:51.200\n which is sort of simulating the properties of electrons.\n\n1:17:51.200 --> 1:17:54.280\n If you can do that, you can basically describe\n\n1:17:54.280 --> 1:17:58.040\n how elements and materials and substances work.\n\n1:17:58.040 --> 1:18:00.040\n So, it's kind of like fundamental\n\n1:18:00.040 --> 1:18:02.840\n if you want to advance material science.\n\n1:18:02.840 --> 1:18:05.240\n And we have Schrodinger's equation\n\n1:18:05.240 --> 1:18:06.480\n and then we have approximations\n\n1:18:06.480 --> 1:18:08.400\n to that density functional theory.\n\n1:18:08.400 --> 1:18:10.560\n These things are famous.\n\n1:18:10.560 --> 1:18:13.200\n And people try and write approximations\n\n1:18:13.200 --> 1:18:17.040\n to these functionals and kind of come up\n\n1:18:17.040 --> 1:18:19.880\n with descriptions of the electron clouds,\n\n1:18:19.880 --> 1:18:20.720\n where they're going to go,\n\n1:18:20.720 --> 1:18:22.120\n how they're going to interact\n\n1:18:22.120 --> 1:18:24.240\n when you put two elements together.\n\n1:18:24.240 --> 1:18:26.760\n And what we try to do is learn a simulation,\n\n1:18:27.680 --> 1:18:30.560\n learn a functional that will describe more chemistry,\n\n1:18:30.560 --> 1:18:31.760\n types of chemistry.\n\n1:18:31.760 --> 1:18:35.560\n So, until now, you can run expensive simulations,\n\n1:18:35.560 --> 1:18:38.760\n but then you can only simulate very small molecules,\n\n1:18:38.760 --> 1:18:40.160\n very simple molecules.\n\n1:18:40.160 --> 1:18:43.080\n We would like to simulate large materials.\n\n1:18:43.080 --> 1:18:45.760\n And so, today there's no way of doing that.\n\n1:18:45.760 --> 1:18:48.560\n And we're building up towards building functionals\n\n1:18:48.560 --> 1:18:51.240\n that approximate Schrodinger's equation\n\n1:18:51.240 --> 1:18:55.600\n and then allow you to describe what the electrons are doing.\n\n1:18:55.600 --> 1:18:57.480\n And all material sort of science\n\n1:18:57.480 --> 1:18:59.920\n and material properties are governed by the electrons\n\n1:18:59.920 --> 1:19:01.360\n and how they interact.\n\n1:19:01.360 --> 1:19:05.840\n So, have a good summarization of the simulation\n\n1:19:05.840 --> 1:19:07.080\n through the functional,\n\n1:19:08.720 --> 1:19:11.360\n but one that is still close\n\n1:19:11.360 --> 1:19:13.200\n to what the actual simulation would come out with.\n\n1:19:13.200 --> 1:19:16.720\n So, how difficult is that task?\n\n1:19:16.720 --> 1:19:17.760\n What's involved in that task?\n\n1:19:17.760 --> 1:19:20.720\n Is it running those complicated simulations\n\n1:19:20.720 --> 1:19:23.280\n and learning the task of mapping\n\n1:19:23.280 --> 1:19:24.560\n from the initial conditions\n\n1:19:24.560 --> 1:19:26.400\n and the parameters of the simulation,\n\n1:19:26.400 --> 1:19:27.720\n learning what the functional would be?\n\n1:19:27.720 --> 1:19:28.560\n Yeah.\n\n1:19:28.560 --> 1:19:29.440\n So, it's pretty tricky.\n\n1:19:29.440 --> 1:19:31.320\n And we've done it with,\n\n1:19:31.320 --> 1:19:35.440\n the nice thing is we can run a lot of the simulations,\n\n1:19:35.440 --> 1:19:39.080\n the molecular dynamic simulations on our compute clusters.\n\n1:19:39.080 --> 1:19:40.840\n And so, that generates a lot of data.\n\n1:19:40.840 --> 1:19:42.800\n So, in this case, the data is generated.\n\n1:19:42.800 --> 1:19:45.880\n So, we like those sort of systems and that's why we use games.\n\n1:19:45.880 --> 1:19:48.480\n It's simulated, generated data.\n\n1:19:48.480 --> 1:19:51.160\n And we can kind of create as much of it as we want, really.\n\n1:19:51.160 --> 1:19:53.280\n And just let's leave some,\n\n1:19:53.280 --> 1:19:55.280\n if any computers are free in the cloud,\n\n1:19:55.280 --> 1:19:57.680\n we just run, we run some of these calculations, right?\n\n1:19:57.680 --> 1:19:59.360\n Compute cluster calculation.\n\n1:19:59.360 --> 1:20:01.080\n I like how the free compute time\n\n1:20:01.080 --> 1:20:02.200\n is used up on quantum mechanics.\n\n1:20:02.200 --> 1:20:03.560\n Yeah, quantum mechanics, exactly.\n\n1:20:03.560 --> 1:20:06.280\n Simulations and protein simulations and other things.\n\n1:20:06.280 --> 1:20:09.880\n And so, when you're not searching on YouTube\n\n1:20:09.880 --> 1:20:11.360\n for free video, cat videos,\n\n1:20:11.360 --> 1:20:13.960\n we're using those computers usefully in quantum chemistry.\n\n1:20:13.960 --> 1:20:14.800\n It's the idea.\n\n1:20:14.800 --> 1:20:15.640\n Finally.\n\n1:20:15.640 --> 1:20:17.000\n And putting them to good use.\n\n1:20:17.000 --> 1:20:19.760\n And then, yeah, and then all of that computational data\n\n1:20:19.760 --> 1:20:20.840\n that's generated,\n\n1:20:20.840 --> 1:20:23.480\n we can then try and learn the functionals from that,\n\n1:20:23.480 --> 1:20:25.640\n which of course are way more efficient\n\n1:20:25.640 --> 1:20:27.080\n once we learn the functional\n\n1:20:27.080 --> 1:20:30.560\n than running those simulations would be.\n\n1:20:30.560 --> 1:20:33.120\n Do you think one day AI may allow us\n\n1:20:33.120 --> 1:20:36.360\n to do something like basically crack open physics?\n\n1:20:36.360 --> 1:20:39.520\n So, do something like travel faster than the speed of light?\n\n1:20:39.520 --> 1:20:41.600\n My ultimate aim is always being with AI\n\n1:20:41.600 --> 1:20:45.560\n is the reason I am personally working on AI\n\n1:20:45.560 --> 1:20:48.200\n for my whole life, it was to build a tool\n\n1:20:48.200 --> 1:20:50.360\n to help us understand the universe.\n\n1:20:50.360 --> 1:20:53.800\n So, I wanted to, and that means physics, really,\n\n1:20:53.800 --> 1:20:54.920\n and the nature of reality.\n\n1:20:54.920 --> 1:20:58.000\n So, I don't think we have systems\n\n1:20:58.000 --> 1:20:59.400\n that are capable of doing that yet,\n\n1:20:59.400 --> 1:21:01.000\n but when we get towards AGI,\n\n1:21:01.000 --> 1:21:02.920\n I think that's one of the first things\n\n1:21:02.920 --> 1:21:05.320\n I think we should apply AGI to.\n\n1:21:05.320 --> 1:21:07.160\n I would like to test the limits of physics\n\n1:21:07.160 --> 1:21:08.600\n and our knowledge of physics.\n\n1:21:08.600 --> 1:21:10.080\n There's so many things we don't know.\n\n1:21:10.080 --> 1:21:12.320\n This is one thing I find fascinating about science.\n\n1:21:12.320 --> 1:21:15.080\n And as a huge proponent of the scientific method\n\n1:21:15.080 --> 1:21:17.880\n as being one of the greatest ideas humanity has ever had\n\n1:21:17.880 --> 1:21:20.160\n and allowed us to progress with our knowledge,\n\n1:21:20.160 --> 1:21:22.000\n but I think as a true scientist,\n\n1:21:22.000 --> 1:21:25.200\n I think what you find is the more you find out,\n\n1:21:25.200 --> 1:21:27.040\n the more you realize we don't know.\n\n1:21:27.040 --> 1:21:29.880\n And I always think that it's surprising\n\n1:21:29.880 --> 1:21:31.880\n that more people aren't troubled.\n\n1:21:31.880 --> 1:21:34.000\n Every night I think about all these things\n\n1:21:34.000 --> 1:21:35.240\n we interact with all the time,\n\n1:21:35.240 --> 1:21:36.880\n that we have no idea how they work.\n\n1:21:36.880 --> 1:21:41.440\n Time, consciousness, gravity, life, we can't,\n\n1:21:41.440 --> 1:21:43.840\n I mean, these are all the fundamental things of nature.\n\n1:21:43.840 --> 1:21:47.320\n I think the way we don't really know what they are.\n\n1:21:47.320 --> 1:21:51.480\n To live life, we pin certain assumptions on them\n\n1:21:51.480 --> 1:21:55.240\n and kind of treat our assumptions as if they're a fact.\n\n1:21:55.240 --> 1:21:57.560\n That allows us to sort of box them off somehow.\n\n1:21:57.560 --> 1:21:59.000\n Yeah, box them off somehow.\n\n1:21:59.000 --> 1:22:02.320\n But the reality is when you think of time,\n\n1:22:02.320 --> 1:22:03.560\n you should remind yourself,\n\n1:22:03.560 --> 1:22:06.760\n you should take it off the shelf\n\n1:22:06.760 --> 1:22:09.040\n and realize like, no, we have a bunch of assumptions.\n\n1:22:09.040 --> 1:22:11.520\n There's still a lot of, there's even now a lot of debate.\n\n1:22:11.520 --> 1:22:15.520\n There's a lot of uncertainty about exactly what is time.\n\n1:22:15.520 --> 1:22:17.480\n Is there an error of time?\n\n1:22:17.480 --> 1:22:19.480\n You know, there's a lot of fundamental questions\n\n1:22:19.480 --> 1:22:21.160\n that you can't just make assumptions about.\n\n1:22:21.160 --> 1:22:26.160\n And maybe AI allows you to not put anything on the shelf.\n\n1:22:27.680 --> 1:22:28.520\n Yeah.\n\n1:22:28.520 --> 1:22:30.200\n Not make any hard assumptions\n\n1:22:30.200 --> 1:22:32.080\n and really open it up and see what's.\n\n1:22:32.080 --> 1:22:34.640\n Exactly, I think we should be truly open minded about that.\n\n1:22:34.640 --> 1:22:39.040\n And exactly that, not be dogmatic to a particular theory.\n\n1:22:39.040 --> 1:22:41.960\n It'll also allow us to build better tools,\n\n1:22:41.960 --> 1:22:44.400\n experimental tools eventually,\n\n1:22:44.400 --> 1:22:46.280\n that can then test certain theories\n\n1:22:46.280 --> 1:22:48.080\n that may not be testable today.\n\n1:22:48.080 --> 1:22:51.240\n Things about like what we spoke about at the beginning\n\n1:22:51.240 --> 1:22:53.520\n about the computational nature of the universe.\n\n1:22:53.520 --> 1:22:55.320\n How one might, if that was true,\n\n1:22:55.320 --> 1:22:57.360\n how one might go about testing that, right?\n\n1:22:57.360 --> 1:22:59.840\n And how much, you know, there are people\n\n1:22:59.840 --> 1:23:02.520\n who've conjectured people like Scott Aaronson and others\n\n1:23:02.520 --> 1:23:04.720\n about, you know, how much information\n\n1:23:04.720 --> 1:23:08.040\n can a specific plank unit of space and time\n\n1:23:08.040 --> 1:23:09.200\n contain, right?\n\n1:23:09.200 --> 1:23:11.960\n So one might be able to think about testing those ideas\n\n1:23:11.960 --> 1:23:15.360\n if you had AI helping you build\n\n1:23:15.360 --> 1:23:19.400\n some new exquisite experimental tools.\n\n1:23:19.400 --> 1:23:20.960\n This is what I imagine that, you know,\n\n1:23:20.960 --> 1:23:23.120\n many decades from now we'll be able to do.\n\n1:23:23.120 --> 1:23:25.840\n And what kind of questions can be answered\n\n1:23:25.840 --> 1:23:28.840\n through running a simulation of them?\n\n1:23:28.840 --> 1:23:30.760\n So there's a bunch of physics simulations\n\n1:23:30.760 --> 1:23:32.600\n you can imagine that could be run\n\n1:23:32.600 --> 1:23:35.760\n in some kind of efficient way,\n\n1:23:35.760 --> 1:23:38.760\n much like you're doing in the quantum simulation work.\n\n1:23:40.320 --> 1:23:42.160\n And perhaps even the origin of life.\n\n1:23:42.160 --> 1:23:45.160\n So figuring out how going even back\n\n1:23:45.160 --> 1:23:47.640\n before the work of AlphaFold begins\n\n1:23:47.640 --> 1:23:52.640\n of how this whole thing emerges from a rock.\n\n1:23:52.640 --> 1:23:53.480\n Yes.\n\n1:23:53.480 --> 1:23:54.320\n From a static thing.\n\n1:23:54.320 --> 1:23:57.040\n What do you think AI will allow us to,\n\n1:23:57.040 --> 1:23:58.920\n is that something you have your eye on?\n\n1:23:58.920 --> 1:24:01.560\n It's trying to understand the origin of life.\n\n1:24:01.560 --> 1:24:06.320\n First of all, yourself, what do you think,\n\n1:24:06.320 --> 1:24:08.760\n how the heck did life originate on Earth?\n\n1:24:08.760 --> 1:24:11.120\n Yeah, well, maybe I'll come to that in a second,\n\n1:24:11.120 --> 1:24:13.800\n but I think the ultimate use of AI\n\n1:24:13.800 --> 1:24:18.120\n is to kind of use it to accelerate science to the maximum.\n\n1:24:18.120 --> 1:24:21.040\n So I think of it a little bit\n\n1:24:21.040 --> 1:24:22.600\n like the tree of all knowledge.\n\n1:24:22.600 --> 1:24:24.160\n If you imagine that's all the knowledge there is\n\n1:24:24.160 --> 1:24:25.840\n in the universe to attain.\n\n1:24:25.840 --> 1:24:29.320\n And we sort of barely scratched the surface of that so far.\n\n1:24:29.320 --> 1:24:31.960\n And even though we've done pretty well\n\n1:24:31.960 --> 1:24:34.320\n since the enlightenment, right, as humanity.\n\n1:24:34.320 --> 1:24:36.840\n And I think AI will turbocharge all of that,\n\n1:24:36.840 --> 1:24:38.600\n like we've seen with AlphaFold.\n\n1:24:38.600 --> 1:24:41.400\n And I want to explore as much of that tree of knowledge\n\n1:24:41.400 --> 1:24:42.920\n as is possible to do.\n\n1:24:42.920 --> 1:24:46.400\n And I think that involves AI helping us\n\n1:24:46.400 --> 1:24:49.680\n with understanding or finding patterns,\n\n1:24:49.680 --> 1:24:52.200\n but also potentially designing and building new tools,\n\n1:24:52.200 --> 1:24:53.600\n experimental tools.\n\n1:24:53.600 --> 1:24:54.840\n So I think that's all,\n\n1:24:56.040 --> 1:24:58.920\n and also running simulations and learning simulations,\n\n1:24:58.920 --> 1:25:03.920\n all of that we're sort of doing at a baby steps level here.\n\n1:25:05.000 --> 1:25:08.560\n But I can imagine that in the decades to come\n\n1:25:08.560 --> 1:25:12.920\n as what's the full flourishing of that line of thinking.\n\n1:25:12.920 --> 1:25:15.160\n It's gonna be truly incredible, I would say.\n\n1:25:15.160 --> 1:25:17.320\n If I visualized this tree of knowledge,\n\n1:25:17.320 --> 1:25:20.840\n something tells me that that tree of knowledge for humans\n\n1:25:20.840 --> 1:25:24.440\n is much smaller in the set of all possible trees\n\n1:25:24.440 --> 1:25:26.600\n of knowledge, it's actually quite small\n\n1:25:26.600 --> 1:25:30.320\n given our cognitive limitations,\n\n1:25:31.480 --> 1:25:33.680\n limited cognitive capabilities,\n\n1:25:33.680 --> 1:25:35.720\n that even with the tools we build,\n\n1:25:35.720 --> 1:25:38.120\n we still won't be able to understand a lot of things.\n\n1:25:38.120 --> 1:25:41.160\n And that's perhaps what nonhuman systems\n\n1:25:41.160 --> 1:25:44.920\n might be able to reach farther, not just as tools,\n\n1:25:44.920 --> 1:25:47.200\n but in themselves understanding something\n\n1:25:47.200 --> 1:25:48.480\n that they can bring back.\n\n1:25:48.480 --> 1:25:50.200\n Yeah, it could well be.\n\n1:25:50.200 --> 1:25:51.800\n So, I mean, there's so many things\n\n1:25:51.800 --> 1:25:55.000\n that are sort of encapsulated in what you just said there.\n\n1:25:55.000 --> 1:25:58.320\n I think first of all, there's two different things.\n\n1:25:58.320 --> 1:26:00.560\n There's like, what do we understand today?\n\n1:26:00.560 --> 1:26:02.680\n What could the human mind understand?\n\n1:26:02.680 --> 1:26:06.400\n And what is the totality of what is there to be understood?\n\n1:26:06.400 --> 1:26:08.640\n And so there's three concentric,\n\n1:26:08.640 --> 1:26:10.720\n you can think of them as three larger and larger trees\n\n1:26:10.720 --> 1:26:12.880\n or exploring more branches of that tree.\n\n1:26:12.880 --> 1:26:15.960\n And I think with AI, we're gonna explore that whole lot.\n\n1:26:15.960 --> 1:26:19.120\n Now, the question is, if you think about\n\n1:26:19.120 --> 1:26:21.840\n what is the totality of what could be understood,\n\n1:26:22.680 --> 1:26:24.800\n there may be some fundamental physics reasons\n\n1:26:24.800 --> 1:26:26.280\n why certain things can't be understood,\n\n1:26:26.280 --> 1:26:29.000\n like what's outside a simulation or outside the universe.\n\n1:26:29.000 --> 1:26:32.320\n Maybe it's not understandable from within the universe.\n\n1:26:32.320 --> 1:26:34.840\n So there may be some hard constraints like that.\n\n1:26:34.840 --> 1:26:36.000\n It could be smaller constraints,\n\n1:26:36.000 --> 1:26:40.520\n like we think of space time as fundamental.\n\n1:26:40.520 --> 1:26:42.880\n Our human brains are really used to this idea\n\n1:26:42.880 --> 1:26:46.040\n of a three dimensional world with time, maybe.\n\n1:26:46.040 --> 1:26:47.760\n But our tools could go beyond that.\n\n1:26:47.760 --> 1:26:49.760\n They wouldn't have that limitation necessarily.\n\n1:26:49.760 --> 1:26:51.760\n They could think in 11 dimensions, 12 dimensions,\n\n1:26:51.760 --> 1:26:52.920\n whatever is needed.\n\n1:26:52.920 --> 1:26:55.640\n But we could still maybe understand that\n\n1:26:55.640 --> 1:26:56.720\n in several different ways.\n\n1:26:56.720 --> 1:26:59.040\n The example I always give is,\n\n1:26:59.040 --> 1:27:01.400\n when I play Garry Kasparov for speed chess,\n\n1:27:01.400 --> 1:27:04.400\n or we've talked about chess and these kinds of things,\n\n1:27:04.400 --> 1:27:07.520\n you know, if you're reasonably good at chess,\n\n1:27:07.520 --> 1:27:11.200\n you can't come up with the move Garry comes up with\n\n1:27:11.200 --> 1:27:13.320\n in his move, but he can explain it to you.\n\n1:27:13.320 --> 1:27:14.160\n And you can understand.\n\n1:27:14.160 --> 1:27:16.720\n And you can understand post hoc the reasoning.\n\n1:27:16.720 --> 1:27:19.400\n So I think there's an even further level of like,\n\n1:27:19.400 --> 1:27:21.640\n well, maybe you couldn't have invented that thing,\n\n1:27:21.640 --> 1:27:24.320\n but going back to using language again,\n\n1:27:24.320 --> 1:27:27.040\n perhaps you can understand and appreciate that.\n\n1:27:27.040 --> 1:27:28.920\n Same way that you can appreciate, you know,\n\n1:27:28.920 --> 1:27:31.120\n Vivaldi or Mozart or something without,\n\n1:27:31.120 --> 1:27:32.680\n you can appreciate the beauty of that\n\n1:27:32.680 --> 1:27:35.800\n without being able to construct it yourself, right?\n\n1:27:35.800 --> 1:27:37.400\n Invent the music yourself.\n\n1:27:37.400 --> 1:27:39.280\n So I think we see this in all forms of life.\n\n1:27:39.280 --> 1:27:42.440\n So it will be that times, you know, a million,\n\n1:27:42.440 --> 1:27:45.800\n but you can imagine also one sign of intelligence\n\n1:27:45.800 --> 1:27:49.320\n is the ability to explain things clearly and simply, right?\n\n1:27:49.320 --> 1:27:50.400\n You know, people like Richard Feynman,\n\n1:27:50.400 --> 1:27:52.400\n another one of my old time heroes used to say that, right?\n\n1:27:52.400 --> 1:27:54.480\n If you can't, you know, if you can explain it\n\n1:27:54.480 --> 1:27:57.360\n something simply, then that's the best sign,\n\n1:27:57.360 --> 1:27:58.640\n a complex topic simply,\n\n1:27:58.640 --> 1:28:00.680\n then that's one of the best signs of you understanding it.\n\n1:28:00.680 --> 1:28:01.520\n Yeah.\n\n1:28:01.520 --> 1:28:04.600\n I can see myself talking trash in the AI system in that way.\n\n1:28:04.600 --> 1:28:05.680\n Yes.\n\n1:28:05.680 --> 1:28:07.800\n It gets frustrated how dumb I am\n\n1:28:07.800 --> 1:28:09.880\n and trying to explain something to me.\n\n1:28:09.880 --> 1:28:11.600\n I was like, well, that means you're not intelligent\n\n1:28:11.600 --> 1:28:12.720\n because if you were intelligent,\n\n1:28:12.720 --> 1:28:14.440\n you'd be able to explain it simply.\n\n1:28:14.440 --> 1:28:16.720\n Yeah, of course, you know, there's also the other option.\n\n1:28:16.720 --> 1:28:19.560\n Of course, we could enhance ourselves and with our devices,\n\n1:28:19.560 --> 1:28:23.120\n we are already sort of symbiotic with our compute devices,\n\n1:28:23.120 --> 1:28:24.600\n right, with our phones and other things.\n\n1:28:24.600 --> 1:28:27.120\n And, you know, there's stuff like Neuralink and Xceptra\n\n1:28:27.120 --> 1:28:30.000\n that could advance that further.\n\n1:28:30.000 --> 1:28:33.880\n So I think there's lots of really amazing possibilities\n\n1:28:33.880 --> 1:28:35.360\n that I could foresee from here.\n\n1:28:35.360 --> 1:28:37.040\n Well, let me ask you some wild questions.\n\n1:28:37.040 --> 1:28:39.920\n So out there looking for friends,\n\n1:28:39.920 --> 1:28:43.120\n do you think there's a lot of alien civilizations out there?\n\n1:28:43.120 --> 1:28:44.960\n So I guess this also goes back\n\n1:28:44.960 --> 1:28:46.640\n to your origin of life question too,\n\n1:28:46.640 --> 1:28:48.240\n because I think that that's key.\n\n1:28:48.240 --> 1:28:51.360\n My personal opinion, looking at all this,\n\n1:28:51.360 --> 1:28:53.680\n and, you know, it's one of my hobbies, physics, I guess.\n\n1:28:53.680 --> 1:28:56.880\n So, you know, it's something I think about a lot\n\n1:28:56.880 --> 1:29:00.760\n and talk to a lot of experts on and read a lot of books on.\n\n1:29:00.760 --> 1:29:05.280\n And I think my feeling currently is that we are alone.\n\n1:29:05.280 --> 1:29:07.160\n I think that's the most likely scenario\n\n1:29:07.160 --> 1:29:08.800\n given what evidence we have.\n\n1:29:08.800 --> 1:29:13.160\n So, and the reasoning is I think that, you know,\n\n1:29:13.160 --> 1:29:16.120\n we've tried since things like SETI program\n\n1:29:16.120 --> 1:29:19.840\n and I guess since the dawning of the space age,\n\n1:29:19.840 --> 1:29:21.240\n we've, you know, had telescopes,\n\n1:29:21.240 --> 1:29:23.280\n open radio telescopes and other things.\n\n1:29:23.280 --> 1:29:27.280\n And if you think about and try to detect signals,\n\n1:29:27.280 --> 1:29:30.120\n now, if you think about the evolution of humans on earth,\n\n1:29:30.120 --> 1:29:33.960\n we could have easily been a million years ahead\n\n1:29:33.960 --> 1:29:36.280\n of our time now or million years behind,\n\n1:29:36.280 --> 1:29:39.440\n right, easily with just some slightly different quirk\n\n1:29:39.440 --> 1:29:42.120\n thing happening hundreds of thousands of years ago.\n\n1:29:42.120 --> 1:29:43.640\n You know, things could have been slightly different\n\n1:29:43.640 --> 1:29:46.200\n if the meteor would hit the dinosaurs a million years earlier,\n\n1:29:46.200 --> 1:29:48.080\n maybe things would have evolved.\n\n1:29:48.080 --> 1:29:50.920\n We'd be a million years ahead of where we are now.\n\n1:29:50.920 --> 1:29:54.080\n So what that means is if you imagine where humanity will be\n\n1:29:54.080 --> 1:29:56.720\n in a few hundred years, let alone a million years,\n\n1:29:56.720 --> 1:29:59.840\n especially if we hopefully, you know,\n\n1:29:59.840 --> 1:30:02.200\n solve things like climate change and other things,\n\n1:30:02.200 --> 1:30:05.640\n and we continue to flourish and we build things like AI\n\n1:30:05.640 --> 1:30:07.920\n and we do space traveling and all of the stuff\n\n1:30:07.920 --> 1:30:10.800\n that humans have dreamed of forever, right?\n\n1:30:10.800 --> 1:30:14.360\n And sci fi is talked about forever.\n\n1:30:14.360 --> 1:30:16.760\n We will be spreading across the stars, right?\n\n1:30:16.760 --> 1:30:19.240\n And von Neumann famously calculated, you know,\n\n1:30:19.240 --> 1:30:20.800\n it would only take about a million years\n\n1:30:20.800 --> 1:30:23.240\n if you sent out von Neumann probes to the nearest,\n\n1:30:23.240 --> 1:30:26.200\n you know, the nearest other solar systems.\n\n1:30:26.200 --> 1:30:29.040\n And then all they did was build two more versions\n\n1:30:29.040 --> 1:30:30.440\n of themselves and sent those two out\n\n1:30:30.440 --> 1:30:32.240\n to the next nearest systems.\n\n1:30:32.240 --> 1:30:33.480\n You know, within a million years,\n\n1:30:33.480 --> 1:30:35.040\n I think you would have one of these probes\n\n1:30:35.040 --> 1:30:36.920\n in every system in the galaxy.\n\n1:30:36.920 --> 1:30:40.040\n So it's not actually in cosmological time.\n\n1:30:40.040 --> 1:30:42.080\n That's actually a very short amount of time.\n\n1:30:42.080 --> 1:30:44.600\n So, and you know, people like Dyson have thought\n\n1:30:44.600 --> 1:30:47.280\n about constructing Dyson spheres around stars\n\n1:30:47.280 --> 1:30:49.800\n to collect all the energy coming out of the star.\n\n1:30:49.800 --> 1:30:51.800\n You know, there would be constructions like that\n\n1:30:51.800 --> 1:30:54.120\n would be visible across space,\n\n1:30:54.120 --> 1:30:56.000\n probably even across a galaxy.\n\n1:30:56.000 --> 1:30:57.920\n So, and then, you know, if you think about\n\n1:30:57.920 --> 1:31:00.760\n all of our radio, television emissions\n\n1:31:00.760 --> 1:31:04.200\n that have gone out since the, you know, 30s and 40s,\n\n1:31:05.120 --> 1:31:06.720\n imagine a million years of that.\n\n1:31:06.720 --> 1:31:10.000\n And now hundreds of civilizations doing that.\n\n1:31:10.000 --> 1:31:12.200\n When we opened our ears at the point\n\n1:31:12.200 --> 1:31:14.840\n we got technologically sophisticated enough\n\n1:31:14.840 --> 1:31:15.880\n in the space age,\n\n1:31:15.880 --> 1:31:19.120\n we should have heard a cacophony of voices.\n\n1:31:19.120 --> 1:31:20.920\n We should have joined that cacophony of voices.\n\n1:31:20.920 --> 1:31:24.480\n And what we did, we opened our ears and we heard nothing.\n\n1:31:24.480 --> 1:31:27.120\n And many people who argue that there are aliens\n\n1:31:27.120 --> 1:31:28.800\n would say, well, we haven't really done\n\n1:31:28.800 --> 1:31:29.920\n exhaustive search yet.\n\n1:31:29.920 --> 1:31:31.880\n And maybe we're looking in the wrong bands\n\n1:31:31.880 --> 1:31:33.760\n and we've got the wrong devices\n\n1:31:33.760 --> 1:31:36.080\n and we wouldn't notice what an alien form was like\n\n1:31:36.080 --> 1:31:38.280\n because it'd be so different to what we're used to.\n\n1:31:38.280 --> 1:31:40.640\n But, you know, I don't really buy that,\n\n1:31:40.640 --> 1:31:42.640\n that it shouldn't be as difficult as that.\n\n1:31:42.640 --> 1:31:44.320\n Like, I think we've searched enough.\n\n1:31:44.320 --> 1:31:45.680\n There should be everywhere.\n\n1:31:45.680 --> 1:31:47.280\n If it was, yeah, it should be everywhere.\n\n1:31:47.280 --> 1:31:49.240\n We should see Dyson spheres being put up,\n\n1:31:49.240 --> 1:31:50.600\n sun's blinking in and out.\n\n1:31:50.600 --> 1:31:52.000\n You know, there should be a lot of evidence\n\n1:31:52.000 --> 1:31:52.920\n for those things.\n\n1:31:52.920 --> 1:31:54.160\n And then there are other people who argue,\n\n1:31:54.160 --> 1:31:56.000\n well, the sort of safari view of like,\n\n1:31:56.000 --> 1:31:57.840\n well, we're a primitive species still\n\n1:31:57.840 --> 1:31:59.400\n because we're not space faring yet.\n\n1:31:59.400 --> 1:32:01.400\n And we're, you know, there's some kind of global,\n\n1:32:01.400 --> 1:32:03.360\n like universal rule not to interfere,\n\n1:32:03.360 --> 1:32:04.600\n you know, Star Trek rule.\n\n1:32:04.600 --> 1:32:07.360\n But like, look, we can't even coordinate humans\n\n1:32:07.360 --> 1:32:10.040\n to deal with climate change and we're one species.\n\n1:32:10.040 --> 1:32:12.400\n What is the chance that of all of these different\n\n1:32:12.400 --> 1:32:14.800\n human civilization, you know, alien civilizations,\n\n1:32:14.800 --> 1:32:16.760\n they would have the same priorities\n\n1:32:16.760 --> 1:32:20.200\n and agree across these kinds of matters.\n\n1:32:20.200 --> 1:32:21.840\n And even if that was true\n\n1:32:21.840 --> 1:32:25.040\n and we were in some sort of safari for our own good,\n\n1:32:25.040 --> 1:32:26.360\n to me, that's not much different\n\n1:32:26.360 --> 1:32:27.640\n from the simulation hypothesis\n\n1:32:27.640 --> 1:32:29.880\n because what does it mean, the simulation hypothesis?\n\n1:32:29.880 --> 1:32:31.360\n I think in its most fundamental level,\n\n1:32:31.360 --> 1:32:34.960\n it means what we're seeing is not quite reality, right?\n\n1:32:34.960 --> 1:32:37.760\n It's something, there's something more deeper underlying it,\n\n1:32:37.760 --> 1:32:39.120\n maybe computational.\n\n1:32:39.120 --> 1:32:42.600\n Now, if we were in a sort of safari park\n\n1:32:42.600 --> 1:32:44.440\n and everything we were seeing was a hologram\n\n1:32:44.440 --> 1:32:46.520\n and it was projected by the aliens or whatever,\n\n1:32:46.520 --> 1:32:47.840\n that to me is not much different\n\n1:32:47.840 --> 1:32:50.280\n than thinking we're inside of another universe\n\n1:32:50.280 --> 1:32:53.160\n because we still can't see true reality, right?\n\n1:32:53.160 --> 1:32:55.120\n I mean, there's other explanations.\n\n1:32:55.120 --> 1:32:58.000\n It could be that the way they're communicating\n\n1:32:58.000 --> 1:32:59.280\n is just fundamentally different,\n\n1:32:59.280 --> 1:33:02.440\n that we're too dumb to understand the much better methods\n\n1:33:02.440 --> 1:33:03.840\n of communication they have.\n\n1:33:03.840 --> 1:33:06.600\n It could be, I mean, it's silly to say,\n\n1:33:06.600 --> 1:33:09.960\n but our own thoughts could be the methods\n\n1:33:09.960 --> 1:33:11.200\n by which they're communicating.\n\n1:33:11.200 --> 1:33:13.240\n Like the place from which our ideas,\n\n1:33:13.240 --> 1:33:15.160\n writers talk about this, like the muse.\n\n1:33:15.160 --> 1:33:16.000\n Yeah.\n\n1:33:17.120 --> 1:33:20.880\n I mean, it sounds like very kind of wild,\n\n1:33:20.880 --> 1:33:22.160\n but it could be thoughts.\n\n1:33:22.160 --> 1:33:24.600\n It could be some interactions with our mind\n\n1:33:24.600 --> 1:33:27.840\n that we think are originating from us\n\n1:33:27.840 --> 1:33:31.440\n is actually something that is coming\n\n1:33:31.440 --> 1:33:33.040\n from other life forms elsewhere.\n\n1:33:33.040 --> 1:33:34.880\n Consciousness itself might be that.\n\n1:33:34.880 --> 1:33:37.360\n It could be, but I don't see any sensible argument\n\n1:33:37.360 --> 1:33:40.560\n to the why would all of the alien species\n\n1:33:40.560 --> 1:33:41.600\n behave in this way?\n\n1:33:41.600 --> 1:33:43.200\n Yeah, some of them will be more primitive.\n\n1:33:43.200 --> 1:33:44.920\n They will be close to our level.\n\n1:33:44.920 --> 1:33:47.760\n There should be a whole sort of normal distribution\n\n1:33:47.760 --> 1:33:48.680\n of these things, right?\n\n1:33:48.680 --> 1:33:49.640\n Some would be aggressive.\n\n1:33:49.640 --> 1:33:52.120\n Some would be curious.\n\n1:33:52.120 --> 1:33:55.560\n Others would be very historical and philosophical\n\n1:33:55.560 --> 1:33:58.080\n because maybe they're a million years older than us,\n\n1:33:58.080 --> 1:34:00.160\n but it's not, it shouldn't be like,\n\n1:34:00.160 --> 1:34:03.000\n I mean, one alien civilization might be like that,\n\n1:34:03.000 --> 1:34:04.200\n communicating thoughts and others,\n\n1:34:04.200 --> 1:34:07.720\n but I don't see why potentially the hundreds there should be\n\n1:34:07.720 --> 1:34:10.040\n would be uniform in this way, right?\n\n1:34:10.040 --> 1:34:13.040\n It could be a violent dictatorship that the people,\n\n1:34:13.040 --> 1:34:17.000\n the alien civilizations that become successful\n\n1:34:20.560 --> 1:34:23.080\n gain the ability to be destructive,\n\n1:34:23.080 --> 1:34:26.000\n an order of magnitude more destructive,\n\n1:34:26.000 --> 1:34:28.680\n but of course the sad thought,\n\n1:34:29.880 --> 1:34:32.640\n well, either humans are very special.\n\n1:34:32.640 --> 1:34:35.480\n We took a lot of leaps that arrived\n\n1:34:35.480 --> 1:34:36.880\n at what it means to be human.\n\n1:34:38.600 --> 1:34:41.160\n There's a question there, which was the hardest,\n\n1:34:41.160 --> 1:34:42.720\n which was the most special,\n\n1:34:42.720 --> 1:34:45.200\n but also if others have reached this level\n\n1:34:45.200 --> 1:34:47.680\n and maybe many others have reached this level,\n\n1:34:47.680 --> 1:34:52.680\n the great filter that prevented them from going farther\n\n1:34:52.680 --> 1:34:54.800\n to becoming a multi planetary species\n\n1:34:54.800 --> 1:34:57.520\n or reaching out into the stars.\n\n1:34:57.520 --> 1:34:59.960\n And those are really important questions for us,\n\n1:34:59.960 --> 1:35:04.720\n whether there's other alien civilizations out there or not,\n\n1:35:04.720 --> 1:35:06.960\n this is very useful for us to think about.\n\n1:35:06.960 --> 1:35:10.240\n If we destroy ourselves, how will we do it?\n\n1:35:10.240 --> 1:35:11.960\n And how easy is it to do?\n\n1:35:11.960 --> 1:35:14.160\n Yeah, well, these are big questions\n\n1:35:14.160 --> 1:35:15.320\n and I've thought about these a lot,\n\n1:35:15.320 --> 1:35:19.600\n but the interesting thing is that if we're alone,\n\n1:35:19.600 --> 1:35:22.000\n that's somewhat comforting from the great filter perspective\n\n1:35:22.000 --> 1:35:25.240\n because it probably means the great filters were passed us.\n\n1:35:25.240 --> 1:35:26.240\n And I'm pretty sure they are.\n\n1:35:26.240 --> 1:35:29.040\n So going back to your origin of life question,\n\n1:35:29.040 --> 1:35:30.560\n there are some incredible things\n\n1:35:30.560 --> 1:35:31.760\n that no one knows how happened,\n\n1:35:31.760 --> 1:35:35.240\n like obviously the first life form from chemical soup,\n\n1:35:35.240 --> 1:35:36.800\n that seems pretty hard,\n\n1:35:36.800 --> 1:35:38.720\n but I would guess the multicellular,\n\n1:35:38.720 --> 1:35:42.120\n I wouldn't be that surprised if we saw single cell\n\n1:35:42.120 --> 1:35:45.440\n sort of life forms elsewhere, bacteria type things,\n\n1:35:45.440 --> 1:35:48.000\n but multicellular life seems incredibly hard,\n\n1:35:48.000 --> 1:35:50.200\n that step of capturing mitochondria\n\n1:35:50.200 --> 1:35:53.120\n and then sort of using that as part of yourself,\n\n1:35:53.120 --> 1:35:53.960\n you know, when you've just eaten it.\n\n1:35:53.960 --> 1:35:57.560\n Would you say that's the biggest, the most,\n\n1:35:57.560 --> 1:36:01.400\n like if you had to choose one sort of,\n\n1:36:01.400 --> 1:36:04.400\n Hitchhiker's Galaxy, one sentence summary of like,\n\n1:36:04.400 --> 1:36:07.280\n oh, those clever creatures did this,\n\n1:36:07.280 --> 1:36:08.280\n that would be the multicellular.\n\n1:36:08.280 --> 1:36:10.600\n I think that was probably the one that's the biggest.\n\n1:36:10.600 --> 1:36:11.440\n I mean, there's a great book\n\n1:36:11.440 --> 1:36:14.760\n called The 10 Great Inventions of Evolution by Nick Lane,\n\n1:36:14.760 --> 1:36:17.440\n and he speculates on 10 of these, you know,\n\n1:36:17.440 --> 1:36:19.800\n what could be great filters.\n\n1:36:19.800 --> 1:36:21.000\n I think that's one.\n\n1:36:21.000 --> 1:36:23.880\n I think the advent of intelligence\n\n1:36:23.880 --> 1:36:26.360\n and conscious intelligence and in order, you know,\n\n1:36:26.360 --> 1:36:28.600\n to us to be able to do science and things like that\n\n1:36:28.600 --> 1:36:29.880\n is huge as well.\n\n1:36:29.880 --> 1:36:32.840\n I mean, it's only evolved once as far as, you know,\n\n1:36:32.840 --> 1:36:34.880\n in Earth history.\n\n1:36:34.880 --> 1:36:37.160\n So that would be a later candidate,\n\n1:36:37.160 --> 1:36:39.160\n but there's certainly for the early candidates,\n\n1:36:39.160 --> 1:36:41.440\n I think multicellular life forms is huge.\n\n1:36:41.440 --> 1:36:43.560\n By the way, what it's interesting to ask you,\n\n1:36:43.560 --> 1:36:45.760\n if you can hypothesize about\n\n1:36:45.760 --> 1:36:48.000\n what is the origin of intelligence?\n\n1:36:48.000 --> 1:36:53.000\n Is it that we started cooking meat over fire?\n\n1:36:53.640 --> 1:36:55.520\n Is it that we somehow figured out\n\n1:36:55.520 --> 1:36:58.120\n that we could be very powerful when we started collaborating?\n\n1:36:58.120 --> 1:37:03.120\n So cooperation between our ancestors\n\n1:37:03.560 --> 1:37:05.920\n so that we can overthrow the alpha male.\n\n1:37:07.040 --> 1:37:07.880\n What is it, Richard?\n\n1:37:07.880 --> 1:37:08.920\n I talked to Richard Ranham,\n\n1:37:08.920 --> 1:37:10.760\n who thinks we're all just beta males\n\n1:37:10.760 --> 1:37:13.840\n who figured out how to collaborate to defeat the one,\n\n1:37:13.840 --> 1:37:16.360\n the dictator, the authoritarian alpha male\n\n1:37:16.360 --> 1:37:18.360\n that controlled the tribe.\n\n1:37:18.360 --> 1:37:20.120\n Is there other explanation?\n\n1:37:20.120 --> 1:37:24.080\n Was there 2001 Space Odyssey type of monolith\n\n1:37:24.080 --> 1:37:25.280\n that came down to Earth?\n\n1:37:25.280 --> 1:37:27.480\n Well, I think all of those things\n\n1:37:27.480 --> 1:37:28.640\n you suggested are good candidates,\n\n1:37:28.640 --> 1:37:30.680\n fire and cooking, right?\n\n1:37:30.680 --> 1:37:35.520\n So that's clearly important for energy efficiency,\n\n1:37:35.520 --> 1:37:39.600\n cooking our meat and then being able to be more efficient\n\n1:37:39.600 --> 1:37:42.840\n about eating it and consuming the energy.\n\n1:37:42.840 --> 1:37:45.720\n I think that's huge and then utilizing fire and tools\n\n1:37:45.720 --> 1:37:48.640\n I think you're right about the tribal cooperation aspects\n\n1:37:48.640 --> 1:37:51.040\n and probably language is part of that\n\n1:37:51.040 --> 1:37:52.400\n because probably that's what allowed us\n\n1:37:52.400 --> 1:37:53.680\n to outcompete Neanderthals\n\n1:37:53.680 --> 1:37:56.160\n and perhaps less cooperative species.\n\n1:37:56.160 --> 1:37:58.760\n So that may be the case.\n\n1:37:58.760 --> 1:38:02.400\n Tool making, spears, axes, I think that let us,\n\n1:38:02.400 --> 1:38:03.920\n I mean, I think it's pretty clear now\n\n1:38:03.920 --> 1:38:05.080\n that humans were responsible\n\n1:38:05.080 --> 1:38:07.840\n for a lot of the extinctions of megafauna,\n\n1:38:07.840 --> 1:38:10.840\n especially in the Americas when humans arrived.\n\n1:38:10.840 --> 1:38:14.440\n So you can imagine once you discover tool usage\n\n1:38:14.440 --> 1:38:15.800\n how powerful that would have been\n\n1:38:15.800 --> 1:38:17.520\n and how scary for animals.\n\n1:38:17.520 --> 1:38:20.720\n So I think all of those could have been explanations for it.\n\n1:38:20.720 --> 1:38:22.760\n The interesting thing is that it's a bit\n\n1:38:22.760 --> 1:38:24.080\n like general intelligence too,\n\n1:38:24.080 --> 1:38:28.040\n is it's very costly to begin with to have a brain\n\n1:38:28.040 --> 1:38:29.520\n and especially a general purpose brain\n\n1:38:29.520 --> 1:38:30.920\n rather than a special purpose one\n\n1:38:30.920 --> 1:38:32.320\n because the amount of energy our brains use,\n\n1:38:32.320 --> 1:38:34.400\n I think it's like 20% of the body's energy\n\n1:38:34.400 --> 1:38:36.680\n and it's massive and even your thinking chest,\n\n1:38:36.680 --> 1:38:39.000\n one of the funny things that we used to say\n\n1:38:39.000 --> 1:38:41.560\n is it's as much as a racing driver uses\n\n1:38:41.560 --> 1:38:43.560\n for a whole Formula One race,\n\n1:38:43.560 --> 1:38:46.360\n just playing a game of serious high level chess,\n\n1:38:46.360 --> 1:38:49.280\n which you wouldn't think just sitting there\n\n1:38:49.280 --> 1:38:52.040\n because the brain's using so much energy.\n\n1:38:52.040 --> 1:38:54.760\n So in order for an animal, an organism to justify that,\n\n1:38:54.760 --> 1:38:57.840\n there has to be a huge payoff.\n\n1:38:57.840 --> 1:39:00.280\n And the problem with half a brain\n\n1:39:00.280 --> 1:39:05.280\n or half intelligence, say an IQs of like a monkey brain,\n\n1:39:06.720 --> 1:39:10.240\n it's not clear you can justify that evolutionary\n\n1:39:10.240 --> 1:39:12.440\n until you get to the human level brain.\n\n1:39:12.440 --> 1:39:14.720\n And so, but how do you do that jump?\n\n1:39:14.720 --> 1:39:15.560\n It's very difficult,\n\n1:39:15.560 --> 1:39:17.120\n which is why I think it has only been done once\n\n1:39:17.120 --> 1:39:19.800\n from the sort of specialized brains that you see in animals\n\n1:39:19.800 --> 1:39:22.480\n to this sort of general purpose,\n\n1:39:22.480 --> 1:39:25.040\n chewing powerful brains that humans have\n\n1:39:26.200 --> 1:39:28.920\n and which allows us to invent the modern world.\n\n1:39:29.800 --> 1:39:33.600\n And it takes a lot to cross that barrier.\n\n1:39:33.600 --> 1:39:35.600\n And I think we've seen the same with AI systems,\n\n1:39:35.600 --> 1:39:38.160\n which is that maybe until very recently,\n\n1:39:38.160 --> 1:39:40.880\n it's always been easier to craft a specific solution\n\n1:39:40.880 --> 1:39:43.040\n to a problem like chess than it has been\n\n1:39:43.040 --> 1:39:44.480\n to build a general learning system\n\n1:39:44.480 --> 1:39:46.280\n that could potentially do many things.\n\n1:39:46.280 --> 1:39:49.480\n Cause initially that system will be way worse\n\n1:39:49.480 --> 1:39:52.120\n than less efficient than the specialized system.\n\n1:39:52.120 --> 1:39:55.880\n So one of the interesting quirks of the human mind\n\n1:39:55.880 --> 1:40:00.880\n of this evolved system is that it appears to be conscious.\n\n1:40:01.320 --> 1:40:02.920\n This thing that we don't quite understand,\n\n1:40:02.920 --> 1:40:07.360\n but it seems very special is ability\n\n1:40:07.360 --> 1:40:08.760\n to have a subjective experience\n\n1:40:08.760 --> 1:40:12.280\n that it feels like something to eat a cookie,\n\n1:40:12.280 --> 1:40:14.320\n the deliciousness of it or see a color\n\n1:40:14.320 --> 1:40:15.560\n and that kind of stuff.\n\n1:40:15.560 --> 1:40:17.960\n Do you think in order to solve intelligence,\n\n1:40:17.960 --> 1:40:20.680\n we also need to solve consciousness along the way?\n\n1:40:20.680 --> 1:40:23.920\n Do you think AGI systems need to have consciousness\n\n1:40:23.920 --> 1:40:28.000\n in order to be truly intelligent?\n\n1:40:28.000 --> 1:40:29.640\n Yeah, we thought about this a lot actually.\n\n1:40:29.640 --> 1:40:33.440\n And I think that my guess is that consciousness\n\n1:40:33.440 --> 1:40:35.800\n and intelligence are double dissociable.\n\n1:40:35.800 --> 1:40:38.360\n So you can have one without the other both ways.\n\n1:40:38.360 --> 1:40:40.920\n And I think you can see that with consciousness\n\n1:40:40.920 --> 1:40:44.160\n in that I think some animals and pets,\n\n1:40:44.160 --> 1:40:46.240\n if you have a pet dog or something like that,\n\n1:40:46.240 --> 1:40:48.560\n you can see some of the higher animals and dolphins,\n\n1:40:48.560 --> 1:40:51.680\n things like that have self awareness\n\n1:40:51.680 --> 1:40:55.720\n and are very sociable, seem to dream.\n\n1:40:57.360 --> 1:40:59.000\n A lot of the traits one would regard\n\n1:40:59.000 --> 1:41:01.600\n as being kind of conscious and self aware,\n\n1:41:02.800 --> 1:41:05.080\n but yet they're not that smart, right?\n\n1:41:05.080 --> 1:41:06.320\n So they're not that intelligent\n\n1:41:06.320 --> 1:41:08.920\n by say IQ standards or something like that.\n\n1:41:08.920 --> 1:41:11.080\n Yeah, it's also possible that our understanding\n\n1:41:11.080 --> 1:41:14.920\n of intelligence is flawed, like putting an IQ to it.\n\n1:41:14.920 --> 1:41:17.360\n Maybe the thing that a dog can do\n\n1:41:17.360 --> 1:41:20.640\n is actually gone very far along the path of intelligence\n\n1:41:20.640 --> 1:41:23.240\n and we humans are just able to play chess\n\n1:41:23.240 --> 1:41:24.840\n and maybe write poems.\n\n1:41:24.840 --> 1:41:27.040\n Right, but if we go back to the idea of AGI\n\n1:41:27.040 --> 1:41:29.480\n and general intelligence, dogs are very specialized, right?\n\n1:41:29.480 --> 1:41:30.920\n Most animals are pretty specialized.\n\n1:41:30.920 --> 1:41:32.360\n They can be amazing at what they do,\n\n1:41:32.360 --> 1:41:35.600\n but they're like kind of elite sports people or something,\n\n1:41:35.600 --> 1:41:38.040\n right, so they do one thing extremely well\n\n1:41:38.040 --> 1:41:40.080\n because their entire brain is optimized.\n\n1:41:40.080 --> 1:41:41.880\n They have somehow convinced the entirety\n\n1:41:41.880 --> 1:41:44.520\n of the human population to feed them and service them.\n\n1:41:44.520 --> 1:41:46.400\n So in some way they're controlling.\n\n1:41:46.400 --> 1:41:47.240\n Yes, exactly.\n\n1:41:47.240 --> 1:41:50.120\n Well, we co evolved to some crazy degree, right?\n\n1:41:50.120 --> 1:41:53.800\n Including the way the dogs even wag their tails\n\n1:41:53.800 --> 1:41:55.160\n and twitch their noses, right?\n\n1:41:55.160 --> 1:41:57.480\n We find inextricably cute.\n\n1:41:58.640 --> 1:42:01.840\n But I think you can also see intelligence on the other side.\n\n1:42:01.840 --> 1:42:03.800\n So systems like artificial systems\n\n1:42:03.800 --> 1:42:07.240\n that are amazingly smart at certain things\n\n1:42:07.240 --> 1:42:09.800\n like maybe playing go and chess and other things,\n\n1:42:09.800 --> 1:42:13.440\n but they don't feel at all in any shape or form conscious\n\n1:42:13.440 --> 1:42:17.240\n in the way that you do to me or I do to you.\n\n1:42:17.240 --> 1:42:21.440\n And I think actually building AI\n\n1:42:21.440 --> 1:42:24.200\n is these intelligent constructs\n\n1:42:24.200 --> 1:42:25.920\n is one of the best ways to explore\n\n1:42:25.920 --> 1:42:28.000\n the mystery of consciousness, to break it down\n\n1:42:28.000 --> 1:42:31.200\n because we're gonna have devices\n\n1:42:31.200 --> 1:42:34.440\n that are pretty smart at certain things\n\n1:42:34.440 --> 1:42:36.200\n or capable at certain things,\n\n1:42:36.200 --> 1:42:39.160\n but potentially won't have any semblance\n\n1:42:39.160 --> 1:42:40.800\n of self awareness or other things.\n\n1:42:40.800 --> 1:42:43.880\n And in fact, I would advocate if there's a choice,\n\n1:42:43.880 --> 1:42:45.680\n building systems in the first place,\n\n1:42:45.680 --> 1:42:48.640\n AI systems that are not conscious to begin with\n\n1:42:48.640 --> 1:42:52.440\n are just tools until we understand them better\n\n1:42:52.440 --> 1:42:53.960\n and the capabilities better.\n\n1:42:53.960 --> 1:42:58.320\n So on that topic, just not as the CEO of DeepMind,\n\n1:42:58.320 --> 1:43:00.880\n just as a human being, let me ask you\n\n1:43:00.880 --> 1:43:03.480\n about this one particular anecdotal evidence\n\n1:43:03.480 --> 1:43:07.080\n of the Google engineer who made a comment\n\n1:43:07.080 --> 1:43:11.800\n or believed that there's some aspect of a language model,\n\n1:43:11.800 --> 1:43:15.960\n the Lambda language model that exhibited sentience.\n\n1:43:15.960 --> 1:43:18.440\n So you said you believe there might be a responsibility\n\n1:43:18.440 --> 1:43:21.120\n to build systems that are not sentient.\n\n1:43:21.120 --> 1:43:23.560\n And this experience of a particular engineer,\n\n1:43:23.560 --> 1:43:25.880\n I think I'd love to get your general opinion\n\n1:43:25.880 --> 1:43:28.000\n on this kind of thing, but I think it will happen\n\n1:43:28.000 --> 1:43:31.480\n more and more and more, which not when engineers,\n\n1:43:31.480 --> 1:43:33.120\n but when people out there that don't have\n\n1:43:33.120 --> 1:43:34.760\n an engineering background start interacting\n\n1:43:34.760 --> 1:43:37.120\n with increasingly intelligent systems,\n\n1:43:37.120 --> 1:43:38.960\n we anthropomorphize them.\n\n1:43:38.960 --> 1:43:43.960\n They start to have deep, impactful interactions with us\n\n1:43:44.680 --> 1:43:47.920\n in a way that we miss them when they're gone.\n\n1:43:47.920 --> 1:43:51.960\n And we sure as heck feel like they're living entities,\n\n1:43:51.960 --> 1:43:54.200\n self aware entities, and maybe even\n\n1:43:54.200 --> 1:43:55.960\n we project sentience onto them.\n\n1:43:55.960 --> 1:43:59.960\n So what's your thought about this particular system?\n\n1:44:01.320 --> 1:44:04.600\n Have you ever met a language model that's sentient?\n\n1:44:04.600 --> 1:44:06.320\n No, no.\n\n1:44:06.320 --> 1:44:10.200\n What do you make of the case of when you kind of feel\n\n1:44:10.200 --> 1:44:12.920\n that there's some elements of sentience to the system?\n\n1:44:12.920 --> 1:44:15.040\n Yeah, so this is an interesting question\n\n1:44:15.040 --> 1:44:17.760\n and obviously a very fundamental one.\n\n1:44:17.760 --> 1:44:20.280\n So the first thing to say is I think that none\n\n1:44:20.280 --> 1:44:22.200\n of the systems we have today, I would say,\n\n1:44:22.200 --> 1:44:25.080\n even have one iota of semblance\n\n1:44:25.080 --> 1:44:26.320\n of consciousness or sentience.\n\n1:44:26.320 --> 1:44:29.720\n That's my personal feeling interacting with them every day.\n\n1:44:29.720 --> 1:44:32.400\n So I think this way premature to be discussing\n\n1:44:32.400 --> 1:44:34.160\n what that engineer talked about.\n\n1:44:34.160 --> 1:44:36.480\n I think at the moment it's more of a projection\n\n1:44:36.480 --> 1:44:37.840\n of the way our own minds work,\n\n1:44:37.840 --> 1:44:42.840\n which is to see sort of purpose and direction\n\n1:44:43.120 --> 1:44:44.600\n in almost anything that we, you know,\n\n1:44:44.600 --> 1:44:48.200\n our brains are trained to interpret agency,\n\n1:44:48.200 --> 1:44:52.280\n basically in things, even inanimate things sometimes.\n\n1:44:52.280 --> 1:44:54.880\n And of course with a language system,\n\n1:44:54.880 --> 1:44:57.080\n because language is so fundamental to intelligence,\n\n1:44:57.080 --> 1:45:00.440\n that's going to be easy for us to anthropomorphize that.\n\n1:45:00.440 --> 1:45:03.840\n I mean, back in the day, even the first, you know,\n\n1:45:03.840 --> 1:45:05.800\n the dumbest sort of template chatbots ever,\n\n1:45:05.800 --> 1:45:09.200\n Eliza and the ilk of the original chatbots\n\n1:45:09.200 --> 1:45:11.160\n back in the sixties fooled some people\n\n1:45:11.160 --> 1:45:12.600\n under certain circumstances, right?\n\n1:45:12.600 --> 1:45:14.040\n It pretended to be a psychologist.\n\n1:45:14.040 --> 1:45:16.080\n So just basically rabbit back to you\n\n1:45:16.080 --> 1:45:18.120\n the same question you asked it back to you.\n\n1:45:19.240 --> 1:45:21.320\n And some people believe that.\n\n1:45:21.320 --> 1:45:23.280\n So I don't think we can, this is why I think\n\n1:45:23.280 --> 1:45:25.440\n the Turing test is a little bit flawed as a formal test\n\n1:45:25.440 --> 1:45:29.240\n because it depends on the sophistication of the judge,\n\n1:45:29.240 --> 1:45:33.280\n whether or not they are qualified to make that distinction.\n\n1:45:33.280 --> 1:45:36.800\n So I think we should talk to, you know,\n\n1:45:36.800 --> 1:45:38.320\n the top philosophers about this,\n\n1:45:38.320 --> 1:45:41.160\n people like Daniel Dennett and David Chalmers and others\n\n1:45:41.160 --> 1:45:43.680\n who've obviously thought deeply about consciousness.\n\n1:45:43.680 --> 1:45:46.040\n Of course, consciousness itself hasn't been well,\n\n1:45:46.040 --> 1:45:47.760\n there's no agreed definition.\n\n1:45:47.760 --> 1:45:52.160\n If I was to, you know, speculate about that, you know,\n\n1:45:52.160 --> 1:45:55.120\n I kind of, the working definition I like is\n\n1:45:55.120 --> 1:45:58.080\n it's the way information feels when it gets processed.\n\n1:45:58.080 --> 1:46:00.160\n I think maybe Max Tegmark came up with that.\n\n1:46:00.160 --> 1:46:01.040\n I like that idea.\n\n1:46:01.040 --> 1:46:02.280\n I don't know if it helps us get towards\n\n1:46:02.280 --> 1:46:03.920\n any more operational thing,\n\n1:46:03.920 --> 1:46:07.800\n but I think it's a nice way of viewing it.\n\n1:46:07.800 --> 1:46:10.000\n I think we can obviously see from neuroscience\n\n1:46:10.000 --> 1:46:11.720\n certain prerequisites that are required,\n\n1:46:11.720 --> 1:46:14.440\n like self awareness, I think is necessary,\n\n1:46:14.440 --> 1:46:16.080\n but not sufficient component.\n\n1:46:16.080 --> 1:46:18.160\n This idea of a self and other\n\n1:46:18.160 --> 1:46:20.520\n and set of coherent preferences\n\n1:46:20.520 --> 1:46:22.480\n that are coherent over time.\n\n1:46:22.480 --> 1:46:24.800\n You know, these things are maybe memory.\n\n1:46:24.800 --> 1:46:26.200\n These things are probably needed\n\n1:46:26.200 --> 1:46:29.320\n for a sentient or conscious being.\n\n1:46:29.320 --> 1:46:31.160\n But the reason, the difficult thing,\n\n1:46:31.160 --> 1:46:32.240\n I think for us when we get,\n\n1:46:32.240 --> 1:46:33.400\n and I think this is a really interesting\n\n1:46:33.400 --> 1:46:37.280\n philosophical debate is when we get closer to AGI\n\n1:46:37.280 --> 1:46:40.680\n and, you know, and much more powerful systems\n\n1:46:40.680 --> 1:46:42.240\n than we have today,\n\n1:46:42.240 --> 1:46:44.440\n how are we going to make this judgment?\n\n1:46:44.440 --> 1:46:46.960\n And one way, which is the Turing test\n\n1:46:46.960 --> 1:46:48.640\n is sort of a behavioral judgment,\n\n1:46:48.640 --> 1:46:52.080\n is the system exhibiting all the behaviors\n\n1:46:52.080 --> 1:46:56.880\n that a human sentient or a sentient being would exhibit?\n\n1:46:56.880 --> 1:46:58.160\n Is it answering the right questions?\n\n1:46:58.160 --> 1:46:59.160\n Is it saying the right things?\n\n1:46:59.160 --> 1:47:01.960\n Is it indistinguishable from a human?\n\n1:47:01.960 --> 1:47:03.360\n And so on.\n\n1:47:03.360 --> 1:47:05.760\n But I think there's a second thing\n\n1:47:05.760 --> 1:47:09.040\n that makes us as humans regard each other as sentient,\n\n1:47:09.040 --> 1:47:09.880\n right?\n\n1:47:09.880 --> 1:47:10.920\n Why do we think this?\n\n1:47:10.920 --> 1:47:12.720\n And I debated this with Daniel Dennett.\n\n1:47:12.720 --> 1:47:13.880\n And I think there's a second reason\n\n1:47:13.880 --> 1:47:15.600\n that's often overlooked,\n\n1:47:15.600 --> 1:47:18.280\n which is that we're running on the same substrate, right?\n\n1:47:18.280 --> 1:47:21.120\n So if we're exhibiting the same behavior,\n\n1:47:21.120 --> 1:47:22.680\n more or less as humans,\n\n1:47:22.680 --> 1:47:24.400\n and we're running on the same, you know,\n\n1:47:24.400 --> 1:47:26.200\n carbon based biological substrate,\n\n1:47:26.200 --> 1:47:29.560\n the squishy, you know, few pounds of flesh in our skulls,\n\n1:47:29.560 --> 1:47:32.800\n then the most parsimonious, I think, explanation\n\n1:47:32.800 --> 1:47:35.520\n is that you're feeling the same thing as I'm feeling, right?\n\n1:47:35.520 --> 1:47:37.840\n But we will never have that second part,\n\n1:47:37.840 --> 1:47:41.200\n the substrate equivalence with a machine, right?\n\n1:47:41.200 --> 1:47:43.880\n So we will have to only judge based on the behavior.\n\n1:47:43.880 --> 1:47:45.920\n And I think the substrate equivalence\n\n1:47:45.920 --> 1:47:48.200\n is a critical part of why we make assumptions\n\n1:47:48.200 --> 1:47:49.080\n that we're conscious.\n\n1:47:49.080 --> 1:47:51.680\n And in fact, even with animals, high level animals,\n\n1:47:51.680 --> 1:47:52.680\n why we think they might be,\n\n1:47:52.680 --> 1:47:54.160\n because they're exhibiting some of the behaviors\n\n1:47:54.160 --> 1:47:55.880\n we would expect from a sentient animal.\n\n1:47:55.880 --> 1:47:57.600\n And we know they're made of the same things,\n\n1:47:57.600 --> 1:47:58.640\n biological neurons.\n\n1:47:58.640 --> 1:48:02.880\n So we're gonna have to come up with explanations\n\n1:48:02.880 --> 1:48:06.320\n or models of the gap between substrate differences,\n\n1:48:06.320 --> 1:48:08.040\n between machines and humans\n\n1:48:08.040 --> 1:48:10.840\n to get anywhere beyond the behavioral.\n\n1:48:10.840 --> 1:48:12.920\n But to me, sort of the practical question\n\n1:48:12.920 --> 1:48:16.040\n is very interesting and very important.\n\n1:48:16.040 --> 1:48:18.640\n When you have millions, perhaps billions of people\n\n1:48:18.640 --> 1:48:20.800\n believing that you have a sentient AI,\n\n1:48:20.800 --> 1:48:23.000\n believing what that Google engineer believed,\n\n1:48:24.040 --> 1:48:28.760\n which I just see as an obvious, very near term future thing,\n\n1:48:28.760 --> 1:48:31.160\n certainly on the path to AGI,\n\n1:48:31.160 --> 1:48:33.160\n how does that change the world?\n\n1:48:33.160 --> 1:48:35.240\n What's the responsibility of the AI system\n\n1:48:35.240 --> 1:48:37.000\n to help those millions of people?\n\n1:48:38.160 --> 1:48:39.760\n And also what's the ethical thing?\n\n1:48:39.760 --> 1:48:44.760\n Because you can make a lot of people happy\n\n1:48:44.760 --> 1:48:48.040\n by creating a meaningful, deep experience\n\n1:48:48.040 --> 1:48:52.800\n with a system that's faking it before it makes it.\n\n1:48:52.800 --> 1:48:56.120\n And I don't, are we the right,\n\n1:48:56.120 --> 1:48:59.720\n who is to say what's the right thing to do?\n\n1:48:59.720 --> 1:49:01.920\n Should AI always be tools?\n\n1:49:01.920 --> 1:49:05.880\n Why are we constraining AI to always be tools\n\n1:49:05.880 --> 1:49:07.680\n as opposed to friends?\n\n1:49:07.680 --> 1:49:11.800\n Yeah, I think, well, I mean, these are fantastic questions\n\n1:49:11.800 --> 1:49:13.840\n and also critical ones.\n\n1:49:13.840 --> 1:49:16.240\n And we've been thinking about this\n\n1:49:16.240 --> 1:49:18.080\n since the start of DeepMind and before that,\n\n1:49:18.080 --> 1:49:19.560\n because we plan for success\n\n1:49:19.560 --> 1:49:24.640\n and however remote that looked like back in 2010.\n\n1:49:24.640 --> 1:49:26.960\n And we've always had sort of these ethical considerations\n\n1:49:26.960 --> 1:49:28.440\n as fundamental at DeepMind.\n\n1:49:29.400 --> 1:49:32.000\n And my current thinking on the language models\n\n1:49:32.000 --> 1:49:33.920\n and large models is they're not ready,\n\n1:49:33.920 --> 1:49:36.480\n we don't understand them well enough yet.\n\n1:49:36.480 --> 1:49:40.240\n And in terms of analysis tools and guard rails,\n\n1:49:40.240 --> 1:49:42.080\n what they can and can't do and so on,\n\n1:49:42.080 --> 1:49:45.440\n to deploy them at scale, because I think,\n\n1:49:45.440 --> 1:49:46.840\n there are big, still ethical questions\n\n1:49:46.840 --> 1:49:48.640\n like should an AI system always announce\n\n1:49:48.640 --> 1:49:50.600\n that it is an AI system to begin with?\n\n1:49:50.600 --> 1:49:51.520\n Probably yes.\n\n1:49:52.800 --> 1:49:55.520\n What do you do about answering those philosophical questions\n\n1:49:55.520 --> 1:49:58.800\n about the feelings people may have about AI systems,\n\n1:49:58.800 --> 1:50:00.760\n perhaps incorrectly attributed?\n\n1:50:00.760 --> 1:50:02.840\n So I think there's a whole bunch of research\n\n1:50:02.840 --> 1:50:06.040\n that needs to be done first to responsibly,\n\n1:50:06.040 --> 1:50:09.120\n before you can responsibly deploy these systems at scale.\n\n1:50:09.120 --> 1:50:12.080\n That will be at least be my current position.\n\n1:50:12.080 --> 1:50:15.080\n Over time, I'm very confident we'll have those tools\n\n1:50:15.080 --> 1:50:20.080\n like interpretability questions and analysis questions.\n\n1:50:20.680 --> 1:50:23.200\n And then with the ethical quandary,\n\n1:50:23.200 --> 1:50:28.200\n I think there it's important to look beyond just science.\n\n1:50:28.520 --> 1:50:31.440\n That's why I think philosophy, social sciences,\n\n1:50:31.440 --> 1:50:34.440\n even theology, other things like that come into it,\n\n1:50:34.440 --> 1:50:37.120\n where arts and humanities,\n\n1:50:37.120 --> 1:50:40.320\n what does it mean to be human and the spirit of being human\n\n1:50:40.320 --> 1:50:43.680\n and to enhance that and the human condition, right?\n\n1:50:43.680 --> 1:50:45.080\n And allow us to experience things\n\n1:50:45.080 --> 1:50:46.400\n we could never experience before\n\n1:50:46.400 --> 1:50:49.080\n and improve the overall human condition\n\n1:50:49.080 --> 1:50:51.640\n and humanity overall, get radical abundance,\n\n1:50:51.640 --> 1:50:54.120\n solve many scientific problems, solve disease.\n\n1:50:54.120 --> 1:50:56.560\n So this is the era I think, this is the amazing era\n\n1:50:56.560 --> 1:50:59.480\n I think we're heading into if we do it right.\n\n1:50:59.480 --> 1:51:00.800\n But we've got to be careful.\n\n1:51:00.800 --> 1:51:02.680\n We've already seen with things like social media,\n\n1:51:02.680 --> 1:51:05.920\n how dual use technologies can be misused by,\n\n1:51:05.920 --> 1:51:10.920\n firstly, by bad actors or naive actors or crazy actors,\n\n1:51:12.040 --> 1:51:14.120\n right, so there's that set of just the common\n\n1:51:14.120 --> 1:51:18.000\n or garden misuse of existing dual use technology.\n\n1:51:18.000 --> 1:51:20.960\n And then of course, there's an additional thing\n\n1:51:20.960 --> 1:51:21.960\n that has to be overcome with AI\n\n1:51:21.960 --> 1:51:24.480\n that eventually it may have its own agency.\n\n1:51:24.480 --> 1:51:28.720\n So it could be good or bad in and of itself.\n\n1:51:28.720 --> 1:51:31.480\n So I think these questions have to be approached\n\n1:51:31.480 --> 1:51:35.360\n very carefully using the scientific method, I would say,\n\n1:51:35.360 --> 1:51:38.680\n in terms of hypothesis generation, careful control testing,\n\n1:51:38.680 --> 1:51:40.680\n not live A, B testing out in the world,\n\n1:51:40.680 --> 1:51:44.400\n because with powerful technologies like AI,\n\n1:51:44.400 --> 1:51:47.640\n if something goes wrong, it may cause a lot of harm\n\n1:51:47.640 --> 1:51:49.120\n before you can fix it.\n\n1:51:49.120 --> 1:51:52.000\n It's not like an imaging app or game app\n\n1:51:52.000 --> 1:51:56.160\n where if something goes wrong, it's relatively easy to fix\n\n1:51:56.160 --> 1:51:57.960\n and the harm is relatively small.\n\n1:51:57.960 --> 1:52:02.720\n So I think it comes with the usual cliche of,\n\n1:52:02.720 --> 1:52:05.240\n like with a lot of power comes a lot of responsibility.\n\n1:52:05.240 --> 1:52:07.800\n And I think that's the case here with things like AI,\n\n1:52:07.800 --> 1:52:11.040\n given the enormous opportunity in front of us.\n\n1:52:11.040 --> 1:52:14.040\n And I think we need a lot of voices\n\n1:52:14.040 --> 1:52:17.160\n and as many inputs into things like the design\n\n1:52:17.160 --> 1:52:19.880\n of the systems and the values they should have\n\n1:52:19.880 --> 1:52:22.400\n and what goals should they be put to.\n\n1:52:22.400 --> 1:52:24.560\n I think as wide a group of voices as possible\n\n1:52:24.560 --> 1:52:27.720\n beyond just the technologists is needed to input into that\n\n1:52:27.720 --> 1:52:29.080\n and to have a say in that,\n\n1:52:29.080 --> 1:52:31.840\n especially when it comes to deployment of these systems,\n\n1:52:31.840 --> 1:52:33.440\n which is when the rubber really hits the road,\n\n1:52:33.440 --> 1:52:35.440\n it really affects the general person in the street\n\n1:52:35.440 --> 1:52:37.400\n rather than fundamental research.\n\n1:52:37.400 --> 1:52:40.240\n And that's why I say, I think as a first step,\n\n1:52:40.240 --> 1:52:42.360\n it would be better if we have the choice\n\n1:52:42.360 --> 1:52:45.120\n to build these systems as tools to give,\n\n1:52:45.120 --> 1:52:47.960\n and I'm not saying that they should never go beyond tools\n\n1:52:47.960 --> 1:52:50.360\n because of course the potential is there\n\n1:52:50.360 --> 1:52:52.960\n for it to go way beyond just tools.\n\n1:52:52.960 --> 1:52:55.800\n But I think that would be a good first step\n\n1:52:55.800 --> 1:52:58.880\n in order for us to allow us to carefully experiment\n\n1:52:58.880 --> 1:53:01.000\n and understand what these things can do.\n\n1:53:01.000 --> 1:53:05.800\n So the leap between tool, the sentient entity being\n\n1:53:05.800 --> 1:53:08.280\n is one we should take very careful of.\n\n1:53:08.280 --> 1:53:11.120\n Let me ask a dark personal question.\n\n1:53:11.120 --> 1:53:13.480\n So you're one of the most brilliant people\n\n1:53:13.480 --> 1:53:16.800\n in the AI community, you're also one of the most kind\n\n1:53:16.800 --> 1:53:20.880\n and if I may say sort of loved people in the community.\n\n1:53:20.880 --> 1:53:25.880\n That said, creation of a super intelligent AI system\n\n1:53:25.880 --> 1:53:30.880\n would be one of the most powerful things in the world,\n\n1:53:32.720 --> 1:53:34.840\n tools or otherwise.\n\n1:53:34.840 --> 1:53:38.400\n And again, as the old saying goes, power corrupts\n\n1:53:38.400 --> 1:53:40.680\n and absolute power corrupts absolutely.\n\n1:53:41.640 --> 1:53:46.640\n You are likely to be one of the people,\n\n1:53:47.280 --> 1:53:50.320\n I would say probably the most likely person\n\n1:53:50.320 --> 1:53:53.280\n to be in the control of such a system.\n\n1:53:53.280 --> 1:53:57.120\n Do you think about the corrupting nature of power\n\n1:53:57.120 --> 1:53:59.560\n when you talk about these kinds of systems\n\n1:53:59.560 --> 1:54:04.560\n that as all dictators and people have caused atrocities\n\n1:54:04.920 --> 1:54:07.760\n in the past, always think they're doing good,\n\n1:54:07.760 --> 1:54:10.400\n but they don't do good because the power\n\n1:54:10.400 --> 1:54:12.560\n has polluted their mind about what is good\n\n1:54:12.560 --> 1:54:13.720\n and what is evil.\n\n1:54:13.720 --> 1:54:14.840\n Do you think about this stuff\n\n1:54:14.840 --> 1:54:16.440\n or are we just focused on language model?\n\n1:54:16.440 --> 1:54:18.760\n No, I think about them all the time\n\n1:54:18.760 --> 1:54:22.360\n and I think what are the defenses against that?\n\n1:54:22.360 --> 1:54:24.840\n I think one thing is to remain very grounded\n\n1:54:24.840 --> 1:54:28.800\n and sort of humble, no matter what you do or achieve.\n\n1:54:28.800 --> 1:54:31.160\n And I try to do that, my best friends\n\n1:54:31.160 --> 1:54:32.200\n are still my set of friends\n\n1:54:32.200 --> 1:54:34.680\n from my undergraduate Cambridge days,\n\n1:54:34.680 --> 1:54:38.080\n my family's and friends are very important.\n\n1:54:39.280 --> 1:54:42.360\n I've always, I think trying to be a multidisciplinary person,\n\n1:54:42.360 --> 1:54:43.760\n it helps to keep you humble\n\n1:54:43.760 --> 1:54:45.880\n because no matter how good you are at one topic,\n\n1:54:45.880 --> 1:54:47.560\n someone will be better than you at that.\n\n1:54:47.560 --> 1:54:50.920\n And always relearning a new topic again from scratch\n\n1:54:50.920 --> 1:54:53.320\n is a new field is very humbling, right?\n\n1:54:53.320 --> 1:54:56.400\n So for me, that's been biology over the last five years,\n\n1:54:56.400 --> 1:55:00.200\n huge area topic and I just love doing that,\n\n1:55:00.200 --> 1:55:01.600\n but it helps to keep you grounded\n\n1:55:01.600 --> 1:55:03.120\n like it keeps you open minded.\n\n1:55:04.320 --> 1:55:06.360\n And then the other important thing\n\n1:55:06.360 --> 1:55:10.040\n is to have a really good, amazing set of people around you\n\n1:55:10.040 --> 1:55:11.840\n at your company or your organization\n\n1:55:11.840 --> 1:55:14.920\n who are also very ethical and grounded themselves\n\n1:55:14.920 --> 1:55:16.840\n and help to keep you that way.\n\n1:55:16.840 --> 1:55:18.880\n And then ultimately just to answer your question,\n\n1:55:18.880 --> 1:55:22.000\n I hope we're gonna be a big part of birthing AI\n\n1:55:22.000 --> 1:55:24.440\n and that being the greatest benefit to humanity\n\n1:55:24.440 --> 1:55:26.800\n of any tool or technology ever,\n\n1:55:26.800 --> 1:55:29.560\n and getting us into a world of radical abundance\n\n1:55:29.560 --> 1:55:33.960\n and curing diseases and solving many of the big challenges\n\n1:55:33.960 --> 1:55:34.840\n we have in front of us.\n\n1:55:34.840 --> 1:55:37.560\n And then ultimately help the ultimate flourishing\n\n1:55:37.560 --> 1:55:39.240\n of humanity to travel the stars\n\n1:55:39.240 --> 1:55:41.160\n and find those aliens if they are there.\n\n1:55:41.160 --> 1:55:43.480\n And if they're not there, find out why they're not there,\n\n1:55:43.480 --> 1:55:45.560\n what is going on here in the universe.\n\n1:55:46.520 --> 1:55:47.360\n This is all to come.\n\n1:55:47.360 --> 1:55:49.440\n And that's what I've always dreamed about.\n\n1:55:50.720 --> 1:55:53.000\n But I think AI is too big an idea.\n\n1:55:53.000 --> 1:55:54.760\n It's not going to be,\n\n1:55:54.760 --> 1:55:57.000\n there'll be a certain set of pioneers who get there first.\n\n1:55:57.000 --> 1:55:58.600\n I hope we're in the vanguard\n\n1:55:58.600 --> 1:56:00.400\n so we can influence how that goes.\n\n1:56:00.400 --> 1:56:02.480\n And I think it matters who builds,\n\n1:56:02.480 --> 1:56:06.480\n which cultures they come from and what values they have,\n\n1:56:06.480 --> 1:56:07.840\n the builders of AI systems.\n\n1:56:07.840 --> 1:56:09.280\n Cause I think even though the AI system\n\n1:56:09.280 --> 1:56:11.560\n is gonna learn for itself most of its knowledge,\n\n1:56:11.560 --> 1:56:14.760\n there'll be a residue in the system of the culture\n\n1:56:14.760 --> 1:56:17.680\n and the values of the creators of that system.\n\n1:56:17.680 --> 1:56:18.720\n And there's interesting questions\n\n1:56:18.720 --> 1:56:21.600\n to discuss about that geopolitically.\n\n1:56:21.600 --> 1:56:22.440\n Different cultures,\n\n1:56:22.440 --> 1:56:24.920\n we're in a more fragmented world than ever, unfortunately.\n\n1:56:24.920 --> 1:56:27.480\n I think in terms of global cooperation,\n\n1:56:27.480 --> 1:56:29.240\n we see that in things like climate\n\n1:56:29.240 --> 1:56:32.000\n where we can't seem to get our act together globally\n\n1:56:32.000 --> 1:56:34.080\n to cooperate on these pressing matters.\n\n1:56:34.080 --> 1:56:35.600\n I hope that will change over time.\n\n1:56:35.600 --> 1:56:38.640\n Perhaps if we get to an era of radical abundance,\n\n1:56:38.640 --> 1:56:40.440\n we don't have to be so competitive anymore.\n\n1:56:40.440 --> 1:56:42.680\n Maybe we can be more cooperative\n\n1:56:42.680 --> 1:56:44.360\n if resources aren't so scarce.\n\n1:56:44.360 --> 1:56:48.240\n It's true that in terms of power corrupting\n\n1:56:48.240 --> 1:56:50.040\n and leading to destructive things,\n\n1:56:50.040 --> 1:56:53.160\n it seems that some of the atrocities of the past happen\n\n1:56:53.160 --> 1:56:56.680\n when there's a significant constraint on resources.\n\n1:56:56.680 --> 1:56:57.560\n I think that's the first thing.\n\n1:56:57.560 --> 1:56:58.400\n I don't think that's enough.\n\n1:56:58.400 --> 1:57:01.560\n I think scarcity is one thing that's led to competition,\n\n1:57:01.560 --> 1:57:03.960\n sort of zero sum game thinking.\n\n1:57:03.960 --> 1:57:06.080\n I would like us to all be in a positive sum world.\n\n1:57:06.080 --> 1:57:08.480\n And I think for that, you have to remove scarcity.\n\n1:57:08.480 --> 1:57:09.840\n I don't think that's enough, unfortunately,\n\n1:57:09.840 --> 1:57:10.800\n to get world peace\n\n1:57:10.800 --> 1:57:12.800\n because there's also other corrupting things\n\n1:57:12.800 --> 1:57:15.520\n like wanting power over people and this kind of stuff,\n\n1:57:15.520 --> 1:57:19.040\n which is not necessarily satisfied by just abundance.\n\n1:57:19.040 --> 1:57:20.280\n But I think it will help.\n\n1:57:22.400 --> 1:57:24.920\n But I think ultimately, AI is not gonna be run\n\n1:57:24.920 --> 1:57:26.800\n by any one person or one organization.\n\n1:57:26.800 --> 1:57:29.600\n I think it should belong to the world, belong to humanity.\n\n1:57:29.600 --> 1:57:33.120\n And I think there'll be many ways this will happen.\n\n1:57:33.120 --> 1:57:36.840\n And ultimately, everybody should have a say in that.\n\n1:57:36.840 --> 1:57:41.840\n Do you have advice for young people in high school,\n\n1:57:42.000 --> 1:57:45.800\n in college, maybe if they're interested in AI\n\n1:57:45.800 --> 1:57:50.640\n or interested in having a big impact on the world,\n\n1:57:50.640 --> 1:57:53.200\n what they should do to have a career they can be proud of\n\n1:57:53.200 --> 1:57:55.000\n or to have a life they can be proud of?\n\n1:57:55.000 --> 1:57:57.400\n I love giving talks to the next generation.\n\n1:57:57.400 --> 1:57:59.120\n What I say to them is actually two things.\n\n1:57:59.120 --> 1:58:02.160\n I think the most important things to learn about\n\n1:58:02.160 --> 1:58:04.520\n and to find out about when you're young\n\n1:58:04.520 --> 1:58:07.080\n is what are your true passions is first of all,\n\n1:58:07.080 --> 1:58:07.920\n as two things.\n\n1:58:07.920 --> 1:58:09.720\n One is find your true passions.\n\n1:58:09.720 --> 1:58:11.720\n And I think you can do that by,\n\n1:58:11.720 --> 1:58:14.600\n the way to do that is to explore as many things as possible\n\n1:58:14.600 --> 1:58:16.520\n when you're young and you have the time\n\n1:58:16.520 --> 1:58:19.160\n and you can take those risks.\n\n1:58:19.160 --> 1:58:21.080\n I would also encourage people to look at\n\n1:58:21.080 --> 1:58:24.600\n finding the connections between things in a unique way.\n\n1:58:24.600 --> 1:58:27.280\n I think that's a really great way to find a passion.\n\n1:58:27.280 --> 1:58:30.600\n Second thing I would say, advise is know yourself.\n\n1:58:30.600 --> 1:58:33.920\n So spend a lot of time understanding\n\n1:58:33.920 --> 1:58:35.600\n how you work best.\n\n1:58:35.600 --> 1:58:37.680\n Like what are the optimal times to work?\n\n1:58:37.680 --> 1:58:39.880\n What are the optimal ways that you study?\n\n1:58:39.880 --> 1:58:42.240\n What are your, how do you deal with pressure?\n\n1:58:42.240 --> 1:58:44.560\n Sort of test yourself in various scenarios\n\n1:58:44.560 --> 1:58:47.240\n and try and improve your weaknesses,\n\n1:58:47.240 --> 1:58:50.720\n but also find out what your unique skills and strengths are\n\n1:58:50.720 --> 1:58:52.160\n and then hone those.\n\n1:58:52.160 --> 1:58:54.520\n So then that's what will be your super value\n\n1:58:54.520 --> 1:58:55.880\n in the world later on.\n\n1:58:55.880 --> 1:58:57.840\n And if you can then combine those two things\n\n1:58:57.840 --> 1:59:01.200\n and find passions that you're genuinely excited about\n\n1:59:01.200 --> 1:59:05.360\n that intersect with what your unique strong skills are,\n\n1:59:05.360 --> 1:59:07.880\n then you're onto something incredible\n\n1:59:07.880 --> 1:59:10.920\n and I think you can make a huge difference in the world.\n\n1:59:10.920 --> 1:59:12.760\n So let me ask about know yourself.\n\n1:59:12.760 --> 1:59:13.600\n This is fun.\n\n1:59:13.600 --> 1:59:14.440\n This is fun.\n\n1:59:14.440 --> 1:59:18.120\n Quick questions about day in the life, the perfect day,\n\n1:59:18.120 --> 1:59:21.200\n the perfect productive day in the life of Demis's Hub.\n\n1:59:21.200 --> 1:59:26.200\n Maybe these days you're, there's a lot involved.\n\n1:59:26.200 --> 1:59:29.000\n So maybe a slightly younger Demis's Hub\n\n1:59:29.000 --> 1:59:31.400\n where you could focus on a single project maybe.\n\n1:59:33.120 --> 1:59:34.440\n How early do you wake up?\n\n1:59:34.440 --> 1:59:35.600\n Are you a night owl?\n\n1:59:35.600 --> 1:59:36.760\n Do you wake up early in the morning?\n\n1:59:36.760 --> 1:59:39.160\n What are some interesting habits?\n\n1:59:39.160 --> 1:59:42.400\n How many dozens of cups of coffees do you drink a day?\n\n1:59:42.400 --> 1:59:46.320\n What's the computer that you use?\n\n1:59:46.320 --> 1:59:47.160\n What's the setup?\n\n1:59:47.160 --> 1:59:47.980\n How many screens?\n\n1:59:47.980 --> 1:59:49.120\n What kind of keyboard?\n\n1:59:49.120 --> 1:59:51.400\n Are we talking Emacs Vim\n\n1:59:51.400 --> 1:59:53.320\n or are we talking something more modern?\n\n1:59:53.320 --> 1:59:54.480\n So there's a bunch of those questions.\n\n1:59:54.480 --> 1:59:58.960\n So maybe day in the life, what's the perfect day involved?\n\n1:59:58.960 --> 2:00:00.880\n Well, these days it's quite different\n\n2:00:00.880 --> 2:00:02.680\n from say 10, 20 years ago.\n\n2:00:02.680 --> 2:00:05.480\n Back 10, 20 years ago, it would have been\n\n2:00:05.480 --> 2:00:10.480\n a whole day of research, individual research or programming,\n\n2:00:10.900 --> 2:00:12.580\n doing some experiment, neuroscience,\n\n2:00:12.580 --> 2:00:14.080\n computer science experiment,\n\n2:00:14.080 --> 2:00:16.640\n reading lots of research papers.\n\n2:00:16.640 --> 2:00:18.420\n And then perhaps at nighttime,\n\n2:00:19.720 --> 2:00:24.720\n reading science fiction books or playing some games.\n\n2:00:25.440 --> 2:00:28.360\n But lots of focus, so like deep focused work\n\n2:00:28.360 --> 2:00:32.440\n on whether it's programming or reading research papers.\n\n2:00:32.440 --> 2:00:35.300\n Yes, so that would be lots of deep focus work.\n\n2:00:35.300 --> 2:00:39.560\n These days for the last sort of, I guess, five to 10 years,\n\n2:00:39.560 --> 2:00:41.020\n I've actually got quite a structure\n\n2:00:41.020 --> 2:00:42.360\n that works very well for me now,\n\n2:00:42.360 --> 2:00:46.140\n which is that I'm a complete night owl, always have been.\n\n2:00:46.140 --> 2:00:47.680\n So I optimize for that.\n\n2:00:47.680 --> 2:00:50.760\n So I'll basically do a normal day's work,\n\n2:00:50.760 --> 2:00:52.560\n get into work about 11 o clock\n\n2:00:52.560 --> 2:00:56.400\n and sort of do work to about seven in the office.\n\n2:00:56.400 --> 2:00:58.960\n And I will arrange back to back meetings\n\n2:00:58.960 --> 2:01:00.920\n for the entire time of that.\n\n2:01:00.920 --> 2:01:03.200\n And with as many, meet as many people as possible.\n\n2:01:03.200 --> 2:01:06.500\n So that's my collaboration management part of the day.\n\n2:01:06.500 --> 2:01:10.680\n Then I go home, spend time with the family and friends,\n\n2:01:10.680 --> 2:01:13.600\n have dinner, relax a little bit.\n\n2:01:13.600 --> 2:01:15.240\n And then I start a second day of work.\n\n2:01:15.240 --> 2:01:18.500\n I call it my second day of work around 10 p.m., 11 p.m.\n\n2:01:18.500 --> 2:01:21.260\n And that's the time to about the small hours of the morning,\n\n2:01:21.260 --> 2:01:24.760\n four or five in the morning, where I will do my thinking\n\n2:01:24.760 --> 2:01:28.060\n and reading and research, writing research papers.\n\n2:01:29.000 --> 2:01:30.960\n Sadly, I don't have time to code anymore,\n\n2:01:30.960 --> 2:01:34.900\n but it's not efficient to do that these days,\n\n2:01:34.900 --> 2:01:37.120\n given the amount of time I have.\n\n2:01:37.120 --> 2:01:38.360\n But that's when I do, you know,\n\n2:01:38.360 --> 2:01:40.760\n maybe do the long kind of stretches\n\n2:01:40.760 --> 2:01:42.440\n of thinking and planning.\n\n2:01:42.440 --> 2:01:45.280\n And then probably, you know, using email, other things,\n\n2:01:45.280 --> 2:01:47.880\n I would set, I would fire off a lot of things to my team\n\n2:01:47.880 --> 2:01:49.360\n to deal with the next morning.\n\n2:01:49.360 --> 2:01:51.640\n But actually thinking about this overnight,\n\n2:01:51.640 --> 2:01:53.200\n we should go for this project\n\n2:01:53.200 --> 2:01:54.880\n or arrange this meeting the next day.\n\n2:01:54.880 --> 2:01:56.120\n When you're thinking through a problem,\n\n2:01:56.120 --> 2:01:58.160\n are you talking about a sheet of paper with a pen?\n\n2:01:58.160 --> 2:02:01.040\n Is there some structured process?\n\n2:02:01.040 --> 2:02:04.360\n I still like pencil and paper best for working out things,\n\n2:02:04.360 --> 2:02:06.720\n but these days it's just so efficient\n\n2:02:06.720 --> 2:02:08.720\n to read research papers just on the screen.\n\n2:02:08.720 --> 2:02:10.220\n I still often print them out, actually.\n\n2:02:10.220 --> 2:02:12.540\n I still prefer to mark out things.\n\n2:02:12.540 --> 2:02:14.880\n And I find it goes into the brain better\n\n2:02:14.880 --> 2:02:16.000\n and sticks in the brain better\n\n2:02:16.000 --> 2:02:19.440\n when you're still using physical pen and pencil and paper.\n\n2:02:19.440 --> 2:02:20.800\n So you take notes with the...\n\n2:02:20.800 --> 2:02:22.440\n I have lots of notes, electronic ones,\n\n2:02:22.440 --> 2:02:27.440\n and also whole stacks of notebooks that I use at home, yeah.\n\n2:02:27.640 --> 2:02:30.320\n On some of these most challenging next steps, for example,\n\n2:02:30.320 --> 2:02:33.800\n stuff none of us know about that you're working on,\n\n2:02:33.800 --> 2:02:35.580\n you're thinking,\n\n2:02:35.580 --> 2:02:37.640\n there's some deep thinking required there, right?\n\n2:02:37.640 --> 2:02:39.420\n Like what is the right problem?\n\n2:02:39.420 --> 2:02:41.280\n What is the right approach?\n\n2:02:41.280 --> 2:02:43.920\n Because you're gonna have to invest a huge amount of time\n\n2:02:43.920 --> 2:02:44.800\n for the whole team.\n\n2:02:44.800 --> 2:02:46.760\n They're going to have to pursue this thing.\n\n2:02:46.760 --> 2:02:48.560\n What's the right way to do it?\n\n2:02:48.560 --> 2:02:50.040\n Is RL gonna work here or not?\n\n2:02:50.040 --> 2:02:50.880\n Yes.\n\n2:02:50.880 --> 2:02:53.120\n What's the right thing to try?\n\n2:02:53.120 --> 2:02:55.120\n What's the right benchmark to use?\n\n2:02:55.120 --> 2:02:57.320\n Do we need to construct a benchmark from scratch?\n\n2:02:57.320 --> 2:02:58.200\n All those kinds of things.\n\n2:02:58.200 --> 2:02:59.040\n Yes.\n\n2:02:59.040 --> 2:03:00.200\n So I think of all those kinds of things\n\n2:03:00.200 --> 2:03:03.480\n in the nighttime phase, but also much more,\n\n2:03:03.480 --> 2:03:07.660\n I find I've always found the quiet hours of the morning\n\n2:03:07.660 --> 2:03:11.420\n when everyone's asleep, it's super quiet outside.\n\n2:03:11.420 --> 2:03:12.280\n I love that time.\n\n2:03:12.280 --> 2:03:13.360\n It's the golden hours,\n\n2:03:13.360 --> 2:03:16.480\n like between one and three in the morning.\n\n2:03:16.480 --> 2:03:18.880\n Put some music on, some inspiring music on,\n\n2:03:18.880 --> 2:03:21.600\n and then think these deep thoughts.\n\n2:03:21.600 --> 2:03:24.240\n So that's when I would read my philosophy books\n\n2:03:24.240 --> 2:03:28.820\n and Spinoza's, my recent favorite can, all these things.\n\n2:03:28.820 --> 2:03:33.660\n And I read about a great scientist of history,\n\n2:03:33.660 --> 2:03:35.640\n how they did things, how they thought things.\n\n2:03:35.640 --> 2:03:37.240\n So that's when you do all your creative,\n\n2:03:37.240 --> 2:03:39.160\n that's when I do all my creative thinking.\n\n2:03:39.160 --> 2:03:41.840\n And it's good, I think people recommend\n\n2:03:41.840 --> 2:03:45.120\n you do your sort of creative thinking in one block.\n\n2:03:45.120 --> 2:03:47.160\n And the way I organize the day,\n\n2:03:47.160 --> 2:03:48.560\n that way I don't get interrupted.\n\n2:03:48.560 --> 2:03:51.460\n There's obviously no one else is up at those times.\n\n2:03:51.460 --> 2:03:55.880\n So I can go, I can sort of get super deep\n\n2:03:55.880 --> 2:03:57.560\n and super into flow.\n\n2:03:57.560 --> 2:03:59.640\n The other nice thing about doing it nighttime wise\n\n2:03:59.640 --> 2:04:02.760\n is if I'm really onto something\n\n2:04:02.760 --> 2:04:04.940\n or I've got really deep into something,\n\n2:04:04.940 --> 2:04:06.840\n I can choose to extend it\n\n2:04:06.840 --> 2:04:09.000\n and I'll go into six in the morning, whatever.\n\n2:04:09.000 --> 2:04:10.760\n And then I'll just pay for it the next day.\n\n2:04:10.760 --> 2:04:12.960\n So I'll be a bit tired and I won't be my best,\n\n2:04:12.960 --> 2:04:13.900\n but that's fine.\n\n2:04:13.900 --> 2:04:16.840\n I can decide looking at my schedule the next day\n\n2:04:16.840 --> 2:04:19.360\n and given where I'm at with this particular thought\n\n2:04:19.360 --> 2:04:22.840\n or creative idea that I'm gonna pay that cost the next day.\n\n2:04:22.840 --> 2:04:26.220\n So I think that's more flexible than morning people\n\n2:04:26.220 --> 2:04:28.780\n who do that, they get up at four in the morning.\n\n2:04:28.780 --> 2:04:31.000\n They can also do those golden hours then,\n\n2:04:31.000 --> 2:04:32.640\n but then their start of their scheduled day\n\n2:04:32.640 --> 2:04:34.440\n starts at breakfast, 8 a.m.,\n\n2:04:34.440 --> 2:04:36.040\n whatever they have their first meeting.\n\n2:04:36.040 --> 2:04:37.880\n And then it's hard, you have to reschedule a day\n\n2:04:37.880 --> 2:04:38.960\n if you're in flow.\n\n2:04:38.960 --> 2:04:39.800\n So I don't have to do that.\n\n2:04:39.800 --> 2:04:41.880\n So that could be a true special thread of thoughts\n\n2:04:41.880 --> 2:04:45.160\n that you're too passionate about.\n\n2:04:45.160 --> 2:04:46.740\n This is where some of the greatest ideas\n\n2:04:46.740 --> 2:04:49.320\n could potentially come is when you just lose yourself\n\n2:04:49.320 --> 2:04:51.360\n late into the night.\n\n2:04:51.360 --> 2:04:53.860\n And for the meetings, I mean, you're loading in\n\n2:04:53.860 --> 2:04:56.520\n really hard problems in a very short amount of time.\n\n2:04:56.520 --> 2:04:58.800\n So you have to do some kind of first principles thinking\n\n2:04:58.800 --> 2:05:00.160\n here, it's like, what's the problem?\n\n2:05:00.160 --> 2:05:01.360\n What's the state of things?\n\n2:05:01.360 --> 2:05:03.120\n What's the right next steps?\n\n2:05:03.120 --> 2:05:05.120\n You have to get really good at context switching,\n\n2:05:05.120 --> 2:05:07.200\n which is one of the hardest things,\n\n2:05:07.200 --> 2:05:09.020\n because especially as we do so many things,\n\n2:05:09.020 --> 2:05:10.800\n if you include all the scientific things we do,\n\n2:05:10.800 --> 2:05:12.600\n scientific fields we're working in,\n\n2:05:12.600 --> 2:05:15.380\n these are complex fields in themselves.\n\n2:05:15.380 --> 2:05:18.960\n And you have to sort of keep abreast of that.\n\n2:05:18.960 --> 2:05:20.000\n But I enjoy it.\n\n2:05:20.000 --> 2:05:23.840\n I've always been a sort of generalist in a way.\n\n2:05:23.840 --> 2:05:25.600\n And that's actually what happened in my games career\n\n2:05:25.600 --> 2:05:26.440\n after chess.\n\n2:05:27.880 --> 2:05:29.260\n One of the reasons I stopped playing chess\n\n2:05:29.260 --> 2:05:30.320\n was because I got into computers,\n\n2:05:30.320 --> 2:05:32.280\n but also I started realizing there were many other\n\n2:05:32.280 --> 2:05:33.880\n great games out there to play too.\n\n2:05:33.880 --> 2:05:36.920\n So I've always been that way inclined, multidisciplinary.\n\n2:05:36.920 --> 2:05:39.120\n And there's too many interesting things in the world\n\n2:05:39.120 --> 2:05:41.680\n to spend all your time just on one thing.\n\n2:05:41.680 --> 2:05:45.640\n So you mentioned Spinoza, gotta ask the big, ridiculously\n\n2:05:45.640 --> 2:05:47.640\n big question about life.\n\n2:05:47.640 --> 2:05:50.480\n What do you think is the meaning of this whole thing?\n\n2:05:50.480 --> 2:05:52.560\n Why are we humans here?\n\n2:05:52.560 --> 2:05:55.120\n You've already mentioned that perhaps the universe\n\n2:05:55.120 --> 2:05:56.720\n created us.\n\n2:05:56.720 --> 2:05:58.920\n Is that why you think we're here?\n\n2:05:58.920 --> 2:06:00.120\n To understand how the universe works?\n\n2:06:00.120 --> 2:06:02.080\n Yeah, I think my answer to that would be,\n\n2:06:02.080 --> 2:06:03.960\n and at least the life I'm living,\n\n2:06:03.960 --> 2:06:08.120\n is to gain and understand the knowledge,\n\n2:06:08.120 --> 2:06:10.600\n to gain knowledge and understand the universe.\n\n2:06:10.600 --> 2:06:13.560\n That's what I think, I can't see any higher purpose\n\n2:06:13.560 --> 2:06:15.720\n than that if you think back to the classical Greeks,\n\n2:06:15.720 --> 2:06:17.560\n the virtue of gaining knowledge.\n\n2:06:17.560 --> 2:06:20.440\n It's, I think it's one of the few true virtues\n\n2:06:20.440 --> 2:06:23.600\n is to understand the world around us\n\n2:06:23.600 --> 2:06:25.680\n and the context and humanity better.\n\n2:06:25.680 --> 2:06:29.140\n And I think if you do that, you become more compassionate\n\n2:06:29.140 --> 2:06:32.080\n and more understanding yourself and more tolerant\n\n2:06:32.080 --> 2:06:33.580\n and all these, I think all these other things\n\n2:06:33.580 --> 2:06:34.760\n may flow from that.\n\n2:06:34.760 --> 2:06:37.640\n And to me, understanding the nature of reality,\n\n2:06:37.640 --> 2:06:38.760\n that is the biggest question.\n\n2:06:38.760 --> 2:06:41.400\n What is going on here is sometimes the colloquial way I say.\n\n2:06:41.400 --> 2:06:43.600\n What is really going on here?\n\n2:06:43.600 --> 2:06:44.900\n It's so mysterious.\n\n2:06:44.900 --> 2:06:47.040\n I feel like we're in some huge puzzle.\n\n2:06:47.040 --> 2:06:49.960\n And it's, but the world is also seems to be,\n\n2:06:49.960 --> 2:06:52.880\n the universe seems to be structured in a way.\n\n2:06:52.880 --> 2:06:54.280\n You know, why is it structured in a way\n\n2:06:54.280 --> 2:06:55.840\n that science is even possible?\n\n2:06:55.840 --> 2:06:58.160\n That, you know, methods, the scientific method works,\n\n2:06:58.160 --> 2:06:59.280\n things are repeatable.\n\n2:07:00.240 --> 2:07:02.560\n It feels like it's almost structured in a way\n\n2:07:02.560 --> 2:07:05.000\n to be conducive to gaining knowledge.\n\n2:07:05.000 --> 2:07:06.480\n So I feel like, and you know,\n\n2:07:06.480 --> 2:07:07.960\n why should computers be even possible?\n\n2:07:07.960 --> 2:07:11.880\n Wasn't that amazing that computational electronic devices\n\n2:07:11.880 --> 2:07:15.300\n can be possible, and they're made of sand,\n\n2:07:15.300 --> 2:07:17.280\n our most common element that we have,\n\n2:07:17.280 --> 2:07:19.960\n you know, silicon on the Earth's crust.\n\n2:07:19.960 --> 2:07:21.480\n It could have been made of diamond or something,\n\n2:07:21.480 --> 2:07:23.800\n then we would have only had one computer.\n\n2:07:23.800 --> 2:07:26.560\n So a lot of things are kind of slightly suspicious to me.\n\n2:07:26.560 --> 2:07:29.220\n It sure as heck sounds, this puzzle sure as heck sounds\n\n2:07:29.220 --> 2:07:30.760\n like something we talked about earlier,\n\n2:07:30.760 --> 2:07:35.120\n what it takes to design a game that's really fun to play\n\n2:07:35.120 --> 2:07:36.620\n for prolonged periods of time.\n\n2:07:36.620 --> 2:07:40.420\n And it does seem like this puzzle, like you mentioned,\n\n2:07:40.420 --> 2:07:42.280\n the more you learn about it,\n\n2:07:42.280 --> 2:07:44.860\n the more you realize how little you know.\n\n2:07:44.860 --> 2:07:46.820\n So it humbles you, but excites you\n\n2:07:46.820 --> 2:07:49.020\n by the possibility of learning more.\n\n2:07:49.020 --> 2:07:53.560\n It's one heck of a puzzle we got going on here.\n\n2:07:53.560 --> 2:07:56.420\n So like I mentioned, of all the people in the world,\n\n2:07:56.420 --> 2:08:01.380\n you're very likely to be the one who creates the AGI system\n\n2:08:02.580 --> 2:08:06.320\n that achieves human level intelligence and goes beyond it.\n\n2:08:06.320 --> 2:08:08.360\n So if you got a chance and very well,\n\n2:08:08.360 --> 2:08:10.340\n you could be the person that goes into the room\n\n2:08:10.340 --> 2:08:13.140\n with the system and have a conversation.\n\n2:08:13.140 --> 2:08:15.260\n Maybe you only get to ask one question.\n\n2:08:15.260 --> 2:08:18.100\n If you do, what question would you ask her?\n\n2:08:19.460 --> 2:08:23.660\n I would probably ask, what is the true nature of reality?\n\n2:08:23.660 --> 2:08:24.560\n I think that's the question.\n\n2:08:24.560 --> 2:08:25.980\n I don't know if I'd understand the answer\n\n2:08:25.980 --> 2:08:28.540\n because maybe it would be 42 or something like that,\n\n2:08:28.540 --> 2:08:30.980\n but that's the question I would ask.\n\n2:08:32.420 --> 2:08:34.820\n And then there'll be a deep sigh from the systems,\n\n2:08:34.820 --> 2:08:37.460\n like, all right, how do I explain to this human?\n\n2:08:37.460 --> 2:08:41.860\n All right, let me, I don't have time to explain.\n\n2:08:41.860 --> 2:08:44.660\n Maybe I'll draw you a picture that it is.\n\n2:08:44.660 --> 2:08:49.660\n I mean, how do you even begin to answer that question?\n\n2:08:51.280 --> 2:08:52.780\n Well, I think it would.\n\n2:08:52.780 --> 2:08:55.680\n What would you think the answer could possibly look like?\n\n2:08:55.680 --> 2:08:58.380\n I think it could start looking like\n\n2:08:59.940 --> 2:09:02.060\n more fundamental explanations of physics\n\n2:09:02.060 --> 2:09:03.900\n would be the beginning.\n\n2:09:03.900 --> 2:09:05.740\n More careful specification of that,\n\n2:09:05.740 --> 2:09:07.700\n taking you, walking us through by the hand\n\n2:09:07.700 --> 2:09:10.620\n as to what one would do to maybe prove those things out.\n\n2:09:10.620 --> 2:09:13.700\n Maybe giving you glimpses of what things\n\n2:09:13.700 --> 2:09:15.740\n you totally miss in the physics of today.\n\n2:09:15.740 --> 2:09:16.700\n Exactly, exactly.\n\n2:09:16.700 --> 2:09:20.740\n Just here's glimpses of, no, like there's a much,\n\n2:09:22.260 --> 2:09:23.640\n a much more elaborate world\n\n2:09:23.640 --> 2:09:25.540\n or a much simpler world or something.\n\n2:09:26.780 --> 2:09:30.260\n A much deeper, maybe simpler explanation of things,\n\n2:09:30.260 --> 2:09:31.900\n right, than the standard model of physics,\n\n2:09:31.900 --> 2:09:34.860\n which we know doesn't work, but we still keep adding to.\n\n2:09:34.860 --> 2:09:37.940\n So, and that's how I think the beginning\n\n2:09:37.940 --> 2:09:38.940\n of an explanation would look.\n\n2:09:38.940 --> 2:09:41.260\n And it would start encompassing many of the mysteries\n\n2:09:41.260 --> 2:09:43.380\n that we have wondered about for thousands of years,\n\n2:09:43.380 --> 2:09:47.900\n like consciousness, dreaming, life, and gravity,\n\n2:09:47.900 --> 2:09:48.820\n all of these things.\n\n2:09:48.820 --> 2:09:52.620\n Yeah, giving us glimpses of explanations for those things.\n\n2:09:52.620 --> 2:09:57.180\n Well, Damasir, one of the special human beings\n\n2:09:57.180 --> 2:09:59.080\n in this giant puzzle of ours,\n\n2:09:59.080 --> 2:10:01.020\n and it's a huge honor that you would take a pause\n\n2:10:01.020 --> 2:10:03.260\n from the bigger puzzle to solve this small puzzle\n\n2:10:03.260 --> 2:10:04.780\n of a conversation with me today.\n\n2:10:04.780 --> 2:10:06.300\n It's truly an honor and a pleasure.\n\n2:10:06.300 --> 2:10:07.140\n Thank you so much.\n\n2:10:07.140 --> 2:10:07.960\n Thank you, I really enjoyed it.\n\n2:10:07.960 --> 2:10:09.100\n Thanks, Lex.\n\n2:10:09.100 --> 2:10:10.580\n Thanks for listening to this conversation\n\n2:10:10.580 --> 2:10:11.980\n with Damas Ashabis.\n\n2:10:11.980 --> 2:10:13.180\n To support this podcast,\n\n2:10:13.180 --> 2:10:15.820\n please check out our sponsors in the description.\n\n2:10:15.820 --> 2:10:17.900\n And now, let me leave you with some words\n\n2:10:17.900 --> 2:10:20.380\n from Edgar Dykstra.\n\n2:10:20.380 --> 2:10:23.500\n Computer science is no more about computers\n\n2:10:23.500 --> 2:10:26.260\n than astronomy is about telescopes.\n\n2:10:26.260 --> 2:10:30.260\n Thank you for listening, and hope to see you next time.\n\n"
}