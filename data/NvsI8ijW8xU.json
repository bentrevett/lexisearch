{
  "title": "William MacAskill: Effective Altruism | Lex Fridman Podcast #84",
  "id": "NvsI8ijW8xU",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:03.600\n The following is a conversation with William McCaskill.\n\n00:03.600 --> 00:09.300\n He's a philosopher, ethicist, and one of the originators of the effective altruism movement.\n\n00:09.300 --> 00:13.000\n His research focuses on the fundamentals of effective altruism,\n\n00:13.000 --> 00:19.500\n or the use of evidence and reason to help others by as much as possible with our time and money,\n\n00:19.500 --> 00:24.400\n with a particular concentration on how to act given moral uncertainty.\n\n00:24.400 --> 00:28.600\n He's the author of Doing Good, Better, Effective Altruism,\n\n00:28.600 --> 00:31.200\n and a Radical New Way to Make a Difference.\n\n00:31.200 --> 00:37.100\n He is a cofounder and the president of the Center of Effective Altruism, CEA,\n\n00:37.100 --> 00:43.900\n that encourages people to commit to donate at least 10% of their income to the most effective charities.\n\n00:43.900 --> 00:49.200\n He cofounded 80,000 Hours, which is a nonprofit that provides research and advice\n\n00:49.200 --> 00:52.600\n on how you can best make a difference through your career.\n\n00:52.600 --> 00:57.800\n This conversation was recorded before the outbreak of the coronavirus pandemic.\n\n00:57.800 --> 01:02.300\n For everyone feeling the medical, psychological, and financial burden of this crisis,\n\n01:02.300 --> 01:04.200\n I'm sending love your way.\n\n01:04.200 --> 01:09.100\n Stay strong. We're in this together. We'll beat this thing.\n\n01:09.100 --> 01:11.900\n This is the Artificial Intelligence Podcast.\n\n01:11.900 --> 01:16.200\n If you enjoy it, subscribe on YouTube, review it with five stars on Apple Podcast,\n\n01:16.200 --> 01:23.100\n support it on Patreon, or simply connect with me on Twitter at Lex Friedman, spelled F R I D M A N.\n\n01:23.100 --> 01:25.800\n As usual, I'll do one or two minutes of ads now,\n\n01:25.800 --> 01:29.700\n and never any ads in the middle that can break the flow of the conversation.\n\n01:29.700 --> 01:34.700\n I hope that works for you and doesn't hurt the listening experience.\n\n01:34.700 --> 01:39.000\n This show is presented by Cash App, the number one finance app in the App Store.\n\n01:39.000 --> 01:42.100\n When you get it, use code LEXPODCAST.\n\n01:42.100 --> 01:48.900\n Cash App lets you send money to friends, buy Bitcoin, and invest in the stock market with as little as $1.\n\n01:48.900 --> 01:52.800\n Since Cash App allows you to send and receive money digitally, peer to peer,\n\n01:52.800 --> 01:56.100\n and security in all digital transactions is very important,\n\n01:56.100 --> 02:01.300\n let me mention the PCI data security standard that Cash App is compliant with.\n\n02:01.300 --> 02:04.300\n I'm a big fan of standards for safety and security.\n\n02:04.300 --> 02:07.100\n PCI DSS is a good example of that,\n\n02:07.100 --> 02:10.000\n where a bunch of competitors got together and agreed\n\n02:10.000 --> 02:14.400\n that there needs to be a global standard around the security of transactions.\n\n02:14.400 --> 02:19.300\n Now, we just need to do the same for autonomous vehicles and AI systems in general.\n\n02:19.300 --> 02:22.600\n So again, if you get Cash App from the App Store or Google Play,\n\n02:22.600 --> 02:28.800\n and use the code LEXPODCAST, you get $10, and Cash App will also donate $10 to FIRST,\n\n02:28.800 --> 02:34.500\n an organization that is helping to advance robotics and STEM education for young people around the world.\n\n02:34.500 --> 02:39.100\n And now, here's my conversation with William McCaskill.\n\n02:39.100 --> 02:43.500\n What does utopia for humans and all life on Earth look like for you?\n\n02:43.500 --> 02:45.400\n That's a great question.\n\n02:45.400 --> 02:49.200\n What I want to say is that we don't know,\n\n02:49.200 --> 02:55.500\n and the utopia we want to get to is an indirect one that I call the long reflection.\n\n02:55.500 --> 03:01.200\n So, period of post scarcity, no longer have the kind of urgent problems we have today,\n\n03:01.200 --> 03:06.200\n but instead can spend, perhaps it's tens of thousands of years debating,\n\n03:06.200 --> 03:12.100\n engaging in ethical reflection in order, before we take any kind of drastic lock in,\n\n03:12.100 --> 03:14.500\n actions like spreading to the stars,\n\n03:14.500 --> 03:20.500\n and then we can figure out what is of kind of moral value.\n\n03:20.500 --> 03:25.100\n The long reflection, that's a really beautiful term.\n\n03:25.100 --> 03:29.600\n So, if we look at Twitter for just a second,\n\n03:29.600 --> 03:37.300\n do you think human beings are able to reflect in a productive way?\n\n03:37.300 --> 03:39.500\n I don't mean to make it sound bad,\n\n03:39.500 --> 03:45.000\n because there is a lot of fights and politics and division in our discourse.\n\n03:45.000 --> 03:48.900\n Maybe if you zoom out, it actually is civilized discourse.\n\n03:48.900 --> 03:51.000\n It might not feel like it, but when you zoom out.\n\n03:51.000 --> 03:55.100\n So, I don't want to say that Twitter is not civilized discourse.\n\n03:55.100 --> 03:56.100\n I actually believe it.\n\n03:56.100 --> 03:58.400\n It's more civilized than people give it credit for.\n\n03:58.400 --> 04:03.600\n But do you think the long reflection can actually be stable,\n\n04:03.600 --> 04:08.400\n where we as human beings with our descendant of eight brains\n\n04:08.400 --> 04:13.100\n would be able to sort of rationally discuss things together and arrive at ideas?\n\n04:13.100 --> 04:19.800\n I think, overall, we're pretty good at discussing things rationally,\n\n04:19.800 --> 04:28.500\n and at least in the earlier stages of our lives being open to many different ideas,\n\n04:28.500 --> 04:33.300\n and being able to be convinced and change our views.\n\n04:33.300 --> 04:38.800\n I think that Twitter is designed almost to bring out all the worst tendencies.\n\n04:38.800 --> 04:43.200\n So, if the long reflection were conducted on Twitter,\n\n04:43.200 --> 04:46.200\n maybe it would be better just not even to bother.\n\n04:46.200 --> 04:50.300\n But I think the challenge really is getting to a stage\n\n04:50.300 --> 04:55.700\n where we have a society that is as conducive as possible\n\n04:55.700 --> 04:59.000\n to rational reflection, to deliberation.\n\n04:59.000 --> 05:04.000\n I think we're actually very lucky to be in a liberal society\n\n05:04.000 --> 05:06.900\n where people are able to discuss a lot of ideas and so on.\n\n05:06.900 --> 05:08.100\n I think when we look to the future,\n\n05:08.100 --> 05:12.400\n that's not at all guaranteed that society would be like that,\n\n05:12.400 --> 05:16.900\n rather than a society where there's a fixed canon of values\n\n05:16.900 --> 05:20.600\n that are being imposed on all of society,\n\n05:20.600 --> 05:22.300\n and where you aren't able to question that.\n\n05:22.300 --> 05:23.900\n That would be very bad from my perspective,\n\n05:23.900 --> 05:27.900\n because it means we wouldn't be able to figure out what the truth is.\n\n05:27.900 --> 05:31.300\n I can already sense we're going to go down a million tangents,\n\n05:31.300 --> 05:36.800\n but what do you think is the...\n\n05:36.800 --> 05:38.700\n If Twitter is not optimal,\n\n05:38.700 --> 05:43.300\n what kind of mechanism in this modern age of technology\n\n05:43.300 --> 05:49.300\n can we design where the exchange of ideas could be both civilized and productive,\n\n05:49.300 --> 05:52.600\n and yet not be too constrained\n\n05:52.600 --> 05:55.300\n where there's rules of what you can say and can't say,\n\n05:55.300 --> 05:57.900\n which is, as you say, is not desirable,\n\n05:57.900 --> 06:02.800\n but yet not have some limits as to what can be said or not and so on?\n\n06:02.800 --> 06:05.700\n Do you have any ideas, thoughts on the possible future?\n\n06:05.700 --> 06:07.200\n Of course, nobody knows how to do it,\n\n06:07.200 --> 06:10.900\n but do you have thoughts of what a better Twitter might look like?\n\n06:10.900 --> 06:16.200\n I think that text based media are intrinsically going to be very hard\n\n06:16.200 --> 06:20.000\n to be conducive to rational discussion,\n\n06:20.000 --> 06:24.100\n because if you think about it from an informational perspective,\n\n06:24.100 --> 06:27.200\n if I just send you a text of less than,\n\n06:27.200 --> 06:31.700\n what is it now, 240 characters, 280 characters, I think,\n\n06:31.700 --> 06:36.100\n that's a tiny amount of information compared to, say, you and I talking now,\n\n06:36.100 --> 06:40.100\n where you have access to the words I say, which is the same as in text,\n\n06:40.100 --> 06:43.800\n but also my tone, also my body language,\n\n06:43.800 --> 06:47.800\n and we're very poorly designed to be able to assess...\n\n06:47.800 --> 06:50.300\n I have to read all of this context into anything you say,\n\n06:50.300 --> 06:56.500\n so maybe your partner sends you a text and has a full stop at the end.\n\n06:56.500 --> 06:58.000\n Are they mad at you?\n\n06:58.000 --> 06:58.600\n You don't know.\n\n06:58.600 --> 07:02.400\n You have to infer everything about this person's mental state\n\n07:02.400 --> 07:04.700\n from whether they put a full stop at the end of a text or not.\n\n07:04.700 --> 07:08.800\n Well, the flip side of that is it truly text that's the problem here,\n\n07:08.800 --> 07:14.700\n because there's a viral aspect to the text,\n\n07:14.700 --> 07:17.200\n where you could just post text nonstop.\n\n07:17.200 --> 07:19.800\n It's very immediate.\n\n07:19.800 --> 07:23.200\n The times before Twitter, before the internet,\n\n07:23.200 --> 07:28.500\n the way you would exchange texts is you would write books.\n\n07:28.500 --> 07:33.200\n And that, while it doesn't get body language, it doesn't get tone, it doesn't...\n\n07:33.200 --> 07:36.700\n so on, but it does actually boil down after some time of thinking,\n\n07:36.700 --> 07:40.000\n some editing, and so on, boil down ideas.\n\n07:40.000 --> 07:45.600\n So is the immediacy and the viral nature,\n\n07:45.600 --> 07:49.400\n which produces the outrage mobs and so on, the potential problem?\n\n07:49.400 --> 07:51.100\n I think that is a big issue.\n\n07:51.100 --> 07:56.200\n I think there's going to be this strong selection effect where\n\n07:56.200 --> 07:59.000\n something that provokes outrage, well, that's high arousal,\n\n07:59.000 --> 08:04.400\n you're more likely to retweet that,\n\n08:04.400 --> 08:08.800\n whereas kind of sober analysis is not as sexy, not as viral.\n\n08:08.800 --> 08:16.400\n I do agree that long form content is much better to productive discussion.\n\n08:16.400 --> 08:19.400\n In terms of the media that are very popular at the moment,\n\n08:19.400 --> 08:25.400\n I think that podcasting is great where your podcasts are two hours long,\n\n08:25.400 --> 08:28.900\n so they're much more in depth than Twitter are,\n\n08:28.900 --> 08:33.500\n and you are able to convey so much more nuance,\n\n08:33.500 --> 08:36.800\n so much more caveat, because it's an actual conversation.\n\n08:36.800 --> 08:40.200\n It's more like the sort of communication that we've evolved to do,\n\n08:40.200 --> 08:44.900\n rather than these very small little snippets of ideas that,\n\n08:44.900 --> 08:46.900\n when also combined with bad incentives,\n\n08:46.900 --> 08:49.800\n just clearly aren't designed for helping us get to the truth.\n\n08:49.800 --> 08:53.700\n It's kind of interesting that it's not just the length of the podcast medium,\n\n08:53.700 --> 08:59.300\n but it's the fact that it was started by people that don't give a damn about\n\n08:59.300 --> 09:05.100\n quote unquote demand, that there's a relaxed,\n\n09:05.100 --> 09:08.100\n sort of the style that Joe Rogan does,\n\n09:08.100 --> 09:12.800\n there's a freedom to express ideas\n\n09:12.800 --> 09:15.300\n in an unconstrained way that's very real.\n\n09:15.300 --> 09:22.100\n It's kind of funny that it feels so refreshingly real to us today,\n\n09:22.100 --> 09:24.900\n and I wonder what the future looks like.\n\n09:24.900 --> 09:29.700\n It's a little bit sad now that quite a lot of sort of more popular people\n\n09:29.700 --> 09:31.600\n are getting into podcasting,\n\n09:31.600 --> 09:37.300\n and they try to sort of create, they try to control it,\n\n09:37.300 --> 09:40.200\n they try to constrain it in different kinds of ways.\n\n09:40.200 --> 09:43.400\n People I love, like Conan O Brien and so on, different comedians,\n\n09:43.400 --> 09:50.600\n and I'd love to see where the real aspects of this podcasting medium persist,\n\n09:50.600 --> 09:52.500\n maybe in TV, maybe in YouTube,\n\n09:52.500 --> 09:55.600\n maybe Netflix is pushing those kind of ideas,\n\n09:55.600 --> 09:58.400\n and it's kind of, it's a really exciting word,\n\n09:58.400 --> 10:00.200\n that kind of sharing of knowledge.\n\n10:00.200 --> 10:02.100\n Yeah, I mean, I think it's a double edged sword\n\n10:02.100 --> 10:04.300\n as it becomes more popular and more profitable,\n\n10:04.300 --> 10:08.400\n where on the one hand you'll get a lot more creativity,\n\n10:08.400 --> 10:10.700\n people doing more interesting things with the medium,\n\n10:10.700 --> 10:12.700\n but also perhaps you get this place to the bottom\n\n10:12.700 --> 10:18.100\n where suddenly maybe it'll be hard to find good content on podcasts\n\n10:18.100 --> 10:24.300\n because it'll be so overwhelmed by the latest bit of viral outrage.\n\n10:24.300 --> 10:31.100\n So speaking of that, jumping on Effective Altruism for a second,\n\n10:31.100 --> 10:36.200\n so much of that internet content is funded by advertisements.\n\n10:36.200 --> 10:39.800\n Just in the context of Effective Altruism,\n\n10:39.800 --> 10:44.100\n we're talking about the richest companies in the world,\n\n10:44.100 --> 10:45.800\n they're funded by advertisements essentially,\n\n10:45.800 --> 10:48.800\n Google, that's their primary source of income.\n\n10:48.800 --> 10:51.000\n Do you see that as,\n\n10:51.000 --> 10:55.200\n do you have any criticism of that source of income?\n\n10:55.200 --> 10:57.500\n Do you see that source of money\n\n10:57.500 --> 11:01.000\n as a potentially powerful source of money that could be used,\n\n11:01.000 --> 11:03.200\n well, certainly could be used for good,\n\n11:03.200 --> 11:05.900\n but is there something bad about that source of money?\n\n11:05.900 --> 11:08.100\n I think there's significant worries with it,\n\n11:08.100 --> 11:13.200\n where it means that the incentives of the company\n\n11:13.200 --> 11:20.600\n might be quite misaligned with making people's lives better,\n\n11:20.600 --> 11:28.400\n where again, perhaps the incentives are towards increasing drama\n\n11:28.400 --> 11:32.300\n and debate on your social media feed\n\n11:32.300 --> 11:36.300\n in order that more people are going to be engaged,\n\n11:36.300 --> 11:42.200\n perhaps compulsively involved with the platform.\n\n11:42.200 --> 11:45.600\n Whereas there are other business models\n\n11:45.600 --> 11:49.100\n like having an opt in subscription service\n\n11:49.100 --> 11:51.500\n where perhaps they have other issues,\n\n11:51.500 --> 11:57.600\n but there's much more of an incentive to provide a product\n\n11:57.600 --> 12:00.500\n that its users are just really wanting,\n\n12:00.500 --> 12:02.900\n because now I'm paying for this product.\n\n12:02.900 --> 12:05.400\n I'm paying for this thing that I want to buy\n\n12:05.400 --> 12:09.200\n rather than I'm trying to use this thing\n\n12:09.200 --> 12:11.600\n and it's going to get a profit mechanism\n\n12:11.600 --> 12:13.600\n that is somewhat orthogonal to me\n\n12:13.600 --> 12:19.000\n actually just wanting to use the product.\n\n12:19.000 --> 12:23.000\n And so, I mean, in some cases it'll work better than others.\n\n12:23.000 --> 12:27.100\n I can imagine, I can in theory imagine Facebook\n\n12:27.100 --> 12:28.800\n having a subscription service,\n\n12:28.800 --> 12:32.200\n but I think it's unlikely to happen anytime soon.\n\n12:32.200 --> 12:34.200\n Well, it's interesting and it's weird\n\n12:34.200 --> 12:36.200\n now that you bring it up that it's unlikely.\n\n12:36.200 --> 12:41.000\n For example, I pay I think 10 bucks a month for YouTube Red\n\n12:41.000 --> 12:45.300\n and I don't think I get it much for that\n\n12:45.300 --> 12:50.200\n except just for no ads,\n\n12:50.200 --> 12:52.900\n but in general it's just a slightly better experience.\n\n12:52.900 --> 12:56.100\n And I would gladly, now I'm not wealthy,\n\n12:56.100 --> 12:59.200\n in fact I'm operating very close to zero dollars,\n\n12:59.200 --> 13:01.800\n but I would pay 10 bucks a month to Facebook\n\n13:01.800 --> 13:04.000\n and 10 bucks a month to Twitter\n\n13:04.000 --> 13:07.500\n for some kind of more control\n\n13:07.500 --> 13:09.100\n in terms of advertisements and so on.\n\n13:09.100 --> 13:13.700\n But the other aspect of that is data, personal data.\n\n13:13.700 --> 13:16.200\n People are really sensitive about this\n\n13:16.200 --> 13:19.400\n and I as one who hopes to one day\n\n13:20.700 --> 13:25.600\n create a company that may use people's data\n\n13:25.600 --> 13:27.500\n to do good for the world,\n\n13:27.500 --> 13:28.900\n wonder about this.\n\n13:28.900 --> 13:32.300\n One, the psychology of why people are so paranoid.\n\n13:32.300 --> 13:33.300\n Well, I understand why,\n\n13:33.300 --> 13:35.200\n but they seem to be more paranoid\n\n13:35.200 --> 13:37.700\n than is justified at times.\n\n13:37.700 --> 13:39.400\n And the other is how do you do it right?\n\n13:39.400 --> 13:43.500\n So it seems that Facebook is,\n\n13:43.500 --> 13:46.200\n it seems that Facebook is doing it wrong.\n\n13:47.300 --> 13:49.500\n That's certainly the popular narrative.\n\n13:49.500 --> 13:52.000\n It's unclear to me actually how wrong.\n\n13:53.000 --> 13:55.400\n Like I tend to give them more benefit of the doubt\n\n13:55.400 --> 13:59.900\n because it's a really hard thing to do right\n\n13:59.900 --> 14:01.300\n and people don't necessarily realize it,\n\n14:01.300 --> 14:05.900\n but how do we respect in your view people's privacy?\n\n14:05.900 --> 14:10.700\n Yeah, I mean in the case of how worried are people\n\n14:10.700 --> 14:12.300\n about using their data,\n\n14:12.300 --> 14:15.200\n I mean there's a lot of public debate\n\n14:15.200 --> 14:16.600\n and criticism about it.\n\n14:18.600 --> 14:22.100\n When we look at people's revealed preferences,\n\n14:22.100 --> 14:24.200\n people's continuing massive use\n\n14:24.200 --> 14:26.400\n of these sorts of services.\n\n14:27.600 --> 14:30.500\n It's not clear to me how much people really do care.\n\n14:30.500 --> 14:31.500\n Perhaps they care a bit,\n\n14:31.500 --> 14:35.500\n but they're happy to in effect kind of sell their data\n\n14:35.500 --> 14:37.500\n in order to be able to kind of use a certain service.\n\n14:37.500 --> 14:39.300\n That's a great term, revealed preferences.\n\n14:39.300 --> 14:42.500\n So these aren't preferences you self report in the survey.\n\n14:42.500 --> 14:44.500\n This is like your actions speak.\n\n14:44.500 --> 14:45.340\n Yeah, exactly.\n\n14:45.340 --> 14:46.500\n So you might say,\n\n14:46.500 --> 14:51.000\n oh yeah, I hate the idea of Facebook having my data.\n\n14:51.000 --> 14:52.700\n But then when it comes to it,\n\n14:52.700 --> 14:55.600\n you actually are willing to give that data in exchange\n\n14:55.600 --> 14:58.900\n for being able to use the service.\n\n15:00.400 --> 15:01.600\n And if that's the case,\n\n15:01.600 --> 15:05.300\n then I think unless we have some explanation\n\n15:05.300 --> 15:10.300\n about why there's some negative externality from that\n\n15:11.000 --> 15:13.400\n or why there's some coordination failure,\n\n15:15.800 --> 15:18.000\n or if there's something that consumers\n\n15:18.000 --> 15:19.700\n are just really misled about\n\n15:19.700 --> 15:23.100\n where they don't realize why giving away data like this\n\n15:23.100 --> 15:25.300\n is a really bad thing to do,\n\n15:27.400 --> 15:30.800\n then ultimately I kind of want to,\n\n15:30.800 --> 15:32.300\n you know, respect people's preferences.\n\n15:32.300 --> 15:34.500\n They can give away their data if they want.\n\n15:35.500 --> 15:36.500\n I think there's a big difference\n\n15:36.500 --> 15:39.700\n between companies use of data\n\n15:39.700 --> 15:42.640\n and governments having data where,\n\n15:43.600 --> 15:45.800\n you know, looking at the track record of history,\n\n15:45.800 --> 15:50.800\n governments knowing a lot about their people can be very bad\n\n15:51.600 --> 15:55.000\n if the government chooses to do bad things with it.\n\n15:55.000 --> 15:57.100\n And that's more worrying, I think.\n\n15:57.100 --> 15:59.700\n So let's jump into it a little bit.\n\n15:59.700 --> 16:03.900\n Most people know, but actually I, two years ago,\n\n16:03.900 --> 16:07.000\n had no idea what effective altruism was\n\n16:07.000 --> 16:09.100\n until I saw there was a cool looking event\n\n16:09.100 --> 16:10.800\n in an MIT group here.\n\n16:10.800 --> 16:15.800\n I think it's called the Effective Altruism Club or a group.\n\n16:17.900 --> 16:19.800\n I was like, what the heck is that?\n\n16:19.800 --> 16:23.200\n And one of my friends said,\n\n16:23.200 --> 16:27.200\n I mean, he said that they're just\n\n16:27.200 --> 16:30.000\n a bunch of eccentric characters.\n\n16:30.000 --> 16:31.600\n So I was like, hell yes, I'm in.\n\n16:31.600 --> 16:32.800\n So I went to one of their events\n\n16:32.800 --> 16:34.400\n and looked up what's it about.\n\n16:34.400 --> 16:37.000\n It's quite a fascinating philosophical\n\n16:37.000 --> 16:38.900\n and just a movement of ideas.\n\n16:38.900 --> 16:42.600\n So can you tell me what is effective altruism?\n\n16:42.600 --> 16:44.800\n Great, so the core of effective altruism\n\n16:44.800 --> 16:46.500\n is about trying to answer this question,\n\n16:46.500 --> 16:49.400\n which is how can I do as much good as possible\n\n16:49.400 --> 16:53.200\n with my scarce resources, my time and with my money?\n\n16:53.200 --> 16:57.200\n And then once we have our best guess answers to that,\n\n16:57.200 --> 17:00.200\n trying to take those ideas and put that into practice,\n\n17:00.200 --> 17:03.000\n and do those things that we believe will do the most good.\n\n17:03.000 --> 17:05.000\n And we're now a community of people,\n\n17:06.100 --> 17:08.100\n many thousands of us around the world,\n\n17:08.100 --> 17:10.800\n who really are trying to answer that question\n\n17:10.800 --> 17:13.100\n as best we can and then use our time and money\n\n17:13.100 --> 17:15.200\n to make the world better.\n\n17:15.200 --> 17:18.600\n So what's the difference between sort of\n\n17:18.600 --> 17:22.300\n classical general idea of altruism\n\n17:22.300 --> 17:24.700\n and effective altruism?\n\n17:24.700 --> 17:28.300\n So normally when people try to do good,\n\n17:28.300 --> 17:33.300\n they often just aren't so reflective about those attempts.\n\n17:34.100 --> 17:36.300\n So someone might approach you on the street\n\n17:36.300 --> 17:38.600\n asking you to give to charity.\n\n17:38.600 --> 17:42.200\n And if you're feeling altruistic,\n\n17:42.200 --> 17:44.400\n you'll give to the person on the street.\n\n17:44.400 --> 17:48.100\n Or if you think, oh, I wanna do some good in my life,\n\n17:48.100 --> 17:50.000\n you might volunteer at a local place.\n\n17:50.000 --> 17:52.900\n Or perhaps you'll decide, pursue a career\n\n17:52.900 --> 17:56.500\n where you're working in a field\n\n17:56.500 --> 17:58.200\n that's kind of more obviously beneficial\n\n17:58.200 --> 18:02.300\n like being a doctor or a nurse or a healthcare professional.\n\n18:02.300 --> 18:07.300\n But it's very rare that people apply the same level\n\n18:07.900 --> 18:11.800\n of rigor and analytical thinking\n\n18:11.800 --> 18:14.400\n to lots of other areas we think about.\n\n18:14.400 --> 18:16.400\n So take the case of someone approaching you on the street.\n\n18:16.400 --> 18:18.700\n Imagine if that person instead was saying,\n\n18:18.700 --> 18:20.200\n hey, I've got this amazing company.\n\n18:20.200 --> 18:22.400\n Do you want to invest in it?\n\n18:22.400 --> 18:23.800\n It would be insane.\n\n18:23.800 --> 18:25.500\n No one would ever think, oh, of course,\n\n18:25.500 --> 18:28.000\n I'm just a company like you'd think it was a scam.\n\n18:29.200 --> 18:31.300\n But somehow we don't have that same level of rigor\n\n18:31.300 --> 18:32.400\n when it comes to doing good,\n\n18:32.400 --> 18:34.600\n even though the stakes are more important\n\n18:34.600 --> 18:36.100\n when it comes to trying to help others\n\n18:36.100 --> 18:38.500\n than trying to make money for ourselves.\n\n18:38.500 --> 18:40.700\n Well, first of all, so there is a psychology\n\n18:40.700 --> 18:44.900\n at the individual level of doing good just feels good.\n\n18:46.200 --> 18:51.200\n And so in some sense, on that pure psychological part,\n\n18:51.700 --> 18:52.900\n it doesn't matter.\n\n18:52.900 --> 18:56.400\n In fact, you don't wanna know if it does good or not\n\n18:56.400 --> 19:01.400\n because most of the time it won't.\n\n19:01.500 --> 19:04.800\n So like in a certain sense,\n\n19:04.800 --> 19:06.900\n it's understandable why altruism\n\n19:06.900 --> 19:09.800\n without the effective part is so appealing\n\n19:09.800 --> 19:11.300\n to a certain population.\n\n19:11.300 --> 19:15.300\n By the way, let's zoom off for a second.\n\n19:15.300 --> 19:18.700\n Do you think most people, two questions.\n\n19:18.700 --> 19:20.900\n Do you think most people are good?\n\n19:20.900 --> 19:22.200\n And question number two is,\n\n19:22.200 --> 19:24.900\n do you think most people wanna do good?\n\n19:24.900 --> 19:26.600\n So are most people good?\n\n19:26.600 --> 19:28.000\n I think it's just super dependent\n\n19:28.000 --> 19:31.700\n on the circumstances that someone is in.\n\n19:31.700 --> 19:34.800\n I think that the actions people take\n\n19:34.800 --> 19:37.700\n and their moral worth is just much more dependent\n\n19:37.700 --> 19:41.900\n on circumstance than it is on someone's intrinsic character.\n\n19:41.900 --> 19:43.800\n So is there evil within all of us?\n\n19:43.800 --> 19:47.900\n It seems like with the better angels of our nature,\n\n19:47.900 --> 19:50.400\n there's a tendency of us as a society\n\n19:50.400 --> 19:53.300\n to tend towards good, less war.\n\n19:53.300 --> 19:55.100\n I mean, with all these metrics.\n\n19:56.200 --> 20:00.100\n Is that us becoming who we want to be\n\n20:00.100 --> 20:03.300\n or is that some kind of societal force?\n\n20:03.300 --> 20:05.300\n What's the nature versus nurture thing here?\n\n20:05.300 --> 20:07.100\n Yeah, so in that case, I just think,\n\n20:07.100 --> 20:10.600\n yeah, so violence has massively declined over time.\n\n20:10.600 --> 20:14.200\n I think that's a slow process of cultural evolution,\n\n20:14.200 --> 20:17.600\n institutional evolution such that now the incentives\n\n20:17.600 --> 20:21.700\n for you and I to be violent are very, very small indeed.\n\n20:21.700 --> 20:23.700\n In contrast, when we were hunter gatherers,\n\n20:23.700 --> 20:25.800\n the incentives were quite large.\n\n20:25.800 --> 20:30.800\n If there was someone who was potentially disturbing\n\n20:31.900 --> 20:35.300\n the social order and hunter gatherer setting,\n\n20:35.300 --> 20:37.800\n there was a very strong incentive to kill that person\n\n20:37.800 --> 20:41.400\n and people did and it was just the guarded 10% of deaths\n\n20:41.400 --> 20:44.800\n among hunter gatherers were murders.\n\n20:44.800 --> 20:48.700\n After hunter gatherers, when you have actual societies\n\n20:48.700 --> 20:51.300\n is when violence can probably go up\n\n20:51.300 --> 20:54.300\n because there's more incentive to do mass violence, right?\n\n20:54.300 --> 20:58.800\n To take over, conquer other people's lands\n\n20:58.800 --> 21:01.200\n and murder everybody in place and so on.\n\n21:01.200 --> 21:03.800\n Yeah, I mean, I think total death rate\n\n21:03.800 --> 21:06.900\n from human causes does go down,\n\n21:06.900 --> 21:10.400\n but you're right that if you're in a hunter gatherer situation\n\n21:10.400 --> 21:15.000\n you're kind of a group that you're part of is very small\n\n21:15.000 --> 21:17.300\n then you can't have massive wars\n\n21:17.300 --> 21:19.600\n that just massive communities don't exist.\n\n21:19.600 --> 21:21.300\n But anyway, the second question,\n\n21:21.300 --> 21:23.400\n do you think most people want to do good?\n\n21:23.400 --> 21:26.100\n Yeah, and then I think that is true for most people.\n\n21:26.100 --> 21:31.100\n I think you see that with the fact that most people donate,\n\n21:31.800 --> 21:33.800\n a large proportion of people volunteer.\n\n21:33.800 --> 21:35.500\n If you give people opportunities\n\n21:35.500 --> 21:38.700\n to easily help other people, they will take it.\n\n21:38.700 --> 21:39.700\n But at the same time,\n\n21:39.700 --> 21:43.700\n we're a product of our circumstances\n\n21:43.700 --> 21:47.400\n and if it were more socially awarded to be doing more good,\n\n21:47.400 --> 21:49.600\n if it were more socially awarded to do good effectively\n\n21:49.600 --> 21:51.300\n rather than not effectively,\n\n21:51.300 --> 21:53.600\n then we would see that behavior a lot more.\n\n21:55.100 --> 21:58.700\n So why should we do good?\n\n21:58.700 --> 22:01.400\n Yeah, my answer to this is\n\n22:01.400 --> 22:04.100\n there's no kind of deeper level of explanation.\n\n22:04.100 --> 22:08.500\n So my answer to kind of why should you do good is\n\n22:08.500 --> 22:11.300\n well, there is someone whose life is on the line,\n\n22:11.300 --> 22:13.700\n for example, whose life you can save\n\n22:13.700 --> 22:17.800\n via donating just actually a few thousand dollars\n\n22:17.800 --> 22:20.000\n to an effective nonprofit\n\n22:20.000 --> 22:21.800\n like the Against Malaria Foundation.\n\n22:21.800 --> 22:23.900\n That is a sufficient reason to do good.\n\n22:23.900 --> 22:27.000\n And then if you ask, well, why ought I to do that?\n\n22:27.000 --> 22:29.700\n I'm like, I just show you the same facts again.\n\n22:29.700 --> 22:32.000\n It's that fact that is the reason to do good.\n\n22:32.000 --> 22:34.600\n There's nothing more fundamental than that.\n\n22:34.600 --> 22:38.200\n I'd like to sort of make more concrete\n\n22:38.200 --> 22:41.000\n the thing we're trying to make better.\n\n22:41.000 --> 22:43.100\n So you just mentioned malaria.\n\n22:43.100 --> 22:45.600\n So there's a huge amount of suffering in the world.\n\n22:46.600 --> 22:50.000\n Are we trying to remove?\n\n22:50.000 --> 22:53.500\n So is ultimately the goal, not ultimately,\n\n22:53.500 --> 22:58.300\n but the first step is to remove the worst of the suffering.\n\n22:59.000 --> 23:01.600\n So there's some kind of threshold of suffering\n\n23:01.600 --> 23:04.200\n that we want to make sure does not exist in the world.\n\n23:06.400 --> 23:11.100\n Or do we really naturally want to take a much further step\n\n23:11.100 --> 23:13.700\n and look at things like income inequality?\n\n23:14.600 --> 23:17.000\n So not just getting everybody above a certain threshold,\n\n23:17.000 --> 23:19.200\n but making sure that there's some,\n\n23:21.500 --> 23:23.600\n that broadly speaking,\n\n23:23.600 --> 23:27.400\n there's less injustice in the world, unfairness,\n\n23:27.400 --> 23:29.200\n in some definition, of course,\n\n23:29.200 --> 23:31.200\n very difficult to define a fairness.\n\n23:31.200 --> 23:35.500\n Yeah, so the metric I use is how many people do we affect\n\n23:35.500 --> 23:37.300\n and by how much do we affect them?\n\n23:37.300 --> 23:43.200\n And so that can, often that means eliminating suffering,\n\n23:43.200 --> 23:44.200\n but it doesn't have to,\n\n23:44.200 --> 23:47.800\n could be helping promote a flourishing life instead.\n\n23:47.800 --> 23:53.000\n And so if I was comparing reducing income inequality\n\n23:53.000 --> 23:58.300\n or getting people from the very pits of suffering\n\n23:58.300 --> 24:00.600\n to a higher level,\n\n24:00.600 --> 24:03.100\n the question I would ask is just a quantitative one\n\n24:03.100 --> 24:06.200\n of just if I do this first thing or the second thing,\n\n24:06.200 --> 24:08.100\n how many people am I going to benefit\n\n24:08.100 --> 24:10.000\n and by how much am I going to benefit?\n\n24:10.000 --> 24:13.500\n Am I going to move that one person from kind of 10%,\n\n24:13.500 --> 24:17.200\n 0% well being to 10% well being?\n\n24:17.200 --> 24:20.200\n Perhaps that's just not as good as moving a hundred people\n\n24:20.200 --> 24:22.800\n from 10% well being to 50% well being.\n\n24:22.800 --> 24:27.200\n And the idea is the diminishing returns is the idea of\n\n24:27.200 --> 24:32.800\n when you're in terrible poverty,\n\n24:32.800 --> 24:38.200\n then the $1 that you give goes much further\n\n24:38.200 --> 24:40.700\n than if you were in the middle class in the United States,\n\n24:40.700 --> 24:41.700\n for example.\n\n24:41.700 --> 24:42.300\n Absolutely.\n\n24:42.300 --> 24:44.500\n And this fact is really striking.\n\n24:44.500 --> 24:51.600\n So if you take even just quite a conservative estimate\n\n24:51.600 --> 24:56.900\n of how we are able to turn money into well being,\n\n24:56.900 --> 25:00.100\n the economists put it as like a log curve.\n\n25:00.100 --> 25:02.000\n That's the or steeper.\n\n25:02.000 --> 25:04.600\n But that means that any proportional increase\n\n25:04.600 --> 25:09.300\n in your income has the same impact on your well being.\n\n25:09.300 --> 25:11.500\n And so someone moving from $1,000 a year\n\n25:11.500 --> 25:15.800\n to $2,000 a year has the same impact\n\n25:15.800 --> 25:20.600\n as someone moving from $100,000 a year to $200,000 a year.\n\n25:20.600 --> 25:23.200\n And then when you combine that with the fact that we\n\n25:23.200 --> 25:28.700\n in middle class members of rich countries are 100 times richer\n\n25:28.700 --> 25:31.100\n than financial terms in the global poor,\n\n25:31.100 --> 25:33.700\n that means we can do a hundred times to benefit the poorest people\n\n25:33.700 --> 25:37.600\n in the world as we can to benefit people of our income level.\n\n25:37.600 --> 25:39.400\n And that's this astonishing fact.\n\n25:39.400 --> 25:40.900\n Yeah, it's quite incredible.\n\n25:40.900 --> 25:47.600\n A lot of these facts and ideas are just difficult to think about\n\n25:47.600 --> 25:56.000\n because there's an overwhelming amount of suffering in the world.\n\n25:56.000 --> 26:00.700\n And even acknowledging it is difficult.\n\n26:00.700 --> 26:02.300\n Not exactly sure why that is.\n\n26:02.300 --> 26:07.700\n I mean, I mean, it's difficult because you have to bring to mind,\n\n26:07.700 --> 26:10.000\n you know, it's an unpleasant experience thinking\n\n26:10.000 --> 26:11.700\n about other people's suffering.\n\n26:11.700 --> 26:14.700\n It's unpleasant to be empathizing with it, firstly.\n\n26:14.700 --> 26:16.700\n And then secondly, thinking about it means\n\n26:16.700 --> 26:19.000\n that maybe we'd have to change our lifestyles.\n\n26:19.000 --> 26:22.900\n And if you're very attached to the income that you've got,\n\n26:22.900 --> 26:26.500\n perhaps you don't want to be confronting ideas or arguments\n\n26:26.500 --> 26:31.400\n that might cause you to use some of that money to help others.\n\n26:31.400 --> 26:34.600\n So it's quite understandable in the psychological terms,\n\n26:34.600 --> 26:38.100\n even if it's not the right thing that we ought to be doing.\n\n26:38.100 --> 26:40.100\n So how can we do better?\n\n26:40.100 --> 26:42.400\n How can we be more effective?\n\n26:42.400 --> 26:44.400\n How does data help?\n\n26:44.400 --> 26:47.500\n Yeah, in general, how can we do better?\n\n26:47.500 --> 26:48.800\n It's definitely hard.\n\n26:48.800 --> 26:54.700\n And we have spent the last 10 years engaged in kind of some deep research projects,\n\n26:54.700 --> 26:59.500\n to try and answer kind of two questions.\n\n26:59.500 --> 27:02.500\n One is, of all the many problems the world is facing,\n\n27:02.500 --> 27:04.700\n what are the problems we ought to be focused on?\n\n27:04.700 --> 27:08.600\n And then within those problems that we judge to be kind of the most pressing,\n\n27:08.600 --> 27:13.200\n where we use this idea of focusing on problems that are the biggest in scale,\n\n27:13.200 --> 27:15.600\n that are the most tractable,\n\n27:15.600 --> 27:20.900\n where we can make the most progress on that problem,\n\n27:20.900 --> 27:23.800\n and that are the most neglected.\n\n27:23.800 --> 27:27.500\n Within them, what are the things that have the kind of best evidence,\n\n27:27.500 --> 27:32.000\n or we have the best guess, will do the most good.\n\n27:32.000 --> 27:34.500\n And so we have a bunch of organizations.\n\n27:34.500 --> 27:39.200\n So GiveWell, for example, is focused on global health and development,\n\n27:39.200 --> 27:42.300\n and has a list of seven top recommended charities.\n\n27:42.300 --> 27:44.600\n So the idea in general, and sorry to interrupt,\n\n27:44.600 --> 27:48.600\n is, so we'll talk about sort of poverty and animal welfare and existential risk.\n\n27:48.600 --> 27:52.200\n Those are all fascinating topics, but in general,\n\n27:52.200 --> 27:56.200\n the idea is there should be a group,\n\n27:56.200 --> 28:04.100\n sorry, there's a lot of groups that seek to convert money into good.\n\n28:04.100 --> 28:11.500\n And then you also on top of that want to have a accounting\n\n28:11.500 --> 28:15.900\n of how good they actually perform that conversion,\n\n28:15.900 --> 28:18.400\n how well they did in converting money to good.\n\n28:18.400 --> 28:20.400\n So ranking of these different groups,\n\n28:20.400 --> 28:24.000\n ranking these charities.\n\n28:24.000 --> 28:29.600\n So does that apply across basically all aspects of effective altruism?\n\n28:29.600 --> 28:31.700\n So there should be a group of people,\n\n28:31.700 --> 28:35.700\n and they should report on certain metrics of how well they've done,\n\n28:35.700 --> 28:39.900\n and you should only give your money to groups that do a good job.\n\n28:39.900 --> 28:43.500\n That's the core idea. I'd make two comments.\n\n28:43.500 --> 28:45.300\n One is just, it's not just about money.\n\n28:45.300 --> 28:49.700\n So we're also trying to encourage people to work in areas\n\n28:49.700 --> 28:51.300\n where they'll have the biggest impact.\n\n28:51.300 --> 28:51.900\n Absolutely.\n\n28:51.900 --> 28:56.400\n And in some areas, you know, they're really people heavy, but money poor.\n\n28:56.400 --> 28:59.700\n Other areas are kind of money rich and people poor.\n\n28:59.700 --> 29:05.200\n And so whether it's better to focus time or money depends on the cause area.\n\n29:05.200 --> 29:08.300\n And then the second is that you mentioned metrics,\n\n29:08.300 --> 29:12.300\n and while that's the ideal, and in some areas we do,\n\n29:12.300 --> 29:15.100\n we are able to get somewhat quantitative information\n\n29:15.100 --> 29:18.900\n about how much impact an area is having.\n\n29:18.900 --> 29:20.200\n That's not always true.\n\n29:20.200 --> 29:23.800\n For some of the issues, like you mentioned existential risks,\n\n29:23.800 --> 29:30.400\n well, we're not able to measure in any sort of precise way\n\n29:30.400 --> 29:32.400\n like how much progress we're making.\n\n29:32.400 --> 29:38.500\n And so you have to instead fall back on just rigorous argument and evaluation,\n\n29:38.500 --> 29:41.000\n even in the absence of data.\n\n29:41.000 --> 29:47.400\n So let's first sort of linger on your own story for a second.\n\n29:47.400 --> 29:51.100\n How do you yourself practice effective altruism in your own life?\n\n29:51.100 --> 29:54.700\n Because I think that's a really interesting place to start.\n\n29:54.700 --> 30:00.100\n So I've tried to build effective altruism into at least many components of my life.\n\n30:00.100 --> 30:06.200\n So on the donation side, my plan is to give away most of my income\n\n30:06.200 --> 30:07.500\n over the course of my life.\n\n30:07.500 --> 30:12.400\n I've set a bar I feel happy with and I just donate above that bar.\n\n30:12.400 --> 30:17.300\n So at the moment, I donate about 20% of my income.\n\n30:17.300 --> 30:22.000\n Then on the career side, I've also shifted kind of what I do,\n\n30:22.000 --> 30:28.400\n where I was initially planning to work on very esoteric topics\n\n30:28.400 --> 30:30.800\n in the philosophy of logic, philosophy of language,\n\n30:30.800 --> 30:33.000\n things that are intellectually extremely interesting,\n\n30:33.000 --> 30:37.400\n but the path by which they really make a difference to the world is,\n\n30:37.400 --> 30:40.600\n let's just say it's very unclear at best.\n\n30:40.600 --> 30:44.600\n And so I switched instead to researching ethics to actually just working\n\n30:44.600 --> 30:48.400\n on this question of how we can do as much good as possible.\n\n30:48.400 --> 30:53.300\n And then I've also spent a very large chunk of my life over the last 10 years\n\n30:53.300 --> 30:56.400\n creating a number of nonprofits who again in different ways\n\n30:56.400 --> 31:00.000\n are tackling this question of how we can do the most good\n\n31:00.000 --> 31:02.000\n and helping them to grow over time too.\n\n31:02.000 --> 31:06.600\n Yeah, we mentioned a few of them with the career selection, 80,000.\n\n31:06.600 --> 31:07.500\n 80,000 hours.\n\n31:07.500 --> 31:11.100\n 80,000 hours is a really interesting group.\n\n31:11.100 --> 31:18.400\n So maybe also just a quick pause on the origins of effective altruism\n\n31:18.400 --> 31:21.700\n because you paint a picture who the key figures are,\n\n31:21.700 --> 31:26.800\n including yourself in the effective altruism movement today.\n\n31:26.800 --> 31:31.300\n Yeah, there are two main strands that kind of came together\n\n31:31.300 --> 31:34.800\n to form the effective altruism movement.\n\n31:34.800 --> 31:40.400\n So one was two philosophers, myself and Toby Ord at Oxford,\n\n31:40.400 --> 31:43.900\n and we had been very influenced by the work of Peter Singer,\n\n31:43.900 --> 31:47.200\n an Australian model philosopher who had argued for many decades\n\n31:47.200 --> 31:52.900\n that because one can do so much good at such little cost to oneself,\n\n31:52.900 --> 31:55.600\n we have an obligation to give away most of our income\n\n31:55.600 --> 31:58.200\n to benefit those in extreme poverty,\n\n31:58.200 --> 32:01.300\n just in the same way that we have an obligation to run in\n\n32:01.300 --> 32:04.700\n and save a child from drowning in a shallow pond\n\n32:04.700 --> 32:10.300\n if it would just ruin your suit that cost a few thousand dollars.\n\n32:10.300 --> 32:13.100\n And we set up Giving What We Can in 2009,\n\n32:13.100 --> 32:16.000\n which is encouraging people to give at least 10% of their income\n\n32:16.000 --> 32:18.100\n to the most effective charities.\n\n32:18.100 --> 32:21.300\n And the second main strand was the formation of GiveWell,\n\n32:21.300 --> 32:26.300\n which was originally based in New York and started in about 2007.\n\n32:26.300 --> 32:30.200\n And that was set up by Holden Carnovsky and Elie Hassenfeld,\n\n32:30.200 --> 32:36.200\n who were two hedge fund dudes who were making good money\n\n32:36.200 --> 32:38.400\n and thinking, well, where should I donate?\n\n32:38.400 --> 32:42.100\n And in the same way as if they wanted to buy a product for themselves,\n\n32:42.100 --> 32:44.100\n they would look at Amazon reviews.\n\n32:44.100 --> 32:46.600\n They were like, well, what are the best charities?\n\n32:46.600 --> 32:49.300\n Found they just weren't really good answers to that question,\n\n32:49.300 --> 32:51.200\n certainly not that they were satisfied with.\n\n32:51.200 --> 32:56.200\n And so they formed GiveWell in order to try and work out\n\n32:56.200 --> 32:59.000\n what are those charities where they can have the biggest impact.\n\n32:59.000 --> 33:02.200\n And then from there and some other influences,\n\n33:02.200 --> 33:05.200\n kind of community grew and spread.\n\n33:05.200 --> 33:08.600\n Can we explore the philosophical and political space\n\n33:08.600 --> 33:11.400\n that effective altruism occupies a little bit?\n\n33:11.400 --> 33:16.600\n So from the little and distant in my own lifetime\n\n33:16.600 --> 33:21.100\n that I've read of Ayn Rand's work, Ayn Rand's philosophy of objectivism,\n\n33:21.100 --> 33:26.700\n espouses, and it's interesting to put her philosophy in contrast\n\n33:26.700 --> 33:28.000\n with effective altruism.\n\n33:28.000 --> 33:34.000\n So it espouses selfishness as the best thing you can do.\n\n33:34.000 --> 33:37.600\n But it's not actually against altruism.\n\n33:37.600 --> 33:43.100\n It's just you have that choice, but you should be selfish in it, right?\n\n33:43.100 --> 33:44.800\n Or not, maybe you can disagree here.\n\n33:44.800 --> 33:49.500\n But so it can be viewed as the complete opposite of effective altruism\n\n33:49.500 --> 33:55.500\n or it can be viewed as similar because the word effective is really interesting.\n\n33:55.500 --> 34:02.200\n Because if you want to do good, then you should be damn good at doing good, right?\n\n34:02.200 --> 34:08.600\n I think that would fit within the morality that's defined by objectivism.\n\n34:08.600 --> 34:11.100\n So do you see a connection between these two philosophies\n\n34:11.100 --> 34:17.300\n and other perhaps in this complicated space of beliefs\n\n34:17.300 --> 34:24.700\n that effective altruism is positioned as opposing or aligned with?\n\n34:24.700 --> 34:27.800\n I would definitely say that objectivism, Ayn Rand's philosophy,\n\n34:27.800 --> 34:33.100\n is a philosophy that's quite fundamentally opposed to effective altruism.\n\n34:33.100 --> 34:34.300\n In which way?\n\n34:34.300 --> 34:38.600\n Insofar as Ayn Rand's philosophy is about championing egoism\n\n34:38.600 --> 34:42.800\n and saying that I'm never quite sure whether the philosophy is meant to say\n\n34:42.800 --> 34:47.300\n that just you ought to do whatever will best benefit yourself,\n\n34:47.300 --> 34:50.700\n that's ethical egoism, no matter what the consequences are.\n\n34:50.700 --> 34:55.200\n Or second, if there's this alternative view, which is, well,\n\n34:55.200 --> 34:59.800\n you ought to try and benefit yourself because that's actually the best way\n\n34:59.800 --> 35:02.900\n of benefiting society.\n\n35:02.900 --> 35:07.500\n Certainly, in Atlas Shalaguchi is presenting her philosophy\n\n35:07.500 --> 35:12.000\n as a way that's actually going to bring about a flourishing society.\n\n35:12.000 --> 35:16.100\n And if it's the former, then well, effective altruism is all about promoting\n\n35:16.100 --> 35:18.800\n the idea of altruism and saying, in fact,\n\n35:18.800 --> 35:22.400\n we ought to really be trying to help others as much as possible.\n\n35:22.400 --> 35:23.900\n So it's opposed there.\n\n35:23.900 --> 35:28.700\n And then on the second side, I would just dispute the empirical premise.\n\n35:28.700 --> 35:31.500\n It would seem, given the major problems in the world today,\n\n35:31.500 --> 35:34.200\n it would seem like this remarkable coincidence,\n\n35:34.200 --> 35:38.500\n quite suspicious, one might say, if benefiting myself was actually\n\n35:38.500 --> 35:41.100\n the best way to bring about a better world.\n\n35:41.100 --> 35:46.800\n So on that point, and I think that connects also with career selection\n\n35:46.800 --> 35:53.100\n that we'll talk about, but let's consider not objectives, but capitalism.\n\n35:53.100 --> 36:00.900\n And the idea that you focusing on the thing that you are damn good at,\n\n36:00.900 --> 36:05.800\n whatever that is, may be the best thing for the world.\n\n36:05.800 --> 36:09.800\n Part of it is also mindset, right?\n\n36:09.800 --> 36:13.200\n The thing I love is robots.\n\n36:13.200 --> 36:17.500\n So maybe I should focus on building robots\n\n36:17.500 --> 36:22.500\n and never even think about the idea of effective altruism,\n\n36:22.500 --> 36:25.000\n which is kind of the capitalist notion.\n\n36:25.000 --> 36:28.500\n Is there any value in that idea in just finding the thing you're good at\n\n36:28.500 --> 36:31.500\n and maximizing your productivity in this world\n\n36:31.500 --> 36:38.600\n and thereby sort of lifting all boats and benefiting society as a result?\n\n36:38.600 --> 36:41.000\n Yeah, I think there's two things I'd want to say on that.\n\n36:41.000 --> 36:43.500\n So one is what your comparative advantages,\n\n36:43.500 --> 36:45.400\n what your strengths are when it comes to career.\n\n36:45.400 --> 36:49.300\n That's obviously super important because there's lots of career paths\n\n36:49.300 --> 36:53.800\n I would be terrible at if I thought being an artist was the best thing one could do.\n\n36:53.800 --> 36:59.300\n Well, I'd be doomed, just really quite astonishingly bad.\n\n36:59.300 --> 37:05.800\n And so I do think, at least within the realm of things that could plausibly be very high impact,\n\n37:05.800 --> 37:11.500\n choose the thing that you think you're going to be able to really be passionate at\n\n37:11.500 --> 37:15.100\n and excel at over the long term.\n\n37:15.100 --> 37:19.000\n Then on this question of should one just do that in an unrestricted way\n\n37:19.000 --> 37:22.300\n and not even think about what the most important problems are.\n\n37:22.300 --> 37:27.800\n I do think that in a kind of perfectly designed society, that might well be the case.\n\n37:27.800 --> 37:31.500\n That would be a society where we've corrected all market failures,\n\n37:31.500 --> 37:34.700\n we've internalized all externalities,\n\n37:34.700 --> 37:41.700\n and then we've managed to set up incentives such that people just pursuing their own strengths\n\n37:41.700 --> 37:44.100\n is the best way of doing good.\n\n37:44.100 --> 37:46.200\n But we're very far from that society.\n\n37:46.200 --> 37:53.000\n So if one did that, then it would be very unlikely that you would focus\n\n37:53.000 --> 37:57.900\n on improving the lives of nonhuman animals that aren't participating in markets\n\n37:57.900 --> 38:00.000\n or ensuring the long run future goes well,\n\n38:00.000 --> 38:03.200\n where future people certainly aren't participating in markets\n\n38:03.200 --> 38:06.400\n or benefiting the global poor who do participate,\n\n38:06.400 --> 38:11.000\n but have so much less kind of power from a starting perspective\n\n38:11.000 --> 38:18.900\n that their views aren't accurately kind of represented by market forces too.\n\n38:18.900 --> 38:19.500\n Got it.\n\n38:19.500 --> 38:22.700\n So yeah, instead of pure definition capitalism,\n\n38:22.700 --> 38:27.000\n it just may very well ignore the people that are suffering the most,\n\n38:27.000 --> 38:28.900\n the white swath of them.\n\n38:28.900 --> 38:35.400\n So if you could allow me this line of thinking here.\n\n38:35.400 --> 38:38.800\n So I've listened to a lot of your conversations online.\n\n38:38.800 --> 38:46.000\n I find, if I can compliment you, they're very interesting conversations.\n\n38:46.000 --> 38:50.100\n Your conversation on Rogan, on Joe Rogan was really interesting,\n\n38:50.100 --> 38:55.600\n with Sam Harris and so on, whatever.\n\n38:55.600 --> 38:58.000\n There's a lot of stuff that's really good out there.\n\n38:58.000 --> 39:01.600\n And yet, when I look at the internet and I look at YouTube,\n\n39:01.600 --> 39:08.200\n which has certain mobs, certain swaths of right leaning folks,\n\n39:08.200 --> 39:12.500\n whom I dearly love.\n\n39:12.500 --> 39:19.000\n I love all people, especially people with ideas.\n\n39:19.000 --> 39:22.700\n They seem to not like you very much.\n\n39:22.700 --> 39:26.200\n So I don't understand why exactly.\n\n39:26.200 --> 39:31.100\n So my own sort of hypothesis is there is a right left divide\n\n39:31.100 --> 39:36.100\n that absurdly so caricatured in politics,\n\n39:36.100 --> 39:38.300\n at least in the United States.\n\n39:38.300 --> 39:42.700\n And maybe you're somehow pigeonholed into one of those sides.\n\n39:42.700 --> 39:46.600\n And maybe that's what it is.\n\n39:46.600 --> 39:49.600\n Maybe your message is somehow politicized.\n\n39:49.600 --> 39:50.800\n Yeah, I mean.\n\n39:50.800 --> 39:52.200\n How do you make sense of that?\n\n39:52.200 --> 39:54.400\n Because you're extremely interesting.\n\n39:54.400 --> 39:58.600\n Like you got the comments I see on Joe Rogan.\n\n39:58.600 --> 40:00.400\n There's a bunch of negative stuff.\n\n40:00.400 --> 40:03.200\n And yet, if you listen to it, the conversation is fascinating.\n\n40:03.200 --> 40:08.300\n I'm not speaking, I'm not some kind of lefty extremist,\n\n40:08.300 --> 40:10.100\n but just it's a fascinating conversation.\n\n40:10.100 --> 40:13.800\n So why are you getting some small amount of hate?\n\n40:13.800 --> 40:18.100\n So I'm actually pretty glad that Effective Altruism has managed\n\n40:18.100 --> 40:24.000\n to stay relatively unpoliticized because I think the core message\n\n40:24.000 --> 40:27.100\n to just use some of your time and money to do as much good as possible\n\n40:27.100 --> 40:30.100\n to fight some of the problems in the world can be appealing\n\n40:30.100 --> 40:31.700\n across the political spectrum.\n\n40:31.700 --> 40:35.500\n And we do have a diversity of political viewpoints among people\n\n40:35.500 --> 40:38.800\n who have engaged in Effective Altruism.\n\n40:38.800 --> 40:42.700\n We do, however, do get some criticism from the left and the right.\n\n40:42.700 --> 40:43.400\n Oh, interesting.\n\n40:43.400 --> 40:44.400\n What's the criticism?\n\n40:44.400 --> 40:45.800\n Both would be interesting to hear.\n\n40:45.800 --> 40:49.300\n Yeah, so criticism from the left is that we're not focused enough\n\n40:49.300 --> 40:54.100\n on dismantling the capitalist system that they see as the root\n\n40:54.100 --> 40:58.500\n of most of the problems that we're talking about.\n\n40:58.500 --> 41:06.800\n And there I kind of disagree on partly the premise where I don't\n\n41:06.800 --> 41:11.900\n think relevant alternative systems would say to the animals or to the\n\n41:11.900 --> 41:15.400\n global poor or to the future generations kind of much better.\n\n41:15.400 --> 41:19.000\n And then also the tactics where I think there are particular ways\n\n41:19.000 --> 41:22.400\n we can change society that would massively benefit, you know,\n\n41:22.400 --> 41:27.600\n be massively beneficial on those things that don't go via dismantling\n\n41:27.600 --> 41:30.900\n like the entire system, which is perhaps a million times harder to do.\n\n41:30.900 --> 41:34.900\n Then criticism on the right, there's definitely like in response\n\n41:34.900 --> 41:36.900\n to the Joe Rogan podcast.\n\n41:36.900 --> 41:40.000\n There definitely were a number of Ayn Rand fans who weren't keen\n\n41:40.000 --> 41:43.000\n on the idea of promoting altruism.\n\n41:43.000 --> 41:46.900\n There was a remarkable set of ideas.\n\n41:46.900 --> 41:50.700\n Just the idea that Effective Altruism was unmanly, I think, was\n\n41:50.700 --> 41:52.100\n driving a lot of criticism.\n\n41:52.100 --> 41:56.700\n Okay, so I love fighting.\n\n41:56.700 --> 41:58.900\n I've been in street fights my whole life.\n\n41:58.900 --> 42:04.100\n I'm as alpha in everything I do as it gets.\n\n42:04.100 --> 42:08.700\n And the fact that Joe Rogan said that I thought Scent of a Woman\n\n42:08.700 --> 42:14.600\n is a better movie than John Wick put me into this beta category\n\n42:14.600 --> 42:20.700\n amongst people who are like basically saying this, yeah, unmanly\n\n42:20.700 --> 42:21.500\n or it's not tough.\n\n42:21.500 --> 42:26.900\n It's not some principled view of strength that is represented\n\n42:26.900 --> 42:27.700\n by a spasmodic.\n\n42:27.700 --> 42:31.200\n So actually, so how do you think about this?\n\n42:31.200 --> 42:41.400\n Because to me, altruism, especially Effective Altruism, I don't\n\n42:41.400 --> 42:44.800\n know what the female version of that is, but on the male side, manly\n\n42:44.800 --> 42:46.300\n as fuck, if I may say so.\n\n42:46.300 --> 42:51.500\n So how do you think about that kind of criticism?\n\n42:51.500 --> 42:55.400\n I think people who would make that criticism are just occupying\n\n42:55.400 --> 42:59.200\n a like state of mind that I think is just so different from my\n\n42:59.200 --> 43:03.300\n state of mind that I kind of struggle to maybe even understand it\n\n43:03.300 --> 43:07.700\n where if something's manly or unmanly or feminine or unfeminine,\n\n43:07.700 --> 43:08.700\n I'm like, I don't care.\n\n43:08.700 --> 43:11.000\n Like, is it the right thing to do or the wrong thing to do?\n\n43:11.000 --> 43:14.700\n So let me put it not in terms of man or woman.\n\n43:14.700 --> 43:20.100\n I don't think that's useful, but I think there's a notion of acting\n\n43:20.100 --> 43:26.700\n out of fear as opposed to out of principle and strength.\n\n43:26.700 --> 43:27.400\n Yeah.\n\n43:27.400 --> 43:28.400\n So, okay.\n\n43:28.400 --> 43:28.600\n Yeah.\n\n43:28.600 --> 43:33.500\n Here's something that I do feel as an intuition and that I think\n\n43:33.500 --> 43:38.200\n drives some people who do find Canvaean Land attractive and so on\n\n43:38.200 --> 43:43.000\n as a philosophy, which is a kind of taking control of your own\n\n43:43.000 --> 43:51.300\n life and having power over how you're steering your life and not\n\n43:51.300 --> 43:55.500\n kind of kowtowing to others, you know, really thinking things through.\n\n43:55.500 --> 43:59.800\n I find like that set of ideas just very compelling and inspirational.\n\n43:59.800 --> 44:04.300\n I actually think of effect of altruism has really, you know, that\n\n44:04.300 --> 44:05.300\n side of my personality.\n\n44:05.300 --> 44:11.400\n It's like scratch that itch where you are just not taking the kind\n\n44:11.400 --> 44:14.100\n of priorities that society is giving you as granted.\n\n44:14.100 --> 44:19.300\n Instead, you're choosing to act in accordance with the priorities\n\n44:19.300 --> 44:21.200\n that you think are most important in the world.\n\n44:21.200 --> 44:29.400\n And often that involves then doing quite unusual things from a\n\n44:29.400 --> 44:33.400\n societal perspective, like donating a large chunk of your earnings\n\n44:33.400 --> 44:38.100\n or working on these weird issues about AI and so on that other\n\n44:38.100 --> 44:39.200\n people might not understand.\n\n44:39.200 --> 44:42.000\n Yeah, I think that's a really gutsy thing to do.\n\n44:42.000 --> 44:43.400\n That is taking control.\n\n44:43.400 --> 44:45.600\n That's at least at this stage.\n\n44:45.600 --> 44:53.300\n I mean, that's you taking ownership, not of just yourself, but\n\n44:53.300 --> 44:58.500\n your presence in this world that's full of suffering and saying\n\n44:58.500 --> 45:02.300\n as opposed to being paralyzed by that notion is taking control\n\n45:02.300 --> 45:03.600\n and saying I could do something.\n\n45:03.600 --> 45:05.900\n Yeah, I mean, that's really powerful.\n\n45:05.900 --> 45:09.500\n But I mean, sort of the one thing I personally hate too about the\n\n45:09.500 --> 45:15.500\n left currently that I think those folks to detect is the social\n\n45:15.500 --> 45:21.600\n signaling. When you look at yourself, sort of late at night, would\n\n45:21.600 --> 45:25.900\n you do everything you're doing in terms of effective altruism if\n\n45:25.900 --> 45:29.300\n your name, because you're quite popular, but if your name was\n\n45:29.300 --> 45:32.400\n totally unattached to it, so if it was in secret.\n\n45:32.400 --> 45:34.800\n Yeah, I mean, I think I would.\n\n45:34.800 --> 45:39.800\n To be honest, I think the kind of popularity is like, you know,\n\n45:39.800 --> 45:43.300\n it's mixed bag, but there are serious costs.\n\n45:43.300 --> 45:45.600\n And I don't particularly, I don't like love it.\n\n45:45.600 --> 45:49.700\n Like, it means you get all these people calling you a cuck on\n\n45:49.700 --> 45:50.300\n Joe Rogan.\n\n45:50.300 --> 45:51.900\n It's like not the most fun thing.\n\n45:51.900 --> 45:56.100\n But you also get a lot of sort of brownie points for doing good\n\n45:56.100 --> 45:56.700\n for the world.\n\n45:56.700 --> 45:57.800\n Yeah, you do.\n\n45:57.800 --> 46:02.200\n But I think my ideal life, I would be like in some library solving\n\n46:02.200 --> 46:06.500\n logic puzzles all day and I'd like really be like learning maths\n\n46:06.500 --> 46:07.100\n and so on.\n\n46:07.100 --> 46:10.600\n So you have a like good body of friends and so on.\n\n46:10.600 --> 46:14.500\n So your instinct for effective altruism is something deep.\n\n46:14.500 --> 46:19.100\n It's not one that is communicating\n\n46:19.100 --> 46:21.300\n socially. It's more in your heart.\n\n46:21.300 --> 46:23.200\n You want to do good for the world.\n\n46:23.200 --> 46:26.700\n Yeah, I mean, so we can look back to early giving what we can.\n\n46:26.700 --> 46:31.800\n So, you know, we're setting this up, me and Toby.\n\n46:31.800 --> 46:36.500\n And I really thought that doing this would be a big hit to my\n\n46:36.500 --> 46:40.100\n academic career because I was now spending, you know, at that time\n\n46:40.100 --> 46:43.700\n more than half my time setting up this nonprofit at the crucial\n\n46:43.700 --> 46:46.500\n time when you should be like producing your best academic work\n\n46:46.500 --> 46:47.000\n and so on.\n\n46:47.000 --> 46:49.700\n And it was also the case at the time.\n\n46:49.700 --> 46:52.900\n It was kind of like the Toby order club.\n\n46:52.900 --> 46:55.300\n You know, he was he was the most popular.\n\n46:55.300 --> 46:57.700\n There's this personal interest story about him and his plans\n\n46:57.700 --> 47:02.600\n donate and sorry to interrupt but Toby was donating a large\n\n47:02.600 --> 47:05.100\n amount. Can you tell just briefly what he was doing?\n\n47:05.100 --> 47:09.000\n Yeah, so he made this public commitment to give everything\n\n47:09.000 --> 47:13.900\n he earned above 20,000 pounds per year to the most effective\n\n47:13.900 --> 47:17.400\n causes. And even as a graduate student, he was still donating\n\n47:17.400 --> 47:21.600\n about 15, 20% of his income, which is so quite significant\n\n47:21.600 --> 47:24.100\n given that graduate students are not known for being super\n\n47:24.100 --> 47:24.500\n wealthy.\n\n47:24.500 --> 47:28.500\n That's right. And when we launched Giving What We Can, the\n\n47:28.500 --> 47:31.500\n media just loved this as like a personal interest story.\n\n47:31.500 --> 47:38.500\n So the story about him and his pledge was the most, yeah, it\n\n47:38.500 --> 47:40.500\n was actually the most popular news story of the day.\n\n47:40.500 --> 47:43.400\n And we kind of ran the same story a year later and it was\n\n47:43.400 --> 47:45.800\n the most popular news story of the day a year later too.\n\n47:45.800 --> 47:53.100\n And so it really was kind of several years before then I\n\n47:53.100 --> 47:55.400\n was also kind of giving more talks and starting to do more\n\n47:55.400 --> 47:58.000\n writing and then especially with, you know, I wrote this book\n\n47:58.000 --> 48:02.100\n Doing Good Better that then there started to be kind of attention\n\n48:02.100 --> 48:06.300\n and so on. But deep inside your own relationship with effective\n\n48:06.300 --> 48:12.300\n altruism was, I mean, it had nothing to do with the publicity.\n\n48:12.300 --> 48:14.400\n Did you see yourself?\n\n48:14.400 --> 48:16.900\n How did the publicity connect with it?\n\n48:16.900 --> 48:19.700\n Yeah, I mean, that's kind of what I'm saying is I think the\n\n48:19.700 --> 48:22.900\n publicity came like several years afterwards.\n\n48:22.900 --> 48:25.400\n I mean, at the early stage when we set up Giving What We Can,\n\n48:25.400 --> 48:30.200\n it was really just every person we get to pledge 10% is, you\n\n48:30.200 --> 48:34.800\n know, something like $100,000 over their lifetime.\n\n48:34.800 --> 48:35.800\n That's huge.\n\n48:35.800 --> 48:39.600\n And so it was just we had started with 23 members, every single\n\n48:39.600 --> 48:43.200\n person was just this like kind of huge accomplishment.\n\n48:43.200 --> 48:46.500\n And at the time, I just really thought, you know, maybe over\n\n48:46.500 --> 48:49.700\n time we'll have a hundred members and that'll be like amazing.\n\n48:49.700 --> 48:52.900\n Whereas now we have, you know, over four thousand and one and\n\n48:52.900 --> 48:54.100\n a half billion dollars pledged.\n\n48:54.100 --> 48:59.100\n That's just unimaginable to me at the time when I was first kind\n\n48:59.100 --> 49:02.000\n of getting this, you know, getting the stuff off the ground.\n\n49:02.000 --> 49:10.100\n So can we talk about poverty and the biggest problems that you\n\n49:10.100 --> 49:15.300\n think in the near term effective altruism can attack in each\n\n49:15.300 --> 49:18.900\n one. So poverty obviously is a huge one.\n\n49:18.900 --> 49:21.400\n Yeah. How can we help?\n\n49:21.400 --> 49:22.200\n Great.\n\n49:22.200 --> 49:22.400\n Yeah.\n\n49:22.400 --> 49:24.800\n So poverty, absolutely this huge problem.\n\n49:24.800 --> 49:28.800\n 700 million people in extreme poverty living in less than two\n\n49:28.800 --> 49:33.800\n dollars per day where that's what that means is what two dollars\n\n49:33.800 --> 49:34.900\n would buy in the US.\n\n49:34.900 --> 49:36.900\n So think about that.\n\n49:36.900 --> 49:38.800\n It's like some rice, maybe some beans.\n\n49:38.800 --> 49:40.600\n It's very, you know, really not much.\n\n49:40.600 --> 49:45.600\n And at the same time, we can do an enormous amount to improve\n\n49:45.600 --> 49:47.400\n the lives of people in extreme poverty.\n\n49:47.400 --> 49:51.800\n So the things that we tend to focus on interventions in global\n\n49:51.800 --> 49:54.600\n health and that's for a couple of few reasons.\n\n49:54.600 --> 49:58.100\n One is like global health just has this amazing track record\n\n49:58.100 --> 50:02.700\n life expectancy globally is up 50% relative to 60 or 70 years\n\n50:02.700 --> 50:06.600\n ago. We've eradicated smallpox that's which killed 2 million\n\n50:06.600 --> 50:08.900\n lives every year almost eradicated polio.\n\n50:08.900 --> 50:13.800\n Second is that we just have great data on what works when it\n\n50:13.800 --> 50:14.600\n comes to global health.\n\n50:14.600 --> 50:20.500\n So we just know that bed nets protect children from prevent\n\n50:20.500 --> 50:21.600\n them from dying from malaria.\n\n50:21.600 --> 50:26.300\n And then the third is just that's extremely cost effective.\n\n50:26.300 --> 50:30.800\n So it costs $5 to buy one bed net, protects two children for\n\n50:30.800 --> 50:31.900\n two years against malaria.\n\n50:31.900 --> 50:35.600\n If you spend about $3,000 on bed nets, then statistically\n\n50:35.600 --> 50:37.300\n speaking, you're going to save a child's life.\n\n50:37.300 --> 50:40.900\n And there are other interventions too.\n\n50:40.900 --> 50:45.300\n And so given the people in such suffering and we have this\n\n50:45.300 --> 50:50.800\n opportunity to, you know, do such huge good for such low cost.\n\n50:50.800 --> 50:52.000\n Well, yeah, why not?\n\n50:52.000 --> 50:53.300\n So the individual.\n\n50:53.300 --> 50:59.400\n So for me today, if I wanted to look at poverty, how would\n\n50:59.400 --> 51:03.700\n I help? And I wanted to say, I think donating 10% of your\n\n51:03.700 --> 51:07.000\n income is a very interesting idea or some percentage or some\n\n51:07.000 --> 51:09.400\n setting a bar and sort of sticking to it.\n\n51:09.400 --> 51:14.700\n How do we then take the step towards the effective part?\n\n51:14.700 --> 51:19.200\n So you've conveyed some notions, but who do you give the\n\n51:19.200 --> 51:21.300\n money to? Yeah.\n\n51:21.300 --> 51:25.900\n So GiveWell, this organization I mentioned, well, it makes\n\n51:25.900 --> 51:29.300\n charity recommendations and some of its top recommendations.\n\n51:29.300 --> 51:34.200\n So Against Malaria Foundation is this organization that buys\n\n51:34.200 --> 51:37.300\n and distributes these insecticide seeded bed nets.\n\n51:37.300 --> 51:41.400\n And then it has a total of seven charities that it recommends\n\n51:41.400 --> 51:46.100\n very highly. So that recommendation, is it almost like a star\n\n51:46.100 --> 51:48.800\n of approval or is there some metrics?\n\n51:48.800 --> 51:54.600\n So what are the ways that GiveWell conveys that this is a\n\n51:54.600 --> 51:57.200\n great charity organization?\n\n51:57.200 --> 51:58.000\n Yeah.\n\n51:58.000 --> 52:01.700\n So GiveWell is looking at metrics and it's trying to compare\n\n52:01.700 --> 52:05.800\n charities ultimately in the number of lives that you can save\n\n52:05.800 --> 52:07.500\n or an equivalent benefit.\n\n52:07.500 --> 52:11.700\n So one of the charities it recommends is GiveDirectly, which\n\n52:11.700 --> 52:17.100\n simply just transfers cash to the poorest families where poor\n\n52:17.100 --> 52:20.800\n family will get a cash transfer of $1,000 and they kind of\n\n52:20.800 --> 52:24.600\n regard that as the baseline intervention because it's so simple\n\n52:24.600 --> 52:27.300\n and people, you know, they know what to do with how to benefit\n\n52:27.300 --> 52:30.400\n themselves. That's quite powerful, by the way.\n\n52:30.400 --> 52:34.600\n So before GiveWell, before the Effective Altruism Movement, was\n\n52:34.600 --> 52:39.000\n there, I imagine there's a huge amount of corruption, funny\n\n52:39.000 --> 52:42.100\n enough, in charity organizations or misuse of money.\n\n52:42.100 --> 52:42.500\n Yeah.\n\n52:43.500 --> 52:46.200\n So there was nothing like GiveWell before that?\n\n52:46.200 --> 52:46.500\n No.\n\n52:46.500 --> 52:47.500\n I mean, there were some.\n\n52:47.700 --> 52:49.500\n So, I mean, the charity corruption, I mean, obviously\n\n52:49.500 --> 52:53.800\n there's some, I don't think it's a huge issue.\n\n52:53.800 --> 52:57.700\n They're also just focusing on the long things. Prior to GiveWell,\n\n52:57.700 --> 53:00.900\n there were some organizations like Charity Navigator, which\n\n53:00.900 --> 53:04.600\n were more aimed at worrying about corruption and so on.\n\n53:04.600 --> 53:07.300\n So they weren't saying, these are the charities where you're\n\n53:07.300 --> 53:10.300\n going to do the most good. Instead, it was like, how good\n\n53:10.300 --> 53:12.700\n are the charities financials?\n\n53:12.700 --> 53:14.100\n How good is its health?\n\n53:14.100 --> 53:16.800\n Are they transparent? And yeah, so that would be more useful\n\n53:16.800 --> 53:18.800\n for weeding out some of those worst charities.\n\n53:19.200 --> 53:21.900\n So GiveWell has just taken a step further, sort of in this\n\n53:21.900 --> 53:24.700\n 21st century of data.\n\n53:25.200 --> 53:28.700\n It's actually looking at the effective part.\n\n53:28.700 --> 53:32.100\n Yeah. So it's like, you know, if you know the wire cutter for\n\n53:32.100 --> 53:34.200\n if you want to buy a pair of headphones, they will just look\n\n53:34.200 --> 53:36.400\n at all the headphones and be like, these are the best headphones\n\n53:36.400 --> 53:37.100\n you can buy.\n\n53:37.800 --> 53:38.900\n That's the idea with GiveWell.\n\n53:39.300 --> 53:39.700\n Okay.\n\n53:39.700 --> 53:44.400\n So do you think there's a bar of what suffering is?\n\n53:44.400 --> 53:47.800\n And do you think one day we can eradicate suffering in our\n\n53:47.800 --> 53:49.400\n world? Yeah.\n\n53:49.400 --> 53:50.200\n Amongst humans?\n\n53:50.200 --> 53:52.300\n Let's talk humans for now. Talk humans.\n\n53:52.300 --> 53:55.000\n But in general, yeah, actually.\n\n53:55.000 --> 54:00.800\n So there's a colleague of mine calling the term abolitionism\n\n54:00.800 --> 54:02.800\n for the idea that we should just be trying to abolish\n\n54:02.800 --> 54:06.100\n suffering. And in the long run, I mean, I don't expect to\n\n54:06.100 --> 54:07.700\n anytime soon, but I think we can.\n\n54:09.100 --> 54:11.800\n I think that would require, you know, quite change, quite\n\n54:11.900 --> 54:15.400\n drastic changes to the way society is structured and perhaps\n\n54:15.400 --> 54:21.600\n even the, you know, the human, in fact, even changes to human\n\n54:21.600 --> 54:25.400\n nature. But I do think that suffering whenever it occurs\n\n54:25.400 --> 54:28.200\n is bad and we should want it to not occur.\n\n54:28.300 --> 54:31.400\n So there's a line.\n\n54:31.500 --> 54:33.900\n There's a gray area between suffering.\n\n54:33.900 --> 54:34.700\n Now I'm Russian.\n\n54:34.700 --> 54:37.200\n So I romanticize some aspects of suffering.\n\n54:38.600 --> 54:41.400\n There's a gray line between struggle, gray area between\n\n54:41.400 --> 54:42.700\n struggle and suffering.\n\n54:42.700 --> 54:50.200\n So one, do we want to eradicate all struggle in the world?\n\n54:51.800 --> 54:59.600\n So there's an idea, you know, that the human condition\n\n54:59.900 --> 55:04.000\n inherently has suffering in it and it's a creative force.\n\n55:04.800 --> 55:09.400\n It's a struggle of our lives and we somehow grow from that.\n\n55:09.400 --> 55:13.600\n How do you think about, how do you think about that?\n\n55:13.600 --> 55:15.600\n I agree that's true.\n\n55:15.600 --> 55:20.300\n So, you know, often, you know, great artists can be also\n\n55:20.300 --> 55:24.300\n suffering from, you know, major health conditions or depression\n\n55:24.300 --> 55:26.600\n and so on. They come from abusive parents.\n\n55:26.600 --> 55:29.900\n Most great artists, I think, come from abusive parents.\n\n55:29.900 --> 55:33.200\n Yeah, that seems to be at least commonly the case, but I\n\n55:33.200 --> 55:37.100\n want to distinguish between suffering as being instrumentally\n\n55:37.100 --> 55:40.900\n good, you know, it causes people to produce good things and\n\n55:40.900 --> 55:43.300\n whether it's intrinsically good and I think intrinsically\n\n55:43.300 --> 55:44.200\n it's always bad.\n\n55:44.500 --> 55:47.700\n And so if we can produce these, you know, great achievements\n\n55:48.000 --> 55:52.200\n via some other means where, you know, if we look at the\n\n55:52.200 --> 55:55.000\n scientific enterprise, we've produced incredible things\n\n55:55.000 --> 55:58.800\n often from people who aren't suffering, have, you know,\n\n55:59.300 --> 56:00.000\n pretty good lives.\n\n56:00.000 --> 56:02.700\n They're just, they're driven instead of, you know, being\n\n56:02.700 --> 56:04.200\n pushed by a certain sort of anguish.\n\n56:04.200 --> 56:06.200\n They're being driven by intellectual curiosity.\n\n56:06.200 --> 56:11.300\n If we can instead produce a society where it's all cavet\n\n56:11.300 --> 56:13.900\n and no stick, that's better from my perspective.\n\n56:14.000 --> 56:17.000\n Yeah, but I'm going to disagree with the notion that that's\n\n56:17.000 --> 56:21.600\n possible, but I would say most of the suffering in the world\n\n56:21.600 --> 56:22.700\n is not productive.\n\n56:23.100 --> 56:28.200\n So I would dream of effective altruism curing that suffering.\n\n56:28.200 --> 56:30.800\n Yeah, but then I would say that there is some suffering that\n\n56:30.800 --> 56:35.600\n is productive that we want to keep the because but that's\n\n56:35.600 --> 56:38.800\n not even the focus of because most of the suffering is just\n\n56:38.800 --> 56:44.100\n absurd and needs to be eliminated.\n\n56:44.100 --> 56:47.700\n So let's not even romanticize this usual notion I have,\n\n56:47.700 --> 56:51.800\n but nevertheless struggle has some kind of inherent value\n\n56:51.800 --> 56:56.900\n that to me at least, you're right.\n\n56:56.900 --> 56:59.400\n There's some elements of human nature that also have to\n\n56:59.400 --> 57:01.900\n be modified in order to cure all suffering.\n\n57:01.900 --> 57:03.900\n Yeah, I mean, there's an interesting question of whether\n\n57:03.900 --> 57:04.500\n it's possible.\n\n57:04.500 --> 57:07.000\n So at the moment, you know, most of the time we're kind\n\n57:07.000 --> 57:10.300\n of neutral and then we burn ourselves and that's negative\n\n57:10.300 --> 57:13.200\n and that's really good that we get that negative signal\n\n57:13.200 --> 57:15.300\n because it means we won't burn ourselves again.\n\n57:15.800 --> 57:21.100\n There's a question like could you design agents humans such\n\n57:21.100 --> 57:23.600\n that you're not hovering around the zero level you're hovering\n\n57:23.600 --> 57:24.500\n it like bliss.\n\n57:24.600 --> 57:26.700\n Yeah, and then you touch the flame and you're like, oh no,\n\n57:26.700 --> 57:28.300\n you're just slightly worse bliss.\n\n57:28.300 --> 57:31.800\n Yeah, but that's really bad compared to the bliss you\n\n57:31.800 --> 57:34.200\n were normally in so that you can have like a gradient of\n\n57:34.200 --> 57:37.300\n bliss instead of like pain and pleasure on that point.\n\n57:37.300 --> 57:41.100\n I think it's a really important point on the experience\n\n57:41.200 --> 57:45.600\n of suffering the relative nature of it.\n\n57:46.500 --> 57:51.500\n Maybe having grown up in the Soviet Union were quite poor\n\n57:52.100 --> 57:57.700\n by any measure and when I when I was in my childhood,\n\n57:58.100 --> 58:01.000\n but it didn't feel like you're poor because everybody around\n\n58:01.000 --> 58:06.100\n you were poor there's a and then in America, I feel I for\n\n58:06.100 --> 58:09.100\n the first time begin to feel poor.\n\n58:09.200 --> 58:09.500\n Yeah.\n\n58:09.500 --> 58:11.900\n Yeah, because of the road there's different.\n\n58:11.900 --> 58:15.200\n There's some cultural aspects to it that really emphasize\n\n58:15.200 --> 58:16.500\n that it's good to be rich.\n\n58:17.200 --> 58:19.500\n And then there's just the notion that there is a lot of\n\n58:19.500 --> 58:23.000\n income inequality and therefore you experience that inequality.\n\n58:23.000 --> 58:23.900\n That's where suffering go.\n\n58:24.200 --> 58:27.400\n Do you so what do you think about the inequality of suffering\n\n58:27.400 --> 58:32.900\n that that we have to think about do you think we have to\n\n58:32.900 --> 58:37.200\n think about that as part of effective altruism?\n\n58:37.300 --> 58:40.700\n Yeah, I think we're just things vary in terms of whether\n\n58:41.800 --> 58:45.000\n you get benefits or costs from them just in relative terms\n\n58:45.000 --> 58:46.300\n or in absolute terms.\n\n58:46.700 --> 58:49.300\n So a lot of the time yeah, there's this hedonic treadmill\n\n58:49.300 --> 58:56.800\n where if you get you know, there's money is useful because\n\n58:56.800 --> 58:59.700\n it helps you buy things or good for you because it helps\n\n58:59.700 --> 59:02.100\n you buy things, but there's also a status component too\n\n59:02.500 --> 59:06.600\n and that status component is kind of zero sum if you were\n\n59:06.600 --> 59:10.900\n saying like in Russia, you know, no one else felt poor\n\n59:10.900 --> 59:13.500\n because everyone around you is poor.\n\n59:13.500 --> 59:16.600\n Whereas now you've got this these other people who are\n\n59:17.600 --> 59:19.700\n you know super rich and maybe that makes you feel.\n\n59:22.600 --> 59:24.100\n You know less good about yourself.\n\n59:24.100 --> 59:27.300\n There are some other things however, which are just\n\n59:27.300 --> 59:28.800\n intrinsically good or bad.\n\n59:28.800 --> 59:33.000\n So commuting for example, it's just people hate it.\n\n59:33.000 --> 59:35.500\n It doesn't really change knowing the other people are\n\n59:35.500 --> 59:40.000\n commuting to doesn't make it any any kind of less bad,\n\n59:40.000 --> 59:42.800\n but it's sort of to push back on that for a second.\n\n59:42.800 --> 59:48.300\n I mean, yes, but also if some people were, you know on\n\n59:48.300 --> 59:52.200\n horseback your commute on the train might feel a lot better.\n\n59:52.200 --> 59:55.400\n Yeah, you know the there is a relative Nick.\n\n59:55.400 --> 59:59.400\n I mean everybody's complaining about society today forgetting\n\n59:59.400 --> 1:00:04.400\n it's forgetting how much better is the better angels of\n\n1:00:04.400 --> 1:00:07.200\n our nature how the technologies improve fundamentally\n\n1:00:07.200 --> 1:00:09.200\n improving most of the world's lives.\n\n1:00:09.300 --> 1:00:13.000\n Yeah, and actually there's some psychological research\n\n1:00:13.000 --> 1:00:16.800\n on the well being benefits of volunteering where people\n\n1:00:16.800 --> 1:00:20.900\n who volunteer tend to just feel happier about their lives\n\n1:00:20.900 --> 1:00:23.600\n and one of the suggested explanations is it because it\n\n1:00:23.700 --> 1:00:25.200\n extends your reference class.\n\n1:00:25.600 --> 1:00:28.700\n So no longer you comparing yourself to the Joneses who\n\n1:00:28.700 --> 1:00:31.500\n have their slightly better car because you realize that\n\n1:00:31.500 --> 1:00:34.300\n you know people in much worse conditions than you and\n\n1:00:34.300 --> 1:00:37.600\n so now, you know your life doesn't seem so bad.\n\n1:00:37.900 --> 1:00:39.800\n That's actually on the psychological level.\n\n1:00:39.800 --> 1:00:42.600\n One of the fundamental benefits of effective altruism.\n\n1:00:42.700 --> 1:00:47.700\n Yeah is is I mean, I guess it's the altruism part of\n\n1:00:47.700 --> 1:00:51.700\n effective altruism is exposing yourself to the suffering\n\n1:00:51.700 --> 1:00:54.700\n in the world allows you to be more.\n\n1:00:55.700 --> 1:00:59.900\n Yeah happier and actually allows you in the sort of\n\n1:00:59.900 --> 1:01:03.000\n meditative introspective way realize that you don't need\n\n1:01:03.000 --> 1:01:07.400\n most of the wealth you have to to be happy.\n\n1:01:07.800 --> 1:01:08.300\n Absolutely.\n\n1:01:08.300 --> 1:01:10.400\n I mean, I think effective options have been this huge\n\n1:01:10.400 --> 1:01:13.400\n benefit for me and I really don't think that if I had\n\n1:01:13.400 --> 1:01:16.400\n more money that I was living on that that would change\n\n1:01:16.400 --> 1:01:17.900\n my level of well being at all.\n\n1:01:18.100 --> 1:01:21.500\n Whereas engaging in something that I think is meaningful\n\n1:01:21.500 --> 1:01:25.100\n that I think is stealing humanity in a positive direction.\n\n1:01:25.200 --> 1:01:26.400\n That's extremely rewarding.\n\n1:01:27.400 --> 1:01:32.400\n And so yeah, I mean despite my best attempts at sacrifice.\n\n1:01:32.500 --> 1:01:35.000\n Um, I don't you know, I think I've actually ended up\n\n1:01:35.000 --> 1:01:37.500\n happier as a result of engaging in effective altruism\n\n1:01:37.500 --> 1:01:38.700\n than I would have done.\n\n1:01:38.800 --> 1:01:40.200\n That's such an interesting idea.\n\n1:01:40.300 --> 1:01:43.200\n Yeah, so let's let's talk about animal welfare.\n\n1:01:43.200 --> 1:01:46.400\n Sure, easy question. What is consciousness?\n\n1:01:46.700 --> 1:01:50.400\n Yeah, especially as it has to do with the capacity to\n\n1:01:50.400 --> 1:01:53.600\n suffer. I think there seems to be a connection between\n\n1:01:53.600 --> 1:01:57.100\n how conscious something is the amount of consciousness\n\n1:01:57.400 --> 1:02:01.100\n and stability to suffer and that all comes into play\n\n1:02:01.100 --> 1:02:03.300\n about us thinking how much suffering there's in the\n\n1:02:03.300 --> 1:02:05.600\n world with regard to animals.\n\n1:02:05.600 --> 1:02:08.600\n So how do you think about animal welfare and consciousness?\n\n1:02:08.700 --> 1:02:09.100\n Okay.\n\n1:02:09.200 --> 1:02:10.700\n Well consciousness easy question.\n\n1:02:10.700 --> 1:02:11.100\n Okay.\n\n1:02:11.100 --> 1:02:13.800\n Um, yeah, I mean, I think we don't have a good understanding\n\n1:02:13.800 --> 1:02:14.500\n of consciousness.\n\n1:02:14.500 --> 1:02:17.000\n My best guess is it's got and by consciousness.\n\n1:02:17.000 --> 1:02:21.200\n I'm meaning what it is feels like to be you the subjective\n\n1:02:21.200 --> 1:02:24.000\n experience that's seems to be different from everything\n\n1:02:24.000 --> 1:02:25.300\n else we know about in the world.\n\n1:02:26.000 --> 1:02:27.400\n Yeah, I think it's clear.\n\n1:02:27.400 --> 1:02:29.300\n It's very poorly understood at the moment.\n\n1:02:29.400 --> 1:02:31.700\n I think it has something to do with information processing.\n\n1:02:32.000 --> 1:02:35.000\n So the fact that the brain is a computer or something\n\n1:02:35.000 --> 1:02:35.800\n like a computer.\n\n1:02:36.300 --> 1:02:40.300\n So that would mean that very advanced AI could be conscious\n\n1:02:40.300 --> 1:02:43.800\n of information processors in general could be conscious\n\n1:02:44.000 --> 1:02:48.300\n with some suitable complexity, but that also some suitable\n\n1:02:48.300 --> 1:02:49.100\n complexity.\n\n1:02:49.200 --> 1:02:51.500\n It's a question whether greater complexity creates some\n\n1:02:51.500 --> 1:02:54.800\n kind of greater consciousness which relates to animals.\n\n1:02:54.900 --> 1:02:55.600\n Yeah, right.\n\n1:02:55.600 --> 1:02:59.400\n Is there if it's an information processing system and it's\n\n1:02:59.400 --> 1:03:03.700\n smaller and smaller is an ant less conscious than a cow\n\n1:03:04.100 --> 1:03:06.200\n less conscious than a monkey.\n\n1:03:06.200 --> 1:03:10.900\n Yeah, and again this super hard question, but I think my\n\n1:03:10.900 --> 1:03:14.500\n best guess is yes, like if you if I think well consciousness,\n\n1:03:14.500 --> 1:03:17.700\n it's not some magical thing that appears out of nowhere.\n\n1:03:17.700 --> 1:03:20.800\n It's not you know, Descartes thought it was just comes in\n\n1:03:20.800 --> 1:03:23.600\n from this other realm and then enters through the pineal\n\n1:03:23.600 --> 1:03:27.300\n gland in your brain and that's kind of soul and it's conscious.\n\n1:03:28.400 --> 1:03:30.200\n So it's got something to do with what's going on in your\n\n1:03:30.200 --> 1:03:30.700\n brain.\n\n1:03:30.700 --> 1:03:34.200\n A chicken has one three hundredth of the size of the brain\n\n1:03:34.200 --> 1:03:36.100\n that you have ants.\n\n1:03:36.100 --> 1:03:37.300\n I don't know how small it is.\n\n1:03:37.500 --> 1:03:41.900\n Maybe it's a millionth the size my best guess which I may\n\n1:03:41.900 --> 1:03:45.300\n well be wrong about because this is so hard is that in some\n\n1:03:45.300 --> 1:03:49.400\n relevant sense the chicken is experiencing consciousness\n\n1:03:49.400 --> 1:03:51.900\n to a less degree than the human and the ants significantly\n\n1:03:51.900 --> 1:03:52.500\n less again.\n\n1:03:52.900 --> 1:03:55.400\n I don't think it's as little as three hundredth as much.\n\n1:03:55.400 --> 1:03:59.100\n I think there's everyone who's ever seen a chicken that's\n\n1:03:59.100 --> 1:04:02.500\n there's evolutionary reasons for thinking that like the\n\n1:04:02.500 --> 1:04:06.000\n ability to feel pain comes on the scene relatively early\n\n1:04:06.000 --> 1:04:08.800\n on and we have lots of our brain that's dedicated stuff\n\n1:04:08.800 --> 1:04:10.800\n that doesn't seem to have to do in anything to do with\n\n1:04:10.800 --> 1:04:12.800\n consciousness language processing and so on.\n\n1:04:13.900 --> 1:04:16.900\n So it seems like the easy so there's a lot of complicated\n\n1:04:16.900 --> 1:04:21.300\n questions there that we can't ask the animals about but\n\n1:04:21.300 --> 1:04:24.800\n it seems that there is easy questions in terms of suffering\n\n1:04:24.800 --> 1:04:29.100\n which is things like factory farming that could be addressed.\n\n1:04:29.400 --> 1:04:32.300\n Yeah, is that is that the lowest hanging fruit?\n\n1:04:32.300 --> 1:04:36.600\n If I may use crude terms here of animal welfare.\n\n1:04:37.000 --> 1:04:37.700\n Absolutely.\n\n1:04:37.700 --> 1:04:39.100\n I think that's the lowest hanging fruit.\n\n1:04:39.100 --> 1:04:43.200\n So at the moment we kill we raise and kill about 50 billion\n\n1:04:43.200 --> 1:04:44.400\n animals every year.\n\n1:04:44.600 --> 1:04:47.800\n So how many 50 billion in?\n\n1:04:48.000 --> 1:04:52.300\n Yeah, so for every human on the planet several times that\n\n1:04:52.300 --> 1:04:55.200\n number of being killed and the vast majority of them are\n\n1:04:55.200 --> 1:04:59.200\n raised in factory farms where basically whatever your view\n\n1:04:59.200 --> 1:05:02.400\n on animals, I think you should agree even if you think well,\n\n1:05:02.400 --> 1:05:03.900\n maybe it's not bad to kill an animal.\n\n1:05:03.900 --> 1:05:06.500\n Maybe if the animal was raised in good conditions, that's\n\n1:05:06.500 --> 1:05:07.900\n just not the empirical reality.\n\n1:05:07.900 --> 1:05:11.700\n The empirical reality is that they are kept in incredible\n\n1:05:11.700 --> 1:05:12.900\n cage confinement.\n\n1:05:12.900 --> 1:05:18.000\n They are de beaked or detailed without an aesthetic, you\n\n1:05:18.000 --> 1:05:20.900\n know chickens often peck each other to death other like\n\n1:05:20.900 --> 1:05:22.600\n otherwise because of them such stress.\n\n1:05:23.800 --> 1:05:26.900\n It's really, you know, I think when a chicken gets killed\n\n1:05:26.900 --> 1:05:29.200\n that's the best thing that happened to the chicken in the\n\n1:05:29.200 --> 1:05:32.500\n course of its life and it's also completely unnecessary.\n\n1:05:32.700 --> 1:05:35.900\n This is in order to save, you know a few pence for the price\n\n1:05:35.900 --> 1:05:41.400\n of meat or price of eggs and we have indeed found it's also\n\n1:05:41.400 --> 1:05:44.500\n just inconsistent with consumer preference as well people\n\n1:05:44.500 --> 1:05:49.000\n who buy the products if they could they all they when you\n\n1:05:49.000 --> 1:05:52.500\n do surveys are extremely against suffering in factory farms.\n\n1:05:52.800 --> 1:05:55.300\n It's just they don't appreciate how bad it is and you know,\n\n1:05:55.300 --> 1:05:56.900\n just tend to go with easy options.\n\n1:05:57.500 --> 1:06:00.800\n And so then the best the most effective programs I know of\n\n1:06:00.800 --> 1:06:04.700\n at the moment are nonprofits that go to companies and work\n\n1:06:04.700 --> 1:06:09.900\n with companies to get them to take a pledge to cut certain\n\n1:06:09.900 --> 1:06:12.900\n sorts of animal products like eggs from cage confinement\n\n1:06:13.200 --> 1:06:14.700\n out of their supply chain.\n\n1:06:14.700 --> 1:06:19.400\n And it's now the case that the top 50 food retailers and\n\n1:06:19.400 --> 1:06:23.700\n fast food companies have all made these kind of cage free\n\n1:06:23.700 --> 1:06:27.000\n pledges and when you do the numbers you get the conclusion\n\n1:06:27.000 --> 1:06:29.800\n that every dollar you're giving to these nonprofits result\n\n1:06:29.800 --> 1:06:33.000\n in hundreds of chickens being spared from cage confinement.\n\n1:06:33.300 --> 1:06:37.600\n And then they're working to other other types of animals\n\n1:06:37.600 --> 1:06:38.500\n other products too.\n\n1:06:39.300 --> 1:06:43.300\n So is that the most effective way to do in have a ripple\n\n1:06:43.300 --> 1:06:47.600\n effect essentially it's supposed to directly having regulation\n\n1:06:48.100 --> 1:06:50.600\n from on top that says you can't do this.\n\n1:06:51.500 --> 1:06:54.700\n So I would be more open to the regulation approach, but\n\n1:06:55.500 --> 1:06:59.000\n at least in the US there's quite intense regulatory capture\n\n1:06:59.100 --> 1:07:00.700\n from the agricultural industry.\n\n1:07:01.000 --> 1:07:05.300\n And so attempts that we've seen to try and change regulation\n\n1:07:05.800 --> 1:07:08.700\n have it's been a real uphill struggle.\n\n1:07:08.700 --> 1:07:13.300\n There are some examples of ballot initiatives where the\n\n1:07:13.300 --> 1:07:16.500\n people have been able to vote in a ballot to say we want\n\n1:07:16.500 --> 1:07:19.600\n to ban eggs from cage conditions and that's been huge.\n\n1:07:19.600 --> 1:07:22.600\n That's been really good, but beyond that it's much more\n\n1:07:22.600 --> 1:07:27.500\n limited. So I've been really interested in the idea of\n\n1:07:27.500 --> 1:07:32.800\n hunting in general and wild animals and seeing nature as\n\n1:07:32.800 --> 1:07:41.300\n a form of cruelty that I am ethically more okay with.\n\n1:07:41.400 --> 1:07:46.100\n Okay, just from my perspective and then I read about wild\n\n1:07:46.100 --> 1:07:48.900\n animal suffering that I'm just I'm just giving you the\n\n1:07:48.900 --> 1:07:53.900\n kind of yeah notion of how I felt because animal because\n\n1:07:53.900 --> 1:07:57.000\n animal factory farming is so bad.\n\n1:07:57.000 --> 1:08:00.100\n Yeah that living in the woods seem good.\n\n1:08:00.100 --> 1:08:04.200\n Yeah, and yet when you actually start to think about it\n\n1:08:04.300 --> 1:08:08.600\n all I mean all of the animals in the animal world the\n\n1:08:08.600 --> 1:08:11.100\n living in like terrible poverty, right?\n\n1:08:11.300 --> 1:08:11.600\n Yeah.\n\n1:08:11.600 --> 1:08:15.000\n Yeah, so you have all the medical conditions all of that.\n\n1:08:15.100 --> 1:08:17.000\n I mean they're living horrible lives.\n\n1:08:17.000 --> 1:08:18.200\n It could be improved.\n\n1:08:18.700 --> 1:08:21.400\n That's a really interesting notion that I think may not\n\n1:08:21.400 --> 1:08:24.600\n even be useful to talk about because factory farming is\n\n1:08:24.600 --> 1:08:26.400\n such a big thing to focus on.\n\n1:08:26.500 --> 1:08:29.800\n Yeah, but it's nevertheless an interesting notion to think\n\n1:08:29.800 --> 1:08:32.900\n of all the animals in the wild as suffering in the same\n\n1:08:32.900 --> 1:08:34.900\n way that humans in poverty are suffering.\n\n1:08:34.900 --> 1:08:38.400\n Yeah, I mean and often even worse so many animals we\n\n1:08:38.400 --> 1:08:39.800\n produce by our selection.\n\n1:08:39.800 --> 1:08:44.700\n So you have a very large number of children in the expectation\n\n1:08:44.700 --> 1:08:46.300\n that only a small number survive.\n\n1:08:46.700 --> 1:08:49.900\n And so for those animals almost all of them just live short\n\n1:08:49.900 --> 1:08:51.700\n lives where they starve to death.\n\n1:08:53.100 --> 1:08:55.100\n So yeah, there's huge amounts of suffering in nature that\n\n1:08:55.100 --> 1:09:00.000\n I don't think we should you know pretend that it's this kind\n\n1:09:00.000 --> 1:09:02.800\n of wonderful paradise for most animals.\n\n1:09:04.900 --> 1:09:09.600\n Yeah, their life is filled with hunger and fear and disease.\n\n1:09:10.400 --> 1:09:13.600\n Yeah, I did agree with you entirely that when it comes\n\n1:09:13.600 --> 1:09:15.700\n to focusing on animal welfare, we should focus in factory\n\n1:09:15.700 --> 1:09:20.400\n farming, but we also yeah should be aware to the reality\n\n1:09:20.400 --> 1:09:22.300\n of what life for most animals is like.\n\n1:09:22.300 --> 1:09:26.400\n So let's talk about a topic I've talked a lot about and\n\n1:09:26.400 --> 1:09:29.700\n you've actually quite eloquently talked about which is the\n\n1:09:29.700 --> 1:09:34.900\n third priority that effective altruism considers is really\n\n1:09:34.900 --> 1:09:37.600\n important is existential risks.\n\n1:09:37.600 --> 1:09:41.500\n Yeah, when you think about the existential risks that\n\n1:09:41.500 --> 1:09:45.600\n are facing our civilization, what's before us?\n\n1:09:45.600 --> 1:09:46.600\n What concerns you?\n\n1:09:46.600 --> 1:09:49.200\n What should we be thinking about from in the especially\n\n1:09:49.200 --> 1:09:51.100\n from an effective altruism perspective?\n\n1:09:51.100 --> 1:09:53.900\n Great. So the reason I started getting concerned about\n\n1:09:53.900 --> 1:09:59.500\n this was thinking about future generations where the key\n\n1:09:59.500 --> 1:10:02.000\n idea is just well future people matter morally.\n\n1:10:03.200 --> 1:10:05.300\n There are vast numbers of future people.\n\n1:10:05.300 --> 1:10:07.400\n If we don't cause our own extinction, there's no reason\n\n1:10:07.400 --> 1:10:11.100\n why civilization might not last a million years.\n\n1:10:11.900 --> 1:10:14.200\n I mean we last as long as a typical mammalian species\n\n1:10:14.500 --> 1:10:18.700\n or a billion years is when the Earth is no longer habitable\n\n1:10:18.700 --> 1:10:21.500\n or if we can take to the stars then perhaps it's trillions\n\n1:10:21.500 --> 1:10:22.400\n of years beyond that.\n\n1:10:23.100 --> 1:10:25.500\n So the future could be very big indeed and it seems like\n\n1:10:25.500 --> 1:10:27.700\n we're potentially very early on in civilization.\n\n1:10:29.000 --> 1:10:31.100\n Then the second idea is just well, maybe there are things\n\n1:10:31.100 --> 1:10:33.600\n that are going to really derail that things that actually\n\n1:10:33.600 --> 1:10:36.900\n could prevent us from having this long wonderful civilization\n\n1:10:37.400 --> 1:10:43.600\n and instead could cause our own cause our own extinction\n\n1:10:43.900 --> 1:10:48.100\n or otherwise perhaps like lock ourselves into a very bad\n\n1:10:48.100 --> 1:10:53.100\n state. And what ways could that happen?\n\n1:10:53.100 --> 1:10:56.700\n Well causing our own extinction development of nuclear\n\n1:10:56.700 --> 1:11:00.600\n weapons in the 20th century at least put on the table\n\n1:11:00.600 --> 1:11:03.200\n that we now had weapons that were powerful enough that\n\n1:11:04.100 --> 1:11:07.300\n you could very significantly destroy society perhaps\n\n1:11:07.600 --> 1:11:09.900\n and all that nuclear war would cause a nuclear winter.\n\n1:11:09.900 --> 1:11:14.100\n Perhaps that would be enough for the human race to go\n\n1:11:14.100 --> 1:11:14.700\n extinct.\n\n1:11:14.700 --> 1:11:18.000\n Why do you think we haven't done it? Sorry to interrupt.\n\n1:11:18.000 --> 1:11:19.300\n Why do you think we haven't done it yet?\n\n1:11:19.300 --> 1:11:26.800\n Is it surprising to you that having, you know, always\n\n1:11:26.800 --> 1:11:30.500\n for the past few decades several thousand of active ready\n\n1:11:30.500 --> 1:11:35.400\n to launch nuclear weapons warheads and yet we have not\n\n1:11:35.400 --> 1:11:42.100\n launched them ever since the initial launch on Hiroshima\n\n1:11:42.100 --> 1:11:42.900\n and Nagasaki.\n\n1:11:42.900 --> 1:11:46.200\n I think it's a mix of luck.\n\n1:11:46.400 --> 1:11:48.300\n So I think it's definitely not inevitable that we haven't\n\n1:11:48.300 --> 1:11:48.800\n used them.\n\n1:11:49.300 --> 1:11:52.300\n So John F. Kennedy, general Cuban Missile Crisis put the\n\n1:11:52.300 --> 1:11:55.700\n estimate of nuclear exchange between the US and USSR\n\n1:11:55.700 --> 1:11:59.100\n that somewhere between one and three and even so, you know,\n\n1:11:59.100 --> 1:12:00.300\n we really did come close.\n\n1:12:03.000 --> 1:12:06.000\n At the same time, I do think mutually assured destruction\n\n1:12:06.900 --> 1:12:08.600\n is a reason why people don't go to war.\n\n1:12:08.600 --> 1:12:11.900\n It would be, you know, why nuclear powers don't go to war.\n\n1:12:11.900 --> 1:12:15.200\n Do you think that holds if you can linger on that for a\n\n1:12:15.200 --> 1:12:20.100\n second, like my dad is a physicist amongst other things\n\n1:12:20.600 --> 1:12:24.900\n and he believes that nuclear weapons are actually just\n\n1:12:24.900 --> 1:12:29.600\n really hard to build which is one of the really big benefits\n\n1:12:29.600 --> 1:12:34.600\n of them currently so that you don't have it's very hard\n\n1:12:34.600 --> 1:12:38.200\n if you're crazy to build to acquire a nuclear weapon.\n\n1:12:38.700 --> 1:12:41.200\n So the mutually shared destruction really works when you\n\n1:12:41.200 --> 1:12:46.200\n talk seems to work better when it's nation states, when\n\n1:12:46.200 --> 1:12:49.900\n it's serious people, even if they're a little bit, you\n\n1:12:49.900 --> 1:12:52.000\n know, dictatorial and so on.\n\n1:12:52.900 --> 1:12:56.200\n Do you think this mutually sure destruction idea will\n\n1:12:56.200 --> 1:13:01.000\n carry how far will it carry us in terms of different kinds\n\n1:13:01.000 --> 1:13:01.800\n of weapons?\n\n1:13:02.200 --> 1:13:06.600\n Oh, yeah, I think it's your point that nuclear weapons\n\n1:13:06.700 --> 1:13:09.600\n are very hard to build and relatively easy to control\n\n1:13:09.600 --> 1:13:12.700\n because you can control fissile material is a really\n\n1:13:12.700 --> 1:13:16.000\n important one and future technology that's equally destructive\n\n1:13:16.000 --> 1:13:18.000\n might not have those properties.\n\n1:13:18.500 --> 1:13:23.600\n So for example, if in the future people are able to design\n\n1:13:23.700 --> 1:13:29.600\n viruses, perhaps using a DNA printing kit that's on that,\n\n1:13:29.600 --> 1:13:31.300\n you know, one can just buy.\n\n1:13:31.300 --> 1:13:37.500\n In fact, there are companies in the process of creating\n\n1:13:37.500 --> 1:13:42.800\n home DNA printing kits. Well, then perhaps that's just\n\n1:13:42.800 --> 1:13:44.000\n totally democratized.\n\n1:13:44.000 --> 1:13:48.600\n Perhaps the power to reap huge destructive potential is\n\n1:13:48.600 --> 1:13:52.000\n in the hands of most people in the world or certainly\n\n1:13:52.000 --> 1:13:55.300\n most people with effort and then yeah, I no longer trust\n\n1:13:55.300 --> 1:13:59.300\n mutually assured destruction because some for some people\n\n1:13:59.500 --> 1:14:01.800\n the idea that they would die is just not a disincentive.\n\n1:14:03.600 --> 1:14:05.200\n There was a Japanese cult, for example.\n\n1:14:05.200 --> 1:14:10.400\n Ohm Shinrikyo in the 90s that had they what they believed\n\n1:14:10.400 --> 1:14:14.800\n was that Armageddon was coming if you died before Armageddon,\n\n1:14:14.800 --> 1:14:17.200\n you would get good karma.\n\n1:14:17.200 --> 1:14:20.300\n You wouldn't go to hell if you died during Armageddon.\n\n1:14:20.300 --> 1:14:25.500\n Maybe you would go to hell and they had a biological weapons\n\n1:14:25.500 --> 1:14:28.600\n program chemical weapons program when they were finally\n\n1:14:28.600 --> 1:14:29.300\n apprehended.\n\n1:14:29.300 --> 1:14:33.500\n They hadn't stocks of southern gas that were sufficient to\n\n1:14:33.500 --> 1:14:36.600\n kill 4 million people engaged in multiple terrorist acts.\n\n1:14:36.900 --> 1:14:40.300\n If they had had the ability to print a virus at home,\n\n1:14:40.300 --> 1:14:41.500\n that would have been very scary.\n\n1:14:42.500 --> 1:14:45.900\n So it's not impossible to imagine groups of people that\n\n1:14:45.900 --> 1:14:54.200\n hold that kind of belief of death as suicide as a good\n\n1:14:54.200 --> 1:14:58.100\n thing for passage into the next world and so on and then\n\n1:14:58.100 --> 1:15:03.800\n connect them with some weapons then ideology and weaponry\n\n1:15:04.400 --> 1:15:07.000\n may create serious problems for us.\n\n1:15:07.000 --> 1:15:09.800\n Let me ask you a quick question on what do you think is\n\n1:15:09.800 --> 1:15:13.800\n the line between killing most humans and killing all humans?\n\n1:15:14.300 --> 1:15:17.500\n How hard is it to kill everybody?\n\n1:15:17.600 --> 1:15:19.000\n Yeah, have you thought about this?\n\n1:15:19.800 --> 1:15:20.700\n I've thought about it a bit.\n\n1:15:20.700 --> 1:15:22.600\n I think it is very hard to kill everybody.\n\n1:15:22.600 --> 1:15:26.600\n So in the case of let's say an all out nuclear exchange\n\n1:15:26.600 --> 1:15:28.300\n and let's say that leads to nuclear winter.\n\n1:15:28.300 --> 1:15:32.700\n We don't really know but we you know might well happen\n\n1:15:34.400 --> 1:15:38.300\n that would I think result in billions of deaths would\n\n1:15:38.300 --> 1:15:39.300\n it kill everybody?\n\n1:15:39.500 --> 1:15:42.600\n It's quite it's quite hard to see how that how it would\n\n1:15:42.600 --> 1:15:45.500\n kill everybody for a few reasons.\n\n1:15:45.500 --> 1:15:47.500\n One is just those are so many people.\n\n1:15:47.900 --> 1:15:49.600\n Yes, you know seven and a half billion people.\n\n1:15:49.600 --> 1:15:54.200\n So this bad event has to kill all you know, all almost\n\n1:15:54.200 --> 1:15:54.800\n all of them.\n\n1:15:54.800 --> 1:15:57.600\n Secondly live in such a diversity of locations.\n\n1:15:57.600 --> 1:16:00.800\n So a nuclear exchange or the virus that has to kill people\n\n1:16:00.800 --> 1:16:04.600\n who live in the coast of New Zealand which is going to\n\n1:16:04.600 --> 1:16:08.700\n be climatically much more stable than other areas in the\n\n1:16:08.700 --> 1:16:14.400\n world or people who are on submarines or who have access\n\n1:16:14.400 --> 1:16:15.000\n to bunkers.\n\n1:16:15.000 --> 1:16:18.000\n So there's a very like there's just like I'm sure there's\n\n1:16:18.000 --> 1:16:20.800\n like two guys in Siberia just badass.\n\n1:16:20.800 --> 1:16:25.400\n There's the just human nature somehow just perseveres.\n\n1:16:25.400 --> 1:16:28.400\n Yeah, and then the second thing is just if there's some\n\n1:16:28.400 --> 1:16:31.600\n catastrophic event people really don't want to die.\n\n1:16:31.600 --> 1:16:34.200\n So there's going to be like, you know, huge amounts of\n\n1:16:34.200 --> 1:16:37.100\n effort to ensure that it doesn't affect everyone.\n\n1:16:37.100 --> 1:16:42.200\n Have you thought about what it takes to rebuild a society\n\n1:16:42.200 --> 1:16:45.400\n with smaller smaller numbers like how big of a setback\n\n1:16:45.400 --> 1:16:47.200\n these kinds of things are?\n\n1:16:47.200 --> 1:16:50.100\n Yeah, so then that's something where there's real uncertainty\n\n1:16:50.100 --> 1:16:55.100\n I think where at some point you just lose genetic sufficient\n\n1:16:55.100 --> 1:16:57.500\n genetic diversity such that you can't come back.\n\n1:16:58.300 --> 1:17:03.700\n There's it's unclear how small that population is.\n\n1:17:03.700 --> 1:17:07.300\n But if you've only got say a thousand people or fewer\n\n1:17:07.300 --> 1:17:09.100\n than a thousand, then maybe that's small enough.\n\n1:17:09.100 --> 1:17:12.900\n What about human knowledge and then there's human knowledge.\n\n1:17:14.900 --> 1:17:19.400\n I mean, it's striking how short on geological timescales\n\n1:17:19.400 --> 1:17:23.200\n or evolutionary timescales the progress in or how quickly\n\n1:17:23.200 --> 1:17:26.000\n the progress in human knowledge has been like agriculture.\n\n1:17:26.000 --> 1:17:31.600\n We only invented in 10,000 BC cities were only, you know,\n\n1:17:31.600 --> 1:17:35.500\n 3000 BC whereas typical mammal species is half a million\n\n1:17:35.500 --> 1:17:36.900\n years to a million years.\n\n1:17:37.400 --> 1:17:40.200\n Do you think it's inevitable in some sense agriculture\n\n1:17:40.200 --> 1:17:45.800\n everything that came the Industrial Revolution cars planes\n\n1:17:45.800 --> 1:17:50.700\n the internet that level of innovation you think is inevitable.\n\n1:17:50.700 --> 1:17:55.200\n I think given how quickly it arose.\n\n1:17:55.200 --> 1:17:58.000\n So in the case of agriculture, I think that was dependent\n\n1:17:58.000 --> 1:17:58.500\n on climate.\n\n1:17:58.500 --> 1:18:05.600\n So it was the kind of glacial period was over the earth\n\n1:18:05.600 --> 1:18:10.300\n warmed up a bit that made it much more likely that humans\n\n1:18:10.300 --> 1:18:14.000\n would develop agriculture when it comes to the Industrial\n\n1:18:14.000 --> 1:18:19.100\n Revolution. It's just you know, again only took a few thousand\n\n1:18:19.100 --> 1:18:22.700\n years from cities to Industrial Revolution if we think okay,\n\n1:18:22.700 --> 1:18:26.700\n we've gone back to this even let's say agricultural era,\n\n1:18:27.300 --> 1:18:29.600\n but there's no reason why we wouldn't go extinct in the\n\n1:18:29.600 --> 1:18:32.200\n coming tens of thousands of years or hundreds of thousands\n\n1:18:32.200 --> 1:18:32.700\n of years.\n\n1:18:33.100 --> 1:18:34.100\n It seems just vet.\n\n1:18:34.200 --> 1:18:37.500\n It would be very surprising if we didn't rebound unless\n\n1:18:37.500 --> 1:18:39.800\n there's some special reason that makes things different.\n\n1:18:40.000 --> 1:18:40.400\n Yes.\n\n1:18:40.400 --> 1:18:44.600\n So perhaps we just have a much greater like disease burden\n\n1:18:44.600 --> 1:18:46.600\n now so HIV exists.\n\n1:18:46.600 --> 1:18:50.500\n It didn't exist before and perhaps that's kind of latent\n\n1:18:50.500 --> 1:18:53.500\n and you know and being suppressed by modern medicine\n\n1:18:53.500 --> 1:18:57.800\n and sanitation and so on but would be a much bigger problem\n\n1:18:57.800 --> 1:19:02.600\n for some, you know, utterly destroyed the society that\n\n1:19:02.600 --> 1:19:06.600\n was trying to rebound or there's just maybe there's something\n\n1:19:06.600 --> 1:19:07.500\n we don't know about.\n\n1:19:07.500 --> 1:19:14.400\n So another existential risk comes from the mysterious the\n\n1:19:14.400 --> 1:19:16.600\n beautiful artificial intelligence.\n\n1:19:16.600 --> 1:19:16.900\n Yeah.\n\n1:19:17.500 --> 1:19:22.000\n So what what's the shape of your concerns about AI?\n\n1:19:22.700 --> 1:19:25.300\n I think there are quite a lot of concerns about AI and\n\n1:19:25.300 --> 1:19:29.700\n sometimes the different risks don't get distinguished enough.\n\n1:19:30.400 --> 1:19:35.400\n So the kind of classic worry most is closely associated\n\n1:19:35.400 --> 1:19:39.900\n with Nick Bostrom and Elias Joukowski is that we at some\n\n1:19:39.900 --> 1:19:43.000\n point move from having narrow AI systems to artificial\n\n1:19:43.000 --> 1:19:44.100\n general intelligence.\n\n1:19:44.400 --> 1:19:48.300\n You get this very fast feedback effect where AGI is able\n\n1:19:48.300 --> 1:19:51.300\n to build, you know, artificial intelligence helps you to\n\n1:19:51.300 --> 1:19:53.600\n build greater artificial intelligence.\n\n1:19:53.900 --> 1:19:57.100\n We have this one system that suddenly very powerful far\n\n1:19:57.100 --> 1:20:01.000\n more powerful than others than perhaps far more powerful\n\n1:20:01.000 --> 1:20:07.000\n than, you know, the rest of the world combined and then\n\n1:20:07.000 --> 1:20:10.000\n secondly, it has goals that are misaligned with human goals.\n\n1:20:10.400 --> 1:20:13.000\n And so it pursues its own goals.\n\n1:20:13.000 --> 1:20:16.500\n It realize, hey, there's this competition namely from humans.\n\n1:20:16.500 --> 1:20:19.300\n It would be better if we eliminated them in just the same\n\n1:20:19.300 --> 1:20:22.700\n way as homo sapiens eradicated the Neanderthals.\n\n1:20:22.700 --> 1:20:28.400\n In fact, it in fact killed off most large animals on the\n\n1:20:28.400 --> 1:20:32.200\n planet that walk the planet. So that's kind of one set of\n\n1:20:32.200 --> 1:20:37.700\n worries. I think that's not my I think these shouldn't\n\n1:20:37.700 --> 1:20:39.200\n be dismissed as science fiction.\n\n1:20:41.000 --> 1:20:44.500\n I think it's something we should be taking very seriously,\n\n1:20:44.800 --> 1:20:47.200\n but it's not the thing you visualize when you're concerned\n\n1:20:47.200 --> 1:20:49.300\n about the biggest near term.\n\n1:20:49.700 --> 1:20:54.100\n Yeah, I think it's I think it's like one possible scenario\n\n1:20:54.100 --> 1:20:55.400\n that would be astronomically bad.\n\n1:20:55.500 --> 1:20:57.900\n I think that other scenarios that would also be extremely\n\n1:20:57.900 --> 1:21:01.000\n bad comparably bad that are more likely to occur.\n\n1:21:01.000 --> 1:21:05.100\n So one is just we are able to control AI.\n\n1:21:05.600 --> 1:21:08.000\n So we're able to get it to do what we want it to do.\n\n1:21:10.000 --> 1:21:13.600\n And perhaps there's not like this fast takeoff of AI capabilities\n\n1:21:13.600 --> 1:21:14.700\n within a single system.\n\n1:21:14.700 --> 1:21:17.900\n It's distributed across many systems that do somewhat different\n\n1:21:17.900 --> 1:21:23.400\n things, but you do get very rapid economic and technological\n\n1:21:23.400 --> 1:21:27.000\n progress as a result that concentrates power into the hands\n\n1:21:27.000 --> 1:21:29.600\n of a very small number of individuals, perhaps a single\n\n1:21:29.600 --> 1:21:35.500\n dictator. And secondly, that single individual is or small\n\n1:21:35.500 --> 1:21:38.400\n group of individuals or single country is then able to like\n\n1:21:38.400 --> 1:21:43.100\n lock in their values indefinitely via transmitting those\n\n1:21:43.100 --> 1:21:46.300\n values to artificial systems that have no reason to die\n\n1:21:46.400 --> 1:21:49.100\n like, you know, their code is copyable.\n\n1:21:49.500 --> 1:21:53.900\n Perhaps, you know, Donald Trump or Xi Jinping creates their\n\n1:21:53.900 --> 1:21:58.200\n kind of AI progeny in their own image. And once you have\n\n1:21:58.200 --> 1:22:02.200\n a system that's once you have a society that's controlled\n\n1:22:02.200 --> 1:22:06.400\n by AI, you no longer have one of the main drivers of change\n\n1:22:06.400 --> 1:22:10.100\n historically, which is the fact that human lifespans are\n\n1:22:10.600 --> 1:22:12.300\n you know, only a hundred years give or take.\n\n1:22:12.300 --> 1:22:13.200\n So that's really interesting.\n\n1:22:13.200 --> 1:22:18.100\n So as opposed to sort of killing off all humans is locking\n\n1:22:18.100 --> 1:22:25.000\n in creating a hell on earth, basically a set of principles\n\n1:22:25.000 --> 1:22:28.900\n under which the society operates that's extremely undesirable.\n\n1:22:28.900 --> 1:22:31.000\n So everybody is suffering indefinitely.\n\n1:22:31.200 --> 1:22:33.900\n Or it doesn't, I mean, it also doesn't need to be hell on\n\n1:22:33.900 --> 1:22:35.700\n earth. It could just be the long values.\n\n1:22:35.700 --> 1:22:40.400\n So we talked at the very beginning about how I want to\n\n1:22:40.400 --> 1:22:43.300\n see this kind of diversity of different values and exploration\n\n1:22:43.300 --> 1:22:46.900\n so that we can just work out what is kind of morally like\n\n1:22:46.900 --> 1:22:49.600\n what is good, what is bad and then pursue the thing that's\n\n1:22:49.600 --> 1:22:55.000\n bad. So actually, so the idea of wrong values is actually\n\n1:22:55.000 --> 1:22:59.200\n probably the beautiful thing is there's no such thing as\n\n1:22:59.200 --> 1:23:01.200\n right and wrong values because we don't know the right\n\n1:23:01.200 --> 1:23:04.700\n answer. We just kind of have a sense of which value is more\n\n1:23:04.700 --> 1:23:06.200\n right, which is more wrong.\n\n1:23:06.500 --> 1:23:10.500\n So any kind of lock in makes a value wrong because it\n\n1:23:10.500 --> 1:23:13.000\n prevents exploration of this kind.\n\n1:23:13.000 --> 1:23:17.500\n Yeah, and just, you know, imagine if fascist value, you\n\n1:23:17.500 --> 1:23:21.000\n know, imagine if there was Hitler's utopia or Stalin's utopia\n\n1:23:21.000 --> 1:23:23.800\n or Donald Trump's or Xi Jinping's forever.\n\n1:23:24.100 --> 1:23:28.900\n Yeah, you know, how good or bad would that be compared\n\n1:23:28.900 --> 1:23:33.400\n to the best possible future we could create? And my suggestion\n\n1:23:33.400 --> 1:23:36.200\n is it would really suck compared to the best possible\n\n1:23:36.200 --> 1:23:37.000\n future we could create.\n\n1:23:37.000 --> 1:23:38.400\n And you're just one individual.\n\n1:23:38.400 --> 1:23:44.400\n There's some individuals for whom Donald Trump is perhaps\n\n1:23:44.400 --> 1:23:45.800\n the best possible future.\n\n1:23:46.100 --> 1:23:49.900\n And so that's the whole point of us individuals exploring\n\n1:23:49.900 --> 1:23:51.000\n the space together.\n\n1:23:51.000 --> 1:23:51.500\n Exactly.\n\n1:23:51.500 --> 1:23:54.800\n Yeah, and what's trying to figure out which is the path\n\n1:23:54.800 --> 1:23:56.400\n that will make America great again.\n\n1:23:56.500 --> 1:23:57.500\n Yeah, exactly.\n\n1:23:58.200 --> 1:24:03.100\n So how can effective altruism help?\n\n1:24:03.200 --> 1:24:05.100\n I mean, this is a really interesting notion they actually\n\n1:24:05.100 --> 1:24:09.800\n describing of artificial intelligence being used as extremely\n\n1:24:09.800 --> 1:24:13.300\n powerful technology in the hands of very few potentially\n\n1:24:13.300 --> 1:24:17.000\n one person to create some very undesirable effect.\n\n1:24:17.300 --> 1:24:21.300\n So as opposed to AI and again, the source of the undesirableness\n\n1:24:21.300 --> 1:24:22.900\n there is the human.\n\n1:24:23.000 --> 1:24:25.300\n Yeah, AI is just a really powerful tool.\n\n1:24:26.200 --> 1:24:30.500\n So whether it's that or whether AI's AGI just runs away\n\n1:24:30.500 --> 1:24:31.600\n from us completely.\n\n1:24:31.600 --> 1:24:38.400\n How as individuals, as people in the effective altruism\n\n1:24:38.400 --> 1:24:41.100\n movement, how can we think about something like this?\n\n1:24:41.100 --> 1:24:44.200\n I understand poverty and animal welfare, but this is a far\n\n1:24:44.200 --> 1:24:47.400\n out incredibly mysterious and difficult problem.\n\n1:24:47.500 --> 1:24:47.800\n Great.\n\n1:24:47.800 --> 1:24:50.600\n Well, I think there's three paths as an individual.\n\n1:24:50.600 --> 1:24:55.400\n So if you're thinking about, you know, career paths you\n\n1:24:55.400 --> 1:24:56.000\n can pursue.\n\n1:24:56.000 --> 1:24:59.100\n So one is going down the line of technical AI safety.\n\n1:24:59.100 --> 1:25:05.800\n So this is most relevant to the kind of AI winning AI taking\n\n1:25:05.800 --> 1:25:10.700\n over scenarios where this is just technical work on current\n\n1:25:10.700 --> 1:25:13.600\n machine learning systems often sometimes going more theoretical\n\n1:25:13.600 --> 1:25:17.800\n to on how we can ensure that an AI is able to learn human\n\n1:25:17.800 --> 1:25:21.200\n values and able to act in the way that you want it to act.\n\n1:25:21.500 --> 1:25:26.800\n And that's a pretty mainstream issue and approach in machine\n\n1:25:26.800 --> 1:25:27.500\n learning today.\n\n1:25:27.500 --> 1:25:30.800\n So, you know, we definitely need more people doing that.\n\n1:25:31.400 --> 1:25:34.100\n Second is on the policy side of things, which I think is\n\n1:25:34.100 --> 1:25:40.400\n even more important at the moment, which is how should developments\n\n1:25:40.400 --> 1:25:42.300\n in AI be managed on a political level?\n\n1:25:43.200 --> 1:25:47.600\n How can you ensure that the benefits of AI are very distributed?\n\n1:25:47.600 --> 1:25:50.500\n It's not being, power isn't being concentrated in the hands\n\n1:25:50.500 --> 1:25:54.200\n of a small set of individuals.\n\n1:25:54.200 --> 1:25:59.100\n How do you ensure that there aren't arms races between different\n\n1:25:59.100 --> 1:26:06.000\n AI companies that might result in them, you know, cutting corners\n\n1:26:06.000 --> 1:26:07.100\n with respect to safety.\n\n1:26:07.200 --> 1:26:11.000\n And so there the input as individuals who can have is this.\n\n1:26:11.000 --> 1:26:12.300\n We're not talking about money.\n\n1:26:12.300 --> 1:26:13.600\n We're talking about effort.\n\n1:26:14.000 --> 1:26:15.600\n We're talking about career choices.\n\n1:26:15.600 --> 1:26:16.900\n We're talking about career choice.\n\n1:26:16.900 --> 1:26:20.700\n Yeah, but then it is the case that supposing, you know, you're\n\n1:26:20.700 --> 1:26:22.200\n like, I've already decided my career.\n\n1:26:22.200 --> 1:26:24.300\n I'm doing something quite different.\n\n1:26:24.500 --> 1:26:28.000\n You can contribute with money too, where at the Center for Effective\n\n1:26:28.000 --> 1:26:31.100\n Altruism, we set up the Long Term Future Fund.\n\n1:26:31.400 --> 1:26:36.300\n So if you go on to effectivealtruism.org, you can donate where\n\n1:26:36.800 --> 1:26:40.600\n a group of individuals will then work out what's the highest value\n\n1:26:40.600 --> 1:26:44.200\n place they can donate to work on existential risk issues with\n\n1:26:44.200 --> 1:26:46.200\n a particular focus on AI.\n\n1:26:46.900 --> 1:26:48.200\n What's path number three?\n\n1:26:48.400 --> 1:26:49.500\n This was path number three.\n\n1:26:49.500 --> 1:26:53.400\n This is donations with the third option I was thinking of.\n\n1:26:53.400 --> 1:26:53.800\n Okay.\n\n1:26:53.900 --> 1:26:58.500\n And then, yeah, there are, you can also donate directly to organizations\n\n1:26:58.500 --> 1:27:01.600\n working on this, like Center for Human Compatible AI at Berkeley,\n\n1:27:01.900 --> 1:27:08.300\n Future of Humanity Institute at Oxford, or other organizations too.\n\n1:27:08.500 --> 1:27:10.200\n Does AI keep you up at night?\n\n1:27:10.200 --> 1:27:11.400\n This kind of concern?\n\n1:27:13.000 --> 1:27:17.300\n Yeah, it's kind of a mix where I think it's very likely things are\n\n1:27:17.300 --> 1:27:21.400\n going to go well. I think we're going to be able to solve these\n\n1:27:21.400 --> 1:27:25.500\n problems. I think that's by far the most likely outcome, at least\n\n1:27:25.500 --> 1:27:25.900\n over the next.\n\n1:27:25.900 --> 1:27:26.800\n By far the most likely.\n\n1:27:26.800 --> 1:27:30.800\n So if you look at all the trajectories running away from our\n\n1:27:30.800 --> 1:27:36.600\n current moment in the next hundred years, you see AI creating\n\n1:27:36.600 --> 1:27:41.300\n destructive consequences as a small subset of those possible\n\n1:27:41.300 --> 1:27:41.700\n trajectories.\n\n1:27:41.700 --> 1:27:44.900\n Or at least, yeah, kind of eternal, destructive consequences.\n\n1:27:44.900 --> 1:27:46.500\n I think that being a small subset.\n\n1:27:46.500 --> 1:27:48.500\n At the same time, it still freaks me out.\n\n1:27:48.500 --> 1:27:51.600\n I mean, when we're talking about the entire future of civilization,\n\n1:27:51.600 --> 1:27:56.900\n then small probabilities, you know, 1% probability, that's terrifying.\n\n1:27:56.900 --> 1:28:02.500\n What do you think about Elon Musk's strong worry that we should\n\n1:28:02.500 --> 1:28:05.200\n be really concerned about existential risks of AI?\n\n1:28:05.200 --> 1:28:09.100\n Yeah, I mean, I think, you know, broadly speaking, I think he's\n\n1:28:09.100 --> 1:28:09.300\n right.\n\n1:28:09.300 --> 1:28:13.200\n I think if we talked, we would probably have very different\n\n1:28:13.200 --> 1:28:16.200\n probabilities on how likely it is that we're doomed.\n\n1:28:16.200 --> 1:28:19.700\n But again, when it comes to talking about the entire future of\n\n1:28:19.700 --> 1:28:23.200\n civilization, it doesn't really matter if it's 1% or if it's\n\n1:28:23.200 --> 1:28:26.700\n 50%, we ought to be taking every possible safeguard we can to\n\n1:28:26.700 --> 1:28:28.300\n ensure that things go well rather than poorly.\n\n1:28:30.300 --> 1:28:34.000\n Last question, if you yourself could eradicate one problem from\n\n1:28:34.000 --> 1:28:35.700\n the world, what would that problem be?\n\n1:28:35.700 --> 1:28:37.600\n That's a great question.\n\n1:28:37.600 --> 1:28:42.900\n I don't know if I'm cheating in saying this, but I think the\n\n1:28:42.900 --> 1:28:45.300\n thing I would most want to change is just the fact that people\n\n1:28:45.300 --> 1:28:50.500\n don't actually care about ensuring the long run future goes well.\n\n1:28:50.500 --> 1:28:52.500\n People don't really care about future generations.\n\n1:28:52.500 --> 1:28:53.300\n They don't think about it.\n\n1:28:53.300 --> 1:28:54.300\n It's not part of their aims.\n\n1:28:54.300 --> 1:28:58.800\n In some sense, you're not cheating at all because in speaking\n\n1:28:58.800 --> 1:29:02.200\n the way you do, in writing the things you're writing, you're\n\n1:29:02.200 --> 1:29:05.800\n doing, you're addressing exactly this aspect.\n\n1:29:05.800 --> 1:29:06.500\n Exactly.\n\n1:29:06.500 --> 1:29:10.800\n That is your input into the effective altruism movement.\n\n1:29:10.800 --> 1:29:12.900\n So for that, Will, thank you so much.\n\n1:29:12.900 --> 1:29:14.300\n It's an honor to talk to you.\n\n1:29:14.300 --> 1:29:15.000\n I really enjoyed it.\n\n1:29:15.000 --> 1:29:15.900\n Thanks so much for having me on.\n\n1:30:10.300 --> 1:30:13.300\n If that were the case, we'd probably be pretty generous.\n\n1:30:13.300 --> 1:30:17.500\n Next round's on me, but that's effectively the situation we're\n\n1:30:17.500 --> 1:30:18.800\n in all the time.\n\n1:30:18.800 --> 1:30:23.400\n It's like a 99% off sale or buy one get 99 free.\n\n1:30:23.400 --> 1:30:27.000\n Might be the most amazing deal you'll see in your life.\n\n1:30:27.000 --> 1:30:47.200\n Thank you for listening and hope to see you next time.\n\n"
}