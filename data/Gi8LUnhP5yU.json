{
  "title": "Max Tegmark: Life 3.0 | Lex Fridman Podcast #1",
  "id": "Gi8LUnhP5yU",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:04.200\n As part of MIT course 6S099, Artificial General Intelligence,\n\n00:04.200 --> 00:06.600\n I've gotten the chance to sit down with Max Tegmark.\n\n00:06.600 --> 00:08.680\n He is a professor here at MIT.\n\n00:08.680 --> 00:11.920\n He's a physicist, spent a large part of his career\n\n00:11.920 --> 00:16.960\n studying the mysteries of our cosmological universe.\n\n00:16.960 --> 00:20.680\n But he's also studied and delved into the beneficial\n\n00:20.680 --> 00:24.000\n possibilities and the existential risks\n\n00:24.000 --> 00:25.800\n of artificial intelligence.\n\n00:25.800 --> 00:29.040\n Amongst many other things, he is the cofounder\n\n00:29.040 --> 00:33.080\n of the Future of Life Institute, author of two books,\n\n00:33.080 --> 00:35.160\n both of which I highly recommend.\n\n00:35.160 --> 00:37.260\n First, Our Mathematical Universe.\n\n00:37.260 --> 00:40.160\n Second is Life 3.0.\n\n00:40.160 --> 00:44.080\n He's truly an out of the box thinker and a fun personality,\n\n00:44.080 --> 00:45.480\n so I really enjoy talking to him.\n\n00:45.480 --> 00:47.980\n If you'd like to see more of these videos in the future,\n\n00:47.980 --> 00:50.640\n please subscribe and also click the little bell icon\n\n00:50.640 --> 00:52.720\n to make sure you don't miss any videos.\n\n00:52.720 --> 00:56.840\n Also, Twitter, LinkedIn, agi.mit.edu\n\n00:56.840 --> 00:59.600\n if you wanna watch other lectures\n\n00:59.600 --> 01:01.080\n or conversations like this one.\n\n01:01.080 --> 01:04.000\n Better yet, go read Max's book, Life 3.0.\n\n01:04.000 --> 01:07.940\n Chapter seven on goals is my favorite.\n\n01:07.940 --> 01:10.480\n It's really where philosophy and engineering come together\n\n01:10.480 --> 01:13.440\n and it opens with a quote by Dostoevsky.\n\n01:14.400 --> 01:17.940\n The mystery of human existence lies not in just staying alive\n\n01:17.940 --> 01:20.520\n but in finding something to live for.\n\n01:20.520 --> 01:23.920\n Lastly, I believe that every failure rewards us\n\n01:23.920 --> 01:26.560\n with an opportunity to learn\n\n01:26.560 --> 01:28.360\n and in that sense, I've been very fortunate\n\n01:28.360 --> 01:30.960\n to fail in so many new and exciting ways\n\n01:31.840 --> 01:34.020\n and this conversation was no different.\n\n01:34.020 --> 01:36.160\n I've learned about something called\n\n01:36.160 --> 01:40.840\n radio frequency interference, RFI, look it up.\n\n01:40.840 --> 01:42.960\n Apparently, music and conversations\n\n01:42.960 --> 01:45.480\n from local radio stations can bleed into the audio\n\n01:45.480 --> 01:47.080\n that you're recording in such a way\n\n01:47.080 --> 01:49.360\n that it almost completely ruins that audio.\n\n01:49.360 --> 01:52.060\n It's an exceptionally difficult sound source to remove.\n\n01:53.240 --> 01:55.520\n So, I've gotten the opportunity to learn\n\n01:55.520 --> 02:00.200\n how to avoid RFI in the future during recording sessions.\n\n02:00.200 --> 02:02.680\n I've also gotten the opportunity to learn\n\n02:02.680 --> 02:06.240\n how to use Adobe Audition and iZotope RX 6\n\n02:06.240 --> 02:11.240\n to do some noise, some audio repair.\n\n02:11.720 --> 02:14.380\n Of course, this is an exceptionally difficult noise\n\n02:14.380 --> 02:15.220\n to remove.\n\n02:15.220 --> 02:16.280\n I am an engineer.\n\n02:16.280 --> 02:18.240\n I'm not an audio engineer.\n\n02:18.240 --> 02:20.180\n Neither is anybody else in our group\n\n02:20.180 --> 02:21.880\n but we did our best.\n\n02:21.880 --> 02:25.040\n Nevertheless, I thank you for your patience\n\n02:25.040 --> 02:27.960\n and I hope you're still able to enjoy this conversation.\n\n02:27.960 --> 02:29.320\n Do you think there's intelligent life\n\n02:29.320 --> 02:31.360\n out there in the universe?\n\n02:31.360 --> 02:33.480\n Let's open up with an easy question.\n\n02:33.480 --> 02:36.240\n I have a minority view here actually.\n\n02:36.240 --> 02:39.440\n When I give public lectures, I often ask for a show of hands\n\n02:39.440 --> 02:42.920\n who thinks there's intelligent life out there somewhere else\n\n02:42.920 --> 02:45.440\n and almost everyone put their hands up\n\n02:45.440 --> 02:47.360\n and when I ask why, they'll be like,\n\n02:47.360 --> 02:50.900\n oh, there's so many galaxies out there, there's gotta be.\n\n02:51.840 --> 02:54.560\n But I'm a numbers nerd, right?\n\n02:54.560 --> 02:56.640\n So when you look more carefully at it,\n\n02:56.640 --> 02:58.040\n it's not so clear at all.\n\n02:59.080 --> 03:00.680\n When we talk about our universe, first of all,\n\n03:00.680 --> 03:03.040\n we don't mean all of space.\n\n03:03.040 --> 03:04.040\n We actually mean, I don't know,\n\n03:04.040 --> 03:05.440\n you can throw me the universe if you want,\n\n03:05.440 --> 03:07.280\n it's behind you there.\n\n03:07.280 --> 03:11.440\n It's, we simply mean the spherical region of space\n\n03:11.440 --> 03:15.360\n from which light has a time to reach us so far\n\n03:15.360 --> 03:17.040\n during the 14.8 billion year,\n\n03:17.040 --> 03:19.320\n 13.8 billion years since our Big Bang.\n\n03:19.320 --> 03:22.320\n There's more space here but this is what we call a universe\n\n03:22.320 --> 03:24.040\n because that's all we have access to.\n\n03:24.040 --> 03:25.960\n So is there intelligent life here\n\n03:25.960 --> 03:28.920\n that's gotten to the point of building telescopes\n\n03:28.920 --> 03:29.960\n and computers?\n\n03:31.160 --> 03:34.540\n My guess is no, actually.\n\n03:34.540 --> 03:37.800\n The probability of it happening on any given planet\n\n03:39.240 --> 03:42.620\n is some number we don't know what it is.\n\n03:42.620 --> 03:47.620\n And what we do know is that the number can't be super high\n\n03:48.480 --> 03:50.300\n because there's over a billion Earth like planets\n\n03:50.300 --> 03:52.880\n in the Milky Way galaxy alone,\n\n03:52.880 --> 03:56.280\n many of which are billions of years older than Earth.\n\n03:56.280 --> 04:00.600\n And aside from some UFO believers,\n\n04:00.600 --> 04:01.880\n there isn't much evidence\n\n04:01.880 --> 04:05.600\n that any superduran civilization has come here at all.\n\n04:05.600 --> 04:08.440\n And so that's the famous Fermi paradox, right?\n\n04:08.440 --> 04:10.180\n And then if you work the numbers,\n\n04:10.180 --> 04:13.440\n what you find is that if you have no clue\n\n04:13.440 --> 04:16.880\n what the probability is of getting life on a given planet,\n\n04:16.880 --> 04:19.680\n so it could be 10 to the minus 10, 10 to the minus 20,\n\n04:19.680 --> 04:22.960\n or 10 to the minus two, or any power of 10\n\n04:22.960 --> 04:23.800\n is sort of equally likely\n\n04:23.800 --> 04:25.480\n if you wanna be really open minded,\n\n04:25.480 --> 04:27.600\n that translates into it being equally likely\n\n04:27.600 --> 04:31.800\n that our nearest neighbor is 10 to the 16 meters away,\n\n04:31.800 --> 04:33.880\n 10 to the 17 meters away, 10 to the 18.\n\n04:35.400 --> 04:40.400\n By the time you get much less than 10 to the 16 already,\n\n04:41.080 --> 04:45.960\n we pretty much know there is nothing else that close.\n\n04:45.960 --> 04:47.280\n And when you get beyond 10.\n\n04:47.280 --> 04:48.680\n Because they would have discovered us.\n\n04:48.680 --> 04:50.360\n Yeah, they would have been discovered as long ago,\n\n04:50.360 --> 04:51.440\n or if they're really close,\n\n04:51.440 --> 04:53.560\n we would have probably noted some engineering projects\n\n04:53.560 --> 04:54.640\n that they're doing.\n\n04:54.640 --> 04:57.880\n And if it's beyond 10 to the 26 meters,\n\n04:57.880 --> 05:00.000\n that's already outside of here.\n\n05:00.000 --> 05:05.000\n So my guess is actually that we are the only life in here\n\n05:05.800 --> 05:09.040\n that's gotten the point of building advanced tech,\n\n05:09.040 --> 05:10.720\n which I think is very,\n\n05:12.680 --> 05:15.360\n puts a lot of responsibility on our shoulders, not screw up.\n\n05:15.360 --> 05:17.240\n I think people who take for granted\n\n05:17.240 --> 05:20.120\n that it's okay for us to screw up,\n\n05:20.120 --> 05:22.760\n have an accidental nuclear war or go extinct somehow\n\n05:22.760 --> 05:25.960\n because there's a sort of Star Trek like situation out there\n\n05:25.960 --> 05:28.360\n where some other life forms are gonna come and bail us out\n\n05:28.360 --> 05:30.400\n and it doesn't matter as much.\n\n05:30.400 --> 05:33.400\n I think they're leveling us into a false sense of security.\n\n05:33.400 --> 05:35.200\n I think it's much more prudent to say,\n\n05:35.200 --> 05:36.400\n let's be really grateful\n\n05:36.400 --> 05:38.720\n for this amazing opportunity we've had\n\n05:38.720 --> 05:43.720\n and make the best of it just in case it is down to us.\n\n05:44.080 --> 05:45.680\n So from a physics perspective,\n\n05:45.680 --> 05:48.800\n do you think intelligent life,\n\n05:48.800 --> 05:51.360\n so it's unique from a sort of statistical view\n\n05:51.360 --> 05:52.560\n of the size of the universe,\n\n05:52.560 --> 05:55.840\n but from the basic matter of the universe,\n\n05:55.840 --> 05:59.040\n how difficult is it for intelligent life to come about?\n\n05:59.040 --> 06:01.280\n The kind of advanced tech building life\n\n06:03.120 --> 06:05.720\n is implied in your statement that it's really difficult\n\n06:05.720 --> 06:07.640\n to create something like a human species.\n\n06:07.640 --> 06:11.560\n Well, I think what we know is that going from no life\n\n06:11.560 --> 06:15.720\n to having life that can do a level of tech,\n\n06:15.720 --> 06:18.720\n there's some sort of two going beyond that\n\n06:18.720 --> 06:22.200\n than actually settling our whole universe with life.\n\n06:22.200 --> 06:26.560\n There's some major roadblock there,\n\n06:26.560 --> 06:30.880\n which is some great filter as it's sometimes called,\n\n06:30.880 --> 06:33.520\n which is tough to get through.\n\n06:33.520 --> 06:37.160\n It's either that roadblock is either behind us\n\n06:37.160 --> 06:38.720\n or in front of us.\n\n06:38.720 --> 06:41.080\n I'm hoping very much that it's behind us.\n\n06:41.080 --> 06:45.960\n I'm super excited every time we get a new report from NASA\n\n06:45.960 --> 06:48.480\n saying they failed to find any life on Mars.\n\n06:48.480 --> 06:50.080\n I'm like, yes, awesome.\n\n06:50.080 --> 06:51.680\n Because that suggests that the hard part,\n\n06:51.680 --> 06:54.240\n maybe it was getting the first ribosome\n\n06:54.240 --> 06:59.240\n or some very low level kind of stepping stone\n\n06:59.520 --> 07:00.400\n so that we're home free.\n\n07:00.400 --> 07:01.720\n Because if that's true,\n\n07:01.720 --> 07:03.640\n then the future is really only limited\n\n07:03.640 --> 07:05.200\n by our own imagination.\n\n07:05.200 --> 07:07.360\n It would be much suckier if it turns out\n\n07:07.360 --> 07:11.440\n that this level of life is kind of a dime a dozen,\n\n07:11.440 --> 07:12.760\n but maybe there's some other problem.\n\n07:12.760 --> 07:16.160\n Like as soon as a civilization gets advanced technology,\n\n07:16.160 --> 07:17.000\n within a hundred years,\n\n07:17.000 --> 07:20.320\n they get into some stupid fight with themselves and poof.\n\n07:20.320 --> 07:21.760\n That would be a bummer.\n\n07:21.760 --> 07:26.160\n Yeah, so you've explored the mysteries of the universe,\n\n07:26.160 --> 07:29.000\n the cosmological universe, the one that's sitting\n\n07:29.000 --> 07:30.000\n between us today.\n\n07:31.080 --> 07:35.960\n I think you've also begun to explore the other universe,\n\n07:35.960 --> 07:38.000\n which is sort of the mystery,\n\n07:38.000 --> 07:40.960\n the mysterious universe of the mind of intelligence,\n\n07:40.960 --> 07:42.840\n of intelligent life.\n\n07:42.840 --> 07:45.280\n So is there a common thread between your interest\n\n07:45.280 --> 07:48.760\n or the way you think about space and intelligence?\n\n07:48.760 --> 07:51.040\n Oh yeah, when I was a teenager,\n\n07:53.040 --> 07:57.280\n I was already very fascinated by the biggest questions.\n\n07:57.280 --> 08:00.560\n And I felt that the two biggest mysteries of all in science\n\n08:00.560 --> 08:05.000\n were our universe out there and our universe in here.\n\n08:05.000 --> 08:08.120\n So it's quite natural after having spent\n\n08:08.120 --> 08:11.040\n a quarter of a century on my career,\n\n08:11.040 --> 08:12.680\n thinking a lot about this one,\n\n08:12.680 --> 08:14.320\n that I'm now indulging in the luxury\n\n08:14.320 --> 08:15.960\n of doing research on this one.\n\n08:15.960 --> 08:17.720\n It's just so cool.\n\n08:17.720 --> 08:20.120\n I feel the time is ripe now\n\n08:20.120 --> 08:25.120\n for you trans greatly deepening our understanding of this.\n\n08:25.120 --> 08:26.640\n Just start exploring this one.\n\n08:26.640 --> 08:29.560\n Yeah, because I think a lot of people view intelligence\n\n08:29.560 --> 08:33.520\n as something mysterious that can only exist\n\n08:33.520 --> 08:36.120\n in biological organisms like us,\n\n08:36.120 --> 08:37.680\n and therefore dismiss all talk\n\n08:37.680 --> 08:41.160\n about artificial general intelligence as science fiction.\n\n08:41.160 --> 08:43.200\n But from my perspective as a physicist,\n\n08:43.200 --> 08:46.680\n I am a blob of quarks and electrons\n\n08:46.680 --> 08:48.360\n moving around in a certain pattern\n\n08:48.360 --> 08:50.080\n and processing information in certain ways.\n\n08:50.080 --> 08:53.600\n And this is also a blob of quarks and electrons.\n\n08:53.600 --> 08:55.360\n I'm not smarter than the water bottle\n\n08:55.360 --> 08:57.880\n because I'm made of different kinds of quarks.\n\n08:57.880 --> 08:59.640\n I'm made of up quarks and down quarks,\n\n08:59.640 --> 09:01.400\n exact same kind as this.\n\n09:01.400 --> 09:05.080\n There's no secret sauce, I think, in me.\n\n09:05.080 --> 09:08.560\n It's all about the pattern of the information processing.\n\n09:08.560 --> 09:12.240\n And this means that there's no law of physics\n\n09:12.240 --> 09:15.600\n saying that we can't create technology,\n\n09:15.600 --> 09:19.960\n which can help us by being incredibly intelligent\n\n09:19.960 --> 09:21.680\n and help us crack mysteries that we couldn't.\n\n09:21.680 --> 09:23.560\n In other words, I think we've really only seen\n\n09:23.560 --> 09:26.480\n the tip of the intelligence iceberg so far.\n\n09:26.480 --> 09:29.960\n Yeah, so the perceptronium.\n\n09:29.960 --> 09:31.280\n Yeah.\n\n09:31.280 --> 09:33.200\n So you coined this amazing term.\n\n09:33.200 --> 09:35.760\n It's a hypothetical state of matter,\n\n09:35.760 --> 09:38.360\n sort of thinking from a physics perspective,\n\n09:38.360 --> 09:40.080\n what is the kind of matter that can help,\n\n09:40.080 --> 09:42.920\n as you're saying, subjective experience emerge,\n\n09:42.920 --> 09:44.280\n consciousness emerge.\n\n09:44.280 --> 09:46.640\n So how do you think about consciousness\n\n09:46.640 --> 09:48.160\n from this physics perspective?\n\n09:49.960 --> 09:50.800\n Very good question.\n\n09:50.800 --> 09:55.800\n So again, I think many people have underestimated\n\n09:55.800 --> 09:59.120\n our ability to make progress on this\n\n09:59.120 --> 10:01.320\n by convincing themselves it's hopeless\n\n10:01.320 --> 10:05.840\n because somehow we're missing some ingredient that we need.\n\n10:05.840 --> 10:09.560\n There's some new consciousness particle or whatever.\n\n10:09.560 --> 10:12.720\n I happen to think that we're not missing anything\n\n10:12.720 --> 10:16.320\n and that it's not the interesting thing\n\n10:16.320 --> 10:18.560\n about consciousness that gives us\n\n10:18.560 --> 10:21.400\n this amazing subjective experience of colors\n\n10:21.400 --> 10:23.320\n and sounds and emotions.\n\n10:23.320 --> 10:26.320\n It's rather something at the higher level\n\n10:26.320 --> 10:28.800\n about the patterns of information processing.\n\n10:28.800 --> 10:33.160\n And that's why I like to think about this idea\n\n10:33.160 --> 10:34.480\n of perceptronium.\n\n10:34.480 --> 10:36.920\n What does it mean for an arbitrary physical system\n\n10:36.920 --> 10:41.920\n to be conscious in terms of what its particles are doing\n\n10:41.920 --> 10:43.560\n or its information is doing?\n\n10:43.560 --> 10:46.080\n I don't think, I hate carbon chauvinism,\n\n10:46.080 --> 10:47.960\n this attitude you have to be made of carbon atoms\n\n10:47.960 --> 10:50.160\n to be smart or conscious.\n\n10:50.160 --> 10:53.520\n There's something about the information processing\n\n10:53.520 --> 10:55.360\n that this kind of matter performs.\n\n10:55.360 --> 10:57.840\n Yeah, and you can see I have my favorite equations here\n\n10:57.840 --> 11:00.720\n describing various fundamental aspects of the world.\n\n11:00.720 --> 11:02.560\n I feel that I think one day,\n\n11:02.560 --> 11:04.360\n maybe someone who's watching this will come up\n\n11:04.360 --> 11:07.280\n with the equations that information processing\n\n11:07.280 --> 11:08.760\n has to satisfy to be conscious.\n\n11:08.760 --> 11:11.800\n I'm quite convinced there is big discovery\n\n11:11.800 --> 11:15.400\n to be made there because let's face it,\n\n11:15.400 --> 11:18.720\n we know that so many things are made up of information.\n\n11:18.720 --> 11:21.960\n We know that some information processing is conscious\n\n11:21.960 --> 11:25.520\n because we are conscious.\n\n11:25.520 --> 11:27.600\n But we also know that a lot of information processing\n\n11:27.600 --> 11:28.440\n is not conscious.\n\n11:28.440 --> 11:30.040\n Like most of the information processing happening\n\n11:30.040 --> 11:32.680\n in your brain right now is not conscious.\n\n11:32.680 --> 11:36.040\n There are like 10 megabytes per second coming in\n\n11:36.040 --> 11:38.080\n even just through your visual system.\n\n11:38.080 --> 11:40.480\n You're not conscious about your heartbeat regulation\n\n11:40.480 --> 11:42.120\n or most things.\n\n11:42.120 --> 11:45.680\n Even if I just ask you to like read what it says here,\n\n11:45.680 --> 11:48.040\n you look at it and then, oh, now you know what it said.\n\n11:48.040 --> 11:51.560\n But you're not aware of how the computation actually happened.\n\n11:51.560 --> 11:53.680\n Your consciousness is like the CEO\n\n11:53.680 --> 11:56.680\n that got an email at the end with the final answer.\n\n11:56.680 --> 12:01.000\n So what is it that makes a difference?\n\n12:01.000 --> 12:05.120\n I think that's both a great science mystery.\n\n12:05.120 --> 12:07.080\n We're actually studying it a little bit in my lab here\n\n12:07.080 --> 12:10.920\n at MIT, but I also think it's just a really urgent question\n\n12:10.920 --> 12:12.080\n to answer.\n\n12:12.080 --> 12:14.880\n For starters, I mean, if you're an emergency room doctor\n\n12:14.880 --> 12:17.160\n and you have an unresponsive patient coming in,\n\n12:17.160 --> 12:19.600\n wouldn't it be great if in addition to having\n\n12:22.360 --> 12:25.320\n a CT scanner, you had a consciousness scanner\n\n12:25.320 --> 12:27.920\n that could figure out whether this person\n\n12:27.920 --> 12:30.960\n is actually having locked in syndrome\n\n12:30.960 --> 12:32.440\n or is actually comatose.\n\n12:33.360 --> 12:37.000\n And in the future, imagine if we build robots\n\n12:37.000 --> 12:41.480\n or the machine that we can have really good conversations\n\n12:41.480 --> 12:44.840\n with, which I think is very likely to happen.\n\n12:44.840 --> 12:47.760\n Wouldn't you want to know if your home helper robot\n\n12:47.760 --> 12:51.320\n is actually experiencing anything or just like a zombie,\n\n12:51.320 --> 12:53.520\n I mean, would you prefer it?\n\n12:53.520 --> 12:54.360\n What would you prefer?\n\n12:54.360 --> 12:56.200\n Would you prefer that it's actually unconscious\n\n12:56.200 --> 12:58.560\n so that you don't have to feel guilty about switching it off\n\n12:58.560 --> 13:02.120\n or giving boring chores or what would you prefer?\n\n13:02.120 --> 13:06.520\n Well, certainly we would prefer,\n\n13:06.520 --> 13:08.960\n I would prefer the appearance of consciousness.\n\n13:08.960 --> 13:11.720\n But the question is whether the appearance of consciousness\n\n13:11.720 --> 13:15.040\n is different than consciousness itself.\n\n13:15.040 --> 13:18.200\n And sort of to ask that as a question,\n\n13:18.200 --> 13:21.760\n do you think we need to understand what consciousness is,\n\n13:21.760 --> 13:23.520\n solve the hard problem of consciousness\n\n13:23.520 --> 13:28.240\n in order to build something like an AGI system?\n\n13:28.240 --> 13:30.440\n No, I don't think that.\n\n13:30.440 --> 13:34.520\n And I think we will probably be able to build things\n\n13:34.520 --> 13:36.080\n even if we don't answer that question.\n\n13:36.080 --> 13:37.720\n But if we want to make sure that what happens\n\n13:37.720 --> 13:40.960\n is a good thing, we better solve it first.\n\n13:40.960 --> 13:44.960\n So it's a wonderful controversy you're raising there\n\n13:44.960 --> 13:47.960\n where you have basically three points of view\n\n13:47.960 --> 13:48.800\n about the hard problem.\n\n13:48.800 --> 13:52.800\n So there are two different points of view.\n\n13:52.800 --> 13:55.160\n They both conclude that the hard problem of consciousness\n\n13:55.160 --> 13:56.840\n is BS.\n\n13:56.840 --> 13:59.320\n On one hand, you have some people like Daniel Dennett\n\n13:59.320 --> 14:01.480\n who say that consciousness is just BS\n\n14:01.480 --> 14:05.000\n because consciousness is the same thing as intelligence.\n\n14:05.000 --> 14:06.440\n There's no difference.\n\n14:06.440 --> 14:11.080\n So anything which acts conscious is conscious,\n\n14:11.080 --> 14:13.480\n just like we are.\n\n14:13.480 --> 14:15.960\n And then there are also a lot of people,\n\n14:15.960 --> 14:18.400\n including many top AI researchers I know,\n\n14:18.400 --> 14:19.920\n who say, oh, consciousness is just bullshit\n\n14:19.920 --> 14:22.760\n because, of course, machines can never be conscious.\n\n14:22.760 --> 14:24.520\n They're always going to be zombies.\n\n14:24.520 --> 14:27.880\n You never have to feel guilty about how you treat them.\n\n14:27.880 --> 14:30.880\n And then there's a third group of people,\n\n14:30.880 --> 14:34.920\n including Giulio Tononi, for example,\n\n14:34.920 --> 14:37.440\n and Krzysztof Koch and a number of others.\n\n14:37.440 --> 14:39.520\n I would put myself also in this middle camp\n\n14:39.520 --> 14:41.880\n who say that actually some information processing\n\n14:41.880 --> 14:44.160\n is conscious and some is not.\n\n14:44.160 --> 14:46.960\n So let's find the equation which can be used\n\n14:46.960 --> 14:49.080\n to determine which it is.\n\n14:49.080 --> 14:52.040\n And I think we've just been a little bit lazy,\n\n14:52.040 --> 14:54.960\n kind of running away from this problem for a long time.\n\n14:54.960 --> 14:57.840\n It's been almost taboo to even mention the C word\n\n14:57.840 --> 15:00.520\n in a lot of circles because,\n\n15:00.520 --> 15:03.520\n but we should stop making excuses.\n\n15:03.520 --> 15:07.920\n This is a science question and there are ways\n\n15:07.920 --> 15:11.960\n we can even test any theory that makes predictions for this.\n\n15:11.960 --> 15:13.640\n And coming back to this helper robot,\n\n15:13.640 --> 15:16.080\n I mean, so you said you'd want your helper robot\n\n15:16.080 --> 15:18.160\n to certainly act conscious and treat you,\n\n15:18.160 --> 15:20.880\n like have conversations with you and stuff.\n\n15:20.880 --> 15:21.720\n I think so.\n\n15:21.720 --> 15:22.560\n But wouldn't you, would you feel,\n\n15:22.560 --> 15:23.920\n would you feel a little bit creeped out\n\n15:23.920 --> 15:27.680\n if you realized that it was just a glossed up tape recorder,\n\n15:27.680 --> 15:31.560\n you know, that was just zombie and was a faking emotion?\n\n15:31.560 --> 15:34.560\n Would you prefer that it actually had an experience\n\n15:34.560 --> 15:37.000\n or would you prefer that it's actually\n\n15:37.000 --> 15:39.120\n not experiencing anything so you feel,\n\n15:39.120 --> 15:42.200\n you don't have to feel guilty about what you do to it?\n\n15:42.200 --> 15:45.040\n It's such a difficult question because, you know,\n\n15:45.040 --> 15:47.280\n it's like when you're in a relationship and you say,\n\n15:47.280 --> 15:48.120\n well, I love you.\n\n15:48.120 --> 15:49.760\n And the other person said, I love you back.\n\n15:49.760 --> 15:52.640\n It's like asking, well, do they really love you back\n\n15:52.640 --> 15:55.360\n or are they just saying they love you back?\n\n15:55.360 --> 15:58.120\n Don't you really want them to actually love you?\n\n15:58.120 --> 16:03.120\n It's hard to, it's hard to really know the difference\n\n16:03.520 --> 16:08.520\n between everything seeming like there's consciousness\n\n16:09.000 --> 16:10.640\n present, there's intelligence present,\n\n16:10.640 --> 16:13.840\n there's affection, passion, love,\n\n16:13.840 --> 16:16.200\n and it actually being there.\n\n16:16.200 --> 16:17.720\n I'm not sure, do you have?\n\n16:17.720 --> 16:19.400\n But like, can I ask you a question about this?\n\n16:19.400 --> 16:20.760\n Like to make it a bit more pointed.\n\n16:20.760 --> 16:22.920\n So Mass General Hospital is right across the river, right?\n\n16:22.920 --> 16:23.760\n Yes.\n\n16:23.760 --> 16:26.720\n Suppose you're going in for a medical procedure\n\n16:26.720 --> 16:29.320\n and they're like, you know, for anesthesia,\n\n16:29.320 --> 16:31.000\n what we're going to do is we're going to give you\n\n16:31.000 --> 16:33.160\n muscle relaxants so you won't be able to move\n\n16:33.160 --> 16:35.040\n and you're going to feel excruciating pain\n\n16:35.040 --> 16:35.880\n during the whole surgery,\n\n16:35.880 --> 16:37.600\n but you won't be able to do anything about it.\n\n16:37.600 --> 16:39.200\n But then we're going to give you this drug\n\n16:39.200 --> 16:40.760\n that erases your memory of it.\n\n16:41.960 --> 16:43.440\n Would you be cool about that?\n\n16:44.960 --> 16:47.600\n What's the difference that you're conscious about it\n\n16:48.600 --> 16:51.640\n or not if there's no behavioral change, right?\n\n16:51.640 --> 16:54.520\n Right, that's a really, that's a really clear way to put it.\n\n16:54.520 --> 16:57.400\n That's, yeah, it feels like in that sense,\n\n16:57.400 --> 17:01.080\n experiencing it is a valuable quality.\n\n17:01.080 --> 17:04.800\n So actually being able to have subjective experiences,\n\n17:05.840 --> 17:09.120\n at least in that case, is valuable.\n\n17:09.120 --> 17:11.240\n And I think we humans have a little bit\n\n17:11.240 --> 17:13.600\n of a bad track record also of making\n\n17:13.600 --> 17:15.480\n these self serving arguments\n\n17:15.480 --> 17:18.040\n that other entities aren't conscious.\n\n17:18.040 --> 17:19.160\n You know, people often say,\n\n17:19.160 --> 17:21.800\n oh, these animals can't feel pain.\n\n17:21.800 --> 17:24.040\n It's okay to boil lobsters because we ask them\n\n17:24.040 --> 17:25.960\n if it hurt and they didn't say anything.\n\n17:25.960 --> 17:27.400\n And now there was just a paper out saying,\n\n17:27.400 --> 17:29.320\n lobsters do feel pain when you boil them\n\n17:29.320 --> 17:31.040\n and they're banning it in Switzerland.\n\n17:31.040 --> 17:33.560\n And we did this with slaves too often and said,\n\n17:33.560 --> 17:34.680\n oh, they don't mind.\n\n17:36.240 --> 17:39.480\n They don't maybe aren't conscious\n\n17:39.480 --> 17:41.160\n or women don't have souls or whatever.\n\n17:41.160 --> 17:43.200\n So I'm a little bit nervous when I hear people\n\n17:43.200 --> 17:46.360\n just take as an axiom that machines\n\n17:46.360 --> 17:48.960\n can't have experience ever.\n\n17:48.960 --> 17:51.560\n I think this is just a really fascinating science question\n\n17:51.560 --> 17:52.400\n is what it is.\n\n17:52.400 --> 17:54.720\n Let's research it and try to figure out\n\n17:54.720 --> 17:56.000\n what it is that makes the difference\n\n17:56.000 --> 17:58.880\n between unconscious intelligent behavior\n\n17:58.880 --> 18:01.120\n and conscious intelligent behavior.\n\n18:01.120 --> 18:04.680\n So in terms of, so if you think of a Boston Dynamics\n\n18:04.680 --> 18:07.680\n human or robot being sort of with a broom\n\n18:07.680 --> 18:11.920\n being pushed around, it starts pushing\n\n18:11.920 --> 18:13.320\n on a consciousness question.\n\n18:13.320 --> 18:17.040\n So let me ask, do you think an AGI system\n\n18:17.040 --> 18:19.720\n like a few neuroscientists believe\n\n18:19.720 --> 18:22.320\n needs to have a physical embodiment?\n\n18:22.320 --> 18:25.720\n Needs to have a body or something like a body?\n\n18:25.720 --> 18:28.280\n No, I don't think so.\n\n18:28.280 --> 18:30.560\n You mean to have a conscious experience?\n\n18:30.560 --> 18:31.640\n To have consciousness.\n\n18:33.160 --> 18:36.080\n I do think it helps a lot to have a physical embodiment\n\n18:36.080 --> 18:38.440\n to learn the kind of things about the world\n\n18:38.440 --> 18:41.480\n that are important to us humans, for sure.\n\n18:42.560 --> 18:45.600\n But I don't think the physical embodiment\n\n18:45.600 --> 18:47.120\n is necessary after you've learned it\n\n18:47.120 --> 18:48.760\n to just have the experience.\n\n18:48.760 --> 18:51.400\n Think about when you're dreaming, right?\n\n18:51.400 --> 18:52.600\n Your eyes are closed.\n\n18:52.600 --> 18:54.240\n You're not getting any sensory input.\n\n18:54.240 --> 18:55.960\n You're not behaving or moving in any way\n\n18:55.960 --> 18:58.160\n but there's still an experience there, right?\n\n18:59.720 --> 19:01.400\n And so clearly the experience that you have\n\n19:01.400 --> 19:03.320\n when you see something cool in your dreams\n\n19:03.320 --> 19:04.800\n isn't coming from your eyes.\n\n19:04.800 --> 19:08.640\n It's just the information processing itself in your brain\n\n19:08.640 --> 19:10.920\n which is that experience, right?\n\n19:10.920 --> 19:13.640\n But if I put it another way, I'll say\n\n19:13.640 --> 19:15.120\n because it comes from neuroscience\n\n19:15.120 --> 19:18.280\n is the reason you want to have a body and a physical\n\n19:18.280 --> 19:23.280\n something like a physical, you know, a physical system\n\n19:23.920 --> 19:27.040\n is because you want to be able to preserve something.\n\n19:27.040 --> 19:30.840\n In order to have a self, you could argue,\n\n19:30.840 --> 19:35.840\n would you need to have some kind of embodiment of self\n\n19:36.400 --> 19:37.960\n to want to preserve?\n\n19:38.920 --> 19:42.400\n Well, now we're getting a little bit anthropomorphic\n\n19:42.400 --> 19:45.200\n into anthropomorphizing things.\n\n19:45.200 --> 19:47.280\n Maybe talking about self preservation instincts.\n\n19:47.280 --> 19:50.560\n I mean, we are evolved organisms, right?\n\n19:50.560 --> 19:53.520\n So Darwinian evolution endowed us\n\n19:53.520 --> 19:57.120\n and other evolved organism with a self preservation instinct\n\n19:57.120 --> 20:00.560\n because those that didn't have those self preservation genes\n\n20:00.560 --> 20:02.960\n got cleaned out of the gene pool, right?\n\n20:02.960 --> 20:06.880\n But if you build an artificial general intelligence\n\n20:06.880 --> 20:10.040\n the mind space that you can design is much, much larger\n\n20:10.040 --> 20:14.440\n than just a specific subset of minds that can evolve.\n\n20:14.440 --> 20:17.280\n So an AGI mind doesn't necessarily have\n\n20:17.280 --> 20:19.880\n to have any self preservation instinct.\n\n20:19.880 --> 20:21.600\n It also doesn't necessarily have to be\n\n20:21.600 --> 20:24.040\n so individualistic as us.\n\n20:24.040 --> 20:26.080\n Like, imagine if you could just, first of all,\n\n20:26.080 --> 20:27.960\n or we are also very afraid of death.\n\n20:27.960 --> 20:29.920\n You know, I suppose you could back yourself up\n\n20:29.920 --> 20:32.000\n every five minutes and then your airplane\n\n20:32.000 --> 20:32.840\n is about to crash.\n\n20:32.840 --> 20:36.680\n You're like, shucks, I'm gonna lose the last five minutes\n\n20:36.680 --> 20:39.520\n of experiences since my last cloud backup, dang.\n\n20:39.520 --> 20:41.520\n You know, it's not as big a deal.\n\n20:41.520 --> 20:45.680\n Or if we could just copy experiences between our minds\n\n20:45.680 --> 20:47.640\n easily like we, which we could easily do\n\n20:47.640 --> 20:50.360\n if we were silicon based, right?\n\n20:50.360 --> 20:54.040\n Then maybe we would feel a little bit more\n\n20:54.040 --> 20:56.560\n like a hive mind actually, that maybe it's the,\n\n20:56.560 --> 20:59.960\n so I don't think we should take for granted at all\n\n20:59.960 --> 21:03.000\n that AGI will have to have any of those sort of\n\n21:04.880 --> 21:07.360\n competitive as alpha male instincts.\n\n21:07.360 --> 21:10.160\n On the other hand, you know, this is really interesting\n\n21:10.160 --> 21:13.840\n because I think some people go too far and say,\n\n21:13.840 --> 21:16.680\n of course we don't have to have any concerns either\n\n21:16.680 --> 21:20.800\n that advanced AI will have those instincts\n\n21:20.800 --> 21:22.680\n because we can build anything we want.\n\n21:22.680 --> 21:26.280\n That there's a very nice set of arguments going back\n\n21:26.280 --> 21:28.560\n to Steve Omohundro and Nick Bostrom and others\n\n21:28.560 --> 21:32.280\n just pointing out that when we build machines,\n\n21:32.280 --> 21:34.680\n we normally build them with some kind of goal, you know,\n\n21:34.680 --> 21:38.520\n win this chess game, drive this car safely or whatever.\n\n21:38.520 --> 21:40.960\n And as soon as you put in a goal into machine,\n\n21:40.960 --> 21:42.760\n especially if it's kind of open ended goal\n\n21:42.760 --> 21:44.640\n and the machine is very intelligent,\n\n21:44.640 --> 21:47.000\n it'll break that down into a bunch of sub goals.\n\n21:48.280 --> 21:51.280\n And one of those goals will almost always\n\n21:51.280 --> 21:54.200\n be self preservation because if it breaks or dies\n\n21:54.200 --> 21:56.120\n in the process, it's not gonna accomplish the goal, right?\n\n21:56.120 --> 21:58.040\n Like suppose you just build a little,\n\n21:58.040 --> 22:01.000\n you have a little robot and you tell it to go down\n\n22:01.000 --> 22:04.040\n the store market here and get you some food,\n\n22:04.040 --> 22:06.200\n make you cook an Italian dinner, you know,\n\n22:06.200 --> 22:08.400\n and then someone mugs it and tries to break it\n\n22:08.400 --> 22:09.480\n on the way.\n\n22:09.480 --> 22:12.920\n That robot has an incentive to not get destroyed\n\n22:12.920 --> 22:14.720\n and defend itself or run away,\n\n22:14.720 --> 22:17.720\n because otherwise it's gonna fail in cooking your dinner.\n\n22:17.720 --> 22:19.560\n It's not afraid of death,\n\n22:19.560 --> 22:22.960\n but it really wants to complete the dinner cooking goal.\n\n22:22.960 --> 22:25.040\n So it will have a self preservation instinct.\n\n22:25.040 --> 22:27.920\n Continue being a functional agent somehow.\n\n22:27.920 --> 22:32.920\n And similarly, if you give any kind of more ambitious goal\n\n22:33.720 --> 22:37.000\n to an AGI, it's very likely they wanna acquire\n\n22:37.000 --> 22:39.840\n more resources so it can do that better.\n\n22:39.840 --> 22:42.720\n And it's exactly from those sort of sub goals\n\n22:42.720 --> 22:43.800\n that we might not have intended\n\n22:43.800 --> 22:47.160\n that some of the concerns about AGI safety come.\n\n22:47.160 --> 22:50.600\n You give it some goal that seems completely harmless.\n\n22:50.600 --> 22:53.360\n And then before you realize it,\n\n22:53.360 --> 22:55.480\n it's also trying to do these other things\n\n22:55.480 --> 22:56.920\n which you didn't want it to do.\n\n22:56.920 --> 22:59.160\n And it's maybe smarter than us.\n\n22:59.160 --> 23:01.000\n So it's fascinating.\n\n23:01.000 --> 23:05.680\n And let me pause just because I am in a very kind\n\n23:05.680 --> 23:08.720\n of human centric way, see fear of death\n\n23:08.720 --> 23:11.840\n as a valuable motivator.\n\n23:11.840 --> 23:16.440\n So you don't think, you think that's an artifact\n\n23:16.440 --> 23:19.120\n of evolution, so that's the kind of mind space\n\n23:19.120 --> 23:22.120\n evolution created that we're sort of almost obsessed\n\n23:22.120 --> 23:24.400\n about self preservation, some kind of genetic flow.\n\n23:24.400 --> 23:29.400\n You don't think that's necessary to be afraid of death.\n\n23:29.480 --> 23:32.920\n So not just a kind of sub goal of self preservation\n\n23:32.920 --> 23:34.920\n just so you can keep doing the thing,\n\n23:34.920 --> 23:38.720\n but more fundamentally sort of have the finite thing\n\n23:38.720 --> 23:43.080\n like this ends for you at some point.\n\n23:43.080 --> 23:44.160\n Interesting.\n\n23:44.160 --> 23:47.440\n Do I think it's necessary for what precisely?\n\n23:47.440 --> 23:50.920\n For intelligence, but also for consciousness.\n\n23:50.920 --> 23:55.040\n So for those, for both, do you think really\n\n23:55.040 --> 23:59.120\n like a finite death and the fear of it is important?\n\n23:59.120 --> 24:04.120\n So before I can answer, before we can agree\n\n24:05.160 --> 24:06.960\n on whether it's necessary for intelligence\n\n24:06.960 --> 24:08.360\n or for consciousness, we should be clear\n\n24:08.360 --> 24:09.800\n on how we define those two words.\n\n24:09.800 --> 24:11.960\n Cause a lot of really smart people define them\n\n24:11.960 --> 24:13.320\n in very different ways.\n\n24:13.320 --> 24:17.080\n I was on this panel with AI experts\n\n24:17.080 --> 24:20.080\n and they couldn't agree on how to define intelligence even.\n\n24:20.080 --> 24:22.000\n So I define intelligence simply\n\n24:22.000 --> 24:24.760\n as the ability to accomplish complex goals.\n\n24:25.640 --> 24:27.280\n I like your broad definition, because again\n\n24:27.280 --> 24:29.040\n I don't want to be a carbon chauvinist.\n\n24:29.040 --> 24:30.400\n Right.\n\n24:30.400 --> 24:34.600\n And in that case, no, certainly\n\n24:34.600 --> 24:36.480\n it doesn't require fear of death.\n\n24:36.480 --> 24:40.120\n I would say alpha go, alpha zero is quite intelligent.\n\n24:40.120 --> 24:43.080\n I don't think alpha zero has any fear of being turned off\n\n24:43.080 --> 24:46.320\n because it doesn't understand the concept of it even.\n\n24:46.320 --> 24:48.440\n And similarly consciousness.\n\n24:48.440 --> 24:52.240\n I mean, you could certainly imagine very simple\n\n24:52.240 --> 24:53.920\n kind of experience.\n\n24:53.920 --> 24:57.200\n If certain plants have any kind of experience\n\n24:57.200 --> 24:58.560\n I don't think they're very afraid of dying\n\n24:58.560 --> 25:00.920\n or there's nothing they can do about it anyway much.\n\n25:00.920 --> 25:04.560\n So there wasn't that much value in, but more seriously\n\n25:04.560 --> 25:09.200\n I think if you ask, not just about being conscious\n\n25:09.200 --> 25:14.200\n but maybe having what you would, we might call\n\n25:14.320 --> 25:16.400\n an exciting life where you feel passion\n\n25:16.400 --> 25:21.400\n and really appreciate the things.\n\n25:21.480 --> 25:24.440\n Maybe there somehow, maybe there perhaps it does help\n\n25:24.440 --> 25:27.880\n having a backdrop that, Hey, it's finite.\n\n25:27.880 --> 25:31.200\n No, let's make the most of this, let's live to the fullest.\n\n25:31.200 --> 25:33.800\n So if you knew you were going to live forever\n\n25:34.880 --> 25:37.400\n do you think you would change your?\n\n25:37.400 --> 25:39.560\n Yeah, I mean, in some perspective\n\n25:39.560 --> 25:43.960\n it would be an incredibly boring life living forever.\n\n25:43.960 --> 25:47.360\n So in the sort of loose subjective terms that you said\n\n25:47.360 --> 25:50.480\n of something exciting and something in this\n\n25:50.480 --> 25:53.240\n that other humans would understand, I think is, yeah\n\n25:53.240 --> 25:57.120\n it seems that the finiteness of it is important.\n\n25:57.120 --> 25:59.560\n Well, the good news I have for you then is\n\n25:59.560 --> 26:02.120\n based on what we understand about cosmology\n\n26:02.120 --> 26:05.120\n everything is in our universe is probably\n\n26:05.120 --> 26:07.960\n ultimately probably finite, although.\n\n26:07.960 --> 26:11.560\n Big crunch or big, what's the, the infinite expansion.\n\n26:11.560 --> 26:13.840\n Yeah, we could have a big chill or a big crunch\n\n26:13.840 --> 26:18.440\n or a big rip or that's the big snap or death bubbles.\n\n26:18.440 --> 26:20.040\n All of them are more than a billion years away.\n\n26:20.040 --> 26:24.600\n So we should, we certainly have vastly more time\n\n26:24.600 --> 26:27.920\n than our ancestors thought, but there is still\n\n26:29.160 --> 26:32.360\n it's still pretty hard to squeeze in an infinite number\n\n26:32.360 --> 26:36.560\n of compute cycles, even though there are some loopholes\n\n26:36.560 --> 26:37.720\n that just might be possible.\n\n26:37.720 --> 26:41.960\n But I think, you know, some people like to say\n\n26:41.960 --> 26:44.760\n that you should live as if you're about to\n\n26:44.760 --> 26:46.720\n you're going to die in five years or so.\n\n26:46.720 --> 26:47.960\n And that's sort of optimal.\n\n26:47.960 --> 26:50.560\n Maybe it's a good assumption.\n\n26:50.560 --> 26:54.680\n We should build our civilization as if it's all finite\n\n26:54.680 --> 26:55.680\n to be on the safe side.\n\n26:55.680 --> 26:56.960\n Right, exactly.\n\n26:56.960 --> 26:59.720\n So you mentioned defining intelligence\n\n26:59.720 --> 27:02.960\n as the ability to solve complex goals.\n\n27:02.960 --> 27:05.440\n Where would you draw a line or how would you try\n\n27:05.440 --> 27:08.200\n to define human level intelligence\n\n27:08.200 --> 27:10.680\n and superhuman level intelligence?\n\n27:10.680 --> 27:13.280\n Where is consciousness part of that definition?\n\n27:13.280 --> 27:16.640\n No, consciousness does not come into this definition.\n\n27:16.640 --> 27:20.280\n So, so I think of intelligence as it's a spectrum\n\n27:20.280 --> 27:21.960\n but there are very many different kinds of goals\n\n27:21.960 --> 27:22.800\n you can have.\n\n27:22.800 --> 27:24.000\n You can have a goal to be a good chess player\n\n27:24.000 --> 27:28.520\n a good goal player, a good car driver, a good investor\n\n27:28.520 --> 27:31.160\n good poet, et cetera.\n\n27:31.160 --> 27:34.320\n So intelligence that by its very nature\n\n27:34.320 --> 27:36.680\n isn't something you can measure by this one number\n\n27:36.680 --> 27:37.960\n or some overall goodness.\n\n27:37.960 --> 27:38.800\n No, no.\n\n27:38.800 --> 27:40.320\n There are some people who are more better at this.\n\n27:40.320 --> 27:42.360\n Some people are better than that.\n\n27:42.360 --> 27:45.440\n Right now we have machines that are much better than us\n\n27:45.440 --> 27:49.040\n at some very narrow tasks like multiplying large numbers\n\n27:49.040 --> 27:53.200\n fast, memorizing large databases, playing chess\n\n27:53.200 --> 27:56.280\n playing go and soon driving cars.\n\n27:57.480 --> 28:00.080\n But there's still no machine that can match\n\n28:00.080 --> 28:02.720\n a human child in general intelligence\n\n28:02.720 --> 28:05.720\n but artificial general intelligence, AGI\n\n28:05.720 --> 28:07.880\n the name of your course, of course\n\n28:07.880 --> 28:12.880\n that is by its very definition, the quest\n\n28:13.400 --> 28:16.000\n to build a machine that can do everything\n\n28:16.000 --> 28:17.800\n as well as we can.\n\n28:17.800 --> 28:21.960\n So the old Holy grail of AI from back to its inception\n\n28:21.960 --> 28:25.560\n in the sixties, if that ever happens, of course\n\n28:25.560 --> 28:27.320\n I think it's going to be the biggest transition\n\n28:27.320 --> 28:29.040\n in the history of life on earth\n\n28:29.040 --> 28:33.200\n but it doesn't necessarily have to wait the big impact\n\n28:33.200 --> 28:35.400\n until machines are better than us at knitting\n\n28:35.400 --> 28:39.160\n that the really big change doesn't come exactly\n\n28:39.160 --> 28:41.800\n at the moment they're better than us at everything.\n\n28:41.800 --> 28:44.120\n The really big change comes first\n\n28:44.120 --> 28:45.840\n there are big changes when they start becoming better\n\n28:45.840 --> 28:48.800\n at us at doing most of the jobs that we do\n\n28:48.800 --> 28:51.160\n because that takes away much of the demand\n\n28:51.160 --> 28:53.200\n for human labor.\n\n28:53.200 --> 28:55.640\n And then the really whopping change comes\n\n28:55.640 --> 29:00.640\n when they become better than us at AI research, right?\n\n29:01.040 --> 29:03.760\n Because right now the timescale of AI research\n\n29:03.760 --> 29:08.400\n is limited by the human research and development cycle\n\n29:08.400 --> 29:10.160\n of years typically, you know\n\n29:10.160 --> 29:13.480\n how long does it take from one release of some software\n\n29:13.480 --> 29:15.720\n or iPhone or whatever to the next?\n\n29:15.720 --> 29:20.720\n But once Google can replace 40,000 engineers\n\n29:20.920 --> 29:25.920\n by 40,000 equivalent pieces of software or whatever\n\n29:26.400 --> 29:29.680\n but then there's no reason that has to be years\n\n29:29.680 --> 29:31.840\n it can be in principle much faster\n\n29:31.840 --> 29:36.040\n and the timescale of future progress in AI\n\n29:36.040 --> 29:39.320\n and all of science and technology will be driven\n\n29:39.320 --> 29:40.960\n by machines, not humans.\n\n29:40.960 --> 29:45.960\n So it's this simple point which gives right\n\n29:46.520 --> 29:48.720\n this incredibly fun controversy\n\n29:48.720 --> 29:51.880\n about whether there can be intelligence explosion\n\n29:51.880 --> 29:54.400\n so called singularity as Werner Vinge called it.\n\n29:54.400 --> 29:57.040\n Now the idea is articulated by I.J. Good\n\n29:57.040 --> 29:59.480\n is obviously way back fifties\n\n29:59.480 --> 30:01.040\n but you can see Alan Turing\n\n30:01.040 --> 30:03.640\n and others thought about it even earlier.\n\n30:06.920 --> 30:10.080\n So you asked me what exactly would I define\n\n30:10.080 --> 30:12.800\n human level intelligence, yeah.\n\n30:12.800 --> 30:15.680\n So the glib answer is to say something\n\n30:15.680 --> 30:18.520\n which is better than us at all cognitive tasks\n\n30:18.520 --> 30:21.800\n with a better than any human at all cognitive tasks\n\n30:21.800 --> 30:23.080\n but the really interesting bar\n\n30:23.080 --> 30:25.760\n I think goes a little bit lower than that actually.\n\n30:25.760 --> 30:27.920\n It's when they can, when they're better than us\n\n30:27.920 --> 30:31.760\n at AI programming and general learning\n\n30:31.760 --> 30:35.360\n so that they can if they want to get better\n\n30:35.360 --> 30:37.240\n than us at anything by just studying.\n\n30:37.240 --> 30:40.560\n So they're better is a key word and better is towards\n\n30:40.560 --> 30:44.120\n this kind of spectrum of the complexity of goals\n\n30:44.120 --> 30:45.680\n it's able to accomplish.\n\n30:45.680 --> 30:50.360\n So another way to, and that's certainly\n\n30:50.360 --> 30:53.040\n a very clear definition of human love.\n\n30:53.040 --> 30:55.240\n So there's, it's almost like a sea that's rising\n\n30:55.240 --> 30:56.800\n you can do more and more and more things\n\n30:56.800 --> 30:58.640\n it's a geographic that you show\n\n30:58.640 --> 30:59.880\n it's really nice way to put it.\n\n30:59.880 --> 31:01.560\n So there's some peaks that\n\n31:01.560 --> 31:03.280\n and there's an ocean level elevating\n\n31:03.280 --> 31:04.800\n and you solve more and more problems\n\n31:04.800 --> 31:07.720\n but just kind of to take a pause\n\n31:07.720 --> 31:09.000\n and we took a bunch of questions\n\n31:09.000 --> 31:10.240\n and a lot of social networks\n\n31:10.240 --> 31:11.720\n and a bunch of people asked\n\n31:11.720 --> 31:14.480\n a sort of a slightly different direction\n\n31:14.480 --> 31:19.480\n on creativity and things that perhaps aren't a peak.\n\n31:23.560 --> 31:24.720\n Human beings are flawed\n\n31:24.720 --> 31:28.720\n and perhaps better means having contradiction\n\n31:28.720 --> 31:30.200\n being flawed in some way.\n\n31:30.200 --> 31:34.960\n So let me sort of start easy, first of all.\n\n31:34.960 --> 31:36.600\n So you have a lot of cool equations.\n\n31:36.600 --> 31:39.760\n Let me ask, what's your favorite equation, first of all?\n\n31:39.760 --> 31:42.760\n I know they're all like your children, but like\n\n31:42.760 --> 31:43.680\n which one is that?\n\n31:43.680 --> 31:45.560\n This is the shirt in your equation.\n\n31:45.560 --> 31:48.640\n It's the master key of quantum mechanics\n\n31:48.640 --> 31:49.880\n of the micro world.\n\n31:49.880 --> 31:52.800\n So this equation will protect everything\n\n31:52.800 --> 31:55.840\n to do with atoms, molecules and all the way up.\n\n31:55.840 --> 31:58.560\n Right?\n\n31:58.560 --> 31:59.760\n Yeah, so, okay.\n\n31:59.760 --> 32:02.080\n So quantum mechanics is certainly a beautiful\n\n32:02.080 --> 32:05.160\n mysterious formulation of our world.\n\n32:05.160 --> 32:08.760\n So I'd like to sort of ask you, just as an example\n\n32:08.760 --> 32:12.160\n it perhaps doesn't have the same beauty as physics does\n\n32:12.160 --> 32:16.960\n but in mathematics abstract, the Andrew Wiles\n\n32:16.960 --> 32:19.360\n who proved the Fermat's last theorem.\n\n32:19.360 --> 32:22.040\n So he just saw this recently\n\n32:22.040 --> 32:24.160\n and it kind of caught my eye a little bit.\n\n32:24.160 --> 32:27.960\n This is 358 years after it was conjectured.\n\n32:27.960 --> 32:29.960\n So this is very simple formulation.\n\n32:29.960 --> 32:32.640\n Everybody tried to prove it, everybody failed.\n\n32:32.640 --> 32:34.800\n And so here's this guy comes along\n\n32:34.800 --> 32:38.640\n and eventually proves it and then fails to prove it\n\n32:38.640 --> 32:41.320\n and then proves it again in 94.\n\n32:41.320 --> 32:43.480\n And he said like the moment when everything connected\n\n32:43.480 --> 32:46.040\n into place in an interview said\n\n32:46.040 --> 32:47.880\n it was so indescribably beautiful.\n\n32:47.880 --> 32:51.040\n That moment when you finally realize the connecting piece\n\n32:51.040 --> 32:52.800\n of two conjectures.\n\n32:52.800 --> 32:55.280\n He said, it was so indescribably beautiful.\n\n32:55.280 --> 32:57.040\n It was so simple and so elegant.\n\n32:57.040 --> 32:58.760\n I couldn't understand how I'd missed it.\n\n32:58.760 --> 33:02.080\n And I just stared at it in disbelief for 20 minutes.\n\n33:02.080 --> 33:05.240\n Then during the day, I walked around the department\n\n33:05.240 --> 33:07.880\n and I keep coming back to my desk\n\n33:07.880 --> 33:09.840\n looking to see if it was still there.\n\n33:09.840 --> 33:10.680\n It was still there.\n\n33:10.680 --> 33:11.760\n I couldn't contain myself.\n\n33:11.760 --> 33:12.880\n I was so excited.\n\n33:12.880 --> 33:15.880\n It was the most important moment on my working life.\n\n33:15.880 --> 33:18.960\n Nothing I ever do again will mean as much.\n\n33:18.960 --> 33:20.800\n So that particular moment.\n\n33:20.800 --> 33:24.640\n And it kind of made me think of what would it take?\n\n33:24.640 --> 33:27.960\n And I think we have all been there at small levels.\n\n33:29.480 --> 33:32.240\n Maybe let me ask, have you had a moment like that\n\n33:32.240 --> 33:34.880\n in your life where you just had an idea?\n\n33:34.880 --> 33:37.040\n It's like, wow, yes.\n\n33:40.000 --> 33:42.480\n I wouldn't mention myself in the same breath\n\n33:42.480 --> 33:44.760\n as Andrew Wiles, but I've certainly had a number\n\n33:44.760 --> 33:52.200\n of aha moments when I realized something very cool\n\n33:52.200 --> 33:56.000\n about physics, which has completely made my head explode.\n\n33:56.000 --> 33:58.320\n In fact, some of my favorite discoveries I made later,\n\n33:58.320 --> 34:01.080\n I later realized that they had been discovered earlier\n\n34:01.080 --> 34:03.240\n by someone who sometimes got quite famous for it.\n\n34:03.240 --> 34:05.480\n So it's too late for me to even publish it,\n\n34:05.480 --> 34:07.440\n but that doesn't diminish in any way.\n\n34:07.440 --> 34:09.760\n The emotional experience you have when you realize it,\n\n34:09.760 --> 34:11.320\n like, wow.\n\n34:11.320 --> 34:15.520\n Yeah, so what would it take in that moment, that wow,\n\n34:15.520 --> 34:17.320\n that was yours in that moment?\n\n34:17.320 --> 34:21.440\n So what do you think it takes for an intelligence system,\n\n34:21.440 --> 34:24.520\n an AGI system, an AI system to have a moment like that?\n\n34:25.640 --> 34:26.760\n That's a tricky question\n\n34:26.760 --> 34:29.200\n because there are actually two parts to it, right?\n\n34:29.200 --> 34:33.920\n One of them is, can it accomplish that proof?\n\n34:33.920 --> 34:37.640\n Can it prove that you can never write A to the N\n\n34:37.640 --> 34:42.760\n plus B to the N equals three to that equal Z to the N\n\n34:42.760 --> 34:45.320\n for all integers, et cetera, et cetera,\n\n34:45.320 --> 34:48.720\n when N is bigger than two?\n\n34:48.720 --> 34:51.360\n That's simply a question about intelligence.\n\n34:51.360 --> 34:54.120\n Can you build machines that are that intelligent?\n\n34:54.120 --> 34:57.280\n And I think by the time we get a machine\n\n34:57.280 --> 35:00.840\n that can independently come up with that level of proofs,\n\n35:00.840 --> 35:03.360\n probably quite close to AGI.\n\n35:03.360 --> 35:07.240\n The second question is a question about consciousness.\n\n35:07.240 --> 35:11.760\n When will we, how likely is it that such a machine\n\n35:11.760 --> 35:14.240\n will actually have any experience at all,\n\n35:14.240 --> 35:16.160\n as opposed to just being like a zombie?\n\n35:16.160 --> 35:20.560\n And would we expect it to have some sort of emotional response\n\n35:20.560 --> 35:24.640\n to this or anything at all akin to human emotion\n\n35:24.640 --> 35:28.320\n where when it accomplishes its machine goal,\n\n35:28.320 --> 35:31.920\n it views it as somehow something very positive\n\n35:31.920 --> 35:39.160\n and sublime and deeply meaningful?\n\n35:39.160 --> 35:41.440\n I would certainly hope that if in the future\n\n35:41.440 --> 35:45.120\n we do create machines that are our peers\n\n35:45.120 --> 35:50.160\n or even our descendants, that I would certainly\n\n35:50.160 --> 35:55.480\n hope that they do have this sublime appreciation of life.\n\n35:55.480 --> 35:58.840\n In a way, my absolutely worst nightmare\n\n35:58.840 --> 36:05.760\n would be that at some point in the future,\n\n36:05.760 --> 36:07.400\n the distant future, maybe our cosmos\n\n36:07.400 --> 36:10.600\n is teeming with all this post biological life doing\n\n36:10.600 --> 36:12.880\n all the seemingly cool stuff.\n\n36:12.880 --> 36:16.480\n And maybe the last humans, by the time\n\n36:16.480 --> 36:20.120\n our species eventually fizzles out,\n\n36:20.120 --> 36:21.920\n will be like, well, that's OK because we're\n\n36:21.920 --> 36:23.600\n so proud of our descendants here.\n\n36:23.600 --> 36:26.680\n And look what all the, my worst nightmare\n\n36:26.680 --> 36:30.360\n is that we haven't solved the consciousness problem.\n\n36:30.360 --> 36:32.880\n And we haven't realized that these are all the zombies.\n\n36:32.880 --> 36:36.200\n They're not aware of anything any more than a tape recorder\n\n36:36.200 --> 36:37.840\n has any kind of experience.\n\n36:37.840 --> 36:40.040\n So the whole thing has just become\n\n36:40.040 --> 36:41.520\n a play for empty benches.\n\n36:41.520 --> 36:44.640\n That would be the ultimate zombie apocalypse.\n\n36:44.640 --> 36:47.200\n So I would much rather, in that case,\n\n36:47.200 --> 36:52.240\n that we have these beings which can really\n\n36:52.240 --> 36:57.000\n appreciate how amazing it is.\n\n36:57.000 --> 37:01.080\n And in that picture, what would be the role of creativity?\n\n37:01.080 --> 37:04.960\n A few people ask about creativity.\n\n37:04.960 --> 37:07.080\n When you think about intelligence,\n\n37:07.080 --> 37:09.840\n certainly the story you told at the beginning of your book\n\n37:09.840 --> 37:15.200\n involved creating movies and so on, making money.\n\n37:15.200 --> 37:17.240\n You can make a lot of money in our modern world\n\n37:17.240 --> 37:18.600\n with music and movies.\n\n37:18.600 --> 37:20.880\n So if you are an intelligent system,\n\n37:20.880 --> 37:22.960\n you may want to get good at that.\n\n37:22.960 --> 37:26.280\n But that's not necessarily what I mean by creativity.\n\n37:26.280 --> 37:29.640\n Is it important on that complex goals\n\n37:29.640 --> 37:31.600\n where the sea is rising for there\n\n37:31.600 --> 37:33.800\n to be something creative?\n\n37:33.800 --> 37:37.400\n Or am I being very human centric and thinking creativity\n\n37:37.400 --> 37:41.880\n somehow special relative to intelligence?\n\n37:41.880 --> 37:47.240\n My hunch is that we should think of creativity simply\n\n37:47.240 --> 37:50.760\n as an aspect of intelligence.\n\n37:50.760 --> 37:57.840\n And we have to be very careful with human vanity.\n\n37:57.840 --> 37:59.520\n We have this tendency to very often want\n\n37:59.520 --> 38:01.560\n to say, as soon as machines can do something,\n\n38:01.560 --> 38:03.560\n we try to diminish it and say, oh, but that's\n\n38:03.560 --> 38:05.920\n not real intelligence.\n\n38:05.920 --> 38:08.400\n Isn't it creative or this or that?\n\n38:08.400 --> 38:12.200\n The other thing, if we ask ourselves\n\n38:12.200 --> 38:14.320\n to write down a definition of what we actually mean\n\n38:14.320 --> 38:18.840\n by being creative, what we mean by Andrew Wiles, what he did\n\n38:18.840 --> 38:21.880\n there, for example, don't we often mean that someone takes\n\n38:21.880 --> 38:26.000\n a very unexpected leap?\n\n38:26.000 --> 38:29.680\n It's not like taking 573 and multiplying it\n\n38:29.680 --> 38:33.840\n by 224 by just a step of straightforward cookbook\n\n38:33.840 --> 38:36.520\n like rules, right?\n\n38:36.520 --> 38:39.680\n You can maybe make a connection between two things\n\n38:39.680 --> 38:42.640\n that people had never thought was connected or something\n\n38:42.640 --> 38:44.480\n like that.\n\n38:44.480 --> 38:47.720\n I think this is an aspect of intelligence.\n\n38:47.720 --> 38:53.000\n And this is actually one of the most important aspects of it.\n\n38:53.000 --> 38:55.520\n Maybe the reason we humans tend to be better at it\n\n38:55.520 --> 38:57.840\n than traditional computers is because it's\n\n38:57.840 --> 38:59.640\n something that comes more naturally if you're\n\n38:59.640 --> 39:04.120\n a neural network than if you're a traditional logic gate\n\n39:04.120 --> 39:05.720\n based computer machine.\n\n39:05.720 --> 39:08.640\n We physically have all these connections.\n\n39:08.640 --> 39:13.800\n And you activate here, activate here, activate here.\n\n39:13.800 --> 39:16.560\n Bing.\n\n39:16.560 --> 39:21.040\n My hunch is that if we ever build a machine where you could\n\n39:21.040 --> 39:29.200\n just give it the task, hey, you say, hey, I just realized\n\n39:29.200 --> 39:32.320\n I want to travel around the world instead this month.\n\n39:32.320 --> 39:34.600\n Can you teach my AGI course for me?\n\n39:34.600 --> 39:35.960\n And it's like, OK, I'll do it.\n\n39:35.960 --> 39:37.920\n And it does everything that you would have done\n\n39:37.920 --> 39:39.760\n and improvises and stuff.\n\n39:39.760 --> 39:43.360\n That would, in my mind, involve a lot of creativity.\n\n39:43.360 --> 39:45.680\n Yeah, so it's actually a beautiful way to put it.\n\n39:45.680 --> 39:52.640\n I think we do try to grasp at the definition of intelligence\n\n39:52.640 --> 39:56.360\n is everything we don't understand how to build.\n\n39:56.360 --> 39:59.360\n So we as humans try to find things\n\n39:59.360 --> 40:01.240\n that we have and machines don't have.\n\n40:01.240 --> 40:03.800\n And maybe creativity is just one of the things, one\n\n40:03.800 --> 40:05.480\n of the words we use to describe that.\n\n40:05.480 --> 40:07.200\n That's a really interesting way to put it.\n\n40:07.200 --> 40:09.520\n I don't think we need to be that defensive.\n\n40:09.520 --> 40:11.560\n I don't think anything good comes out of saying,\n\n40:11.560 --> 40:18.080\n well, we're somehow special, you know?\n\n40:18.080 --> 40:21.040\n Contrary wise, there are many examples in history\n\n40:21.040 --> 40:27.840\n of where trying to pretend that we're somehow superior\n\n40:27.840 --> 40:33.120\n to all other intelligent beings has led to pretty bad results,\n\n40:33.120 --> 40:35.960\n right?\n\n40:35.960 --> 40:38.440\n Nazi Germany, they said that they were somehow superior\n\n40:38.440 --> 40:40.080\n to other people.\n\n40:40.080 --> 40:42.440\n Today, we still do a lot of cruelty to animals\n\n40:42.440 --> 40:44.440\n by saying that we're so superior somehow,\n\n40:44.440 --> 40:46.440\n and they can't feel pain.\n\n40:46.440 --> 40:48.480\n Slavery was justified by the same kind\n\n40:48.480 --> 40:52.200\n of just really weak arguments.\n\n40:52.200 --> 40:57.120\n And I don't think if we actually go ahead and build\n\n40:57.120 --> 40:59.440\n artificial general intelligence, it\n\n40:59.440 --> 41:01.360\n can do things better than us, I don't\n\n41:01.360 --> 41:04.080\n think we should try to found our self worth on some sort\n\n41:04.080 --> 41:09.760\n of bogus claims of superiority in terms\n\n41:09.760 --> 41:12.120\n of our intelligence.\n\n41:12.120 --> 41:18.080\n I think we should instead find our calling\n\n41:18.080 --> 41:23.360\n and the meaning of life from the experiences that we have.\n\n41:23.360 --> 41:28.720\n I can have very meaningful experiences\n\n41:28.720 --> 41:32.920\n even if there are other people who are smarter than me.\n\n41:32.920 --> 41:34.400\n When I go to a faculty meeting here,\n\n41:34.400 --> 41:36.520\n and we talk about something, and then I certainly realize,\n\n41:36.520 --> 41:39.080\n oh, boy, he has an old prize, he has an old prize,\n\n41:39.080 --> 41:40.800\n he has an old prize, I don't have one.\n\n41:40.800 --> 41:43.760\n Does that make me enjoy life any less\n\n41:43.760 --> 41:47.560\n or enjoy talking to those people less?\n\n41:47.560 --> 41:49.560\n Of course not.\n\n41:49.560 --> 41:54.160\n And the contrary, I feel very honored and privileged\n\n41:54.160 --> 41:58.760\n to get to interact with other very intelligent beings that\n\n41:58.760 --> 42:00.680\n are better than me at a lot of stuff.\n\n42:00.680 --> 42:02.840\n So I don't think there's any reason why\n\n42:02.840 --> 42:06.080\n we can't have the same approach with intelligent machines.\n\n42:06.080 --> 42:07.320\n That's a really interesting.\n\n42:07.320 --> 42:08.920\n So people don't often think about that.\n\n42:08.920 --> 42:10.600\n They think about when there's going,\n\n42:10.600 --> 42:13.320\n if there's machines that are more intelligent,\n\n42:13.320 --> 42:15.080\n you naturally think that that's not\n\n42:15.080 --> 42:19.080\n going to be a beneficial type of intelligence.\n\n42:19.080 --> 42:23.000\n You don't realize it could be like peers with Nobel prizes\n\n42:23.000 --> 42:25.120\n that would be just fun to talk with,\n\n42:25.120 --> 42:27.560\n and they might be clever about certain topics,\n\n42:27.560 --> 42:32.240\n and you can have fun having a few drinks with them.\n\n42:32.240 --> 42:35.880\n Well, also, another example we can all\n\n42:35.880 --> 42:39.320\n relate to of why it doesn't have to be a terrible thing\n\n42:39.320 --> 42:42.560\n to be in the presence of people who are even smarter than us\n\n42:42.560 --> 42:45.600\n all around is when you and I were both two years old,\n\n42:45.600 --> 42:48.360\n I mean, our parents were much more intelligent than us,\n\n42:48.360 --> 42:49.040\n right?\n\n42:49.040 --> 42:51.960\n Worked out OK, because their goals\n\n42:51.960 --> 42:53.960\n were aligned with our goals.\n\n42:53.960 --> 42:58.680\n And that, I think, is really the number one key issue\n\n42:58.680 --> 43:02.280\n we have to solve if we value align the value alignment\n\n43:02.280 --> 43:03.080\n problem, exactly.\n\n43:03.080 --> 43:06.520\n Because people who see too many Hollywood movies\n\n43:06.520 --> 43:10.000\n with lousy science fiction plot lines,\n\n43:10.000 --> 43:12.200\n they worry about the wrong thing, right?\n\n43:12.200 --> 43:16.320\n They worry about some machine suddenly turning evil.\n\n43:16.320 --> 43:21.480\n It's not malice that is the concern.\n\n43:21.480 --> 43:22.880\n It's competence.\n\n43:22.880 --> 43:27.440\n By definition, intelligent makes you very competent.\n\n43:27.440 --> 43:31.920\n If you have a more intelligent goal playing,\n\n43:31.920 --> 43:33.680\n computer playing is a less intelligent one.\n\n43:33.680 --> 43:36.120\n And when we define intelligence as the ability\n\n43:36.120 --> 43:38.600\n to accomplish goal winning, it's going\n\n43:38.600 --> 43:40.560\n to be the more intelligent one that wins.\n\n43:40.560 --> 43:43.560\n And if you have a human and then you\n\n43:43.560 --> 43:47.720\n have an AGI that's more intelligent in all ways\n\n43:47.720 --> 43:49.520\n and they have different goals, guess who's\n\n43:49.520 --> 43:50.720\n going to get their way, right?\n\n43:50.720 --> 43:57.120\n So I was just reading about this particular rhinoceros species\n\n43:57.120 --> 43:59.200\n that was driven extinct just a few years ago.\n\n43:59.200 --> 44:02.280\n Ellen Bummer is looking at this cute picture of a mommy\n\n44:02.280 --> 44:05.080\n rhinoceros with its child.\n\n44:05.080 --> 44:09.320\n And why did we humans drive it to extinction?\n\n44:09.320 --> 44:12.800\n It wasn't because we were evil rhino haters as a whole.\n\n44:12.800 --> 44:14.920\n It was just because our goals weren't aligned\n\n44:14.920 --> 44:16.000\n with those of the rhinoceros.\n\n44:16.000 --> 44:17.680\n And it didn't work out so well for the rhinoceros\n\n44:17.680 --> 44:19.560\n because we were more intelligent, right?\n\n44:19.560 --> 44:21.240\n So I think it's just so important\n\n44:21.240 --> 44:27.120\n that if we ever do build AGI, before we unleash anything,\n\n44:27.120 --> 44:31.840\n we have to make sure that it learns\n\n44:31.840 --> 44:36.000\n to understand our goals, that it adopts our goals,\n\n44:36.000 --> 44:37.920\n and that it retains those goals.\n\n44:37.920 --> 44:40.520\n So the cool, interesting problem there\n\n44:40.520 --> 44:47.040\n is us as human beings trying to formulate our values.\n\n44:47.040 --> 44:51.360\n So you could think of the United States Constitution as a way\n\n44:51.360 --> 44:56.680\n that people sat down, at the time a bunch of white men,\n\n44:56.680 --> 44:59.680\n which is a good example, I should say.\n\n44:59.680 --> 45:01.480\n They formulated the goals for this country.\n\n45:01.480 --> 45:03.760\n And a lot of people agree that those goals actually\n\n45:03.760 --> 45:05.360\n held up pretty well.\n\n45:05.360 --> 45:07.160\n That's an interesting formulation of values\n\n45:07.160 --> 45:09.440\n and failed miserably in other ways.\n\n45:09.440 --> 45:13.320\n So for the value alignment problem and the solution to it,\n\n45:13.320 --> 45:19.560\n we have to be able to put on paper or in a program\n\n45:19.560 --> 45:20.400\n human values.\n\n45:20.400 --> 45:22.400\n How difficult do you think that is?\n\n45:22.400 --> 45:24.040\n Very.\n\n45:24.040 --> 45:25.880\n But it's so important.\n\n45:25.880 --> 45:28.000\n We really have to give it our best.\n\n45:28.000 --> 45:30.120\n And it's difficult for two separate reasons.\n\n45:30.120 --> 45:33.440\n There's the technical value alignment problem\n\n45:33.440 --> 45:39.120\n of figuring out just how to make machines understand our goals,\n\n45:39.120 --> 45:40.440\n adopt them, and retain them.\n\n45:40.440 --> 45:43.200\n And then there's the separate part of it,\n\n45:43.200 --> 45:44.200\n the philosophical part.\n\n45:44.200 --> 45:45.920\n Whose values anyway?\n\n45:45.920 --> 45:48.320\n And since it's not like we have any great consensus\n\n45:48.320 --> 45:52.040\n on this planet on values, what mechanism should we\n\n45:52.040 --> 45:54.120\n create then to aggregate and decide, OK,\n\n45:54.120 --> 45:56.520\n what's a good compromise?\n\n45:56.520 --> 45:58.440\n That second discussion can't just\n\n45:58.440 --> 46:01.560\n be left to tech nerds like myself.\n\n46:01.560 --> 46:05.720\n And if we refuse to talk about it and then AGI gets built,\n\n46:05.720 --> 46:07.160\n who's going to be actually making\n\n46:07.160 --> 46:08.480\n the decision about whose values?\n\n46:08.480 --> 46:12.080\n It's going to be a bunch of dudes in some tech company.\n\n46:12.080 --> 46:17.240\n And are they necessarily so representative of all\n\n46:17.240 --> 46:19.400\n of humankind that we want to just entrust it to them?\n\n46:19.400 --> 46:23.000\n Are they even uniquely qualified to speak\n\n46:23.000 --> 46:25.240\n to future human happiness just because they're\n\n46:25.240 --> 46:26.480\n good at programming AI?\n\n46:26.480 --> 46:30.200\n I'd much rather have this be a really inclusive conversation.\n\n46:30.200 --> 46:32.560\n But do you think it's possible?\n\n46:32.560 --> 46:37.560\n So you create a beautiful vision that includes the diversity,\n\n46:37.560 --> 46:40.960\n cultural diversity, and various perspectives on discussing\n\n46:40.960 --> 46:43.600\n rights, freedoms, human dignity.\n\n46:43.600 --> 46:46.520\n But how hard is it to come to that consensus?\n\n46:46.520 --> 46:50.400\n Do you think it's certainly a really important thing\n\n46:50.400 --> 46:51.880\n that we should all try to do?\n\n46:51.880 --> 46:54.240\n But do you think it's feasible?\n\n46:54.240 --> 47:00.160\n I think there's no better way to guarantee failure than to\n\n47:00.160 --> 47:02.840\n refuse to talk about it or refuse to try.\n\n47:02.840 --> 47:05.320\n And I also think it's a really bad strategy\n\n47:05.320 --> 47:08.560\n to say, OK, let's first have a discussion for a long time.\n\n47:08.560 --> 47:11.040\n And then once we reach complete consensus,\n\n47:11.040 --> 47:13.360\n then we'll try to load it into some machine.\n\n47:13.360 --> 47:16.560\n No, we shouldn't let perfect be the enemy of good.\n\n47:16.560 --> 47:20.600\n Instead, we should start with the kindergarten ethics\n\n47:20.600 --> 47:22.120\n that pretty much everybody agrees on\n\n47:22.120 --> 47:24.360\n and put that into machines now.\n\n47:24.360 --> 47:25.880\n We're not doing that even.\n\n47:25.880 --> 47:31.000\n Look at anyone who builds this passenger aircraft,\n\n47:31.000 --> 47:33.000\n wants it to never under any circumstances\n\n47:33.000 --> 47:35.600\n fly into a building or a mountain.\n\n47:35.600 --> 47:38.480\n Yet the September 11 hijackers were able to do that.\n\n47:38.480 --> 47:41.800\n And even more embarrassingly, Andreas Lubitz,\n\n47:41.800 --> 47:43.960\n this depressed Germanwings pilot,\n\n47:43.960 --> 47:47.360\n when he flew his passenger jet into the Alps killing over 100\n\n47:47.360 --> 47:50.640\n people, he just told the autopilot to do it.\n\n47:50.640 --> 47:53.200\n He told the freaking computer to change the altitude\n\n47:53.200 --> 47:55.040\n to 100 meters.\n\n47:55.040 --> 47:58.160\n And even though it had the GPS maps, everything,\n\n47:58.160 --> 48:00.640\n the computer was like, OK.\n\n48:00.640 --> 48:05.320\n So we should take those very basic values,\n\n48:05.320 --> 48:08.400\n where the problem is not that we don't agree.\n\n48:08.400 --> 48:10.120\n The problem is just we've been too lazy\n\n48:10.120 --> 48:11.480\n to try to put it into our machines\n\n48:11.480 --> 48:15.520\n and make sure that from now on, airplanes will just,\n\n48:15.520 --> 48:16.920\n which all have computers in them,\n\n48:16.920 --> 48:19.720\n but will just refuse to do something like that.\n\n48:19.720 --> 48:22.160\n Go into safe mode, maybe lock the cockpit door,\n\n48:22.160 --> 48:24.480\n go over to the nearest airport.\n\n48:24.480 --> 48:28.080\n And there's so much other technology in our world\n\n48:28.080 --> 48:31.320\n as well now, where it's really becoming quite timely\n\n48:31.320 --> 48:34.120\n to put in some sort of very basic values like this.\n\n48:34.120 --> 48:39.240\n Even in cars, we've had enough vehicle terrorism attacks\n\n48:39.240 --> 48:42.040\n by now, where people have driven trucks and vans\n\n48:42.040 --> 48:45.480\n into pedestrians, that it's not at all a crazy idea\n\n48:45.480 --> 48:48.680\n to just have that hardwired into the car.\n\n48:48.680 --> 48:50.280\n Because yeah, there are a lot of,\n\n48:50.280 --> 48:52.240\n there's always going to be people who for some reason\n\n48:52.240 --> 48:54.800\n want to harm others, but most of those people\n\n48:54.800 --> 48:56.760\n don't have the technical expertise to figure out\n\n48:56.760 --> 48:58.520\n how to work around something like that.\n\n48:58.520 --> 49:01.760\n So if the car just won't do it, it helps.\n\n49:01.760 --> 49:02.840\n So let's start there.\n\n49:02.840 --> 49:04.960\n So there's a lot of, that's a great point.\n\n49:04.960 --> 49:06.800\n So not chasing perfect.\n\n49:06.800 --> 49:10.840\n There's a lot of things that most of the world agrees on.\n\n49:10.840 --> 49:11.840\n Yeah, let's start there.\n\n49:11.840 --> 49:12.680\n Let's start there.\n\n49:12.680 --> 49:14.560\n And then once we start there,\n\n49:14.560 --> 49:17.240\n we'll also get into the habit of having\n\n49:17.240 --> 49:18.520\n these kind of conversations about, okay,\n\n49:18.520 --> 49:21.760\n what else should we put in here and have these discussions?\n\n49:21.760 --> 49:23.920\n This should be a gradual process then.\n\n49:23.920 --> 49:28.600\n Great, so, but that also means describing these things\n\n49:28.600 --> 49:31.240\n and describing it to a machine.\n\n49:31.240 --> 49:34.200\n So one thing, we had a few conversations\n\n49:34.200 --> 49:35.640\n with Stephen Wolfram.\n\n49:35.640 --> 49:37.080\n I'm not sure if you're familiar with Stephen.\n\n49:37.080 --> 49:38.360\n Oh yeah, I know him quite well.\n\n49:38.360 --> 49:42.040\n So he is, he works with a bunch of things,\n\n49:42.040 --> 49:46.560\n but cellular automata, these simple computable things,\n\n49:46.560 --> 49:47.960\n these computation systems.\n\n49:47.960 --> 49:49.880\n And he kind of mentioned that,\n\n49:49.880 --> 49:52.480\n we probably have already within these systems\n\n49:52.480 --> 49:54.680\n already something that's AGI,\n\n49:56.120 --> 49:58.720\n meaning like we just don't know it\n\n49:58.720 --> 50:00.400\n because we can't talk to it.\n\n50:00.400 --> 50:04.800\n So if you give me this chance to try to at least\n\n50:04.800 --> 50:06.720\n form a question out of this is,\n\n50:07.600 --> 50:10.880\n I think it's an interesting idea to think\n\n50:10.880 --> 50:12.680\n that we can have intelligent systems,\n\n50:12.680 --> 50:15.600\n but we don't know how to describe something to them\n\n50:15.600 --> 50:17.360\n and they can't communicate with us.\n\n50:17.360 --> 50:19.840\n I know you're doing a little bit of work in explainable AI,\n\n50:19.840 --> 50:22.040\n trying to get AI to explain itself.\n\n50:22.040 --> 50:25.520\n So what are your thoughts of natural language processing\n\n50:25.520 --> 50:27.640\n or some kind of other communication?\n\n50:27.640 --> 50:30.120\n How does the AI explain something to us?\n\n50:30.120 --> 50:33.640\n How do we explain something to it, to machines?\n\n50:33.640 --> 50:35.320\n Or you think of it differently?\n\n50:35.320 --> 50:39.960\n So there are two separate parts to your question there.\n\n50:39.960 --> 50:42.440\n One of them has to do with communication,\n\n50:42.440 --> 50:44.440\n which is super interesting, I'll get to that in a sec.\n\n50:44.440 --> 50:47.280\n The other is whether we already have AGI\n\n50:47.280 --> 50:49.240\n but we just haven't noticed it there.\n\n50:49.240 --> 50:50.080\n Right.\n\n50:51.800 --> 50:53.000\n There I beg to differ.\n\n50:54.280 --> 50:56.480\n I don't think there's anything in any cellular automaton\n\n50:56.480 --> 50:59.040\n or anything or the internet itself or whatever\n\n50:59.040 --> 51:03.560\n that has artificial general intelligence\n\n51:03.560 --> 51:05.520\n and that it can really do exactly everything\n\n51:05.520 --> 51:07.000\n we humans can do better.\n\n51:07.000 --> 51:11.600\n I think the day that happens, when that happens,\n\n51:11.600 --> 51:15.600\n we will very soon notice, we'll probably notice even before\n\n51:15.600 --> 51:17.440\n because in a very, very big way.\n\n51:17.440 --> 51:18.840\n But for the second part, though.\n\n51:18.840 --> 51:20.720\n Wait, can I ask, sorry.\n\n51:20.720 --> 51:24.400\n So, because you have this beautiful way\n\n51:24.400 --> 51:29.400\n to formulating consciousness as information processing,\n\n51:30.360 --> 51:31.360\n and you can think of intelligence\n\n51:31.360 --> 51:32.280\n as information processing,\n\n51:32.280 --> 51:34.320\n and you can think of the entire universe\n\n51:34.320 --> 51:38.720\n as these particles and these systems roaming around\n\n51:38.720 --> 51:41.360\n that have this information processing power.\n\n51:41.360 --> 51:44.840\n You don't think there is something with the power\n\n51:44.840 --> 51:49.040\n to process information in the way that we human beings do\n\n51:49.040 --> 51:54.040\n that's out there that needs to be sort of connected to.\n\n51:55.400 --> 51:57.880\n It seems a little bit philosophical, perhaps,\n\n51:57.880 --> 52:00.080\n but there's something compelling to the idea\n\n52:00.080 --> 52:01.920\n that the power is already there,\n\n52:01.920 --> 52:05.440\n which the focus should be more on being able\n\n52:05.440 --> 52:07.360\n to communicate with it.\n\n52:07.360 --> 52:11.960\n Well, I agree that in a certain sense,\n\n52:11.960 --> 52:15.360\n the hardware processing power is already out there\n\n52:15.360 --> 52:19.000\n because our universe itself can think of it\n\n52:19.000 --> 52:21.000\n as being a computer already, right?\n\n52:21.000 --> 52:23.800\n It's constantly computing what water waves,\n\n52:23.800 --> 52:26.120\n how it devolved the water waves in the River Charles\n\n52:26.120 --> 52:28.440\n and how to move the air molecules around.\n\n52:28.440 --> 52:30.480\n Seth Lloyd has pointed out, my colleague here,\n\n52:30.480 --> 52:32.920\n that you can even in a very rigorous way\n\n52:32.920 --> 52:35.480\n think of our entire universe as being a quantum computer.\n\n52:35.480 --> 52:37.680\n It's pretty clear that our universe\n\n52:37.680 --> 52:40.320\n supports this amazing processing power\n\n52:40.320 --> 52:42.160\n because you can even,\n\n52:42.160 --> 52:44.920\n within this physics computer that we live in, right?\n\n52:44.920 --> 52:47.040\n We can even build actual laptops and stuff,\n\n52:47.040 --> 52:49.000\n so clearly the power is there.\n\n52:49.000 --> 52:52.040\n It's just that most of the compute power that nature has,\n\n52:52.040 --> 52:54.240\n it's, in my opinion, kind of wasting on boring stuff\n\n52:54.240 --> 52:56.520\n like simulating yet another ocean wave somewhere\n\n52:56.520 --> 52:58.040\n where no one is even looking, right?\n\n52:58.040 --> 53:00.880\n So in a sense, what life does, what we are doing\n\n53:00.880 --> 53:03.880\n when we build computers is we're rechanneling\n\n53:03.880 --> 53:07.200\n all this compute that nature is doing anyway\n\n53:07.200 --> 53:09.360\n into doing things that are more interesting\n\n53:09.360 --> 53:11.440\n than just yet another ocean wave,\n\n53:11.440 --> 53:13.200\n and let's do something cool here.\n\n53:14.080 --> 53:17.080\n So the raw hardware power is there, for sure,\n\n53:17.080 --> 53:21.080\n but then even just computing what's going to happen\n\n53:21.080 --> 53:23.520\n for the next five seconds in this water bottle,\n\n53:23.520 --> 53:26.000\n takes a ridiculous amount of compute\n\n53:26.000 --> 53:27.920\n if you do it on a human computer.\n\n53:27.920 --> 53:29.920\n This water bottle just did it.\n\n53:29.920 --> 53:33.440\n But that does not mean that this water bottle has AGI\n\n53:34.760 --> 53:37.040\n because AGI means it should also be able to,\n\n53:37.040 --> 53:40.160\n like I've written my book, done this interview.\n\n53:40.160 --> 53:42.080\n And I don't think it's just communication problems.\n\n53:42.080 --> 53:46.760\n I don't really think it can do it.\n\n53:46.760 --> 53:49.280\n Although Buddhists say when they watch the water\n\n53:49.280 --> 53:51.240\n and that there is some beauty,\n\n53:51.240 --> 53:53.720\n that there's some depth and beauty in nature\n\n53:53.720 --> 53:54.840\n that they can communicate with.\n\n53:54.840 --> 53:56.480\n Communication is also very important though\n\n53:56.480 --> 54:01.200\n because I mean, look, part of my job is being a teacher.\n\n54:01.200 --> 54:06.200\n And I know some very intelligent professors even\n\n54:06.200 --> 54:09.800\n who just have a bit of hard time communicating.\n\n54:09.800 --> 54:12.640\n They come up with all these brilliant ideas,\n\n54:12.640 --> 54:14.520\n but to communicate with somebody else,\n\n54:14.520 --> 54:16.920\n you have to also be able to simulate their own mind.\n\n54:16.920 --> 54:18.360\n Yes, empathy.\n\n54:18.360 --> 54:20.640\n Build well enough and understand model of their mind\n\n54:20.640 --> 54:24.400\n that you can say things that they will understand.\n\n54:24.400 --> 54:26.480\n And that's quite difficult.\n\n54:26.480 --> 54:28.280\n And that's why today it's so frustrating\n\n54:28.280 --> 54:32.600\n if you have a computer that makes some cancer diagnosis\n\n54:32.600 --> 54:34.120\n and you ask it, well, why are you saying\n\n54:34.120 --> 54:36.120\n I should have this surgery?\n\n54:36.120 --> 54:37.960\n And if it can only reply,\n\n54:37.960 --> 54:40.800\n I was trained on five terabytes of data\n\n54:40.800 --> 54:45.080\n and this is my diagnosis, boop, boop, beep, beep.\n\n54:45.080 --> 54:49.120\n It doesn't really instill a lot of confidence, right?\n\n54:49.120 --> 54:51.120\n So I think we have a lot of work to do\n\n54:51.120 --> 54:54.320\n on communication there.\n\n54:54.320 --> 54:58.040\n So what kind of, I think you're doing a little bit of work\n\n54:58.040 --> 54:59.320\n in explainable AI.\n\n54:59.320 --> 55:01.320\n What do you think are the most promising avenues?\n\n55:01.320 --> 55:05.240\n Is it mostly about sort of the Alexa problem\n\n55:05.240 --> 55:07.200\n of natural language processing of being able\n\n55:07.200 --> 55:11.600\n to actually use human interpretable methods\n\n55:11.600 --> 55:13.160\n of communication?\n\n55:13.160 --> 55:16.000\n So being able to talk to a system and it talk back to you,\n\n55:16.000 --> 55:18.640\n or is there some more fundamental problems to be solved?\n\n55:18.640 --> 55:21.160\n I think it's all of the above.\n\n55:21.160 --> 55:23.520\n The natural language processing is obviously important,\n\n55:23.520 --> 55:27.600\n but there are also more nerdy fundamental problems.\n\n55:27.600 --> 55:31.640\n Like if you take, you play chess?\n\n55:31.640 --> 55:33.040\n Of course, I'm Russian.\n\n55:33.040 --> 55:33.880\n I have to.\n\n55:33.880 --> 55:34.720\n You speak Russian?\n\n55:34.720 --> 55:35.560\n Yes, I speak Russian.\n\n55:35.560 --> 55:38.040\n Excellent, I didn't know.\n\n55:38.040 --> 55:39.160\n When did you learn Russian?\n\n55:39.160 --> 55:41.800\n I speak very bad Russian, I'm only an autodidact,\n\n55:41.800 --> 55:44.560\n but I bought a book, Teach Yourself Russian,\n\n55:44.560 --> 55:47.720\n read a lot, but it was very difficult.\n\n55:47.720 --> 55:48.560\n Wow.\n\n55:48.560 --> 55:49.960\n That's why I speak so bad.\n\n55:49.960 --> 55:51.960\n How many languages do you know?\n\n55:51.960 --> 55:53.840\n Wow, that's really impressive.\n\n55:53.840 --> 55:56.320\n I don't know, my wife has some calculation,\n\n55:56.320 --> 55:58.400\n but my point was, if you play chess,\n\n55:58.400 --> 56:01.040\n have you looked at the AlphaZero games?\n\n56:01.040 --> 56:02.600\n The actual games, no.\n\n56:02.600 --> 56:05.000\n Check it out, some of them are just mind blowing,\n\n56:06.320 --> 56:07.720\n really beautiful.\n\n56:07.720 --> 56:12.400\n And if you ask, how did it do that?\n\n56:13.760 --> 56:16.520\n You go talk to Demis Hassabis,\n\n56:16.520 --> 56:18.240\n I know others from DeepMind,\n\n56:19.120 --> 56:20.600\n all they'll ultimately be able to give you\n\n56:20.600 --> 56:23.920\n is big tables of numbers, matrices,\n\n56:23.920 --> 56:25.720\n that define the neural network.\n\n56:25.720 --> 56:28.080\n And you can stare at these tables of numbers\n\n56:28.080 --> 56:29.600\n till your face turn blue,\n\n56:29.600 --> 56:32.520\n and you're not gonna understand much\n\n56:32.520 --> 56:34.520\n about why it made that move.\n\n56:34.520 --> 56:37.640\n And even if you have natural language processing\n\n56:37.640 --> 56:40.280\n that can tell you in human language about,\n\n56:40.280 --> 56:42.520\n oh, five, seven, points, two, eight,\n\n56:42.520 --> 56:43.560\n still not gonna really help.\n\n56:43.560 --> 56:47.480\n So I think there's a whole spectrum of fun challenges\n\n56:47.480 --> 56:50.520\n that are involved in taking a computation\n\n56:50.520 --> 56:52.240\n that does intelligent things\n\n56:52.240 --> 56:56.240\n and transforming it into something equally good,\n\n56:57.760 --> 57:01.840\n equally intelligent, but that's more understandable.\n\n57:01.840 --> 57:03.240\n And I think that's really valuable\n\n57:03.240 --> 57:07.440\n because I think as we put machines in charge\n\n57:07.440 --> 57:09.760\n of ever more infrastructure in our world,\n\n57:09.760 --> 57:12.680\n the power grid, the trading on the stock market,\n\n57:12.680 --> 57:14.320\n weapon systems and so on,\n\n57:14.320 --> 57:17.760\n it's absolutely crucial that we can trust\n\n57:17.760 --> 57:19.400\n these AIs to do all we want.\n\n57:19.400 --> 57:21.520\n And trust really comes from understanding\n\n57:22.520 --> 57:24.400\n in a very fundamental way.\n\n57:24.400 --> 57:27.560\n And that's why I'm working on this,\n\n57:27.560 --> 57:29.160\n because I think the more,\n\n57:29.160 --> 57:31.840\n if we're gonna have some hope of ensuring\n\n57:31.840 --> 57:33.520\n that machines have adopted our goals\n\n57:33.520 --> 57:35.800\n and that they're gonna retain them,\n\n57:35.800 --> 57:38.800\n that kind of trust, I think,\n\n57:38.800 --> 57:41.200\n needs to be based on things you can actually understand,\n\n57:41.200 --> 57:44.240\n preferably even improve theorems on.\n\n57:44.240 --> 57:46.080\n Even with a self driving car, right?\n\n57:47.040 --> 57:48.680\n If someone just tells you it's been trained\n\n57:48.680 --> 57:50.640\n on tons of data and it never crashed,\n\n57:50.640 --> 57:54.200\n it's less reassuring than if someone actually has a proof.\n\n57:54.200 --> 57:55.960\n Maybe it's a computer verified proof,\n\n57:55.960 --> 57:58.800\n but still it says that under no circumstances\n\n57:58.800 --> 58:02.320\n is this car just gonna swerve into oncoming traffic.\n\n58:02.320 --> 58:04.640\n And that kind of information helps to build trust\n\n58:04.640 --> 58:08.080\n and helps build the alignment of goals,\n\n58:09.400 --> 58:12.200\n at least awareness that your goals, your values are aligned.\n\n58:12.200 --> 58:13.840\n And I think even in the very short term,\n\n58:13.840 --> 58:16.360\n if you look at how, you know, today, right?\n\n58:16.360 --> 58:19.320\n This absolutely pathetic state of cybersecurity\n\n58:19.320 --> 58:21.720\n that we have, where is it?\n\n58:21.720 --> 58:25.960\n Three billion Yahoo accounts we can't pack,\n\n58:27.200 --> 58:31.720\n almost every American's credit card and so on.\n\n58:32.800 --> 58:34.120\n Why is this happening?\n\n58:34.120 --> 58:37.960\n It's ultimately happening because we have software\n\n58:37.960 --> 58:41.200\n that nobody fully understood how it worked.\n\n58:41.200 --> 58:44.800\n That's why the bugs hadn't been found, right?\n\n58:44.800 --> 58:47.480\n And I think AI can be used very effectively\n\n58:47.480 --> 58:49.640\n for offense, for hacking,\n\n58:49.640 --> 58:52.320\n but it can also be used for defense.\n\n58:52.320 --> 58:55.360\n Hopefully automating verifiability\n\n58:55.360 --> 59:00.360\n and creating systems that are built in different ways\n\n59:00.680 --> 59:02.920\n so you can actually prove things about them.\n\n59:02.920 --> 59:05.240\n And it's important.\n\n59:05.240 --> 59:07.680\n So speaking of software that nobody understands\n\n59:07.680 --> 59:10.640\n how it works, of course, a bunch of people ask\n\n59:10.640 --> 59:12.160\n about your paper, about your thoughts\n\n59:12.160 --> 59:14.680\n of why does deep and cheap learning work so well?\n\n59:14.680 --> 59:15.520\n That's the paper.\n\n59:15.520 --> 59:18.320\n But what are your thoughts on deep learning?\n\n59:18.320 --> 59:21.880\n These kind of simplified models of our own brains\n\n59:21.880 --> 59:26.440\n have been able to do some successful perception work,\n\n59:26.440 --> 59:29.560\n pattern recognition work, and now with AlphaZero and so on,\n\n59:29.560 --> 59:30.880\n do some clever things.\n\n59:30.880 --> 59:33.880\n What are your thoughts about the promise limitations\n\n59:33.880 --> 59:35.680\n of this piece?\n\n59:35.680 --> 59:40.680\n Great, I think there are a number of very important insights,\n\n59:43.080 --> 59:44.640\n very important lessons we can always draw\n\n59:44.640 --> 59:47.120\n from these kinds of successes.\n\n59:47.120 --> 59:48.960\n One of them is when you look at the human brain,\n\n59:48.960 --> 59:51.480\n you see it's very complicated, 10th of 11 neurons,\n\n59:51.480 --> 59:53.320\n and there are all these different kinds of neurons\n\n59:53.320 --> 59:55.040\n and yada, yada, and there's been this long debate\n\n59:55.040 --> 59:57.200\n about whether the fact that we have dozens\n\n59:57.200 --> 1:00:00.160\n of different kinds is actually necessary for intelligence.\n\n1:00:01.560 --> 1:00:03.360\n We can now, I think, quite convincingly answer\n\n1:00:03.360 --> 1:00:07.640\n that question of no, it's enough to have just one kind.\n\n1:00:07.640 --> 1:00:09.920\n If you look under the hood of AlphaZero,\n\n1:00:09.920 --> 1:00:11.080\n there's only one kind of neuron\n\n1:00:11.080 --> 1:00:15.000\n and it's ridiculously simple mathematical thing.\n\n1:00:15.000 --> 1:00:17.280\n So it's just like in physics,\n\n1:00:17.280 --> 1:00:20.320\n it's not, if you have a gas with waves in it,\n\n1:00:20.320 --> 1:00:23.240\n it's not the detailed nature of the molecule that matter,\n\n1:00:24.240 --> 1:00:26.040\n it's the collective behavior somehow.\n\n1:00:26.040 --> 1:00:30.720\n Similarly, it's this higher level structure\n\n1:00:30.720 --> 1:00:31.760\n of the network that matters,\n\n1:00:31.760 --> 1:00:34.080\n not that you have 20 kinds of neurons.\n\n1:00:34.080 --> 1:00:37.040\n I think our brain is such a complicated mess\n\n1:00:37.040 --> 1:00:41.720\n because it wasn't evolved just to be intelligent,\n\n1:00:41.720 --> 1:00:45.840\n it was involved to also be self assembling\n\n1:00:47.000 --> 1:00:48.760\n and self repairing, right?\n\n1:00:48.760 --> 1:00:51.920\n And evolutionarily attainable.\n\n1:00:51.920 --> 1:00:53.560\n And so on and so on.\n\n1:00:53.560 --> 1:00:54.720\n So I think it's pretty,\n\n1:00:54.720 --> 1:00:57.040\n my hunch is that we're going to understand\n\n1:00:57.040 --> 1:00:59.520\n how to build AGI before we fully understand\n\n1:00:59.520 --> 1:01:02.600\n how our brains work, just like we understood\n\n1:01:02.600 --> 1:01:05.560\n how to build flying machines long before\n\n1:01:05.560 --> 1:01:07.800\n we were able to build a mechanical bird.\n\n1:01:07.800 --> 1:01:08.640\n Yeah, that's right.\n\n1:01:08.640 --> 1:01:13.280\n You've given the example exactly of mechanical birds\n\n1:01:13.280 --> 1:01:15.680\n and airplanes and airplanes do a pretty good job\n\n1:01:15.680 --> 1:01:18.560\n of flying without really mimicking bird flight.\n\n1:01:18.560 --> 1:01:20.920\n And even now after 100 years later,\n\n1:01:20.920 --> 1:01:23.880\n did you see the Ted talk with this German mechanical bird?\n\n1:01:23.880 --> 1:01:25.040\n I heard you mention it.\n\n1:01:25.040 --> 1:01:26.520\n Check it out, it's amazing.\n\n1:01:26.520 --> 1:01:27.760\n But even after that, right,\n\n1:01:27.760 --> 1:01:29.360\n we still don't fly in mechanical birds\n\n1:01:29.360 --> 1:01:32.720\n because it turned out the way we came up with was simpler\n\n1:01:32.720 --> 1:01:33.840\n and it's better for our purposes.\n\n1:01:33.840 --> 1:01:35.280\n And I think it might be the same there.\n\n1:01:35.280 --> 1:01:36.280\n That's one lesson.\n\n1:01:37.520 --> 1:01:42.520\n And another lesson, it's more what our paper was about.\n\n1:01:42.640 --> 1:01:45.800\n First, as a physicist thought it was fascinating\n\n1:01:45.800 --> 1:01:48.240\n how there's a very close mathematical relationship\n\n1:01:48.240 --> 1:01:50.800\n actually between our artificial neural networks\n\n1:01:50.800 --> 1:01:54.560\n and a lot of things that we've studied for in physics\n\n1:01:54.560 --> 1:01:57.520\n go by nerdy names like the renormalization group equation\n\n1:01:57.520 --> 1:01:59.800\n and Hamiltonians and yada, yada, yada.\n\n1:01:59.800 --> 1:02:04.360\n And when you look a little more closely at this,\n\n1:02:05.720 --> 1:02:06.560\n you have,\n\n1:02:10.320 --> 1:02:12.360\n at first I was like, well, there's something crazy here\n\n1:02:12.360 --> 1:02:13.520\n that doesn't make sense.\n\n1:02:13.520 --> 1:02:18.520\n Because we know that if you even want to build\n\n1:02:19.200 --> 1:02:22.560\n a super simple neural network to tell apart cat pictures\n\n1:02:22.560 --> 1:02:23.400\n and dog pictures, right,\n\n1:02:23.400 --> 1:02:25.400\n that you can do that very, very well now.\n\n1:02:25.400 --> 1:02:27.520\n But if you think about it a little bit,\n\n1:02:27.520 --> 1:02:29.080\n you convince yourself it must be impossible\n\n1:02:29.080 --> 1:02:31.920\n because if I have one megapixel,\n\n1:02:31.920 --> 1:02:34.160\n even if each pixel is just black or white,\n\n1:02:34.160 --> 1:02:36.960\n there's two to the power of 1 million possible images,\n\n1:02:36.960 --> 1:02:38.960\n which is way more than there are atoms in our universe,\n\n1:02:38.960 --> 1:02:41.000\n right, so in order to,\n\n1:02:42.040 --> 1:02:43.200\n and then for each one of those,\n\n1:02:43.200 --> 1:02:44.640\n I have to assign a number,\n\n1:02:44.640 --> 1:02:47.080\n which is the probability that it's a dog.\n\n1:02:47.080 --> 1:02:49.440\n So an arbitrary function of images\n\n1:02:49.440 --> 1:02:54.440\n is a list of more numbers than there are atoms in our universe.\n\n1:02:54.440 --> 1:02:57.360\n So clearly I can't store that under the hood of my GPU\n\n1:02:57.360 --> 1:03:00.640\n or my computer, yet somehow it works.\n\n1:03:00.640 --> 1:03:01.480\n So what does that mean?\n\n1:03:01.480 --> 1:03:04.960\n Well, it means that out of all of the problems\n\n1:03:04.960 --> 1:03:08.200\n that you could try to solve with a neural network,\n\n1:03:10.120 --> 1:03:12.880\n almost all of them are impossible to solve\n\n1:03:12.880 --> 1:03:14.560\n with a reasonably sized one.\n\n1:03:15.480 --> 1:03:17.440\n But then what we showed in our paper\n\n1:03:17.440 --> 1:03:22.360\n was that the fraction, the kind of problems,\n\n1:03:22.360 --> 1:03:23.800\n the fraction of all the problems\n\n1:03:23.800 --> 1:03:26.520\n that you could possibly pose,\n\n1:03:26.520 --> 1:03:29.480\n that we actually care about given the laws of physics\n\n1:03:29.480 --> 1:03:32.480\n is also an infinite testimony, tiny little part.\n\n1:03:32.480 --> 1:03:35.440\n And amazingly, they're basically the same part.\n\n1:03:35.440 --> 1:03:37.560\n Yeah, it's almost like our world was created for,\n\n1:03:37.560 --> 1:03:39.000\n I mean, they kind of come together.\n\n1:03:39.000 --> 1:03:42.800\n Yeah, well, you could say maybe where the world was created\n\n1:03:42.800 --> 1:03:44.960\n for us, but I have a more modest interpretation,\n\n1:03:44.960 --> 1:03:46.680\n which is that the world was created for us,\n\n1:03:46.680 --> 1:03:48.040\n but I have a more modest interpretation,\n\n1:03:48.040 --> 1:03:50.360\n which is that instead evolution endowed us\n\n1:03:50.360 --> 1:03:53.120\n with neural networks precisely for that reason.\n\n1:03:53.120 --> 1:03:54.640\n Because this particular architecture,\n\n1:03:54.640 --> 1:03:56.040\n as opposed to the one in your laptop,\n\n1:03:56.040 --> 1:04:01.040\n is very, very well adapted to solving the kind of problems\n\n1:04:02.480 --> 1:04:05.560\n that nature kept presenting our ancestors with.\n\n1:04:05.560 --> 1:04:08.120\n So it makes sense that why do we have a brain\n\n1:04:08.120 --> 1:04:09.280\n in the first place?\n\n1:04:09.280 --> 1:04:11.880\n It's to be able to make predictions about the future\n\n1:04:11.880 --> 1:04:12.880\n and so on.\n\n1:04:12.880 --> 1:04:16.440\n So if we had a sucky system, which could never solve it,\n\n1:04:16.440 --> 1:04:18.280\n we wouldn't have a world.\n\n1:04:18.280 --> 1:04:23.280\n So this is, I think, a very beautiful fact.\n\n1:04:23.680 --> 1:04:24.520\n Yeah.\n\n1:04:24.520 --> 1:04:29.000\n We also realize that there's been earlier work\n\n1:04:29.000 --> 1:04:32.040\n on why deeper networks are good,\n\n1:04:32.040 --> 1:04:34.680\n but we were able to show an additional cool fact there,\n\n1:04:34.680 --> 1:04:38.360\n which is that even incredibly simple problems,\n\n1:04:38.360 --> 1:04:41.080\n like suppose I give you a thousand numbers\n\n1:04:41.080 --> 1:04:42.720\n and ask you to multiply them together,\n\n1:04:42.720 --> 1:04:46.680\n and you can write a few lines of code, boom, done, trivial.\n\n1:04:46.680 --> 1:04:49.520\n If you just try to do that with a neural network\n\n1:04:49.520 --> 1:04:52.440\n that has only one single hidden layer in it,\n\n1:04:52.440 --> 1:04:53.400\n you can do it,\n\n1:04:54.320 --> 1:04:57.360\n but you're going to need two to the power of a thousand\n\n1:04:57.360 --> 1:05:00.920\n neurons to multiply a thousand numbers,\n\n1:05:00.920 --> 1:05:02.520\n which is, again, more neurons than there are atoms\n\n1:05:02.520 --> 1:05:03.360\n in our universe.\n\n1:05:04.600 --> 1:05:05.480\n That's fascinating.\n\n1:05:05.480 --> 1:05:09.960\n But if you allow yourself to make it a deep network\n\n1:05:09.960 --> 1:05:13.240\n with many layers, you only need 4,000 neurons.\n\n1:05:13.240 --> 1:05:14.520\n It's perfectly feasible.\n\n1:05:16.400 --> 1:05:17.960\n That's really interesting.\n\n1:05:17.960 --> 1:05:18.800\n Yeah.\n\n1:05:18.800 --> 1:05:21.040\n So on another architecture type,\n\n1:05:21.040 --> 1:05:22.720\n I mean, you mentioned Schrodinger's equation,\n\n1:05:22.720 --> 1:05:26.360\n and what are your thoughts about quantum computing\n\n1:05:27.240 --> 1:05:32.240\n and the role of this kind of computational unit\n\n1:05:32.400 --> 1:05:34.880\n in creating an intelligence system?\n\n1:05:34.880 --> 1:05:39.520\n In some Hollywood movies that I will not mention by name\n\n1:05:39.520 --> 1:05:41.040\n because I don't want to spoil them.\n\n1:05:41.040 --> 1:05:44.240\n The way they get AGI is building a quantum computer.\n\n1:05:45.480 --> 1:05:47.600\n Because the word quantum sounds cool and so on.\n\n1:05:47.600 --> 1:05:48.440\n That's right.\n\n1:05:50.040 --> 1:05:52.880\n First of all, I think we don't need quantum computers\n\n1:05:52.880 --> 1:05:54.920\n to build AGI.\n\n1:05:54.920 --> 1:05:59.240\n I suspect your brain is not a quantum computer\n\n1:05:59.240 --> 1:06:00.640\n in any profound sense.\n\n1:06:01.600 --> 1:06:03.200\n So you don't even wrote a paper about that\n\n1:06:03.200 --> 1:06:04.560\n a lot many years ago.\n\n1:06:04.560 --> 1:06:08.120\n I calculated the so called decoherence time,\n\n1:06:08.120 --> 1:06:10.320\n how long it takes until the quantum computerness\n\n1:06:10.320 --> 1:06:13.400\n of what your neurons are doing gets erased\n\n1:06:15.320 --> 1:06:17.960\n by just random noise from the environment.\n\n1:06:17.960 --> 1:06:21.320\n And it's about 10 to the minus 21 seconds.\n\n1:06:21.320 --> 1:06:24.600\n So as cool as it would be to have a quantum computer\n\n1:06:24.600 --> 1:06:27.320\n in my head, I don't think that fast.\n\n1:06:27.320 --> 1:06:28.360\n On the other hand,\n\n1:06:28.360 --> 1:06:33.040\n there are very cool things you could do\n\n1:06:33.040 --> 1:06:34.200\n with quantum computers.\n\n1:06:35.240 --> 1:06:37.480\n Or I think we'll be able to do soon\n\n1:06:37.480 --> 1:06:39.360\n when we get bigger ones.\n\n1:06:39.360 --> 1:06:40.960\n That might actually help machine learning\n\n1:06:40.960 --> 1:06:43.160\n do even better than the brain.\n\n1:06:43.160 --> 1:06:45.640\n So for example,\n\n1:06:47.040 --> 1:06:50.760\n one, this is just a moonshot,\n\n1:06:50.760 --> 1:06:55.760\n but learning is very much same thing as search.\n\n1:07:01.800 --> 1:07:03.160\n If you're trying to train a neural network\n\n1:07:03.160 --> 1:07:06.240\n to get really learned to do something really well,\n\n1:07:06.240 --> 1:07:07.280\n you have some loss function,\n\n1:07:07.280 --> 1:07:10.360\n you have a bunch of knobs you can turn,\n\n1:07:10.360 --> 1:07:12.080\n represented by a bunch of numbers,\n\n1:07:12.080 --> 1:07:12.920\n and you're trying to tweak them\n\n1:07:12.920 --> 1:07:15.080\n so that it becomes as good as possible at this thing.\n\n1:07:15.080 --> 1:07:19.680\n So if you think of a landscape with some valley,\n\n1:07:20.720 --> 1:07:22.120\n where each dimension of the landscape\n\n1:07:22.120 --> 1:07:24.120\n corresponds to some number you can change,\n\n1:07:24.120 --> 1:07:25.640\n you're trying to find the minimum.\n\n1:07:25.640 --> 1:07:26.760\n And it's well known that\n\n1:07:26.760 --> 1:07:29.040\n if you have a very high dimensional landscape,\n\n1:07:29.040 --> 1:07:31.840\n complicated things, it's super hard to find the minimum.\n\n1:07:31.840 --> 1:07:35.840\n Quantum mechanics is amazingly good at this.\n\n1:07:35.840 --> 1:07:38.240\n Like if I want to know what's the lowest energy state\n\n1:07:38.240 --> 1:07:39.720\n this water can possibly have,\n\n1:07:41.720 --> 1:07:42.560\n incredibly hard to compute,\n\n1:07:42.560 --> 1:07:45.400\n but nature will happily figure this out for you\n\n1:07:45.400 --> 1:07:48.000\n if you just cool it down, make it very, very cold.\n\n1:07:49.800 --> 1:07:50.880\n If you put a ball somewhere,\n\n1:07:50.880 --> 1:07:52.240\n it'll roll down to its minimum.\n\n1:07:52.240 --> 1:07:54.280\n And this happens metaphorically\n\n1:07:54.280 --> 1:07:56.320\n at the energy landscape too.\n\n1:07:56.320 --> 1:07:59.280\n And quantum mechanics even uses some clever tricks,\n\n1:07:59.280 --> 1:08:02.520\n which today's machine learning systems don't.\n\n1:08:02.520 --> 1:08:04.160\n Like if you're trying to find the minimum\n\n1:08:04.160 --> 1:08:06.960\n and you get stuck in the little local minimum here,\n\n1:08:06.960 --> 1:08:08.760\n in quantum mechanics you can actually tunnel\n\n1:08:08.760 --> 1:08:11.840\n through the barrier and get unstuck again.\n\n1:08:13.480 --> 1:08:14.320\n That's really interesting.\n\n1:08:14.320 --> 1:08:16.120\n Yeah, so it may be, for example,\n\n1:08:16.120 --> 1:08:19.160\n that we'll one day use quantum computers\n\n1:08:19.160 --> 1:08:22.840\n that help train neural networks better.\n\n1:08:22.840 --> 1:08:23.680\n That's really interesting.\n\n1:08:23.680 --> 1:08:27.040\n Okay, so as a component of kind of the learning process,\n\n1:08:27.040 --> 1:08:27.880\n for example.\n\n1:08:27.880 --> 1:08:29.440\n Yeah.\n\n1:08:29.440 --> 1:08:33.080\n Let me ask sort of wrapping up here a little bit,\n\n1:08:33.080 --> 1:08:36.880\n let me return to the questions of our human nature\n\n1:08:36.880 --> 1:08:40.000\n and love, as I mentioned.\n\n1:08:40.000 --> 1:08:41.640\n So do you think,\n\n1:08:44.280 --> 1:08:46.000\n you mentioned sort of a helper robot,\n\n1:08:46.000 --> 1:08:48.640\n but you could think of also personal robots.\n\n1:08:48.640 --> 1:08:52.480\n Do you think the way we human beings fall in love\n\n1:08:52.480 --> 1:08:54.680\n and get connected to each other\n\n1:08:54.680 --> 1:08:58.040\n is possible to achieve in an AI system\n\n1:08:58.040 --> 1:09:00.360\n and human level AI intelligence system?\n\n1:09:00.360 --> 1:09:03.720\n Do you think we would ever see that kind of connection?\n\n1:09:03.720 --> 1:09:06.160\n Or, you know, in all this discussion\n\n1:09:06.160 --> 1:09:08.520\n about solving complex goals,\n\n1:09:08.520 --> 1:09:10.760\n is this kind of human social connection,\n\n1:09:10.760 --> 1:09:12.560\n do you think that's one of the goals\n\n1:09:12.560 --> 1:09:16.280\n on the peaks and valleys with the raising sea levels\n\n1:09:16.280 --> 1:09:17.360\n that we'll be able to achieve?\n\n1:09:17.360 --> 1:09:20.040\n Or do you think that's something that's ultimately,\n\n1:09:20.040 --> 1:09:21.760\n or at least in the short term,\n\n1:09:21.760 --> 1:09:23.640\n relative to the other goals is not achievable?\n\n1:09:23.640 --> 1:09:25.120\n I think it's all possible.\n\n1:09:25.120 --> 1:09:27.600\n And I mean, in recent,\n\n1:09:27.600 --> 1:09:30.840\n there's a very wide range of guesses, as you know,\n\n1:09:30.840 --> 1:09:33.720\n among AI researchers, when we're going to get AGI.\n\n1:09:35.120 --> 1:09:37.640\n Some people, you know, like our friend Rodney Brooks\n\n1:09:37.640 --> 1:09:41.040\n says it's going to be hundreds of years at least.\n\n1:09:41.040 --> 1:09:42.200\n And then there are many others\n\n1:09:42.200 --> 1:09:44.040\n who think it's going to happen much sooner.\n\n1:09:44.040 --> 1:09:45.520\n And recent polls,\n\n1:09:46.840 --> 1:09:48.640\n maybe half or so of AI researchers\n\n1:09:48.640 --> 1:09:50.920\n think we're going to get AGI within decades.\n\n1:09:50.920 --> 1:09:52.720\n So if that happens, of course,\n\n1:09:52.720 --> 1:09:55.040\n then I think these things are all possible.\n\n1:09:55.040 --> 1:09:56.840\n But in terms of whether it will happen,\n\n1:09:56.840 --> 1:10:00.600\n I think we shouldn't spend so much time asking\n\n1:10:00.600 --> 1:10:03.240\n what do we think will happen in the future?\n\n1:10:03.240 --> 1:10:05.160\n As if we are just some sort of pathetic,\n\n1:10:05.160 --> 1:10:07.040\n your passive bystanders, you know,\n\n1:10:07.040 --> 1:10:09.280\n waiting for the future to happen to us.\n\n1:10:09.280 --> 1:10:11.640\n Hey, we're the ones creating this future, right?\n\n1:10:11.640 --> 1:10:15.520\n So we should be proactive about it\n\n1:10:15.520 --> 1:10:16.920\n and ask ourselves what sort of future\n\n1:10:16.920 --> 1:10:18.240\n we would like to have happen.\n\n1:10:18.240 --> 1:10:19.920\n We're going to make it like that.\n\n1:10:19.920 --> 1:10:22.720\n Well, what I prefer is just some sort of incredibly boring,\n\n1:10:22.720 --> 1:10:24.320\n zombie like future where there's all these\n\n1:10:24.320 --> 1:10:26.040\n mechanical things happening and there's no passion,\n\n1:10:26.040 --> 1:10:28.040\n no emotion, no experience, maybe even.\n\n1:10:29.600 --> 1:10:32.040\n No, I would of course, much rather prefer it\n\n1:10:32.040 --> 1:10:35.240\n if all the things that we find that we value the most\n\n1:10:36.240 --> 1:10:40.680\n about humanity are our subjective experience,\n\n1:10:40.680 --> 1:10:43.000\n passion, inspiration, love, you know.\n\n1:10:43.000 --> 1:10:48.000\n If we can create a future where those things do happen,\n\n1:10:48.000 --> 1:10:50.840\n where those things do exist, you know,\n\n1:10:50.840 --> 1:10:54.560\n I think ultimately it's not our universe\n\n1:10:54.560 --> 1:10:57.960\n giving meaning to us, it's us giving meaning to our universe.\n\n1:10:57.960 --> 1:11:01.840\n And if we build more advanced intelligence,\n\n1:11:01.840 --> 1:11:03.680\n let's make sure we build it in such a way\n\n1:11:03.680 --> 1:11:08.680\n that meaning is part of it.\n\n1:11:09.120 --> 1:11:11.400\n A lot of people that seriously study this problem\n\n1:11:11.400 --> 1:11:13.600\n and think of it from different angles\n\n1:11:13.600 --> 1:11:16.880\n have trouble in the majority of cases,\n\n1:11:16.880 --> 1:11:19.160\n if they think through that happen,\n\n1:11:19.160 --> 1:11:22.520\n are the ones that are not beneficial to humanity.\n\n1:11:22.520 --> 1:11:25.560\n And so, yeah, so what are your thoughts?\n\n1:11:25.560 --> 1:11:29.400\n What's should people, you know,\n\n1:11:29.400 --> 1:11:32.040\n I really don't like people to be terrified.\n\n1:11:33.440 --> 1:11:35.040\n What's a way for people to think about it\n\n1:11:35.040 --> 1:11:39.600\n in a way we can solve it and we can make it better?\n\n1:11:39.600 --> 1:11:42.960\n No, I don't think panicking is going to help in any way.\n\n1:11:42.960 --> 1:11:44.840\n It's not going to increase chances\n\n1:11:44.840 --> 1:11:45.880\n of things going well either.\n\n1:11:45.880 --> 1:11:48.400\n Even if you are in a situation where there is a real threat,\n\n1:11:48.400 --> 1:11:51.080\n does it help if everybody just freaks out?\n\n1:11:51.080 --> 1:11:52.680\n No, of course, of course not.\n\n1:11:53.640 --> 1:11:56.600\n I think, yeah, there are of course ways\n\n1:11:56.600 --> 1:11:58.440\n in which things can go horribly wrong.\n\n1:11:59.560 --> 1:12:03.680\n First of all, it's important when we think about this thing,\n\n1:12:03.680 --> 1:12:05.280\n about the problems and risks,\n\n1:12:05.280 --> 1:12:07.160\n to also remember how huge the upsides can be\n\n1:12:07.160 --> 1:12:08.440\n if we get it right, right?\n\n1:12:08.440 --> 1:12:12.360\n Everything we love about society and civilization\n\n1:12:12.360 --> 1:12:13.400\n is a product of intelligence.\n\n1:12:13.400 --> 1:12:15.320\n So if we can amplify our intelligence\n\n1:12:15.320 --> 1:12:18.760\n with machine intelligence and not anymore lose our loved one\n\n1:12:18.760 --> 1:12:21.080\n to what we're told is an incurable disease\n\n1:12:21.080 --> 1:12:24.800\n and things like this, of course, we should aspire to that.\n\n1:12:24.800 --> 1:12:26.680\n So that can be a motivator, I think,\n\n1:12:26.680 --> 1:12:29.120\n reminding ourselves that the reason we try to solve problems\n\n1:12:29.120 --> 1:12:33.520\n is not just because we're trying to avoid gloom,\n\n1:12:33.520 --> 1:12:35.760\n but because we're trying to do something great.\n\n1:12:35.760 --> 1:12:37.680\n But then in terms of the risks,\n\n1:12:37.680 --> 1:12:42.680\n I think the really important question is to ask,\n\n1:12:42.680 --> 1:12:45.480\n what can we do today that will actually help\n\n1:12:45.480 --> 1:12:47.320\n make the outcome good, right?\n\n1:12:47.320 --> 1:12:49.880\n And dismissing the risk is not one of them.\n\n1:12:51.240 --> 1:12:54.800\n I find it quite funny often when I'm in discussion panels\n\n1:12:54.800 --> 1:12:55.960\n about these things,\n\n1:12:55.960 --> 1:13:00.960\n how the people who work for companies,\n\n1:13:01.200 --> 1:13:03.120\n always be like, oh, nothing to worry about,\n\n1:13:03.120 --> 1:13:04.760\n nothing to worry about, nothing to worry about.\n\n1:13:04.760 --> 1:13:09.600\n And it's only academics sometimes express concerns.\n\n1:13:09.600 --> 1:13:11.880\n That's not surprising at all if you think about it.\n\n1:13:11.880 --> 1:13:12.880\n Right.\n\n1:13:12.880 --> 1:13:15.200\n Upton Sinclair quipped, right,\n\n1:13:15.200 --> 1:13:18.040\n that it's hard to make a man believe in something\n\n1:13:18.040 --> 1:13:20.120\n when his income depends on not believing in it.\n\n1:13:20.120 --> 1:13:24.080\n And frankly, we know a lot of these people in companies\n\n1:13:24.080 --> 1:13:26.240\n that they're just as concerned as anyone else.\n\n1:13:26.240 --> 1:13:28.480\n But if you're the CEO of a company,\n\n1:13:28.480 --> 1:13:30.280\n that's not something you want to go on record saying\n\n1:13:30.280 --> 1:13:33.440\n when you have silly journalists who are gonna put a picture\n\n1:13:33.440 --> 1:13:35.720\n of a Terminator robot when they quote you.\n\n1:13:35.720 --> 1:13:39.040\n So the issues are real.\n\n1:13:39.040 --> 1:13:41.920\n And the way I think about what the issue is,\n\n1:13:41.920 --> 1:13:46.920\n is basically the real choice we have is,\n\n1:13:48.040 --> 1:13:50.840\n first of all, are we gonna just dismiss the risks\n\n1:13:50.840 --> 1:13:54.480\n and say, well, let's just go ahead and build machines\n\n1:13:54.480 --> 1:13:57.560\n that can do everything we can do better and cheaper.\n\n1:13:57.560 --> 1:14:00.200\n Let's just make ourselves obsolete as fast as possible.\n\n1:14:00.200 --> 1:14:01.720\n What could possibly go wrong?\n\n1:14:01.720 --> 1:14:03.440\n That's one attitude.\n\n1:14:03.440 --> 1:14:05.440\n The opposite attitude, I think, is to say,\n\n1:14:06.400 --> 1:14:08.800\n here's this incredible potential,\n\n1:14:08.800 --> 1:14:11.960\n let's think about what kind of future\n\n1:14:11.960 --> 1:14:14.640\n we're really, really excited about.\n\n1:14:14.640 --> 1:14:18.480\n What are the shared goals that we can really aspire towards?\n\n1:14:18.480 --> 1:14:19.960\n And then let's think really hard\n\n1:14:19.960 --> 1:14:22.000\n about how we can actually get there.\n\n1:14:22.000 --> 1:14:24.160\n So start with, don't start thinking about the risks,\n\n1:14:24.160 --> 1:14:26.720\n start thinking about the goals.\n\n1:14:26.720 --> 1:14:28.200\n And then when you do that,\n\n1:14:28.200 --> 1:14:30.480\n then you can think about the obstacles you want to avoid.\n\n1:14:30.480 --> 1:14:32.840\n I often get students coming in right here into my office\n\n1:14:32.840 --> 1:14:34.120\n for career advice.\n\n1:14:34.120 --> 1:14:35.560\n I always ask them this very question,\n\n1:14:35.560 --> 1:14:37.920\n where do you want to be in the future?\n\n1:14:37.920 --> 1:14:40.640\n If all she can say is, oh, maybe I'll have cancer,\n\n1:14:40.640 --> 1:14:42.480\n maybe I'll get run over by a truck.\n\n1:14:42.480 --> 1:14:44.280\n Yeah, focus on the obstacles instead of the goals.\n\n1:14:44.280 --> 1:14:46.880\n She's just going to end up a hypochondriac paranoid.\n\n1:14:47.920 --> 1:14:49.920\n Whereas if she comes in and fire in her eyes\n\n1:14:49.920 --> 1:14:51.840\n and is like, I want to be there.\n\n1:14:51.840 --> 1:14:53.960\n And then we can talk about the obstacles\n\n1:14:53.960 --> 1:14:55.760\n and see how we can circumvent them.\n\n1:14:55.760 --> 1:14:58.880\n That's, I think, a much, much healthier attitude.\n\n1:14:58.880 --> 1:15:03.880\n And I feel it's very challenging to come up with a vision\n\n1:15:03.880 --> 1:15:08.120\n for the future, which we are unequivocally excited about.\n\n1:15:08.120 --> 1:15:10.320\n I'm not just talking now in the vague terms,\n\n1:15:10.320 --> 1:15:12.360\n like, yeah, let's cure cancer, fine.\n\n1:15:12.360 --> 1:15:14.720\n I'm talking about what kind of society\n\n1:15:14.720 --> 1:15:15.840\n do we want to create?\n\n1:15:15.840 --> 1:15:20.360\n What do we want it to mean to be human in the age of AI,\n\n1:15:20.360 --> 1:15:21.720\n in the age of AGI?\n\n1:15:22.840 --> 1:15:25.360\n So if we can have this conversation,\n\n1:15:25.360 --> 1:15:28.200\n broad, inclusive conversation,\n\n1:15:28.200 --> 1:15:31.400\n and gradually start converging towards some,\n\n1:15:31.400 --> 1:15:34.240\n some future that with some direction, at least,\n\n1:15:34.240 --> 1:15:35.400\n that we want to steer towards, right,\n\n1:15:35.400 --> 1:15:38.160\n then we'll be much more motivated\n\n1:15:38.160 --> 1:15:39.960\n to constructively take on the obstacles.\n\n1:15:39.960 --> 1:15:43.560\n And I think if I had, if I had to,\n\n1:15:43.560 --> 1:15:46.640\n if I try to wrap this up in a more succinct way,\n\n1:15:46.640 --> 1:15:51.480\n I think we can all agree already now\n\n1:15:51.480 --> 1:15:56.160\n that we should aspire to build AGI\n\n1:15:56.160 --> 1:16:05.160\n that doesn't overpower us, but that empowers us.\n\n1:16:05.160 --> 1:16:08.560\n And think of the many various ways that can do that,\n\n1:16:08.560 --> 1:16:11.000\n whether that's from my side of the world\n\n1:16:11.000 --> 1:16:12.720\n of autonomous vehicles.\n\n1:16:12.720 --> 1:16:14.720\n I'm personally actually from the camp\n\n1:16:14.720 --> 1:16:16.800\n that believes this human level intelligence\n\n1:16:16.800 --> 1:16:20.480\n is required to achieve something like vehicles\n\n1:16:20.480 --> 1:16:23.880\n that would actually be something we would enjoy using\n\n1:16:23.880 --> 1:16:25.120\n and being part of.\n\n1:16:25.120 --> 1:16:27.040\n So that's one example, and certainly there's a lot\n\n1:16:27.040 --> 1:16:30.920\n of other types of robots and medicine and so on.\n\n1:16:30.920 --> 1:16:33.880\n So focusing on those and then coming up with the obstacles,\n\n1:16:33.880 --> 1:16:35.920\n coming up with the ways that that can go wrong\n\n1:16:35.920 --> 1:16:38.160\n and solving those one at a time.\n\n1:16:38.160 --> 1:16:41.520\n And just because you can build an autonomous vehicle,\n\n1:16:41.520 --> 1:16:42.800\n even if you could build one\n\n1:16:42.800 --> 1:16:45.080\n that would drive just fine without you,\n\n1:16:45.080 --> 1:16:46.720\n maybe there are some things in life\n\n1:16:46.720 --> 1:16:48.400\n that we would actually want to do ourselves.\n\n1:16:48.400 --> 1:16:49.240\n That's right.\n\n1:16:49.240 --> 1:16:51.400\n Right, like, for example,\n\n1:16:51.400 --> 1:16:53.040\n if you think of our society as a whole,\n\n1:16:53.040 --> 1:16:56.320\n there are some things that we find very meaningful to do.\n\n1:16:57.200 --> 1:16:59.640\n And that doesn't mean we have to stop doing them\n\n1:16:59.640 --> 1:17:02.000\n just because machines can do them better.\n\n1:17:02.000 --> 1:17:04.080\n I'm not gonna stop playing tennis\n\n1:17:04.080 --> 1:17:07.360\n just the day someone builds a tennis robot and beat me.\n\n1:17:07.360 --> 1:17:09.600\n People are still playing chess and even go.\n\n1:17:09.600 --> 1:17:14.600\n Yeah, and in the very near term even,\n\n1:17:14.600 --> 1:17:18.880\n some people are advocating basic income, replace jobs.\n\n1:17:18.880 --> 1:17:20.840\n But if the government is gonna be willing\n\n1:17:20.840 --> 1:17:24.040\n to just hand out cash to people for doing nothing,\n\n1:17:24.040 --> 1:17:25.840\n then one should also seriously consider\n\n1:17:25.840 --> 1:17:27.640\n whether the government should also hire\n\n1:17:27.640 --> 1:17:29.480\n a lot more teachers and nurses\n\n1:17:29.480 --> 1:17:32.160\n and the kind of jobs which people often\n\n1:17:32.160 --> 1:17:34.440\n find great fulfillment in doing, right?\n\n1:17:34.440 --> 1:17:36.320\n We get very tired of hearing politicians saying,\n\n1:17:36.320 --> 1:17:39.320\n oh, we can't afford hiring more teachers,\n\n1:17:39.320 --> 1:17:41.480\n but we're gonna maybe have basic income.\n\n1:17:41.480 --> 1:17:44.000\n If we can have more serious research and thought\n\n1:17:44.000 --> 1:17:46.200\n into what gives meaning to our lives,\n\n1:17:46.200 --> 1:17:48.960\n the jobs give so much more than income, right?\n\n1:17:48.960 --> 1:17:50.520\n Mm hmm.\n\n1:17:50.520 --> 1:17:53.320\n And then think about in the future,\n\n1:17:53.320 --> 1:17:58.320\n what are the roles that we wanna have people\n\n1:18:00.000 --> 1:18:03.040\n continually feeling empowered by machines?\n\n1:18:03.040 --> 1:18:06.120\n And I think sort of, I come from Russia,\n\n1:18:06.120 --> 1:18:07.240\n from the Soviet Union.\n\n1:18:07.240 --> 1:18:10.160\n And I think for a lot of people in the 20th century,\n\n1:18:10.160 --> 1:18:14.080\n going to the moon, going to space was an inspiring thing.\n\n1:18:14.080 --> 1:18:18.080\n I feel like the universe of the mind,\n\n1:18:18.080 --> 1:18:20.880\n so AI, understanding, creating intelligence\n\n1:18:20.880 --> 1:18:23.240\n is that for the 21st century.\n\n1:18:23.240 --> 1:18:24.400\n So it's really surprising.\n\n1:18:24.400 --> 1:18:25.640\n And I've heard you mention this.\n\n1:18:25.640 --> 1:18:27.400\n It's really surprising to me,\n\n1:18:27.400 --> 1:18:29.240\n both on the research funding side,\n\n1:18:29.240 --> 1:18:31.760\n that it's not funded as greatly as it could be,\n\n1:18:31.760 --> 1:18:34.760\n but most importantly, on the politician side,\n\n1:18:34.760 --> 1:18:36.520\n that it's not part of the public discourse\n\n1:18:36.520 --> 1:18:40.800\n except in the killer bots terminator kind of view,\n\n1:18:40.800 --> 1:18:44.880\n that people are not yet, I think, perhaps excited\n\n1:18:44.880 --> 1:18:46.680\n by the possible positive future\n\n1:18:46.680 --> 1:18:48.120\n that we can build together.\n\n1:18:48.120 --> 1:18:51.520\n So we should be, because politicians usually just focus\n\n1:18:51.520 --> 1:18:53.320\n on the next election cycle, right?\n\n1:18:54.480 --> 1:18:57.160\n The single most important thing I feel we humans have learned\n\n1:18:57.160 --> 1:18:59.320\n in the entire history of science\n\n1:18:59.320 --> 1:19:02.040\n is they were the masters of underestimation.\n\n1:19:02.040 --> 1:19:07.040\n We underestimated the size of our cosmos again and again,\n\n1:19:08.480 --> 1:19:10.200\n realizing that everything we thought existed\n\n1:19:10.200 --> 1:19:12.240\n was just a small part of something grander, right?\n\n1:19:12.240 --> 1:19:16.640\n Planet, solar system, the galaxy, clusters of galaxies.\n\n1:19:16.640 --> 1:19:17.560\n The universe.\n\n1:19:18.440 --> 1:19:23.120\n And we now know that the future has just\n\n1:19:23.120 --> 1:19:25.160\n so much more potential\n\n1:19:25.160 --> 1:19:27.640\n than our ancestors could ever have dreamt of.\n\n1:19:27.640 --> 1:19:32.360\n This cosmos, imagine if all of Earth\n\n1:19:33.600 --> 1:19:35.440\n was completely devoid of life,\n\n1:19:36.640 --> 1:19:38.520\n except for Cambridge, Massachusetts.\n\n1:19:39.560 --> 1:19:42.680\n Wouldn't it be kind of lame if all we ever aspired to\n\n1:19:42.680 --> 1:19:45.560\n was to stay in Cambridge, Massachusetts forever\n\n1:19:45.560 --> 1:19:47.160\n and then go extinct in one week,\n\n1:19:47.160 --> 1:19:49.760\n even though Earth was gonna continue on for longer?\n\n1:19:49.760 --> 1:19:52.800\n That sort of attitude I think we have now\n\n1:19:54.200 --> 1:19:57.800\n on the cosmic scale, life can flourish on Earth,\n\n1:19:57.800 --> 1:20:00.840\n not for four years, but for billions of years.\n\n1:20:00.840 --> 1:20:02.920\n I can even tell you about how to move it out of harm's way\n\n1:20:02.920 --> 1:20:04.840\n when the sun gets too hot.\n\n1:20:04.840 --> 1:20:09.520\n And then we have so much more resources out here,\n\n1:20:09.520 --> 1:20:12.480\n which today, maybe there are a lot of other planets\n\n1:20:12.480 --> 1:20:14.960\n with bacteria or cow like life on them,\n\n1:20:14.960 --> 1:20:19.880\n but most of this, all this opportunity seems,\n\n1:20:19.880 --> 1:20:22.440\n as far as we can tell, to be largely dead,\n\n1:20:22.440 --> 1:20:23.560\n like the Sahara Desert.\n\n1:20:23.560 --> 1:20:28.480\n And yet we have the opportunity to help life flourish\n\n1:20:28.480 --> 1:20:30.280\n around this for billions of years.\n\n1:20:30.280 --> 1:20:32.680\n So let's quit squabbling about\n\n1:20:34.080 --> 1:20:36.480\n whether some little border should be drawn\n\n1:20:36.480 --> 1:20:38.440\n one mile to the left or right,\n\n1:20:38.440 --> 1:20:41.080\n and look up into the skies and realize,\n\n1:20:41.080 --> 1:20:44.040\n hey, we can do such incredible things.\n\n1:20:44.040 --> 1:20:46.640\n Yeah, and that's, I think, why it's really exciting\n\n1:20:46.640 --> 1:20:49.440\n that you and others are connected\n\n1:20:49.440 --> 1:20:51.880\n with some of the work Elon Musk is doing,\n\n1:20:51.880 --> 1:20:54.480\n because he's literally going out into that space,\n\n1:20:54.480 --> 1:20:57.000\n really exploring our universe, and it's wonderful.\n\n1:20:57.000 --> 1:21:02.000\n That is exactly why Elon Musk is so misunderstood, right?\n\n1:21:02.000 --> 1:21:05.000\n Misconstrued him as some kind of pessimistic doomsayer.\n\n1:21:05.000 --> 1:21:07.640\n The reason he cares so much about AI safety\n\n1:21:07.640 --> 1:21:12.080\n is because he more than almost anyone else appreciates\n\n1:21:12.080 --> 1:21:14.280\n these amazing opportunities that we'll squander\n\n1:21:14.280 --> 1:21:16.640\n if we wipe out here on Earth.\n\n1:21:16.640 --> 1:21:19.680\n We're not just going to wipe out the next generation,\n\n1:21:19.680 --> 1:21:23.320\n all generations, and this incredible opportunity\n\n1:21:23.320 --> 1:21:25.400\n that's out there, and that would really be a waste.\n\n1:21:25.400 --> 1:21:30.080\n And AI, for people who think that it would be better\n\n1:21:30.080 --> 1:21:33.600\n to do without technology, let me just mention that\n\n1:21:34.680 --> 1:21:36.320\n if we don't improve our technology,\n\n1:21:36.320 --> 1:21:39.320\n the question isn't whether humanity is going to go extinct.\n\n1:21:39.320 --> 1:21:41.160\n The question is just whether we're going to get taken out\n\n1:21:41.160 --> 1:21:44.800\n by the next big asteroid or the next super volcano\n\n1:21:44.800 --> 1:21:48.280\n or something else dumb that we could easily prevent\n\n1:21:48.280 --> 1:21:49.840\n with more tech, right?\n\n1:21:49.840 --> 1:21:53.160\n And if we want life to flourish throughout the cosmos,\n\n1:21:53.160 --> 1:21:54.760\n AI is the key to it.\n\n1:21:56.120 --> 1:21:59.840\n As I mentioned in a lot of detail in my book right there,\n\n1:21:59.840 --> 1:22:04.840\n even many of the most inspired sci fi writers,\n\n1:22:04.880 --> 1:22:08.120\n I feel have totally underestimated the opportunities\n\n1:22:08.120 --> 1:22:11.240\n for space travel, especially at the other galaxies,\n\n1:22:11.240 --> 1:22:15.360\n because they weren't thinking about the possibility of AGI,\n\n1:22:15.360 --> 1:22:17.520\n which just makes it so much easier.\n\n1:22:17.520 --> 1:22:18.440\n Right, yeah.\n\n1:22:18.440 --> 1:22:23.440\n So that goes to your view of AGI that enables our progress,\n\n1:22:24.080 --> 1:22:25.760\n that enables a better life.\n\n1:22:25.760 --> 1:22:28.320\n So that's a beautiful way to put it\n\n1:22:28.320 --> 1:22:29.960\n and then something to strive for.\n\n1:22:29.960 --> 1:22:31.440\n So Max, thank you so much.\n\n1:22:31.440 --> 1:22:32.560\n Thank you for your time today.\n\n1:22:32.560 --> 1:22:33.560\n It's been awesome.\n\n1:22:33.560 --> 1:22:34.400\n Thank you so much.\n\n1:22:34.400 --> 1:22:35.240\n Thanks.\n\n1:22:35.240 --> 1:22:40.240\n Have a great day.\n\n"
}