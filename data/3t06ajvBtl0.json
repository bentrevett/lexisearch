{
  "title": "Matt Botvinick: Neuroscience, Psychology, and AI at DeepMind | Lex Fridman Podcast #106",
  "id": "3t06ajvBtl0",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:03.440\n The following is a conversation with Matt Botmanek,\n\n00:03.440 --> 00:06.680\n Director of Neuroscience Research at DeepMind.\n\n00:06.680 --> 00:09.360\n He's a brilliant, cross disciplinary mind,\n\n00:09.360 --> 00:12.480\n navigating effortlessly between cognitive psychology,\n\n00:12.480 --> 00:16.760\n computational neuroscience, and artificial intelligence.\n\n00:16.760 --> 00:18.320\n Quick summary of the ads.\n\n00:18.320 --> 00:21.060\n Two sponsors, The Jordan Harbinger Show\n\n00:21.060 --> 00:23.880\n and Magic Spoon Cereal.\n\n00:23.880 --> 00:25.600\n Please consider supporting the podcast\n\n00:25.600 --> 00:29.320\n by going to jordanharbinger.com slash lex\n\n00:29.320 --> 00:33.800\n and also going to magicspoon.com slash lex\n\n00:33.800 --> 00:36.120\n and using code lex at checkout\n\n00:36.120 --> 00:39.080\n after you buy all of their cereal.\n\n00:39.080 --> 00:40.920\n Click the links, buy the stuff.\n\n00:40.920 --> 00:43.040\n It's the best way to support this podcast\n\n00:43.040 --> 00:44.740\n and the journey I'm on.\n\n00:44.740 --> 00:47.680\n If you enjoy this podcast, subscribe on YouTube,\n\n00:47.680 --> 00:49.920\n review it with five stars on Apple Podcast,\n\n00:49.920 --> 00:52.380\n follow on Spotify, support on Patreon,\n\n00:52.380 --> 00:55.600\n or connect with me on Twitter at lexfriedman,\n\n00:55.600 --> 00:58.920\n spelled surprisingly without the E,\n\n00:58.920 --> 01:01.180\n just F R I D M A N.\n\n01:02.080 --> 01:03.920\n As usual, I'll do a few minutes of ads now\n\n01:03.920 --> 01:05.160\n and never any ads in the middle\n\n01:05.160 --> 01:07.620\n that can break the flow of the conversation.\n\n01:07.620 --> 01:11.740\n This episode is supported by The Jordan Harbinger Show.\n\n01:11.740 --> 01:15.200\n Go to jordanharbinger.com slash lex.\n\n01:15.200 --> 01:16.900\n It's how he knows I sent you.\n\n01:16.900 --> 01:19.400\n On that page, subscribe to his podcast\n\n01:19.400 --> 01:24.320\n on Apple Podcast, Spotify, and you know where to look.\n\n01:24.320 --> 01:26.120\n I've been binging on his podcast.\n\n01:26.120 --> 01:28.400\n Jordan is a great interviewer\n\n01:28.400 --> 01:30.280\n and even a better human being.\n\n01:30.280 --> 01:32.760\n I recently listened to his conversation with Jack Barsky,\n\n01:32.760 --> 01:36.120\n former sleeper agent for the KGB in the 80s\n\n01:36.120 --> 01:38.880\n and author of Deep Undercover,\n\n01:38.880 --> 01:40.740\n which is a memoir that paints yet another\n\n01:40.740 --> 01:43.440\n interesting perspective on the Cold War era.\n\n01:43.440 --> 01:46.720\n I've been reading a lot about the Stalin\n\n01:46.720 --> 01:49.280\n and then Gorbachev and Putin eras of Russia,\n\n01:49.280 --> 01:50.800\n but this conversation made me realize\n\n01:50.800 --> 01:53.680\n that I need to do a deep dive into the Cold War era\n\n01:53.680 --> 01:57.120\n to get a complete picture of Russia's recent history.\n\n01:57.120 --> 02:01.160\n Again, go to jordanharbinger.com slash lex.\n\n02:01.160 --> 02:02.880\n Subscribe to his podcast.\n\n02:02.880 --> 02:04.440\n It's how he knows I sent you.\n\n02:04.440 --> 02:06.740\n It's awesome, you won't regret it.\n\n02:06.740 --> 02:10.320\n This episode is also supported by Magic Spoon,\n\n02:10.320 --> 02:15.320\n low carb, keto friendly, super amazingly delicious cereal.\n\n02:15.700 --> 02:18.300\n I've been on a keto or very low carb diet\n\n02:18.300 --> 02:19.480\n for a long time now.\n\n02:19.480 --> 02:21.300\n It helps with my mental performance.\n\n02:21.300 --> 02:22.840\n It helps with my physical performance,\n\n02:22.840 --> 02:26.520\n even during this crazy push up, pull up challenge I'm doing,\n\n02:26.520 --> 02:29.680\n including the running, it just feels great.\n\n02:29.680 --> 02:31.320\n I used to love cereal.\n\n02:31.320 --> 02:33.840\n Obviously, I can't have it now\n\n02:33.840 --> 02:36.820\n because most cereals have crazy amounts of sugar,\n\n02:36.820 --> 02:40.140\n which is terrible for you, so I quit it years ago.\n\n02:40.140 --> 02:44.260\n But Magic Spoon, amazingly, somehow,\n\n02:44.260 --> 02:45.920\n is a totally different thing.\n\n02:45.920 --> 02:48.340\n Zero sugar, 11 grams of protein,\n\n02:48.340 --> 02:50.920\n and only three net grams of carbs.\n\n02:50.920 --> 02:53.140\n It tastes delicious.\n\n02:53.140 --> 02:55.200\n It has a lot of flavors, two new ones,\n\n02:55.200 --> 02:56.760\n including peanut butter.\n\n02:56.760 --> 02:58.520\n But if you know what's good for you,\n\n02:58.520 --> 03:01.560\n you'll go with cocoa, my favorite flavor,\n\n03:01.560 --> 03:04.200\n and the flavor of champions.\n\n03:04.200 --> 03:07.880\n Click the magicspoon.com slash lex link in the description\n\n03:07.880 --> 03:11.040\n and use code lex at checkout for free shipping\n\n03:11.040 --> 03:13.100\n and to let them know I sent you.\n\n03:13.100 --> 03:16.480\n They have agreed to sponsor this podcast for a long time.\n\n03:16.480 --> 03:19.920\n They're an amazing sponsor and an even better cereal.\n\n03:19.920 --> 03:21.760\n I highly recommend it.\n\n03:21.760 --> 03:24.720\n It's delicious, it's good for you, you won't regret it.\n\n03:24.720 --> 03:28.460\n And now, here's my conversation with Matt Botpenik.\n\n03:29.600 --> 03:32.360\n How much of the human brain do you think we understand?\n\n03:33.400 --> 03:36.920\n I think we're at a weird moment\n\n03:36.920 --> 03:40.080\n in the history of neuroscience in the sense that\n\n03:45.200 --> 03:47.320\n I feel like we understand a lot about the brain\n\n03:47.320 --> 03:52.320\n at a very high level, but a very coarse level.\n\n03:52.600 --> 03:54.280\n When you say high level, what are you thinking?\n\n03:54.280 --> 03:55.440\n Are you thinking functional?\n\n03:55.440 --> 03:56.960\n Are you thinking structurally?\n\n03:56.960 --> 04:00.960\n So in other words, what is the brain for?\n\n04:00.960 --> 04:03.680\n What kinds of computation does the brain do?\n\n04:05.000 --> 04:10.000\n What kinds of behaviors would we have to explain\n\n04:12.320 --> 04:14.920\n if we were gonna look down at the mechanistic level?\n\n04:16.560 --> 04:18.440\n And at that level, I feel like we understand\n\n04:18.440 --> 04:19.680\n much, much more about the brain\n\n04:19.680 --> 04:22.060\n than we did when I was in high school.\n\n04:22.060 --> 04:25.240\n But it's almost like we're seeing it through a fog.\n\n04:25.240 --> 04:26.600\n It's only at a very coarse level.\n\n04:26.600 --> 04:30.200\n We don't really understand what the neuronal mechanisms are\n\n04:30.200 --> 04:32.500\n that underlie these computations.\n\n04:32.500 --> 04:34.600\n We've gotten better at saying,\n\n04:34.600 --> 04:36.720\n what are the functions that the brain is computing\n\n04:36.720 --> 04:38.400\n that we would have to understand\n\n04:38.400 --> 04:40.200\n if we were gonna get down to the neuronal level?\n\n04:40.200 --> 04:42.120\n And at the other end of the spectrum,\n\n04:45.500 --> 04:49.600\n in the last few years, incredible progress has been made\n\n04:49.600 --> 04:54.600\n in terms of technologies that allow us to see,\n\n04:54.880 --> 04:57.220\n actually literally see, in some cases,\n\n04:57.220 --> 05:01.040\n what's going on at the single unit level,\n\n05:01.040 --> 05:02.640\n even the dendritic level.\n\n05:02.640 --> 05:05.800\n And then there's this yawning gap in between.\n\n05:05.800 --> 05:06.640\n Well, that's interesting.\n\n05:06.640 --> 05:07.460\n So at the high level,\n\n05:07.460 --> 05:09.600\n so that's almost a cognitive science level.\n\n05:09.600 --> 05:11.900\n And then at the neuronal level,\n\n05:11.900 --> 05:14.600\n that's neurobiology and neuroscience,\n\n05:14.600 --> 05:16.040\n just studying single neurons,\n\n05:16.040 --> 05:19.800\n the synaptic connections and all the dopamine,\n\n05:19.800 --> 05:21.560\n all the kind of neurotransmitters.\n\n05:21.560 --> 05:23.360\n One blanket statement I should probably make\n\n05:23.360 --> 05:27.760\n is that as I've gotten older,\n\n05:27.760 --> 05:30.200\n I have become more and more reluctant\n\n05:30.200 --> 05:33.400\n to make a distinction between psychology and neuroscience.\n\n05:33.400 --> 05:37.240\n To me, the point of neuroscience\n\n05:37.240 --> 05:41.780\n is to study what the brain is for.\n\n05:41.780 --> 05:44.360\n If you're a nephrologist\n\n05:44.360 --> 05:46.560\n and you wanna learn about the kidney,\n\n05:46.560 --> 05:50.000\n you start by saying, what is this thing for?\n\n05:50.000 --> 05:55.000\n Well, it seems to be for taking blood on one side\n\n05:55.800 --> 06:00.260\n that has metabolites in it that shouldn't be there,\n\n06:01.120 --> 06:03.320\n sucking them out of the blood\n\n06:03.320 --> 06:05.160\n while leaving the good stuff behind,\n\n06:05.160 --> 06:07.060\n and then excreting that in the form of urine.\n\n06:07.060 --> 06:08.400\n That's what the kidney is for.\n\n06:08.400 --> 06:10.240\n It's like obvious.\n\n06:10.240 --> 06:13.200\n So the rest of the work is deciding how it does that.\n\n06:13.200 --> 06:14.800\n And this, it seems to me,\n\n06:14.800 --> 06:17.080\n is the right approach to take to the brain.\n\n06:17.080 --> 06:19.120\n You say, well, what is the brain for?\n\n06:19.120 --> 06:22.760\n The brain, as far as I can tell, is for producing behavior.\n\n06:22.760 --> 06:27.020\n It's for going from perceptual inputs to behavioral outputs,\n\n06:27.980 --> 06:30.280\n and the behavioral outputs should be adaptive.\n\n06:31.420 --> 06:33.620\n So that's what psychology is about.\n\n06:33.620 --> 06:35.920\n It's about understanding the structure of that function.\n\n06:35.920 --> 06:38.920\n And then the rest of neuroscience is about figuring out\n\n06:38.920 --> 06:41.880\n how those operations are actually carried out\n\n06:41.880 --> 06:44.160\n at a mechanistic level.\n\n06:44.160 --> 06:47.960\n That's really interesting, but so unlike the kidney,\n\n06:47.960 --> 06:52.020\n the brain, the gap between the electrical signal\n\n06:52.020 --> 06:57.020\n and behavior, so you truly see neuroscience\n\n06:57.120 --> 07:01.220\n as the science that touches behavior,\n\n07:01.220 --> 07:03.260\n how the brain generates behavior,\n\n07:03.260 --> 07:07.400\n or how the brain converts raw visual information\n\n07:07.400 --> 07:08.960\n into understanding.\n\n07:08.960 --> 07:12.520\n Like, you basically see cognitive science,\n\n07:12.520 --> 07:15.860\n psychology, and neuroscience as all one science.\n\n07:15.860 --> 07:19.240\n Yeah, it's a personal statement.\n\n07:19.240 --> 07:22.920\n Is that a hopeful or a realistic statement?\n\n07:22.920 --> 07:26.880\n So certainly you will be correct in your feeling\n\n07:26.880 --> 07:29.240\n in some number of years, but that number of years\n\n07:29.240 --> 07:31.440\n could be 200, 300 years from now.\n\n07:31.440 --> 07:33.400\n Oh, well, there's a...\n\n07:33.400 --> 07:37.600\n Is that aspirational or is that pragmatic engineering\n\n07:37.600 --> 07:39.360\n feeling that you have?\n\n07:39.360 --> 07:44.360\n It's both in the sense that this is what I hope\n\n07:46.520 --> 07:51.520\n and expect will bear fruit over the coming decades,\n\n07:53.360 --> 07:57.560\n but it's also pragmatic in the sense that I'm not sure\n\n07:57.560 --> 08:02.560\n what we're doing in either psychology or neuroscience\n\n08:02.840 --> 08:04.920\n if that's not the framing.\n\n08:04.920 --> 08:09.760\n I don't know what it means to understand the brain\n\n08:09.760 --> 08:14.320\n if there's no, if part of the enterprise\n\n08:14.320 --> 08:18.520\n is not about understanding the behavior\n\n08:18.520 --> 08:20.020\n that's being produced.\n\n08:20.020 --> 08:23.040\n I mean, yeah, but I would compare it\n\n08:23.040 --> 08:25.880\n to maybe astronomers looking at the movement\n\n08:25.880 --> 08:30.120\n of the planets and the stars without any interest\n\n08:30.120 --> 08:32.360\n of the underlying physics, right?\n\n08:32.360 --> 08:35.560\n And I would argue that at least in the early days,\n\n08:35.560 --> 08:37.780\n there is some value to just tracing the movement\n\n08:37.780 --> 08:41.680\n of the planets and the stars without thinking\n\n08:41.680 --> 08:44.100\n about the physics too much because it's such a big leap\n\n08:44.100 --> 08:45.600\n to start thinking about the physics\n\n08:45.600 --> 08:48.640\n before you even understand even the basic structural\n\n08:48.640 --> 08:49.520\n elements of...\n\n08:49.520 --> 08:50.420\n Oh, I agree with that.\n\n08:50.420 --> 08:51.260\n I agree.\n\n08:51.260 --> 08:53.240\n But you're saying in the end, the goal should be\n\n08:53.240 --> 08:54.760\n to deeply understand.\n\n08:54.760 --> 08:57.300\n Well, right, and I think...\n\n08:57.300 --> 08:59.240\n So I thought about this a lot when I was in grad school\n\n08:59.240 --> 09:00.600\n because a lot of what I studied in grad school\n\n09:00.600 --> 09:05.600\n was psychology and I found myself a little bit confused\n\n09:06.120 --> 09:08.680\n about what it meant to...\n\n09:08.680 --> 09:11.500\n It seems like what we were talking about a lot of the time\n\n09:11.500 --> 09:14.800\n were virtual causal mechanisms.\n\n09:14.800 --> 09:18.500\n Like, oh, well, you know, attentional selection\n\n09:18.500 --> 09:22.200\n then selects some object in the environment\n\n09:22.200 --> 09:25.600\n and that is then passed on to the motor, you know,\n\n09:25.600 --> 09:27.800\n information about that is passed on to the motor system.\n\n09:27.800 --> 09:29.760\n But these are virtual mechanisms.\n\n09:29.760 --> 09:31.480\n These are, you know, they're metaphors.\n\n09:31.480 --> 09:36.480\n They're, you know, there's no reduction going on\n\n09:37.040 --> 09:40.200\n in that conversation to some physical mechanism that,\n\n09:40.200 --> 09:43.240\n you know, which is really what it would take\n\n09:43.240 --> 09:47.320\n to fully understand, you know, how behavior is rising.\n\n09:47.320 --> 09:50.780\n But the causal mechanisms are definitely neurons interacting.\n\n09:50.780 --> 09:53.360\n I'm willing to say that at this point in history.\n\n09:53.360 --> 09:56.240\n So in psychology, at least for me personally,\n\n09:56.240 --> 10:00.160\n there was this strange insecurity about trafficking\n\n10:00.160 --> 10:02.680\n in these metaphors, you know,\n\n10:02.680 --> 10:05.740\n which were supposed to explain the function of the mind.\n\n10:07.360 --> 10:09.400\n If you can't ground them in physical mechanisms,\n\n10:09.400 --> 10:14.400\n then what is the explanatory validity of these explanations?\n\n10:16.120 --> 10:21.120\n And I managed to soothe my own nerves\n\n10:21.120 --> 10:26.120\n by thinking about the history of genetics research.\n\n10:29.400 --> 10:32.460\n So I'm very far from being an expert\n\n10:32.460 --> 10:34.660\n on the history of this field.\n\n10:34.660 --> 10:38.160\n But I know enough to say that, you know,\n\n10:38.160 --> 10:42.800\n Mendelian genetics preceded, you know, Watson and Crick.\n\n10:42.800 --> 10:45.520\n And so there was a significant period of time\n\n10:45.520 --> 10:49.600\n during which people were, you know,\n\n10:49.600 --> 10:54.600\n productively investigating the structure of inheritance\n\n10:54.760 --> 10:56.880\n using what was essentially a metaphor,\n\n10:56.880 --> 10:58.600\n the notion of a gene, you know.\n\n10:58.600 --> 11:00.760\n Oh, genes do this and genes do that.\n\n11:00.760 --> 11:02.520\n But, you know, where are the genes?\n\n11:02.520 --> 11:06.080\n They're sort of an explanatory thing that we made up.\n\n11:06.080 --> 11:08.880\n And we ascribed to them these causal properties.\n\n11:08.880 --> 11:10.640\n Oh, there's a dominant, there's a recessive,\n\n11:10.640 --> 11:12.800\n and then they recombine it.\n\n11:12.800 --> 11:17.460\n And then later, there was a kind of blank there\n\n11:17.460 --> 11:21.620\n that was filled in with a physical mechanism.\n\n11:21.620 --> 11:22.880\n That connection was made.\n\n11:24.300 --> 11:26.800\n But it was worth having that metaphor\n\n11:26.800 --> 11:29.360\n because that gave us a good sense\n\n11:29.360 --> 11:34.280\n of what kind of causal mechanism we were looking for.\n\n11:34.280 --> 11:38.880\n And the fundamental metaphor of cognition, you said,\n\n11:38.880 --> 11:40.780\n is the interaction of neurons.\n\n11:40.780 --> 11:42.680\n Is that, what is the metaphor?\n\n11:42.680 --> 11:44.280\n No, no, the metaphor,\n\n11:44.280 --> 11:47.640\n the metaphors we use in cognitive psychology\n\n11:47.640 --> 11:52.640\n are things like attention, the way that memory works.\n\n11:56.040 --> 11:59.440\n I retrieve something from memory, right?\n\n11:59.440 --> 12:01.880\n A memory retrieval occurs.\n\n12:01.880 --> 12:02.860\n What is that?\n\n12:02.860 --> 12:06.620\n You know, that's not a physical mechanism\n\n12:06.620 --> 12:08.960\n that I can examine in its own right.\n\n12:08.960 --> 12:13.840\n But it's still worth having, that metaphorical level.\n\n12:13.840 --> 12:16.000\n Yeah, so yeah, I misunderstood actually.\n\n12:16.000 --> 12:17.640\n So the higher level of abstractions\n\n12:17.640 --> 12:19.640\n is the metaphor that's most useful.\n\n12:19.640 --> 12:20.480\n Yes.\n\n12:20.480 --> 12:24.420\n But what about, so how does that connect\n\n12:24.420 --> 12:29.420\n to the idea that that arises from interaction of neurons?\n\n12:33.000 --> 12:35.940\n Well, even, is the interaction of neurons\n\n12:35.940 --> 12:38.080\n also not a metaphor to you?\n\n12:38.080 --> 12:42.400\n Or is it literally, like that's no longer a metaphor.\n\n12:42.400 --> 12:46.160\n That's already the lowest level of abstractions\n\n12:46.160 --> 12:48.960\n that could actually be directly studied.\n\n12:50.280 --> 12:53.840\n Well, I'm hesitating because I think\n\n12:53.840 --> 12:57.000\n what I want to say could end up being controversial.\n\n12:57.960 --> 12:59.960\n So what I want to say is, yes,\n\n12:59.960 --> 13:03.040\n the interactions of neurons, that's not metaphorical.\n\n13:03.040 --> 13:04.680\n That's a physical fact.\n\n13:04.680 --> 13:08.500\n That's where the causal interactions actually occur.\n\n13:08.500 --> 13:09.880\n Now, I suppose you could say,\n\n13:09.880 --> 13:12.720\n well, even that is metaphorical relative\n\n13:12.720 --> 13:14.880\n to the quantum events that underlie.\n\n13:15.840 --> 13:17.320\n I don't want to go down that rabbit hole.\n\n13:17.320 --> 13:18.920\n It's always turtles on top of turtles.\n\n13:18.920 --> 13:21.200\n Yeah, there's turtles all the way down.\n\n13:21.200 --> 13:22.560\n There's a reduction that you can do.\n\n13:22.560 --> 13:24.640\n You can say these psychological phenomena\n\n13:25.720 --> 13:28.200\n can be explained through a very different\n\n13:28.200 --> 13:29.160\n kind of causal mechanism,\n\n13:29.160 --> 13:31.440\n which has to do with neurotransmitter release.\n\n13:31.440 --> 13:33.800\n And so what we're really trying to do\n\n13:33.800 --> 13:37.120\n in neuroscience writ large, as I say,\n\n13:37.120 --> 13:39.760\n which for me includes psychology,\n\n13:39.760 --> 13:44.400\n is to take these psychological phenomena\n\n13:44.400 --> 13:48.500\n and map them onto neural events.\n\n13:49.980 --> 13:54.980\n I think remaining forever at the level of description\n\n13:57.160 --> 14:00.520\n that is natural for psychology,\n\n14:00.520 --> 14:02.280\n for me personally, would be disappointing.\n\n14:02.280 --> 14:05.640\n I want to understand how mental activity\n\n14:05.640 --> 14:10.360\n arises from neural activity.\n\n14:10.360 --> 14:13.000\n But the converse is also true.\n\n14:13.000 --> 14:15.880\n Studying neural activity without any sense\n\n14:15.880 --> 14:18.520\n of what you're trying to explain,\n\n14:19.800 --> 14:24.800\n to me feels like at best groping around at random.\n\n14:27.280 --> 14:30.280\n Now, you've kind of talked about this bridging\n\n14:30.280 --> 14:32.880\n of the gap between psychology and neuroscience,\n\n14:32.880 --> 14:34.040\n but do you think it's possible,\n\n14:34.040 --> 14:38.280\n like my love is, like I fell in love with psychology\n\n14:38.280 --> 14:40.120\n and psychiatry in general with Freud\n\n14:40.120 --> 14:41.760\n and when I was really young,\n\n14:41.760 --> 14:43.540\n and I hoped to understand the mind.\n\n14:43.540 --> 14:45.240\n And for me, understanding the mind,\n\n14:45.240 --> 14:48.400\n at least at that young age before I discovered AI\n\n14:48.400 --> 14:52.840\n and even neuroscience was to, is psychology.\n\n14:52.840 --> 14:55.840\n And do you think it's possible to understand the mind\n\n14:55.840 --> 14:59.920\n without getting into all the messy details of neuroscience?\n\n14:59.920 --> 15:03.120\n Like you kind of mentioned to you it's appealing\n\n15:03.120 --> 15:06.040\n to try to understand the mechanisms at the lowest level,\n\n15:06.040 --> 15:07.560\n but do you think that's needed,\n\n15:07.560 --> 15:10.200\n that's required to understand how the mind works?\n\n15:11.480 --> 15:14.760\n That's an important part of the whole picture,\n\n15:14.760 --> 15:18.480\n but I would be the last person on earth\n\n15:18.480 --> 15:23.440\n to suggest that that reality\n\n15:23.440 --> 15:27.240\n renders psychology in its own right unproductive.\n\n15:29.440 --> 15:31.160\n I trained as a psychologist.\n\n15:31.160 --> 15:35.000\n I am fond of saying that I have learned much more\n\n15:35.000 --> 15:38.480\n from psychology than I have from neuroscience.\n\n15:38.480 --> 15:43.480\n To me, psychology is a hugely important discipline.\n\n15:43.740 --> 15:47.400\n And one thing that warms in my heart is that\n\n15:50.360 --> 15:54.080\n ways of investigating behavior\n\n15:54.080 --> 15:58.000\n that have been native to cognitive psychology\n\n15:58.000 --> 16:01.600\n since it's dawn in the 60s\n\n16:01.600 --> 16:03.960\n are starting to become,\n\n16:03.960 --> 16:07.680\n they're starting to become interesting to AI researchers\n\n16:07.680 --> 16:09.480\n for a variety of reasons.\n\n16:09.480 --> 16:11.680\n And that's been exciting for me to see.\n\n16:11.680 --> 16:14.920\n Can you maybe talk a little bit about what you see\n\n16:14.920 --> 16:19.320\n as beautiful aspects of psychology,\n\n16:19.320 --> 16:21.920\n maybe limiting aspects of psychology?\n\n16:21.920 --> 16:25.640\n I mean, maybe just start it off as a science, as a field.\n\n16:25.640 --> 16:29.760\n To me, it was when I understood what psychology is,\n\n16:29.760 --> 16:30.880\n analytical psychology,\n\n16:30.880 --> 16:32.760\n like the way it's actually carried out,\n\n16:32.760 --> 16:36.240\n it was really disappointing to see two aspects.\n\n16:36.240 --> 16:39.200\n One is how small the N is,\n\n16:39.200 --> 16:43.040\n how small the number of subject is in the studies.\n\n16:43.040 --> 16:45.320\n And two, it was disappointing to see\n\n16:45.320 --> 16:47.480\n how controlled the entire,\n\n16:47.480 --> 16:49.680\n how much it was in the lab.\n\n16:50.520 --> 16:52.680\n It wasn't studying humans in the wild.\n\n16:52.680 --> 16:55.000\n There was no mechanism for studying humans in the wild.\n\n16:55.000 --> 16:57.640\n So that's where I became a little bit disillusioned\n\n16:57.640 --> 16:59.480\n to psychology.\n\n16:59.480 --> 17:01.680\n And then the modern world of the internet\n\n17:01.680 --> 17:02.960\n is so exciting to me.\n\n17:02.960 --> 17:05.720\n The Twitter data or YouTube data,\n\n17:05.720 --> 17:08.280\n data of human behavior on the internet becomes exciting\n\n17:08.280 --> 17:11.920\n because the N grows and then in the wild grows.\n\n17:11.920 --> 17:13.880\n But that's just my narrow sense.\n\n17:13.880 --> 17:16.560\n Like, do you have a optimistic or pessimistic\n\n17:16.560 --> 17:18.160\n cynical view of psychology?\n\n17:18.160 --> 17:19.840\n How do you see the field broadly?\n\n17:21.120 --> 17:22.720\n When I was in graduate school,\n\n17:22.720 --> 17:27.720\n it was early enough that there was still a thrill\n\n17:27.800 --> 17:32.800\n in seeing that there were ways of doing,\n\n17:32.960 --> 17:35.640\n there were ways of doing experimental science\n\n17:36.560 --> 17:40.040\n that provided insight to the structure of the mind.\n\n17:40.040 --> 17:43.720\n One thing that impressed me most when I was at that stage\n\n17:43.720 --> 17:46.000\n in my education was neuropsychology,\n\n17:46.000 --> 17:51.000\n looking at, analyzing the behavior of populations\n\n17:51.000 --> 17:55.560\n who had brain damage of different kinds\n\n17:55.560 --> 18:00.560\n and trying to understand what the specific deficits were\n\n18:02.920 --> 18:06.760\n that arose from a lesion in a particular part of the brain.\n\n18:06.760 --> 18:08.960\n And the kind of experimentation that was done\n\n18:08.960 --> 18:13.520\n and that's still being done to get answers in that context\n\n18:13.520 --> 18:18.160\n was so creative and it was so deliberate.\n\n18:18.160 --> 18:21.360\n It was good science.\n\n18:21.360 --> 18:24.400\n An experiment answered one question but raised another\n\n18:24.400 --> 18:25.600\n and somebody would do an experiment\n\n18:25.600 --> 18:26.600\n that answered that question.\n\n18:26.600 --> 18:29.360\n And you really felt like you were narrowing in on\n\n18:29.360 --> 18:31.760\n some kind of approximate understanding\n\n18:31.760 --> 18:34.840\n of what this part of the brain was for.\n\n18:34.840 --> 18:36.880\n Do you have an example from memory\n\n18:36.880 --> 18:39.560\n of what kind of aspects of the mind\n\n18:39.560 --> 18:41.400\n could be studied in this kind of way?\n\n18:41.400 --> 18:42.240\n Oh, sure.\n\n18:42.240 --> 18:45.840\n I mean, the very detailed neuropsychological studies\n\n18:45.840 --> 18:49.720\n of language function,\n\n18:49.720 --> 18:52.040\n looking at production and reception\n\n18:52.040 --> 18:57.080\n and the relationship between visual function,\n\n18:57.080 --> 19:00.680\n reading and auditory and semantic.\n\n19:00.680 --> 19:03.920\n There were these, and still are, these beautiful models\n\n19:03.920 --> 19:05.560\n that came out of that kind of research\n\n19:05.560 --> 19:08.480\n that really made you feel like you understood something\n\n19:08.480 --> 19:10.320\n that you hadn't understood before\n\n19:10.320 --> 19:15.320\n about how language processing is organized in the brain.\n\n19:15.320 --> 19:17.280\n But having said all that,\n\n19:20.840 --> 19:25.400\n I think you are, I mean, I agree with you\n\n19:25.400 --> 19:30.400\n that the cost of doing highly controlled experiments\n\n19:30.960 --> 19:35.960\n is that you, by construction, miss out on the richness\n\n19:36.480 --> 19:39.160\n and complexity of the real world.\n\n19:39.160 --> 19:42.360\n One thing that, so I was drawn into science\n\n19:42.360 --> 19:44.960\n by what in those days was called connectionism,\n\n19:44.960 --> 19:49.120\n which is, of course, what we now call deep learning.\n\n19:49.120 --> 19:50.840\n And at that point in history,\n\n19:50.840 --> 19:54.200\n neural networks were primarily being used\n\n19:54.200 --> 19:56.440\n in order to model human cognition.\n\n19:56.440 --> 20:00.200\n They weren't yet really useful for industrial applications.\n\n20:00.200 --> 20:02.080\n So you always found neural networks\n\n20:02.080 --> 20:04.080\n in biological form beautiful.\n\n20:04.080 --> 20:07.160\n Oh, neural networks were very concretely the thing\n\n20:07.160 --> 20:09.160\n that drew me into science.\n\n20:09.160 --> 20:13.320\n I was handed, are you familiar with the PDP books\n\n20:13.320 --> 20:15.720\n from the 80s when I was in,\n\n20:15.720 --> 20:18.240\n I went to medical school before I went into science.\n\n20:18.240 --> 20:19.160\n And, yeah.\n\n20:19.160 --> 20:20.800\n Really, interesting.\n\n20:20.800 --> 20:21.960\n Wow.\n\n20:21.960 --> 20:23.920\n I also did a graduate degree in art history,\n\n20:23.920 --> 20:26.480\n so I'm kind of exploring.\n\n20:26.480 --> 20:28.560\n Well, art history, I understand.\n\n20:28.560 --> 20:31.280\n That's just a curious, creative mind.\n\n20:31.280 --> 20:33.960\n But medical school, with the dream of what,\n\n20:33.960 --> 20:36.560\n if we take that slight tangent?\n\n20:36.560 --> 20:39.120\n What, did you want to be a surgeon?\n\n20:39.120 --> 20:41.680\n I actually was quite interested in surgery.\n\n20:41.680 --> 20:44.200\n I was interested in surgery and psychiatry.\n\n20:44.200 --> 20:49.200\n And I thought, I must be the only person on the planet\n\n20:49.520 --> 20:52.680\n who was torn between those two fields.\n\n20:52.680 --> 20:56.840\n And I said exactly that to my advisor in medical school,\n\n20:56.840 --> 20:59.440\n who turned out, I found out later,\n\n20:59.440 --> 21:01.920\n to be a famous psychoanalyst.\n\n21:01.920 --> 21:05.160\n And he said to me, no, no, it's actually not so uncommon\n\n21:05.160 --> 21:07.520\n to be interested in surgery and psychiatry.\n\n21:07.520 --> 21:10.480\n And he conjectured that the reason\n\n21:10.480 --> 21:12.600\n that people develop these two interests\n\n21:12.600 --> 21:15.480\n is that both fields are about going beneath the surface\n\n21:16.360 --> 21:19.120\n and kind of getting into the kind of secret.\n\n21:19.120 --> 21:21.040\n I mean, maybe you understand this as someone\n\n21:21.040 --> 21:23.440\n who was interested in psychoanalysis.\n\n21:23.440 --> 21:26.200\n There's sort of a, there's a cliche phrase\n\n21:26.200 --> 21:28.400\n that people use now, like in NPR,\n\n21:28.400 --> 21:31.400\n the secret life of blankety blank, right?\n\n21:31.400 --> 21:33.560\n And that was part of the thrill of surgery,\n\n21:33.560 --> 21:38.120\n was seeing the secret activity\n\n21:38.120 --> 21:40.560\n that's inside everybody's abdomen and thorax.\n\n21:40.560 --> 21:43.880\n That's a very poetic way to connect it to disciplines\n\n21:43.880 --> 21:45.560\n that are very, practically speaking,\n\n21:45.560 --> 21:46.520\n different from each other.\n\n21:46.520 --> 21:48.480\n That's for sure, that's for sure, yes.\n\n21:48.480 --> 21:52.480\n So how did we get onto medical school?\n\n21:52.480 --> 21:53.720\n So I was in medical school\n\n21:53.720 --> 21:57.360\n and I was doing a psychiatry rotation\n\n21:57.360 --> 22:01.000\n and my kind of advisor in that rotation\n\n22:02.280 --> 22:04.720\n asked me what I was interested in.\n\n22:04.720 --> 22:07.800\n And I said, well, maybe psychiatry.\n\n22:07.800 --> 22:09.280\n He said, why?\n\n22:09.280 --> 22:11.120\n And I said, well, I've always been interested\n\n22:11.120 --> 22:13.080\n in how the brain works.\n\n22:13.080 --> 22:16.160\n I'm pretty sure that nobody's doing scientific research\n\n22:16.160 --> 22:19.160\n that addresses my interests,\n\n22:19.160 --> 22:21.880\n which are, I didn't have a word for it then,\n\n22:21.880 --> 22:25.200\n but I would have said about cognition.\n\n22:25.200 --> 22:27.680\n And he said, well, you know, I'm not sure that's true.\n\n22:27.680 --> 22:29.600\n You might be interested in these books.\n\n22:29.600 --> 22:32.440\n And he pulled down the PDB books from his shelf\n\n22:32.440 --> 22:33.960\n and they were still shrink wrapped.\n\n22:33.960 --> 22:36.920\n He hadn't read them, but he handed them to me.\n\n22:36.920 --> 22:38.680\n He said, you feel free to borrow these.\n\n22:38.680 --> 22:41.440\n And that was, you know, I went back to my dorm room\n\n22:41.440 --> 22:43.400\n and I just, you know, read them cover to cover.\n\n22:43.400 --> 22:44.960\n And what's PDB?\n\n22:44.960 --> 22:46.520\n Parallel distributed processing,\n\n22:46.520 --> 22:50.840\n which was one of the original names for deep learning.\n\n22:50.840 --> 22:55.000\n And so I apologize for the romanticized question,\n\n22:55.000 --> 22:58.360\n but what idea in the space of neuroscience\n\n22:58.360 --> 23:00.840\n and the space of the human brain is to you\n\n23:00.840 --> 23:03.880\n the most beautiful, mysterious, surprising?\n\n23:03.880 --> 23:07.160\n What had always fascinated me,\n\n23:08.480 --> 23:11.440\n even when I was a pretty young kid, I think,\n\n23:12.320 --> 23:17.320\n was the paradox that lies in the fact\n\n23:21.360 --> 23:25.640\n that the brain is so mysterious\n\n23:25.640 --> 23:28.200\n and seems so distant.\n\n23:30.640 --> 23:32.520\n But at the same time,\n\n23:32.520 --> 23:37.360\n it's responsible for the full transparency\n\n23:37.360 --> 23:39.040\n of everyday life.\n\n23:39.040 --> 23:41.520\n The brain is literally what makes everything obvious\n\n23:41.520 --> 23:43.080\n and familiar.\n\n23:43.080 --> 23:47.280\n And there's always one in the room with you.\n\n23:47.280 --> 23:48.120\n Yeah.\n\n23:48.120 --> 23:50.520\n I used to teach, when I taught at Princeton,\n\n23:50.520 --> 23:53.000\n I used to teach a cognitive neuroscience course.\n\n23:53.000 --> 23:56.720\n And the very last thing I would say to the students was,\n\n23:56.720 --> 24:00.160\n you know, people often,\n\n24:00.160 --> 24:04.200\n when people think of scientific inspiration,\n\n24:04.200 --> 24:08.120\n the metaphor is often, well, look to the stars.\n\n24:08.120 --> 24:12.360\n The stars will inspire you to wonder at the universe\n\n24:12.360 --> 24:15.800\n and think about your place in it and how things work.\n\n24:15.800 --> 24:18.360\n And I'm all for looking at the stars,\n\n24:18.360 --> 24:21.600\n but I've always been much more inspired.\n\n24:21.600 --> 24:25.360\n And my sense of wonder comes from the,\n\n24:25.360 --> 24:28.560\n not from the distant, mysterious stars,\n\n24:28.560 --> 24:33.560\n but from the extremely intimately close brain.\n\n24:34.440 --> 24:35.280\n Yeah.\n\n24:35.280 --> 24:38.680\n There's something just endlessly fascinating\n\n24:38.680 --> 24:40.000\n to me about that.\n\n24:40.000 --> 24:41.360\n The, like, just like you said,\n\n24:41.360 --> 24:45.500\n the one that's close and yet distant\n\n24:45.500 --> 24:48.000\n in terms of our understanding of it.\n\n24:48.000 --> 24:53.000\n Do you, are you also captivated by the fact\n\n24:53.640 --> 24:56.040\n that this very conversation is happening\n\n24:56.040 --> 24:57.560\n because two brains are communicating so that?\n\n24:57.560 --> 24:59.120\n Yes, exactly.\n\n24:59.120 --> 25:03.800\n The, I guess what I mean is the subjective nature\n\n25:03.800 --> 25:06.320\n of the experience, if it can take a small attention\n\n25:06.320 --> 25:10.240\n into the mystical of it, the consciousness,\n\n25:10.240 --> 25:13.320\n or when you were saying you're captivated\n\n25:13.320 --> 25:14.920\n by the idea of the brain,\n\n25:14.920 --> 25:16.320\n are you talking about specifically\n\n25:16.320 --> 25:18.200\n the mechanism of cognition?\n\n25:18.200 --> 25:23.080\n Or are you also just, like, at least for me,\n\n25:23.080 --> 25:26.600\n it's almost like paralyzing the beauty and the mystery\n\n25:26.600 --> 25:29.480\n of the fact that it creates the entirety of the experience,\n\n25:29.480 --> 25:32.880\n not just the reasoning capability, but the experience.\n\n25:32.880 --> 25:37.880\n Well, I definitely resonate with that latter thought.\n\n25:38.920 --> 25:43.920\n And I often find discussions of artificial intelligence\n\n25:45.280 --> 25:49.120\n to be disappointingly narrow.\n\n25:50.720 --> 25:55.720\n Speaking as someone who has always had an interest in art.\n\n25:55.720 --> 25:56.560\n Right.\n\n25:56.560 --> 25:57.400\n I was just gonna go there\n\n25:57.400 --> 26:00.200\n because it sounds like somebody who has an interest in art.\n\n26:00.200 --> 26:04.000\n Yeah, I mean, there are many layers\n\n26:04.000 --> 26:08.200\n to full bore human experience.\n\n26:08.200 --> 26:12.040\n And in some ways it's not enough to say,\n\n26:12.040 --> 26:15.020\n oh, well, don't worry, we're talking about cognition,\n\n26:15.020 --> 26:17.240\n but we'll add emotion, you know?\n\n26:17.240 --> 26:21.200\n There's an incredible scope\n\n26:21.200 --> 26:25.280\n to what humans go through in every moment.\n\n26:25.280 --> 26:30.280\n And yes, so that's part of what fascinates me,\n\n26:33.320 --> 26:37.320\n is that our brains are producing that.\n\n26:40.040 --> 26:43.040\n But at the same time, it's so mysterious to us.\n\n26:43.040 --> 26:43.880\n How?\n\n26:46.240 --> 26:49.120\n Our brains are literally in our heads\n\n26:49.120 --> 26:50.600\n producing this experience.\n\n26:50.600 --> 26:52.120\n Producing the experience.\n\n26:52.120 --> 26:55.100\n And yet it's so mysterious to us.\n\n26:55.100 --> 26:57.000\n And so, and the scientific challenge\n\n26:57.000 --> 27:00.880\n of getting at the actual explanation for that\n\n27:00.880 --> 27:03.360\n is so overwhelming.\n\n27:03.360 --> 27:05.600\n That's just, I don't know.\n\n27:05.600 --> 27:08.440\n Certain people have fixations on particular questions\n\n27:08.440 --> 27:11.680\n and that's always, that's just always been mine.\n\n27:11.680 --> 27:14.020\n Yeah, I would say the poetry of that is fascinating.\n\n27:14.020 --> 27:16.740\n And I'm really interested in natural language as well.\n\n27:16.740 --> 27:19.440\n And when you look at artificial intelligence community,\n\n27:19.440 --> 27:23.880\n it always saddens me how much\n\n27:23.880 --> 27:25.720\n when you try to create a benchmark\n\n27:25.720 --> 27:28.200\n for the community to gather around,\n\n27:28.200 --> 27:30.920\n how much of the magic of language is lost\n\n27:30.920 --> 27:33.240\n when you create that benchmark.\n\n27:33.240 --> 27:35.920\n That there's something, we talk about experience,\n\n27:35.920 --> 27:38.600\n the music of the language, the wit,\n\n27:38.600 --> 27:41.080\n the something that makes a rich experience,\n\n27:41.080 --> 27:43.800\n something that would be required to pass\n\n27:43.800 --> 27:47.660\n the spirit of the Turing test is lost in these benchmarks.\n\n27:47.660 --> 27:50.240\n And I wonder how to get it back in\n\n27:50.240 --> 27:51.920\n because it's very difficult.\n\n27:51.920 --> 27:55.160\n The moment you try to do like real good rigorous science,\n\n27:55.160 --> 27:56.960\n you lose some of that magic.\n\n27:56.960 --> 28:00.160\n When you try to study cognition\n\n28:00.160 --> 28:01.560\n in a rigorous scientific way,\n\n28:01.560 --> 28:03.800\n it feels like you're losing some of the magic.\n\n28:03.800 --> 28:07.520\n The seeing cognition in a mechanistic way\n\n28:07.520 --> 28:10.060\n that AI folk at this stage in our history.\n\n28:10.060 --> 28:13.040\n Well, I agree with you, but at the same time,\n\n28:13.040 --> 28:18.040\n one thing that I found really exciting\n\n28:18.040 --> 28:22.960\n about that first wave of deep learning models in cognition\n\n28:22.960 --> 28:27.960\n was the fact that the people who were building these models\n\n28:29.640 --> 28:32.960\n were focused on the richness and complexity\n\n28:32.960 --> 28:34.800\n of human cognition.\n\n28:34.800 --> 28:39.800\n So an early debate in cognitive science,\n\n28:40.080 --> 28:41.820\n which I sort of witnessed as a grad student\n\n28:41.820 --> 28:44.200\n was about something that sounds very dry,\n\n28:44.200 --> 28:47.180\n which is the formation of the past tense.\n\n28:47.180 --> 28:49.200\n But there were these two camps.\n\n28:49.200 --> 28:54.200\n One said, well, the mind encodes certain rules\n\n28:54.400 --> 28:57.900\n and it also has a list of exceptions\n\n28:57.900 --> 29:00.380\n because of course, the rule is add ED,\n\n29:00.380 --> 29:01.820\n but that's not always what you do.\n\n29:01.820 --> 29:03.720\n So you have to have a list of exceptions.\n\n29:05.000 --> 29:06.960\n And then there were the connectionists\n\n29:06.960 --> 29:10.700\n who evolved into the deep learning people who said,\n\n29:10.700 --> 29:13.820\n well, if you look carefully at the data,\n\n29:13.820 --> 29:18.280\n if you actually look at corpora, like language corpora,\n\n29:18.280 --> 29:20.080\n it turns out to be very rich\n\n29:20.080 --> 29:25.080\n because yes, there are most verbs\n\n29:25.080 --> 29:28.640\n that you just tack on ED, and then there are exceptions,\n\n29:28.640 --> 29:33.640\n but there are rules that the exceptions aren't just random.\n\n29:36.040 --> 29:39.560\n There are certain clues to which verbs\n\n29:39.560 --> 29:41.040\n should be exceptional.\n\n29:41.040 --> 29:44.120\n And then there are exceptions to the exceptions.\n\n29:44.120 --> 29:47.760\n And there was a word that was kind of deployed\n\n29:47.760 --> 29:51.760\n in order to capture this, which was quasi regular.\n\n29:51.760 --> 29:54.740\n In other words, there are rules, but it's messy.\n\n29:54.740 --> 29:58.760\n And there's either structure even among the exceptions.\n\n29:58.760 --> 30:01.280\n And it would be, yeah, you could try to write down,\n\n30:01.280 --> 30:03.820\n we could try to write down the structure\n\n30:03.820 --> 30:04.840\n in some sort of closed form,\n\n30:04.840 --> 30:07.560\n but really the right way to understand\n\n30:07.560 --> 30:09.080\n how the brain is handling all this,\n\n30:09.080 --> 30:11.440\n and by the way, producing all of this,\n\n30:11.440 --> 30:14.000\n is to build a deep neural network\n\n30:14.000 --> 30:15.200\n and train it on this data\n\n30:15.200 --> 30:18.520\n and see how it ends up representing all of this richness.\n\n30:18.520 --> 30:21.420\n So the way that deep learning\n\n30:21.420 --> 30:23.720\n was deployed in cognitive psychology\n\n30:23.720 --> 30:25.960\n was that was the spirit of it.\n\n30:25.960 --> 30:28.080\n It was about that richness.\n\n30:29.560 --> 30:31.960\n And that's something that I always found very compelling,\n\n30:31.960 --> 30:33.160\n still do.\n\n30:33.160 --> 30:36.200\n Is there something especially interesting\n\n30:36.200 --> 30:37.520\n and profound to you\n\n30:37.520 --> 30:40.480\n in terms of our current deep learning neural network,\n\n30:40.480 --> 30:42.640\n artificial neural network approaches,\n\n30:42.640 --> 30:46.300\n and whatever we do understand\n\n30:46.300 --> 30:49.000\n about the biological neural networks in our brain?\n\n30:49.000 --> 30:52.440\n Is there, there's quite a few differences.\n\n30:52.440 --> 30:54.680\n Are some of them to you,\n\n30:54.680 --> 30:58.040\n either interesting or perhaps profound\n\n30:58.040 --> 31:03.040\n in terms of the gap we might want to try to close\n\n31:03.040 --> 31:07.560\n in trying to create a human level intelligence?\n\n31:07.560 --> 31:08.840\n What I would say here is something\n\n31:08.840 --> 31:10.720\n that a lot of people are saying,\n\n31:10.720 --> 31:15.720\n which is that one seeming limitation\n\n31:16.580 --> 31:18.960\n of the systems that we're building now\n\n31:18.960 --> 31:21.860\n is that they lack the kind of flexibility,\n\n31:22.900 --> 31:25.960\n the readiness to sort of turn on a dime\n\n31:25.960 --> 31:28.200\n when the context calls for it\n\n31:28.200 --> 31:32.200\n that is so characteristic of human behavior.\n\n31:32.200 --> 31:34.920\n So is that connected to you to the,\n\n31:34.920 --> 31:37.720\n like which aspect of the neural networks in our brain\n\n31:37.720 --> 31:39.160\n is that connected to?\n\n31:39.160 --> 31:42.660\n Is that closer to the cognitive science level of,\n\n31:45.080 --> 31:47.320\n now again, see like my natural inclination\n\n31:47.320 --> 31:51.640\n is to separate into three disciplines of neuroscience,\n\n31:51.640 --> 31:54.280\n cognitive science and psychology.\n\n31:54.280 --> 31:56.380\n And you've already kind of shut that down\n\n31:56.380 --> 31:58.360\n by saying you're kind of see them as separate,\n\n31:58.360 --> 32:01.500\n but just to look at those layers,\n\n32:01.500 --> 32:05.320\n I guess where is there something about the lowest layer\n\n32:05.320 --> 32:09.160\n of the way the neural neurons interact\n\n32:09.160 --> 32:13.320\n that is profound to you in terms of this difference\n\n32:13.320 --> 32:15.480\n to the artificial neural networks,\n\n32:15.480 --> 32:17.220\n or is all the key differences\n\n32:17.220 --> 32:19.240\n at a higher level of abstraction?\n\n32:20.720 --> 32:22.720\n One thing I often think about is that,\n\n32:24.440 --> 32:27.140\n if you take an introductory computer science course\n\n32:27.140 --> 32:29.600\n and they are introducing you to the notion\n\n32:29.600 --> 32:31.480\n of Turing machines,\n\n32:31.480 --> 32:36.000\n one way of articulating\n\n32:36.000 --> 32:39.320\n what the significance of a Turing machine is,\n\n32:39.320 --> 32:41.620\n is that it's a machine emulator.\n\n32:42.760 --> 32:45.320\n It can emulate any other machine.\n\n32:47.540 --> 32:50.480\n And that to me,\n\n32:52.960 --> 32:54.920\n that way of looking at a Turing machine\n\n32:56.200 --> 32:57.640\n really sticks with me.\n\n32:57.640 --> 33:01.960\n I think of humans as maybe sharing\n\n33:01.960 --> 33:05.000\n in some of that character.\n\n33:05.000 --> 33:06.160\n We're capacity limited,\n\n33:06.160 --> 33:07.540\n we're not Turing machines obviously,\n\n33:07.540 --> 33:11.040\n but we have the ability to adapt behaviors\n\n33:11.040 --> 33:15.420\n that are very much unlike anything we've done before,\n\n33:15.420 --> 33:17.720\n but there's some basic mechanism\n\n33:17.720 --> 33:18.960\n that's implemented in our brain\n\n33:18.960 --> 33:22.400\n that allows us to run software.\n\n33:22.400 --> 33:24.600\n But just on that point, you mentioned Turing machine,\n\n33:24.600 --> 33:26.840\n but nevertheless, it's fundamentally\n\n33:26.840 --> 33:29.720\n our brains are just computational devices in your view.\n\n33:29.720 --> 33:31.160\n Is that what you're getting at?\n\n33:31.160 --> 33:35.680\n It was a little bit unclear to this line you drew.\n\n33:35.680 --> 33:37.800\n Is there any magic in there\n\n33:37.800 --> 33:40.720\n or is it just basic computation?\n\n33:40.720 --> 33:43.320\n I'm happy to think of it as just basic computation,\n\n33:43.320 --> 33:46.120\n but mind you, I won't be satisfied\n\n33:46.120 --> 33:48.280\n until somebody explains to me\n\n33:48.280 --> 33:49.840\n what the basic computations are\n\n33:49.840 --> 33:53.760\n that are leading to the full richness of human cognition.\n\n33:54.760 --> 33:56.680\n It's not gonna be enough for me\n\n33:56.680 --> 33:58.880\n to understand what the computations are\n\n33:58.880 --> 34:02.160\n that allow people to do arithmetic or play chess.\n\n34:02.160 --> 34:06.360\n I want the whole thing.\n\n34:06.360 --> 34:07.780\n And a small tangent,\n\n34:07.780 --> 34:10.480\n because you kind of mentioned coronavirus,\n\n34:10.480 --> 34:12.400\n there's group behavior.\n\n34:12.400 --> 34:13.480\n Oh, sure.\n\n34:13.480 --> 34:14.960\n Is there something interesting\n\n34:14.960 --> 34:17.680\n to your search of understanding the human mind\n\n34:18.720 --> 34:21.520\n where behavior of large groups\n\n34:21.520 --> 34:24.240\n or just behavior of groups is interesting,\n\n34:24.240 --> 34:25.640\n seeing that as a collective mind,\n\n34:25.640 --> 34:27.120\n as a collective intelligence,\n\n34:27.120 --> 34:28.880\n perhaps seeing the groups of people\n\n34:28.880 --> 34:31.080\n as a single intelligent organisms,\n\n34:31.080 --> 34:34.200\n especially looking at the reinforcement learning work\n\n34:34.200 --> 34:35.600\n you've done recently.\n\n34:35.600 --> 34:36.920\n Well, yeah, I can't.\n\n34:36.920 --> 34:41.760\n I mean, I have the honor of working\n\n34:41.760 --> 34:43.640\n with a lot of incredibly smart people\n\n34:43.640 --> 34:45.480\n and I wouldn't wanna take any credit\n\n34:45.480 --> 34:48.820\n for leading the way on the multiagent work\n\n34:48.820 --> 34:51.360\n that's come out of my group or DeepMind lately,\n\n34:51.360 --> 34:53.840\n but I do find it fascinating.\n\n34:53.840 --> 34:58.840\n And I mean, I think it can't be debated.\n\n35:00.760 --> 35:05.760\n You know, human behavior arises within communities.\n\n35:06.000 --> 35:08.960\n That just seems to me self evident.\n\n35:08.960 --> 35:11.400\n But to me, it is self evident,\n\n35:11.400 --> 35:14.720\n but that seems to be a profound aspects\n\n35:14.720 --> 35:16.040\n of something that created.\n\n35:16.040 --> 35:19.160\n That was like, if you look at like 2001 Space Odyssey\n\n35:19.160 --> 35:21.360\n when the monkeys touched the...\n\n35:21.360 --> 35:22.200\n Yeah.\n\n35:22.200 --> 35:25.320\n That's the magical moment I think Yuval Harari argues\n\n35:25.320 --> 35:29.400\n that the ability of our large numbers of humans\n\n35:29.400 --> 35:31.880\n to hold an idea, to converge towards idea together,\n\n35:31.880 --> 35:34.360\n like you said, shaking hands versus bumping elbows,\n\n35:34.360 --> 35:39.360\n somehow converge without being in a room altogether,\n\n35:40.880 --> 35:43.380\n just kind of this like distributed convergence\n\n35:43.380 --> 35:46.720\n towards an idea over a particular period of time\n\n35:46.720 --> 35:51.520\n seems to be fundamental to just every aspect\n\n35:51.520 --> 35:53.400\n of our cognition, of our intelligence,\n\n35:53.400 --> 35:56.720\n because humans, I will talk about reward,\n\n35:56.720 --> 35:58.720\n but it seems like we don't really have\n\n35:58.720 --> 36:01.320\n a clear objective function under which we operate,\n\n36:01.320 --> 36:04.160\n but we all kind of converge towards one somehow.\n\n36:04.160 --> 36:06.740\n And that to me has always been a mystery\n\n36:07.600 --> 36:09.840\n that I think is somehow productive\n\n36:09.840 --> 36:13.620\n for also understanding AI systems.\n\n36:13.620 --> 36:16.520\n But I guess that's the next step.\n\n36:16.520 --> 36:18.780\n The first step is try to understand the mind.\n\n36:18.780 --> 36:19.700\n Well, I don't know.\n\n36:19.700 --> 36:22.520\n I mean, I think there's something to the argument\n\n36:22.520 --> 36:27.520\n that that kind of like strictly bottom up approach\n\n36:27.520 --> 36:29.920\n is wrongheaded.\n\n36:29.920 --> 36:33.920\n In other words, there are basic phenomena,\n\n36:34.880 --> 36:36.860\n basic aspects of human intelligence\n\n36:36.860 --> 36:41.860\n that can only be understood in the context of groups.\n\n36:43.280 --> 36:44.680\n I'm perfectly open to that.\n\n36:44.680 --> 36:48.680\n I've never been particularly convinced by the notion\n\n36:48.680 --> 36:52.360\n that we should consider intelligence\n\n36:52.360 --> 36:55.600\n to inhere at the level of communities.\n\n36:55.600 --> 36:58.720\n I don't know why, I'm sort of stuck on the notion\n\n36:58.720 --> 37:01.380\n that the basic unit that we want to understand\n\n37:01.380 --> 37:02.720\n is individual humans.\n\n37:02.720 --> 37:05.880\n And if we have to understand that\n\n37:05.880 --> 37:07.680\n in the context of other humans, fine.\n\n37:08.560 --> 37:11.320\n But for me, intelligence is just,\n\n37:11.320 --> 37:14.640\n I stubbornly define it as something\n\n37:14.640 --> 37:18.800\n that is an aspect of an individual human.\n\n37:18.800 --> 37:20.200\n That's just my, I don't know if that's a matter of taste.\n\n37:20.200 --> 37:22.880\n I'm with you, but that could be the reductionist dream\n\n37:22.880 --> 37:26.400\n of a scientist because you can understand a single human.\n\n37:26.400 --> 37:30.760\n It also is very possible that intelligence can only arise\n\n37:30.760 --> 37:33.040\n when there's multiple intelligences.\n\n37:33.040 --> 37:37.480\n When there's multiple sort of, it's a sad thing,\n\n37:37.480 --> 37:39.880\n if that's true, because it's very difficult to study.\n\n37:39.880 --> 37:42.440\n But if it's just one human,\n\n37:42.440 --> 37:44.880\n that one human would not be homosapien,\n\n37:44.880 --> 37:46.520\n would not become that intelligent.\n\n37:46.520 --> 37:48.500\n That's a possibility.\n\n37:48.500 --> 37:50.040\n I'm with you.\n\n37:50.040 --> 37:52.800\n One thing I will say along these lines\n\n37:52.800 --> 37:57.800\n is that I think a serious effort\n\n38:01.280 --> 38:03.440\n to understand human intelligence\n\n38:05.600 --> 38:09.680\n and maybe to build humanlike intelligence\n\n38:09.680 --> 38:11.840\n needs to pay just as much attention\n\n38:11.840 --> 38:14.000\n to the structure of the environment\n\n38:14.000 --> 38:17.000\n as to the structure of the cognizing system,\n\n38:20.040 --> 38:22.180\n whether it's a brain or an AI system.\n\n38:23.260 --> 38:24.640\n That's one thing I took away actually\n\n38:24.640 --> 38:27.920\n from my early studies with the pioneers\n\n38:27.920 --> 38:29.900\n of neural network research,\n\n38:29.900 --> 38:32.240\n people like Jay McClelland and John Cohen.\n\n38:34.080 --> 38:38.600\n The structure of cognition is really,\n\n38:38.600 --> 38:43.600\n it's only partly a function of the architecture of the brain\n\n38:44.480 --> 38:46.980\n and the learning algorithms that it implements.\n\n38:46.980 --> 38:51.520\n What really shapes it is the interaction of those things\n\n38:51.520 --> 38:54.460\n with the structure of the world\n\n38:54.460 --> 38:56.680\n in which those things are embedded.\n\n38:56.680 --> 38:58.280\n And that's especially important for,\n\n38:58.280 --> 39:00.880\n that's made most clear in reinforcement learning\n\n39:00.880 --> 39:03.720\n where the simulated environment is,\n\n39:03.720 --> 39:05.800\n you can only learn as much as you can simulate.\n\n39:05.800 --> 39:09.360\n And that's what DeepMind made very clear\n\n39:09.360 --> 39:11.080\n with the other aspect of the environment,\n\n39:11.080 --> 39:15.600\n which is the self play mechanism of the other agent,\n\n39:15.600 --> 39:16.840\n of the competitive behavior,\n\n39:16.840 --> 39:20.000\n which the other agent becomes the environment essentially.\n\n39:20.000 --> 39:24.080\n And that's, I mean, one of the most exciting ideas in AI\n\n39:24.080 --> 39:27.960\n is the self play mechanism that's able to learn successfully.\n\n39:27.960 --> 39:28.800\n So there you go.\n\n39:28.800 --> 39:31.600\n There's a thing where competition is essential\n\n39:31.600 --> 39:35.040\n for learning, at least in that context.\n\n39:35.040 --> 39:37.960\n So if we can step back into another sort of beautiful world,\n\n39:37.960 --> 39:42.040\n which is the actual mechanics,\n\n39:42.040 --> 39:44.680\n the dirty mess of it of the human brain,\n\n39:44.680 --> 39:48.520\n is there something for people who might not know?\n\n39:49.440 --> 39:51.120\n Is there something you can comment on\n\n39:51.120 --> 39:53.960\n or describe the key parts of the brain\n\n39:53.960 --> 39:56.840\n that are important for intelligence or just in general,\n\n39:56.840 --> 39:58.620\n what are the different parts of the brain\n\n39:58.620 --> 40:01.120\n that you're curious about that you've studied\n\n40:01.120 --> 40:03.880\n and that are just good to know about\n\n40:03.880 --> 40:06.240\n when you're thinking about cognition?\n\n40:06.240 --> 40:11.200\n Well, my area of expertise, if I have one,\n\n40:11.200 --> 40:14.200\n is prefrontal cortex.\n\n40:14.200 --> 40:16.560\n So, you know. What's that?\n\n40:16.560 --> 40:18.200\n Where do we?\n\n40:18.200 --> 40:19.520\n It depends on who you ask.\n\n40:21.520 --> 40:25.640\n The technical definition is anatomical.\n\n40:25.640 --> 40:30.640\n There are parts of your brain\n\n40:30.680 --> 40:32.480\n that are responsible for motor behavior\n\n40:32.480 --> 40:34.620\n and they're very easy to identify.\n\n40:35.740 --> 40:40.740\n And the region of your cerebral cortex,\n\n40:40.760 --> 40:43.960\n the sort of outer crust of your brain\n\n40:43.960 --> 40:46.440\n that lies in front of those\n\n40:46.440 --> 40:49.360\n is defined as the prefrontal cortex.\n\n40:49.360 --> 40:51.960\n And when you say anatomical, sorry to interrupt,\n\n40:51.960 --> 40:56.960\n so that's referring to sort of the geographic region\n\n40:57.160 --> 41:00.160\n as opposed to some kind of functional definition.\n\n41:00.160 --> 41:04.400\n Exactly, so this is kind of the coward's way out.\n\n41:04.400 --> 41:06.000\n I'm telling you what the prefrontal cortex is\n\n41:06.000 --> 41:09.640\n just in terms of what part of the real estate it occupies.\n\n41:09.640 --> 41:10.720\n It's the thing in the front of the brain.\n\n41:10.720 --> 41:11.680\n Yeah, exactly.\n\n41:11.680 --> 41:14.960\n And in fact, the early history\n\n41:14.960 --> 41:19.960\n of neuroscientific investigation\n\n41:20.840 --> 41:23.480\n of what this front part of the brain does\n\n41:23.480 --> 41:25.760\n is sort of funny to read\n\n41:25.760 --> 41:30.760\n because it was really World War I\n\n41:32.280 --> 41:34.580\n that started people down this road\n\n41:34.580 --> 41:37.280\n of trying to figure out what different parts of the brain,\n\n41:37.280 --> 41:39.440\n the human brain do in the sense\n\n41:39.440 --> 41:42.560\n that there were a lot of people with brain damage\n\n41:42.560 --> 41:44.800\n who came back from the war with brain damage.\n\n41:44.800 --> 41:47.740\n And that provided, as tragic as that was,\n\n41:47.740 --> 41:49.900\n it provided an opportunity for scientists\n\n41:49.900 --> 41:53.440\n to try to identify the functions of different brain regions.\n\n41:53.440 --> 41:56.200\n And that was actually incredibly productive,\n\n41:56.200 --> 41:59.480\n but one of the frustrations that neuropsychologists faced\n\n41:59.480 --> 42:02.160\n was they couldn't really identify exactly\n\n42:02.160 --> 42:05.040\n what the deficit was that arose from damage\n\n42:05.040 --> 42:08.440\n to these most kind of frontal parts of the brain.\n\n42:08.440 --> 42:13.440\n It was just a very difficult thing to pin down.\n\n42:13.680 --> 42:16.080\n There were a couple of neuropsychologists\n\n42:16.080 --> 42:20.600\n who identified through a large amount\n\n42:20.600 --> 42:23.000\n of clinical experience and close observation,\n\n42:23.000 --> 42:26.240\n they started to put their finger on a syndrome\n\n42:26.240 --> 42:27.680\n that was associated with frontal damage.\n\n42:27.680 --> 42:30.480\n Actually, one of them was a Russian neuropsychologist\n\n42:30.480 --> 42:35.120\n named Luria, who students of cognitive psychology still read.\n\n42:36.160 --> 42:41.160\n And what he started to figure out was that\n\n42:41.360 --> 42:45.600\n the frontal cortex was somehow involved in flexibility,\n\n42:48.060 --> 42:52.320\n in guiding behaviors that required someone\n\n42:52.320 --> 42:57.320\n to override a habit, or to do something unusual,\n\n42:57.600 --> 43:01.040\n or to change what they were doing in a very flexible way\n\n43:01.040 --> 43:02.560\n from one moment to another.\n\n43:02.560 --> 43:05.080\n So focused on like new experiences.\n\n43:05.080 --> 43:08.800\n And so the way your brain processes\n\n43:08.800 --> 43:10.960\n and acts in new experiences.\n\n43:10.960 --> 43:14.760\n Yeah, what later helped bring this function\n\n43:14.760 --> 43:17.240\n into better focus was a distinction\n\n43:17.240 --> 43:19.880\n between controlled and automatic behavior,\n\n43:19.880 --> 43:23.680\n or in other literatures, this is referred to\n\n43:23.680 --> 43:28.280\n as habitual behavior versus goal directed behavior.\n\n43:28.280 --> 43:33.280\n So it's very, very clear that the human brain\n\n43:33.440 --> 43:36.600\n has pathways that are dedicated to habits,\n\n43:36.600 --> 43:39.360\n to things that you do all the time,\n\n43:39.360 --> 43:42.440\n and they need to be automatized\n\n43:42.440 --> 43:45.140\n so that they don't require you to concentrate too much.\n\n43:45.140 --> 43:47.840\n So that leaves your cognitive capacity\n\n43:47.840 --> 43:49.800\n free to do other things.\n\n43:49.800 --> 43:51.640\n Just think about the difference\n\n43:51.640 --> 43:55.960\n between driving when you're learning to drive\n\n43:55.960 --> 43:59.160\n versus driving after you're a fairly expert.\n\n43:59.160 --> 44:03.560\n There are brain pathways that slowly absorb\n\n44:03.560 --> 44:07.840\n those frequently performed behaviors\n\n44:07.840 --> 44:12.360\n so that they can be habits, so that they can be automatic.\n\n44:12.360 --> 44:14.900\n That's kind of like the purest form of learning.\n\n44:14.900 --> 44:18.360\n I guess it's happening there, which is why,\n\n44:18.360 --> 44:20.000\n I mean, this is kind of jumping ahead,\n\n44:20.000 --> 44:22.480\n which is why that perhaps is the most useful for us\n\n44:22.480 --> 44:24.080\n to focusing on and trying to see\n\n44:24.080 --> 44:27.340\n how artificial intelligence systems can learn.\n\n44:27.340 --> 44:28.180\n Is that the way you think?\n\n44:28.180 --> 44:29.000\n It's interesting.\n\n44:29.000 --> 44:30.040\n I do think about this distinction\n\n44:30.040 --> 44:31.440\n between controlled and automatic,\n\n44:31.440 --> 44:34.600\n or goal directed and habitual behavior a lot\n\n44:34.600 --> 44:38.640\n in thinking about where we are in AI research.\n\n44:42.960 --> 44:46.480\n But just to finish the kind of dissertation here,\n\n44:46.480 --> 44:51.380\n the role of the prefrontal cortex\n\n44:51.380 --> 44:54.600\n is generally understood these days\n\n44:54.600 --> 44:59.600\n sort of in contradistinction to that habitual domain.\n\n45:00.440 --> 45:02.320\n In other words, the prefrontal cortex\n\n45:02.320 --> 45:05.840\n is what helps you override those habits.\n\n45:05.840 --> 45:07.440\n It's what allows you to say,\n\n45:07.440 --> 45:10.800\n well, what I usually do in this situation is X,\n\n45:10.800 --> 45:14.160\n but given the context, I probably should do Y.\n\n45:14.160 --> 45:18.080\n I mean, the elbow bump is a great example, right?\n\n45:18.080 --> 45:19.300\n Reaching out and shaking hands\n\n45:19.300 --> 45:22.520\n is probably a habitual behavior,\n\n45:22.520 --> 45:26.000\n and it's the prefrontal cortex that allows us\n\n45:26.000 --> 45:28.760\n to bear in mind that there's something unusual\n\n45:28.760 --> 45:31.360\n going on right now, and in this situation,\n\n45:31.360 --> 45:33.480\n I need to not do the usual thing.\n\n45:34.720 --> 45:38.560\n The kind of behaviors that Luria reported,\n\n45:38.560 --> 45:42.040\n and he built tests for detecting these kinds of things,\n\n45:42.040 --> 45:43.460\n were exactly like this.\n\n45:43.460 --> 45:46.680\n So in other words, when I stick out my hand,\n\n45:47.540 --> 45:49.760\n I want you instead to present your elbow.\n\n45:49.760 --> 45:51.080\n A patient with frontal damage\n\n45:51.080 --> 45:53.520\n would have a great deal of trouble with that.\n\n45:53.520 --> 45:57.760\n Somebody proffering their hand would elicit a handshake.\n\n45:58.800 --> 46:00.920\n The prefrontal cortex is what allows us to say,\n\n46:00.920 --> 46:03.840\n hold on, hold on, that's the usual thing,\n\n46:03.840 --> 46:07.120\n but I have the ability to bear in mind\n\n46:07.120 --> 46:10.520\n even very unusual contexts and to reason about\n\n46:10.520 --> 46:13.240\n what behavior is appropriate there.\n\n46:13.240 --> 46:17.560\n Just to get a sense, are us humans special\n\n46:17.560 --> 46:20.680\n in the presence of the prefrontal cortex?\n\n46:20.680 --> 46:22.640\n Do mice have a prefrontal cortex?\n\n46:22.640 --> 46:25.900\n Do other mammals that we can study?\n\n46:25.900 --> 46:30.040\n If no, then how do they integrate new experiences?\n\n46:30.040 --> 46:33.760\n Yeah, that's a really tricky question\n\n46:33.760 --> 46:35.840\n and a very timely question\n\n46:35.840 --> 46:42.840\n because we have revolutionary new technologies\n\n46:44.040 --> 46:48.280\n for monitoring, measuring,\n\n46:48.280 --> 46:52.040\n and also causally influencing neural behavior\n\n46:52.040 --> 46:57.000\n in mice and fruit flies.\n\n46:57.000 --> 47:00.640\n And these techniques are not fully available\n\n47:00.640 --> 47:05.640\n even for studying brain function in monkeys,\n\n47:06.080 --> 47:07.280\n let alone humans.\n\n47:08.160 --> 47:12.920\n And so it's a very sort of, for me at least,\n\n47:12.920 --> 47:16.160\n a very urgent question whether the kinds of things\n\n47:16.160 --> 47:18.000\n that we wanna understand about human intelligence\n\n47:18.000 --> 47:22.000\n can be pursued in these other organisms.\n\n47:22.000 --> 47:26.500\n And to put it briefly, there's disagreement.\n\n47:26.500 --> 47:31.500\n People who study fruit flies will often tell you,\n\n47:32.960 --> 47:35.520\n hey, fruit flies are smarter than you think.\n\n47:35.520 --> 47:37.600\n And they'll point to experiments where fruit flies\n\n47:37.600 --> 47:40.320\n were able to learn new behaviors,\n\n47:40.320 --> 47:44.180\n were able to generalize from one stimulus to another\n\n47:44.180 --> 47:47.500\n in a way that suggests that they have abstractions\n\n47:47.500 --> 47:49.340\n that guide their generalization.\n\n47:51.880 --> 47:53.840\n I've had many conversations in which\n\n47:53.840 --> 47:56.520\n I will start by observing,\n\n47:58.160 --> 48:03.160\n recounting some observation about mouse behavior\n\n48:05.200 --> 48:09.060\n where it seemed like mice were taking an awfully long time\n\n48:09.060 --> 48:13.660\n to learn a task that for a human would be profoundly trivial.\n\n48:13.660 --> 48:16.460\n And I will conclude from that,\n\n48:16.460 --> 48:18.800\n that mice really don't have the cognitive flexibility\n\n48:18.800 --> 48:20.100\n that we want to explain.\n\n48:20.100 --> 48:21.760\n And then a mouse researcher will say to me,\n\n48:21.760 --> 48:26.360\n well, hold on, that experiment may not have worked\n\n48:26.360 --> 48:31.280\n because you asked a mouse to deal with stimuli\n\n48:31.280 --> 48:34.300\n and behaviors that were very unnatural for the mouse.\n\n48:34.300 --> 48:38.760\n If instead you kept the logic of the experiment the same,\n\n48:38.760 --> 48:43.760\n but presented the information in a way\n\n48:44.440 --> 48:46.880\n that aligns with what mice are used to dealing with\n\n48:46.880 --> 48:48.480\n in their natural habitats,\n\n48:48.480 --> 48:51.080\n you might find that a mouse actually has more intelligence\n\n48:51.080 --> 48:52.440\n than you think.\n\n48:52.440 --> 48:54.920\n And then they'll go on to show you videos\n\n48:54.920 --> 48:57.440\n of mice doing things in their natural habitat,\n\n48:57.440 --> 49:00.000\n which seem strikingly intelligent,\n\n49:00.000 --> 49:02.920\n dealing with physical problems.\n\n49:02.920 --> 49:07.180\n I have to drag this piece of food back to my lair,\n\n49:07.180 --> 49:08.560\n but there's something in my way\n\n49:08.560 --> 49:10.400\n and how do I get rid of that thing?\n\n49:10.400 --> 49:13.160\n So I think these are open questions\n\n49:13.160 --> 49:15.400\n to put it, to sum that up.\n\n49:15.400 --> 49:18.520\n And then taking a small step back related to that\n\n49:18.520 --> 49:21.440\n is you kind of mentioned we're taking a little shortcut\n\n49:21.440 --> 49:26.440\n by saying it's a geographic part of the prefrontal cortex\n\n49:26.600 --> 49:28.280\n is a region of the brain.\n\n49:28.280 --> 49:33.280\n But if we, what's your sense in a bigger philosophical view,\n\n49:33.720 --> 49:36.260\n prefrontal cortex and the brain in general,\n\n49:36.260 --> 49:38.840\n do you have a sense that it's a set of subsystems\n\n49:38.840 --> 49:41.180\n in the way we've kind of implied\n\n49:41.180 --> 49:46.180\n that are pretty distinct or to what degree is it that\n\n49:46.180 --> 49:49.460\n or to what degree is it a giant interconnected mess\n\n49:49.460 --> 49:51.380\n where everything kind of does everything\n\n49:51.380 --> 49:53.820\n and it's impossible to disentangle them?\n\n49:54.920 --> 49:57.020\n I think there's overwhelming evidence\n\n49:57.020 --> 50:00.060\n that there's functional differentiation,\n\n50:00.060 --> 50:03.460\n that it's clearly not the case\n\n50:03.460 --> 50:07.100\n that all parts of the brain are doing the same thing.\n\n50:07.100 --> 50:11.100\n This follows immediately from the kinds of studies\n\n50:11.100 --> 50:14.620\n of brain damage that we were chatting about before.\n\n50:14.620 --> 50:18.060\n It's obvious from what you see\n\n50:18.060 --> 50:19.620\n if you stick an electrode in the brain\n\n50:19.620 --> 50:24.540\n and measure what's going on at the level of neural activity.\n\n50:25.960 --> 50:30.680\n Having said that, there are two other things to add,\n\n50:30.680 --> 50:32.740\n which kind of, I don't know,\n\n50:32.740 --> 50:34.340\n maybe tug in the other direction.\n\n50:34.340 --> 50:39.340\n One is that it's when you look carefully\n\n50:39.740 --> 50:42.220\n at functional differentiation in the brain,\n\n50:42.220 --> 50:44.900\n what you usually end up concluding,\n\n50:44.900 --> 50:48.140\n at least this is my observation of the literature,\n\n50:48.140 --> 50:52.780\n is that the differences between regions are graded\n\n50:52.780 --> 50:55.180\n rather than being discreet.\n\n50:55.180 --> 50:57.460\n So it doesn't seem like it's easy\n\n50:57.460 --> 51:01.740\n to divide the brain up into true modules\n\n51:03.300 --> 51:07.460\n that have clear boundaries and that have\n\n51:07.460 --> 51:12.460\n you know, clear channels of communication between them.\n\n51:16.020 --> 51:18.020\n And this applies to the prefrontal cortex?\n\n51:18.020 --> 51:18.860\n Yeah, oh yeah.\n\n51:18.860 --> 51:20.200\n The prefrontal cortex is made up\n\n51:20.200 --> 51:22.080\n of a bunch of different subregions,\n\n51:23.140 --> 51:27.380\n the functions of which are not clearly defined\n\n51:27.380 --> 51:30.820\n and the borders of which seem to be quite vague.\n\n51:32.300 --> 51:34.420\n And then there's another thing that's popping up\n\n51:34.420 --> 51:37.400\n in very recent research, which, you know, which,\n\n51:40.280 --> 51:43.640\n involves application of these new techniques,\n\n51:44.940 --> 51:47.740\n which there are a number of studies that suggest that\n\n51:48.820 --> 51:51.540\n parts of the brain that we would have previously thought\n\n51:51.540 --> 51:56.540\n were quite focused in their function\n\n51:57.740 --> 51:59.100\n are actually carrying signals\n\n51:59.100 --> 52:01.340\n that we wouldn't have thought would be there.\n\n52:01.340 --> 52:04.500\n For example, looking in the primary visual cortex,\n\n52:04.500 --> 52:07.900\n which is classically thought of as basically\n\n52:07.900 --> 52:09.380\n the first cortical way station\n\n52:09.380 --> 52:10.900\n for processing visual information.\n\n52:10.900 --> 52:12.980\n Basically what it should care about is, you know,\n\n52:12.980 --> 52:15.840\n where are the edges in this scene that I'm viewing?\n\n52:17.460 --> 52:19.460\n It turns out that if you have enough data,\n\n52:19.460 --> 52:22.220\n you can recover information from primary visual cortex\n\n52:22.220 --> 52:23.220\n about all sorts of things.\n\n52:23.220 --> 52:26.180\n Like, you know, what behavior the animal is engaged\n\n52:26.180 --> 52:29.340\n in right now and how much reward is on offer\n\n52:29.340 --> 52:31.340\n in the task that it's pursuing.\n\n52:31.340 --> 52:36.340\n So it's clear that even regions whose function\n\n52:36.740 --> 52:40.540\n is pretty well defined at a core screen\n\n52:40.540 --> 52:42.860\n are nonetheless carrying some information\n\n52:42.860 --> 52:47.060\n about information from very different domains.\n\n52:47.060 --> 52:49.780\n So, you know, the history of neuroscience\n\n52:49.780 --> 52:52.660\n is sort of this oscillation between the two views\n\n52:52.660 --> 52:55.460\n that you articulated, you know, the kind of modular view\n\n52:55.460 --> 52:57.740\n and then the big, you know, mush view.\n\n52:57.740 --> 53:01.580\n And, you know, I think, I guess we're gonna end up\n\n53:01.580 --> 53:02.800\n somewhere in the middle.\n\n53:02.800 --> 53:05.580\n Which is unfortunate for our understanding\n\n53:05.580 --> 53:08.880\n because there's something about our, you know,\n\n53:08.880 --> 53:11.380\n conceptual system that finds it's easy to think about\n\n53:11.380 --> 53:13.680\n a modularized system and easy to think about\n\n53:13.680 --> 53:15.500\n a completely undifferentiated system.\n\n53:15.500 --> 53:19.980\n But something that kind of lies in between is confusing.\n\n53:19.980 --> 53:21.860\n But we're gonna have to get used to it, I think.\n\n53:21.860 --> 53:24.660\n Unless we can understand deeply the lower level mechanism\n\n53:24.660 --> 53:25.860\n of neuronal communication.\n\n53:25.860 --> 53:26.760\n Yeah, yeah.\n\n53:26.760 --> 53:29.660\n But on that topic, you kind of mentioned information.\n\n53:29.660 --> 53:31.860\n Just to get a sense, I imagine something\n\n53:31.860 --> 53:34.620\n that there's still mystery and disagreement on\n\n53:34.620 --> 53:38.060\n is how does the brain carry information and signal?\n\n53:38.060 --> 53:43.060\n Like what in your sense is the basic mechanism\n\n53:43.380 --> 53:46.420\n of communication in the brain?\n\n53:46.420 --> 53:51.420\n Well, I guess I'm old fashioned in that I consider\n\n53:52.020 --> 53:54.340\n the networks that we use in deep learning research\n\n53:54.340 --> 53:59.080\n to be a reasonable approximation to, you know,\n\n53:59.080 --> 54:02.500\n the mechanisms that carry information in the brain.\n\n54:02.500 --> 54:06.180\n So the usual way of articulating that is to say,\n\n54:06.180 --> 54:08.540\n what really matters is a rate code.\n\n54:08.540 --> 54:13.540\n What matters is how quickly is an individual neuron spiking?\n\n54:14.580 --> 54:16.380\n You know, what's the frequency at which it's spiking?\n\n54:16.380 --> 54:17.200\n Is it right?\n\n54:17.200 --> 54:18.040\n So the timing of the spike.\n\n54:18.040 --> 54:20.340\n Yeah, is it firing fast or slow?\n\n54:20.340 --> 54:22.740\n Let's, you know, let's put a number on that.\n\n54:22.740 --> 54:24.380\n And that number is enough to capture\n\n54:24.380 --> 54:26.140\n what neurons are doing.\n\n54:26.140 --> 54:30.620\n There's, you know, there's still uncertainty\n\n54:30.620 --> 54:34.500\n about whether that's an adequate description\n\n54:34.500 --> 54:39.500\n of how information is transmitted within the brain.\n\n54:39.880 --> 54:42.820\n There, you know, there are studies that suggest\n\n54:42.820 --> 54:46.060\n that the precise timing of spikes matters.\n\n54:46.060 --> 54:50.660\n There are studies that suggest that there are computations\n\n54:50.660 --> 54:54.520\n that go on within the dendritic tree, within a neuron,\n\n54:54.520 --> 54:57.100\n that are quite rich and structured\n\n54:57.100 --> 54:59.980\n and that really don't equate to anything that we're doing\n\n54:59.980 --> 55:01.740\n in our artificial neural networks.\n\n55:02.820 --> 55:05.360\n Having said that, I feel like we can get,\n\n55:05.360 --> 55:08.260\n I feel like we're getting somewhere\n\n55:08.260 --> 55:11.620\n by sticking to this high level of abstraction.\n\n55:11.620 --> 55:13.380\n Just the rate, and by the way,\n\n55:13.380 --> 55:16.220\n we're talking about the electrical signal.\n\n55:16.220 --> 55:20.060\n I remember reading some vague paper somewhere recently\n\n55:20.060 --> 55:23.420\n where the mechanical signal, like the vibrations\n\n55:23.420 --> 55:28.420\n or something of the neurons, also communicates information.\n\n55:28.820 --> 55:30.260\n I haven't seen that, but.\n\n55:30.260 --> 55:32.100\n There's somebody who was arguing\n\n55:32.100 --> 55:36.840\n that the electrical signal, this is in a nature paper,\n\n55:36.840 --> 55:38.780\n something like that, where the electrical signal\n\n55:38.780 --> 55:43.740\n is actually a side effect of the mechanical signal.\n\n55:43.740 --> 55:46.100\n But I don't think that changes the story.\n\n55:46.100 --> 55:49.060\n But it's almost an interesting idea\n\n55:49.060 --> 55:52.420\n that there could be a deeper, it's always like in physics\n\n55:52.420 --> 55:55.740\n with quantum mechanics, there's always a deeper story\n\n55:55.740 --> 55:57.500\n that could be underlying the whole thing.\n\n55:57.500 --> 56:00.540\n But you think it's basically the rate of spiking\n\n56:00.540 --> 56:02.820\n that gets us, that's like the lowest hanging fruit\n\n56:02.820 --> 56:04.060\n that can get us really far.\n\n56:04.060 --> 56:06.580\n This is a classical view.\n\n56:06.580 --> 56:10.700\n I mean, this is not, the only way in which this stance\n\n56:10.700 --> 56:13.580\n would be controversial is in the sense\n\n56:13.580 --> 56:17.100\n that there are members of the neuroscience community\n\n56:17.100 --> 56:18.820\n who are interested in alternatives.\n\n56:18.820 --> 56:21.400\n But this is really a very mainstream view.\n\n56:21.400 --> 56:22.940\n The way that neurons communicate\n\n56:22.940 --> 56:27.460\n is that neurotransmitters arrive,\n\n56:30.180 --> 56:34.500\n they wash up on a neuron, the neuron has receptors\n\n56:34.500 --> 56:39.040\n for those transmitters, the meeting of the transmitter\n\n56:39.040 --> 56:42.340\n with these receptors changes the voltage of the neuron.\n\n56:42.340 --> 56:46.860\n And if enough voltage change occurs, then a spike occurs,\n\n56:46.860 --> 56:48.660\n one of these like discrete events.\n\n56:48.660 --> 56:52.300\n And it's that spike that is conducted down the axon\n\n56:52.300 --> 56:54.580\n and leads to neurotransmitter release.\n\n56:54.580 --> 56:56.860\n This is just like neuroscience 101.\n\n56:56.860 --> 56:59.300\n This is like the way the brain is supposed to work.\n\n56:59.300 --> 57:03.660\n Now, what we do when we build artificial neural networks\n\n57:03.660 --> 57:06.780\n of the kind that are now popular in the AI community\n\n57:08.060 --> 57:11.780\n is that we don't worry about those individual spikes.\n\n57:11.780 --> 57:14.220\n We just worry about the frequency\n\n57:14.220 --> 57:16.980\n at which those spikes are being generated.\n\n57:16.980 --> 57:21.980\n And people talk about that as the activity of a neuron.\n\n57:22.340 --> 57:27.180\n And so the activity of units in a deep learning system\n\n57:27.180 --> 57:32.180\n is broadly analogous to the spike rate of a neuron.\n\n57:32.900 --> 57:37.900\n There are people who believe that there are other forms\n\n57:38.020 --> 57:39.180\n of communication in the brain.\n\n57:39.180 --> 57:41.260\n In fact, I've been involved in some research recently\n\n57:41.260 --> 57:46.260\n that suggests that the voltage fluctuations\n\n57:46.260 --> 57:49.260\n that occur in populations of neurons\n\n57:49.260 --> 57:54.260\n that are sort of below the level of spike production\n\n57:54.860 --> 57:57.220\n may be important for communication.\n\n57:57.220 --> 58:00.220\n But I'm still pretty old school in the sense\n\n58:00.220 --> 58:02.700\n that I think that the things that we're building\n\n58:02.700 --> 58:06.980\n in AI research constitute reasonable models\n\n58:06.980 --> 58:08.260\n of how a brain would work.\n\n58:10.300 --> 58:14.220\n Let me ask just for fun a crazy question, because I can.\n\n58:14.220 --> 58:17.020\n Do you think it's possible we're completely wrong\n\n58:17.020 --> 58:20.060\n about the way this basic mechanism\n\n58:20.060 --> 58:23.700\n of neuronal communication, that the information\n\n58:23.700 --> 58:26.340\n is stored in some very different kind of way in the brain?\n\n58:26.340 --> 58:27.580\n Oh, heck yes.\n\n58:27.580 --> 58:29.900\n I mean, look, I wouldn't be a scientist\n\n58:29.900 --> 58:32.500\n if I didn't think there was any chance we were wrong.\n\n58:32.500 --> 58:36.420\n But I mean, if you look at the history\n\n58:36.420 --> 58:39.900\n of deep learning research as it's been applied\n\n58:39.900 --> 58:42.620\n to neuroscience, of course the vast majority\n\n58:42.620 --> 58:45.380\n of deep learning research these days isn't about neuroscience.\n\n58:45.380 --> 58:49.060\n But if you go back to the 1980s,\n\n58:49.060 --> 58:52.740\n there's sort of an unbroken chain of research\n\n58:52.740 --> 58:54.940\n in which a particular strategy is taken,\n\n58:54.940 --> 58:59.940\n which is, hey, let's train a deep learning system.\n\n59:00.180 --> 59:04.060\n Let's train a multi layer neural network\n\n59:04.060 --> 59:09.060\n on this task that we trained our rat on,\n\n59:09.260 --> 59:12.300\n or our monkey on, or this human being on.\n\n59:12.300 --> 59:15.700\n And then let's look at what the units\n\n59:15.700 --> 59:17.700\n deep in the system are doing.\n\n59:17.700 --> 59:20.780\n And let's ask whether what they're doing\n\n59:20.780 --> 59:23.260\n resembles what we know about what neurons\n\n59:23.260 --> 59:24.620\n deep in the brain are doing.\n\n59:24.620 --> 59:28.540\n And over and over and over and over,\n\n59:28.540 --> 59:31.140\n that strategy works in the sense that\n\n59:32.020 --> 59:34.340\n the learning algorithms that we have access to,\n\n59:34.340 --> 59:37.740\n which typically center on back propagation,\n\n59:37.740 --> 59:42.060\n they give rise to patterns of activity,\n\n59:42.060 --> 59:44.100\n patterns of response,\n\n59:45.220 --> 59:48.740\n patterns of neuronal behavior in these artificial models\n\n59:48.740 --> 59:53.660\n that look hauntingly similar to what you see in the brain.\n\n59:53.660 --> 59:57.380\n And is that a coincidence?\n\n59:57.380 --> 1:00:00.780\n At a certain point, it starts looking like such coincidence\n\n1:00:00.780 --> 1:00:03.340\n is unlikely to not be deeply meaningful, yeah.\n\n1:00:03.340 --> 1:00:07.140\n Yeah, the circumstantial evidence is overwhelming.\n\n1:00:07.140 --> 1:00:07.980\n But it could be.\n\n1:00:07.980 --> 1:00:10.460\n But you're always open to total flipping at the table.\n\n1:00:10.460 --> 1:00:11.620\n Hey, of course.\n\n1:00:11.620 --> 1:00:15.140\n So you have coauthored several recent papers\n\n1:00:15.140 --> 1:00:17.860\n that sort of weave beautifully between the world\n\n1:00:17.860 --> 1:00:20.660\n of neuroscience and artificial intelligence.\n\n1:00:20.660 --> 1:00:25.660\n And maybe if we could, can we just try to dance around\n\n1:00:26.380 --> 1:00:27.500\n and talk about some of them?\n\n1:00:27.500 --> 1:00:29.740\n Maybe try to pick out interesting ideas\n\n1:00:29.740 --> 1:00:32.300\n that jump to your mind from memory.\n\n1:00:32.300 --> 1:00:34.300\n So maybe looking at, we were talking about\n\n1:00:34.300 --> 1:00:38.220\n the prefrontal cortex, the 2018, I believe, paper\n\n1:00:38.220 --> 1:00:40.060\n called the Prefrontal Cortex\n\n1:00:40.060 --> 1:00:42.140\n as a Meta Reinforcement Learning System.\n\n1:00:42.140 --> 1:00:44.340\n What, is there a key idea\n\n1:00:44.340 --> 1:00:46.700\n that you can speak to from that paper?\n\n1:00:47.660 --> 1:00:52.660\n Yeah, I mean, the key idea is about meta learning.\n\n1:00:53.860 --> 1:00:54.860\n What is meta learning?\n\n1:00:54.860 --> 1:00:58.620\n Meta learning is, by definition,\n\n1:01:00.940 --> 1:01:04.900\n a situation in which you have a learning algorithm\n\n1:01:06.100 --> 1:01:09.780\n and the learning algorithm operates in such a way\n\n1:01:09.780 --> 1:01:14.060\n that it gives rise to another learning algorithm.\n\n1:01:14.060 --> 1:01:17.140\n In the earliest applications of this idea,\n\n1:01:17.140 --> 1:01:20.340\n you had one learning algorithm sort of adjusting\n\n1:01:20.340 --> 1:01:23.060\n the parameters on another learning algorithm.\n\n1:01:23.060 --> 1:01:25.100\n But the case that we're interested in this paper\n\n1:01:25.100 --> 1:01:29.140\n is one where you start with just one learning algorithm\n\n1:01:29.140 --> 1:01:33.020\n and then another learning algorithm kind of emerges\n\n1:01:33.020 --> 1:01:35.180\n out of thin air.\n\n1:01:35.180 --> 1:01:36.700\n I can say more about what I mean by that.\n\n1:01:36.700 --> 1:01:39.780\n I don't mean to be scurrentist,\n\n1:01:39.780 --> 1:01:44.140\n but that's the idea of meta learning.\n\n1:01:44.140 --> 1:01:46.020\n It relates to the old idea in psychology\n\n1:01:46.020 --> 1:01:47.300\n of learning to learn.\n\n1:01:49.380 --> 1:01:54.300\n Situations where you have experiences\n\n1:01:54.300 --> 1:01:57.980\n that make you better at learning something new.\n\n1:01:57.980 --> 1:02:01.380\n A familiar example would be learning a foreign language.\n\n1:02:01.380 --> 1:02:02.860\n The first time you learn a foreign language,\n\n1:02:02.860 --> 1:02:06.420\n it may be quite laborious and disorienting\n\n1:02:06.420 --> 1:02:10.300\n and novel, but let's say you've learned\n\n1:02:10.300 --> 1:02:12.220\n two foreign languages.\n\n1:02:12.220 --> 1:02:14.140\n The third foreign language, obviously,\n\n1:02:14.140 --> 1:02:15.940\n is gonna be much easier to pick up.\n\n1:02:15.940 --> 1:02:16.780\n And why?\n\n1:02:16.780 --> 1:02:18.220\n Because you've learned how to learn.\n\n1:02:18.220 --> 1:02:20.220\n You know how this goes.\n\n1:02:20.220 --> 1:02:22.140\n You know, okay, I'm gonna have to learn how to conjugate.\n\n1:02:22.140 --> 1:02:23.940\n I'm gonna have to...\n\n1:02:23.940 --> 1:02:26.340\n That's a simple form of meta learning\n\n1:02:26.340 --> 1:02:30.260\n in the sense that there's some slow learning mechanism\n\n1:02:30.260 --> 1:02:33.020\n that's helping you kind of update\n\n1:02:33.020 --> 1:02:34.300\n your fast learning mechanism.\n\n1:02:34.300 --> 1:02:35.660\n Does that make sense?\n\n1:02:35.660 --> 1:02:40.540\n So how from our understanding from the psychology world,\n\n1:02:40.540 --> 1:02:43.180\n from neuroscience, our understanding\n\n1:02:43.180 --> 1:02:47.180\n how meta learning might work in the human brain,\n\n1:02:47.180 --> 1:02:49.980\n what lessons can we draw from that\n\n1:02:49.980 --> 1:02:53.060\n that we can bring into the artificial intelligence world?\n\n1:02:53.060 --> 1:02:55.980\n Well, yeah, so the origin of that paper\n\n1:02:55.980 --> 1:03:00.180\n was in AI work that we were doing in my group.\n\n1:03:00.180 --> 1:03:03.700\n We were looking at what happens\n\n1:03:03.700 --> 1:03:06.260\n when you train a recurrent neural network\n\n1:03:06.260 --> 1:03:10.180\n using standard reinforcement learning algorithms.\n\n1:03:10.180 --> 1:03:12.660\n But you train that network, not just in one task,\n\n1:03:12.660 --> 1:03:15.140\n but you train it in a bunch of interrelated tasks.\n\n1:03:15.140 --> 1:03:18.700\n And then you ask what happens when you give it\n\n1:03:18.700 --> 1:03:23.380\n yet another task in that sort of line of interrelated tasks.\n\n1:03:23.380 --> 1:03:27.500\n And what we started to realize is that\n\n1:03:29.380 --> 1:03:31.860\n a form of meta learning spontaneously happens\n\n1:03:31.860 --> 1:03:33.780\n in recurrent neural networks.\n\n1:03:33.780 --> 1:03:37.700\n And the simplest way to explain it is to say\n\n1:03:39.540 --> 1:03:43.500\n a recurrent neural network has a kind of memory\n\n1:03:43.500 --> 1:03:45.340\n in its activation patterns.\n\n1:03:45.340 --> 1:03:47.540\n It's recurrent by definition in the sense\n\n1:03:47.540 --> 1:03:50.180\n that you have units that connect to other units,\n\n1:03:50.180 --> 1:03:51.060\n that connect to other units.\n\n1:03:51.060 --> 1:03:53.660\n So you have sort of loops of connectivity,\n\n1:03:53.660 --> 1:03:55.740\n which allows activity to stick around\n\n1:03:55.740 --> 1:03:57.380\n and be updated over time.\n\n1:03:57.380 --> 1:03:59.020\n In psychology we call, in neuroscience\n\n1:03:59.020 --> 1:04:00.100\n we call this working memory.\n\n1:04:00.100 --> 1:04:03.020\n It's like actively holding something in mind.\n\n1:04:04.260 --> 1:04:09.260\n And so that memory gives\n\n1:04:09.260 --> 1:04:13.100\n the recurrent neural network a dynamics, right?\n\n1:04:13.100 --> 1:04:17.700\n The way that the activity pattern evolves over time\n\n1:04:17.700 --> 1:04:19.980\n is inherent to the connectivity\n\n1:04:19.980 --> 1:04:21.580\n of the recurrent neural network, okay?\n\n1:04:21.580 --> 1:04:23.500\n So that's idea number one.\n\n1:04:23.500 --> 1:04:26.020\n Now, the dynamics of that network are shaped\n\n1:04:26.020 --> 1:04:29.660\n by the connectivity, by the synaptic weights.\n\n1:04:29.660 --> 1:04:31.660\n And those synaptic weights are being shaped\n\n1:04:31.660 --> 1:04:33.860\n by this reinforcement learning algorithm\n\n1:04:33.860 --> 1:04:36.020\n that you're training the network with.\n\n1:04:37.700 --> 1:04:41.260\n So the punchline is if you train a recurrent neural network\n\n1:04:41.260 --> 1:04:43.140\n with a reinforcement learning algorithm\n\n1:04:43.140 --> 1:04:44.180\n that's adjusting its weights,\n\n1:04:44.180 --> 1:04:45.940\n and you do that for long enough,\n\n1:04:47.060 --> 1:04:50.860\n the activation dynamics will become very interesting, right?\n\n1:04:50.860 --> 1:04:53.180\n So imagine I give you a task\n\n1:04:53.180 --> 1:04:56.060\n where you have to press one button or another,\n\n1:04:56.060 --> 1:04:57.580\n left button or right button.\n\n1:04:57.580 --> 1:05:00.820\n And there's some probability\n\n1:05:00.820 --> 1:05:02.260\n that I'm gonna give you an M&M\n\n1:05:02.260 --> 1:05:04.220\n if you press the left button,\n\n1:05:04.220 --> 1:05:06.220\n and there's some probability I'll give you an M&M\n\n1:05:06.220 --> 1:05:07.620\n if you press the other button.\n\n1:05:07.620 --> 1:05:09.340\n And you have to figure out what those probabilities are\n\n1:05:09.340 --> 1:05:10.700\n just by trying things out.\n\n1:05:12.060 --> 1:05:13.780\n But as I said before,\n\n1:05:13.780 --> 1:05:15.500\n instead of just giving you one of these tasks,\n\n1:05:15.500 --> 1:05:17.020\n I give you a whole sequence.\n\n1:05:17.020 --> 1:05:18.700\n You know, I give you two buttons\n\n1:05:18.700 --> 1:05:19.860\n and you figure out which one's best.\n\n1:05:19.860 --> 1:05:22.180\n And I go, good job, here's a new box.\n\n1:05:22.180 --> 1:05:24.100\n Two new buttons, you have to figure out which one's best.\n\n1:05:24.100 --> 1:05:25.420\n Good job, here's a new box.\n\n1:05:25.420 --> 1:05:27.340\n And every box has its own probabilities\n\n1:05:27.340 --> 1:05:28.300\n and you have to figure it out.\n\n1:05:28.300 --> 1:05:30.420\n So if you train a recurrent neural network\n\n1:05:30.420 --> 1:05:32.660\n on that kind of sequence of tasks,\n\n1:05:33.700 --> 1:05:37.380\n what happens, it seemed almost magical to us\n\n1:05:37.380 --> 1:05:41.180\n when we first started kind of realizing what was going on.\n\n1:05:41.180 --> 1:05:43.620\n The slow learning algorithm that's adjusting\n\n1:05:43.620 --> 1:05:45.540\n the synaptic weights,\n\n1:05:46.980 --> 1:05:51.380\n those slow synaptic changes give rise to a network dynamics\n\n1:05:51.380 --> 1:05:53.020\n that themselves, that, you know,\n\n1:05:53.020 --> 1:05:56.860\n the dynamics themselves turn into a learning algorithm.\n\n1:05:56.860 --> 1:05:59.060\n So in other words, you can tell this is happening\n\n1:05:59.060 --> 1:06:01.020\n by just freezing the synaptic weights saying,\n\n1:06:01.020 --> 1:06:03.460\n okay, no more learning, you're done.\n\n1:06:03.460 --> 1:06:07.620\n Here's a new box, figure out which button is best.\n\n1:06:07.620 --> 1:06:09.620\n And the recurrent neural network will do this just fine.\n\n1:06:09.620 --> 1:06:13.060\n There's no, like it figures out which button is best.\n\n1:06:13.060 --> 1:06:16.700\n It kind of transitions from exploring the two buttons\n\n1:06:16.700 --> 1:06:18.380\n to just pressing the one that it likes best\n\n1:06:18.380 --> 1:06:20.700\n in a very rational way.\n\n1:06:20.700 --> 1:06:21.660\n How is that happening?\n\n1:06:21.660 --> 1:06:24.700\n It's happening because the activity dynamics\n\n1:06:24.700 --> 1:06:28.460\n of the network have been shaped by the slow learning process\n\n1:06:28.460 --> 1:06:30.660\n that's occurred over many, many boxes.\n\n1:06:30.660 --> 1:06:34.660\n And so what's happened is that this slow learning algorithm\n\n1:06:34.660 --> 1:06:37.140\n that's slowly adjusting the weights\n\n1:06:37.140 --> 1:06:39.740\n is changing the dynamics of the network,\n\n1:06:39.740 --> 1:06:43.460\n the activity dynamics into its own learning algorithm.\n\n1:06:43.460 --> 1:06:48.460\n And as we were kind of realizing that this is a thing,\n\n1:06:51.340 --> 1:06:53.740\n it just so happened that the group that was working on this\n\n1:06:53.740 --> 1:06:56.020\n included a bunch of neuroscientists\n\n1:06:56.020 --> 1:06:59.900\n and it started kind of ringing a bell for us,\n\n1:06:59.900 --> 1:07:02.860\n which is to say that we thought this sounds a lot\n\n1:07:02.860 --> 1:07:06.180\n like the distinction between synaptic learning\n\n1:07:06.180 --> 1:07:08.460\n and activity, synaptic memory\n\n1:07:08.460 --> 1:07:10.460\n and activity based memory in the brain.\n\n1:07:11.700 --> 1:07:15.900\n And it also reminded us of recurrent connectivity\n\n1:07:15.900 --> 1:07:18.420\n that's very characteristic of prefrontal function.\n\n1:07:19.620 --> 1:07:22.820\n So this is kind of why it's good to have people working\n\n1:07:22.820 --> 1:07:26.180\n on AI that know a little bit about neuroscience\n\n1:07:26.180 --> 1:07:29.340\n and vice versa, because we started thinking\n\n1:07:29.340 --> 1:07:32.340\n about whether we could apply this principle to neuroscience.\n\n1:07:32.340 --> 1:07:33.660\n And that's where the paper came from.\n\n1:07:33.660 --> 1:07:37.540\n So the kind of principle of the recurrence\n\n1:07:37.540 --> 1:07:39.540\n they can see in the prefrontal cortex,\n\n1:07:39.540 --> 1:07:43.660\n then you start to realize that it's possible\n\n1:07:43.660 --> 1:07:46.340\n for something like an idea of a learning\n\n1:07:46.340 --> 1:07:50.860\n to learn emerging from this learning process\n\n1:07:50.860 --> 1:07:54.500\n as long as you keep varying the environment sufficiently.\n\n1:07:54.500 --> 1:07:59.300\n Exactly, so the kind of metaphorical transition\n\n1:07:59.300 --> 1:08:00.740\n we made to neuroscience was to think,\n\n1:08:00.740 --> 1:08:03.660\n okay, well, we know that the prefrontal cortex\n\n1:08:03.660 --> 1:08:04.940\n is highly recurrent.\n\n1:08:04.940 --> 1:08:08.500\n We know that it's an important locus for working memory\n\n1:08:08.500 --> 1:08:11.260\n for activation based memory.\n\n1:08:11.260 --> 1:08:13.660\n So maybe the prefrontal cortex\n\n1:08:13.660 --> 1:08:15.620\n supports reinforcement learning.\n\n1:08:15.620 --> 1:08:19.260\n In other words, what is reinforcement learning?\n\n1:08:19.260 --> 1:08:21.620\n You take an action, you see how much reward you got,\n\n1:08:21.620 --> 1:08:23.580\n you update your policy of behavior.\n\n1:08:24.580 --> 1:08:26.860\n Maybe the prefrontal cortex is doing that sort of thing\n\n1:08:26.860 --> 1:08:28.500\n strictly in its activation patterns.\n\n1:08:28.500 --> 1:08:31.900\n It's keeping around a memory in its activity patterns\n\n1:08:31.900 --> 1:08:35.340\n of what you did, how much reward you got,\n\n1:08:35.340 --> 1:08:38.980\n and it's using that activity based memory\n\n1:08:38.980 --> 1:08:41.100\n as a basis for updating behavior.\n\n1:08:41.100 --> 1:08:42.180\n But then the question is, well,\n\n1:08:42.180 --> 1:08:44.540\n how did the prefrontal cortex get so smart?\n\n1:08:44.540 --> 1:08:48.020\n In other words, where did these activity dynamics come from?\n\n1:08:48.020 --> 1:08:50.780\n How did that program that's implemented\n\n1:08:50.780 --> 1:08:54.460\n in the recurrent dynamics of the prefrontal cortex arise?\n\n1:08:54.460 --> 1:08:58.060\n And one answer that became evident in this work was,\n\n1:08:58.060 --> 1:09:00.940\n well, maybe the mechanisms that operate\n\n1:09:00.940 --> 1:09:05.020\n on the synaptic level, which we believe are mediated\n\n1:09:05.020 --> 1:09:08.740\n by dopamine, are responsible for shaping those dynamics.\n\n1:09:10.180 --> 1:09:12.420\n So this may be a silly question,\n\n1:09:12.420 --> 1:09:17.420\n but because this kind of several temporal sort of classes\n\n1:09:19.340 --> 1:09:23.020\n of learning are happening and the learning to learnism\n\n1:09:23.020 --> 1:09:28.020\n emerges, can you keep building stacks of learning\n\n1:09:28.660 --> 1:09:30.940\n to learn to learn, learning to learn to learn\n\n1:09:30.940 --> 1:09:32.900\n to learn to learn because it keeps,\n\n1:09:32.900 --> 1:09:37.020\n I mean, basically abstractions of more powerful abilities\n\n1:09:37.020 --> 1:09:41.140\n to generalize of learning complex rules.\n\n1:09:41.140 --> 1:09:46.100\n Yeah, that's overstretching this kind of mechanism.\n\n1:09:46.100 --> 1:09:51.100\n Well, one of the people in AI who started thinking\n\n1:09:51.260 --> 1:09:54.700\n about meta learning from very early on,\n\n1:09:54.700 --> 1:09:59.700\n J\u00fcrgen Schmidhuber sort of cheekily suggested,\n\n1:09:59.780 --> 1:10:03.900\n I think it may have been in his PhD thesis,\n\n1:10:03.900 --> 1:10:06.900\n that we should think about meta, meta, meta,\n\n1:10:06.900 --> 1:10:08.740\n meta, meta, meta learning.\n\n1:10:08.740 --> 1:10:13.140\n That's really what's gonna get us to true intelligence.\n\n1:10:13.140 --> 1:10:15.380\n Certainly there's a poetic aspect to it\n\n1:10:15.380 --> 1:10:19.260\n and it seems interesting and correct\n\n1:10:19.260 --> 1:10:21.660\n that that kind of levels of abstraction would be powerful,\n\n1:10:21.660 --> 1:10:23.940\n but is that something you see in the brain?\n\n1:10:23.940 --> 1:10:27.780\n This kind of, is it useful to think of learning\n\n1:10:27.780 --> 1:10:32.100\n in these meta, meta, meta way or is it just meta learning?\n\n1:10:32.100 --> 1:10:35.300\n Well, one thing that really fascinated me\n\n1:10:35.300 --> 1:10:39.020\n about this mechanism that we were starting to look at,\n\n1:10:39.020 --> 1:10:41.100\n and other groups started talking\n\n1:10:41.100 --> 1:10:44.740\n about very similar things at the same time.\n\n1:10:44.740 --> 1:10:47.020\n And then a kind of explosion of interest\n\n1:10:47.020 --> 1:10:48.980\n in meta learning happened in the AI community\n\n1:10:48.980 --> 1:10:50.580\n shortly after that.\n\n1:10:50.580 --> 1:10:52.060\n I don't know if we had anything to do with that,\n\n1:10:52.060 --> 1:10:55.620\n but I was gratified to see that a lot of people\n\n1:10:55.620 --> 1:10:57.780\n started talking about meta learning.\n\n1:10:57.780 --> 1:11:01.380\n One of the things that I liked about the kind of flavor\n\n1:11:01.380 --> 1:11:04.060\n of meta learning that we were studying was that\n\n1:11:04.060 --> 1:11:05.940\n it didn't require anything special.\n\n1:11:05.940 --> 1:11:08.620\n It was just, if you took a system that had\n\n1:11:08.620 --> 1:11:12.460\n some form of memory that the function of which\n\n1:11:12.460 --> 1:11:16.860\n could be shaped by pick URL algorithm,\n\n1:11:16.860 --> 1:11:19.100\n then this would just happen, right?\n\n1:11:19.100 --> 1:11:21.300\n I mean, there are a lot of forms of,\n\n1:11:21.300 --> 1:11:23.180\n there are a lot of meta learning algorithms\n\n1:11:23.180 --> 1:11:24.500\n that have been proposed since then\n\n1:11:24.500 --> 1:11:26.580\n that are fascinating and effective\n\n1:11:26.580 --> 1:11:29.780\n in their domains of application.\n\n1:11:29.780 --> 1:11:32.580\n But they're engineered, they're things that somebody\n\n1:11:32.580 --> 1:11:34.340\n had to say, well, gee, if we wanted meta learning\n\n1:11:34.340 --> 1:11:35.700\n to happen, how would we do that?\n\n1:11:35.700 --> 1:11:37.060\n Here's an algorithm that would,\n\n1:11:37.060 --> 1:11:39.500\n but there's something about the kind of meta learning\n\n1:11:39.500 --> 1:11:42.540\n that we were studying that seemed to me special\n\n1:11:42.540 --> 1:11:44.980\n in the sense that it wasn't an algorithm.\n\n1:11:44.980 --> 1:11:48.740\n It was just something that automatically happened\n\n1:11:48.740 --> 1:11:51.060\n if you had a system that had memory\n\n1:11:51.060 --> 1:11:54.020\n and it was trained with a reinforcement learning algorithm.\n\n1:11:54.020 --> 1:11:59.020\n And in that sense, it can be as meta as it wants to be.\n\n1:11:59.020 --> 1:12:04.020\n There's no limit on how abstract the meta learning can get\n\n1:12:04.700 --> 1:12:07.980\n because it's not reliant on a human engineering\n\n1:12:07.980 --> 1:12:11.540\n a particular meta learning algorithm to get there.\n\n1:12:11.540 --> 1:12:15.140\n And that's, I also, I don't know,\n\n1:12:15.140 --> 1:12:17.820\n I guess I hope that that's relevant in the brain.\n\n1:12:17.820 --> 1:12:19.180\n I think there's a kind of beauty\n\n1:12:19.180 --> 1:12:23.380\n in the ability of this emergent.\n\n1:12:23.380 --> 1:12:26.460\n The emergent aspect of it, as opposed to engineered.\n\n1:12:26.460 --> 1:12:29.020\n Exactly, it's something that just, it just happens\n\n1:12:29.020 --> 1:12:33.620\n in a sense, in a sense, you can't avoid this happening.\n\n1:12:33.620 --> 1:12:35.820\n If you have a system that has memory\n\n1:12:35.820 --> 1:12:39.660\n and the function of that memory is shaped\n\n1:12:39.660 --> 1:12:42.740\n by reinforcement learning, and this system is trained\n\n1:12:42.740 --> 1:12:46.900\n in a series of interrelated tasks, this is gonna happen.\n\n1:12:46.900 --> 1:12:48.460\n You can't stop it.\n\n1:12:48.460 --> 1:12:50.140\n As long as you have certain properties,\n\n1:12:50.140 --> 1:12:52.540\n maybe like a recurrent structure to.\n\n1:12:52.540 --> 1:12:53.380\n You have to have memory.\n\n1:12:53.380 --> 1:12:55.220\n It actually doesn't have to be a recurrent neural network.\n\n1:12:55.220 --> 1:12:58.740\n One of, a paper that I was honored to be involved\n\n1:12:58.740 --> 1:13:02.260\n with even earlier, used a kind of slot based memory.\n\n1:13:02.260 --> 1:13:03.100\n Do you remember the title?\n\n1:13:03.100 --> 1:13:05.060\n Just for people to understand.\n\n1:13:05.060 --> 1:13:08.140\n It was Memory Augmented Neural Networks.\n\n1:13:08.140 --> 1:13:10.180\n I think it was, I think the title was\n\n1:13:10.180 --> 1:13:13.040\n Meta Learning in Memory Augmented Neural Networks.\n\n1:13:14.660 --> 1:13:17.940\n And it was the same exact story.\n\n1:13:17.940 --> 1:13:21.100\n If you have a system with memory,\n\n1:13:21.100 --> 1:13:22.780\n here it was a different kind of memory,\n\n1:13:22.780 --> 1:13:26.860\n but the function of that memory is shaped\n\n1:13:26.860 --> 1:13:28.600\n by reinforcement learning.\n\n1:13:29.900 --> 1:13:34.300\n Here it was the reads and writes that occurred\n\n1:13:34.300 --> 1:13:36.420\n on this slot based memory.\n\n1:13:36.420 --> 1:13:38.080\n This will just happen.\n\n1:13:39.940 --> 1:13:42.060\n But this brings us back to something I was saying earlier\n\n1:13:42.060 --> 1:13:44.500\n about the importance of the environment.\n\n1:13:46.340 --> 1:13:49.940\n This will happen if the system is being trained\n\n1:13:49.940 --> 1:13:53.060\n in a setting where there's like a sequence of tasks\n\n1:13:53.060 --> 1:13:55.240\n that all share some abstract structure.\n\n1:13:56.100 --> 1:13:59.020\n Sometimes we talk about task distributions.\n\n1:13:59.020 --> 1:14:04.020\n And that's something that's very obviously true\n\n1:14:04.180 --> 1:14:06.360\n of the world that humans inhabit.\n\n1:14:09.500 --> 1:14:13.140\n Like if you just kind of think about what you do every day,\n\n1:14:13.140 --> 1:14:16.280\n you never do exactly the same thing\n\n1:14:16.280 --> 1:14:17.640\n that you did the day before.\n\n1:14:17.640 --> 1:14:21.060\n But everything that you do sort of has a family resemblance.\n\n1:14:21.060 --> 1:14:23.500\n It shares a structure with something that you did before.\n\n1:14:23.500 --> 1:14:26.700\n And so the real world is sort of\n\n1:14:29.260 --> 1:14:32.700\n saturated with this kind of, this property.\n\n1:14:32.700 --> 1:14:37.540\n It's endless variety with endless redundancy.\n\n1:14:37.540 --> 1:14:38.700\n And that's the setting in which\n\n1:14:38.700 --> 1:14:40.540\n this kind of meta learning happens.\n\n1:14:40.540 --> 1:14:44.980\n And it does seem like we're just so good at finding,\n\n1:14:44.980 --> 1:14:47.820\n just like in this emergent phenomena you described,\n\n1:14:47.820 --> 1:14:50.020\n we're really good at finding that redundancy,\n\n1:14:50.020 --> 1:14:53.480\n finding those similarities, the family resemblance.\n\n1:14:53.480 --> 1:14:56.560\n Some people call it sort of, what is it?\n\n1:14:56.560 --> 1:14:59.180\n Melanie Mitchell was talking about analogies.\n\n1:14:59.180 --> 1:15:01.940\n So we're able to connect concepts together\n\n1:15:01.940 --> 1:15:03.860\n in this kind of way,\n\n1:15:03.860 --> 1:15:06.020\n in this same kind of automated emergent way,\n\n1:15:06.020 --> 1:15:08.620\n which there's so many echoes here\n\n1:15:08.620 --> 1:15:10.640\n of psychology and neuroscience.\n\n1:15:10.640 --> 1:15:15.300\n And obviously now with reinforcement learning\n\n1:15:15.300 --> 1:15:18.260\n with recurrent neural networks at the core.\n\n1:15:18.260 --> 1:15:20.180\n If we could talk a little bit about dopamine,\n\n1:15:20.180 --> 1:15:23.780\n you have really, you're a part of coauthoring\n\n1:15:23.780 --> 1:15:26.420\n really exciting recent paper, very recent,\n\n1:15:26.420 --> 1:15:28.900\n in terms of release on dopamine\n\n1:15:28.900 --> 1:15:31.040\n and temporal difference learning.\n\n1:15:31.040 --> 1:15:34.820\n Can you describe the key ideas of that paper?\n\n1:15:34.820 --> 1:15:35.660\n Sure, yeah.\n\n1:15:35.660 --> 1:15:37.740\n I mean, one thing I want to pause to do\n\n1:15:37.740 --> 1:15:39.460\n is acknowledge my coauthors\n\n1:15:39.460 --> 1:15:41.540\n on actually both of the papers we're talking about.\n\n1:15:41.540 --> 1:15:42.660\n So this dopamine paper.\n\n1:15:42.660 --> 1:15:45.700\n I'll just, I'll certainly post all their names.\n\n1:15:45.700 --> 1:15:46.540\n Okay, wonderful.\n\n1:15:46.540 --> 1:15:49.300\n Yeah, because I'm sort of abashed\n\n1:15:49.300 --> 1:15:51.000\n to be the spokesperson for these papers\n\n1:15:51.000 --> 1:15:55.180\n when I had such amazing collaborators on both.\n\n1:15:55.180 --> 1:15:56.980\n So it's a comfort to me to know\n\n1:15:56.980 --> 1:15:58.580\n that you'll acknowledge them.\n\n1:15:58.580 --> 1:16:00.420\n Yeah, there's an incredible team there, but yeah.\n\n1:16:00.420 --> 1:16:03.080\n Oh yeah, it's such a, it's so much fun.\n\n1:16:03.080 --> 1:16:06.360\n And in the case of the dopamine paper,\n\n1:16:06.360 --> 1:16:09.020\n we also collaborated with Naochit at Harvard,\n\n1:16:09.020 --> 1:16:11.180\n who, you know, obviously a paper simply\n\n1:16:11.180 --> 1:16:12.620\n wouldn't have happened without him.\n\n1:16:12.620 --> 1:16:17.540\n But so you were asking for like a thumbnail sketch of.\n\n1:16:17.540 --> 1:16:20.820\n Yeah, thumbnail sketch or key ideas or, you know,\n\n1:16:20.820 --> 1:16:22.500\n things, the insights that are, you know,\n\n1:16:22.500 --> 1:16:24.780\n continuing on our kind of discussion here\n\n1:16:24.780 --> 1:16:26.900\n between neuroscience and AI.\n\n1:16:26.900 --> 1:16:28.860\n Yeah, I mean, this was another,\n\n1:16:28.860 --> 1:16:30.620\n a lot of the work that we've done so far\n\n1:16:30.620 --> 1:16:35.380\n is taking ideas that have bubbled up in AI\n\n1:16:35.380 --> 1:16:39.660\n and, you know, asking the question of whether the brain\n\n1:16:39.660 --> 1:16:41.460\n might be doing something related,\n\n1:16:41.460 --> 1:16:45.420\n which I think on the surface sounds like something\n\n1:16:45.420 --> 1:16:48.360\n that's really mainly of use to neuroscience.\n\n1:16:49.380 --> 1:16:53.600\n We see it also as a way of validating\n\n1:16:53.600 --> 1:16:55.320\n what we're doing on the AI side.\n\n1:16:55.320 --> 1:16:57.940\n If we can gain some evidence that the brain\n\n1:16:57.940 --> 1:17:01.760\n is using some technique that we've been trying out\n\n1:17:01.760 --> 1:17:05.500\n in our AI work, that gives us confidence\n\n1:17:05.500 --> 1:17:07.780\n that, you know, it may be a good idea,\n\n1:17:07.780 --> 1:17:11.560\n that it'll, you know, scale to rich, complex tasks,\n\n1:17:11.560 --> 1:17:14.840\n that it'll interface well with other mechanisms.\n\n1:17:14.840 --> 1:17:16.860\n So you see it as a two way road.\n\n1:17:16.860 --> 1:17:18.520\n Yeah, for sure. Just because a particular paper\n\n1:17:18.520 --> 1:17:21.140\n is a little bit focused on from one to the,\n\n1:17:21.140 --> 1:17:25.620\n from AI, from neural networks to neuroscience.\n\n1:17:25.620 --> 1:17:28.380\n Ultimately the discussion, the thinking,\n\n1:17:28.380 --> 1:17:30.840\n the productive longterm aspect of it\n\n1:17:30.840 --> 1:17:33.220\n is the two way road nature of the whole interaction.\n\n1:17:33.220 --> 1:17:36.260\n Yeah, I mean, we've talked about the notion\n\n1:17:36.260 --> 1:17:39.300\n of a virtuous circle between AI and neuroscience.\n\n1:17:39.300 --> 1:17:41.820\n And, you know, the way I see it,\n\n1:17:42.660 --> 1:17:47.460\n that's always been there since the two fields,\n\n1:17:47.460 --> 1:17:49.040\n you know, jointly existed.\n\n1:17:50.100 --> 1:17:52.140\n There have been some phases in that history\n\n1:17:52.140 --> 1:17:53.540\n when AI was sort of ahead.\n\n1:17:53.540 --> 1:17:56.340\n There are some phases when neuroscience was sort of ahead.\n\n1:17:56.340 --> 1:18:00.660\n I feel like given the burst of innovation\n\n1:18:00.660 --> 1:18:03.780\n that's happened recently on the AI side,\n\n1:18:03.780 --> 1:18:06.320\n AI is kind of ahead in the sense that\n\n1:18:06.320 --> 1:18:10.620\n there are all of these ideas that we, you know,\n\n1:18:10.620 --> 1:18:12.660\n for which it's exciting to consider\n\n1:18:12.660 --> 1:18:14.720\n that there might be neural analogs.\n\n1:18:16.100 --> 1:18:19.620\n And neuroscience, you know,\n\n1:18:19.620 --> 1:18:22.420\n in a sense has been focusing on approaches\n\n1:18:22.420 --> 1:18:24.860\n to studying behavior that come from, you know,\n\n1:18:24.860 --> 1:18:27.540\n that are kind of derived from this earlier era\n\n1:18:27.540 --> 1:18:29.620\n of cognitive psychology.\n\n1:18:29.620 --> 1:18:33.540\n And, you know, so in some ways fail to connect\n\n1:18:33.540 --> 1:18:36.700\n with some of the issues that we're grappling with in AI.\n\n1:18:36.700 --> 1:18:37.940\n Like how do we deal with, you know,\n\n1:18:37.940 --> 1:18:40.180\n large, you know, complex environments.\n\n1:18:41.560 --> 1:18:45.220\n But, you know, I think it's inevitable\n\n1:18:45.220 --> 1:18:47.920\n that this circle will keep turning\n\n1:18:47.920 --> 1:18:49.540\n and there will be a moment\n\n1:18:49.540 --> 1:18:51.300\n in the not too different distant future\n\n1:18:51.300 --> 1:18:54.640\n when neuroscience is pelting AI researchers\n\n1:18:54.640 --> 1:18:58.260\n with insights that may change the direction of our work.\n\n1:18:58.260 --> 1:19:00.940\n Just a quick human question.\n\n1:19:00.940 --> 1:19:05.460\n Is it, you have parts of your brain,\n\n1:19:05.460 --> 1:19:08.260\n this is very meta, but they're able to both think\n\n1:19:08.260 --> 1:19:10.300\n about neuroscience and AI.\n\n1:19:10.300 --> 1:19:14.220\n You know, I don't often meet people like that.\n\n1:19:14.220 --> 1:19:19.220\n So do you think, let me ask a meta plasticity question.\n\n1:19:19.780 --> 1:19:22.660\n Do you think a human being can be both good at AI\n\n1:19:22.660 --> 1:19:23.580\n and neuroscience?\n\n1:19:23.580 --> 1:19:26.500\n It's like what, on the team at DeepMind,\n\n1:19:26.500 --> 1:19:30.180\n what kind of human can occupy these two realms?\n\n1:19:30.180 --> 1:19:33.340\n And is that something you see everybody should be doing,\n\n1:19:33.340 --> 1:19:36.620\n can be doing, or is that a very special few\n\n1:19:36.620 --> 1:19:37.460\n can kind of jump?\n\n1:19:37.460 --> 1:19:39.180\n Just like we talk about art history,\n\n1:19:39.180 --> 1:19:41.020\n I would think it's a special person\n\n1:19:41.020 --> 1:19:43.620\n that can major in art history\n\n1:19:43.620 --> 1:19:46.860\n and also consider being a surgeon.\n\n1:19:46.860 --> 1:19:48.380\n Otherwise known as a dilettante.\n\n1:19:48.380 --> 1:19:50.140\n A dilettante, yeah.\n\n1:19:50.140 --> 1:19:52.100\n Easily distracted.\n\n1:19:52.100 --> 1:19:57.100\n No, I think it does take a special kind of person\n\n1:19:58.620 --> 1:20:02.660\n to be truly world class at both AI and neuroscience.\n\n1:20:02.660 --> 1:20:04.460\n And I am not on that list.\n\n1:20:05.940 --> 1:20:10.300\n I happen to be someone whose interest in neuroscience\n\n1:20:10.300 --> 1:20:15.300\n and psychology involved using the kinds\n\n1:20:15.940 --> 1:20:20.940\n of modeling techniques that are now very central in AI.\n\n1:20:20.940 --> 1:20:24.140\n And that sort of, I guess, bought me a ticket\n\n1:20:24.140 --> 1:20:26.500\n to be involved in all of the amazing things\n\n1:20:26.500 --> 1:20:29.500\n that are going on in AI research right now.\n\n1:20:29.500 --> 1:20:32.660\n I do know a few people who I would consider\n\n1:20:32.660 --> 1:20:34.780\n pretty expert on both fronts,\n\n1:20:34.780 --> 1:20:36.260\n and I won't embarrass them by naming them,\n\n1:20:36.260 --> 1:20:40.540\n but there are exceptional people out there\n\n1:20:40.540 --> 1:20:41.380\n who are like this.\n\n1:20:41.380 --> 1:20:45.900\n The one thing that I find is a barrier\n\n1:20:45.900 --> 1:20:49.300\n to being truly world class on both fronts\n\n1:20:49.300 --> 1:20:54.300\n is just the complexity of the technology\n\n1:20:54.980 --> 1:20:58.180\n that's involved in both disciplines now.\n\n1:20:58.180 --> 1:21:02.980\n So the engineering expertise that it takes\n\n1:21:02.980 --> 1:21:07.860\n to do truly frontline, hands on AI research\n\n1:21:07.860 --> 1:21:10.620\n is really, really considerable.\n\n1:21:10.620 --> 1:21:11.940\n The learning curve of the tools,\n\n1:21:11.940 --> 1:21:15.260\n just like the specifics of just whether it's programming\n\n1:21:15.260 --> 1:21:17.500\n or the kind of tools necessary to collect the data,\n\n1:21:17.500 --> 1:21:19.780\n to manage the data, to distribute, to compute,\n\n1:21:19.780 --> 1:21:20.780\n all that kind of stuff.\n\n1:21:20.780 --> 1:21:22.380\n And on the neuroscience, I guess, side,\n\n1:21:22.380 --> 1:21:24.580\n there'll be all different sets of tools.\n\n1:21:24.580 --> 1:21:26.820\n Exactly, especially with the recent explosion\n\n1:21:26.820 --> 1:21:28.980\n in neuroscience methods.\n\n1:21:28.980 --> 1:21:32.100\n So having said all that,\n\n1:21:32.100 --> 1:21:37.100\n I think the best scenario for both neuroscience\n\n1:21:39.860 --> 1:21:44.860\n and AI is to have people interacting\n\n1:21:44.860 --> 1:21:48.140\n who live at every point on this spectrum\n\n1:21:48.140 --> 1:21:51.900\n from exclusively focused on neuroscience\n\n1:21:51.900 --> 1:21:55.540\n to exclusively focused on the engineering side of AI.\n\n1:21:55.540 --> 1:22:00.540\n But to have those people inhabiting a community\n\n1:22:01.060 --> 1:22:03.740\n where they're talking to people who live elsewhere\n\n1:22:03.740 --> 1:22:04.820\n on the spectrum.\n\n1:22:04.820 --> 1:22:08.660\n And I may be someone who's very close to the center\n\n1:22:08.660 --> 1:22:12.180\n in the sense that I have one foot in the neuroscience world\n\n1:22:12.180 --> 1:22:14.020\n and one foot in the AI world,\n\n1:22:14.020 --> 1:22:17.220\n and that central position, I will admit,\n\n1:22:17.220 --> 1:22:19.060\n prevents me, at least someone\n\n1:22:19.060 --> 1:22:21.300\n with my limited cognitive capacity,\n\n1:22:21.300 --> 1:22:26.300\n from having true technical expertise in either domain.\n\n1:22:26.820 --> 1:22:30.140\n But at the same time, I at least hope\n\n1:22:30.140 --> 1:22:32.340\n that it's worthwhile having people around\n\n1:22:32.340 --> 1:22:34.980\n who can kind of see the connections.\n\n1:22:34.980 --> 1:22:39.100\n Yeah, the community, the emergent intelligence\n\n1:22:39.100 --> 1:22:43.300\n of the community when it's nicely distributed is useful.\n\n1:22:43.300 --> 1:22:44.580\n Exactly, yeah.\n\n1:22:44.580 --> 1:22:46.620\n So hopefully that, I mean, I've seen that work,\n\n1:22:46.620 --> 1:22:48.420\n I've seen that work out well at DeepMind.\n\n1:22:48.420 --> 1:22:52.860\n There are people who, I mean, even if you just focus\n\n1:22:52.860 --> 1:22:55.820\n on the AI work that happens at DeepMind,\n\n1:22:55.820 --> 1:22:59.540\n it's been a good thing to have some people around\n\n1:22:59.540 --> 1:23:03.260\n doing that kind of work whose PhDs are in neuroscience\n\n1:23:03.260 --> 1:23:04.780\n or psychology.\n\n1:23:04.780 --> 1:23:09.780\n Every academic discipline has its kind of blind spots\n\n1:23:09.780 --> 1:23:14.780\n and kind of unfortunate obsessions and its metaphors\n\n1:23:16.820 --> 1:23:18.260\n and its reference points,\n\n1:23:18.260 --> 1:23:23.260\n and having some intellectual diversity is really healthy.\n\n1:23:24.020 --> 1:23:28.420\n People get each other unstuck, I think.\n\n1:23:28.420 --> 1:23:30.620\n I see it all the time at DeepMind.\n\n1:23:30.620 --> 1:23:33.060\n And I like to think that the people\n\n1:23:33.060 --> 1:23:35.940\n who bring some neuroscience background to the table\n\n1:23:35.940 --> 1:23:37.460\n are helping with that.\n\n1:23:37.460 --> 1:23:41.420\n So one of my probably the deepest passion for me,\n\n1:23:41.420 --> 1:23:44.140\n what I would say, maybe we kind of spoke off mic\n\n1:23:44.140 --> 1:23:49.140\n a little bit about it, but that I think is a blind spot\n\n1:23:49.460 --> 1:23:51.380\n for at least robotics and AI folks\n\n1:23:51.380 --> 1:23:55.540\n is human robot interaction, human agent interaction.\n\n1:23:55.540 --> 1:24:00.540\n Maybe do you have thoughts about how we reduce the size\n\n1:24:01.860 --> 1:24:02.980\n of that blind spot?\n\n1:24:02.980 --> 1:24:07.460\n Do you also share the feeling that not enough folks\n\n1:24:07.460 --> 1:24:10.260\n are studying this aspect of interaction?\n\n1:24:10.260 --> 1:24:14.540\n Well, I'm actually pretty intensively interested\n\n1:24:14.540 --> 1:24:17.060\n in this issue now, and there are people in my group\n\n1:24:17.060 --> 1:24:20.940\n who've actually pivoted pretty hard over the last few years\n\n1:24:20.940 --> 1:24:24.180\n from doing more traditional cognitive psychology\n\n1:24:24.180 --> 1:24:28.060\n and cognitive neuroscience to doing experimental work\n\n1:24:28.060 --> 1:24:30.220\n on human agent interaction.\n\n1:24:30.220 --> 1:24:33.700\n And there are a couple of reasons that I'm\n\n1:24:33.700 --> 1:24:35.500\n pretty passionately interested in this.\n\n1:24:35.500 --> 1:24:40.500\n One is it's kind of the outcome of having thought\n\n1:24:42.460 --> 1:24:46.900\n for a few years now about what we're up to.\n\n1:24:46.900 --> 1:24:49.340\n Like what are we doing?\n\n1:24:49.340 --> 1:24:53.420\n Like what is this AI research for?\n\n1:24:53.420 --> 1:24:57.020\n So what does it mean to make the world a better place?\n\n1:24:57.020 --> 1:24:59.740\n I think I'm pretty sure that means making life better\n\n1:24:59.740 --> 1:25:00.580\n for humans.\n\n1:25:02.620 --> 1:25:05.820\n And so how do you make life better for humans?\n\n1:25:05.820 --> 1:25:10.540\n That's a proposition that when you look at it carefully\n\n1:25:10.540 --> 1:25:15.540\n and honestly is rather horrendously complicated,\n\n1:25:15.860 --> 1:25:18.820\n especially when the AI systems\n\n1:25:18.820 --> 1:25:23.820\n that you're building are learning systems.\n\n1:25:25.220 --> 1:25:29.060\n They're not, you're not programming something\n\n1:25:29.060 --> 1:25:31.420\n that you then introduce to the world\n\n1:25:31.420 --> 1:25:33.140\n and it just works as programmed,\n\n1:25:33.140 --> 1:25:34.860\n like Google Maps or something.\n\n1:25:36.500 --> 1:25:39.700\n We're building systems that learn from experience.\n\n1:25:39.700 --> 1:25:43.500\n So that typically leads to AI safety questions.\n\n1:25:43.500 --> 1:25:45.420\n How do we keep these things from getting out of control?\n\n1:25:45.420 --> 1:25:49.060\n How do we keep them from doing things that harm humans?\n\n1:25:49.060 --> 1:25:51.820\n And I mean, I hasten to say,\n\n1:25:51.820 --> 1:25:54.500\n I consider those hugely important issues.\n\n1:25:54.500 --> 1:25:58.900\n And there are large sectors of the research community\n\n1:25:58.900 --> 1:26:00.780\n at DeepMind and of course elsewhere\n\n1:26:00.780 --> 1:26:03.460\n who are dedicated to thinking hard all day,\n\n1:26:03.460 --> 1:26:04.980\n every day about that.\n\n1:26:04.980 --> 1:26:09.620\n But there's, I guess I would say a positive side to this too\n\n1:26:09.620 --> 1:26:13.300\n which is to say, well, what would it mean\n\n1:26:13.300 --> 1:26:15.900\n to make human life better?\n\n1:26:15.900 --> 1:26:20.140\n And how can we imagine learning systems doing that?\n\n1:26:21.180 --> 1:26:23.500\n And in talking to my colleagues about that,\n\n1:26:23.500 --> 1:26:25.700\n we reached the initial conclusion\n\n1:26:25.700 --> 1:26:30.100\n that it's not sufficient to philosophize about that.\n\n1:26:30.100 --> 1:26:32.060\n You actually have to take into account\n\n1:26:32.060 --> 1:26:37.060\n how humans actually work and what humans want\n\n1:26:37.860 --> 1:26:40.500\n and the difficulties of knowing what humans want\n\n1:26:41.740 --> 1:26:43.780\n and the difficulties that arise\n\n1:26:43.780 --> 1:26:46.340\n when humans want different things.\n\n1:26:47.380 --> 1:26:50.900\n And so human agent interaction has become,\n\n1:26:50.900 --> 1:26:55.060\n a quite intensive focus of my group lately.\n\n1:26:56.460 --> 1:26:58.020\n If for no other reason that,\n\n1:26:59.020 --> 1:27:04.020\n in order to really address that issue in an adequate way,\n\n1:27:04.660 --> 1:27:07.340\n you have to, I mean, psychology becomes part of the picture.\n\n1:27:07.340 --> 1:27:10.380\n Yeah, and so there's a few elements there.\n\n1:27:10.380 --> 1:27:12.900\n So if you focus on solving like the,\n\n1:27:12.900 --> 1:27:14.700\n if you focus on the robotics problem,\n\n1:27:14.700 --> 1:27:18.140\n let's say AGI without humans in the picture\n\n1:27:18.140 --> 1:27:22.300\n is you're missing fundamentally the final step.\n\n1:27:22.300 --> 1:27:24.580\n When you do want to help human civilization,\n\n1:27:24.580 --> 1:27:27.340\n you eventually have to interact with humans.\n\n1:27:27.340 --> 1:27:31.380\n And when you create a learning system, just as you said,\n\n1:27:31.380 --> 1:27:34.340\n that will eventually have to interact with humans,\n\n1:27:34.340 --> 1:27:37.900\n the interaction itself has to be become,\n\n1:27:37.900 --> 1:27:40.780\n has to become part of the learning process.\n\n1:27:40.780 --> 1:27:43.820\n So you can't just watch, well, my sense is,\n\n1:27:43.820 --> 1:27:46.580\n it sounds like your sense is you can't just watch humans\n\n1:27:46.580 --> 1:27:48.260\n to learn about humans.\n\n1:27:48.260 --> 1:27:50.220\n You have to also be part of the human world.\n\n1:27:50.220 --> 1:27:51.420\n You have to interact with humans.\n\n1:27:51.420 --> 1:27:52.260\n Yeah, exactly.\n\n1:27:52.260 --> 1:27:57.260\n And I mean, then questions arise that start imperceptibly,\n\n1:27:57.380 --> 1:28:02.380\n but inevitably to slip beyond the realm of engineering.\n\n1:28:02.380 --> 1:28:05.940\n So questions like, if you have an agent\n\n1:28:05.940 --> 1:28:08.940\n that can do something that you can't do,\n\n1:28:10.900 --> 1:28:13.780\n under what conditions do you want that agent to do it?\n\n1:28:13.780 --> 1:28:18.780\n So if I have a robot that can play Beethoven sonatas\n\n1:28:24.700 --> 1:28:29.700\n better than any human, in the sense that the sensitivity,\n\n1:28:30.740 --> 1:28:33.940\n the expression is just beyond what any human,\n\n1:28:33.940 --> 1:28:36.300\n do I want to listen to that?\n\n1:28:36.300 --> 1:28:38.780\n Do I want to go to a concert and hear a robot play?\n\n1:28:38.780 --> 1:28:41.340\n These aren't engineering questions.\n\n1:28:41.340 --> 1:28:44.340\n These are questions about human preference\n\n1:28:44.340 --> 1:28:45.980\n and human culture.\n\n1:28:45.980 --> 1:28:47.940\n Psychology bordering on philosophy.\n\n1:28:47.940 --> 1:28:50.260\n Yeah, and then you start asking,\n\n1:28:50.260 --> 1:28:54.660\n well, even if we knew the answer to that,\n\n1:28:54.660 --> 1:28:57.060\n is it our place as AI engineers\n\n1:28:57.060 --> 1:28:59.180\n to build that into these agents?\n\n1:28:59.180 --> 1:29:02.140\n Probably the agents should interact with humans\n\n1:29:03.500 --> 1:29:05.620\n beyond the population of AI engineers\n\n1:29:05.620 --> 1:29:07.820\n and figure out what those humans want.\n\n1:29:08.780 --> 1:29:10.620\n And then when you start,\n\n1:29:10.620 --> 1:29:11.780\n I referred this the moment ago,\n\n1:29:11.780 --> 1:29:14.340\n but even that becomes complicated.\n\n1:29:14.340 --> 1:29:19.100\n Be quote, what if two humans want different things?\n\n1:29:19.100 --> 1:29:22.380\n And you have only one agent that's able to interact with them\n\n1:29:22.380 --> 1:29:24.620\n and try to satisfy their preferences.\n\n1:29:24.620 --> 1:29:27.060\n Then you're into the realm of economics\n\n1:29:30.340 --> 1:29:33.660\n and social choice theory and even politics.\n\n1:29:33.660 --> 1:29:35.540\n So there's a sense in which,\n\n1:29:35.540 --> 1:29:37.980\n if you kind of follow what we're doing\n\n1:29:37.980 --> 1:29:39.940\n to its logical conclusion,\n\n1:29:39.940 --> 1:29:44.940\n then it goes beyond questions of engineering and technology\n\n1:29:45.060 --> 1:29:48.420\n and starts to shade imperceptibly into questions\n\n1:29:48.420 --> 1:29:51.660\n about what kind of society do you want?\n\n1:29:51.660 --> 1:29:55.740\n And actually, once that dawned on me,\n\n1:29:55.740 --> 1:29:57.300\n I actually felt,\n\n1:29:58.620 --> 1:29:59.860\n I don't know what the right word is,\n\n1:29:59.860 --> 1:30:03.020\n quite refreshed in my involvement in AI research.\n\n1:30:03.020 --> 1:30:06.300\n It was almost like building this kind of stuff\n\n1:30:06.300 --> 1:30:10.220\n is gonna lead us back to asking really fundamental questions\n\n1:30:10.220 --> 1:30:13.860\n about what is this,\n\n1:30:13.860 --> 1:30:16.700\n what's the good life and who gets to decide\n\n1:30:16.700 --> 1:30:21.700\n and bringing in viewpoints from multiple sub communities\n\n1:30:23.780 --> 1:30:26.300\n to help us shape the way that we live.\n\n1:30:27.460 --> 1:30:30.820\n There's something, it started making me feel like\n\n1:30:30.820 --> 1:30:36.300\n doing AI research in a fully responsible way, would,\n\n1:30:38.300 --> 1:30:42.820\n could potentially lead to a kind of like cultural renewal.\n\n1:30:42.820 --> 1:30:47.820\n Yeah, it's the way to understand human beings\n\n1:30:48.180 --> 1:30:50.340\n at the individual, at the societal level.\n\n1:30:50.340 --> 1:30:54.020\n It may become a way to answer all the silly human questions\n\n1:30:54.020 --> 1:30:57.060\n of the meaning of life and all those kinds of things.\n\n1:30:57.060 --> 1:30:58.060\n Even if it doesn't give us a way\n\n1:30:58.060 --> 1:30:59.220\n of answering those questions,\n\n1:30:59.220 --> 1:31:03.660\n it may force us back to thinking about them.\n\n1:31:03.660 --> 1:31:06.940\n And it might bring, it might restore a certain,\n\n1:31:06.940 --> 1:31:10.460\n I don't know, a certain depth to,\n\n1:31:10.460 --> 1:31:15.460\n or even dare I say spirituality to the way that,\n\n1:31:16.380 --> 1:31:18.060\n to the world, I don't know.\n\n1:31:18.060 --> 1:31:19.380\n Maybe that's too grandiose.\n\n1:31:19.380 --> 1:31:21.020\n Well, I'm with you.\n\n1:31:21.020 --> 1:31:26.020\n I think it's AI will be the philosophy of the 21st century,\n\n1:31:27.620 --> 1:31:29.020\n the way which will open the door.\n\n1:31:29.020 --> 1:31:32.500\n I think a lot of AI researchers are afraid to open that door\n\n1:31:32.500 --> 1:31:35.660\n of exploring the beautiful richness\n\n1:31:35.660 --> 1:31:39.540\n of the human agent interaction, human AI interaction.\n\n1:31:39.540 --> 1:31:42.380\n I'm really happy that somebody like you\n\n1:31:42.380 --> 1:31:43.700\n have opened that door.\n\n1:31:43.700 --> 1:31:48.700\n And one thing I often think about is the usual schema\n\n1:31:49.500 --> 1:31:54.500\n for thinking about human agent interaction\n\n1:31:54.500 --> 1:31:59.500\n as this kind of dystopian, oh, our robot overlords.\n\n1:32:00.460 --> 1:32:03.500\n And again, I hasten to say AI safety is hugely important.\n\n1:32:03.500 --> 1:32:06.420\n And I'm not saying we shouldn't be thinking\n\n1:32:06.420 --> 1:32:09.540\n about those risks, totally on board for that.\n\n1:32:09.540 --> 1:32:14.060\n But there's, having said that,\n\n1:32:17.060 --> 1:32:18.860\n what often follows for me is the thought\n\n1:32:18.860 --> 1:32:22.980\n that there's another kind of narrative\n\n1:32:22.980 --> 1:32:24.780\n that might be relevant, which is,\n\n1:32:24.780 --> 1:32:29.780\n when we think of humans gaining more and more information\n\n1:32:31.020 --> 1:32:36.020\n about human life, the narrative there is usually\n\n1:32:36.380 --> 1:32:38.540\n that they gain more and more wisdom\n\n1:32:38.540 --> 1:32:40.700\n and they get closer to enlightenment\n\n1:32:40.700 --> 1:32:43.260\n and they become more benevolent.\n\n1:32:43.260 --> 1:32:47.300\n And the Buddha is like, that's a totally different narrative.\n\n1:32:47.300 --> 1:32:50.380\n And why isn't it the case that we imagine\n\n1:32:50.380 --> 1:32:52.460\n that the AI systems that we're creating\n\n1:32:52.460 --> 1:32:53.980\n are just gonna, like, they're gonna figure out\n\n1:32:53.980 --> 1:32:55.660\n more and more about the way the world works\n\n1:32:55.660 --> 1:32:56.820\n and the way that humans interact\n\n1:32:56.820 --> 1:32:59.180\n and they'll become beneficent.\n\n1:32:59.180 --> 1:33:00.500\n I'm not saying that will happen.\n\n1:33:00.500 --> 1:33:05.420\n I don't honestly expect that to happen\n\n1:33:05.420 --> 1:33:08.820\n without some careful, setting things up very carefully.\n\n1:33:08.820 --> 1:33:11.340\n But it's another way things could go, right?\n\n1:33:11.340 --> 1:33:13.820\n And yeah, and I would even push back on that.\n\n1:33:13.820 --> 1:33:18.820\n I personally believe that the most trajectories,\n\n1:33:18.820 --> 1:33:23.820\n natural human trajectories will lead us towards progress.\n\n1:33:25.460 --> 1:33:28.420\n So for me, there is a kind of sense\n\n1:33:28.420 --> 1:33:30.820\n that most trajectories in AI development\n\n1:33:30.820 --> 1:33:32.540\n will lead us into trouble.\n\n1:33:32.540 --> 1:33:37.140\n To me, and we over focus on the worst case.\n\n1:33:37.140 --> 1:33:38.500\n It's like in computer science,\n\n1:33:38.500 --> 1:33:40.860\n theoretical computer science has been this focus\n\n1:33:40.860 --> 1:33:42.060\n on worst case analysis.\n\n1:33:42.060 --> 1:33:45.180\n There's something appealing to our human mind\n\n1:33:45.180 --> 1:33:47.660\n at some lowest level to be good.\n\n1:33:47.660 --> 1:33:50.220\n I mean, we don't wanna be eaten by the tiger, I guess.\n\n1:33:50.220 --> 1:33:52.300\n So we wanna do the worst case analysis.\n\n1:33:52.300 --> 1:33:55.660\n But the reality is that shouldn't stop us\n\n1:33:55.660 --> 1:33:58.620\n from actually building out all the other trajectories\n\n1:33:58.620 --> 1:34:01.900\n which are potentially leading to all the positive worlds,\n\n1:34:01.900 --> 1:34:04.540\n all the enlightenment.\n\n1:34:04.540 --> 1:34:05.700\n There's a book, Enlightenment Now,\n\n1:34:05.700 --> 1:34:06.980\n with Steven Pinker and so on.\n\n1:34:06.980 --> 1:34:09.660\n This is looking generally at human progress.\n\n1:34:09.660 --> 1:34:12.300\n And there's so many ways that human progress\n\n1:34:12.300 --> 1:34:13.900\n can happen with AI.\n\n1:34:13.900 --> 1:34:16.300\n And I think you have to do that research.\n\n1:34:16.300 --> 1:34:17.380\n You have to do that work.\n\n1:34:17.380 --> 1:34:20.700\n You have to do the, not just the AI safety work\n\n1:34:20.700 --> 1:34:22.500\n of the one worst case analysis.\n\n1:34:22.500 --> 1:34:23.500\n How do we prevent that?\n\n1:34:23.500 --> 1:34:27.540\n But the actual tools and the glue\n\n1:34:27.540 --> 1:34:31.340\n and the mechanisms of human AI interaction\n\n1:34:31.340 --> 1:34:34.180\n that would lead to all the positive actions that can go.\n\n1:34:34.180 --> 1:34:36.540\n It's a super exciting area, right?\n\n1:34:36.540 --> 1:34:38.340\n Yeah, we should be spending,\n\n1:34:38.340 --> 1:34:40.820\n we should be spending a lot of our time saying\n\n1:34:40.820 --> 1:34:42.860\n what can go wrong.\n\n1:34:42.860 --> 1:34:47.860\n I think it's harder to see that there's work to be done\n\n1:34:47.860 --> 1:34:51.540\n to bring into focus the question of what it would look like\n\n1:34:51.540 --> 1:34:52.980\n for things to go right.\n\n1:34:54.420 --> 1:34:56.500\n That's not obvious.\n\n1:34:57.660 --> 1:34:59.620\n And we wouldn't be doing this if we didn't have the sense\n\n1:34:59.620 --> 1:35:01.980\n there was huge potential, right?\n\n1:35:01.980 --> 1:35:05.100\n We're not doing this for no reason.\n\n1:35:05.100 --> 1:35:10.100\n We have a sense that AGI would be a major boom to humanity.\n\n1:35:10.100 --> 1:35:13.700\n But I think it's worth starting now,\n\n1:35:13.700 --> 1:35:15.620\n even when our technology is quite primitive,\n\n1:35:15.620 --> 1:35:19.420\n asking exactly what would that mean?\n\n1:35:19.420 --> 1:35:21.060\n We can start now with applications\n\n1:35:21.060 --> 1:35:22.580\n that are already gonna make the world a better place,\n\n1:35:22.580 --> 1:35:25.060\n like solving protein folding.\n\n1:35:25.060 --> 1:35:27.860\n I think DeepMind has gotten heavy\n\n1:35:27.860 --> 1:35:30.060\n into science applications lately,\n\n1:35:30.060 --> 1:35:34.380\n which I think is a wonderful, wonderful move\n\n1:35:34.380 --> 1:35:36.060\n for us to be making.\n\n1:35:36.060 --> 1:35:37.260\n But when we think about AGI,\n\n1:35:37.260 --> 1:35:39.860\n when we think about building fully intelligent\n\n1:35:39.860 --> 1:35:42.460\n agents that are gonna be able to, in a sense,\n\n1:35:42.460 --> 1:35:43.780\n do whatever they want,\n\n1:35:45.540 --> 1:35:46.740\n we should start thinking about\n\n1:35:46.740 --> 1:35:48.940\n what do we want them to want, right?\n\n1:35:48.940 --> 1:35:51.460\n What kind of world do we wanna live in?\n\n1:35:52.300 --> 1:35:54.300\n That's not an easy question.\n\n1:35:54.300 --> 1:35:56.700\n And I think we just need to start working on it.\n\n1:35:56.700 --> 1:35:58.620\n And even on the path to,\n\n1:35:58.620 --> 1:35:59.900\n it doesn't have to be AGI,\n\n1:35:59.900 --> 1:36:02.300\n but just intelligent agents that interact with us\n\n1:36:02.300 --> 1:36:06.220\n and help us enrich our own existence on social networks,\n\n1:36:06.220 --> 1:36:08.820\n for example, on recommender systems of various intelligence.\n\n1:36:08.820 --> 1:36:10.540\n And there's so much interesting interaction\n\n1:36:10.540 --> 1:36:12.300\n that's yet to be understood and studied.\n\n1:36:12.300 --> 1:36:15.540\n And how do you create,\n\n1:36:15.540 --> 1:36:19.460\n I mean, Twitter is struggling with this very idea,\n\n1:36:19.460 --> 1:36:21.420\n how do you create AI systems\n\n1:36:21.420 --> 1:36:24.380\n that increase the quality and the health of a conversation?\n\n1:36:24.380 --> 1:36:25.220\n For sure.\n\n1:36:25.220 --> 1:36:28.500\n That's a beautiful human psychology question.\n\n1:36:28.500 --> 1:36:29.740\n And how do you do that\n\n1:36:29.740 --> 1:36:34.740\n without deception being involved,\n\n1:36:34.740 --> 1:36:38.100\n without manipulation being involved,\n\n1:36:38.100 --> 1:36:41.060\n maximizing human autonomy?\n\n1:36:42.420 --> 1:36:45.820\n And how do you make these choices in a democratic way?\n\n1:36:45.820 --> 1:36:50.180\n How do we face the,\n\n1:36:50.180 --> 1:36:52.740\n again, I'm speaking for myself here.\n\n1:36:52.740 --> 1:36:54.660\n How do we face the fact that\n\n1:36:55.700 --> 1:36:57.740\n it's a small group of people\n\n1:36:57.740 --> 1:37:01.340\n who have the skillset to build these kinds of systems,\n\n1:37:01.340 --> 1:37:05.860\n but what it means to make the world a better place\n\n1:37:05.860 --> 1:37:09.020\n is something that we all have to be talking about.\n\n1:37:09.020 --> 1:37:14.020\n Yeah, the world that we're trying to make a better place\n\n1:37:14.020 --> 1:37:18.020\n includes a huge variety of different kinds of people.\n\n1:37:18.020 --> 1:37:19.420\n Yeah, how do we cope with that?\n\n1:37:19.420 --> 1:37:22.820\n This is a problem that has been discussed\n\n1:37:22.820 --> 1:37:27.820\n in gory, extensive detail in social choice theory.\n\n1:37:28.500 --> 1:37:29.900\n One thing I'm really interested in\n\n1:37:29.900 --> 1:37:32.900\n and one thing I'm really enjoying\n\n1:37:32.900 --> 1:37:35.180\n about the recent direction work has taken\n\n1:37:35.180 --> 1:37:36.900\n in some parts of my team is that,\n\n1:37:36.900 --> 1:37:38.620\n yeah, we're reading the AI literature,\n\n1:37:38.620 --> 1:37:39.940\n we're reading the neuroscience literature,\n\n1:37:39.940 --> 1:37:42.940\n but we've also started reading economics\n\n1:37:42.940 --> 1:37:44.820\n and, as I mentioned, social choice theory,\n\n1:37:44.820 --> 1:37:45.940\n even some political theory,\n\n1:37:45.940 --> 1:37:50.380\n because it turns out that it all becomes relevant.\n\n1:37:50.380 --> 1:37:51.580\n It all becomes relevant.\n\n1:37:53.540 --> 1:37:55.660\n But at the same time,\n\n1:37:55.660 --> 1:38:00.140\n we've been trying not to write philosophy papers,\n\n1:38:00.140 --> 1:38:01.980\n we've been trying not to write physician papers.\n\n1:38:01.980 --> 1:38:03.780\n We're trying to figure out ways\n\n1:38:03.780 --> 1:38:05.740\n of doing actual empirical research\n\n1:38:05.740 --> 1:38:07.780\n that kind of take the first small steps\n\n1:38:07.780 --> 1:38:10.820\n to thinking about what it really means\n\n1:38:10.820 --> 1:38:13.580\n for humans with all of their complexity\n\n1:38:13.580 --> 1:38:16.980\n and contradiction and paradox\n\n1:38:18.540 --> 1:38:22.340\n to be brought into contact with these AI systems\n\n1:38:22.340 --> 1:38:25.540\n in a way that really makes the world a better place.\n\n1:38:25.540 --> 1:38:27.540\n Often, reinforcement learning frameworks\n\n1:38:27.540 --> 1:38:30.860\n actually kind of allow you to do that,\n\n1:38:30.860 --> 1:38:33.580\n machine learning, and so that's the exciting thing about AI\n\n1:38:33.580 --> 1:38:37.260\n is it allows you to reduce the unsolvable problem,\n\n1:38:37.260 --> 1:38:40.380\n philosophical problem, into something more concrete\n\n1:38:40.380 --> 1:38:41.700\n that you can get ahold of.\n\n1:38:41.700 --> 1:38:43.900\n Yeah, and it allows you to kind of define the problem\n\n1:38:43.900 --> 1:38:48.900\n in some way that allows for growth in the system\n\n1:38:49.980 --> 1:38:51.140\n that's sort of, you know,\n\n1:38:51.140 --> 1:38:54.100\n you're not responsible for the details, right?\n\n1:38:54.100 --> 1:38:56.700\n You say, this is generally what I want you to do,\n\n1:38:56.700 --> 1:38:58.740\n and then learning takes care of the rest.\n\n1:38:59.580 --> 1:39:04.100\n Of course, the safety issues arise in that context,\n\n1:39:04.100 --> 1:39:05.980\n but I think also some of these positive issues\n\n1:39:05.980 --> 1:39:06.940\n arise in that context.\n\n1:39:06.940 --> 1:39:09.180\n What would it mean for an AI system\n\n1:39:09.180 --> 1:39:12.700\n to really come to understand what humans want?\n\n1:39:14.780 --> 1:39:18.940\n And with all of the subtleties of that, right?\n\n1:39:18.940 --> 1:39:23.940\n You know, humans want help with certain things,\n\n1:39:24.660 --> 1:39:27.420\n but they don't want everything done for them, right?\n\n1:39:27.420 --> 1:39:29.660\n There is, part of the satisfaction\n\n1:39:29.660 --> 1:39:32.700\n that humans get from life is in accomplishing things.\n\n1:39:32.700 --> 1:39:34.660\n So if there were devices around that did everything for,\n\n1:39:34.660 --> 1:39:37.500\n you know, I often think of the movie WALLI, right?\n\n1:39:37.500 --> 1:39:39.380\n That's like dystopian in a totally different way.\n\n1:39:39.380 --> 1:39:41.340\n It's like, the machines are doing everything for us.\n\n1:39:41.340 --> 1:39:43.780\n That's not what we wanted.\n\n1:39:43.780 --> 1:39:46.700\n You know, anyway, I find this, you know,\n\n1:39:46.700 --> 1:39:50.500\n this opens up a whole landscape of research\n\n1:39:50.500 --> 1:39:52.740\n that feels affirmative and exciting.\n\n1:39:52.740 --> 1:39:56.020\n To me, it's one of the most exciting, and it's wide open.\n\n1:39:56.020 --> 1:39:58.260\n We have to, because it's a cool paper,\n\n1:39:58.260 --> 1:39:59.300\n talk about dopamine.\n\n1:39:59.300 --> 1:40:01.100\n Oh yeah, okay, so I can.\n\n1:40:01.100 --> 1:40:04.980\n We were gonna, I was gonna give you a quick summary.\n\n1:40:04.980 --> 1:40:09.900\n Yeah, a quick summary of, what's the title of the paper?\n\n1:40:09.900 --> 1:40:14.900\n I think we called it a distributional code for value\n\n1:40:14.900 --> 1:40:19.020\n in dopamine based reinforcement learning, yes.\n\n1:40:19.020 --> 1:40:24.020\n So that's another project that grew out of pure AI research.\n\n1:40:25.740 --> 1:40:29.620\n A number of people at DeepMind and a few other places\n\n1:40:29.620 --> 1:40:32.340\n had started working on a new version\n\n1:40:32.340 --> 1:40:33.740\n of reinforcement learning,\n\n1:40:35.740 --> 1:40:38.940\n which was defined by taking something\n\n1:40:38.940 --> 1:40:41.420\n in traditional reinforcement learning and just tweaking it.\n\n1:40:41.420 --> 1:40:42.740\n So the thing that they took\n\n1:40:42.740 --> 1:40:46.860\n from traditional reinforcement learning was a value signal.\n\n1:40:46.860 --> 1:40:49.540\n So at the center of reinforcement learning,\n\n1:40:49.540 --> 1:40:52.580\n at least most algorithms, is some representation\n\n1:40:52.580 --> 1:40:54.140\n of how well things are going,\n\n1:40:54.140 --> 1:40:57.660\n your expected cumulative future reward.\n\n1:40:57.660 --> 1:41:01.220\n And that's usually represented as a single number.\n\n1:41:01.220 --> 1:41:04.260\n So if you imagine a gambler in a casino\n\n1:41:04.260 --> 1:41:07.980\n and the gambler's thinking, well, I have this probability\n\n1:41:07.980 --> 1:41:09.540\n of winning such and such an amount of money,\n\n1:41:09.540 --> 1:41:11.260\n and I have this probability of losing such and such\n\n1:41:11.260 --> 1:41:14.860\n an amount of money, that situation would be represented\n\n1:41:14.860 --> 1:41:17.260\n as a single number, which is like the expected,\n\n1:41:17.260 --> 1:41:19.580\n the weighted average of all those outcomes.\n\n1:41:20.580 --> 1:41:23.740\n And this new form of reinforcement learning said,\n\n1:41:23.740 --> 1:41:26.460\n well, what if we generalize that\n\n1:41:26.460 --> 1:41:28.140\n to a distributional representation?\n\n1:41:28.140 --> 1:41:30.820\n So now we think of the gambler as literally thinking,\n\n1:41:30.820 --> 1:41:32.260\n well, there's this probability\n\n1:41:32.260 --> 1:41:33.620\n that I'll win this amount of money,\n\n1:41:33.620 --> 1:41:34.580\n and there's this probability\n\n1:41:34.580 --> 1:41:35.700\n that I'll lose that amount of money,\n\n1:41:35.700 --> 1:41:37.820\n and we don't reduce that to a single number.\n\n1:41:37.820 --> 1:41:40.580\n And it had been observed through experiments,\n\n1:41:40.580 --> 1:41:42.420\n through just trying this out,\n\n1:41:42.420 --> 1:41:45.900\n that that kind of distributional representation\n\n1:41:45.900 --> 1:41:49.620\n really accelerated reinforcement learning\n\n1:41:49.620 --> 1:41:52.380\n and led to better policies.\n\n1:41:52.380 --> 1:41:53.620\n What's your intuition about,\n\n1:41:53.620 --> 1:41:55.260\n so we're talking about rewards.\n\n1:41:55.260 --> 1:41:56.100\n Yeah.\n\n1:41:56.100 --> 1:41:58.420\n So what's your intuition why that is, why does it do that?\n\n1:41:58.420 --> 1:42:02.620\n Well, it's kind of a surprising historical note,\n\n1:42:02.620 --> 1:42:04.460\n at least surprised me when I learned it,\n\n1:42:04.460 --> 1:42:07.260\n that this had been proven to be true.\n\n1:42:07.260 --> 1:42:09.820\n This had been tried out in a kind of heuristic way.\n\n1:42:09.820 --> 1:42:12.500\n People thought, well, gee, what would happen if we tried?\n\n1:42:12.500 --> 1:42:14.580\n And then it had this, empirically,\n\n1:42:14.580 --> 1:42:17.300\n it had this striking effect.\n\n1:42:17.300 --> 1:42:19.300\n And it was only then that people started thinking,\n\n1:42:19.300 --> 1:42:21.380\n well, gee, wait, why?\n\n1:42:21.380 --> 1:42:22.220\n Wait, why?\n\n1:42:22.220 --> 1:42:23.420\n Why is this working?\n\n1:42:23.420 --> 1:42:26.180\n And that's led to a series of studies\n\n1:42:26.180 --> 1:42:29.740\n just trying to figure out why it works, which is ongoing.\n\n1:42:29.740 --> 1:42:31.780\n But one thing that's already clear from that research\n\n1:42:31.780 --> 1:42:34.340\n is that one reason that it helps\n\n1:42:34.340 --> 1:42:38.300\n is that it drives richer representation learning.\n\n1:42:39.420 --> 1:42:43.060\n So if you imagine two situations\n\n1:42:43.060 --> 1:42:45.300\n that have the same expected value,\n\n1:42:45.300 --> 1:42:47.300\n the same kind of weighted average value,\n\n1:42:48.980 --> 1:42:51.300\n standard deep reinforcement learning algorithms\n\n1:42:51.300 --> 1:42:53.500\n are going to take those two situations\n\n1:42:53.500 --> 1:42:55.020\n and kind of, in terms of the way\n\n1:42:55.020 --> 1:42:56.460\n they're represented internally,\n\n1:42:56.460 --> 1:42:58.180\n they're gonna squeeze them together\n\n1:42:58.180 --> 1:43:02.580\n because the thing that you're trying to represent,\n\n1:43:02.580 --> 1:43:04.180\n which is their expected value, is the same.\n\n1:43:04.180 --> 1:43:06.260\n So all the way through the system,\n\n1:43:06.260 --> 1:43:08.420\n things are gonna be mushed together.\n\n1:43:08.420 --> 1:43:11.060\n But what if those two situations\n\n1:43:11.060 --> 1:43:13.940\n actually have different value distributions?\n\n1:43:13.940 --> 1:43:16.900\n They have the same average value,\n\n1:43:16.900 --> 1:43:19.900\n but they have different distributions of value.\n\n1:43:19.900 --> 1:43:22.300\n In that situation, distributional learning\n\n1:43:22.300 --> 1:43:25.100\n will maintain the distinction between these two things.\n\n1:43:25.100 --> 1:43:26.820\n So to make a long story short,\n\n1:43:26.820 --> 1:43:30.020\n distributional learning can keep things separate\n\n1:43:30.020 --> 1:43:32.180\n in the internal representation\n\n1:43:32.180 --> 1:43:35.140\n that might otherwise be conflated or squished together.\n\n1:43:35.140 --> 1:43:36.380\n And maintaining those distinctions\n\n1:43:36.380 --> 1:43:40.180\n can be useful when the system is now faced\n\n1:43:40.180 --> 1:43:43.260\n with some other task where the distinction is important.\n\n1:43:43.260 --> 1:43:44.540\n If we look at the optimistic\n\n1:43:44.540 --> 1:43:46.580\n and pessimistic dopamine neurons.\n\n1:43:46.580 --> 1:43:49.580\n So first of all, what is dopamine?\n\n1:43:50.900 --> 1:43:51.740\n Oh, God.\n\n1:43:51.740 --> 1:43:54.740\n Why is this at all useful\n\n1:43:58.220 --> 1:44:00.740\n to think about in the artificial intelligence sense?\n\n1:44:00.740 --> 1:44:04.180\n But what do we know about dopamine in the human brain?\n\n1:44:04.180 --> 1:44:05.620\n What is it?\n\n1:44:05.620 --> 1:44:06.460\n Why is it useful?\n\n1:44:06.460 --> 1:44:07.460\n Why is it interesting?\n\n1:44:07.460 --> 1:44:09.380\n What does it have to do with the prefrontal cortex\n\n1:44:09.380 --> 1:44:10.260\n and learning in general?\n\n1:44:10.260 --> 1:44:15.260\n Yeah, so, well, this is also a case\n\n1:44:15.540 --> 1:44:19.660\n where there's a huge amount of detail and debate.\n\n1:44:19.660 --> 1:44:24.660\n But one currently prevailing idea\n\n1:44:24.740 --> 1:44:29.060\n is that the function of this neurotransmitter dopamine\n\n1:44:29.060 --> 1:44:33.460\n resembles a particular component\n\n1:44:33.460 --> 1:44:36.860\n of standard reinforcement learning algorithms,\n\n1:44:36.860 --> 1:44:39.860\n which is called the reward prediction error.\n\n1:44:39.860 --> 1:44:41.580\n So I was talking a moment ago\n\n1:44:41.580 --> 1:44:44.220\n about these value representations.\n\n1:44:44.220 --> 1:44:45.180\n How do you learn them?\n\n1:44:45.180 --> 1:44:46.900\n How do you update them based on experience?\n\n1:44:46.900 --> 1:44:51.820\n Well, if you made some prediction about a future reward\n\n1:44:51.820 --> 1:44:54.460\n and then you get more reward than you were expecting,\n\n1:44:54.460 --> 1:44:56.020\n then probably retrospectively,\n\n1:44:56.020 --> 1:45:00.740\n you want to go back and increase the value representation\n\n1:45:00.740 --> 1:45:03.820\n that you attached to that earlier situation.\n\n1:45:03.820 --> 1:45:06.180\n If you got less reward than you were expecting,\n\n1:45:06.180 --> 1:45:08.540\n you should probably decrement that estimate.\n\n1:45:08.540 --> 1:45:10.300\n And that's the process of temporal difference.\n\n1:45:10.300 --> 1:45:12.020\n Exactly, this is the central mechanism\n\n1:45:12.020 --> 1:45:12.860\n of temporal difference learning,\n\n1:45:12.860 --> 1:45:17.660\n which is one of several sort of the backbone\n\n1:45:17.660 --> 1:45:20.420\n of our momentarium in NRL.\n\n1:45:20.420 --> 1:45:25.020\n And this connection between the reward prediction error\n\n1:45:25.020 --> 1:45:30.020\n and dopamine was made in the 1990s.\n\n1:45:31.940 --> 1:45:33.420\n And there's been a huge amount of research\n\n1:45:33.420 --> 1:45:35.860\n that seems to back it up.\n\n1:45:35.860 --> 1:45:37.340\n Dopamine may be doing other things,\n\n1:45:37.340 --> 1:45:39.860\n but this is clearly, at least roughly,\n\n1:45:39.860 --> 1:45:42.460\n one of the things that it's doing.\n\n1:45:42.460 --> 1:45:45.100\n But the usual idea was that dopamine\n\n1:45:45.100 --> 1:45:48.060\n was representing these reward prediction errors,\n\n1:45:48.060 --> 1:45:51.340\n again, in this like kind of single number way\n\n1:45:51.340 --> 1:45:56.340\n that representing your surprise with a single number.\n\n1:45:56.700 --> 1:45:58.500\n And in distributional reinforcement learning,\n\n1:45:58.500 --> 1:46:02.780\n this kind of new elaboration of the standard approach,\n\n1:46:03.660 --> 1:46:06.060\n it's not only the value function\n\n1:46:06.060 --> 1:46:08.460\n that's represented as a single number,\n\n1:46:08.460 --> 1:46:10.940\n it's also the reward prediction error.\n\n1:46:10.940 --> 1:46:15.940\n And so what happened was that Will Dabney,\n\n1:46:16.180 --> 1:46:18.980\n one of my collaborators who was one of the first people\n\n1:46:18.980 --> 1:46:22.300\n to work on distributional temporal difference learning,\n\n1:46:22.300 --> 1:46:25.740\n talked to a guy in my group, Zeb Kurt Nelson,\n\n1:46:25.740 --> 1:46:27.660\n who's a computational neuroscientist,\n\n1:46:27.660 --> 1:46:29.580\n and said, gee, you know, is it possible\n\n1:46:29.580 --> 1:46:31.740\n that dopamine might be doing something\n\n1:46:31.740 --> 1:46:33.420\n like this distributional coding thing?\n\n1:46:33.420 --> 1:46:35.980\n And they started looking at what was in the literature,\n\n1:46:35.980 --> 1:46:36.820\n and then they brought me in,\n\n1:46:36.820 --> 1:46:39.220\n and we started talking to Nao Uchida,\n\n1:46:39.220 --> 1:46:41.300\n and we came up with some specific predictions\n\n1:46:41.300 --> 1:46:43.500\n about if the brain is using\n\n1:46:43.500 --> 1:46:45.140\n this kind of distributional coding,\n\n1:46:45.140 --> 1:46:47.340\n then in the tasks that Nao has studied,\n\n1:46:47.340 --> 1:46:49.300\n you should see this, this, this, and this,\n\n1:46:49.300 --> 1:46:50.620\n and that's where the paper came from.\n\n1:46:50.620 --> 1:46:53.540\n We kind of enumerated a set of predictions,\n\n1:46:53.540 --> 1:46:56.420\n all of which ended up being fairly clearly confirmed,\n\n1:46:57.260 --> 1:47:00.740\n and all of which leads to at least some initial indication\n\n1:47:00.740 --> 1:47:02.180\n that the brain might be doing something\n\n1:47:02.180 --> 1:47:03.420\n like this distributional coding,\n\n1:47:03.420 --> 1:47:06.780\n that dopamine might be representing surprise signals\n\n1:47:06.780 --> 1:47:09.980\n in a way that is not just collapsing everything\n\n1:47:09.980 --> 1:47:12.180\n to a single number, but instead is kind of respecting\n\n1:47:12.180 --> 1:47:16.620\n the variety of future outcomes, if that makes sense.\n\n1:47:16.620 --> 1:47:19.580\n So yeah, so that's showing, suggesting possibly\n\n1:47:19.580 --> 1:47:21.900\n that dopamine has a really interesting\n\n1:47:21.900 --> 1:47:25.940\n representation scheme in the human brain\n\n1:47:25.940 --> 1:47:27.660\n for its reward signal.\n\n1:47:27.660 --> 1:47:29.660\n Exactly. That's fascinating.\n\n1:47:29.660 --> 1:47:32.140\n That's another beautiful example of AI\n\n1:47:32.140 --> 1:47:34.460\n revealing something nice about neuroscience,\n\n1:47:34.460 --> 1:47:36.260\n potentially suggesting possibilities.\n\n1:47:36.260 --> 1:47:37.100\n Well, you never know.\n\n1:47:37.100 --> 1:47:39.260\n So the minute you publish a paper like that,\n\n1:47:39.260 --> 1:47:42.620\n the next thing you think is, I hope that replicates.\n\n1:47:42.620 --> 1:47:44.940\n Like, I hope we see that same thing in other data sets,\n\n1:47:44.940 --> 1:47:47.380\n but of course, several labs now\n\n1:47:47.380 --> 1:47:50.180\n are doing the followup experiments, so we'll know soon.\n\n1:47:50.180 --> 1:47:52.580\n But it has been a lot of fun for us\n\n1:47:52.580 --> 1:47:54.780\n to take these ideas from AI\n\n1:47:54.780 --> 1:47:56.820\n and kind of bring them into neuroscience\n\n1:47:56.820 --> 1:47:58.980\n and see how far we can get.\n\n1:47:58.980 --> 1:48:01.300\n So we kind of talked about it a little bit,\n\n1:48:01.300 --> 1:48:04.020\n but where do you see the field of neuroscience\n\n1:48:04.020 --> 1:48:07.740\n and artificial intelligence heading broadly?\n\n1:48:07.740 --> 1:48:12.580\n Like, what are the possible exciting areas\n\n1:48:12.580 --> 1:48:15.300\n that you can see breakthroughs in the next,\n\n1:48:15.300 --> 1:48:17.980\n let's get crazy, not just three or five years,\n\n1:48:17.980 --> 1:48:20.020\n but the next 10, 20, 30 years\n\n1:48:22.340 --> 1:48:26.100\n that would make you excited\n\n1:48:26.100 --> 1:48:27.980\n and perhaps you'd be part of?\n\n1:48:29.020 --> 1:48:31.020\n On the neuroscience side,\n\n1:48:32.980 --> 1:48:34.420\n there's a great deal of interest now\n\n1:48:34.420 --> 1:48:36.780\n in what's going on in AI.\n\n1:48:36.780 --> 1:48:41.500\n And at the same time,\n\n1:48:41.500 --> 1:48:45.900\n I feel like, so neuroscience,\n\n1:48:45.900 --> 1:48:50.100\n especially the part of neuroscience\n\n1:48:50.100 --> 1:48:54.180\n that's focused on circuits and systems,\n\n1:48:54.180 --> 1:48:56.340\n kind of like really mechanism focused,\n\n1:48:57.780 --> 1:49:01.980\n there's been this explosion in new technology.\n\n1:49:01.980 --> 1:49:05.100\n And up until recently,\n\n1:49:05.100 --> 1:49:08.940\n the experiments that have exploited this technology\n\n1:49:08.940 --> 1:49:13.340\n have not involved a lot of interesting behavior.\n\n1:49:13.340 --> 1:49:15.420\n And this is for a variety of reasons,\n\n1:49:16.300 --> 1:49:18.700\n one of which is in order to employ\n\n1:49:18.700 --> 1:49:19.860\n some of these technologies,\n\n1:49:19.860 --> 1:49:22.260\n you actually have to, if you're studying a mouse,\n\n1:49:22.260 --> 1:49:23.620\n you have to head fix the mouse.\n\n1:49:23.620 --> 1:49:26.260\n In other words, you have to like immobilize the mouse.\n\n1:49:26.260 --> 1:49:28.700\n And so it's been tricky to come up\n\n1:49:28.700 --> 1:49:30.860\n with ways of eliciting interesting behavior\n\n1:49:30.860 --> 1:49:33.460\n from a mouse that's restrained in this way,\n\n1:49:33.460 --> 1:49:35.660\n but people have begun to create\n\n1:49:35.660 --> 1:49:39.460\n very interesting solutions to this,\n\n1:49:39.460 --> 1:49:41.300\n like virtual reality environments\n\n1:49:41.300 --> 1:49:43.860\n where the animal can kind of move a track ball.\n\n1:49:43.860 --> 1:49:48.780\n And as people have kind of begun to explore\n\n1:49:48.780 --> 1:49:50.260\n what you can do with these technologies,\n\n1:49:50.260 --> 1:49:52.820\n I feel like more and more people are asking,\n\n1:49:52.820 --> 1:49:55.740\n well, let's try to bring behavior into the picture.\n\n1:49:55.740 --> 1:49:58.220\n Let's try to like reintroduce behavior,\n\n1:49:58.220 --> 1:50:01.020\n which was supposed to be what this whole thing was about.\n\n1:50:01.020 --> 1:50:05.700\n And I'm hoping that those two trends,\n\n1:50:05.700 --> 1:50:09.180\n the kind of growing interest in behavior\n\n1:50:09.180 --> 1:50:14.180\n and the widespread interest in what's going on in AI,\n\n1:50:14.180 --> 1:50:17.580\n will come together to kind of open a new chapter\n\n1:50:17.580 --> 1:50:22.580\n in neuroscience research where there's a kind of\n\n1:50:22.580 --> 1:50:25.820\n a rebirth of interest in the structure of behavior\n\n1:50:25.820 --> 1:50:27.540\n and its underlying substrates,\n\n1:50:27.540 --> 1:50:31.340\n but that that research is being informed\n\n1:50:31.340 --> 1:50:33.700\n by computational mechanisms\n\n1:50:33.700 --> 1:50:35.620\n that we're coming to understand in AI.\n\n1:50:36.740 --> 1:50:39.580\n If we can do that, then we might be taking a step closer\n\n1:50:39.580 --> 1:50:43.260\n to this utopian future that we were talking about earlier\n\n1:50:43.260 --> 1:50:44.860\n where there's really no distinction\n\n1:50:44.860 --> 1:50:46.940\n between psychology and neuroscience.\n\n1:50:46.940 --> 1:50:50.900\n Neuroscience is about studying the mechanisms\n\n1:50:50.900 --> 1:50:54.660\n that underlie whatever it is the brain is for,\n\n1:50:54.660 --> 1:50:56.340\n and what is the brain for?\n\n1:50:56.340 --> 1:50:58.460\n What is the brain for? It's for behavior.\n\n1:50:58.460 --> 1:51:03.100\n I feel like we could maybe take a step toward that now\n\n1:51:03.100 --> 1:51:05.180\n if people are motivated in the right way.\n\n1:51:06.780 --> 1:51:08.780\n You also asked about AI.\n\n1:51:08.780 --> 1:51:10.340\n So that was a neuroscience question.\n\n1:51:10.340 --> 1:51:12.180\n You said neuroscience, that's right.\n\n1:51:12.180 --> 1:51:13.740\n And especially places like DeepMind\n\n1:51:13.740 --> 1:51:15.260\n are interested in both branches.\n\n1:51:15.260 --> 1:51:18.720\n So what about the engineering of intelligence systems?\n\n1:51:20.820 --> 1:51:24.900\n I think one of the key challenges\n\n1:51:24.900 --> 1:51:28.700\n that a lot of people are seeing now in AI\n\n1:51:28.700 --> 1:51:33.700\n is to build systems that have the kind of flexibility\n\n1:51:34.300 --> 1:51:38.580\n and the kind of flexibility that humans have in two senses.\n\n1:51:38.580 --> 1:51:41.860\n One is that humans can be good at many things.\n\n1:51:41.860 --> 1:51:44.300\n They're not just expert at one thing.\n\n1:51:44.300 --> 1:51:45.620\n And they're also flexible in the sense\n\n1:51:45.620 --> 1:51:49.660\n that they can switch between things very easily\n\n1:51:49.660 --> 1:51:52.060\n and they can pick up new things very quickly\n\n1:51:52.060 --> 1:51:57.060\n because they very ably see what a new task has in common\n\n1:51:57.620 --> 1:51:59.420\n with other things that they've done.\n\n1:52:01.860 --> 1:52:05.340\n And that's something that our AI systems\n\n1:52:05.340 --> 1:52:08.060\n just blatantly do not have.\n\n1:52:09.100 --> 1:52:11.380\n There are some people who like to argue\n\n1:52:11.380 --> 1:52:13.740\n that deep learning and deep RL\n\n1:52:13.740 --> 1:52:17.080\n are simply wrong for getting that kind of flexibility.\n\n1:52:17.080 --> 1:52:20.060\n I don't share that belief,\n\n1:52:20.060 --> 1:52:22.620\n but the simpler fact of the matter\n\n1:52:22.620 --> 1:52:23.860\n is we're not building things yet\n\n1:52:23.860 --> 1:52:25.500\n that do have that kind of flexibility.\n\n1:52:25.500 --> 1:52:28.700\n And I think the attention of a large part\n\n1:52:28.700 --> 1:52:31.500\n of the AI community is starting to pivot to that question.\n\n1:52:31.500 --> 1:52:32.460\n How do we get that?\n\n1:52:33.460 --> 1:52:38.060\n That's gonna lead to a focus on abstraction.\n\n1:52:38.060 --> 1:52:40.460\n It's gonna lead to a focus on\n\n1:52:40.460 --> 1:52:43.620\n what in psychology we call cognitive control,\n\n1:52:43.620 --> 1:52:45.900\n which is the ability to switch between tasks,\n\n1:52:45.900 --> 1:52:49.300\n the ability to quickly put together a program of behavior\n\n1:52:49.300 --> 1:52:51.740\n that you've never executed before,\n\n1:52:51.740 --> 1:52:55.260\n but you know makes sense for a particular set of demands.\n\n1:52:55.260 --> 1:52:58.140\n It's very closely related to what the prefrontal cortex does\n\n1:52:59.140 --> 1:53:01.060\n on the neuroscience side.\n\n1:53:01.060 --> 1:53:05.380\n So I think it's gonna be an interesting new chapter.\n\n1:53:05.380 --> 1:53:07.420\n So that's the reasoning side and cognition side,\n\n1:53:07.420 --> 1:53:10.540\n but let me ask the over romanticized question.\n\n1:53:10.540 --> 1:53:13.700\n Do you think we'll ever engineer an AGI system\n\n1:53:13.700 --> 1:53:17.140\n that we humans would be able to love\n\n1:53:17.140 --> 1:53:19.580\n and that would love us back?\n\n1:53:19.580 --> 1:53:23.060\n So have that level and depth of connection?\n\n1:53:26.220 --> 1:53:27.860\n I love that question.\n\n1:53:27.860 --> 1:53:31.980\n And it relates closely to things\n\n1:53:31.980 --> 1:53:33.900\n that I've been thinking about a lot lately,\n\n1:53:33.900 --> 1:53:36.620\n in the context of this human AI research.\n\n1:53:36.620 --> 1:53:39.940\n There's social psychology research\n\n1:53:41.140 --> 1:53:44.940\n in particular by Susan Fisk at Princeton\n\n1:53:44.940 --> 1:53:47.100\n the department where I used to work,\n\n1:53:48.420 --> 1:53:53.420\n where she dissects human attitudes toward other humans\n\n1:53:54.500 --> 1:53:59.500\n into a sort of two dimensional scheme.\n\n1:53:59.900 --> 1:54:03.940\n And one dimension is about ability.\n\n1:54:03.940 --> 1:54:08.220\n How able, how capable is this other person?\n\n1:54:10.100 --> 1:54:11.780\n But the other dimension is warmth.\n\n1:54:11.780 --> 1:54:15.580\n So you can imagine another person who's very skilled\n\n1:54:15.580 --> 1:54:17.740\n and capable, but is very cold.\n\n1:54:19.540 --> 1:54:22.500\n And you wouldn't really like highly,\n\n1:54:22.500 --> 1:54:25.140\n you might have some reservations about that other person.\n\n1:54:26.660 --> 1:54:28.980\n But there's also a kind of reservation\n\n1:54:28.980 --> 1:54:31.020\n that we might have about another person\n\n1:54:31.020 --> 1:54:34.860\n who elicits in us or displays a lot of human warmth,\n\n1:54:34.860 --> 1:54:37.940\n but is not good at getting things done.\n\n1:54:37.940 --> 1:54:40.940\n We reserve our greatest esteem really\n\n1:54:40.940 --> 1:54:43.820\n for people who are both highly capable\n\n1:54:43.820 --> 1:54:47.300\n and also quite warm.\n\n1:54:47.300 --> 1:54:49.820\n That's like the best of the best.\n\n1:54:49.820 --> 1:54:53.300\n This isn't a normative statement I'm making.\n\n1:54:53.300 --> 1:54:55.780\n This is just an empirical statement.\n\n1:54:55.780 --> 1:54:57.180\n This is what humans seem...\n\n1:54:57.180 --> 1:54:59.740\n These are the two dimensions that people seem to kind of like\n\n1:54:59.740 --> 1:55:02.660\n along which people size other people up.\n\n1:55:02.660 --> 1:55:03.980\n And in AI research,\n\n1:55:03.980 --> 1:55:06.580\n there's a lot of people who think that humans are\n\n1:55:06.580 --> 1:55:08.700\n very capable, and in AI research,\n\n1:55:08.700 --> 1:55:11.420\n we really focus on this capability thing.\n\n1:55:11.420 --> 1:55:13.420\n We want our agents to be able to do stuff.\n\n1:55:13.420 --> 1:55:15.460\n This thing can play go at a superhuman level.\n\n1:55:15.460 --> 1:55:16.860\n That's awesome.\n\n1:55:16.860 --> 1:55:18.700\n But that's only one dimension.\n\n1:55:18.700 --> 1:55:20.060\n What about the other dimension?\n\n1:55:20.060 --> 1:55:25.060\n What would it mean for an AI system to be warm?\n\n1:55:25.060 --> 1:55:27.620\n And I don't know, maybe there are easy solutions here.\n\n1:55:27.620 --> 1:55:30.620\n Like we can put a face on our AI systems.\n\n1:55:30.620 --> 1:55:32.020\n It's cute, it has big ears.\n\n1:55:32.020 --> 1:55:33.820\n I mean, that's probably part of it.\n\n1:55:33.820 --> 1:55:36.540\n But I think it also has to do with a pattern of behavior.\n\n1:55:36.540 --> 1:55:40.180\n A pattern of what would it mean for an AI system\n\n1:55:40.180 --> 1:55:43.460\n to display caring, compassionate behavior\n\n1:55:43.460 --> 1:55:47.740\n in a way that actually made us feel like it was for real?\n\n1:55:47.740 --> 1:55:49.940\n That we didn't feel like it was simulated.\n\n1:55:49.940 --> 1:55:51.900\n We didn't feel like we were being duped.\n\n1:55:53.100 --> 1:55:55.740\n To me, people talk about the Turing test\n\n1:55:55.740 --> 1:55:57.860\n or some descendant of it.\n\n1:55:57.860 --> 1:56:01.140\n I feel like that's the ultimate Turing test.\n\n1:56:01.140 --> 1:56:05.460\n Is there an AI system that can not only convince us\n\n1:56:05.460 --> 1:56:07.180\n that it knows how to reason\n\n1:56:07.180 --> 1:56:09.100\n and it knows how to interpret language,\n\n1:56:09.100 --> 1:56:12.700\n but that we're comfortable saying,\n\n1:56:12.700 --> 1:56:14.580\n yeah, that AI system's a good guy.\n\n1:56:15.980 --> 1:56:18.700\n On the warmth scale, whatever warmth is,\n\n1:56:18.700 --> 1:56:20.860\n we kind of intuitively understand it,\n\n1:56:20.860 --> 1:56:25.060\n but we also wanna be able to, yeah,\n\n1:56:25.060 --> 1:56:29.180\n we don't understand it explicitly enough yet\n\n1:56:29.180 --> 1:56:30.940\n to be able to engineer it.\n\n1:56:30.940 --> 1:56:31.780\n Exactly.\n\n1:56:31.780 --> 1:56:33.620\n And that's an open scientific question.\n\n1:56:33.620 --> 1:56:35.340\n You kind of alluded it several times\n\n1:56:35.340 --> 1:56:37.220\n in the human AI interaction.\n\n1:56:37.220 --> 1:56:38.900\n That's a question that should be studied\n\n1:56:38.900 --> 1:56:42.300\n and probably one of the most important questions\n\n1:56:42.300 --> 1:56:43.540\n as we move to AGI.\n\n1:56:43.540 --> 1:56:46.020\n We humans are so good at it.\n\n1:56:46.020 --> 1:56:46.860\n Yeah.\n\n1:56:46.860 --> 1:56:50.140\n It's not just that we're born warm.\n\n1:56:50.140 --> 1:56:53.060\n I suppose some people are warmer than others\n\n1:56:53.060 --> 1:56:55.700\n given whatever genes they manage to inherit.\n\n1:56:55.700 --> 1:57:00.700\n But there are also learned skills involved.\n\n1:57:01.620 --> 1:57:04.740\n There are ways of communicating to other people\n\n1:57:04.740 --> 1:57:07.740\n that you care, that they matter to you,\n\n1:57:07.740 --> 1:57:11.100\n that you're enjoying interacting with them, right?\n\n1:57:11.100 --> 1:57:14.140\n And we learn these skills from one another.\n\n1:57:14.140 --> 1:57:16.740\n And it's not out of the question\n\n1:57:16.740 --> 1:57:20.020\n that we could build engineered systems.\n\n1:57:20.020 --> 1:57:21.460\n I think it's hopeless, as you say,\n\n1:57:21.460 --> 1:57:23.580\n that we could somehow hand design\n\n1:57:23.580 --> 1:57:26.100\n these sorts of behaviors.\n\n1:57:26.100 --> 1:57:27.060\n But it's not out of the question\n\n1:57:27.060 --> 1:57:30.060\n that we could build systems that kind of,\n\n1:57:30.060 --> 1:57:34.460\n we instill in them something that sets them out\n\n1:57:34.460 --> 1:57:35.980\n in the right direction,\n\n1:57:35.980 --> 1:57:39.580\n so that they end up learning what it is\n\n1:57:39.580 --> 1:57:40.540\n to interact with humans\n\n1:57:40.540 --> 1:57:44.180\n in a way that's gratifying to humans.\n\n1:57:44.180 --> 1:57:47.500\n I mean, honestly, if that's not where we're headed,\n\n1:57:49.220 --> 1:57:50.340\n I want out.\n\n1:57:50.340 --> 1:57:54.940\n I think it's exciting as a scientific problem,\n\n1:57:54.940 --> 1:57:56.820\n just as you described.\n\n1:57:56.820 --> 1:57:59.500\n I honestly don't see a better way to end it\n\n1:57:59.500 --> 1:58:01.180\n than talking about warmth and love.\n\n1:58:01.180 --> 1:58:05.380\n And Matt, I don't think I've ever had such a wonderful\n\n1:58:05.380 --> 1:58:07.540\n conversation where my questions were so bad\n\n1:58:07.540 --> 1:58:09.380\n and your answers were so beautiful.\n\n1:58:09.380 --> 1:58:10.740\n So I deeply appreciate it.\n\n1:58:10.740 --> 1:58:11.580\n I really enjoyed it.\n\n1:58:11.580 --> 1:58:12.420\n Thanks for talking to me.\n\n1:58:12.420 --> 1:58:13.260\n Well, it's been very fun.\n\n1:58:13.260 --> 1:58:14.580\n As you can probably tell,\n\n1:58:17.140 --> 1:58:19.020\n there's something I like about kind of thinking\n\n1:58:19.020 --> 1:58:21.060\n outside the box and like,\n\n1:58:21.060 --> 1:58:22.940\n so it's good having an opportunity to do that.\n\n1:58:22.940 --> 1:58:23.780\n Awesome.\n\n1:58:23.780 --> 1:58:25.620\n Thanks so much for doing it.\n\n1:58:25.620 --> 1:58:27.180\n Thanks for listening to this conversation\n\n1:58:27.180 --> 1:58:28.420\n with Matt Bopenik.\n\n1:58:28.420 --> 1:58:30.540\n And thank you to our sponsors,\n\n1:58:30.540 --> 1:58:32.300\n The Jordan Harbinger Show\n\n1:58:32.300 --> 1:58:36.140\n and Magic Spoon Low Carb Keto Cereal.\n\n1:58:36.140 --> 1:58:38.020\n Please consider supporting this podcast\n\n1:58:38.020 --> 1:58:41.020\n by going to jordanharbinger.com slash lex\n\n1:58:41.020 --> 1:58:44.940\n and also going to magicspoon.com slash lex\n\n1:58:44.940 --> 1:58:48.220\n and using code lex at checkout.\n\n1:58:48.220 --> 1:58:50.900\n Click the links, buy all the stuff.\n\n1:58:50.900 --> 1:58:52.860\n It's the best way to support this podcast\n\n1:58:52.860 --> 1:58:57.260\n and the journey I'm on in my research and the startup.\n\n1:58:57.260 --> 1:58:59.580\n If you enjoy this thing, subscribe on YouTube,\n\n1:58:59.580 --> 1:59:02.380\n review it with the five stars in Apple Podcasts,\n\n1:59:02.380 --> 1:59:05.380\n support it on Patreon, follow on Spotify\n\n1:59:05.380 --> 1:59:08.220\n or connect with me on Twitter at lexfreedman.\n\n1:59:08.220 --> 1:59:12.220\n Again, spelled miraculously without the E,\n\n1:59:12.220 --> 1:59:15.060\n just F R I D M A N.\n\n1:59:15.060 --> 1:59:17.100\n And now let me leave you with some words\n\n1:59:17.100 --> 1:59:20.820\n from neurologist V.S. Amarachandran.\n\n1:59:20.820 --> 1:59:23.340\n How can a three pound mass of jelly\n\n1:59:23.340 --> 1:59:26.620\n that you can hold in your palm imagine angels,\n\n1:59:26.620 --> 1:59:28.700\n contemplate the meaning of an infinity\n\n1:59:28.700 --> 1:59:31.740\n and even question its own place in the cosmos?\n\n1:59:31.740 --> 1:59:35.660\n Especially awe inspiring is the fact that any single brain,\n\n1:59:35.660 --> 1:59:38.580\n including yours, is made up of atoms\n\n1:59:38.580 --> 1:59:40.060\n that were forged in the hearts\n\n1:59:40.060 --> 1:59:45.060\n of countless far flung stars billions of years ago.\n\n1:59:45.500 --> 1:59:48.340\n These particles drifted for eons and light years\n\n1:59:48.340 --> 1:59:53.180\n until gravity and change brought them together here now.\n\n1:59:53.180 --> 1:59:57.540\n These atoms now form a conglomerate, your brain,\n\n1:59:57.540 --> 2:00:00.860\n that can not only ponder the very stars they gave at birth,\n\n2:00:00.860 --> 2:00:04.180\n but can also think about its own ability to think\n\n2:00:04.180 --> 2:00:07.820\n and wonder about its own ability to wander.\n\n2:00:07.820 --> 2:00:10.660\n With the arrival of humans, it has been said,\n\n2:00:10.660 --> 2:00:14.580\n the universe has suddenly become conscious of itself.\n\n2:00:14.580 --> 2:00:18.620\n This truly is the greatest mystery of all.\n\n2:00:18.620 --> 2:00:31.620\n Thank you for listening and hope to see you next time.\n\n"
}