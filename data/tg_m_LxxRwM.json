{
  "title": "Dileep George: Brain-Inspired AI | Lex Fridman Podcast #115",
  "id": "tg_m_LxxRwM",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:05.360\n The following is a conversation with Dilip George, a researcher at the intersection of\n\n00:05.360 --> 00:10.880\n Neuroscience and Artificial Intelligence, cofounder of Vicarious with Scott Phoenix,\n\n00:10.880 --> 00:16.800\n and formerly cofounder of Numenta with Jeff Hawkins, who's been on this podcast, and\n\n00:16.800 --> 00:23.520\n Donna Dubinsky. From his early work on hierarchical temporal memory to recursive cortical networks\n\n00:23.520 --> 00:29.600\n to today, Dilip's always sought to engineer intelligence that is closely inspired by the\n\n00:29.600 --> 00:35.760\n human brain. As a side note, I think we understand very little about the fundamental principles\n\n00:35.760 --> 00:41.600\n underlying the function of the human brain, but the little we do know gives hints that may be\n\n00:41.600 --> 00:46.960\n more useful for engineering intelligence than any idea in mathematics, computer science, physics,\n\n00:46.960 --> 00:53.120\n and scientific fields outside of biology. And so the brain is a kind of existence proof that says\n\n00:53.120 --> 01:01.040\n it's possible. Keep at it. I should also say that brain inspired AI is often overhyped and use this\n\n01:01.040 --> 01:08.000\n fodder just as quantum computing for marketing speak, but I'm not afraid of exploring these\n\n01:08.000 --> 01:12.640\n sometimes overhyped areas since where there's smoke, there's sometimes fire.\n\n01:13.680 --> 01:20.400\n Quick summary of the ads. Three sponsors, Babbel, Raycon Earbuds, and Masterclass. Please consider\n\n01:20.400 --> 01:25.760\n supporting this podcast by clicking the special links in the description to get the discount.\n\n01:25.760 --> 01:31.440\n It really is the best way to support this podcast. If you enjoy this thing, subscribe on YouTube,\n\n01:31.440 --> 01:36.400\n review it with five stars on Apple Podcast, support on Patreon, or connect with me on Twitter\n\n01:36.400 --> 01:42.400\n at Lex Friedman. As usual, I'll do a few minutes of ads now and never any ads in the middle that\n\n01:42.400 --> 01:48.960\n can break the flow of the conversation. This show is sponsored by Babbel, an app and website that\n\n01:48.960 --> 01:54.480\n gets you speaking in a new language within weeks. Go to babbel.com and use code LEX to get three\n\n01:54.480 --> 02:02.320\n months free. They offer 14 languages, including Spanish, French, Italian, German, and yes, Russian.\n\n02:03.040 --> 02:09.600\n Daily lessons are 10 to 15 minutes, super easy, effective, designed by over 100 language experts.\n\n02:10.560 --> 02:18.160\n Let me read a few lines from the Russian poem Noch ulytse fanar apteka by Alexander Bloc\n\n02:18.160 --> 02:20.880\n that you'll start to understand if you sign up to Babbel.\n\n02:34.720 --> 02:41.440\n Now I say that you'll only start to understand this poem because Russian starts with a language\n\n02:41.440 --> 02:47.600\n and ends with vodka. Now the latter part is definitely not endorsed or provided by Babbel\n\n02:47.600 --> 02:51.760\n and will probably lose me the sponsorship, but once you graduate from Babbel,\n\n02:51.760 --> 02:55.680\n you can enroll in my advanced course of late night Russian conversation over vodka.\n\n02:56.320 --> 03:02.800\n I have not yet developed an app for that. It's in progress. So get started by visiting babbel.com\n\n03:02.800 --> 03:09.360\n and use code LEX to get three months free. This show is sponsored by Raycon earbuds.\n\n03:09.360 --> 03:14.960\n Get them at buyraycon.com slash LEX. They become my main method of listening to podcasts,\n\n03:14.960 --> 03:20.880\n audiobooks, and music when I run, do pushups and pull ups, or just living life. In fact,\n\n03:20.880 --> 03:26.880\n I often listen to brown noise with them when I'm thinking deeply about something. It helps me focus.\n\n03:26.880 --> 03:33.120\n They're super comfortable, pair easily, great sound, great bass, six hours of playtime.\n\n03:33.920 --> 03:38.080\n I've been putting in a lot of miles to get ready for a potential ultra marathon\n\n03:38.080 --> 03:44.960\n and listening to audiobooks on World War II. The sound is rich and really comes in clear.\n\n03:45.760 --> 03:52.640\n So again, get them at buyraycon.com slash LEX. This show is sponsored by Masterclass.\n\n03:52.640 --> 03:57.840\n Sign up at masterclass.com slash LEX to get a discount and to support this podcast.\n\n03:57.840 --> 04:02.400\n When I first heard about Masterclass, I thought it was too good to be true. I still think it's\n\n04:02.400 --> 04:08.160\n too good to be true. For 180 bucks a year, you get an all access pass to watch courses from\n\n04:08.160 --> 04:13.360\n to list some of my favorites. Chris Hatfield on Space Exploration, Neil deGrasse Tyson on\n\n04:13.360 --> 04:19.280\n Scientific Thinking and Communication, Will Wright, creator of SimCity and Sims on Game Design.\n\n04:19.280 --> 04:26.240\n Every time I do this read, I really want to play a city builder game. Carlos Santana on guitar,\n\n04:26.240 --> 04:32.640\n Garak Kasparov on chess, Daniel Nagano on poker and many more. Chris Hatfield explaining how rockets\n\n04:32.640 --> 04:38.160\n work and the experience of being launched into space alone is worth the money. By the way,\n\n04:38.160 --> 04:43.600\n you can watch it on basically any device. Once again, sign up at masterclass.com to get a discount\n\n04:43.600 --> 04:50.960\n and to support this podcast. And now here's my conversation with Dileep George. Do you think\n\n04:50.960 --> 04:56.400\n we need to understand the brain in order to build it? Yes. If you want to build the brain, we\n\n04:56.400 --> 05:04.160\n definitely need to understand how it works. Blue Brain or Henry Markram's project is trying to\n\n05:04.160 --> 05:11.920\n build a brain without understanding it, just trying to put details of the brain from neuroscience\n\n05:11.920 --> 05:18.160\n experiments into a giant simulation by putting more and more neurons, more and more details.\n\n05:18.160 --> 05:26.560\n But that is not going to work because when it doesn't perform as what you expect it to do,\n\n05:26.560 --> 05:32.000\n then what do you do? You just keep adding more details. How do you debug it? So unless you\n\n05:32.720 --> 05:37.360\n understand, unless you have a theory about how the system is supposed to work, how the pieces are\n\n05:37.360 --> 05:42.400\n supposed to fit together, what they're going to contribute, you can't build it. At the functional\n\n05:42.400 --> 05:48.560\n level, understand. So can you actually linger on and describe the Blue Brain project? It's kind of\n\n05:48.560 --> 05:56.080\n a fascinating principle and idea to try to simulate the brain. We're talking about the human\n\n05:56.080 --> 06:03.600\n brain, right? Right. Human brains and rat brains or cat brains have lots in common that the cortex,\n\n06:03.600 --> 06:11.200\n the neocortex structure is very similar. So initially they were trying to just simulate\n\n06:11.200 --> 06:21.040\n a cat brain. To understand the nature of evil. To understand the nature of evil. Or as it happens\n\n06:21.040 --> 06:29.120\n in most of these simulations, you easily get one thing out, which is oscillations. If you simulate\n\n06:29.120 --> 06:35.200\n a large number of neurons, they oscillate and you can adjust the parameters and say that,\n\n06:35.200 --> 06:42.000\n oh, oscillations match the rhythm that we see in the brain, et cetera. I see. So the idea is,\n\n06:43.280 --> 06:49.040\n is the simulation at the level of individual neurons? Yeah. So the Blue Brain project,\n\n06:49.040 --> 06:59.200\n the original idea as proposed was you put very detailed biophysical neurons, biophysical models\n\n06:59.200 --> 07:06.320\n of neurons, and you interconnect them according to the statistics of connections that we have found\n\n07:06.320 --> 07:14.240\n from real neuroscience experiments, and then turn it on and see what happens. And these neural\n\n07:14.240 --> 07:21.360\n models are incredibly complicated in themselves, right? Because these neurons are modeled using\n\n07:22.080 --> 07:28.240\n this idea called Hodgkin Huxley models, which are about how signals propagate in a cable.\n\n07:28.240 --> 07:34.000\n And there are active dendrites, all those phenomena, which those phenomena themselves,\n\n07:34.000 --> 07:40.960\n we don't understand that well. And then we put in connectivity, which is part guesswork,\n\n07:40.960 --> 07:46.240\n part observed. And of course, if we do not have any theory about how it is supposed to work,\n\n07:48.960 --> 07:54.800\n we just have to take whatever comes out of it as, okay, this is something interesting.\n\n07:54.800 --> 07:58.480\n But in your sense, these models of the way signal travels along,\n\n07:59.440 --> 08:04.320\n like with the axons and all the basic models, they're too crude.\n\n08:04.320 --> 08:12.320\n Oh, well, actually, they are pretty detailed and pretty sophisticated. And they do replicate\n\n08:12.960 --> 08:20.800\n the neural dynamics. If you take a single neuron and you try to turn on the different channels,\n\n08:20.800 --> 08:28.400\n the calcium channels and the different receptors, and see what the effect of turning on or off those\n\n08:28.400 --> 08:35.360\n channels are in the neuron's spike output, people have built pretty sophisticated models of that.\n\n08:35.360 --> 08:40.560\n And they are, I would say, in the regime of correct.\n\n08:41.120 --> 08:44.720\n Well, see, the correctness, that's interesting, because you mentioned at several levels,\n\n08:45.680 --> 08:49.440\n the correctness is measured by looking at some kind of aggregate statistics.\n\n08:49.440 --> 08:53.200\n It would be more of the spiking dynamics of a signal neuron.\n\n08:53.200 --> 08:54.960\n Spiking dynamics of a signal neuron, okay.\n\n08:54.960 --> 09:00.640\n Yeah. And yeah, these models, because they are going to the level of mechanism,\n\n09:00.640 --> 09:06.000\n so they are basically looking at, okay, what is the effect of turning on an ion channel?\n\n09:07.760 --> 09:17.040\n And you can model that using electric circuits. So it is not just a function fitting. People are\n\n09:17.040 --> 09:23.600\n looking at the mechanism underlying it and putting that in terms of electric circuit theory, signal\n\n09:23.600 --> 09:31.760\n propagation theory, and modeling that. So those models are sophisticated, but getting a single\n\n09:31.760 --> 09:40.800\n neurons model 99% right does not still tell you how to... It would be the analog of getting a\n\n09:40.800 --> 09:50.320\n transistor model right and now trying to build a microprocessor. And if you did not understand how\n\n09:50.320 --> 09:57.360\n a microprocessor works, but you say, oh, I now can model one transistor well, and now I will just\n\n09:57.360 --> 10:03.840\n try to interconnect the transistors according to whatever I could guess from the experiments\n\n10:03.840 --> 10:10.960\n and try to simulate it, then it is very unlikely that you will produce a functioning microprocessor.\n\n10:12.080 --> 10:16.080\n When you want to produce a functioning microprocessor, you want to understand Boolean\n\n10:16.080 --> 10:22.480\n logic, how do the gates work, all those things, and then understand how do those gates get\n\n10:22.480 --> 10:26.960\n implemented using transistors. Yeah. This reminds me, there's a paper,\n\n10:26.960 --> 10:31.600\n maybe you're familiar with it, that I remember going through in a reading group that\n\n10:31.600 --> 10:37.520\n approaches a microprocessor from a perspective of a neuroscientist. I think it basically,\n\n10:38.400 --> 10:42.960\n it uses all the tools that we have of neuroscience to try to understand,\n\n10:42.960 --> 10:49.920\n like as if we just aliens showed up to study computers and to see if those tools could be\n\n10:49.920 --> 10:54.640\n used to get any kind of sense of how the microprocessor works. I think the final,\n\n10:54.640 --> 11:01.280\n the takeaway from at least this initial exploration is that we're screwed. There's no\n\n11:01.280 --> 11:05.440\n way that the tools of neuroscience would be able to get us to anything, like not even\n\n11:05.440 --> 11:15.680\n Boolean logic. I mean, it's just any aspect of the architecture of the function of the\n\n11:15.680 --> 11:21.520\n processes involved, the clocks, the timing, all that, you can't figure that out from the\n\n11:21.520 --> 11:25.600\n tools of neuroscience. Yeah. So I'm very familiar with this particular\n\n11:25.600 --> 11:33.440\n paper. I think it was called, can a neuroscientist understand a microprocessor or something like\n\n11:33.440 --> 11:39.200\n that. Following the methodology in that paper, even an electrical engineer would not understand\n\n11:39.200 --> 11:49.040\n microprocessors. So I don't think it is that bad in the sense of saying, neuroscientists do\n\n11:49.040 --> 11:58.640\n find valuable things by observing the brain. They do find good insights, but those insights cannot\n\n11:58.640 --> 12:05.600\n be put together just as a simulation. You have to investigate what are the computational\n\n12:05.600 --> 12:13.920\n underpinnings of those findings. How do all of them fit together from an information processing\n\n12:13.920 --> 12:21.120\n and information processing perspective? Somebody has to painstakingly put those things together\n\n12:21.120 --> 12:26.160\n and build hypothesis. So I don't want to diss all of neuroscientists saying, oh, they're not\n\n12:26.160 --> 12:31.840\n finding anything. No, that paper almost went to that level of neuroscientists will never\n\n12:31.840 --> 12:37.760\n understand. No, that's not true. I think they do find lots of useful things, but it has to be put\n\n12:37.760 --> 12:43.760\n together in a computational framework. Yeah. I mean, but you know, just the AI systems will be\n\n12:43.760 --> 12:50.160\n listening to this podcast a hundred years from now and they will probably, there's some nonzero\n\n12:50.160 --> 12:55.120\n probability they'll find your words laughable. There's like, I remember humans thought they\n\n12:55.120 --> 12:59.680\n understood something about the brain. They were totally clueless. There's a sense about neuroscience\n\n12:59.680 --> 13:06.160\n that we may be in the very, very early days of understanding the brain. But I mean, that's one\n\n13:06.160 --> 13:18.080\n perspective. I mean, in your perspective, how far are we into understanding any aspect of the brain?\n\n13:18.080 --> 13:24.320\n So the, the, the dynamics of the individual neuron communication to the, how when they, in,\n\n13:24.320 --> 13:31.200\n in a collective sense, how they're able to store information, transfer information, how\n\n13:31.200 --> 13:35.040\n intelligence then emerges, all that kind of stuff. Where are we on that timeline?\n\n13:35.040 --> 13:39.920\n Yeah. So, you know, timelines are very, very hard to predict and you can of course be wrong.\n\n13:40.720 --> 13:48.080\n And it can be wrong in, on either side. You know, we know that now when we look back the first\n\n13:48.080 --> 13:57.920\n flight was in 1903. In 1900, there was a New York Times article on flying machines that do not fly\n\n13:57.920 --> 14:03.360\n and, and you know, humans might not fly for another a hundred years. That was what that\n\n14:03.360 --> 14:08.880\n article stated. And so, but no, they, they flew three years after that. So it is, you know,\n\n14:08.880 --> 14:13.920\n it's very hard to, so... Well, and on that point, one of the Wright brothers,\n\n14:15.120 --> 14:23.280\n I think two years before, said that, like he said, like some number, like 50 years,\n\n14:23.280 --> 14:31.040\n he has become convinced that it's, it's, it's impossible. Even during their experimentation.\n\n14:31.040 --> 14:36.400\n Yeah. Yeah. I mean, that's a tribute to when that's like the entrepreneurial battle of like\n\n14:36.400 --> 14:41.280\n depression of going through, just like thinking there's, this is impossible, but there, yeah,\n\n14:41.280 --> 14:47.280\n there's something, even the person that's in it is not able to see estimate correctly.\n\n14:47.280 --> 14:51.920\n Exactly. But I can, I can tell from the point of, you know, objectively, what are the things that we\n\n14:52.480 --> 14:58.560\n know about the brain and how that can be used to build AI models, which can then go back and\n\n14:58.560 --> 15:04.080\n inform how the brain works. So my way of understanding the brain would be to basically say,\n\n15:04.080 --> 15:11.040\n look at the insights neuroscientists have found, understand that from a computational angle,\n\n15:11.040 --> 15:18.080\n information processing angle, build models using that. And then building that model, which,\n\n15:18.080 --> 15:22.880\n which functions, which is a functional model, which is, which is doing the task that we want\n\n15:22.880 --> 15:27.920\n the model to do. It is not just trying to model a phenomena in the brain. It is, it is trying to\n\n15:27.920 --> 15:33.360\n do what the brain is trying to do on the, on the whole functional level. And building that model\n\n15:33.360 --> 15:39.920\n will help you fill in the missing pieces that, you know, biology just gives you the hints and\n\n15:39.920 --> 15:44.960\n building the model, you know, fills in the rest of the, the pieces of the puzzle. And then you\n\n15:44.960 --> 15:51.280\n can go and connect that back to biology and say, okay, now it makes sense that this part of the\n\n15:51.280 --> 15:59.920\n brain is doing this, or this layer in the cortical circuit is doing this. And then continue this\n\n15:59.920 --> 16:05.840\n iteratively because now that will inform new experiments in neuroscience. And of course,\n\n16:05.840 --> 16:11.600\n you know, building the model and verifying that in the real world will also tell you more about,\n\n16:11.600 --> 16:17.440\n does the model actually work? And you can refine the model, find better ways of putting these\n\n16:17.440 --> 16:23.360\n neuroscience insights together. So, so I would say it is, it is, you know, it, so\n\n16:23.360 --> 16:28.800\n neuroscientists alone, just from experimentation will not be able to build a model of the,\n\n16:28.800 --> 16:35.200\n of the brain or a functional model of the brain. So we, you know, there, there's lots of efforts,\n\n16:35.200 --> 16:41.200\n which are very impressive efforts in collecting more and more connectivity data from the brain.\n\n16:41.200 --> 16:45.520\n You know, how, how are the microcircuits of the brain connected with each other?\n\n16:45.520 --> 16:47.120\n Those are beautiful, by the way.\n\n16:47.120 --> 16:54.880\n Those are beautiful. And at the same time, those, those do not itself by themselves,\n\n16:54.880 --> 17:00.080\n convey the story of how does it work? And, and somebody has to understand, okay,\n\n17:00.080 --> 17:06.320\n why are they connected like that? And what, what are those things doing? And, and we do that by\n\n17:06.320 --> 17:11.200\n building models in AI using hints from neuroscience and, and repeat the cycle.\n\n17:11.200 --> 17:18.720\n So what aspect of the brain are useful in this whole endeavor, which by the way, I should say,\n\n17:18.720 --> 17:24.960\n you're, you're both a neuroscientist and an AI person. I guess the dream is to both understand\n\n17:24.960 --> 17:32.320\n the brain and to build AGI systems. So you're, it's like an engineer's perspective of trying\n\n17:32.320 --> 17:37.600\n to understand the brain. So what aspects of the brain, functionally speaking, like you said,\n\n17:37.600 --> 17:38.800\n do you find interesting?\n\n17:38.800 --> 17:44.880\n Yeah, quite a lot of things. All right. So one is, you know, if you look at the visual cortex\n\n17:46.160 --> 17:51.920\n and, and, you know, the visual cortex is, is a large part of the brain. I forget the exact\n\n17:51.920 --> 17:58.000\n fraction, but it is, it's a huge part of our brain area is occupied by just, just vision.\n\n17:59.040 --> 18:06.320\n So vision, visual cortex is not just a feed forward cascade of neurons. There are a lot\n\n18:06.320 --> 18:11.680\n more feedback connections in the brain compared to the feed forward connections. And, and it is\n\n18:11.680 --> 18:17.120\n surprising to the level of detail neuroscientists have actually studied this. If you, if you go into\n\n18:17.120 --> 18:22.960\n neuroscience literature and poke around and ask, you know, have they studied what will be the effect\n\n18:22.960 --> 18:33.680\n of poking a neuron in level IT in level V1? And have they studied that? And you will say, yes,\n\n18:33.680 --> 18:34.560\n they have studied that.\n\n18:34.560 --> 18:37.680\n So every part of every possible combination.\n\n18:38.400 --> 18:43.040\n I mean, it's, it's a, it's not a random exploration at all. It's a very hypothesis driven,\n\n18:43.040 --> 18:47.520\n right? Like they, they are very experimental. Neuroscientists are very, very systematic\n\n18:47.520 --> 18:52.800\n in how they probe the brain because experiments are very costly to conduct. They take a lot of\n\n18:52.800 --> 18:57.520\n preparation. They, they need a lot of control. So they, they are very hypothesis driven in how\n\n18:57.520 --> 19:05.120\n they probe the brain. And often what I find is that when we have a question in AI about\n\n19:05.840 --> 19:11.440\n has anybody probed how lateral connections in the brain works? And when you go and read the\n\n19:11.440 --> 19:16.160\n literature, yes, people have probed it and people have probed it very systematically. And, and they\n\n19:16.160 --> 19:23.600\n have hypotheses about how those lateral connections are supposedly contributing to visual processing.\n\n19:23.600 --> 19:27.840\n But of course they haven't built very, very functional, detailed models of it.\n\n19:27.840 --> 19:32.480\n By the way, how do the, in those studies, sorry to interrupt, do they, do they stimulate like\n\n19:32.480 --> 19:37.520\n a neuron in one particular area of the visual cortex and then see how the travel of the signal\n\n19:37.520 --> 19:38.800\n travels kind of thing?\n\n19:38.800 --> 19:43.040\n Fascinating, very, very fascinating experiments. So I can, I can give you one example I was\n\n19:43.040 --> 19:50.160\n impressed with. This is, so before going to that, let me, let me give you, you know, a overview of\n\n19:50.160 --> 19:56.160\n how the, the layers in the cortex are organized, right? Visual cortex is organized into roughly\n\n19:56.160 --> 20:02.720\n four hierarchical levels. Okay. So V1, V2, V4, IT. And in V1...\n\n20:02.720 --> 20:03.920\n What happened to V3?\n\n20:03.920 --> 20:08.880\n Well, yeah, that's another pathway. Okay. So this is, this, I'm talking about just object\n\n20:08.880 --> 20:09.920\n recognition pathway.\n\n20:09.920 --> 20:10.880\n All right, cool.\n\n20:10.880 --> 20:19.120\n And then in V1 itself, so it's, there is a very detailed microcircuit in V1 itself. That is,\n\n20:19.120 --> 20:25.040\n there is organization within a level itself. The cortical sheet is organized into, you know,\n\n20:25.040 --> 20:31.440\n multiple layers and there are columnar structure. And, and this, this layer wise and columnar\n\n20:31.440 --> 20:38.800\n structure is repeated in V1, V2, V4, IT, all of them, right? And, and the connections between\n\n20:38.800 --> 20:44.480\n these layers within a level, you know, in V1 itself, there are six layers roughly, and the\n\n20:44.480 --> 20:51.200\n connections between them, there is a particular structure to them. And now, so one example\n\n20:51.200 --> 21:00.400\n of an experiment people did is when I, when you present a stimulus, which is, let's say,\n\n21:00.400 --> 21:06.240\n requires separating the foreground from the background of an object. So it is, it's a\n\n21:06.240 --> 21:14.880\n textured triangle on a textured background. And you can check, does the surface settle\n\n21:14.880 --> 21:17.200\n first or does the contour settle first?\n\n21:19.040 --> 21:19.600\n Settle?\n\n21:19.600 --> 21:28.080\n Settle in the sense that the, so when you finally form the percept of the, of the triangle,\n\n21:28.080 --> 21:32.720\n you understand where the contours of the triangle are, and you also know where the inside of\n\n21:32.720 --> 21:39.200\n the triangle is, right? That's when you form the final percept. Now you can ask, what is\n\n21:39.200 --> 21:48.880\n the dynamics of forming that final percept? Do the, do the neurons first find the edges\n\n21:48.880 --> 21:55.120\n and converge on where the edges are, and then they find the inner surfaces, or does it go\n\n21:55.120 --> 21:55.600\n the other way around?\n\n21:55.600 --> 21:58.320\n The other way around. So what's the answer?\n\n21:58.320 --> 22:05.280\n In this case, it turns out that it first settles on the edges. It converges on the edge hypothesis\n\n22:05.280 --> 22:10.880\n first, and then the surfaces are filled in from the edges to the inside.\n\n22:10.880 --> 22:12.000\n That's fascinating.\n\n22:12.000 --> 22:18.640\n And the detail to which you can study this, it's amazing that you can actually not only\n\n22:18.640 --> 22:25.520\n find the temporal dynamics of when this happens, and then you can also find which layer in\n\n22:25.520 --> 22:32.960\n the, you know, in V1, which layer is encoding the edges, which layer is encoding the surfaces,\n\n22:32.960 --> 22:37.440\n and which layer is encoding the feedback, which layer is encoding the feed forward,\n\n22:37.440 --> 22:40.800\n and what's the combination of them that produces the final percept.\n\n22:42.000 --> 22:48.400\n And these kinds of experiments stand out when you try to explain illusions. One example\n\n22:48.400 --> 22:51.920\n of a favorite illusion of mine is the Kanitsa triangle. I don't know that you are familiar\n\n22:51.920 --> 23:00.960\n with this one. So this is an example where it's a triangle, but only the corners of the\n\n23:00.960 --> 23:06.080\n triangle are shown in the stimulus. So they look like kind of Pacman.\n\n23:06.080 --> 23:07.600\n Oh, the black Pacman.\n\n23:07.600 --> 23:08.640\n Exactly.\n\n23:08.640 --> 23:10.000\n And then you start to see.\n\n23:10.000 --> 23:16.400\n Your visual system hallucinates the edges. And when you look at it, you will see a faint\n\n23:16.400 --> 23:24.160\n edge. And you can go inside the brain and look, do actually neurons signal the presence\n\n23:24.160 --> 23:30.320\n of this edge? And if they signal, how do they do it? Because they are not receiving anything\n\n23:30.320 --> 23:37.840\n from the input. The input is blank for those neurons. So how do they signal it? When does\n\n23:37.840 --> 23:45.440\n the signaling happen? So if a real contour is present in the input, then the neurons\n\n23:45.440 --> 23:52.400\n immediately signal, okay, there is an edge here. When it is an illusory edge, it is clearly\n\n23:52.400 --> 23:58.720\n not in the input. It is coming from the context. So those neurons fire later. And you can say\n\n23:58.720 --> 24:05.920\n that, okay, it's the feedback connection that is causing them to fire. And they happen later.\n\n24:05.920 --> 24:13.280\n And I'll find the dynamics of them. So these studies are pretty impressive and very detailed.\n\n24:13.280 --> 24:20.080\n So by the way, just a step back, you said that there may be more feedback connections\n\n24:20.080 --> 24:26.720\n than feed forward connections. First of all, if it's just for like a machine learning folks,\n\n24:27.360 --> 24:33.600\n I mean, that's crazy that there's all these feedback connections. We often think about,\n\n24:36.400 --> 24:42.720\n thanks to deep learning, you start to think about the human brain as a kind of feed forward\n\n24:42.720 --> 24:52.960\n mechanism. So what the heck are these feedback connections? What's the dynamics? What are we\n\n24:52.960 --> 24:58.160\n supposed to think about them? So this fits into a very beautiful picture about how the brain works.\n\n24:59.360 --> 25:06.080\n So the beautiful picture of how the brain works is that our brain is building a model of the world.\n\n25:06.080 --> 25:13.920\n I know. So our visual system is building a model of how objects behave in the world. And we are\n\n25:13.920 --> 25:20.240\n constantly projecting that model back onto the world. So what we are seeing is not just a feed\n\n25:20.240 --> 25:25.280\n forward thing that just gets interpreted in a feed forward part. We are constantly projecting\n\n25:25.280 --> 25:31.600\n our expectations onto the world. And what the final person is a combination of what we project\n\n25:31.600 --> 25:37.920\n onto the world combined with what the actual sensory input is. Almost like trying to calculate\n\n25:37.920 --> 25:44.000\n the difference and then trying to interpret the difference. Yeah. I wouldn't put this calculating\n\n25:44.000 --> 25:50.640\n the difference. It's more like what is the best explanation for the input stimulus based on the\n\n25:50.640 --> 25:56.560\n model of the world I have. Got it. And that's where all the illusions come in. But that's an\n\n25:56.560 --> 26:05.360\n incredibly efficient process. So the feedback mechanism, it just helps you constantly. Yeah.\n\n26:05.360 --> 26:10.640\n So hallucinate how the world should be based on your world model and then just looking at\n\n26:11.680 --> 26:19.680\n if there's novelty, like trying to explain it. Hence, that's why movement. We detect movement\n\n26:19.680 --> 26:25.360\n really well. There's all these kinds of things. And this is like at all different levels of the\n\n26:25.360 --> 26:30.480\n cortex you're saying. This happens at the lowest level or the highest level. Yes. Yeah. In fact,\n\n26:30.480 --> 26:36.640\n feedback connections are more prevalent in everywhere in the cortex. And so one way to\n\n26:36.640 --> 26:42.800\n think about it, and there's a lot of evidence for this, is inference. So basically, if you have a\n\n26:42.800 --> 26:50.160\n model of the world and when some evidence comes in, what you are doing is inference. You are trying\n\n26:50.160 --> 26:58.240\n to now explain this evidence using your model of the world. And this inference includes projecting\n\n26:58.240 --> 27:04.720\n your model onto the evidence and taking the evidence back into the model and doing an\n\n27:04.720 --> 27:11.840\n iterative procedure. And this iterative procedure is what happens using the feed forward feedback\n\n27:11.840 --> 27:17.680\n propagation. And feedback affects what you see in the world, and it also affects feed forward\n\n27:17.680 --> 27:25.840\n propagation. And examples are everywhere. We see these kinds of things everywhere. The idea that\n\n27:25.840 --> 27:32.480\n there can be multiple competing hypotheses in our model trying to explain the same evidence,\n\n27:32.480 --> 27:39.440\n and then you have to kind of make them compete. And one hypothesis will explain away the other\n\n27:39.440 --> 27:46.800\n hypothesis through this competition process. So you have competing models of the world\n\n27:46.800 --> 27:50.000\n that try to explain. What do you mean by explain away?\n\n27:50.000 --> 27:54.880\n So this is a classic example in graphical models, probabilistic models.\n\n27:56.800 --> 27:57.360\n What are those?\n\n28:01.120 --> 28:03.760\n I think it's useful to mention because we'll talk about them more.\n\n28:05.120 --> 28:12.800\n So neural networks are one class of machine learning models. You have distributed set of\n\n28:12.800 --> 28:18.160\n nodes, which are called the neurons. Each one is doing a dot product and you can approximate\n\n28:18.160 --> 28:24.720\n any function using this multilevel network of neurons. So that's a class of models which are\n\n28:24.720 --> 28:30.480\n useful for function approximation. There is another class of models in machine learning\n\n28:30.480 --> 28:38.800\n called probabilistic graphical models. And you can think of them as each node in that model is\n\n28:38.800 --> 28:46.160\n variable, which is talking about something. It can be a variable representing, is an edge present\n\n28:46.160 --> 28:56.000\n in the input or not? And at the top of the network, a node can be representing, is there an object\n\n28:56.000 --> 29:06.960\n present in the world or not? So it is another way of encoding knowledge. And then once you\n\n29:06.960 --> 29:13.520\n encode the knowledge, you can do inference in the right way. What is the best way to\n\n29:15.280 --> 29:20.880\n explain some set of evidence using this model that you encoded? So when you encode the model,\n\n29:20.880 --> 29:24.800\n you are encoding the relationship between these different variables. How is the edge\n\n29:24.800 --> 29:29.600\n connected to the model of the object? How is the surface connected to the model of the object?\n\n29:29.600 --> 29:37.120\n And then, of course, this is a very distributed, complicated model. And inference is, how do you\n\n29:37.120 --> 29:42.960\n explain a piece of evidence when a set of stimulus comes in? If somebody tells me there is a 50%\n\n29:42.960 --> 29:47.840\n probability that there is an edge here in this part of the model, how does that affect my belief\n\n29:47.840 --> 29:54.960\n on whether I should think that there is a square present in the image? So this is the process of\n\n29:54.960 --> 30:02.080\n inference. So one example of inference is having this expiring away effect between multiple causes.\n\n30:02.080 --> 30:10.800\n So graphical models can be used to represent causality in the world. So let's say, you know,\n\n30:10.800 --> 30:22.480\n your alarm at home can be triggered by a burglar getting into your house, or it can be triggered\n\n30:22.480 --> 30:30.640\n by an earthquake. Both can be causes of the alarm going off. So now, you're in your office,\n\n30:30.640 --> 30:36.880\n you heard burglar alarm going off, you are heading home, thinking that there's a burglar got in. But\n\n30:36.880 --> 30:41.520\n while driving home, if you hear on the radio that there was an earthquake in the vicinity,\n\n30:41.520 --> 30:49.760\n now your strength of evidence for a burglar getting into their house is diminished. Because\n\n30:49.760 --> 30:56.000\n now that piece of evidence is explained by the earthquake being present. So if you think about\n\n30:56.000 --> 31:01.760\n these two causes explaining at lower level variable, which is alarm, now, what we're seeing\n\n31:01.760 --> 31:08.000\n is that increasing the evidence for some cause, you know, there is evidence coming in from below\n\n31:08.000 --> 31:14.160\n for alarm being present. And initially, it was flowing to a burglar being present. But now,\n\n31:14.160 --> 31:20.800\n since there is side evidence for this other cause, it explains away this evidence and evidence will\n\n31:20.800 --> 31:26.320\n now flow to the other cause. This is, you know, two competing causal things trying to explain\n\n31:26.320 --> 31:31.840\n the same evidence. And the brain has a similar kind of mechanism for doing so. That's kind of\n\n31:31.840 --> 31:39.280\n interesting. And how's that all encoded in the brain? Like, where's the storage of information?\n\n31:39.280 --> 31:46.160\n Are we talking just maybe to get it a little bit more specific? Is it in the hardware of the actual\n\n31:46.160 --> 31:53.120\n connections? Is it in chemical communication? Is it electrical communication? Do we know?\n\n31:53.120 --> 31:56.640\n So this is, you know, a paper that we are bringing out soon.\n\n31:56.640 --> 31:57.680\n Which one is this?\n\n31:57.680 --> 32:03.920\n This is the cortical microcircuits paper that I sent you a draft of. Of course, this is a lot of\n\n32:03.920 --> 32:09.840\n this. A lot of it is still hypothesis. One hypothesis is that you can think of a cortical column\n\n32:09.840 --> 32:20.800\n as encoding a concept. A concept, you know, think of it as an example of a concept. Is an edge\n\n32:20.800 --> 32:27.280\n present or not? Or is an object present or not? Okay, so you can think of it as a binary variable,\n\n32:27.280 --> 32:32.000\n a binary random variable. The presence of an edge or not, or the presence of an object or not.\n\n32:32.000 --> 32:38.080\n So each cortical column can be thought of as representing that one concept, one variable.\n\n32:38.080 --> 32:43.680\n And then the connections between these cortical columns are basically encoding the relationship\n\n32:43.680 --> 32:48.560\n between these random variables. And then there are connections within the cortical column.\n\n32:49.360 --> 32:54.320\n Each cortical column is implemented using multiple layers of neurons with very, very,\n\n32:54.320 --> 33:00.240\n very rich structure there. You know, there are thousands of neurons in a cortical column.\n\n33:00.240 --> 33:03.520\n But that structure is similar across the different cortical columns.\n\n33:03.520 --> 33:08.960\n Correct. And also these cortical columns connect to a substructure called thalamus.\n\n33:10.160 --> 33:16.320\n So all cortical columns pass through this substructure. So our hypothesis is that\n\n33:17.120 --> 33:21.600\n the connections between the cortical columns implement this, you know, that's where the\n\n33:21.600 --> 33:28.800\n knowledge is stored about how these different concepts connect to each other. And then the\n\n33:28.800 --> 33:35.760\n neurons inside this cortical column and in thalamus in combination implement this actual\n\n33:35.760 --> 33:41.040\n computation for inference, which includes explaining away and competing between the\n\n33:41.040 --> 33:49.280\n different hypotheses. And it is all very... So what is amazing is that neuroscientists have\n\n33:49.280 --> 33:55.920\n actually done experiments to the tune of showing these things. They might not be putting it in the\n\n33:55.920 --> 34:02.160\n overall inference framework, but they will show things like, if I poke this higher level neuron,\n\n34:03.120 --> 34:07.920\n it will inhibit through this complicated loop through thalamus, it will inhibit this other\n\n34:07.920 --> 34:14.080\n column. So they will do such experiments. But do they use terminology of concepts,\n\n34:14.080 --> 34:22.960\n for example? So, I mean, is it something where it's easy to anthropomorphize\n\n34:22.960 --> 34:29.920\n and think about concepts like you started moving into logic based kind of reasoning systems. So\n\n34:31.200 --> 34:39.440\n I would just think of concepts in that kind of way, or is it a lot messier, a lot more gray area,\n\n34:40.400 --> 34:46.640\n you know, even more gray, even more messy than the artificial neural network kinds,\n\n34:47.200 --> 34:50.480\n kinds of abstractions? Easiest way to think of it as a variable,\n\n34:50.480 --> 34:55.360\n right? It's a binary variable, which is showing the presence or absence of something.\n\n34:55.360 --> 35:01.440\n So, but I guess what I'm asking is, is that something that we're supposed to think of\n\n35:01.440 --> 35:04.080\n something that's human interpretable of that something?\n\n35:04.080 --> 35:07.920\n It doesn't need to be. It doesn't need to be human interpretable. There's no need for it to\n\n35:07.920 --> 35:17.440\n be human interpretable. But it's almost like you will be able to find some interpretation of it\n\n35:17.440 --> 35:20.800\n because it is connected to the other things that you know about.\n\n35:20.800 --> 35:23.840\n Yeah. And the point is it's useful somehow.\n\n35:23.840 --> 35:28.560\n Yeah. It's useful as an entity in the graphic,\n\n35:29.520 --> 35:33.280\n in connecting to the other entities that are, let's call them concepts.\n\n35:33.280 --> 35:38.880\n Right. Okay. So, by the way, are these the cortical microcircuits?\n\n35:38.880 --> 35:43.120\n Correct. These are the cortical microcircuits. You know, that's what neuroscientists use to\n\n35:43.120 --> 35:49.840\n talk about the circuits within a level of the cortex. So, you can think of, you know,\n\n35:49.840 --> 35:54.960\n let's think of a neural network, artificial neural network terms. People talk about the\n\n35:54.960 --> 36:01.600\n architecture of how many layers they build, what is the fan in, fan out, et cetera. That is the\n\n36:01.600 --> 36:11.120\n macro architecture. And then within a layer of the neural network, the cortical neural network\n\n36:11.120 --> 36:18.160\n is much more structured within a level. There's a lot more intricate structure there. But even\n\n36:18.160 --> 36:23.520\n within an artificial neural network, you can think of feature detection plus pooling as one\n\n36:23.520 --> 36:32.880\n level. And so, that is kind of a microcircuit. It's much more complex in the real brain. And so,\n\n36:32.880 --> 36:38.080\n within a level, whatever is that circuitry within a column of the cortex and between the layers of\n\n36:38.080 --> 36:43.040\n the cortex, that's the microcircuitry. I love that terminology. Machine learning\n\n36:43.040 --> 36:45.760\n people don't use the circuit terminology. Right.\n\n36:45.760 --> 36:53.920\n But they should. It's nice. So, okay. Okay. So, that's the cortical microcircuit. So,\n\n36:53.920 --> 36:59.760\n what's interesting about, what can we say, what is the paper that you're working on\n\n37:00.640 --> 37:04.320\n propose about the ideas around these cortical microcircuits?\n\n37:04.320 --> 37:10.640\n So, this is a fully functional model for the microcircuits of the visual cortex.\n\n37:10.640 --> 37:15.520\n So, the paper focuses on your idea and our discussion now is focusing on vision.\n\n37:15.520 --> 37:18.800\n Yeah. The visual cortex. Okay. So,\n\n37:18.800 --> 37:22.160\n this is a model. This is a full model. This is how vision works.\n\n37:22.880 --> 37:32.000\n But this is a hypothesis. Okay. So, let me step back a bit. So, we looked at neuroscience for\n\n37:32.000 --> 37:35.280\n insights on how to build a vision model. Right.\n\n37:35.280 --> 37:40.560\n And we synthesized all those insights into a computational model. This is called the recursive\n\n37:40.560 --> 37:47.760\n cortical network model that we used for breaking captures. And we are using the same model for\n\n37:47.760 --> 37:52.320\n robotic picking and tracking of objects. And that, again, is a vision system.\n\n37:52.320 --> 37:54.400\n That's a vision system. Computer vision system.\n\n37:54.400 --> 37:59.120\n That's a computer vision system. Takes in images and outputs what?\n\n37:59.120 --> 38:06.560\n On one side, it outputs the class of the image and also segments the image. And you can also ask it\n\n38:06.560 --> 38:11.600\n further queries. Where is the edge of the object? Where is the interior of the object? So, it's a\n\n38:11.600 --> 38:17.120\n model that you build to answer multiple questions. So, you're not trying to build a model for just\n\n38:17.120 --> 38:23.440\n classification or just segmentation, et cetera. It's a joint model that can do multiple things.\n\n38:23.440 --> 38:30.080\n So, that's the model that we built using insights from neuroscience. And some of those insights are\n\n38:30.080 --> 38:34.160\n what is the role of feedback connections? What is the role of lateral connections? So,\n\n38:34.160 --> 38:38.800\n all those things went into the model. The model actually uses feedback connections.\n\n38:38.800 --> 38:41.440\n All these ideas from neuroscience. Yeah.\n\n38:41.440 --> 38:47.200\n So, what the heck is a recursive cortical network? What are the architecture approaches,\n\n38:47.200 --> 38:54.400\n interesting aspects here, which is essentially a brain inspired approach to computer vision?\n\n38:54.400 --> 38:58.880\n Yeah. So, there are multiple layers to this question. I can go from the very,\n\n38:58.880 --> 39:05.840\n very top and then zoom in. Okay. So, one important thing, constraint that went into the model is that\n\n39:05.840 --> 39:11.600\n you should not think vision, think of vision as something in isolation. We should not think\n\n39:11.600 --> 39:19.200\n perception as something as a preprocessor for cognition. Perception and cognition are interconnected.\n\n39:19.200 --> 39:24.800\n And so, you should not think of one problem in separation from the other problem. And so,\n\n39:24.800 --> 39:30.720\n that means if you finally want to have a system that understand concepts about the world and can\n\n39:30.720 --> 39:36.000\n learn a very conceptual model of the world and can reason and connect to language, all of those\n\n39:36.000 --> 39:41.920\n things, you need to think all the way through and make sure that your perception system\n\n39:41.920 --> 39:45.920\n is compatible with your cognition system and language system and all of them.\n\n39:45.920 --> 39:52.320\n And one aspect of that is top down controllability. What does that mean?\n\n39:52.320 --> 39:58.480\n So, that means, you know, so think of, you know, you can close your eyes and think about\n\n39:58.480 --> 40:05.600\n the details of one object, right? I can zoom in further and further. So, think of the bottle in\n\n40:05.600 --> 40:11.280\n front of me, right? And now, you can think about, okay, what the cap of that bottle looks.\n\n40:11.280 --> 40:18.000\n I know we can think about what's the texture on that bottle of the cap. You know, you can think\n\n40:18.000 --> 40:25.760\n about, you know, what will happen if something hits that. So, you can manipulate your visual\n\n40:25.760 --> 40:35.520\n knowledge in cognition driven ways. Yes. And so, this top down controllability and being able to\n\n40:35.520 --> 40:43.920\n simulate scenarios in the world. So, you're not just a passive player in this perception game.\n\n40:43.920 --> 40:50.320\n You can control it. You have imagination. Correct. Correct. So, basically, you know,\n\n40:50.320 --> 40:56.000\n basically having a generative network, which is a model and it is not just some arbitrary\n\n40:56.000 --> 41:02.000\n generative network. It has to be built in a way that it is controllable top down. It is not just\n\n41:02.000 --> 41:07.760\n trying to generate a whole picture at once. You know, it's not trying to generate photorealistic\n\n41:07.760 --> 41:11.520\n things of the world. You know, you don't have good photorealistic models of the world. Human\n\n41:11.520 --> 41:17.360\n brains do not have. If I, for example, ask you the question, what is the color of the letter E\n\n41:17.360 --> 41:25.360\n in the Google logo? You have no idea. Although, you have seen it millions of times, hundreds of\n\n41:25.360 --> 41:32.240\n times. So, it's not, our model is not photorealistic, but it has other properties that we can\n\n41:32.240 --> 41:37.840\n manipulate it. And you can think about filling in a different color in that logo. You can think\n\n41:37.840 --> 41:44.400\n about expanding the letter E. You know, you can see what, so you can imagine the consequence of,\n\n41:44.400 --> 41:49.040\n you know, actions that you have never performed. So, these are the kind of characteristics the\n\n41:49.040 --> 41:52.800\n generative model need to have. So, this is one constraint that went into our model. Like, you\n\n41:52.800 --> 41:57.920\n know, so this is, when you read the, just the perception side of the paper, it is not obvious\n\n41:57.920 --> 42:02.720\n that this was a constraint into the, that went into the model, this top down controllability\n\n42:02.720 --> 42:10.480\n of the generative model. So, what does top down controllability in a model look like? It's a\n\n42:10.480 --> 42:16.000\n really interesting concept. Fascinating concept. What does that, is that the recursiveness gives\n\n42:16.000 --> 42:22.080\n you that? Or how do you do it? Quite a few things. It's like, what does the model factor,\n\n42:22.080 --> 42:26.720\n factorize? You know, what are the, what is the model representing as different pieces in the\n\n42:26.720 --> 42:33.440\n puzzle? Like, you know, so, so in the RCN network, it thinks of the world, you know, so what I said,\n\n42:33.440 --> 42:39.040\n the background of an image is modeled separately from the foreground of the image. So,\n\n42:39.040 --> 42:43.200\n the objects are separate from the background. They are different entities. So, there's a kind\n\n42:43.200 --> 42:49.840\n of segmentation that's built in fundamentally. And then even that object is composed of parts.\n\n42:49.840 --> 42:57.440\n And also, another one is the shape of the object is differently modeled from the texture of the\n\n42:57.440 --> 43:08.800\n object. Got it. So, there's like these, you know who Francois Chollet is? Yeah. So, there's, he\n\n43:08.800 --> 43:15.440\n developed this like IQ test type of thing for ARC challenge for, and it's kind of cool that there's\n\n43:16.160 --> 43:22.560\n these concepts, priors that he defines that you bring to the table in order to be able to reason\n\n43:22.560 --> 43:30.080\n about basic shapes and things in IQ test. So, here you're making it quite explicit that here are the\n\n43:30.080 --> 43:36.960\n things that you should be, these are like distinct things that you should be able to model in this.\n\n43:36.960 --> 43:42.240\n Keep in mind that you can derive this from much more general principles. It doesn't, you don't\n\n43:42.240 --> 43:48.880\n need to explicitly put it as, oh, objects versus foreground versus background, the surface versus\n\n43:48.880 --> 43:55.440\n the structure. No, these are, these are derivable from more fundamental principles of how, you know,\n\n43:55.440 --> 44:01.520\n what's the property of continuity of natural signals. What's the property of continuity of\n\n44:01.520 --> 44:07.120\n natural signals? Yeah. By the way, that sounds very poetic, but yeah. So, you're saying that's a,\n\n44:07.920 --> 44:12.560\n there's some low level properties from which emerges the idea that shapes should be different\n\n44:12.560 --> 44:18.640\n than like there should be a parts of an object. There should be, I mean, kind of like Francois,\n\n44:18.640 --> 44:23.840\n I mean, there's objectness, there's all these things that it's kind of crazy that we humans,\n\n44:25.040 --> 44:30.240\n I guess, evolved to have because it's useful for us to perceive the world. Yeah. Correct. And it\n\n44:30.240 --> 44:38.080\n derives mostly from the properties of natural signals. And so, natural signals. So, natural\n\n44:38.080 --> 44:43.200\n signals are the kind of things we'll perceive in the natural world. Correct. I don't know. I don't\n\n44:43.200 --> 44:48.080\n know why that sounds so beautiful. Natural signals. Yeah. As opposed to a QR code, right? Which is an\n\n44:48.080 --> 44:52.880\n artificial signal that we created. Humans are not very good at classifying QR codes. We are very\n\n44:52.880 --> 44:58.480\n good at saying something is a cat or a dog, but not very good at, you know, where computers are\n\n44:58.480 --> 45:05.600\n very good at classifying QR codes. So, our visual system is tuned for natural signals. So,\n\n45:05.600 --> 45:11.680\n it's tuned for natural signals. And there are fundamental assumptions in the architecture\n\n45:11.680 --> 45:18.640\n that are derived from natural signals properties. I wonder when you take hallucinogenic drugs,\n\n45:18.640 --> 45:25.120\n does that go into natural or is that closer to the QR code? It's still natural. It's still natural?\n\n45:25.120 --> 45:30.480\n Yeah. Because it is still operating using your brains. By the way, on that topic, I mean,\n\n45:30.480 --> 45:34.640\n I haven't been following. I think they're becoming legalized and certain. I can't wait\n\n45:34.640 --> 45:40.080\n they become legalized to a degree that you, like, vision science researchers could study it.\n\n45:40.080 --> 45:47.600\n Yeah. Just like through medical, chemical ways, modify. There could be ethical concerns, but\n\n45:47.600 --> 45:53.280\n modify. That's another way to study the brain is to be able to chemically modify it. There's\n\n45:53.280 --> 46:01.200\n probably very long a way to figure out how to do it ethically. Yeah, but I think there are studies\n\n46:01.200 --> 46:07.360\n on that already. Yeah, I think so. Because it's not unethical to give it to rats.\n\n46:08.080 --> 46:15.600\n Oh, that's true. That's true. There's a lot of drugged up rats out there. Okay, cool. Sorry.\n\n46:15.600 --> 46:23.840\n Sorry. It's okay. So, there's these low level things from natural signals that...\n\n46:23.840 --> 46:33.840\n...from which these properties will emerge. But it is still a very hard problem on how to encode\n\n46:33.840 --> 46:44.880\n that. So, you mentioned the priors Francho wanted to encode in the abstract reasoning challenge,\n\n46:44.880 --> 46:50.960\n but it is not straightforward how to encode those priors. So, some of those challenges,\n\n46:50.960 --> 46:57.040\n like the object completion challenges are things that we purely use our visual system to do.\n\n46:57.840 --> 47:03.200\n It looks like abstract reasoning, but it is purely an output of the vision system. For example,\n\n47:03.200 --> 47:07.120\n completing the corners of that condenser triangle, completing the lines of that condenser triangle.\n\n47:07.120 --> 47:12.160\n It's purely a visual system property. There is no abstract reasoning involved. It uses all these\n\n47:12.160 --> 47:18.720\n priors, but it is stored in our visual system in a particular way that is amenable to inference.\n\n47:18.720 --> 47:25.440\n That is one of the things that we tackled in the... Basically saying, okay, these are the\n\n47:25.440 --> 47:31.440\n prior knowledge which will be derived from the world, but then how is that prior knowledge\n\n47:31.440 --> 47:38.080\n represented in the model such that inference when some piece of evidence comes in can be\n\n47:38.080 --> 47:44.640\n done very efficiently and in a very distributed way? Because there are so many ways of representing\n\n47:44.640 --> 47:53.840\n knowledge, which is not amenable to very quick inference, quick lookups. So that's one core part\n\n47:53.840 --> 48:01.920\n of what we tackled in the RCN model. How do you encode visual knowledge to do very quick inference?\n\n48:02.800 --> 48:07.920\n Can you maybe comment on... So folks listening to this in general may be familiar with\n\n48:08.560 --> 48:10.720\n different kinds of architectures of a neural networks.\n\n48:10.720 --> 48:16.240\n What are we talking about with RCN? What does the architecture look like? What are the different\n\n48:16.240 --> 48:20.720\n components? Is it close to neural networks? Is it far away from neural networks? What does it look\n\n48:20.720 --> 48:27.040\n like? Yeah. So you can think of the Delta between the model and a convolutional neural network,\n\n48:27.040 --> 48:31.440\n if people are familiar with convolutional neural networks. So convolutional neural networks have\n\n48:31.440 --> 48:37.440\n this feed forward processing cascade, which is called feature detectors and pooling. And that\n\n48:37.440 --> 48:46.320\n is repeated in a multi level system. And if you want an intuitive idea of what is happening,\n\n48:46.320 --> 48:53.920\n feature detectors are detecting interesting co occurrences in the input. It can be a line,\n\n48:53.920 --> 49:03.200\n a corner, an eye or a piece of texture, et cetera. And the pooling neurons are doing some local\n\n49:03.200 --> 49:07.840\n transformation of that and making it invariant to local transformations. So this is what the\n\n49:07.840 --> 49:14.880\n structure of convolutional neural network is. Recursive cortical network has a similar structure\n\n49:14.880 --> 49:19.600\n when you look at just the feed forward pathway. But in addition to that, it is also structured\n\n49:19.600 --> 49:25.680\n in a way that it is generative so that it can run it backward and combine the forward with the\n\n49:25.680 --> 49:37.280\n backward. Another aspect that it has is it has lateral connections. So if you have an edge here\n\n49:37.280 --> 49:42.080\n and an edge here, it has connections between these edges. It is not just feed forward connections.\n\n49:42.080 --> 49:49.280\n It is something between these edges, which is the nodes representing these edges, which is to\n\n49:49.280 --> 49:53.920\n enforce compatibility between them. So otherwise what will happen is that constraints. It's a\n\n49:53.920 --> 50:01.200\n constraint. It's basically if you do just feature detection followed by pooling, then your\n\n50:01.200 --> 50:07.760\n transformations in different parts of the visual field are not coordinated. And so you will create\n\n50:07.760 --> 50:14.480\n a jagged, when you generate from the model, you will create jagged things and uncoordinated\n\n50:14.480 --> 50:20.160\n transformations. So these lateral connections are enforcing the transformations.\n\n50:20.160 --> 50:22.160\n Is the whole thing still differentiable?\n\n50:22.160 --> 50:27.440\n No, it's not. It's not trained using backprop.\n\n50:27.440 --> 50:32.720\n Okay. That's really important. So there's this feed forward, there's feedback mechanisms.\n\n50:33.280 --> 50:41.040\n There's some interesting connectivity things. It's still layered like multiple layers.\n\n50:41.040 --> 50:48.240\n Okay. Very, very interesting. And yeah. Okay. So the interconnection between adjacent connections\n\n50:48.240 --> 50:52.880\n across service constraints that keep the thing stable.\n\n50:52.880 --> 50:53.680\n Correct.\n\n50:53.680 --> 50:55.840\n Okay. So what else?\n\n50:55.840 --> 51:02.320\n And then there's this idea of doing inference. A neural network does not do inference on the fly.\n\n51:03.120 --> 51:09.200\n So an example of why this inference is important is, you know, so one of the first applications\n\n51:09.200 --> 51:15.040\n that we showed in the paper was to crack text based captures.\n\n51:15.040 --> 51:16.000\n What are captures?\n\n51:16.000 --> 51:21.040\n I mean, by the way, one of the most awesome, like the people don't use this term anymore\n\n51:21.040 --> 51:26.640\n as human computation, I think. I love this term. The guy who created captures,\n\n51:26.640 --> 51:32.640\n I think came up with this term. I love it. Anyway. What are captures?\n\n51:32.640 --> 51:38.480\n So captures are those things that you fill in when you're, you know, if you're\n\n51:38.480 --> 51:43.200\n opening a new account in Google, they show you a picture, you know, usually\n\n51:43.200 --> 51:48.720\n it used to be set of garbled letters that you have to kind of figure out what is that string\n\n51:48.720 --> 51:56.640\n of characters and type it. And the reason captures exist is because, you know, Google or Twitter\n\n51:56.640 --> 52:03.200\n do not want automatic creation of accounts. You can use a computer to create millions of accounts\n\n52:03.200 --> 52:10.560\n and use that for nefarious purposes. So you want to make sure that to the extent possible,\n\n52:10.560 --> 52:16.080\n the interaction that their system is having is with a human. So it's a, it's called a human\n\n52:16.080 --> 52:23.120\n interaction proof. A capture is a human interaction proof. So, so this is a captures are by design,\n\n52:23.840 --> 52:27.360\n things that are easy for humans to solve, but hard for computers.\n\n52:27.360 --> 52:28.240\n Hard for robots.\n\n52:28.240 --> 52:36.320\n Yeah. So, and text based captures was the one which is prevalent around 2014,\n\n52:36.320 --> 52:42.240\n because at that time, text based captures were hard for computers to crack. Even now,\n\n52:42.240 --> 52:48.240\n they are actually in the sense of an arbitrary text based capture will be unsolvable even now,\n\n52:48.240 --> 52:52.320\n but with the techniques that we have developed, it can be, you know, you can quickly develop\n\n52:52.320 --> 52:55.360\n a mechanism that solves the capture.\n\n52:55.360 --> 53:00.320\n They've probably gotten a lot harder too. They've been getting clever and clever\n\n53:00.320 --> 53:06.640\n generating these text captures. So, okay. So that was one of the things you've tested it on is these\n\n53:06.640 --> 53:15.120\n kinds of captures in 2014, 15, that kind of stuff. So what, I mean, why, by the way, why captures?\n\n53:15.120 --> 53:21.920\n Yeah. Even now, I would say capture is a very, very good challenge problem. If you want to\n\n53:21.920 --> 53:27.040\n understand how human perception works, and if you want to build systems that work,\n\n53:27.040 --> 53:32.880\n like the human brain, and I wouldn't say capture is a solved problem. We have cracked the fundamental\n\n53:32.880 --> 53:40.000\n defense of captures, but it is not solved in the way that humans solve it. So I can give an example.\n\n53:40.000 --> 53:48.640\n I can take a five year old child who has just learned characters and show them any new capture\n\n53:48.640 --> 53:56.400\n that we create. They will be able to solve it. I can show you, I can show you a picture of a\n\n53:56.400 --> 54:02.000\n character. I can show you pretty much any new capture from any new website. You'll be able to\n\n54:02.000 --> 54:06.640\n solve it without getting any training examples from that particular style of capture.\n\n54:06.640 --> 54:08.000\n You're assuming I'm human. Yeah.\n\n54:08.000 --> 54:14.560\n Yes. Yeah. That's right. So if you are human, otherwise I will be able to figure that out\n\n54:15.440 --> 54:22.000\n using this one. But this whole podcast is just a touring test, a long touring test. Anyway,\n\n54:22.000 --> 54:28.880\n yeah. So humans can figure it out with very few examples. Or no training examples. No training\n\n54:28.880 --> 54:37.760\n examples from that particular style of capture. So even now this is unreachable for the current\n\n54:37.760 --> 54:41.760\n deep learning system. So basically there is no, I don't think a system exists where you can\n\n54:41.760 --> 54:47.840\n basically say, train on whatever you want. And then now say, hey, I will show you a new capture,\n\n54:47.840 --> 54:54.160\n which I did not show you in the training setup. Will the system be able to solve it? It still\n\n54:54.160 --> 55:01.760\n doesn't exist. So that is the magic of human perception. And Doug Hofstadter put this very\n\n55:01.760 --> 55:11.440\n beautifully in one of his talks. The central problem in AI is what is the letter A. If you\n\n55:11.440 --> 55:17.600\n can build a system that reliably can detect all the variations of the letter A, you don't even\n\n55:17.600 --> 55:23.040\n know to go to the B and the C. Yeah. You don't even know to go to the B and the C or the strings\n\n55:23.040 --> 55:28.880\n of characters. And so that is the spirit with which we tackle that problem.\n\n55:28.880 --> 55:36.160\n What does it mean by that? I mean, is it like without training examples, try to figure out\n\n55:36.160 --> 55:43.520\n the fundamental elements that make up the letter A in all of its forms?\n\n55:43.520 --> 55:47.920\n In all of its forms. A can be made with two humans standing, leaning against each other,\n\n55:47.920 --> 55:51.360\n holding the hands. And it can be made of leaves.\n\n55:52.080 --> 55:56.480\n Yeah. You might have to understand everything about this world in order to understand the\n\n55:56.480 --> 55:57.920\n letter A. Yeah. Exactly.\n\n55:57.920 --> 56:00.400\n So it's common sense reasoning, essentially. Yeah.\n\n56:00.400 --> 56:06.720\n Right. So to finally, to really solve, finally to say that you have solved capture,\n\n56:07.760 --> 56:08.880\n you have to solve the whole problem.\n\n56:08.880 --> 56:18.560\n Yeah. Okay. So how does this kind of the RCN architecture help us to do a better job of that\n\n56:18.560 --> 56:24.400\n kind of thing? Yeah. So as I mentioned, one of the important things was being able to do inference,\n\n56:24.960 --> 56:26.480\n being able to dynamically do inference.\n\n56:28.640 --> 56:33.040\n Can you clarify what you mean? Because you said like neural networks don't do inference.\n\n56:33.040 --> 56:35.840\n Yeah. So what do you mean by inference in this context then?\n\n56:35.840 --> 56:42.560\n So, okay. So in captures, what they do to confuse people is to make these characters crowd together.\n\n56:43.360 --> 56:48.400\n Yes. Okay. And when you make the characters crowd together, what happens is that you will now start\n\n56:48.400 --> 56:53.920\n seeing combinations of characters as some other new character or an existing character. So you\n\n56:53.920 --> 57:02.320\n would put an R and N together. It will start looking like an M. And so locally, there is\n\n57:02.320 --> 57:11.520\n very strong evidence for it being some incorrect character. But globally, the only explanation that\n\n57:11.520 --> 57:17.600\n fits together is something that is different from what you can find locally. Yes. So this is\n\n57:18.240 --> 57:25.840\n inference. You are basically taking local evidence and putting it in the global context and often\n\n57:25.840 --> 57:29.920\n coming to a conclusion locally, which is conflicting with the local information.\n\n57:29.920 --> 57:36.560\n So actually, so you mean inference like in the way it's used when you talk about reasoning,\n\n57:36.560 --> 57:42.240\n for example, as opposed to like inference, which is with artificial neural networks,\n\n57:42.240 --> 57:47.840\n which is a single pass to the network. Okay. So like you're basically doing some basic forms of\n\n57:47.840 --> 57:54.480\n reasoning, like integration of like how local things fit into the global picture.\n\n57:54.480 --> 57:59.840\n And things like explaining a way coming into this one, because you are explaining that piece\n\n57:59.840 --> 58:06.960\n of evidence as something else, because globally, that's the only thing that makes sense. So now\n\n58:08.160 --> 58:15.600\n you can amortize this inference in a neural network. If you want to do this, you can brute\n\n58:15.600 --> 58:23.120\n force it. You can just show it all combinations of things that you want your reasoning to work over.\n\n58:23.120 --> 58:30.880\n And you can just train the help out of that neural network and it will look like it is doing inference\n\n58:30.880 --> 58:37.680\n on the fly, but it is really just doing amortized inference. It is because you have shown it a lot\n\n58:37.680 --> 58:43.840\n of these combinations during training time. So what you want to do is be able to do dynamic\n\n58:43.840 --> 58:48.480\n inference rather than just being able to show all those combinations in the training time.\n\n58:48.480 --> 58:54.080\n And that's something we emphasized in the model. What does it mean, dynamic inference? Is that\n\n58:54.080 --> 59:00.320\n that has to do with the feedback thing? Yes. Like what is dynamic? I'm trying to visualize what\n\n59:00.320 --> 59:05.920\n dynamic inference would be in this case. Like what is it doing with the input? It's shown the input\n\n59:05.920 --> 59:13.840\n the first time. Yeah. And is like what's changing over temporally? What's the dynamics of this\n\n59:13.840 --> 59:19.840\n inference process? So you can think of it as you have at the top of the model, the characters that\n\n59:19.840 --> 59:26.720\n you are trained on. They are the causes that you are trying to explain the pixels using the\n\n59:26.720 --> 59:33.600\n characters as the causes. The characters are the things that cause the pixels. Yeah. So there's\n\n59:33.600 --> 59:38.960\n this causality thing. So the reason you mentioned causality, I guess, is because there's a temporal\n\n59:38.960 --> 59:43.280\n aspect to this whole thing. In this particular case, the temporal aspect is not important.\n\n59:43.280 --> 59:50.000\n It is more like when if I turn the character on, the pixels will turn on. Yeah, it will be after\n\n59:50.000 --> 59:55.520\n this a little bit. Okay. So that is causality in the sense of like a logic causality, like\n\n59:55.520 --> 1:00:03.200\n hence inference. Okay. The dynamics is that even though locally it will look like, okay, this is an\n\n1:00:03.200 --> 1:00:11.280\n A. And locally, just when I look at just that patch of the image, it looks like an A. But when I look\n\n1:00:11.280 --> 1:00:17.600\n at it in the context of all the other causes, A is not something that makes sense. So that is\n\n1:00:17.600 --> 1:00:24.720\n something you have to kind of recursively figure out. Yeah. So, okay. And this thing performed\n\n1:00:24.720 --> 1:00:32.080\n pretty well on the CAPTCHAs. Correct. And I mean, is there some kind of interesting intuition you\n\n1:00:32.080 --> 1:00:37.840\n can provide why it did well? Like what did it look like? Is there visualizations that could be human\n\n1:00:37.840 --> 1:00:43.360\n interpretable to us humans? Yes. Yeah. So the good thing about the model is that it is extremely,\n\n1:00:44.320 --> 1:00:50.400\n so it is not just doing a classification, right? It is providing a full explanation for the scene.\n\n1:00:50.400 --> 1:00:59.600\n So when it operates on a scene, it is coming back and saying, look, this is the part is the A,\n\n1:00:59.600 --> 1:01:06.880\n and these are the pixels that turned on. These are the pixels in the input that makes me think that\n\n1:01:06.880 --> 1:01:14.640\n it is an A. And also, these are the portions I hallucinated. It provides a complete explanation\n\n1:01:14.640 --> 1:01:21.360\n of that form. And then these are the contours. This is the interior. And this is in front of\n\n1:01:21.360 --> 1:01:28.400\n this other object. So that's the kind of explanation the inference network provides.\n\n1:01:28.400 --> 1:01:38.800\n So that is useful and interpretable. And then the kind of errors it makes are also,\n\n1:01:40.000 --> 1:01:47.040\n I don't want to read too much into it, but the kind of errors the network makes are very similar\n\n1:01:47.040 --> 1:01:51.120\n to the kinds of errors humans would make in a similar situation. So there's something about\n\n1:01:51.120 --> 1:02:00.240\n the structure that feels reminiscent of the way humans visual system works. Well, I mean,\n\n1:02:00.240 --> 1:02:03.760\n how hardcoded is this to the capture problem, this idea?\n\n1:02:04.320 --> 1:02:09.840\n Not really hardcoded because the assumptions, as I mentioned, are general, right? It is more,\n\n1:02:11.280 --> 1:02:17.680\n and those themselves can be applied in many situations which are natural signals. So it's\n\n1:02:17.680 --> 1:02:24.320\n the foreground versus background factorization and the factorization of the surfaces versus\n\n1:02:24.320 --> 1:02:27.600\n the contours. So these are all generally applicable assumptions.\n\n1:02:27.600 --> 1:02:36.000\n In all vision. So why attack the capture problem, which is quite unique in the computer vision\n\n1:02:36.000 --> 1:02:42.800\n context versus like the traditional benchmarks of ImageNet and all those kinds of image\n\n1:02:42.800 --> 1:02:49.120\n classification or even segmentation tasks and all of that kind of stuff. What's your thinking about\n\n1:02:49.120 --> 1:02:55.760\n those kinds of benchmarks in this context? I mean, those benchmarks are useful for deep\n\n1:02:55.760 --> 1:03:03.600\n learning kind of algorithms. So the settings that deep learning works in are here is my huge\n\n1:03:03.600 --> 1:03:10.480\n training set and here is my test set. So the training set is almost 100x, 1000x bigger than\n\n1:03:10.480 --> 1:03:18.480\n the test set in many, many cases. What we wanted to do was invert that. The training set is way\n\n1:03:18.480 --> 1:03:30.080\n smaller than the test set. And capture is a problem that is by definition hard for computers\n\n1:03:30.080 --> 1:03:36.640\n and it has these good properties of strong generalization, strong out of training distribution\n\n1:03:36.640 --> 1:03:44.480\n generalization. If you are interested in studying that and having your model have that property,\n\n1:03:44.480 --> 1:03:49.840\n then it's a good data set to tackle. So have you attempted to, which I think,\n\n1:03:49.840 --> 1:03:58.080\n I believe there's quite a growing body of work on looking at MNIST and ImageNet without training.\n\n1:03:58.080 --> 1:04:05.760\n So it's like taking the basic challenge is what tiny fraction of the training set can we take in\n\n1:04:05.760 --> 1:04:13.680\n order to do a reasonable job of the classification task? Have you explored that angle in these\n\n1:04:13.680 --> 1:04:20.640\n classic benchmarks? Yes. So we did do MNIST. So it's not just capture. So there was also\n\n1:04:23.440 --> 1:04:28.720\n multiple versions of MNIST, including the standard version where we inverted the problem,\n\n1:04:28.720 --> 1:04:36.400\n which is basically saying rather than train on 60,000 training data, how quickly can you get\n\n1:04:37.200 --> 1:04:42.080\n to high level accuracy with very little training data? Is there some performance you remember,\n\n1:04:42.080 --> 1:04:50.400\n like how well did it do? How many examples did it need? Yeah. I remember that it was\n\n1:04:50.400 --> 1:05:00.880\n on the order of tens or hundreds of examples to get into 95% accuracy. And it was definitely\n\n1:05:00.880 --> 1:05:03.840\n better than the other systems out there at that time.\n\n1:05:03.840 --> 1:05:07.920\n At that time. Yeah. They're really pushing. I think that's a really interesting space,\n\n1:05:07.920 --> 1:05:17.360\n actually. I think there's an actual name for MNIST. There's different names to the different\n\n1:05:17.360 --> 1:05:21.600\n sizes of training sets. I mean, people are like attacking this problem. I think it's\n\n1:05:21.600 --> 1:05:28.240\n super interesting. It's funny how like the MNIST will probably be with us all the way to AGI.\n\n1:05:29.760 --> 1:05:37.680\n It's a data set that just sticks by. It's a clean, simple data set to study the fundamentals of\n\n1:05:37.680 --> 1:05:43.280\n learning with just like captures. It's interesting. Not enough people. I don't know. Maybe you can\n\n1:05:43.280 --> 1:05:48.240\n correct me, but I feel like captures don't show up as often in papers as they probably should.\n\n1:05:48.240 --> 1:05:56.640\n That's correct. Yeah. Because usually these things have a momentum. Once something gets\n\n1:05:56.640 --> 1:06:04.880\n established as a standard benchmark, there is a dynamics of how graduate students operate and how\n\n1:06:06.000 --> 1:06:10.640\n academic system works that pushes people to track that benchmark.\n\n1:06:10.640 --> 1:06:19.600\n Yeah. Nobody wants to think outside the box. Okay. Okay. So good performance on the captures.\n\n1:06:20.480 --> 1:06:25.520\n What else is there interesting on the RCN side before we talk about the cortical micros?\n\n1:06:25.520 --> 1:06:31.760\n Yeah. So the same model. So the important part of the model was that it trains very\n\n1:06:31.760 --> 1:06:37.440\n quickly with very little training data and it's quite robust to out of distribution\n\n1:06:37.440 --> 1:06:45.760\n perturbations. And we are using that very fruitfully at Vicarious in many of the\n\n1:06:45.760 --> 1:06:51.840\n robotics tasks we are solving. Well, let me ask you this kind of touchy question. I have to,\n\n1:06:51.840 --> 1:06:59.520\n I've spoken with your friend, colleague, Jeff Hawkins, too. I have to kind of ask,\n\n1:06:59.520 --> 1:07:05.680\n there is a bit of, whenever you have brain inspired stuff and you make big claims,\n\n1:07:05.680 --> 1:07:14.720\n big sexy claims, there's critics, I mean, machine learning subreddit, don't get me started on those\n\n1:07:14.720 --> 1:07:23.680\n people. Criticism is good, but they're a bit over the top. There is quite a bit of sort of\n\n1:07:23.680 --> 1:07:31.040\n skepticism and criticism. Is this work really as good as it promises to be? Do you have thoughts\n\n1:07:31.040 --> 1:07:36.800\n on that kind of skepticism? Do you have comments on the kind of criticism I might have received\n\n1:07:36.800 --> 1:07:44.880\n about, you know, is this approach legit? Is this a promising approach? Or at least as promising as\n\n1:07:44.880 --> 1:07:52.480\n it seems to be, you know, advertised as? Yeah, I can comment on it. So, you know, our RCN paper\n\n1:07:52.480 --> 1:07:58.560\n is published in Science, which I would argue is a very high quality journal, very hard to publish\n\n1:07:58.560 --> 1:08:08.160\n in. And, you know, usually it is indicative of the quality of the work. And I am very,\n\n1:08:08.160 --> 1:08:13.760\n very certain that the ideas that we brought together in that paper, in terms of the importance\n\n1:08:13.760 --> 1:08:20.160\n of feedback connections, recursive inference, lateral connections, coming to best explanation\n\n1:08:20.160 --> 1:08:27.360\n of the scene as the problem to solve, trying to solve recognition, segmentation, all jointly,\n\n1:08:27.360 --> 1:08:31.920\n in a way that is compatible with higher level cognition, top down attention, all those ideas\n\n1:08:31.920 --> 1:08:36.000\n that we brought together into something, you know, coherent and workable in the world and\n\n1:08:36.000 --> 1:08:40.880\n solving a challenging, tackling a challenging problem. I think that will stay and that\n\n1:08:40.880 --> 1:08:49.360\n contribution I stand by. Now, I can tell you a story which is funny in the context of this. So,\n\n1:08:49.360 --> 1:08:53.360\n if you read the abstract of the paper and, you know, the argument we are putting in, you know,\n\n1:08:53.360 --> 1:08:59.120\n we are putting in, look, current deep learning systems take a lot of training data. They don't\n\n1:08:59.120 --> 1:09:03.760\n use these insights. And here is our new model, which is not a deep neural network. It's a\n\n1:09:03.760 --> 1:09:08.560\n graphical model. It does inference. This is how the paper is, right? Now, once the paper was\n\n1:09:08.560 --> 1:09:14.800\n accepted and everything, it went to the press department in Science, you know, AAAS Science\n\n1:09:14.800 --> 1:09:18.880\n Office. We didn't do any press release when it was published. It went to the press department.\n\n1:09:18.880 --> 1:09:23.200\n What was the press release that they wrote up? A new deep learning model.\n\n1:09:24.880 --> 1:09:25.920\n Solves CAPTCHAs.\n\n1:09:25.920 --> 1:09:32.400\n Solves CAPTCHAs. And so, you can see where was, you know, what was being hyped in that thing,\n\n1:09:32.400 --> 1:09:42.160\n right? So, there is a dynamic in the community of, you know, so that especially happens when\n\n1:09:42.160 --> 1:09:46.720\n there are lots of new people coming into the field and they get attracted to one thing.\n\n1:09:46.720 --> 1:09:52.560\n And some people are trying to think different compared to that. So, there is some, I think\n\n1:09:52.560 --> 1:09:59.360\n skepticism is science is important and it is, you know, very much required. But it's also,\n\n1:09:59.360 --> 1:10:04.480\n it's not skepticism. Usually, it's mostly bandwagon effect that is happening rather than.\n\n1:10:05.200 --> 1:10:09.760\n Well, but that's not even that. I mean, I'll tell you what they react to, which is like,\n\n1:10:09.760 --> 1:10:16.960\n I'm sensitive to as well. If you look at just companies, OpenAI, DeepMind, Vicarious, I mean,\n\n1:10:16.960 --> 1:10:27.520\n they just, there's a little bit of a race to the top and hype, right? It's like, it doesn't pay off\n\n1:10:27.520 --> 1:10:37.600\n to be humble. So, like, and the press is just irresponsible often. They just, I mean, don't\n\n1:10:37.600 --> 1:10:42.880\n get me started on the state of journalism today. Like, it seems like the people who write articles\n\n1:10:42.880 --> 1:10:49.280\n about these things, they literally have not even spent an hour on the Wikipedia article about what\n\n1:10:49.280 --> 1:10:55.440\n is neural networks. Like, they haven't like invested just even the language to laziness.\n\n1:10:56.160 --> 1:11:06.800\n It's like, robots beat humans. Like, they write this kind of stuff that just, and then of course,\n\n1:11:06.800 --> 1:11:11.760\n the researchers are quite sensitive to that because it gets a lot of attention. They're like,\n\n1:11:11.760 --> 1:11:18.240\n why did this word get so much attention? That's over the top and people get really sensitive.\n\n1:11:18.240 --> 1:11:24.080\n The same kind of criticism with OpenAI did work with Rubik's cube with the robot that people\n\n1:11:24.080 --> 1:11:33.120\n criticized. Same with GPT2 and 3, they criticize. Same thing with DeepMinds with AlphaZero. I mean,\n\n1:11:33.120 --> 1:11:39.280\n yeah, I'm sensitive to it. But, and of course, with your work, you mentioned deep learning, but\n\n1:11:39.280 --> 1:11:45.520\n there's something super sexy to the public about brain inspired. I mean, that immediately grabs\n\n1:11:45.520 --> 1:11:52.240\n people's imagination, not even like neural networks, but like really brain inspired, like\n\n1:11:53.600 --> 1:12:00.480\n brain like neural networks. That seems really compelling to people and to me as well, to the\n\n1:12:00.480 --> 1:12:10.400\n world as a narrative. And so people hook up, hook onto that. And sometimes the skepticism engine\n\n1:12:10.400 --> 1:12:17.600\n turns on in the research community and they're skeptical. But I think putting aside the ideas\n\n1:12:17.600 --> 1:12:22.480\n of the actual performance and captures or performance in any data set. I mean, to me,\n\n1:12:22.480 --> 1:12:28.720\n all these data sets are useless anyway. It's nice to have them. But in the grand scheme of things,\n\n1:12:28.720 --> 1:12:36.080\n they're silly toy examples. The point is, is there intuition about the ideas, just like you\n\n1:12:36.080 --> 1:12:42.400\n mentioned, bringing the ideas together in a unique way? Is there something there? Is there some value\n\n1:12:42.400 --> 1:12:46.400\n there? And is it going to stand the test of time? And that's the hope. That's the hope.\n\n1:12:46.400 --> 1:12:53.440\n Yes. My confidence there is very high. I don't treat brain inspired as a marketing term.\n\n1:12:53.440 --> 1:13:01.920\n I am looking into the details of biology and puzzling over those things and I am grappling\n\n1:13:01.920 --> 1:13:07.600\n with those things. And so it is not a marketing term at all. You can use it as a marketing term\n\n1:13:07.600 --> 1:13:13.680\n and people often use it and you can get combined with them. And when people don't understand\n\n1:13:13.680 --> 1:13:20.480\n how you're approaching the problem, it is easy to be misunderstood and think of it as purely\n\n1:13:20.480 --> 1:13:26.560\n marketing. But that's not the way we are. So you really, I mean, as a scientist,\n\n1:13:27.120 --> 1:13:33.120\n you believe that if we kind of just stick to really understanding the brain, that's going to,\n\n1:13:33.760 --> 1:13:39.440\n that's the right, like you should constantly meditate on the, how does the brain do this?\n\n1:13:39.440 --> 1:13:43.520\n Because that's going to be really helpful for engineering and technology systems.\n\n1:13:43.520 --> 1:13:51.680\n Yes. You need to, so I think it's one input and it is helpful, but you should know when to deviate\n\n1:13:51.680 --> 1:13:59.120\n from it too. So an example is convolutional neural networks, right? Convolution is not an\n\n1:13:59.120 --> 1:14:06.240\n operation brain implements. The visual cortex is not convolutional. Visual cortex has local\n\n1:14:06.240 --> 1:14:17.840\n receptive fields, local connectivity, but there is no translation invariance in the network weights\n\n1:14:18.640 --> 1:14:24.080\n in the visual cortex. That is a computational trick, which is a very good engineering trick\n\n1:14:24.080 --> 1:14:31.840\n that we use for sharing the training between the different nodes. And that trick will be with us\n\n1:14:31.840 --> 1:14:41.600\n for some time. It will go away when we have robots with eyes and heads that move. And so then that\n\n1:14:41.600 --> 1:14:49.040\n trick will go away. It will not be useful at that time. So the brain doesn't have translational\n\n1:14:49.040 --> 1:14:54.720\n invariance. It has the focal point, like it has a thing it focuses on. Correct. It has a phobia.\n\n1:14:54.720 --> 1:15:01.920\n And because of the phobia, the receptive fields are not like the copying of the weights. Like the\n\n1:15:01.920 --> 1:15:05.760\n weights in the center are very different from the weights in the periphery. Yes. At the periphery.\n\n1:15:05.760 --> 1:15:12.720\n I mean, I did this, actually wrote a paper and just gotten a chance to really study peripheral\n\n1:15:12.720 --> 1:15:21.600\n vision, which is a fascinating thing. Very under understood thing of what the brain, you know,\n\n1:15:21.600 --> 1:15:28.240\n at every level the brain does with the periphery. It does some funky stuff. Yeah. So it's another\n\n1:15:28.240 --> 1:15:39.040\n kind of trick than convolutional. Like it does, it's, you know, convolution in neural networks is\n\n1:15:39.040 --> 1:15:44.160\n a trick for efficiency, is efficiency trick. And the brain does a whole nother kind of thing.\n\n1:15:44.160 --> 1:15:51.280\n Correct. So you need to understand the principles or processing so that you can still apply\n\n1:15:51.280 --> 1:15:55.840\n engineering tricks where you want it to. You don't want to be slavishly mimicking all the things of\n\n1:15:55.840 --> 1:16:01.280\n the brain. And so, yeah, so it should be one input. And I think it is extremely helpful,\n\n1:16:02.000 --> 1:16:06.720\n but it should be the point of really understanding so that you know when to deviate from it.\n\n1:16:06.720 --> 1:16:14.560\n So, okay. That's really cool. That's work from a few years ago. You did work in Umenta with Jeff\n\n1:16:14.560 --> 1:16:23.040\n Hawkins with hierarchical temporal memory. How is your just, if you could give a brief history,\n\n1:16:23.040 --> 1:16:30.240\n how is your view of the way the models of the brain changed over the past few years leading up\n\n1:16:30.240 --> 1:16:36.960\n to now? Is there some interesting aspects where there was an adjustment to your understanding of\n\n1:16:36.960 --> 1:16:41.680\n the brain or is it all just building on top of each other? In terms of the higher level ideas,\n\n1:16:42.720 --> 1:16:47.920\n especially the ones Jeff wrote about in the book, if you blur out, right. Yeah. On intelligence.\n\n1:16:47.920 --> 1:16:52.560\n Right. On intelligence. If you blur out the details and if you just zoom out and at the\n\n1:16:52.560 --> 1:17:02.320\n higher level idea, things are, I would say, consistent with what he wrote about. But many\n\n1:17:02.320 --> 1:17:08.160\n things will be consistent with that because it's a blur. Deep learning systems are also\n\n1:17:08.160 --> 1:17:16.960\n multi level, hierarchical, all of those things. But in terms of the detail, a lot of things are\n\n1:17:16.960 --> 1:17:28.000\n different. And those details matter a lot. So one point of difference I had with Jeff was how to\n\n1:17:28.000 --> 1:17:34.640\n approach, how much of biological plausibility and realism do you want in the learning algorithms?\n\n1:17:36.080 --> 1:17:41.520\n So when I was there, this was almost 10 years ago now.\n\n1:17:41.520 --> 1:17:43.760\n It flies when you're having fun.\n\n1:17:43.760 --> 1:17:49.760\n Yeah. I don't know what Jeff thinks now, but 10 years ago, the difference was that\n\n1:17:49.760 --> 1:17:56.880\n I did not want to be so constrained on saying my learning algorithms need to be\n\n1:17:56.880 --> 1:18:03.200\n biologically plausible based on some filter of biological plausibility available at that time.\n\n1:18:03.200 --> 1:18:09.200\n To me, that is a dangerous cut to make because we are discovering more and more things about\n\n1:18:09.200 --> 1:18:14.560\n the brain all the time. New biophysical mechanisms, new channels are being discovered\n\n1:18:14.560 --> 1:18:21.360\n all the time. So I don't want to upfront kill off a learning algorithm just because we don't\n\n1:18:21.360 --> 1:18:27.680\n really understand the full biophysics or whatever of how the brain learns.\n\n1:18:27.680 --> 1:18:29.120\n Exactly. Exactly.\n\n1:18:29.120 --> 1:18:34.720\n Let me ask and I'm sorry to interrupt. What's your sense? What's our best understanding of\n\n1:18:34.720 --> 1:18:36.000\n how the brain learns?\n\n1:18:36.000 --> 1:18:42.720\n So things like backpropagation, credit assignment. So many of these algorithms have,\n\n1:18:42.720 --> 1:18:47.600\n learning algorithms have things in common, right? It is a backpropagation is one way of\n\n1:18:47.600 --> 1:18:52.560\n credit assignment. There is another algorithm called expectation maximization, which is,\n\n1:18:52.560 --> 1:18:55.520\n you know, another weight adjustment algorithm.\n\n1:18:55.520 --> 1:18:58.320\n But is it your sense the brain does something like this?\n\n1:18:58.320 --> 1:19:04.960\n Has to. There is no way around it in the sense of saying that you do have to adjust the\n\n1:19:04.960 --> 1:19:06.240\n connections.\n\n1:19:06.240 --> 1:19:09.600\n So yeah, and you're saying credit assignment, you have to reward the connections that were\n\n1:19:09.600 --> 1:19:14.320\n useful in making a correct prediction and not, yeah, I guess what else, but yeah, it\n\n1:19:14.320 --> 1:19:16.800\n doesn't have to be differentiable.\n\n1:19:16.800 --> 1:19:22.320\n Yeah, it doesn't have to be differentiable. Yeah. But you have to have a, you know, you\n\n1:19:22.320 --> 1:19:27.760\n have a model that you start with, you have data comes in and you have to have a way of\n\n1:19:27.760 --> 1:19:33.920\n adjusting the model such that it better fits the data. So that is all of learning, right?\n\n1:19:33.920 --> 1:19:40.400\n And some of them can be using backprop to do that. Some of it can be using, you know,\n\n1:19:40.400 --> 1:19:44.320\n very local graph changes to do that.\n\n1:19:45.360 --> 1:19:52.160\n That can be, you know, many of these learning algorithms have similar update properties\n\n1:19:52.160 --> 1:19:56.640\n locally in terms of what the neurons need to do locally.\n\n1:19:57.200 --> 1:20:01.120\n I wonder if small differences in learning algorithms can have huge differences in the\n\n1:20:01.120 --> 1:20:09.920\n actual effect. So the dynamics of, I mean, sort of the reverse like spiking, like if\n\n1:20:09.920 --> 1:20:17.040\n credit assignment is like a lightning versus like a rainstorm or something, like whether\n\n1:20:18.480 --> 1:20:26.240\n there's like a looping local type of situation with the credit assignment, whether there is\n\n1:20:26.240 --> 1:20:34.720\n like regularization, like how it injects robustness into the whole thing, like whether\n\n1:20:34.720 --> 1:20:42.080\n it's chemical or electrical or mechanical. Yeah. All those kinds of things. I feel like\n\n1:20:42.080 --> 1:20:48.800\n it, that, yeah, I feel like those differences could be essential, right? It could be. It's\n\n1:20:48.800 --> 1:20:54.880\n just that you don't know enough to, on the learning side, you don't know, you don't know\n\n1:20:54.880 --> 1:20:59.840\n enough to say that is definitely not the way the brain does it. Got it. So you don't want\n\n1:20:59.840 --> 1:21:04.800\n to be stuck to it. So that, yeah. So you've been open minded on that side of things.\n\n1:21:04.800 --> 1:21:09.920\n On the inference side, on the recognition side, I am much more, I'm able to be constrained\n\n1:21:09.920 --> 1:21:13.600\n because it's much easier to do experiments because, you know, it's like, okay, here's\n\n1:21:13.600 --> 1:21:18.000\n the stimulus, you know, how many steps did it get to take the answer? I can trace it\n\n1:21:18.000 --> 1:21:23.120\n back. I can, I can understand the speed of that computation, et cetera. I'm able to do\n\n1:21:23.120 --> 1:21:28.400\n of that computation, et cetera, much more readily on the inference side. Got it. And\n\n1:21:28.400 --> 1:21:34.880\n then you can't do good experiments on the learning side. Correct. So let's go right\n\n1:21:34.880 --> 1:21:42.080\n into the cortical microcircuits right back. So what are these ideas beyond recursive cortical\n\n1:21:42.080 --> 1:21:48.960\n network that you're looking at now? So we have made a, you know, pass through multiple\n\n1:21:48.960 --> 1:21:54.480\n of the steps that, you know, as I mentioned earlier, you know, we were looking at perception\n\n1:21:54.480 --> 1:21:58.720\n from the angle of cognition, right? It was not just perception for perception's sake.\n\n1:21:58.720 --> 1:22:04.400\n How do you, how do you connect it to cognition? How do you learn concepts and how do you learn\n\n1:22:04.400 --> 1:22:13.280\n abstract reasoning? Similar to some of the things Francois talked about, right? So we\n\n1:22:13.280 --> 1:22:19.600\n have taken one pass through it basically saying, what is the basic cognitive architecture that\n\n1:22:19.600 --> 1:22:25.120\n you need to have, which has a perceptual system, which has a system that learns dynamics of\n\n1:22:25.120 --> 1:22:32.240\n the world and then has something like a routine program learning system on top of it to learn\n\n1:22:32.240 --> 1:22:38.320\n concepts. So we have built one, you know, the version point one of that system. This\n\n1:22:38.320 --> 1:22:44.640\n was another science robotics paper. It's the title of that paper was, you know, something\n\n1:22:44.640 --> 1:22:49.760\n like cognitive programs. How do you build cognitive programs? And the application there\n\n1:22:49.760 --> 1:22:56.400\n was on manipulation, robotic manipulation? It was, so think of it like this. Suppose\n\n1:22:56.960 --> 1:23:04.800\n you wanted to tell a new person that you met, you don't know the language that person uses.\n\n1:23:04.800 --> 1:23:10.080\n You want to communicate to that person to achieve some task, right? So I want to say,\n\n1:23:10.080 --> 1:23:17.280\n hey, you need to pick up all the red cups from the kitchen counter and put it here, right?\n\n1:23:17.280 --> 1:23:21.920\n How do you communicate that, right? You can show pictures. You can basically say, look,\n\n1:23:21.920 --> 1:23:28.080\n this is the starting state. The things are here. This is the ending state. And what does\n\n1:23:28.080 --> 1:23:32.400\n the person need to understand from that? The person needs to understand what conceptually\n\n1:23:32.400 --> 1:23:39.120\n happened in those pictures from the input to the output, right? So we are looking at\n\n1:23:39.120 --> 1:23:45.360\n preverbal conceptual understanding. Without language, how do you have a set of concepts\n\n1:23:45.360 --> 1:23:52.240\n that you can manipulate in your head? And from a set of images of input and output,\n\n1:23:52.240 --> 1:23:55.600\n can you infer what is happening in those images?\n\n1:23:55.600 --> 1:24:02.400\n Got it. With concepts that are pre language. Okay. So what's it mean for a concept to be pre language?\n\n1:24:02.400 --> 1:24:09.440\n Like why is language so important here?\n\n1:24:10.080 --> 1:24:16.320\n So I want to make a distinction between concepts that are just learned from text\n\n1:24:17.520 --> 1:24:23.440\n by just feeding brute force text. You can start extracting things like, okay,\n\n1:24:23.440 --> 1:24:30.640\n a cow is likely to be on grass. So those kinds of things, you can extract purely from text.\n\n1:24:32.160 --> 1:24:37.520\n But that's kind of a simple association thing rather than a concept as an abstraction of\n\n1:24:37.520 --> 1:24:44.480\n something that happens in the real world in a grounded way that I can simulate it in my\n\n1:24:44.480 --> 1:24:51.200\n mind and connect it back to the real world. And you think kind of the visual world,\n\n1:24:51.200 --> 1:24:57.920\n concepts in the visual world are somehow lower level than just the language?\n\n1:24:58.800 --> 1:25:03.280\n The lower level kind of makes it feel like, okay, that's unimportant. It's more like,\n\n1:25:04.720 --> 1:25:15.440\n I would say the concepts in the visual and the motor system and the concept learning system,\n\n1:25:15.440 --> 1:25:20.320\n which if you cut off the language part, just what we learn by interacting with the world\n\n1:25:20.320 --> 1:25:25.600\n and abstractions from that, that is a prerequisite for any real language understanding.\n\n1:25:26.480 --> 1:25:31.440\n So you disagree with Chomsky because he says language is at the bottom of everything.\n\n1:25:32.080 --> 1:25:38.320\n No, I disagree with Chomsky completely on how many levels from universal grammar to...\n\n1:25:39.680 --> 1:25:43.120\n So that was a paper in science beyond the recursive cortical network.\n\n1:25:43.120 --> 1:25:50.480\n What other interesting problems are there, the open problems and brain inspired approaches\n\n1:25:50.480 --> 1:25:51.600\n that you're thinking about?\n\n1:25:51.600 --> 1:26:00.640\n I mean, everything is open, right? No problem is solved, solved. I think of perception as kind of\n\n1:26:02.080 --> 1:26:07.760\n the first thing that you have to build, but the last thing that you will be actually solved.\n\n1:26:07.760 --> 1:26:12.880\n Because if you do not build perception system in the right way, you cannot build concept system in\n\n1:26:12.880 --> 1:26:18.560\n the right way. So you have to build a perception system, however wrong that might be, you have to\n\n1:26:18.560 --> 1:26:24.880\n still build that and learn concepts from there and then keep iterating. And finally, perception\n\n1:26:24.880 --> 1:26:30.240\n will get solved fully when perception, cognition, language, all those things work together finally.\n\n1:26:30.240 --> 1:26:37.920\n So great, we've talked a lot about perception, but then maybe on the concept side and like common\n\n1:26:37.920 --> 1:26:45.280\n sense or just general reasoning side, is there some intuition you can draw from the brain about\n\n1:26:45.280 --> 1:26:46.880\n how we can do that?\n\n1:26:46.880 --> 1:26:56.560\n So I have this classic example I give. So suppose I give you a few sentences and then ask you a\n\n1:26:56.560 --> 1:27:01.920\n question following that sentence. This is a natural language processing problem, right? So here\n\n1:27:01.920 --> 1:27:10.400\n it goes. I'm telling you, Sally pounded a nail on the ceiling. Okay, that's a sentence. Now I'm\n\n1:27:10.400 --> 1:27:13.040\n asking you a question. Was the nail horizontal or vertical?\n\n1:27:14.080 --> 1:27:15.040\n Vertical.\n\n1:27:15.040 --> 1:27:16.400\n Okay, how did you answer that?\n\n1:27:16.400 --> 1:27:24.960\n Well, I imagined Sally, it was kind of hard to imagine what the hell she was doing, but I\n\n1:27:24.960 --> 1:27:28.320\n imagined I had a visual of the whole situation.\n\n1:27:28.320 --> 1:27:34.400\n Exactly, exactly. So here, you know, I post a question in natural language. The answer to\n\n1:27:34.400 --> 1:27:40.720\n that question was you got the answer from actually simulating the scene. Now I can go more and more\n\n1:27:40.720 --> 1:27:46.640\n detailed about, okay, was Sally standing on something while doing this? Could she have been\n\n1:27:47.280 --> 1:27:53.360\n standing on a light bulb to do this? I could ask more and more questions about this and I can ask,\n\n1:27:53.360 --> 1:27:59.200\n make you simulate the scene in more and more detail, right? Where is all that knowledge that\n\n1:27:59.200 --> 1:28:05.600\n you're accessing stored? It is not in your language system. It was not just by reading\n\n1:28:05.600 --> 1:28:11.760\n text, you got that knowledge. It is stored from the everyday experiences that you have had from,\n\n1:28:12.320 --> 1:28:18.720\n and by the age of five, you have pretty much all of this, right? And it is stored in your visual\n\n1:28:18.720 --> 1:28:23.280\n system, motor system in a way such that it can be accessed through language.\n\n1:28:24.480 --> 1:28:30.000\n Got it. I mean, right. So the language is just almost sort of the query into the whole visual\n\n1:28:30.000 --> 1:28:36.800\n cortex and that does the whole feedback thing. But I mean, it is all reasoning kind of connected to\n\n1:28:36.800 --> 1:28:43.920\n the perception system in some way. You can do a lot of it. You know, you can still do a lot of it\n\n1:28:43.920 --> 1:28:49.760\n by quick associations without having to go into the depth. And most of the time you will be right,\n\n1:28:49.760 --> 1:28:55.440\n right? You can just do quick associations, but I can easily create tricky situations for you.\n\n1:28:55.440 --> 1:29:00.080\n Where that quick associations is wrong and you have to actually run the simulation.\n\n1:29:00.080 --> 1:29:06.800\n So figuring out how these concepts connect. Do I have a good idea of how to do that?\n\n1:29:06.800 --> 1:29:13.760\n That's exactly one of the problems that we are working on. And the way we are approaching that\n\n1:29:13.760 --> 1:29:20.400\n is basically saying, okay, you need to, so the takeaway is that language,\n\n1:29:20.400 --> 1:29:28.960\n is simulation control and your perceptual plus a motor system is building a simulation of the world.\n\n1:29:28.960 --> 1:29:34.720\n And so that's basically the way we are approaching it. And the first thing that we built was a\n\n1:29:34.720 --> 1:29:40.160\n controllable perceptual system. And we built a schema networks, which was a controllable dynamic\n\n1:29:40.160 --> 1:29:44.960\n system. Then we built a concept learning system that puts all these things together\n\n1:29:44.960 --> 1:29:51.600\n into programs or subtractions that you can run and simulate. And now we are taking the step\n\n1:29:51.600 --> 1:29:57.760\n of connecting it to language. And it will be very simple examples. Initially, it will not be\n\n1:29:57.760 --> 1:30:02.640\n the GPT3 like examples, but it will be grounded simulation based language.\n\n1:30:02.640 --> 1:30:08.400\n And for like the querying would be like question answering kind of thing?\n\n1:30:08.400 --> 1:30:13.600\n Correct. Correct. And so that's what we're trying to do. We're trying to build a system\n\n1:30:13.600 --> 1:30:18.480\n kind of thing. Correct. Correct. And it will be in some simple world initially on, you know,\n\n1:30:19.120 --> 1:30:25.280\n but it will be about, okay, can the system connect the language and ground it in the right way and\n\n1:30:25.280 --> 1:30:29.600\n run the right simulations to come up with the answer. And the goal is to try to do things that,\n\n1:30:29.600 --> 1:30:38.720\n for example, GPT3 couldn't do. Correct. Speaking of which, if we could talk about GPT3 a little\n\n1:30:38.720 --> 1:30:46.080\n bit, I think it's an interesting thought provoking set of ideas that OpenAI is pushing forward. I\n\n1:30:46.080 --> 1:30:51.360\n think it's good for us to talk about the limits and the possibilities in the neural network. So\n\n1:30:51.360 --> 1:30:58.800\n in general, what are your thoughts about this recently released very large 175 billion parameter\n\n1:30:58.800 --> 1:31:05.600\n language model? So I haven't directly evaluated it yet. From what I have seen on Twitter and\n\n1:31:05.600 --> 1:31:09.840\n other people evaluating it, it looks very intriguing. I am very intrigued by some of\n\n1:31:09.840 --> 1:31:17.360\n the properties it is displaying. And of course the text generation part of that was already\n\n1:31:17.360 --> 1:31:26.480\n evident in GPT2 that it can generate coherent text over long distances. But of course the\n\n1:31:26.480 --> 1:31:32.000\n weaknesses are also pretty visible in saying that, okay, it is not really carrying a world state\n\n1:31:32.000 --> 1:31:39.200\n around. And sometimes you get sentences like, I went up the hill to reach the valley or the thing\n\n1:31:39.200 --> 1:31:46.080\n like some completely incompatible statements, or when you're traveling from one place to the other,\n\n1:31:46.080 --> 1:31:50.800\n it doesn't take into account the time of travel, things like that. So those things I think will\n\n1:31:50.800 --> 1:31:59.040\n happen less in GPT3 because it is trained on even more data and it can do even more longer distance\n\n1:31:59.040 --> 1:32:06.560\n coherence. But it will still have the fundamental limitations that it doesn't have a world model\n\n1:32:07.600 --> 1:32:13.280\n and it can't run simulations in its head to find whether something is true in the world or not.\n\n1:32:13.280 --> 1:32:19.680\n So it's taking a huge amount of text from the internet and forming a compressed representation.\n\n1:32:20.400 --> 1:32:27.600\n Do you think in that could emerge something that's an approximation of a world model,\n\n1:32:27.600 --> 1:32:35.920\n which essentially could be used for reasoning? I'm not talking about GPT3, I'm talking about GPT4,\n\n1:32:35.920 --> 1:32:42.320\n 5 and GPT10. Yeah, I mean they will look more impressive than GPT3. So if you take that to\n\n1:32:42.320 --> 1:32:51.520\n the extreme then a Markov chain of just first order and if you go to, I'm taking the other\n\n1:32:51.520 --> 1:32:59.200\n extreme, if you read Shannon's book, he has a model of English text which is based on first\n\n1:32:59.200 --> 1:33:03.120\n order Markov chains, second order Markov chains, third order Markov chains and saying that okay,\n\n1:33:03.120 --> 1:33:09.600\n third order Markov chains look better than first order Markov chains. So does that mean a first\n\n1:33:09.600 --> 1:33:18.160\n order Markov chain has a model of the world? Yes, it does. So yes, in that level when you go higher\n\n1:33:18.160 --> 1:33:24.160\n order models or more sophisticated structure in the model like the transformer networks have,\n\n1:33:24.160 --> 1:33:32.640\n yes they have a model of the text world, but that is not a model of the world. It's a model\n\n1:33:32.640 --> 1:33:41.120\n of the text world and it will have interesting properties and it will be useful, but just scaling\n\n1:33:41.120 --> 1:33:49.280\n it up is not going to give us AGI or natural language understanding or meaning. Well the\n\n1:33:49.280 --> 1:33:58.880\n question is whether being forced to compress a very large amount of text forces you to construct\n\n1:33:58.880 --> 1:34:06.800\n things that are very much like, because the ideas of concepts and meaning is a spectrum.\n\n1:34:06.800 --> 1:34:12.320\n Sure, yeah. So in order to form that kind of compression,\n\n1:34:13.920 --> 1:34:24.160\n maybe it will be forced to figure out abstractions which look awfully a lot like the kind of things\n\n1:34:24.160 --> 1:34:31.120\n that we think about as concepts, as world models, as common sense. Is that possible?\n\n1:34:31.120 --> 1:34:34.320\n No, I don't think it is possible because the information is not there.\n\n1:34:34.320 --> 1:34:38.640\n The information is there behind the text, right?\n\n1:34:38.640 --> 1:34:44.400\n No, unless somebody has written down all the details about how everything works in the world\n\n1:34:44.400 --> 1:34:51.040\n to the absurd amounts like, okay, it is easier to walk forward than backward, that you have to open\n\n1:34:51.040 --> 1:34:56.560\n the door to go out of the thing, doctors wear underwear. Unless all these things somebody\n\n1:34:56.560 --> 1:35:01.680\n has written down somewhere or somehow the program found it to be useful for compression from some\n\n1:35:01.680 --> 1:35:07.840\n other text, the information is not there. So that's an argument that text is a lot\n\n1:35:07.840 --> 1:35:13.040\n lower fidelity than the experience of our physical world.\n\n1:35:13.040 --> 1:35:15.680\n Right, correct. Pictures worth a thousand words.\n\n1:35:17.440 --> 1:35:24.080\n Well, in this case, pictures aren't really... So the richest aspect of the physical world isn't\n\n1:35:24.080 --> 1:35:28.240\n even just pictures, it's the interactivity with the world.\n\n1:35:28.240 --> 1:35:29.200\n Exactly, yeah.\n\n1:35:29.200 --> 1:35:34.480\n It's being able to interact. It's almost like...\n\n1:35:36.720 --> 1:35:42.880\n It's almost like if you could interact... Well, maybe I agree with you that pictures\n\n1:35:42.880 --> 1:35:45.760\n worth a thousand words, but a thousand...\n\n1:35:45.760 --> 1:35:49.760\n It's still... Yeah, you could capture it with the GPTX.\n\n1:35:49.760 --> 1:35:54.400\n So I wonder if there's some interactive element where a system could live in text world where it\n\n1:35:54.400 --> 1:36:03.040\n could be part of the chat, be part of talking to people. It's interesting. I mean, fundamentally...\n\n1:36:03.040 --> 1:36:10.960\n So you're making a statement about the limitation of text. Okay, so let's say we have a text\n\n1:36:10.960 --> 1:36:19.280\n corpus that includes basically every experience we could possibly have. I mean, just a very large\n\n1:36:19.280 --> 1:36:25.440\n corpus of text and also interactive components. I guess the question is whether the neural network\n\n1:36:25.440 --> 1:36:32.400\n architecture, these very simple transformers, but if they had like hundreds of trillions or\n\n1:36:33.200 --> 1:36:40.800\n whatever comes after a trillion parameters, whether that could store the information\n\n1:36:42.080 --> 1:36:46.880\n needed, that's architecturally. Do you have thoughts about the limitation on that side of\n\n1:36:46.880 --> 1:36:52.160\n things with neural networks? I mean, so transformers are still a feed forward neural\n\n1:36:52.160 --> 1:36:59.200\n network. It has a very interesting architecture, which is good for text modeling and probably some\n\n1:36:59.200 --> 1:37:04.560\n aspects of video modeling, but it is still a feed forward architecture. You believe in the\n\n1:37:04.560 --> 1:37:11.280\n feedback mechanism, the recursion. Oh, and also causality, being able to do counterfactual\n\n1:37:11.280 --> 1:37:20.080\n reasoning, being able to do interventions, which is actions in the world. So all those things\n\n1:37:20.080 --> 1:37:28.400\n require different kinds of models to be built. I don't think transformers captures that family. It\n\n1:37:28.400 --> 1:37:35.280\n is very good at statistical modeling of text and it will become better and better with more data,\n\n1:37:35.280 --> 1:37:44.240\n bigger models, but that is only going to get so far. So I had this joke on Twitter saying that,\n\n1:37:44.240 --> 1:37:51.600\n hey, this is a model that has read all of quantum mechanics and theory of relativity and we are\n\n1:37:51.600 --> 1:37:59.280\n asking you to do text completion or we are asking you to solve simple puzzles. When you have AGI,\n\n1:37:59.280 --> 1:38:08.240\n that is not what you ask the system to do. We will ask the system to do experiments and come\n\n1:38:08.240 --> 1:38:13.680\n up with hypothesis and revise the hypothesis based on evidence from experiments, all those things.\n\n1:38:13.680 --> 1:38:18.800\n Those are the things that we want the system to do when we have AGI, not solve simple puzzles.\n\n1:38:20.000 --> 1:38:24.080\n Like impressive demos, somebody generating a red button in HTML.\n\n1:38:24.080 --> 1:38:29.920\n Right, which are all useful. There is no dissing the usefulness of it.\n\n1:38:29.920 --> 1:38:36.160\n So by the way, I am playing a little bit of a devil's advocate, so calm down internet.\n\n1:38:37.280 --> 1:38:47.040\n So I am curious almost in which ways will a dumb but large neural network will surprise us.\n\n1:38:47.040 --> 1:38:56.880\n I completely agree with your intuition. It is just that I do not want to dogmatically\n\n1:38:58.400 --> 1:39:06.160\n 100% put all the chips there. We have been surprised so much. Even the current GPT2 and\n\n1:39:06.160 --> 1:39:18.640\n GPT3 are so surprising. The self play mechanisms of AlphaZero are really surprising. The fact that\n\n1:39:18.640 --> 1:39:23.440\n reinforcement learning works at all to me is really surprising. The fact that neural networks work at\n\n1:39:23.440 --> 1:39:30.320\n all is quite surprising given how nonlinear the space is, the fact that it is able to find local\n\n1:39:30.320 --> 1:39:39.760\n minima that are at all reasonable. It is very surprising. I wonder sometimes whether us humans\n\n1:39:39.760 --> 1:39:51.760\n just want for AGI not to be such a dumb thing. Because exactly what you are saying is like\n\n1:39:52.560 --> 1:39:57.600\n the ideas of concepts and be able to reason with those concepts and connect those concepts in\n\n1:39:57.600 --> 1:40:05.360\n hierarchical ways and then to be able to have world models. Just everything we are describing\n\n1:40:05.360 --> 1:40:11.120\n in human language in this poetic way seems to make sense. That is what intelligence and reasoning\n\n1:40:11.120 --> 1:40:17.680\n are like. I wonder if at the core of it, it could be much dumber. Well, finally it is still\n\n1:40:17.680 --> 1:40:24.880\n connections and messages passing over. So in that way it is dumb. So I guess the recursion,\n\n1:40:24.880 --> 1:40:29.760\n the feedback mechanism, that does seem to be a fundamental kind of thing.\n\n1:40:32.560 --> 1:40:39.920\n The idea of concepts. Also memory. Correct. Having an episodic memory. That seems to be\n\n1:40:39.920 --> 1:40:45.760\n an important thing. So how do we get memory? So we have another piece of work which came\n\n1:40:45.760 --> 1:40:52.080\n out recently on how do you form episodic memories and form abstractions from them.\n\n1:40:52.080 --> 1:40:57.680\n And we haven't figured out all the connections of that to the overall cognitive architecture.\n\n1:40:57.680 --> 1:41:04.720\n But what are your ideas about how you could have episodic memory? So at least it is very clear\n\n1:41:04.720 --> 1:41:11.920\n that you need to have two kinds of memory. That is very, very clear. There are things that happen\n\n1:41:13.600 --> 1:41:19.760\n as statistical patterns in the world, but then there is the one timeline of things that happen\n\n1:41:19.760 --> 1:41:27.360\n only once in your life. And this day is not going to happen ever again. And that needs to be stored\n\n1:41:27.360 --> 1:41:36.000\n as just a stream of strings. This is my experience. And then the question is about\n\n1:41:36.000 --> 1:41:40.880\n how do you take that experience and connect it to the statistical part of it? How do you\n\n1:41:40.880 --> 1:41:47.040\n now say that, okay, I experienced this thing. Now I want to be careful about similar situations.\n\n1:41:47.040 --> 1:41:57.920\n So you need to be able to index that similarity using your other giants that is the model of the\n\n1:41:57.920 --> 1:42:02.000\n world that you have learned. Although the situation came from the episode, you need to be able to\n\n1:42:02.000 --> 1:42:13.200\n index the other one. So the episodic memory being implemented as an indexing over the other model\n\n1:42:13.200 --> 1:42:24.000\n that you're building. So the memories remain and they're indexed into the statistical thing\n\n1:42:24.000 --> 1:42:30.560\n that you form. Yeah, statistical causal structural model that you built over time. So it's basically\n\n1:42:30.560 --> 1:42:41.360\n the idea is that the hippocampus is just storing or sequencing a set of pointers that happens over\n\n1:42:41.360 --> 1:42:48.880\n time. And then whenever you want to reconstitute that memory and evaluate the different aspects of\n\n1:42:48.880 --> 1:42:54.080\n it, whether it was good, bad, do I need to encounter the situation again? You need the cortex\n\n1:42:55.200 --> 1:43:00.880\n to reinstantiate, to replay that memory. So how do you find that memory? Like which\n\n1:43:00.880 --> 1:43:05.760\n direction is the important direction? Both directions are again, bidirectional.\n\n1:43:05.760 --> 1:43:11.840\n I mean, I guess how do you retrieve the memory? So this is again, hypothesis. We're making this\n\n1:43:11.840 --> 1:43:21.200\n up. So when you come to a new situation, your cortex is doing inference over in the new situation.\n\n1:43:21.200 --> 1:43:27.600\n And then of course, hippocampus is connected to different parts of the cortex and you have this\n\n1:43:27.600 --> 1:43:35.680\n deja vu situation, right? Okay, I have seen this thing before. And then in the hippocampus, you can\n\n1:43:35.680 --> 1:43:44.480\n have an index of, okay, this is when it happened as a timeline. And then you can use the hippocampus\n\n1:43:44.480 --> 1:43:52.240\n to drive the similar timelines to say now I am, rather than being driven by my current input\n\n1:43:52.240 --> 1:43:58.400\n stimuli, I am going back in time and rewinding my experience from there, putting back into the\n\n1:43:58.400 --> 1:44:03.680\n cortex. And then putting it back into the cortex of course affects what you're going to see next\n\n1:44:03.680 --> 1:44:08.640\n in your current situation. Got it. Yeah. So that's the whole thing, having a world model and then\n\n1:44:09.280 --> 1:44:16.320\n yeah, connecting to the perception. Yeah, it does seem to be that that's what's happening. On the\n\n1:44:16.320 --> 1:44:24.240\n neural network side, it's interesting to think of how we actually do that. Yeah. To have a knowledge\n\n1:44:24.240 --> 1:44:31.120\n base. Yes. It is possible that you can put many of these structures into neural networks and we will\n\n1:44:31.120 --> 1:44:39.440\n find ways of combining properties of neural networks and graphical models. So, I mean,\n\n1:44:39.440 --> 1:44:43.840\n it's already started happening. Graph neural networks are kind of a merge between them.\n\n1:44:43.840 --> 1:44:50.320\n Yeah. And there will be more of that thing. So, but to me it is, the direction is pretty clear,\n\n1:44:51.440 --> 1:44:59.600\n looking at biology and the history of evolutionary history of intelligence, it is pretty clear that,\n\n1:44:59.600 --> 1:45:06.480\n okay, what is needed is more structure in the models and modeling of the world and supporting\n\n1:45:06.480 --> 1:45:13.600\n dynamic inference. Well, let me ask you, there's a guy named Elon Musk, there's a company called\n\n1:45:13.600 --> 1:45:18.960\n Neuralink and there's a general field called brain computer interfaces. Yeah. It's kind of a\n\n1:45:20.480 --> 1:45:26.560\n interface between your two loves. Yes. The brain and the intelligence. So there's like\n\n1:45:26.560 --> 1:45:32.160\n very direct applications of brain computer interfaces for people with different conditions,\n\n1:45:32.160 --> 1:45:38.320\n more in the short term. Yeah. But there's also these sci fi futuristic kinds of ideas of AI\n\n1:45:38.320 --> 1:45:45.600\n systems being able to communicate in a high bandwidth way with the brain, bidirectional.\n\n1:45:45.600 --> 1:45:53.840\n Yeah. What are your thoughts about Neuralink and BCI in general as a possibility? So I think BCI\n\n1:45:53.840 --> 1:46:02.240\n is a cool research area. And in fact, when I got interested in brains initially, when I was\n\n1:46:02.240 --> 1:46:07.840\n enrolled at Stanford and when I got interested in brains, it was through a brain computer\n\n1:46:07.840 --> 1:46:12.880\n interface talk that Krishna Shenoy gave. That's when I even started thinking about the problem.\n\n1:46:14.160 --> 1:46:21.200\n So it is definitely a fascinating research area and the applications are enormous. So there is a\n\n1:46:21.200 --> 1:46:26.160\n science fiction scenario of brains directly communicating. Let's keep that aside for the\n\n1:46:26.160 --> 1:46:32.400\n time being. Even just the intermediate milestones that pursuing, which are very reasonable as far\n\n1:46:32.400 --> 1:46:40.560\n as I can see, being able to control an external limb using direct connections from the brain\n\n1:46:40.560 --> 1:46:48.560\n and being able to write things into the brain. So those are all good steps to take and they have\n\n1:46:49.120 --> 1:46:55.280\n enormous applications. People losing limbs being able to control prosthetics, quadriplegics being\n\n1:46:55.280 --> 1:47:01.440\n able to control something, and therapeutics. I also know about another company working in\n\n1:47:01.440 --> 1:47:09.120\n the space called Paradromics. They're based on a different electrode array, but trying to attack\n\n1:47:09.120 --> 1:47:14.800\n some of the same problems. So I think it's a very... Also surgery? Correct. Surgically implanted\n\n1:47:14.800 --> 1:47:22.560\n electrodes. Yeah. So yeah, I think of it as a very, very promising field, especially when it is\n\n1:47:22.560 --> 1:47:29.040\n helping people overcome some limitations. Now, at some point, of course, it will advance the level of\n\n1:47:29.040 --> 1:47:37.440\n being able to communicate. How hard is that problem do you think? Let's say we magically solve\n\n1:47:37.440 --> 1:47:45.600\n what I think is a really hard problem of doing all of this safely. Yeah. So being able to connect\n\n1:47:45.600 --> 1:47:51.440\n electrodes and not just thousands, but like millions to the brain. I think it's very,\n\n1:47:51.440 --> 1:47:58.160\n very hard because you also do not know what will happen to the brain with that in the sense of how\n\n1:47:58.160 --> 1:48:03.680\n does the brain adapt to something like that? And as we were learning, the brain is quite,\n\n1:48:04.800 --> 1:48:10.480\n in terms of neuroplasticity, is pretty malleable. Correct. So it's going to adjust. Correct. So the\n\n1:48:10.480 --> 1:48:14.480\n machine learning side, the computer side is going to adjust, and then the brain is going to adjust.\n\n1:48:14.480 --> 1:48:20.400\n Exactly. And then what soup does this land us into? The kind of hallucinations you might get\n\n1:48:20.400 --> 1:48:28.080\n from this that might be pretty intense. Just connecting to all of Wikipedia. It's interesting\n\n1:48:28.080 --> 1:48:34.400\n whether we need to be able to figure out the basic protocol of the brain's communication schemes\n\n1:48:34.960 --> 1:48:41.120\n in order to get them to the machine and the brain to talk. Because another possibility is the brain\n\n1:48:41.120 --> 1:48:45.280\n actually just adjust to whatever the heck the computer is doing. Exactly. That's the way I think\n\n1:48:45.280 --> 1:48:51.440\n that I find that to be a more promising way. It's basically saying, okay, attach electrodes\n\n1:48:51.440 --> 1:48:58.320\n to some part of the cortex. Maybe if it is done from birth, the brain will adapt. It says that\n\n1:48:58.880 --> 1:49:02.880\n that part is not damaged. It was not used for anything. These electrodes are attached there.\n\n1:49:02.880 --> 1:49:09.120\n And now you train that part of the brain to do this high bandwidth communication between\n\n1:49:09.120 --> 1:49:15.680\n something else. And if you do it like that, then it is brain adapting to... And of course,\n\n1:49:15.680 --> 1:49:21.200\n your external system is designed so that it is adaptable. Just like we designed computers\n\n1:49:21.200 --> 1:49:28.720\n or mouse, keyboard, all of them to be interacting with humans. So of course, that feedback system\n\n1:49:28.720 --> 1:49:37.360\n is designed to be human compatible, but now it is not trying to record from all of the brain.\n\n1:49:37.360 --> 1:49:44.160\n And now two systems trying to adapt to each other. It's the brain adapting into one way.\n\n1:49:44.160 --> 1:49:51.520\n That's fascinating. The brain is connected to the internet. Just imagine just connecting it\n\n1:49:51.520 --> 1:49:59.760\n to Twitter and just taking that stream of information. Yeah. But again, if we take a\n\n1:49:59.760 --> 1:50:08.000\n step back, I don't know what your intuition is. I feel like that is not as hard of a problem as the\n\n1:50:08.720 --> 1:50:19.200\n doing it safely. There's a huge barrier to surgery because the biological system, it's a mush of\n\n1:50:19.200 --> 1:50:26.800\n like weird stuff. So that the surgery part of it, biology part of it, the longterm repercussions\n\n1:50:26.800 --> 1:50:35.440\n part of it. I don't know what else will... We often find after a long time in biology that,\n\n1:50:35.440 --> 1:50:42.960\n okay, that idea was wrong. So people used to cut off the gland called the thymus or something.\n\n1:50:43.680 --> 1:50:48.000\n And then they found that, oh no, that actually causes cancer.\n\n1:50:50.560 --> 1:50:55.440\n And then there's a subtle like millions of variables involved. But this whole process,\n\n1:50:55.440 --> 1:51:02.000\n the nice thing, just like again with Elon, just like colonizing Mars, seems like a ridiculously\n\n1:51:02.000 --> 1:51:08.320\n difficult idea. But in the process of doing it, we might learn a lot about the biology of the\n\n1:51:08.320 --> 1:51:13.520\n neurobiology of the brain, the neuroscience side of things. It's like, if you want to learn\n\n1:51:13.520 --> 1:51:19.520\n something, do the most difficult version of it and see what you learn. The intermediate steps\n\n1:51:19.520 --> 1:51:25.680\n that they are taking sounded all very reasonable to me. It's great. Well, but like everything with\n\n1:51:25.680 --> 1:51:33.280\n Elon is the timeline seems insanely fast. So that's the only awful question. Well,\n\n1:51:34.000 --> 1:51:36.960\n we've been talking about cognition a little bit. So like reasoning,\n\n1:51:38.640 --> 1:51:43.840\n we haven't mentioned the other C word, which is consciousness. Do you ever think about that one?\n\n1:51:43.840 --> 1:51:51.520\n Is that useful at all in this whole context of what it takes to create an intelligent reasoning\n\n1:51:51.520 --> 1:51:58.400\n being? Or is that completely outside of your, like the engineering perspective of intelligence?\n\n1:51:58.400 --> 1:52:05.120\n It is not outside the realm, but it doesn't on a day to day basis inform what we do,\n\n1:52:05.120 --> 1:52:12.160\n but it's more, so in many ways, the company name is connected to this idea of consciousness.\n\n1:52:12.160 --> 1:52:19.600\n What's the company name? Vicarious. So Vicarious is the company name. And so what does Vicarious\n\n1:52:19.600 --> 1:52:29.360\n mean? At the first level, it is about modeling the world and it is internalizing the external actions.\n\n1:52:29.360 --> 1:52:34.960\n So you interact with the world and learn a lot about the world. And now after having learned\n\n1:52:34.960 --> 1:52:42.080\n a lot about the world, you can run those things in your mind without actually having to act\n\n1:52:42.080 --> 1:52:48.800\n in the world. So you can run things vicariously just in your brain. And similarly, you can\n\n1:52:48.800 --> 1:52:54.560\n experience another person's thoughts by having a model of how that person works\n\n1:52:54.560 --> 1:53:01.280\n and running there, putting yourself in some other person's shoes. So that is being vicarious.\n\n1:53:01.280 --> 1:53:06.800\n Now it's the same modeling apparatus that you're using to model the external world\n\n1:53:06.800 --> 1:53:14.320\n or some other person's thoughts. You can turn it to yourself. If that same modeling thing is\n\n1:53:14.320 --> 1:53:21.040\n applied to your own modeling apparatus, then that is what gives rise to consciousness, I think.\n\n1:53:21.040 --> 1:53:25.840\n Well, that's more like self awareness. There's the hard problem of consciousness, which is\n\n1:53:25.840 --> 1:53:37.680\n when the model feels like something, when this whole process is like you really are in it.\n\n1:53:37.680 --> 1:53:43.920\n You feel like an entity in this world. Not just you know that you're an entity, but it feels like\n\n1:53:43.920 --> 1:53:54.400\n something to be that entity. And thereby, we attribute this. Then it starts to be where\n\n1:53:54.400 --> 1:53:59.120\n something that has consciousness can suffer. You start to have these kinds of things that we can\n\n1:53:59.120 --> 1:54:09.520\n reason about that is much heavier. It seems like there's much greater cost to your decisions.\n\n1:54:09.520 --> 1:54:18.640\n And mortality is tied up into that. The fact that these things end. First of all, I end at some\n\n1:54:18.640 --> 1:54:27.840\n point, and then other things end. That somehow seems to be, at least for us humans, a deep\n\n1:54:27.840 --> 1:54:38.320\n motivator. That idea of motivation in general, we talk about goals in AI, but goals aren't quite\n\n1:54:38.320 --> 1:54:46.560\n the same thing as our mortality. It feels like, first of all, humans don't have a goal, and they\n\n1:54:46.560 --> 1:54:54.240\n just kind of create goals at different levels. They make up goals because we're terrified by\n\n1:54:54.240 --> 1:55:02.880\n the mystery of the thing that gets us all. We make these goals up. We're like a goal generation\n\n1:55:02.880 --> 1:55:10.880\n machine, as opposed to a machine which optimizes the trajectory towards a singular goal. It feels\n\n1:55:10.880 --> 1:55:18.480\n like that's an important part of cognition, that whole mortality thing. Well, it is a part of human\n\n1:55:18.480 --> 1:55:30.080\n cognition, but there is no reason for that mortality to come to the equation for an artificial\n\n1:55:30.080 --> 1:55:36.800\n system, because we can copy the artificial system. The problem with humans is that I can't clone\n\n1:55:36.800 --> 1:55:45.760\n you. Even if I clone you as the hardware, your experience that was stored in your brain,\n\n1:55:45.760 --> 1:55:52.880\n your episodic memory, all those will not be captured in the new clone. But that's not the\n\n1:55:52.880 --> 1:56:02.320\n same with an AI system. But it's also possible that the thing that you mentioned with us humans\n\n1:56:02.320 --> 1:56:07.760\n is actually of fundamental importance for intelligence. The fact that you can copy an AI\n\n1:56:07.760 --> 1:56:18.240\n system means that that AI system is not yet an AGI. If you look at existence proof, if we reason\n\n1:56:18.240 --> 1:56:24.080\n based on existence proof, you could say that it doesn't feel like death is a fundamental property\n\n1:56:24.080 --> 1:56:33.040\n of an intelligent system. But we don't yet. Give me an example of an immortal intelligent being.\n\n1:56:33.840 --> 1:56:42.240\n We don't have those. It's very possible that that is a fundamental property of intelligence,\n\n1:56:42.240 --> 1:56:49.840\n is a thing that has a deadline for itself. Well, you can think of it like this. Suppose you invent\n\n1:56:49.840 --> 1:56:58.160\n a way to freeze people for a long time. It's not dying. So you can be frozen and woken up\n\n1:56:58.160 --> 1:57:08.000\n thousands of years from now. So it's no fear of death. Well, no, it's not about time. It's about\n\n1:57:08.000 --> 1:57:17.120\n the knowledge that it's temporary. And that aspect of it, the finiteness of it, I think\n\n1:57:17.120 --> 1:57:23.200\n creates a kind of urgency. Correct. For us, for humans. Yeah, for humans. Yes. And that is part\n\n1:57:23.200 --> 1:57:35.040\n of our drives. And that's why I'm not too worried about AI having motivations to kill all humans\n\n1:57:35.040 --> 1:57:43.440\n and those kinds of things. Why? Just wait. So why do you need to do that? I've never heard that\n\n1:57:43.440 --> 1:57:51.120\n before. That's a good point. Yeah, just murder seems like a lot of work. Let's just wait it out.\n\n1:57:52.560 --> 1:58:01.440\n They'll probably hurt themselves. Let me ask you, people often kind of wonder, world class researchers\n\n1:58:01.440 --> 1:58:10.320\n such as yourself, what kind of books, technical fiction, philosophical, had an impact on you and\n\n1:58:10.320 --> 1:58:17.920\n your life and maybe ones you could possibly recommend that others read? Maybe if you have\n\n1:58:17.920 --> 1:58:23.920\n three books that pop into mind. Yeah. So I definitely liked Judea Pearl's book,\n\n1:58:23.920 --> 1:58:30.640\n Probabilistic Reasoning and Intelligent Systems. It's a very deep technical book. But what I liked\n\n1:58:30.640 --> 1:58:36.400\n is that, so there are many places where you can learn about probabilistic graphical models from.\n\n1:58:36.400 --> 1:58:42.960\n But throughout this book, Judea Pearl kind of sprinkles his philosophical observations and he\n\n1:58:42.960 --> 1:58:48.400\n thinks about, connects us to how the brain thinks and attentions and resources, all those things. So\n\n1:58:48.400 --> 1:58:54.400\n that whole thing makes it more interesting to read. He emphasizes the importance of causality.\n\n1:58:54.400 --> 1:58:58.800\n So that was in his later book. So this was the first book, Probabilistic Reasoning and Intelligent\n\n1:58:58.800 --> 1:59:05.040\n Systems. He mentions causality, but he hadn't really sunk his teeth into causality. But he\n\n1:59:05.040 --> 1:59:11.360\n really sunk his teeth into, how do you actually formalize it? And the second book,\n\n1:59:11.360 --> 1:59:17.040\n Causality, the one in 2000, that one is really hard. So I would recommend that.\n\n1:59:17.840 --> 1:59:21.840\n Yeah. So that looks at the mathematical, his model of...\n\n1:59:22.560 --> 1:59:23.120\n Do calculus.\n\n1:59:23.120 --> 1:59:25.520\n Do calculus. Yeah. It was pretty dense mathematically.\n\n1:59:25.520 --> 1:59:28.880\n Right. The book of Y is definitely more enjoyable.\n\n1:59:28.880 --> 1:59:29.360\n For sure.\n\n1:59:29.360 --> 1:59:34.160\n Yeah. So I would recommend Probabilistic Reasoning and Intelligent Systems.\n\n1:59:34.160 --> 1:59:41.360\n Another book I liked was one from Doug Hofstadter. This was a long time ago. He had a book,\n\n1:59:41.360 --> 1:59:49.200\n I think it was called The Mind's Eye. It was probably Hofstadter and Daniel Dennett together.\n\n1:59:49.200 --> 1:59:54.880\n Yeah. And I actually was, I bought that book. It's on my show. I haven't read it yet,\n\n1:59:54.880 --> 2:00:00.800\n but I couldn't get an electronic version of it, which is annoying because you read everything on\n\n2:00:00.800 --> 2:00:06.560\n Kindle. So you had to actually purchase the physical. It's one of the only physical books\n\n2:00:06.560 --> 2:00:11.200\n I have because anyway, a lot of people recommended it highly. So yeah.\n\n2:00:11.200 --> 2:00:18.000\n And the third one I would definitely recommend reading is, this is not a technical book. It is\n\n2:00:18.720 --> 2:00:25.040\n history. The name of the book, I think, is Bishop's Boys. It's about Wright brothers\n\n2:00:25.040 --> 2:00:34.560\n and their path and how it was... There are multiple books on this topic and all of them\n\n2:00:34.560 --> 2:00:45.840\n are great. It's fascinating how flight was treated as an unsolvable problem. And also,\n\n2:00:46.400 --> 2:00:51.520\n what aspects did people emphasize? People thought, oh, it is all about\n\n2:00:51.520 --> 2:01:00.160\n just powerful engines. You just need to have powerful lightweight engines. And so some people\n\n2:01:00.160 --> 2:01:04.000\n thought of it as, how far can we just throw the thing? Just throw it.\n\n2:01:04.000 --> 2:01:05.040\n Like a catapult.\n\n2:01:05.040 --> 2:01:11.520\n Yeah. So it's very fascinating. And even after they made the invention,\n\n2:01:11.520 --> 2:01:13.040\n people are not believing it.\n\n2:01:13.040 --> 2:01:15.360\n Ah, the social aspect of it.\n\n2:01:15.360 --> 2:01:18.240\n The social aspect. It's very fascinating.\n\n2:01:18.240 --> 2:01:28.320\n I mean, do you draw any parallels between birds fly? So there's the natural approach to flight\n\n2:01:28.320 --> 2:01:33.920\n and then there's the engineered approach. Do you see the same kind of thing with the brain\n\n2:01:33.920 --> 2:01:35.840\n and our trying to engineer intelligence?\n\n2:01:37.280 --> 2:01:43.920\n Yeah. It's a good analogy to have. Of course, all analogies have their limits.\n\n2:01:43.920 --> 2:01:54.000\n So people in AI often use airplanes as an example of, hey, we didn't learn anything from birds.\n\n2:01:55.120 --> 2:02:02.560\n But the funny thing is that, and the saying is, airplanes don't flap wings. This is what they\n\n2:02:02.560 --> 2:02:09.520\n say. The funny thing and the ironic thing is that you don't need to flap to fly is something\n\n2:02:09.520 --> 2:02:18.640\n Wright brothers found by observing birds. So they have in their notebook, in some of these books,\n\n2:02:18.640 --> 2:02:25.680\n they show their notebook drawings. They make detailed notes about buzzards just soaring over\n\n2:02:26.240 --> 2:02:31.440\n thermals. And they basically say, look, flapping is not the important, propulsion is not the\n\n2:02:31.440 --> 2:02:37.120\n important problem to solve here. We want to solve control. And once you solve control,\n\n2:02:37.120 --> 2:02:42.640\n propulsion will fall into place. All of these are people, they realize this by observing birds.\n\n2:02:44.400 --> 2:02:49.280\n Beautifully put. That's actually brilliant because people do use that analogy a lot. I'm\n\n2:02:49.280 --> 2:02:54.480\n going to have to remember that one. Do you have advice for people interested in artificial\n\n2:02:54.480 --> 2:02:58.080\n intelligence like young folks today? I talk to undergraduate students all the time,\n\n2:02:59.200 --> 2:03:03.840\n interested in neuroscience, interested in understanding how the brain works. Is there\n\n2:03:03.840 --> 2:03:08.720\n advice you would give them about their career, maybe about their life in general?\n\n2:03:09.520 --> 2:03:14.080\n Sure. I think every piece of advice should be taken with a pinch of salt, of course,\n\n2:03:14.720 --> 2:03:20.400\n because each person is different, their motivations are different. But I can definitely\n\n2:03:20.400 --> 2:03:28.480\n say if your goal is to understand the brain from the angle of wanting to build one, then\n\n2:03:28.480 --> 2:03:36.240\n being an experimental neuroscientist might not be the way to go about it. A better way to pursue it\n\n2:03:36.240 --> 2:03:42.560\n might be through computer science, electrical engineering, machine learning, and AI. And of\n\n2:03:42.560 --> 2:03:48.800\n course, you have to study the neuroscience, but that you can do on your own. If you're more\n\n2:03:48.800 --> 2:03:53.680\n attracted by finding something intriguing about, discovering something intriguing about the brain,\n\n2:03:53.680 --> 2:03:58.480\n then of course, it is better to be an experimentalist. So find that motivation,\n\n2:03:58.480 --> 2:04:03.120\n what are you intrigued by? And of course, find your strengths too. Some people are very good\n\n2:04:03.120 --> 2:04:09.360\n experimentalists and they enjoy doing that. And it's interesting to see which department,\n\n2:04:10.160 --> 2:04:18.880\n if you're picking in terms of your education path, whether to go with like, at MIT, it's\n\n2:04:18.880 --> 2:04:29.120\n brain and computer, no, it'd be CS. Yeah. Brain and cognitive sciences, yeah. Or the CS side of\n\n2:04:29.120 --> 2:04:34.240\n things. And actually the brain folks, the neuroscience folks are more and more now\n\n2:04:34.240 --> 2:04:44.400\n embracing of learning TensorFlow and PyTorch, right? They see the power of trying to engineer\n\n2:04:44.400 --> 2:04:52.720\n ideas that they get from the brain into, and then explore how those could be used to create\n\n2:04:52.720 --> 2:04:58.640\n intelligent systems. So that might be the right department actually. Yeah. So this was a question\n\n2:04:58.640 --> 2:05:06.160\n in one of the Redwood Neuroscience Institute workshops that Jeff Hawkins organized almost 10\n\n2:05:06.160 --> 2:05:11.040\n years ago. This question was put to a panel, right? What should be the undergrad major you should\n\n2:05:11.040 --> 2:05:17.200\n take if you want to understand the brain? And the majority opinion in that one was electrical\n\n2:05:17.200 --> 2:05:23.840\n engineering. Interesting. Because, I mean, I'm a double undergrad, so I got lucky in that way.\n\n2:05:25.040 --> 2:05:30.080\n But I think it does have some of the right ingredients because you learn about circuits.\n\n2:05:30.080 --> 2:05:37.920\n You learn about how you can construct circuits to approach, do functions. You learn about\n\n2:05:37.920 --> 2:05:43.040\n microprocessors. You learn information theory. You learn signal processing. You learn continuous\n\n2:05:43.040 --> 2:05:50.880\n math. So in that way, it's a good step. If you want to go to computer science or neuroscience,\n\n2:05:50.880 --> 2:05:56.640\n it's a good step. The downside, you're more likely to be forced to use MATLAB.\n\n2:05:56.640 --> 2:06:07.920\n You're more likely to be forced to use MATLAB. So one of the interesting things about, I mean,\n\n2:06:07.920 --> 2:06:13.840\n this is changing. The world is changing. But certain departments lagged on the programming\n\n2:06:13.840 --> 2:06:19.280\n side of things, on developing good habits in terms of software engineering. But I think that's more\n\n2:06:19.280 --> 2:06:26.000\n and more changing. And students can take that into their own hands, like learn to program. I feel\n\n2:06:26.000 --> 2:06:34.800\n like everybody should learn to program because it, like everyone in the sciences, because it\n\n2:06:34.800 --> 2:06:40.400\n empowers, it puts the data at your fingertips. So you can organize it. You can find all kinds of\n\n2:06:40.400 --> 2:06:45.520\n things in the data. And then you can also, for the appropriate sciences, build systems that,\n\n2:06:46.240 --> 2:06:49.760\n like based on that. So like then engineer intelligent systems.\n\n2:06:49.760 --> 2:06:58.560\n We already talked about mortality. So we hit a ridiculous point. But let me ask you,\n\n2:07:04.800 --> 2:07:13.200\n one of the things about intelligence is it's goal driven. And you study the brain. So the question\n\n2:07:13.200 --> 2:07:17.360\n is like, what's the goal that the brain is operating under? What's the meaning of it all\n\n2:07:17.360 --> 2:07:23.920\n for us humans in your view? What's the meaning of life? The meaning of life is whatever you\n\n2:07:23.920 --> 2:07:31.760\n construct out of it. It's completely open. It's open. So there's nothing, like you mentioned,\n\n2:07:31.760 --> 2:07:42.000\n you like constraints. So it's wide open. Is there some useful aspect that you think about in terms\n\n2:07:42.000 --> 2:07:50.480\n of like the openness of it and just the basic mechanisms of generating goals in studying\n\n2:07:50.480 --> 2:07:56.640\n cognition in the brain that you think about? Or is it just about, because everything we've talked\n\n2:07:56.640 --> 2:08:00.640\n about kind of the perception system is to understand the environment. That's like to be\n\n2:08:00.640 --> 2:08:09.360\n able to like not die, like not fall over and like be able to, you don't think we need to\n\n2:08:09.360 --> 2:08:15.600\n think about anything bigger than that. Yeah, I think so, because it's basically being able to\n\n2:08:16.160 --> 2:08:21.600\n understand the machinery of the world such that you can pursue whatever goals you want.\n\n2:08:21.600 --> 2:08:26.800\n So the machinery of the world is really ultimately what we should be striving to understand. The\n\n2:08:26.800 --> 2:08:31.840\n rest is just whatever the heck you want to do or whatever fun you have.\n\n2:08:31.840 --> 2:08:42.640\n One who is culturally popular. I think that's beautifully put. I don't think there's a better\n\n2:08:42.640 --> 2:08:49.840\n way to end it. Dilip, I'm so honored that you show up here and waste your time with me. It's\n\n2:08:49.840 --> 2:08:54.400\n been an awesome conversation. Thanks so much for talking today. Oh, thank you so much. This was\n\n2:08:54.400 --> 2:09:00.880\n so much more fun than I expected. Thank you. Thanks for listening to this conversation with\n\n2:09:00.880 --> 2:09:07.920\n Dilip George. And thank you to our sponsors, Babbel, Raycon Earbuds, and Masterclass. Please\n\n2:09:07.920 --> 2:09:14.720\n consider supporting this podcast by going to babbel.com and use code LEX, going to buyraycon.com\n\n2:09:16.080 --> 2:09:22.240\n and signing up at masterclass.com. Click the links, get the discount. It really is the best\n\n2:09:22.240 --> 2:09:27.440\n way to support this podcast. If you enjoy this thing, subscribe on YouTube, review the Five\n\n2:09:27.440 --> 2:09:33.920\n Stars Napa podcast, support it on Patreon, or connect with me on Twitter at Lex Friedman,\n\n2:09:33.920 --> 2:09:43.120\n spelled yes, without the E, just F R I D M A M. And now let me leave you with some words from Marcus\n\n2:09:43.120 --> 2:09:51.360\n Aurelius. You have power over your mind, not outside events. Realize this and you will find\n\n2:09:51.360 --> 2:09:58.080\n strength. Thank you for listening and hope to see you next time.\n\n"
}