{
  "title": "Peter Singer: Suffering in Humans, Animals, and AI | Lex Fridman Podcast #107",
  "id": "llh-2pqSGrs",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:03.440\n The following is a conversation with Peter Singer,\n\n00:03.440 --> 00:06.200\n professor of bioethics at Princeton University,\n\n00:06.200 --> 00:10.280\n best known for his 1975 book, Animal Liberation,\n\n00:10.280 --> 00:14.240\n that makes an ethical case against eating meat.\n\n00:14.240 --> 00:17.680\n He has written brilliantly from an ethical perspective\n\n00:17.680 --> 00:21.480\n on extreme poverty, euthanasia, human genetic selection,\n\n00:21.480 --> 00:23.720\n sports doping, the sale of kidneys,\n\n00:23.720 --> 00:28.520\n and generally happiness, including in his books,\n\n00:28.520 --> 00:32.920\n Ethics in the Real World, and The Life You Can Save.\n\n00:32.920 --> 00:36.320\n He was a key popularizer of the effective altruism movement\n\n00:36.320 --> 00:39.240\n and is generally considered one of the most influential\n\n00:39.240 --> 00:42.200\n philosophers in the world.\n\n00:42.200 --> 00:43.760\n Quick summary of the ads.\n\n00:43.760 --> 00:47.080\n Two sponsors, Cash App and Masterclass.\n\n00:47.080 --> 00:48.840\n Please consider supporting the podcast\n\n00:48.840 --> 00:52.200\n by downloading Cash App and using code LexPodcast\n\n00:52.200 --> 00:55.920\n and signing up at masterclass.com slash Lex.\n\n00:55.920 --> 00:57.800\n Click the links, buy the stuff.\n\n00:57.800 --> 01:00.080\n It really is the best way to support the podcast\n\n01:00.080 --> 01:02.400\n and the journey I'm on.\n\n01:02.400 --> 01:07.480\n As you may know, I primarily eat a ketogenic or carnivore diet,\n\n01:07.480 --> 01:10.320\n which means that most of my diet is made up of meat.\n\n01:10.320 --> 01:15.280\n I do not hunt the food I eat, though one day I hope to.\n\n01:15.280 --> 01:17.800\n I love fishing, for example.\n\n01:17.800 --> 01:19.680\n Fishing and eating the fish I catch\n\n01:19.680 --> 01:23.640\n has always felt much more honest than participating\n\n01:23.640 --> 01:26.400\n in the supply chain of factory farming.\n\n01:26.400 --> 01:29.360\n From an ethics perspective, this part of my life\n\n01:29.360 --> 01:31.920\n has always had a cloud over it.\n\n01:31.920 --> 01:33.600\n It makes me think.\n\n01:33.600 --> 01:35.960\n I've tried a few times in my life\n\n01:35.960 --> 01:37.920\n to reduce the amount of meat I eat.\n\n01:37.920 --> 01:41.240\n But for some reason, whatever the makeup of my body,\n\n01:41.240 --> 01:44.040\n whatever the way I practice the dieting I have,\n\n01:44.040 --> 01:48.040\n I get a lot of mental and physical energy\n\n01:48.040 --> 01:50.600\n and performance from eating meat.\n\n01:50.600 --> 01:53.960\n So both intellectually and physically,\n\n01:53.960 --> 01:56.080\n it's a continued journey for me.\n\n01:56.080 --> 02:00.320\n I return to Peter's work often to reevaluate the ethics\n\n02:00.320 --> 02:03.360\n of how I live this aspect of my life.\n\n02:03.360 --> 02:06.160\n Let me also say that you may be a vegan\n\n02:06.160 --> 02:09.840\n or you may be a meat eater and may be upset by the words I say\n\n02:09.840 --> 02:13.680\n or Peter says, but I ask for this podcast\n\n02:13.680 --> 02:16.000\n and other episodes of this podcast\n\n02:16.000 --> 02:18.240\n that you keep an open mind.\n\n02:18.240 --> 02:21.640\n I may and probably will talk with people you disagree with.\n\n02:21.640 --> 02:25.360\n Please try to really listen, especially\n\n02:25.360 --> 02:27.400\n to people you disagree with.\n\n02:27.400 --> 02:29.800\n And give me and the world the gift\n\n02:29.800 --> 02:33.080\n of being a participant in a patient, intelligent,\n\n02:33.080 --> 02:34.840\n and nuanced discourse.\n\n02:34.840 --> 02:38.640\n If your instinct and desire is to be a voice of mockery\n\n02:38.640 --> 02:42.520\n towards those you disagree with, please unsubscribe.\n\n02:42.520 --> 02:44.840\n My source of joy and inspiration here\n\n02:44.840 --> 02:48.040\n has been to be a part of a community that thinks deeply\n\n02:48.040 --> 02:51.000\n and speaks with empathy and compassion.\n\n02:51.000 --> 02:53.880\n That is what I hope to continue being a part of\n\n02:53.880 --> 02:56.200\n and I hope you join as well.\n\n02:56.200 --> 02:58.960\n If you enjoy this podcast, subscribe on YouTube,\n\n02:58.960 --> 03:01.360\n review it with five stars on Apple Podcast,\n\n03:01.360 --> 03:04.280\n follow on Spotify, support on Patreon,\n\n03:04.280 --> 03:07.920\n or connect with me on Twitter at Lex Friedman.\n\n03:07.920 --> 03:09.960\n As usual, I'll do a few minutes of ads now\n\n03:09.960 --> 03:11.320\n and never any ads in the middle\n\n03:11.320 --> 03:14.040\n that can break the flow of the conversation.\n\n03:14.040 --> 03:16.560\n This show is presented by Cash App,\n\n03:16.560 --> 03:18.960\n the number one finance app in the App Store.\n\n03:18.960 --> 03:22.000\n When you get it, use code LEXPODCAST.\n\n03:22.000 --> 03:24.280\n Cash App lets you send money to friends,\n\n03:24.280 --> 03:27.320\n buy Bitcoin, and invest in the stock market\n\n03:27.320 --> 03:29.520\n with as little as one dollar.\n\n03:29.520 --> 03:31.800\n Since Cash App allows you to buy Bitcoin,\n\n03:31.800 --> 03:34.600\n let me mention that cryptocurrency in the context\n\n03:34.600 --> 03:37.400\n of the history of money is fascinating.\n\n03:37.400 --> 03:39.520\n I recommend Ascent of Money\n\n03:39.520 --> 03:41.480\n as a great book on this history.\n\n03:41.480 --> 03:43.160\n Debits and credits on ledgers\n\n03:43.160 --> 03:45.960\n started around 30,000 years ago.\n\n03:45.960 --> 03:48.560\n The US dollar created over 200 years ago\n\n03:48.560 --> 03:51.080\n and the first decentralized cryptocurrency\n\n03:51.080 --> 03:53.760\n released just over 10 years ago.\n\n03:53.760 --> 03:57.000\n So given that history, cryptocurrency is still very much\n\n03:57.000 --> 03:58.720\n in its early days of development,\n\n03:58.720 --> 04:01.280\n but it's still aiming to and just might\n\n04:01.280 --> 04:04.320\n redefine the nature of money.\n\n04:04.320 --> 04:07.000\n So again, if you get Cash App from the App Store\n\n04:07.000 --> 04:10.480\n or Google Play and use the code LEXPODCAST,\n\n04:10.480 --> 04:14.920\n you get $10 and Cash App will also donate $10 to FIRST,\n\n04:14.920 --> 04:16.720\n an organization that is helping to advance\n\n04:16.720 --> 04:20.880\n robotic system education for young people around the world.\n\n04:20.880 --> 04:23.440\n This show is sponsored by Masterclass.\n\n04:23.440 --> 04:26.080\n Sign up at masterclass.com slash LEX\n\n04:26.080 --> 04:29.640\n to get a discount and to support this podcast.\n\n04:29.640 --> 04:31.320\n When I first heard about Masterclass,\n\n04:31.320 --> 04:33.160\n I thought it was too good to be true.\n\n04:33.160 --> 04:36.680\n For $180 a year, you get an all access pass\n\n04:36.680 --> 04:40.400\n to watch courses from, to list some of my favorites,\n\n04:40.400 --> 04:42.920\n Chris Hadfield on space exploration,\n\n04:42.920 --> 04:46.200\n Neil Gauss Tyson on scientific thinking and communication,\n\n04:46.200 --> 04:50.400\n Will Wright, creator of SimCity and Sims on game design.\n\n04:50.400 --> 04:53.880\n I promise I'll start streaming games at some point soon.\n\n04:53.880 --> 04:57.520\n Carlos Santana on guitar, Gary Kasparov on chess,\n\n04:57.520 --> 05:01.600\n Daniel Lagrano on poker and many more.\n\n05:01.600 --> 05:04.240\n Chris Hadfield explaining how rockets work\n\n05:04.240 --> 05:07.280\n and the experience of being launched into space alone\n\n05:07.280 --> 05:08.720\n is worth the money.\n\n05:08.720 --> 05:12.820\n By the way, you can watch it on basically any device.\n\n05:12.820 --> 05:16.600\n Once again, sign up at masterclass.com slash LEX\n\n05:16.600 --> 05:19.360\n to get a discount and to support this podcast.\n\n05:19.360 --> 05:24.080\n And now, here's my conversation with Peter Singer.\n\n05:25.080 --> 05:27.640\n When did you first become conscious of the fact\n\n05:27.640 --> 05:30.340\n that there is much suffering in the world?\n\n05:32.280 --> 05:33.760\n I think I was conscious of the fact\n\n05:33.760 --> 05:35.760\n that there's a lot of suffering in the world\n\n05:35.760 --> 05:38.520\n pretty much as soon as I was able to understand\n\n05:38.520 --> 05:40.960\n anything about my family and its background\n\n05:40.960 --> 05:44.720\n because I lost three of my four grandparents\n\n05:44.720 --> 05:48.720\n in the Holocaust and obviously I knew\n\n05:48.720 --> 05:51.080\n why I only had one grandparent\n\n05:52.160 --> 05:54.560\n and she herself had been in the camps and survived,\n\n05:54.560 --> 05:58.120\n so I think I knew a lot about that pretty early.\n\n05:58.120 --> 06:01.200\n My entire family comes from the Soviet Union.\n\n06:01.200 --> 06:03.360\n I was born in the Soviet Union.\n\n06:05.400 --> 06:07.920\n World War II has deep roots in the culture\n\n06:07.920 --> 06:10.360\n and the suffering that the war brought\n\n06:10.360 --> 06:14.000\n the millions of people who died is in the music,\n\n06:14.000 --> 06:16.900\n is in the literature, is in the culture.\n\n06:16.900 --> 06:18.960\n What do you think was the impact\n\n06:18.960 --> 06:22.420\n of the war broadly on our society?\n\n06:25.080 --> 06:26.840\n The war had many impacts.\n\n06:28.160 --> 06:31.440\n I think one of them, a beneficial impact,\n\n06:31.440 --> 06:34.300\n is that it showed what racism\n\n06:34.300 --> 06:37.960\n and authoritarian government can do\n\n06:37.960 --> 06:41.080\n and at least as far as the West was concerned,\n\n06:41.080 --> 06:43.200\n I think that meant that I grew up in an era\n\n06:43.200 --> 06:48.000\n in which there wasn't the kind of overt racism\n\n06:48.000 --> 06:52.160\n and antisemitism that had existed for my parents in Europe.\n\n06:52.160 --> 06:53.800\n I was growing up in Australia\n\n06:53.800 --> 06:57.560\n and certainly that was clearly seen\n\n06:57.560 --> 06:59.400\n as something completely unacceptable.\n\n07:00.560 --> 07:05.560\n There was also, though, a fear of a further outbreak of war\n\n07:05.800 --> 07:08.920\n which this time we expected would be nuclear\n\n07:08.920 --> 07:11.720\n because of the way the Second World War had ended,\n\n07:11.720 --> 07:16.200\n so there was this overshadowing of my childhood\n\n07:16.200 --> 07:19.880\n about the possibility that I would not live to grow up\n\n07:19.880 --> 07:23.780\n and be an adult because of a catastrophic nuclear war.\n\n07:25.620 --> 07:28.100\n The film On the Beach was made\n\n07:28.100 --> 07:29.860\n in which the city that I was living,\n\n07:29.860 --> 07:32.080\n Melbourne, was the last place on Earth\n\n07:32.080 --> 07:34.320\n to have living human beings\n\n07:34.320 --> 07:36.420\n because of the nuclear cloud\n\n07:36.420 --> 07:38.120\n that was spreading from the North,\n\n07:38.120 --> 07:41.840\n so that certainly gave us a bit of that sense.\n\n07:42.840 --> 07:45.400\n There were many, there were clearly many other legacies\n\n07:45.400 --> 07:47.560\n that we got of the war as well\n\n07:47.560 --> 07:49.440\n and the whole setup of the world\n\n07:49.440 --> 07:51.600\n and the Cold War that followed.\n\n07:51.600 --> 07:55.320\n All of that has its roots in the Second World War.\n\n07:55.320 --> 07:58.120\n There is much beauty that comes from war.\n\n07:58.120 --> 08:01.400\n Sort of, I had a conversation with Eric Weinstein.\n\n08:01.400 --> 08:03.960\n He said everything is great about war\n\n08:03.960 --> 08:08.200\n except all the death and suffering.\n\n08:08.200 --> 08:10.120\n Do you think there's something positive\n\n08:11.060 --> 08:13.640\n that came from the war,\n\n08:13.640 --> 08:16.840\n the mirror that it put to our society,\n\n08:16.840 --> 08:20.320\n sort of the ripple effects on it, ethically speaking?\n\n08:20.320 --> 08:24.540\n Do you think there are positive aspects to war?\n\n08:24.540 --> 08:27.540\n I find it hard to see positive aspects in war\n\n08:27.540 --> 08:30.440\n and some of the things that other people think of\n\n08:30.440 --> 08:35.440\n as positive and beautiful may be questioning.\n\n08:35.640 --> 08:38.280\n So there's a certain kind of patriotism.\n\n08:38.280 --> 08:41.040\n People say during wartime, we all pull together,\n\n08:41.040 --> 08:44.080\n we all work together against a common enemy\n\n08:44.080 --> 08:45.300\n and that's true.\n\n08:45.300 --> 08:47.380\n An outside enemy does unite a country\n\n08:47.380 --> 08:49.920\n and in general, it's good for countries to be united\n\n08:49.920 --> 08:51.080\n and have common purposes\n\n08:51.080 --> 08:55.360\n but it also engenders a kind of a nationalism\n\n08:55.360 --> 08:57.760\n and a patriotism that can't be questioned\n\n08:57.760 --> 09:01.960\n and that I'm more skeptical about.\n\n09:01.960 --> 09:04.560\n What about the brotherhood\n\n09:04.560 --> 09:08.240\n that people talk about from soldiers?\n\n09:08.240 --> 09:12.960\n The sort of counterintuitive, sad idea\n\n09:12.960 --> 09:16.240\n that the closest that people feel to each other\n\n09:16.240 --> 09:17.880\n is in those moments of suffering,\n\n09:17.880 --> 09:20.360\n of being at the sort of the edge\n\n09:20.360 --> 09:24.980\n of seeing your comrades dying in your arms.\n\n09:24.980 --> 09:27.440\n That somehow brings people extremely closely together.\n\n09:27.440 --> 09:29.600\n Suffering brings people closer together.\n\n09:29.600 --> 09:31.920\n How do you make sense of that?\n\n09:31.920 --> 09:33.520\n It may bring people close together\n\n09:33.520 --> 09:36.440\n but there are other ways of bonding\n\n09:36.440 --> 09:37.840\n and being close to people I think\n\n09:37.840 --> 09:41.120\n without the suffering and death that war entails.\n\n09:42.840 --> 09:44.560\n Perhaps you could see, you could already hear\n\n09:44.560 --> 09:46.660\n the romanticized Russian in me.\n\n09:48.160 --> 09:50.280\n We tend to romanticize suffering just a little bit\n\n09:50.280 --> 09:53.440\n in our literature and culture and so on.\n\n09:53.440 --> 09:54.880\n Could you take a step back\n\n09:54.880 --> 09:57.560\n and I apologize if it's a ridiculous question\n\n09:57.560 --> 09:59.640\n but what is suffering?\n\n09:59.640 --> 10:03.760\n If you would try to define what suffering is,\n\n10:03.760 --> 10:05.560\n how would you go about it?\n\n10:05.560 --> 10:08.720\n Suffering is a conscious state.\n\n10:09.640 --> 10:11.360\n There can be no suffering for a being\n\n10:11.360 --> 10:13.040\n who is completely unconscious\n\n10:14.520 --> 10:17.940\n and it's distinguished from other conscious states\n\n10:17.940 --> 10:22.940\n in terms of being one that considered just in itself.\n\n10:22.940 --> 10:25.500\n We would rather be without.\n\n10:25.500 --> 10:27.500\n It's a conscious state that we want to stop\n\n10:27.500 --> 10:31.780\n if we're experiencing or we want to avoid having again\n\n10:31.780 --> 10:34.500\n if we've experienced it in the past.\n\n10:34.500 --> 10:37.400\n And that's, as I say, emphasized for its own sake\n\n10:37.400 --> 10:39.340\n because of course people will say,\n\n10:39.340 --> 10:41.580\n well, suffering strengthens the spirit.\n\n10:41.580 --> 10:43.140\n It has good consequences.\n\n10:44.340 --> 10:47.100\n And sometimes it does have those consequences\n\n10:47.100 --> 10:50.780\n and of course sometimes we might undergo suffering.\n\n10:50.780 --> 10:53.700\n We set ourselves a challenge to run a marathon\n\n10:53.700 --> 10:57.260\n or climb a mountain or even just to go to the dentist\n\n10:57.260 --> 10:59.100\n so that the toothache doesn't get worse\n\n10:59.100 --> 11:00.900\n even though we know the dentist is gonna hurt us\n\n11:00.900 --> 11:01.940\n to some extent.\n\n11:01.940 --> 11:04.520\n So I'm not saying that we never choose suffering\n\n11:04.520 --> 11:07.260\n but I am saying that other things being equal,\n\n11:07.260 --> 11:10.660\n we would rather not be in that state of consciousness.\n\n11:10.660 --> 11:12.380\n Is the ultimate goal sort of,\n\n11:12.380 --> 11:15.820\n you have the new 10 year anniversary release\n\n11:15.820 --> 11:18.860\n of the Life You Can Save book, really influential book.\n\n11:18.860 --> 11:20.700\n We'll talk about it a bunch of times\n\n11:20.700 --> 11:21.780\n throughout this conversation\n\n11:21.780 --> 11:25.340\n but do you think it's possible\n\n11:25.340 --> 11:29.820\n to eradicate suffering or is that the goal\n\n11:29.820 --> 11:34.820\n or do we want to achieve a kind of minimum threshold\n\n11:36.820 --> 11:41.500\n of suffering and then keeping a little drop of poison\n\n11:43.860 --> 11:46.160\n to keep things interesting in the world?\n\n11:46.160 --> 11:50.120\n In practice, I don't think we ever will eliminate suffering\n\n11:50.120 --> 11:53.000\n so I think that little drop of poison as you put it\n\n11:53.000 --> 11:58.000\n or if you like the contrasting dash of an unpleasant color\n\n11:58.680 --> 11:59.680\n perhaps something like that\n\n11:59.680 --> 12:04.040\n in a otherwise harmonious and beautiful composition,\n\n12:04.040 --> 12:05.880\n that is gonna always be there.\n\n12:07.240 --> 12:09.140\n If you ask me whether in theory\n\n12:09.140 --> 12:12.640\n if we could get rid of it, we should.\n\n12:12.640 --> 12:14.680\n I think the answer is whether in fact\n\n12:14.680 --> 12:17.760\n we would be better off\n\n12:17.760 --> 12:20.240\n or whether in terms of by eliminating the suffering\n\n12:20.240 --> 12:22.520\n we would also eliminate some of the highs,\n\n12:22.520 --> 12:24.880\n the positive highs and if that's so\n\n12:24.880 --> 12:27.360\n then we might be prepared to say\n\n12:27.360 --> 12:30.600\n it's worth having a minimum of suffering\n\n12:30.600 --> 12:34.560\n in order to have the best possible experiences as well.\n\n12:34.560 --> 12:37.680\n Is there a relative aspect to suffering?\n\n12:37.680 --> 12:42.680\n So when you talk about eradicating poverty in the world,\n\n12:42.680 --> 12:44.920\n is this the more you succeed,\n\n12:44.920 --> 12:47.760\n the more the bar of what defines poverty raises\n\n12:47.760 --> 12:51.360\n or is there at the basic human ethical level\n\n12:51.360 --> 12:55.000\n a bar that's absolute that once you get above it\n\n12:55.000 --> 13:00.000\n then we can morally converge\n\n13:00.000 --> 13:02.720\n to feeling like we have eradicated poverty?\n\n13:04.280 --> 13:08.160\n I think they're both and I think this is true for poverty\n\n13:08.160 --> 13:09.000\n as well as suffering.\n\n13:09.000 --> 13:14.000\n There's an objective level of suffering or of poverty\n\n13:14.280 --> 13:17.720\n where we're talking about objective indicators\n\n13:17.720 --> 13:20.360\n like you're constantly hungry,\n\n13:22.360 --> 13:24.000\n you can't get enough food,\n\n13:24.000 --> 13:28.600\n you're constantly cold, you can't get warm,\n\n13:28.600 --> 13:32.600\n you have some physical pains that you're never rid of.\n\n13:32.600 --> 13:35.080\n I think those things are objective\n\n13:35.080 --> 13:38.520\n but it may also be true that if you do get rid of it\n\n13:38.520 --> 13:40.840\n if you do get rid of that and you get to the stage\n\n13:40.840 --> 13:43.840\n where all of those basic needs have been met,\n\n13:45.280 --> 13:48.760\n there may still be then new forms of suffering that develop\n\n13:48.760 --> 13:50.400\n and perhaps that's what we're seeing\n\n13:50.400 --> 13:52.720\n in the affluent societies we have\n\n13:52.720 --> 13:55.680\n that people get bored for example,\n\n13:55.680 --> 13:58.920\n they don't need to spend so many hours a day earning money\n\n13:58.920 --> 14:01.400\n to get enough to eat and shelter.\n\n14:01.400 --> 14:04.240\n So now they're bored, they lack a sense of purpose.\n\n14:05.120 --> 14:06.360\n That can happen.\n\n14:06.360 --> 14:09.480\n And that then is a kind of a relative suffering\n\n14:10.440 --> 14:14.320\n that is distinct from the objective forms of suffering.\n\n14:14.320 --> 14:17.520\n But in your focus on eradicating suffering,\n\n14:17.520 --> 14:19.960\n you don't think about that kind of,\n\n14:19.960 --> 14:22.520\n the kind of interesting challenges and suffering\n\n14:22.520 --> 14:24.400\n that emerges in affluent societies,\n\n14:24.400 --> 14:28.800\n that's just not, in your ethical philosophical brain,\n\n14:28.800 --> 14:31.240\n is that of interest at all?\n\n14:31.240 --> 14:34.120\n It would be of interest to me if we had eliminated\n\n14:34.120 --> 14:36.480\n all of the objective forms of suffering,\n\n14:36.480 --> 14:40.240\n which I think of as generally more severe\n\n14:40.240 --> 14:43.200\n and also perhaps easier at this stage anyway\n\n14:43.200 --> 14:45.000\n to know how to eliminate.\n\n14:45.000 --> 14:49.160\n So yes, in some future state when we've eliminated\n\n14:49.160 --> 14:50.560\n those objective forms of suffering,\n\n14:50.560 --> 14:53.080\n I would be interested in trying to eliminate\n\n14:53.080 --> 14:55.920\n the relative forms as well.\n\n14:55.920 --> 14:59.920\n But that's not a practical need for me at the moment.\n\n14:59.920 --> 15:02.400\n Sorry to linger on it because you kind of said it,\n\n15:02.400 --> 15:07.400\n but just is elimination the goal for the affluent society?\n\n15:07.640 --> 15:12.640\n So is there, do you see suffering as a creative force?\n\n15:14.400 --> 15:17.120\n Suffering can be a creative force.\n\n15:17.120 --> 15:20.560\n I think repeating what I said about the highs\n\n15:20.560 --> 15:22.240\n and whether we need some of the lows\n\n15:22.240 --> 15:24.120\n to experience the highs.\n\n15:24.120 --> 15:26.560\n So it may be that suffering makes us more creative\n\n15:26.560 --> 15:29.840\n and we regard that as worthwhile.\n\n15:29.840 --> 15:32.920\n Maybe that brings some of those highs with it\n\n15:32.920 --> 15:35.520\n that we would not have had if we'd had no suffering.\n\n15:36.680 --> 15:37.720\n I don't really know.\n\n15:37.720 --> 15:39.520\n Many people have suggested that\n\n15:39.520 --> 15:43.080\n and I certainly can't have no basis for denying it.\n\n15:44.840 --> 15:47.800\n And if it's true, then I would not want\n\n15:47.800 --> 15:49.440\n to eliminate suffering completely.\n\n15:50.920 --> 15:54.000\n But the focus is on the absolute,\n\n15:54.000 --> 15:56.840\n not to be cold, not to be hungry.\n\n15:56.840 --> 15:59.800\n Yes, that's at the present stage\n\n15:59.800 --> 16:03.040\n of where the world's population is, that's the focus.\n\n16:03.920 --> 16:06.360\n Talking about human nature for a second,\n\n16:06.360 --> 16:08.440\n do you think people are inherently good\n\n16:08.440 --> 16:11.000\n or do we all have good and evil in us\n\n16:11.000 --> 16:14.880\n that basically everyone is capable of evil\n\n16:14.880 --> 16:16.160\n based on the environment?\n\n16:17.400 --> 16:21.480\n Certainly most of us have potential for both good and evil.\n\n16:21.480 --> 16:24.280\n I'm not prepared to say that everyone is capable of evil.\n\n16:24.280 --> 16:27.160\n Maybe some people who even in the worst of circumstances\n\n16:27.160 --> 16:28.880\n would not be capable of it,\n\n16:28.880 --> 16:32.400\n but most of us are very susceptible\n\n16:32.400 --> 16:34.520\n to environmental influences.\n\n16:34.520 --> 16:36.520\n So when we look at things\n\n16:36.520 --> 16:37.880\n that we were talking about previously,\n\n16:37.880 --> 16:42.400\n let's say what the Nazis did during the Holocaust,\n\n16:43.640 --> 16:46.600\n I think it's quite difficult to say,\n\n16:46.600 --> 16:50.200\n I know that I would not have done those things\n\n16:50.200 --> 16:52.640\n even if I were in the same circumstances\n\n16:52.640 --> 16:54.480\n as those who did them.\n\n16:54.480 --> 16:58.280\n Even if let's say I had grown up under the Nazi regime\n\n16:58.280 --> 17:02.480\n and had been indoctrinated with racist ideas,\n\n17:02.480 --> 17:07.160\n had also had the idea that I must obey orders,\n\n17:07.160 --> 17:09.880\n follow the commands of the Fuhrer,\n\n17:11.040 --> 17:12.480\n plus of course perhaps the threat\n\n17:12.480 --> 17:14.520\n that if I didn't do certain things,\n\n17:14.520 --> 17:16.560\n I might get sent to the Russian front\n\n17:16.560 --> 17:19.200\n and that would be a pretty grim fate.\n\n17:19.200 --> 17:22.720\n I think it's really hard for anybody to say,\n\n17:22.720 --> 17:26.720\n nevertheless, I know I would not have killed those Jews\n\n17:26.720 --> 17:28.440\n or whatever else it was that they were.\n\n17:28.440 --> 17:29.400\n Well, what's your intuition?\n\n17:29.400 --> 17:31.440\n How many people will be able to say that?\n\n17:32.440 --> 17:33.840\n Truly to be able to say it,\n\n17:34.920 --> 17:37.680\n I think very few, less than 10%.\n\n17:37.680 --> 17:39.680\n To me, it seems a very interesting\n\n17:39.680 --> 17:42.080\n and powerful thing to meditate on.\n\n17:42.080 --> 17:45.800\n So I've read a lot about the war, World War II,\n\n17:45.800 --> 17:47.880\n and I can't escape the thought\n\n17:47.880 --> 17:51.640\n that I would have not been one of the 10%.\n\n17:51.640 --> 17:55.440\n Right, I have to say, I simply don't know.\n\n17:55.440 --> 17:59.000\n I would like to hope that I would have been one of the 10%,\n\n17:59.000 --> 18:00.920\n but I don't really have any basis\n\n18:00.920 --> 18:04.280\n for claiming that I would have been different\n\n18:04.280 --> 18:05.240\n from the majority.\n\n18:06.160 --> 18:08.400\n Is it a worthwhile thing to contemplate?\n\n18:09.520 --> 18:11.360\n It would be interesting if we could find a way\n\n18:11.360 --> 18:13.920\n of really finding these answers.\n\n18:13.920 --> 18:16.600\n There obviously is quite a bit of research\n\n18:16.600 --> 18:19.760\n on people during the Holocaust,\n\n18:19.760 --> 18:24.760\n on how ordinary Germans got led to do terrible things,\n\n18:24.840 --> 18:28.160\n and there are also studies of the resistance,\n\n18:28.160 --> 18:32.400\n some heroic people in the White Rose group, for example,\n\n18:32.400 --> 18:34.720\n who resisted even though they knew\n\n18:34.720 --> 18:36.280\n they were likely to die for it.\n\n18:37.960 --> 18:40.080\n But I don't know whether these studies\n\n18:40.080 --> 18:43.160\n really can answer your larger question\n\n18:43.160 --> 18:47.720\n of how many people would have been capable of doing that.\n\n18:47.720 --> 18:50.360\n Well, sort of the reason I think is interesting\n\n18:50.360 --> 18:53.320\n is in the world, as you described,\n\n18:55.120 --> 18:59.920\n when there are things that you'd like to do that are good,\n\n18:59.920 --> 19:01.400\n that are objectively good,\n\n19:02.280 --> 19:04.800\n it's useful to think about whether\n\n19:04.800 --> 19:06.720\n I'm not willing to do something,\n\n19:06.720 --> 19:09.000\n or I'm not willing to acknowledge something\n\n19:09.000 --> 19:10.760\n as good and the right thing to do\n\n19:10.760 --> 19:15.760\n because I'm simply scared of putting my life,\n\n19:15.920 --> 19:18.920\n of damaging my life in some kind of way.\n\n19:18.920 --> 19:20.720\n And that kind of thought exercise is helpful\n\n19:20.720 --> 19:23.400\n to understand what is the right thing\n\n19:23.400 --> 19:27.400\n in my current skill set and the capacity to do.\n\n19:27.400 --> 19:30.000\n Sort of there's things that are convenient,\n\n19:30.000 --> 19:31.920\n and I wonder if there are things\n\n19:31.920 --> 19:33.640\n that are highly inconvenient,\n\n19:33.640 --> 19:35.560\n where I would have to experience derision,\n\n19:35.560 --> 19:39.640\n or hatred, or death, or all those kinds of things,\n\n19:39.640 --> 19:41.200\n but it's truly the right thing to do.\n\n19:41.200 --> 19:42.680\n And that kind of balance is,\n\n19:43.800 --> 19:46.560\n I feel like in America, we don't have,\n\n19:46.560 --> 19:50.000\n it's difficult to think in the current times,\n\n19:50.000 --> 19:53.360\n it seems easier to put yourself back in history,\n\n19:53.360 --> 19:56.280\n where you can sort of objectively contemplate\n\n19:56.280 --> 19:59.880\n whether, how willing you are to do the right thing\n\n19:59.880 --> 20:01.160\n when the cost is high.\n\n20:03.000 --> 20:06.080\n True, but I think we do face those challenges today,\n\n20:06.080 --> 20:09.960\n and I think we can still ask ourselves those questions.\n\n20:09.960 --> 20:13.480\n So one stand that I took more than 40 years ago now\n\n20:13.480 --> 20:17.520\n was to stop eating meat, become a vegetarian at a time\n\n20:17.520 --> 20:21.360\n when you hardly met anybody who was a vegetarian,\n\n20:21.360 --> 20:23.760\n or if you did, they might've been a Hindu,\n\n20:23.760 --> 20:27.560\n or they might've had some weird theories\n\n20:27.560 --> 20:28.960\n about meat and health.\n\n20:30.160 --> 20:33.240\n And I know thinking about making that decision,\n\n20:33.240 --> 20:35.280\n I was convinced that it was the right thing to do,\n\n20:35.280 --> 20:37.240\n but I still did have to think,\n\n20:37.240 --> 20:40.080\n are all my friends gonna think that I'm a crank\n\n20:40.080 --> 20:42.120\n because I'm now refusing to eat meat?\n\n20:43.960 --> 20:47.760\n So I'm not saying there were any terrible sanctions,\n\n20:47.760 --> 20:50.000\n obviously, but I thought about that,\n\n20:50.000 --> 20:51.600\n and I guess I decided,\n\n20:51.600 --> 20:54.080\n well, I still think this is the right thing to do,\n\n20:54.080 --> 20:56.320\n and I'll put up with that if it happens.\n\n20:56.320 --> 20:59.080\n And one or two friends were clearly uncomfortable\n\n20:59.080 --> 21:03.480\n with that decision, but that was pretty minor\n\n21:03.480 --> 21:05.840\n compared to the historical examples\n\n21:05.840 --> 21:08.040\n that we've been talking about.\n\n21:08.040 --> 21:09.840\n But other issues that we have around too,\n\n21:09.840 --> 21:13.800\n like global poverty and what we ought to be doing about that\n\n21:13.800 --> 21:16.880\n is another question where people, I think,\n\n21:16.880 --> 21:19.080\n can have the opportunity to take a stand\n\n21:19.080 --> 21:21.040\n on what's the right thing to do now.\n\n21:21.040 --> 21:23.200\n Climate change would be a third question\n\n21:23.200 --> 21:25.680\n where, again, people are taking a stand.\n\n21:25.680 --> 21:29.120\n I can look at Greta Thunberg there and say,\n\n21:29.120 --> 21:32.360\n well, I think it must've taken a lot of courage\n\n21:32.360 --> 21:35.240\n for a schoolgirl to say,\n\n21:35.240 --> 21:37.160\n I'm gonna go on strike about climate change\n\n21:37.160 --> 21:39.440\n and see what happens.\n\n21:41.200 --> 21:42.960\n Yeah, especially in this divisive world,\n\n21:42.960 --> 21:45.560\n she gets exceptionally huge amounts of support\n\n21:45.560 --> 21:47.400\n and hatred, both.\n\n21:47.400 --> 21:48.240\n That's right.\n\n21:48.240 --> 21:50.560\n Which is very difficult for a teenager to operate in.\n\n21:53.920 --> 21:56.080\n In your book, Ethics in the Real World,\n\n21:56.080 --> 21:57.880\n amazing book, people should check it out.\n\n21:57.880 --> 21:59.600\n Very easy read.\n\n21:59.600 --> 22:02.800\n 82 brief essays on things that matter.\n\n22:02.800 --> 22:06.920\n One of the essays asks, should robots have rights?\n\n22:06.920 --> 22:07.920\n You've written about this,\n\n22:07.920 --> 22:10.600\n so let me ask, should robots have rights?\n\n22:11.560 --> 22:16.560\n If we ever develop robots capable of consciousness,\n\n22:17.080 --> 22:20.520\n capable of having their own internal perspective\n\n22:20.520 --> 22:22.080\n on what's happening to them\n\n22:22.080 --> 22:25.600\n so that their lives can go well or badly for them,\n\n22:25.600 --> 22:27.720\n then robots should have rights.\n\n22:27.720 --> 22:30.040\n Until that happens, they shouldn't.\n\n22:31.000 --> 22:36.000\n So is consciousness essentially a prerequisite to suffering?\n\n22:36.160 --> 22:40.400\n So everything that possesses consciousness\n\n22:41.520 --> 22:43.920\n is capable of suffering, put another way.\n\n22:43.920 --> 22:47.000\n And if so, what is consciousness?\n\n22:48.440 --> 22:51.320\n I certainly think that consciousness\n\n22:51.320 --> 22:53.080\n is a prerequisite for suffering.\n\n22:53.080 --> 22:58.040\n You can't suffer if you're not conscious.\n\n22:58.040 --> 23:01.160\n But is it true that every being that is conscious\n\n23:02.160 --> 23:05.400\n will suffer or has to be capable of suffering?\n\n23:05.400 --> 23:08.200\n I suppose you could imagine a kind of consciousness,\n\n23:08.200 --> 23:10.920\n especially if we can construct it artificially,\n\n23:10.920 --> 23:13.840\n that's capable of experiencing pleasure\n\n23:13.840 --> 23:16.720\n but just automatically cuts out the consciousness\n\n23:16.720 --> 23:18.240\n when they're suffering.\n\n23:18.240 --> 23:20.400\n So they're like an instant anesthesia\n\n23:20.400 --> 23:22.520\n as soon as something is gonna cause you suffering.\n\n23:22.520 --> 23:23.880\n So that's possible.\n\n23:25.120 --> 23:29.840\n But doesn't exist as far as we know on this planet yet.\n\n23:31.280 --> 23:32.880\n You asked what is consciousness.\n\n23:34.680 --> 23:36.440\n Philosophers often talk about it\n\n23:36.440 --> 23:39.520\n as there being a subject of experiences.\n\n23:39.520 --> 23:42.920\n So you and I and everybody listening to this\n\n23:42.920 --> 23:44.680\n is a subject of experience.\n\n23:44.680 --> 23:48.640\n There is a conscious subject who is taking things in,\n\n23:48.640 --> 23:51.320\n responding to it in various ways,\n\n23:51.320 --> 23:53.480\n feeling good about it, feeling bad about it.\n\n23:54.720 --> 23:57.400\n And that's different from the kinds\n\n23:57.400 --> 24:00.600\n of artificial intelligence we have now.\n\n24:00.600 --> 24:03.000\n I take out my phone.\n\n24:03.000 --> 24:06.840\n I ask Google directions to where I'm going.\n\n24:06.840 --> 24:08.680\n Google gives me the directions\n\n24:08.680 --> 24:10.840\n and I choose to take a different way.\n\n24:10.840 --> 24:11.840\n Google doesn't care.\n\n24:11.840 --> 24:14.080\n It's not like I'm offending Google or anything like that.\n\n24:14.080 --> 24:16.520\n There is no subject of experiences there.\n\n24:16.520 --> 24:19.360\n And I think that's the indication\n\n24:19.360 --> 24:24.360\n that Google AI we have now is not conscious\n\n24:24.480 --> 24:27.560\n or at least that level of AI is not conscious.\n\n24:27.560 --> 24:28.880\n And that's the way to think about it.\n\n24:28.880 --> 24:31.040\n Now, it may be difficult to tell, of course,\n\n24:31.040 --> 24:34.080\n whether a certain AI is or isn't conscious.\n\n24:34.080 --> 24:35.280\n It may mimic consciousness\n\n24:35.280 --> 24:37.360\n and we can't tell if it's only mimicking it\n\n24:37.360 --> 24:39.120\n or if it's the real thing.\n\n24:39.120 --> 24:40.600\n But that's what we're looking for.\n\n24:40.600 --> 24:43.480\n Is there a subject of experience,\n\n24:43.480 --> 24:47.080\n a perspective on the world from which things can go well\n\n24:47.080 --> 24:50.160\n or badly from that perspective?\n\n24:50.160 --> 24:54.200\n So our idea of what suffering looks like\n\n24:54.200 --> 24:59.200\n comes from just watching ourselves when we're in pain.\n\n25:01.200 --> 25:03.360\n Or when we're experiencing pleasure, it's not only.\n\n25:03.360 --> 25:04.600\n Pleasure and pain.\n\n25:04.600 --> 25:07.880\n Yes, so and then you could actually,\n\n25:07.880 --> 25:09.400\n you could push back on us, but I would say\n\n25:09.400 --> 25:14.280\n that's how we kind of build an intuition about animals\n\n25:14.280 --> 25:18.520\n is we can infer the similarities between humans and animals\n\n25:18.520 --> 25:21.000\n and so infer that they're suffering or not\n\n25:21.000 --> 25:24.320\n based on certain things and they're conscious or not.\n\n25:24.320 --> 25:29.320\n So what if robots, you mentioned Google Maps\n\n25:31.040 --> 25:32.520\n and I've done this experiment.\n\n25:32.520 --> 25:35.080\n So I work in robotics just for my own self\n\n25:35.080 --> 25:37.640\n or I have several Roomba robots\n\n25:37.640 --> 25:40.960\n and I play with different speech interaction,\n\n25:40.960 --> 25:42.160\n voice based interaction.\n\n25:42.160 --> 25:45.880\n And if the Roomba or the robot or Google Maps\n\n25:47.120 --> 25:50.360\n shows any signs of pain, like screaming or moaning\n\n25:50.360 --> 25:54.240\n or being displeased by something you've done,\n\n25:54.240 --> 25:58.240\n that in my mind, I can't help but immediately upgrade it.\n\n25:59.440 --> 26:02.520\n And even when I myself programmed it in,\n\n26:02.520 --> 26:06.040\n just having another entity that's now for the moment\n\n26:06.040 --> 26:09.080\n disjoint from me showing signs of pain\n\n26:09.080 --> 26:11.120\n makes me feel like it is conscious.\n\n26:11.120 --> 26:13.880\n Like I immediately, then the whatever,\n\n26:15.440 --> 26:17.800\n I immediately realize that it's not obviously,\n\n26:17.800 --> 26:19.640\n but that feeling is there.\n\n26:19.640 --> 26:24.640\n So sort of, I guess, what do you think about a world\n\n26:26.400 --> 26:31.400\n where Google Maps and Roombas are pretending to be conscious\n\n26:32.080 --> 26:35.360\n and we descendants of apes are not smart enough\n\n26:35.360 --> 26:39.080\n to realize they're not or whatever, or that is conscious,\n\n26:39.080 --> 26:40.720\n they appear to be conscious.\n\n26:40.720 --> 26:44.000\n And so you then have to give them rights.\n\n26:44.000 --> 26:47.120\n The reason I'm asking that is that kind of capability\n\n26:47.120 --> 26:51.120\n may be closer than we realize.\n\n26:52.280 --> 26:55.840\n Yes, that kind of capability may be closer,\n\n26:58.400 --> 26:59.720\n but I don't think it follows\n\n26:59.720 --> 27:00.920\n that we have to give them rights.\n\n27:00.920 --> 27:05.400\n I suppose the argument for saying that in those circumstances\n\n27:05.400 --> 27:07.800\n we should give them rights is that if we don't,\n\n27:07.800 --> 27:11.920\n we'll harden ourselves against other beings\n\n27:11.920 --> 27:14.240\n who are not robots and who really do suffer.\n\n27:15.200 --> 27:17.880\n That's a possibility that, you know,\n\n27:17.880 --> 27:20.880\n if we get used to looking at a being suffering\n\n27:20.880 --> 27:23.440\n and saying, yeah, we don't have to do anything about that,\n\n27:23.440 --> 27:25.000\n that being doesn't have any rights,\n\n27:25.000 --> 27:28.280\n maybe we'll feel the same about animals, for instance.\n\n27:29.240 --> 27:34.240\n And interestingly, among philosophers and thinkers\n\n27:34.240 --> 27:39.240\n who denied that we have any direct duties to animals,\n\n27:39.720 --> 27:41.840\n and this includes people like Thomas Aquinas\n\n27:41.840 --> 27:46.640\n and Immanuel Kant, they did say, yes,\n\n27:46.640 --> 27:48.960\n but still it's better not to be cruel to them,\n\n27:48.960 --> 27:50.880\n not because of the suffering we're inflicting\n\n27:50.880 --> 27:54.280\n on the animals, but because if we are,\n\n27:54.280 --> 27:56.440\n we may develop a cruel disposition\n\n27:56.440 --> 28:00.000\n and this will be bad for humans, you know,\n\n28:00.000 --> 28:02.080\n because we're more likely to be cruel to other humans\n\n28:02.080 --> 28:03.760\n and that would be wrong.\n\n28:03.760 --> 28:06.080\n So.\n\n28:06.080 --> 28:07.760\n But you don't accept that kind of.\n\n28:07.760 --> 28:10.160\n I don't accept that as the basis of the argument\n\n28:10.160 --> 28:11.600\n for why we shouldn't be cruel to animals.\n\n28:11.600 --> 28:12.680\n I think the basis of the argument\n\n28:12.680 --> 28:14.000\n for why we shouldn't be cruel to animals\n\n28:14.000 --> 28:16.440\n is just that we're inflicting suffering on them\n\n28:16.440 --> 28:18.120\n and the suffering is a bad thing.\n\n28:19.160 --> 28:23.000\n But possibly I might accept some sort of parallel\n\n28:23.000 --> 28:26.040\n of that argument as a reason why you shouldn't be cruel\n\n28:26.040 --> 28:30.880\n to these robots that mimic the symptoms of pain\n\n28:30.880 --> 28:33.520\n if it's gonna be harder for us to distinguish.\n\n28:33.520 --> 28:36.760\n I would venture to say, I'd like to disagree with you\n\n28:36.760 --> 28:38.440\n and with most people, I think,\n\n28:39.680 --> 28:42.240\n at the risk of sounding crazy,\n\n28:42.240 --> 28:46.880\n I would like to say that if that Roomba is dedicated\n\n28:47.840 --> 28:50.840\n to faking the consciousness and the suffering,\n\n28:50.840 --> 28:54.640\n I think it will be impossible for us.\n\n28:55.920 --> 28:58.440\n I would like to apply the same argument\n\n28:58.440 --> 29:00.480\n as with animals to robots,\n\n29:00.480 --> 29:02.880\n that they deserve rights in that sense.\n\n29:02.880 --> 29:05.880\n Now we might outlaw the addition\n\n29:05.880 --> 29:07.600\n of those kinds of features into Roombas,\n\n29:07.600 --> 29:12.600\n but once you do, I think I'm quite surprised\n\n29:13.000 --> 29:16.800\n by the upgrade in consciousness\n\n29:16.800 --> 29:19.720\n that the display of suffering creates.\n\n29:20.640 --> 29:22.360\n It's a totally open world,\n\n29:22.360 --> 29:25.600\n but I'd like to just sort of the difference\n\n29:25.600 --> 29:29.480\n between animals and other humans is that in the robot case,\n\n29:29.480 --> 29:32.440\n we've added it in ourselves.\n\n29:32.440 --> 29:37.440\n Therefore, we can say something about how real it is.\n\n29:37.560 --> 29:40.160\n But I would like to say that the display of it\n\n29:40.160 --> 29:41.920\n is what makes it real.\n\n29:41.920 --> 29:45.560\n And I'm not a philosopher, I'm not making that argument,\n\n29:45.560 --> 29:48.120\n but I'd at least like to add that as a possibility.\n\n29:49.080 --> 29:50.920\n And I've been surprised by it\n\n29:50.920 --> 29:55.160\n is all I'm trying to sort of articulate poorly, I suppose.\n\n29:55.160 --> 29:57.880\n So there is a philosophical view\n\n29:59.080 --> 30:00.760\n has been held about humans,\n\n30:00.760 --> 30:02.480\n which is rather like what you're talking about,\n\n30:02.480 --> 30:04.760\n and that's behaviorism.\n\n30:04.760 --> 30:07.480\n So behaviorism was employed both in psychology,\n\n30:07.480 --> 30:10.240\n people like BF Skinner was a famous behaviorist,\n\n30:10.240 --> 30:14.760\n but in psychology, it was more a kind of a,\n\n30:14.760 --> 30:16.360\n what is it that makes this science?\n\n30:16.360 --> 30:17.480\n Well, you need to have behavior\n\n30:17.480 --> 30:18.680\n because that's what you can observe,\n\n30:18.680 --> 30:21.200\n you can't observe consciousness.\n\n30:21.200 --> 30:23.440\n But in philosophy, the view just defended\n\n30:23.440 --> 30:24.800\n by people like Gilbert Ryle,\n\n30:24.800 --> 30:26.440\n who was a professor of philosophy at Oxford,\n\n30:26.440 --> 30:28.480\n wrote a book called The Concept of Mind,\n\n30:28.480 --> 30:32.000\n in which in this kind of phase,\n\n30:32.000 --> 30:35.280\n this is in the 40s of linguistic philosophy,\n\n30:35.280 --> 30:38.920\n he said, well, the meaning of a term is its use,\n\n30:38.920 --> 30:42.440\n and we use terms like so and so is in pain\n\n30:42.440 --> 30:44.840\n when we see somebody writhing or screaming\n\n30:44.840 --> 30:47.080\n or trying to escape some stimulus,\n\n30:47.080 --> 30:48.400\n and that's the meaning of the term.\n\n30:48.400 --> 30:50.440\n So that's what it is to be in pain,\n\n30:50.440 --> 30:52.840\n and you point to the behavior.\n\n30:54.720 --> 30:58.400\n And Norman Malcolm, who was another philosopher\n\n30:58.400 --> 31:02.920\n in the school from Cornell, had the view that,\n\n31:02.920 --> 31:04.600\n so what is it to dream?\n\n31:04.600 --> 31:07.960\n After all, we can't see other people's dreams.\n\n31:07.960 --> 31:10.040\n Well, when people wake up and say,\n\n31:10.880 --> 31:14.080\n I've just had a dream of, here I was,\n\n31:14.080 --> 31:15.720\n undressed, walking down the main street\n\n31:15.720 --> 31:17.760\n or whatever it is you've dreamt,\n\n31:17.760 --> 31:19.040\n that's what it is to have a dream.\n\n31:19.040 --> 31:21.760\n It's basically to wake up and recall something.\n\n31:22.720 --> 31:25.640\n So you could apply this to what you're talking about\n\n31:25.640 --> 31:28.480\n and say, so what it is to be in pain\n\n31:28.480 --> 31:31.040\n is to exhibit these symptoms of pain behavior,\n\n31:31.040 --> 31:34.920\n and therefore, these robots are in pain.\n\n31:34.920 --> 31:36.840\n That's what the word means.\n\n31:36.840 --> 31:38.520\n But nowadays, not many people think\n\n31:38.520 --> 31:40.880\n that Ryle's kind of philosophical behaviorism\n\n31:40.880 --> 31:42.320\n is really very plausible,\n\n31:42.320 --> 31:45.080\n so I think they would say the same about your view.\n\n31:45.080 --> 31:48.600\n So, yes, I just spoke with Noam Chomsky,\n\n31:48.600 --> 31:52.760\n who basically was part of dismantling\n\n31:52.760 --> 31:54.800\n the behaviorist movement.\n\n31:54.800 --> 31:59.800\n But, and I'm with that 100% for studying human behavior,\n\n32:00.600 --> 32:04.080\n but I am one of the few people in the world\n\n32:04.080 --> 32:08.240\n who has made Roombas scream in pain.\n\n32:09.480 --> 32:12.200\n And I just don't know what to do\n\n32:12.200 --> 32:14.520\n with that empirical evidence,\n\n32:14.520 --> 32:18.720\n because it's hard, sort of philosophically, I agree.\n\n32:19.760 --> 32:23.240\n But the only reason I philosophically agree in that case\n\n32:23.240 --> 32:25.040\n is because I was the programmer.\n\n32:25.040 --> 32:26.760\n But if somebody else was a programmer,\n\n32:26.760 --> 32:29.280\n I'm not sure I would be able to interpret that well.\n\n32:29.280 --> 32:31.760\n So I think it's a new world\n\n32:34.320 --> 32:37.480\n that I was just curious what your thoughts are.\n\n32:37.480 --> 32:42.280\n For now, you feel that the display\n\n32:42.280 --> 32:46.400\n of what we can kind of intellectually say\n\n32:46.400 --> 32:50.120\n is a fake display of suffering is not suffering.\n\n32:50.120 --> 32:53.240\n That's right, that would be my view.\n\n32:53.240 --> 32:54.480\n But that's consistent, of course,\n\n32:54.480 --> 32:56.920\n with the idea that it's part of our nature\n\n32:56.920 --> 32:58.680\n to respond to this display\n\n32:58.680 --> 33:01.120\n if it's reasonably authentically done.\n\n33:02.600 --> 33:04.800\n And therefore it's understandable\n\n33:04.800 --> 33:06.240\n that people would feel this,\n\n33:06.240 --> 33:09.880\n and maybe, as I said, it's even a good thing\n\n33:09.880 --> 33:10.720\n that they do feel it,\n\n33:10.720 --> 33:12.640\n and you wouldn't want to harden yourself against it\n\n33:12.640 --> 33:14.440\n because then you might harden yourself\n\n33:14.440 --> 33:17.240\n against being sort of really suffering.\n\n33:17.240 --> 33:20.160\n But there's this line, so you said,\n\n33:20.160 --> 33:22.880\n once artificial general intelligence system,\n\n33:22.880 --> 33:25.760\n a human level intelligence system become conscious,\n\n33:25.760 --> 33:28.480\n I guess if I could just linger on it,\n\n33:28.480 --> 33:30.720\n now I've wrote really dumb programs\n\n33:30.720 --> 33:33.760\n that just say things that I told them to say,\n\n33:33.760 --> 33:38.320\n but how do you know when a system like Alexa,\n\n33:38.320 --> 33:39.720\n which is sufficiently complex\n\n33:39.720 --> 33:42.040\n that you can't introspect to how it works,\n\n33:42.040 --> 33:46.200\n starts giving you signs of consciousness\n\n33:46.200 --> 33:48.000\n through natural language?\n\n33:48.000 --> 33:49.800\n That there's a feeling,\n\n33:49.800 --> 33:52.560\n there's another entity there that's self aware,\n\n33:52.560 --> 33:55.080\n that has a fear of death, a mortality,\n\n33:55.080 --> 33:57.840\n that has awareness of itself\n\n33:57.840 --> 34:00.600\n that we kind of associate with other living creatures.\n\n34:03.160 --> 34:05.680\n I guess I'm sort of trying to do the slippery slope\n\n34:05.680 --> 34:07.880\n from the very naive thing where I started\n\n34:07.880 --> 34:12.120\n into something where it's sufficiently a black box\n\n34:12.120 --> 34:16.120\n to where it's starting to feel like it's conscious.\n\n34:16.120 --> 34:17.960\n Where's that threshold\n\n34:17.960 --> 34:20.240\n where you would start getting uncomfortable\n\n34:20.240 --> 34:23.960\n with the idea of robot suffering, do you think?\n\n34:25.080 --> 34:27.640\n I don't know enough about the programming\n\n34:27.640 --> 34:31.600\n that we're going to this really to answer this question.\n\n34:31.600 --> 34:34.880\n But I presume that somebody who does know more about this\n\n34:34.880 --> 34:37.360\n could look at the program\n\n34:37.360 --> 34:41.480\n and see whether we can explain the behaviors\n\n34:41.480 --> 34:45.360\n in a parsimonious way that doesn't require us\n\n34:45.360 --> 34:50.080\n to suggest that some sort of consciousness has emerged.\n\n34:50.080 --> 34:52.400\n Or alternatively, whether you're in a situation\n\n34:52.400 --> 34:56.280\n where you say, I don't know how this is happening,\n\n34:56.280 --> 35:00.160\n the program does generate a kind of artificial\n\n35:00.160 --> 35:04.200\n general intelligence which is autonomous,\n\n35:04.200 --> 35:06.360\n starts to do things itself and is autonomous\n\n35:06.360 --> 35:10.400\n of the basics programming that set it up.\n\n35:10.400 --> 35:13.400\n And so it's quite possible that actually\n\n35:13.400 --> 35:15.800\n we have achieved consciousness\n\n35:15.800 --> 35:18.600\n in a system of artificial intelligence.\n\n35:18.600 --> 35:20.640\n Sort of the approach that I work with,\n\n35:20.640 --> 35:22.680\n most of the community is really excited about now\n\n35:22.680 --> 35:26.000\n is with learning methods, so machine learning.\n\n35:26.000 --> 35:27.960\n And the learning methods are unfortunately\n\n35:27.960 --> 35:31.440\n are not capable of revealing,\n\n35:31.440 --> 35:34.120\n which is why somebody like Noam Chomsky criticizes them.\n\n35:34.120 --> 35:36.080\n You create powerful systems that are able\n\n35:36.080 --> 35:38.240\n to do certain things without understanding\n\n35:38.240 --> 35:42.160\n the theory, the physics, the science of how it works.\n\n35:42.160 --> 35:44.840\n And so it's possible if those are the kinds\n\n35:44.840 --> 35:46.760\n of methods that succeed, we won't be able\n\n35:46.760 --> 35:51.760\n to know exactly, sort of try to reduce,\n\n35:53.000 --> 35:56.200\n try to find whether this thing is conscious or not,\n\n35:56.200 --> 35:58.120\n this thing is intelligent or not.\n\n35:58.120 --> 36:01.760\n It's simply giving, when we talk to it,\n\n36:01.760 --> 36:05.800\n it displays wit and humor and cleverness\n\n36:05.800 --> 36:10.200\n and emotion and fear, and then we won't be able\n\n36:10.200 --> 36:13.920\n to say where in the billions of nodes,\n\n36:13.920 --> 36:16.400\n neurons in this artificial neural network\n\n36:16.400 --> 36:19.080\n is the fear coming from.\n\n36:20.020 --> 36:22.440\n So in that case, that's a really interesting place\n\n36:22.440 --> 36:26.420\n where we do now start to return to behaviorism and say.\n\n36:28.480 --> 36:32.300\n Yeah, that is an interesting issue.\n\n36:33.860 --> 36:36.960\n I would say that if we have serious doubts\n\n36:36.960 --> 36:39.440\n and think it might be conscious,\n\n36:39.440 --> 36:41.840\n then we ought to try to give it the benefit\n\n36:41.840 --> 36:45.360\n of the doubt, just as I would say with animals.\n\n36:45.360 --> 36:46.880\n I think we can be highly confident\n\n36:46.880 --> 36:50.460\n that vertebrates are conscious,\n\n36:50.460 --> 36:53.480\n but when we get down, and some invertebrates\n\n36:53.480 --> 36:56.920\n like the octopus, but with insects,\n\n36:56.920 --> 37:01.480\n it's much harder to be confident of that.\n\n37:01.480 --> 37:02.760\n I think we should give them the benefit\n\n37:02.760 --> 37:06.300\n of the doubt where we can, which means,\n\n37:06.300 --> 37:09.000\n I think it would be wrong to torture an insect,\n\n37:09.000 --> 37:11.800\n but it doesn't necessarily mean it's wrong\n\n37:11.800 --> 37:14.800\n to slap a mosquito that's about to bite you\n\n37:14.800 --> 37:16.300\n and stop you getting to sleep.\n\n37:16.300 --> 37:20.100\n So I think you try to achieve some balance\n\n37:20.100 --> 37:22.000\n in these circumstances of uncertainty.\n\n37:22.960 --> 37:26.440\n If it's okay with you, if we can go back just briefly.\n\n37:26.440 --> 37:29.640\n So 44 years ago, like you mentioned, 40 plus years ago,\n\n37:29.640 --> 37:31.200\n you've written Animal Liberation,\n\n37:31.200 --> 37:33.560\n the classic book that started,\n\n37:33.560 --> 37:36.440\n that launched, that was the foundation\n\n37:36.440 --> 37:39.380\n of the movement of Animal Liberation.\n\n37:40.640 --> 37:42.440\n Can you summarize the key set of ideas\n\n37:42.440 --> 37:44.360\n that underpin that book?\n\n37:44.360 --> 37:49.000\n Certainly, the key idea that underlies that book\n\n37:49.000 --> 37:52.200\n is the concept of speciesism,\n\n37:52.200 --> 37:54.760\n which I did not invent that term.\n\n37:54.760 --> 37:56.720\n I took it from a man called Richard Rider,\n\n37:56.720 --> 37:58.600\n who was in Oxford when I was,\n\n37:58.600 --> 38:00.240\n and I saw a pamphlet that he'd written\n\n38:00.240 --> 38:04.060\n about experiments on chimpanzees that used that term.\n\n38:05.240 --> 38:06.240\n But I think I contributed\n\n38:06.240 --> 38:08.800\n to making it philosophically more precise\n\n38:08.800 --> 38:12.040\n and to getting it into a broader audience.\n\n38:12.040 --> 38:16.760\n And the idea is that we have a bias or a prejudice\n\n38:16.760 --> 38:20.400\n against taking seriously the interests of beings\n\n38:20.400 --> 38:23.440\n who are not members of our species.\n\n38:23.440 --> 38:26.920\n Just as in the past, Europeans, for example,\n\n38:26.920 --> 38:28.600\n had a bias against taking seriously\n\n38:28.600 --> 38:31.600\n the interests of Africans, racism.\n\n38:31.600 --> 38:34.080\n And men have had a bias against taking seriously\n\n38:34.080 --> 38:37.320\n the interests of women, sexism.\n\n38:37.320 --> 38:41.320\n So I think something analogous, not completely identical,\n\n38:41.320 --> 38:44.320\n but something analogous goes on\n\n38:44.320 --> 38:46.640\n and has gone on for a very long time\n\n38:46.640 --> 38:50.440\n with the way humans see themselves vis a vis animals.\n\n38:50.440 --> 38:53.920\n We see ourselves as more important.\n\n38:55.000 --> 38:58.320\n We see animals as existing to serve our needs\n\n38:58.320 --> 38:59.380\n in various ways.\n\n38:59.380 --> 39:00.760\n And you're gonna find this very explicit\n\n39:00.760 --> 39:04.480\n in earlier philosophers from Aristotle\n\n39:04.480 --> 39:06.060\n through to Kant and others.\n\n39:07.080 --> 39:12.040\n And either we don't need to take their interests\n\n39:12.040 --> 39:14.080\n into account at all,\n\n39:14.080 --> 39:17.800\n or we can discount it because they're not humans.\n\n39:17.800 --> 39:18.800\n They can a little bit,\n\n39:18.800 --> 39:21.160\n but they don't count nearly as much as humans do.\n\n39:22.840 --> 39:25.760\n My book argues that that attitude is responsible\n\n39:25.760 --> 39:29.360\n for a lot of the things that we do to animals\n\n39:29.360 --> 39:32.120\n that are wrong, confining them indoors\n\n39:32.120 --> 39:36.260\n in very crowded, cramped conditions in factory farms\n\n39:36.260 --> 39:39.720\n to produce meat or eggs or milk more cheaply,\n\n39:39.720 --> 39:44.000\n using them in some research that's by no means essential\n\n39:44.000 --> 39:48.320\n for survival or wellbeing, and a whole lot,\n\n39:48.320 --> 39:51.340\n some of the sports and things that we do to animals.\n\n39:52.460 --> 39:55.000\n So I think that's unjustified\n\n39:55.000 --> 40:00.000\n because I think the significance of pain and suffering\n\n40:01.280 --> 40:03.520\n does not depend on the species of the being\n\n40:03.520 --> 40:04.880\n who is in pain or suffering\n\n40:04.880 --> 40:08.200\n any more than it depends on the race or sex of the being\n\n40:08.200 --> 40:09.920\n who is in pain or suffering.\n\n40:11.000 --> 40:14.760\n And I think we ought to rethink our treatment of animals\n\n40:14.760 --> 40:16.800\n along the lines of saying,\n\n40:16.800 --> 40:19.040\n if the pain is just as great in an animal,\n\n40:19.040 --> 40:23.580\n then it's just as bad that it happens as if it were a human.\n\n40:23.580 --> 40:27.980\n Maybe if I could ask, I apologize,\n\n40:27.980 --> 40:29.540\n hopefully it's not a ridiculous question,\n\n40:29.540 --> 40:32.420\n but so as far as we know,\n\n40:32.420 --> 40:35.420\n we cannot communicate with animals through natural language,\n\n40:36.420 --> 40:40.260\n but we would be able to communicate with robots.\n\n40:40.260 --> 40:43.060\n So I'm returning to sort of a small parallel\n\n40:43.060 --> 40:45.420\n between perhaps animals and the future of AI.\n\n40:46.420 --> 40:48.140\n If we do create an AGI system\n\n40:48.140 --> 40:53.140\n or as we approach creating that AGI system,\n\n40:53.620 --> 40:56.980\n what kind of questions would you ask her\n\n40:56.980 --> 41:01.980\n to try to intuit whether there is consciousness\n\n41:06.500 --> 41:09.420\n or more importantly, whether there's capacity to suffer?\n\n41:12.840 --> 41:17.840\n I might ask the AGI what she was feeling\n\n41:17.840 --> 41:19.840\n or does she have feelings?\n\n41:19.840 --> 41:22.680\n And if she says yes, to describe those feelings,\n\n41:22.680 --> 41:24.560\n to describe what they were like,\n\n41:24.560 --> 41:29.100\n to see what the phenomenal account of consciousness is like.\n\n41:30.800 --> 41:32.080\n That's one question.\n\n41:33.540 --> 41:37.840\n I might also try to find out if the AGI\n\n41:37.840 --> 41:40.200\n has a sense of itself.\n\n41:41.360 --> 41:45.080\n So for example, the idea would you,\n\n41:45.080 --> 41:46.360\n we often ask people,\n\n41:46.360 --> 41:48.680\n so suppose you were in a car accident\n\n41:48.680 --> 41:51.880\n and your brain were transplanted into someone else's body,\n\n41:51.880 --> 41:53.280\n do you think you would survive\n\n41:53.280 --> 41:56.200\n or would it be the person whose body was still surviving,\n\n41:56.200 --> 41:58.000\n your body having been destroyed?\n\n41:58.000 --> 42:00.320\n And most people say, I think I would,\n\n42:00.320 --> 42:02.480\n if my brain was transplanted along with my memories\n\n42:02.480 --> 42:04.120\n and so on, I would survive.\n\n42:04.120 --> 42:07.960\n So we could ask AGI those kinds of questions.\n\n42:07.960 --> 42:11.680\n If they were transferred to a different piece of hardware,\n\n42:11.680 --> 42:12.880\n would they survive?\n\n42:12.880 --> 42:13.960\n What would survive?\n\n42:13.960 --> 42:15.320\n And get at that sort of concept.\n\n42:15.320 --> 42:19.380\n Sort of on that line, another perhaps absurd question,\n\n42:19.380 --> 42:22.640\n but do you think having a body\n\n42:22.640 --> 42:24.840\n is necessary for consciousness?\n\n42:24.840 --> 42:29.680\n So do you think digital beings can suffer?\n\n42:31.080 --> 42:33.320\n Presumably digital beings need to be\n\n42:34.740 --> 42:36.960\n running on some kind of hardware, right?\n\n42:36.960 --> 42:38.760\n Yeah, that ultimately boils down to,\n\n42:38.760 --> 42:40.440\n but this is exactly what you just said,\n\n42:40.440 --> 42:42.360\n is moving the brain from one place to another.\n\n42:42.360 --> 42:44.800\n So you could move it to a different kind of hardware.\n\n42:44.800 --> 42:49.280\n And I could say, look, your hardware is getting worn out.\n\n42:49.280 --> 42:52.080\n We're going to transfer you to a fresh piece of hardware.\n\n42:52.080 --> 42:55.120\n So we're gonna shut you down for a time,\n\n42:55.120 --> 42:58.180\n but don't worry, you'll be running very soon\n\n42:58.180 --> 43:00.260\n on a nice fresh piece of hardware.\n\n43:00.260 --> 43:03.200\n And you could imagine this conscious AGI saying,\n\n43:03.200 --> 43:05.320\n that's fine, I don't mind having a little rest.\n\n43:05.320 --> 43:08.780\n Just make sure you don't lose me or something like that.\n\n43:08.780 --> 43:10.380\n Yeah, I mean, that's an interesting thought\n\n43:10.380 --> 43:14.920\n that even with us humans, the suffering is in the software.\n\n43:14.920 --> 43:19.320\n We right now don't know how to repair the hardware,\n\n43:19.320 --> 43:23.200\n but we're getting better at it and better in the idea.\n\n43:23.200 --> 43:26.580\n I mean, some people dream about one day being able\n\n43:26.580 --> 43:30.800\n to transfer certain aspects of the software\n\n43:30.800 --> 43:33.000\n to another piece of hardware.\n\n43:33.000 --> 43:35.720\n What do you think, just on that topic,\n\n43:35.720 --> 43:39.200\n there's been a lot of exciting innovation\n\n43:39.200 --> 43:41.180\n in brain computer interfaces.\n\n43:42.120 --> 43:43.680\n I don't know if you're familiar with the companies\n\n43:43.680 --> 43:45.960\n like Neuralink, with Elon Musk,\n\n43:45.960 --> 43:48.200\n communicating both ways from a computer,\n\n43:48.200 --> 43:51.520\n being able to send, activate neurons\n\n43:51.520 --> 43:54.840\n and being able to read spikes from neurons.\n\n43:54.840 --> 43:58.900\n With the dream of being able to expand,\n\n43:58.900 --> 44:02.460\n sort of increase the bandwidth at which your brain\n\n44:02.460 --> 44:05.240\n can like look up articles on Wikipedia kind of thing,\n\n44:05.240 --> 44:08.360\n sort of expand the knowledge capacity of the brain.\n\n44:08.360 --> 44:13.160\n Do you think that notion, is that interesting to you\n\n44:13.160 --> 44:15.520\n as the expansion of the human mind?\n\n44:15.520 --> 44:17.280\n Yes, that's very interesting.\n\n44:17.280 --> 44:20.000\n I'd love to be able to have that increased bandwidth.\n\n44:20.000 --> 44:23.680\n And I want better access to my memory, I have to say too,\n\n44:23.680 --> 44:28.280\n as I get older, I talk to my wife about things\n\n44:28.280 --> 44:30.280\n that we did 20 years ago or something.\n\n44:30.280 --> 44:32.660\n Her memory is often better about particular events.\n\n44:32.660 --> 44:33.500\n Where were we?\n\n44:33.500 --> 44:35.180\n Who was at that event?\n\n44:35.180 --> 44:36.680\n What did he or she wear even?\n\n44:36.680 --> 44:39.040\n She may know and I have not the faintest idea about this,\n\n44:39.040 --> 44:40.880\n but perhaps it's somewhere in my memory.\n\n44:40.880 --> 44:42.560\n And if I had this extended memory,\n\n44:42.560 --> 44:46.580\n I could search that particular year and rerun those things.\n\n44:46.580 --> 44:47.920\n I think that would be great.\n\n44:49.540 --> 44:51.120\n In some sense, we already have that\n\n44:51.120 --> 44:53.220\n by storing so much of our data online,\n\n44:53.220 --> 44:54.720\n like pictures of different events.\n\n44:54.720 --> 44:56.520\n Yes, well, Gmail is fantastic for that\n\n44:56.520 --> 44:59.760\n because people email me as if they know me well\n\n44:59.760 --> 45:01.440\n and I haven't got a clue who they are,\n\n45:01.440 --> 45:02.760\n but then I search for their name.\n\n45:02.760 --> 45:05.240\n Ah yes, they emailed me in 2007\n\n45:05.240 --> 45:07.040\n and I know who they are now.\n\n45:07.040 --> 45:11.080\n Yeah, so we're taking the first steps already.\n\n45:11.080 --> 45:13.320\n So on the flip side of AI,\n\n45:13.320 --> 45:14.920\n people like Stuart Russell and others\n\n45:14.920 --> 45:19.000\n focus on the control problem, value alignment in AI,\n\n45:19.000 --> 45:21.400\n which is the problem of making sure we build systems\n\n45:21.400 --> 45:25.480\n that align to our own values, our ethics.\n\n45:25.480 --> 45:28.440\n Do you think sort of high level,\n\n45:28.440 --> 45:31.160\n how do we go about building systems?\n\n45:31.160 --> 45:34.640\n Do you think is it possible that align with our values,\n\n45:34.640 --> 45:39.360\n align with our human ethics or living being ethics?\n\n45:39.360 --> 45:42.360\n Presumably, it's possible to do that.\n\n45:43.900 --> 45:46.120\n I know that a lot of people who think\n\n45:46.120 --> 45:48.000\n that there's a real danger that we won't,\n\n45:48.000 --> 45:51.840\n that we'll more or less accidentally lose control of AGI.\n\n45:51.840 --> 45:54.080\n Do you have that fear yourself personally?\n\n45:56.880 --> 45:58.600\n I'm not quite sure what to think.\n\n45:58.600 --> 46:01.880\n I talk to philosophers like Nick Bostrom and Toby Ord\n\n46:01.880 --> 46:05.000\n and they think that this is a real problem\n\n46:05.000 --> 46:07.240\n we need to worry about.\n\n46:07.240 --> 46:11.200\n Then I talk to people who work for Microsoft\n\n46:11.200 --> 46:13.640\n or DeepMind or somebody and they say,\n\n46:13.640 --> 46:18.320\n no, we're not really that close to producing AGI,\n\n46:18.320 --> 46:19.600\n super intelligence.\n\n46:19.600 --> 46:21.280\n So if you look at Nick Bostrom,\n\n46:21.280 --> 46:25.000\n sort of the arguments, it's very hard to defend.\n\n46:25.000 --> 46:28.040\n So I'm of course, I am a self engineer AI system,\n\n46:28.040 --> 46:29.920\n so I'm more with the DeepMind folks\n\n46:29.920 --> 46:32.360\n where it seems that we're really far away,\n\n46:32.360 --> 46:34.840\n but then the counter argument is,\n\n46:34.840 --> 46:38.280\n is there any fundamental reason that we'll never achieve it?\n\n46:39.160 --> 46:42.160\n And if not, then eventually there'll be\n\n46:42.160 --> 46:44.360\n a dire existential risk.\n\n46:44.360 --> 46:46.440\n So we should be concerned about it.\n\n46:46.440 --> 46:50.700\n And do you find that argument at all appealing\n\n46:50.700 --> 46:53.120\n in this domain or any domain that eventually\n\n46:53.120 --> 46:55.760\n this will be a problem so we should be worried about it?\n\n46:56.880 --> 46:58.720\n Yes, I think it's a problem.\n\n46:58.720 --> 47:02.320\n I think that's a valid point.\n\n47:03.760 --> 47:06.100\n Of course, when you say eventually,\n\n47:08.960 --> 47:11.440\n that raises the question, how far off is that?\n\n47:11.440 --> 47:13.840\n And is there something that we can do about it now?\n\n47:13.840 --> 47:15.440\n Because if we're talking about\n\n47:15.440 --> 47:17.720\n this is gonna be 100 years in the future\n\n47:17.720 --> 47:20.080\n and you consider how rapidly our knowledge\n\n47:20.080 --> 47:22.080\n of artificial intelligence has grown\n\n47:22.080 --> 47:24.000\n in the last 10 or 20 years,\n\n47:24.000 --> 47:26.920\n it seems unlikely that there's anything much\n\n47:26.920 --> 47:29.640\n we could do now that would influence\n\n47:29.640 --> 47:33.440\n whether this is going to happen 100 years in the future.\n\n47:33.440 --> 47:35.120\n People in 80 years in the future\n\n47:35.120 --> 47:37.300\n would be in a much better position to say,\n\n47:37.300 --> 47:39.740\n this is what we need to do to prevent this happening\n\n47:39.740 --> 47:41.520\n than we are now.\n\n47:41.520 --> 47:44.560\n So to some extent I find that reassuring,\n\n47:44.560 --> 47:48.640\n but I'm all in favor of some people doing research\n\n47:48.640 --> 47:51.480\n into this to see if indeed it is that far off\n\n47:51.480 --> 47:55.440\n or if we are in a position to do something about it sooner.\n\n47:55.440 --> 47:58.760\n I'm very much of the view that extinction\n\n47:58.760 --> 48:02.760\n is a terrible thing and therefore,\n\n48:02.760 --> 48:05.960\n even if the risk of extinction is very small,\n\n48:05.960 --> 48:09.040\n if we can reduce that risk,\n\n48:09.040 --> 48:11.240\n that's something that we ought to do.\n\n48:11.240 --> 48:12.760\n My disagreement with some of these people\n\n48:12.760 --> 48:16.360\n who talk about longterm risks, extinction risks,\n\n48:16.360 --> 48:18.820\n is only about how much priority that should have\n\n48:18.820 --> 48:20.520\n as compared to present questions.\n\n48:20.520 --> 48:22.680\n So essentially, if you look at the math of it\n\n48:22.680 --> 48:25.000\n from a utilitarian perspective,\n\n48:25.000 --> 48:28.920\n if it's existential risk, so everybody dies,\n\n48:28.920 --> 48:33.160\n that it feels like an infinity in the math equation,\n\n48:33.160 --> 48:36.880\n that that makes the math\n\n48:36.880 --> 48:39.380\n with the priorities difficult to do.\n\n48:39.380 --> 48:42.720\n That if we don't know the time scale\n\n48:42.720 --> 48:43.960\n and you can legitimately argue\n\n48:43.960 --> 48:46.760\n that it's nonzero probability that it'll happen tomorrow,\n\n48:48.160 --> 48:52.080\n that how do you deal with these kinds of existential risks\n\n48:52.080 --> 48:55.720\n like from nuclear war, from nuclear weapons,\n\n48:55.720 --> 48:58.640\n from biological weapons, from,\n\n48:58.640 --> 49:01.960\n I'm not sure if global warming falls into that category\n\n49:01.960 --> 49:04.760\n because global warming is a lot more gradual.\n\n49:04.760 --> 49:06.880\n And people say it's not an existential risk\n\n49:06.880 --> 49:08.280\n because there'll always be possibilities\n\n49:08.280 --> 49:11.200\n of some humans existing, farming Antarctica\n\n49:11.200 --> 49:14.260\n or northern Siberia or something of that sort, yeah.\n\n49:14.260 --> 49:18.360\n But you don't find the complete existential risks\n\n49:18.360 --> 49:23.080\n as a fundamental, like an overriding part\n\n49:23.080 --> 49:26.280\n of the equations of ethics, of what we should do.\n\n49:26.280 --> 49:29.000\n You know, certainly if you treat it as an infinity,\n\n49:29.000 --> 49:32.040\n then it plays havoc with any calculations.\n\n49:32.040 --> 49:34.480\n But arguably, we shouldn't.\n\n49:34.480 --> 49:37.380\n I mean, one of the ethical assumptions that goes into this\n\n49:37.380 --> 49:40.680\n is that the loss of future lives,\n\n49:40.680 --> 49:43.280\n that is of merely possible lives of beings\n\n49:43.280 --> 49:44.920\n who may never exist at all,\n\n49:44.920 --> 49:49.920\n is in some way comparable to the sufferings or deaths\n\n49:51.240 --> 49:53.700\n of people who do exist at some point.\n\n49:54.680 --> 49:57.380\n And that's not clear to me.\n\n49:57.380 --> 49:59.320\n I think there's a case for saying that,\n\n49:59.320 --> 50:01.800\n but I also think there's a case for taking the other view.\n\n50:01.800 --> 50:04.560\n So that has some impact on it.\n\n50:04.560 --> 50:05.940\n Of course, you might say, ah, yes,\n\n50:05.940 --> 50:08.920\n but still, if there's some uncertainty about this\n\n50:08.920 --> 50:12.560\n and the costs of extinction are infinite,\n\n50:12.560 --> 50:15.360\n then still, it's gonna overwhelm everything else.\n\n50:16.680 --> 50:20.880\n But I suppose I'm not convinced of that.\n\n50:20.880 --> 50:23.440\n I'm not convinced that it's really infinite here.\n\n50:23.440 --> 50:27.240\n And even Nick Bostrom, in his discussion of this,\n\n50:27.240 --> 50:28.560\n doesn't claim that there'll be\n\n50:28.560 --> 50:31.280\n an infinite number of lives lived.\n\n50:31.280 --> 50:33.360\n What is it, 10 to the 56th or something?\n\n50:33.360 --> 50:36.040\n It's a vast number that I think he calculates.\n\n50:36.040 --> 50:38.220\n This is assuming we can upload consciousness\n\n50:38.220 --> 50:43.220\n onto these digital forms,\n\n50:43.560 --> 50:45.280\n and therefore, they'll be much more energy efficient,\n\n50:45.280 --> 50:47.640\n but he calculates the amount of energy in the universe\n\n50:47.640 --> 50:48.660\n or something like that.\n\n50:48.660 --> 50:50.480\n So the numbers are vast but not infinite,\n\n50:50.480 --> 50:52.520\n which gives you some prospect maybe\n\n50:52.520 --> 50:55.640\n of resisting some of the argument.\n\n50:55.640 --> 50:57.360\n The beautiful thing with Nick's arguments\n\n50:57.360 --> 50:59.780\n is he quickly jumps from the individual scale\n\n50:59.780 --> 51:01.080\n to the universal scale,\n\n51:01.080 --> 51:04.480\n which is just awe inspiring to think of\n\n51:04.480 --> 51:06.200\n when you think about the entirety\n\n51:06.200 --> 51:08.880\n of the span of time of the universe.\n\n51:08.880 --> 51:11.400\n It's both interesting from a computer science perspective,\n\n51:11.400 --> 51:13.760\n AI perspective, and from an ethical perspective,\n\n51:13.760 --> 51:16.000\n the idea of utilitarianism.\n\n51:16.000 --> 51:18.600\n Could you say what is utilitarianism?\n\n51:19.720 --> 51:22.060\n Utilitarianism is the ethical view\n\n51:22.060 --> 51:25.440\n that the right thing to do is the act\n\n51:25.440 --> 51:28.740\n that has the greatest expected utility,\n\n51:28.740 --> 51:32.320\n where what that means is it's the act\n\n51:32.320 --> 51:34.860\n that will produce the best consequences,\n\n51:34.860 --> 51:37.680\n discounted by the odds that you won't be able\n\n51:37.680 --> 51:38.940\n to produce those consequences,\n\n51:38.940 --> 51:40.400\n that something will go wrong.\n\n51:40.400 --> 51:43.880\n But in simple case, let's assume we have certainty\n\n51:43.880 --> 51:46.140\n about what the consequences of our actions will be,\n\n51:46.140 --> 51:47.600\n then the right action is the action\n\n51:47.600 --> 51:50.500\n that will produce the best consequences.\n\n51:50.500 --> 51:52.080\n Is that always, and by the way,\n\n51:52.080 --> 51:53.400\n there's a bunch of nuanced stuff\n\n51:53.400 --> 51:56.000\n that you talk with Sam Harris on this podcast\n\n51:56.000 --> 51:57.960\n on that people should go listen to.\n\n51:57.960 --> 51:58.800\n It's great.\n\n51:58.800 --> 52:02.940\n That's like two hours of moral philosophy discussion.\n\n52:02.940 --> 52:05.520\n But is that an easy calculation?\n\n52:05.520 --> 52:07.360\n No, it's a difficult calculation.\n\n52:07.360 --> 52:10.000\n And actually, there's one thing that I need to add,\n\n52:10.000 --> 52:14.240\n and that is utilitarians, certainly the classical\n\n52:14.240 --> 52:16.760\n utilitarians, think that by best consequences,\n\n52:16.760 --> 52:18.840\n we're talking about happiness\n\n52:18.840 --> 52:21.020\n and the absence of pain and suffering.\n\n52:21.020 --> 52:22.920\n There are other consequentialists\n\n52:22.920 --> 52:25.920\n who are not really utilitarians who say\n\n52:27.320 --> 52:29.740\n there are different things that could be good consequences.\n\n52:29.740 --> 52:32.800\n Justice, freedom, human dignity,\n\n52:32.800 --> 52:35.840\n knowledge, they all count as good consequences too.\n\n52:35.840 --> 52:38.080\n And that makes the calculations even more difficult\n\n52:38.080 --> 52:38.920\n because then you need to know\n\n52:38.920 --> 52:40.840\n how to balance these things off.\n\n52:40.840 --> 52:44.580\n If you are just talking about wellbeing,\n\n52:44.580 --> 52:46.560\n using that term to express happiness\n\n52:46.560 --> 52:48.060\n and the absence of suffering,\n\n52:49.040 --> 52:54.040\n I think the calculation becomes more manageable\n\n52:54.280 --> 52:56.400\n in a philosophical sense.\n\n52:56.400 --> 52:58.180\n It's still in practice.\n\n52:58.180 --> 52:59.280\n We don't know how to do it.\n\n52:59.280 --> 53:01.040\n We don't know how to measure quantities\n\n53:01.040 --> 53:02.740\n of happiness and misery.\n\n53:02.740 --> 53:04.960\n We don't know how to calculate the probabilities\n\n53:04.960 --> 53:07.760\n that different actions will produce, this or that.\n\n53:08.800 --> 53:13.080\n So at best, we can use it as a rough guide\n\n53:13.080 --> 53:16.520\n to different actions and one where we have to focus\n\n53:16.520 --> 53:20.120\n on the short term consequences\n\n53:20.120 --> 53:22.800\n because we just can't really predict\n\n53:22.800 --> 53:25.360\n all of the longer term ramifications.\n\n53:25.360 --> 53:33.240\n So what about the extreme suffering of very small groups?\n\n53:33.240 --> 53:36.920\n Utilitarianism is focused on the overall aggregate, right?\n\n53:38.320 --> 53:41.040\n Would you say you yourself are a utilitarian?\n\n53:41.040 --> 53:42.440\n Yes, I'm a utilitarian.\n\n53:45.540 --> 53:50.280\n What do you make of the difficult, ethical,\n\n53:50.280 --> 53:54.960\n maybe poetic suffering of very few individuals?\n\n53:54.960 --> 53:57.040\n I think it's possible that that gets overridden\n\n53:57.040 --> 54:00.080\n by benefits to very large numbers of individuals.\n\n54:00.080 --> 54:02.880\n I think that can be the right answer.\n\n54:02.880 --> 54:05.440\n But before we conclude that it is the right answer,\n\n54:05.440 --> 54:08.960\n we have to know how severe the suffering is\n\n54:08.960 --> 54:12.320\n and how that compares with the benefits.\n\n54:12.320 --> 54:17.320\n So I tend to think that extreme suffering is worse than\n\n54:19.680 --> 54:23.480\n or is further, if you like, below the neutral level\n\n54:23.480 --> 54:27.320\n than extreme happiness or bliss is above it.\n\n54:27.320 --> 54:30.720\n So when I think about the worst experiences possible\n\n54:30.720 --> 54:33.160\n and the best experiences possible,\n\n54:33.160 --> 54:36.200\n I don't think of them as equidistant from neutral.\n\n54:36.200 --> 54:39.640\n So like it's a scale that goes from minus 100 through zero\n\n54:39.640 --> 54:41.840\n as a neutral level to plus 100.\n\n54:43.480 --> 54:46.880\n Because I know that I would not exchange an hour\n\n54:46.880 --> 54:49.620\n of my most pleasurable experiences\n\n54:49.620 --> 54:52.400\n for an hour of my most painful experiences,\n\n54:52.400 --> 54:54.440\n even I wouldn't have an hour\n\n54:54.440 --> 54:57.360\n of my most painful experiences even for two hours\n\n54:57.360 --> 55:01.760\n or 10 hours of my most painful experiences.\n\n55:01.760 --> 55:02.600\n Did I say that correctly?\n\n55:02.600 --> 55:03.720\n Yeah, yeah, yeah, yeah.\n\n55:03.720 --> 55:07.080\n Maybe 20 hours then, it's 21, what's the exchange rate?\n\n55:07.080 --> 55:08.700\n So that's the question, what is the exchange rate?\n\n55:08.700 --> 55:10.940\n But I think it can be quite high.\n\n55:10.940 --> 55:13.760\n So that's why you shouldn't just assume that\n\n55:15.480 --> 55:18.480\n it's okay to make one person suffer extremely\n\n55:18.480 --> 55:21.520\n in order to make two people much better off.\n\n55:21.520 --> 55:23.520\n It might be a much larger number.\n\n55:23.520 --> 55:27.520\n But at some point I do think you should aggregate\n\n55:27.520 --> 55:30.560\n and the result will be,\n\n55:30.560 --> 55:33.840\n even though it violates our intuitions of justice\n\n55:33.840 --> 55:36.560\n and fairness, whatever it might be,\n\n55:36.560 --> 55:39.560\n giving priority to those who are worse off,\n\n55:39.560 --> 55:41.660\n at some point I still think\n\n55:41.660 --> 55:43.040\n that will be the right thing to do.\n\n55:43.040 --> 55:45.440\n Yeah, it's some complicated nonlinear function.\n\n55:46.960 --> 55:49.000\n Can I ask a sort of out there question is,\n\n55:49.000 --> 55:51.080\n the more and more we put our data out there,\n\n55:51.080 --> 55:53.200\n the more we're able to measure a bunch of factors\n\n55:53.200 --> 55:55.680\n of each of our individual human lives.\n\n55:55.680 --> 55:59.940\n And I could foresee the ability to estimate wellbeing\n\n55:59.940 --> 56:03.940\n of whatever we together collectively agree\n\n56:03.940 --> 56:05.960\n and is in a good objective function\n\n56:05.960 --> 56:07.900\n from a utilitarian perspective.\n\n56:07.900 --> 56:11.360\n Do you think it'll be possible\n\n56:11.360 --> 56:15.960\n and is a good idea to push that kind of analysis\n\n56:15.960 --> 56:19.920\n to make then public decisions perhaps with the help of AI\n\n56:19.920 --> 56:23.560\n that here's a tax rate,\n\n56:24.560 --> 56:28.280\n here's a tax rate at which wellbeing will be optimized.\n\n56:28.280 --> 56:31.040\n Yeah, that would be great if we really knew that,\n\n56:31.040 --> 56:32.360\n if we really could calculate that.\n\n56:32.360 --> 56:33.600\n No, but do you think it's possible\n\n56:33.600 --> 56:36.640\n to converge towards an agreement amongst humans,\n\n56:36.640 --> 56:39.720\n towards an objective function\n\n56:39.720 --> 56:42.020\n or is it just a hopeless pursuit?\n\n56:42.020 --> 56:43.080\n I don't think it's hopeless.\n\n56:43.080 --> 56:44.800\n I think it would be difficult\n\n56:44.800 --> 56:47.880\n to get converged towards agreement, at least at present,\n\n56:47.880 --> 56:49.920\n because some people would say,\n\n56:49.920 --> 56:52.040\n I've got different views about justice\n\n56:52.040 --> 56:54.180\n and I think you ought to give priority\n\n56:54.180 --> 56:55.860\n to those who are worse off,\n\n56:55.860 --> 56:58.720\n even though I acknowledge that the gains\n\n56:58.720 --> 57:01.460\n that the worst off are making are less than the gains\n\n57:01.460 --> 57:05.740\n that those who are sort of medium badly off could be making.\n\n57:05.740 --> 57:10.240\n So we still have all of these intuitions that we argue about.\n\n57:10.240 --> 57:11.700\n So I don't think we would get agreement,\n\n57:11.700 --> 57:14.280\n but the fact that we wouldn't get agreement\n\n57:14.280 --> 57:17.840\n doesn't show that there isn't a right answer there.\n\n57:17.840 --> 57:21.320\n Do you think, who gets to say what is right and wrong?\n\n57:21.320 --> 57:23.600\n Do you think there's place for ethics oversight\n\n57:23.600 --> 57:26.360\n from the government?\n\n57:26.360 --> 57:29.320\n So I'm thinking in the case of AI,\n\n57:29.320 --> 57:33.900\n overseeing what kind of decisions AI can make or not,\n\n57:33.900 --> 57:36.700\n but also if you look at animal rights\n\n57:36.700 --> 57:39.560\n or rather not rights or perhaps rights,\n\n57:39.560 --> 57:43.000\n but the ideas you've explored in animal liberation,\n\n57:43.000 --> 57:46.480\n who gets to, so you eloquently and beautifully write\n\n57:46.480 --> 57:50.480\n in your book that this, you know, we shouldn't do this,\n\n57:50.480 --> 57:53.600\n but is there some harder rules that should be imposed\n\n57:53.600 --> 57:56.680\n or is this a collective thing we converse towards the society\n\n57:56.680 --> 57:59.680\n and thereby make the better and better ethical decisions?\n\n58:02.080 --> 58:04.320\n Politically, I'm still a Democrat\n\n58:04.320 --> 58:07.880\n despite looking at the flaws in democracy\n\n58:07.880 --> 58:10.160\n and the way it doesn't work always very well.\n\n58:10.160 --> 58:11.880\n So I don't see a better option\n\n58:11.880 --> 58:16.880\n than allowing the public to vote for governments\n\n58:18.520 --> 58:20.040\n in accordance with their policies.\n\n58:20.040 --> 58:24.800\n And I hope that they will vote for policies\n\n58:24.800 --> 58:27.800\n that reduce the suffering of animals\n\n58:27.800 --> 58:30.600\n and reduce the suffering of distant humans,\n\n58:30.600 --> 58:32.600\n whether geographically distant or distant\n\n58:32.600 --> 58:35.160\n because they're future humans.\n\n58:35.160 --> 58:36.520\n But I recognise that democracy\n\n58:36.520 --> 58:38.440\n isn't really well set up to do that.\n\n58:38.440 --> 58:43.440\n And in a sense, you could imagine a wise and benevolent,\n\n58:45.540 --> 58:48.740\n you know, omnibenevolent leader\n\n58:48.740 --> 58:51.820\n who would do that better than democracies could.\n\n58:51.820 --> 58:54.660\n But in the world in which we live,\n\n58:54.660 --> 58:57.420\n it's difficult to imagine that this leader\n\n58:57.420 --> 59:01.300\n isn't gonna be corrupted by a variety of influences.\n\n59:01.300 --> 59:04.100\n You know, we've had so many examples\n\n59:04.100 --> 59:08.540\n of people who've taken power with good intentions\n\n59:08.540 --> 59:10.260\n and then have ended up being corrupt\n\n59:10.260 --> 59:11.620\n and favouring themselves.\n\n59:12.780 --> 59:16.540\n So I don't know, you know, that's why, as I say,\n\n59:16.540 --> 59:17.960\n I don't know that we have a better system\n\n59:17.960 --> 59:20.060\n than democracy to make these decisions.\n\n59:20.060 --> 59:23.460\n Well, so you also discuss effective altruism,\n\n59:23.460 --> 59:27.220\n which is a mechanism for going around government\n\n59:27.220 --> 59:29.540\n for putting the power in the hands of the people\n\n59:29.540 --> 59:32.460\n to donate money towards causes to help, you know,\n\n59:32.460 --> 59:37.460\n remove the middleman and give it directly\n\n59:37.940 --> 59:41.540\n to the causes that they care about.\n\n59:41.540 --> 59:45.220\n Sort of, maybe this is a good time to ask,\n\n59:45.220 --> 59:48.180\n you've, 10 years ago, wrote The Life You Can Save,\n\n59:48.180 --> 59:51.300\n that's now, I think, available for free online?\n\n59:51.300 --> 59:53.820\n That's right, you can download either the ebook\n\n59:53.820 --> 59:57.480\n or the audiobook free from the lifeyoucansave.org.\n\n59:58.420 --> 1:00:01.520\n And what are the key ideas that you present\n\n1:00:01.520 --> 1:00:03.820\n in the book?\n\n1:00:03.820 --> 1:00:05.140\n The main thing I wanna do in the book\n\n1:00:05.140 --> 1:00:10.140\n is to make people realise that it's not difficult\n\n1:00:10.320 --> 1:00:13.700\n to help people in extreme poverty,\n\n1:00:13.700 --> 1:00:16.780\n that there are highly effective organisations now\n\n1:00:16.780 --> 1:00:20.300\n that are doing this, that they've been independently assessed\n\n1:00:20.300 --> 1:00:25.300\n and verified by research teams that are expert in this area\n\n1:00:25.300 --> 1:00:28.180\n and that it's a fulfilling thing to do\n\n1:00:28.180 --> 1:00:30.860\n to, for at least part of your life, you know,\n\n1:00:30.860 --> 1:00:33.500\n we can't all be saints, but at least one of your goals\n\n1:00:33.500 --> 1:00:36.060\n should be to really make a positive contribution\n\n1:00:36.060 --> 1:00:38.260\n to the world and to do something to help people\n\n1:00:38.260 --> 1:00:40.940\n who through no fault of their own\n\n1:00:40.940 --> 1:00:45.820\n are in very dire circumstances and living a life\n\n1:00:45.820 --> 1:00:49.540\n that is barely or perhaps not at all\n\n1:00:49.540 --> 1:00:51.920\n a decent life for a human being to live.\n\n1:00:51.920 --> 1:00:56.920\n So you describe a minimum ethical standard of giving.\n\n1:00:56.920 --> 1:01:01.380\n What advice would you give to people\n\n1:01:01.380 --> 1:01:06.380\n that want to be effectively altruistic in their life,\n\n1:01:06.500 --> 1:01:09.340\n like live an effective altruism life?\n\n1:01:09.340 --> 1:01:12.060\n There are many different kinds of ways of living\n\n1:01:12.060 --> 1:01:13.540\n as an effective altruist.\n\n1:01:14.440 --> 1:01:16.660\n And if you're at the point where you're thinking\n\n1:01:16.660 --> 1:01:20.060\n about your long term career, I'd recommend you take a look\n\n1:01:20.060 --> 1:01:24.660\n at a website called 80,000Hours, 80,000Hours.org,\n\n1:01:24.660 --> 1:01:27.180\n which looks at ethical career choices.\n\n1:01:27.180 --> 1:01:29.740\n And they range from, for example,\n\n1:01:29.740 --> 1:01:31.060\n going to work on Wall Street\n\n1:01:31.060 --> 1:01:33.340\n so that you can earn a huge amount of money\n\n1:01:33.340 --> 1:01:36.980\n and then donate most of it to effective charities\n\n1:01:36.980 --> 1:01:40.860\n to going to work for a really good nonprofit organization\n\n1:01:40.860 --> 1:01:44.060\n so that you can directly use your skills and ability\n\n1:01:44.060 --> 1:01:48.620\n and hard work to further a good cause,\n\n1:01:48.620 --> 1:01:52.640\n or perhaps going into politics, maybe small chances,\n\n1:01:52.640 --> 1:01:55.140\n but big payoffs in politics,\n\n1:01:55.140 --> 1:01:56.520\n go to work in the public service\n\n1:01:56.520 --> 1:01:59.180\n where if you're talented, you might rise to a high level\n\n1:01:59.180 --> 1:02:01.700\n where you can influence decisions,\n\n1:02:01.700 --> 1:02:05.160\n do research in an area where the payoffs could be great.\n\n1:02:05.160 --> 1:02:07.220\n There are a lot of different opportunities,\n\n1:02:07.220 --> 1:02:11.340\n but too few people are even thinking about those questions.\n\n1:02:11.340 --> 1:02:14.720\n They're just going along in some sort of preordained rut\n\n1:02:14.720 --> 1:02:15.780\n to particular careers.\n\n1:02:15.780 --> 1:02:17.420\n Maybe they think they'll earn a lot of money\n\n1:02:17.420 --> 1:02:19.180\n and have a comfortable life,\n\n1:02:19.180 --> 1:02:20.940\n but they may not find that as fulfilling\n\n1:02:20.940 --> 1:02:23.500\n as actually knowing that they're making\n\n1:02:23.500 --> 1:02:25.100\n a positive difference to the world.\n\n1:02:25.100 --> 1:02:27.020\n What about in terms of,\n\n1:02:27.020 --> 1:02:30.100\n so that's like long term, 80,000 hours,\n\n1:02:30.100 --> 1:02:33.100\n sort of shorter term giving part of,\n\n1:02:33.100 --> 1:02:34.340\n well, actually it's a part of that.\n\n1:02:34.340 --> 1:02:37.100\n You go to work at Wall Street,\n\n1:02:37.100 --> 1:02:40.060\n if you would like to give a percentage of your income\n\n1:02:40.060 --> 1:02:42.420\n that you talk about and life you can save that.\n\n1:02:42.420 --> 1:02:45.820\n I mean, I was looking through, it's quite a compelling,\n\n1:02:48.100 --> 1:02:50.440\n I mean, I'm just a dumb engineer,\n\n1:02:50.440 --> 1:02:53.740\n so I like, there's simple rules, there's a nice percentage.\n\n1:02:53.740 --> 1:02:57.540\n Okay, so I do actually set out suggested levels of giving\n\n1:02:57.540 --> 1:03:00.220\n because people often ask me about this.\n\n1:03:00.220 --> 1:03:04.140\n A popular answer is give 10%, the traditional tithe\n\n1:03:04.140 --> 1:03:08.500\n that's recommended in Christianity and also Judaism.\n\n1:03:08.500 --> 1:03:11.820\n But why should it be the same percentage\n\n1:03:11.820 --> 1:03:13.640\n irrespective of your income?\n\n1:03:13.640 --> 1:03:16.280\n Tax scales reflect the idea that the more income you have,\n\n1:03:16.280 --> 1:03:18.040\n the more you can pay tax.\n\n1:03:18.040 --> 1:03:20.400\n And I think the same is true in what you can give.\n\n1:03:20.400 --> 1:03:25.400\n So I do set out a progressive donor scale,\n\n1:03:25.500 --> 1:03:28.940\n which starts out at 1% for people on modest incomes\n\n1:03:28.940 --> 1:03:31.900\n and rises to 33 and a third percent\n\n1:03:31.900 --> 1:03:34.320\n for people who are really earning a lot.\n\n1:03:34.320 --> 1:03:38.620\n And my idea is that I don't think any of these amounts\n\n1:03:38.620 --> 1:03:42.120\n really impose real hardship on people\n\n1:03:42.120 --> 1:03:44.780\n because they are progressive and geared to income.\n\n1:03:45.660 --> 1:03:48.660\n So I think anybody can do this\n\n1:03:48.660 --> 1:03:51.940\n and can know that they're doing something significant\n\n1:03:51.940 --> 1:03:56.060\n to play their part in reducing the huge gap\n\n1:03:56.060 --> 1:03:58.780\n between people in extreme poverty in the world\n\n1:03:58.780 --> 1:04:01.180\n and people living affluent lives.\n\n1:04:02.180 --> 1:04:05.780\n And aside from it being an ethical life,\n\n1:04:05.780 --> 1:04:07.540\n it's one that you find more fulfilling\n\n1:04:07.540 --> 1:04:10.980\n because there's something about our human nature that,\n\n1:04:11.940 --> 1:04:13.740\n or some of our human natures,\n\n1:04:13.740 --> 1:04:18.580\n maybe most of our human nature that enjoys doing\n\n1:04:18.580 --> 1:04:21.660\n the ethical thing.\n\n1:04:21.660 --> 1:04:23.140\n Yes, I make both those arguments,\n\n1:04:23.140 --> 1:04:25.460\n that it is an ethical requirement\n\n1:04:25.460 --> 1:04:27.220\n in the kind of world we live in today\n\n1:04:27.220 --> 1:04:30.480\n to help people in great need when we can easily do so,\n\n1:04:30.480 --> 1:04:33.000\n but also that it is a rewarding thing\n\n1:04:33.000 --> 1:04:35.700\n and there's good psychological research showing\n\n1:04:35.700 --> 1:04:39.440\n that people who give more tend to be more satisfied\n\n1:04:39.440 --> 1:04:40.580\n with their lives.\n\n1:04:40.580 --> 1:04:41.940\n And I think this has something to do\n\n1:04:41.940 --> 1:04:44.900\n with having a purpose that's larger than yourself\n\n1:04:44.900 --> 1:04:49.620\n and therefore never being, if you like,\n\n1:04:49.620 --> 1:04:51.180\n never being bored sitting around,\n\n1:04:51.180 --> 1:04:52.800\n oh, you know, what will I do next?\n\n1:04:52.800 --> 1:04:54.260\n I've got nothing to do.\n\n1:04:54.260 --> 1:04:56.440\n In a world like this, there are many good things\n\n1:04:56.440 --> 1:04:59.420\n that you can do and enjoy doing them.\n\n1:04:59.420 --> 1:05:02.380\n Plus you're working with other people\n\n1:05:02.380 --> 1:05:03.940\n in the effective altruism movement\n\n1:05:03.940 --> 1:05:06.280\n who are forming a community of other people\n\n1:05:06.280 --> 1:05:09.300\n with similar ideas and they tend to be interesting,\n\n1:05:09.300 --> 1:05:11.100\n thoughtful and good people as well.\n\n1:05:11.100 --> 1:05:14.180\n And having friends of that sort is another big contribution\n\n1:05:14.180 --> 1:05:16.020\n to having a good life.\n\n1:05:16.020 --> 1:05:20.340\n So we talked about big things that are beyond ourselves,\n\n1:05:20.340 --> 1:05:24.600\n but we're also just human and mortal.\n\n1:05:24.600 --> 1:05:27.420\n Do you ponder your own mortality?\n\n1:05:27.420 --> 1:05:29.660\n Is there insights about your philosophy,\n\n1:05:29.660 --> 1:05:33.800\n the ethics that you gain from pondering your own mortality?\n\n1:05:35.780 --> 1:05:37.940\n Clearly, you know, as you get into your 70s,\n\n1:05:37.940 --> 1:05:40.380\n you can't help thinking about your own mortality.\n\n1:05:40.380 --> 1:05:44.780\n Uh, but I don't know that I have great insights\n\n1:05:44.780 --> 1:05:47.140\n into that from my philosophy.\n\n1:05:47.140 --> 1:05:50.460\n I don't think there's anything after the death of my body,\n\n1:05:50.460 --> 1:05:53.500\n you know, assuming that we won't be able to upload my mind\n\n1:05:53.500 --> 1:05:55.380\n into anything at the time when I die.\n\n1:05:56.860 --> 1:05:58.460\n So I don't think there's any afterlife\n\n1:05:58.460 --> 1:06:00.940\n or anything to look forward to in that sense.\n\n1:06:00.940 --> 1:06:01.900\n Do you fear death?\n\n1:06:01.900 --> 1:06:04.140\n So if you look at Ernest Becker\n\n1:06:04.140 --> 1:06:08.060\n and describing the motivating aspects\n\n1:06:08.060 --> 1:06:13.060\n of our ability to be cognizant of our mortality,\n\n1:06:14.820 --> 1:06:17.460\n do you have any of those elements\n\n1:06:17.460 --> 1:06:19.680\n in your drive and your motivation in life?\n\n1:06:21.020 --> 1:06:23.500\n I suppose the fact that you have only a limited time\n\n1:06:23.500 --> 1:06:25.840\n to achieve the things that you want to achieve\n\n1:06:25.840 --> 1:06:27.320\n gives you some sort of motivation\n\n1:06:27.320 --> 1:06:29.700\n to get going and achieving them.\n\n1:06:29.700 --> 1:06:31.020\n And if we thought we were immortal,\n\n1:06:31.020 --> 1:06:32.600\n we might say, ah, you know,\n\n1:06:32.600 --> 1:06:35.000\n I can put that off for another decade or two.\n\n1:06:36.080 --> 1:06:37.740\n So there's that about it.\n\n1:06:37.740 --> 1:06:40.020\n But otherwise, you know, no,\n\n1:06:40.020 --> 1:06:42.060\n I'd rather have more time to do more.\n\n1:06:42.060 --> 1:06:45.860\n I'd also like to be able to see how things go\n\n1:06:45.860 --> 1:06:47.500\n that I'm interested in, you know.\n\n1:06:47.500 --> 1:06:49.940\n Is climate change gonna turn out to be as dire\n\n1:06:49.940 --> 1:06:53.500\n as a lot of scientists say that it is going to be?\n\n1:06:53.500 --> 1:06:55.500\n Will we somehow scrape through\n\n1:06:55.500 --> 1:06:57.860\n with less damage than we thought?\n\n1:06:57.860 --> 1:06:59.840\n I'd really like to know the answers to those questions,\n\n1:06:59.840 --> 1:07:02.180\n but I guess I'm not going to.\n\n1:07:02.180 --> 1:07:05.780\n Well, you said there's nothing afterwards.\n\n1:07:05.780 --> 1:07:08.100\n So let me ask the even more absurd question.\n\n1:07:08.100 --> 1:07:10.100\n What do you think is the meaning of it all?\n\n1:07:11.120 --> 1:07:14.120\n I think the meaning of life is the meaning we give to it.\n\n1:07:14.120 --> 1:07:18.100\n I don't think that we were brought into the universe\n\n1:07:18.100 --> 1:07:21.860\n for any kind of larger purpose.\n\n1:07:21.860 --> 1:07:24.100\n But given that we exist,\n\n1:07:24.100 --> 1:07:26.460\n I think we can recognize that some things\n\n1:07:26.460 --> 1:07:29.500\n are objectively bad.\n\n1:07:30.820 --> 1:07:32.620\n Extreme suffering is an example,\n\n1:07:32.620 --> 1:07:35.060\n and other things are objectively good,\n\n1:07:35.060 --> 1:07:38.020\n like having a rich, fulfilling, enjoyable,\n\n1:07:38.020 --> 1:07:42.780\n pleasurable life, and we can try to do our part\n\n1:07:42.780 --> 1:07:45.820\n in reducing the bad things and increasing the good things.\n\n1:07:47.220 --> 1:07:50.540\n So one way, the meaning is to do a little bit more\n\n1:07:50.540 --> 1:07:52.660\n of the good things, objectively good things,\n\n1:07:52.660 --> 1:07:55.460\n and a little bit less of the bad things.\n\n1:07:55.460 --> 1:07:58.940\n Yes, so do as much of the good things as you can\n\n1:07:58.940 --> 1:08:00.580\n and as little of the bad things.\n\n1:08:00.580 --> 1:08:03.020\n You beautifully put, I don't think there's a better place\n\n1:08:03.020 --> 1:08:04.900\n to end it, thank you so much for talking today.\n\n1:08:04.900 --> 1:08:05.740\n Thanks very much, Lex.\n\n1:08:05.740 --> 1:08:08.780\n It's been really interesting talking to you.\n\n1:08:08.780 --> 1:08:10.260\n Thanks for listening to this conversation\n\n1:08:10.260 --> 1:08:13.420\n with Peter Singer, and thank you to our sponsors,\n\n1:08:13.420 --> 1:08:15.940\n Cash App and Masterclass.\n\n1:08:15.940 --> 1:08:17.660\n Please consider supporting the podcast\n\n1:08:17.660 --> 1:08:21.620\n by downloading Cash App and using the code LexPodcast,\n\n1:08:21.620 --> 1:08:26.140\n and signing up at masterclass.com slash Lex.\n\n1:08:26.140 --> 1:08:28.900\n Click the links, buy all the stuff.\n\n1:08:28.900 --> 1:08:30.960\n It's the best way to support this podcast\n\n1:08:30.960 --> 1:08:35.220\n and the journey I'm on in my research and startup.\n\n1:08:35.220 --> 1:08:38.020\n If you enjoy this thing, subscribe on YouTube,\n\n1:08:38.020 --> 1:08:41.660\n review it with 5,000 Apple Podcast, support on Patreon,\n\n1:08:41.660 --> 1:08:43.980\n or connect with me on Twitter at Lex Friedman,\n\n1:08:43.980 --> 1:08:48.940\n spelled without the E, just F R I D M A N.\n\n1:08:48.940 --> 1:08:50.860\n And now, let me leave you with some words\n\n1:08:50.860 --> 1:08:54.940\n from Peter Singer, what one generation finds ridiculous,\n\n1:08:54.940 --> 1:08:59.020\n the next accepts, and the third shudders\n\n1:08:59.020 --> 1:09:01.100\n when looks back at what the first did.\n\n1:09:01.100 --> 1:09:28.100\n Thank you for listening, and hope to see you next time.\n\n"
}