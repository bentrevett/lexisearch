{
  "title": "Leslie Kaelbling: Reinforcement Learning, Planning, and Robotics | Lex Fridman Podcast #15",
  "id": "Er7Dy8rvqOc",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:05.360\n The following is a conversation with Leslie Kaelbling. She is a roboticist and professor at\n\n00:05.360 --> 00:12.080\n MIT. She is recognized for her work in reinforcement learning, planning, robot navigation, and several\n\n00:12.080 --> 00:18.560\n other topics in AI. She won the IJCAI Computers and Thought Award and was the editor in chief\n\n00:18.560 --> 00:24.320\n of the prestigious Journal of Machine Learning Research. This conversation is part of the\n\n00:24.320 --> 00:30.400\n Artificial Intelligence podcast at MIT and beyond. If you enjoy it, subscribe on YouTube,\n\n00:30.400 --> 00:36.240\n iTunes, or simply connect with me on Twitter at Lex Friedman, spelled F R I D.\n\n00:36.960 --> 00:41.040\n And now, here's my conversation with Leslie Kaelbling.\n\n00:42.800 --> 00:47.680\n What made me get excited about AI, I can say that, is I read G\u00f6del Escher Bach when I was\n\n00:47.680 --> 00:56.240\n in high school. That was pretty formative for me because it exposed the interestingness of\n\n00:57.200 --> 01:01.520\n primitives and combination and how you can make complex things out of simple parts\n\n01:02.320 --> 01:07.760\n and ideas of AI and what kinds of programs might generate intelligent behavior. So...\n\n01:07.760 --> 01:12.720\n So you first fell in love with AI reasoning logic versus robots?\n\n01:12.720 --> 01:18.160\n Yeah, the robots came because my first job, so I finished an undergraduate degree in philosophy\n\n01:18.160 --> 01:24.160\n at Stanford and was about to finish a master's in computer science. And I got hired at SRI\n\n01:25.360 --> 01:30.960\n in their AI lab and they were building a robot. It was a kind of a follow on to shaky,\n\n01:30.960 --> 01:35.840\n but all the shaky people were not there anymore. And so my job was to try to get this robot to\n\n01:35.840 --> 01:39.280\n do stuff. And that's really kind of what got me interested in robots.\n\n01:39.280 --> 01:44.400\n So maybe taking a small step back to your bachelor's in Stanford in philosophy,\n\n01:44.400 --> 01:49.440\n did master's and PhD in computer science, but the bachelor's in philosophy. So what was that\n\n01:49.440 --> 01:55.200\n journey like? What elements of philosophy do you think you bring to your work in computer science?\n\n01:55.200 --> 01:59.840\n So it's surprisingly relevant. So the part of the reason that I didn't do a computer\n\n01:59.840 --> 02:03.440\n science undergraduate degree was that there wasn't one at Stanford at the time,\n\n02:03.440 --> 02:07.360\n but that there's a part of philosophy and in fact, Stanford has a special submajor in\n\n02:07.360 --> 02:13.280\n something called now symbolic systems, which is logic, model theory, formal semantics of\n\n02:13.280 --> 02:20.080\n natural language. And so that's actually a perfect preparation for work in AI and computer science.\n\n02:20.080 --> 02:23.840\n That's kind of interesting. So if you were interested in artificial intelligence,\n\n02:26.000 --> 02:30.800\n what kind of majors were people even thinking about taking? What is it in your science?\n\n02:31.840 --> 02:37.120\n So besides philosophies, what were you supposed to do if you were fascinated by the idea of creating\n\n02:37.120 --> 02:41.920\n intelligence? There weren't enough people who did that for that even to be a conversation.\n\n02:41.920 --> 02:47.600\n I mean, I think probably, probably philosophy. I mean, it's interesting in my class,\n\n02:48.320 --> 02:56.800\n my graduating class of undergraduate philosophers, probably maybe slightly less than half went on in\n\n02:56.800 --> 03:02.320\n computer science, slightly less than half went on in law and like one or two went on in philosophy.\n\n03:02.320 --> 03:06.960\n So it was a common kind of connection. Do you think AI researchers have a role\n\n03:06.960 --> 03:11.440\n to be part time philosophers or should they stick to the solid science and engineering\n\n03:11.440 --> 03:16.240\n without sort of taking the philosophizing tangents? I mean, you work with robots,\n\n03:16.240 --> 03:21.200\n you think about what it takes to create intelligent beings. Aren't you the perfect\n\n03:21.200 --> 03:25.920\n person to think about the big picture philosophy at all? The parts of philosophy that are closest\n\n03:25.920 --> 03:29.680\n to AI, I think, or at least the closest to AI that I think about are stuff like\n\n03:29.680 --> 03:35.040\n belief and knowledge and denotation and that kind of stuff. And that's, you know,\n\n03:35.040 --> 03:40.800\n it's quite formal. And it's like just one step away from the kinds of computer science work that\n\n03:40.800 --> 03:50.160\n we do kind of routinely. I think that there are important questions still about what you can do\n\n03:50.160 --> 03:54.560\n with a machine and what you can't and so on. Although at least my personal view is that I'm\n\n03:54.560 --> 04:00.080\n completely a materialist. And I don't think that there's any reason why we can't make a robot be\n\n04:00.960 --> 04:04.880\n behaviorally indistinguishable from a human. And the question of whether it's\n\n04:06.560 --> 04:11.680\n distinguishable internally, whether it's a zombie or not in philosophy terms, I actually don't,\n\n04:12.720 --> 04:15.280\n I don't know. And I don't know if I care too much about that.\n\n04:15.280 --> 04:20.400\n Right. But there is a philosophical notions. They're mathematical and philosophical because\n\n04:20.400 --> 04:25.600\n we don't know so much of how difficult it is. How difficult is the perception problem?\n\n04:25.600 --> 04:30.800\n How difficult is the planning problem? How difficult is it to operate in this world successfully?\n\n04:30.800 --> 04:35.920\n Because our robots are not currently as successful as human beings in many tasks.\n\n04:35.920 --> 04:42.480\n The question about the gap between current robots and human beings borders a little bit\n\n04:42.480 --> 04:50.160\n on philosophy. You know, the expanse of knowledge that's required to operate in a human world,\n\n04:50.160 --> 04:55.520\n required to operate in this world and the ability to form common sense knowledge, the ability to\n\n04:55.520 --> 05:01.520\n reason about uncertainty. Much of the work you've been doing, there's open questions there that,\n\n05:02.880 --> 05:07.280\n I don't know, required to activate a certain big picture view.\n\n05:07.840 --> 05:12.960\n To me, that doesn't seem like a philosophical gap at all. To me, there is a big technical gap.\n\n05:12.960 --> 05:19.360\n There's a huge technical gap, but I don't see any reason why it's more than a technical gap.\n\n05:19.360 --> 05:28.400\n Perfect. So, when you mentioned AI, you mentioned SRI, and maybe can you describe to me when you\n\n05:28.400 --> 05:37.680\n first fell in love with robotics, with robots or inspired, so you mentioned Flaky or Shaky Flaky,\n\n05:38.400 --> 05:42.800\n and what was the robot that first captured your imagination, what's possible?\n\n05:42.800 --> 05:47.120\n Right. Well, so the first robot I worked with was Flaky. Shaky was a robot that the SRI\n\n05:47.120 --> 05:52.960\n people had built, but by the time, I think when I arrived, it was sitting in a corner of somebody's\n\n05:52.960 --> 06:00.240\n office dripping hydraulic fluid into a pan, but it's iconic and really everybody should read the\n\n06:00.240 --> 06:06.560\n Shaky Tech Report because it has so many good ideas in it. I mean, they invented ASTAR search\n\n06:06.560 --> 06:14.240\n and symbolic planning and learning macro operators. They had low level kind of\n\n06:14.240 --> 06:19.360\n configuration space planning for their robot. They had vision. That's the basic ideas of\n\n06:19.360 --> 06:26.240\n a ton of things. Can you take a step back? Shaky have arms. What was the job? Shaky was a mobile\n\n06:26.240 --> 06:32.080\n robot, but it could push objects, and so it would move things around. With which actuator? With\n\n06:32.080 --> 06:43.600\n itself, with its base. Okay, great. And they had painted the baseboards black, so it used vision\n\n06:43.600 --> 06:49.360\n to localize itself in a map. It detected objects. It could detect objects that were surprising to\n\n06:49.360 --> 06:55.520\n it. It would plan and replan based on what it saw. It reasoned about whether to look and take\n\n06:55.520 --> 07:02.240\n pictures. I mean, it really had the basics of so many of the things that we think about now.\n\n07:03.280 --> 07:08.960\n How did it represent the space around it? So it had representations at a bunch of different levels\n\n07:08.960 --> 07:13.920\n of abstraction. So it had, I think, a kind of an occupancy grid of some sort at the lowest level.\n\n07:14.880 --> 07:21.440\n At the high level, it was abstract symbolic kind of rooms and connectivity. So where does flaky\n\n07:21.440 --> 07:28.240\n come in? Yeah, okay. So I showed up at SRI and we were building a brand new robot. As I said,\n\n07:28.240 --> 07:33.200\n none of the people from the previous project were kind of there or involved anymore. So we were kind\n\n07:33.200 --> 07:40.880\n of starting from scratch and my advisor was Stan Rosenstein. He ended up being my thesis advisor\n\n07:40.880 --> 07:48.000\n and he was motivated by this idea of situated computation or situated automata. And the idea was\n\n07:49.600 --> 07:58.480\n that the tools of logical reasoning were important, but possibly only for the engineers\n\n07:58.480 --> 08:04.560\n or designers to use in the analysis of a system, but not necessarily to be manipulated in the head\n\n08:04.560 --> 08:09.920\n of the system itself. So I might use logic to prove a theorem about the behavior of my robot,\n\n08:10.480 --> 08:14.400\n even if the robot's not using logic in its head to prove theorems. So that was kind of the\n\n08:14.400 --> 08:23.440\n distinction. And so the idea was to kind of use those principles to make a robot do stuff. But\n\n08:23.440 --> 08:29.600\n a lot of the basic things we had to kind of learn for ourselves because I had zero background in\n\n08:29.600 --> 08:33.600\n robotics. I didn't know anything about control. I didn't know anything about sensors. So we\n\n08:33.600 --> 08:37.280\n reinvented a lot of wheels on the way to getting that robot to do stuff. Do you think that was\n\n08:37.280 --> 08:44.800\n an advantage or a hindrance? Oh no, I mean, I'm big in favor of wheel reinvention actually. I mean,\n\n08:44.800 --> 08:49.680\n I think you learn a lot by doing it. It's important though to eventually have the pointers\n\n08:49.680 --> 08:56.640\n to, so that you can see what's really going on. But I think you can appreciate much better the\n\n08:56.640 --> 09:00.400\n good solutions once you've messed around a little bit on your own and found a bad one.\n\n09:00.400 --> 09:04.880\n Yeah. I think you mentioned reinventing reinforcement learning and referring to\n\n09:04.880 --> 09:11.440\n rewards as pleasures, pleasure. Yeah. Or I think, which I think is a nice name for it.\n\n09:11.440 --> 09:18.960\n Yeah. It's more fun almost. Do you think you could tell the history of AI machine learning\n\n09:18.960 --> 09:23.600\n reinforcement learning and how you think about it from the fifties to now?\n\n09:23.600 --> 09:29.360\n One thing is that it's oscillates, right? So things become fashionable and then they go out\n\n09:29.360 --> 09:33.680\n and then something else becomes cool and that goes out and so on. And I think there's, so there's\n\n09:33.680 --> 09:38.160\n some interesting sociological process that actually drives a lot of what's going on.\n\n09:38.880 --> 09:46.240\n Early days was kind of cybernetics and control, right? And the idea that of homeostasis,\n\n09:46.240 --> 09:51.680\n right? People have made these robots that could, I don't know, try to plug into the wall when they\n\n09:51.680 --> 09:59.200\n needed power and then come loose and roll around and do stuff. And then I think over time, the\n\n09:59.200 --> 10:03.200\n thought, well, that was inspiring, but people said, no, no, no, we want to get maybe closer to what\n\n10:03.200 --> 10:10.160\n feels like real intelligence or human intelligence. And then maybe the expert systems people tried\n\n10:10.160 --> 10:20.240\n to do that, but maybe a little too superficially, right? So, oh, we get the surface understanding of\n\n10:20.240 --> 10:24.960\n what intelligence is like, because I understand how a steel mill works and I can try to explain\n\n10:24.960 --> 10:28.480\n it to you and you can write it down in logic and then we can make a computer and for that.\n\n10:29.200 --> 10:36.400\n And then that didn't work out. But what's interesting, I think, is when a thing starts to not\n\n10:36.400 --> 10:43.200\n be working very well, it's not only do we change methods, we change problems, right? So it's not\n\n10:43.200 --> 10:47.040\n like we have better ways of doing the problem of the expert systems people were trying to do. We\n\n10:47.040 --> 10:54.880\n have no ways of trying to do that problem. Oh, yeah, no, I think maybe a few, but we kind of\n\n10:54.880 --> 11:00.720\n give up on that problem and we switched to a different problem and we worked that for a while\n\n11:00.720 --> 11:04.320\n and we make progress. As a broad community. As a community, yeah. And there's a lot of people who\n\n11:04.320 --> 11:09.520\n would argue, you don't give up on the problem, it's just you decrease the number of people working\n\n11:09.520 --> 11:13.920\n on it. You almost kind of like put it on the shelf, say, we'll come back to this 20 years later.\n\n11:13.920 --> 11:19.360\n Yeah, I think that's right. Or you might decide that it's malformed. Like you might say,\n\n11:21.600 --> 11:26.160\n it's wrong to just try to make something that does superficial symbolic reasoning\n\n11:26.160 --> 11:33.200\n behave like a doctor. You can't do that until you've had the sensory motor experience of being\n\n11:33.200 --> 11:38.320\n a doctor or something. So there's arguments that say that that problem was not well formed. Or it\n\n11:38.320 --> 11:43.760\n could be that it is well formed, but we just weren't approaching it well. So you mentioned\n\n11:43.760 --> 11:48.800\n that your favorite part of logic and symbolic systems is that they give short names for large\n\n11:48.800 --> 11:56.400\n sets. So there is some use to this. They use symbolic reasoning. So looking at expert systems\n\n11:56.960 --> 12:01.680\n and symbolic computing, what do you think are the roadblocks that were hit in the 80s and 90s?\n\n12:01.680 --> 12:07.920\n Ah, okay. So right. So the fact that I'm not a fan of expert systems doesn't mean that I'm not a\n\n12:07.920 --> 12:15.520\n fan of some kinds of symbolic reasoning, right? So let's see, roadblocks. Well, the main road\n\n12:15.520 --> 12:21.280\n block, I think, was that the idea that humans could articulate their knowledge effectively\n\n12:22.080 --> 12:26.240\n into some kind of logical statements.\n\n12:26.240 --> 12:31.280\n So it's not just the cost, the effort, but really just the capability of doing it.\n\n12:31.280 --> 12:36.720\n Right. Because we're all experts in vision, right? But totally don't have introspective\n\n12:36.720 --> 12:44.960\n access into how we do that. Right. And it's true that, I mean, I think the idea was, well,\n\n12:44.960 --> 12:48.240\n of course, even people then would know, of course, I wouldn't ask you to please write\n\n12:48.240 --> 12:52.800\n down the rules that you use for recognizing a water bottle. That's crazy. And everyone\n\n12:52.800 --> 12:58.640\n understood that. But we might ask you to please write down the rules you use for deciding,\n\n12:58.640 --> 13:04.640\n I don't know, what tie to put on or how to set up a microphone or something like that.\n\n13:04.640 --> 13:10.880\n But even those things, I think people maybe, I think what they found, I'm not sure about\n\n13:10.880 --> 13:16.000\n this, but I think what they found was that the so called experts could give explanations\n\n13:16.000 --> 13:20.080\n that sort of post hoc explanations for how and why they did things, but they weren't\n\n13:20.080 --> 13:28.960\n necessarily very good. And then they depended on maybe some kinds of perceptual things,\n\n13:28.960 --> 13:35.840\n which again, they couldn't really define very well. So I think fundamentally, I think the\n\n13:35.840 --> 13:40.960\n underlying problem with that was the assumption that people could articulate how and why they\n\n13:40.960 --> 13:45.680\n make their decisions. Right. So it's almost encoding the knowledge\n\n13:45.680 --> 13:51.440\n from converting from expert to something that a machine could understand and reason with.\n\n13:51.440 --> 13:54.560\n No, no, no, no, not even just encoding, but getting it out of you.\n\n13:56.320 --> 14:01.520\n Right. Not, not, not writing it. I mean, yes, hard also to write it down for the computer,\n\n14:02.240 --> 14:08.320\n but I don't think that people can produce it. You can tell me a story about why you do stuff,\n\n14:08.320 --> 14:14.400\n but I'm not so sure that's the why. Great. So there are still on the\n\n14:14.400 --> 14:24.560\n hierarchical planning side, places where symbolic reasoning is very useful. So as you've talked\n\n14:24.560 --> 14:34.960\n about, so where's the gap? Yeah. Okay, good. So saying that humans can't provide a description\n\n14:34.960 --> 14:41.040\n of their reasoning processes. That's okay. Fine. But that doesn't mean that it's not good to do\n\n14:41.040 --> 14:47.120\n reasoning of various styles inside a computer. Those are just two orthogonal points. So then\n\n14:47.120 --> 14:50.720\n the question is what kind of reasoning should you do inside a computer? Right.\n\n14:52.240 --> 14:56.240\n And the answer is, I think you need to do all different kinds of reasoning inside a computer,\n\n14:56.240 --> 15:02.400\n depending on what kinds of problems you face. I guess the question is what kind of things can you\n\n15:02.400 --> 15:12.480\n encode symbolically so you can reason about? I think the idea about, and even symbolic,\n\n15:12.480 --> 15:17.040\n I don't even like that terminology because I don't know what it means technically and formally.\n\n15:17.760 --> 15:24.160\n I do believe in abstractions. So abstractions are critical, right? You cannot reason at completely\n\n15:24.160 --> 15:29.520\n fine grain about everything in your life, right? You can't make a plan at the level of images and\n\n15:29.520 --> 15:36.960\n torques for getting a PhD. So you have to reduce the size of the state space and you have to reduce\n\n15:36.960 --> 15:42.160\n the horizon if you're going to reason about getting a PhD or even buying the ingredients to\n\n15:42.160 --> 15:49.280\n make dinner. And so how can you reduce the spaces and the horizon of the reasoning you have to do?\n\n15:49.280 --> 15:53.760\n And the answer is abstraction, spatial abstraction, temporal abstraction. I think abstraction along\n\n15:53.760 --> 16:00.480\n the lines of goals is also interesting, like you might, well, abstraction and decomposition. Goals\n\n16:00.480 --> 16:05.600\n is maybe more of a decomposition thing. So I think that's where these kinds of, if you want to call\n\n16:05.600 --> 16:12.800\n it symbolic or discrete models come in. You talk about a room of your house instead of your pose.\n\n16:12.800 --> 16:20.800\n You talk about doing something during the afternoon instead of at 2.54. And you do that because it\n\n16:20.800 --> 16:25.200\n and you do that because it makes your reasoning problem easier. And also because\n\n16:27.120 --> 16:34.800\n you have, you don't have enough information to reason in high fidelity about your pose of your\n\n16:34.800 --> 16:39.520\n elbow at 2.35 this afternoon anyway. Right. When you're trying to get a PhD.\n\n16:39.520 --> 16:42.000\n Or when you're doing anything really. Yeah. Okay.\n\n16:42.720 --> 16:46.080\n Except for at that moment, at that moment, you do have to reason about the pose of your elbow,\n\n16:46.080 --> 16:50.000\n maybe, but then you, maybe you do that in some continuous joint space kind of model.\n\n16:50.000 --> 16:57.840\n And so again, I, my biggest point about all of this is that there should be the dogma is not\n\n16:58.640 --> 17:02.880\n the thing, right? We shouldn't, it shouldn't be that I'm in favor against symbolic reasoning\n\n17:02.880 --> 17:08.800\n and you're in favor against neural networks. It should be that just, just computer science\n\n17:08.800 --> 17:12.560\n tells us what the right answer to all these questions is. If we were smart enough to figure\n\n17:12.560 --> 17:17.200\n it out. Well, yeah. When you try to actually solve the problem with computers, the right answer comes\n\n17:17.200 --> 17:22.880\n out. But you mentioned abstractions. I mean, neural networks form abstractions or rather\n\n17:24.480 --> 17:28.960\n there's, there's automated ways to form abstractions and there's expert driven ways to\n\n17:28.960 --> 17:35.280\n form abstractions and expert human driven ways. And humans just seem to be way better at forming\n\n17:35.280 --> 17:44.000\n abstractions currently and certain problems. So when you're referring to 2.45 PM versus afternoon,\n\n17:44.000 --> 17:49.200\n how do we construct that taxonomy? Is there any room for automated construction of such\n\n17:50.080 --> 17:54.400\n abstractions? Oh, I think eventually, yeah. I mean, I think when we get to be better\n\n17:55.280 --> 18:01.120\n and machine learning engineers, we'll build algorithms that build awesome abstractions.\n\n18:01.120 --> 18:05.760\n That are useful in this kind of way that you're describing. Yeah. So let's then step from\n\n18:05.760 --> 18:14.800\n the, the abstraction discussion and let's talk about POMM MDPs. Partially observable\n\n18:14.800 --> 18:20.080\n Markov decision processes. So uncertainty. So first, what are Markov decision processes?\n\n18:20.080 --> 18:26.320\n What are Markov decision processes? And maybe how much of our world can be models and MDPs? How\n\n18:26.320 --> 18:30.160\n much, when you wake up in the morning and you're making breakfast, how do you, do you think of\n\n18:30.160 --> 18:36.640\n yourself as an MDP? So how do you think about MDPs and how they relate to our world? Well, so\n\n18:36.640 --> 18:41.520\n there's a stance question, right? So a stance is a position that I take with respect to a problem.\n\n18:42.240 --> 18:50.720\n So I, as a researcher or a person who designs systems, can decide to make a model of the world\n\n18:50.720 --> 18:57.600\n around me in some terms. So I take this messy world and I say, I'm going to treat it as if it\n\n18:57.600 --> 19:02.960\n were a problem of this formal kind, and then I can apply solution concepts or algorithms or whatever\n\n19:02.960 --> 19:07.920\n to solve that formal thing, right? So of course the world is not anything. It's not an MDP or a\n\n19:07.920 --> 19:12.880\n POMM DP. I don't know what it is, but I can model aspects of it in some way or some other way.\n\n19:12.880 --> 19:17.600\n And when I model some aspect of it in a certain way, that gives me some set of algorithms I can\n\n19:17.600 --> 19:26.160\n use. You can model the world in all kinds of ways. Some have, some are, some are, some are\n\n19:26.160 --> 19:33.360\n more accepting of uncertainty, more easily modeling uncertainty of the world. Some really force the\n\n19:33.360 --> 19:42.000\n world to be deterministic. And so certainly MDPs model the uncertainty of the world. Yes. Model\n\n19:42.000 --> 19:47.520\n some uncertainty. They model not present state uncertainty, but they model uncertainty in the\n\n19:47.520 --> 19:54.560\n way the future will unfold. Right. So what are Markov decision processes? So Markov decision\n\n19:54.560 --> 19:59.760\n process is a model. It's a kind of a model that you could make that says, I know completely the\n\n19:59.760 --> 20:05.760\n current state of my system. And what it means to be a state is that I, that all the, I have all\n\n20:05.760 --> 20:11.120\n the information right now that will let me make predictions about the future as well as I can.\n\n20:11.120 --> 20:14.720\n So that remembering anything about my history wouldn't make my predictions any better.\n\n20:17.760 --> 20:23.280\n And, but then it also says that then I can take some actions that might change the state of the\n\n20:23.280 --> 20:28.400\n world. And that I don't have a deterministic model of those changes. I have a probabilistic\n\n20:28.400 --> 20:34.160\n model of how the world might change. It's a, it's a useful model for some kinds of systems.\n\n20:34.160 --> 20:42.480\n I think it's a, I mean, it's certainly not a good model for most problems, I think, because for most\n\n20:42.480 --> 20:48.720\n problems you don't actually know the state. For most problems you, it's partially observed. So\n\n20:48.720 --> 20:55.600\n that's now a different problem class. So, okay. That's where the POMDPs, the part that we observe\n\n20:55.600 --> 21:01.360\n with the Markov decision processes step in. So how do they address the fact that you can't\n\n21:01.360 --> 21:07.760\n observe most incomplete information about most of the world around you? Right. So now the idea is\n\n21:07.760 --> 21:12.880\n we still kind of postulate that there exists a state. We think that there is some information\n\n21:12.880 --> 21:18.000\n about the world out there such that if we knew that we could make good predictions, but we don't\n\n21:18.000 --> 21:23.680\n know the state. And so then we have to think about how, but we do get observations. Maybe I get\n\n21:23.680 --> 21:29.680\n images or I hear things or I feel things, and those might be local or noisy. And so therefore\n\n21:29.680 --> 21:34.720\n they don't tell me everything about what's going on. And then I have to reason about given the\n\n21:34.720 --> 21:39.920\n history of actions I've taken and observations I've gotten, what do I think is going on in the\n\n21:39.920 --> 21:43.760\n world? And then given my own kind of uncertainty about what's going on in the world, I can decide\n\n21:43.760 --> 21:50.080\n what actions to take. And so how difficult is this problem of planning under uncertainty in your\n\n21:50.080 --> 21:57.840\n view and your long experience of modeling the world, trying to deal with this uncertainty in\n\n21:57.840 --> 22:05.040\n especially in real world systems? Optimal planning for even discrete POMDPs can be undecidable\n\n22:05.040 --> 22:12.960\n depending on how you set it up. And so lots of people say, I don't use POMDPs because they are\n\n22:12.960 --> 22:19.600\n intractable. And I think that that's kind of a very funny thing to say because the problem you\n\n22:19.600 --> 22:24.320\n have to solve is the problem you have to solve. So if the problem you have to solve is intractable,\n\n22:24.320 --> 22:28.720\n that's what makes us AI people, right? So we solve, we understand that the problem we're\n\n22:28.720 --> 22:34.320\n solving is wildly intractable that we can't, we will never be able to solve it optimally,\n\n22:34.320 --> 22:41.360\n at least I don't. Yeah, right. So later we can come back to an idea about bounded optimality\n\n22:41.360 --> 22:44.960\n and something. But anyway, we can't come up with optimal solutions to these problems.\n\n22:45.520 --> 22:50.640\n So we have to make approximations, approximations in modeling, approximations in the solution\n\n22:50.640 --> 22:56.960\n algorithms and so on. And so I don't have a problem with saying, yeah, my problem actually,\n\n22:56.960 --> 23:02.480\n it is POMDP in continuous space with continuous observations. And it's so computationally complex,\n\n23:02.480 --> 23:09.600\n I can't even think about it's, you know, big O whatever. But that doesn't prevent me from,\n\n23:09.600 --> 23:16.160\n it helps me, gives me some clarity to think about it that way and to then take steps to\n\n23:16.160 --> 23:20.880\n make approximation after approximation to get down to something that's like computable\n\n23:20.880 --> 23:26.720\n in some reasonable time. When you think about optimality, the community broadly has shifted on\n\n23:26.720 --> 23:34.880\n that, I think a little bit in how much they value the idea of optimality, of chasing an optimal\n\n23:34.880 --> 23:40.960\n solution. How has your views of chasing an optimal solution changed over the years when\n\n23:40.960 --> 23:48.880\n you work with robots? That's interesting. I think we have a little bit of a methodological crisis\n\n23:48.880 --> 23:53.520\n actually from the theoretical side. I mean, I do think that theory is important and that right now\n\n23:53.520 --> 24:00.080\n we're not doing much of it. So there's lots of empirical hacking around and training this and\n\n24:00.080 --> 24:05.120\n doing that and reporting numbers, but is it good? Is it bad? We don't know. It's very hard to say\n\n24:05.120 --> 24:16.400\n things. And if you look at like computer science theory, so people talked for a while, everyone was\n\n24:16.400 --> 24:22.320\n about solving problems optimally or completely. And then there were interesting relaxations. So\n\n24:22.320 --> 24:30.640\n people look at, oh, are there regret bounds or can I do some kind of approximation? Can I prove\n\n24:30.640 --> 24:34.880\n something that I can approximately solve this problem or that I get closer to the solution as\n\n24:34.880 --> 24:42.560\n I spend more time and so on? What's interesting I think is that we don't have good approximate\n\n24:42.560 --> 24:51.520\n solution concepts for very difficult problems. I like to say that I'm interested in doing a very\n\n24:51.520 --> 25:00.400\n bad job of very big problems. Right. So very bad job, very big problems. I like to do that,\n\n25:00.400 --> 25:09.120\n but I wish I could say something. I wish I had a, I don't know, some kind of a formal solution\n\n25:09.120 --> 25:16.240\n concept that I could use to say, oh, this algorithm actually, it gives me something.\n\n25:16.240 --> 25:19.840\n Like I know what I'm going to get. I can do something other than just run it and get out.\n\n25:19.840 --> 25:26.240\n So that, that notion is still somewhere deeply compelling to you. The notion that you can say,\n\n25:27.520 --> 25:32.400\n you can drop thing on the table says this, you can expect this, this algorithm will\n\n25:32.400 --> 25:36.400\n give me some good results. I hope there's, I hope science will, I mean,\n\n25:37.280 --> 25:40.880\n there's engineering and there's science. I think that they're not exactly the same.\n\n25:42.320 --> 25:47.040\n And I think right now we're making huge engineering, like leaps and bounds. So the\n\n25:47.040 --> 25:52.240\n engineering is running away ahead of the science, which is cool. And often how it goes, right? So\n\n25:52.240 --> 25:59.680\n we're making things and nobody knows how and why they work roughly, but we need to turn that into\n\n25:59.680 --> 26:05.440\n science. There's some form. It's a, yeah, there's some room for formalizing. We need to know what\n\n26:05.440 --> 26:09.840\n the principles are. Why does this work? Why does that not work? I mean, for a while, people built\n\n26:09.840 --> 26:14.480\n bridges by trying, but now we can often predict whether it's going to work or not without building\n\n26:14.480 --> 26:20.640\n it. Can we do that for learning systems or for robots? So your hope is from a materialistic\n\n26:20.640 --> 26:27.600\n perspective that intelligence, artificial intelligence systems, robots are just fancier\n\n26:27.600 --> 26:33.040\n bridges. Belief space. What's the difference between belief space and state space? So you\n\n26:33.040 --> 26:39.840\n mentioned MDPs, FOMDPs, reasoning about, you sense the world, there's a state.\n\n26:39.840 --> 26:44.400\n Uh, what, what's this belief space idea? That sounds so good.\n\n26:44.400 --> 26:51.600\n It sounds good. So belief space, that is instead of thinking about what's the state of the world\n\n26:51.600 --> 26:58.880\n and trying to control that as a robot, I think about what is the space of beliefs that I could\n\n26:58.880 --> 27:03.520\n have about the world. What's, if I think of a belief as a probability distribution of our ways\n\n27:03.520 --> 27:10.080\n the world could be, a belief state is a distribution. And then my control problem, if I'm reasoning\n\n27:10.080 --> 27:16.160\n about how to move through a world I'm uncertain about, my control problem is actually the problem\n\n27:16.160 --> 27:21.360\n of controlling my beliefs. So I think about taking actions, not just what effect they'll have on the\n\n27:21.360 --> 27:26.080\n world outside, but what effect they'll have on my own understanding of the world outside. And so\n\n27:26.080 --> 27:32.800\n that might compel me to ask a question or look somewhere to gather information, which may not\n\n27:32.800 --> 27:38.400\n really change the world state, but it changes my own belief about the world. That's a powerful way\n\n27:38.400 --> 27:46.320\n to, to empower the agent, to reason about the world, to explore the world. So what kind of\n\n27:46.320 --> 27:51.680\n problems does it allow you to solve to, to consider belief space versus just state space?\n\n27:52.400 --> 27:58.400\n Well, any problem that requires deliberate information gathering, right? So if in some\n\n27:58.400 --> 28:04.240\n problems like chess, there's no uncertainty, or maybe there's uncertainty about the opponent,\n\n28:05.040 --> 28:10.320\n there's no uncertainty about the state. And some problems, there's uncertainty,\n\n28:10.320 --> 28:16.080\n but you gather information as you go, right? You might say, Oh, I'm driving my autonomous car down\n\n28:16.080 --> 28:20.000\n the road and it doesn't know perfectly where it is, but the light hours are all going all the time.\n\n28:20.560 --> 28:25.360\n So I don't have to think about whether to gather information. But if you're a human driving down\n\n28:25.360 --> 28:30.480\n the road, you sometimes look over your shoulder to see what's going on behind you in the lane.\n\n28:31.360 --> 28:37.840\n And you have to decide whether you should do that now. And you have to trade off the fact that\n\n28:37.840 --> 28:41.520\n you're not seeing in front of you and you're looking behind you and how valuable is that\n\n28:41.520 --> 28:47.200\n information and so on. And so to make choices about information gathering, you have to reasonably\n\n28:47.200 --> 28:56.800\n space. Also, I mean, also to just take into account your own uncertainty before trying to\n\n28:56.800 --> 29:03.840\n do things. So you might say, if I understand where I'm standing relative to the door jam,\n\n29:05.360 --> 29:08.640\n pretty accurately, then it's okay for me to go through the door. But if I'm really\n\n29:08.640 --> 29:12.960\n not sure where the door is, then it might be better to not do that right now.\n\n29:12.960 --> 29:17.760\n The degree of your uncertainty about the world is actually part of the thing you're trying to\n\n29:17.760 --> 29:25.760\n optimize in forming the plan, right? So this idea of a long horizon of planning for a PhD or just\n\n29:25.760 --> 29:31.920\n even how to get out of the house or how to make breakfast. You show this presentation of the WTF,\n\n29:31.920 --> 29:40.640\n where's the fork of robot looking at a sink. And can you describe how we plan in this world\n\n29:40.640 --> 29:47.360\n of this idea of hierarchical planning we've mentioned? So yeah, how can a robot hope to\n\n29:47.360 --> 29:53.840\n plan about something with such a long horizon where the goal is quite far away?\n\n29:54.480 --> 29:59.840\n People since probably reasoning began have thought about hierarchical reasoning,\n\n29:59.840 --> 30:03.040\n the temporal hierarchy in particular. Well, there's spatial hierarchy, but let's talk\n\n30:03.040 --> 30:08.960\n about temporal hierarchy. So you might say, oh, I have this long execution I have to do,\n\n30:08.960 --> 30:15.360\n but I can divide it into some segments abstractly, right? So maybe you have to get out of the house,\n\n30:15.360 --> 30:22.720\n I have to get in the car, I have to drive and so on. And so you can plan if you can build\n\n30:22.720 --> 30:26.960\n abstractions. So this we started out by talking about abstractions. And we're back to that now,\n\n30:26.960 --> 30:34.560\n if you can build abstractions in your state space, and abstractions sort of temporal abstractions,\n\n30:34.560 --> 30:39.760\n then you can make plans at a high level. And you can say, I'm going to go to town and then I'll\n\n30:39.760 --> 30:43.520\n have to get gas and then I can go here and I can do this other thing. And you can reason about the\n\n30:43.520 --> 30:49.440\n dependencies and constraints among these actions, again, without thinking about the complete\n\n30:50.080 --> 30:56.800\n details. What we do in our hierarchical planning work is then say, all right, I make a plan at a\n\n30:56.800 --> 31:03.760\n high level of abstraction, I have to have some reason to think that it's feasible without working\n\n31:03.760 --> 31:08.720\n it out in complete detail. And that's actually the interesting step. I always like to talk about\n\n31:08.720 --> 31:15.120\n walking through an airport, like you can plan to go to New York and arrive at the airport, and then\n\n31:15.120 --> 31:20.080\n find yourself an office building later. You can't even tell me in advance what your plan is for\n\n31:20.080 --> 31:24.960\n walking through the airport, partly because you're too lazy to think about it, maybe, but partly\n\n31:24.960 --> 31:28.800\n also because you just don't have the information, you don't know what gate you're landing in, or\n\n31:28.800 --> 31:34.080\n what people are going to be in front of you or anything. So there's no point in planning in\n\n31:34.080 --> 31:40.640\n detail, but you have to have, you have to make a leap of faith that you can figure it out once you\n\n31:40.640 --> 31:50.000\n get there. And it's really interesting to me how you arrive at that. How do you, so you have learned\n\n31:50.000 --> 31:54.720\n over your lifetime to be able to make some kinds of predictions about how hard it is to achieve some\n\n31:54.720 --> 32:00.720\n kinds of sub goals. And that's critical. Like you would never plan to fly somewhere if you couldn't,\n\n32:00.720 --> 32:04.800\n didn't have a model of how hard it was to do some of the intermediate steps. So one of the things\n\n32:04.800 --> 32:12.480\n we're thinking about now is how do you do this kind of very aggressive generalization to situations\n\n32:12.480 --> 32:16.880\n that you haven't been in and so on to predict how long will it take to walk through the Kuala Lumpur\n\n32:16.880 --> 32:22.480\n airport. Like you could give me an estimate and it wouldn't be crazy. And you have to have an\n\n32:22.480 --> 32:27.280\n estimate of that in order to make plans that involve walking through the Kuala Lumpur airport,\n\n32:27.280 --> 32:32.480\n even if you don't need to know it in detail. So I'm really interested in these kinds of abstract\n\n32:32.480 --> 32:37.280\n models and how do we acquire them. But once we have them, we can use them to do hierarchical\n\n32:37.280 --> 32:43.280\n reasoning, which is, I think is very important. Yeah. There's this notion of goal regression and\n\n32:43.280 --> 32:50.080\n preimage backchaining, this idea of starting at the goal and just forming these big clouds of\n\n32:50.080 --> 33:01.920\n states. I mean, it's almost like saying to the airport, you know, once you show up to the airport\n\n33:01.920 --> 33:08.160\n that you're like a few steps away from the goal. So like thinking of it this way, it's kind of\n\n33:08.160 --> 33:14.480\n interesting. I don't know if you have sort of further comments on that of starting at the goal.\n\n33:14.480 --> 33:22.000\n Yeah. I mean, it's interesting that Simon, Herb Simon back in the early days of AI talked a lot\n\n33:22.000 --> 33:26.480\n about means ends reasoning and reasoning back from the goal. There's a kind of an intuition that\n\n33:26.480 --> 33:34.480\n people have that the number of that state space is big. The number of actions you could take is\n\n33:34.480 --> 33:39.120\n really big. So if you say, here I sit and I want to search forward from where I am, what are all\n\n33:39.120 --> 33:44.720\n the things I could do? That's just overwhelming. If you say, if you can reason at this other level\n\n33:44.720 --> 33:49.520\n and say, here's what I'm hoping to achieve, what could I do to make that true? That somehow the\n\n33:49.520 --> 33:54.000\n branching is smaller. Now what's interesting is that like in the AI planning community,\n\n33:54.880 --> 33:59.120\n that hasn't worked out in the class of problems that they look at and the methods that they tend\n\n33:59.120 --> 34:04.560\n to use. It hasn't turned out that it's better to go backward. It's still kind of my intuition that\n\n34:04.560 --> 34:10.000\n it is, but I can't prove that to you right now. Right. I share your intuition, at least for us\n\n34:10.720 --> 34:21.120\n mere humans. Speaking of which, when you maybe now we take a little step into that philosophy circle.\n\n34:22.400 --> 34:28.080\n How hard would it, when you think about human life, you give those examples often. How hard do\n\n34:28.080 --> 34:33.360\n you think it is to formulate human life as a planning problem or aspects of human life? So\n\n34:33.360 --> 34:37.600\n when you look at robots, you're often trying to think about object manipulation,\n\n34:38.640 --> 34:46.240\n tasks about moving a thing. When you take a slight step outside the room, let the robot\n\n34:46.240 --> 34:54.480\n leave and go get lunch, or maybe try to pursue more fuzzy goals. How hard do you think is that\n\n34:54.480 --> 35:00.800\n problem? If you were to try to maybe put another way, try to formulate human life as a planning\n\n35:00.800 --> 35:05.760\n problem. Well, that would be a mistake. I mean, it's not all a planning problem, right? I think\n\n35:05.760 --> 35:11.920\n it's really, really important that we understand that you have to put together pieces and parts\n\n35:11.920 --> 35:18.080\n that have different styles of reasoning and representation and learning. I think it seems\n\n35:18.640 --> 35:25.680\n probably clear to anybody that it can't all be this or all be that. Brains aren't all like this\n\n35:25.680 --> 35:30.160\n or all like that, right? They have different pieces and parts and substructure and so on.\n\n35:30.160 --> 35:34.320\n So I don't think that there's any good reason to think that there's going to be like one true\n\n35:34.320 --> 35:40.720\n algorithmic thing that's going to do the whole job. So it's a bunch of pieces together designed\n\n35:40.720 --> 35:49.120\n to solve a bunch of specific problems. Or maybe styles of problems. I mean, there's probably some\n\n35:49.120 --> 35:57.360\n reasoning that needs to go on in image space. I think, again, there's this model based versus\n\n35:57.360 --> 36:01.200\n model free idea, right? So in reinforcement learning, people talk about, oh, should I learn,\n\n36:02.480 --> 36:08.400\n I could learn a policy, just straight up a way of behaving. I could learn it's popular\n\n36:08.400 --> 36:14.960\n on a value function. That's some kind of weird intermediate ground. Or I could learn a transition\n\n36:14.960 --> 36:20.240\n model, which tells me something about the dynamics of the world. If I take it, imagine that I learned\n\n36:20.240 --> 36:25.600\n a transition model and I couple it with a planner and I draw a box around that, I have a policy\n\n36:25.600 --> 36:32.800\n again. It's just stored a different way, right? But it's just as much of a policy as the other\n\n36:32.800 --> 36:39.520\n policy. It's just I've made, I think the way I see it is it's a time space trade off in computation,\n\n36:40.160 --> 36:46.160\n right? A more overt policy representation. Maybe it takes more space, but maybe I can\n\n36:46.160 --> 36:51.200\n compute quickly what action I should take. On the other hand, maybe a very compact model of\n\n36:51.200 --> 36:57.120\n the world dynamics plus a planner lets me compute what action to take to just more slowly. There's\n\n36:57.120 --> 37:02.320\n no, I don't, I mean, I don't think there's no argument to be had. It's just like a question of\n\n37:02.320 --> 37:09.360\n what form of computation is best for us for the various sub problems. Right. So, and, and so like\n\n37:10.320 --> 37:16.000\n learning to do algebra manipulations for some reason is, I mean, that's probably gonna want\n\n37:16.000 --> 37:21.280\n naturally a sort of a different representation than writing a unicycle at the time constraints\n\n37:21.280 --> 37:27.760\n on the unicycle are serious. The space is maybe smaller. I don't know, but so I could be the more\n\n37:27.760 --> 37:36.000\n human size of falling in love, having a relationship that might be another, another style of how to\n\n37:36.000 --> 37:43.280\n model that. Yeah. Let's first solve the algebra and the object manipulation. What do you think\n\n37:43.280 --> 37:52.160\n is harder perception or planning perception? That's why understanding that's why. So what do you think\n\n37:52.160 --> 37:56.480\n is so hard about perception by understanding the world around you? Well, I mean, I think the big\n\n37:56.480 --> 38:08.160\n question is representational. Hugely the question is representation. So perception has made great\n\n38:08.160 --> 38:15.360\n strides lately, right? And we can classify images and we can play certain kinds of games and predict\n\n38:15.360 --> 38:23.520\n how to steer the car and all this sort of stuff. Um, I don't think we have a very good idea of\n\n38:24.800 --> 38:29.760\n what perception should deliver, right? So if you, if you believe in modularity, okay, there's,\n\n38:29.760 --> 38:38.000\n there's a very strong view which says we shouldn't build in any modularity. We should make a giant\n\n38:38.000 --> 38:43.440\n gigantic neural network, train it end to end to do the thing. And that's the best way forward.\n\n38:44.320 --> 38:51.440\n And it's hard to argue with that except on a sample complexity basis, right? So you might say,\n\n38:51.440 --> 38:55.280\n Oh, well if I want to do end to end reinforcement learning on this giant, giant neural network,\n\n38:55.280 --> 39:05.520\n it's going to take a lot of data and a lot of like broken robots and stuff. So then the only answer\n\n39:05.520 --> 39:11.760\n is to say, okay, we have to build something in, build in some structure or some bias. We know\n\n39:11.760 --> 39:15.760\n from theory of machine learning, the only way to cut down the sample complexity is to kind of cut\n\n39:15.760 --> 39:22.480\n down, somehow cut down the hypothesis space. You can do that by building in bias. There's all kinds\n\n39:22.480 --> 39:30.640\n of reasons to think that nature built bias into humans. Um, convolution is a bias, right? It's a\n\n39:30.640 --> 39:37.520\n very strong bias and it's a very critical bias. So my own view is that we should look for more\n\n39:37.520 --> 39:42.880\n things that are like convolution, but the address other aspects of reasoning, right? So convolution\n\n39:42.880 --> 39:46.960\n helps us a lot with a certain kind of spatial reasoning. That's quite close to the imaging.\n\n39:48.320 --> 39:56.880\n I think there's other ideas like that. Maybe some amount of forward search, maybe some notions of\n\n39:56.880 --> 40:02.080\n abstraction, maybe the notion that objects exist. Actually, I think that's pretty important. And a\n\n40:02.080 --> 40:07.440\n lot of people won't give you that to start with. Right? So almost like a convolution in the, uh,\n\n40:08.960 --> 40:13.840\n uh, in the object, semantic object space or some kind of, some kind of ideas in there.\n\n40:13.840 --> 40:17.760\n That's right. And people are starting like the graph, graph convolutions are an idea that are\n\n40:17.760 --> 40:25.840\n related to relation, relational representations. And so, so I think there are, so you, I've come\n\n40:25.840 --> 40:30.720\n I've come far field from perception, but I think, um, I think the thing that's going to make\n\n40:30.720 --> 40:36.240\n perception that kind of the next step is actually understanding better what it should produce.\n\n40:36.800 --> 40:40.640\n Right? So what are we going to do with the output of it? Right? It's fine when what we're going to\n\n40:40.640 --> 40:46.960\n do with the output is steer. It's less clear when we're just trying to make a one integrated\n\n40:46.960 --> 40:52.560\n intelligent agent, what should the output of perception be? We have no idea. And how should\n\n40:52.560 --> 40:57.920\n that hook up to the other stuff? We don't know. So I think the pressing question is,\n\n40:59.040 --> 41:02.960\n what kinds of structure can we build in that are like the moral equivalent of convolution\n\n41:03.520 --> 41:09.440\n that will make a really awesome superstructure that then learning can kind of progress on\n\n41:09.440 --> 41:13.840\n efficiently. I agree. Very compelling description of actually where we stand with the perception\n\n41:13.840 --> 41:19.120\n problem. You're teaching a course on embodied intelligence. What do you think it takes to\n\n41:19.120 --> 41:24.800\n build a robot with human level intelligence? I don't know if we knew we would do it.\n\n41:27.680 --> 41:34.240\n If you were to, I mean, okay. So do you think a robot needs to have a self awareness,\n\n41:36.000 --> 41:44.160\n consciousness, fear of mortality, or is it, is it simpler than that? Or is consciousness a simple\n\n41:44.160 --> 41:50.240\n thing? Like, do you think about these notions? I don't think much about consciousness. Even\n\n41:50.880 --> 41:55.840\n most philosophers who care about it will give you that you could have robots that are zombies,\n\n41:55.840 --> 42:00.320\n right? That behave like humans, but are not conscious. And I, at this moment would be happy\n\n42:00.320 --> 42:03.760\n enough with that. So I'm not really worried one way or the other. So the technical side,\n\n42:03.760 --> 42:09.920\n you're not thinking of the use of self awareness. Well, but I, okay, but then what does self\n\n42:09.920 --> 42:16.960\n awareness mean? I mean, that you need to have some part of the system that can observe other\n\n42:16.960 --> 42:20.560\n parts of the system and tell whether they're working well or not. That seems critical.\n\n42:21.200 --> 42:27.360\n So does that count as, I mean, does that count as self awareness or not? Well, it depends on whether\n\n42:27.360 --> 42:33.120\n you think that there's somebody at home who can articulate whether they're self aware. But clearly,\n\n42:33.120 --> 42:37.600\n if I have like, you know, some piece of code that's counting how many times this procedure gets\n\n42:37.600 --> 42:43.680\n executed, that's a kind of self awareness, right? So there's a big spectrum. It's clear you have to\n\n42:43.680 --> 42:48.160\n have some of it. Right. You know, we're quite far away in many dimensions, but is there a direction\n\n42:48.160 --> 42:54.720\n of research that's most compelling to you for, you know, trying to achieve human level intelligence\n\n42:54.720 --> 43:00.880\n in our robots? Well, to me, I guess the thing that seems most compelling to me at the moment is this\n\n43:00.880 --> 43:10.160\n question of what to build in and what to learn. Um, I think we're, we don't, we're missing a bunch\n\n43:10.160 --> 43:17.200\n of ideas and, and we, you know, people, you know, don't you dare ask me how many years it's going to\n\n43:17.200 --> 43:22.320\n be until that happens because I won't even participate in the conversation because I think\n\n43:22.320 --> 43:26.400\n we're missing ideas and I don't know how long it's going to take to find them. So I won't ask you how\n\n43:26.400 --> 43:34.240\n many years, but, uh, maybe I'll ask you what it, when you'll be sufficiently impressed that we've\n\n43:34.240 --> 43:40.080\n achieved it. So what's, what's a good test of intelligence? Do you like the Turing test, the\n\n43:40.080 --> 43:46.400\n natural language in the robotic space? Is there something where you would sit back and think,\n\n43:46.400 --> 43:52.000\n Oh, that's, that's pretty impressive. Uh, as a test, as a benchmark, do you think about these\n\n43:52.000 --> 43:57.760\n kinds of problems? No, I resist. I mean, I think all the time that we spend arguing about those\n\n43:57.760 --> 44:03.520\n kinds of things could be better spent just making the robots work better. Uh, so you don't value\n\n44:03.520 --> 44:10.000\n competition. So, I mean, there's a nature of benchmark benchmarks and datasets or Turing\n\n44:10.000 --> 44:14.240\n test challenges where everybody kind of gets together and tries to build a better robot\n\n44:14.240 --> 44:18.640\n cause they want to out compete each other. Like the DARPA challenge with the autonomous vehicles.\n\n44:18.640 --> 44:25.040\n Do you see the value of that or it can get in the way? I think it can get in the way. I mean,\n\n44:25.040 --> 44:29.520\n some people, many people find it motivating. And so that's good. I find it anti motivating\n\n44:29.520 --> 44:36.800\n personally. Uh, but I think what, I mean, I think you get an interesting cycle where for a contest,\n\n44:37.440 --> 44:42.000\n a bunch of smart people get super motivated and they hack their brains out and much of what gets\n\n44:42.000 --> 44:47.200\n done is just hacks, but sometimes really cool ideas emerge. And then that gives us something\n\n44:47.200 --> 44:54.400\n to chew on after that. So I'm, it's not a thing for me, but I don't, I don't regret that other\n\n44:54.400 --> 44:59.120\n people do it. Yeah. It's like you said with everything else that it makes us good. So jumping\n\n44:59.120 --> 45:05.440\n topics a little bit, you started the journal of machine learning research and served as its editor\n\n45:05.440 --> 45:13.760\n in chief. Uh, how did the publication come about and what do you think about the current publishing\n\n45:13.760 --> 45:19.680\n model space in machine learning artificial intelligence? Okay, good. So it came about\n\n45:19.680 --> 45:24.000\n because there was a journal called machine learning, which still exists, which was owned by\n\n45:24.000 --> 45:30.880\n Cluer and there was, I was on the editorial board and we used to have these meetings annually where\n\n45:30.880 --> 45:34.640\n we would complain to Cluer that it was too expensive for the libraries and that people\n\n45:34.640 --> 45:39.200\n couldn't publish. And we would really like to have some kind of relief on those fronts and they would\n\n45:39.200 --> 45:46.960\n always sympathize, but not do anything. So, uh, we just decided to make a new journal and, uh,\n\n45:46.960 --> 45:52.720\n there was the journal of AI research, which has, was on the same model, which had been in existence\n\n45:52.720 --> 45:59.920\n for maybe five years or so, and it was going on pretty well. So, uh, we just made a new journal.\n\n45:59.920 --> 46:05.280\n It wasn't, I mean, um, I don't know, I guess it was work, but it wasn't that hard. So basically\n\n46:05.280 --> 46:14.560\n the editorial board, probably 75% of the editorial board of, uh, machine learning resigned and we\n\n46:14.560 --> 46:21.760\n founded the new journal, but it was sort of, it was more open. Yeah. Right. So it's completely\n\n46:21.760 --> 46:28.960\n open. It's open access. Actually, uh, uh, I had a postdoc, George Conidaris who wanted to call\n\n46:28.960 --> 46:36.240\n these journals free for all, uh, because there were, I mean, it both has no page charges and has\n\n46:36.240 --> 46:44.640\n no, uh, uh, access restrictions. And the reason, and so lots of people, I mean, there were, there\n\n46:44.640 --> 46:48.960\n were people who were mad about the existence of this journal who thought it was a fraud or\n\n46:48.960 --> 46:54.320\n something. It would be impossible. They said to run a journal like this with basically, I mean,\n\n46:54.320 --> 47:00.320\n for a long time, I didn't even have a bank account. Uh, I paid for the lawyer to incorporate and the\n\n47:00.320 --> 47:06.960\n IP address and it just did cost a couple of hundred dollars a year to run. It's a little bit\n\n47:06.960 --> 47:13.920\n more now, but not that much more, but that's because I think computer scientists are competent\n\n47:13.920 --> 47:19.440\n and autonomous in a way that many scientists and other fields aren't. I mean, at doing these kinds\n\n47:19.440 --> 47:24.480\n of things, we already types out our own papers. We all have students and people who can hack a\n\n47:24.480 --> 47:29.280\n website together in an afternoon. So the infrastructure for us was like, not a problem,\n\n47:29.280 --> 47:33.760\n but for other people in other fields, it's a harder thing to do. Yeah. And this kind of\n\n47:34.320 --> 47:40.640\n open access journal is nevertheless one of the most prestigious journals. So it's not like, uh,\n\n47:41.600 --> 47:47.520\n prestige and it can be achieved without any of the... Paper is not required for prestige.\n\n47:47.520 --> 47:53.600\n Yeah. It turns out. Yeah. So on the review process side of actually a long time ago,\n\n47:53.600 --> 47:59.440\n I don't remember when I reviewed a paper where you were also a reviewer. And I remember reading\n\n47:59.440 --> 48:04.080\n your review being influenced by it and it was really well written. It influenced how I write\n\n48:04.080 --> 48:11.520\n feature reviews. Uh, you disagreed with me actually. Uh, and you made it, uh, my review,\n\n48:11.520 --> 48:18.480\n but much better. So, but nevertheless, the review process, you know, has its, uh, flaws.\n\n48:19.280 --> 48:23.600\n And how do you think, what do you think works well? How can it be improved?\n\n48:23.600 --> 48:27.600\n So actually when I started JMLR, I wanted to do something completely different.\n\n48:28.720 --> 48:34.800\n And I didn't because it felt like we needed a traditional journal of record. And so we just\n\n48:34.800 --> 48:41.680\n made JMLR be almost like a normal journal, except for the open access parts of it, basically. Um,\n\n48:43.200 --> 48:47.600\n increasingly of course, publication is not even a sensible word. You can publish something by\n\n48:47.600 --> 48:54.240\n putting it in an archive so I can publish everything tomorrow. So making stuff public\n\n48:54.240 --> 49:04.560\n is, there's no barrier. We still need curation and evaluation. I don't have time to read all\n\n49:04.560 --> 49:20.000\n of archive. And you could argue that kind of social thumbs upping of articles suffices,\n\n49:20.000 --> 49:24.880\n right? You might say, Oh, heck with this. We don't need journals at all. We'll put everything\n\n49:24.880 --> 49:29.840\n on archive and people will upvote and downvote the articles. And then your CV will say, Oh man,\n\n49:29.840 --> 49:36.960\n he got a lot upvotes. So, uh, that's good. Um, but I think there's still\n\n49:39.040 --> 49:46.320\n value in careful reading and commentary of things. And it's hard to tell when people are\n\n49:46.320 --> 49:53.280\n upvoting and downvoting or arguing about your paper on Twitter and Reddit, whether they know\n\n49:53.280 --> 49:57.760\n what they're talking about, right? So then I have the second order problem of trying to decide whose\n\n49:57.760 --> 50:04.480\n opinions I should value and such. So I don't know what I w if I had infinite time, which I don't,\n\n50:04.480 --> 50:10.160\n and I'm not going to do this because I really want to make robots work. But if I felt inclined to do\n\n50:10.160 --> 50:14.560\n something more in the publication direction, I would do this other thing, which I thought about\n\n50:14.560 --> 50:19.840\n doing the first time, which is to get together some set of people whose opinions I value and\n\n50:19.840 --> 50:25.200\n who are pretty articulate. And I guess we would be public, although we could be private. I'm not sure.\n\n50:25.200 --> 50:29.360\n And we would review papers. We wouldn't publish them and you wouldn't submit them. We would just\n\n50:29.360 --> 50:36.240\n find papers and we would write reviews and we would make those reviews public. And maybe if you,\n\n50:37.040 --> 50:42.880\n you know, so we're Leslie's friends who review papers and maybe eventually if, if we, our opinion\n\n50:42.880 --> 50:48.560\n was sufficiently valued, like the opinion of JMLR is valued, then you'd say on your CV that Leslie's\n\n50:48.560 --> 50:53.200\n friends gave my paper a five star rating. And that would be just as good as saying, I got it,\n\n50:53.200 --> 51:00.880\n so, you know, accepted into this journal. So I think, I think we should have good public commentary\n\n51:01.840 --> 51:06.320\n and organize it in some way, but I don't really know how to do it. It's interesting times.\n\n51:06.320 --> 51:10.000\n The way you describe it actually is really interesting. I mean, we do it for movies,\n\n51:10.000 --> 51:15.440\n imdb.com. There's experts, critics come in, they write reviews, but there's also\n\n51:16.000 --> 51:19.840\n regular non critics, humans write reviews and they're separated.\n\n51:19.840 --> 51:29.280\n I like open review. The iClear process I think is interesting.\n\n51:29.280 --> 51:35.760\n It's a step in the right direction, but it's still not as compelling as reviewing movies or\n\n51:35.760 --> 51:41.600\n video games. I mean, it sometimes almost, it might be silly, at least from my perspective to say,\n\n51:41.600 --> 51:46.720\n but it boils down to the user interface, how fun and easy it is to actually perform the reviews,\n\n51:46.720 --> 51:54.400\n how efficient, how much you as a reviewer get street cred for being a good reviewer.\n\n51:54.400 --> 51:56.640\n Those elements, those human elements come into play.\n\n51:57.200 --> 52:04.000\n No, it's a big investment to do a good review of a paper and the flood of papers is out of control.\n\n52:04.000 --> 52:08.480\n Right. So, you know, there aren't 3000 new, I don't know how many new movies are there in a year.\n\n52:08.480 --> 52:12.160\n I don't know, but that's probably going to be less than how many machine learning papers are\n\n52:12.160 --> 52:21.760\n in a year now. And I'm worried, you know, I, right. So I'm like an old person. So of course,\n\n52:21.760 --> 52:28.720\n I'm going to say, things are moving too fast. I'm a stick in the mud. So I can say that,\n\n52:28.720 --> 52:35.760\n but my particular flavor of that is I think the horizon for researchers has gotten very short,\n\n52:35.760 --> 52:41.760\n that students want to publish a lot of papers and there's a huge, there's value. It's exciting. And\n\n52:41.760 --> 52:49.200\n there's value in that and you get patted on the head for it and so on. But, and some of that is\n\n52:49.200 --> 52:59.200\n fine, but I'm worried that we're driving out people who would spend two years thinking about\n\n52:59.200 --> 53:06.080\n something. Back in my day, when we worked on our thesis, we did not publish papers. You did your\n\n53:06.080 --> 53:11.360\n thesis for years. You picked a hard problem and then you worked and chewed on it and did stuff\n\n53:11.360 --> 53:16.320\n and wasted time and for a long time. And when it was roughly, when it was done, you would write\n\n53:16.320 --> 53:23.520\n papers. And so I don't know how to, and I don't think that everybody has to work in that mode,\n\n53:23.520 --> 53:27.760\n but I think there's some problems that are hard enough that it's important to have a long\n\n53:27.760 --> 53:33.040\n research horizon. And I'm worried that we don't incentivize that at all at this point.\n\n53:33.040 --> 53:41.840\n In this current structure. Yeah. So what do you see as, what are your hopes and fears about the\n\n53:41.840 --> 53:50.080\n future of AI and continuing on this theme? So AI has gone through a few winters, ups and downs. Do\n\n53:50.080 --> 53:58.720\n you see another winter of AI coming? Are you more hopeful about making robots work, as you said?\n\n53:58.720 --> 54:05.680\n I think the cycles are inevitable, but I think each time we get higher, right? I mean, so, you\n\n54:05.680 --> 54:15.280\n know, it's like climbing some kind of landscape with a noisy optimizer. So it's clear that the,\n\n54:15.280 --> 54:22.560\n you know, the deep learning stuff has made deep and important improvements. And so the high water\n\n54:22.560 --> 54:29.360\n mark is now higher. There's no question. But of course, I think people are overselling and\n\n54:29.360 --> 54:37.040\n eventually investors, I guess, and other people will look around and say, well, you're not quite\n\n54:37.040 --> 54:43.120\n delivering on this grand claim and that wild hypothesis. It's probably, it's going to crash\n\n54:43.120 --> 54:49.760\n some amount and then it's okay. I mean, but I don't, I can't imagine that there's like\n\n54:49.760 --> 54:58.320\n some awesome monotonic improvement from here to human level AI. So in, you know, I have to ask\n\n54:58.320 --> 55:05.520\n this question, I probably anticipate answers, the answers, but do you have a worry short term or\n\n55:05.520 --> 55:14.560\n long term about the existential threats of AI and maybe short term, less existential, but more\n\n55:15.280 --> 55:17.120\n robots taking away jobs?\n\n55:17.120 --> 55:25.520\n Well, actually, let me talk a little bit about utility. Actually, I had an interesting\n\n55:25.520 --> 55:30.880\n conversation with some military ethicists who wanted to talk to me about autonomous weapons.\n\n55:30.880 --> 55:37.440\n And they're, they were interesting, smart, well educated guys who didn't know too much about AI\n\n55:37.440 --> 55:41.440\n or machine learning. And the first question they asked me was, has your robot ever done\n\n55:41.440 --> 55:46.480\n something you didn't expect? And I like burst out laughing because anybody who's ever done\n\n55:46.480 --> 55:51.680\n something on the robot right knows that they don't do it. And what I realized was that their\n\n55:51.680 --> 55:56.960\n model of how we program a robot was completely wrong. Their model of how we can program a robot\n\n55:56.960 --> 56:03.120\n was like Lego mind storms, like, Oh, go forward a meter, turn left, take a picture, do this, do\n\n56:03.120 --> 56:08.800\n that. And so if you have that model of programming, then it's true. It's kind of weird that your robot\n\n56:08.800 --> 56:13.760\n would do something that you didn't anticipate. But the fact is, and actually, so now this is my\n\n56:13.760 --> 56:20.640\n new educational mission. If I have to talk to non experts, I try to teach them the idea that\n\n56:20.640 --> 56:26.400\n we don't operate, we operate at least one or maybe many levels of abstraction about that. And we say,\n\n56:26.400 --> 56:31.200\n Oh, here's a hypothesis class, maybe it's a space of plans, or maybe it's a space of\n\n56:31.200 --> 56:36.400\n classifiers or whatever. But there's some set of answers and an objective function. And then we\n\n56:36.400 --> 56:41.840\n work on some optimization method that tries to optimize a solution solution in that class.\n\n56:43.200 --> 56:47.600\n And we don't know what solution is going to come out. Right. So I think it's important to\n\n56:47.600 --> 56:52.160\n communicate that. So I mean, of course, probably people who listen to this, they, they know that\n\n56:52.160 --> 56:56.320\n lesson. But I think it's really critical to communicate that lesson. And then lots of people\n\n56:56.320 --> 57:01.840\n are now talking about, you know, the value alignment problem. So you want to be sure as\n\n57:01.840 --> 57:07.840\n robots or software systems get more competent, that their objectives are aligned with your\n\n57:07.840 --> 57:13.760\n objectives, or that our objectives are compatible in some way, or we have a good way of mediating\n\n57:13.760 --> 57:19.120\n when they have different objectives. And so I think it is important to start thinking in terms\n\n57:19.120 --> 57:25.360\n like, you don't have to be freaked out by the robot apocalypse, to accept that it's important\n\n57:25.360 --> 57:30.080\n to think about objective functions of value alignment. Yes. And that you have to really\n\n57:30.080 --> 57:34.160\n everyone who's done optimization knows that you have to be careful what you wish for that,\n\n57:34.160 --> 57:38.800\n you know, sometimes you get the optimal solution, and you realize, man, that was that objective was\n\n57:38.800 --> 57:47.040\n wrong. So pragmatically, in the shortest term, it seems to me that that that those are really\n\n57:47.040 --> 57:50.960\n interesting and critical questions. And the idea that we're going to go from being people who\n\n57:50.960 --> 57:56.480\n engineer algorithms to being people who engineer objective functions. I think that's, that's\n\n57:56.480 --> 58:00.560\n definitely going to happen. And that's going to change our thinking and methodology. And so we're\n\n58:00.560 --> 58:05.920\n gonna you started at Stanford philosophy, that's where she could be. And I will go back to\n\n58:05.920 --> 58:11.920\n philosophy maybe. Well, I mean, they're mixed together, because because, as we also know,\n\n58:11.920 --> 58:16.160\n as machine learning people, right? When you design, in fact, this is the lecture I gave in\n\n58:16.160 --> 58:20.960\n class today, when you design an objective function, you have to wear both hats, there's\n\n58:21.600 --> 58:26.080\n the hat that says, what do I want? And there's the hat that says, but I know what my optimizer\n\n58:26.080 --> 58:31.840\n can do to some degree. And I have to take that into account. So it's it's always a trade off,\n\n58:31.840 --> 58:38.560\n and we have to kind of be mindful of that. The part about taking people's jobs, I understand\n\n58:38.560 --> 58:45.440\n that that's important. I don't understand sociology or economics or people very well. So I\n\n58:45.440 --> 58:50.160\n don't know how to think about that. So that's Yeah, so there might be a sociological aspect\n\n58:50.160 --> 58:54.640\n there, the economic aspect that's very difficult to think about. Okay. I mean, I think other people\n\n58:54.640 --> 58:58.480\n should be thinking about it. But I'm just that's not my strength. So what do you think is the most\n\n58:58.480 --> 59:03.600\n exciting area of research in the short term, for the community and for your for yourself?\n\n59:03.600 --> 59:10.160\n Well, so I mean, there's the story I've been telling about how to engineer intelligent robots.\n\n59:10.160 --> 59:15.680\n So that's what we want to do. We all kind of want to do well, I mean, some set of us want to do this.\n\n59:16.160 --> 59:20.560\n And the question is, what's the most effective strategy? And we've tried it. And there's a bunch\n\n59:20.560 --> 59:25.360\n of different things you could do at the extremes, right? One super extreme is, what's the most\n\n59:25.360 --> 59:29.600\n effective strategy? And there's a bunch of different things you could do at the extremes,\n\n59:29.600 --> 59:35.920\n right? One super extreme is, we do introspection, and we write a program. Okay, that has not worked\n\n59:35.920 --> 59:41.280\n out very well. Another extreme is we take a giant bunch of neural goo, and we try and train it up to\n\n59:41.280 --> 59:46.960\n do something. I don't think that's going to work either. So the question is, what's the middle\n\n59:46.960 --> 59:54.240\n ground? And, and again, this isn't a theological question or anything like that. It's just like,\n\n59:54.240 --> 1:00:00.640\n what's the middle ground? And I think it's clear, it's a combination of learning, to me, it's clear,\n\n1:00:00.640 --> 1:00:05.760\n it's a combination of learning and not learning. And what should that combination be? And what's\n\n1:00:05.760 --> 1:00:10.080\n the stuff we build in? So to me, that's the most compelling question. And when you say engineer\n\n1:00:10.080 --> 1:00:16.560\n robots, you mean engineering systems that work in the real world? Is that, that's the emphasis?\n\n1:00:16.560 --> 1:00:23.200\n Okay. Last question. Which robots or robot is your favorite from science fiction?\n\n1:00:24.480 --> 1:00:30.400\n So you can go with Star Wars or RTD2, or you can go with more modern,\n\n1:00:32.240 --> 1:00:37.040\n maybe Hal from... I don't think I have a favorite robot from science fiction.\n\n1:00:38.080 --> 1:00:45.520\n This is, this is back to, you like to make robots work in the real world here, not, not in...\n\n1:00:45.520 --> 1:00:50.000\n I mean, I love the process and I care more about the process.\n\n1:00:50.000 --> 1:00:51.040\n The engineering process.\n\n1:00:51.600 --> 1:00:55.760\n Yeah. I mean, I do research because it's fun, not because I care about what we produce.\n\n1:00:57.520 --> 1:01:00.640\n Well, that's a, that's a beautiful note actually. And Leslie,\n\n1:01:00.640 --> 1:01:02.000\n thank you so much for talking today.\n\n1:01:02.000 --> 1:01:16.800\n Sure. It's been fun.\n\n"
}