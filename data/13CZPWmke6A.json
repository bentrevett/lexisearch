{
  "title": "Ilya Sutskever: Deep Learning | Lex Fridman Podcast #94",
  "id": "13CZPWmke6A",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:03.160\n The following is a conversation with Ilya Sotskever,\n\n00:03.160 --> 00:06.120\n cofounder and chief scientist of OpenAI,\n\n00:06.120 --> 00:09.360\n one of the most cited computer scientists in history\n\n00:09.360 --> 00:13.480\n with over 165,000 citations,\n\n00:13.480 --> 00:17.080\n and to me, one of the most brilliant and insightful minds\n\n00:17.080 --> 00:20.000\n ever in the field of deep learning.\n\n00:20.000 --> 00:21.680\n There are very few people in this world\n\n00:21.680 --> 00:24.040\n who I would rather talk to and brainstorm with\n\n00:24.040 --> 00:27.760\n about deep learning, intelligence, and life in general\n\n00:27.760 --> 00:30.680\n than Ilya, on and off the mic.\n\n00:30.680 --> 00:32.840\n This was an honor and a pleasure.\n\n00:33.720 --> 00:35.240\n This conversation was recorded\n\n00:35.240 --> 00:37.200\n before the outbreak of the pandemic.\n\n00:37.200 --> 00:39.480\n For everyone feeling the medical, psychological,\n\n00:39.480 --> 00:41.440\n and financial burden of this crisis,\n\n00:41.440 --> 00:43.160\n I'm sending love your way.\n\n00:43.160 --> 00:47.160\n Stay strong, we're in this together, we'll beat this thing.\n\n00:47.160 --> 00:49.640\n This is the Artificial Intelligence Podcast.\n\n00:49.640 --> 00:51.760\n If you enjoy it, subscribe on YouTube,\n\n00:51.760 --> 00:54.060\n review it with five stars on Apple Podcast,\n\n00:54.060 --> 00:55.120\n support it on Patreon,\n\n00:55.120 --> 00:57.000\n or simply connect with me on Twitter\n\n00:57.000 --> 01:00.560\n at lexfriedman, spelled F R I D M A N.\n\n01:00.560 --> 01:03.000\n As usual, I'll do a few minutes of ads now\n\n01:03.000 --> 01:04.320\n and never any ads in the middle\n\n01:04.320 --> 01:06.600\n that can break the flow of the conversation.\n\n01:06.600 --> 01:07.980\n I hope that works for you\n\n01:07.980 --> 01:10.120\n and doesn't hurt the listening experience.\n\n01:10.960 --> 01:13.440\n This show is presented by Cash App,\n\n01:13.440 --> 01:15.720\n the number one finance app in the App Store.\n\n01:15.720 --> 01:18.840\n When you get it, use code LEXPODCAST.\n\n01:18.840 --> 01:20.960\n Cash App lets you send money to friends,\n\n01:20.960 --> 01:23.440\n buy Bitcoin, invest in the stock market\n\n01:23.440 --> 01:25.440\n with as little as $1.\n\n01:25.440 --> 01:27.520\n Since Cash App allows you to buy Bitcoin,\n\n01:27.520 --> 01:29.320\n let me mention that cryptocurrency\n\n01:29.320 --> 01:33.080\n in the context of the history of money is fascinating.\n\n01:33.080 --> 01:36.840\n I recommend Ascent of Money as a great book on this history.\n\n01:36.840 --> 01:39.600\n Both the book and audio book are great.\n\n01:39.600 --> 01:41.040\n Debits and credits on ledgers\n\n01:41.040 --> 01:43.920\n started around 30,000 years ago.\n\n01:43.920 --> 01:47.200\n The US dollar created over 200 years ago,\n\n01:47.200 --> 01:50.040\n and Bitcoin, the first decentralized cryptocurrency,\n\n01:50.040 --> 01:52.080\n released just over 10 years ago.\n\n01:52.080 --> 01:53.520\n So given that history,\n\n01:53.520 --> 01:55.960\n cryptocurrency is still very much in its early days\n\n01:55.960 --> 01:58.200\n of development, but it's still aiming to\n\n01:58.200 --> 02:01.840\n and just might redefine the nature of money.\n\n02:01.840 --> 02:04.240\n So again, if you get Cash App from the App Store\n\n02:04.240 --> 02:08.040\n or Google Play and use the code LEXPODCAST,\n\n02:08.040 --> 02:12.480\n you get $10 and Cash App will also donate $10 to FIRST,\n\n02:12.480 --> 02:14.880\n an organization that is helping advance robotics\n\n02:14.880 --> 02:17.660\n and STEM education for young people around the world.\n\n02:18.600 --> 02:22.460\n And now here's my conversation with Ilya Satsgever.\n\n02:22.460 --> 02:26.740\n You were one of the three authors with Alex Kaszewski,\n\n02:26.740 --> 02:30.140\n Geoff Hinton of the famed AlexNet paper\n\n02:30.140 --> 02:33.500\n that is arguably the paper that marked\n\n02:33.500 --> 02:35.140\n the big catalytic moment\n\n02:35.140 --> 02:37.860\n that launched the deep learning revolution.\n\n02:37.860 --> 02:39.620\n At that time, take us back to that time,\n\n02:39.620 --> 02:42.260\n what was your intuition about neural networks,\n\n02:42.260 --> 02:46.000\n about the representational power of neural networks?\n\n02:46.000 --> 02:48.860\n And maybe you could mention how did that evolve\n\n02:48.860 --> 02:51.780\n over the next few years up to today,\n\n02:51.780 --> 02:53.460\n over the 10 years?\n\n02:53.460 --> 02:55.260\n Yeah, I can answer that question.\n\n02:55.260 --> 02:58.620\n At some point in about 2010 or 2011,\n\n03:00.060 --> 03:02.620\n I connected two facts in my mind.\n\n03:02.620 --> 03:06.720\n Basically, the realization was this,\n\n03:07.580 --> 03:11.300\n at some point we realized that we can train very large,\n\n03:11.300 --> 03:13.380\n I shouldn't say very, tiny by today's standards,\n\n03:13.380 --> 03:16.560\n but large and deep neural networks\n\n03:16.560 --> 03:18.540\n end to end with backpropagation.\n\n03:18.540 --> 03:22.380\n At some point, different people obtained this result.\n\n03:22.380 --> 03:23.800\n I obtained this result.\n\n03:23.800 --> 03:26.420\n The first moment in which I realized\n\n03:26.420 --> 03:28.980\n that deep neural networks are powerful\n\n03:28.980 --> 03:30.780\n was when James Martens invented\n\n03:30.780 --> 03:33.620\n the Hessian free optimizer in 2010.\n\n03:33.620 --> 03:37.100\n And he trained a 10 layer neural network end to end\n\n03:37.100 --> 03:40.600\n without pre training from scratch.\n\n03:41.620 --> 03:43.940\n And when that happened, I thought this is it.\n\n03:43.940 --> 03:45.620\n Because if you can train a big neural network,\n\n03:45.620 --> 03:49.500\n a big neural network can represent very complicated function.\n\n03:49.500 --> 03:52.700\n Because if you have a neural network with 10 layers,\n\n03:52.700 --> 03:55.260\n it's as though you allow the human brain\n\n03:55.260 --> 03:58.340\n to run for some number of milliseconds.\n\n03:58.340 --> 04:00.380\n Neuron firings are slow.\n\n04:00.380 --> 04:03.220\n And so in maybe 100 milliseconds,\n\n04:03.220 --> 04:04.700\n your neurons only fire 10 times.\n\n04:04.700 --> 04:06.780\n So it's also kind of like 10 layers.\n\n04:06.780 --> 04:08.140\n And in 100 milliseconds,\n\n04:08.140 --> 04:10.460\n you can perfectly recognize any object.\n\n04:10.460 --> 04:13.100\n So I thought, so I already had the idea then\n\n04:13.100 --> 04:16.100\n that we need to train a very big neural network\n\n04:16.100 --> 04:18.160\n on lots of supervised data.\n\n04:18.160 --> 04:19.420\n And then it must succeed\n\n04:19.420 --> 04:21.360\n because we can find the best neural network.\n\n04:21.360 --> 04:22.740\n And then there's also theory\n\n04:22.740 --> 04:24.500\n that if you have more data than parameters,\n\n04:24.500 --> 04:25.760\n you won't overfit.\n\n04:25.760 --> 04:28.100\n Today, we know that actually this theory is very incomplete\n\n04:28.100 --> 04:29.780\n and you won't overfit even if you have less data\n\n04:29.780 --> 04:31.320\n than parameters, but definitely,\n\n04:31.320 --> 04:33.340\n if you have more data than parameters, you won't overfit.\n\n04:33.340 --> 04:34.700\n So the fact that neural networks\n\n04:34.700 --> 04:39.100\n were heavily overparameterized wasn't discouraging to you?\n\n04:39.100 --> 04:41.220\n So you were thinking about the theory\n\n04:41.220 --> 04:43.080\n that the number of parameters,\n\n04:43.080 --> 04:45.220\n the fact that there's a huge number of parameters is okay?\n\n04:45.220 --> 04:46.060\n Is it gonna be okay?\n\n04:46.060 --> 04:48.260\n I mean, there was some evidence before that it was okayish,\n\n04:48.260 --> 04:49.460\n but the theory was most,\n\n04:49.460 --> 04:51.500\n the theory was that if you had a big data set\n\n04:51.500 --> 04:53.080\n and a big neural net, it was going to work.\n\n04:53.080 --> 04:55.500\n The overparameterization just didn't really\n\n04:55.500 --> 04:57.060\n figure much as a problem.\n\n04:57.060 --> 04:57.940\n I thought, well, with images,\n\n04:57.940 --> 04:59.280\n you're just gonna add some data augmentation\n\n04:59.280 --> 05:00.420\n and it's gonna be okay.\n\n05:00.420 --> 05:02.460\n So where was any doubt coming from?\n\n05:02.460 --> 05:04.420\n The main doubt was, can we train a bigger,\n\n05:04.420 --> 05:05.580\n will we have enough computer train\n\n05:05.580 --> 05:06.420\n a big enough neural net?\n\n05:06.420 --> 05:07.580\n With backpropagation.\n\n05:07.580 --> 05:09.440\n Backpropagation I thought would work.\n\n05:09.440 --> 05:10.660\n The thing which wasn't clear\n\n05:10.660 --> 05:12.480\n was whether there would be enough compute\n\n05:12.480 --> 05:14.100\n to get a very convincing result.\n\n05:14.100 --> 05:15.780\n And then at some point, Alex Kerchevsky wrote\n\n05:15.780 --> 05:17.500\n these insanely fast CUDA kernels\n\n05:17.500 --> 05:19.180\n for training convolutional neural nets.\n\n05:19.180 --> 05:20.880\n Net was bam, let's do this.\n\n05:20.880 --> 05:23.420\n Let's get image in it and it's gonna be the greatest thing.\n\n05:23.420 --> 05:25.940\n Was your intuition, most of your intuition\n\n05:25.940 --> 05:29.540\n from empirical results by you and by others?\n\n05:29.540 --> 05:31.140\n So like just actually demonstrating\n\n05:31.140 --> 05:33.160\n that a piece of program can train\n\n05:33.160 --> 05:34.660\n a 10 layer neural network?\n\n05:34.660 --> 05:37.360\n Or was there some pen and paper\n\n05:37.360 --> 05:41.180\n or marker and whiteboard thinking intuition?\n\n05:41.180 --> 05:43.900\n Like, cause you just connected a 10 layer\n\n05:43.900 --> 05:45.520\n large neural network to the brain.\n\n05:45.520 --> 05:46.580\n So you just mentioned the brain.\n\n05:46.580 --> 05:49.180\n So in your intuition about neural networks\n\n05:49.180 --> 05:53.820\n does the human brain come into play as a intuition builder?\n\n05:53.820 --> 05:54.980\n Definitely.\n\n05:54.980 --> 05:57.500\n I mean, you gotta be precise with these analogies\n\n05:57.500 --> 06:00.260\n between artificial neural networks and the brain.\n\n06:00.260 --> 06:04.080\n But there is no question that the brain is a huge source\n\n06:04.080 --> 06:07.420\n of intuition and inspiration for deep learning researchers\n\n06:07.420 --> 06:10.800\n since all the way from Rosenblatt in the 60s.\n\n06:10.800 --> 06:13.820\n Like if you look at the whole idea of a neural network\n\n06:13.820 --> 06:15.700\n is directly inspired by the brain.\n\n06:15.700 --> 06:18.060\n You had people like McCallum and Pitts who were saying,\n\n06:18.060 --> 06:22.020\n hey, you got these neurons in the brain.\n\n06:22.020 --> 06:23.820\n And hey, we recently learned about the computer\n\n06:23.820 --> 06:24.660\n and automata.\n\n06:24.660 --> 06:26.420\n Can we use some ideas from the computer and automata\n\n06:26.420 --> 06:28.740\n to design some kind of computational object\n\n06:28.740 --> 06:31.660\n that's going to be simple, computational\n\n06:31.660 --> 06:34.380\n and kind of like the brain and they invented the neuron.\n\n06:34.380 --> 06:35.980\n So they were inspired by it back then.\n\n06:35.980 --> 06:38.580\n Then you had the convolutional neural network from Fukushima\n\n06:38.580 --> 06:40.420\n and then later Yann LeCun who said, hey,\n\n06:40.420 --> 06:42.680\n if you limit the receptive fields of a neural network,\n\n06:42.680 --> 06:45.460\n it's going to be especially suitable for images\n\n06:45.460 --> 06:46.980\n as it turned out to be true.\n\n06:46.980 --> 06:49.940\n So there was a very small number of examples\n\n06:49.940 --> 06:52.340\n where analogies to the brain were successful.\n\n06:52.340 --> 06:55.100\n And I thought, well, probably an artificial neuron\n\n06:55.100 --> 06:56.740\n is not that different from the brain\n\n06:56.740 --> 06:57.660\n if it's cleaned hard enough.\n\n06:57.660 --> 07:00.940\n So let's just assume it is and roll with it.\n\n07:00.940 --> 07:02.780\n So now we're not at a time where deep learning\n\n07:02.780 --> 07:03.800\n is very successful.\n\n07:03.800 --> 07:08.800\n So let us squint less and say, let's open our eyes\n\n07:08.900 --> 07:12.060\n and say, what do you use an interesting difference\n\n07:12.060 --> 07:13.820\n between the human brain?\n\n07:13.820 --> 07:16.380\n Now, I know you're probably not an expert\n\n07:16.380 --> 07:18.220\n neither in your scientists and your biologists,\n\n07:18.220 --> 07:20.420\n but loosely speaking, what's the difference\n\n07:20.420 --> 07:22.420\n between the human brain and artificial neural networks?\n\n07:22.420 --> 07:26.300\n That's interesting to you for the next decade or two.\n\n07:26.300 --> 07:27.860\n That's a good question to ask.\n\n07:27.860 --> 07:29.700\n What is an interesting difference between the neurons\n\n07:29.700 --> 07:32.900\n between the brain and our artificial neural networks?\n\n07:32.900 --> 07:37.140\n So I feel like today, artificial neural networks,\n\n07:37.140 --> 07:39.380\n so we all agree that there are certain dimensions\n\n07:39.380 --> 07:43.000\n in which the human brain vastly outperforms our models.\n\n07:43.000 --> 07:44.400\n But I also think that there are some ways\n\n07:44.400 --> 07:46.180\n in which our artificial neural networks\n\n07:46.180 --> 07:50.380\n have a number of very important advantages over the brain.\n\n07:50.380 --> 07:52.540\n Looking at the advantages versus disadvantages\n\n07:52.540 --> 07:55.600\n is a good way to figure out what is the important difference.\n\n07:55.600 --> 08:00.100\n So the brain uses spikes, which may or may not be important.\n\n08:00.100 --> 08:01.380\n Yeah, it's a really interesting question.\n\n08:01.380 --> 08:03.860\n Do you think it's important or not?\n\n08:03.860 --> 08:06.380\n That's one big architectural difference\n\n08:06.380 --> 08:08.380\n between artificial neural networks.\n\n08:08.380 --> 08:11.700\n It's hard to tell, but my prior is not very high\n\n08:11.700 --> 08:13.500\n and I can say why.\n\n08:13.500 --> 08:14.340\n There are people who are interested\n\n08:14.340 --> 08:15.380\n in spiking neural networks.\n\n08:15.380 --> 08:17.460\n And basically what they figured out\n\n08:17.460 --> 08:19.260\n is that they need to simulate\n\n08:19.260 --> 08:21.620\n the non spiking neural networks in spikes.\n\n08:22.740 --> 08:24.300\n And that's how they're gonna make them work.\n\n08:24.300 --> 08:26.340\n If you don't simulate the non spiking neural networks\n\n08:26.340 --> 08:27.780\n in spikes, it's not going to work\n\n08:27.780 --> 08:29.580\n because the question is why should it work?\n\n08:29.580 --> 08:31.820\n And that connects to questions around back propagation\n\n08:31.820 --> 08:34.860\n and questions around deep learning.\n\n08:34.860 --> 08:36.900\n You've got this giant neural network.\n\n08:36.900 --> 08:38.420\n Why should it work at all?\n\n08:38.420 --> 08:40.460\n Why should the learning rule work at all?\n\n08:43.220 --> 08:44.660\n It's not a self evident question,\n\n08:44.660 --> 08:47.060\n especially if you, let's say if you were just starting\n\n08:47.060 --> 08:49.340\n in the field and you read the very early papers,\n\n08:49.340 --> 08:51.580\n you can say, hey, people are saying,\n\n08:51.580 --> 08:53.740\n let's build neural networks.\n\n08:53.740 --> 08:55.900\n That's a great idea because the brain is a neural network.\n\n08:55.900 --> 08:58.020\n So it would be useful to build neural networks.\n\n08:58.020 --> 09:00.420\n Now let's figure out how to train them.\n\n09:00.420 --> 09:03.420\n It should be possible to train them probably, but how?\n\n09:03.420 --> 09:06.360\n And so the big idea is the cost function.\n\n09:07.260 --> 09:08.780\n That's the big idea.\n\n09:08.780 --> 09:11.900\n The cost function is a way of measuring the performance\n\n09:11.900 --> 09:14.940\n of the system according to some measure.\n\n09:14.940 --> 09:17.180\n By the way, that is a big, actually let me think,\n\n09:17.180 --> 09:21.180\n is that one, a difficult idea to arrive at\n\n09:21.180 --> 09:22.740\n and how big of an idea is that?\n\n09:22.740 --> 09:24.980\n That there's a single cost function.\n\n09:27.620 --> 09:28.940\n Sorry, let me take a pause.\n\n09:28.940 --> 09:33.340\n Is supervised learning a difficult concept to come to?\n\n09:33.340 --> 09:34.660\n I don't know.\n\n09:34.660 --> 09:36.460\n All concepts are very easy in retrospect.\n\n09:36.460 --> 09:38.100\n Yeah, that's what it seems trivial now,\n\n09:38.100 --> 09:40.540\n but I, because the reason I asked that,\n\n09:40.540 --> 09:43.460\n and we'll talk about it, is there other things?\n\n09:43.460 --> 09:47.180\n Is there things that don't necessarily have a cost function,\n\n09:47.180 --> 09:48.620\n maybe have many cost functions\n\n09:48.620 --> 09:50.900\n or maybe have dynamic cost functions\n\n09:50.900 --> 09:54.180\n or maybe a totally different kind of architectures?\n\n09:54.180 --> 09:55.500\n Because we have to think like that\n\n09:55.500 --> 09:57.980\n in order to arrive at something new, right?\n\n09:57.980 --> 09:59.940\n So the only, so the good examples of things\n\n09:59.940 --> 10:02.460\n which don't have clear cost functions are GANs.\n\n10:03.940 --> 10:05.740\n Right. And a GAN, you have a game.\n\n10:05.740 --> 10:08.240\n So instead of thinking of a cost function,\n\n10:08.240 --> 10:09.260\n where you wanna optimize,\n\n10:09.260 --> 10:12.100\n where you know that you have an algorithm gradient descent,\n\n10:12.100 --> 10:13.940\n which will optimize the cost function,\n\n10:13.940 --> 10:16.340\n and then you can reason about the behavior of your system\n\n10:16.340 --> 10:18.140\n in terms of what it optimizes.\n\n10:18.140 --> 10:20.060\n With a GAN, you say, I have a game\n\n10:20.060 --> 10:22.220\n and I'll reason about the behavior of the system\n\n10:22.220 --> 10:24.540\n in terms of the equilibrium of the game.\n\n10:24.540 --> 10:26.540\n But it's all about coming up with these mathematical objects\n\n10:26.540 --> 10:30.140\n that help us reason about the behavior of our system.\n\n10:30.140 --> 10:31.180\n Right, that's really interesting.\n\n10:31.180 --> 10:33.420\n Yeah, so GAN is the only one, it's kind of a,\n\n10:33.420 --> 10:36.900\n the cost function is emergent from the comparison.\n\n10:36.900 --> 10:38.980\n It's, I don't know if it has a cost function.\n\n10:38.980 --> 10:39.820\n I don't know if it's meaningful\n\n10:39.820 --> 10:41.340\n to talk about the cost function of a GAN.\n\n10:41.340 --> 10:44.020\n It's kind of like the cost function of biological evolution\n\n10:44.020 --> 10:45.700\n or the cost function of the economy.\n\n10:45.700 --> 10:49.460\n It's, you can talk about regions\n\n10:49.460 --> 10:52.780\n to which it will go towards, but I don't think,\n\n10:55.260 --> 10:57.460\n I don't think the cost function analogy is the most useful.\n\n10:57.460 --> 11:00.100\n So if evolution doesn't, that's really interesting.\n\n11:00.100 --> 11:02.660\n So if evolution doesn't really have a cost function,\n\n11:02.660 --> 11:04.900\n like a cost function based on its,\n\n11:06.540 --> 11:09.860\n something akin to our mathematical conception\n\n11:09.860 --> 11:12.740\n of a cost function, then do you think cost functions\n\n11:12.740 --> 11:15.140\n in deep learning are holding us back?\n\n11:15.140 --> 11:18.300\n Yeah, so you just kind of mentioned that cost function\n\n11:18.300 --> 11:21.380\n is a nice first profound idea.\n\n11:21.380 --> 11:23.340\n Do you think that's a good idea?\n\n11:23.340 --> 11:26.740\n Do you think it's an idea we'll go past?\n\n11:26.740 --> 11:29.540\n So self play starts to touch on that a little bit\n\n11:29.540 --> 11:31.700\n in reinforcement learning systems.\n\n11:31.700 --> 11:32.540\n That's right.\n\n11:32.540 --> 11:34.700\n Self play and also ideas around exploration\n\n11:34.700 --> 11:36.580\n where you're trying to take action\n\n11:36.580 --> 11:39.060\n that surprise a predictor.\n\n11:39.060 --> 11:40.500\n I'm a big fan of cost functions.\n\n11:40.500 --> 11:41.660\n I think cost functions are great\n\n11:41.660 --> 11:42.740\n and they serve us really well.\n\n11:42.740 --> 11:44.220\n And I think that whenever we can do things\n\n11:44.220 --> 11:45.940\n with cost functions, we should.\n\n11:45.940 --> 11:47.740\n And you know, maybe there is a chance\n\n11:47.740 --> 11:49.020\n that we will come up with some,\n\n11:49.020 --> 11:51.340\n yet another profound way of looking at things\n\n11:51.340 --> 11:54.220\n that will involve cost functions in a less central way.\n\n11:54.220 --> 11:55.780\n But I don't know, I think cost functions are,\n\n11:55.780 --> 12:00.780\n I mean, I would not bet against cost functions.\n\n12:01.780 --> 12:04.140\n Is there other things about the brain\n\n12:04.140 --> 12:06.940\n that pop into your mind that might be different\n\n12:06.940 --> 12:09.740\n and interesting for us to consider\n\n12:09.740 --> 12:12.260\n in designing artificial neural networks?\n\n12:12.260 --> 12:14.300\n So we talked about spiking a little bit.\n\n12:14.300 --> 12:16.620\n I mean, one thing which may potentially be useful,\n\n12:16.620 --> 12:18.660\n I think people, neuroscientists have figured out\n\n12:18.660 --> 12:20.180\n something about the learning rule of the brain\n\n12:20.180 --> 12:22.780\n or I'm talking about spike time independent plasticity\n\n12:22.780 --> 12:24.340\n and it would be nice if some people\n\n12:24.340 --> 12:26.340\n would just study that in simulation.\n\n12:26.340 --> 12:28.820\n Wait, sorry, spike time independent plasticity?\n\n12:28.820 --> 12:29.660\n Yeah, that's right.\n\n12:29.660 --> 12:30.500\n What's that?\n\n12:30.500 --> 12:31.340\n STD.\n\n12:31.340 --> 12:33.700\n It's a particular learning rule that uses spike timing\n\n12:33.700 --> 12:37.660\n to figure out how to determine how to update the synapses.\n\n12:37.660 --> 12:40.620\n So it's kind of like if a synapse fires into the neuron\n\n12:40.620 --> 12:42.420\n before the neuron fires,\n\n12:42.420 --> 12:44.380\n then it strengthens the synapse,\n\n12:44.380 --> 12:46.220\n and if the synapse fires into the neurons\n\n12:46.220 --> 12:47.860\n shortly after the neuron fired,\n\n12:47.860 --> 12:49.020\n then it weakens the synapse.\n\n12:49.020 --> 12:50.500\n Something along this line.\n\n12:50.500 --> 12:54.460\n I'm 90% sure it's right, so if I said something wrong here,\n\n12:54.460 --> 12:57.780\n don't get too angry.\n\n12:57.780 --> 12:59.340\n But you sounded brilliant while saying it.\n\n12:59.340 --> 13:02.500\n But the timing, that's one thing that's missing.\n\n13:02.500 --> 13:05.820\n The temporal dynamics is not captured.\n\n13:05.820 --> 13:08.340\n I think that's like a fundamental property of the brain\n\n13:08.340 --> 13:12.340\n is the timing of the timing of the timing\n\n13:12.340 --> 13:13.380\n of the signals.\n\n13:13.380 --> 13:15.500\n Well, you have recurrent neural networks.\n\n13:15.500 --> 13:18.100\n But you think of that as this,\n\n13:18.100 --> 13:20.340\n I mean, that's a very crude, simplified,\n\n13:21.380 --> 13:22.380\n what's that called?\n\n13:23.500 --> 13:27.660\n There's a clock, I guess, to recurrent neural networks.\n\n13:27.660 --> 13:30.140\n It's, this seems like the brain is the general,\n\n13:30.140 --> 13:31.980\n the continuous version of that,\n\n13:31.980 --> 13:36.100\n the generalization where all possible timings are possible,\n\n13:36.100 --> 13:39.940\n and then within those timings is contained some information.\n\n13:39.940 --> 13:42.060\n You think recurrent neural networks,\n\n13:42.060 --> 13:45.460\n the recurrence in recurrent neural networks\n\n13:45.460 --> 13:50.260\n can capture the same kind of phenomena as the timing\n\n13:51.300 --> 13:54.260\n that seems to be important for the brain,\n\n13:54.260 --> 13:56.340\n in the firing of neurons in the brain?\n\n13:56.340 --> 14:00.740\n I mean, I think recurrent neural networks are amazing,\n\n14:00.740 --> 14:03.900\n and they can do, I think they can do anything\n\n14:03.900 --> 14:07.700\n we'd want them to, we'd want a system to do.\n\n14:07.700 --> 14:09.060\n Right now, recurrent neural networks\n\n14:09.060 --> 14:10.500\n have been superseded by transformers,\n\n14:10.500 --> 14:12.740\n but maybe one day they'll make a comeback,\n\n14:12.740 --> 14:14.380\n maybe they'll be back, we'll see.\n\n14:15.460 --> 14:17.700\n Let me, on a small tangent, say,\n\n14:17.700 --> 14:19.100\n do you think they'll be back?\n\n14:19.100 --> 14:21.340\n So, so much of the breakthroughs recently\n\n14:21.340 --> 14:24.420\n that we'll talk about on natural language processing\n\n14:24.420 --> 14:28.060\n and language modeling has been with transformers\n\n14:28.060 --> 14:29.980\n that don't emphasize recurrence.\n\n14:30.860 --> 14:33.300\n Do you think recurrence will make a comeback?\n\n14:33.300 --> 14:37.020\n Well, some kind of recurrence, I think very likely.\n\n14:37.020 --> 14:41.500\n Recurrent neural networks, as they're typically thought of\n\n14:41.500 --> 14:44.980\n for processing sequences, I think it's also possible.\n\n14:44.980 --> 14:47.940\n What is, to you, a recurrent neural network?\n\n14:47.940 --> 14:49.300\n In generally speaking, I guess,\n\n14:49.300 --> 14:50.940\n what is a recurrent neural network?\n\n14:50.940 --> 14:52.380\n You have a neural network which maintains\n\n14:52.380 --> 14:54.940\n a high dimensional hidden state,\n\n14:54.940 --> 14:56.820\n and then when an observation arrives,\n\n14:56.820 --> 14:59.300\n it updates its high dimensional hidden state\n\n14:59.300 --> 15:03.460\n through its connections in some way.\n\n15:03.460 --> 15:08.140\n So do you think, that's what expert systems did, right?\n\n15:08.140 --> 15:12.380\n Symbolic AI, the knowledge based,\n\n15:12.380 --> 15:17.220\n growing a knowledge base is maintaining a hidden state,\n\n15:17.220 --> 15:18.460\n which is its knowledge base,\n\n15:18.460 --> 15:20.300\n and is growing it by sequential processing.\n\n15:20.300 --> 15:22.700\n Do you think of it more generally in that way,\n\n15:22.700 --> 15:27.700\n or is it simply, is it the more constrained form\n\n15:28.300 --> 15:31.340\n of a hidden state with certain kind of gating units\n\n15:31.340 --> 15:34.500\n that we think of as today with LSTMs and that?\n\n15:34.500 --> 15:36.220\n I mean, the hidden state is technically\n\n15:36.220 --> 15:37.820\n what you described there, the hidden state\n\n15:37.820 --> 15:41.340\n that goes inside the LSTM or the RNN or something like this.\n\n15:41.340 --> 15:43.220\n But then what should be contained,\n\n15:43.220 --> 15:46.300\n if you want to make the expert system analogy,\n\n15:46.300 --> 15:49.140\n I'm not, I mean, you could say that\n\n15:49.140 --> 15:51.060\n the knowledge is stored in the connections,\n\n15:51.060 --> 15:53.220\n and then the short term processing\n\n15:53.220 --> 15:55.420\n is done in the hidden state.\n\n15:56.300 --> 15:58.460\n Yes, could you say that?\n\n15:58.460 --> 16:01.660\n So sort of, do you think there's a future of building\n\n16:01.660 --> 16:05.620\n large scale knowledge bases within the neural networks?\n\n16:05.620 --> 16:06.460\n Definitely.\n\n16:09.020 --> 16:11.180\n So we're gonna pause on that confidence,\n\n16:11.180 --> 16:12.740\n because I want to explore that.\n\n16:12.740 --> 16:14.940\n Well, let me zoom back out and ask,\n\n16:16.900 --> 16:19.340\n back to the history of ImageNet.\n\n16:19.340 --> 16:21.380\n Neural networks have been around for many decades,\n\n16:21.380 --> 16:22.740\n as you mentioned.\n\n16:22.740 --> 16:24.260\n What do you think were the key ideas\n\n16:24.260 --> 16:25.860\n that led to their success,\n\n16:25.860 --> 16:28.700\n that ImageNet moment and beyond,\n\n16:28.700 --> 16:32.540\n the success in the past 10 years?\n\n16:32.540 --> 16:33.500\n Okay, so the question is,\n\n16:33.500 --> 16:35.500\n to make sure I didn't miss anything,\n\n16:35.500 --> 16:37.460\n the key ideas that led to the success\n\n16:37.460 --> 16:39.340\n of deep learning over the past 10 years.\n\n16:39.340 --> 16:42.860\n Exactly, even though the fundamental thing\n\n16:42.860 --> 16:45.340\n behind deep learning has been around for much longer.\n\n16:45.340 --> 16:50.340\n So the key idea about deep learning,\n\n16:51.300 --> 16:53.900\n or rather the key fact about deep learning\n\n16:53.900 --> 16:58.220\n before deep learning started to be successful,\n\n16:58.220 --> 16:59.740\n is that it was underestimated.\n\n17:01.260 --> 17:02.860\n People who worked in machine learning\n\n17:02.860 --> 17:06.220\n simply didn't think that neural networks could do much.\n\n17:06.220 --> 17:08.740\n People didn't believe that large neural networks\n\n17:08.740 --> 17:10.500\n could be trained.\n\n17:10.500 --> 17:13.340\n People thought that, well, there was lots of,\n\n17:13.340 --> 17:15.620\n there was a lot of debate going on in machine learning\n\n17:15.620 --> 17:17.260\n about what are the right methods and so on.\n\n17:17.260 --> 17:21.300\n And people were arguing because there were no,\n\n17:21.300 --> 17:23.340\n there was no way to get hard facts.\n\n17:23.340 --> 17:25.420\n And by that, I mean, there were no benchmarks\n\n17:25.420 --> 17:28.420\n which were truly hard that if you do really well on them,\n\n17:28.420 --> 17:31.660\n then you can say, look, here's my system.\n\n17:32.500 --> 17:33.900\n That's when you switch from,\n\n17:35.220 --> 17:37.620\n that's when this field becomes a little bit more\n\n17:37.620 --> 17:38.580\n of an engineering field.\n\n17:38.580 --> 17:39.620\n So in terms of deep learning,\n\n17:39.620 --> 17:41.420\n to answer the question directly,\n\n17:42.300 --> 17:43.500\n the ideas were all there.\n\n17:43.500 --> 17:46.780\n The thing that was missing was a lot of supervised data\n\n17:46.780 --> 17:47.900\n and a lot of compute.\n\n17:49.700 --> 17:52.580\n Once you have a lot of supervised data and a lot of compute,\n\n17:52.580 --> 17:54.700\n then there is a third thing which is needed as well.\n\n17:54.700 --> 17:56.340\n And that is conviction.\n\n17:56.340 --> 17:59.140\n Conviction that if you take the right stuff,\n\n17:59.140 --> 18:01.700\n which already exists, and apply and mix it\n\n18:01.700 --> 18:03.540\n with a lot of data and a lot of compute,\n\n18:03.540 --> 18:04.980\n that it will in fact work.\n\n18:05.940 --> 18:07.740\n And so that was the missing piece.\n\n18:07.740 --> 18:10.660\n It was, you had the, you needed the data,\n\n18:10.660 --> 18:14.140\n you needed the compute, which showed up in terms of GPUs,\n\n18:14.140 --> 18:15.780\n and you needed the conviction to realize\n\n18:15.780 --> 18:17.540\n that you need to mix them together.\n\n18:18.420 --> 18:19.420\n So that's really interesting.\n\n18:19.420 --> 18:23.100\n So I guess the presence of compute\n\n18:23.100 --> 18:25.140\n and the presence of supervised data\n\n18:26.100 --> 18:29.660\n allowed the empirical evidence to do the convincing\n\n18:29.660 --> 18:32.020\n of the majority of the computer science community.\n\n18:32.020 --> 18:36.860\n So I guess there's a key moment with Jitendra Malik\n\n18:36.860 --> 18:41.860\n and Alex Alyosha Efros who were very skeptical, right?\n\n18:42.580 --> 18:43.980\n And then there's a Jeffrey Hinton\n\n18:43.980 --> 18:46.660\n that was the opposite of skeptical.\n\n18:46.660 --> 18:48.220\n And there was a convincing moment.\n\n18:48.220 --> 18:50.220\n And I think ImageNet had served as that moment.\n\n18:50.220 --> 18:51.060\n That's right.\n\n18:51.060 --> 18:52.940\n And they represented this kind of,\n\n18:52.940 --> 18:55.860\n were the big pillars of computer vision community,\n\n18:55.860 --> 18:59.700\n kind of the wizards got together,\n\n18:59.700 --> 19:01.460\n and then all of a sudden there was a shift.\n\n19:01.460 --> 19:05.260\n And it's not enough for the ideas to all be there\n\n19:05.260 --> 19:06.300\n and the compute to be there,\n\n19:06.300 --> 19:11.380\n it's for it to convince the cynicism that existed.\n\n19:11.380 --> 19:14.020\n It's interesting that people just didn't believe\n\n19:14.020 --> 19:15.900\n for a couple of decades.\n\n19:15.900 --> 19:18.540\n Yeah, well, but it's more than that.\n\n19:18.540 --> 19:20.820\n It's kind of, when put this way,\n\n19:20.820 --> 19:23.140\n it sounds like, well, those silly people\n\n19:23.140 --> 19:25.540\n who didn't believe, what were they missing?\n\n19:25.540 --> 19:27.500\n But in reality, things were confusing\n\n19:27.500 --> 19:30.220\n because neural networks really did not work on anything.\n\n19:30.220 --> 19:31.420\n And they were not the best method\n\n19:31.420 --> 19:33.540\n on pretty much anything as well.\n\n19:33.540 --> 19:35.780\n And it was pretty rational to say,\n\n19:35.780 --> 19:37.900\n yeah, this stuff doesn't have any traction.\n\n19:39.580 --> 19:42.260\n And that's why you need to have these very hard tasks\n\n19:42.260 --> 19:44.860\n which produce undeniable evidence.\n\n19:44.860 --> 19:46.900\n And that's how we make progress.\n\n19:46.900 --> 19:48.580\n And that's why the field is making progress today\n\n19:48.580 --> 19:50.660\n because we have these hard benchmarks\n\n19:50.660 --> 19:52.740\n which represent true progress.\n\n19:52.740 --> 19:56.660\n And so, and this is why we are able to avoid endless debate.\n\n19:58.300 --> 20:00.500\n So incredibly you've contributed\n\n20:00.500 --> 20:03.020\n some of the biggest recent ideas in AI\n\n20:03.020 --> 20:07.020\n in computer vision, language, natural language processing,\n\n20:07.020 --> 20:11.300\n reinforcement learning, sort of everything in between,\n\n20:11.300 --> 20:12.500\n maybe not GANs.\n\n20:12.500 --> 20:16.180\n But there may not be a topic you haven't touched.\n\n20:16.180 --> 20:19.580\n And of course, the fundamental science of deep learning.\n\n20:19.580 --> 20:24.140\n What is the difference to you between vision, language,\n\n20:24.140 --> 20:26.900\n and as in reinforcement learning, action,\n\n20:26.900 --> 20:28.260\n as learning problems?\n\n20:28.260 --> 20:29.540\n And what are the commonalities?\n\n20:29.540 --> 20:31.500\n Do you see them as all interconnected?\n\n20:31.500 --> 20:33.780\n Are they fundamentally different domains\n\n20:33.780 --> 20:36.740\n that require different approaches?\n\n20:38.180 --> 20:39.620\n Okay, that's a good question.\n\n20:39.620 --> 20:41.860\n Machine learning is a field with a lot of unity,\n\n20:41.860 --> 20:44.060\n a huge amount of unity.\n\n20:44.060 --> 20:45.300\n In fact. What do you mean by unity?\n\n20:45.300 --> 20:48.340\n Like overlap of ideas?\n\n20:48.340 --> 20:50.140\n Overlap of ideas, overlap of principles.\n\n20:50.140 --> 20:52.660\n In fact, there's only one or two or three principles\n\n20:52.660 --> 20:54.340\n which are very, very simple.\n\n20:54.340 --> 20:57.340\n And then they apply in almost the same way,\n\n20:57.340 --> 20:59.940\n in almost the same way to the different modalities,\n\n20:59.940 --> 21:01.340\n to the different problems.\n\n21:01.340 --> 21:04.100\n And that's why today, when someone writes a paper\n\n21:04.100 --> 21:07.140\n on improving optimization of deep learning and vision,\n\n21:07.140 --> 21:09.300\n it improves the different NLP applications\n\n21:09.300 --> 21:10.140\n and it improves the different\n\n21:10.140 --> 21:12.340\n reinforcement learning applications.\n\n21:12.340 --> 21:13.260\n Reinforcement learning.\n\n21:13.260 --> 21:15.820\n So I would say that computer vision\n\n21:15.820 --> 21:18.620\n and NLP are very similar to each other.\n\n21:18.620 --> 21:20.980\n Today they differ in that they have\n\n21:20.980 --> 21:22.180\n slightly different architectures.\n\n21:22.180 --> 21:23.900\n We use transformers in NLP\n\n21:23.900 --> 21:26.500\n and we use convolutional neural networks in vision.\n\n21:26.500 --> 21:28.900\n But it's also possible that one day this will change\n\n21:28.900 --> 21:31.820\n and everything will be unified with a single architecture.\n\n21:31.820 --> 21:33.660\n Because if you go back a few years ago\n\n21:33.660 --> 21:35.420\n in natural language processing,\n\n21:36.580 --> 21:39.340\n there were a huge number of architectures\n\n21:39.340 --> 21:42.260\n for every different tiny problem had its own architecture.\n\n21:43.380 --> 21:45.900\n Today, there's just one transformer\n\n21:45.900 --> 21:47.460\n for all those different tasks.\n\n21:47.460 --> 21:49.700\n And if you go back in time even more,\n\n21:49.700 --> 21:51.380\n you had even more and more fragmentation\n\n21:51.380 --> 21:53.820\n and every little problem in AI\n\n21:53.820 --> 21:55.940\n had its own little subspecialization\n\n21:55.940 --> 21:58.660\n and sub, you know, little set of collection of skills,\n\n21:58.660 --> 22:00.980\n people who would know how to engineer the features.\n\n22:00.980 --> 22:02.900\n Now it's all been subsumed by deep learning.\n\n22:02.900 --> 22:04.180\n We have this unification.\n\n22:04.180 --> 22:06.860\n And so I expect vision to become unified\n\n22:06.860 --> 22:08.540\n with natural language as well.\n\n22:08.540 --> 22:10.500\n Or rather, I shouldn't say expect, I think it's possible.\n\n22:10.500 --> 22:12.500\n I don't wanna be too sure because\n\n22:12.500 --> 22:13.780\n I think on the convolutional neural net\n\n22:13.780 --> 22:15.540\n is very computationally efficient.\n\n22:15.540 --> 22:16.860\n RL is different.\n\n22:16.860 --> 22:18.860\n RL does require slightly different techniques\n\n22:18.860 --> 22:20.820\n because you really do need to take action.\n\n22:20.820 --> 22:23.860\n You really need to do something about exploration.\n\n22:23.860 --> 22:26.020\n Your variance is much higher.\n\n22:26.020 --> 22:28.220\n But I think there is a lot of unity even there.\n\n22:28.220 --> 22:29.980\n And I would expect, for example, that at some point\n\n22:29.980 --> 22:33.500\n there will be some broader unification\n\n22:33.500 --> 22:35.260\n between RL and supervised learning\n\n22:35.260 --> 22:37.180\n where somehow the RL will be making decisions\n\n22:37.180 --> 22:38.580\n to make the supervised learning go better.\n\n22:38.580 --> 22:41.780\n And it will be, I imagine, one big black box\n\n22:41.780 --> 22:44.980\n and you just throw, you know, you shovel things into it\n\n22:44.980 --> 22:46.260\n and it just figures out what to do\n\n22:46.260 --> 22:48.060\n with whatever you shovel at it.\n\n22:48.060 --> 22:50.740\n I mean, reinforcement learning has some aspects\n\n22:50.740 --> 22:55.180\n of language and vision combined almost.\n\n22:55.180 --> 22:57.780\n There's elements of a long term memory\n\n22:57.780 --> 22:58.900\n that you should be utilizing\n\n22:58.900 --> 23:03.100\n and there's elements of a really rich sensory space.\n\n23:03.100 --> 23:08.100\n So it seems like the union of the two or something like that.\n\n23:08.420 --> 23:10.020\n I'd say something slightly differently.\n\n23:10.020 --> 23:12.740\n I'd say that reinforcement learning is neither,\n\n23:12.740 --> 23:14.900\n but it naturally interfaces\n\n23:14.900 --> 23:17.380\n and integrates with the two of them.\n\n23:17.380 --> 23:19.300\n Do you think action is fundamentally different?\n\n23:19.300 --> 23:21.340\n So yeah, what is interesting about,\n\n23:21.340 --> 23:26.060\n what is unique about policy of learning to act?\n\n23:26.060 --> 23:27.540\n Well, so one example, for instance,\n\n23:27.540 --> 23:29.860\n is that when you learn to act,\n\n23:29.860 --> 23:33.300\n you are fundamentally in a non stationary world\n\n23:33.300 --> 23:35.860\n because as your actions change,\n\n23:35.860 --> 23:38.140\n the things you see start changing.\n\n23:38.140 --> 23:41.380\n You experience the world in a different way.\n\n23:41.380 --> 23:43.300\n And this is not the case for\n\n23:43.300 --> 23:44.980\n the more traditional static problem\n\n23:44.980 --> 23:46.380\n where you have some distribution\n\n23:46.380 --> 23:48.580\n and you just apply a model to that distribution.\n\n23:49.540 --> 23:51.260\n You think it's a fundamentally different problem\n\n23:51.260 --> 23:55.060\n or is it just a more difficult generalization\n\n23:55.060 --> 23:57.020\n of the problem of understanding?\n\n23:57.020 --> 23:59.860\n I mean, it's a question of definitions almost.\n\n23:59.860 --> 24:02.020\n There is a huge amount of commonality for sure.\n\n24:02.020 --> 24:04.180\n You take gradients, you try, you take gradients.\n\n24:04.180 --> 24:06.180\n We try to approximate gradients in both cases.\n\n24:06.180 --> 24:08.020\n In the case of reinforcement learning,\n\n24:08.020 --> 24:11.180\n you have some tools to reduce the variance of the gradients.\n\n24:11.180 --> 24:12.020\n You do that.\n\n24:13.020 --> 24:13.980\n There's lots of commonality.\n\n24:13.980 --> 24:16.340\n Use the same neural net in both cases.\n\n24:16.340 --> 24:18.940\n You compute the gradient, you apply Adam in both cases.\n\n24:20.820 --> 24:24.300\n So, I mean, there's lots in common for sure,\n\n24:24.300 --> 24:26.900\n but there are some small differences\n\n24:26.900 --> 24:28.940\n which are not completely insignificant.\n\n24:28.940 --> 24:30.980\n It's really just a matter of your point of view,\n\n24:30.980 --> 24:32.700\n what frame of reference,\n\n24:32.700 --> 24:35.020\n how much do you wanna zoom in or out\n\n24:35.020 --> 24:37.260\n as you look at these problems?\n\n24:37.260 --> 24:39.820\n Which problem do you think is harder?\n\n24:39.820 --> 24:41.660\n So people like Noam Chomsky believe\n\n24:41.660 --> 24:43.980\n that language is fundamental to everything.\n\n24:43.980 --> 24:45.700\n So it underlies everything.\n\n24:45.700 --> 24:48.660\n Do you think language understanding is harder\n\n24:48.660 --> 24:51.660\n than visual scene understanding or vice versa?\n\n24:52.580 --> 24:56.260\n I think that asking if a problem is hard is slightly wrong.\n\n24:56.260 --> 24:57.500\n I think the question is a little bit wrong\n\n24:57.500 --> 24:59.460\n and I wanna explain why.\n\n24:59.460 --> 25:02.580\n So what does it mean for a problem to be hard?\n\n25:04.340 --> 25:07.220\n Okay, the non interesting dumb answer to that\n\n25:07.220 --> 25:10.700\n is there's a benchmark\n\n25:10.700 --> 25:13.660\n and there's a human level performance on that benchmark\n\n25:13.660 --> 25:16.660\n and how is the effort required\n\n25:16.660 --> 25:19.060\n to reach the human level benchmark.\n\n25:19.060 --> 25:20.620\n So from the perspective of how much\n\n25:20.620 --> 25:25.280\n until we get to human level on a very good benchmark.\n\n25:25.280 --> 25:28.840\n Yeah, I understand what you mean by that.\n\n25:28.840 --> 25:32.200\n So what I was going to say that a lot of it depends on,\n\n25:32.200 --> 25:34.000\n once you solve a problem, it stops being hard\n\n25:34.000 --> 25:35.960\n and that's always true.\n\n25:35.960 --> 25:38.160\n And so whether something is hard or not depends\n\n25:38.160 --> 25:39.720\n on what our tools can do today.\n\n25:39.720 --> 25:43.680\n So you say today through human level,\n\n25:43.680 --> 25:46.280\n language understanding and visual perception are hard\n\n25:46.280 --> 25:48.920\n in the sense that there is no way\n\n25:48.920 --> 25:52.000\n of solving the problem completely in the next three months.\n\n25:52.000 --> 25:53.920\n So I agree with that statement.\n\n25:53.920 --> 25:56.600\n Beyond that, my guess would be as good as yours,\n\n25:56.600 --> 25:57.440\n I don't know.\n\n25:57.440 --> 26:00.360\n Oh, okay, so you don't have a fundamental intuition\n\n26:00.360 --> 26:02.800\n about how hard language understanding is.\n\n26:02.800 --> 26:04.280\n I think, I know I changed my mind.\n\n26:04.280 --> 26:06.800\n I'd say language is probably going to be harder.\n\n26:06.800 --> 26:09.160\n I mean, it depends on how you define it.\n\n26:09.160 --> 26:11.240\n Like if you mean absolute top notch,\n\n26:11.240 --> 26:14.000\n 100% language understanding, I'll go with language.\n\n26:16.160 --> 26:18.880\n But then if I show you a piece of paper with letters on it,\n\n26:18.880 --> 26:21.720\n is that, you see what I mean?\n\n26:21.720 --> 26:22.600\n You have a vision system,\n\n26:22.600 --> 26:25.080\n you say it's the best human level vision system.\n\n26:25.080 --> 26:28.760\n I show you, I open a book and I show you letters.\n\n26:28.760 --> 26:30.880\n Will it understand how these letters form into word\n\n26:30.880 --> 26:32.240\n and sentences and meaning?\n\n26:32.240 --> 26:33.720\n Is this part of the vision problem?\n\n26:33.720 --> 26:36.080\n Where does vision end and language begin?\n\n26:36.080 --> 26:38.240\n Yeah, so Chomsky would say it starts at language.\n\n26:38.240 --> 26:40.440\n So vision is just a little example of the kind\n\n26:40.440 --> 26:45.440\n of a structure and fundamental hierarchy of ideas\n\n26:46.520 --> 26:49.080\n that's already represented in our brains somehow\n\n26:49.080 --> 26:51.400\n that's represented through language.\n\n26:51.400 --> 26:56.400\n But where does vision stop and language begin?\n\n26:57.960 --> 27:00.640\n That's a really interesting question.\n\n27:07.760 --> 27:09.880\n So one possibility is that it's impossible\n\n27:09.880 --> 27:14.720\n to achieve really deep understanding in either images\n\n27:14.720 --> 27:18.400\n or language without basically using the same kind of system.\n\n27:18.400 --> 27:21.440\n So you're going to get the other for free.\n\n27:21.440 --> 27:23.080\n I think it's pretty likely that yes,\n\n27:23.080 --> 27:25.840\n if we can get one, our machine learning is probably\n\n27:25.840 --> 27:27.320\n that good that we can get the other.\n\n27:27.320 --> 27:30.160\n But I'm not 100% sure.\n\n27:30.160 --> 27:34.520\n And also, I think a lot of it really does depend\n\n27:34.520 --> 27:35.560\n on your definitions.\n\n27:36.680 --> 27:37.800\n Definitions of?\n\n27:37.800 --> 27:39.160\n Of like perfect vision.\n\n27:40.040 --> 27:43.320\n Because reading is vision, but should it count?\n\n27:44.640 --> 27:47.440\n Yeah, to me, so my definition is if a system looked\n\n27:47.440 --> 27:52.240\n at an image and then a system looked at a piece of text\n\n27:52.240 --> 27:56.040\n and then told me something about that\n\n27:56.040 --> 27:57.480\n and I was really impressed.\n\n27:58.400 --> 27:59.480\n That's relative.\n\n27:59.480 --> 28:01.280\n You'll be impressed for half an hour\n\n28:01.280 --> 28:02.520\n and then you're gonna say, well, I mean,\n\n28:02.520 --> 28:05.200\n all the systems do that, but here's the thing they don't do.\n\n28:05.200 --> 28:07.120\n Yeah, but I don't have that with humans.\n\n28:07.120 --> 28:08.920\n Humans continue to impress me.\n\n28:08.920 --> 28:09.760\n Is that true?\n\n28:10.600 --> 28:14.000\n Well, the ones, okay, so I'm a fan of monogamy.\n\n28:14.000 --> 28:16.000\n So I like the idea of marrying somebody,\n\n28:16.000 --> 28:18.080\n being with them for several decades.\n\n28:18.080 --> 28:20.600\n So I believe in the fact that yes, it's possible\n\n28:20.600 --> 28:22.960\n to have somebody continuously giving you\n\n28:24.480 --> 28:28.560\n pleasurable, interesting, witty new ideas, friends.\n\n28:28.560 --> 28:29.960\n Yeah, I think so.\n\n28:29.960 --> 28:32.080\n They continue to surprise you.\n\n28:32.080 --> 28:37.080\n The surprise, it's that injection of randomness.\n\n28:37.080 --> 28:47.080\n It seems to be a nice source of, yeah, continued inspiration,\n\n28:47.080 --> 28:48.680\n like the wit, the humor.\n\n28:48.680 --> 28:53.560\n I think, yeah, that would be,\n\n28:53.560 --> 28:54.840\n it's a very subjective test,\n\n28:54.840 --> 28:58.480\n but I think if you have enough humans in the room.\n\n28:58.480 --> 29:00.440\n Yeah, I understand what you mean.\n\n29:00.440 --> 29:02.000\n Yeah, I feel like I misunderstood\n\n29:02.000 --> 29:02.960\n what you meant by impressing you.\n\n29:02.960 --> 29:06.440\n I thought you meant to impress you with its intelligence,\n\n29:06.440 --> 29:10.120\n with how well it understands an image.\n\n29:10.120 --> 29:11.640\n I thought you meant something like,\n\n29:11.640 --> 29:13.200\n I'm gonna show it a really complicated image\n\n29:13.200 --> 29:14.040\n and it's gonna get it right.\n\n29:14.040 --> 29:15.720\n And you're gonna say, wow, that's really cool.\n\n29:15.720 --> 29:19.880\n Our systems of January 2020 have not been doing that.\n\n29:19.880 --> 29:23.440\n Yeah, no, I think it all boils down to like\n\n29:23.440 --> 29:26.040\n the reason people click like on stuff on the internet,\n\n29:26.040 --> 29:28.280\n which is like, it makes them laugh.\n\n29:28.280 --> 29:32.640\n So it's like humor or wit or insight.\n\n29:32.640 --> 29:35.360\n I'm sure we'll get that as well.\n\n29:35.360 --> 29:38.120\n So forgive the romanticized question,\n\n29:38.120 --> 29:40.400\n but looking back to you,\n\n29:40.400 --> 29:43.080\n what is the most beautiful or surprising idea\n\n29:43.080 --> 29:46.760\n in deep learning or AI in general you've come across?\n\n29:46.760 --> 29:49.160\n So I think the most beautiful thing about deep learning\n\n29:49.160 --> 29:51.640\n is that it actually works.\n\n29:51.640 --> 29:53.120\n And I mean it, because you got these ideas,\n\n29:53.120 --> 29:54.640\n you got the little neural network,\n\n29:54.640 --> 29:56.520\n you got the back propagation algorithm.\n\n29:58.920 --> 30:00.640\n And then you've got some theories as to,\n\n30:00.640 --> 30:02.040\n this is kind of like the brain.\n\n30:02.040 --> 30:03.560\n So maybe if you make it large,\n\n30:03.560 --> 30:04.840\n if you make the neural network large\n\n30:04.840 --> 30:05.920\n and you train it on a lot of data,\n\n30:05.920 --> 30:09.640\n then it will do the same function that the brain does.\n\n30:09.640 --> 30:12.480\n And it turns out to be true, that's crazy.\n\n30:12.480 --> 30:14.120\n And now we just train these neural networks\n\n30:14.120 --> 30:16.640\n and you make them larger and they keep getting better.\n\n30:16.640 --> 30:17.880\n And I find it unbelievable.\n\n30:17.880 --> 30:20.600\n I find it unbelievable that this whole AI stuff\n\n30:20.600 --> 30:22.480\n with neural networks works.\n\n30:22.480 --> 30:24.960\n Have you built up an intuition of why?\n\n30:24.960 --> 30:27.920\n Are there a lot of bits and pieces of intuitions,\n\n30:27.920 --> 30:31.320\n of insights of why this whole thing works?\n\n30:31.320 --> 30:33.240\n I mean, some, definitely.\n\n30:33.240 --> 30:36.080\n While we know that optimization, we now have good,\n\n30:37.400 --> 30:40.800\n we've had lots of empirical,\n\n30:40.800 --> 30:42.320\n huge amounts of empirical reasons\n\n30:42.320 --> 30:44.280\n to believe that optimization should work\n\n30:44.280 --> 30:46.200\n on most problems we care about.\n\n30:47.520 --> 30:48.680\n Do you have insights of why?\n\n30:48.680 --> 30:50.720\n So you just said empirical evidence.\n\n30:50.720 --> 30:55.720\n Is most of your sort of empirical evidence\n\n30:56.760 --> 30:58.360\n kind of convinces you?\n\n30:58.360 --> 31:00.360\n It's like evolution is empirical.\n\n31:00.360 --> 31:01.400\n It shows you that, look,\n\n31:01.400 --> 31:03.920\n this evolutionary process seems to be a good way\n\n31:03.920 --> 31:08.240\n to design organisms that survive in their environment,\n\n31:08.240 --> 31:11.400\n but it doesn't really get you to the insights\n\n31:11.400 --> 31:13.960\n of how the whole thing works.\n\n31:13.960 --> 31:16.480\n I think a good analogy is physics.\n\n31:16.480 --> 31:19.040\n You know how you say, hey, let's do some physics calculation\n\n31:19.040 --> 31:20.480\n and come up with some new physics theory\n\n31:20.480 --> 31:21.720\n and make some prediction.\n\n31:21.720 --> 31:23.920\n But then you got around the experiment.\n\n31:23.920 --> 31:26.040\n You know, you got around the experiment, it's important.\n\n31:26.040 --> 31:27.440\n So it's a bit the same here,\n\n31:27.440 --> 31:29.760\n except that maybe sometimes the experiment\n\n31:29.760 --> 31:31.040\n came before the theory.\n\n31:31.040 --> 31:32.040\n But it still is the case.\n\n31:32.040 --> 31:33.840\n You know, you have some data\n\n31:33.840 --> 31:35.000\n and you come up with some prediction.\n\n31:35.000 --> 31:36.560\n You say, yeah, let's make a big neural network.\n\n31:36.560 --> 31:37.400\n Let's train it.\n\n31:37.400 --> 31:39.840\n And it's going to work much better than anything before it.\n\n31:39.840 --> 31:41.440\n And it will in fact continue to get better\n\n31:41.440 --> 31:42.720\n as you make it larger.\n\n31:42.720 --> 31:43.600\n And it turns out to be true.\n\n31:43.600 --> 31:47.360\n That's amazing when a theory is validated like this.\n\n31:47.360 --> 31:48.720\n It's not a mathematical theory.\n\n31:48.720 --> 31:50.800\n It's more of a biological theory almost.\n\n31:51.680 --> 31:53.960\n So I think there are not terrible analogies\n\n31:53.960 --> 31:55.560\n between deep learning and biology.\n\n31:55.560 --> 31:57.520\n I would say it's like the geometric mean\n\n31:57.520 --> 31:58.760\n of biology and physics.\n\n31:58.760 --> 32:00.240\n That's deep learning.\n\n32:00.240 --> 32:03.880\n The geometric mean of biology and physics.\n\n32:03.880 --> 32:05.160\n I think I'm going to need a few hours\n\n32:05.160 --> 32:06.560\n to wrap my head around that.\n\n32:07.680 --> 32:10.480\n Because just to find the geometric,\n\n32:10.480 --> 32:15.480\n just to find the set of what biology represents.\n\n32:16.480 --> 32:19.480\n Well, in biology, things are really complicated.\n\n32:19.480 --> 32:21.000\n Theories are really, really,\n\n32:21.000 --> 32:22.840\n it's really hard to have good predictive theory.\n\n32:22.840 --> 32:25.400\n And in physics, the theories are too good.\n\n32:25.400 --> 32:27.920\n In physics, people make these super precise theories\n\n32:27.920 --> 32:29.360\n which make these amazing predictions.\n\n32:29.360 --> 32:31.440\n And in machine learning, we're kind of in between.\n\n32:31.440 --> 32:33.800\n Kind of in between, but it'd be nice\n\n32:33.800 --> 32:35.920\n if machine learning somehow helped us\n\n32:35.920 --> 32:37.720\n discover the unification of the two\n\n32:37.720 --> 32:39.520\n as opposed to sort of the in between.\n\n32:40.800 --> 32:41.640\n But you're right.\n\n32:41.640 --> 32:44.920\n That's, you're kind of trying to juggle both.\n\n32:44.920 --> 32:46.760\n So do you think there are still beautiful\n\n32:46.760 --> 32:48.800\n and mysterious properties in neural networks\n\n32:48.800 --> 32:50.160\n that are yet to be discovered?\n\n32:50.160 --> 32:51.360\n Definitely.\n\n32:51.360 --> 32:53.560\n I think that we are still massively underestimating\n\n32:53.560 --> 32:54.400\n deep learning.\n\n32:55.440 --> 32:56.640\n What do you think it will look like?\n\n32:56.640 --> 32:59.560\n Like what, if I knew, I would have done it, you know?\n\n33:01.080 --> 33:04.000\n So, but if you look at all the progress\n\n33:04.000 --> 33:07.040\n from the past 10 years, I would say most of it,\n\n33:07.040 --> 33:08.880\n I would say there've been a few cases\n\n33:08.880 --> 33:12.080\n where some were things that felt like really new ideas\n\n33:12.080 --> 33:15.080\n showed up, but by and large it was every year\n\n33:15.080 --> 33:17.160\n we thought, okay, deep learning goes this far.\n\n33:17.160 --> 33:19.000\n Nope, it actually goes further.\n\n33:19.000 --> 33:22.480\n And then the next year, okay, now this is peak deep learning.\n\n33:22.480 --> 33:23.320\n We are really done.\n\n33:23.320 --> 33:24.440\n Nope, it goes further.\n\n33:24.440 --> 33:26.040\n It just keeps going further each year.\n\n33:26.040 --> 33:27.600\n So that means that we keep underestimating,\n\n33:27.600 --> 33:29.160\n we keep not understanding it.\n\n33:29.160 --> 33:31.360\n It has surprising properties all the time.\n\n33:31.360 --> 33:33.600\n Do you think it's getting harder and harder?\n\n33:33.600 --> 33:34.440\n To make progress?\n\n33:34.440 --> 33:36.000\n Need to make progress?\n\n33:36.000 --> 33:36.840\n It depends on what you mean.\n\n33:36.840 --> 33:39.960\n I think the field will continue to make very robust progress\n\n33:39.960 --> 33:41.120\n for quite a while.\n\n33:41.120 --> 33:42.800\n I think for individual researchers,\n\n33:42.800 --> 33:46.120\n especially people who are doing research,\n\n33:46.120 --> 33:48.240\n it can be harder because there is a very large number\n\n33:48.240 --> 33:50.080\n of researchers right now.\n\n33:50.080 --> 33:51.800\n I think that if you have a lot of compute,\n\n33:51.800 --> 33:54.720\n then you can make a lot of very interesting discoveries,\n\n33:54.720 --> 33:57.440\n but then you have to deal with the challenge\n\n33:57.440 --> 34:01.680\n of managing a huge compute cluster\n\n34:01.680 --> 34:02.520\n to run your experiments.\n\n34:02.520 --> 34:03.360\n It's a little bit harder.\n\n34:03.360 --> 34:04.920\n So I'm asking all these questions\n\n34:04.920 --> 34:06.440\n that nobody knows the answer to,\n\n34:06.440 --> 34:08.280\n but you're one of the smartest people I know,\n\n34:08.280 --> 34:10.440\n so I'm gonna keep asking.\n\n34:10.440 --> 34:12.400\n So let's imagine all the breakthroughs\n\n34:12.400 --> 34:15.240\n that happen in the next 30 years in deep learning.\n\n34:15.240 --> 34:17.120\n Do you think most of those breakthroughs\n\n34:17.120 --> 34:20.840\n can be done by one person with one computer?\n\n34:22.040 --> 34:23.760\n Sort of in the space of breakthroughs,\n\n34:23.760 --> 34:25.680\n do you think compute will be,\n\n34:26.840 --> 34:31.840\n compute and large efforts will be necessary?\n\n34:32.360 --> 34:34.040\n I mean, I can't be sure.\n\n34:34.040 --> 34:36.680\n When you say one computer, you mean how large?\n\n34:36.680 --> 34:40.760\n You're clever.\n\n34:40.760 --> 34:42.640\n I mean, one GPU.\n\n34:42.640 --> 34:43.960\n I see.\n\n34:43.960 --> 34:46.120\n I think it's pretty unlikely.\n\n34:47.520 --> 34:48.720\n I think it's pretty unlikely.\n\n34:48.720 --> 34:51.000\n I think that there are many,\n\n34:51.000 --> 34:53.800\n the stack of deep learning is starting to be quite deep.\n\n34:54.680 --> 34:59.680\n If you look at it, you've got all the way from the ideas,\n\n34:59.840 --> 35:02.200\n the systems to build the data sets,\n\n35:02.200 --> 35:04.200\n the distributed programming,\n\n35:04.200 --> 35:06.480\n the building the actual cluster,\n\n35:06.480 --> 35:09.040\n the GPU programming, putting it all together.\n\n35:09.040 --> 35:10.600\n So now the stack is getting really deep\n\n35:10.600 --> 35:12.280\n and I think it becomes,\n\n35:12.280 --> 35:14.160\n it can be quite hard for a single person\n\n35:14.160 --> 35:15.680\n to become, to be world class\n\n35:15.680 --> 35:17.960\n in every single layer of the stack.\n\n35:17.960 --> 35:21.120\n What about the, what like Vlad and Ravapnik\n\n35:21.120 --> 35:23.200\n really insist on is taking MNIST\n\n35:23.200 --> 35:26.000\n and trying to learn from very few examples.\n\n35:26.000 --> 35:29.120\n So being able to learn more efficiently.\n\n35:29.120 --> 35:32.120\n Do you think that's, there'll be breakthroughs in that space\n\n35:32.120 --> 35:34.880\n that would, may not need the huge compute?\n\n35:34.880 --> 35:37.920\n I think there will be a large number of breakthroughs\n\n35:37.920 --> 35:40.640\n in general that will not need a huge amount of compute.\n\n35:40.640 --> 35:42.160\n So maybe I should clarify that.\n\n35:42.160 --> 35:45.440\n I think that some breakthroughs will require a lot of compute\n\n35:45.440 --> 35:48.680\n and I think building systems which actually do things\n\n35:48.680 --> 35:50.200\n will require a huge amount of compute.\n\n35:50.200 --> 35:51.360\n That one is pretty obvious.\n\n35:51.360 --> 35:54.720\n If you want to do X and X requires a huge neural net,\n\n35:54.720 --> 35:56.560\n you gotta get a huge neural net.\n\n35:56.560 --> 35:59.360\n But I think there will be lots of,\n\n35:59.360 --> 36:02.520\n I think there is lots of room for very important work\n\n36:02.520 --> 36:05.120\n being done by small groups and individuals.\n\n36:05.120 --> 36:07.480\n Can you maybe sort of on the topic\n\n36:07.480 --> 36:10.040\n of the science of deep learning,\n\n36:10.040 --> 36:12.000\n talk about one of the recent papers\n\n36:12.000 --> 36:15.640\n that you released, the Deep Double Descent,\n\n36:15.640 --> 36:18.120\n where bigger models and more data hurt.\n\n36:18.120 --> 36:19.600\n I think it's a really interesting paper.\n\n36:19.600 --> 36:22.280\n Can you describe the main idea?\n\n36:22.280 --> 36:23.480\n Yeah, definitely.\n\n36:23.480 --> 36:25.600\n So what happened is that some,\n\n36:25.600 --> 36:28.840\n over the years, some small number of researchers noticed\n\n36:28.840 --> 36:30.760\n that it is kind of weird that when you make\n\n36:30.760 --> 36:32.120\n the neural network larger, it works better\n\n36:32.120 --> 36:33.320\n and it seems to go in contradiction\n\n36:33.320 --> 36:34.720\n with statistical ideas.\n\n36:34.720 --> 36:36.880\n And then some people made an analysis showing\n\n36:36.880 --> 36:38.880\n that actually you got this double descent bump.\n\n36:38.880 --> 36:42.760\n And what we've done was to show that double descent occurs\n\n36:42.760 --> 36:46.400\n for pretty much all practical deep learning systems.\n\n36:46.400 --> 36:49.880\n And that it'll be also, so can you step back?\n\n36:51.560 --> 36:55.960\n What's the X axis and the Y axis of a double descent plot?\n\n36:55.960 --> 36:57.000\n Okay, great.\n\n36:57.000 --> 37:02.000\n So you can look, you can do things like,\n\n37:02.680 --> 37:04.960\n you can take your neural network\n\n37:04.960 --> 37:07.600\n and you can start increasing its size slowly\n\n37:07.600 --> 37:10.000\n while keeping your data set fixed.\n\n37:10.000 --> 37:14.760\n So if you increase the size of the neural network slowly,\n\n37:14.760 --> 37:16.880\n and if you don't do early stopping,\n\n37:16.880 --> 37:19.000\n that's a pretty important detail,\n\n37:20.360 --> 37:22.480\n then when the neural network is really small,\n\n37:22.480 --> 37:23.560\n you make it larger,\n\n37:23.560 --> 37:26.040\n you get a very rapid increase in performance.\n\n37:26.040 --> 37:27.280\n Then you continue to make it larger.\n\n37:27.280 --> 37:30.160\n And at some point performance will get worse.\n\n37:30.160 --> 37:33.920\n And it gets the worst exactly at the point\n\n37:33.920 --> 37:36.240\n at which it achieves zero training error,\n\n37:36.240 --> 37:38.640\n precisely zero training loss.\n\n37:38.640 --> 37:39.600\n And then as you make it larger,\n\n37:39.600 --> 37:41.480\n it starts to get better again.\n\n37:41.480 --> 37:42.840\n And it's kind of counterintuitive\n\n37:42.840 --> 37:44.600\n because you'd expect deep learning phenomena\n\n37:44.600 --> 37:46.800\n to be monotonic.\n\n37:46.800 --> 37:50.040\n And it's hard to be sure what it means,\n\n37:50.040 --> 37:53.120\n but it also occurs in the case of linear classifiers.\n\n37:53.120 --> 37:55.920\n And the intuition basically boils down to the following.\n\n37:57.040 --> 38:02.040\n When you have a large data set and a small model,\n\n38:03.560 --> 38:05.000\n then small, tiny random,\n\n38:05.000 --> 38:07.120\n so basically what is overfitting?\n\n38:07.120 --> 38:12.000\n Overfitting is when your model is somehow very sensitive\n\n38:12.000 --> 38:16.080\n to the small random unimportant stuff in your data set.\n\n38:16.080 --> 38:17.000\n In the training data.\n\n38:17.000 --> 38:19.000\n In the training data set, precisely.\n\n38:19.000 --> 38:23.400\n So if you have a small model and you have a big data set,\n\n38:23.400 --> 38:24.760\n and there may be some random thing,\n\n38:24.760 --> 38:27.480\n some training cases are randomly in the data set\n\n38:27.480 --> 38:29.080\n and others may not be there,\n\n38:29.080 --> 38:31.640\n but the small model is kind of insensitive\n\n38:31.640 --> 38:34.400\n to this randomness because it's the same,\n\n38:34.400 --> 38:37.080\n there is pretty much no uncertainty about the model\n\n38:37.080 --> 38:38.320\n when the data set is large.\n\n38:38.320 --> 38:39.160\n So, okay.\n\n38:39.160 --> 38:41.200\n So at the very basic level to me,\n\n38:41.200 --> 38:43.360\n it is the most surprising thing\n\n38:43.360 --> 38:48.360\n that neural networks don't overfit every time very quickly\n\n38:51.840 --> 38:54.040\n before ever being able to learn anything.\n\n38:54.040 --> 38:56.280\n The huge number of parameters.\n\n38:56.280 --> 38:57.680\n So here is, so there is one way, okay.\n\n38:57.680 --> 39:00.240\n So maybe, so let me try to give the explanation\n\n39:00.240 --> 39:02.040\n and maybe that will be, that will work.\n\n39:02.040 --> 39:03.640\n So you've got a huge neural network.\n\n39:03.640 --> 39:07.640\n Let's suppose you've got, you have a huge neural network,\n\n39:07.640 --> 39:09.760\n you have a huge number of parameters.\n\n39:09.760 --> 39:11.360\n And now let's pretend everything is linear,\n\n39:11.360 --> 39:13.120\n which is not, let's just pretend.\n\n39:13.120 --> 39:15.560\n Then there is this big subspace\n\n39:15.560 --> 39:18.040\n where your neural network achieves zero error.\n\n39:18.040 --> 39:21.920\n And SGD is going to find approximately the point.\n\n39:21.920 --> 39:22.760\n That's right.\n\n39:22.760 --> 39:24.480\n Approximately the point with the smallest norm\n\n39:24.480 --> 39:25.480\n in that subspace.\n\n39:26.720 --> 39:27.560\n Okay.\n\n39:27.560 --> 39:30.280\n And that can also be proven to be insensitive\n\n39:30.280 --> 39:33.520\n to the small randomness in the data\n\n39:33.520 --> 39:35.360\n when the dimensionality is high.\n\n39:35.360 --> 39:37.160\n But when the dimensionality of the data\n\n39:37.160 --> 39:39.360\n is equal to the dimensionality of the model,\n\n39:39.360 --> 39:41.040\n then there is a one to one correspondence\n\n39:41.040 --> 39:44.400\n between all the data sets and the models.\n\n39:44.400 --> 39:45.680\n So small changes in the data set\n\n39:45.680 --> 39:47.360\n actually lead to large changes in the model.\n\n39:47.360 --> 39:48.800\n And that's why performance gets worse.\n\n39:48.800 --> 39:51.000\n So this is the best explanation more or less.\n\n39:52.280 --> 39:54.000\n So then it would be good for the model\n\n39:54.000 --> 39:58.640\n to have more parameters, so to be bigger than the data.\n\n39:58.640 --> 39:59.480\n That's right.\n\n39:59.480 --> 40:00.800\n But only if you don't early stop.\n\n40:00.800 --> 40:02.840\n If you introduce early stop in your regularization,\n\n40:02.840 --> 40:04.640\n you can make the double descent bump\n\n40:04.640 --> 40:06.120\n almost completely disappear.\n\n40:06.120 --> 40:07.120\n What is early stop?\n\n40:07.120 --> 40:09.960\n Early stopping is when you train your model\n\n40:09.960 --> 40:12.800\n and you monitor your validation performance.\n\n40:13.640 --> 40:15.200\n And then if at some point validation performance\n\n40:15.200 --> 40:17.640\n starts to get worse, you say, okay, let's stop training.\n\n40:17.640 --> 40:20.000\n We are good enough.\n\n40:20.000 --> 40:23.160\n So the magic happens after that moment.\n\n40:23.160 --> 40:25.080\n So you don't want to do the early stopping.\n\n40:25.080 --> 40:26.680\n Well, if you don't do the early stopping,\n\n40:26.680 --> 40:29.200\n you get the very pronounced double descent.\n\n40:29.200 --> 40:31.880\n Do you have any intuition why this happens?\n\n40:31.880 --> 40:32.880\n Double descent?\n\n40:32.880 --> 40:33.880\n Oh, sorry, early stopping?\n\n40:33.880 --> 40:34.880\n No, the double descent.\n\n40:34.880 --> 40:35.880\n So the...\n\n40:35.880 --> 40:36.880\n Well, yeah, so I try...\n\n40:36.880 --> 40:37.880\n Let's see.\n\n40:37.880 --> 40:39.880\n The intuition is basically is this,\n\n40:39.880 --> 40:44.120\n that when the data set has as many degrees of freedom\n\n40:44.120 --> 40:47.560\n as the model, then there is a one to one correspondence\n\n40:47.560 --> 40:48.560\n between them.\n\n40:48.560 --> 40:50.760\n And so small changes to the data set\n\n40:50.760 --> 40:53.640\n lead to noticeable changes in the model.\n\n40:53.640 --> 40:55.920\n So your model is very sensitive to all the randomness.\n\n40:55.920 --> 40:57.960\n It is unable to discard it.\n\n40:57.960 --> 41:01.360\n Whereas it turns out that when you have\n\n41:01.360 --> 41:03.160\n a lot more data than parameters\n\n41:03.160 --> 41:05.200\n or a lot more parameters than data,\n\n41:05.200 --> 41:07.480\n the resulting solution will be insensitive\n\n41:07.480 --> 41:09.040\n to small changes in the data set.\n\n41:09.040 --> 41:12.120\n Oh, so it's able to, let's nicely put,\n\n41:12.120 --> 41:14.800\n discard the small changes, the randomness.\n\n41:14.800 --> 41:15.800\n The randomness, exactly.\n\n41:15.800 --> 41:19.120\n The spurious correlation which you don't want.\n\n41:19.120 --> 41:22.120\n Jeff Hinton suggested we need to throw back propagation.\n\n41:22.120 --> 41:23.840\n We already kind of talked about this a little bit,\n\n41:23.840 --> 41:25.720\n but he suggested that we need to throw away\n\n41:25.720 --> 41:28.160\n back propagation and start over.\n\n41:28.160 --> 41:30.680\n I mean, of course some of that is a little bit\n\n41:32.080 --> 41:34.960\n wit and humor, but what do you think?\n\n41:34.960 --> 41:36.440\n What could be an alternative method\n\n41:36.440 --> 41:37.920\n of training neural networks?\n\n41:37.920 --> 41:40.560\n Well, the thing that he said precisely is that\n\n41:40.560 --> 41:42.440\n to the extent that you can't find back propagation\n\n41:42.440 --> 41:45.960\n in the brain, it's worth seeing if we can learn something\n\n41:45.960 --> 41:47.480\n from how the brain learns.\n\n41:47.480 --> 41:48.960\n But back propagation is very useful\n\n41:48.960 --> 41:50.760\n and we should keep using it.\n\n41:50.760 --> 41:52.960\n Oh, you're saying that once we discover\n\n41:52.960 --> 41:54.720\n the mechanism of learning in the brain,\n\n41:54.720 --> 41:56.520\n or any aspects of that mechanism,\n\n41:56.520 --> 41:59.040\n we should also try to implement that in neural networks?\n\n41:59.040 --> 42:00.640\n If it turns out that we can't find\n\n42:00.640 --> 42:01.960\n back propagation in the brain.\n\n42:01.960 --> 42:04.480\n If we can't find back propagation in the brain.\n\n42:06.280 --> 42:10.160\n Well, so I guess your answer to that is\n\n42:10.160 --> 42:12.200\n back propagation is pretty damn useful.\n\n42:12.200 --> 42:14.280\n So why are we complaining?\n\n42:14.280 --> 42:16.800\n I mean, I personally am a big fan of back propagation.\n\n42:16.800 --> 42:18.760\n I think it's a great algorithm because it solves\n\n42:18.760 --> 42:20.320\n an extremely fundamental problem,\n\n42:20.320 --> 42:24.920\n which is finding a neural circuit\n\n42:24.920 --> 42:26.360\n subject to some constraints.\n\n42:27.240 --> 42:28.800\n And I don't see that problem going away.\n\n42:28.800 --> 42:33.280\n So that's why I really, I think it's pretty unlikely\n\n42:33.280 --> 42:35.680\n that we'll have anything which is going to be\n\n42:35.680 --> 42:37.040\n dramatically different.\n\n42:37.040 --> 42:39.840\n It could happen, but I wouldn't bet on it right now.\n\n42:41.640 --> 42:45.200\n So let me ask a sort of big picture question.\n\n42:45.200 --> 42:49.160\n Do you think neural networks can be made\n\n42:49.160 --> 42:50.720\n to reason?\n\n42:50.720 --> 42:51.560\n Why not?\n\n42:52.440 --> 42:55.880\n Well, if you look, for example, at AlphaGo or AlphaZero,\n\n42:57.320 --> 43:00.720\n the neural network of AlphaZero plays Go,\n\n43:00.720 --> 43:04.080\n which we all agree is a game that requires reasoning,\n\n43:04.080 --> 43:07.600\n better than 99.9% of all humans.\n\n43:07.600 --> 43:09.440\n Just the neural network, without the search,\n\n43:09.440 --> 43:11.320\n just the neural network itself.\n\n43:11.320 --> 43:14.160\n Doesn't that give us an existence proof\n\n43:14.160 --> 43:15.720\n that neural networks can reason?\n\n43:15.720 --> 43:18.320\n To push back and disagree a little bit,\n\n43:18.320 --> 43:20.800\n we all agree that Go is reasoning.\n\n43:20.800 --> 43:24.800\n I think I agree, I don't think it's a trivial,\n\n43:24.800 --> 43:27.080\n so obviously reasoning like intelligence\n\n43:27.080 --> 43:31.080\n is a loose gray area term a little bit.\n\n43:31.080 --> 43:32.640\n Maybe you disagree with that.\n\n43:32.640 --> 43:36.560\n But yes, I think it has some of the same elements\n\n43:36.560 --> 43:37.960\n of reasoning.\n\n43:37.960 --> 43:41.640\n Reasoning is almost like akin to search, right?\n\n43:41.640 --> 43:45.680\n There's a sequential element of reasoning\n\n43:45.680 --> 43:50.680\n of stepwise consideration of possibilities\n\n43:51.520 --> 43:54.320\n and sort of building on top of those possibilities\n\n43:54.320 --> 43:57.680\n in a sequential manner until you arrive at some insight.\n\n43:57.680 --> 44:00.560\n So yeah, I guess playing Go is kind of like that.\n\n44:00.560 --> 44:02.320\n And when you have a single neural network\n\n44:02.320 --> 44:04.960\n doing that without search, it's kind of like that.\n\n44:04.960 --> 44:06.160\n So there's an existence proof\n\n44:06.160 --> 44:08.160\n in a particular constrained environment\n\n44:08.160 --> 44:13.200\n that a process akin to what many people call reasoning\n\n44:13.200 --> 44:17.160\n exists, but more general kind of reasoning.\n\n44:17.160 --> 44:18.880\n So off the board.\n\n44:18.880 --> 44:20.520\n There is one other existence proof.\n\n44:20.520 --> 44:22.160\n Oh boy, which one?\n\n44:22.160 --> 44:23.000\n Us humans?\n\n44:23.000 --> 44:23.840\n Yes.\n\n44:23.840 --> 44:28.840\n Okay, all right, so do you think the architecture\n\n44:29.840 --> 44:33.400\n that will allow neural networks to reason\n\n44:33.400 --> 44:37.360\n will look similar to the neural network architectures\n\n44:37.360 --> 44:38.840\n we have today?\n\n44:38.840 --> 44:39.680\n I think it will.\n\n44:39.680 --> 44:41.720\n I think, well, I don't wanna make\n\n44:41.720 --> 44:44.040\n two overly definitive statements.\n\n44:44.040 --> 44:45.800\n I think it's definitely possible\n\n44:45.800 --> 44:48.520\n that the neural networks that will produce\n\n44:48.520 --> 44:50.240\n the reasoning breakthroughs of the future\n\n44:50.240 --> 44:53.640\n will be very similar to the architectures that exist today.\n\n44:53.640 --> 44:55.360\n Maybe a little bit more recurrent,\n\n44:55.360 --> 44:57.120\n maybe a little bit deeper.\n\n44:57.120 --> 45:02.120\n But these neural nets are so insanely powerful.\n\n45:02.920 --> 45:05.560\n Why wouldn't they be able to learn to reason?\n\n45:05.560 --> 45:07.240\n Humans can reason.\n\n45:07.240 --> 45:09.320\n So why can't neural networks?\n\n45:09.320 --> 45:11.640\n So do you think the kind of stuff we've seen\n\n45:11.640 --> 45:14.640\n neural networks do is a kind of just weak reasoning?\n\n45:14.640 --> 45:16.600\n So it's not a fundamentally different process.\n\n45:16.600 --> 45:19.680\n Again, this is stuff nobody knows the answer to.\n\n45:19.680 --> 45:23.000\n So when it comes to our neural networks,\n\n45:23.000 --> 45:25.560\n the thing which I would say is that neural networks\n\n45:25.560 --> 45:27.240\n are capable of reasoning.\n\n45:28.200 --> 45:30.560\n But if you train a neural network on a task\n\n45:30.560 --> 45:34.000\n which doesn't require reasoning, it's not going to reason.\n\n45:34.000 --> 45:36.360\n This is a well known effect where the neural network\n\n45:36.360 --> 45:41.360\n will solve the problem that you pose in front of it\n\n45:41.360 --> 45:43.380\n in the easiest way possible.\n\n45:44.440 --> 45:49.440\n Right, that takes us to one of the brilliant sort of ways\n\n45:51.560 --> 45:52.840\n you've described neural networks,\n\n45:52.840 --> 45:55.480\n which is you've referred to neural networks\n\n45:55.480 --> 45:57.920\n as the search for small circuits\n\n45:57.920 --> 46:01.160\n and maybe general intelligence\n\n46:01.160 --> 46:03.360\n as the search for small programs,\n\n46:04.520 --> 46:06.960\n which I found as a metaphor very compelling.\n\n46:06.960 --> 46:09.200\n Can you elaborate on that difference?\n\n46:09.200 --> 46:13.720\n Yeah, so the thing which I said precisely was that\n\n46:13.720 --> 46:17.280\n if you can find the shortest program\n\n46:17.280 --> 46:20.940\n that outputs the data at your disposal,\n\n46:20.940 --> 46:22.280\n then you will be able to use it\n\n46:22.280 --> 46:24.240\n to make the best prediction possible.\n\n46:25.680 --> 46:27.000\n And that's a theoretical statement\n\n46:27.000 --> 46:29.240\n which can be proved mathematically.\n\n46:29.240 --> 46:31.160\n Now, you can also prove mathematically\n\n46:31.160 --> 46:33.920\n that finding the shortest program\n\n46:33.920 --> 46:38.920\n which generates some data is not a computable operation.\n\n46:38.920 --> 46:41.880\n No finite amount of compute can do this.\n\n46:42.740 --> 46:46.060\n So then with neural networks,\n\n46:46.060 --> 46:47.900\n neural networks are the next best thing\n\n46:47.900 --> 46:50.140\n that actually works in practice.\n\n46:50.140 --> 46:52.860\n We are not able to find the best,\n\n46:52.860 --> 46:55.740\n the shortest program which generates our data,\n\n46:55.740 --> 46:58.840\n but we are able to find a small,\n\n46:58.840 --> 47:01.580\n but now that statement should be amended,\n\n47:01.580 --> 47:05.280\n even a large circuit which fits our data in some way.\n\n47:05.280 --> 47:07.180\n Well, I think what you meant by the small circuit\n\n47:07.180 --> 47:10.020\n is the smallest needed circuit.\n\n47:10.020 --> 47:12.620\n Well, the thing which I would change now,\n\n47:12.620 --> 47:14.780\n back then I really haven't fully internalized\n\n47:14.780 --> 47:17.100\n the overparameterized results.\n\n47:17.100 --> 47:20.460\n The things we know about overparameterized neural nets,\n\n47:20.460 --> 47:23.140\n now I would phrase it as a large circuit\n\n47:24.540 --> 47:27.780\n whose weights contain a small amount of information,\n\n47:27.780 --> 47:29.160\n which I think is what's going on.\n\n47:29.160 --> 47:31.500\n If you imagine the training process of a neural network\n\n47:31.500 --> 47:33.780\n as you slowly transmit entropy\n\n47:33.780 --> 47:37.040\n from the dataset to the parameters,\n\n47:37.040 --> 47:41.060\n then somehow the amount of information in the weights\n\n47:41.060 --> 47:42.920\n ends up being not very large,\n\n47:42.920 --> 47:45.220\n which would explain why they generalize so well.\n\n47:45.220 --> 47:49.380\n So the large circuit might be one that's helpful\n\n47:49.380 --> 47:51.900\n for the generalization.\n\n47:51.900 --> 47:53.260\n Yeah, something like this.\n\n47:54.660 --> 47:59.660\n But do you see it important to be able to try\n\n48:00.220 --> 48:02.420\n to learn something like programs?\n\n48:02.420 --> 48:04.860\n I mean, if we can, definitely.\n\n48:04.860 --> 48:08.140\n I think it's kind of, the answer is kind of yes,\n\n48:08.140 --> 48:11.140\n if we can do it, we should do things that we can do it.\n\n48:11.140 --> 48:14.100\n It's the reason we are pushing on deep learning,\n\n48:15.300 --> 48:18.780\n the fundamental reason, the root cause\n\n48:18.780 --> 48:20.480\n is that we are able to train them.\n\n48:21.520 --> 48:23.880\n So in other words, training comes first.\n\n48:23.880 --> 48:27.500\n We've got our pillar, which is the training pillar.\n\n48:27.500 --> 48:30.020\n And now we're trying to contort our neural networks\n\n48:30.020 --> 48:30.900\n around the training pillar.\n\n48:30.900 --> 48:31.940\n We gotta stay trainable.\n\n48:31.940 --> 48:36.380\n This is an invariant we cannot violate.\n\n48:36.380 --> 48:40.540\n And so being trainable means starting from scratch,\n\n48:40.540 --> 48:42.820\n knowing nothing, you can actually pretty quickly\n\n48:42.820 --> 48:44.580\n converge towards knowing a lot.\n\n48:44.580 --> 48:45.900\n Or even slowly.\n\n48:45.900 --> 48:49.500\n But it means that given the resources at your disposal,\n\n48:50.700 --> 48:52.380\n you can train the neural net\n\n48:52.380 --> 48:55.380\n and get it to achieve useful performance.\n\n48:55.380 --> 48:57.500\n Yeah, that's a pillar we can't move away from.\n\n48:57.500 --> 48:58.340\n That's right.\n\n48:58.340 --> 49:01.460\n Because if you say, hey, let's find the shortest program,\n\n49:01.460 --> 49:02.800\n well, we can't do that.\n\n49:02.800 --> 49:06.060\n So it doesn't matter how useful that would be.\n\n49:06.060 --> 49:07.260\n We can't do it.\n\n49:07.260 --> 49:08.460\n So we won't.\n\n49:08.460 --> 49:09.820\n So do you think, you kind of mentioned\n\n49:09.820 --> 49:12.220\n that the neural networks are good at finding small circuits\n\n49:12.220 --> 49:13.380\n or large circuits.\n\n49:14.440 --> 49:17.540\n Do you think then the matter of finding small programs\n\n49:17.540 --> 49:19.280\n is just the data?\n\n49:19.280 --> 49:20.120\n No.\n\n49:20.120 --> 49:25.120\n So the, sorry, not the size or the type of data.\n\n49:25.880 --> 49:28.940\n Sort of ask, giving it programs.\n\n49:28.940 --> 49:31.960\n Well, I think the thing is that right now,\n\n49:31.960 --> 49:34.540\n finding, there are no good precedents\n\n49:34.540 --> 49:38.900\n of people successfully finding programs really well.\n\n49:38.900 --> 49:40.660\n And so the way you'd find programs\n\n49:40.660 --> 49:44.320\n is you'd train a deep neural network to do it basically.\n\n49:44.320 --> 49:45.900\n Right.\n\n49:45.900 --> 49:48.140\n Which is the right way to go about it.\n\n49:48.140 --> 49:50.700\n But there's not good illustrations of that.\n\n49:50.700 --> 49:51.860\n It hasn't been done yet.\n\n49:51.860 --> 49:54.300\n But in principle, it should be possible.\n\n49:56.500 --> 49:58.200\n Can you elaborate a little bit,\n\n49:58.200 --> 50:00.260\n what's your answer in principle?\n\n50:00.260 --> 50:04.180\n Put another way, you don't see why it's not possible.\n\n50:04.180 --> 50:07.900\n Well, it's kind of like more, it's more a statement of,\n\n50:09.500 --> 50:12.020\n I think that it's, I think that it's unwise\n\n50:12.020 --> 50:13.420\n to bet against deep learning.\n\n50:13.420 --> 50:16.920\n And if it's a cognitive function\n\n50:16.920 --> 50:18.680\n that humans seem to be able to do,\n\n50:18.680 --> 50:21.700\n then it doesn't take too long\n\n50:21.700 --> 50:24.540\n for some deep neural net to pop up that can do it too.\n\n50:25.740 --> 50:27.820\n Yeah, I'm there with you.\n\n50:27.820 --> 50:32.820\n I've stopped betting against neural networks at this point\n\n50:33.140 --> 50:35.740\n because they continue to surprise us.\n\n50:35.740 --> 50:37.280\n What about long term memory?\n\n50:37.280 --> 50:39.060\n Can neural networks have long term memory?\n\n50:39.060 --> 50:42.220\n Something like knowledge bases.\n\n50:42.220 --> 50:45.540\n So being able to aggregate important information\n\n50:45.540 --> 50:49.420\n over long periods of time that would then serve\n\n50:49.420 --> 50:54.420\n as useful sort of representations of state\n\n50:54.420 --> 50:57.720\n that you can make decisions by,\n\n50:57.720 --> 50:59.480\n so have a long term context\n\n50:59.480 --> 51:01.560\n based on which you're making the decision.\n\n51:01.560 --> 51:04.800\n So in some sense, the parameters already do that.\n\n51:06.000 --> 51:09.000\n The parameters are an aggregation of the neural,\n\n51:09.000 --> 51:10.880\n of the entirety of the neural nets experience,\n\n51:10.880 --> 51:14.220\n and so they count as long term knowledge.\n\n51:15.600 --> 51:17.740\n And people have trained various neural nets\n\n51:17.740 --> 51:20.140\n to act as knowledge bases and, you know,\n\n51:20.140 --> 51:22.360\n investigated with, people have investigated\n\n51:22.360 --> 51:23.640\n language models as knowledge bases.\n\n51:23.640 --> 51:27.260\n So there is work there.\n\n51:27.260 --> 51:29.840\n Yeah, but in some sense, do you think in every sense,\n\n51:29.840 --> 51:34.840\n do you think there's a, it's all just a matter of coming up\n\n51:35.700 --> 51:38.440\n with a better mechanism of forgetting the useless stuff\n\n51:38.440 --> 51:40.240\n and remembering the useful stuff?\n\n51:40.240 --> 51:43.080\n Because right now, I mean, there's not been mechanisms\n\n51:43.080 --> 51:46.880\n that do remember really long term information.\n\n51:46.880 --> 51:48.880\n What do you mean by that precisely?\n\n51:48.880 --> 51:51.780\n Precisely, I like the word precisely.\n\n51:51.780 --> 51:56.780\n So I'm thinking of the kind of compression of information\n\n51:58.160 --> 52:00.480\n the knowledge bases represent.\n\n52:00.480 --> 52:05.480\n Sort of creating a, now I apologize for my sort of\n\n52:05.680 --> 52:08.720\n human centric thinking about what knowledge is,\n\n52:08.720 --> 52:12.880\n because neural networks aren't interpretable necessarily\n\n52:12.880 --> 52:15.780\n with the kind of knowledge they have discovered.\n\n52:15.780 --> 52:18.720\n But a good example for me is knowledge bases,\n\n52:18.720 --> 52:21.280\n being able to build up over time something like\n\n52:21.280 --> 52:24.080\n the knowledge that Wikipedia represents.\n\n52:24.080 --> 52:28.080\n It's a really compressed, structured knowledge base.\n\n52:30.840 --> 52:34.360\n Obviously not the actual Wikipedia or the language,\n\n52:34.360 --> 52:37.040\n but like a semantic web, the dream that semantic web\n\n52:37.040 --> 52:40.360\n represented, so it's a really nice compressed knowledge base\n\n52:40.360 --> 52:44.560\n or something akin to that in the noninterpretable sense\n\n52:44.560 --> 52:46.980\n as neural networks would have.\n\n52:46.980 --> 52:48.560\n Well, the neural networks would be noninterpretable\n\n52:48.560 --> 52:50.820\n if you look at their weights, but their outputs\n\n52:50.820 --> 52:52.200\n should be very interpretable.\n\n52:52.200 --> 52:55.840\n Okay, so yeah, how do you make very smart neural networks\n\n52:55.840 --> 52:58.040\n like language models interpretable?\n\n52:58.040 --> 53:00.280\n Well, you ask them to generate some text\n\n53:00.280 --> 53:02.120\n and the text will generally be interpretable.\n\n53:02.120 --> 53:04.720\n Do you find that the epitome of interpretability,\n\n53:04.720 --> 53:06.000\n like can you do better?\n\n53:06.000 --> 53:08.600\n Like can you add, because you can't, okay,\n\n53:08.600 --> 53:12.240\n I'd like to know what does it know and what doesn't it know?\n\n53:12.240 --> 53:15.720\n I would like the neural network to come up with examples\n\n53:15.720 --> 53:18.640\n where it's completely dumb and examples\n\n53:18.640 --> 53:20.320\n where it's completely brilliant.\n\n53:20.320 --> 53:22.280\n And the only way I know how to do that now\n\n53:22.280 --> 53:26.440\n is to generate a lot of examples and use my human judgment.\n\n53:26.440 --> 53:28.160\n But it would be nice if a neural network\n\n53:28.160 --> 53:31.720\n had some self awareness about it.\n\n53:31.720 --> 53:34.800\n Yeah, 100%, I'm a big believer in self awareness\n\n53:34.800 --> 53:39.800\n and I think that, I think neural net self awareness\n\n53:39.940 --> 53:42.540\n will allow for things like the capabilities,\n\n53:42.540 --> 53:44.840\n like the ones you described, like for them to know\n\n53:44.840 --> 53:47.000\n what they know and what they don't know\n\n53:47.000 --> 53:48.740\n and for them to know where to invest\n\n53:48.740 --> 53:50.800\n to increase their skills most optimally.\n\n53:50.800 --> 53:52.280\n And to your question of interpretability,\n\n53:52.280 --> 53:54.360\n there are actually two answers to that question.\n\n53:54.360 --> 53:56.480\n One answer is, you know, we have the neural net\n\n53:56.480 --> 53:59.600\n so we can analyze the neurons and we can try to understand\n\n53:59.600 --> 54:02.000\n what the different neurons and different layers mean.\n\n54:02.000 --> 54:03.440\n And you can actually do that\n\n54:03.440 --> 54:05.920\n and OpenAI has done some work on that.\n\n54:05.920 --> 54:09.160\n But there is a different answer, which is that,\n\n54:10.000 --> 54:13.160\n I would say that's the human centric answer where you say,\n\n54:13.160 --> 54:16.520\n you know, you look at a human being, you can't read,\n\n54:16.520 --> 54:18.800\n how do you know what a human being is thinking?\n\n54:18.800 --> 54:20.600\n You ask them, you say, hey, what do you think about this?\n\n54:20.600 --> 54:22.340\n What do you think about that?\n\n54:22.340 --> 54:24.120\n And you get some answers.\n\n54:24.120 --> 54:26.040\n The answers you get are sticky in the sense\n\n54:26.040 --> 54:28.000\n you already have a mental model.\n\n54:28.000 --> 54:32.680\n You already have a mental model of that human being.\n\n54:32.680 --> 54:37.200\n You already have an understanding of like a big conception\n\n54:37.200 --> 54:40.400\n of that human being, how they think, what they know,\n\n54:40.400 --> 54:42.880\n how they see the world and then everything you ask,\n\n54:42.880 --> 54:45.520\n you're adding onto that.\n\n54:45.520 --> 54:48.680\n And that stickiness seems to be,\n\n54:49.760 --> 54:51.680\n that's one of the really interesting qualities\n\n54:51.680 --> 54:55.000\n of the human being is that information is sticky.\n\n54:55.000 --> 54:57.520\n You don't, you seem to remember the useful stuff,\n\n54:57.520 --> 55:00.400\n aggregate it well and forget most of the information\n\n55:00.400 --> 55:02.960\n that's not useful, that process.\n\n55:02.960 --> 55:05.520\n But that's also pretty similar to the process\n\n55:05.520 --> 55:06.760\n that neural networks do.\n\n55:06.760 --> 55:09.040\n It's just that neural networks are much crappier\n\n55:09.040 --> 55:10.640\n at this time.\n\n55:10.640 --> 55:13.260\n It doesn't seem to be fundamentally that different.\n\n55:13.260 --> 55:16.000\n But just to stick on reasoning for a little longer,\n\n55:17.000 --> 55:18.720\n you said, why not?\n\n55:18.720 --> 55:19.920\n Why can't I reason?\n\n55:19.920 --> 55:23.960\n What's a good impressive feat, benchmark to you\n\n55:23.960 --> 55:28.720\n of reasoning that you'll be impressed by\n\n55:28.720 --> 55:30.600\n if neural networks were able to do?\n\n55:30.600 --> 55:32.840\n Is that something you already have in mind?\n\n55:32.840 --> 55:35.280\n Well, I think writing really good code,\n\n55:36.520 --> 55:39.280\n I think proving really hard theorems,\n\n55:39.280 --> 55:43.100\n solving open ended problems with out of the box solutions.\n\n55:45.880 --> 55:49.480\n And sort of theorem type, mathematical problems.\n\n55:49.480 --> 55:52.080\n Yeah, I think those ones are a very natural example\n\n55:52.080 --> 55:52.920\n as well.\n\n55:52.920 --> 55:54.480\n If you can prove an unproven theorem,\n\n55:54.480 --> 55:56.480\n then it's hard to argue you don't reason.\n\n55:57.920 --> 55:59.400\n And so by the way, and this comes back to the point\n\n55:59.400 --> 56:04.400\n about the hard results, if you have machine learning,\n\n56:04.400 --> 56:06.080\n deep learning as a field is very fortunate\n\n56:06.080 --> 56:08.720\n because we have the ability to sometimes produce\n\n56:08.720 --> 56:10.840\n these unambiguous results.\n\n56:10.840 --> 56:13.120\n And when they happen, the debate changes,\n\n56:13.120 --> 56:14.160\n the conversation changes.\n\n56:14.160 --> 56:16.720\n It's a converse, we have the ability\n\n56:16.720 --> 56:19.480\n to produce conversation changing results.\n\n56:19.480 --> 56:21.600\n Conversation, and then of course, just like you said,\n\n56:21.600 --> 56:23.040\n people kind of take that for granted\n\n56:23.040 --> 56:25.080\n and say that wasn't actually a hard problem.\n\n56:25.080 --> 56:27.040\n Well, I mean, at some point we'll probably run out\n\n56:27.040 --> 56:28.080\n of hard problems.\n\n56:29.320 --> 56:33.640\n Yeah, that whole mortality thing is kind of a sticky problem\n\n56:33.640 --> 56:35.100\n that we haven't quite figured out.\n\n56:35.100 --> 56:37.200\n Maybe we'll solve that one.\n\n56:37.200 --> 56:39.120\n I think one of the fascinating things\n\n56:39.120 --> 56:40.880\n in your entire body of work,\n\n56:40.880 --> 56:43.040\n but also the work at OpenAI recently,\n\n56:43.040 --> 56:44.840\n one of the conversation changes has been\n\n56:44.840 --> 56:47.160\n in the world of language models.\n\n56:47.160 --> 56:50.280\n Can you briefly kind of try to describe\n\n56:50.280 --> 56:52.240\n the recent history of using neural networks\n\n56:52.240 --> 56:54.620\n in the domain of language and text?\n\n56:54.620 --> 56:56.620\n Well, there's been lots of history.\n\n56:56.620 --> 57:00.240\n I think the Elman network was a small,\n\n57:00.240 --> 57:02.080\n tiny recurrent neural network applied to language\n\n57:02.080 --> 57:03.840\n back in the 80s.\n\n57:03.840 --> 57:08.640\n So the history is really, you know, fairly long at least.\n\n57:08.640 --> 57:10.640\n And the thing that started,\n\n57:10.640 --> 57:13.440\n the thing that changed the trajectory\n\n57:13.440 --> 57:14.920\n of neural networks and language\n\n57:14.920 --> 57:17.200\n is the thing that changed the trajectory\n\n57:17.200 --> 57:19.660\n of all deep learning and that's data and compute.\n\n57:19.660 --> 57:22.720\n So suddenly you move from small language models,\n\n57:22.720 --> 57:24.400\n which learn a little bit,\n\n57:24.400 --> 57:26.600\n and with language models in particular,\n\n57:26.600 --> 57:28.440\n there's a very clear explanation\n\n57:28.440 --> 57:31.620\n for why they need to be large to be good,\n\n57:31.620 --> 57:34.600\n because they're trying to predict the next word.\n\n57:34.600 --> 57:36.840\n So when you don't know anything,\n\n57:36.840 --> 57:40.240\n you'll notice very, very broad strokes,\n\n57:40.240 --> 57:41.480\n surface level patterns,\n\n57:41.480 --> 57:44.840\n like sometimes there are characters\n\n57:44.840 --> 57:46.480\n and there is a space between those characters.\n\n57:46.480 --> 57:47.960\n You'll notice this pattern.\n\n57:47.960 --> 57:50.040\n And you'll notice that sometimes there is a comma\n\n57:50.040 --> 57:51.920\n and then the next character is a capital letter.\n\n57:51.920 --> 57:53.600\n You'll notice that pattern.\n\n57:53.600 --> 57:55.000\n Eventually you may start to notice\n\n57:55.000 --> 57:57.160\n that there are certain words occur often.\n\n57:57.160 --> 57:59.400\n You may notice that spellings are a thing.\n\n57:59.400 --> 58:00.920\n You may notice syntax.\n\n58:00.920 --> 58:03.680\n And when you get really good at all these,\n\n58:03.680 --> 58:05.880\n you start to notice the semantics.\n\n58:05.880 --> 58:07.820\n You start to notice the facts.\n\n58:07.820 --> 58:08.880\n But for that to happen,\n\n58:08.880 --> 58:11.440\n the language model needs to be larger.\n\n58:11.440 --> 58:14.040\n So that's, let's linger on that,\n\n58:14.040 --> 58:16.560\n because that's where you and Noam Chomsky disagree.\n\n58:18.680 --> 58:23.680\n So you think we're actually taking incremental steps,\n\n58:23.740 --> 58:25.720\n a sort of larger network, larger compute\n\n58:25.720 --> 58:29.480\n will be able to get to the semantics,\n\n58:29.480 --> 58:32.000\n to be able to understand language\n\n58:32.000 --> 58:35.520\n without what Noam likes to sort of think of\n\n58:35.520 --> 58:38.640\n as a fundamental understandings\n\n58:38.640 --> 58:40.440\n of the structure of language,\n\n58:40.440 --> 58:43.360\n like imposing your theory of language\n\n58:43.360 --> 58:45.860\n onto the learning mechanism.\n\n58:45.860 --> 58:48.000\n So you're saying the learning,\n\n58:48.000 --> 58:50.580\n you can learn from raw data,\n\n58:50.580 --> 58:53.400\n the mechanism that underlies language.\n\n58:53.400 --> 58:56.760\n Well, I think it's pretty likely,\n\n58:56.760 --> 59:01.240\n but I also want to say that I don't really know precisely\n\n59:01.240 --> 59:05.520\n what Chomsky means when he talks about him.\n\n59:05.520 --> 59:08.780\n You said something about imposing your structural language.\n\n59:08.780 --> 59:10.520\n I'm not 100% sure what he means,\n\n59:10.520 --> 59:12.680\n but empirically it seems that\n\n59:12.680 --> 59:14.640\n when you inspect those larger language models,\n\n59:14.640 --> 59:16.640\n they exhibit signs of understanding the semantics\n\n59:16.640 --> 59:18.520\n whereas the smaller language models do not.\n\n59:18.520 --> 59:19.800\n We've seen that a few years ago\n\n59:19.800 --> 59:21.920\n when we did work on the sentiment neuron.\n\n59:21.920 --> 59:24.040\n We trained a small, you know,\n\n59:24.040 --> 59:27.320\n smallish LSTM to predict the next character\n\n59:27.320 --> 59:28.600\n in Amazon reviews.\n\n59:28.600 --> 59:31.680\n And we noticed that when you increase the size of the LSTM\n\n59:31.680 --> 59:35.400\n from 500 LSTM cells to 4,000 LSTM cells,\n\n59:35.400 --> 59:38.600\n then one of the neurons starts to represent the sentiment\n\n59:38.600 --> 59:41.020\n of the article, sorry, of the review.\n\n59:42.040 --> 59:42.960\n Now, why is that?\n\n59:42.960 --> 59:45.280\n Sentiment is a pretty semantic attribute.\n\n59:45.280 --> 59:46.880\n It's not a syntactic attribute.\n\n59:46.880 --> 59:48.400\n And for people who might not know,\n\n59:48.400 --> 59:49.480\n I don't know if that's a standard term,\n\n59:49.480 --> 59:51.200\n but sentiment is whether it's a positive\n\n59:51.200 --> 59:52.040\n or a negative review.\n\n59:52.040 --> 59:52.880\n That's right.\n\n59:52.880 --> 59:54.320\n Is the person happy with something\n\n59:54.320 --> 59:55.960\n or is the person unhappy with something?\n\n59:55.960 --> 59:58.800\n And so here we had very clear evidence\n\n59:58.800 --> 1:00:01.960\n that a small neural net does not capture sentiment\n\n1:00:01.960 --> 1:00:03.640\n while a large neural net does.\n\n1:00:03.640 --> 1:00:04.760\n And why is that?\n\n1:00:04.760 --> 1:00:07.480\n Well, our theory is that at some point\n\n1:00:07.480 --> 1:00:08.840\n you run out of syntax to models,\n\n1:00:08.840 --> 1:00:11.040\n you start to gotta focus on something else.\n\n1:00:11.040 --> 1:00:15.840\n And with size, you quickly run out of syntax to model\n\n1:00:15.840 --> 1:00:18.360\n and then you really start to focus on the semantics\n\n1:00:18.360 --> 1:00:19.420\n would be the idea.\n\n1:00:19.420 --> 1:00:20.260\n That's right.\n\n1:00:20.260 --> 1:00:22.160\n And so I don't wanna imply that our models\n\n1:00:22.160 --> 1:00:23.840\n have complete semantic understanding\n\n1:00:23.840 --> 1:00:25.360\n because that's not true,\n\n1:00:25.360 --> 1:00:28.260\n but they definitely are showing signs\n\n1:00:28.260 --> 1:00:29.400\n of semantic understanding,\n\n1:00:29.400 --> 1:00:30.800\n partial semantic understanding,\n\n1:00:30.800 --> 1:00:33.680\n but the smaller models do not show those signs.\n\n1:00:34.520 --> 1:00:36.600\n Can you take a step back and say,\n\n1:00:36.600 --> 1:00:40.540\n what is GPT2, which is one of the big language models\n\n1:00:40.540 --> 1:00:42.520\n that was the conversation changer\n\n1:00:42.520 --> 1:00:43.760\n in the past couple of years?\n\n1:00:43.760 --> 1:00:48.120\n Yeah, so GPT2 is a transformer\n\n1:00:48.120 --> 1:00:50.360\n with one and a half billion parameters\n\n1:00:50.360 --> 1:00:55.360\n that was trained on about 40 billion tokens of text\n\n1:00:56.320 --> 1:00:58.840\n which were obtained from web pages\n\n1:00:58.840 --> 1:01:01.080\n that were linked to from Reddit articles\n\n1:01:01.080 --> 1:01:02.320\n with more than three outputs.\n\n1:01:02.320 --> 1:01:03.920\n And what's a transformer?\n\n1:01:03.920 --> 1:01:06.680\n The transformer, it's the most important advance\n\n1:01:06.680 --> 1:01:09.800\n in neural network architectures in recent history.\n\n1:01:09.800 --> 1:01:11.480\n What is attention maybe too?\n\n1:01:11.480 --> 1:01:13.280\n Cause I think that's an interesting idea,\n\n1:01:13.280 --> 1:01:15.000\n not necessarily sort of technically speaking,\n\n1:01:15.000 --> 1:01:18.680\n but the idea of attention versus maybe\n\n1:01:18.680 --> 1:01:21.080\n what recurrent neural networks represent.\n\n1:01:21.080 --> 1:01:23.320\n Yeah, so the thing is the transformer\n\n1:01:23.320 --> 1:01:25.840\n is a combination of multiple ideas simultaneously\n\n1:01:25.840 --> 1:01:28.140\n of which attention is one.\n\n1:01:28.140 --> 1:01:29.380\n Do you think attention is the key?\n\n1:01:29.380 --> 1:01:32.460\n No, it's a key, but it's not the key.\n\n1:01:32.460 --> 1:01:34.520\n The transformer is successful\n\n1:01:34.520 --> 1:01:36.760\n because it is the simultaneous combination\n\n1:01:36.760 --> 1:01:37.700\n of multiple ideas.\n\n1:01:37.700 --> 1:01:39.040\n And if you were to remove either idea,\n\n1:01:39.040 --> 1:01:41.480\n it would be much less successful.\n\n1:01:41.480 --> 1:01:43.880\n So the transformer uses a lot of attention,\n\n1:01:43.880 --> 1:01:45.860\n but attention existed for a few years.\n\n1:01:45.860 --> 1:01:48.440\n So that can't be the main innovation.\n\n1:01:48.440 --> 1:01:53.180\n The transformer is designed in such a way\n\n1:01:53.180 --> 1:01:55.160\n that it runs really fast on the GPU.\n\n1:01:56.120 --> 1:01:58.200\n And that makes a huge amount of difference.\n\n1:01:58.200 --> 1:01:59.360\n This is one thing.\n\n1:01:59.360 --> 1:02:02.840\n The second thing is that transformer is not recurrent.\n\n1:02:02.840 --> 1:02:04.680\n And that is really important too,\n\n1:02:04.680 --> 1:02:06.380\n because it is more shallow\n\n1:02:06.380 --> 1:02:08.440\n and therefore much easier to optimize.\n\n1:02:08.440 --> 1:02:10.400\n So in other words, users attention,\n\n1:02:10.400 --> 1:02:14.260\n it is a really great fit to the GPU\n\n1:02:14.260 --> 1:02:15.320\n and it is not recurrent,\n\n1:02:15.320 --> 1:02:17.800\n so therefore less deep and easier to optimize.\n\n1:02:17.800 --> 1:02:20.720\n And the combination of those factors make it successful.\n\n1:02:20.720 --> 1:02:24.160\n So now it makes great use of your GPU.\n\n1:02:24.160 --> 1:02:26.360\n It allows you to achieve better results\n\n1:02:26.360 --> 1:02:28.680\n for the same amount of compute.\n\n1:02:28.680 --> 1:02:30.240\n And that's why it's successful.\n\n1:02:31.080 --> 1:02:34.200\n Were you surprised how well transformers worked\n\n1:02:34.200 --> 1:02:36.120\n and GPT2 worked?\n\n1:02:36.120 --> 1:02:37.840\n So you worked on language.\n\n1:02:37.840 --> 1:02:39.760\n You've had a lot of great ideas\n\n1:02:39.760 --> 1:02:42.880\n before transformers came about in language.\n\n1:02:42.880 --> 1:02:44.960\n So you got to see the whole set of revolutions\n\n1:02:44.960 --> 1:02:46.160\n before and after.\n\n1:02:46.160 --> 1:02:47.560\n Were you surprised?\n\n1:02:47.560 --> 1:02:48.680\n Yeah, a little.\n\n1:02:48.680 --> 1:02:50.040\n A little?\n\n1:02:50.040 --> 1:02:51.920\n I mean, it's hard to remember\n\n1:02:51.920 --> 1:02:54.520\n because you adapt really quickly,\n\n1:02:54.520 --> 1:02:55.920\n but it definitely was surprising.\n\n1:02:55.920 --> 1:02:56.880\n It definitely was.\n\n1:02:56.880 --> 1:02:59.060\n In fact, you know what?\n\n1:02:59.060 --> 1:03:00.480\n I'll retract my statement.\n\n1:03:00.480 --> 1:03:02.480\n It was pretty amazing.\n\n1:03:02.480 --> 1:03:06.000\n It was just amazing to see generate this text of this.\n\n1:03:06.000 --> 1:03:07.380\n And you know, you gotta keep in mind\n\n1:03:07.380 --> 1:03:10.480\n that at that time we've seen all this progress in GANs\n\n1:03:10.480 --> 1:03:13.280\n in improving the samples produced by GANs\n\n1:03:13.280 --> 1:03:14.720\n were just amazing.\n\n1:03:14.720 --> 1:03:15.960\n You have these realistic faces,\n\n1:03:15.960 --> 1:03:17.960\n but text hasn't really moved that much.\n\n1:03:17.960 --> 1:03:20.520\n And suddenly we moved from, you know,\n\n1:03:20.520 --> 1:03:23.120\n whatever GANs were in 2015\n\n1:03:23.120 --> 1:03:26.200\n to the best, most amazing GANs in one step.\n\n1:03:26.200 --> 1:03:27.520\n And that was really stunning.\n\n1:03:27.520 --> 1:03:29.000\n Even though theory predicted,\n\n1:03:29.000 --> 1:03:30.420\n yeah, you train a big language model,\n\n1:03:30.420 --> 1:03:31.840\n of course you should get this,\n\n1:03:31.840 --> 1:03:33.200\n but then to see it with your own eyes,\n\n1:03:33.200 --> 1:03:34.880\n it's something else.\n\n1:03:34.880 --> 1:03:37.240\n And yet we adapt really quickly.\n\n1:03:37.240 --> 1:03:42.240\n And now there's sort of some cognitive scientists\n\n1:03:42.240 --> 1:03:47.040\n write articles saying that GPT2 models\n\n1:03:47.040 --> 1:03:49.320\n don't truly understand language.\n\n1:03:49.320 --> 1:03:51.880\n So we adapt quickly to how amazing\n\n1:03:51.880 --> 1:03:55.680\n the fact that they're able to model the language so well is.\n\n1:03:55.680 --> 1:03:57.940\n So what do you think is the bar?\n\n1:03:58.840 --> 1:03:59.680\n For what?\n\n1:03:59.680 --> 1:04:02.440\n For impressing us that it...\n\n1:04:02.440 --> 1:04:03.720\n I don't know.\n\n1:04:03.720 --> 1:04:06.080\n Do you think that bar will continuously be moved?\n\n1:04:06.080 --> 1:04:06.920\n Definitely.\n\n1:04:06.920 --> 1:04:08.840\n I think when you start to see\n\n1:04:08.840 --> 1:04:11.240\n really dramatic economic impact,\n\n1:04:11.240 --> 1:04:13.800\n that's when I think that's in some sense the next barrier.\n\n1:04:13.800 --> 1:04:16.880\n Because right now, if you think about the work in AI,\n\n1:04:16.880 --> 1:04:18.880\n it's really confusing.\n\n1:04:18.880 --> 1:04:22.560\n It's really hard to know what to make of all these advances.\n\n1:04:22.560 --> 1:04:25.560\n It's kind of like, okay, you got an advance\n\n1:04:25.560 --> 1:04:26.840\n and now you can do more things\n\n1:04:26.840 --> 1:04:29.080\n and you've got another improvement\n\n1:04:29.080 --> 1:04:30.400\n and you've got another cool demo.\n\n1:04:30.400 --> 1:04:35.400\n At some point, I think people who are outside of AI,\n\n1:04:36.160 --> 1:04:38.700\n they can no longer distinguish this progress anymore.\n\n1:04:38.700 --> 1:04:40.040\n So we were talking offline\n\n1:04:40.040 --> 1:04:41.760\n about translating Russian to English\n\n1:04:41.760 --> 1:04:44.120\n and how there's a lot of brilliant work in Russian\n\n1:04:44.120 --> 1:04:46.440\n that the rest of the world doesn't know about.\n\n1:04:46.440 --> 1:04:47.580\n That's true for Chinese,\n\n1:04:47.580 --> 1:04:50.080\n it's true for a lot of scientists\n\n1:04:50.080 --> 1:04:52.220\n and just artistic work in general.\n\n1:04:52.220 --> 1:04:53.880\n Do you think translation is the place\n\n1:04:53.880 --> 1:04:57.080\n where we're going to see sort of economic big impact?\n\n1:04:57.080 --> 1:04:57.920\n I don't know.\n\n1:04:57.920 --> 1:05:00.040\n I think there is a huge number of...\n\n1:05:00.040 --> 1:05:01.080\n I mean, first of all,\n\n1:05:01.080 --> 1:05:05.520\n I wanna point out that translation already today is huge.\n\n1:05:05.520 --> 1:05:07.500\n I think billions of people interact\n\n1:05:07.500 --> 1:05:11.080\n with big chunks of the internet primarily through translation.\n\n1:05:11.080 --> 1:05:13.060\n So translation is already huge\n\n1:05:13.060 --> 1:05:16.400\n and it's hugely positive too.\n\n1:05:16.400 --> 1:05:20.320\n I think self driving is going to be hugely impactful\n\n1:05:20.320 --> 1:05:24.440\n and that's, it's unknown exactly when it happens,\n\n1:05:24.440 --> 1:05:27.960\n but again, I would not bet against deep learning, so I...\n\n1:05:27.960 --> 1:05:29.320\n So there's deep learning in general,\n\n1:05:29.320 --> 1:05:30.160\n but you think this...\n\n1:05:30.160 --> 1:05:31.920\n Deep learning for self driving.\n\n1:05:31.920 --> 1:05:33.120\n Yes, deep learning for self driving.\n\n1:05:33.120 --> 1:05:35.320\n But I was talking about sort of language models.\n\n1:05:35.320 --> 1:05:36.160\n I see.\n\n1:05:36.160 --> 1:05:36.980\n Just to check.\n\n1:05:36.980 --> 1:05:38.080\n Beard off a little bit.\n\n1:05:38.080 --> 1:05:38.920\n Just to check,\n\n1:05:38.920 --> 1:05:41.120\n you're not seeing a connection between driving and language.\n\n1:05:41.120 --> 1:05:41.960\n No, no.\n\n1:05:41.960 --> 1:05:42.800\n Okay.\n\n1:05:42.800 --> 1:05:44.040\n Or rather both use neural nets.\n\n1:05:44.040 --> 1:05:45.560\n That'd be a poetic connection.\n\n1:05:45.560 --> 1:05:47.160\n I think there might be some,\n\n1:05:47.160 --> 1:05:49.160\n like you said, there might be some kind of unification\n\n1:05:49.160 --> 1:05:54.160\n towards a kind of multitask transformers\n\n1:05:54.480 --> 1:05:58.200\n that can take on both language and vision tasks.\n\n1:05:58.200 --> 1:06:01.400\n That'd be an interesting unification.\n\n1:06:01.400 --> 1:06:03.920\n Now let's see, what can I ask about GPT two more?\n\n1:06:04.940 --> 1:06:05.780\n It's simple.\n\n1:06:05.780 --> 1:06:06.980\n There's not much to ask.\n\n1:06:06.980 --> 1:06:09.960\n It's, you take a transform, you make it bigger,\n\n1:06:09.960 --> 1:06:10.800\n you give it more data,\n\n1:06:10.800 --> 1:06:12.700\n and suddenly it does all those amazing things.\n\n1:06:12.700 --> 1:06:14.920\n Yeah, one of the beautiful things is that GPT,\n\n1:06:14.920 --> 1:06:17.920\n the transformers are fundamentally simple to explain,\n\n1:06:17.920 --> 1:06:18.760\n to train.\n\n1:06:20.320 --> 1:06:23.960\n Do you think bigger will continue\n\n1:06:23.960 --> 1:06:27.060\n to show better results in language?\n\n1:06:27.060 --> 1:06:28.240\n Probably.\n\n1:06:28.240 --> 1:06:29.760\n Sort of like what are the next steps\n\n1:06:29.760 --> 1:06:31.440\n with GPT two, do you think?\n\n1:06:31.440 --> 1:06:34.000\n I mean, I think for sure seeing\n\n1:06:34.000 --> 1:06:37.600\n what larger versions can do is one direction.\n\n1:06:37.600 --> 1:06:41.200\n Also, I mean, there are many questions.\n\n1:06:41.200 --> 1:06:42.720\n There's one question which I'm curious about\n\n1:06:42.720 --> 1:06:43.960\n and that's the following.\n\n1:06:43.960 --> 1:06:45.360\n So right now GPT two,\n\n1:06:45.360 --> 1:06:46.960\n so we feed it all this data from the internet,\n\n1:06:46.960 --> 1:06:48.120\n which means that it needs to memorize\n\n1:06:48.120 --> 1:06:51.840\n all those random facts about everything in the internet.\n\n1:06:51.840 --> 1:06:56.840\n And it would be nice if the model could somehow\n\n1:06:56.840 --> 1:06:59.800\n use its own intelligence to decide\n\n1:06:59.800 --> 1:07:01.800\n what data it wants to accept\n\n1:07:01.800 --> 1:07:03.560\n and what data it wants to reject.\n\n1:07:03.560 --> 1:07:04.400\n Just like people.\n\n1:07:04.400 --> 1:07:07.160\n People don't learn all data indiscriminately.\n\n1:07:07.160 --> 1:07:09.760\n We are super selective about what we learn.\n\n1:07:09.760 --> 1:07:11.560\n And I think this kind of active learning,\n\n1:07:11.560 --> 1:07:13.320\n I think would be very nice to have.\n\n1:07:14.240 --> 1:07:16.720\n Yeah, listen, I love active learning.\n\n1:07:16.720 --> 1:07:21.120\n So let me ask, does the selection of data,\n\n1:07:21.120 --> 1:07:23.040\n can you just elaborate that a little bit more?\n\n1:07:23.040 --> 1:07:26.380\n Do you think the selection of data is,\n\n1:07:28.160 --> 1:07:29.880\n like I have this kind of sense\n\n1:07:29.880 --> 1:07:33.760\n that the optimization of how you select data,\n\n1:07:33.760 --> 1:07:38.520\n so the active learning process is going to be a place\n\n1:07:38.520 --> 1:07:42.120\n for a lot of breakthroughs, even in the near future?\n\n1:07:42.120 --> 1:07:44.040\n Because there hasn't been many breakthroughs there\n\n1:07:44.040 --> 1:07:45.080\n that are public.\n\n1:07:45.080 --> 1:07:47.560\n I feel like there might be private breakthroughs\n\n1:07:47.560 --> 1:07:49.320\n that companies keep to themselves\n\n1:07:49.320 --> 1:07:51.480\n because the fundamental problem has to be solved\n\n1:07:51.480 --> 1:07:52.920\n if you want to solve self driving,\n\n1:07:52.920 --> 1:07:55.280\n if you want to solve a particular task.\n\n1:07:55.280 --> 1:07:57.800\n What do you think about the space in general?\n\n1:07:57.800 --> 1:08:00.160\n Yeah, so I think that for something like active learning,\n\n1:08:00.160 --> 1:08:03.760\n or in fact, for any kind of capability, like active learning,\n\n1:08:03.760 --> 1:08:05.800\n the thing that it really needs is a problem.\n\n1:08:05.800 --> 1:08:07.940\n It needs a problem that requires it.\n\n1:08:09.360 --> 1:08:12.080\n It's very hard to do research about the capability\n\n1:08:12.080 --> 1:08:12.980\n if you don't have a task,\n\n1:08:12.980 --> 1:08:14.200\n because then what's going to happen\n\n1:08:14.200 --> 1:08:16.720\n is that you will come up with an artificial task,\n\n1:08:16.720 --> 1:08:19.760\n get good results, but not really convince anyone.\n\n1:08:20.640 --> 1:08:22.960\n Right, like we're now past the stage\n\n1:08:22.960 --> 1:08:27.960\n where getting a result on MNIST, some clever formulation\n\n1:08:28.880 --> 1:08:30.800\n of MNIST will convince people.\n\n1:08:30.800 --> 1:08:33.280\n That's right, in fact, you could quite easily\n\n1:08:33.280 --> 1:08:35.320\n come up with a simple active learning scheme on MNIST\n\n1:08:35.320 --> 1:08:39.560\n and get a 10x speed up, but then, so what?\n\n1:08:39.560 --> 1:08:41.760\n And I think that with active learning,\n\n1:08:41.760 --> 1:08:45.480\n the need, active learning will naturally arise\n\n1:08:45.480 --> 1:08:49.240\n as problems that require it pop up.\n\n1:08:49.240 --> 1:08:51.840\n That's how I would, that's my take on it.\n\n1:08:51.840 --> 1:08:54.140\n There's another interesting thing\n\n1:08:54.140 --> 1:08:56.100\n that OpenAI has brought up with GPT2,\n\n1:08:56.100 --> 1:09:00.240\n which is when you create a powerful\n\n1:09:00.240 --> 1:09:01.460\n artificial intelligence system,\n\n1:09:01.460 --> 1:09:04.660\n and it was unclear what kind of detrimental,\n\n1:09:04.660 --> 1:09:07.460\n once you release GPT2,\n\n1:09:07.460 --> 1:09:09.580\n what kind of detrimental effect it will have.\n\n1:09:09.580 --> 1:09:11.540\n Because if you have a model\n\n1:09:11.540 --> 1:09:14.080\n that can generate a pretty realistic text,\n\n1:09:14.080 --> 1:09:18.340\n you can start to imagine that it would be used by bots\n\n1:09:18.340 --> 1:09:21.740\n in some way that we can't even imagine.\n\n1:09:21.740 --> 1:09:24.460\n So there's this nervousness about what is possible to do.\n\n1:09:24.460 --> 1:09:27.100\n So you did a really kind of brave\n\n1:09:27.100 --> 1:09:28.180\n and I think profound thing,\n\n1:09:28.180 --> 1:09:30.100\n which is start a conversation about this.\n\n1:09:30.100 --> 1:09:34.900\n How do we release powerful artificial intelligence models\n\n1:09:34.900 --> 1:09:36.100\n to the public?\n\n1:09:36.100 --> 1:09:39.780\n If we do it all, how do we privately discuss\n\n1:09:39.780 --> 1:09:42.200\n with other, even competitors,\n\n1:09:42.200 --> 1:09:46.060\n about how we manage the use of the systems and so on?\n\n1:09:46.060 --> 1:09:47.980\n So from this whole experience,\n\n1:09:47.980 --> 1:09:49.580\n you released a report on it,\n\n1:09:49.580 --> 1:09:51.820\n but in general, are there any insights\n\n1:09:51.820 --> 1:09:55.340\n that you've gathered from just thinking about this,\n\n1:09:55.340 --> 1:09:57.740\n about how you release models like this?\n\n1:09:57.740 --> 1:10:00.700\n I mean, I think that my take on this\n\n1:10:00.700 --> 1:10:05.060\n is that the field of AI has been in a state of childhood.\n\n1:10:05.060 --> 1:10:06.860\n And now it's exiting that state\n\n1:10:06.860 --> 1:10:09.660\n and it's entering a state of maturity.\n\n1:10:09.660 --> 1:10:12.340\n What that means is that AI is very successful\n\n1:10:12.340 --> 1:10:14.140\n and also very impactful.\n\n1:10:14.140 --> 1:10:16.980\n And its impact is not only large, but it's also growing.\n\n1:10:16.980 --> 1:10:21.980\n And so for that reason, it seems wise to start thinking\n\n1:10:21.980 --> 1:10:24.940\n about the impact of our systems before releasing them,\n\n1:10:24.940 --> 1:10:28.700\n maybe a little bit too soon, rather than a little bit too late.\n\n1:10:28.700 --> 1:10:31.900\n And with the case of GPT2, like I mentioned earlier,\n\n1:10:31.900 --> 1:10:34.060\n the results really were stunning.\n\n1:10:34.060 --> 1:10:37.700\n And it seemed plausible, it didn't seem certain,\n\n1:10:37.700 --> 1:10:40.540\n it seemed plausible that something like GPT2\n\n1:10:40.540 --> 1:10:44.540\n could easily use to reduce the cost of this information.\n\n1:10:44.540 --> 1:10:47.060\n And so there was a question of what's the best way\n\n1:10:47.060 --> 1:10:49.380\n to release it, and a staged release seemed logical.\n\n1:10:49.380 --> 1:10:51.220\n A small model was released,\n\n1:10:51.220 --> 1:10:53.940\n and there was time to see the,\n\n1:10:54.980 --> 1:10:57.300\n many people use these models in lots of cool ways.\n\n1:10:57.300 --> 1:10:59.700\n There've been lots of really cool applications.\n\n1:10:59.700 --> 1:11:03.820\n There haven't been any negative application to be known of.\n\n1:11:03.820 --> 1:11:05.180\n And so eventually it was released,\n\n1:11:05.180 --> 1:11:07.620\n but also other people replicated similar models.\n\n1:11:07.620 --> 1:11:10.260\n That's an interesting question though that we know of.\n\n1:11:10.260 --> 1:11:12.860\n So in your view, staged release,\n\n1:11:12.860 --> 1:11:17.860\n is at least part of the answer to the question of how do we,\n\n1:11:20.620 --> 1:11:22.980\n what do we do once we create a system like this?\n\n1:11:22.980 --> 1:11:24.980\n It's part of the answer, yes.\n\n1:11:24.980 --> 1:11:26.900\n Is there any other insights?\n\n1:11:26.900 --> 1:11:29.340\n Like say you don't wanna release the model at all,\n\n1:11:29.340 --> 1:11:32.820\n because it's useful to you for whatever the business is.\n\n1:11:32.820 --> 1:11:36.020\n Well, plenty of people don't release models already.\n\n1:11:36.020 --> 1:11:39.660\n Right, of course, but is there some moral,\n\n1:11:39.660 --> 1:11:43.340\n ethical responsibility when you have a very powerful model\n\n1:11:43.340 --> 1:11:44.860\n to sort of communicate?\n\n1:11:44.860 --> 1:11:48.580\n Like, just as you said, when you had GPT2,\n\n1:11:48.580 --> 1:11:51.340\n it was unclear how much it could be used for misinformation.\n\n1:11:51.340 --> 1:11:54.780\n It's an open question, and getting an answer to that\n\n1:11:54.780 --> 1:11:57.700\n might require that you talk to other really smart people\n\n1:11:57.700 --> 1:12:00.940\n that are outside of your particular group.\n\n1:12:00.940 --> 1:12:05.500\n Have you, please tell me there's some optimistic pathway\n\n1:12:05.500 --> 1:12:08.900\n for people to be able to use this model\n\n1:12:08.900 --> 1:12:11.380\n for people across the world to collaborate\n\n1:12:11.380 --> 1:12:12.700\n on these kinds of cases?\n\n1:12:14.740 --> 1:12:17.940\n Or is it still really difficult from one company\n\n1:12:17.940 --> 1:12:19.660\n to talk to another company?\n\n1:12:19.660 --> 1:12:21.380\n So it's definitely possible.\n\n1:12:21.380 --> 1:12:26.220\n It's definitely possible to discuss these kind of models\n\n1:12:26.220 --> 1:12:28.380\n with colleagues elsewhere,\n\n1:12:28.380 --> 1:12:32.300\n and to get their take on what to do.\n\n1:12:32.300 --> 1:12:33.740\n How hard is it though?\n\n1:12:33.740 --> 1:12:34.580\n I mean.\n\n1:12:36.540 --> 1:12:38.140\n Do you see that happening?\n\n1:12:38.140 --> 1:12:40.620\n I think that's a place where it's important\n\n1:12:40.620 --> 1:12:43.380\n to gradually build trust between companies.\n\n1:12:43.380 --> 1:12:47.180\n Because ultimately, all the AI developers\n\n1:12:47.180 --> 1:12:48.860\n are building technology which is going to be\n\n1:12:48.860 --> 1:12:50.860\n increasingly more powerful.\n\n1:12:50.860 --> 1:12:52.020\n And so it's,\n\n1:12:54.780 --> 1:12:56.340\n the way to think about it is that ultimately\n\n1:12:56.340 --> 1:12:57.580\n we're all in it together.\n\n1:12:58.660 --> 1:13:03.660\n Yeah, I tend to believe in the better angels of our nature,\n\n1:13:03.660 --> 1:13:08.660\n but I do hope that when you build a really powerful\n\n1:13:09.820 --> 1:13:11.860\n AI system in a particular domain,\n\n1:13:11.860 --> 1:13:14.700\n that you also think about the potential\n\n1:13:14.700 --> 1:13:17.380\n negative consequences of, yeah.\n\n1:13:21.420 --> 1:13:23.020\n It's an interesting and scary possibility\n\n1:13:23.020 --> 1:13:26.340\n that there will be a race for AI development\n\n1:13:26.340 --> 1:13:29.340\n that would push people to close that development,\n\n1:13:29.340 --> 1:13:31.180\n and not share ideas with others.\n\n1:13:31.180 --> 1:13:32.460\n I don't love this.\n\n1:13:32.460 --> 1:13:34.340\n I've been a pure academic for 10 years.\n\n1:13:34.340 --> 1:13:37.340\n I really like sharing ideas and it's fun, it's exciting.\n\n1:13:39.220 --> 1:13:40.420\n What do you think it takes to,\n\n1:13:40.420 --> 1:13:42.180\n let's talk about AGI a little bit.\n\n1:13:42.180 --> 1:13:44.100\n What do you think it takes to build a system\n\n1:13:44.100 --> 1:13:45.660\n of human level intelligence?\n\n1:13:45.660 --> 1:13:47.300\n We talked about reasoning,\n\n1:13:47.300 --> 1:13:50.060\n we talked about long term memory, but in general,\n\n1:13:50.060 --> 1:13:51.380\n what does it take, do you think?\n\n1:13:51.380 --> 1:13:53.900\n Well, I can't be sure.\n\n1:13:55.140 --> 1:13:57.100\n But I think the deep learning,\n\n1:13:57.100 --> 1:13:58.940\n plus maybe another,\n\n1:13:58.940 --> 1:14:03.740\n plus maybe another small idea.\n\n1:14:03.740 --> 1:14:05.580\n Do you think self play will be involved?\n\n1:14:05.580 --> 1:14:09.020\n So you've spoken about the powerful mechanism of self play\n\n1:14:09.020 --> 1:14:14.020\n where systems learn by sort of exploring the world\n\n1:14:15.300 --> 1:14:18.340\n in a competitive setting against other entities\n\n1:14:18.340 --> 1:14:20.540\n that are similarly skilled as them,\n\n1:14:20.540 --> 1:14:23.020\n and so incrementally improve in this way.\n\n1:14:23.020 --> 1:14:24.540\n Do you think self play will be a component\n\n1:14:24.540 --> 1:14:26.660\n of building an AGI system?\n\n1:14:26.660 --> 1:14:29.420\n Yeah, so what I would say, to build AGI,\n\n1:14:29.420 --> 1:14:34.180\n I think it's going to be deep learning plus some ideas.\n\n1:14:34.180 --> 1:14:36.580\n And I think self play will be one of those ideas.\n\n1:14:37.780 --> 1:14:39.540\n I think that that is a very,\n\n1:14:41.380 --> 1:14:43.980\n self play has this amazing property\n\n1:14:43.980 --> 1:14:48.780\n that it can surprise us in truly novel ways.\n\n1:14:48.780 --> 1:14:53.020\n For example, like we, I mean,\n\n1:14:53.020 --> 1:14:55.740\n pretty much every self play system,\n\n1:14:55.740 --> 1:14:58.420\n both are Dota bot.\n\n1:14:58.420 --> 1:15:02.660\n I don't know if, OpenAI had a release about multi agent\n\n1:15:02.660 --> 1:15:04.340\n where you had two little agents\n\n1:15:04.340 --> 1:15:06.060\n who were playing hide and seek.\n\n1:15:06.060 --> 1:15:08.220\n And of course, also alpha zero.\n\n1:15:08.220 --> 1:15:11.020\n They were all produced surprising behaviors.\n\n1:15:11.020 --> 1:15:13.180\n They all produce behaviors that we didn't expect.\n\n1:15:13.180 --> 1:15:15.820\n They are creative solutions to problems.\n\n1:15:15.820 --> 1:15:18.700\n And that seems like an important part of AGI\n\n1:15:18.700 --> 1:15:21.340\n that our systems don't exhibit routinely right now.\n\n1:15:22.180 --> 1:15:24.900\n And so that's why I like this area.\n\n1:15:24.900 --> 1:15:27.540\n I like this direction because of its ability to surprise us.\n\n1:15:27.540 --> 1:15:28.380\n To surprise us.\n\n1:15:28.380 --> 1:15:31.180\n And an AGI system would surprise us fundamentally.\n\n1:15:31.180 --> 1:15:32.020\n Yes.\n\n1:15:32.020 --> 1:15:34.500\n And to be precise, not just a random surprise,\n\n1:15:34.500 --> 1:15:37.900\n but to find the surprising solution to a problem\n\n1:15:37.900 --> 1:15:39.140\n that's also useful.\n\n1:15:39.140 --> 1:15:39.980\n Right.\n\n1:15:39.980 --> 1:15:42.620\n Now, a lot of the self play mechanisms\n\n1:15:42.620 --> 1:15:45.620\n have been used in the game context\n\n1:15:45.620 --> 1:15:48.380\n or at least in the simulation context.\n\n1:15:48.380 --> 1:15:53.380\n How far along the path to AGI\n\n1:15:55.100 --> 1:15:56.700\n do you think will be done in simulation?\n\n1:15:56.700 --> 1:16:01.340\n How much faith, promise do you have in simulation\n\n1:16:01.340 --> 1:16:03.060\n versus having to have a system\n\n1:16:03.060 --> 1:16:05.620\n that operates in the real world?\n\n1:16:05.620 --> 1:16:09.860\n Whether it's the real world of digital real world data\n\n1:16:09.860 --> 1:16:13.220\n or real world like actual physical world of robotics.\n\n1:16:13.220 --> 1:16:15.060\n I don't think it's an easy or.\n\n1:16:15.060 --> 1:16:17.540\n I think simulation is a tool and it helps.\n\n1:16:17.540 --> 1:16:19.700\n It has certain strengths and certain weaknesses\n\n1:16:19.700 --> 1:16:21.500\n and we should use it.\n\n1:16:21.500 --> 1:16:24.540\n Yeah, but okay, I understand that.\n\n1:16:24.540 --> 1:16:29.540\n That's true, but one of the criticisms of self play,\n\n1:16:32.740 --> 1:16:34.820\n one of the criticisms of reinforcement learning\n\n1:16:34.820 --> 1:16:39.820\n is one of the, its current power, its current results,\n\n1:16:41.060 --> 1:16:42.940\n while amazing, have been demonstrated\n\n1:16:42.940 --> 1:16:44.820\n in a simulated environments\n\n1:16:44.820 --> 1:16:46.420\n or very constrained physical environments.\n\n1:16:46.420 --> 1:16:49.180\n Do you think it's possible to escape them,\n\n1:16:49.180 --> 1:16:50.780\n escape the simulator environments\n\n1:16:50.780 --> 1:16:53.420\n and be able to learn in non simulator environments?\n\n1:16:53.420 --> 1:16:57.020\n Or do you think it's possible to also just simulate\n\n1:16:57.020 --> 1:17:01.140\n in a photo realistic and physics realistic way,\n\n1:17:01.140 --> 1:17:03.780\n the real world in a way that we can solve real problems\n\n1:17:03.780 --> 1:17:06.740\n with self play in simulation?\n\n1:17:06.740 --> 1:17:10.380\n So I think that transfer from simulation to the real world\n\n1:17:10.380 --> 1:17:14.140\n is definitely possible and has been exhibited many times\n\n1:17:14.140 --> 1:17:16.060\n by many different groups.\n\n1:17:16.060 --> 1:17:18.660\n It's been especially successful in vision.\n\n1:17:18.660 --> 1:17:22.660\n Also open AI in the summer has demonstrated a robot hand\n\n1:17:22.660 --> 1:17:25.260\n which was trained entirely in simulation\n\n1:17:25.260 --> 1:17:27.820\n in a certain way that allowed for seem to real transfer\n\n1:17:27.820 --> 1:17:28.660\n to occur.\n\n1:17:29.860 --> 1:17:31.420\n Is this for the Rubik's cube?\n\n1:17:31.420 --> 1:17:32.660\n Yeah, that's right.\n\n1:17:32.660 --> 1:17:34.660\n I wasn't aware that was trained in simulation.\n\n1:17:34.660 --> 1:17:37.020\n It was trained in simulation entirely.\n\n1:17:37.020 --> 1:17:39.420\n Really, so it wasn't in the physical,\n\n1:17:39.420 --> 1:17:40.980\n the hand wasn't trained?\n\n1:17:40.980 --> 1:17:44.820\n No, 100% of the training was done in simulation\n\n1:17:44.820 --> 1:17:46.900\n and the policy that was learned in simulation\n\n1:17:46.900 --> 1:17:48.980\n was trained to be very adaptive.\n\n1:17:48.980 --> 1:17:50.940\n So adaptive that when you transfer it,\n\n1:17:50.940 --> 1:17:53.940\n it could very quickly adapt to the physical world.\n\n1:17:53.940 --> 1:17:57.380\n So the kind of perturbations with the giraffe\n\n1:17:57.380 --> 1:17:58.900\n or whatever the heck it was,\n\n1:17:58.900 --> 1:18:01.860\n those weren't, were those part of the simulation?\n\n1:18:01.860 --> 1:18:04.140\n Well, the simulation was generally,\n\n1:18:04.140 --> 1:18:07.060\n so the simulation was trained to be robust\n\n1:18:07.060 --> 1:18:08.140\n to many different things,\n\n1:18:08.140 --> 1:18:10.580\n but not the kind of perturbations we've had in the video.\n\n1:18:10.580 --> 1:18:12.660\n So it's never been trained with a glove.\n\n1:18:12.660 --> 1:18:17.060\n It's never been trained with a stuffed giraffe.\n\n1:18:17.060 --> 1:18:19.340\n So in theory, these are novel perturbations.\n\n1:18:19.340 --> 1:18:22.020\n Correct, it's not in theory, in practice.\n\n1:18:22.020 --> 1:18:23.780\n Those are novel perturbations?\n\n1:18:23.780 --> 1:18:25.100\n Well, that's okay.\n\n1:18:26.420 --> 1:18:28.460\n That's a clean, small scale,\n\n1:18:28.460 --> 1:18:29.940\n but clean example of a transfer\n\n1:18:29.940 --> 1:18:32.140\n from the simulated world to the physical world.\n\n1:18:32.140 --> 1:18:33.220\n Yeah, and I will also say\n\n1:18:33.220 --> 1:18:35.620\n that I expect the transfer capabilities\n\n1:18:35.620 --> 1:18:38.180\n of deep learning to increase in general.\n\n1:18:38.180 --> 1:18:40.540\n And the better the transfer capabilities are,\n\n1:18:40.540 --> 1:18:42.660\n the more useful simulation will become.\n\n1:18:43.660 --> 1:18:45.260\n Because then you could take,\n\n1:18:45.260 --> 1:18:48.540\n you could experience something in simulation\n\n1:18:48.540 --> 1:18:50.340\n and then learn a moral of the story,\n\n1:18:50.340 --> 1:18:53.540\n which you could then carry with you to the real world.\n\n1:18:53.540 --> 1:18:56.980\n As humans do all the time when they play computer games.\n\n1:18:56.980 --> 1:19:01.740\n So let me ask sort of a embodied question,\n\n1:19:01.740 --> 1:19:03.580\n staying on AGI for a sec.\n\n1:19:04.660 --> 1:19:07.740\n Do you think AGI system would need to have a body?\n\n1:19:07.740 --> 1:19:09.580\n We need to have some of those human elements\n\n1:19:09.580 --> 1:19:13.020\n of self awareness, consciousness,\n\n1:19:13.020 --> 1:19:15.100\n sort of fear of mortality,\n\n1:19:15.100 --> 1:19:18.140\n sort of self preservation in the physical space,\n\n1:19:18.140 --> 1:19:20.340\n which comes with having a body.\n\n1:19:20.340 --> 1:19:22.420\n I think having a body will be useful.\n\n1:19:22.420 --> 1:19:24.340\n I don't think it's necessary,\n\n1:19:24.340 --> 1:19:26.260\n but I think it's very useful to have a body for sure,\n\n1:19:26.260 --> 1:19:28.900\n because you can learn a whole new,\n\n1:19:28.900 --> 1:19:32.500\n you can learn things which cannot be learned without a body.\n\n1:19:32.500 --> 1:19:35.420\n But at the same time, I think that if you don't have a body,\n\n1:19:35.420 --> 1:19:38.580\n you could compensate for it and still succeed.\n\n1:19:38.580 --> 1:19:39.420\n You think so?\n\n1:19:39.420 --> 1:19:40.260\n Yes.\n\n1:19:40.260 --> 1:19:41.100\n Well, there is evidence for this.\n\n1:19:41.100 --> 1:19:43.340\n For example, there are many people who were born deaf\n\n1:19:43.340 --> 1:19:46.580\n and blind and they were able to compensate\n\n1:19:46.580 --> 1:19:48.260\n for the lack of modalities.\n\n1:19:48.260 --> 1:19:50.500\n I'm thinking about Helen Keller specifically.\n\n1:19:51.580 --> 1:19:53.860\n So even if you're not able to physically interact\n\n1:19:53.860 --> 1:19:56.940\n with the world, and if you're not able to,\n\n1:19:56.940 --> 1:19:58.740\n I mean, I actually was getting at,\n\n1:19:59.660 --> 1:20:02.700\n maybe let me ask on the more particular,\n\n1:20:02.700 --> 1:20:05.380\n I'm not sure if it's connected to having a body or not,\n\n1:20:05.380 --> 1:20:07.860\n but the idea of consciousness\n\n1:20:07.860 --> 1:20:11.260\n and a more constrained version of that is self awareness.\n\n1:20:11.260 --> 1:20:14.500\n Do you think an AGI system should have consciousness?\n\n1:20:16.300 --> 1:20:19.420\n We can't define, whatever the heck you think consciousness is.\n\n1:20:19.420 --> 1:20:21.580\n Yeah, hard question to answer,\n\n1:20:21.580 --> 1:20:23.220\n given how hard it is to define it.\n\n1:20:24.780 --> 1:20:26.460\n Do you think it's useful to think about?\n\n1:20:26.460 --> 1:20:28.380\n I mean, it's definitely interesting.\n\n1:20:28.380 --> 1:20:29.860\n It's fascinating.\n\n1:20:29.860 --> 1:20:31.820\n I think it's definitely possible\n\n1:20:31.820 --> 1:20:33.900\n that our systems will be conscious.\n\n1:20:33.900 --> 1:20:36.420\n Do you think that's an emergent thing that just comes from,\n\n1:20:36.420 --> 1:20:37.780\n do you think consciousness could emerge\n\n1:20:37.780 --> 1:20:40.860\n from the representation that's stored within neural networks?\n\n1:20:40.860 --> 1:20:42.980\n So like that it naturally just emerges\n\n1:20:42.980 --> 1:20:45.100\n when you become more and more,\n\n1:20:45.100 --> 1:20:47.020\n you're able to represent more and more of the world?\n\n1:20:47.020 --> 1:20:48.780\n Well, I'd say I'd make the following argument,\n\n1:20:48.780 --> 1:20:53.780\n which is humans are conscious.\n\n1:20:53.820 --> 1:20:56.060\n And if you believe that artificial neural nets\n\n1:20:56.060 --> 1:20:59.540\n are sufficiently similar to the brain,\n\n1:20:59.540 --> 1:21:02.700\n then there should at least exist artificial neural nets\n\n1:21:02.700 --> 1:21:04.260\n you should be conscious too.\n\n1:21:04.260 --> 1:21:06.620\n You're leaning on that existence proof pretty heavily.\n\n1:21:06.620 --> 1:21:11.620\n Okay, so that's the best answer I can give.\n\n1:21:12.100 --> 1:21:15.980\n No, I know, I know, I know.\n\n1:21:15.980 --> 1:21:17.100\n There's still an open question\n\n1:21:17.100 --> 1:21:20.780\n if there's not some magic in the brain that we're not,\n\n1:21:20.780 --> 1:21:23.620\n I mean, I don't mean a non materialistic magic,\n\n1:21:23.620 --> 1:21:27.780\n but that the brain might be a lot more complicated\n\n1:21:27.780 --> 1:21:29.900\n and interesting than we give it credit for.\n\n1:21:29.900 --> 1:21:32.500\n If that's the case, then it should show up.\n\n1:21:32.500 --> 1:21:35.140\n And at some point we will find out\n\n1:21:35.140 --> 1:21:36.580\n that we can't continue to make progress.\n\n1:21:36.580 --> 1:21:38.740\n But I think it's unlikely.\n\n1:21:38.740 --> 1:21:40.180\n So we talk about consciousness,\n\n1:21:40.180 --> 1:21:42.380\n but let me talk about another poorly defined concept\n\n1:21:42.380 --> 1:21:43.440\n of intelligence.\n\n1:21:44.580 --> 1:21:46.860\n Again, we've talked about reasoning,\n\n1:21:46.860 --> 1:21:48.100\n we've talked about memory.\n\n1:21:48.100 --> 1:21:51.660\n What do you think is a good test of intelligence for you?\n\n1:21:51.660 --> 1:21:55.700\n Are you impressed by the test that Alan Turing formulated\n\n1:21:55.700 --> 1:21:58.580\n with the imitation game with natural language?\n\n1:21:58.580 --> 1:22:01.100\n Is there something in your mind\n\n1:22:01.100 --> 1:22:04.260\n that you will be deeply impressed by\n\n1:22:04.260 --> 1:22:06.420\n if a system was able to do?\n\n1:22:06.420 --> 1:22:07.980\n I mean, lots of things.\n\n1:22:07.980 --> 1:22:12.100\n There's a certain frontier of capabilities today.\n\n1:22:13.260 --> 1:22:16.900\n And there exist things outside of that frontier.\n\n1:22:16.900 --> 1:22:18.980\n And I would be impressed by any such thing.\n\n1:22:18.980 --> 1:22:23.980\n For example, I would be impressed by a deep learning system\n\n1:22:24.580 --> 1:22:27.260\n which solves a very pedestrian task,\n\n1:22:27.260 --> 1:22:29.700\n like machine translation or computer vision task\n\n1:22:29.700 --> 1:22:33.420\n or something which never makes mistake\n\n1:22:33.420 --> 1:22:37.300\n a human wouldn't make under any circumstances.\n\n1:22:37.300 --> 1:22:38.540\n I think that is something\n\n1:22:38.540 --> 1:22:40.060\n which have not yet been demonstrated\n\n1:22:40.060 --> 1:22:42.740\n and I would find it very impressive.\n\n1:22:42.740 --> 1:22:44.860\n Yeah, so right now they make mistakes in different,\n\n1:22:44.860 --> 1:22:46.580\n they might be more accurate than human beings,\n\n1:22:46.580 --> 1:22:49.100\n but they still, they make a different set of mistakes.\n\n1:22:49.100 --> 1:22:53.420\n So my, I would guess that a lot of the skepticism\n\n1:22:53.420 --> 1:22:55.780\n that some people have about deep learning\n\n1:22:55.780 --> 1:22:57.380\n is when they look at their mistakes and they say,\n\n1:22:57.380 --> 1:23:00.260\n well, those mistakes, they make no sense.\n\n1:23:00.260 --> 1:23:01.660\n Like if you understood the concept,\n\n1:23:01.660 --> 1:23:04.060\n you wouldn't make that mistake.\n\n1:23:04.060 --> 1:23:07.380\n And I think that changing that would be,\n\n1:23:07.380 --> 1:23:09.380\n that would inspire me.\n\n1:23:09.380 --> 1:23:12.580\n That would be, yes, this is progress.\n\n1:23:12.580 --> 1:23:15.460\n Yeah, that's a really nice way to put it.\n\n1:23:15.460 --> 1:23:18.580\n But I also just don't like that human instinct\n\n1:23:18.580 --> 1:23:21.540\n to criticize a model is not intelligent.\n\n1:23:21.540 --> 1:23:23.180\n That's the same instinct as we do\n\n1:23:23.180 --> 1:23:27.740\n when we criticize any group of creatures as the other.\n\n1:23:28.820 --> 1:23:33.500\n Because it's very possible that GPT2\n\n1:23:33.500 --> 1:23:36.420\n is much smarter than human beings at many things.\n\n1:23:36.420 --> 1:23:37.620\n That's definitely true.\n\n1:23:37.620 --> 1:23:39.380\n It has a lot more breadth of knowledge.\n\n1:23:39.380 --> 1:23:41.020\n Yes, breadth of knowledge\n\n1:23:41.020 --> 1:23:44.980\n and even perhaps depth on certain topics.\n\n1:23:46.140 --> 1:23:48.380\n It's kind of hard to judge what depth means,\n\n1:23:48.380 --> 1:23:51.180\n but there's definitely a sense in which\n\n1:23:51.180 --> 1:23:54.860\n humans don't make mistakes that these models do.\n\n1:23:54.860 --> 1:23:57.780\n The same is applied to autonomous vehicles.\n\n1:23:57.780 --> 1:23:59.700\n The same is probably gonna continue being applied\n\n1:23:59.700 --> 1:24:01.740\n to a lot of artificial intelligence systems.\n\n1:24:01.740 --> 1:24:04.100\n We find, this is the annoying thing.\n\n1:24:04.100 --> 1:24:06.820\n This is the process of, in the 21st century,\n\n1:24:06.820 --> 1:24:09.460\n the process of analyzing the progress of AI\n\n1:24:09.460 --> 1:24:13.380\n is the search for one case where the system fails\n\n1:24:13.380 --> 1:24:17.020\n in a big way where humans would not.\n\n1:24:17.020 --> 1:24:20.460\n And then many people writing articles about it.\n\n1:24:20.460 --> 1:24:24.820\n And then broadly, the public generally gets convinced\n\n1:24:24.820 --> 1:24:26.580\n that the system is not intelligent.\n\n1:24:26.580 --> 1:24:29.860\n And we pacify ourselves by thinking it's not intelligent\n\n1:24:29.860 --> 1:24:31.980\n because of this one anecdotal case.\n\n1:24:31.980 --> 1:24:34.540\n And this seems to continue happening.\n\n1:24:34.540 --> 1:24:36.900\n Yeah, I mean, there is truth to that.\n\n1:24:36.900 --> 1:24:38.140\n Although I'm sure that plenty of people\n\n1:24:38.140 --> 1:24:39.220\n are also extremely impressed\n\n1:24:39.220 --> 1:24:40.860\n by the system that exists today.\n\n1:24:40.860 --> 1:24:42.500\n But I think this connects to the earlier point\n\n1:24:42.500 --> 1:24:45.020\n we discussed that it's just confusing\n\n1:24:45.020 --> 1:24:47.100\n to judge progress in AI.\n\n1:24:47.100 --> 1:24:47.940\n Yeah.\n\n1:24:47.940 --> 1:24:50.700\n And you have a new robot demonstrating something.\n\n1:24:50.700 --> 1:24:52.700\n How impressed should you be?\n\n1:24:52.700 --> 1:24:55.980\n And I think that people will start to be impressed\n\n1:24:55.980 --> 1:24:59.340\n once AI starts to really move the needle on the GDP.\n\n1:25:00.380 --> 1:25:02.020\n So you're one of the people that might be able\n\n1:25:02.020 --> 1:25:03.740\n to create an AGI system here.\n\n1:25:03.740 --> 1:25:05.700\n Not you, but you and OpenAI.\n\n1:25:06.820 --> 1:25:09.020\n If you do create an AGI system\n\n1:25:09.020 --> 1:25:11.940\n and you get to spend sort of the evening\n\n1:25:11.940 --> 1:25:17.900\n with it, him, her, what would you talk about, do you think?\n\n1:25:17.900 --> 1:25:19.140\n The very first time?\n\n1:25:19.140 --> 1:25:19.980\n First time.\n\n1:25:19.980 --> 1:25:23.620\n Well, the first time I would just ask all kinds of questions\n\n1:25:23.620 --> 1:25:25.700\n and try to get it to make a mistake.\n\n1:25:25.700 --> 1:25:28.100\n And I would be amazed that it doesn't make mistakes\n\n1:25:28.100 --> 1:25:33.100\n and just keep asking broad questions.\n\n1:25:33.100 --> 1:25:34.940\n What kind of questions do you think?\n\n1:25:34.940 --> 1:25:39.100\n Would they be factual or would they be personal,\n\n1:25:39.100 --> 1:25:40.940\n emotional, psychological?\n\n1:25:40.940 --> 1:25:42.500\n What do you think?\n\n1:25:42.500 --> 1:25:43.420\n All of the above.\n\n1:25:46.100 --> 1:25:47.260\n Would you ask for advice?\n\n1:25:47.260 --> 1:25:48.100\n Definitely.\n\n1:25:49.260 --> 1:25:51.580\n I mean, why would I limit myself\n\n1:25:51.580 --> 1:25:53.140\n talking to a system like this?\n\n1:25:53.140 --> 1:25:56.100\n Now, again, let me emphasize the fact\n\n1:25:56.100 --> 1:25:57.780\n that you truly are one of the people\n\n1:25:57.780 --> 1:26:01.220\n that might be in the room where this happens.\n\n1:26:01.220 --> 1:26:06.540\n So let me ask sort of a profound question about,\n\n1:26:06.540 --> 1:26:08.540\n I've just talked to a Stalin historian.\n\n1:26:08.540 --> 1:26:13.180\n I've been talking to a lot of people who are studying power.\n\n1:26:13.180 --> 1:26:14.780\n Abraham Lincoln said,\n\n1:26:14.780 --> 1:26:17.700\n \"'Nearly all men can stand adversity,\n\n1:26:17.700 --> 1:26:21.380\n \"'but if you want to test a man's character, give him power.'\"\n\n1:26:21.380 --> 1:26:24.700\n I would say the power of the 21st century,\n\n1:26:24.700 --> 1:26:28.460\n maybe the 22nd, but hopefully the 21st,\n\n1:26:28.460 --> 1:26:30.260\n would be the creation of an AGI system\n\n1:26:30.260 --> 1:26:33.420\n and the people who have control,\n\n1:26:33.420 --> 1:26:36.260\n direct possession and control of the AGI system.\n\n1:26:36.260 --> 1:26:39.500\n So what do you think, after spending that evening\n\n1:26:39.500 --> 1:26:42.900\n having a discussion with the AGI system,\n\n1:26:42.900 --> 1:26:44.300\n what do you think you would do?\n\n1:26:45.500 --> 1:26:47.980\n Well, the ideal world I'd like to imagine\n\n1:26:50.180 --> 1:26:52.820\n is one where humanity,\n\n1:26:52.820 --> 1:26:57.820\n I like, the board members of a company\n\n1:26:57.940 --> 1:26:59.500\n where the AGI is the CEO.\n\n1:26:59.500 --> 1:27:04.500\n So it would be, I would like,\n\n1:27:04.500 --> 1:27:05.860\n the picture which I would imagine\n\n1:27:05.860 --> 1:27:09.540\n is you have some kind of different entities,\n\n1:27:09.540 --> 1:27:11.700\n different countries or cities,\n\n1:27:11.700 --> 1:27:13.220\n and the people that leave their vote\n\n1:27:13.220 --> 1:27:16.220\n for what the AGI that represents them should do,\n\n1:27:16.220 --> 1:27:18.660\n and the AGI that represents them goes and does it.\n\n1:27:18.660 --> 1:27:23.660\n I think a picture like that, I find very appealing.\n\n1:27:23.660 --> 1:27:24.500\n You could have multiple AGI,\n\n1:27:24.500 --> 1:27:26.620\n you would have an AGI for a city, for a country,\n\n1:27:26.620 --> 1:27:27.980\n and there would be multiple AGI's,\n\n1:27:27.980 --> 1:27:30.740\n for a city, for a country, and there would be,\n\n1:27:30.740 --> 1:27:33.980\n it would be trying to, in effect,\n\n1:27:33.980 --> 1:27:36.060\n take the democratic process to the next level.\n\n1:27:36.060 --> 1:27:38.660\n And the board can always fire the CEO.\n\n1:27:38.660 --> 1:27:40.660\n Essentially, press the reset button, say.\n\n1:27:40.660 --> 1:27:41.500\n Press the reset button.\n\n1:27:41.500 --> 1:27:42.940\n Rerandomize the parameters.\n\n1:27:42.940 --> 1:27:45.980\n But let me sort of, that's actually,\n\n1:27:45.980 --> 1:27:49.060\n okay, that's a beautiful vision, I think,\n\n1:27:49.060 --> 1:27:52.420\n as long as it's possible to press the reset button.\n\n1:27:53.460 --> 1:27:54.980\n Do you think it will always be possible\n\n1:27:54.980 --> 1:27:56.380\n to press the reset button?\n\n1:27:56.380 --> 1:28:00.380\n So I think that it definitely will be possible to build.\n\n1:28:02.100 --> 1:28:03.860\n So you're talking, so the question\n\n1:28:03.860 --> 1:28:06.620\n that I really understand from you is,\n\n1:28:06.620 --> 1:28:11.620\n will humans or humans people have control\n\n1:28:12.500 --> 1:28:14.260\n over the AI systems that they build?\n\n1:28:14.260 --> 1:28:15.100\n Yes.\n\n1:28:15.100 --> 1:28:17.300\n And my answer is, it's definitely possible\n\n1:28:17.300 --> 1:28:19.580\n to build AI systems which will want\n\n1:28:19.580 --> 1:28:21.820\n to be controlled by their humans.\n\n1:28:21.820 --> 1:28:24.020\n Wow, that's part of their,\n\n1:28:24.020 --> 1:28:26.180\n so it's not that just they can't help but be controlled,\n\n1:28:26.180 --> 1:28:31.180\n but that's the, they exist,\n\n1:28:31.540 --> 1:28:33.500\n the one of the objectives of their existence\n\n1:28:33.500 --> 1:28:34.500\n is to be controlled.\n\n1:28:34.500 --> 1:28:37.740\n In the same way that human parents\n\n1:28:39.780 --> 1:28:42.460\n generally want to help their children,\n\n1:28:42.460 --> 1:28:44.420\n they want their children to succeed.\n\n1:28:44.420 --> 1:28:46.020\n It's not a burden for them.\n\n1:28:46.020 --> 1:28:49.340\n They are excited to help children and to feed them\n\n1:28:49.340 --> 1:28:52.460\n and to dress them and to take care of them.\n\n1:28:52.460 --> 1:28:56.300\n And I believe with high conviction\n\n1:28:56.300 --> 1:28:58.900\n that the same will be possible for an AGI.\n\n1:28:58.900 --> 1:29:00.500\n It will be possible to program an AGI,\n\n1:29:00.500 --> 1:29:01.700\n to design it in such a way\n\n1:29:01.700 --> 1:29:04.820\n that it will have a similar deep drive\n\n1:29:04.820 --> 1:29:07.060\n that it will be delighted to fulfill.\n\n1:29:07.060 --> 1:29:09.900\n And the drive will be to help humans flourish.\n\n1:29:11.180 --> 1:29:13.940\n But let me take a step back to that moment\n\n1:29:13.940 --> 1:29:15.460\n where you create the AGI system.\n\n1:29:15.460 --> 1:29:17.460\n I think this is a really crucial moment.\n\n1:29:17.460 --> 1:29:21.660\n And between that moment\n\n1:29:21.660 --> 1:29:26.660\n and the Democratic board members with the AGI at the head,\n\n1:29:28.900 --> 1:29:31.860\n there has to be a relinquishing of power.\n\n1:29:31.860 --> 1:29:36.500\n So as George Washington, despite all the bad things he did,\n\n1:29:36.500 --> 1:29:39.340\n one of the big things he did is he relinquished power.\n\n1:29:39.340 --> 1:29:42.180\n He, first of all, didn't want to be president.\n\n1:29:42.180 --> 1:29:43.780\n And even when he became president,\n\n1:29:43.780 --> 1:29:45.960\n he gave, he didn't keep just serving\n\n1:29:45.960 --> 1:29:48.080\n as most dictators do for indefinitely.\n\n1:29:49.140 --> 1:29:54.140\n Do you see yourself being able to relinquish control\n\n1:29:55.180 --> 1:29:56.300\n over an AGI system,\n\n1:29:56.300 --> 1:29:59.300\n given how much power you can have over the world,\n\n1:29:59.300 --> 1:30:02.780\n at first financial, just make a lot of money, right?\n\n1:30:02.780 --> 1:30:07.020\n And then control by having possession as AGI system.\n\n1:30:07.020 --> 1:30:09.060\n I'd find it trivial to do that.\n\n1:30:09.060 --> 1:30:11.500\n I'd find it trivial to relinquish this kind of power.\n\n1:30:11.500 --> 1:30:15.100\n I mean, the kind of scenario you are describing\n\n1:30:15.100 --> 1:30:17.420\n sounds terrifying to me.\n\n1:30:17.420 --> 1:30:19.020\n That's all.\n\n1:30:19.020 --> 1:30:22.420\n I would absolutely not want to be in that position.\n\n1:30:22.420 --> 1:30:25.680\n Do you think you represent the majority\n\n1:30:25.680 --> 1:30:29.420\n or the minority of people in the AI community?\n\n1:30:29.420 --> 1:30:30.740\n Well, I mean.\n\n1:30:30.740 --> 1:30:32.920\n Say open question, an important one.\n\n1:30:33.780 --> 1:30:36.540\n Are most people good is another way to ask it.\n\n1:30:36.540 --> 1:30:39.340\n So I don't know if most people are good,\n\n1:30:39.340 --> 1:30:44.340\n but I think that when it really counts,\n\n1:30:44.340 --> 1:30:46.140\n people can be better than we think.\n\n1:30:47.040 --> 1:30:49.260\n That's beautifully put, yeah.\n\n1:30:49.260 --> 1:30:51.480\n Are there specific mechanism you can think of\n\n1:30:51.480 --> 1:30:54.580\n of aligning AI values to human values?\n\n1:30:54.580 --> 1:30:56.680\n Is that, do you think about these problems\n\n1:30:56.680 --> 1:31:00.320\n of continued alignment as we develop the AI systems?\n\n1:31:00.320 --> 1:31:01.380\n Yeah, definitely.\n\n1:31:02.780 --> 1:31:07.320\n In some sense, the kind of question which you are asking is,\n\n1:31:07.320 --> 1:31:10.660\n so if I were to translate the question to today's terms,\n\n1:31:10.660 --> 1:31:15.660\n it would be a question about how to get an RL agent\n\n1:31:17.040 --> 1:31:21.160\n that's optimizing a value function which itself is learned.\n\n1:31:21.160 --> 1:31:23.160\n And if you look at humans, humans are like that\n\n1:31:23.160 --> 1:31:26.280\n because the reward function, the value function of humans\n\n1:31:26.280 --> 1:31:28.800\n is not external, it is internal.\n\n1:31:28.800 --> 1:31:30.160\n That's right.\n\n1:31:30.160 --> 1:31:33.880\n And there are definite ideas\n\n1:31:33.880 --> 1:31:36.760\n of how to train a value function.\n\n1:31:36.760 --> 1:31:39.120\n Basically an objective, you know,\n\n1:31:39.120 --> 1:31:41.560\n and as objective as possible perception system\n\n1:31:42.560 --> 1:31:47.560\n that will be trained separately to recognize,\n\n1:31:47.640 --> 1:31:51.960\n to internalize human judgments on different situations.\n\n1:31:51.960 --> 1:31:54.640\n And then that component would then be integrated\n\n1:31:54.640 --> 1:31:56.520\n as the base value function\n\n1:31:56.520 --> 1:31:59.040\n for some more capable RL system.\n\n1:31:59.040 --> 1:32:00.600\n You could imagine a process like this.\n\n1:32:00.600 --> 1:32:02.440\n I'm not saying this is the process,\n\n1:32:02.440 --> 1:32:03.800\n I'm saying this is an example\n\n1:32:03.800 --> 1:32:05.700\n of the kind of thing you could do.\n\n1:32:05.700 --> 1:32:10.700\n So on that topic of the objective functions\n\n1:32:11.140 --> 1:32:12.120\n of human existence,\n\n1:32:12.120 --> 1:32:15.020\n what do you think is the objective function\n\n1:32:15.020 --> 1:32:17.420\n that's implicit in human existence?\n\n1:32:17.420 --> 1:32:18.920\n What's the meaning of life?\n\n1:32:18.920 --> 1:32:19.760\n Oh.\n\n1:32:28.860 --> 1:32:31.460\n I think the question is wrong in some way.\n\n1:32:31.460 --> 1:32:33.780\n I think that the question implies\n\n1:32:33.780 --> 1:32:35.620\n that there is an objective answer\n\n1:32:35.620 --> 1:32:36.580\n which is an external answer,\n\n1:32:36.580 --> 1:32:38.620\n you know, your meaning of life is X.\n\n1:32:38.620 --> 1:32:40.740\n I think what's going on is that we exist\n\n1:32:40.740 --> 1:32:44.220\n and that's amazing.\n\n1:32:44.220 --> 1:32:45.660\n And we should try to make the most of it\n\n1:32:45.660 --> 1:32:48.180\n and try to maximize our own value\n\n1:32:48.180 --> 1:32:53.180\n and enjoyment of a very short time while we do exist.\n\n1:32:53.220 --> 1:32:54.060\n It's funny,\n\n1:32:54.060 --> 1:32:56.180\n because action does require an objective function\n\n1:32:56.180 --> 1:32:58.600\n is definitely there in some form,\n\n1:32:58.600 --> 1:33:01.080\n but it's difficult to make it explicit\n\n1:33:01.080 --> 1:33:02.840\n and maybe impossible to make it explicit,\n\n1:33:02.840 --> 1:33:03.940\n I guess is what you're getting at.\n\n1:33:03.940 --> 1:33:08.140\n And that's an interesting fact of an RL environment.\n\n1:33:08.140 --> 1:33:10.540\n Well, but I was making a slightly different point\n\n1:33:10.540 --> 1:33:13.360\n is that humans want things\n\n1:33:13.360 --> 1:33:16.980\n and their wants create the drives that cause them to,\n\n1:33:16.980 --> 1:33:19.900\n you know, our wants are our objective functions,\n\n1:33:19.900 --> 1:33:21.960\n our individual objective functions.\n\n1:33:21.960 --> 1:33:24.340\n We can later decide that we want to change,\n\n1:33:24.340 --> 1:33:26.060\n that what we wanted before is no longer good\n\n1:33:26.060 --> 1:33:27.280\n and we want something else.\n\n1:33:27.280 --> 1:33:29.020\n Yeah, but they're so dynamic,\n\n1:33:29.020 --> 1:33:32.180\n there's gotta be some underlying sort of Freud,\n\n1:33:32.180 --> 1:33:33.980\n there's things, there's like sexual stuff,\n\n1:33:33.980 --> 1:33:37.220\n there's people who think it's the fear of death\n\n1:33:37.220 --> 1:33:40.300\n and there's also the desire for knowledge\n\n1:33:40.300 --> 1:33:42.100\n and you know, all these kinds of things,\n\n1:33:42.100 --> 1:33:46.220\n procreation, sort of all the evolutionary arguments,\n\n1:33:46.220 --> 1:33:47.100\n it seems to be,\n\n1:33:47.100 --> 1:33:49.500\n there might be some kind of fundamental objective function\n\n1:33:49.500 --> 1:33:54.100\n from which everything else emerges,\n\n1:33:54.100 --> 1:33:56.860\n but it seems like it's very difficult to make it explicit.\n\n1:33:56.860 --> 1:33:58.900\n I think that probably is an evolutionary objective function\n\n1:33:58.900 --> 1:34:00.260\n which is to survive and procreate\n\n1:34:00.260 --> 1:34:02.560\n and make sure you make your children succeed.\n\n1:34:02.560 --> 1:34:04.260\n That would be my guess,\n\n1:34:04.260 --> 1:34:06.860\n but it doesn't give an answer to the question\n\n1:34:06.860 --> 1:34:08.180\n of what's the meaning of life.\n\n1:34:08.180 --> 1:34:13.180\n I think you can see how humans are part of this big process,\n\n1:34:13.260 --> 1:34:14.340\n this ancient process.\n\n1:34:14.340 --> 1:34:19.340\n We exist on a small planet and that's it.\n\n1:34:20.780 --> 1:34:24.220\n So given that we exist, try to make the most of it\n\n1:34:24.220 --> 1:34:28.080\n and try to enjoy more and suffer less as much as we can.\n\n1:34:28.080 --> 1:34:31.240\n Let me ask two silly questions about life.\n\n1:34:32.800 --> 1:34:34.780\n One, do you have regrets?\n\n1:34:34.780 --> 1:34:39.000\n Moments that if you went back, you would do differently.\n\n1:34:39.000 --> 1:34:42.320\n And two, are there moments that you're especially proud of\n\n1:34:42.320 --> 1:34:43.680\n that made you truly happy?\n\n1:34:44.720 --> 1:34:47.520\n So I can answer that, I can answer both questions.\n\n1:34:47.520 --> 1:34:51.240\n Of course, there's a huge number of choices\n\n1:34:51.240 --> 1:34:52.440\n and decisions that I've made\n\n1:34:52.440 --> 1:34:54.240\n that with the benefit of hindsight,\n\n1:34:54.240 --> 1:34:55.480\n I wouldn't have made them.\n\n1:34:55.480 --> 1:34:56.940\n And I do experience some regret,\n\n1:34:56.940 --> 1:35:00.120\n but I try to take solace in the knowledge\n\n1:35:00.120 --> 1:35:02.920\n that at the time I did the best I could.\n\n1:35:02.920 --> 1:35:04.680\n And in terms of things that I'm proud of,\n\n1:35:04.680 --> 1:35:07.600\n I'm very fortunate to have done things I'm proud of\n\n1:35:08.680 --> 1:35:10.920\n and they made me happy for some time,\n\n1:35:10.920 --> 1:35:13.680\n but I don't think that that is the source of happiness.\n\n1:35:14.640 --> 1:35:17.360\n So your academic accomplishments, all the papers,\n\n1:35:17.360 --> 1:35:19.940\n you're one of the most cited people in the world.\n\n1:35:19.940 --> 1:35:21.720\n All of the breakthroughs I mentioned\n\n1:35:21.720 --> 1:35:23.840\n in computer vision and language and so on,\n\n1:35:23.840 --> 1:35:28.840\n what is the source of happiness and pride for you?\n\n1:35:29.560 --> 1:35:31.400\n I mean, all those things are a source of pride for sure.\n\n1:35:31.400 --> 1:35:35.180\n I'm very grateful for having done all those things\n\n1:35:35.180 --> 1:35:37.440\n and it was very fun to do them.\n\n1:35:37.440 --> 1:35:40.220\n But happiness comes, but you know, happiness,\n\n1:35:40.220 --> 1:35:42.600\n well, my current view is that happiness comes\n\n1:35:42.600 --> 1:35:45.260\n from our, to a very large degree,\n\n1:35:45.260 --> 1:35:47.740\n from the way we look at things.\n\n1:35:47.740 --> 1:35:49.160\n You know, you can have a simple meal\n\n1:35:49.160 --> 1:35:51.320\n and be quite happy as a result,\n\n1:35:51.320 --> 1:35:54.880\n or you can talk to someone and be happy as a result as well.\n\n1:35:54.880 --> 1:35:58.200\n Or conversely, you can have a meal and be disappointed\n\n1:35:58.200 --> 1:36:00.420\n that the meal wasn't a better meal.\n\n1:36:00.420 --> 1:36:02.360\n So I think a lot of happiness comes from that,\n\n1:36:02.360 --> 1:36:05.520\n but I'm not sure, I don't want to be too confident.\n\n1:36:05.520 --> 1:36:07.840\n Being humble in the face of the uncertainty\n\n1:36:07.840 --> 1:36:12.140\n seems to be also a part of this whole happiness thing.\n\n1:36:12.140 --> 1:36:14.040\n Well, I don't think there's a better way to end it\n\n1:36:14.040 --> 1:36:17.880\n than meaning of life and discussions of happiness.\n\n1:36:17.880 --> 1:36:19.720\n So Ilya, thank you so much.\n\n1:36:19.720 --> 1:36:22.600\n You've given me a few incredible ideas.\n\n1:36:22.600 --> 1:36:24.860\n You've given the world many incredible ideas.\n\n1:36:24.860 --> 1:36:27.480\n I really appreciate it and thanks for talking today.\n\n1:36:27.480 --> 1:36:30.520\n Yeah, thanks for stopping by, I really enjoyed it.\n\n1:36:30.520 --> 1:36:32.040\n Thanks for listening to this conversation\n\n1:36:32.040 --> 1:36:33.960\n with Ilya Setskever and thank you\n\n1:36:33.960 --> 1:36:36.360\n to our presenting sponsor, Cash App.\n\n1:36:36.360 --> 1:36:38.120\n Please consider supporting the podcast\n\n1:36:38.120 --> 1:36:42.600\n by downloading Cash App and using the code LEXPodcast.\n\n1:36:42.600 --> 1:36:45.400\n If you enjoy this podcast, subscribe on YouTube,\n\n1:36:45.400 --> 1:36:47.960\n review it with five stars on Apple Podcast,\n\n1:36:47.960 --> 1:36:51.420\n support on Patreon, or simply connect with me on Twitter\n\n1:36:51.420 --> 1:36:52.960\n at Lex Friedman.\n\n1:36:54.120 --> 1:36:56.320\n And now let me leave you with some words\n\n1:36:56.320 --> 1:36:58.860\n from Alan Turing on machine learning.\n\n1:37:00.140 --> 1:37:01.880\n Instead of trying to produce a program\n\n1:37:01.880 --> 1:37:03.740\n to simulate the adult mind,\n\n1:37:03.740 --> 1:37:06.240\n why not rather try to produce one\n\n1:37:06.240 --> 1:37:08.740\n which simulates the child?\n\n1:37:08.740 --> 1:37:10.240\n If this were then subjected\n\n1:37:10.240 --> 1:37:12.500\n to an appropriate course of education,\n\n1:37:12.500 --> 1:37:15.200\n one would obtain the adult brain.\n\n1:37:15.200 --> 1:37:19.300\n Thank you for listening and hope to see you next time.\n\n"
}