{
  "title": "Dmitri Dolgov: Waymo and the Future of Self-Driving Cars | Lex Fridman Podcast #147",
  "id": "P6prRXkI5HM",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:05.420\n The following is a conversation with Dimitri Dolgov, the CTO of Waymo, which\n\n00:05.420 --> 00:09.260\n is an autonomous driving company that started as Google self driving car\n\n00:09.260 --> 00:13.160\n project in 2009 and became Waymo in 2016.\n\n00:13.940 --> 00:15.700\n Dimitri was there all along.\n\n00:16.180 --> 00:20.140\n Waymo is currently leading in the fully autonomous vehicle space and that they\n\n00:20.220 --> 00:25.460\n actually have an at scale deployment of publicly accessible autonomous vehicles\n\n00:25.460 --> 00:32.060\n driving passengers around with no safety driver, with nobody in the driver's seat.\n\n00:32.560 --> 00:37.400\n This to me is an incredible accomplishment of engineering on one of\n\n00:37.400 --> 00:41.300\n the most difficult and exciting artificial intelligence challenges of\n\n00:41.300 --> 00:42.440\n the 21st century.\n\n00:43.200 --> 00:46.900\n Quick mention of a sponsor, followed by some thoughts related to the episode.\n\n00:47.440 --> 00:51.860\n Thank you to Triolabs, a company that helps businesses apply machine\n\n00:51.860 --> 00:54.260\n learning to solve real world problems.\n\n00:54.260 --> 00:58.500\n Blinkist, an app I use for reading through summaries of books, better\n\n00:58.500 --> 01:02.900\n help, online therapy with a licensed professional, and Cash App, the app\n\n01:02.900 --> 01:04.220\n I use to send money to friends.\n\n01:04.820 --> 01:08.020\n Please check out the sponsors in the description to get a discount\n\n01:08.060 --> 01:09.380\n at the support this podcast.\n\n01:10.060 --> 01:13.980\n As a side note, let me say that autonomous and semi autonomous driving\n\n01:14.160 --> 01:18.420\n was the focus of my work at MIT and as a problem space that I find\n\n01:18.420 --> 01:23.300\n fascinating and full of open questions from both robotics and a human\n\n01:23.300 --> 01:24.500\n psychology perspective.\n\n01:25.220 --> 01:29.420\n There's quite a bit that I could say here about my experiences in academia\n\n01:29.420 --> 01:35.700\n on this topic that revealed to me, let's say the less admirable size of human\n\n01:35.700 --> 01:39.620\n beings, but I choose to focus on the positive, on solutions.\n\n01:40.020 --> 01:44.220\n I'm brilliant engineers like Dimitri and the team at Waymo, who work\n\n01:44.220 --> 01:48.300\n tirelessly to innovate and to build amazing technology that will define\n\n01:48.300 --> 01:48.900\n our future.\n\n01:48.900 --> 01:53.020\n Because of Dimitri and others like him, I'm excited for this future.\n\n01:53.900 --> 01:59.100\n And who knows, perhaps I too will help contribute something of value to it.\n\n01:59.900 --> 02:03.220\n If you enjoy this thing, subscribe on YouTube, review it with five stars\n\n02:03.220 --> 02:07.380\n and up a podcast, follow on Spotify, support on Patreon, or connect with\n\n02:07.380 --> 02:09.340\n me on Twitter at Lex Friedman.\n\n02:10.140 --> 02:13.700\n And now here's my conversation with Dimitri Dolgov.\n\n02:14.940 --> 02:17.500\n When did you first fall in love with MIT?\n\n02:17.500 --> 02:20.860\n When did you first fall in love with robotics or even computer\n\n02:20.860 --> 02:21.740\n science more in general?\n\n02:22.300 --> 02:27.340\n Computer science first at a fairly young age, then robotics happened much later.\n\n02:28.340 --> 02:39.020\n I think my first interesting introduction to computers was in the late 80s when\n\n02:39.020 --> 02:44.580\n we got our first computer, I think it was an IBM, I think IBM AT.\n\n02:44.580 --> 02:48.020\n Those things that had like a turbo button in the front, the radio\n\n02:48.020 --> 02:50.020\n precedent, you know, make, make the thing goes faster.\n\n02:50.500 --> 02:52.100\n Did that already have floppy disks?\n\n02:52.420 --> 02:52.740\n Yeah.\n\n02:52.780 --> 02:52.980\n Yeah.\n\n02:52.980 --> 02:53.140\n Yeah.\n\n02:53.140 --> 02:53.340\n Yeah.\n\n02:53.340 --> 02:57.060\n Like the, the 5.4 inch ones.\n\n02:57.140 --> 02:58.740\n I think there was a bigger inch.\n\n02:58.780 --> 02:59.220\n So good.\n\n02:59.220 --> 03:01.700\n When something then five inches and three inches.\n\n03:02.060 --> 03:03.100\n Yeah, I think that was the five.\n\n03:03.100 --> 03:06.580\n I don't, I maybe that was before that was the giant plates and it didn't get that.\n\n03:07.180 --> 03:09.300\n But it was definitely not the, not the three inch ones.\n\n03:09.300 --> 03:15.660\n Anyway, so that, that, you know, we got that computer, I spent the first few\n\n03:15.660 --> 03:20.660\n months just playing video games as you would expect, I got bored of that.\n\n03:20.900 --> 03:25.740\n So I started messing around and trying to figure out how to, you know, make\n\n03:25.740 --> 03:33.340\n the thing do other stuff, got into exploring programming and a couple of\n\n03:33.340 --> 03:39.580\n years later, it got to a point where, I actually wrote a game, a lot of games\n\n03:39.620 --> 03:43.660\n and a game developer, a Japanese game developer actually offered to buy it\n\n03:43.660 --> 03:45.100\n for me for a few hundred bucks.\n\n03:45.100 --> 03:48.740\n But you know, for, for a kid in Russia, that's a big deal.\n\n03:48.740 --> 03:49.180\n That's a big deal.\n\n03:49.180 --> 03:49.460\n Yeah.\n\n03:49.780 --> 03:50.660\n I did not take the deal.\n\n03:51.140 --> 03:51.500\n Wow.\n\n03:51.700 --> 03:52.260\n Integrity.\n\n03:52.500 --> 03:52.780\n Yeah.\n\n03:53.020 --> 03:58.860\n I, I instead, yes, that was not the most acute financial move that I made in my\n\n03:58.860 --> 04:02.380\n life, you know, looking back at it now, I, I instead put it, well, you know, I had\n\n04:02.380 --> 04:07.180\n a reason I put it online, it was, what'd you call it back in the days?\n\n04:07.180 --> 04:08.420\n It was a freeware thing, right?\n\n04:08.460 --> 04:11.260\n It was not open source, but you could upload the binaries, you would put the\n\n04:11.260 --> 04:14.420\n game online and the idea was that, you know, people like it and then they, you\n\n04:14.420 --> 04:16.500\n know, contribute on the send you a little donations, right?\n\n04:16.540 --> 04:20.020\n So I did my quick math of like, you know, of course, you know, thousands and\n\n04:20.020 --> 04:22.620\n millions of people are going to play my game, send me a couple of bucks a piece,\n\n04:22.620 --> 04:23.700\n you know, should definitely do that.\n\n04:24.500 --> 04:26.580\n As I said, not, not the best.\n\n04:26.580 --> 04:29.060\n You're already playing with business models at that young age.\n\n04:29.300 --> 04:30.500\n Remember what language it was?\n\n04:30.500 --> 04:35.020\n What programming, it was a Pascal, which what Pascal, Pascal, and that\n\n04:35.020 --> 04:37.100\n a graphical component, so it's not text based.\n\n04:37.180 --> 04:37.460\n Yeah.\n\n04:37.460 --> 04:37.660\n Yeah.\n\n04:37.660 --> 04:43.140\n It was, uh, like, uh, I think there are 300, 320 by 200, uh, whatever it was.\n\n04:43.180 --> 04:46.340\n I think that kind of the earlier, that's the resolution, right?\n\n04:46.420 --> 04:49.420\n And I actually think the reason why this company wanted to buy it is not like the\n\n04:49.420 --> 04:51.340\n fancy graphics or the implementation.\n\n04:51.340 --> 04:55.620\n That was maybe the idea, uh, of my actual game, the idea of the game.\n\n04:57.140 --> 04:59.020\n Well, one of the things I, it's so funny.\n\n04:59.020 --> 05:05.540\n I'm used to play this game called golden X and the simplicity of the graphics and\n\n05:05.860 --> 05:10.260\n something about the simplicity of the music, like it's still haunts me.\n\n05:10.740 --> 05:12.060\n I don't know if that's a childhood thing.\n\n05:12.060 --> 05:14.940\n I don't know if that's the same thing for call of duty these days for young kids,\n\n05:15.340 --> 05:20.580\n but I still think that the simple one of the games are simple.\n\n05:21.260 --> 05:28.260\n That simple purity makes for like allows your imagination to take over and\n\n05:28.260 --> 05:30.300\n thereby creating a more magical experience.\n\n05:30.780 --> 05:34.100\n Like now with better and better graphics, it feels like your\n\n05:34.100 --> 05:38.980\n imagination doesn't get to, uh, create worlds, which is kind of interesting.\n\n05:38.980 --> 05:43.020\n Um, it could be just an old man on a porch, like way waving at kids\n\n05:43.020 --> 05:44.780\n these days that have no respect.\n\n05:44.780 --> 05:49.140\n But I still think that graphics almost get in the way of the experience.\n\n05:49.860 --> 05:50.220\n I don't know.\n\n05:50.740 --> 05:51.340\n Flip a bird.\n\n05:51.860 --> 05:57.300\n Yeah, I don't know if the imagination is closed.\n\n05:57.300 --> 06:01.540\n I don't yet, but that that's more about games that op like that's more\n\n06:01.540 --> 06:09.580\n like Tetris world where they optimally masterfully, like create a fun, short\n\n06:09.580 --> 06:14.540\n term dopamine experience versus I'm more referring to like role playing\n\n06:14.540 --> 06:18.860\n games where there's like a story you can live in it for months or years.\n\n06:18.860 --> 06:23.900\n Um, like, uh, there's an elder scroll series, which is probably my favorite\n\n06:23.900 --> 06:26.540\n set of games that was a magical experience.\n\n06:26.540 --> 06:28.100\n And that the graphics are terrible.\n\n06:28.460 --> 06:31.460\n The characters were all randomly generated, but they're, I don't know.\n\n06:31.500 --> 06:33.660\n That's it pulls you in.\n\n06:33.700 --> 06:34.660\n There's a story.\n\n06:34.700 --> 06:40.180\n It's like an interactive version of an elder scrolls Tolkien world.\n\n06:40.580 --> 06:41.660\n And you get to live in it.\n\n06:42.140 --> 06:42.580\n I don't know.\n\n06:43.460 --> 06:44.020\n I miss it.\n\n06:44.580 --> 06:49.540\n It's one of the things that suck about being an adult is there's no, you have\n\n06:49.540 --> 06:54.060\n to live in the real world as opposed to the elder scrolls world, you know, whatever\n\n06:54.060 --> 06:54.940\n brings you joy, right?\n\n06:54.940 --> 06:55.940\n Minecraft, right?\n\n06:55.940 --> 06:56.780\n Minecraft is a great example.\n\n06:56.780 --> 07:01.260\n You create, like it's not the fancy graphics, but it's the creation of your own worlds.\n\n07:01.420 --> 07:02.460\n Yeah, that one is crazy.\n\n07:02.700 --> 07:06.700\n You know, one of the pitches for being a parent that people tell me is that you\n\n07:06.700 --> 07:11.700\n can like use the excuse of parenting to, to go back into the video game world.\n\n07:12.180 --> 07:17.940\n And like, like that's like, you know, father, son, father, daughter time, but\n\n07:17.940 --> 07:19.820\n really you just get to play video games with your kids.\n\n07:19.820 --> 07:27.260\n So anyway, at that time, did you have any ridiculously ambitious dreams of where as\n\n07:27.260 --> 07:29.780\n a creator, you might go as an engineer?\n\n07:29.780 --> 07:33.820\n Did you, what, what did you think of yourself as, as an engineer, as a tinker,\n\n07:33.860 --> 07:36.100\n or did you want to be like an astronaut or something like that?\n\n07:37.220 --> 07:42.220\n You know, I'm tempted to make something up about, you know, robots, uh, engineering\n\n07:42.220 --> 07:45.940\n or, you know, mysteries of the universe, but that's not the actual memory that\n\n07:45.940 --> 07:48.780\n pops into my mind when you, when you asked me about childhood dreams.\n\n07:48.780 --> 07:55.860\n So I'll actually share the, the, the real thing, uh, when I was maybe four or five\n\n07:55.860 --> 08:00.340\n years old, I, you know, as we all do, I thought about, you know, what I wanted\n\n08:00.340 --> 08:07.820\n to do when I grow up and I had this dream of being a traffic control cop.\n\n08:08.660 --> 08:11.380\n Uh, you know, they don't have those today's I think, but you know, back in\n\n08:11.380 --> 08:15.300\n the eighties and in Russia, uh, you probably are familiar with that Lex.\n\n08:15.300 --> 08:19.300\n They had these, uh, you know, police officers that would stand in the middle\n\n08:19.300 --> 08:21.940\n of intersection all day and they would have their like stripe back, black and\n\n08:21.940 --> 08:26.060\n white batons that they would use to control the flow of traffic and, you\n\n08:26.060 --> 08:29.940\n know, for whatever reasons, I was strangely infatuated with this whole\n\n08:29.940 --> 08:31.740\n process and like that, that was my dream.\n\n08:32.220 --> 08:36.580\n Uh, that's what I wanted to do when I grew up and, you know, my parents, uh,\n\n08:37.100 --> 08:41.820\n both physics profs, by the way, I think were, you know, a little concerned, uh,\n\n08:41.860 --> 08:44.220\n with that level of ambition coming from their child.\n\n08:44.220 --> 08:46.420\n Uh, uh, you know, that age.\n\n08:46.740 --> 08:50.020\n Well, that it's an interesting, I don't know if you can relate,\n\n08:50.060 --> 08:51.900\n but I very much love that idea.\n\n08:52.300 --> 08:57.980\n I have a OCD nature that I think lends itself very close to the engineering\n\n08:57.980 --> 09:05.580\n mindset, which is you want to kind of optimize, you know, solve a problem by\n\n09:05.580 --> 09:11.220\n create, creating an automated solution, like a, like a set of rules, that set\n\n09:11.220 --> 09:14.340\n of rules you can follow and then thereby make it ultra efficient.\n\n09:14.820 --> 09:17.300\n I don't know if that's, it was of that nature.\n\n09:17.340 --> 09:18.380\n I certainly have that.\n\n09:18.580 --> 09:22.420\n There's like fact, like SimCity and factory building games, all those\n\n09:22.420 --> 09:26.020\n kinds of things kind of speak to that engineering mindset, or\n\n09:26.020 --> 09:27.220\n did you just like the uniform?\n\n09:27.500 --> 09:28.900\n I think it was more of the latter.\n\n09:28.900 --> 09:33.140\n I think it was the uniform and the, you know, the, the stripe baton that\n\n09:33.140 --> 09:35.780\n made cars go in the right directions.\n\n09:36.820 --> 09:40.980\n But I guess, you know, I, it is, I did end up, uh, I guess, uh,\n\n09:40.980 --> 09:44.860\n you know, working on the transportation industry one way or another uniform.\n\n09:44.860 --> 09:45.900\n No, but that's right.\n\n09:46.900 --> 09:51.500\n Maybe, maybe, maybe it was my, you know, deep inner infatuation with the,\n\n09:51.620 --> 09:55.180\n you know, traffic control batons that led to this career.\n\n09:55.460 --> 09:55.740\n Okay.\n\n09:55.740 --> 10:00.620\n What, uh, when did you, when was the leap from programming to robotics?\n\n10:00.940 --> 10:01.540\n That happened later.\n\n10:01.620 --> 10:05.340\n That was after grad school, uh, after, and I actually, the most self driving\n\n10:05.340 --> 10:10.020\n cars was I think my first real hands on introduction to robotics.\n\n10:10.020 --> 10:14.580\n But I never really had that much hands on experience in school and training.\n\n10:14.580 --> 10:17.020\n I, you know, worked on applied math and physics.\n\n10:17.340 --> 10:22.780\n Then in college, I did more half, uh, abstract computer science.\n\n10:23.540 --> 10:28.380\n And it was after grad school that I really got involved in robotics, which\n\n10:28.380 --> 10:29.900\n was actually self driving cars.\n\n10:29.980 --> 10:32.300\n And, you know, that was a big flip.\n\n10:32.500 --> 10:33.740\n What, uh, what grad school?\n\n10:34.140 --> 10:37.020\n So I went to grad school in Michigan, and then I did a postdoc at Stanford,\n\n10:37.020 --> 10:41.380\n uh, which is, that was the postdoc where I got to play with self driving cars.\n\n10:42.260 --> 10:42.540\n Yeah.\n\n10:42.540 --> 10:43.620\n So we'll return there.\n\n10:43.740 --> 10:46.020\n Let's go back to, uh, to Moscow.\n\n10:46.020 --> 10:50.260\n So, uh, you know, for episode 100, I talked to my dad and also I\n\n10:50.260 --> 10:51.780\n grew up with my dad, I guess.\n\n10:53.860 --> 11:01.020\n Uh, so I had to put up with them for many years and, uh, he, he went to the\n\n11:01.260 --> 11:06.260\n FISTIEG or MIPT, it's weird to say in English, cause I've heard all this\n\n11:06.260 --> 11:09.860\n in Russian, Moscow Institute of Physics and Technology.\n\n11:09.900 --> 11:15.500\n And to me, that was like, I met some super interesting, as a child, I met\n\n11:15.500 --> 11:17.060\n some super interesting characters.\n\n11:17.740 --> 11:21.460\n It felt to me like the greatest university in the world, the most elite\n\n11:21.460 --> 11:26.340\n university in the world, and just the people that I met that came out of there\n\n11:26.340 --> 11:32.300\n were like, not only brilliant, but also special humans.\n\n11:32.300 --> 11:37.900\n It seems like that place really tested the soul, uh, both like in terms\n\n11:37.900 --> 11:40.180\n of technically and like spiritually.\n\n11:40.620 --> 11:43.660\n So that could be just the romanticization of that place.\n\n11:43.660 --> 11:47.660\n I'm not sure, but so maybe you can speak to it, but is it correct to\n\n11:47.660 --> 11:49.980\n say that you spent some time at FISTIEG?\n\n11:50.060 --> 11:50.660\n Yeah, that's right.\n\n11:50.740 --> 11:51.380\n Six years.\n\n11:51.460 --> 11:54.780\n Uh, I got my bachelor's and master's in physics and math there.\n\n11:55.260 --> 11:59.220\n And it's actually interesting cause my, my dad, and actually both my parents,\n\n11:59.220 --> 12:03.740\n uh, went there and I think all the stories that I heard, uh, like, just\n\n12:03.740 --> 12:07.700\n like you, Alex, uh, growing up about the place and, you know, how interesting\n\n12:07.700 --> 12:11.820\n and special and magical it was, I think that was a significant, maybe the\n\n12:11.820 --> 12:16.420\n main reason, uh, I wanted to go there, uh, for college, uh, enough so that\n\n12:16.460 --> 12:21.300\n I actually went back to Russia from the U S I graduated high school in the U S.\n\n12:21.820 --> 12:23.740\n Um, and you went back there.\n\n12:23.780 --> 12:24.300\n I went back there.\n\n12:24.300 --> 12:28.220\n Yeah, that's exactly the reaction most of my peers in college had.\n\n12:28.220 --> 12:32.300\n But, you know, perhaps a little bit stronger that like, you know, point\n\n12:32.300 --> 12:34.780\n me out as this crazy kid, were your parents supportive of that?\n\n12:34.780 --> 12:35.280\n Yeah.\n\n12:35.540 --> 12:35.740\n Yeah.\n\n12:35.740 --> 12:38.780\n My games, your previous question, they, uh, they supported me and, you know,\n\n12:38.780 --> 12:43.380\n letting me kind of pursue my passions and the things that I was interested in.\n\n12:43.380 --> 12:44.140\n That's a bold move.\n\n12:44.140 --> 12:44.340\n Wow.\n\n12:44.380 --> 12:45.140\n What was it like there?\n\n12:45.340 --> 12:49.140\n It was interesting, you know, definitely fairly hardcore on the fundamentals\n\n12:49.220 --> 12:53.260\n of, you know, math and physics and, uh, you know, lots of good memories,\n\n12:53.300 --> 12:54.940\n uh, from, you know, from those times.\n\n12:55.460 --> 12:56.100\n So, okay.\n\n12:56.180 --> 12:57.140\n So Stanford.\n\n12:57.140 --> 12:58.700\n How'd you get into autonomous vehicles?\n\n12:59.180 --> 13:06.060\n I had the great fortune, uh, and great honor to join Stanford's\n\n13:06.060 --> 13:07.340\n DARPA urban challenge team.\n\n13:07.460 --> 13:12.300\n And, uh, 2006 there, this was a third in the sequence of the DARPA challenges.\n\n13:12.300 --> 13:14.940\n There were two grand challenges prior to that.\n\n13:14.940 --> 13:19.220\n And then in 2007, they held the DARPA urban challenge.\n\n13:19.380 --> 13:25.900\n So, you know, I was doing my, my postdoc I had, I joined the team and, uh, worked\n\n13:25.900 --> 13:29.940\n on motion planning, uh, for, you know, that, that competition.\n\n13:30.220 --> 13:30.700\n So, okay.\n\n13:30.700 --> 13:35.740\n So for people who might not know, I know from, from certain autonomous vehicles is\n\n13:35.740 --> 13:39.460\n a funny world in a certain circle of people, everybody knows everything.\n\n13:39.860 --> 13:45.020\n And then the certain circle, uh, nobody knows anything in terms of general public.\n\n13:45.020 --> 13:46.020\n So it's interesting.\n\n13:46.020 --> 13:50.420\n It's, it's a good question of what to talk about, but I do think that the urban\n\n13:50.420 --> 13:56.060\n challenge is worth revisiting. It's a fun little challenge.\n\n13:56.100 --> 14:03.020\n One that, first of all, like sparked so much, so many incredible minds to focus\n\n14:03.020 --> 14:06.740\n on one of the hardest problems of our time in artificial intelligence.\n\n14:06.740 --> 14:10.580\n So that's, that's a success from a perspective of a single little challenge.\n\n14:11.140 --> 14:14.100\n But can you talk about like, what did the challenge involve?\n\n14:14.220 --> 14:18.660\n So were there pedestrians, were there other cars, what was the goal?\n\n14:18.660 --> 14:20.220\n Uh, who was on the team?\n\n14:20.540 --> 14:24.540\n How long did it take any fun, fun sort of specs?\n\n14:25.460 --> 14:26.180\n Sure, sure, sure.\n\n14:26.220 --> 14:29.900\n So the way the challenge was constructed and just a little bit of backgrounding,\n\n14:29.900 --> 14:34.020\n as I mentioned, this was the third, uh, competition in that series.\n\n14:34.220 --> 14:36.940\n The first year we're at the grand challenge called the grand challenge.\n\n14:36.940 --> 14:40.220\n The goal there was to just drive in a completely static environment.\n\n14:40.260 --> 14:45.780\n You know, you had to drive in a desert, uh, that was very successful.\n\n14:45.780 --> 14:49.980\n So then DARPA followed with what they called the urban challenge, where the\n\n14:49.980 --> 14:54.780\n goal was to have, you know, build vehicles that could operate in more dynamic\n\n14:54.780 --> 14:56.780\n environments and, you know, share them with other vehicles.\n\n14:56.780 --> 15:00.420\n There were no pedestrians there, but what DARPA did is they took over\n\n15:00.460 --> 15:02.060\n an abandoned air force base.\n\n15:02.460 --> 15:06.060\n Uh, and it was kind of like a little fake city that they built out there.\n\n15:06.460 --> 15:11.500\n And they had a bunch of, uh, robots, uh, you know, cars, uh, that were\n\n15:11.500 --> 15:13.740\n autonomous, uh, in there all at the same time.\n\n15:13.740 --> 15:19.420\n Uh, mixed in with other vehicles driven by professional, uh, drivers and each\n\n15:19.420 --> 15:24.940\n car, uh, had a mission and so there's a crude map that they received, uh,\n\n15:24.980 --> 15:28.060\n beginning and they had a mission and go here and then there and over here.\n\n15:28.300 --> 15:32.980\n Um, and they kind of all were sharing this environment at the same time.\n\n15:32.980 --> 15:34.300\n They had to interact with each other.\n\n15:34.300 --> 15:35.820\n They had to interact with the human drivers.\n\n15:36.060 --> 15:42.340\n There's this very first, very rudimentary, um, version of, uh,\n\n15:42.340 --> 15:47.300\n self driving car that, you know, could operate, uh, and, uh, in a, in an\n\n15:47.300 --> 15:50.620\n environment, you know, shared with other dynamic actors that, as you said,\n\n15:50.860 --> 15:54.260\n you know, really, you know, many ways, you know, kickstarted this whole industry.\n\n15:55.220 --> 15:55.420\n Okay.\n\n15:55.420 --> 15:58.380\n So who was on the team and how'd you do?\n\n15:58.420 --> 15:58.780\n I forget.\n\n15:59.780 --> 16:01.980\n Uh, I came in second.\n\n16:02.460 --> 16:05.140\n Uh, perhaps that was my contribution to the team.\n\n16:05.140 --> 16:07.660\n I think the Stanford team came in first in the DARPA challenge.\n\n16:07.700 --> 16:10.300\n Uh, but then I joined the team and, you know, you were the one with the\n\n16:10.300 --> 16:13.860\n bug in the code, I mean, do you have sort of memories of some\n\n16:13.860 --> 16:18.900\n particularly challenging things or, you know, one of the cool things,\n\n16:18.900 --> 16:23.900\n it's not, you know, this isn't a product, this isn't the thing that, uh, you know,\n\n16:24.220 --> 16:27.220\n it there's, you have a little bit more freedom to experiment so you can take\n\n16:27.220 --> 16:30.140\n risks and there's, uh, so you can make mistakes.\n\n16:30.460 --> 16:32.540\n Uh, so is there interesting mistakes?\n\n16:33.100 --> 16:36.060\n Is there interesting challenges that stand out to you as some, like, taught\n\n16:36.060 --> 16:42.180\n you, um, a good technical lesson or a good philosophical lesson from that time?\n\n16:42.540 --> 16:42.860\n Yeah.\n\n16:43.020 --> 16:46.260\n Uh, you know, definitely, definitely a very memorable time, not really\n\n16:46.260 --> 16:51.740\n challenged, but like one of the most vivid memories that I have from the time.\n\n16:52.100 --> 16:58.660\n And I think that was actually one of the days that really got me hooked, uh, on\n\n16:58.660 --> 17:05.660\n this whole field was, uh, the first time I got to run my software and I got to\n\n17:05.660 --> 17:11.500\n software on the car and, uh, I was working on a part of our planning algorithm,\n\n17:11.580 --> 17:13.820\n uh, that had to navigate in parking lots.\n\n17:13.860 --> 17:16.580\n So it was something that, you know, called free space emotion planning.\n\n17:16.780 --> 17:20.940\n So the very first version of that, uh, was, you know, we tried on the car, it\n\n17:20.940 --> 17:24.420\n was on Stanford's campus, uh, in the middle of the night and you had this\n\n17:24.420 --> 17:28.380\n little course constructed with cones, uh, in the middle of a parking lot.\n\n17:28.380 --> 17:31.700\n So we're there in like 3 am, you know, by the time we got the code to, you\n\n17:31.700 --> 17:36.300\n know, uh, uh, you know, compile and turn over, uh, and, you know, it drove, I\n\n17:36.300 --> 17:39.980\n could actually did something quite reasonable and, you know, it was of\n\n17:39.980 --> 17:46.620\n course very buggy at the time and had all kinds of problems, but it was pretty\n\n17:46.700 --> 17:48.140\n darn magical.\n\n17:48.180 --> 17:52.300\n I remember going back and, you know, later at night and trying to fall\n\n17:52.300 --> 17:55.220\n asleep and just, you know, being unable to fall asleep for the rest of the\n\n17:55.220 --> 17:57.900\n night, uh, just my mind was blown.\n\n17:57.900 --> 18:02.340\n Just like, and that, that, that's what I've been doing ever since for more\n\n18:02.340 --> 18:06.460\n than a decade, uh, in terms of challenges and, uh, you know, interesting\n\n18:06.460 --> 18:09.780\n memories, like on the day of the competition, uh, it was pretty nerve\n\n18:09.780 --> 18:10.260\n wrecking.\n\n18:10.300 --> 18:13.780\n Uh, I remember standing there with Mike Montemarillo, who was, uh, the\n\n18:13.780 --> 18:15.740\n software lead and wrote most of the code.\n\n18:15.780 --> 18:19.060\n I think I did one little part of the planner, Mike, you know, incredibly\n\n18:19.420 --> 18:22.820\n that, you know, pretty much the rest of it, uh, with, with, you know, a bunch\n\n18:22.820 --> 18:25.660\n of other incredible people, but I remember standing on the day of the\n\n18:25.660 --> 18:29.860\n competition, uh, you know, watching the car, you know, with Mike and cars\n\n18:29.860 --> 18:32.180\n are completely empty, right?\n\n18:32.180 --> 18:35.300\n They're all there lined up in the beginning of the race and then, you\n\n18:35.300 --> 18:38.340\n know, DARPA sends them, you know, on their mission one by one.\n\n18:38.500 --> 18:42.180\n So then leave and Mike, you just, they had these sirens, they all had\n\n18:42.180 --> 18:43.580\n their different silence silence, right?\n\n18:43.580 --> 18:46.100\n Each siren had its own personality, if you will.\n\n18:46.260 --> 18:48.460\n So, you know, off they go and you don't see them.\n\n18:48.460 --> 18:50.740\n You just kind of, and then every once in a while they come a little bit\n\n18:50.740 --> 18:55.060\n closer to where the audience is and you can kind of hear, you know, the\n\n18:55.060 --> 18:57.380\n sound of your car and then, you know, it seems to be moving along.\n\n18:57.380 --> 18:58.420\n So that, you know, gives you hope.\n\n18:58.700 --> 19:01.500\n And then, you know, it goes away and you can't hear it for too long.\n\n19:01.500 --> 19:02.420\n You start getting anxious, right?\n\n19:02.420 --> 19:04.140\n So it's a little bit like, you know, sending your kids to college and like,\n\n19:04.140 --> 19:05.500\n you know, kind of you invested in them.\n\n19:05.700 --> 19:09.060\n You hope you, you, you, you, you, you, you build it properly, but like,\n\n19:09.100 --> 19:11.180\n it's still, uh, anxiety inducing.\n\n19:11.700 --> 19:16.860\n Uh, so that was, uh, an incredibly, uh, fun, uh, few days in terms of, you\n\n19:16.860 --> 19:20.740\n know, bugs, as you mentioned, you know, one that that was my bug that caused\n\n19:20.740 --> 19:24.540\n us the loss of the first place, uh, is still a debate that, you know,\n\n19:24.540 --> 19:27.820\n occasionally have with people on the CMU team, CMU came first, I should\n\n19:27.820 --> 19:32.380\n mention, uh, that you haven't heard of them, but yeah, it's something, you\n\n19:32.380 --> 19:35.340\n know, it's a small school, but it's, it's, it's, you know, really a glitch\n\n19:35.340 --> 19:38.140\n that, you know, they happen to succeed at something robotics related.\n\n19:38.140 --> 19:39.060\n Very scenic though.\n\n19:39.060 --> 19:41.340\n So most people go there for the scenery.\n\n19:41.460 --> 19:43.780\n Um, yeah, it's a beautiful campus.\n\n19:45.340 --> 19:46.580\n I'm like, unlike Stanford.\n\n19:46.780 --> 19:48.420\n So for people, yeah, that's true.\n\n19:48.420 --> 19:51.540\n Unlike Stanford, for people who don't know, CMU is one of the great robotics\n\n19:51.540 --> 19:55.300\n and sort of artificial intelligence universities in the world, CMU, Carnegie\n\n19:55.300 --> 19:57.660\n Mellon university, okay, sorry, go ahead.\n\n19:58.380 --> 19:59.180\n Good, good PSA.\n\n19:59.420 --> 20:06.380\n So in the part that I contributed to, which was navigating parking lots and\n\n20:06.380 --> 20:12.180\n the way that part of the mission work is, uh, you in a parking lot, you\n\n20:12.180 --> 20:15.700\n would get from DARPA an outline of the map.\n\n20:15.700 --> 20:18.540\n You basically get this, you know, giant polygon that defined the\n\n20:18.540 --> 20:21.700\n perimeter of the parking lot, uh, and there would be an entrance and, you\n\n20:21.700 --> 20:25.300\n know, so maybe multiple entrances or access to it, and then you would get a\n\n20:25.300 --> 20:32.180\n goal, uh, within that open space, uh, X, Y, you know, heading where the car had\n\n20:32.180 --> 20:36.380\n to park and had no information about the optical, so obstacles that the car might\n\n20:36.380 --> 20:36.860\n encounter there.\n\n20:36.860 --> 20:40.740\n So it had to navigate a kind of completely free space, uh, from the\n\n20:40.740 --> 20:43.740\n entrance to the parking lot into that parking space.\n\n20:43.740 --> 20:50.100\n And then, uh, once parked there, it had to, uh, exit the parking lot, you know,\n\n20:50.100 --> 20:53.020\n while of course, I'm counting and reasoning about all the obstacles that\n\n20:53.060 --> 20:54.620\n it encounters in real time.\n\n20:54.860 --> 21:00.940\n So, uh, Our interpretation, or at least my interpretation of the rules was that\n\n21:00.940 --> 21:03.220\n you had to reverse out of the parking spot.\n\n21:03.420 --> 21:04.860\n And that's what our cars did.\n\n21:04.900 --> 21:08.540\n Even if there's no obstacle in front, that's not what CMU's car did.\n\n21:08.620 --> 21:10.580\n And it just kind of drove right through.\n\n21:10.620 --> 21:12.260\n So there's still a debate.\n\n21:12.260 --> 21:14.860\n And of course, you know, as you stop and then reverse out and go out the\n\n21:14.860 --> 21:16.460\n different way that costs you some time.\n\n21:16.580 --> 21:20.260\n And so there's still a debate whether, you know, it was my poor implementation\n\n21:20.300 --> 21:26.100\n that cost us extra time or whether it was, you know, CMU, uh, violating an\n\n21:26.100 --> 21:27.380\n important rule of the competition.\n\n21:27.380 --> 21:30.700\n And, you know, I have my own, uh, opinion here in terms of other bugs.\n\n21:30.700 --> 21:34.380\n And like, uh, I, I have to apologize to Mike Montemarila, uh, for sharing this\n\n21:34.380 --> 21:38.180\n on air, but it is actually, uh, one of the more memorable ones.\n\n21:38.180 --> 21:42.940\n Uh, and it's something that's kind of become a bit of, uh, a metaphor and\n\n21:42.940 --> 21:46.100\n a label in the industry, uh, since then, I think, you know, at least in some\n\n21:46.100 --> 21:48.820\n circles, it's called the victory circle or victory lap.\n\n21:49.020 --> 21:52.860\n Um, and, uh, uh, our cars did that.\n\n21:53.060 --> 21:57.540\n So in one of the missions in the urban challenge, in one of the courses, uh,\n\n21:57.580 --> 22:02.020\n there was this big oval, right by the start and finish of the race.\n\n22:02.020 --> 22:05.620\n So the ARPA had a lot of the missions would finish kind of in that same location.\n\n22:05.620 --> 22:08.620\n Uh, and it was pretty cool because you could see the cars come by, you know,\n\n22:08.620 --> 22:11.780\n kind of finished that part leg of the trip, that leg of the mission, and then,\n\n22:11.780 --> 22:14.860\n you know, go on and finish the rest of it.\n\n22:15.220 --> 22:22.260\n Uh, and other vehicles would, you know, come hit their waypoint, uh, and, you\n\n22:22.260 --> 22:24.100\n know, exit the oval and off they would go.\n\n22:24.340 --> 22:28.060\n Our car on the hand, which hit the checkpoint, and then it would do an extra\n\n22:28.060 --> 22:31.620\n lap around the oval and only then, you know, uh, leave and go on its merry way.\n\n22:31.620 --> 22:34.620\n So over the course of the full day, it accumulated, uh, uh,\n\n22:34.620 --> 22:38.100\n some extra time and the problem was that we had a bug where it wouldn't, you know,\n\n22:38.100 --> 22:41.380\n start reasoning about the next waypoint and plan a route to get to that next\n\n22:41.380 --> 22:42.660\n point until it hit a previous one.\n\n22:42.820 --> 22:46.180\n And in that particular case, by the time you hit the, that, that one, it was too\n\n22:46.180 --> 22:49.140\n late for us to consider the next one and kind of make a lane change.\n\n22:49.140 --> 22:50.900\n So at every time we would do like an extra lap.\n\n22:50.940 --> 22:54.980\n So, you know, and that's the Stanford victory lap.\n\n22:55.060 --> 22:56.300\n The victory lap.\n\n22:57.100 --> 22:59.580\n Oh, that's there's, I feel like there's something philosophically\n\n22:59.580 --> 23:03.620\n profound in there somehow, but, uh, I mean, ultimately everybody is\n\n23:03.620 --> 23:05.180\n a winner in that kind of competition.\n\n23:06.140 --> 23:13.100\n And it led to sort of famously to the creation of, um, Google self driving\n\n23:13.100 --> 23:15.180\n car project and now Waymo.\n\n23:15.740 --> 23:19.900\n So can we, uh, give an overview of how is Waymo born?\n\n23:20.340 --> 23:22.860\n How's the Google self driving car project born?\n\n23:23.180 --> 23:24.620\n What's the, what is the mission?\n\n23:24.780 --> 23:25.700\n What is the hope?\n\n23:26.300 --> 23:32.460\n What is it is the engineering kind of, uh, set of milestones that\n\n23:32.460 --> 23:35.700\n it seeks to accomplish, there's a lot of questions in there.\n\n23:35.780 --> 23:40.060\n Uh, yeah, uh, I don't know, kind of the DARPA urban challenge and the DARPA\n\n23:40.060 --> 23:44.380\n and previous DARPA grand challenges, uh, kind of led, I think to a very large\n\n23:44.420 --> 23:48.100\n degree to that next step and then, you know, Larry and Sergey, um, uh, Larry\n\n23:48.100 --> 23:52.180\n Page and Sergey Brin, uh, uh, Google founders course, uh, I saw that\n\n23:52.180 --> 23:54.820\n competition and believed in the technology.\n\n23:54.940 --> 23:59.900\n So, you know, the Google self driving car project was born, you know, at that time.\n\n23:59.900 --> 24:04.300\n And we started in 2009, it was a pretty small group of us, about a dozen people,\n\n24:04.820 --> 24:09.300\n uh, who came together, uh, to, to work on this project at Google.\n\n24:09.620 --> 24:18.140\n At that time we saw an incredible early result in the DARPA urban challenge.\n\n24:18.140 --> 24:23.980\n I think we're all incredibly excited, uh, about where we got to and we believed\n\n24:23.980 --> 24:27.500\n in the future of the technology, but we still had a very, you know,\n\n24:27.500 --> 24:30.940\n very, you know, rudimentary understanding of the problem space.\n\n24:31.660 --> 24:37.620\n So the first goal of this project in 2009 was to really better\n\n24:37.660 --> 24:39.260\n understand what we're up against.\n\n24:39.620 --> 24:44.340\n Uh, and, you know, with that goal in mind, when we started the project, we created a\n\n24:44.340 --> 24:46.860\n few milestones for ourselves, uh, that.\n\n24:48.300 --> 24:49.460\n Maximized learnings.\n\n24:49.700 --> 24:54.300\n Well, the two milestones were, you know, uh, one was to drive a hundred thousand\n\n24:54.300 --> 24:57.940\n miles in autonomous mode, which was at that time, you know, orders of magnitude\n\n24:57.940 --> 25:00.620\n that, uh, more than anybody has ever done.\n\n25:01.100 --> 25:07.060\n And the second milestone was to drive 10 routes, uh, each one was a hundred miles\n\n25:07.060 --> 25:12.700\n long, uh, and there were specifically chosen to become extra spicy and extra\n\n25:12.700 --> 25:18.460\n complicated and sample the full complexity of the, that, that, uh, domain.\n\n25:18.460 --> 25:24.100\n Um, uh, and you had to drive each one from beginning to end with no intervention,\n\n25:24.140 --> 25:24.900\n no human intervention.\n\n25:24.900 --> 25:28.420\n So you would get to the beginning of the course, uh, you would press the button\n\n25:28.460 --> 25:32.900\n that would engage in autonomy and you had to go for a hundred miles, you know,\n\n25:32.900 --> 25:35.140\n beginning to end, uh, with no interventions.\n\n25:35.220 --> 25:40.460\n Um, and it sampled again, the full complexity of driving conditions.\n\n25:40.460 --> 25:42.940\n Some, uh, were on freeways.\n\n25:42.940 --> 25:45.180\n We had one route that went all through all the freeways and all\n\n25:45.180 --> 25:46.820\n the bridges in the Bay area.\n\n25:46.820 --> 25:50.500\n You know, we had, uh, some that went around Lake Tahoe and kind of mountains,\n\n25:50.540 --> 25:51.700\n uh, roads.\n\n25:52.060 --> 25:56.900\n We had some that drove through dense urban, um, environments like in downtown\n\n25:56.900 --> 25:59.180\n Palo Alto and through San Francisco.\n\n25:59.460 --> 26:04.820\n So it was incredibly, uh, interesting, uh, to work on.\n\n26:04.900 --> 26:10.940\n And it, uh, it took us just under two years, uh, about a year and a half,\n\n26:10.940 --> 26:14.180\n a little bit more to finish both of these milestones.\n\n26:14.180 --> 26:20.100\n And in that process, uh, you know, it was an incredible amount of fun,\n\n26:20.100 --> 26:22.740\n probably the most fun I had in my professional career.\n\n26:22.780 --> 26:24.700\n And you're just learning so much.\n\n26:24.700 --> 26:26.820\n You are, you know, the goal here is to learn and prototype.\n\n26:26.820 --> 26:29.180\n You're not yet starting to build a production system, right?\n\n26:29.180 --> 26:33.380\n So you just, you were, you know, this is when you're kind of working 24 seven\n\n26:33.380 --> 26:34.700\n and you're hacking things together.\n\n26:34.740 --> 26:37.580\n And you also don't know how hard this is.\n\n26:37.620 --> 26:38.500\n I mean, that's the point.\n\n26:38.820 --> 26:42.780\n Like, so, I mean, that's an ambitious, if I put myself in that mindset, even\n\n26:42.780 --> 26:45.940\n still, that's a really ambitious set of goals.\n\n26:46.660 --> 26:56.260\n Like just those two picking, picking 10 different, difficult, spicy challenges.\n\n26:56.580 --> 26:58.860\n And then having zero interventions.\n\n26:59.460 --> 27:05.940\n So like not saying gradually we're going to like, you know, over a period of 10\n\n27:05.940 --> 27:09.580\n years, we're going to have a bunch of routes and gradually reduce the number\n\n27:09.580 --> 27:13.980\n of interventions, you know, that literally says like, by as soon as\n\n27:13.980 --> 27:17.580\n possible, we want to have zero and on hard roads.\n\n27:17.980 --> 27:22.620\n So like, to me, if I was facing that, it's unclear that whether that takes\n\n27:23.180 --> 27:25.220\n two years or whether that takes 20 years.\n\n27:26.420 --> 27:27.780\n I mean, it took us under two.\n\n27:27.820 --> 27:32.940\n I guess that that speaks to a really big difference between doing something\n\n27:32.980 --> 27:37.820\n once and having a prototype where you are going after, you know, learning\n\n27:37.820 --> 27:42.780\n about the problem versus how you go about engineering a product that, you\n\n27:42.780 --> 27:47.180\n know, where you look at, you know, you do properly do evaluation, you look\n\n27:47.180 --> 27:49.980\n at metrics, you drive down and you're confident that you can do that.\n\n27:50.380 --> 27:55.820\n And I guess that's the, you know, why it took a dozen people, you know, 16\n\n27:55.820 --> 28:00.780\n months or a little bit more than that back in 2009 and 2010 with the\n\n28:00.780 --> 28:05.420\n technology of, you know, the more than a decade ago that amount of time to\n\n28:05.420 --> 28:10.220\n achieve that milestone of, you know, 10 routes, a hundred miles each and no\n\n28:10.220 --> 28:17.340\n interventions, and, you know, it took us a little bit longer to get to, you\n\n28:17.340 --> 28:19.980\n know, a full driverless product that customers use.\n\n28:20.380 --> 28:21.740\n That's another really important moment.\n\n28:21.740 --> 28:29.580\n Is there some memories of technical lessons or just one, like, what did you\n\n28:29.580 --> 28:32.220\n learn about the problem of driving from that experience?\n\n28:32.220 --> 28:36.540\n I mean, we can, we can now talk about like what you learned from modern day\n\n28:36.540 --> 28:41.340\n Waymo, but I feel like you may have learned some profound things in those\n\n28:41.420 --> 28:47.580\n early days, even more so because it feels like what Waymo is now is to trying\n\n28:47.580 --> 28:51.020\n to, you know, how to do scale, how to make sure you create a product, how to\n\n28:51.020 --> 28:54.140\n make sure it's like safety and all those things, which is all fascinating\n\n28:54.140 --> 28:59.500\n challenges, but like you were facing the more fundamental philosophical\n\n28:59.500 --> 29:02.540\n problem of driving in those early days.\n\n29:02.540 --> 29:07.820\n Like what the hell is driving as an autonomous, or maybe I'm again\n\n29:07.820 --> 29:14.540\n romanticizing it, but is it, is there, is there some valuable lessons you\n\n29:14.540 --> 29:16.620\n picked up over there at those two years?\n\n29:18.060 --> 29:18.620\n A ton.\n\n29:19.020 --> 29:25.500\n The most important one is probably that we believe that it's doable and we've\n\n29:25.500 --> 29:31.500\n gotten far enough into the problem that, you know, we had a, I think only a\n\n29:31.500 --> 29:37.020\n glimpse of the true complexity of the, that the domain, you know, it's a\n\n29:37.020 --> 29:39.260\n little bit like, you know, climbing a mountain where you kind of, you know,\n\n29:39.260 --> 29:42.140\n see the next peak and you think that's kind of the summit, but then you get\n\n29:42.140 --> 29:45.260\n to that and you kind of see that, that this is just the start of the journey.\n\n29:46.140 --> 29:50.620\n But we've tried, we've sampled enough of the problem space and we've made\n\n29:50.620 --> 29:56.540\n enough rapid success, even, you know, with technology of 2009, 2010, that\n\n29:57.020 --> 30:02.220\n it gave us confidence to then, you know, pursue this as a real product.\n\n30:02.940 --> 30:04.060\n So, okay.\n\n30:04.140 --> 30:09.260\n So the next step, you mentioned the milestones that you had in the, in those\n\n30:09.260 --> 30:13.500\n two years, what are the next milestones that then led to the creation of Waymo\n\n30:13.500 --> 30:14.060\n and beyond?\n\n30:14.780 --> 30:18.140\n Yeah, we had a, it was a really interesting journey and, you know, Waymo\n\n30:18.140 --> 30:24.780\n came a little bit later, then, you know, we completed those milestones in 2010.\n\n30:25.020 --> 30:30.300\n That was the pivot when we decided to focus on actually building a product\n\n30:30.300 --> 30:31.420\n using this technology.\n\n30:32.460 --> 30:37.660\n The initial couple of years after that, we were focused on a freeway, you\n\n30:37.660 --> 30:41.180\n know, what you would call a driver assist, maybe, you know, an L3 driver\n\n30:41.180 --> 30:42.780\n assist program.\n\n30:42.780 --> 30:49.500\n Then around 2013, we've learned enough about the space and thought more deeply\n\n30:49.500 --> 30:54.940\n about, you know, the product that we wanted to build, that we pivoted, we\n\n30:54.940 --> 31:01.900\n pivoted towards this vision of building a driver and deploying it fully driverless\n\n31:01.900 --> 31:02.940\n vehicles without a person.\n\n31:02.940 --> 31:05.100\n And that that's the path that we've been on since then.\n\n31:05.100 --> 31:08.540\n And very, it was exactly the right decision for us.\n\n31:08.540 --> 31:13.580\n So there was a moment where you're also considered like, what is the right\n\n31:13.580 --> 31:14.620\n trajectory here?\n\n31:14.780 --> 31:18.140\n What is the right role of automation in the, in the task of driving?\n\n31:18.140 --> 31:23.180\n There's still, it wasn't from the early days, obviously you want to go fully\n\n31:23.180 --> 31:23.740\n autonomous.\n\n31:24.060 --> 31:25.020\n From the early days, it was not.\n\n31:25.100 --> 31:31.740\n I think it was in 20, around 2013, maybe that we've, that became very clear and\n\n31:31.740 --> 31:36.860\n we made that pivot and also became very clear and that it's either the way you\n\n31:36.860 --> 31:41.500\n go building a driver assist system is, you know, fundamentally different from\n\n31:41.500 --> 31:43.500\n how you go building a fully driverless vehicle.\n\n31:43.500 --> 31:48.700\n So, you know, we've pivoted towards the ladder and that's what we've been\n\n31:48.700 --> 31:49.820\n working on ever since.\n\n31:50.620 --> 31:57.900\n And so that was around 2013, then there's sequence of really meaningful for us\n\n31:57.900 --> 32:00.540\n really important defining milestones since then.\n\n32:00.540 --> 32:11.740\n And in 2015, we had our first, actually the world's first fully driverless\n\n32:12.220 --> 32:14.780\n trade on public roads.\n\n32:15.020 --> 32:17.500\n It was in a custom built vehicle that we had.\n\n32:17.500 --> 32:18.380\n I must've seen those.\n\n32:18.380 --> 32:21.100\n We called them the Firefly, that, you know, funny looking marshmallow looking\n\n32:21.100 --> 32:21.340\n thing.\n\n32:22.700 --> 32:30.060\n And we put a passenger, his name was Steve Mann, you know, great friend of\n\n32:30.060 --> 32:34.300\n our project from the early days, the man happens to be blind.\n\n32:34.540 --> 32:35.900\n So we put them in that vehicle.\n\n32:36.060 --> 32:38.140\n The car had no steering wheel, no pedals.\n\n32:38.140 --> 32:39.580\n It was an uncontrolled environment.\n\n32:40.460 --> 32:44.060\n You know, no, you know, lead or chase cars, no police escorts.\n\n32:44.540 --> 32:47.740\n And, you know, we did that trip a few times in Austin, Texas.\n\n32:47.900 --> 32:49.500\n So that was a really big milestone.\n\n32:49.500 --> 32:50.460\n But that was in Austin.\n\n32:50.620 --> 32:50.860\n Yeah.\n\n32:51.180 --> 32:51.500\n Okay.\n\n32:52.860 --> 32:56.620\n And, you know, we only, but at that time we're only, it took a tremendous\n\n32:56.620 --> 32:57.340\n amount of engineering.\n\n32:57.340 --> 33:00.380\n It took a tremendous amount of validation to get to that point.\n\n33:01.020 --> 33:03.580\n But, you know, we only did it a few times.\n\n33:03.820 --> 33:04.540\n We only did that.\n\n33:04.540 --> 33:05.340\n It was a fixed route.\n\n33:05.500 --> 33:08.060\n It was not kind of a controlled environment, but it was a fixed route.\n\n33:08.060 --> 33:09.180\n And we only did a few times.\n\n33:10.220 --> 33:19.820\n Then in 2016, end of 2016, beginning of 2017 is when we founded Waymo, the\n\n33:19.820 --> 33:20.220\n company.\n\n33:20.220 --> 33:25.100\n That's when we kind of, that was the next phase of the project where I\n\n33:25.100 --> 33:30.220\n wanted, we believed in kind of the commercial vision of this technology.\n\n33:30.460 --> 33:33.420\n And it made sense to create an independent entity, you know, within\n\n33:33.420 --> 33:38.300\n that alphabet umbrella to pursue this product at scale.\n\n33:39.420 --> 33:46.540\n Beyond that in 2017, later in 2017 was another really huge step for us.\n\n33:46.540 --> 33:52.460\n Really big milestone where we started, I think it was October of 2017 where\n\n33:52.460 --> 33:59.340\n when we started regular driverless operations on public roads, that first\n\n33:59.340 --> 34:02.780\n day of operations, we drove in one day.\n\n34:02.780 --> 34:05.980\n And that first day, a hundred miles and driverless fashion.\n\n34:05.980 --> 34:08.460\n And then we've now the most, the most important thing about that milestone\n\n34:08.460 --> 34:11.500\n was not that, you know, a hundred miles in one day, but that it was the\n\n34:11.500 --> 34:14.940\n start of kind of regular ongoing driverless operations.\n\n34:14.940 --> 34:18.140\n And when you say driverless, it means no driver.\n\n34:19.100 --> 34:19.740\n That's exactly right.\n\n34:19.740 --> 34:24.780\n So on that first day, we actually hit a mix and in some, we didn't want\n\n34:24.780 --> 34:27.100\n to like, you know, be on YouTube and Twitter that same day.\n\n34:27.100 --> 34:32.460\n So in, in many of the rides we had somebody in the driver's seat, but\n\n34:32.460 --> 34:36.860\n they could not disengage like the car, not disengage, but actually on that\n\n34:36.860 --> 34:42.540\n first day, some of the miles were driven and just completely empty driver's seat.\n\n34:42.540 --> 34:46.780\n And this is the key distinction that I think people don't realize it's, you\n\n34:46.780 --> 34:53.020\n know, that oftentimes when you talk about autonomous vehicles, you're, there's\n\n34:53.020 --> 34:59.420\n often a driver in the seat that's ready to to take over what's called a safety\n\n34:59.420 --> 35:05.420\n driver and then Waymo is really one of the only companies at least that I'm\n\n35:05.420 --> 35:10.860\n aware of, or at least as like boldly and carefully and all, and all of that is\n\n35:10.940 --> 35:12.540\n actually has cases.\n\n35:12.540 --> 35:16.780\n And now we'll talk about more and more where there's literally no driver.\n\n35:17.100 --> 35:21.500\n So that's another, the interesting case of where the driver's not supposed\n\n35:21.500 --> 35:24.700\n to disengage, that's like a nice middle ground, they're still there, but\n\n35:24.700 --> 35:28.380\n they're not supposed to disengage, but really there's the case when there's\n\n35:28.380 --> 35:34.540\n no, okay, there's something magical about there being nobody in the driver's seat.\n\n35:34.540 --> 35:41.260\n Like, just like to me, you mentioned the first time you wrote some code for free\n\n35:41.260 --> 35:46.700\n space navigation of the parking lot, that was like a magical moment to me, just\n\n35:46.700 --> 35:53.900\n sort of as an observer of robots, the first magical moment is seeing an\n\n35:53.900 --> 36:01.660\n autonomous vehicle turn, like make a left turn, like apply sufficient torque to\n\n36:01.660 --> 36:05.740\n the steering wheel to where it, like, there's a lot of rotation and for some\n\n36:05.740 --> 36:09.660\n reason, and there's nobody in the driver's seat, for some reason that\n\n36:10.300 --> 36:16.060\n communicates that here's a being with power that makes a decision.\n\n36:16.060 --> 36:19.660\n There's something about like the steering wheel, cause we perhaps romanticize\n\n36:19.660 --> 36:24.300\n the notion of the steering wheel, it's so essential to our conception, our 20th\n\n36:24.300 --> 36:28.380\n century conception of a car and it turning the steering wheel with nobody\n\n36:28.380 --> 36:34.460\n in driver's seat, that to me, I think maybe to others, it's really powerful.\n\n36:34.460 --> 36:39.100\n Like this thing is in control and then there's this leap of trust that you give.\n\n36:39.100 --> 36:42.620\n Like I'm going to put my life in the hands of this thing that's in control.\n\n36:42.620 --> 36:47.420\n So in that sense, when there's no, but no driver in the driver's seat, that's a\n\n36:47.420 --> 36:49.260\n magical moment for robots.\n\n36:49.820 --> 36:54.700\n So I'm, I've gotten a chance to last year to take a ride in a, in a\n\n36:54.700 --> 36:58.700\n way more vehicle and that, that was the magical moment. There's like nobody in\n\n36:58.700 --> 37:03.180\n the driver's seat. It's, it's like the little details. You would think it\n\n37:03.180 --> 37:07.500\n doesn't matter whether there's a driver or not, but like if there's no driver\n\n37:07.500 --> 37:12.380\n and the steering wheel is turning on its own, I don't know. That's magical.\n\n37:13.260 --> 37:17.260\n It's absolutely magical. I, I have taken many of these rides and like completely\n\n37:17.260 --> 37:22.220\n empty car, no human in the car pulls up, you know, you call it on your cell phone.\n\n37:22.220 --> 37:27.740\n It pulls up, you get in, it takes you on its way. There's nobody in the car, but\n\n37:27.740 --> 37:31.340\n you, right? That's something called, you know, fully driverless, you know, our\n\n37:31.980 --> 37:39.900\n writer only mode of operation. Yeah. It, it is magical. It is, you know,\n\n37:39.900 --> 37:44.780\n transformative. This is what we hear from our writers. It kind of really\n\n37:44.780 --> 37:48.780\n changes your experience. And not like that, that really is what unlocks the\n\n37:48.780 --> 37:53.580\n real potential of this technology. But, you know, coming back to our journey,\n\n37:53.580 --> 37:58.780\n you know, that was 2017 when we started, you know, truly driverless operations.\n\n37:58.780 --> 38:05.740\n Then in 2018, we've launched our public commercial service that we called\n\n38:05.740 --> 38:13.820\n Waymo One in Phoenix. In 2019, we started offering truly driverless writer\n\n38:13.820 --> 38:22.940\n only rides to our early rider population of users. And then, you know, 2020 has\n\n38:22.940 --> 38:26.700\n also been a pretty interesting year. One of the first ones, less about\n\n38:26.700 --> 38:31.180\n technology, but more about the maturing and the growth of Waymo as a company.\n\n38:31.980 --> 38:37.500\n We raised our first round of external financing this year, you know, we were\n\n38:37.500 --> 38:42.060\n part of Alphabet. So obviously we have access to, you know, significant resources\n\n38:42.060 --> 38:45.900\n but as kind of on the journey of Waymo maturing as a company, it made sense\n\n38:45.900 --> 38:50.620\n for us to, you know, partially go externally in this round. So, you know,\n\n38:50.620 --> 38:59.740\n we're raised about $3.2 billion from that round. We've also started putting\n\n38:59.740 --> 39:05.420\n our fifth generation of our driver, our hardware, that is on the new vehicle,\n\n39:05.420 --> 39:10.380\n but it's also a qualitatively different set of self driving hardware.\n\n39:10.380 --> 39:18.620\n That is now on the JLR pace. So that was a very important step for us.\n\n39:19.340 --> 39:25.580\n Hardware specs, fifth generation. I think it'd be fun to maybe, I apologize if\n\n39:25.580 --> 39:31.980\n I'm interrupting, but maybe talk about maybe the generations with a focus on\n\n39:31.980 --> 39:35.660\n what we're talking about on the fifth generation in terms of hardware specs,\n\n39:35.660 --> 39:36.700\n like what's on this car.\n\n39:36.700 --> 39:41.580\n Sure. So we separated out, you know, the actual car that we are driving from\n\n39:41.580 --> 39:45.820\n the self driving hardware we put on it. Right now we have, so this is, as I\n\n39:45.820 --> 39:49.980\n mentioned, the fifth generation, you know, we've gone through, we started,\n\n39:49.980 --> 39:54.860\n you know, building our own hardware, you know, many, many years ago. And\n\n39:56.060 --> 40:01.020\n that, you know, Firefly vehicle also had the hardware suite that was mostly\n\n40:01.020 --> 40:07.020\n designed, engineered, and built in house. Lighters are one of the more important\n\n40:07.580 --> 40:11.820\n components that we design and build from the ground up. So on the fifth\n\n40:11.820 --> 40:18.700\n generation of our drivers of our self driving hardware that we're switching\n\n40:18.700 --> 40:24.220\n to right now, we have, as with previous generations, in terms of sensing,\n\n40:24.220 --> 40:29.580\n we have lighters, cameras, and radars, and we have a pretty beefy computer\n\n40:29.580 --> 40:33.420\n that processes all that information and makes decisions in real time on\n\n40:33.420 --> 40:41.180\n board the car. So in all of the, and it's really a qualitative jump forward\n\n40:41.180 --> 40:45.660\n in terms of the capabilities and the various parameters and the specs of\n\n40:45.660 --> 40:49.260\n the hardware compared to what we had before and compared to what you can\n\n40:49.260 --> 40:51.580\n kind of get off the shelf in the market today.\n\n40:51.580 --> 40:54.700\n Meaning from fifth to fourth or from fifth to first?\n\n40:54.700 --> 40:57.340\n Definitely from first to fifth, but also from the fourth.\n\n40:57.340 --> 40:58.700\n That was the world's dumbest question.\n\n40:58.700 --> 41:07.500\n Definitely from fourth to fifth, as well as the last step is a big step forward.\n\n41:07.500 --> 41:13.900\n So everything's in house. So like LIDAR is built in house and cameras are\n\n41:13.900 --> 41:14.540\n built in house?\n\n41:15.740 --> 41:18.780\n You know, it's different. We work with partners and there's some components\n\n41:19.340 --> 41:26.780\n that we get from our manufacturing and supply chain partners. What exactly\n\n41:26.780 --> 41:34.140\n is in house is a bit different. We do a lot of custom design on all of\n\n41:34.140 --> 41:37.580\n our sensing modalities, lighters, radars, cameras, you know, exactly.\n\n41:37.580 --> 41:43.180\n There's lighters are almost exclusively in house and some of the\n\n41:43.180 --> 41:45.980\n technologies that we have, some of the fundamental technologies there\n\n41:45.980 --> 41:51.420\n are completely unique to Waymo. That is also largely true about radars\n\n41:51.420 --> 41:55.180\n and cameras. It's a little bit more of a mix in terms of what we do\n\n41:55.180 --> 41:57.980\n ourselves versus what we get from partners.\n\n41:57.980 --> 42:01.580\n Is there something super sexy about the computer that you can mention\n\n42:01.580 --> 42:08.300\n that's not top secret? Like for people who enjoy computers for, I\n\n42:08.300 --> 42:12.700\n mean, there's a lot of machine learning involved, but there's a lot\n\n42:12.700 --> 42:17.260\n of just basic compute. You have to probably do a lot of signal\n\n42:17.260 --> 42:20.780\n processing on all the different sensors. You have to integrate everything\n\n42:20.780 --> 42:23.820\n has to be in real time. There's probably some kind of redundancy\n\n42:23.820 --> 42:27.420\n type of situation. Is there something interesting you can say about\n\n42:27.420 --> 42:31.820\n the computer for the people who love hardware? It does have all of\n\n42:31.820 --> 42:34.380\n the characteristics, all the properties that you just mentioned.\n\n42:34.380 --> 42:41.020\n Redundancy, very beefy compute for general processing, as well as\n\n42:41.020 --> 42:45.260\n inference and ML models. It is some of the more sensitive stuff that\n\n42:45.260 --> 42:49.420\n I don't want to get into for IP reasons, but it can be shared a\n\n42:49.420 --> 42:54.860\n little bit in terms of the specs of the sensors that we have on the\n\n42:54.860 --> 42:58.460\n car. We actually shared some videos of what our\n\n43:00.700 --> 43:05.100\n lighters see in the world. We have 29 cameras. We have five lighters.\n\n43:05.100 --> 43:09.260\n We have six radars on these vehicles, and you can get a feel for\n\n43:09.260 --> 43:12.380\n the amount of data that they're producing. That all has to be\n\n43:12.380 --> 43:16.860\n processed in real time to do perception, to do complex\n\n43:16.860 --> 43:19.740\n reasoning. That kind of gives you some idea of how beefy those computers\n\n43:19.740 --> 43:22.540\n are, but I don't want to get into specifics of exactly how we build\n\n43:22.540 --> 43:25.500\n them. Okay, well, let me try some more questions that you can get\n\n43:25.500 --> 43:28.780\n into the specifics of, like GPU wise. Is that something you can get\n\n43:28.780 --> 43:33.020\n into? I know that Google works with GPUs and so on. I mean, for\n\n43:33.020 --> 43:35.980\n machine learning folks, it's kind of interesting. Or is there no...\n\n43:38.780 --> 43:43.340\n How do I ask it? I've been talking to people in the government about\n\n43:43.340 --> 43:46.860\n UFOs and they don't answer any questions. So this is how I feel\n\n43:46.860 --> 43:51.980\n right now asking about GPUs. But is there something interesting that\n\n43:51.980 --> 43:57.500\n you could reveal? Or is it just... Or leave it up to our\n\n43:57.500 --> 44:02.060\n imagination, some of the compute. Is there any, I guess, is there any\n\n44:02.060 --> 44:05.820\n fun trickery? Like I talked to Chris Latner for a second time and he\n\n44:05.820 --> 44:09.580\n was a key person about GPUs, and there's a lot of fun stuff going\n\n44:09.580 --> 44:15.580\n on in Google in terms of hardware that optimizes for machine\n\n44:15.580 --> 44:19.420\n learning. Is there something you can reveal in terms of how much,\n\n44:19.420 --> 44:22.380\n you mentioned customization, how much customization there is for\n\n44:23.100 --> 44:26.220\n hardware for machine learning purposes? I'm going to be like that\n\n44:26.220 --> 44:34.540\n government person who bought UFOs. I guess I will say that it's\n\n44:34.540 --> 44:41.340\n really... Compute is really important. We have very data hungry\n\n44:41.340 --> 44:45.900\n and compute hungry ML models all over our stack. And this is where\n\n44:48.300 --> 44:52.060\n both being part of Alphabet, as well as designing our own sensors\n\n44:52.060 --> 44:55.820\n and the entire hardware suite together, where on one hand you\n\n44:55.820 --> 45:01.740\n get access to really rich raw sensor data that you can pipe\n\n45:01.740 --> 45:07.820\n from your sensors into your compute platform and build like\n\n45:07.820 --> 45:11.020\n build the whole pipe from sensor raw sensor data to the big\n\n45:11.020 --> 45:14.060\n compute as then have the massive compute to process all that\n\n45:14.060 --> 45:17.420\n data. And this is where we're finding that having a lot of\n\n45:17.420 --> 45:21.260\n control of that hardware part of the stack is really\n\n45:21.260 --> 45:24.780\n advantageous. One of the fascinating magical places to me\n\n45:25.340 --> 45:29.980\n again, might not be able to speak to the details, but it is\n\n45:29.980 --> 45:32.940\n the other compute, which is like, we're just talking about a\n\n45:32.940 --> 45:39.340\n single car, but the driving experience is a source of a lot\n\n45:39.340 --> 45:42.060\n of fascinating data. And you have a huge amount of data\n\n45:42.060 --> 45:47.820\n coming in on the car and the infrastructure of storing some\n\n45:47.820 --> 45:52.460\n of that data to then train or to analyze or so on. That's a\n\n45:52.460 --> 45:58.220\n fascinating piece of it that I understand a single car. I\n\n45:58.220 --> 46:00.940\n don't understand how you pull it all together in a nice way.\n\n46:00.940 --> 46:03.100\n Is that something that you could speak to in terms of the\n\n46:03.100 --> 46:08.460\n challenges of seeing the network of cars and then\n\n46:08.460 --> 46:12.620\n bringing the data back and analyzing things that like edge\n\n46:12.620 --> 46:15.340\n cases of driving, be able to learn on them to improve the\n\n46:15.340 --> 46:20.060\n system to see where things went wrong, where things went right\n\n46:20.060 --> 46:22.220\n and analyze all that kind of stuff. Is there something\n\n46:22.220 --> 46:25.340\n interesting there from an engineering perspective?\n\n46:25.340 --> 46:30.780\n Oh, there's an incredible amount of really interesting\n\n46:30.780 --> 46:35.100\n work that's happening there, both in the real time operation\n\n46:35.100 --> 46:38.220\n of the fleet of cars and the information that they exchange\n\n46:38.220 --> 46:42.300\n with each other in real time to make better decisions as well\n\n46:43.340 --> 46:46.700\n as on the kind of the off board component where you have to\n\n46:46.700 --> 46:50.380\n deal with massive amounts of data for training your ML\n\n46:50.380 --> 46:54.620\n models, evaluating the ML models for simulating the entire\n\n46:54.620 --> 46:57.980\n system and for evaluating your entire system. And this is\n\n46:57.980 --> 47:02.300\n where being part of Alphabet has once again been tremendously\n\n47:03.420 --> 47:06.460\n advantageous because we consume an incredible amount of\n\n47:06.460 --> 47:10.140\n compute for ML infrastructure. We build a lot of custom\n\n47:10.140 --> 47:15.740\n frameworks to get good at data mining, finding the\n\n47:16.460 --> 47:19.180\n interesting edge cases for training and for evaluation of\n\n47:19.180 --> 47:23.900\n the system for both training and evaluating some components\n\n47:23.900 --> 47:27.020\n and your sub parts of the system and various ML models,\n\n47:27.020 --> 47:30.300\n as well as the evaluating the entire system and simulation.\n\n47:31.020 --> 47:33.820\n Okay. That first piece that you mentioned that cars\n\n47:33.820 --> 47:36.700\n communicating to each other, essentially, I mean, through\n\n47:36.700 --> 47:40.060\n perhaps through a centralized point, but what that's\n\n47:40.060 --> 47:43.420\n fascinating too, how much does that help you? Like if you\n\n47:43.420 --> 47:46.940\n imagine, you know, right now the number of way more vehicles\n\n47:46.940 --> 47:50.460\n is whatever X. I don't know if you can talk to what that\n\n47:50.460 --> 47:54.380\n number is, but it's not in the hundreds of millions yet. And\n\n47:55.100 --> 47:59.340\n imagine if the whole world is way more vehicles, like that\n\n47:59.340 --> 48:03.660\n changes potentially the power of connectivity. Like the more\n\n48:03.660 --> 48:06.300\n cars you have, I guess, actually, if you look at\n\n48:06.300 --> 48:09.980\n Phoenix, cause there's enough vehicles, there's enough, when\n\n48:09.980 --> 48:13.100\n there's like some level of density, you can start to\n\n48:13.100 --> 48:15.900\n probably do some really interesting stuff with the fact\n\n48:15.900 --> 48:21.420\n that cars can negotiate, can be, can communicate with each\n\n48:21.420 --> 48:24.300\n other and thereby make decisions. Is there something\n\n48:24.300 --> 48:27.820\n interesting there that you can talk to about like, how does\n\n48:27.820 --> 48:31.100\n that help with the driving problem from, as compared to\n\n48:31.100 --> 48:34.220\n just a single car solving the driving problem by itself?\n\n48:35.660 --> 48:40.460\n Yeah, it's a spectrum. I first and say that, you know, it's,\n\n48:40.460 --> 48:44.140\n it helps and it helps in various ways, but it's not required\n\n48:44.140 --> 48:46.700\n right now with the way we build our system, like each cars can\n\n48:46.700 --> 48:49.180\n operate independently. They can operate with no connectivity.\n\n48:49.660 --> 48:53.180\n So I think it is important that, you know, you have a fully\n\n48:53.740 --> 48:59.580\n autonomous, fully capable driver that, you know, computerized\n\n48:59.580 --> 49:03.340\n driver that each car has. Then, you know, they do share\n\n49:03.340 --> 49:06.140\n information and they share information in real time. It\n\n49:06.140 --> 49:11.820\n really, really helps. So the way we do this today is, you know,\n\n49:11.820 --> 49:15.180\n whenever one car encounters something interesting in the\n\n49:15.180 --> 49:17.980\n world, whether it might be an accident or a new construction\n\n49:17.980 --> 49:21.420\n zone, that information immediately gets, you know,\n\n49:21.420 --> 49:23.900\n uploaded over the air and it's propagated to the rest of the\n\n49:23.900 --> 49:26.860\n fleet. So, and that's kind of how we think about maps as\n\n49:27.420 --> 49:32.940\n priors in terms of the knowledge of our drivers, of our fleet of\n\n49:32.940 --> 49:38.140\n drivers that is distributed across the fleet and it's\n\n49:38.140 --> 49:41.740\n updated in real time. So that's one use case. And\n\n49:41.740 --> 49:46.940\n you know, you can imagine as the, you know, the density of\n\n49:46.940 --> 49:50.060\n these vehicles go up, that they can exchange more information\n\n49:50.060 --> 49:53.260\n in terms of what they're planning to do and start\n\n49:53.820 --> 49:56.540\n influencing how they interact with each other, as well as,\n\n49:56.540 --> 49:59.660\n you know, potentially sharing some observations, right, to\n\n49:59.660 --> 50:01.820\n help with, you know, if you have enough density of these\n\n50:01.820 --> 50:04.060\n vehicles where, you know, one car might be seeing something\n\n50:04.060 --> 50:06.780\n that another is relevant to another car that is very\n\n50:06.780 --> 50:08.940\n dynamic. You know, it's not part of kind of your updating\n\n50:08.940 --> 50:11.500\n your static prior of the map of the world, but it's more of a\n\n50:11.500 --> 50:14.220\n dynamic information that could be relevant to the decisions\n\n50:14.220 --> 50:16.380\n that another car is making real time. So you can see them\n\n50:16.380 --> 50:18.860\n exchanging that information and you can build on that. But\n\n50:18.860 --> 50:23.660\n again, I see that as an advantage, but it's not a\n\n50:23.660 --> 50:28.460\n requirement. So what about the human in the loop? So when I\n\n50:28.460 --> 50:34.780\n got a chance to drive with a ride in a Waymo, you know,\n\n50:34.780 --> 50:39.740\n there's customer service. So like there is somebody that's\n\n50:39.740 --> 50:48.300\n able to dynamically like tune in and help you out. What role\n\n50:48.300 --> 50:51.500\n does the human play in that picture? That's a fascinating\n\n50:51.500 --> 50:53.980\n like, you know, the idea of teleoperation, be able to\n\n50:53.980 --> 50:57.180\n remotely control a vehicle. So here, what we're talking\n\n50:57.180 --> 51:03.900\n about is like, like frictionless, like a human being\n\n51:03.900 --> 51:08.460\n able to in a in a frictionless way, sort of help you out. I\n\n51:08.460 --> 51:10.780\n don't know if they're able to actually control the vehicle.\n\n51:10.780 --> 51:14.300\n Is that something you could talk to? Yes. Okay. To be clear,\n\n51:14.300 --> 51:16.300\n we don't do teleporation. I kind of believe in\n\n51:16.300 --> 51:19.100\n teleporation for various reasons. That's not what we\n\n51:19.100 --> 51:22.300\n have in our cars. We do, as you mentioned, have, you know,\n\n51:22.300 --> 51:24.780\n version of, you know, customer support. You know, we call it\n\n51:24.780 --> 51:28.860\n life health. In fact, we find it that it's very important for\n\n51:28.860 --> 51:32.300\n our ride experience, especially if it's your first trip, you've\n\n51:32.300 --> 51:35.020\n never been in a fully driverless ride or only way more\n\n51:35.020 --> 51:37.660\n vehicle you get in, there's nobody there. And so you can\n\n51:37.660 --> 51:40.460\n imagine having all kinds of, you know, questions in your head,\n\n51:40.460 --> 51:43.260\n like how this thing works. So we've put a lot of thought into\n\n51:43.260 --> 51:47.420\n kind of guiding our, our writers or customers through that\n\n51:47.420 --> 51:49.500\n experience, especially for the first time they get some\n\n51:49.500 --> 51:54.380\n information on the phone. If the fully driverless vehicle is\n\n51:54.380 --> 51:58.060\n used to service their trip, when you get into the car, we\n\n51:58.060 --> 52:01.260\n have an in car, you know, screen and audio that kind of guides\n\n52:01.260 --> 52:05.820\n them and explains what to expect. They also have a button\n\n52:05.820 --> 52:09.660\n that they can push that will connect them to, you know, a\n\n52:09.660 --> 52:13.260\n real life human being that they can talk to, right, about this\n\n52:13.260 --> 52:16.460\n whole process. So that's one aspect of it. There is, you\n\n52:16.460 --> 52:20.460\n know, I should mention that there is another function that\n\n52:21.100 --> 52:24.700\n humans provide to our cars, but it's not teleoperation. You can\n\n52:24.700 --> 52:26.620\n think of it a little bit more like, you know, fleet\n\n52:26.620 --> 52:29.980\n assistance, kind of like, you know, traffic control that you\n\n52:29.980 --> 52:34.860\n have, where our cars, again, they're responsible on their own\n\n52:34.860 --> 52:37.740\n for making all of the decisions, all of the driving decisions\n\n52:37.740 --> 52:40.460\n that don't require connectivity. They, you know,\n\n52:40.460 --> 52:44.300\n anything that is safety or latency critical is done, you\n\n52:44.300 --> 52:49.020\n know, purely autonomously by onboard, our onboard system.\n\n52:49.020 --> 52:51.260\n But there are situations where, you know, if connectivity is\n\n52:51.260 --> 52:53.980\n available, when a car encounters a particularly challenging\n\n52:53.980 --> 52:57.100\n situation, you can imagine like a super hairy scene of an\n\n52:57.100 --> 53:00.780\n accident, the cars will do their best, they will recognize that\n\n53:00.780 --> 53:05.660\n it's an off nominal situation, they will do their best to come\n\n53:05.660 --> 53:07.500\n up with the right interpretation, the best course\n\n53:07.500 --> 53:09.980\n of action in that scenario. But if connectivity is available,\n\n53:09.980 --> 53:13.420\n they can ask for confirmation from, you know, human\n\n53:15.660 --> 53:18.940\n assistant to kind of confirm those actions and perhaps\n\n53:19.580 --> 53:22.140\n provide a little bit of kind of contextual information and\n\n53:22.140 --> 53:26.380\n guidance. So October 8th was when you're talking about the\n\n53:26.380 --> 53:33.500\n was Waymo launched the fully self, the public version of\n\n53:33.500 --> 53:38.300\n its fully driverless, that's the right term, I think, service\n\n53:38.300 --> 53:41.580\n in Phoenix. Is that October 8th? That's right. It was the\n\n53:41.580 --> 53:43.820\n introduction of fully driverless, right, our only\n\n53:43.820 --> 53:47.660\n vehicles into our public Waymo One service. Okay, so that's\n\n53:47.660 --> 53:51.420\n that's amazing. So it's like anybody can get into Waymo in\n\n53:51.420 --> 53:57.100\n Phoenix. So we previously had early people in our early\n\n53:57.100 --> 54:01.100\n rider program, taking fully driverless rides in Phoenix.\n\n54:01.100 --> 54:06.220\n And just this a little while ago, we opened on October 8th,\n\n54:06.220 --> 54:09.500\n we opened that mode of operation to the public. So I\n\n54:09.500 --> 54:14.300\n can download the app and go on a ride. There's a lot more\n\n54:14.300 --> 54:17.100\n demand right now for that service. And then we have\n\n54:17.100 --> 54:20.300\n capacity. So we're kind of managing that. But that's\n\n54:20.300 --> 54:22.540\n exactly the way to describe it. Yeah, that's interesting. So\n\n54:22.540 --> 54:28.700\n there's more demand than you can handle. Like what has been\n\n54:28.700 --> 54:34.620\n reception so far? I mean, okay, so this is a product,\n\n54:34.620 --> 54:38.140\n right? That's a whole nother discussion of like how\n\n54:38.140 --> 54:41.420\n compelling of a product it is. Great. But it's also like one\n\n54:41.420 --> 54:43.980\n of the most kind of transformational technologies of\n\n54:43.980 --> 54:48.300\n the 21st century. So it's also like a tourist attraction.\n\n54:48.300 --> 54:52.380\n Like it's fun to, you know, to be a part of it. So it'd be\n\n54:52.380 --> 54:56.540\n interesting to see like, what do people say? What do people,\n\n54:56.540 --> 54:59.500\n what have been the feedback so far? You know, still early\n\n54:59.500 --> 55:04.140\n days, but so far, the feedback has been incredible, incredibly\n\n55:04.140 --> 55:07.180\n positive. They, you know, we asked them for feedback during\n\n55:07.180 --> 55:10.700\n the ride, we asked them for feedback after the ride as part\n\n55:10.700 --> 55:12.780\n of their trip. We asked them some questions, we asked them\n\n55:12.780 --> 55:17.100\n to rate the performance of our driver. Most by far, you know,\n\n55:17.100 --> 55:21.740\n most of our drivers give us five stars in our app, which is\n\n55:21.740 --> 55:24.700\n absolutely great to see. And you know, that's and we're\n\n55:24.700 --> 55:26.620\n they're also giving us feedback on you know, things we can\n\n55:26.620 --> 55:29.180\n improve. And you know, that's that's one of the main reasons\n\n55:29.180 --> 55:31.340\n we're doing this as Phoenix and you know, over the last couple\n\n55:31.340 --> 55:35.420\n of years, and every day today, we are just learning a\n\n55:35.420 --> 55:38.300\n tremendous amount of new stuff from our users. There's there's\n\n55:38.300 --> 55:41.980\n no substitute for actually doing the real thing, actually\n\n55:41.980 --> 55:44.780\n having a fully driverless product out there in the field\n\n55:44.780 --> 55:48.140\n with, you know, users that are actually paying us money to\n\n55:48.140 --> 55:51.740\n get from point A to point B. So this is a legitimate like,\n\n55:51.740 --> 55:56.140\n there's a paid service. That's right. And the idea is you use\n\n55:56.140 --> 55:59.340\n the app to go from point A to point B. And then what what are\n\n55:59.340 --> 56:03.260\n the A's? What are the what's the freedom of the of the starting\n\n56:03.260 --> 56:07.900\n and ending places? It's an area of geography where that\n\n56:07.900 --> 56:12.140\n service is enabled. It's a decent size of geography of\n\n56:12.140 --> 56:15.420\n territory. It's actually larger than the size of San Francisco.\n\n56:16.300 --> 56:20.220\n And you know, within that, you have full freedom of, you know,\n\n56:20.220 --> 56:22.540\n selecting where you want to go. You know, of course, there's\n\n56:22.540 --> 56:27.100\n some and you on your app, you get a map, you tell the car\n\n56:27.100 --> 56:31.340\n where you want to be picked up, where you want the car to pull\n\n56:31.340 --> 56:33.020\n over and pick you up. And then you tell it where you want to\n\n56:33.020 --> 56:34.940\n be dropped off. All right. And of course, there are some\n\n56:34.940 --> 56:37.740\n exclusions, right? You want to be you know, you were in terms\n\n56:37.740 --> 56:40.860\n of where the car is allowed to pull over, right? So that you\n\n56:40.860 --> 56:43.820\n can do. But you know, besides that, it's amazing. It's not\n\n56:43.820 --> 56:45.900\n like a fixed just would be very I guess. I don't know. Maybe\n\n56:45.900 --> 56:47.740\n that's what's the question behind your question. But it's\n\n56:47.740 --> 56:51.420\n not a, you know, preset set of yes, I guess. So within the\n\n56:51.420 --> 56:54.220\n geographic constraints with that within that area anywhere\n\n56:54.220 --> 56:56.780\n else, it can be you can be picked up and dropped off\n\n56:56.780 --> 56:59.740\n anywhere. That's right. And you know, people use them on like\n\n56:59.740 --> 57:02.540\n all kinds of trips. They we have and we have an incredible\n\n57:02.540 --> 57:05.340\n spectrum of riders. We I think the youngest actually have car\n\n57:05.340 --> 57:07.180\n seats them and we have, you know, people taking their kids\n\n57:07.180 --> 57:09.900\n and rides. I think the youngest riders we had on cars are, you\n\n57:09.900 --> 57:12.220\n know, one or two years old, you know, and the full spectrum of\n\n57:12.220 --> 57:17.020\n use cases people you can take them to, you know, schools to,\n\n57:17.020 --> 57:21.500\n you know, go grocery shopping, to restaurants, to bars, you\n\n57:21.500 --> 57:24.220\n know, run errands, you know, go shopping, etc, etc. You can go\n\n57:24.220 --> 57:27.180\n to your office, right? Like the full spectrum of use cases,\n\n57:27.180 --> 57:31.740\n and people are going to use them in their daily lives to get\n\n57:31.740 --> 57:37.020\n around. And we see all kinds of really interesting use cases\n\n57:37.020 --> 57:40.140\n and that that that's providing us incredibly valuable\n\n57:40.140 --> 57:43.740\n experience that we then, you know, use to improve our\n\n57:43.740 --> 57:50.220\n product. So as somebody who's been on done a few long rants\n\n57:50.220 --> 57:53.740\n with Joe Rogan and others about the toxicity of the internet\n\n57:53.740 --> 57:56.860\n and the comments and the negativity in the comments, I'm\n\n57:56.860 --> 58:01.740\n fascinated by feedback. I believe that most people are\n\n58:01.740 --> 58:07.420\n good and kind and intelligent and can provide, like, even in\n\n58:07.420 --> 58:11.100\n disagreement, really fascinating ideas. So on a product\n\n58:11.100 --> 58:14.540\n side, it's fascinating to me, like, how do you get the richest\n\n58:14.540 --> 58:19.500\n possible user feedback, like, to improve? What's, what are the\n\n58:19.500 --> 58:23.980\n channels that you use to measure? Because, like, you're\n\n58:23.980 --> 58:28.540\n no longer, that's one of the magical things about autonomous\n\n58:28.540 --> 58:32.300\n vehicles is it's not like it's frictionless interaction with\n\n58:32.300 --> 58:35.820\n the human. So like, you don't get to, you know, it's just\n\n58:35.820 --> 58:39.100\n giving a ride. So like, how do you get feedback from people\n\n58:39.100 --> 58:39.980\n to in order to improve?\n\n58:40.780 --> 58:44.940\n Yeah, great question, various mechanisms. So as part of the\n\n58:44.940 --> 58:48.220\n normal flow, we ask people for feedback, they as the car is\n\n58:48.220 --> 58:51.260\n driving around, we have on the phone and in the car, and we\n\n58:51.260 --> 58:54.060\n have a touchscreen in the car, you can actually click some\n\n58:54.060 --> 58:57.660\n buttons and provide real time feedback on how the car is\n\n58:57.660 --> 59:00.460\n doing, and how the car is handling a particular situation,\n\n59:00.460 --> 59:02.540\n you know, both positive and negative. So that's one\n\n59:02.540 --> 59:05.900\n channel, we have, as we discussed, customer support or\n\n59:05.900 --> 59:09.020\n life help, where, you know, if a customer wants to, has a\n\n59:09.020 --> 59:13.660\n question, or he has some sort of concern, they can talk to a\n\n59:13.660 --> 59:16.460\n person in real time. So that that is another mechanism that\n\n59:16.460 --> 59:21.340\n gives us feedback. At the end of a trip, you know, we also ask\n\n59:21.340 --> 59:25.100\n them how things went, they give us comments, and you know, star\n\n59:25.100 --> 59:29.340\n rating. And you know, if it's, we also, you know, ask them to\n\n59:30.780 --> 59:33.420\n explain what you know, one, well, and you know, what could\n\n59:33.420 --> 59:39.420\n be improved. And we have our writers providing very rich\n\n59:40.060 --> 59:44.300\n feedback, they're a lot, a large fraction is very passionate,\n\n59:44.300 --> 59:45.980\n very excited about this technology. So we get really\n\n59:45.980 --> 59:49.660\n good feedback. We also run UXR studies, right, you know,\n\n59:49.660 --> 59:53.340\n specific and that are kind of more, you know, go more in\n\n59:53.340 --> 59:56.220\n depth. And we will run both kind of lateral and longitudinal\n\n59:56.220 --> 1:00:01.260\n studies, where we have deeper engagement with our customers,\n\n1:00:01.260 --> 1:00:04.220\n you know, we have our user experience research team,\n\n1:00:04.220 --> 1:00:07.020\n tracking over time, that's things about longitudinal is\n\n1:00:07.020 --> 1:00:09.260\n cool. That's that's exactly right. And you know, that's\n\n1:00:09.260 --> 1:00:12.700\n another really valuable feedback, source of feedback.\n\n1:00:12.700 --> 1:00:15.340\n And we're just covering a tremendous amount, right?\n\n1:00:16.380 --> 1:00:19.420\n People go grocery shopping, and they like want to load, you\n\n1:00:19.420 --> 1:00:22.140\n know, 20 bags of groceries in our cars and like that, that's\n\n1:00:22.140 --> 1:00:26.700\n one workflow that you maybe don't think about, you know,\n\n1:00:26.700 --> 1:00:29.500\n getting just right when you're building the driverless\n\n1:00:29.500 --> 1:00:34.940\n product. I have people like, you know, who bike as part of\n\n1:00:34.940 --> 1:00:37.100\n their trip. So they, you know, bike somewhere, then they get\n\n1:00:37.100 --> 1:00:39.660\n on our cars, they take apart their bike, they load into our\n\n1:00:39.660 --> 1:00:42.140\n vehicle, then go and that's, you know, how they, you know,\n\n1:00:42.140 --> 1:00:45.340\n where we want to pull over and how that, you know, get in and\n\n1:00:45.340 --> 1:00:51.020\n get out process works, provides very useful feedback in terms\n\n1:00:51.020 --> 1:00:55.420\n of what makes a good pickup and drop off location, we get\n\n1:00:55.420 --> 1:01:00.780\n really valuable feedback. And in fact, we had to do some really\n\n1:01:00.780 --> 1:01:05.180\n interesting work with high definition maps, and thinking\n\n1:01:05.180 --> 1:01:08.700\n about walking directions. And if you imagine you're in a store,\n\n1:01:08.700 --> 1:01:11.020\n right in some giant space, and then you know, you want to be\n\n1:01:11.020 --> 1:01:14.380\n picked up somewhere, like if you just drop a pin at a current\n\n1:01:14.380 --> 1:01:16.780\n location, which is maybe in the middle of a shopping mall, like\n\n1:01:16.780 --> 1:01:20.140\n what's the best location for the car to come pick you up? And\n\n1:01:20.140 --> 1:01:22.220\n you can have simple heuristics where you're just going to take\n\n1:01:22.220 --> 1:01:25.500\n your you know, you clean in distance and find the nearest\n\n1:01:25.500 --> 1:01:28.300\n spot where the car can pull over that's closest to you. But\n\n1:01:28.300 --> 1:01:30.220\n oftentimes, that's not the most convenient one. You know, I have\n\n1:01:30.220 --> 1:01:32.860\n many anecdotes where that heuristic breaks in horrible\n\n1:01:32.860 --> 1:01:38.220\n ways. One example that I often mentioned is somebody wanted to\n\n1:01:38.220 --> 1:01:44.300\n be, you know, dropped off in Phoenix. And you know, we got\n\n1:01:44.300 --> 1:01:49.180\n car picked location that was close, the closest to there,\n\n1:01:49.180 --> 1:01:51.820\n you know, where the pin was dropped on the map in terms of,\n\n1:01:51.820 --> 1:01:55.180\n you know, latitude and longitude. But it happened to be\n\n1:01:55.180 --> 1:01:58.460\n on the other side of a parking lot that had this row of\n\n1:01:58.460 --> 1:02:01.500\n cacti. And the poor person had to like walk all around the\n\n1:02:01.500 --> 1:02:04.300\n parking lot to get to where they wanted to be in 110 degree\n\n1:02:04.300 --> 1:02:06.620\n heat. So that, you know, that was about so then, you know, we\n\n1:02:06.620 --> 1:02:10.060\n took all take all of these, all that feedback from our users\n\n1:02:10.060 --> 1:02:14.060\n and incorporate it into our system and improve it. Yeah, I\n\n1:02:14.060 --> 1:02:17.900\n feel like that's like requires AGI to solve the problem of\n\n1:02:17.900 --> 1:02:21.260\n like, when you're, which is a very common case, when you're in\n\n1:02:21.260 --> 1:02:24.700\n a big space of some kind, like apartment building, it doesn't\n\n1:02:24.700 --> 1:02:29.180\n matter, it's some large space. And then you call the, like a\n\n1:02:29.180 --> 1:02:32.780\n Waymo from there, right? Like, whatever, it doesn't matter,\n\n1:02:32.780 --> 1:02:37.580\n ride share vehicle. And like, where's the pin supposed to\n\n1:02:37.580 --> 1:02:41.580\n drop? I feel like that's, you don't think, I think that\n\n1:02:41.580 --> 1:02:45.660\n requires AGI. I'm gonna, in order to solve. Okay, the\n\n1:02:45.660 --> 1:02:49.980\n alternative, which I think the Google search engine is taught\n\n1:02:50.700 --> 1:02:55.420\n is like, there's something really valuable about the\n\n1:02:55.420 --> 1:02:58.620\n perhaps slightly dumb answer, but a really powerful one,\n\n1:02:58.620 --> 1:03:02.780\n which is like, what was done in the past by others? Like, what\n\n1:03:02.780 --> 1:03:06.380\n was the choice made by others? That seems to be like in terms\n\n1:03:06.380 --> 1:03:09.900\n of Google search, when you have like billions of searches, you\n\n1:03:09.900 --> 1:03:13.820\n could, you could see which, like when they recommend what you\n\n1:03:13.820 --> 1:03:17.660\n might possibly mean, they suggest based on not some machine\n\n1:03:17.660 --> 1:03:20.860\n learning thing, which they also do, but like, on what was\n\n1:03:20.860 --> 1:03:23.580\n successful for others in the past and finding a thing that\n\n1:03:23.580 --> 1:03:27.820\n they were happy with. Is that integrated at all? Waymo, like\n\n1:03:27.820 --> 1:03:31.740\n what, what pickups worked for others? It is. I think you're\n\n1:03:31.740 --> 1:03:34.220\n exactly right. So there's a real, it's an interesting\n\n1:03:34.220 --> 1:03:43.580\n problem. Naive solutions have interesting failure modes. So\n\n1:03:43.580 --> 1:03:48.780\n there's definitely lots of things that can be done to\n\n1:03:48.780 --> 1:03:54.940\n improve. And both learning from, you know, what works, but\n\n1:03:54.940 --> 1:03:57.980\n doesn't work in actual heal from getting richer data and\n\n1:03:57.980 --> 1:04:01.500\n getting more information about the environment and richer\n\n1:04:01.500 --> 1:04:04.060\n maps. But you're absolutely right, that there's something\n\n1:04:04.060 --> 1:04:07.580\n like there's some properties of solutions that in terms of the\n\n1:04:07.580 --> 1:04:10.140\n effect that they have on users so much, much, much better than\n\n1:04:10.140 --> 1:04:11.900\n others, right? And predictability and\n\n1:04:11.900 --> 1:04:14.460\n understandability is important. So you can have maybe\n\n1:04:14.460 --> 1:04:17.260\n something that is not quite as optimal, but is very natural\n\n1:04:17.260 --> 1:04:21.580\n and predictable to the user and kind of works the same way all\n\n1:04:21.580 --> 1:04:25.260\n the time. And that matters, that matters a lot for the user\n\n1:04:25.260 --> 1:04:30.300\n experience. And but you know, to get to the basics, the pretty\n\n1:04:30.300 --> 1:04:35.420\n fundamental property is that the car actually arrives where you\n\n1:04:35.420 --> 1:04:37.180\n told it to, right? Like, you can always, you know, change it,\n\n1:04:37.180 --> 1:04:39.100\n see it on the map, and you can move it around if you don't\n\n1:04:39.100 --> 1:04:42.620\n like it. And but like, that property that the car actually\n\n1:04:42.620 --> 1:04:47.740\n shows up reliably is critical, which, you know, where compared\n\n1:04:47.740 --> 1:04:52.780\n to some of the human driven analogs, I think, you know, you\n\n1:04:52.780 --> 1:04:56.460\n can have more predictability. It's actually the fact, if I\n\n1:04:56.460 --> 1:05:00.140\n have a little bit of a detour here, I think the fact that\n\n1:05:00.140 --> 1:05:03.100\n it's, you know, your phone and the cars, two computers talking\n\n1:05:03.100 --> 1:05:06.140\n to each other, can lead to some really interesting things we\n\n1:05:06.140 --> 1:05:09.740\n can do in terms of the user interfaces, both in terms of\n\n1:05:09.740 --> 1:05:13.340\n function, like the car actually shows up exactly where you told\n\n1:05:13.340 --> 1:05:16.140\n it, you want it to be, but also some, you know, really\n\n1:05:16.140 --> 1:05:18.380\n interesting things on the user interface, like as the car is\n\n1:05:18.380 --> 1:05:21.180\n driving, as you call it, and it's on the way to come pick\n\n1:05:21.180 --> 1:05:23.580\n you up. And of course, you get the position of the car and the\n\n1:05:23.580 --> 1:05:26.860\n route on the map. But and they actually follow that route, of\n\n1:05:26.860 --> 1:05:29.580\n course. But it can also share some really interesting\n\n1:05:29.580 --> 1:05:34.140\n information about what it's doing. So, you know, our cars, as\n\n1:05:34.140 --> 1:05:36.940\n they are coming to pick you up, if it's come, if a car is\n\n1:05:36.940 --> 1:05:39.180\n coming up to a stop sign, it will actually show you that\n\n1:05:39.180 --> 1:05:41.340\n like, it's there sitting, because it's at a stop sign or\n\n1:05:41.340 --> 1:05:42.860\n a traffic light will show you that it's got, you know,\n\n1:05:42.860 --> 1:05:44.700\n sitting at a red light. So, you know, they're like little\n\n1:05:44.700 --> 1:05:51.340\n things, right? But I find those little touches really\n\n1:05:51.340 --> 1:05:54.620\n interesting, really magical. And it's just, you know, little\n\n1:05:54.620 --> 1:05:57.180\n things like that, that you can do to kind of delight your\n\n1:05:57.180 --> 1:06:02.940\n users. You know, this makes me think of, there's some products\n\n1:06:02.940 --> 1:06:07.340\n that I just love. Like, there's a there's a company called\n\n1:06:07.340 --> 1:06:13.500\n Rev, Rev.com, where I like for this podcast, for example, I\n\n1:06:13.500 --> 1:06:17.900\n can drag and drop a video. And then they do all the\n\n1:06:17.900 --> 1:06:21.340\n captioning. It's humans doing the captioning, but they\n\n1:06:21.340 --> 1:06:24.780\n connect, they automate everything of connecting you to\n\n1:06:24.780 --> 1:06:27.180\n the humans, and they do the captioning and transcription.\n\n1:06:27.180 --> 1:06:29.980\n It's all effortless. And it like, I remember when I first\n\n1:06:29.980 --> 1:06:35.500\n started using them, I was like, life's good. Like, because it\n\n1:06:35.500 --> 1:06:39.020\n was so painful to figure that out earlier. The same thing\n\n1:06:39.020 --> 1:06:43.260\n with something called iZotope RX, this company I use for\n\n1:06:43.260 --> 1:06:46.380\n cleaning up audio, like the sound cleanup they do. It's\n\n1:06:46.380 --> 1:06:49.580\n like drag and drop, and it just cleans everything up very\n\n1:06:49.580 --> 1:06:52.940\n nicely. Another experience like that I had with Amazon\n\n1:06:52.940 --> 1:06:57.180\n OneClick purchase, first time. I mean, other places do that\n\n1:06:57.180 --> 1:07:00.140\n now, but just the effortlessness of purchasing,\n\n1:07:00.140 --> 1:07:04.380\n making it frictionless. It kind of communicates to me, like,\n\n1:07:04.380 --> 1:07:08.700\n I'm a fan of design. I'm a fan of products that you can just\n\n1:07:08.700 --> 1:07:12.540\n create a really pleasant experience. The simplicity of\n\n1:07:12.540 --> 1:07:16.380\n it, the elegance just makes you fall in love with it. So on\n\n1:07:16.380 --> 1:07:19.820\n the, do you think about this kind of stuff? I mean, it's\n\n1:07:19.820 --> 1:07:22.540\n exactly what we've been talking about. It's like the little\n\n1:07:22.540 --> 1:07:25.500\n details that somehow make you fall in love with the product.\n\n1:07:25.500 --> 1:07:30.860\n Is that, we went from like urban challenge days, where\n\n1:07:30.860 --> 1:07:34.540\n love was not part of the conversation, probably. And to\n\n1:07:34.540 --> 1:07:39.180\n this point where there's a, where there's human beings and\n\n1:07:39.180 --> 1:07:42.780\n you want them to fall in love with the experience. Is that\n\n1:07:42.780 --> 1:07:45.020\n something you're trying to optimize for? Try to think\n\n1:07:45.020 --> 1:07:47.820\n about, like, how do you create an experience that people love?\n\n1:07:48.700 --> 1:07:55.100\n Absolutely. I think that's the vision is removing any friction\n\n1:07:55.100 --> 1:08:02.300\n or complexity from getting our users, our writers to where\n\n1:08:02.300 --> 1:08:06.780\n they want to go. Making that as simple as possible. And then,\n\n1:08:06.780 --> 1:08:10.620\n you know, beyond that, just transportation, making things\n\n1:08:10.620 --> 1:08:13.580\n and goods get to their destination as seamlessly as\n\n1:08:13.580 --> 1:08:17.020\n possible. I talked about a drag and drop experience where I\n\n1:08:17.020 --> 1:08:20.460\n kind of express your intent and then it just magically happens.\n\n1:08:20.460 --> 1:08:23.100\n And for our writers, that's what we're trying to get to is\n\n1:08:23.100 --> 1:08:28.380\n you download an app and you click and car shows up. It's\n\n1:08:28.380 --> 1:08:33.580\n the same car. It's very predictable. It's a safe and\n\n1:08:33.580 --> 1:08:37.500\n high quality experience. And then it gets you in a very\n\n1:08:37.500 --> 1:08:43.900\n reliable, very convenient, frictionless way to where you\n\n1:08:43.900 --> 1:08:47.900\n want to be. And along the journey, I think we also want to\n\n1:08:47.900 --> 1:08:52.940\n do little things to delight our users. Like the ride sharing\n\n1:08:52.940 --> 1:08:56.620\n companies, because they don't control the experience, I\n\n1:08:56.620 --> 1:09:00.300\n think they can't make people fall in love necessarily with\n\n1:09:00.300 --> 1:09:04.140\n the experience. Or maybe they, they haven't put in the effort,\n\n1:09:04.140 --> 1:09:08.060\n but I think if I were to speak to the ride sharing experience\n\n1:09:08.060 --> 1:09:11.340\n I currently have, it's just very, it's just very\n\n1:09:11.340 --> 1:09:16.540\n convenient, but there's a lot of room for like falling in love\n\n1:09:16.540 --> 1:09:20.140\n with it. Like we can speak to sort of car companies, car\n\n1:09:20.140 --> 1:09:22.380\n companies do this. Well, you can fall in love with a car,\n\n1:09:22.380 --> 1:09:26.620\n right? And be like a loyal car person, like whatever. Like I\n\n1:09:26.620 --> 1:09:31.260\n like badass hot rods, I guess, 69 Corvette. And at this point,\n\n1:09:31.260 --> 1:09:35.580\n you know, you can't really, cars are so, owning a car is so\n\n1:09:35.580 --> 1:09:41.020\n 20th century, man. But is there something about the Waymo\n\n1:09:41.020 --> 1:09:43.660\n experience where you hope that people will fall in love with\n\n1:09:43.660 --> 1:09:48.780\n it? Is that part of it? Or is it part of, is it just about\n\n1:09:48.780 --> 1:09:52.380\n making a convenient ride, not ride sharing, I don't know what\n\n1:09:52.380 --> 1:09:56.060\n the right term is, but just a convenient A to B autonomous\n\n1:09:56.060 --> 1:10:02.460\n transport or like, do you want them to fall in love with\n\n1:10:02.460 --> 1:10:06.140\n Waymo? To maybe elaborate a little bit. I mean, almost like\n\n1:10:06.140 --> 1:10:11.820\n from a business perspective, I'm curious, like how, do you\n\n1:10:11.820 --> 1:10:15.260\n want to be in the background invisible or do you want to be\n\n1:10:15.260 --> 1:10:20.060\n like a source of joy that's in very much in the foreground? I\n\n1:10:20.060 --> 1:10:24.540\n want to provide the best, most enjoyable transportation\n\n1:10:24.540 --> 1:10:31.260\n solution. And that means building it, building our\n\n1:10:31.260 --> 1:10:34.300\n product and building our service in a way that people do.\n\n1:10:34.300 --> 1:10:41.900\n Kind of use in a very seamless, frictionless way in their\n\n1:10:41.900 --> 1:10:45.580\n day to day lives. And I think that does mean, you know, in\n\n1:10:45.580 --> 1:10:48.300\n some way falling in love in that product, right, just kind of\n\n1:10:48.300 --> 1:10:54.700\n becomes part of your routine. It comes down my mind to safety,\n\n1:10:54.700 --> 1:11:02.060\n predictability of the experience, and privacy aspects\n\n1:11:02.060 --> 1:11:07.100\n of it, right? Our cars, you get the same car, you get very\n\n1:11:07.100 --> 1:11:11.340\n predictable behavior. And you get a lot of different\n\n1:11:11.340 --> 1:11:14.940\n things. And that is important. And if you're going to use it\n\n1:11:14.940 --> 1:11:18.700\n in your daily life, privacy, and when you're in a car, you\n\n1:11:18.700 --> 1:11:21.020\n can do other things. You're spending a bunch, just another\n\n1:11:21.020 --> 1:11:24.380\n space where you're spending a significant part of your life.\n\n1:11:24.380 --> 1:11:27.820\n And so not having to share it with other people who you don't\n\n1:11:27.820 --> 1:11:32.380\n want to share it with, I think is a very nice property. Maybe\n\n1:11:32.380 --> 1:11:34.540\n you want to take a phone call or do something else in the\n\n1:11:34.540 --> 1:11:40.620\n vehicle. And, you know, safety on the quality of the driving,\n\n1:11:40.620 --> 1:11:45.660\n as well as the physical safety of not having to share that\n\n1:11:45.660 --> 1:11:52.300\n ride is important to a lot of people. What about the idea\n\n1:11:52.300 --> 1:11:56.940\n that when there's somebody like a human driving, and they do\n\n1:11:56.940 --> 1:12:01.180\n a rolling stop on a stop sign, like sometimes like, you know,\n\n1:12:01.180 --> 1:12:04.220\n you get an Uber or Lyft or whatever, like human driver,\n\n1:12:04.220 --> 1:12:07.980\n and, you know, they can be a little bit aggressive as\n\n1:12:07.980 --> 1:12:14.540\n drivers. It feels like there's not all aggression is bad. Now\n\n1:12:14.540 --> 1:12:17.500\n that may be a wrong, again, 20th century conception of\n\n1:12:17.500 --> 1:12:21.100\n driving. Maybe it's possible to create a driving experience.\n\n1:12:21.100 --> 1:12:24.940\n Like if you're in the back, busy doing something, maybe\n\n1:12:24.940 --> 1:12:27.740\n aggression is not a good thing. It's a very different kind of\n\n1:12:27.740 --> 1:12:32.540\n experience perhaps. But it feels like in order to navigate\n\n1:12:32.540 --> 1:12:38.540\n this world, you need to, how do I phrase this? You need to kind\n\n1:12:38.540 --> 1:12:42.140\n of bend the rules a little bit, or at least test the rules. I\n\n1:12:42.140 --> 1:12:44.540\n don't know what language politicians use to discuss this,\n\n1:12:44.540 --> 1:12:48.700\n but whatever language they use, you like flirt with the rules.\n\n1:12:48.700 --> 1:12:55.580\n I don't know. But like you sort of have a bit of an aggressive\n\n1:12:55.580 --> 1:13:00.460\n way of driving that asserts your presence in this world,\n\n1:13:00.460 --> 1:13:03.660\n thereby making other vehicles and people respect your\n\n1:13:03.660 --> 1:13:06.780\n presence and thereby allowing you to sort of navigate\n\n1:13:06.780 --> 1:13:10.060\n through intersections in a timely fashion. I don't know if\n\n1:13:10.060 --> 1:13:14.300\n any of that made sense, but like, how does that fit into the\n\n1:13:14.300 --> 1:13:18.620\n experience of driving autonomously? Is that?\n\n1:13:18.620 --> 1:13:20.460\n It's a lot of thoughts. This is you're hitting on a very\n\n1:13:20.460 --> 1:13:27.500\n important point of a number of behavioral components and, you\n\n1:13:27.500 --> 1:13:34.380\n know, parameters that make your driving feel assertive and\n\n1:13:34.380 --> 1:13:37.260\n natural and comfortable and predictable. Our cars will\n\n1:13:37.260 --> 1:13:39.740\n follow rules, right? They will do the safest thing possible in\n\n1:13:39.740 --> 1:13:43.580\n all situations. Let me be clear on that. But if you think of\n\n1:13:43.580 --> 1:13:47.660\n really, really good drivers, just think about\n\n1:13:47.660 --> 1:13:49.740\n professional lemon drivers, right? They will follow the\n\n1:13:49.740 --> 1:13:53.900\n rules. They're very, very smooth, and yet they're very\n\n1:13:53.900 --> 1:13:58.140\n efficient. But they're assertive. They're comfortable\n\n1:13:58.140 --> 1:14:02.140\n for the people in the vehicle. They're predictable for the\n\n1:14:02.140 --> 1:14:03.820\n other people outside the vehicle that they share the\n\n1:14:03.820 --> 1:14:06.540\n environment with. And that's the kind of driver that we want\n\n1:14:06.540 --> 1:14:11.100\n to build. And you think if maybe there's a sport analogy\n\n1:14:11.100 --> 1:14:17.740\n there, right? You can do in very many sports, the true\n\n1:14:17.740 --> 1:14:20.620\n professionals are very efficient in their movements,\n\n1:14:20.620 --> 1:14:25.100\n right? They don't do like, you know, hectic flailing, right?\n\n1:14:25.100 --> 1:14:29.020\n They're, you know, smooth and precise, right? And they get\n\n1:14:29.020 --> 1:14:30.860\n the best results. So that's the kind of driver that we want to\n\n1:14:30.860 --> 1:14:33.100\n build. In terms of, you know, aggressiveness. Yeah, you can\n\n1:14:33.100 --> 1:14:35.740\n like, you know, roll through the stop signs. You can do crazy\n\n1:14:35.740 --> 1:14:38.060\n lane changes. It typically doesn't get you to your\n\n1:14:38.060 --> 1:14:40.700\n destination faster. Typically not the safest or most\n\n1:14:40.700 --> 1:14:45.820\n predictable, very most comfortable thing to do. But\n\n1:14:45.820 --> 1:14:49.660\n there is a way to do both. And that's what we're\n\n1:14:49.660 --> 1:14:53.820\n doing. We're trying to build the driver that is safe,\n\n1:14:53.820 --> 1:14:58.140\n comfortable, smooth, and predictable. Yeah, that's a\n\n1:14:58.140 --> 1:15:00.380\n really interesting distinction. I think in the early days of\n\n1:15:00.380 --> 1:15:03.660\n autonomous vehicles, the vehicles felt cautious as\n\n1:15:03.660 --> 1:15:08.620\n opposed to efficient. And I'm still probably, but when I\n\n1:15:08.620 --> 1:15:13.500\n rode in the Waymo, I mean, there was, it was, it was quite\n\n1:15:13.500 --> 1:15:19.740\n assertive. It moved pretty quickly. Like, yeah, then he's\n\n1:15:19.740 --> 1:15:22.940\n one of the surprising feelings was that it actually, it went\n\n1:15:22.940 --> 1:15:28.300\n fast. And it didn't feel like, awkwardly cautious than\n\n1:15:28.300 --> 1:15:31.900\n autonomous vehicle. Like, like, so I've also programmed\n\n1:15:31.900 --> 1:15:34.860\n autonomous vehicles and everything I've ever built was\n\n1:15:34.860 --> 1:15:39.260\n felt awkwardly, either overly aggressive. Okay, especially\n\n1:15:39.260 --> 1:15:44.860\n when it was my code, or like, awkwardly cautious is the way\n\n1:15:44.860 --> 1:15:53.180\n I would put it. And Waymo's vehicle felt like, assertive\n\n1:15:53.180 --> 1:15:57.180\n and I think efficient is like the right terminology here.\n\n1:15:57.180 --> 1:16:01.340\n It wasn't, and I also like the professional limo driver,\n\n1:16:01.340 --> 1:16:06.060\n because we often think like, you know, an Uber driver or a\n\n1:16:06.060 --> 1:16:09.820\n bus driver or a taxi. This is the funny thing is people\n\n1:16:09.820 --> 1:16:14.940\n think they track taxi drivers are professionals. They, I\n\n1:16:14.940 --> 1:16:18.460\n mean, it's, it's like, that that's like saying, I'm a\n\n1:16:18.460 --> 1:16:20.780\n professional walker, just because I've been walking all\n\n1:16:20.780 --> 1:16:25.580\n my life. I think there's an art to it, right? And if you take\n\n1:16:25.580 --> 1:16:30.700\n it seriously as an art form, then there's a certain way that\n\n1:16:30.700 --> 1:16:33.900\n mastery looks like. It's interesting to think about what\n\n1:16:33.900 --> 1:16:39.180\n does mastery look like in driving? And perhaps what we\n\n1:16:39.180 --> 1:16:43.020\n associate with like aggressiveness is unnecessary,\n\n1:16:43.020 --> 1:16:46.940\n like, it's not part of the experience of driving. It's\n\n1:16:46.940 --> 1:16:54.860\n like, unnecessary fluff, that efficiency, you can be,\n\n1:16:54.860 --> 1:17:00.380\n you can create a good driving experience within the rules.\n\n1:17:00.380 --> 1:17:03.100\n That's, I mean, you're the first person to tell me this.\n\n1:17:03.100 --> 1:17:04.940\n So it's, it's kind of interesting. I need to think\n\n1:17:04.940 --> 1:17:07.900\n about this, but that's exactly what it felt like with Waymo.\n\n1:17:07.900 --> 1:17:10.060\n I kind of had this intuition. Maybe it's the Russian thing.\n\n1:17:10.060 --> 1:17:13.740\n I don't know that you have to break the rules in life to get\n\n1:17:13.740 --> 1:17:19.020\n anywhere, but maybe, maybe it's possible that that's not the\n\n1:17:19.020 --> 1:17:23.500\n case in driving. I have to think about that, but it\n\n1:17:23.500 --> 1:17:25.980\n certainly felt that way on the streets of Phoenix when I was\n\n1:17:25.980 --> 1:17:29.340\n there in Waymo, that, that, that that was a very pleasant\n\n1:17:29.340 --> 1:17:32.460\n experience and it wasn't frustrating in that like, come\n\n1:17:32.460 --> 1:17:35.260\n on, move already kind of feeling. It wasn't, that wasn't\n\n1:17:35.260 --> 1:17:37.900\n there. Yeah. I mean, that's what, that's what we're going\n\n1:17:37.900 --> 1:17:41.420\n after. I don't think you have to pick one. I think truly good\n\n1:17:41.420 --> 1:17:45.020\n driving. It gives you both efficiency, a certainness, but\n\n1:17:45.020 --> 1:17:49.900\n also comfort and predictability and safety. And, you know, it's,\n\n1:17:49.900 --> 1:17:54.940\n that's what fundamental improvements in the core\n\n1:17:54.940 --> 1:17:59.260\n capabilities truly unlock. And you can kind of think of it as,\n\n1:17:59.260 --> 1:18:01.980\n you know, a precision and recall trade off. You have certain\n\n1:18:01.980 --> 1:18:04.460\n capabilities of your model. And then it's very easy when, you\n\n1:18:04.460 --> 1:18:06.540\n know, you have some curve of precision and recall, you can\n\n1:18:06.540 --> 1:18:08.540\n move things around and can choose your operating point and\n\n1:18:08.540 --> 1:18:10.700\n your training of precision versus recall, false positives\n\n1:18:10.700 --> 1:18:14.220\n versus false negatives. Right. But then, and you know, you can\n\n1:18:14.220 --> 1:18:16.940\n tune things on that curve and be kind of more cautious or more\n\n1:18:16.940 --> 1:18:19.340\n aggressive, but then aggressive is bad or, you know, cautious is\n\n1:18:19.340 --> 1:18:22.540\n bad, but true capabilities come from actually moving the whole\n\n1:18:22.540 --> 1:18:28.540\n curve up. And then you are kind of on a very different plane of\n\n1:18:28.540 --> 1:18:31.340\n those trade offs. And that, that's what we're trying to do\n\n1:18:31.340 --> 1:18:34.700\n here is to move the whole curve up. Before I forget, let's talk\n\n1:18:34.700 --> 1:18:39.420\n about trucks a little bit. So I also got a chance to check out\n\n1:18:39.420 --> 1:18:44.300\n some of the Waymo trucks. I'm not sure if we want to go too\n\n1:18:44.300 --> 1:18:47.180\n much into that space, but it's a fascinating one. So maybe we\n\n1:18:47.180 --> 1:18:51.020\n can mention at least briefly, you know, Waymo is also now\n\n1:18:51.020 --> 1:18:56.540\n doing autonomous trucking and how different like\n\n1:18:56.540 --> 1:18:58.780\n philosophically and technically is that whole space of\n\n1:18:58.780 --> 1:19:06.060\n problems. It's one of our two big products and you know,\n\n1:19:06.060 --> 1:19:09.020\n commercial applications of our driver, right? Right. Hailing\n\n1:19:09.020 --> 1:19:12.700\n and deliveries. You know, we have Waymo One and Waymo Via\n\n1:19:12.700 --> 1:19:16.220\n moving people and moving goods. You know, trucking is an\n\n1:19:16.220 --> 1:19:21.580\n example of moving goods. We've been working on trucking since\n\n1:19:21.580 --> 1:19:31.340\n 2017. It is a very interesting space. And your question of\n\n1:19:31.340 --> 1:19:35.020\n how different is it? It has this really nice property that\n\n1:19:35.020 --> 1:19:38.780\n the first order challenges, like the science, the hard\n\n1:19:38.780 --> 1:19:42.140\n engineering, whether it's, you know, hardware or, you know,\n\n1:19:42.140 --> 1:19:45.420\n onboard software or off board software, all of the, you know,\n\n1:19:45.420 --> 1:19:48.780\n systems that you build for, you know, training your ML models\n\n1:19:48.780 --> 1:19:51.820\n for, you know, evaluating your time system. Like those\n\n1:19:51.820 --> 1:19:56.460\n fundamentals carry over. Like the true challenges of, you\n\n1:19:56.460 --> 1:20:00.620\n know, driving perception, semantic understanding,\n\n1:20:00.620 --> 1:20:04.860\n prediction, decision making, planning, evaluation, the\n\n1:20:04.860 --> 1:20:08.780\n simulator, ML infrastructure, those carry over. Like the data\n\n1:20:08.780 --> 1:20:12.380\n and the application and kind of the domains might be\n\n1:20:12.380 --> 1:20:16.060\n different, but the most difficult problems, all of that\n\n1:20:16.060 --> 1:20:19.420\n carries over between the domains. So that's very nice.\n\n1:20:19.420 --> 1:20:22.300\n So that's how we approach it. We're kind of build investing\n\n1:20:22.300 --> 1:20:26.220\n in the core, the technical core. And then there's\n\n1:20:26.220 --> 1:20:30.620\n specialization of that core technology to different\n\n1:20:30.620 --> 1:20:34.540\n product lines, to different commercial applications. So on\n\n1:20:34.540 --> 1:20:38.140\n just to tease it apart a little bit on trucks. So starting with\n\n1:20:38.140 --> 1:20:42.140\n the hardware, the configuration of the sensors is different.\n\n1:20:42.140 --> 1:20:46.300\n They're different physically, geometrically, you know, different\n\n1:20:46.300 --> 1:20:50.860\n vehicles. So for example, we have two of our main laser on\n\n1:20:50.860 --> 1:20:54.380\n the trucks on both sides so that we have, you know, not have the\n\n1:20:54.380 --> 1:20:59.100\n blind spots. Whereas on the JLR eye pace, we have, you know, one\n\n1:20:59.100 --> 1:21:02.940\n of it sitting at the very top, but the actual sensors are\n\n1:21:02.940 --> 1:21:06.700\n almost the same. Now we're largely the same. So all of the\n\n1:21:06.700 --> 1:21:11.180\n investment that over the years we've put into building our\n\n1:21:11.180 --> 1:21:13.580\n custom lighters, custom radars, pulling the whole system\n\n1:21:13.580 --> 1:21:16.540\n together, that carries over very nicely. Then, you know, on the\n\n1:21:16.540 --> 1:21:20.780\n perception side, the like the fundamental challenges of\n\n1:21:20.780 --> 1:21:22.940\n seeing, understanding the world, whether it's, you know, object\n\n1:21:22.940 --> 1:21:25.740\n detection, classification, you know, tracking, semantic\n\n1:21:25.740 --> 1:21:28.300\n understanding, all that carries over. You know, yes, there's\n\n1:21:28.300 --> 1:21:31.100\n some specialization when you're driving on freeways, you know,\n\n1:21:31.100 --> 1:21:33.820\n range becomes more important. The domain is a little bit\n\n1:21:33.820 --> 1:21:36.860\n different. But again, the fundamentals carry over very,\n\n1:21:36.860 --> 1:21:41.100\n very nicely. Same, and you guess you get into prediction or\n\n1:21:41.100 --> 1:21:45.260\n decision making, right, the fundamentals of what it takes to\n\n1:21:45.260 --> 1:21:49.580\n predict what other people are going to do to find the long\n\n1:21:49.580 --> 1:21:53.420\n tail to improve your system in that long tail of behavior\n\n1:21:53.420 --> 1:21:56.060\n prediction and response that carries over right and so on and\n\n1:21:56.060 --> 1:22:00.060\n so on. So I mean, that's pretty exciting. By the way, does\n\n1:22:00.060 --> 1:22:05.100\n Waymo via include using the smaller vehicles for\n\n1:22:05.100 --> 1:22:07.580\n transportation of goods? That's an interesting distinction. So\n\n1:22:07.580 --> 1:22:12.220\n I would say there's three interesting modes of operation.\n\n1:22:13.020 --> 1:22:16.860\n So one is moving humans, one is moving goods, and one is like\n\n1:22:16.860 --> 1:22:21.020\n moving nothing, zero occupancy, meaning like you're going to\n\n1:22:21.740 --> 1:22:27.580\n the destination, your empty vehicle. I mean, it's the third\n\n1:22:27.580 --> 1:22:29.820\n is the less of it. If that's the entirety of it, it's the less,\n\n1:22:29.820 --> 1:22:31.660\n you know, exciting from the commercial perspective.\n\n1:22:31.660 --> 1:22:38.140\n Well, I mean, in terms of like, if you think about what's\n\n1:22:38.140 --> 1:22:42.060\n inside a vehicle as it's moving, because it does, you\n\n1:22:42.060 --> 1:22:45.580\n know, some significant fraction of the vehicle's movement has\n\n1:22:45.580 --> 1:22:50.700\n to be empty. I mean, it's kind of fascinating. Maybe just on\n\n1:22:50.700 --> 1:22:57.340\n that small point, is there different control and like\n\n1:22:57.340 --> 1:23:01.500\n policies that are applied for zero occupancy vehicle? So\n\n1:23:01.500 --> 1:23:04.940\n vehicle with nothing in it, or is it just move as if there is\n\n1:23:04.940 --> 1:23:08.780\n a person inside? What was with some subtle differences?\n\n1:23:09.500 --> 1:23:13.100\n As a first order approximation, there are no differences. And\n\n1:23:13.100 --> 1:23:17.740\n if you think about, you know, safety and comfort and quality\n\n1:23:17.740 --> 1:23:26.540\n of driving, only part of it has to do with the people or the\n\n1:23:26.540 --> 1:23:29.340\n goods inside of the vehicle. But you don't want to be, you\n\n1:23:29.340 --> 1:23:31.820\n know, you want to drive smoothly, as we discussed, not\n\n1:23:31.820 --> 1:23:34.780\n for the purely for the benefit of whatever you have inside the\n\n1:23:34.780 --> 1:23:38.540\n car, right? It's also for the benefit of the people outside\n\n1:23:38.540 --> 1:23:41.660\n kind of fitting naturally and predictably into that whole\n\n1:23:41.660 --> 1:23:43.820\n environment, right? So, you know, yes, there are some\n\n1:23:43.820 --> 1:23:47.180\n second order things you can do, you can change your route, and\n\n1:23:47.180 --> 1:23:50.860\n you optimize maybe kind of your fleet, things at the fleet\n\n1:23:50.860 --> 1:23:54.300\n scale. And you would take into account whether some of your\n\n1:23:54.300 --> 1:23:58.780\n you know, some of your cars are actually, you know, serving a\n\n1:23:58.780 --> 1:24:01.180\n useful trip, whether with people or with goods, whereas, you\n\n1:24:01.180 --> 1:24:05.180\n know, other cars are, you know, driving completely empty to that\n\n1:24:05.180 --> 1:24:09.500\n next valuable trip that they're going to provide. But that those\n\n1:24:09.500 --> 1:24:13.260\n are mostly second order effects. Okay, cool. So Phoenix\n\n1:24:14.380 --> 1:24:18.780\n is, is an incredible place. And what you've announced in\n\n1:24:18.780 --> 1:24:23.340\n Phoenix is, it's kind of amazing. But, you know, that's\n\n1:24:23.340 --> 1:24:30.220\n just like one city. How do you take over the world? I mean,\n\n1:24:30.220 --> 1:24:33.420\n I'm asking for a friend. One step at a time.\n\n1:24:35.980 --> 1:24:40.460\n Is that a cartoon pinky in the brain? Yeah. Okay. But, you\n\n1:24:40.460 --> 1:24:44.540\n know, gradually is a true answer. So I think the heart of\n\n1:24:44.540 --> 1:24:48.860\n your question is, can you ask a better question than I asked?\n\n1:24:48.860 --> 1:24:52.940\n You're asking a great question. Answer that one. I'm just\n\n1:24:52.940 --> 1:24:56.300\n gonna, you know, phrase it in the terms that I want to\n\n1:24:56.300 --> 1:25:01.660\n answer. Exactly right. Brilliant. Please. You know,\n\n1:25:01.660 --> 1:25:04.940\n where are we today? And, you know, what happens next? And\n\n1:25:04.940 --> 1:25:08.220\n what does it take to go beyond Phoenix? And what does it\n\n1:25:08.220 --> 1:25:13.660\n take to get this technology to more places and more people\n\n1:25:13.660 --> 1:25:23.100\n around the world, right? So our next big area of focus is\n\n1:25:23.100 --> 1:25:26.700\n exactly that. Larger scale commercialization and just,\n\n1:25:26.700 --> 1:25:35.340\n you know, scaling up. If I think about, you know, the\n\n1:25:35.340 --> 1:25:39.100\n main, and, you know, Phoenix gives us that platform and\n\n1:25:39.100 --> 1:25:44.940\n gives us that foundation of upon which we can build. And\n\n1:25:44.940 --> 1:25:51.580\n it's, there are few really challenging aspects of this\n\n1:25:51.580 --> 1:25:56.460\n whole problem that you have to pull together in order to build\n\n1:25:56.460 --> 1:26:03.900\n the technology in order to deploy it into the field to go\n\n1:26:03.900 --> 1:26:09.820\n from a driverless car to a fleet of cars that are providing a\n\n1:26:09.820 --> 1:26:14.140\n service, and then all the way to commercialization. So, and\n\n1:26:14.140 --> 1:26:15.980\n then, you know, this is what we have in Phoenix. We've taken\n\n1:26:15.980 --> 1:26:21.100\n the technology from a proof point to an actual deployment\n\n1:26:21.100 --> 1:26:25.980\n and have taken our driver from, you know, one car to a fleet\n\n1:26:25.980 --> 1:26:29.980\n that can provide a service. Beyond that, if I think about\n\n1:26:29.980 --> 1:26:35.820\n what it will take to scale up and, you know, deploy in, you\n\n1:26:35.820 --> 1:26:41.740\n know, more places with more customers, I tend to think about\n\n1:26:41.740 --> 1:26:48.380\n three main dimensions, three main axes of scale. One is the\n\n1:26:48.380 --> 1:26:51.660\n core technology, you know, the hardware and software core\n\n1:26:51.660 --> 1:26:56.540\n capabilities of our driver. The second dimension is\n\n1:26:56.540 --> 1:27:01.900\n evaluation and deployment. And the third one is the, you know,\n\n1:27:01.900 --> 1:27:06.060\n product, commercial, and operational excellence. So you\n\n1:27:06.060 --> 1:27:09.660\n can talk a bit about where we are along, you know, each one of\n\n1:27:09.660 --> 1:27:11.900\n those three dimensions about where we are today and, you\n\n1:27:11.900 --> 1:27:16.780\n know, what has, what will happen next. On, you know, the core\n\n1:27:16.780 --> 1:27:19.580\n technology, you know, the hardware and software, you\n\n1:27:19.580 --> 1:27:25.420\n know, together comprise a driver, we, you know, obviously\n\n1:27:25.420 --> 1:27:30.460\n have that foundation that is providing fully driverless\n\n1:27:30.460 --> 1:27:34.780\n trips to our customers as we speak, in fact. And we've\n\n1:27:34.780 --> 1:27:39.500\n learned a tremendous amount from that. So now what we're\n\n1:27:39.500 --> 1:27:44.380\n doing is we are incorporating all those lessons into some\n\n1:27:44.380 --> 1:27:47.180\n pretty fundamental improvements in our core technology, both on\n\n1:27:47.180 --> 1:27:51.660\n the hardware side and on the software side to build a more\n\n1:27:51.660 --> 1:27:54.860\n general, more robust solution that then will enable us to\n\n1:27:54.860 --> 1:28:00.460\n massively scale beyond Phoenix. So on the hardware side, all of\n\n1:28:00.460 --> 1:28:05.180\n those lessons are now incorporated into this fifth\n\n1:28:05.180 --> 1:28:09.500\n generation hardware platform that is, you know, being\n\n1:28:09.500 --> 1:28:13.180\n deployed right now. And that's the platform, the fourth\n\n1:28:13.180 --> 1:28:14.860\n generation, the thing that we have right now driving in\n\n1:28:14.860 --> 1:28:18.700\n Phoenix, it's good enough to operate fully driverlessly,\n\n1:28:18.700 --> 1:28:21.500\n you know, night and day, you know, various speeds and\n\n1:28:21.500 --> 1:28:25.020\n various conditions, but the fifth generation is the platform\n\n1:28:25.020 --> 1:28:30.140\n upon which we want to go to massive scale. We, in turn,\n\n1:28:30.140 --> 1:28:32.620\n we've really made qualitative improvements in terms of the\n\n1:28:32.620 --> 1:28:35.980\n capability of the system, the simplicity of the architecture,\n\n1:28:35.980 --> 1:28:39.900\n the reliability of the redundancy. It is designed to be\n\n1:28:39.900 --> 1:28:42.300\n manufacturable at very large scale and, you know, provides\n\n1:28:42.300 --> 1:28:46.380\n the right unit economics. So that's the next big step for us\n\n1:28:46.380 --> 1:28:49.580\n on the hardware side. That's already there for scale,\n\n1:28:49.580 --> 1:28:53.500\n the version five. That's right. And is that coincidence or\n\n1:28:53.500 --> 1:28:55.580\n should we look into a conspiracy theory that it's the\n\n1:28:55.580 --> 1:28:59.660\n same version as the pixel phone? Is that what's the\n\n1:28:59.660 --> 1:29:04.220\n hardware? They neither confirm nor deny. All right, cool. So,\n\n1:29:04.220 --> 1:29:08.140\n sorry. So that's the, okay, that's that axis. What else?\n\n1:29:08.140 --> 1:29:11.100\n So similarly, you know, hardware is a very discreet\n\n1:29:11.100 --> 1:29:14.940\n jump, but, you know, similar to how we're making that change\n\n1:29:14.940 --> 1:29:16.940\n from the fourth generation hardware to the fifth, we're\n\n1:29:16.940 --> 1:29:19.420\n making similar improvements on the software side to make it\n\n1:29:19.420 --> 1:29:22.300\n more, you know, robust and more general and allow us to kind of\n\n1:29:22.300 --> 1:29:25.740\n quickly scale beyond Phoenix. So that's the first dimension of\n\n1:29:25.740 --> 1:29:27.980\n core technology. The second dimension is evaluation and\n\n1:29:27.980 --> 1:29:34.300\n deployment. How do you measure your system? How do you\n\n1:29:34.300 --> 1:29:37.500\n evaluate it? How do you build a release and deployment process\n\n1:29:37.500 --> 1:29:40.780\n where, you know, with confidence, you can, you know,\n\n1:29:40.780 --> 1:29:45.420\n regularly release new versions of your driver into a fleet?\n\n1:29:45.420 --> 1:29:49.180\n How do you get good at it so that it is not, you know, a\n\n1:29:49.180 --> 1:29:52.540\n huge tax on your researchers and engineers that, you know, so\n\n1:29:52.540 --> 1:29:55.740\n you can, how do you build all these, you know, processes, the\n\n1:29:55.740 --> 1:29:58.620\n frameworks, the simulation, the evaluation, the data science,\n\n1:29:58.620 --> 1:30:01.340\n the validation, so that, you know, people can focus on\n\n1:30:01.340 --> 1:30:04.380\n improving the system and kind of the releases just go out the\n\n1:30:04.380 --> 1:30:07.340\n door and get deployed across the fleet. So we've gotten really\n\n1:30:07.340 --> 1:30:11.820\n good at that in Phoenix. That's been a tremendously difficult\n\n1:30:11.820 --> 1:30:15.180\n problem, but that's what we have in Phoenix right now that gives\n\n1:30:15.180 --> 1:30:17.660\n us that foundation. And now we're working on kind of\n\n1:30:17.660 --> 1:30:20.220\n incorporating all the lessons that we've learned to make it\n\n1:30:20.220 --> 1:30:22.860\n more efficient, to go to new places, you know, and scale up\n\n1:30:22.860 --> 1:30:25.660\n and just kind of, you know, stamp things out. So that's that\n\n1:30:25.660 --> 1:30:28.700\n second dimension of evaluation and deployment. And the third\n\n1:30:28.700 --> 1:30:33.340\n dimension is product, commercial, and operational\n\n1:30:33.340 --> 1:30:38.140\n excellence, right? And again, Phoenix there is providing an\n\n1:30:38.140 --> 1:30:40.940\n incredibly valuable platform. You know, that's why we're doing\n\n1:30:40.940 --> 1:30:43.660\n things end to end in Phoenix. We're learning, as you know, we\n\n1:30:43.660 --> 1:30:47.900\n discussed a little earlier today, tremendous amount of\n\n1:30:47.900 --> 1:30:50.460\n really valuable lessons from our users getting really\n\n1:30:50.460 --> 1:30:54.860\n incredible feedback. And we'll continue to iterate on that and\n\n1:30:54.860 --> 1:30:59.420\n incorporate all those lessons into making our product, you\n\n1:30:59.420 --> 1:31:01.660\n know, even better and more convenient for our users.\n\n1:31:01.660 --> 1:31:06.620\n So you're converting this whole process in Phoenix into\n\n1:31:06.620 --> 1:31:11.260\n something that could be copy and pasted elsewhere. So like,\n\n1:31:11.260 --> 1:31:13.180\n perhaps you didn't think of it that way when you were doing\n\n1:31:13.180 --> 1:31:17.660\n the experimentation in Phoenix, but so how long did you\n\n1:31:17.660 --> 1:31:22.140\n basically, and you can correct me, but you've, I mean, it's\n\n1:31:22.140 --> 1:31:24.700\n still early days, but you've taken the full journey in\n\n1:31:24.700 --> 1:31:29.180\n Phoenix, right? As you were saying of like what it takes to\n\n1:31:29.180 --> 1:31:31.900\n basically automate. I mean, it's not the entirety of Phoenix,\n\n1:31:31.900 --> 1:31:36.300\n right? But I imagine it can encompass the entirety of\n\n1:31:36.300 --> 1:31:41.340\n Phoenix. That's some near term date, but that's not even\n\n1:31:41.340 --> 1:31:43.740\n perhaps important. Like as long as it's a large enough\n\n1:31:43.740 --> 1:31:51.580\n geographic area. So what, how copy pastable is that process\n\n1:31:51.580 --> 1:31:58.300\n currently and how like, you know, like when you copy and\n\n1:31:58.300 --> 1:32:05.260\n paste in Google docs, I think now in, or in word, you can\n\n1:32:05.260 --> 1:32:09.340\n like apply source formatting or apply destination formatting.\n\n1:32:09.340 --> 1:32:14.620\n So how, when you copy and paste the Phoenix into like, say\n\n1:32:14.620 --> 1:32:20.060\n Boston, how do you apply the destination formatting? Like\n\n1:32:20.060 --> 1:32:25.980\n how much of the core of the entire process of bringing an\n\n1:32:25.980 --> 1:32:30.460\n actual public transportation, autonomous transportation\n\n1:32:30.460 --> 1:32:35.340\n service to a city is there in Phoenix that you understand\n\n1:32:35.340 --> 1:32:39.660\n enough to copy and paste into Boston or wherever? So we're\n\n1:32:39.660 --> 1:32:41.980\n not quite there yet. We're not at a point where we're kind of\n\n1:32:41.980 --> 1:32:47.100\n massively copy and pasting all over the place. But Phoenix,\n\n1:32:47.100 --> 1:32:50.940\n what we did in Phoenix, and we very intentionally have chosen\n\n1:32:50.940 --> 1:32:56.620\n Phoenix as our first full deployment area, you know,\n\n1:32:56.620 --> 1:32:59.580\n exactly for that reason to kind of tease the problem apart,\n\n1:32:59.580 --> 1:33:03.180\n look at each dimension and focus on the fundamentals of\n\n1:33:03.180 --> 1:33:06.460\n complexity and de risking those dimensions, and then bringing\n\n1:33:06.460 --> 1:33:09.340\n the entire thing together to get all the way and force\n\n1:33:09.340 --> 1:33:12.460\n ourselves to learn all those hard lessons on technology,\n\n1:33:12.460 --> 1:33:15.740\n hardware and software, on the evaluation deployment, on\n\n1:33:15.740 --> 1:33:20.060\n operating a service, operating a business using actually\n\n1:33:20.060 --> 1:33:22.860\n serving our customers all the way so that we're fully\n\n1:33:22.860 --> 1:33:27.580\n informed about the most difficult, most important\n\n1:33:27.580 --> 1:33:31.180\n challenges to get us to that next step of massive copy and\n\n1:33:31.180 --> 1:33:38.860\n pasting as you said. And that's what we're doing right now.\n\n1:33:38.860 --> 1:33:41.740\n We're incorporating all those things that we learned into\n\n1:33:41.740 --> 1:33:44.860\n that next system that then will allow us to kind of copy and\n\n1:33:44.860 --> 1:33:47.500\n paste all over the place and to massively scale to, you know,\n\n1:33:47.500 --> 1:33:50.300\n more users and more locations. I mean, you know, just talk a\n\n1:33:50.300 --> 1:33:52.380\n little bit about, you know, what does that mean along those\n\n1:33:52.380 --> 1:33:55.020\n different dimensions? So on the hardware side, for example,\n\n1:33:55.020 --> 1:33:57.980\n again, it's that switch from the fourth to the fifth\n\n1:33:57.980 --> 1:34:00.380\n generation. And the fifth generation is designed to kind\n\n1:34:00.380 --> 1:34:04.540\n of have that property. Can you say what other cities you're\n\n1:34:04.540 --> 1:34:09.020\n thinking about? Like, I'm thinking about, sorry, we're in\n\n1:34:09.020 --> 1:34:12.380\n San Francisco now. I thought I want to move to San Francisco,\n\n1:34:12.380 --> 1:34:16.540\n but I'm thinking about moving to Austin. I don't know why\n\n1:34:16.540 --> 1:34:19.580\n people are not being very nice about San Francisco currently,\n\n1:34:19.580 --> 1:34:23.340\n but maybe it's a small, maybe it's in vogue right now.\n\n1:34:23.340 --> 1:34:28.060\n But Austin seems, I visited there and it was, I was in a\n\n1:34:28.060 --> 1:34:32.860\n Walmart. It's funny, these moments like turn your life.\n\n1:34:32.860 --> 1:34:38.860\n There's this very nice woman with kind eyes, just like stopped\n\n1:34:38.860 --> 1:34:44.460\n and said, he looks so handsome in that tie, honey, to me. This\n\n1:34:44.460 --> 1:34:47.260\n has never happened to me in my life, but just the sweetness of\n\n1:34:47.260 --> 1:34:49.980\n this woman is something I've never experienced, certainly on\n\n1:34:49.980 --> 1:34:53.100\n the streets of Boston, but even in San Francisco where people\n\n1:34:53.100 --> 1:34:57.100\n wouldn't, that's just not how they speak or think. I don't\n\n1:34:57.100 --> 1:35:00.700\n know. There's a warmth to Austin that love. And since\n\n1:35:00.700 --> 1:35:04.060\n Waymo does have a little bit of a history there, is that a\n\n1:35:04.060 --> 1:35:07.980\n possibility? Is this your version of asking the question\n\n1:35:07.980 --> 1:35:09.980\n of like, you know, Dimitri, I know you can't share your\n\n1:35:09.980 --> 1:35:12.780\n commercial and deployment roadmap, but I'm thinking about\n\n1:35:12.780 --> 1:35:16.300\n moving to San Francisco, Austin, like, you know, blink twice if\n\n1:35:16.300 --> 1:35:19.900\n you think I should move to it. That's true. That's true. You\n\n1:35:19.900 --> 1:35:23.900\n got me. You know, we've been testing all over the place. I\n\n1:35:23.900 --> 1:35:26.860\n think we've been testing more than 25 cities. We drive\n\n1:35:26.860 --> 1:35:31.740\n in San Francisco. We drive in, you know, Michigan for snow.\n\n1:35:31.740 --> 1:35:34.220\n We are doing significant amount of testing in the Bay Area,\n\n1:35:34.220 --> 1:35:37.340\n including San Francisco, which is not like, because we're\n\n1:35:37.340 --> 1:35:40.060\n talking about the very different thing, which is like a\n\n1:35:40.060 --> 1:35:46.380\n full on large geographic area, public service. You can't share\n\n1:35:46.380 --> 1:35:54.140\n and you, okay. What about Moscow? When is that happening?\n\n1:35:54.140 --> 1:35:58.700\n Take on Yandex. I'm not paying attention to those folks.\n\n1:35:58.700 --> 1:36:02.380\n They're doing, you know, there's a lot of fun. I mean,\n\n1:36:02.380 --> 1:36:10.540\n maybe as a way of a question, you didn't speak to sort of like\n\n1:36:10.540 --> 1:36:15.020\n policy or like, is there tricky things with government and so\n\n1:36:15.020 --> 1:36:20.860\n on? Like, is there other friction that you've\n\n1:36:20.860 --> 1:36:25.260\n encountered except sort of technological friction of\n\n1:36:25.260 --> 1:36:28.540\n solving this very difficult problem? Is there other stuff\n\n1:36:28.540 --> 1:36:33.340\n that you have to overcome when deploying a public service in\n\n1:36:33.340 --> 1:36:38.860\n a city? That's interesting. It's very important. So we\n\n1:36:38.860 --> 1:36:44.540\n put significant effort in creating those partnerships and\n\n1:36:44.540 --> 1:36:48.380\n you know, those relationships with governments at all levels,\n\n1:36:48.380 --> 1:36:50.860\n local governments, municipalities, state level,\n\n1:36:50.860 --> 1:36:53.900\n federal level. We've been engaged in very deep\n\n1:36:53.900 --> 1:36:57.020\n conversations from the earliest days of our projects.\n\n1:36:57.020 --> 1:37:01.020\n Whenever at all of these levels, whenever we go\n\n1:37:01.020 --> 1:37:07.500\n to test or operate in a new area, we always lead\n\n1:37:07.500 --> 1:37:10.860\n with a conversation with the local officials.\n\n1:37:10.860 --> 1:37:13.740\n But the result of that investment is that no,\n\n1:37:13.740 --> 1:37:16.780\n it's not challenges we have to overcome, but it is very\n\n1:37:16.780 --> 1:37:19.980\n important that we continue to have this conversation.\n\n1:37:19.980 --> 1:37:27.340\n Oh, yeah. I love politicians too. Okay, so Mr. Elon Musk said that\n\n1:37:27.340 --> 1:37:32.300\n LiDAR is a crutch. What are your thoughts?\n\n1:37:32.940 --> 1:37:36.540\n I wouldn't characterize it exactly that way. I know I think LiDAR is\n\n1:37:36.540 --> 1:37:42.540\n very important. It is a key sensor that we use just like\n\n1:37:42.540 --> 1:37:46.700\n other modalities, right? As we discussed, our cars use cameras, LiDAR\n\n1:37:46.700 --> 1:37:52.700\n and radars. They are all very important. They are\n\n1:37:52.700 --> 1:37:57.900\n at the kind of the physical level. They are very different. They have very\n\n1:37:57.900 --> 1:38:00.300\n different, you know, physical characteristics.\n\n1:38:00.300 --> 1:38:03.100\n Cameras are passive. LiDARs and radars are active.\n\n1:38:03.100 --> 1:38:07.420\n Use different wavelengths. So that means they complement each other\n\n1:38:07.420 --> 1:38:14.700\n very nicely and together combined, they can be used to\n\n1:38:14.700 --> 1:38:20.620\n build a much safer and much more capable system.\n\n1:38:20.620 --> 1:38:25.020\n So, you know, to me it's more of a question,\n\n1:38:25.020 --> 1:38:28.700\n you know, why the heck would you handicap yourself and not use one\n\n1:38:28.700 --> 1:38:32.860\n or more of those sensing modalities when they, you know, undoubtedly just make your\n\n1:38:32.860 --> 1:38:39.100\n system more capable and safer. Now,\n\n1:38:39.100 --> 1:38:45.180\n it, you know, what might make sense for one product or\n\n1:38:45.180 --> 1:38:48.380\n one business might not make sense for another one.\n\n1:38:48.380 --> 1:38:51.980\n So if you're talking about driver assist technologies, you make certain design\n\n1:38:51.980 --> 1:38:55.260\n decisions and you make certain trade offs and make different ones if you are\n\n1:38:55.260 --> 1:38:59.820\n building a driver that you deploy in fully driverless\n\n1:38:59.820 --> 1:39:04.940\n vehicles. And, you know, in LiDAR specifically, when this question comes up,\n\n1:39:04.940 --> 1:39:11.820\n I, you know, typically the criticisms that I hear or, you know, the\n\n1:39:11.820 --> 1:39:16.060\n counterpoints is that cost and aesthetics.\n\n1:39:16.060 --> 1:39:20.460\n And I don't find either of those, honestly, very compelling.\n\n1:39:20.460 --> 1:39:24.380\n So on the cost side, there's nothing fundamentally prohibitive\n\n1:39:24.380 --> 1:39:28.620\n about, you know, the cost of LiDARs. You know, radars used to be very expensive\n\n1:39:28.620 --> 1:39:32.140\n before people started, you know, before people made certain advances in\n\n1:39:32.140 --> 1:39:35.980\n technology and, you know, started to manufacture them at massive scale and\n\n1:39:35.980 --> 1:39:39.740\n deploy them in vehicles, right? You know, similar with LiDARs. And this is\n\n1:39:39.740 --> 1:39:43.260\n where the LiDARs that we have on our cars, especially the fifth generation,\n\n1:39:43.260 --> 1:39:48.220\n you know, we've been able to make some pretty qualitative discontinuous\n\n1:39:48.220 --> 1:39:51.580\n jumps in terms of the fundamental technology that allow us to\n\n1:39:51.580 --> 1:39:56.380\n manufacture those things at very significant scale and at a fraction\n\n1:39:56.380 --> 1:40:00.300\n of the cost of both our previous generation\n\n1:40:00.300 --> 1:40:03.980\n as well as a fraction of the cost of, you know, what might be available\n\n1:40:03.980 --> 1:40:07.100\n on the market, you know, off the shelf right now. And, you know, that improvement\n\n1:40:07.100 --> 1:40:10.700\n will continue. So I think, you know, cost is not a\n\n1:40:10.700 --> 1:40:14.300\n real issue. Second one is, you know, aesthetics.\n\n1:40:14.300 --> 1:40:18.060\n You know, I don't think that's, you know, a real issue either.\n\n1:40:18.060 --> 1:40:22.860\n Beauty is in the eye of the beholder. Yeah. You can make LiDAR sexy again.\n\n1:40:22.860 --> 1:40:25.740\n I think you're exactly right. I think it is sexy. Like, honestly, I think form\n\n1:40:25.740 --> 1:40:30.060\n all of function. Well, okay. You know, I was actually, somebody brought this up to\n\n1:40:30.060 --> 1:40:34.940\n me. I mean, all forms of LiDAR, even\n\n1:40:34.940 --> 1:40:37.580\n like the ones that are like big, you can make\n\n1:40:37.580 --> 1:40:40.700\n look, I mean, you can make look beautiful.\n\n1:40:40.700 --> 1:40:44.060\n There's no sense in which you can't integrate it into design.\n\n1:40:44.060 --> 1:40:47.820\n Like, there's all kinds of awesome designs. I don't think\n\n1:40:47.820 --> 1:40:51.260\n small and humble is beautiful. It could be\n\n1:40:51.260 --> 1:40:55.580\n like, you know, brutalism or like, it could be\n\n1:40:55.580 --> 1:40:59.340\n like harsh corners. I mean, like I said, like hot rods. Like, I don't like, I don't\n\n1:40:59.340 --> 1:41:02.700\n necessarily like, like, oh man, I'm going to start so much\n\n1:41:02.700 --> 1:41:07.420\n controversy with this. I don't like Porsches. Okay.\n\n1:41:07.420 --> 1:41:10.700\n The Porsche 911, like everyone says it's the most beautiful.\n\n1:41:10.700 --> 1:41:15.340\n No, no. It's like, it's like a baby car. It doesn't make any sense.\n\n1:41:15.340 --> 1:41:18.940\n But everyone, it's beauty is in the eye of the beholder. You're already looking at\n\n1:41:18.940 --> 1:41:24.060\n me like, what is this kid talking about? I'm happy to talk about. You're digging your\n\n1:41:24.060 --> 1:41:27.980\n own hole. The form and function and my take on the\n\n1:41:27.980 --> 1:41:30.940\n beauty of the hardware that we put on our vehicles,\n\n1:41:30.940 --> 1:41:34.700\n you know, I will not comment on your Porsche monologue.\n\n1:41:34.700 --> 1:41:39.340\n Okay. All right. So, but aesthetics, fine. But there's an underlying, like,\n\n1:41:39.340 --> 1:41:43.900\n philosophical question behind the kind of lighter question is\n\n1:41:43.900 --> 1:41:48.060\n like, how much of the problem can be solved\n\n1:41:48.060 --> 1:41:51.660\n with computer vision, with machine learning?\n\n1:41:51.660 --> 1:41:58.460\n So I think without sort of disagreements and so on,\n\n1:41:58.460 --> 1:42:03.340\n it's nice to put it on the spectrum because Waymo is doing a lot of machine\n\n1:42:03.340 --> 1:42:06.460\n learning as well. It's interesting to think how much of\n\n1:42:06.460 --> 1:42:11.260\n driving, if we look at five years, 10 years, 50 years down the road,\n\n1:42:11.260 --> 1:42:15.340\n what can be learned in almost more and more and more\n\n1:42:15.340 --> 1:42:19.820\n end to end way. If we look at what Tesla is doing\n\n1:42:19.820 --> 1:42:24.300\n with, as a machine learning problem, they're doing a multitask learning\n\n1:42:24.300 --> 1:42:27.820\n thing where it's just, they break up driving into a bunch of learning tasks\n\n1:42:27.820 --> 1:42:30.540\n and they have one single neural network and they're just collecting huge amounts\n\n1:42:30.540 --> 1:42:33.340\n of data that's training that. I've recently hung out with George\n\n1:42:33.340 --> 1:42:36.940\n Hotz. I don't know if you know George.\n\n1:42:37.820 --> 1:42:41.820\n I love him so much. He's just an entertaining human being.\n\n1:42:41.820 --> 1:42:45.340\n We were off mic talking about Hunter S. Thompson. He's the Hunter S. Thompson\n\n1:42:45.340 --> 1:42:49.420\n of autonomous driving. Okay. So he, I didn't realize this with comma\n\n1:42:49.420 --> 1:42:53.180\n AI, but they're like really trying to end to end.\n\n1:42:53.180 --> 1:42:58.460\n They're the machine, like looking at the machine learning problem, they're\n\n1:42:58.460 --> 1:43:01.580\n really not doing multitask learning, but it's\n\n1:43:01.580 --> 1:43:05.980\n computing the drivable area as a machine learning task\n\n1:43:05.980 --> 1:43:11.500\n and hoping that like down the line, this level two system, this driver\n\n1:43:11.500 --> 1:43:15.340\n assistance will eventually lead to\n\n1:43:15.340 --> 1:43:19.260\n allowing you to have a fully autonomous vehicle. Okay. There's an underlying\n\n1:43:19.260 --> 1:43:22.540\n deep philosophical question there, technical question\n\n1:43:22.540 --> 1:43:29.420\n of how much of driving can be learned. So LiDAR is an effective tool today\n\n1:43:29.420 --> 1:43:33.820\n for actually deploying a successful service in Phoenix, right? That's safe,\n\n1:43:33.820 --> 1:43:39.260\n that's reliable, et cetera, et cetera. But the question,\n\n1:43:39.260 --> 1:43:43.100\n and I'm not saying you can't do machine learning on LiDAR, but the question is\n\n1:43:43.100 --> 1:43:47.340\n that like how much of driving can be learned eventually.\n\n1:43:47.340 --> 1:43:49.980\n Can we do fully autonomous? That's learned.\n\n1:43:49.980 --> 1:43:53.340\n Yeah. You know, learning is all over the place\n\n1:43:53.340 --> 1:43:56.620\n and plays a key role in every part of our system.\n\n1:43:56.620 --> 1:44:01.180\n As you said, I would, you know, decouple the sensing modalities\n\n1:44:01.180 --> 1:44:05.180\n from the, you know, ML and the software parts of it.\n\n1:44:05.180 --> 1:44:09.740\n LiDAR, radar, cameras, like it's all machine learning.\n\n1:44:09.740 --> 1:44:12.220\n All of the object detection classification, of course, like that's\n\n1:44:12.220 --> 1:44:15.100\n what, you know, these modern deep nets and count nets are very\n\n1:44:15.100 --> 1:44:19.820\n good at. You feed them raw data, massive amounts of raw data,\n\n1:44:19.820 --> 1:44:23.900\n and that's actually what our custom build LiDARs and radars are really good\n\n1:44:23.900 --> 1:44:25.500\n at. And radars, they don't just give you point\n\n1:44:25.500 --> 1:44:28.060\n estimates of, you know, objects in space, they give you raw,\n\n1:44:28.060 --> 1:44:31.660\n like, physical observations. And then you take all of that raw information,\n\n1:44:31.660 --> 1:44:34.780\n you know, there's colors of the pixels, whether it's, you know, LiDARs returns\n\n1:44:34.780 --> 1:44:36.780\n and some auxiliary information. It's not just distance,\n\n1:44:36.780 --> 1:44:39.500\n right? And, you know, angle and distance is much richer information that you get\n\n1:44:39.500 --> 1:44:42.460\n from those returns, plus really rich information from the\n\n1:44:42.460 --> 1:44:45.820\n radars. You fuse it all together and you feed it into those massive\n\n1:44:45.820 --> 1:44:51.340\n ML models that then, you know, lead to the best results in terms of, you\n\n1:44:51.340 --> 1:44:55.820\n know, object detection, classification, state estimation.\n\n1:44:55.820 --> 1:44:59.020\n So there's a side to interop, but there is a fusion. I mean, that's something\n\n1:44:59.020 --> 1:45:01.020\n that people didn't do for a very long time,\n\n1:45:01.020 --> 1:45:04.540\n which is like at the sensor fusion level, I guess,\n\n1:45:04.540 --> 1:45:07.660\n like early on fusing the information together, whether\n\n1:45:07.660 --> 1:45:11.980\n so that the the sensory information that the vehicle receives from the different\n\n1:45:11.980 --> 1:45:15.180\n modalities or even from different cameras is\n\n1:45:15.180 --> 1:45:19.020\n combined before it is fed into the machine learning models.\n\n1:45:19.020 --> 1:45:21.660\n Yeah, so I think this is one of the trends you're seeing more of that you\n\n1:45:21.660 --> 1:45:24.780\n mentioned end to end. There's different interpretation of end to end. There is\n\n1:45:24.780 --> 1:45:27.980\n kind of the purest interpretation of I'm going to\n\n1:45:27.980 --> 1:45:32.300\n like have one model that goes from raw sensor data to like,\n\n1:45:32.300 --> 1:45:35.100\n you know, steering torque and, you know, gas breaks. That, you know,\n\n1:45:35.100 --> 1:45:37.500\n that's too much. I don't think that's the right way to do it.\n\n1:45:37.500 --> 1:45:40.620\n There's more, you know, smaller versions of end to end\n\n1:45:40.620 --> 1:45:45.500\n where you're kind of doing more end to end learning or core training or\n\n1:45:45.500 --> 1:45:48.700\n depropagation of kind of signals back and forth across\n\n1:45:48.700 --> 1:45:51.900\n the different stages of your system. There's, you know, really good ways it\n\n1:45:51.900 --> 1:45:55.180\n gets into some fairly complex design choices where on one\n\n1:45:55.180 --> 1:45:57.980\n hand you want modularity and decomposability,\n\n1:45:57.980 --> 1:46:01.580\n decomposability of your system. But on the other hand,\n\n1:46:01.580 --> 1:46:05.100\n you don't want to create interfaces that are too narrow or too brittle\n\n1:46:05.100 --> 1:46:08.380\n to engineered where you're giving up on the generality of the solution or you're\n\n1:46:08.380 --> 1:46:12.940\n unable to properly propagate signal, you know, reach signal forward and losses\n\n1:46:12.940 --> 1:46:17.500\n and, you know, back so you can optimize the whole system jointly.\n\n1:46:17.500 --> 1:46:21.180\n So I would decouple and I guess what you're seeing in terms of the fusion\n\n1:46:21.180 --> 1:46:25.580\n of the sensing data from different modalities as well as kind of fusion\n\n1:46:25.580 --> 1:46:30.060\n at in the temporal level going more from, you know, frame by frame\n\n1:46:30.060 --> 1:46:32.780\n where, you know, you would have one net that would do frame by frame detection\n\n1:46:32.780 --> 1:46:35.500\n and camera and then, you know, something that does frame by frame and\n\n1:46:35.500 --> 1:46:39.260\n lighter and then radar and then you fuse it, you know, in a weaker engineered way\n\n1:46:39.260 --> 1:46:41.260\n later. Like the field over the last, you know,\n\n1:46:41.260 --> 1:46:45.260\n decade has been evolving in more kind of joint fusion, more end to end models that\n\n1:46:45.260 --> 1:46:48.060\n are, you know, solving some of these tasks, you know, jointly and there's\n\n1:46:48.060 --> 1:46:50.860\n tremendous power in that and, you know, that's the\n\n1:46:50.860 --> 1:46:54.700\n progression that kind of our technology, our stack has been on as well.\n\n1:46:54.700 --> 1:46:57.980\n Now to your, you know, that so I would decouple the kind of sensing and how\n\n1:46:57.980 --> 1:47:01.340\n that information is fused from the role of ML and the entire stack.\n\n1:47:01.340 --> 1:47:06.460\n And, you know, I guess it's, there's trade offs and, you know, modularity and\n\n1:47:06.460 --> 1:47:11.260\n how do you inject inductive bias into your system?\n\n1:47:11.260 --> 1:47:15.180\n All right, this is, there's tremendous power\n\n1:47:15.180 --> 1:47:19.660\n in being able to do that. So, you know, we have, there's no\n\n1:47:19.660 --> 1:47:25.180\n part of our system that is not heavily, that does not heavily, you know, leverage\n\n1:47:25.180 --> 1:47:29.580\n data driven development or state of the art ML.\n\n1:47:29.580 --> 1:47:33.580\n But there's mapping, there's a simulator, there's perception, you know, object\n\n1:47:33.580 --> 1:47:34.940\n level, you know, perception, whether it's\n\n1:47:34.940 --> 1:47:38.220\n semantic understanding, prediction, decision making, you know, so forth and\n\n1:47:38.220 --> 1:47:40.540\n so on.\n\n1:47:42.060 --> 1:47:45.100\n It's, you know, of course, object detection and classification, like you're\n\n1:47:45.100 --> 1:47:48.460\n finding pedestrians and cars and cyclists and, you know, cones and signs\n\n1:47:48.460 --> 1:47:51.740\n and vegetation and being very good at estimating\n\n1:47:51.740 --> 1:47:54.460\n kind of detection, classification, and state estimation. There's just stable\n\n1:47:54.460 --> 1:47:57.900\n stakes, like that's step zero of this whole stack. You can be\n\n1:47:57.900 --> 1:48:00.700\n incredibly good at that, whether you use cameras or light as a\n\n1:48:00.700 --> 1:48:03.660\n radar, but that's just, you know, that's stable stakes, that's just step zero.\n\n1:48:03.660 --> 1:48:06.380\n Beyond that, you get into the really interesting challenges of semantic\n\n1:48:06.380 --> 1:48:10.140\n understanding at the perception level, you get into scene level reasoning, you\n\n1:48:10.140 --> 1:48:13.900\n get into very deep problems that have to do with prediction and joint\n\n1:48:13.900 --> 1:48:16.140\n prediction and interaction, so the interaction\n\n1:48:16.140 --> 1:48:19.260\n between all the actors in the environment, pedestrians, cyclists, other\n\n1:48:19.260 --> 1:48:22.300\n cars, and you get into decision making, right? So, how do you build a lot of\n\n1:48:22.300 --> 1:48:26.300\n systems? So, we leverage ML very heavily in all of\n\n1:48:26.300 --> 1:48:30.140\n these components. I do believe that the best results you\n\n1:48:30.140 --> 1:48:33.340\n achieve by kind of using a hybrid approach and\n\n1:48:33.340 --> 1:48:38.140\n having different types of ML, having\n\n1:48:38.140 --> 1:48:41.580\n different models with different degrees of inductive bias\n\n1:48:41.580 --> 1:48:45.260\n that you can have, and combining kind of model,\n\n1:48:45.260 --> 1:48:49.180\n you know, free approaches with some model based approaches and some\n\n1:48:49.180 --> 1:48:54.380\n rule based, physics based systems. So, you know, one example I can give\n\n1:48:54.380 --> 1:48:58.940\n you is traffic lights. There's a problem of the detection of\n\n1:48:58.940 --> 1:49:02.700\n traffic light state, and obviously that's a great problem for, you know, computer\n\n1:49:02.700 --> 1:49:05.260\n vision confidence, or, you know, that's their bread and\n\n1:49:05.260 --> 1:49:08.220\n butter, right? That's how you build that. But then the\n\n1:49:08.220 --> 1:49:11.740\n interpretation of, you know, of a traffic light, that you're\n\n1:49:11.740 --> 1:49:15.820\n gonna need to learn that, right? You don't need to build some,\n\n1:49:15.820 --> 1:49:18.940\n you know, complex ML model that, you know, infers\n\n1:49:18.940 --> 1:49:22.540\n with some, you know, precision and recall that red means stop.\n\n1:49:22.540 --> 1:49:25.500\n Like, it's a very clear engineered signal\n\n1:49:25.500 --> 1:49:29.500\n with very clear semantics, right? So you want to induce that bias, like how you\n\n1:49:29.500 --> 1:49:31.740\n induce that bias, and that whether, you know, it's a\n\n1:49:31.740 --> 1:49:36.460\n constraint or a cost, you know, function in your stack, but like\n\n1:49:36.460 --> 1:49:40.860\n it is important to be able to inject that, like, clear semantic\n\n1:49:40.860 --> 1:49:44.220\n signal into your stack. And, you know, that's what we do.\n\n1:49:44.220 --> 1:49:47.340\n And, but then the question of, like, and that's when you\n\n1:49:47.340 --> 1:49:50.860\n apply it to yourself, when you are making decisions whether you want to stop\n\n1:49:50.860 --> 1:49:54.540\n for a red light, you know, or not.\n\n1:49:54.540 --> 1:49:57.820\n But if you think about how other people treat traffic lights,\n\n1:49:57.820 --> 1:50:01.260\n we're back to the ML version of that. You know they're supposed to stop\n\n1:50:01.260 --> 1:50:02.860\n for a red light, but that doesn't mean they will.\n\n1:50:02.860 --> 1:50:07.820\n So then you're back in the, like, very heavy\n\n1:50:07.820 --> 1:50:11.420\n ML domain where you're picking up on, like, very subtle cues about,\n\n1:50:11.420 --> 1:50:15.260\n you know, they have to do with the behavior of objects, pedestrians, cyclists,\n\n1:50:15.260 --> 1:50:19.420\n cars, and the whole, you know, entire configuration of the scene\n\n1:50:19.420 --> 1:50:22.220\n that allow you to make accurate predictions on whether they will, in\n\n1:50:22.220 --> 1:50:27.020\n fact, stop or run a red light. So it sounds like already for Waymo,\n\n1:50:27.020 --> 1:50:29.820\n like, machine learning is a huge part of the stack.\n\n1:50:29.820 --> 1:50:36.300\n So it's a huge part of, like, not just, so obviously the first, the level\n\n1:50:36.300 --> 1:50:38.860\n zero, or whatever you said, which is, like,\n\n1:50:38.860 --> 1:50:42.380\n just the object detection of things that, you know, with no other machine learning\n\n1:50:42.380 --> 1:50:46.380\n can do, but also starting to do prediction behavior and so on to\n\n1:50:46.380 --> 1:50:49.660\n model the, what other, what the other parties in the\n\n1:50:49.660 --> 1:50:51.580\n scene, entities in the scene are going to do.\n\n1:50:51.580 --> 1:50:55.260\n So machine learning is more and more playing a role in that\n\n1:50:55.260 --> 1:50:59.020\n as well. Of course. Oh, absolutely. I think we've been\n\n1:50:59.020 --> 1:51:02.060\n going back to the, you know, earliest days, like, you know, DARPA,\n\n1:51:02.060 --> 1:51:05.820\n the DARPA Grand Challenge, our team was leveraging, you know, machine\n\n1:51:05.820 --> 1:51:08.540\n learning. It was, like, pre, you know, ImageNet, and it was a very\n\n1:51:08.540 --> 1:51:11.660\n different type of ML, but, and I think actually it was before\n\n1:51:11.660 --> 1:51:15.340\n my time, but the Stanford team during the Grand Challenge had a very\n\n1:51:15.340 --> 1:51:18.940\n interesting machine learned system that would, you know, use\n\n1:51:18.940 --> 1:51:21.340\n LiDAR and camera. We've been driving in the\n\n1:51:21.340 --> 1:51:26.940\n desert, and it, we had built the model where it would kind of\n\n1:51:26.940 --> 1:51:29.900\n extend the range of free space reasoning. We get a\n\n1:51:29.900 --> 1:51:33.020\n clear signal from LiDAR, and then it had a model that said, hey, like,\n\n1:51:33.020 --> 1:51:35.900\n this stuff on camera kind of sort of looks like this stuff in LiDAR, and I\n\n1:51:35.900 --> 1:51:38.860\n know this stuff that I'm seeing in LiDAR, I'm very confident it's free space,\n\n1:51:38.860 --> 1:51:43.420\n so let me extend that free space zone into the camera range that would allow\n\n1:51:43.420 --> 1:51:45.980\n the vehicle to drive faster. And then we've been building on top of\n\n1:51:45.980 --> 1:51:48.860\n that and kind of staying and pushing the state of the art in ML,\n\n1:51:48.860 --> 1:51:52.620\n in all kinds of different ML over the years. And in fact,\n\n1:51:52.620 --> 1:51:56.940\n from the early days, I think, you know, 2010 is probably the year\n\n1:51:56.940 --> 1:52:03.500\n where Google, maybe 2011 probably, got pretty heavily involved in\n\n1:52:03.500 --> 1:52:07.660\n machine learning, kind of deep nuts, and at that time it was probably the only\n\n1:52:07.660 --> 1:52:11.980\n company that was very heavily investing in kind of state of the art ML and\n\n1:52:11.980 --> 1:52:16.220\n self driving cars. And they go hand in hand.\n\n1:52:16.220 --> 1:52:19.980\n And we've been on that journey ever since. We're doing, pushing\n\n1:52:19.980 --> 1:52:24.060\n a lot of these areas in terms of research at Waymo, and we\n\n1:52:24.060 --> 1:52:26.620\n collaborate very heavily with the researchers in\n\n1:52:26.620 --> 1:52:30.060\n Alphabet, and all kinds of ML, supervised ML,\n\n1:52:30.060 --> 1:52:34.380\n unsupervised ML, published some\n\n1:52:34.380 --> 1:52:37.900\n interesting research papers in the space,\n\n1:52:37.900 --> 1:52:41.180\n especially recently. It's just a super active learning as well.\n\n1:52:41.180 --> 1:52:45.260\n Yeah, so super, super active. Of course, there's, you know, kind of the more\n\n1:52:45.260 --> 1:52:48.940\n mature stuff, like, you know, ConvNets for, you know, object detection.\n\n1:52:48.940 --> 1:52:52.860\n But there's some really interesting, really active work that's happening\n\n1:52:52.860 --> 1:52:58.300\n in kind of more, you know, in bigger models and, you know,\n\n1:52:58.300 --> 1:53:02.540\n models that have more structure to them,\n\n1:53:02.540 --> 1:53:06.860\n you know, not just, you know, large bitmaps and reason about temporal sequences.\n\n1:53:06.860 --> 1:53:10.700\n And some of the interesting breakthroughs that you've, you know, we've seen\n\n1:53:10.700 --> 1:53:14.140\n in language models, right? You know, transformers,\n\n1:53:14.140 --> 1:53:19.100\n you know, GPT3 inference. There's some really interesting applications of some\n\n1:53:19.100 --> 1:53:21.260\n of the core breakthroughs to those problems\n\n1:53:21.260 --> 1:53:24.540\n of, you know, behavior prediction, as well as, you know, decision making and\n\n1:53:24.540 --> 1:53:27.900\n planning, right? You can think about it, kind of the the behavior,\n\n1:53:27.900 --> 1:53:31.500\n how, you know, the path, the trajectories, the how people drive.\n\n1:53:31.500 --> 1:53:34.620\n They have kind of a share, a lot of the fundamental structure,\n\n1:53:34.620 --> 1:53:38.220\n you know, this problem. There's, you know, sequential,\n\n1:53:38.220 --> 1:53:41.900\n you know, nature. There's a lot of structure in this representation.\n\n1:53:41.900 --> 1:53:45.900\n There is a strong locality, kind of like in sentences, you know, words that follow\n\n1:53:45.900 --> 1:53:48.140\n each other. They're strongly connected, but there's\n\n1:53:48.140 --> 1:53:51.580\n also kind of larger context that doesn't have that locality, and you also see that\n\n1:53:51.580 --> 1:53:53.740\n in driving, right? What, you know, is happening in the scene\n\n1:53:53.740 --> 1:53:57.020\n as a whole has very strong implications on,\n\n1:53:57.020 --> 1:54:00.940\n you know, the kind of the next step in that sequence where\n\n1:54:00.940 --> 1:54:03.980\n whether you're, you know, predicting what other people are going to do, whether\n\n1:54:03.980 --> 1:54:07.020\n you're making your own decisions, or whether in the simulator you're\n\n1:54:07.020 --> 1:54:10.620\n building generative models of, you know, humans walking, cyclists\n\n1:54:10.620 --> 1:54:14.220\n riding, and other cars driving. That's all really fascinating, like how\n\n1:54:14.220 --> 1:54:17.340\n it's fascinating to think that transformer models and all this,\n\n1:54:17.340 --> 1:54:21.900\n all the breakthroughs in language and NLP that might be applicable to like\n\n1:54:21.900 --> 1:54:24.620\n driving at the higher level, at the behavioral level, that's kind of\n\n1:54:24.620 --> 1:54:27.900\n fascinating. Let me ask about pesky little creatures\n\n1:54:27.900 --> 1:54:32.620\n called pedestrians and cyclists. They seem, so humans are a problem. If we\n\n1:54:32.620 --> 1:54:36.940\n can get rid of them, I would. But unfortunately, they're all sort of\n\n1:54:36.940 --> 1:54:39.980\n a source of joy and love and beauty, so let's keep them around.\n\n1:54:39.980 --> 1:54:43.340\n They're also our customers. For your perspective, yes, yes,\n\n1:54:43.340 --> 1:54:46.620\n for sure. They're a source of money, very good.\n\n1:54:46.620 --> 1:54:52.300\n But I don't even know where I was going. Oh yes,\n\n1:54:52.300 --> 1:54:57.260\n pedestrians and cyclists, you know,\n\n1:54:57.260 --> 1:55:00.620\n they're a fascinating injection into the system of\n\n1:55:00.620 --> 1:55:09.020\n uncertainty of like a game theoretic dance of what to do. And also\n\n1:55:09.020 --> 1:55:13.420\n they have perceptions of their own, and they can tweet\n\n1:55:13.420 --> 1:55:17.500\n about your product, so you don't want to run them over\n\n1:55:17.500 --> 1:55:21.580\n from that perspective. I mean, I don't know, I'm joking a lot, but\n\n1:55:21.580 --> 1:55:27.340\n I think in seriousness, like, you know, pedestrians are a complicated\n\n1:55:27.340 --> 1:55:31.340\n computer vision problem, a complicated behavioral problem. Is there something\n\n1:55:31.340 --> 1:55:34.140\n interesting you could say about what you've learned\n\n1:55:34.140 --> 1:55:38.380\n from a machine learning perspective, from also an autonomous vehicle,\n\n1:55:38.380 --> 1:55:42.140\n and a product perspective about just interacting with the humans in this\n\n1:55:42.140 --> 1:55:45.180\n world? Yeah, just to state on record, we care\n\n1:55:45.180 --> 1:55:48.380\n deeply about the safety of pedestrians, you know, even the ones that don't have\n\n1:55:48.380 --> 1:55:52.940\n Twitter accounts. Thank you. All right, cool.\n\n1:55:52.940 --> 1:55:57.500\n Not me. But yes, I'm glad, I'm glad somebody does.\n\n1:55:57.500 --> 1:56:01.340\n Okay. But you know, in all seriousness, safety\n\n1:56:01.340 --> 1:56:07.260\n of vulnerable road users, pedestrians or cyclists, is one of our\n\n1:56:07.260 --> 1:56:12.220\n highest priorities. We do a tremendous amount of testing\n\n1:56:12.220 --> 1:56:16.220\n and validation, and put a very significant emphasis\n\n1:56:16.220 --> 1:56:20.540\n on, you know, the capabilities of our systems that have to do with safety\n\n1:56:20.540 --> 1:56:23.820\n around those unprotected vulnerable road users.\n\n1:56:23.820 --> 1:56:27.660\n You know, cars, just, you know, discussed earlier in Phoenix, we have completely\n\n1:56:27.660 --> 1:56:31.740\n empty cars, completely driverless cars, you know, driving in this very large area,\n\n1:56:31.740 --> 1:56:35.260\n and you know, some people use them to, you know, go to school, so they'll drive\n\n1:56:35.260 --> 1:56:39.660\n through school zones, right? So, kids are kind of the very special\n\n1:56:39.660 --> 1:56:42.220\n class of those vulnerable user road users, right? You want to be,\n\n1:56:42.220 --> 1:56:45.980\n you know, super, super safe, and super, super cautious around those. So, we take\n\n1:56:45.980 --> 1:56:50.460\n it very, very, very seriously. And you know, what does it take to\n\n1:56:50.460 --> 1:56:55.180\n be good at it? You know,\n\n1:56:55.180 --> 1:57:02.060\n an incredible amount of performance across your whole stack. You know,\n\n1:57:02.060 --> 1:57:05.820\n starts with hardware, and again, you want to use all\n\n1:57:05.820 --> 1:57:09.500\n sensing modalities available to you. Imagine driving on a residential road\n\n1:57:09.500 --> 1:57:13.100\n at night, and kind of making a turn, and you don't have, you know, headlights\n\n1:57:13.100 --> 1:57:16.220\n covering some part of the space, and like, you know, a kid might\n\n1:57:16.220 --> 1:57:20.620\n run out. And you know, lighters are amazing at that. They\n\n1:57:20.620 --> 1:57:24.300\n see just as well in complete darkness as they do during the day, right? So, just\n\n1:57:24.300 --> 1:57:27.900\n again, it gives you that extra,\n\n1:57:27.900 --> 1:57:32.540\n you know, margin in terms of, you know, capability, and performance, and safety,\n\n1:57:32.540 --> 1:57:35.420\n and quality. And in fact, we oftentimes, in these\n\n1:57:35.420 --> 1:57:38.460\n kinds of situations, we have our system detect something,\n\n1:57:38.460 --> 1:57:42.140\n in some cases even earlier than our trained operators in the car might do,\n\n1:57:42.140 --> 1:57:46.620\n right? Especially, you know, in conditions like, you know, very dark nights.\n\n1:57:46.620 --> 1:57:50.380\n So, starts with sensing, then, you know, perception\n\n1:57:50.380 --> 1:57:54.300\n has to be incredibly good. And you have to be very, very good\n\n1:57:54.300 --> 1:58:00.780\n at kind of detecting pedestrians in all kinds of situations, and all kinds\n\n1:58:00.780 --> 1:58:03.580\n of environments, including, you know, people in weird poses,\n\n1:58:03.580 --> 1:58:09.900\n people kind of running around, and you know, being partially occluded.\n\n1:58:09.900 --> 1:58:13.180\n So, you know, that's step number one, right?\n\n1:58:13.180 --> 1:58:17.580\n Then, you have to have in very high accuracy,\n\n1:58:17.580 --> 1:58:21.180\n and very low latency, in terms of your reactions\n\n1:58:21.180 --> 1:58:27.500\n to, you know, what, you know, these actors might do, right? And we've put a\n\n1:58:27.500 --> 1:58:30.780\n tremendous amount of engineering, and tremendous amount of validation, in to\n\n1:58:30.780 --> 1:58:35.020\n make sure our system performs properly. And, you know, oftentimes, it\n\n1:58:35.020 --> 1:58:38.140\n does require a very strong reaction to do the safe thing. And, you know, we\n\n1:58:38.140 --> 1:58:41.820\n actually see a lot of cases like that. That's the long tail of really rare,\n\n1:58:41.820 --> 1:58:48.620\n you know, really, you know, crazy events that contribute to the safety\n\n1:58:48.620 --> 1:58:52.300\n around pedestrians. Like, one example that comes to mind, that we actually\n\n1:58:52.300 --> 1:58:56.940\n happened in Phoenix, where we were driving\n\n1:58:56.940 --> 1:59:00.540\n along, and I think it was a 45 mile per hour road, so you have pretty high speed\n\n1:59:00.540 --> 1:59:03.420\n traffic, and there was a sidewalk next to it, and\n\n1:59:03.420 --> 1:59:09.100\n there was a cyclist on the sidewalk. And as we were in the right lane,\n\n1:59:09.100 --> 1:59:13.260\n right next to the side, so it was a multi lane road, so as we got close\n\n1:59:13.260 --> 1:59:17.180\n to the cyclist on the sidewalk, it was a woman, you know, she tripped and fell.\n\n1:59:17.180 --> 1:59:20.540\n Just, you know, fell right into the path of our vehicle, right?\n\n1:59:20.540 --> 1:59:25.820\n And our, you know, car, you know, this was actually with a\n\n1:59:25.820 --> 1:59:29.820\n test driver, our test drivers, did exactly the right thing.\n\n1:59:29.820 --> 1:59:33.100\n They kind of reacted, and came to stop. It requires both very strong steering,\n\n1:59:33.100 --> 1:59:37.020\n and, you know, strong application of the brake. And then we simulated what our\n\n1:59:37.020 --> 1:59:39.260\n system would have done in that situation, and it did, you know,\n\n1:59:39.260 --> 1:59:43.180\n exactly the same thing. And that speaks to, you know, all of\n\n1:59:43.180 --> 1:59:46.620\n those components of really good state estimation and\n\n1:59:46.620 --> 1:59:49.020\n tracking. And, like, imagine, you know, a person\n\n1:59:49.020 --> 1:59:52.140\n on a bike, and they're falling over, and they're doing that right in front of you,\n\n1:59:52.140 --> 1:59:54.300\n right? So you have to be really, like, things are changing. The appearance of\n\n1:59:54.300 --> 1:59:57.820\n that whole thing is changing, right? And a person goes one way, they're falling on\n\n1:59:57.820 --> 2:00:00.380\n the road, they're, you know, being flat on the ground in front of\n\n2:00:00.380 --> 2:00:03.340\n you. You know, the bike goes flying the other direction.\n\n2:00:03.340 --> 2:00:06.060\n Like, the two objects that used to be one, they're now, you know,\n\n2:00:06.060 --> 2:00:09.020\n are splitting apart, and the car has to, like, detect all of that.\n\n2:00:09.020 --> 2:00:12.620\n Like, milliseconds matter, and it doesn't, you know, it's not good enough to just\n\n2:00:12.620 --> 2:00:15.660\n brake. You have to, like, steer and brake, and there's traffic around you.\n\n2:00:15.660 --> 2:00:19.180\n So, like, it all has to come together, and it was really great\n\n2:00:19.180 --> 2:00:22.060\n to see in this case, and other cases like that, that we're actually seeing in the\n\n2:00:22.060 --> 2:00:25.100\n wild, that our system is, you know, performing\n\n2:00:25.100 --> 2:00:28.620\n exactly the way that we would have liked, and is able to,\n\n2:00:28.620 --> 2:00:30.620\n you know, avoid collisions like this.\n\n2:00:30.620 --> 2:00:32.780\n That's such an exciting space for robotics.\n\n2:00:32.780 --> 2:00:37.500\n Like, in that split second to make decisions of life and death.\n\n2:00:37.500 --> 2:00:41.580\n I don't know. The stakes are high, in a sense, but it's also beautiful\n\n2:00:41.580 --> 2:00:47.020\n that for somebody who loves artificial intelligence, the possibility that an AI\n\n2:00:47.020 --> 2:00:49.980\n system might be able to save a human life.\n\n2:00:49.980 --> 2:00:53.740\n That's kind of exciting as a problem, like, to wake up.\n\n2:00:53.740 --> 2:00:57.420\n It's terrifying, probably, for an engineer to wake up,\n\n2:00:57.420 --> 2:01:01.020\n and to think about, but it's also exciting because it's, like,\n\n2:01:01.020 --> 2:01:05.420\n it's in your hands. Let me try to ask a question that's often brought up about\n\n2:01:05.420 --> 2:01:09.420\n autonomous vehicles, and it might be fun to see if you have\n\n2:01:09.420 --> 2:01:14.620\n anything interesting to say, which is about the trolley problem.\n\n2:01:14.620 --> 2:01:19.260\n So, a trolley problem is an interesting philosophical construct\n\n2:01:19.260 --> 2:01:23.260\n that highlights, and there's many others like it,\n\n2:01:23.260 --> 2:01:29.900\n of the difficult ethical decisions that we humans have before us in this\n\n2:01:29.900 --> 2:01:34.060\n complicated world. So, specifically is the choice\n\n2:01:34.060 --> 2:01:39.020\n between if you are forced to choose to kill\n\n2:01:39.020 --> 2:01:42.700\n a group X of people versus a group Y of people, like\n\n2:01:42.700 --> 2:01:48.220\n one person. If you did nothing, you would kill one person, but if\n\n2:01:48.220 --> 2:01:51.340\n you would kill five people, and if you decide to swerve out of the way, you\n\n2:01:51.340 --> 2:01:55.180\n would only kill one person. Do you do nothing, or you choose to do\n\n2:01:55.180 --> 2:01:58.060\n something? You can construct all kinds of, sort of,\n\n2:01:58.060 --> 2:02:05.500\n ethical experiments of this kind that, I think, at least on a positive note,\n\n2:02:05.500 --> 2:02:09.660\n inspire you to think about, like, introspect\n\n2:02:09.660 --> 2:02:16.220\n what are the physics of our morality, and there's usually not\n\n2:02:16.220 --> 2:02:20.700\n good answers there. I think people love it because it's just an exciting\n\n2:02:20.700 --> 2:02:24.060\n thing to think about. I think people who build autonomous\n\n2:02:24.060 --> 2:02:30.060\n vehicles usually roll their eyes, because this is not,\n\n2:02:30.060 --> 2:02:34.060\n this one as constructed, this, like, literally never comes up\n\n2:02:34.060 --> 2:02:38.300\n in reality. You never have to choose between killing\n\n2:02:38.300 --> 2:02:41.660\n one or, like, one of two groups of people,\n\n2:02:41.660 --> 2:02:48.780\n but I wonder if you can speak to, is there some something interesting\n\n2:02:48.780 --> 2:02:52.620\n to you as an engineer of autonomous vehicles that's within the trolley\n\n2:02:52.620 --> 2:02:55.740\n problem, or maybe more generally, are there\n\n2:02:55.740 --> 2:02:58.940\n difficult ethical decisions that you find\n\n2:02:58.940 --> 2:03:03.340\n that an algorithm must make? On the specific version of the trolley problem,\n\n2:03:03.340 --> 2:03:07.900\n which one would you do, if you're driving? The question itself\n\n2:03:07.900 --> 2:03:11.340\n is a profound question, because we humans ourselves\n\n2:03:11.340 --> 2:03:18.700\n cannot answer, and that's the very point. I would kill both.\n\n2:03:18.700 --> 2:03:21.340\n Yeah, humans, I think you're exactly right in that, you know, humans are not\n\n2:03:21.340 --> 2:03:24.460\n particularly good. I think they're kind of phrased as, like, what would a computer do,\n\n2:03:24.460 --> 2:03:28.540\n but, like, humans, you know, are not very good, and actually oftentimes\n\n2:03:28.540 --> 2:03:32.620\n I think that, you know, freezing and kind of not doing anything, because,\n\n2:03:32.620 --> 2:03:35.500\n like, you've taken a few extra milliseconds to just process, and then\n\n2:03:35.500 --> 2:03:38.700\n you end up, like, doing the worst of the possible outcomes, right? So,\n\n2:03:38.700 --> 2:03:42.220\n I do think that, as you've pointed out, it can be\n\n2:03:42.220 --> 2:03:45.660\n a bit of a distraction, and it can be a bit of a kind of red herring. I think\n\n2:03:45.660 --> 2:03:47.820\n it's an interesting, you know, discussion\n\n2:03:47.820 --> 2:03:51.580\n in the realm of philosophy, right? But in terms of\n\n2:03:51.580 --> 2:03:54.780\n what, you know, how that affects the actual\n\n2:03:54.780 --> 2:03:57.660\n engineering and deployment of self driving vehicles,\n\n2:03:57.660 --> 2:04:02.780\n it's not how you go about building a system, right? We've talked\n\n2:04:02.780 --> 2:04:06.460\n about how you engineer a system, how you, you know, go about evaluating\n\n2:04:06.460 --> 2:04:09.820\n the different components and, you know, the safety of the entire thing.\n\n2:04:09.820 --> 2:04:13.740\n How do you kind of inject the, you know, various\n\n2:04:13.740 --> 2:04:17.580\n model based, safety based arguments, and, like, yes, you reason at parts of the\n\n2:04:17.580 --> 2:04:20.540\n system, you know, you reason about the\n\n2:04:20.540 --> 2:04:24.220\n probability of a collision, the severity of that collision, right?\n\n2:04:24.220 --> 2:04:27.180\n And that is incorporated, and there's, you know, you have to properly reason\n\n2:04:27.180 --> 2:04:29.500\n about the uncertainty that flows through the system, right? So,\n\n2:04:29.500 --> 2:04:34.540\n you know, those, you know, factors definitely play a role in how\n\n2:04:34.540 --> 2:04:36.700\n the cars then behave, but they tend to be more\n\n2:04:36.700 --> 2:04:39.740\n of, like, the emergent behavior. And what you see, like, you're absolutely right\n\n2:04:39.740 --> 2:04:43.740\n that these, you know, clear theoretical problems that they, you\n\n2:04:43.740 --> 2:04:46.940\n know, you don't encounter that in the system, and really kind of being\n\n2:04:46.940 --> 2:04:49.980\n back to our previous discussion of, like, what, you know, what, you\n\n2:04:49.980 --> 2:04:53.900\n know, which one do you choose? Well, you know, oftentimes, like,\n\n2:04:53.900 --> 2:04:57.420\n you made a mistake earlier. Like, you shouldn't be in that situation\n\n2:04:57.420 --> 2:05:00.620\n in the first place, right? And in reality, the system comes up.\n\n2:05:00.620 --> 2:05:03.740\n If you build a very good, safe, and capable driver,\n\n2:05:03.740 --> 2:05:08.380\n you have enough, you know, clues in the environment that you\n\n2:05:08.380 --> 2:05:11.340\n drive defensively, so you don't put yourself in that situation, right? And\n\n2:05:11.340 --> 2:05:14.060\n again, you know, it has, you know, this, if you go back to that analogy of, you\n\n2:05:14.060 --> 2:05:16.860\n know, precision and recoil, like, okay, you can make a, you know, very hard trade\n\n2:05:16.860 --> 2:05:19.500\n off, but like, neither answer is really good.\n\n2:05:19.500 --> 2:05:22.460\n But what instead you focus on is kind of moving\n\n2:05:22.460 --> 2:05:26.140\n the whole curve up, and then you focus on building the right capability on the\n\n2:05:26.140 --> 2:05:28.620\n right defensive driving, so that, you know, you don't put yourself in the\n\n2:05:28.620 --> 2:05:32.380\n situation like this. I don't know if you have a good answer\n\n2:05:32.380 --> 2:05:35.420\n for this, but people love it when I ask this question\n\n2:05:35.420 --> 2:05:42.460\n about books. Are there books in your life that you've enjoyed,\n\n2:05:42.460 --> 2:05:47.100\n philosophical, fiction, technical, that had a big impact on you as an engineer or\n\n2:05:47.100 --> 2:05:50.300\n as a human being? You know, everything from science fiction\n\n2:05:50.300 --> 2:05:53.500\n to a favorite textbook. Is there three books that stand out that\n\n2:05:53.500 --> 2:05:57.340\n you can think of? Three books. So I would, you know, that\n\n2:05:57.340 --> 2:06:02.860\n impacted me, I would say,\n\n2:06:02.860 --> 2:06:06.380\n and this one is, you probably know it well,\n\n2:06:06.380 --> 2:06:11.420\n but not generally well known, I think, in the U.S., or kind of\n\n2:06:11.420 --> 2:06:16.620\n internationally, The Master and Margarita. It's one of, actually, my\n\n2:06:16.620 --> 2:06:20.860\n favorite books. It is, you know, by\n\n2:06:20.860 --> 2:06:26.300\n Russian, it's a novel by Russian author Mikhail Bulgakov, and it's just, it's a\n\n2:06:26.300 --> 2:06:28.220\n great book. It's one of those books that you can, like,\n\n2:06:28.220 --> 2:06:32.300\n reread your entire life, and it's very accessible. You can read it as a kid,\n\n2:06:32.300 --> 2:06:35.900\n and, like, it's, you know, the plot is interesting. It's, you know, the\n\n2:06:35.900 --> 2:06:38.140\n devil, you know, visiting the Soviet Union,\n\n2:06:38.140 --> 2:06:41.980\n and, you know, but it, like, you read it, reread it\n\n2:06:41.980 --> 2:06:46.060\n at different stages of your life, and you enjoy it for\n\n2:06:46.060 --> 2:06:49.580\n different, very different reasons, and you keep finding, like, deeper and deeper\n\n2:06:49.580 --> 2:06:52.220\n meaning, and, you know, kind of affected, you know,\n\n2:06:52.220 --> 2:06:57.580\n had a, definitely had an, like, imprint on me, you know, mostly from the,\n\n2:06:57.580 --> 2:07:00.940\n probably kind of the cultural, stylistic aspect. Like, it makes you think one of\n\n2:07:00.940 --> 2:07:04.300\n those books that, you know, is good and makes you think, but also has,\n\n2:07:04.300 --> 2:07:07.740\n like, this really, you know, silly, quirky, dark sense of, you know,\n\n2:07:07.740 --> 2:07:10.140\n humor. It captures the Russian soul more than\n\n2:07:10.140 --> 2:07:13.020\n many, perhaps, many other books. On that, like, slight note,\n\n2:07:13.020 --> 2:07:17.180\n just out of curiosity, one of the saddest things is I've read that book\n\n2:07:17.180 --> 2:07:22.460\n in English. Did you, by chance, read it in English or in Russian?\n\n2:07:22.460 --> 2:07:26.060\n In Russian, only in Russian, and I actually, that is a question I had,\n\n2:07:26.060 --> 2:07:30.780\n kind of posed to myself every once in a while, like, I wonder how well it\n\n2:07:30.780 --> 2:07:33.420\n translates, if it translates at all, and there's the\n\n2:07:33.420 --> 2:07:35.980\n language aspect of it, and then there's the cultural aspect, so\n\n2:07:35.980 --> 2:07:39.260\n I, actually, I'm not sure if, you know, either of those would\n\n2:07:39.260 --> 2:07:43.740\n work well in English. Now, I forget their names, but, so, when the COVID lifts a\n\n2:07:43.740 --> 2:07:48.780\n little bit, I'm traveling to Paris for several reasons. One is just, I've\n\n2:07:48.780 --> 2:07:50.700\n never been to Paris, I want to go to Paris, but\n\n2:07:50.700 --> 2:07:57.020\n there's the most famous translators of Dostoevsky, Tolstoy, of most of\n\n2:07:57.020 --> 2:08:00.540\n Russian literature live there. There's a couple, they're famous,\n\n2:08:00.540 --> 2:08:03.980\n a man and a woman, and I'm going to, sort of, have a series of conversations with\n\n2:08:03.980 --> 2:08:06.780\n them, and in preparation for that, I'm starting\n\n2:08:06.780 --> 2:08:10.380\n to read Dostoevsky in Russian, so I'm really embarrassed to say that I read\n\n2:08:10.380 --> 2:08:13.820\n this, everything I've read in Russian literature of, like,\n\n2:08:13.820 --> 2:08:18.540\n serious depth has been in English, even though\n\n2:08:18.540 --> 2:08:21.820\n I can also read, I mean, obviously, in Russian, but\n\n2:08:21.820 --> 2:08:26.540\n for some reason, it seemed,\n\n2:08:26.940 --> 2:08:31.420\n in the optimization of life, it seemed the improper decision to do, to read in\n\n2:08:31.420 --> 2:08:35.020\n Russian, like, you know, like, I don't need to,\n\n2:08:35.020 --> 2:08:38.700\n I need to think in English, not in Russian, but now I'm changing my mind on\n\n2:08:38.700 --> 2:08:41.340\n that, and so, the question of how well I translate, it's a\n\n2:08:41.340 --> 2:08:43.900\n really fun to method one, like, even with Dostoevsky.\n\n2:08:43.900 --> 2:08:47.340\n So, from what I understand, Dostoevsky translates easier,\n\n2:08:47.340 --> 2:08:52.380\n others don't as much. Obviously, the poetry doesn't translate as well,\n\n2:08:52.380 --> 2:08:57.740\n I'm also the music big fan of Vladimir Vosotsky,\n\n2:08:57.740 --> 2:09:02.700\n he doesn't obviously translate well, people have tried,\n\n2:09:02.700 --> 2:09:06.300\n but mastermind, I don't know, I don't know about that one, I just know in\n\n2:09:06.300 --> 2:09:10.140\n English, you know, as fun as hell in English, so, so, but\n\n2:09:10.140 --> 2:09:13.340\n it's a curious question, and I want to study it rigorously from both the\n\n2:09:13.340 --> 2:09:16.940\n machine learning aspect, and also because I want to do a\n\n2:09:16.940 --> 2:09:21.980\n couple of interviews in Russia, that\n\n2:09:21.980 --> 2:09:27.100\n I'm still unsure of how to properly conduct an interview\n\n2:09:27.100 --> 2:09:30.380\n across a language barrier, it's a fascinating question\n\n2:09:30.380 --> 2:09:34.060\n that ultimately communicates to an American audience. There's a few\n\n2:09:34.060 --> 2:09:39.260\n Russian people that I think are truly special human beings,\n\n2:09:39.260 --> 2:09:44.780\n and I feel, like, I sometimes encounter this with some\n\n2:09:44.780 --> 2:09:48.300\n incredible scientists, and maybe you encounter this\n\n2:09:48.300 --> 2:09:52.780\n as well at some point in your life, that it feels like because of the language\n\n2:09:52.780 --> 2:09:57.660\n barrier, their ideas are lost to history. It's a sad thing, I think about, like,\n\n2:09:57.660 --> 2:10:01.500\n Chinese scientists, or even authors that, like,\n\n2:10:01.500 --> 2:10:05.820\n that we don't, in an English speaking world, don't get to appreciate\n\n2:10:05.820 --> 2:10:09.180\n some, like, the depth of the culture because it's lost in translation,\n\n2:10:09.180 --> 2:10:13.260\n and I feel like I would love to show that to the world,\n\n2:10:13.260 --> 2:10:16.940\n like, I'm just some idiot, but because I have this,\n\n2:10:16.940 --> 2:10:20.860\n like, at least some semblance of skill in speaking Russian,\n\n2:10:20.860 --> 2:10:25.020\n I feel like, and I know how to record stuff on a video camera,\n\n2:10:25.020 --> 2:10:28.700\n I feel like I want to catch, like, Grigori Perlman, who's a mathematician, I'm not\n\n2:10:28.700 --> 2:10:31.740\n sure if you're familiar with him, I want to talk to him, like, he's a\n\n2:10:31.740 --> 2:10:35.980\n fascinating mind, and to bring him to a wider audience in English speaking\n\n2:10:35.980 --> 2:10:40.060\n will be fascinating, but that requires to be rigorous about this question\n\n2:10:40.060 --> 2:10:46.380\n of how well Bulgakov translates. I mean, I know it's a silly\n\n2:10:46.380 --> 2:10:50.940\n concept, but it's a fundamental one, because how do you translate, and\n\n2:10:50.940 --> 2:10:54.940\n that's the thing that Google Translate is also facing\n\n2:10:54.940 --> 2:10:59.020\n as a more machine learning problem, but I wonder as a more\n\n2:10:59.020 --> 2:11:03.020\n bigger problem for AI, how do we capture the magic\n\n2:11:03.020 --> 2:11:08.860\n that's there in the language? I think that's a really interesting,\n\n2:11:08.860 --> 2:11:12.540\n really challenging problem. If you do read it, Master and Margarita\n\n2:11:12.540 --> 2:11:16.700\n in English, sorry, in Russian, I'd be curious\n\n2:11:16.700 --> 2:11:20.620\n to get your opinion, and I think part of it is language, but part of it's just,\n\n2:11:20.620 --> 2:11:23.260\n you know, centuries of culture, that, you know, the cultures are\n\n2:11:23.260 --> 2:11:28.060\n different, so it's hard to connect that.\n\n2:11:28.060 --> 2:11:31.420\n Okay, so that was my first one, right? You had two more. The second one I\n\n2:11:31.420 --> 2:11:35.660\n would probably pick is the science fiction by the\n\n2:11:35.660 --> 2:11:38.460\n Strogatsky brothers. You know, it's up there with, you know,\n\n2:11:38.460 --> 2:11:43.340\n Isaac Asimov and, you know, Ray Bradbury and, you know, company. The\n\n2:11:43.340 --> 2:11:47.740\n Strogatsky brothers kind of appealed more to me. I think it made more of an\n\n2:11:47.740 --> 2:11:53.500\n impression on me growing up. I apologize if I'm\n\n2:11:53.500 --> 2:11:57.100\n showing my complete ignorance. I'm so weak on sci fi. What did\n\n2:11:57.100 --> 2:12:04.060\n they write? Oh, Roadside Picnic,\n\n2:12:04.060 --> 2:12:07.580\n Heart to Be a God,\n\n2:12:07.580 --> 2:12:14.700\n Beetle in an Ant Hill, Monday Starts on Saturday. Like, it's\n\n2:12:14.700 --> 2:12:17.500\n not just science fiction. It also has very interesting, you know,\n\n2:12:17.500 --> 2:12:21.580\n interpersonal and societal questions, and some of the\n\n2:12:21.580 --> 2:12:27.820\n language is just completely hilarious.\n\n2:12:27.820 --> 2:12:31.500\n That's the one. Oh, interesting. Monday Starts on Saturday. So,\n\n2:12:31.500 --> 2:12:36.300\n I need to read. Okay, oh boy. You put that in the category of science fiction?\n\n2:12:36.300 --> 2:12:39.900\n That one is, I mean, this was more of a silly,\n\n2:12:39.900 --> 2:12:43.260\n you know, humorous work. I mean, there is kind of...\n\n2:12:43.260 --> 2:12:46.380\n It's profound too, right? Science fiction, right? It's about, you know, this\n\n2:12:46.380 --> 2:12:50.620\n research institute, and it has deep parallels to\n\n2:12:50.620 --> 2:12:53.660\n serious research, but the setting, of course,\n\n2:12:53.660 --> 2:12:56.380\n is that they're working on, you know, magic, right? And there's a\n\n2:12:56.380 --> 2:13:00.300\n lot of stuff. And that's their style, right?\n\n2:13:00.300 --> 2:13:03.260\n And, you know, other books are very different, right? You know,\n\n2:13:03.260 --> 2:13:07.100\n Heart to Be a God, right? It's about kind of this higher society being injected\n\n2:13:07.100 --> 2:13:09.660\n into this primitive world, and how they operate there,\n\n2:13:09.660 --> 2:13:13.420\n and some of the very deep ethical questions there,\n\n2:13:13.420 --> 2:13:16.540\n right? And, like, they've got this full spectrum. Some is, you know, more about\n\n2:13:16.540 --> 2:13:19.580\n kind of more adventure style. But, like, I enjoy all of\n\n2:13:19.580 --> 2:13:21.820\n their books. There's just, you know, probably a couple.\n\n2:13:21.820 --> 2:13:24.780\n Actually, one I think that they consider their most important work.\n\n2:13:24.780 --> 2:13:29.660\n I think it's The Snail on a Hill. I'm not exactly sure how it\n\n2:13:29.660 --> 2:13:32.620\n translates. I tried reading a couple times. I still don't get it.\n\n2:13:32.620 --> 2:13:36.540\n But everything else I fully enjoyed. And, like, for one of my birthdays as a kid, I\n\n2:13:36.540 --> 2:13:40.060\n got, like, their entire collection, like, occupied a giant shelf in my room, and\n\n2:13:40.060 --> 2:13:42.220\n then, like, over the holidays, I just, like,\n\n2:13:42.220 --> 2:13:44.700\n you know, my parents couldn't drag me out of the room, and I read the whole thing\n\n2:13:44.700 --> 2:13:49.500\n cover to cover. And I really enjoyed it.\n\n2:13:49.500 --> 2:13:52.540\n And that's one more. For the third one, you know, maybe a little bit\n\n2:13:52.540 --> 2:13:56.700\n darker, but, you know, comes to mind is Orwell's\n\n2:13:56.700 --> 2:14:01.180\n 1984. And, you know, you asked what made an\n\n2:14:01.180 --> 2:14:03.900\n impression on me and the books that people should read. That one, I think,\n\n2:14:03.900 --> 2:14:06.860\n falls in the category of both. You know, definitely it's one of those\n\n2:14:06.860 --> 2:14:11.100\n books that you read, and you just kind of, you know, put it\n\n2:14:11.100 --> 2:14:16.460\n down and you stare in space for a while. You know, that kind of work. I think\n\n2:14:16.460 --> 2:14:19.980\n there's, you know, lessons there. People should\n\n2:14:19.980 --> 2:14:24.220\n not ignore. And, you know, nowadays, with, like,\n\n2:14:24.220 --> 2:14:26.060\n everything that's happening in the world, I,\n\n2:14:26.060 --> 2:14:29.420\n like, can't help it, but, you know, have my mind jump to some,\n\n2:14:29.420 --> 2:14:34.220\n you know, parallels with what Orwell described. And, like, there's this whole,\n\n2:14:34.220 --> 2:14:38.460\n you know, concept of double think and ignoring logic and, you know, holding\n\n2:14:38.460 --> 2:14:41.820\n completely contradictory opinions in your mind and not have that not bother\n\n2:14:41.820 --> 2:14:44.140\n you and, you know, sticking to the party line\n\n2:14:44.140 --> 2:14:48.220\n at all costs. Like, you know, there's something there.\n\n2:14:48.220 --> 2:14:52.940\n If anything, 2020 has taught me, and I'm a huge fan of Animal Farm, which is a\n\n2:14:52.940 --> 2:14:57.900\n kind of friendly, as a friend of 1984 by Orwell.\n\n2:14:57.900 --> 2:15:03.660\n It's kind of another thought experiment of how our society\n\n2:15:03.660 --> 2:15:07.340\n may go in directions that we wouldn't like it to go.\n\n2:15:07.340 --> 2:15:14.300\n But if anything that's been kind of heartbreaking to an\n\n2:15:14.300 --> 2:15:18.860\n optimist about 2020 is that\n\n2:15:18.940 --> 2:15:22.140\n that society is kind of fragile. Like, we have this,\n\n2:15:22.140 --> 2:15:25.900\n this is a special little experiment we have going on.\n\n2:15:25.900 --> 2:15:32.300\n And not, it's not unbreakable. Like, we should be careful to, like, preserve\n\n2:15:32.300 --> 2:15:36.380\n whatever the special thing we have going on. I mean, I think 1984\n\n2:15:36.380 --> 2:15:39.820\n and these books, The Brave New World, they're\n\n2:15:39.820 --> 2:15:43.660\n helpful in thinking, like, stuff can go wrong\n\n2:15:43.660 --> 2:15:48.380\n in nonobvious ways. And it's, like, it's up to us to preserve it.\n\n2:15:48.380 --> 2:15:51.980\n And it's, like, it's a responsibility. It's been weighing heavy on me because, like,\n\n2:15:51.980 --> 2:15:57.580\n for some reason, like, more than my mom follows me on Twitter and I\n\n2:15:57.580 --> 2:15:59.980\n feel like I have, like, now somehow a\n\n2:15:59.980 --> 2:16:03.100\n responsibility to\n\n2:16:03.100 --> 2:16:07.980\n do this world. And it dawned on me that, like,\n\n2:16:07.980 --> 2:16:12.300\n me and millions of others are, like, the little ants\n\n2:16:12.300 --> 2:16:17.020\n that maintain this little colony, right? So we have a responsibility not to\n\n2:16:17.020 --> 2:16:20.060\n be, I don't know what the right analogy is, but\n\n2:16:20.060 --> 2:16:23.420\n I'll put a flamethrower to the place. We want to\n\n2:16:23.420 --> 2:16:27.900\n not do that. And there's interesting complicated ways of doing that as 1984\n\n2:16:27.900 --> 2:16:29.820\n shows. It could be through bureaucracy. It could\n\n2:16:29.820 --> 2:16:33.180\n be through incompetence. It could be through misinformation.\n\n2:16:33.180 --> 2:16:36.460\n It could be through division and toxicity.\n\n2:16:36.460 --> 2:16:39.980\n I'm a huge believer in, like, that love will be\n\n2:16:39.980 --> 2:16:46.460\n the, somehow, the solution. So, love and robots. Love and robots, yeah.\n\n2:16:46.460 --> 2:16:49.340\n I think you're exactly right. Unfortunately, I think it's less of a\n\n2:16:49.340 --> 2:16:51.980\n flamethrower type of thing. It's more of a,\n\n2:16:51.980 --> 2:16:55.100\n in many cases, it's going to be more of a slow boil. And that's the\n\n2:16:55.100 --> 2:17:00.220\n danger. Let me ask, it's a fun thing to make\n\n2:17:00.220 --> 2:17:05.100\n a world class roboticist, engineer, and leader uncomfortable with a\n\n2:17:05.100 --> 2:17:09.660\n ridiculous question about life. What is the meaning of life,\n\n2:17:09.660 --> 2:17:14.700\n Dimitri, from a robotics and a human perspective?\n\n2:17:14.700 --> 2:17:19.500\n You only have a couple minutes, or one minute to answer, so.\n\n2:17:19.820 --> 2:17:23.180\n I don't know if that makes it more difficult or easier, actually.\n\n2:17:23.180 --> 2:17:29.740\n You know, they're very tempted to quote one of the\n\n2:17:29.740 --> 2:17:36.060\n stories by Isaac Asimov, actually. Actually, titled,\n\n2:17:36.060 --> 2:17:39.900\n appropriately titled, The Last Question. It's a short story where, you know, the\n\n2:17:39.900 --> 2:17:42.860\n plot is that, you know, humans build this supercomputer,\n\n2:17:42.860 --> 2:17:46.220\n you know, this AI intelligence, and, you know, once it\n\n2:17:46.220 --> 2:17:49.660\n gets powerful enough, they pose this question to it, you know,\n\n2:17:49.660 --> 2:17:54.380\n how can the entropy in the universe be reduced, right? So the computer replies,\n\n2:17:54.380 --> 2:17:58.140\n as of yet, insufficient information to give a meaningful answer,\n\n2:17:58.140 --> 2:18:00.940\n right? And then, you know, thousands of years go by, and they keep posing the\n\n2:18:00.940 --> 2:18:03.980\n same question, and the computer, you know, gets more and more powerful, and keeps\n\n2:18:03.980 --> 2:18:06.540\n giving the same answer, you know, as of yet, insufficient\n\n2:18:06.540 --> 2:18:09.580\n information to give a meaningful answer, or something along those lines,\n\n2:18:09.580 --> 2:18:12.940\n right? And then, you know, it keeps, you know, happening, and\n\n2:18:12.940 --> 2:18:16.060\n happening, you fast forward, like, millions of years into the future, and,\n\n2:18:16.060 --> 2:18:19.100\n you know, billions of years, and, like, at some point, it's just the only entity in\n\n2:18:19.100 --> 2:18:21.580\n the universe, it's, like, absorbed all humanity,\n\n2:18:21.580 --> 2:18:24.460\n and all knowledge in the universe, and it, like, keeps posing the same question\n\n2:18:24.460 --> 2:18:28.700\n to itself, and, you know, finally, it gets to the\n\n2:18:28.700 --> 2:18:31.900\n point where it is able to answer that question, but, of course, at that point,\n\n2:18:31.900 --> 2:18:34.700\n you know, there's, you know, the heat death of the universe has occurred, and\n\n2:18:34.700 --> 2:18:37.500\n that's the only entity, and there's nobody else to provide that\n\n2:18:37.500 --> 2:18:40.140\n answer to, so the only thing it can do is to,\n\n2:18:40.140 --> 2:18:43.980\n you know, answer it by demonstration, so, like, you know, it recreates the big bang,\n\n2:18:43.980 --> 2:18:47.100\n right, and resets the clock, right?\n\n2:18:47.100 --> 2:18:50.540\n But, like, you know, I can try to give kind of a\n\n2:18:50.540 --> 2:18:53.340\n different version of the answer, you know, maybe\n\n2:18:53.340 --> 2:18:56.780\n not on the behalf of all humanity, I think that that might be a little\n\n2:18:56.780 --> 2:19:00.300\n presumptuous for me to speak about the meaning of life on the behalf of all\n\n2:19:00.300 --> 2:19:03.420\n humans, but at least, you know, personally,\n\n2:19:03.420 --> 2:19:06.940\n it changes, right? I think if you think about kind of what\n\n2:19:06.940 --> 2:19:13.660\n gives, you know, you and your life meaning and purpose, and kind of\n\n2:19:13.660 --> 2:19:18.460\n what drives you, it seems to\n\n2:19:18.460 --> 2:19:22.060\n change over time, right, and that lifespan\n\n2:19:22.060 --> 2:19:25.180\n of, you know, kind of your existence, you know, when\n\n2:19:25.180 --> 2:19:27.980\n just when you just enter this world, right, it's all about kind of new\n\n2:19:27.980 --> 2:19:33.180\n experiences, right? You get, like, new smells, new sounds, new emotions, right,\n\n2:19:33.180 --> 2:19:36.380\n and, like, that's what's driving you, right? You're experiencing\n\n2:19:36.380 --> 2:19:40.140\n new amazing things, right, and that's magical, right? That's pretty\n\n2:19:40.140 --> 2:19:43.100\n pretty awesome, right? That gives you kind of meaning.\n\n2:19:43.100 --> 2:19:47.740\n Then, you know, you get a little bit older, you start more intentionally\n\n2:19:47.740 --> 2:19:51.020\n learning about things, right? I guess, actually, before you start intentionally\n\n2:19:51.020 --> 2:19:53.740\n learning, it's probably fun. Fun is a thing that gives you kind of\n\n2:19:53.740 --> 2:19:56.780\n meaning and purpose and purpose and the thing you optimize for, right?\n\n2:19:56.780 --> 2:20:01.020\n And, like, fun is good. Then you get, you know, start learning, and I guess that\n\n2:20:01.020 --> 2:20:05.660\n this joy of comprehension\n\n2:20:05.660 --> 2:20:09.500\n and discovery is another thing that, you know, gives you\n\n2:20:09.500 --> 2:20:12.940\n meaning and purpose and drives you, right? Then, you know, you\n\n2:20:12.940 --> 2:20:17.420\n learn enough stuff and you want to give some of it back, right? And so\n\n2:20:17.420 --> 2:20:20.460\n impact and contributions back to, you know, technology or society,\n\n2:20:20.460 --> 2:20:24.860\n you know, people, you know, local or more globally\n\n2:20:24.860 --> 2:20:28.620\n becomes a new thing that, you know, drives a lot of kind of your behavior\n\n2:20:28.620 --> 2:20:31.900\n and is something that gives you purpose and\n\n2:20:31.900 --> 2:20:35.260\n that you derive, you know, positive feedback from, right?\n\n2:20:35.260 --> 2:20:38.460\n You know, then you go and so on and so forth. You go through various stages of\n\n2:20:38.460 --> 2:20:43.420\n life. If you have kids,\n\n2:20:43.420 --> 2:20:46.220\n like, that definitely changes your perspective on things. You know, I have\n\n2:20:46.220 --> 2:20:48.940\n three that definitely flips some bits in your\n\n2:20:48.940 --> 2:20:52.220\n head in terms of, you know, what you care about and what you\n\n2:20:52.220 --> 2:20:54.940\n optimize for and, you know, what matters, what doesn't matter, right?\n\n2:20:54.940 --> 2:20:58.140\n So, you know, and so on and so forth, right? And I,\n\n2:20:58.140 --> 2:21:02.380\n it seems to me that, you know, it's all of those things and as\n\n2:21:02.380 --> 2:21:06.700\n kind of you go through life, you know,\n\n2:21:06.700 --> 2:21:10.140\n you want these to be additive, right? New experiences,\n\n2:21:10.140 --> 2:21:14.460\n fun, learning, impact. Like, you want to, you know, be accumulating.\n\n2:21:14.460 --> 2:21:17.820\n I don't want to, you know, stop having fun or, you know, experiencing new things and\n\n2:21:17.820 --> 2:21:20.300\n I think it's important that, you know, it just kind of becomes\n\n2:21:20.300 --> 2:21:23.660\n additive as opposed to a replacement or subtraction.\n\n2:21:23.660 --> 2:21:27.500\n But, you know, those fewest problems as far as I got, but, you know, ask me in a\n\n2:21:27.500 --> 2:21:30.220\n few years, I might have one or two more to add to the list.\n\n2:21:30.220 --> 2:21:34.540\n And before you know it, time is up, just like it is for this conversation,\n\n2:21:34.540 --> 2:21:38.460\n but hopefully it was a fun ride. It was a huge honor to meet you.\n\n2:21:38.460 --> 2:21:43.900\n As you know, I've been a fan of yours and a fan of Google Self Driving Car and\n\n2:21:43.900 --> 2:21:47.420\n Waymo for a long time. I can't wait. I mean, it's one of the\n\n2:21:47.420 --> 2:21:50.300\n most exciting, if we look back in the 21st century, I\n\n2:21:50.300 --> 2:21:53.180\n truly believe it'll be one of the most exciting things we\n\n2:21:53.180 --> 2:21:57.100\n descendants of apes have created on this earth. So,\n\n2:21:57.100 --> 2:22:00.540\n I'm a huge fan and I can't wait to see what you do\n\n2:22:00.540 --> 2:22:04.460\n next. Thanks so much for talking to me. Thanks, thanks for having me and it's a\n\n2:22:04.460 --> 2:22:08.540\n also a huge fan doing work, honestly, and I really\n\n2:22:08.540 --> 2:22:11.260\n enjoyed it. Thank you. Thanks for listening to this\n\n2:22:11.260 --> 2:22:14.620\n conversation with Dmitry Dolgov and thank you to our sponsors,\n\n2:22:14.620 --> 2:22:19.340\n Triolabs, a company that helps businesses apply machine learning to\n\n2:22:19.340 --> 2:22:23.100\n solve real world problems, Blinkist, an app I use for reading\n\n2:22:23.100 --> 2:22:27.420\n through summaries of books, BetterHelp, online therapy with a licensed\n\n2:22:27.420 --> 2:22:30.860\n professional, and CashApp, the app I use to send money to\n\n2:22:30.860 --> 2:22:33.260\n friends. Please check out these sponsors in the\n\n2:22:33.260 --> 2:22:37.180\n description to get a discount and to support this podcast. If you\n\n2:22:37.180 --> 2:22:40.380\n enjoy this thing, subscribe on YouTube, review it with Five Stars\n\n2:22:40.380 --> 2:22:44.140\n and Upper Podcast, follow on Spotify, support on Patreon,\n\n2:22:44.140 --> 2:22:47.740\n or connect with me on Twitter at Lex Friedman. And now,\n\n2:22:47.740 --> 2:22:51.420\n let me leave you with some words from Isaac Asimov.\n\n2:22:51.420 --> 2:22:55.660\n Science can amuse and fascinate us all, but it is engineering\n\n2:22:55.660 --> 2:22:59.980\n that changes the world. Thank you for listening and hope to see you\n\n2:22:59.980 --> 2:23:04.060\n next time.\n\n"
}