{
  "title": "Ishan Misra: Self-Supervised Deep Learning in Computer Vision | Lex Fridman Podcast #206",
  "id": "FUS6ceIvUnI",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:03.240\n The following is a conversation with Eshan Mizra,\n\n00:03.240 --> 00:05.800\n research scientist at Facebook AI Research,\n\n00:05.800 --> 00:08.580\n who works on self supervised machine learning\n\n00:08.580 --> 00:10.480\n in the domain of computer vision,\n\n00:10.480 --> 00:14.120\n or in other words, making AI systems understand\n\n00:14.120 --> 00:18.000\n the visual world with minimal help from us humans.\n\n00:18.000 --> 00:21.720\n Transformers and self attention has been successfully used\n\n00:21.720 --> 00:25.600\n by OpenAI's DPT3 and other language models\n\n00:25.600 --> 00:28.560\n to do self supervised learning in the domain of language.\n\n00:28.560 --> 00:31.800\n Eshan, together with Yann LeCun and others,\n\n00:31.800 --> 00:33.960\n is trying to achieve the same success\n\n00:33.960 --> 00:36.400\n in the domain of images and video.\n\n00:36.400 --> 00:38.320\n The goal is to leave a robot\n\n00:38.320 --> 00:40.360\n watching YouTube videos all night,\n\n00:40.360 --> 00:43.600\n and in the morning, come back to a much smarter robot.\n\n00:43.600 --> 00:46.000\n I read the blog post, Self Supervised Learning,\n\n00:46.000 --> 00:50.360\n The Dark Matter of Intelligence by Eshan and Yann LeCun,\n\n00:50.360 --> 00:52.960\n and then listened to Eshan's appearance\n\n00:52.960 --> 00:57.200\n on the excellent Machine Learning Street Talk podcast,\n\n00:57.200 --> 00:59.160\n and I knew I had to talk to him.\n\n00:59.160 --> 01:02.860\n By the way, if you're interested in machine learning and AI,\n\n01:02.860 --> 01:07.860\n I cannot recommend the ML Street Talk podcast highly enough.\n\n01:07.980 --> 01:09.640\n Those guys are great.\n\n01:09.640 --> 01:11.280\n Quick mention of our sponsors.\n\n01:11.280 --> 01:15.400\n Onnit, The Information, Grammarly, and Athletic Greens.\n\n01:15.400 --> 01:18.640\n Check them out in the description to support this podcast.\n\n01:18.640 --> 01:20.480\n As a side note, let me say that,\n\n01:20.480 --> 01:22.560\n for those of you who may have been listening\n\n01:22.560 --> 01:24.960\n for quite a while, this podcast used to be called\n\n01:24.960 --> 01:27.120\n Artificial Intelligence Podcast,\n\n01:27.120 --> 01:29.700\n because my life passion has always been,\n\n01:29.700 --> 01:32.640\n will always be artificial intelligence,\n\n01:32.640 --> 01:35.440\n both narrowly and broadly defined.\n\n01:35.440 --> 01:37.720\n My goal with this podcast is still\n\n01:37.720 --> 01:40.560\n to have many conversations with world class researchers\n\n01:40.560 --> 01:45.120\n in AI, math, physics, biology, and all the other sciences,\n\n01:45.120 --> 01:49.420\n but I also want to talk to historians, musicians, athletes,\n\n01:49.420 --> 01:51.520\n and of course, occasionally comedians.\n\n01:51.520 --> 01:53.600\n In fact, I'm trying out doing this podcast\n\n01:53.600 --> 01:56.200\n three times a week now to give me more freedom\n\n01:56.200 --> 01:59.380\n with guest selection and maybe get a chance\n\n01:59.380 --> 02:00.880\n to have a bit more fun.\n\n02:00.880 --> 02:03.160\n Speaking of fun, in this conversation,\n\n02:03.160 --> 02:05.440\n I challenge the listener to count the number of times\n\n02:05.440 --> 02:08.000\n the word banana is mentioned.\n\n02:08.000 --> 02:12.580\n Ishan and I use the word banana as the canonical example\n\n02:12.580 --> 02:15.200\n at the core of the hard problem of computer vision\n\n02:15.200 --> 02:19.000\n and maybe the hard problem of consciousness.\n\n02:19.880 --> 02:22.640\n This is the Lex Friedman Podcast,\n\n02:22.640 --> 02:26.300\n and here is my conversation with Ishan Mizra.\n\n02:27.240 --> 02:29.880\n What is self supervised learning?\n\n02:29.880 --> 02:32.760\n And maybe even give the bigger basics\n\n02:32.760 --> 02:35.360\n of what is supervised and semi supervised learning,\n\n02:35.360 --> 02:37.640\n and maybe why is self supervised learning\n\n02:37.640 --> 02:40.080\n a better term than unsupervised learning?\n\n02:40.080 --> 02:41.600\n Let's start with supervised learning.\n\n02:41.600 --> 02:43.920\n So typically for machine learning systems,\n\n02:43.920 --> 02:46.920\n the way they're trained is you get a bunch of humans,\n\n02:46.920 --> 02:48.600\n the humans point out particular concepts.\n\n02:48.600 --> 02:50.180\n So if it's in the case of images,\n\n02:50.180 --> 02:52.960\n you want the humans to come and tell you\n\n02:52.960 --> 02:54.400\n what is present in the image,\n\n02:54.400 --> 02:57.240\n draw boxes around them, draw masks of like things,\n\n02:57.240 --> 03:00.520\n pixels, which are of particular categories or not.\n\n03:00.520 --> 03:01.960\n For NLP, again, there are like lots\n\n03:01.960 --> 03:04.760\n of these particular tasks, say about sentiment analysis,\n\n03:04.760 --> 03:06.620\n about entailment and so on.\n\n03:06.620 --> 03:08.080\n So typically for supervised learning,\n\n03:08.080 --> 03:11.280\n we get a big corpus of such annotated or labeled data.\n\n03:11.280 --> 03:12.780\n And then we feed that to a system\n\n03:12.780 --> 03:14.820\n and the system is really trying to mimic.\n\n03:14.820 --> 03:16.600\n So it's taking this input of the data\n\n03:16.600 --> 03:18.360\n and then trying to mimic the output.\n\n03:18.360 --> 03:20.680\n So it looks at an image and the human has tagged\n\n03:20.680 --> 03:22.400\n that this image contains a banana.\n\n03:22.400 --> 03:24.680\n And now the system is basically trying to mimic that.\n\n03:24.680 --> 03:26.680\n So that's its learning signal.\n\n03:26.680 --> 03:28.000\n And so for supervised learning,\n\n03:28.000 --> 03:30.040\n we try to gather lots of such data\n\n03:30.040 --> 03:31.820\n and we train these machine learning models\n\n03:31.820 --> 03:33.460\n to imitate the input output.\n\n03:33.460 --> 03:35.600\n And the hope is basically by doing so,\n\n03:35.600 --> 03:38.080\n now on unseen or like new kinds of data,\n\n03:38.080 --> 03:40.000\n this model can automatically learn\n\n03:40.000 --> 03:41.320\n to predict these concepts.\n\n03:41.320 --> 03:43.400\n So this is a standard sort of supervised setting.\n\n03:43.400 --> 03:45.760\n For semi supervised setting,\n\n03:45.760 --> 03:47.600\n the idea typically is that you have,\n\n03:47.600 --> 03:49.280\n of course, all of the supervised data,\n\n03:49.280 --> 03:50.800\n but you have lots of other data,\n\n03:50.800 --> 03:53.120\n which is unsupervised or which is like not labeled.\n\n03:53.120 --> 03:55.280\n Now, the problem basically with supervised learning\n\n03:55.280 --> 03:57.440\n and why you actually have all of these alternate\n\n03:57.440 --> 03:59.400\n sort of learning paradigms is,\n\n03:59.400 --> 04:01.800\n supervised learning just does not scale.\n\n04:01.800 --> 04:03.900\n So if you look at for computer vision,\n\n04:03.900 --> 04:05.000\n the sort of largest,\n\n04:05.000 --> 04:07.500\n one of the most popular data sets is ImageNet, right?\n\n04:07.500 --> 04:11.680\n So the entire ImageNet data set has about 22,000 concepts\n\n04:11.680 --> 04:13.800\n and about 14 million images.\n\n04:13.800 --> 04:16.160\n So these concepts are basically just nouns\n\n04:16.160 --> 04:18.360\n and they're annotated on images.\n\n04:18.360 --> 04:20.600\n And this entire data set was a mammoth data collection\n\n04:20.600 --> 04:22.320\n effort that actually gave rise\n\n04:22.320 --> 04:23.840\n to a lot of powerful learning algorithms\n\n04:23.840 --> 04:25.640\n is credited with like sort of the rise\n\n04:25.640 --> 04:27.240\n of deep learning as well.\n\n04:27.240 --> 04:30.140\n But this data set took about 22 human years\n\n04:30.140 --> 04:31.960\n to collect, to annotate.\n\n04:31.960 --> 04:33.520\n And it's not even that many concepts, right?\n\n04:33.520 --> 04:34.580\n It's not even that many images,\n\n04:34.580 --> 04:36.800\n 14 million is nothing really.\n\n04:36.800 --> 04:39.360\n Like you have about, I think 400 million images or so,\n\n04:39.360 --> 04:41.920\n or even more than that uploaded to most of the popular\n\n04:41.920 --> 04:44.200\n sort of social media websites today.\n\n04:44.200 --> 04:46.440\n So now supervised learning just doesn't scale.\n\n04:46.440 --> 04:48.680\n If I want to now annotate more concepts,\n\n04:48.680 --> 04:51.340\n if I want to have various types of fine grained concepts,\n\n04:51.340 --> 04:53.240\n then it won't really scale.\n\n04:53.240 --> 04:54.880\n So now you come up to these sort of different\n\n04:54.880 --> 04:57.560\n learning paradigms, for example, semi supervised learning,\n\n04:57.560 --> 04:58.600\n where the idea is you, of course,\n\n04:58.600 --> 05:01.400\n you have this annotated corpus of supervised data\n\n05:01.400 --> 05:03.720\n and you have lots of these unlabeled images.\n\n05:03.720 --> 05:05.860\n And the idea is that the algorithm should basically try\n\n05:05.860 --> 05:08.000\n to measure some kind of consistency\n\n05:08.000 --> 05:10.320\n or really try to measure some kind of signal\n\n05:10.320 --> 05:12.200\n on this sort of unlabeled data\n\n05:12.200 --> 05:14.200\n to make itself more confident\n\n05:14.200 --> 05:16.200\n about what it's really trying to predict.\n\n05:16.200 --> 05:19.680\n So by access to this, lots of unlabeled data,\n\n05:19.680 --> 05:22.240\n the idea is that the algorithm actually learns\n\n05:22.240 --> 05:24.560\n to be more confident and actually gets better\n\n05:24.560 --> 05:26.000\n at predicting these concepts.\n\n05:26.920 --> 05:28.520\n And now we come to the other extreme,\n\n05:28.520 --> 05:30.520\n which is like self supervised learning.\n\n05:30.520 --> 05:33.040\n The idea basically is that the machine or the algorithm\n\n05:33.040 --> 05:35.660\n should really discover concepts or discover things\n\n05:35.660 --> 05:38.200\n about the world or learn representations about the world\n\n05:38.200 --> 05:40.080\n which are useful without access\n\n05:40.080 --> 05:41.800\n to explicit human supervision.\n\n05:41.800 --> 05:44.360\n So the word supervision is still\n\n05:44.360 --> 05:46.280\n in the term self supervised.\n\n05:46.280 --> 05:48.560\n So what is the supervision signal?\n\n05:48.560 --> 05:51.240\n And maybe that perhaps is when Yann LeCun\n\n05:51.240 --> 05:52.920\n and you argue that unsupervised\n\n05:52.920 --> 05:55.040\n is the incorrect terminology here.\n\n05:55.040 --> 05:57.440\n So what is the supervision signal\n\n05:57.440 --> 05:59.720\n when the humans aren't part of the picture\n\n05:59.720 --> 06:02.400\n or not a big part of the picture?\n\n06:02.400 --> 06:04.520\n Right, so self supervised,\n\n06:04.520 --> 06:06.840\n the reason that it has the term supervised in itself\n\n06:06.840 --> 06:10.360\n is because you're using the data itself as supervision.\n\n06:10.360 --> 06:13.200\n So because the data serves as its own source of supervision,\n\n06:13.200 --> 06:15.160\n it's self supervised in that way.\n\n06:15.160 --> 06:16.400\n Now, the reason a lot of people,\n\n06:16.400 --> 06:18.380\n I mean, we did it in that blog post with Yann,\n\n06:18.380 --> 06:20.120\n but a lot of other people have also argued\n\n06:20.120 --> 06:22.080\n for using this term self supervised.\n\n06:22.080 --> 06:25.680\n So starting from like 94 from Virginia Desas group,\n\n06:25.680 --> 06:28.800\n I think UCSD, and now she's at UCSD.\n\n06:28.800 --> 06:31.640\n Jeetendra Malik has said this a bunch of times as well.\n\n06:31.640 --> 06:33.080\n So you have supervised,\n\n06:33.080 --> 06:35.200\n and then unsupervised basically means everything\n\n06:35.200 --> 06:36.400\n which is not supervised,\n\n06:36.400 --> 06:38.640\n but that includes stuff like semi supervised,\n\n06:38.640 --> 06:41.280\n that includes other like transductive learning,\n\n06:41.280 --> 06:43.000\n lots of other sort of settings.\n\n06:43.000 --> 06:46.040\n So that's the reason like now people are preferring\n\n06:46.040 --> 06:47.120\n this term self supervised\n\n06:47.120 --> 06:49.240\n because it explicitly says what's happening.\n\n06:49.240 --> 06:51.620\n The data itself is the source of supervision\n\n06:51.620 --> 06:53.120\n and any sort of learning algorithm\n\n06:53.120 --> 06:56.920\n which tries to extract just sort of data supervision signals\n\n06:56.920 --> 06:59.480\n from the data itself is a self supervised algorithm.\n\n06:59.480 --> 07:02.160\n But there is within the data,\n\n07:02.160 --> 07:05.560\n a set of tricks which unlock the supervision.\n\n07:05.560 --> 07:07.200\n So can you give maybe some examples\n\n07:07.200 --> 07:11.360\n and there's innovation ingenuity required\n\n07:11.360 --> 07:12.840\n to unlock that supervision.\n\n07:12.840 --> 07:15.600\n The data doesn't just speak to you some ground truth,\n\n07:15.600 --> 07:17.760\n you have to do some kind of trick.\n\n07:17.760 --> 07:19.560\n So I don't know what your favorite domain is.\n\n07:19.560 --> 07:23.000\n So you specifically specialize in visual learning,\n\n07:23.000 --> 07:24.480\n but is there favorite examples,\n\n07:24.480 --> 07:26.520\n maybe in language or other domains?\n\n07:26.520 --> 07:28.300\n Perhaps the most successful applications\n\n07:28.300 --> 07:31.060\n have been in NLP, not language processing.\n\n07:31.060 --> 07:34.000\n So the idea basically being that you can train models\n\n07:34.000 --> 07:37.360\n that can you have a sentence and you mask out certain words.\n\n07:37.360 --> 07:40.500\n And now these models learn to predict the masked out words.\n\n07:40.500 --> 07:44.000\n So if you have like the cat jumped over the dog,\n\n07:44.000 --> 07:45.940\n so you can basically mask out cat.\n\n07:45.940 --> 07:47.360\n And now you're essentially asking the model\n\n07:47.360 --> 07:50.280\n to predict what was missing, what did I mask out?\n\n07:50.280 --> 07:53.220\n So the model is going to predict basically a distribution\n\n07:53.220 --> 07:55.320\n over all the possible words that it knows.\n\n07:55.320 --> 07:58.360\n And probably it has like if it's a well trained model,\n\n07:58.360 --> 08:00.580\n it has a sort of higher probability density\n\n08:00.580 --> 08:02.560\n for this word cat.\n\n08:02.560 --> 08:05.520\n For vision, I would say the sort of more,\n\n08:05.520 --> 08:07.480\n I mean, the easier example,\n\n08:07.480 --> 08:09.420\n which is not as widely used these days,\n\n08:09.420 --> 08:12.040\n is basically say, for example, video prediction.\n\n08:12.040 --> 08:14.080\n So video is again, a sequence of things.\n\n08:14.080 --> 08:15.040\n So you can ask the model,\n\n08:15.040 --> 08:17.440\n so if you have a video of say 10 seconds,\n\n08:17.440 --> 08:19.840\n you can feed in the first nine seconds to a model\n\n08:19.840 --> 08:21.960\n and then ask it, hey, what happens basically\n\n08:21.960 --> 08:24.500\n in the 10 second, can you predict what's going to happen?\n\n08:24.500 --> 08:26.760\n And the idea basically is because the model\n\n08:26.760 --> 08:29.440\n is predicting something about the data itself.\n\n08:29.440 --> 08:31.380\n Of course, you didn't need any human\n\n08:31.380 --> 08:32.300\n to tell you what was happening\n\n08:32.300 --> 08:34.600\n because the 10 second video was naturally captured.\n\n08:34.600 --> 08:36.680\n Because the model is predicting what's happening there,\n\n08:36.680 --> 08:39.020\n it's going to automatically learn something\n\n08:39.020 --> 08:41.240\n about the structure of the world, how objects move,\n\n08:41.240 --> 08:44.000\n object permanence, and these kinds of things.\n\n08:44.000 --> 08:45.960\n So like, if I have something at the edge of the table,\n\n08:45.960 --> 08:47.520\n it will fall down.\n\n08:47.520 --> 08:49.280\n Things like these, which you really don't have to sit\n\n08:49.280 --> 08:50.280\n and annotate.\n\n08:50.280 --> 08:51.320\n In a supervised learning setting,\n\n08:51.320 --> 08:52.280\n I would have to sit and annotate.\n\n08:52.280 --> 08:55.200\n This is a cup, now I move this cup, this is still a cup,\n\n08:55.200 --> 08:56.640\n and now I move this cup, it's still a cup,\n\n08:56.640 --> 08:58.840\n and then it falls down, and this is a fallen down cup.\n\n08:58.840 --> 09:00.440\n So I won't have to annotate all of these things\n\n09:00.440 --> 09:02.040\n in a self supervised setting.\n\n09:02.040 --> 09:05.280\n Isn't that kind of a brilliant little trick\n\n09:05.280 --> 09:08.320\n of taking a series of data that is consistent\n\n09:08.320 --> 09:11.920\n and removing one element in that series,\n\n09:11.920 --> 09:16.920\n and then teaching the algorithm to predict that element?\n\n09:17.040 --> 09:19.660\n Isn't that, first of all, that's quite brilliant.\n\n09:20.700 --> 09:23.080\n It seems to be applicable in anything\n\n09:23.080 --> 09:27.920\n that has the constraint of being a sequence\n\n09:27.920 --> 09:30.260\n that is consistent with the physical reality.\n\n09:30.260 --> 09:34.400\n The question is, are there other tricks like this\n\n09:34.400 --> 09:37.840\n that can generate the self supervision signal?\n\n09:37.840 --> 09:41.200\n So sequence is possibly the most widely used one in NLP.\n\n09:41.200 --> 09:44.080\n For vision, the one that is actually used for images,\n\n09:44.080 --> 09:45.840\n which is very popular these days,\n\n09:45.840 --> 09:47.600\n is basically taking an image,\n\n09:47.600 --> 09:50.080\n and now taking different crops of that image.\n\n09:50.080 --> 09:51.400\n So you can basically decide to crop,\n\n09:51.400 --> 09:53.100\n say the top left corner,\n\n09:53.100 --> 09:55.280\n and you crop, say the bottom right corner,\n\n09:55.280 --> 09:58.960\n and asking a network to basically present it with a choice,\n\n09:58.960 --> 10:01.360\n saying that, okay, now you have this image,\n\n10:01.360 --> 10:04.480\n you have this image, are these the same or not?\n\n10:04.480 --> 10:06.680\n And so the idea basically is that because different crop,\n\n10:06.680 --> 10:08.480\n like in an image, different parts of the image\n\n10:08.480 --> 10:09.800\n are going to be related.\n\n10:09.800 --> 10:12.420\n So for example, if you have a chair and a table,\n\n10:12.420 --> 10:14.960\n basically these things are going to be close by,\n\n10:14.960 --> 10:16.860\n versus if you take, again,\n\n10:16.860 --> 10:19.520\n if you have like a zoomed in picture of a chair,\n\n10:19.520 --> 10:20.480\n if you're taking different crops,\n\n10:20.480 --> 10:22.340\n it's going to be different parts of the chair.\n\n10:22.340 --> 10:25.020\n So the idea basically is that different crops\n\n10:25.020 --> 10:26.180\n of the image are related,\n\n10:26.180 --> 10:27.900\n and so the features or the representations\n\n10:27.900 --> 10:29.080\n that you get from these different crops\n\n10:29.080 --> 10:30.320\n should also be related.\n\n10:30.320 --> 10:32.720\n So this is possibly the most like widely used trick\n\n10:32.720 --> 10:35.760\n these days for self supervised learning and computer vision.\n\n10:35.760 --> 10:39.080\n So again, using the consistency that's inherent\n\n10:39.080 --> 10:42.000\n to physical reality in visual domain,\n\n10:42.000 --> 10:45.640\n that's, you know, parts of an image are consistent,\n\n10:45.640 --> 10:48.400\n and then in the language domain,\n\n10:48.400 --> 10:50.280\n or anything that has sequences,\n\n10:50.280 --> 10:53.000\n like language or something that's like a time series,\n\n10:53.000 --> 10:55.440\n then you can chop up parts in time.\n\n10:55.440 --> 10:59.400\n It's similar to the story of RNNs and CNNs,\n\n11:00.280 --> 11:02.300\n of RNNs and ConvNets.\n\n11:02.300 --> 11:06.640\n You and Yann LeCun wrote the blog post in March, 2021,\n\n11:06.640 --> 11:08.840\n titled, Self Supervised Learning,\n\n11:08.840 --> 11:11.080\n The Dark Matter of Intelligence.\n\n11:11.080 --> 11:12.640\n Can you summarize this blog post\n\n11:12.640 --> 11:15.660\n and maybe explain the main idea or set of ideas?\n\n11:15.660 --> 11:18.680\n The blog post was mainly about sort of just telling,\n\n11:18.680 --> 11:21.680\n I mean, this is really a accepted fact,\n\n11:21.680 --> 11:22.940\n I would say for a lot of people now,\n\n11:22.940 --> 11:24.360\n that self supervised learning is something\n\n11:24.360 --> 11:27.200\n that is going to play an important role\n\n11:27.200 --> 11:28.320\n for machine learning algorithms\n\n11:28.320 --> 11:30.560\n that come in the future, and even now.\n\n11:30.560 --> 11:33.840\n Let me just comment that we don't yet\n\n11:33.840 --> 11:36.480\n have a good understanding of what dark matter is.\n\n11:36.480 --> 11:37.320\n That's true.\n\n11:37.320 --> 11:40.040\n So the idea basically being...\n\n11:40.040 --> 11:41.840\n So maybe the metaphor doesn't exactly transfer,\n\n11:41.840 --> 11:44.840\n but maybe it's actually perfectly transfers,\n\n11:44.840 --> 11:47.880\n that we don't know, we have an inkling\n\n11:47.880 --> 11:49.280\n that it'll be a big part\n\n11:49.280 --> 11:51.240\n of whatever solving intelligence looks like.\n\n11:51.240 --> 11:52.960\n Right, so I think self supervised learning,\n\n11:52.960 --> 11:54.880\n the way it's done right now is,\n\n11:54.880 --> 11:56.560\n I would say like the first step towards\n\n11:56.560 --> 11:58.600\n what it probably should end up like learning\n\n11:58.600 --> 12:00.540\n or what it should enable us to do.\n\n12:00.540 --> 12:03.760\n So the idea for that particular piece was,\n\n12:03.760 --> 12:06.200\n self supervised learning is going to be a very powerful way\n\n12:06.200 --> 12:08.420\n to learn common sense about the world,\n\n12:08.420 --> 12:10.840\n or like stuff that is really hard to label.\n\n12:10.840 --> 12:13.760\n For example, like is this piece\n\n12:13.760 --> 12:15.640\n over here heavier than the cup?\n\n12:15.640 --> 12:17.520\n Now, for all these kinds of things,\n\n12:17.520 --> 12:18.760\n you'll have to sit and label these things.\n\n12:18.760 --> 12:21.560\n So supervised learning is clearly not going to scale.\n\n12:21.560 --> 12:23.520\n So what is the thing that's actually going to scale?\n\n12:23.520 --> 12:25.060\n It's probably going to be an agent\n\n12:25.060 --> 12:27.920\n that can either actually interact with it to lift it up,\n\n12:27.920 --> 12:29.980\n or observe me doing it.\n\n12:29.980 --> 12:31.580\n So if I'm basically lifting these things up,\n\n12:31.580 --> 12:32.600\n it can probably reason about,\n\n12:32.600 --> 12:34.760\n hey, this is taking him more time to lift up,\n\n12:34.760 --> 12:36.440\n or the velocity is different,\n\n12:36.440 --> 12:37.840\n whereas the velocity for this is different,\n\n12:37.840 --> 12:39.600\n probably this one is heavier.\n\n12:39.600 --> 12:42.000\n So essentially, by observations of the data,\n\n12:42.000 --> 12:44.820\n you should be able to infer a lot of things about the world\n\n12:44.820 --> 12:46.840\n without someone explicitly telling you,\n\n12:46.840 --> 12:48.720\n this is heavy, this is not,\n\n12:48.720 --> 12:50.000\n this is something that can pour,\n\n12:50.000 --> 12:51.200\n this is something that cannot pour,\n\n12:51.200 --> 12:52.480\n this is somewhere that you can sit,\n\n12:52.480 --> 12:53.920\n this is not somewhere that you can sit.\n\n12:53.920 --> 12:57.360\n But you just mentioned ability to interact with the world.\n\n12:57.360 --> 13:01.000\n There's so many questions that are yet,\n\n13:01.000 --> 13:02.840\n that are still open, which is,\n\n13:02.840 --> 13:04.480\n how do you select the set of data\n\n13:04.480 --> 13:08.640\n over which the self supervised learning process works?\n\n13:08.640 --> 13:11.520\n How much interactivity like in the active learning\n\n13:11.520 --> 13:14.400\n or the machine teaching context is there?\n\n13:14.400 --> 13:16.480\n What are the reward signals?\n\n13:16.480 --> 13:18.560\n Like how much actual interaction there is\n\n13:18.560 --> 13:20.080\n with the physical world?\n\n13:20.080 --> 13:21.440\n That kind of thing.\n\n13:21.440 --> 13:24.800\n So that could be a huge question.\n\n13:24.800 --> 13:26.720\n And then on top of that,\n\n13:26.720 --> 13:28.960\n which I have a million questions about,\n\n13:28.960 --> 13:30.420\n which we don't know the answers to,\n\n13:30.420 --> 13:32.840\n but it's worth talking about is,\n\n13:32.840 --> 13:35.120\n how much reasoning is involved?\n\n13:35.120 --> 13:38.520\n How much accumulation of knowledge\n\n13:38.520 --> 13:40.800\n versus something that's more akin to learning\n\n13:40.800 --> 13:43.240\n or whether that's the same thing.\n\n13:43.240 --> 13:46.560\n But so we're like, it is truly dark matter.\n\n13:46.560 --> 13:49.220\n We don't know how exactly to do it.\n\n13:49.220 --> 13:52.040\n But we are, I mean, a lot of us are actually convinced\n\n13:52.040 --> 13:54.200\n that it's going to be a sort of major thing\n\n13:54.200 --> 13:55.040\n in machine learning.\n\n13:55.040 --> 13:56.600\n So let me reframe it then,\n\n13:56.600 --> 14:01.160\n that human supervision cannot be at large scale\n\n14:01.160 --> 14:04.120\n the source of the solution to intelligence.\n\n14:04.120 --> 14:08.000\n So the machines have to discover the supervision\n\n14:08.000 --> 14:10.240\n in the natural signal of the world.\n\n14:10.240 --> 14:11.560\n I mean, the other thing is also\n\n14:11.560 --> 14:14.200\n that humans are not particularly good labelers.\n\n14:14.200 --> 14:16.000\n They're not very consistent.\n\n14:16.000 --> 14:17.860\n For example, like what's the difference\n\n14:17.860 --> 14:19.840\n between a dining table and a table?\n\n14:19.840 --> 14:21.560\n Is it just the fact that one,\n\n14:21.560 --> 14:23.080\n like if you just look at a particular table,\n\n14:23.080 --> 14:24.600\n what makes us say one is dining table\n\n14:24.600 --> 14:26.500\n and the other is not?\n\n14:26.500 --> 14:28.160\n Humans are not particularly consistent.\n\n14:28.160 --> 14:30.100\n They're not like very good sources of supervision\n\n14:30.100 --> 14:32.320\n for a lot of these kinds of edge cases.\n\n14:32.320 --> 14:37.160\n So it may be also the fact that if we want an algorithm\n\n14:37.160 --> 14:39.640\n or want a machine to solve a particular task for us,\n\n14:39.640 --> 14:42.120\n we can maybe just specify the end goal\n\n14:42.120 --> 14:44.240\n and like the stuff in between,\n\n14:44.240 --> 14:46.080\n we really probably should not be specifying\n\n14:46.080 --> 14:49.320\n because we're not maybe going to confuse it a lot actually.\n\n14:49.320 --> 14:51.460\n Well, humans can't even answer the meaning of life.\n\n14:51.460 --> 14:53.920\n So I'm not sure if we're good supervisors\n\n14:53.920 --> 14:55.220\n of the end goal either.\n\n14:55.220 --> 14:56.960\n So let me ask you about categories.\n\n14:56.960 --> 14:59.040\n Humans are not very good at telling the difference\n\n14:59.040 --> 15:01.940\n between what is and isn't a table, like you mentioned.\n\n15:02.800 --> 15:04.520\n Do you think it's possible,\n\n15:04.520 --> 15:08.140\n let me ask you like pretend you're Plato.\n\n15:10.080 --> 15:14.800\n Is it possible to create a pretty good taxonomy\n\n15:14.800 --> 15:16.400\n of objects in the world?\n\n15:16.400 --> 15:19.000\n It seems like a lot of approaches in machine learning\n\n15:19.000 --> 15:21.400\n kind of assume a hopeful vision\n\n15:21.400 --> 15:24.080\n that it's possible to construct a perfect taxonomy\n\n15:24.080 --> 15:26.520\n or it exists perhaps out of our reach,\n\n15:26.520 --> 15:28.800\n but we can always get closer and closer to it.\n\n15:28.800 --> 15:31.240\n Or is that a hopeless pursuit?\n\n15:31.240 --> 15:33.040\n I think it's hopeless in some way.\n\n15:33.040 --> 15:36.080\n So the thing is for any particular categorization\n\n15:36.080 --> 15:36.920\n that you create,\n\n15:36.920 --> 15:38.760\n if you have a discrete sort of categorization,\n\n15:38.760 --> 15:40.520\n I can always take the nearest two concepts\n\n15:40.520 --> 15:42.600\n or I can take a third concept and I can blend it in\n\n15:42.600 --> 15:44.480\n and I can create a new category.\n\n15:44.480 --> 15:46.560\n So if you were to enumerate N categories,\n\n15:46.560 --> 15:48.880\n I will always find an N plus one category for you.\n\n15:48.880 --> 15:50.680\n That's not going to be in the N categories.\n\n15:50.680 --> 15:52.420\n And I can actually create not just N plus one,\n\n15:52.420 --> 15:55.120\n I can very easily create far more than N categories.\n\n15:55.120 --> 15:57.280\n The thing is a lot of things we talk about\n\n15:57.280 --> 15:58.960\n are actually compositional.\n\n15:58.960 --> 16:01.680\n So it's really hard for us to come and sit\n\n16:01.680 --> 16:03.200\n and enumerate all of these out.\n\n16:03.200 --> 16:05.840\n And they compose in various weird ways, right?\n\n16:05.840 --> 16:08.320\n Like you have like a croissant and a donut come together\n\n16:08.320 --> 16:09.680\n to form a cronut.\n\n16:09.680 --> 16:12.400\n So if you were to like enumerate all the foods up until,\n\n16:12.400 --> 16:15.160\n I don't know, whenever the cronut was about 10 years ago\n\n16:15.160 --> 16:16.440\n or 15 years ago,\n\n16:16.440 --> 16:19.000\n then this entire thing called cronut would not exist.\n\n16:19.000 --> 16:21.760\n Yeah, I remember there was the most awesome video\n\n16:21.760 --> 16:23.500\n of a cat wearing a monkey costume.\n\n16:23.500 --> 16:24.340\n Yeah, yes.\n\n16:26.520 --> 16:28.240\n People should look it up, it's great.\n\n16:28.240 --> 16:31.000\n So is that a monkey or is that a cat?\n\n16:31.000 --> 16:33.840\n It's a very difficult philosophical question.\n\n16:33.840 --> 16:37.280\n So there is a concept of similarity between objects.\n\n16:37.280 --> 16:39.860\n So you think that can take us very far?\n\n16:39.860 --> 16:43.200\n Just kind of getting a good function,\n\n16:43.200 --> 16:47.920\n a good way to tell which parts of things are similar\n\n16:47.920 --> 16:50.720\n and which parts of things are very different.\n\n16:50.720 --> 16:51.780\n I think so, yeah.\n\n16:51.780 --> 16:54.320\n So you don't necessarily need to name everything\n\n16:54.320 --> 16:57.840\n or assign a name to everything to be able to use it, right?\n\n16:57.840 --> 16:59.560\n So there are like lots of...\n\n16:59.560 --> 17:01.720\n Shakespeare said that, what's in a name?\n\n17:01.720 --> 17:03.200\n What's in a name, yeah, okay.\n\n17:03.200 --> 17:05.840\n And I mean, lots of like, for example, animals, right?\n\n17:05.840 --> 17:08.120\n They don't have necessarily a well formed\n\n17:08.120 --> 17:09.520\n like syntactic language,\n\n17:09.520 --> 17:11.800\n but they're able to go about their day perfectly.\n\n17:11.800 --> 17:12.880\n The same thing happens for us.\n\n17:12.880 --> 17:17.080\n So, I mean, we probably look at things and we figure out,\n\n17:17.080 --> 17:19.360\n oh, this is similar to something else that I've seen before.\n\n17:19.360 --> 17:22.000\n And then I can probably learn how to use it.\n\n17:22.000 --> 17:26.280\n So I haven't seen all the possible doorknobs in the world.\n\n17:26.280 --> 17:27.800\n But if you show me,\n\n17:27.800 --> 17:29.840\n like I was able to get into this particular place\n\n17:29.840 --> 17:32.120\n fairly easily, I've never seen that particular doorknob.\n\n17:32.120 --> 17:34.360\n So I of course related to all the doorknobs that I've seen\n\n17:34.360 --> 17:36.520\n and I know exactly how it's going to open.\n\n17:36.520 --> 17:39.440\n I have a pretty good idea of how it's going to open.\n\n17:39.440 --> 17:41.800\n And I think this kind of translation between experiences\n\n17:41.800 --> 17:43.720\n only happens because of similarity.\n\n17:43.720 --> 17:45.360\n Because I'm able to relate it to a doorknob.\n\n17:45.360 --> 17:46.600\n If I related it to a hairdryer,\n\n17:46.600 --> 17:50.400\n I would probably be stuck still outside, not able to get in.\n\n17:50.400 --> 17:52.240\n Again, a bit of a philosophical question,\n\n17:52.240 --> 17:55.600\n but can similarity take us all the way\n\n17:55.600 --> 17:57.680\n to understanding a thing?\n\n17:58.680 --> 18:01.940\n Can having a good function that compares objects\n\n18:01.940 --> 18:04.900\n get us to understand something profound\n\n18:04.900 --> 18:07.200\n about singular objects?\n\n18:07.200 --> 18:08.600\n I think I'll ask you a question back.\n\n18:08.600 --> 18:10.600\n What does it mean to understand objects?\n\n18:11.560 --> 18:13.520\n Well, let me tell you what that's similar to.\n\n18:13.520 --> 18:17.680\n No, so there's an idea of sort of reasoning\n\n18:17.680 --> 18:19.760\n by analogy kind of thing.\n\n18:19.760 --> 18:24.760\n I think understanding is the process of placing that thing\n\n18:24.920 --> 18:28.440\n in some kind of network of knowledge that you have.\n\n18:28.440 --> 18:33.160\n That it perhaps is fundamentally related to other concepts.\n\n18:33.160 --> 18:36.480\n So it's not like understanding is fundamentally related\n\n18:36.480 --> 18:39.280\n by composition of other concepts\n\n18:39.280 --> 18:41.480\n and maybe in relation to other concepts.\n\n18:43.160 --> 18:45.800\n And maybe deeper and deeper understanding\n\n18:45.800 --> 18:50.800\n is maybe just adding more edges to that graph somehow.\n\n18:51.840 --> 18:55.080\n So maybe it is a composition of similarities.\n\n18:55.080 --> 18:59.560\n I mean, ultimately, I suppose it is a kind of embedding\n\n18:59.560 --> 19:02.480\n in that wisdom space.\n\n19:02.480 --> 19:06.480\n Yeah, okay, wisdom space is good.\n\n19:06.480 --> 19:08.040\n I think, I do think, right?\n\n19:08.040 --> 19:10.720\n So similarity does get you very, very far.\n\n19:10.720 --> 19:12.320\n Is it the answer to everything?\n\n19:12.320 --> 19:14.120\n I mean, I don't even know what everything is,\n\n19:14.120 --> 19:16.680\n but it's going to take us really far.\n\n19:16.680 --> 19:19.640\n And I think the thing is things are similar\n\n19:19.640 --> 19:21.640\n in very different contexts, right?\n\n19:21.640 --> 19:24.320\n So an elephant is similar to, I don't know,\n\n19:24.320 --> 19:25.600\n another sort of wild animal.\n\n19:25.600 --> 19:28.500\n Let's just pick, I don't know, lion in a different way\n\n19:28.500 --> 19:30.520\n because they're both four legged creatures.\n\n19:30.520 --> 19:32.040\n They're also land animals.\n\n19:32.040 --> 19:33.120\n But of course they're very different\n\n19:33.120 --> 19:33.960\n in a lot of different ways.\n\n19:33.960 --> 19:37.240\n So elephants are like herbivores, lions are not.\n\n19:37.240 --> 19:40.660\n So similarity and particularly dissimilarity\n\n19:40.660 --> 19:43.720\n also actually helps us understand a lot about things.\n\n19:43.720 --> 19:45.200\n And so that's actually why I think\n\n19:45.200 --> 19:47.600\n discrete categorization is very hard.\n\n19:47.600 --> 19:50.060\n Just like forming this particular category of elephant\n\n19:50.060 --> 19:51.840\n and a particular category of lion,\n\n19:51.840 --> 19:54.360\n maybe it's good for just like taxonomy,\n\n19:54.360 --> 19:55.760\n biological taxonomies.\n\n19:55.760 --> 19:59.760\n But when it comes to other things which are not as maybe,\n\n19:59.760 --> 20:01.720\n for example, like grilled cheese, right?\n\n20:01.720 --> 20:02.560\n I have a grilled cheese,\n\n20:02.560 --> 20:03.960\n I dip it in tomato and I keep it outside.\n\n20:03.960 --> 20:05.040\n Now, is that still a grilled cheese\n\n20:05.040 --> 20:06.720\n or is that something else?\n\n20:06.720 --> 20:09.780\n Right, so categorization is still very useful\n\n20:09.780 --> 20:11.240\n for solving problems.\n\n20:11.240 --> 20:15.920\n But is your intuition then sort of the self supervised\n\n20:15.920 --> 20:20.880\n should be the, to borrow Jan Lekun's terminology,\n\n20:20.880 --> 20:23.640\n should be the cake and then categorization,\n\n20:23.640 --> 20:27.360\n the classification, maybe the supervised like layer\n\n20:27.360 --> 20:29.100\n should be just like the thing on top,\n\n20:29.100 --> 20:31.020\n the cherry or the icing or whatever.\n\n20:31.020 --> 20:32.920\n So if you make it the cake,\n\n20:32.920 --> 20:35.520\n it gets in the way of learning.\n\n20:35.520 --> 20:36.360\n If you make it the cake,\n\n20:36.360 --> 20:39.380\n then you won't be able to sit and annotate everything.\n\n20:39.380 --> 20:40.660\n That's as simple as it is.\n\n20:40.660 --> 20:43.080\n Like that's my very practical view on it.\n\n20:43.080 --> 20:44.920\n It's just, I mean, in my PhD,\n\n20:44.920 --> 20:47.000\n I sat down and annotated like a bunch of cards\n\n20:47.000 --> 20:48.480\n for one of my projects.\n\n20:48.480 --> 20:50.640\n And very quickly, I was just like, it was in a video\n\n20:50.640 --> 20:53.560\n and I was basically drawing boxes around all these cards.\n\n20:53.560 --> 20:55.620\n And I think I spent about a week doing all of that\n\n20:55.620 --> 20:57.640\n and I barely got anything done.\n\n20:57.640 --> 21:00.280\n And basically this was, I think my first year of my PhD\n\n21:00.280 --> 21:02.700\n or like a second year of my master's.\n\n21:02.700 --> 21:04.000\n And then by the end of it, I'm like, okay,\n\n21:04.000 --> 21:05.000\n this is just hopeless.\n\n21:05.000 --> 21:05.960\n I can keep doing it.\n\n21:05.960 --> 21:08.480\n And when I'd done that, someone came up to me\n\n21:08.480 --> 21:10.820\n and they basically told me, oh, this is a pickup truck.\n\n21:10.820 --> 21:11.760\n This is not a card.\n\n21:12.760 --> 21:14.800\n And that's when like, aha, this actually makes sense\n\n21:14.800 --> 21:16.140\n because a pickup truck is not really like,\n\n21:16.140 --> 21:17.000\n what was I annotating?\n\n21:17.000 --> 21:19.560\n Was I annotating anything that is mobile\n\n21:19.560 --> 21:21.400\n or was I annotating particular sedans\n\n21:21.400 --> 21:22.660\n or was I annotating SUVs?\n\n21:22.660 --> 21:23.600\n What was I doing?\n\n21:23.600 --> 21:25.720\n By the way, the annotation was bounding boxes?\n\n21:25.720 --> 21:26.960\n Bounding boxes, yeah.\n\n21:26.960 --> 21:30.040\n There's so many deep, profound questions here\n\n21:30.040 --> 21:32.200\n that you're almost cheating your way out of\n\n21:32.200 --> 21:34.400\n by doing self supervised learning, by the way,\n\n21:34.400 --> 21:37.520\n which is like, what makes for an object?\n\n21:37.520 --> 21:39.080\n As opposed to solve intelligence,\n\n21:39.080 --> 21:41.600\n maybe you don't ever need to answer that question.\n\n21:42.480 --> 21:43.720\n I mean, this is the question\n\n21:43.720 --> 21:45.320\n that anyone that's ever done annotation\n\n21:45.320 --> 21:48.040\n because it's so painful gets to ask,\n\n21:48.040 --> 21:53.040\n like, why am I drawing very careful line around this object?\n\n21:55.480 --> 21:57.540\n Like, what is the value?\n\n21:57.540 --> 22:00.200\n I remember when I first saw semantic segmentation\n\n22:00.200 --> 22:03.640\n where you have like instant segmentation\n\n22:03.640 --> 22:06.240\n where you have a very exact line\n\n22:06.240 --> 22:09.520\n around the object in a 2D plane\n\n22:09.520 --> 22:13.440\n of a fundamentally 3D object projected on a 2D plane.\n\n22:13.440 --> 22:15.820\n So you're drawing a line around a car\n\n22:15.820 --> 22:16.960\n that might be occluded.\n\n22:16.960 --> 22:18.880\n There might be another thing in front of it,\n\n22:18.880 --> 22:20.360\n but you're still drawing the line\n\n22:20.360 --> 22:22.720\n of the part of the car that you see.\n\n22:23.640 --> 22:25.880\n How is that the car?\n\n22:25.880 --> 22:27.880\n Why is that the car?\n\n22:27.880 --> 22:31.040\n Like, I had like an existential crisis every time.\n\n22:31.040 --> 22:33.560\n Like, how's that going to help us understand\n\n22:33.560 --> 22:35.360\n a solved computer vision?\n\n22:35.360 --> 22:38.280\n I'm not sure I have a good answer to what's better.\n\n22:38.280 --> 22:41.560\n And I'm not sure I share the confidence that you have\n\n22:41.560 --> 22:46.560\n that self supervised learning can take us far.\n\n22:46.720 --> 22:48.620\n I think I'm more and more convinced\n\n22:48.620 --> 22:50.880\n that it's a very important component,\n\n22:50.880 --> 22:52.840\n but I still feel like we need to understand\n\n22:52.840 --> 22:57.840\n what makes like this dream of maybe what it's called\n\n23:00.120 --> 23:03.080\n like symbolic AI of arriving,\n\n23:03.080 --> 23:05.580\n like once you have this common sense base,\n\n23:05.580 --> 23:10.580\n be able to play with these concepts and build graphs\n\n23:10.960 --> 23:13.440\n or hierarchies of concepts on top\n\n23:13.440 --> 23:18.440\n in order to then like form a deep sense\n\n23:18.800 --> 23:22.040\n of this three dimensional world or four dimensional world\n\n23:22.040 --> 23:25.480\n and be able to reason and then project that onto 2D plane\n\n23:25.480 --> 23:28.520\n in order to interpret a 2D image.\n\n23:28.520 --> 23:30.960\n Can I ask you just an out there question?\n\n23:30.960 --> 23:35.000\n I remember, I think Andre Karpathy had a blog post\n\n23:35.000 --> 23:39.000\n about computer vision, like being really hard.\n\n23:39.000 --> 23:42.080\n I forgot what the title was, but it was many, many years ago.\n\n23:42.080 --> 23:44.760\n And he had, I think President Obama stepping on a scale\n\n23:44.760 --> 23:47.120\n and there was humor and there was a bunch of people laughing\n\n23:47.120 --> 23:48.440\n and whatever.\n\n23:48.440 --> 23:52.000\n And there's a lot of interesting things about that image\n\n23:52.000 --> 23:55.120\n and I think Andre highlighted a bunch of things\n\n23:55.120 --> 23:56.880\n about the image that us humans are able\n\n23:56.880 --> 23:59.000\n to immediately understand.\n\n23:59.000 --> 24:00.960\n Like the idea, I think of gravity\n\n24:00.960 --> 24:04.040\n and that you have the concept of a weight.\n\n24:04.040 --> 24:08.120\n You immediately project because of our knowledge of pose\n\n24:08.120 --> 24:10.360\n and how human bodies are constructed,\n\n24:10.360 --> 24:13.040\n you understand how the forces are being applied\n\n24:13.040 --> 24:14.560\n with the human body.\n\n24:14.560 --> 24:16.040\n The really interesting other thing\n\n24:16.040 --> 24:17.400\n that you're able to understand,\n\n24:17.400 --> 24:20.480\n there's multiple people looking at each other in the image.\n\n24:20.480 --> 24:22.360\n You're able to have a mental model\n\n24:22.360 --> 24:23.760\n of what the people are thinking about.\n\n24:23.760 --> 24:25.320\n You're able to infer like,\n\n24:25.320 --> 24:27.520\n oh, this person is probably thinks,\n\n24:27.520 --> 24:31.240\n like is laughing at how humorous the situation is.\n\n24:31.240 --> 24:34.200\n And this person is confused about what the situation is\n\n24:34.200 --> 24:35.600\n because they're looking this way.\n\n24:35.600 --> 24:37.560\n We're able to infer all of that.\n\n24:37.560 --> 24:40.480\n So that's human vision.\n\n24:41.400 --> 24:45.040\n How difficult is computer vision?\n\n24:45.040 --> 24:48.440\n Like in order to achieve that level of understanding\n\n24:48.440 --> 24:51.440\n and maybe how big of a part\n\n24:51.440 --> 24:54.360\n does self supervised learning play in that, do you think?\n\n24:54.360 --> 24:56.440\n And do you still, you know, back,\n\n24:56.440 --> 24:58.440\n that was like over a decade ago,\n\n24:58.440 --> 25:00.920\n I think Andre and I think a lot of people agreed\n\n25:00.920 --> 25:03.320\n is computer vision is really hard.\n\n25:03.320 --> 25:06.000\n Do you still think computer vision is really hard?\n\n25:06.000 --> 25:07.520\n I think it is, yes.\n\n25:07.520 --> 25:10.640\n And getting to that kind of understanding,\n\n25:10.640 --> 25:12.480\n I mean, it's really out there.\n\n25:12.480 --> 25:15.360\n So if you ask me to solve just that particular problem,\n\n25:15.360 --> 25:17.560\n I can do it the supervised learning route.\n\n25:17.560 --> 25:19.720\n I can always construct a data set and basically predict,\n\n25:19.720 --> 25:21.680\n oh, is there humor in this or not?\n\n25:21.680 --> 25:22.600\n And of course I can do it.\n\n25:22.600 --> 25:23.560\n Actually, that's a good question.\n\n25:23.560 --> 25:25.200\n Do you think you can, okay, okay.\n\n25:25.200 --> 25:29.000\n Do you think you can do human supervised annotation of humor?\n\n25:29.000 --> 25:29.960\n To some extent, yes.\n\n25:29.960 --> 25:30.880\n I'm sure it will work.\n\n25:30.880 --> 25:34.360\n I mean, it won't be as bad as like randomly guessing.\n\n25:34.360 --> 25:36.600\n I'm sure it can still predict whether it's humorous or not\n\n25:36.600 --> 25:37.840\n in some way.\n\n25:37.840 --> 25:40.400\n Yeah, maybe like Reddit upvotes is the signal.\n\n25:40.400 --> 25:41.240\n I don't know.\n\n25:41.240 --> 25:43.800\n I mean, it won't do a great job, but it'll do something.\n\n25:43.800 --> 25:46.040\n It may actually be like, it may find certain things\n\n25:46.040 --> 25:47.560\n which are not humorous, humorous as well,\n\n25:47.560 --> 25:49.160\n which is going to be bad for us.\n\n25:49.160 --> 25:52.120\n But I mean, it'll do, it won't be random.\n\n25:52.120 --> 25:54.520\n Yeah, kind of like my sense of humor.\n\n25:54.520 --> 25:55.920\n Okay, so fine.\n\n25:55.920 --> 25:57.520\n So you can, that particular problem, yes.\n\n25:57.520 --> 25:59.600\n But the general problem you're saying is hard.\n\n25:59.600 --> 26:00.440\n The general problem is hard.\n\n26:00.440 --> 26:02.320\n And I mean, self supervised learning\n\n26:02.320 --> 26:03.920\n is not the answer to everything.\n\n26:03.920 --> 26:04.760\n Of course it's not.\n\n26:04.760 --> 26:07.800\n I think if you have machines that are going to communicate\n\n26:07.800 --> 26:08.760\n with humans at the end of it,\n\n26:08.760 --> 26:10.880\n you want to understand what the algorithm is doing, right?\n\n26:10.880 --> 26:13.720\n You want it to be able to produce an output\n\n26:13.720 --> 26:15.560\n that you can decipher, that you can understand,\n\n26:15.560 --> 26:17.440\n or it's actually useful for something else,\n\n26:17.440 --> 26:19.360\n which again is a human.\n\n26:19.360 --> 26:22.280\n So at some point in this sort of entire loop,\n\n26:22.280 --> 26:23.720\n a human steps in.\n\n26:23.720 --> 26:26.720\n And now this human needs to understand what's going on.\n\n26:26.720 --> 26:28.960\n And at that point, this entire notion of language\n\n26:28.960 --> 26:30.440\n or semantics really comes in.\n\n26:30.440 --> 26:32.600\n If the machine just spits out something\n\n26:32.600 --> 26:34.000\n and if we can't understand it,\n\n26:34.000 --> 26:36.280\n then it's not really that useful for us.\n\n26:36.280 --> 26:38.440\n So self supervised learning is probably going to be useful\n\n26:38.440 --> 26:40.800\n for a lot of the things before that part,\n\n26:40.800 --> 26:42.880\n before the machine really needs to communicate\n\n26:42.880 --> 26:46.080\n a particular kind of output with a human.\n\n26:46.080 --> 26:47.800\n Because, I mean, otherwise,\n\n26:47.800 --> 26:49.920\n how is it going to do that without language?\n\n26:49.920 --> 26:51.880\n Or some kind of communication.\n\n26:51.880 --> 26:53.640\n But you're saying that it's possible to build\n\n26:53.640 --> 26:55.880\n a big base of understanding or whatever,\n\n26:55.880 --> 26:58.280\n of what's a better? Concepts.\n\n26:58.280 --> 26:59.800\n Of concepts. Concepts, yeah.\n\n26:59.800 --> 27:02.280\n Like common sense concepts. Right.\n\n27:02.280 --> 27:06.120\n Supervised learning in the context of computer vision\n\n27:06.120 --> 27:07.520\n is something you've focused on,\n\n27:07.520 --> 27:09.000\n but that's a really hard domain.\n\n27:09.000 --> 27:10.480\n And it's kind of the cutting edge\n\n27:10.480 --> 27:13.040\n of what we're, as a community, working on today.\n\n27:13.040 --> 27:14.760\n Can we take a little bit of a step back\n\n27:14.760 --> 27:16.320\n and look at language?\n\n27:16.320 --> 27:19.000\n Can you summarize the history of success\n\n27:19.000 --> 27:22.480\n of self supervised learning in natural language processing,\n\n27:22.480 --> 27:23.880\n language modeling?\n\n27:23.880 --> 27:25.600\n What are transformers?\n\n27:25.600 --> 27:28.760\n What is the masking, the sentence completion\n\n27:28.760 --> 27:30.040\n that you mentioned before?\n\n27:31.000 --> 27:33.560\n How does it lead us to understand anything?\n\n27:33.560 --> 27:34.800\n Semantic meaning of words,\n\n27:34.800 --> 27:37.640\n syntactic role of words and sentences?\n\n27:37.640 --> 27:40.120\n So I'm, of course, not the expert on NLP.\n\n27:40.120 --> 27:43.480\n I kind of follow it a little bit from the sides.\n\n27:43.480 --> 27:45.760\n So the main sort of reason\n\n27:45.760 --> 27:47.880\n why all of this masking stuff works is,\n\n27:47.880 --> 27:50.880\n I think it's called the distributional hypothesis in NLP.\n\n27:50.880 --> 27:52.640\n The idea basically being that words\n\n27:52.640 --> 27:54.400\n that occur in the same context\n\n27:54.400 --> 27:55.960\n should have similar meaning.\n\n27:55.960 --> 27:59.040\n So if you have the blank jumped over the blank,\n\n27:59.040 --> 28:01.960\n it basically, whatever is like in the first blank\n\n28:01.960 --> 28:04.120\n is basically an object that can actually jump,\n\n28:04.120 --> 28:05.840\n is going to be something that can jump.\n\n28:05.840 --> 28:08.360\n So a cat or a dog, or I don't know, sheep, something,\n\n28:08.360 --> 28:11.680\n all of these things can basically be in that particular context.\n\n28:11.680 --> 28:13.440\n And now, so essentially the idea is that\n\n28:13.440 --> 28:16.080\n if you have words that are in the same context\n\n28:16.080 --> 28:17.360\n and you predict them,\n\n28:17.360 --> 28:20.040\n you're going to learn lots of useful things\n\n28:20.040 --> 28:21.520\n about how words are related,\n\n28:21.520 --> 28:23.600\n because you're predicting by looking at their context\n\n28:23.600 --> 28:24.920\n where the word is going to be.\n\n28:24.920 --> 28:28.280\n So in this particular case, the blank jumped over the fence.\n\n28:28.280 --> 28:30.960\n So now if it's a sheep, the sheep jumped over the fence,\n\n28:30.960 --> 28:32.440\n the dog jumped over the fence.\n\n28:32.440 --> 28:35.600\n So essentially the algorithm or the representation\n\n28:35.600 --> 28:37.640\n basically puts together these two concepts together.\n\n28:37.640 --> 28:40.280\n So it says, okay, dogs are going to be kind of related to sheep\n\n28:40.280 --> 28:42.760\n because both of them occur in the same context.\n\n28:42.760 --> 28:44.480\n Of course, now you can decide\n\n28:44.480 --> 28:46.800\n depending on your particular application downstream,\n\n28:46.800 --> 28:49.200\n you can say that dogs are absolutely not related to sheep\n\n28:49.200 --> 28:52.120\n because well, I don't, I really care about dog food,\n\n28:52.120 --> 28:54.240\n for example, I'm a dog food person\n\n28:54.240 --> 28:55.640\n and I really want to give this dog food\n\n28:55.640 --> 28:57.320\n to this particular animal.\n\n28:57.320 --> 29:00.120\n So depending on what your downstream application is,\n\n29:00.120 --> 29:03.040\n of course, this notion of similarity or this notion\n\n29:03.040 --> 29:04.320\n or this common sense that you've learned\n\n29:04.320 --> 29:05.840\n may not be applicable.\n\n29:05.840 --> 29:08.080\n But the point is basically that this,\n\n29:08.080 --> 29:09.960\n just predicting what the blanks are\n\n29:09.960 --> 29:11.760\n is going to take you really, really far.\n\n29:11.760 --> 29:14.040\n So there's a nice feature of language\n\n29:14.040 --> 29:18.720\n that the number of words in a particular language\n\n29:18.720 --> 29:20.800\n is very large, but it's finite\n\n29:20.800 --> 29:22.080\n and it's actually not that large\n\n29:22.080 --> 29:24.160\n in the grand scheme of things.\n\n29:24.160 --> 29:26.560\n I still got it because we take it for granted.\n\n29:26.560 --> 29:28.400\n So first of all, when you say masking,\n\n29:28.400 --> 29:31.560\n you're talking about this very process of the blank,\n\n29:31.560 --> 29:33.440\n of removing words from a sentence\n\n29:33.440 --> 29:36.760\n and then having the knowledge of what word went there\n\n29:36.760 --> 29:38.520\n in the initial data set,\n\n29:38.520 --> 29:41.080\n that's the ground truth that you're training on\n\n29:41.080 --> 29:43.480\n and then you're asking the neural network\n\n29:43.480 --> 29:45.080\n to predict what goes there.\n\n29:46.560 --> 29:49.240\n That's like a little trick.\n\n29:49.240 --> 29:50.880\n It's a really powerful trick.\n\n29:50.880 --> 29:53.320\n The question is how far that takes us.\n\n29:53.320 --> 29:56.280\n And the other question is, is there other tricks?\n\n29:56.280 --> 29:58.680\n Because to me, it's very possible\n\n29:58.680 --> 30:00.720\n there's other very fascinating tricks.\n\n30:00.720 --> 30:05.200\n I'll give you an example in autonomous driving,\n\n30:05.200 --> 30:06.920\n there's a bunch of tricks\n\n30:06.920 --> 30:10.360\n that give you the self supervised signal back.\n\n30:10.360 --> 30:15.360\n For example, very similar to sentences, but not really,\n\n30:16.280 --> 30:20.240\n which is you have signals from humans driving the car\n\n30:20.240 --> 30:23.640\n because a lot of us drive cars to places.\n\n30:23.640 --> 30:27.800\n And so you can ask the neural network to predict\n\n30:27.800 --> 30:30.240\n what's going to happen the next two seconds\n\n30:30.240 --> 30:33.400\n for a safe navigation through the environment.\n\n30:33.400 --> 30:36.200\n And the signal comes from the fact\n\n30:36.200 --> 30:38.640\n that you also have knowledge of what happened\n\n30:38.640 --> 30:42.080\n in the next two seconds, because you have video of the data.\n\n30:42.080 --> 30:46.760\n The question in autonomous driving, as it is in language,\n\n30:46.760 --> 30:50.200\n can we learn how to drive autonomously\n\n30:50.200 --> 30:53.480\n based on that kind of self supervision?\n\n30:53.480 --> 30:55.360\n Probably the answer is no.\n\n30:55.360 --> 30:57.800\n The question is how good can we get?\n\n30:57.800 --> 31:00.200\n And the same with language, how good can we get?\n\n31:00.200 --> 31:02.160\n And are there other tricks?\n\n31:02.160 --> 31:04.680\n Like we get sometimes super excited by this trick\n\n31:04.680 --> 31:05.720\n that works really well.\n\n31:05.720 --> 31:09.120\n But I wonder, it's almost like mining for gold.\n\n31:09.120 --> 31:12.760\n I wonder how many signals there are in the data\n\n31:12.760 --> 31:15.280\n that could be leveraged that are like there.\n\n31:17.200 --> 31:18.600\n I just wanted to kind of linger on that\n\n31:18.600 --> 31:20.840\n because sometimes it's easy to think\n\n31:20.840 --> 31:24.840\n that maybe this masking process is self supervised learning.\n\n31:24.840 --> 31:27.200\n No, it's only one method.\n\n31:27.200 --> 31:29.280\n So there could be many, many other methods,\n\n31:29.280 --> 31:33.840\n many tricky methods, maybe interesting ways\n\n31:33.840 --> 31:36.880\n to leverage human computation in very interesting ways\n\n31:36.880 --> 31:39.920\n that might actually border on semi supervised learning,\n\n31:39.920 --> 31:40.840\n something like that.\n\n31:40.840 --> 31:43.520\n Obviously the internet is generated by humans\n\n31:43.520 --> 31:44.720\n at the end of the day.\n\n31:44.720 --> 31:48.760\n So all that to say is what's your sense\n\n31:48.760 --> 31:50.680\n in this particular context of language,\n\n31:50.680 --> 31:54.680\n how far can that masking process take us?\n\n31:54.680 --> 31:56.240\n So it has stood the test of time, right?\n\n31:56.240 --> 31:59.800\n I mean, so Word2vec, the initial sort of NLP technique\n\n31:59.800 --> 32:02.120\n that was using this to now, for example,\n\n32:02.120 --> 32:05.880\n like all the BERT and all these big models that we get,\n\n32:05.880 --> 32:07.560\n BERT and Roberta, for example,\n\n32:07.560 --> 32:08.760\n all of them are still sort of based\n\n32:08.760 --> 32:10.600\n on the same principle of masking.\n\n32:10.600 --> 32:12.120\n It's taken us really far.\n\n32:12.120 --> 32:14.400\n I mean, you can actually do things like,\n\n32:14.400 --> 32:16.240\n oh, these two sentences are similar or not,\n\n32:16.240 --> 32:18.680\n whether this particular sentence follows this other sentence\n\n32:18.680 --> 32:20.480\n in terms of logic, so entailment,\n\n32:20.480 --> 32:21.760\n you can do a lot of these things\n\n32:21.760 --> 32:23.640\n with just this masking trick.\n\n32:23.640 --> 32:28.320\n So I'm not sure if I can predict how far it can take us,\n\n32:28.320 --> 32:31.480\n because when it first came out, when Word2vec was out,\n\n32:31.480 --> 32:33.480\n I don't think a lot of us would have imagined\n\n32:33.480 --> 32:35.960\n that this would actually help us do some kind\n\n32:35.960 --> 32:38.520\n of entailment problems and really that well.\n\n32:38.520 --> 32:40.920\n And so just the fact that by just scaling up\n\n32:40.920 --> 32:42.320\n the amount of data that we're training on\n\n32:42.320 --> 32:45.120\n and using better and more powerful neural network\n\n32:45.120 --> 32:47.600\n architectures has taken us from that to this,\n\n32:47.600 --> 32:52.600\n is just showing you how maybe poor predictors we are,\n\n32:52.600 --> 32:54.880\n as humans, how poor we are at predicting\n\n32:54.880 --> 32:57.360\n how successful a particular technique is going to be.\n\n32:57.360 --> 32:58.680\n So I think I can say something now,\n\n32:58.680 --> 33:00.040\n but like 10 years from now,\n\n33:00.040 --> 33:02.800\n I look completely stupid basically predicting this.\n\n33:02.800 --> 33:07.160\n In the language domain, is there something in your work\n\n33:07.160 --> 33:09.560\n that you find useful and insightful\n\n33:09.560 --> 33:12.560\n and transferable to computer vision,\n\n33:12.560 --> 33:15.720\n but also just, I don't know, beautiful and profound\n\n33:15.720 --> 33:18.160\n that I think carries through to the vision domain?\n\n33:18.160 --> 33:21.000\n I mean, the idea of masking has been very powerful.\n\n33:21.000 --> 33:23.680\n It has been used in vision as well for predicting,\n\n33:23.680 --> 33:25.800\n like you say, the next sort of if you have\n\n33:25.800 --> 33:28.080\n and sort of frames and you predict\n\n33:28.080 --> 33:29.360\n what's going to happen in the next frame.\n\n33:29.360 --> 33:30.960\n So that's been very powerful.\n\n33:30.960 --> 33:32.880\n In terms of modeling, like in just terms\n\n33:32.880 --> 33:34.600\n in terms of architecture, I think you would have asked\n\n33:34.600 --> 33:36.880\n about transformers a while back.\n\n33:36.880 --> 33:38.480\n That has really become like,\n\n33:38.480 --> 33:40.800\n it has become super exciting for computer vision now.\n\n33:40.800 --> 33:42.760\n Like in the past, I would say year and a half,\n\n33:42.760 --> 33:44.160\n it's become really powerful.\n\n33:44.160 --> 33:45.240\n What's a transformer?\n\n33:45.240 --> 33:46.080\n Right.\n\n33:46.080 --> 33:47.440\n I mean, the core part of a transformer\n\n33:47.440 --> 33:49.040\n is something called the self attention model.\n\n33:49.040 --> 33:50.440\n So it came out of Google\n\n33:50.440 --> 33:53.760\n and the idea basically is that if you have N elements,\n\n33:53.760 --> 33:56.480\n what you're creating is a way for all of these N elements\n\n33:56.480 --> 33:57.880\n to talk to each other.\n\n33:57.880 --> 34:01.800\n So the idea basically is that you are paying attention.\n\n34:01.800 --> 34:03.160\n Each element is paying attention\n\n34:03.160 --> 34:04.960\n to each of the other element.\n\n34:04.960 --> 34:06.760\n And basically by doing this,\n\n34:06.760 --> 34:08.960\n it's really trying to figure out,\n\n34:08.960 --> 34:11.440\n you're basically getting a much better view of the data.\n\n34:11.440 --> 34:14.480\n So for example, if you have a sentence of like four words,\n\n34:14.480 --> 34:16.320\n the point is if you get a representation\n\n34:16.320 --> 34:18.320\n or a feature for this entire sentence,\n\n34:18.320 --> 34:21.280\n it's constructed in a way such that each word\n\n34:21.280 --> 34:23.840\n has paid attention to everything else.\n\n34:23.840 --> 34:26.120\n Now, the reason it's like different from say,\n\n34:26.120 --> 34:28.440\n what you would do in a ConvNet\n\n34:28.440 --> 34:29.560\n is basically that in the ConvNet,\n\n34:29.560 --> 34:31.400\n you would only pay attention to a local window.\n\n34:31.400 --> 34:33.160\n So each word would only pay attention\n\n34:33.160 --> 34:36.160\n to its next neighbor or like one neighbor after that.\n\n34:36.160 --> 34:37.840\n And the same thing goes for images.\n\n34:37.840 --> 34:40.120\n In images, you would basically pay attention to pixels\n\n34:40.120 --> 34:42.800\n in a three cross three or a seven cross seven neighborhood.\n\n34:42.800 --> 34:43.680\n And that's it.\n\n34:43.680 --> 34:46.000\n Whereas with the transformer, the self attention mainly,\n\n34:46.000 --> 34:48.760\n the sort of idea is that each element\n\n34:48.760 --> 34:50.440\n needs to pay attention to each other element.\n\n34:50.440 --> 34:51.960\n And when you say attention,\n\n34:51.960 --> 34:53.400\n maybe another way to phrase that\n\n34:53.400 --> 34:57.680\n is you're considering a context,\n\n34:57.680 --> 35:01.560\n a wide context in terms of the wide context of the sentence\n\n35:01.560 --> 35:05.160\n in understanding the meaning of a particular word\n\n35:05.160 --> 35:06.960\n and in computer vision that's understanding\n\n35:06.960 --> 35:10.040\n a larger context to understand the local pattern\n\n35:10.040 --> 35:13.080\n of a particular local part of an image.\n\n35:13.080 --> 35:14.960\n Right, so basically if you have say,\n\n35:14.960 --> 35:16.520\n again, a banana in the image,\n\n35:16.520 --> 35:18.600\n you're looking at the full image first.\n\n35:18.600 --> 35:19.920\n So whether it's like, you know,\n\n35:19.920 --> 35:22.200\n you're looking at all the pixels that are off a kitchen\n\n35:22.200 --> 35:23.760\n or for dining table and so on.\n\n35:23.760 --> 35:25.920\n And then you're basically looking at the banana also.\n\n35:25.920 --> 35:27.200\n Yeah, by the way, in terms of,\n\n35:27.200 --> 35:29.240\n if we were to train the funny classifier,\n\n35:29.240 --> 35:32.000\n there's something funny about the word banana.\n\n35:32.000 --> 35:33.840\n Just wanted to anticipate that.\n\n35:33.840 --> 35:36.200\n I am wearing a banana shirt, so yeah.\n\n35:36.200 --> 35:37.480\n Is there bananas on it?\n\n35:39.720 --> 35:42.440\n Okay, so masking has worked for the vision context as well.\n\n35:42.440 --> 35:44.320\n And so this transformer idea has worked as well.\n\n35:44.320 --> 35:46.280\n So basically looking at all the elements\n\n35:46.280 --> 35:48.160\n to understand a particular element\n\n35:48.160 --> 35:49.920\n has been really powerful in vision.\n\n35:49.920 --> 35:52.080\n The reason is like a lot of things\n\n35:52.080 --> 35:53.480\n when you're looking at them in isolation.\n\n35:53.480 --> 35:55.600\n So if you look at just a blob of pixels,\n\n35:55.600 --> 35:57.520\n so Antonio Torralba at MIT used to have\n\n35:57.520 --> 35:58.960\n this like really famous image,\n\n35:58.960 --> 36:01.040\n which I looked at when I was a PhD student.\n\n36:01.040 --> 36:02.840\n But he would basically have a blob of pixels\n\n36:02.840 --> 36:04.960\n and he would ask you, hey, what is this?\n\n36:04.960 --> 36:06.840\n And it looked basically like a shoe\n\n36:06.840 --> 36:08.880\n or like it could look like a TV remote.\n\n36:08.880 --> 36:10.080\n It could look like anything.\n\n36:10.080 --> 36:12.360\n And it turns out it was a beer bottle.\n\n36:12.360 --> 36:14.120\n But I'm not sure it was one of these three things,\n\n36:14.120 --> 36:15.440\n but basically he showed you the full picture\n\n36:15.440 --> 36:17.560\n and then it was very obvious what it was.\n\n36:17.560 --> 36:19.240\n But the point is just by looking at\n\n36:19.240 --> 36:21.880\n that particular local window, you couldn't figure it out.\n\n36:21.880 --> 36:23.880\n Because of resolution, because of other things,\n\n36:23.880 --> 36:26.080\n it's just not easy always to just figure it out\n\n36:26.080 --> 36:27.960\n by looking at just the neighborhood of pixels,\n\n36:27.960 --> 36:29.680\n what these pixels are.\n\n36:29.680 --> 36:32.000\n And the same thing happens for language as well.\n\n36:32.000 --> 36:33.920\n For the parameters that have to learn\n\n36:33.920 --> 36:35.160\n something about the data,\n\n36:35.160 --> 36:37.200\n you need to give it the capacity\n\n36:37.200 --> 36:39.160\n to learn the essential things.\n\n36:39.160 --> 36:42.680\n Like if it's not actually able to receive the signal at all,\n\n36:42.680 --> 36:44.320\n then it's not gonna be able to learn that signal.\n\n36:44.320 --> 36:47.320\n And in order to understand images, to understand language,\n\n36:47.320 --> 36:50.720\n you have to be able to see words in their full context.\n\n36:50.720 --> 36:54.960\n Okay, what is harder to solve, vision or language?\n\n36:54.960 --> 36:57.880\n Visual intelligence or linguistic intelligence?\n\n36:57.880 --> 36:59.840\n So I'm going to say computer vision is harder.\n\n36:59.840 --> 37:01.640\n My reason for this is basically that\n\n37:02.800 --> 37:05.000\n language of course has a big structure to it\n\n37:05.000 --> 37:06.880\n because we developed it.\n\n37:06.880 --> 37:08.720\n Whereas vision is something that is common\n\n37:08.720 --> 37:09.960\n in a lot of animals.\n\n37:09.960 --> 37:12.520\n Everyone is able to get by a lot of these animals\n\n37:12.520 --> 37:15.080\n on earth are actually able to get by without language.\n\n37:15.080 --> 37:18.280\n And a lot of these animals we also deem to be intelligent.\n\n37:18.280 --> 37:20.920\n So clearly intelligence does have\n\n37:20.920 --> 37:22.520\n like a visual component to it.\n\n37:22.520 --> 37:24.240\n And yes, of course, in the case of humans,\n\n37:24.240 --> 37:26.400\n it of course also has a linguistic component.\n\n37:26.400 --> 37:28.720\n But it means that there is something far more fundamental\n\n37:28.720 --> 37:30.840\n about vision than there is about language.\n\n37:30.840 --> 37:32.960\n And I'm sorry to anyone who disagrees,\n\n37:32.960 --> 37:34.360\n but yes, this is what I feel.\n\n37:34.360 --> 37:38.880\n So that's being a little bit reflected in the challenges\n\n37:38.880 --> 37:40.800\n that have to do with the progress\n\n37:40.800 --> 37:42.520\n of self supervised learning, would you say?\n\n37:42.520 --> 37:45.560\n Or is that just a peculiar accidents\n\n37:45.560 --> 37:47.400\n of the progress of the AI community\n\n37:47.400 --> 37:48.600\n that we focused on like,\n\n37:48.600 --> 37:51.680\n or we discovered self attention and transformers\n\n37:51.680 --> 37:53.640\n in the context of language first?\n\n37:53.640 --> 37:55.520\n So like the self supervised learning success\n\n37:55.520 --> 37:58.880\n was actually for vision has not much to do\n\n37:58.880 --> 37:59.960\n with the transformers part.\n\n37:59.960 --> 38:02.480\n I would say it's actually been independent a little bit.\n\n38:02.480 --> 38:05.360\n I think it's just that the signal was a little bit different\n\n38:05.360 --> 38:08.120\n for vision than there was for like NLP\n\n38:08.120 --> 38:11.240\n and probably NLP folks discovered it before.\n\n38:11.240 --> 38:12.680\n So for vision, the main success\n\n38:12.680 --> 38:14.840\n has basically been this like crops so far,\n\n38:14.840 --> 38:16.960\n like taking different crops of images.\n\n38:16.960 --> 38:18.920\n Whereas for NLP, it was this masking thing.\n\n38:18.920 --> 38:20.480\n But also the level of success\n\n38:20.480 --> 38:22.080\n is still much higher for language.\n\n38:22.080 --> 38:22.920\n It has.\n\n38:22.920 --> 38:24.800\n So that has a lot to do with,\n\n38:24.800 --> 38:26.920\n I mean, I can get into a lot of details.\n\n38:26.920 --> 38:29.040\n For this particular question, let's go for it, okay.\n\n38:29.040 --> 38:32.280\n So the first thing is language is very structured.\n\n38:32.280 --> 38:34.080\n So you are going to produce a distribution\n\n38:34.080 --> 38:35.920\n over a finite vocabulary.\n\n38:35.920 --> 38:37.680\n English has a finite number of words.\n\n38:37.680 --> 38:39.520\n It's actually not that large.\n\n38:39.520 --> 38:41.640\n And you need to produce basically,\n\n38:41.640 --> 38:42.760\n when you're doing this masking thing,\n\n38:42.760 --> 38:44.160\n all you need to do is basically tell me\n\n38:44.160 --> 38:46.440\n which one of these like 50,000 words it is.\n\n38:46.440 --> 38:47.280\n That's it.\n\n38:47.280 --> 38:49.560\n Now for vision, let's imagine doing the same thing.\n\n38:49.560 --> 38:51.480\n Okay, we're basically going to blank out\n\n38:51.480 --> 38:52.600\n a particular part of the image\n\n38:52.600 --> 38:54.680\n and we ask the network or this neural network\n\n38:54.680 --> 38:58.080\n to predict what is present in this missing patch.\n\n38:58.080 --> 38:59.960\n It's combinatorially large, right?\n\n38:59.960 --> 39:02.560\n You have 256 pixel values.\n\n39:02.560 --> 39:04.840\n If you're even producing basically a seven cross seven\n\n39:04.840 --> 39:07.960\n or a 14 cross 14 like window of pixels,\n\n39:07.960 --> 39:11.320\n at each of these 169 or each of these 49 locations,\n\n39:11.320 --> 39:13.720\n you have 256 values to predict.\n\n39:13.720 --> 39:15.240\n And so it's really, really large.\n\n39:15.240 --> 39:18.960\n And very quickly, the kind of like prediction problems\n\n39:18.960 --> 39:20.800\n that we're setting up are going to be extremely\n\n39:20.800 --> 39:22.760\n like interactable for us.\n\n39:22.760 --> 39:24.960\n And so the thing is for NLP, it has been really successful\n\n39:24.960 --> 39:27.520\n because we are very good at predicting,\n\n39:27.520 --> 39:30.840\n like doing this like distribution over a finite set.\n\n39:30.840 --> 39:33.480\n And the problem is when this set becomes really large,\n\n39:33.480 --> 39:35.520\n we are going to become really, really bad\n\n39:35.520 --> 39:36.960\n at making these predictions\n\n39:36.960 --> 39:41.000\n and at solving basically this particular set of problems.\n\n39:41.000 --> 39:44.200\n So if you were to do it exactly in the same way\n\n39:44.200 --> 39:47.000\n as NLP for vision, there is very limited success.\n\n39:47.000 --> 39:48.960\n The way stuff is working right now\n\n39:48.960 --> 39:51.640\n is actually not by predicting these masks.\n\n39:51.640 --> 39:53.640\n It's basically by saying that you take these two\n\n39:53.640 --> 39:55.120\n like crops from the image,\n\n39:55.120 --> 39:57.040\n you get a feature representation from it.\n\n39:57.040 --> 39:58.640\n And just saying that these two features,\n\n39:58.640 --> 40:00.400\n so they're like vectors,\n\n40:00.400 --> 40:02.000\n just saying that the distance between these vectors\n\n40:02.000 --> 40:03.200\n should be small.\n\n40:03.200 --> 40:06.640\n And so it's a very different way of learning\n\n40:06.640 --> 40:09.160\n from the visual signal than there is from NLP.\n\n40:09.160 --> 40:11.360\n Okay, the other reason is the distributional hypothesis\n\n40:11.360 --> 40:12.920\n that we talked about for NLP, right?\n\n40:12.920 --> 40:15.160\n So a word given its context,\n\n40:15.160 --> 40:16.560\n basically the context actually supplies\n\n40:16.560 --> 40:18.440\n a lot of meaning to the word.\n\n40:18.440 --> 40:22.280\n Now, because there are just finite number of words\n\n40:22.280 --> 40:25.760\n and there is a finite way in like which we compose them.\n\n40:25.760 --> 40:27.440\n Of course, the same thing holds for pixels,\n\n40:27.440 --> 40:29.760\n but in language, there's a lot of structure, right?\n\n40:29.760 --> 40:31.000\n So I always say whatever,\n\n40:31.000 --> 40:33.760\n the dash jumped over the fence, for example.\n\n40:33.760 --> 40:36.720\n There are lots of these sentences that you'll get.\n\n40:36.720 --> 40:38.680\n And from this, you can actually look at\n\n40:38.680 --> 40:40.160\n this particular sentence might occur\n\n40:40.160 --> 40:41.480\n in a lot of different contexts as well.\n\n40:41.480 --> 40:42.600\n This exact same sentence\n\n40:42.600 --> 40:44.080\n might occur in a different context.\n\n40:44.080 --> 40:45.560\n So the sheep jumped over the fence,\n\n40:45.560 --> 40:46.800\n the cat jumped over the fence,\n\n40:46.800 --> 40:48.160\n the dog jumped over the fence.\n\n40:48.160 --> 40:50.480\n So you immediately get a lot of these words,\n\n40:50.480 --> 40:52.720\n which are because this particular token itself\n\n40:52.720 --> 40:53.560\n has so much meaning,\n\n40:53.560 --> 40:55.480\n you get a lot of these tokens or these words,\n\n40:55.480 --> 40:57.720\n which are actually going to have sort of\n\n40:57.720 --> 41:00.560\n this related meaning across given this context.\n\n41:00.560 --> 41:02.640\n Whereas for vision, it's much harder\n\n41:02.640 --> 41:04.160\n because just by like pure,\n\n41:04.160 --> 41:05.600\n like the way we capture images,\n\n41:05.600 --> 41:07.440\n lighting can be different.\n\n41:07.440 --> 41:09.800\n There might be like different noise in the sensor.\n\n41:09.800 --> 41:12.240\n So the thing is you're capturing a physical phenomenon\n\n41:12.240 --> 41:13.840\n and then you're basically going through\n\n41:13.840 --> 41:16.400\n a very complicated pipeline of like image processing.\n\n41:16.400 --> 41:18.040\n And then you're translating that into\n\n41:18.040 --> 41:20.400\n some kind of like digital signal.\n\n41:20.400 --> 41:23.520\n Whereas with language, you write it down\n\n41:23.520 --> 41:25.040\n and you transfer it to a digital signal,\n\n41:25.040 --> 41:27.520\n almost like it's a lossless like transfer.\n\n41:27.520 --> 41:30.160\n And each of these tokens are very, very well defined.\n\n41:30.160 --> 41:32.840\n There could be a little bit of an argument there\n\n41:32.840 --> 41:36.120\n because language as written down\n\n41:36.120 --> 41:39.400\n is a projection of thought.\n\n41:39.400 --> 41:42.560\n This is one of the open questions is\n\n41:42.560 --> 41:46.320\n if you perfectly can solve language,\n\n41:46.320 --> 41:50.040\n are you getting close to being able to solve easily\n\n41:50.040 --> 41:52.800\n with flying colors past the towing test kind of thing.\n\n41:52.800 --> 41:56.560\n So that's, it's similar, but different\n\n41:56.560 --> 41:59.760\n and the computer vision problem is in the 2D plane\n\n41:59.760 --> 42:02.640\n is a projection with three dimensional world.\n\n42:02.640 --> 42:05.640\n So perhaps there are similar problems there.\n\n42:05.640 --> 42:06.480\n Maybe this is a good.\n\n42:06.480 --> 42:08.560\n I mean, I think what I'm saying is NLP is not easy.\n\n42:08.560 --> 42:09.520\n Of course, don't get me wrong.\n\n42:09.520 --> 42:12.920\n Like abstract thought expressed in knowledge\n\n42:12.920 --> 42:14.600\n or knowledge basically expressed in language\n\n42:14.600 --> 42:16.720\n is really hard to understand, right?\n\n42:16.720 --> 42:19.160\n I mean, we've been communicating with language for so long\n\n42:19.160 --> 42:22.000\n and it is of course a very complicated concept.\n\n42:22.000 --> 42:27.000\n The thing is at least getting like somewhat reasonable,\n\n42:27.000 --> 42:29.880\n like being able to solve some kind of reasonable tasks\n\n42:29.880 --> 42:32.080\n with language, I would say slightly easier\n\n42:32.080 --> 42:33.640\n than it is with computer vision.\n\n42:33.640 --> 42:35.360\n Yeah, I would say, yeah.\n\n42:35.360 --> 42:36.600\n So that's well put.\n\n42:36.600 --> 42:40.840\n I would say getting impressive performance on language\n\n42:40.840 --> 42:43.360\n is easier.\n\n42:43.360 --> 42:45.320\n I feel like for both language and computer vision,\n\n42:45.320 --> 42:49.440\n there's going to be this wall of like,\n\n42:49.440 --> 42:52.240\n like this hump you have to overcome\n\n42:52.240 --> 42:54.800\n to achieve superhuman level performance\n\n42:54.800 --> 42:56.600\n or human level performance.\n\n42:56.600 --> 43:00.200\n And I feel like for language, that wall is farther away.\n\n43:00.200 --> 43:01.880\n So you can get pretty nice.\n\n43:01.880 --> 43:04.080\n You can do a lot of tricks.\n\n43:04.080 --> 43:06.520\n You can show really impressive performance.\n\n43:06.520 --> 43:09.680\n You can even fool people that you're tweeting\n\n43:09.680 --> 43:11.480\n or you write blog posts writing\n\n43:11.480 --> 43:16.480\n or your question answering has intelligence behind it.\n\n43:16.880 --> 43:21.880\n But to truly demonstrate understanding of dialogue,\n\n43:22.360 --> 43:25.000\n of continuous long form dialogue\n\n43:25.000 --> 43:28.600\n that would require perhaps big breakthroughs.\n\n43:28.600 --> 43:30.440\n In the same way in computer vision,\n\n43:30.440 --> 43:33.400\n I think the big breakthroughs need to happen earlier\n\n43:33.400 --> 43:36.600\n to achieve impressive performance.\n\n43:36.600 --> 43:38.760\n This might be a good place to, you already mentioned it,\n\n43:38.760 --> 43:41.120\n but what is contrastive learning\n\n43:41.120 --> 43:43.840\n and what are energy based models?\n\n43:43.840 --> 43:46.840\n Contrastive learning is sort of the paradigm of learning\n\n43:46.840 --> 43:50.680\n where the idea is that you are learning this embedding space\n\n43:50.680 --> 43:52.680\n or so you're learning this sort of vector space\n\n43:52.680 --> 43:54.520\n of all your concepts.\n\n43:54.520 --> 43:56.760\n And the way you learn that is basically by contrasting.\n\n43:56.760 --> 43:59.120\n So the idea is that you have a sample,\n\n43:59.120 --> 44:01.000\n you have another sample that's related to it.\n\n44:01.000 --> 44:02.840\n So that's called the positive\n\n44:02.840 --> 44:05.080\n and you have another sample that's not related to it.\n\n44:05.080 --> 44:06.080\n So that's negative.\n\n44:06.080 --> 44:08.320\n So for example, let's just take an NLP\n\n44:08.320 --> 44:10.960\n or in a simple example in computer vision.\n\n44:10.960 --> 44:14.480\n So you have an image of a cat, you have an image of a dog\n\n44:14.480 --> 44:16.520\n and for whatever application that you're doing,\n\n44:16.520 --> 44:18.840\n say you're trying to figure out what the pets are,\n\n44:18.840 --> 44:20.280\n you're saying that these two images are related.\n\n44:20.280 --> 44:22.280\n So image of a cat and dog are related,\n\n44:22.280 --> 44:25.400\n but now you have another third image of a banana\n\n44:25.400 --> 44:26.960\n because you don't like that word.\n\n44:26.960 --> 44:28.920\n So now you basically have this banana.\n\n44:28.920 --> 44:30.640\n Thank you for speaking to the crowd.\n\n44:30.640 --> 44:32.560\n And so you take both of these images\n\n44:32.560 --> 44:34.440\n and you take the image from the cat,\n\n44:34.440 --> 44:35.280\n the image from the dog,\n\n44:35.280 --> 44:36.760\n you get a feature from both of them.\n\n44:36.760 --> 44:38.160\n And now what you're training the network to do\n\n44:38.160 --> 44:42.080\n is basically pull both of these features together\n\n44:42.080 --> 44:44.720\n while pushing them away from the feature of a banana.\n\n44:44.720 --> 44:45.840\n So this is the contrastive part.\n\n44:45.840 --> 44:47.840\n So you're contrasting against the banana.\n\n44:47.840 --> 44:51.520\n So there's always this notion of a negative and a positive.\n\n44:51.520 --> 44:54.160\n Now, energy based models are like one way\n\n44:54.160 --> 44:57.480\n that Jan sort of explains a lot of these methods.\n\n44:57.480 --> 45:00.680\n So Jan basically, I think a couple of years\n\n45:00.680 --> 45:02.840\n or more than that, like when I joined Facebook,\n\n45:02.840 --> 45:05.080\n Jan used to keep mentioning this word, energy based models.\n\n45:05.080 --> 45:07.200\n And of course I had no idea what he was talking about.\n\n45:07.200 --> 45:09.680\n So then one day I caught him in one of the conference rooms\n\n45:09.680 --> 45:11.240\n and I'm like, can you please tell me what this is?\n\n45:11.240 --> 45:13.120\n So then like very patiently,\n\n45:13.120 --> 45:15.960\n he sat down with like a marker and a whiteboard.\n\n45:15.960 --> 45:18.280\n And his idea basically is that\n\n45:18.280 --> 45:20.280\n rather than talking about probability distributions,\n\n45:20.280 --> 45:21.920\n you can talk about energies of models.\n\n45:21.920 --> 45:24.000\n So models are trying to minimize certain energies\n\n45:24.000 --> 45:24.960\n in certain space,\n\n45:24.960 --> 45:28.200\n or they're trying to maximize a certain kind of energy.\n\n45:28.200 --> 45:29.760\n And the idea basically is that\n\n45:29.760 --> 45:32.200\n you can explain a lot of the contrastive models,\n\n45:32.200 --> 45:33.280\n GANs, for example,\n\n45:33.280 --> 45:36.000\n which are like Generative Adversarial Networks.\n\n45:36.000 --> 45:37.880\n A lot of these modern learning methods\n\n45:37.880 --> 45:39.880\n or VAEs, which are Variational Autoencoders,\n\n45:39.880 --> 45:41.840\n you can really explain them very nicely\n\n45:41.840 --> 45:43.160\n in terms of an energy function\n\n45:43.160 --> 45:45.320\n that they're trying to minimize or maximize.\n\n45:45.320 --> 45:48.360\n And so by putting this common sort of language\n\n45:48.360 --> 45:49.720\n for all of these models,\n\n45:49.720 --> 45:51.800\n what looks very different in machine learning\n\n45:51.800 --> 45:54.160\n that, oh, VAEs are very different from what GANs are,\n\n45:54.160 --> 45:56.440\n are very, very different from what contrastive models are,\n\n45:56.440 --> 45:57.560\n you actually get a sense of like,\n\n45:57.560 --> 46:00.120\n oh, these are actually very, very related.\n\n46:00.120 --> 46:02.520\n It's just that the way or the mechanism\n\n46:02.520 --> 46:04.200\n in which they're sort of maximizing\n\n46:04.200 --> 46:07.000\n or minimizing this energy function is slightly different.\n\n46:07.000 --> 46:08.920\n It's revealing the commonalities\n\n46:08.920 --> 46:10.400\n between all these approaches\n\n46:10.400 --> 46:13.000\n and putting a sexy word on top of it, like energy.\n\n46:13.000 --> 46:14.360\n And so similarities,\n\n46:14.360 --> 46:16.760\n two things that are similar have low energy.\n\n46:16.760 --> 46:20.360\n Like the low energy signifying similarity.\n\n46:20.360 --> 46:21.200\n Right, exactly.\n\n46:21.200 --> 46:23.560\n So basically the idea is that if you were to imagine\n\n46:23.560 --> 46:26.480\n like the embedding as a manifold, a 2D manifold,\n\n46:26.480 --> 46:28.920\n you would get a hill or like a high sort of peak\n\n46:28.920 --> 46:30.600\n in the energy manifold,\n\n46:30.600 --> 46:32.400\n wherever two things are not related.\n\n46:32.400 --> 46:34.080\n And basically you would have like a dip\n\n46:34.080 --> 46:35.520\n where two things are related.\n\n46:35.520 --> 46:37.080\n So you'd get a dip in the manifold.\n\n46:37.080 --> 46:40.200\n And in the self supervised context,\n\n46:40.200 --> 46:42.280\n how do you know two things are related\n\n46:42.280 --> 46:44.120\n and two things are not related?\n\n46:44.120 --> 46:44.960\n Right.\n\n46:44.960 --> 46:46.920\n So this is where all the sort of ingenuity or tricks\n\n46:46.920 --> 46:47.840\n comes in, right?\n\n46:47.840 --> 46:50.840\n So for example, like you can take\n\n46:50.840 --> 46:52.160\n the fill in the blank problem,\n\n46:52.160 --> 46:54.360\n or you can take in the context problem.\n\n46:54.360 --> 46:55.920\n And what you can say is two words\n\n46:55.920 --> 46:57.800\n that are in the same context are related.\n\n46:57.800 --> 47:00.560\n Two words that are in different contexts are not related.\n\n47:00.560 --> 47:02.280\n For images, basically two crops\n\n47:02.280 --> 47:03.960\n from the same image are related.\n\n47:03.960 --> 47:06.440\n And whereas a third image is not related at all.\n\n47:06.440 --> 47:08.200\n Or for a video, it can be two frames\n\n47:08.200 --> 47:09.200\n from that video are related\n\n47:09.200 --> 47:10.800\n because they're likely to contain\n\n47:10.800 --> 47:12.720\n the same sort of concepts in them.\n\n47:12.720 --> 47:13.720\n Whereas a third frame\n\n47:13.720 --> 47:15.600\n from a different video is not related.\n\n47:15.600 --> 47:18.320\n So it basically is, it's a very general term.\n\n47:18.320 --> 47:19.680\n Contrastive learning is nothing really\n\n47:19.680 --> 47:20.840\n to do with self supervised learning.\n\n47:20.840 --> 47:23.240\n It actually is very popular in for example,\n\n47:23.240 --> 47:25.200\n like any kind of metric learning\n\n47:25.200 --> 47:26.920\n or any kind of embedding learning.\n\n47:26.920 --> 47:28.920\n So it's also used in supervised learning.\n\n47:28.920 --> 47:32.080\n And the thing is because we are not really using labels\n\n47:32.080 --> 47:34.560\n to get these positive or negative pairs,\n\n47:34.560 --> 47:37.640\n it can basically also be used for self supervised learning.\n\n47:37.640 --> 47:39.000\n So you mentioned one of the ideas\n\n47:39.000 --> 47:42.760\n in the vision context that works\n\n47:42.760 --> 47:45.280\n is to have different crops.\n\n47:45.280 --> 47:47.080\n So you could think of that as a way\n\n47:47.080 --> 47:49.480\n to sort of manipulating the data\n\n47:49.480 --> 47:53.280\n to generate examples that are similar.\n\n47:53.280 --> 47:55.800\n Obviously, there's a bunch of other techniques.\n\n47:55.800 --> 47:58.440\n You mentioned lighting as a very,\n\n47:58.440 --> 48:01.680\n in images lighting is something that varies a lot\n\n48:01.680 --> 48:04.520\n and you can artificially change those kinds of things.\n\n48:04.520 --> 48:07.720\n There's the whole broad field of data augmentation,\n\n48:07.720 --> 48:11.800\n which manipulates images in order to increase arbitrarily\n\n48:11.800 --> 48:13.400\n the size of the data set.\n\n48:13.400 --> 48:15.840\n First of all, what is data augmentation?\n\n48:15.840 --> 48:18.120\n And second of all, what's the role of data augmentation\n\n48:18.120 --> 48:22.000\n in self supervised learning and contrastive learning?\n\n48:22.000 --> 48:24.760\n So data augmentation is just a way like you said,\n\n48:24.760 --> 48:26.680\n it's basically a way to augment the data.\n\n48:26.680 --> 48:28.640\n So you have say n samples.\n\n48:28.640 --> 48:30.120\n And what you do is you basically define\n\n48:30.120 --> 48:32.280\n some kind of transforms for the sample.\n\n48:32.280 --> 48:33.640\n So you take your say image\n\n48:33.640 --> 48:34.880\n and then you define a transform\n\n48:34.880 --> 48:37.320\n where you can just increase say the colors\n\n48:37.320 --> 48:39.120\n like the colors or the brightness of the image\n\n48:39.120 --> 48:41.320\n or increase or decrease the contrast of the image\n\n48:41.320 --> 48:44.560\n for example, or take different crops of it.\n\n48:44.560 --> 48:46.240\n So data augmentation is just a process\n\n48:46.240 --> 48:49.040\n to like basically perturb the data\n\n48:49.040 --> 48:51.080\n or like augment the data, right?\n\n48:51.080 --> 48:53.160\n And so it has played a fundamental role\n\n48:53.160 --> 48:56.640\n for computer vision for self supervised learning especially.\n\n48:56.640 --> 48:59.160\n The way most of the current methods work\n\n48:59.160 --> 49:02.720\n contrastive or otherwise is by taking an image\n\n49:02.720 --> 49:05.320\n in the case of images is by taking an image\n\n49:05.320 --> 49:08.560\n and then computing basically two perturbations of it.\n\n49:08.560 --> 49:11.480\n So these can be two different crops of the image\n\n49:11.480 --> 49:12.920\n with like different types of lighting\n\n49:12.920 --> 49:15.000\n or different contrast or different colors.\n\n49:15.000 --> 49:17.840\n So you jitter the colors a little bit and so on.\n\n49:17.840 --> 49:21.720\n And now the idea is basically because it's the same object\n\n49:21.720 --> 49:23.440\n or because it's like related concepts\n\n49:23.440 --> 49:25.200\n in both of these perturbations,\n\n49:25.200 --> 49:27.960\n you want the features from both of these perturbations\n\n49:27.960 --> 49:28.920\n to be similar.\n\n49:28.920 --> 49:31.320\n So now you can use a variety of different ways\n\n49:31.320 --> 49:32.600\n to enforce this constraint,\n\n49:32.600 --> 49:34.200\n like these features being similar.\n\n49:34.200 --> 49:36.040\n You can do this by contrastive learning.\n\n49:36.040 --> 49:38.440\n So basically, both of these things are positives,\n\n49:38.440 --> 49:40.440\n a third sort of image is negative.\n\n49:40.440 --> 49:43.480\n You can do this basically by like clustering.\n\n49:43.480 --> 49:46.960\n For example, you can say that both of these images should,\n\n49:46.960 --> 49:48.120\n the features from both of these images\n\n49:48.120 --> 49:50.560\n should belong in the same cluster because they're related,\n\n49:50.560 --> 49:52.280\n whereas image like another image\n\n49:52.280 --> 49:53.880\n should belong to a different cluster.\n\n49:53.880 --> 49:55.160\n So there's a variety of different ways\n\n49:55.160 --> 49:57.560\n to basically enforce this particular constraint.\n\n49:57.560 --> 49:59.080\n By the way, when you say features,\n\n49:59.080 --> 50:01.680\n it means there's a very large neural network\n\n50:01.680 --> 50:03.640\n that extracting patterns from the image\n\n50:03.640 --> 50:05.160\n and the kind of patterns that extracts\n\n50:05.160 --> 50:08.440\n should be either identical or very similar.\n\n50:08.440 --> 50:09.640\n That's what that means.\n\n50:09.640 --> 50:11.880\n So the neural network basically takes in the image\n\n50:11.880 --> 50:14.160\n and then outputs a set of like,\n\n50:14.160 --> 50:16.600\n basically a vector of like numbers,\n\n50:16.600 --> 50:17.720\n and that's the feature.\n\n50:17.720 --> 50:20.000\n And you want this feature for both of these\n\n50:20.000 --> 50:22.120\n like different crops that you computed to be similar.\n\n50:22.120 --> 50:24.520\n So you want this vector to be identical\n\n50:24.520 --> 50:26.120\n in its like entries, for example.\n\n50:26.120 --> 50:28.120\n Be like literally close\n\n50:28.120 --> 50:31.640\n in this multi dimensional space to each other.\n\n50:31.640 --> 50:32.600\n And like you said,\n\n50:32.600 --> 50:35.960\n close can mean part of the same cluster or something like that\n\n50:35.960 --> 50:37.440\n in this large space.\n\n50:37.440 --> 50:38.920\n First of all, that,\n\n50:38.920 --> 50:40.680\n I wonder if there is connection\n\n50:40.680 --> 50:43.760\n to the way humans learn to this,\n\n50:43.760 --> 50:48.040\n almost like maybe subconsciously,\n\n50:48.040 --> 50:50.120\n in order to understand a thing,\n\n50:50.120 --> 50:54.680\n you kind of have to see it from two, three multiple angles.\n\n50:54.680 --> 50:57.320\n I wonder, I have a lot of friends\n\n50:57.320 --> 51:00.200\n who are neuroscientists maybe and cognitive scientists.\n\n51:00.200 --> 51:03.200\n I wonder if that's in there somewhere.\n\n51:03.200 --> 51:08.200\n Like in order for us to place a concept in its proper place,\n\n51:08.560 --> 51:12.440\n we have to basically crop it in all kinds of ways,\n\n51:12.440 --> 51:14.400\n do basic data augmentation on it\n\n51:14.400 --> 51:17.640\n in whatever very clever ways that the brain likes to do.\n\n51:17.640 --> 51:19.040\n Right.\n\n51:19.040 --> 51:21.160\n Like spinning around in our minds somehow\n\n51:21.160 --> 51:23.080\n that that is very effective.\n\n51:23.080 --> 51:25.040\n So I think for some of them, we like need to do it.\n\n51:25.040 --> 51:27.000\n So like babies, for example, pick up objects,\n\n51:27.000 --> 51:30.120\n like move them and put them close to their eye and whatnot.\n\n51:30.120 --> 51:31.200\n But for certain other things,\n\n51:31.200 --> 51:33.800\n actually we are good at imagining it as well, right?\n\n51:33.800 --> 51:35.960\n So if you, I have never seen, for example,\n\n51:35.960 --> 51:36.960\n an elephant from the top.\n\n51:36.960 --> 51:39.560\n I've never basically looked at it from like top down.\n\n51:39.560 --> 51:40.720\n But if you showed me a picture of it,\n\n51:40.720 --> 51:43.760\n I could very well tell you that that's an elephant.\n\n51:43.760 --> 51:45.320\n So I think some of it, we're just like,\n\n51:45.320 --> 51:47.840\n we naturally build it or transfer it from other objects\n\n51:47.840 --> 51:50.920\n that we've seen to imagine what it's going to look like.\n\n51:50.920 --> 51:53.280\n Has anyone done that with augmentation?\n\n51:53.280 --> 51:56.920\n Like imagine all the possible things\n\n51:56.920 --> 51:59.880\n that are occluded or not there,\n\n51:59.880 --> 52:03.360\n but not just like normal things, like wild things,\n\n52:03.360 --> 52:05.880\n but they're nevertheless physically consistent.\n\n52:06.960 --> 52:09.720\n So, I mean, people do kind of like\n\n52:09.720 --> 52:11.800\n occlusion based augmentation as well.\n\n52:11.800 --> 52:14.760\n So you place in like a random like box, gray box\n\n52:14.760 --> 52:17.440\n to sort of mask out a certain part of the image.\n\n52:17.440 --> 52:20.000\n And the thing is basically you're kind of occluding it.\n\n52:20.000 --> 52:23.600\n For example, you place it say on half of a person's face.\n\n52:23.600 --> 52:24.920\n So basically saying that, you know,\n\n52:24.920 --> 52:26.680\n something below their nose is occluded\n\n52:26.680 --> 52:28.280\n because it's grayed out.\n\n52:28.280 --> 52:31.680\n So, you know, I meant like, you have like, what is it?\n\n52:31.680 --> 52:33.880\n A table and you can't see behind the table.\n\n52:33.880 --> 52:37.080\n And you imagine there's a bunch of elves\n\n52:37.080 --> 52:38.840\n with bananas behind the table.\n\n52:38.840 --> 52:40.440\n Like, I wonder if there's useful\n\n52:40.440 --> 52:44.200\n to have a wild imagination for the network\n\n52:44.200 --> 52:46.120\n because that's possible or maybe not elves,\n\n52:46.120 --> 52:49.000\n but like puppies and kittens or something like that.\n\n52:49.000 --> 52:51.240\n Just have a wild imagination\n\n52:51.240 --> 52:55.080\n and like constantly be generating that wild imagination.\n\n52:55.080 --> 52:57.560\n Because in terms of data augmentation,\n\n52:57.560 --> 53:01.200\n as currently applied, it's super ultra, very boring.\n\n53:01.200 --> 53:02.920\n It's very basic data augmentation.\n\n53:02.920 --> 53:07.040\n I wonder if there's a benefit to being wildly imaginable\n\n53:07.040 --> 53:11.880\n while trying to be consistent with physical reality.\n\n53:11.880 --> 53:14.200\n I think it's a kind of a chicken and egg problem, right?\n\n53:14.200 --> 53:16.400\n Because to have like amazing data augmentation,\n\n53:16.400 --> 53:18.520\n you need to understand what the scene is.\n\n53:18.520 --> 53:20.640\n And what we're trying to do data augmentation\n\n53:20.640 --> 53:22.080\n to learn what a scene is anyway.\n\n53:22.080 --> 53:23.760\n So it's basically just keeps going on.\n\n53:23.760 --> 53:24.800\n Before you understand it,\n\n53:24.800 --> 53:26.400\n just put elves with bananas\n\n53:26.400 --> 53:28.120\n until you know it's not to be true.\n\n53:29.360 --> 53:31.680\n Just like children have a wild imagination\n\n53:31.680 --> 53:33.960\n until the adults ruin it all.\n\n53:33.960 --> 53:36.960\n Okay, so what are the different kinds of data augmentation\n\n53:36.960 --> 53:40.800\n that you've seen to be effective in visual intelligence?\n\n53:40.800 --> 53:42.040\n For like vision,\n\n53:42.040 --> 53:44.160\n it's a lot of these image filtering operations.\n\n53:44.160 --> 53:46.520\n So like blurring the image,\n\n53:46.520 --> 53:48.160\n you know, all the kind of Instagram filters\n\n53:48.160 --> 53:49.440\n that you can think of.\n\n53:49.440 --> 53:52.520\n So like arbitrarily like make the red super red,\n\n53:52.520 --> 53:55.840\n make the green super greens, like saturate the image.\n\n53:55.840 --> 53:56.960\n Rotation, cropping.\n\n53:56.960 --> 53:58.440\n Rotation, cropping, exactly.\n\n53:58.440 --> 53:59.560\n All of these kinds of things.\n\n53:59.560 --> 54:02.600\n Like I said, lighting is a really interesting one to me.\n\n54:02.600 --> 54:04.760\n Like that feels like really complicated to do.\n\n54:04.760 --> 54:05.600\n I mean, they don't,\n\n54:05.600 --> 54:08.040\n the augmentations that we work on aren't like\n\n54:08.040 --> 54:08.880\n that involved,\n\n54:08.880 --> 54:09.720\n they're not going to be like\n\n54:09.720 --> 54:11.280\n physically realistic versions of lighting.\n\n54:11.280 --> 54:12.680\n It's not that you're assuming\n\n54:12.680 --> 54:13.680\n that there's a light source up\n\n54:13.680 --> 54:15.080\n and then you're moving it to the right\n\n54:15.080 --> 54:17.000\n and then what does the thing look like?\n\n54:17.000 --> 54:19.160\n It's really more about like brightness of the image,\n\n54:19.160 --> 54:20.400\n overall brightness of the image\n\n54:20.400 --> 54:22.520\n or overall contrast of the image and so on.\n\n54:22.520 --> 54:25.080\n But this is a really important point to me.\n\n54:25.080 --> 54:28.680\n I always thought that data augmentation\n\n54:28.680 --> 54:30.400\n holds an important key\n\n54:31.640 --> 54:33.840\n to big improvements in machine learning.\n\n54:33.840 --> 54:36.640\n And it seems that it is an important aspect\n\n54:36.640 --> 54:39.080\n of self supervised learning.\n\n54:39.080 --> 54:42.560\n So I wonder if there's big improvements to be achieved\n\n54:42.560 --> 54:46.680\n on much more intelligent kinds of data augmentation.\n\n54:46.680 --> 54:48.320\n For example, currently,\n\n54:48.320 --> 54:50.160\n maybe you can correct me if I'm wrong,\n\n54:50.160 --> 54:52.760\n data augmentation is not parameterized.\n\n54:52.760 --> 54:53.600\n Yeah.\n\n54:53.600 --> 54:54.440\n You're not learning.\n\n54:54.440 --> 54:55.280\n You're not learning.\n\n54:55.280 --> 54:59.800\n To me, it seems like data augmentation potentially\n\n54:59.800 --> 55:02.000\n should involve more learning\n\n55:02.000 --> 55:04.160\n than the learning process itself.\n\n55:04.160 --> 55:05.360\n Right.\n\n55:05.360 --> 55:08.800\n You're almost like thinking of like generative kind of,\n\n55:08.800 --> 55:10.240\n it's the elves with bananas.\n\n55:10.240 --> 55:11.080\n You're trying to,\n\n55:11.080 --> 55:13.280\n it's like very active imagination\n\n55:13.280 --> 55:14.880\n of messing with the world\n\n55:14.880 --> 55:17.640\n and teaching that mechanism for messing with the world\n\n55:17.640 --> 55:19.120\n to be realistic.\n\n55:19.120 --> 55:20.480\n Right.\n\n55:20.480 --> 55:22.640\n Because that feels like,\n\n55:22.640 --> 55:24.200\n I mean, it's imagination.\n\n55:24.200 --> 55:25.600\n It's just, as you said,\n\n55:25.600 --> 55:28.160\n it feels like us humans are able to,\n\n55:29.440 --> 55:30.680\n maybe sometimes subconsciously,\n\n55:30.680 --> 55:33.000\n imagine before we see the thing,\n\n55:33.000 --> 55:35.480\n imagine what we're expecting to see,\n\n55:35.480 --> 55:37.240\n like maybe several options.\n\n55:37.240 --> 55:38.800\n And especially, we probably forgot,\n\n55:38.800 --> 55:40.480\n but when we were younger,\n\n55:40.480 --> 55:44.200\n probably the possibilities were wilder, more numerous.\n\n55:44.200 --> 55:45.160\n And then as we get older,\n\n55:45.160 --> 55:47.400\n we become to understand the world\n\n55:47.400 --> 55:51.040\n and the possibilities of what we might see\n\n55:51.040 --> 55:53.120\n becomes less and less and less.\n\n55:53.120 --> 55:55.600\n So I wonder if you think there's a lot of breakthroughs\n\n55:55.600 --> 55:57.160\n yet to be had in data augmentation.\n\n55:57.160 --> 55:59.760\n And maybe also can you just comment on the stuff we have,\n\n55:59.760 --> 56:02.120\n is that a big part of self supervised learning?\n\n56:02.120 --> 56:02.960\n Yes.\n\n56:02.960 --> 56:05.520\n So data augmentation is like key to self supervised learning\n\n56:05.520 --> 56:08.320\n that has like the kind of augmentation that we're using.\n\n56:08.320 --> 56:11.040\n And basically the fact that we're trying to learn\n\n56:11.040 --> 56:13.920\n these neural networks that are predicting these features\n\n56:13.920 --> 56:17.080\n from images that are robust under data augmentation\n\n56:17.080 --> 56:19.560\n has been the key for visual self supervised learning.\n\n56:19.560 --> 56:22.400\n And they play a fairly fundamental role to it.\n\n56:22.400 --> 56:24.600\n Now, the irony of all of this is that\n\n56:24.600 --> 56:26.720\n for like deep learning purists will say\n\n56:26.720 --> 56:28.640\n the entire point of deep learning is that\n\n56:28.640 --> 56:31.160\n you feed in the pixels to the neural network\n\n56:31.160 --> 56:33.120\n and it should figure out the patterns on its own.\n\n56:33.120 --> 56:34.480\n So if it really wants to look at edges,\n\n56:34.480 --> 56:35.640\n it should look at edges.\n\n56:35.640 --> 56:36.720\n You shouldn't really like really go\n\n56:36.720 --> 56:38.600\n and handcraft these like features, right?\n\n56:38.600 --> 56:41.160\n You shouldn't go tell it that look at edges.\n\n56:41.160 --> 56:42.360\n So data augmentation\n\n56:42.360 --> 56:44.400\n should basically be in the same category, right?\n\n56:44.400 --> 56:46.040\n Why should we tell the network\n\n56:46.040 --> 56:48.200\n or tell this entire learning paradigm\n\n56:48.200 --> 56:50.840\n what kinds of data augmentation that we're looking for?\n\n56:50.840 --> 56:55.200\n We are encoding a very sort of human specific bias there\n\n56:55.200 --> 56:57.560\n that we know things are like,\n\n56:57.560 --> 56:59.200\n if you change the contrast of the image,\n\n56:59.200 --> 57:00.280\n it should still be an apple\n\n57:00.280 --> 57:02.240\n or it should still see apple, not banana.\n\n57:02.240 --> 57:05.880\n And basically if we change like colors,\n\n57:05.880 --> 57:08.040\n it should still be the same kind of concept.\n\n57:08.040 --> 57:09.880\n Of course, this is not one,\n\n57:09.880 --> 57:12.480\n this is doesn't feel like super satisfactory\n\n57:12.480 --> 57:14.560\n because a lot of our human knowledge\n\n57:14.560 --> 57:15.760\n or our human supervision\n\n57:15.760 --> 57:17.600\n is actually going into the data augmentation.\n\n57:17.600 --> 57:19.680\n So although we are calling it self supervised learning,\n\n57:19.680 --> 57:21.040\n a lot of the human knowledge\n\n57:21.040 --> 57:23.520\n is actually being encoded in the data augmentation process.\n\n57:23.520 --> 57:24.360\n So it's really like,\n\n57:24.360 --> 57:27.120\n we've kind of sneaked away the supervision at the input\n\n57:27.120 --> 57:28.520\n and we're like really designing\n\n57:28.520 --> 57:30.360\n these nice list of data augmentations\n\n57:30.360 --> 57:31.640\n that are working very well.\n\n57:31.640 --> 57:33.720\n Of course, the idea is that it's much easier\n\n57:33.720 --> 57:36.600\n to design a list of data augmentation than it is to do.\n\n57:36.600 --> 57:39.640\n So humans are doing nevertheless doing less and less work\n\n57:39.640 --> 57:42.600\n and maybe leveraging their creativity more and more.\n\n57:42.600 --> 57:45.080\n And when we say data augmentation is not parameterized,\n\n57:45.080 --> 57:48.200\n it means it's not part of the learning process.\n\n57:48.200 --> 57:50.560\n Do you think it's possible to integrate\n\n57:50.560 --> 57:53.280\n some of the data augmentation into the learning process?\n\n57:53.280 --> 57:54.120\n I think so.\n\n57:54.120 --> 57:54.960\n I think so.\n\n57:54.960 --> 57:57.440\n And in fact, it will be really beneficial for us\n\n57:57.440 --> 57:59.720\n because a lot of these data augmentations\n\n57:59.720 --> 58:01.840\n that we use in vision are very extreme.\n\n58:01.840 --> 58:05.400\n For example, like when you have certain concepts,\n\n58:05.400 --> 58:08.160\n again, a banana, you take the banana\n\n58:08.160 --> 58:10.560\n and then basically you change the color of the banana, right?\n\n58:10.560 --> 58:12.440\n So you make it a purple banana.\n\n58:12.440 --> 58:14.200\n Now this data augmentation process\n\n58:14.200 --> 58:15.920\n is actually independent of the,\n\n58:15.920 --> 58:18.920\n like it has no notion of what is present in the image.\n\n58:18.920 --> 58:20.520\n So it can change this color arbitrarily.\n\n58:20.520 --> 58:22.560\n It can make it a red banana as well.\n\n58:22.560 --> 58:24.040\n And now what we're doing is we're telling\n\n58:24.040 --> 58:26.160\n the neural network that this red banana\n\n58:26.160 --> 58:29.280\n and so a crop of this image which has the red banana\n\n58:29.280 --> 58:30.960\n and a crop of this image where I changed the color\n\n58:30.960 --> 58:32.360\n to a purple banana should be,\n\n58:32.360 --> 58:34.080\n the features should be the same.\n\n58:34.080 --> 58:36.680\n Now bananas aren't red or purple mostly.\n\n58:36.680 --> 58:38.560\n So really the data augmentation process\n\n58:38.560 --> 58:41.120\n should take into account what is present in the image\n\n58:41.120 --> 58:43.080\n and what are the kinds of physical realities\n\n58:43.080 --> 58:43.920\n that are possible.\n\n58:43.920 --> 58:45.840\n It shouldn't be completely independent of the image.\n\n58:45.840 --> 58:48.840\n So you might get big gains if you,\n\n58:48.840 --> 58:51.560\n instead of being drastic, do subtle augmentation\n\n58:51.560 --> 58:53.280\n but realistic augmentation.\n\n58:53.280 --> 58:54.120\n Right, realistic.\n\n58:54.120 --> 58:56.280\n I'm not sure if it's subtle, but like realistic for sure.\n\n58:56.280 --> 58:59.600\n If it's realistic, then even subtle augmentation\n\n58:59.600 --> 59:00.680\n will give you big benefits.\n\n59:00.680 --> 59:01.840\n Exactly, yeah.\n\n59:01.840 --> 59:05.040\n And it will be like for particular domains\n\n59:05.040 --> 59:06.440\n you might actually see like,\n\n59:06.440 --> 59:08.960\n if for example, now we're doing medical imaging,\n\n59:08.960 --> 59:10.160\n there are going to be certain kinds\n\n59:10.160 --> 59:11.440\n of like geometric augmentation\n\n59:11.440 --> 59:13.480\n which are not really going to be very valid\n\n59:13.480 --> 59:15.080\n for the human body.\n\n59:15.080 --> 59:18.280\n So if you were to like actually loop in data augmentation\n\n59:18.280 --> 59:19.480\n into the learning process,\n\n59:19.480 --> 59:21.320\n it will actually be much more useful.\n\n59:21.320 --> 59:23.280\n Now this actually does take us\n\n59:23.280 --> 59:25.120\n to maybe a semi supervised kind of a setting\n\n59:25.120 --> 59:27.480\n because you do want to understand\n\n59:27.480 --> 59:29.080\n what is it that you're trying to solve.\n\n59:29.080 --> 59:30.880\n So currently self supervised learning\n\n59:30.880 --> 59:32.720\n kind of operates in the wild, right?\n\n59:32.720 --> 59:34.960\n So you do the self supervised learning\n\n59:34.960 --> 59:37.560\n and the purists and all of us basically say that,\n\n59:37.560 --> 59:39.440\n okay, this should learn useful representations\n\n59:39.440 --> 59:42.320\n and they should be useful for any kind of end task,\n\n59:42.320 --> 59:44.280\n no matter it's like banana recognition\n\n59:44.280 --> 59:46.240\n or like autonomous driving.\n\n59:46.240 --> 59:47.760\n Now it's a tall order.\n\n59:47.760 --> 59:50.480\n Maybe the first baby step for us should be that,\n\n59:50.480 --> 59:52.640\n okay, if you're trying to loop in this data augmentation\n\n59:52.640 --> 59:53.920\n into the learning process,\n\n59:53.920 --> 59:56.000\n then we at least need to have some sense\n\n59:56.000 --> 59:56.840\n of what we're trying to do.\n\n59:56.840 --> 59:57.760\n Are we trying to distinguish\n\n59:57.760 --> 59:59.560\n between different types of bananas\n\n59:59.560 --> 1:00:02.040\n or are we trying to distinguish between banana and apple\n\n1:00:02.040 --> 1:00:04.400\n or are we trying to do all of these things at once?\n\n1:00:04.400 --> 1:00:07.920\n And so some notion of like what happens at the end\n\n1:00:07.920 --> 1:00:10.840\n might actually help us do much better at this side.\n\n1:00:10.840 --> 1:00:14.320\n Let me ask you a ridiculous question.\n\n1:00:14.320 --> 1:00:16.280\n If I were to give you like a black box,\n\n1:00:16.280 --> 1:00:19.520\n like a choice to have an arbitrary large data set\n\n1:00:19.520 --> 1:00:21.360\n of real natural data\n\n1:00:22.320 --> 1:00:26.640\n versus really good data augmentation algorithms,\n\n1:00:26.640 --> 1:00:31.320\n which would you like to train in a self supervised way on?\n\n1:00:31.320 --> 1:00:35.040\n So natural data from the internet are arbitrary large,\n\n1:00:35.040 --> 1:00:37.360\n so unlimited data,\n\n1:00:37.360 --> 1:00:41.760\n or it's like more controlled good data augmentation\n\n1:00:41.760 --> 1:00:43.600\n on the finite data set.\n\n1:00:43.600 --> 1:00:44.440\n The thing is like,\n\n1:00:44.440 --> 1:00:47.240\n because our learning algorithms for vision right now\n\n1:00:47.240 --> 1:00:49.360\n really rely on data augmentation,\n\n1:00:49.360 --> 1:00:50.480\n even if you were to give me\n\n1:00:50.480 --> 1:00:52.880\n like an infinite source of like image data,\n\n1:00:52.880 --> 1:00:54.600\n I still need a good data augmentation algorithm.\n\n1:00:54.600 --> 1:00:56.080\n You need something that tells you\n\n1:00:56.080 --> 1:00:57.400\n that two things are similar.\n\n1:00:57.400 --> 1:00:58.240\n Right.\n\n1:00:58.240 --> 1:00:59.080\n And so something,\n\n1:00:59.080 --> 1:01:01.600\n because you've given me an arbitrary large data set,\n\n1:01:01.600 --> 1:01:03.760\n I still need to use data augmentation\n\n1:01:03.760 --> 1:01:05.360\n to take that image construct,\n\n1:01:05.360 --> 1:01:06.920\n like these two perturbations of it,\n\n1:01:06.920 --> 1:01:08.240\n and then learn from it.\n\n1:01:08.240 --> 1:01:09.960\n So the thing is our learning paradigm\n\n1:01:09.960 --> 1:01:11.640\n is very primitive right now.\n\n1:01:11.640 --> 1:01:12.480\n Yeah.\n\n1:01:12.480 --> 1:01:13.800\n Even if you were to give me lots of images,\n\n1:01:13.800 --> 1:01:15.200\n it's still not really useful.\n\n1:01:15.200 --> 1:01:16.520\n A good data augmentation algorithm\n\n1:01:16.520 --> 1:01:18.040\n is actually going to be more useful.\n\n1:01:18.040 --> 1:01:21.160\n So you can like reduce down the amount of data\n\n1:01:21.160 --> 1:01:22.920\n that you give me by like 10 times,\n\n1:01:22.920 --> 1:01:23.760\n but if you were to give me\n\n1:01:23.760 --> 1:01:25.040\n a good data augmentation algorithm,\n\n1:01:25.040 --> 1:01:26.440\n that would probably do better\n\n1:01:26.440 --> 1:01:29.040\n than giving me like 10 times the size of that data,\n\n1:01:29.040 --> 1:01:30.800\n but me having to rely on\n\n1:01:30.800 --> 1:01:32.640\n like a very primitive data augmentation algorithm.\n\n1:01:32.640 --> 1:01:35.040\n Like through tagging and all those kinds of things,\n\n1:01:35.040 --> 1:01:37.240\n is there a way to discover things\n\n1:01:37.240 --> 1:01:39.600\n that are semantically similar on the internet?\n\n1:01:39.600 --> 1:01:42.520\n Obviously there is, but they might be extremely noisy.\n\n1:01:42.520 --> 1:01:45.760\n And the difference might be farther away\n\n1:01:45.760 --> 1:01:47.840\n than you would be comfortable with.\n\n1:01:47.840 --> 1:01:49.720\n So, I mean, yes, tagging will help you a lot.\n\n1:01:49.720 --> 1:01:51.480\n It'll actually go a very long way\n\n1:01:51.480 --> 1:01:54.360\n in figuring out what images are related or not.\n\n1:01:54.360 --> 1:01:57.480\n And then, so, but then the purists would argue\n\n1:01:57.480 --> 1:01:58.880\n that when you're using human tags,\n\n1:01:58.880 --> 1:02:01.200\n because these tags are like supervision,\n\n1:02:01.200 --> 1:02:03.960\n is it really self supervised learning now?\n\n1:02:03.960 --> 1:02:05.320\n Because you're using human tags\n\n1:02:05.320 --> 1:02:07.960\n to figure out which images are like similar.\n\n1:02:07.960 --> 1:02:10.440\n Hashtag no filter means a lot of things.\n\n1:02:10.440 --> 1:02:11.280\n Yes.\n\n1:02:11.280 --> 1:02:12.360\n I mean, there are certain tags\n\n1:02:12.360 --> 1:02:15.280\n which are going to be applicable pretty much to anything.\n\n1:02:15.280 --> 1:02:18.240\n So they're pretty useless for learning.\n\n1:02:18.240 --> 1:02:20.800\n But I mean, certain tags are actually like\n\n1:02:20.800 --> 1:02:22.240\n the Eiffel Tower, for example,\n\n1:02:22.240 --> 1:02:23.800\n or the Taj Mahal, for example.\n\n1:02:23.800 --> 1:02:26.480\n These tags are like very indicative of what's going on.\n\n1:02:26.480 --> 1:02:29.440\n And they are, I mean, they are human supervision.\n\n1:02:29.440 --> 1:02:30.280\n Yeah.\n\n1:02:30.280 --> 1:02:31.880\n This is one of the tasks of discovering\n\n1:02:31.880 --> 1:02:34.880\n from human generated data strong signals\n\n1:02:34.880 --> 1:02:39.560\n that could be leveraged for self supervision.\n\n1:02:39.560 --> 1:02:42.240\n Like humans are doing so much work already.\n\n1:02:42.240 --> 1:02:45.120\n Like many years ago, there was something that was called,\n\n1:02:45.120 --> 1:02:48.000\n I guess, human computation back in the day.\n\n1:02:48.000 --> 1:02:50.240\n Humans are doing so much work.\n\n1:02:50.240 --> 1:02:53.480\n It'd be exciting to discover ways to leverage\n\n1:02:53.480 --> 1:02:55.840\n the work they're doing to teach machines\n\n1:02:55.840 --> 1:02:57.960\n without any extra effort from them.\n\n1:02:57.960 --> 1:03:00.160\n An example could be, like we said, driving,\n\n1:03:00.160 --> 1:03:03.000\n humans driving and machines can learn from the driving.\n\n1:03:03.000 --> 1:03:06.760\n I always hope that there could be some supervision signal\n\n1:03:06.760 --> 1:03:08.160\n discovered in video games,\n\n1:03:08.160 --> 1:03:10.720\n because there's so many people that play video games\n\n1:03:10.720 --> 1:03:15.720\n that it feels like so much effort is put into video games,\n\n1:03:15.840 --> 1:03:17.680\n into playing video games,\n\n1:03:17.680 --> 1:03:21.760\n and you can design video games somewhat cheaply\n\n1:03:21.760 --> 1:03:24.640\n to include whatever signals you want.\n\n1:03:24.640 --> 1:03:27.520\n It feels like that could be leverage somehow.\n\n1:03:27.520 --> 1:03:28.680\n So people are using that.\n\n1:03:28.680 --> 1:03:30.840\n Like there are actually folks right here in UT Austin,\n\n1:03:30.840 --> 1:03:33.760\n like Philip Granbull is a professor at UT Austin.\n\n1:03:33.760 --> 1:03:36.160\n He's been like working on video games\n\n1:03:36.160 --> 1:03:38.000\n as a source of supervision.\n\n1:03:38.000 --> 1:03:39.000\n I mean, it's really fun.\n\n1:03:39.000 --> 1:03:40.040\n Like as a PhD student,\n\n1:03:40.040 --> 1:03:42.200\n getting to basically play video games all day.\n\n1:03:42.200 --> 1:03:44.920\n Yeah, but so I do hope that kind of thing scales\n\n1:03:44.920 --> 1:03:48.080\n and like ultimately boils down to discovering\n\n1:03:48.080 --> 1:03:51.600\n some undeniably very good signal.\n\n1:03:51.600 --> 1:03:54.040\n It's like masking in NLP.\n\n1:03:54.040 --> 1:03:57.640\n But that said, there's non contrastive methods.\n\n1:03:57.640 --> 1:04:00.840\n What do non contrastive energy based\n\n1:04:00.840 --> 1:04:03.520\n self supervised learning methods look like?\n\n1:04:03.520 --> 1:04:05.640\n And why are they promising?\n\n1:04:05.640 --> 1:04:07.800\n So like I said about contrastive learning,\n\n1:04:07.800 --> 1:04:10.720\n you have this notion of a positive and a negative.\n\n1:04:10.720 --> 1:04:13.640\n Now, the thing is, this entire learning paradigm\n\n1:04:13.640 --> 1:04:17.160\n really requires access to a lot of negatives\n\n1:04:17.160 --> 1:04:19.040\n to learn a good sort of feature space.\n\n1:04:19.040 --> 1:04:21.680\n The idea is if I tell you, okay,\n\n1:04:21.680 --> 1:04:23.680\n so a cat and a dog are similar,\n\n1:04:23.680 --> 1:04:25.680\n and they're very different from a banana.\n\n1:04:25.680 --> 1:04:28.000\n The thing is, this is a fairly simple analogy, right?\n\n1:04:28.000 --> 1:04:30.840\n Because bananas look visually very different\n\n1:04:30.840 --> 1:04:32.440\n from what cats and dogs do.\n\n1:04:32.440 --> 1:04:34.440\n So very quickly, if this is the only source\n\n1:04:34.440 --> 1:04:36.600\n of supervision that I'm giving you,\n\n1:04:36.600 --> 1:04:38.080\n your learning is not going to be like,\n\n1:04:38.080 --> 1:04:39.760\n after a point, the neural network\n\n1:04:39.760 --> 1:04:41.640\n is really not going to learn a lot.\n\n1:04:41.640 --> 1:04:42.960\n Because the negative that you're getting\n\n1:04:42.960 --> 1:04:43.880\n is going to be so random.\n\n1:04:43.880 --> 1:04:46.640\n So it can be, oh, a cat and a dog are very similar,\n\n1:04:46.640 --> 1:04:49.880\n but they're very different from a Volkswagen Beetle.\n\n1:04:49.880 --> 1:04:51.920\n Now, like this car looks very different\n\n1:04:51.920 --> 1:04:52.920\n from these animals again.\n\n1:04:52.920 --> 1:04:54.880\n So the thing is in contrastive learning,\n\n1:04:54.880 --> 1:04:58.120\n the quality of the negative sample really matters a lot.\n\n1:04:58.120 --> 1:05:00.800\n And so what has happened is basically that\n\n1:05:00.800 --> 1:05:02.840\n typically these methods that are contrastive\n\n1:05:02.840 --> 1:05:04.880\n really require access to lots of negatives,\n\n1:05:04.880 --> 1:05:06.920\n which becomes harder and harder to sort of scale\n\n1:05:06.920 --> 1:05:09.000\n when designing a learning algorithm.\n\n1:05:09.000 --> 1:05:10.920\n So that's been one of the reasons\n\n1:05:10.920 --> 1:05:13.680\n why non contrastive methods have become like popular\n\n1:05:13.680 --> 1:05:16.360\n and why people think that they're going to be more useful.\n\n1:05:16.360 --> 1:05:18.440\n So a non contrastive method, for example,\n\n1:05:18.440 --> 1:05:20.880\n like clustering is one non contrastive method.\n\n1:05:20.880 --> 1:05:22.480\n The idea basically being that you have\n\n1:05:22.480 --> 1:05:25.880\n two of these samples, so the cat and dog\n\n1:05:25.880 --> 1:05:27.680\n or two crops of this image,\n\n1:05:27.680 --> 1:05:29.280\n they belong to the same cluster.\n\n1:05:30.400 --> 1:05:33.320\n And so essentially you're basically doing clustering online\n\n1:05:33.320 --> 1:05:35.080\n when you're learning this network,\n\n1:05:35.080 --> 1:05:36.720\n and which is very different from having access\n\n1:05:36.720 --> 1:05:38.920\n to a lot of negatives explicitly.\n\n1:05:38.920 --> 1:05:40.840\n The other way which has become really popular\n\n1:05:40.840 --> 1:05:43.120\n is something called self distillation.\n\n1:05:43.120 --> 1:05:45.680\n So the idea basically is that you have a teacher network\n\n1:05:45.680 --> 1:05:47.480\n and a student network,\n\n1:05:47.480 --> 1:05:49.520\n and the teacher network produces a feature.\n\n1:05:49.520 --> 1:05:51.080\n So it takes in the image\n\n1:05:51.080 --> 1:05:53.680\n and basically the neural network figures out the patterns\n\n1:05:53.680 --> 1:05:55.240\n gets the feature out.\n\n1:05:55.240 --> 1:05:56.800\n And there's another neural network\n\n1:05:56.800 --> 1:05:57.960\n which is the student neural network\n\n1:05:57.960 --> 1:05:59.920\n and that also produces a feature.\n\n1:05:59.920 --> 1:06:01.640\n And now all you're doing is basically saying\n\n1:06:01.640 --> 1:06:03.960\n that the features produced by the teacher network\n\n1:06:03.960 --> 1:06:06.120\n and the student network should be very similar.\n\n1:06:06.120 --> 1:06:06.960\n That's it.\n\n1:06:06.960 --> 1:06:09.200\n There is no notion of a negative anymore.\n\n1:06:09.200 --> 1:06:10.040\n And that's it.\n\n1:06:10.040 --> 1:06:11.800\n So it's all about similarity maximization\n\n1:06:11.800 --> 1:06:13.680\n between these two features.\n\n1:06:13.680 --> 1:06:16.320\n And so all I need to now do is figure out\n\n1:06:16.320 --> 1:06:18.680\n how to have these two sorts of parallel networks,\n\n1:06:18.680 --> 1:06:20.600\n a student network and a teacher network.\n\n1:06:20.600 --> 1:06:23.000\n And basically researchers have figured out\n\n1:06:23.000 --> 1:06:24.240\n very cheap methods to do this.\n\n1:06:24.240 --> 1:06:26.760\n So you can actually have for free really\n\n1:06:26.760 --> 1:06:29.000\n two types of neural networks.\n\n1:06:29.000 --> 1:06:30.120\n They're kind of related,\n\n1:06:30.120 --> 1:06:32.040\n but they're different enough that you can actually\n\n1:06:32.040 --> 1:06:34.000\n basically have a learning problem set up.\n\n1:06:34.000 --> 1:06:38.200\n So you can ensure that they always remain different enough.\n\n1:06:38.200 --> 1:06:41.040\n So the thing doesn't collapse into something boring.\n\n1:06:41.040 --> 1:06:41.880\n Exactly.\n\n1:06:41.880 --> 1:06:44.360\n So the main sort of enemy of self supervised learning,\n\n1:06:44.360 --> 1:06:47.560\n any kind of similarity maximization technique is collapse.\n\n1:06:47.560 --> 1:06:50.520\n It's a collapse means that you learn the same feature\n\n1:06:50.520 --> 1:06:53.160\n representation for all the images in the world,\n\n1:06:53.160 --> 1:06:54.640\n which is completely useless.\n\n1:06:54.640 --> 1:06:55.640\n Everything's a banana.\n\n1:06:55.640 --> 1:06:56.560\n Everything is a banana.\n\n1:06:56.560 --> 1:06:57.400\n Everything is a cat.\n\n1:06:57.400 --> 1:06:59.200\n Everything is a car.\n\n1:06:59.200 --> 1:07:02.120\n And so all we need to do is basically come up with ways\n\n1:07:02.120 --> 1:07:03.320\n to prevent collapse.\n\n1:07:03.320 --> 1:07:05.360\n Contrastive learning is one way of doing it.\n\n1:07:05.360 --> 1:07:07.840\n And then for example, like clustering or self distillation\n\n1:07:07.840 --> 1:07:09.240\n or other ways of doing it.\n\n1:07:09.240 --> 1:07:11.840\n We also had a recent paper where we used like\n\n1:07:11.840 --> 1:07:15.400\n de correlation between like two sets of features\n\n1:07:15.400 --> 1:07:16.760\n to prevent collapse.\n\n1:07:16.760 --> 1:07:18.880\n So that's inspired a little bit by like Horace Barlow's\n\n1:07:18.880 --> 1:07:20.680\n neuroscience principles.\n\n1:07:20.680 --> 1:07:23.520\n By the way, I should comment that whoever counts\n\n1:07:23.520 --> 1:07:27.760\n the number of times the word banana, apple, cat and dog\n\n1:07:27.760 --> 1:07:30.120\n were using this conversation wins the internet.\n\n1:07:30.120 --> 1:07:31.120\n I wish you luck.\n\n1:07:32.240 --> 1:07:36.760\n What is Suave and the main improvement proposed\n\n1:07:36.760 --> 1:07:40.360\n in the paper on supervised learning of visual features\n\n1:07:40.360 --> 1:07:42.960\n by contrasting cluster assignments?\n\n1:07:42.960 --> 1:07:46.400\n Suave basically is a clustering based technique,\n\n1:07:46.400 --> 1:07:49.240\n which is for again, the same thing for self supervised\n\n1:07:49.240 --> 1:07:52.440\n learning in vision where we have two crops.\n\n1:07:52.440 --> 1:07:55.280\n And the idea basically is that you want the features\n\n1:07:55.280 --> 1:07:58.920\n from these two crops of an image to lie in the same cluster\n\n1:07:58.920 --> 1:08:02.520\n and basically crops that are coming from different images\n\n1:08:02.520 --> 1:08:03.960\n to be in different clusters.\n\n1:08:03.960 --> 1:08:05.880\n Now, typically in a sort of,\n\n1:08:05.880 --> 1:08:07.120\n if you were to do this clustering,\n\n1:08:07.120 --> 1:08:09.520\n you would perform clustering offline.\n\n1:08:09.520 --> 1:08:11.040\n What that means is you would,\n\n1:08:11.040 --> 1:08:13.160\n if you have a dataset of N examples,\n\n1:08:13.160 --> 1:08:15.360\n you would run over all of these N examples,\n\n1:08:15.360 --> 1:08:17.520\n get features for them, perform clustering.\n\n1:08:17.520 --> 1:08:19.480\n So basically get some clusters\n\n1:08:19.480 --> 1:08:21.960\n and then repeat the process again.\n\n1:08:21.960 --> 1:08:24.640\n So this is offline basically because I need to do one pass\n\n1:08:24.640 --> 1:08:27.200\n through the data to compute its clusters.\n\n1:08:27.200 --> 1:08:30.200\n Suave is basically just a simple way of doing this online.\n\n1:08:30.200 --> 1:08:31.800\n So as you're going through the data,\n\n1:08:31.800 --> 1:08:34.800\n you're actually computing these clusters online.\n\n1:08:34.800 --> 1:08:37.480\n And so of course there is like a lot of tricks involved\n\n1:08:37.480 --> 1:08:40.120\n in how to do this in a robust manner without collapsing,\n\n1:08:40.120 --> 1:08:42.440\n but this is this sort of key idea to it.\n\n1:08:42.440 --> 1:08:45.480\n Is there a nice way to say what is the key methodology\n\n1:08:45.480 --> 1:08:47.640\n of the clustering that enables that?\n\n1:08:47.640 --> 1:08:51.000\n Right, so the idea basically is that\n\n1:08:51.000 --> 1:08:52.720\n when you have N samples,\n\n1:08:52.720 --> 1:08:54.920\n we assume that we have access to,\n\n1:08:54.920 --> 1:08:57.040\n like there are always K clusters in a dataset.\n\n1:08:57.040 --> 1:08:57.880\n K is a fixed number.\n\n1:08:57.880 --> 1:09:00.160\n So for example, K is 3000.\n\n1:09:00.160 --> 1:09:02.200\n And so if you have any,\n\n1:09:02.200 --> 1:09:04.840\n when you look at any sort of small number of examples,\n\n1:09:04.840 --> 1:09:08.000\n all of them must belong to one of these K clusters.\n\n1:09:08.000 --> 1:09:10.320\n And we impose this equipartition constraint.\n\n1:09:10.320 --> 1:09:13.640\n What this means is that basically\n\n1:09:15.200 --> 1:09:16.880\n your entire set of N samples\n\n1:09:16.880 --> 1:09:19.440\n should be equally partitioned into K clusters.\n\n1:09:19.440 --> 1:09:21.800\n So all your K clusters are basically equal,\n\n1:09:21.800 --> 1:09:24.400\n they have equal contribution to these N samples.\n\n1:09:24.400 --> 1:09:26.520\n And this ensures that we never collapse.\n\n1:09:26.520 --> 1:09:28.280\n So collapse can be viewed as a way\n\n1:09:28.280 --> 1:09:30.640\n in which all samples belong to one cluster, right?\n\n1:09:30.640 --> 1:09:33.160\n So all this, if all features become the same,\n\n1:09:33.160 --> 1:09:35.120\n then you have basically just one mega cluster.\n\n1:09:35.120 --> 1:09:38.120\n You don't even have like 10 clusters or 3000 clusters.\n\n1:09:38.120 --> 1:09:40.960\n So Suave basically ensures that at each point,\n\n1:09:40.960 --> 1:09:42.960\n all these 3000 clusters are being used\n\n1:09:42.960 --> 1:09:45.040\n in the clustering process.\n\n1:09:45.040 --> 1:09:46.240\n And that's it.\n\n1:09:46.240 --> 1:09:48.480\n Basically just figure out how to do this online.\n\n1:09:48.480 --> 1:09:50.960\n And again, basically just make sure\n\n1:09:50.960 --> 1:09:54.160\n that two crops from the same image belong to the same cluster\n\n1:09:54.160 --> 1:09:55.720\n and others don't.\n\n1:09:55.720 --> 1:09:58.840\n And the fact they have a fixed K makes things simpler.\n\n1:09:58.840 --> 1:10:00.360\n Fixed K makes things simpler.\n\n1:10:00.360 --> 1:10:02.560\n Our clustering is not like really hard clustering,\n\n1:10:02.560 --> 1:10:03.720\n it's soft clustering.\n\n1:10:03.720 --> 1:10:06.880\n So basically you can be 0.2 to cluster number one\n\n1:10:06.880 --> 1:10:08.440\n and 0.8 to cluster number two.\n\n1:10:08.440 --> 1:10:09.880\n So it's not really hard.\n\n1:10:09.880 --> 1:10:12.720\n So essentially, even though we have like 3000 clusters,\n\n1:10:12.720 --> 1:10:15.160\n we can actually represent a lot of clusters.\n\n1:10:15.160 --> 1:10:19.200\n What is SEER, S E E R?\n\n1:10:19.200 --> 1:10:23.080\n And what are the key results and insights in the paper,\n\n1:10:23.080 --> 1:10:27.360\n Self Supervised Pre Training of Visual Features in the Wild?\n\n1:10:27.360 --> 1:10:30.680\n What is this big, beautiful SEER system?\n\n1:10:30.680 --> 1:10:32.920\n SEER, so I'll first go to Suave\n\n1:10:32.920 --> 1:10:34.360\n because Suave is actually like one\n\n1:10:34.360 --> 1:10:35.760\n of the key components for SEER.\n\n1:10:35.760 --> 1:10:37.800\n So Suave was, when we use Suave,\n\n1:10:37.800 --> 1:10:39.760\n it was demonstrated on ImageNet.\n\n1:10:39.760 --> 1:10:42.920\n So typically like self supervised methods,\n\n1:10:42.920 --> 1:10:46.160\n the way we sort of operate is like in the research community,\n\n1:10:46.160 --> 1:10:47.160\n we kind of cheat.\n\n1:10:47.160 --> 1:10:49.720\n So we take ImageNet, which of course I talked about\n\n1:10:49.720 --> 1:10:51.280\n as having lots of labels.\n\n1:10:51.280 --> 1:10:52.920\n And then we throw away the labels,\n\n1:10:52.920 --> 1:10:54.920\n like throw away all the hard work that went behind\n\n1:10:54.920 --> 1:10:56.800\n basically the labeling process.\n\n1:10:56.800 --> 1:11:00.240\n And we pretend that it is unsupervised.\n\n1:11:00.240 --> 1:11:02.840\n But the problem here is that we have,\n\n1:11:02.840 --> 1:11:05.120\n like when we collected these images,\n\n1:11:05.120 --> 1:11:08.200\n the ImageNet dataset has a particular distribution\n\n1:11:08.200 --> 1:11:09.920\n of concepts, right?\n\n1:11:09.920 --> 1:11:11.720\n So these images are very curated.\n\n1:11:11.720 --> 1:11:15.240\n And what that means is these images, of course,\n\n1:11:15.240 --> 1:11:17.640\n belong to a certain set of noun concepts.\n\n1:11:17.640 --> 1:11:20.360\n And also ImageNet has this bias that all images\n\n1:11:20.360 --> 1:11:22.440\n contain an object, which is like very big\n\n1:11:22.440 --> 1:11:24.040\n and it's typically in the center.\n\n1:11:24.040 --> 1:11:26.120\n So when you're talking about a dog, it's a well framed dog,\n\n1:11:26.120 --> 1:11:28.320\n it's towards the center of the image.\n\n1:11:28.320 --> 1:11:29.760\n So a lot of the data augmentation,\n\n1:11:29.760 --> 1:11:31.480\n a lot of the sort of hidden assumptions\n\n1:11:31.480 --> 1:11:33.400\n in self supervised learning,\n\n1:11:33.400 --> 1:11:37.360\n actually really exploit this bias of ImageNet.\n\n1:11:37.360 --> 1:11:39.680\n And so, I mean, a lot of my work,\n\n1:11:39.680 --> 1:11:42.000\n a lot of work from other people always uses ImageNet\n\n1:11:42.000 --> 1:11:44.200\n sort of as the benchmark to show the success\n\n1:11:44.200 --> 1:11:45.440\n of self supervised learning.\n\n1:11:45.440 --> 1:11:47.680\n So you're implying that there's particular limitations\n\n1:11:47.680 --> 1:11:49.200\n to this kind of dataset?\n\n1:11:49.200 --> 1:11:51.880\n Yes, I mean, it's basically because our data augmentation\n\n1:11:51.880 --> 1:11:55.320\n that we designed, like all data augmentation\n\n1:11:55.320 --> 1:11:57.480\n that we designed for self supervised learning in vision\n\n1:11:57.480 --> 1:11:59.360\n are kind of overfit to ImageNet.\n\n1:11:59.360 --> 1:12:02.400\n But you're saying a little bit hard coded\n\n1:12:02.400 --> 1:12:03.800\n like the cropping.\n\n1:12:03.800 --> 1:12:05.480\n Exactly, the cropping parameters,\n\n1:12:05.480 --> 1:12:07.280\n the kind of lighting that we're using,\n\n1:12:07.280 --> 1:12:08.800\n the kind of blurring that we're using.\n\n1:12:08.800 --> 1:12:11.960\n Yeah, but you would, for more in the wild dataset,\n\n1:12:11.960 --> 1:12:16.240\n you would need to be clever or more careful\n\n1:12:16.240 --> 1:12:17.520\n in setting the range of parameters\n\n1:12:17.520 --> 1:12:18.920\n and those kinds of things.\n\n1:12:18.920 --> 1:12:21.360\n So for SEER, our main goal was twofold.\n\n1:12:21.360 --> 1:12:24.680\n One, basically to move away from ImageNet for training.\n\n1:12:24.680 --> 1:12:27.680\n So the images that we used were like uncurated images.\n\n1:12:27.680 --> 1:12:28.600\n Now there's a lot of debate\n\n1:12:28.600 --> 1:12:30.040\n whether they're actually curated or not,\n\n1:12:30.040 --> 1:12:32.360\n but I'll talk about that later.\n\n1:12:32.360 --> 1:12:33.880\n But the idea was basically,\n\n1:12:33.880 --> 1:12:36.400\n these are going to be random internet images\n\n1:12:36.400 --> 1:12:37.920\n that we're not going to filter out\n\n1:12:37.920 --> 1:12:40.080\n based on like particular categories.\n\n1:12:40.080 --> 1:12:42.880\n So we did not say that, oh, images that belong to dogs\n\n1:12:42.880 --> 1:12:44.280\n and cats should be the only images\n\n1:12:44.280 --> 1:12:47.000\n that come in this dataset, banana.\n\n1:12:47.000 --> 1:12:50.040\n And basically, other images should be thrown out.\n\n1:12:50.040 --> 1:12:51.800\n So we didn't do any of that.\n\n1:12:51.800 --> 1:12:53.560\n So these are random internet images.\n\n1:12:53.560 --> 1:12:56.040\n And of course, it also goes back to like the problem\n\n1:12:56.040 --> 1:12:57.320\n of scale that you talked about.\n\n1:12:57.320 --> 1:13:00.120\n So these were basically about a billion or so images.\n\n1:13:00.120 --> 1:13:01.560\n And for context ImageNet,\n\n1:13:01.560 --> 1:13:02.800\n the ImageNet version that we use\n\n1:13:02.800 --> 1:13:04.280\n was 1 million images earlier.\n\n1:13:04.280 --> 1:13:05.400\n So this is basically going like\n\n1:13:05.400 --> 1:13:07.600\n three orders of magnitude more.\n\n1:13:07.600 --> 1:13:08.600\n The idea was basically to see\n\n1:13:08.600 --> 1:13:11.800\n if we can train a very large convolutional model\n\n1:13:11.800 --> 1:13:14.440\n in a self supervised way on this uncurated,\n\n1:13:14.440 --> 1:13:16.400\n but really large set of images.\n\n1:13:16.400 --> 1:13:18.280\n And how well would this model do?\n\n1:13:18.280 --> 1:13:21.440\n So is self supervised learning really overfit to ImageNet\n\n1:13:21.440 --> 1:13:23.840\n or can it actually work in the wild?\n\n1:13:23.840 --> 1:13:25.720\n And it was also out of curiosity,\n\n1:13:25.720 --> 1:13:27.520\n what kind of things will this model learn?\n\n1:13:27.520 --> 1:13:30.080\n Will it actually be able to still figure out\n\n1:13:30.080 --> 1:13:32.000\n different types of objects and so on?\n\n1:13:32.000 --> 1:13:33.720\n Would there be particular kinds of tasks\n\n1:13:33.720 --> 1:13:38.160\n that would actually do better than an ImageNet train model?\n\n1:13:38.160 --> 1:13:40.960\n And so for Sear, one of our main findings was that\n\n1:13:40.960 --> 1:13:43.120\n we can actually train very large models\n\n1:13:43.120 --> 1:13:44.800\n in a completely self supervised way\n\n1:13:44.800 --> 1:13:46.360\n on lots of internet images\n\n1:13:46.360 --> 1:13:48.600\n without really necessarily filtering them out.\n\n1:13:48.600 --> 1:13:49.760\n Which was in itself a good thing\n\n1:13:49.760 --> 1:13:51.960\n because it's a fairly simple process, right?\n\n1:13:51.960 --> 1:13:54.080\n So you get images which are uploaded\n\n1:13:54.080 --> 1:13:55.800\n and you basically can immediately use them\n\n1:13:55.800 --> 1:13:57.680\n to train a model in an unsupervised way.\n\n1:13:57.680 --> 1:13:59.720\n You don't really need to sit and filter them out.\n\n1:13:59.720 --> 1:14:02.040\n These images can be cartoons, these can be memes,\n\n1:14:02.040 --> 1:14:04.440\n these can be actual pictures uploaded by people.\n\n1:14:04.440 --> 1:14:06.160\n And you don't really care about what these images are.\n\n1:14:06.160 --> 1:14:08.520\n You don't even care about what concepts they contain.\n\n1:14:08.520 --> 1:14:10.280\n So this was a very sort of simple setup.\n\n1:14:10.280 --> 1:14:12.880\n What image selection mechanism would you say\n\n1:14:12.880 --> 1:14:17.880\n is there like inherent in some aspect of the process?\n\n1:14:18.840 --> 1:14:21.280\n So you're kind of implying that there's almost none,\n\n1:14:21.280 --> 1:14:24.960\n but what is there would you say if you were to introspect?\n\n1:14:24.960 --> 1:14:28.480\n Right, so it's not like uncurated can basically\n\n1:14:28.480 --> 1:14:30.400\n like one way of imagining uncurated\n\n1:14:30.400 --> 1:14:32.920\n is basically you have like cameras\n\n1:14:32.920 --> 1:14:35.200\n that can take pictures at random viewpoints.\n\n1:14:35.200 --> 1:14:37.400\n When people upload pictures to the internet,\n\n1:14:37.400 --> 1:14:40.320\n they are typically going to care about the framing of it.\n\n1:14:40.320 --> 1:14:41.840\n They're not going to upload, say,\n\n1:14:41.840 --> 1:14:43.800\n the picture of a zoomed in wall, for example.\n\n1:14:43.800 --> 1:14:46.080\n Well, when you say internet, do you mean social networks?\n\n1:14:46.080 --> 1:14:47.160\n Yes. Okay.\n\n1:14:47.160 --> 1:14:48.680\n So these are not going to be like pictures\n\n1:14:48.680 --> 1:14:51.400\n of like a zoomed in table or a zoomed in wall.\n\n1:14:51.400 --> 1:14:53.160\n So it's not really completely uncurated\n\n1:14:53.160 --> 1:14:55.800\n because people do have the like photographer's bias\n\n1:14:55.800 --> 1:14:57.040\n where they do want to keep things\n\n1:14:57.040 --> 1:14:58.640\n towards the center a little bit,\n\n1:14:58.640 --> 1:15:01.320\n or like really have like nice looking things\n\n1:15:01.320 --> 1:15:02.680\n and so on in the picture.\n\n1:15:02.680 --> 1:15:05.640\n So that's the kind of bias that typically exists\n\n1:15:05.640 --> 1:15:07.720\n in this data set and also the user base, right?\n\n1:15:07.720 --> 1:15:09.320\n You're not going to get lots of pictures\n\n1:15:09.320 --> 1:15:10.520\n from different parts of the world\n\n1:15:10.520 --> 1:15:12.120\n because there are certain parts of the world\n\n1:15:12.120 --> 1:15:14.320\n where people may not actually be uploading\n\n1:15:14.320 --> 1:15:15.440\n a lot of pictures to the internet\n\n1:15:15.440 --> 1:15:17.360\n or may not even have access to a lot of internet.\n\n1:15:17.360 --> 1:15:21.720\n So this is a giant data set and a giant neural network.\n\n1:15:21.720 --> 1:15:24.800\n I don't think we've talked about what architectures\n\n1:15:24.800 --> 1:15:29.320\n work well for SSL, for self supervised learning.\n\n1:15:29.320 --> 1:15:32.480\n For SEER and for SWAB, we were using convolutional networks,\n\n1:15:32.480 --> 1:15:34.160\n but recently in a work called Dyno,\n\n1:15:34.160 --> 1:15:36.880\n we've basically started using transformers for vision.\n\n1:15:36.880 --> 1:15:39.840\n Both seem to work really well, Connets and transformers.\n\n1:15:39.840 --> 1:15:41.120\n And depending on what you want to do,\n\n1:15:41.120 --> 1:15:43.560\n you might choose to use a particular formulation.\n\n1:15:43.560 --> 1:15:45.400\n So for SEER, it was a Connet.\n\n1:15:45.400 --> 1:15:47.480\n It was particularly a RegNet model,\n\n1:15:47.480 --> 1:15:49.720\n which was also a work from Facebook.\n\n1:15:49.720 --> 1:15:52.640\n RegNets are like really good when it comes to compute\n\n1:15:52.640 --> 1:15:54.760\n versus like accuracy.\n\n1:15:54.760 --> 1:15:56.920\n So because it was a very efficient model,\n\n1:15:56.920 --> 1:15:59.680\n compute and memory wise efficient,\n\n1:15:59.680 --> 1:16:02.480\n and basically it worked really well in terms of scaling.\n\n1:16:02.480 --> 1:16:04.200\n So we used a very large RegNet model\n\n1:16:04.200 --> 1:16:05.480\n and trained it on a billion images.\n\n1:16:05.480 --> 1:16:08.640\n Can you maybe quickly comment on what RegNets are?\n\n1:16:09.680 --> 1:16:13.520\n It comes from this paper, Designing Network Design Spaces.\n\n1:16:13.520 --> 1:16:15.520\n This is a super interesting concept\n\n1:16:15.520 --> 1:16:18.400\n that emphasizes how to create efficient neural networks,\n\n1:16:18.400 --> 1:16:19.520\n large neural networks.\n\n1:16:19.520 --> 1:16:21.800\n So one of the sort of key takeaways from this paper,\n\n1:16:21.800 --> 1:16:23.400\n which the authors, like whenever you hear them\n\n1:16:23.400 --> 1:16:26.040\n present this work, they keep saying is,\n\n1:16:26.040 --> 1:16:27.960\n a lot of neural networks are characterized\n\n1:16:27.960 --> 1:16:29.040\n in terms of flops, right?\n\n1:16:29.040 --> 1:16:31.480\n Flops basically being the floating point operations.\n\n1:16:31.480 --> 1:16:33.320\n And people really love to use flops to say,\n\n1:16:33.320 --> 1:16:36.200\n this model is like really computationally heavy,\n\n1:16:36.200 --> 1:16:39.000\n or like our model is computationally cheap and so on.\n\n1:16:39.000 --> 1:16:41.880\n Now it turns out that flops are really not a good indicator\n\n1:16:41.880 --> 1:16:43.840\n of how well a particular network is,\n\n1:16:43.840 --> 1:16:45.960\n like how efficient it is really.\n\n1:16:45.960 --> 1:16:49.120\n And what a better indicator is, is the activation\n\n1:16:49.120 --> 1:16:52.160\n or the memory that is being used by this particular model.\n\n1:16:52.160 --> 1:16:55.000\n And so designing, like one of the key findings\n\n1:16:55.000 --> 1:16:57.400\n from this paper was basically that you need to design\n\n1:16:57.400 --> 1:17:00.160\n network families or neural network architectures\n\n1:17:00.160 --> 1:17:02.800\n that are actually very efficient in the memory space as well,\n\n1:17:02.800 --> 1:17:04.840\n not just in terms of pure flops.\n\n1:17:04.840 --> 1:17:07.600\n So RegNet is basically a network architecture family\n\n1:17:07.600 --> 1:17:10.280\n that came out of this paper that is particularly good\n\n1:17:10.280 --> 1:17:13.600\n at both flops and the sort of memory required for it.\n\n1:17:13.600 --> 1:17:15.800\n And of course it builds upon like earlier work,\n\n1:17:15.800 --> 1:17:18.640\n like ResNet being like the sort of more popular inspiration\n\n1:17:18.640 --> 1:17:20.440\n for it, where you have residual connections.\n\n1:17:20.440 --> 1:17:22.440\n But one of the things in this work is basically\n\n1:17:22.440 --> 1:17:25.120\n they also use like squeeze excitation blocks.\n\n1:17:25.120 --> 1:17:27.120\n So it's a lot of nice sort of technical innovation\n\n1:17:27.120 --> 1:17:28.760\n in all of this from prior work,\n\n1:17:28.760 --> 1:17:31.440\n and a lot of the ingenuity of these particular authors\n\n1:17:31.440 --> 1:17:34.160\n in how to combine these multiple building blocks.\n\n1:17:34.160 --> 1:17:36.880\n But the key constraint was optimize for both flops\n\n1:17:36.880 --> 1:17:38.360\n and memory when you're basically doing this,\n\n1:17:38.360 --> 1:17:39.600\n don't just look at flops.\n\n1:17:39.600 --> 1:17:42.360\n And that allows you to what have a,\n\n1:17:42.360 --> 1:17:47.320\n sort of have very large networks through this process,\n\n1:17:47.320 --> 1:17:51.280\n can optimize for low, like for efficiency, for low memory.\n\n1:17:51.280 --> 1:17:53.600\n Also in just in terms of pure hardware,\n\n1:17:53.600 --> 1:17:55.880\n they fit very well on GPU memory.\n\n1:17:55.880 --> 1:17:57.920\n So they can be like really powerful neural network\n\n1:17:57.920 --> 1:18:00.200\n architectures with lots of parameters, lots of flops,\n\n1:18:00.200 --> 1:18:02.760\n but also because they're like efficient in terms of\n\n1:18:02.760 --> 1:18:04.040\n the amount of memory that they're using,\n\n1:18:04.040 --> 1:18:06.600\n you can actually fit a lot of these on like a,\n\n1:18:06.600 --> 1:18:09.600\n you can fit a very large model on a single GPU for example.\n\n1:18:09.600 --> 1:18:14.280\n Would you say that the choice of architecture\n\n1:18:14.280 --> 1:18:17.640\n matters more than the choice of maybe data augmentation\n\n1:18:17.640 --> 1:18:18.560\n techniques?\n\n1:18:18.560 --> 1:18:21.720\n Is there a possibility to say what matters more?\n\n1:18:21.720 --> 1:18:24.400\n You kind of imply that you can probably go really far\n\n1:18:24.400 --> 1:18:27.600\n with just using basic conv nuts.\n\n1:18:27.600 --> 1:18:30.600\n All right, I think like data and data augmentation,\n\n1:18:30.600 --> 1:18:33.280\n the algorithm being used for the self supervised training\n\n1:18:33.280 --> 1:18:36.400\n matters a lot more than the particular kind of architecture.\n\n1:18:36.400 --> 1:18:37.680\n With different types of architecture,\n\n1:18:37.680 --> 1:18:40.320\n you will get different like properties in the resulting\n\n1:18:40.320 --> 1:18:41.720\n sort of representation.\n\n1:18:41.720 --> 1:18:44.640\n But really, I mean, the secret sauce is in the augmentation\n\n1:18:44.640 --> 1:18:47.080\n and the algorithm being used to train them.\n\n1:18:47.080 --> 1:18:49.240\n The architectures, I mean, at this point,\n\n1:18:49.240 --> 1:18:51.680\n a lot of them perform very similarly,\n\n1:18:51.680 --> 1:18:53.840\n depending on like the particular task that you care about,\n\n1:18:53.840 --> 1:18:56.400\n they have certain advantages and disadvantages.\n\n1:18:56.400 --> 1:18:58.680\n Is there something interesting to be said about what it\n\n1:18:58.680 --> 1:19:01.920\n takes with Sears to train a giant neural network?\n\n1:19:01.920 --> 1:19:04.160\n You're talking about a huge amount of data,\n\n1:19:04.160 --> 1:19:05.800\n a huge neural network.\n\n1:19:05.800 --> 1:19:08.280\n Is there something interesting to be said of how to\n\n1:19:08.280 --> 1:19:11.280\n effectively train something like that fast?\n\n1:19:11.280 --> 1:19:13.000\n Lots of GPUs.\n\n1:19:13.000 --> 1:19:13.840\n Okay.\n\n1:19:15.480 --> 1:19:18.800\n I mean, so the model was like a billion parameters.\n\n1:19:18.800 --> 1:19:20.840\n And it was trained on a billion images.\n\n1:19:20.840 --> 1:19:23.320\n So if like, basically the same number of parameters\n\n1:19:23.320 --> 1:19:26.160\n as the number of images, and it took a while.\n\n1:19:26.160 --> 1:19:28.600\n I don't remember the exact number, it's in the paper,\n\n1:19:28.600 --> 1:19:29.600\n but it took a while.\n\n1:19:31.840 --> 1:19:34.640\n I guess I'm trying to get at is,\n\n1:19:34.640 --> 1:19:38.680\n when you're thinking of scaling this kind of thing,\n\n1:19:38.680 --> 1:19:42.600\n I mean, one of the exciting possibilities of self\n\n1:19:42.600 --> 1:19:45.920\n supervised learning is the several orders of magnitude\n\n1:19:45.920 --> 1:19:49.000\n scaling of everything, both the neural network\n\n1:19:49.000 --> 1:19:50.920\n and the size of the data.\n\n1:19:50.920 --> 1:19:52.600\n And so the question is,\n\n1:19:52.600 --> 1:19:56.520\n do you think there's some interesting tricks to do large\n\n1:19:56.520 --> 1:19:57.880\n scale distributed compute,\n\n1:19:57.880 --> 1:20:00.920\n or is that really outside of even deep learning?\n\n1:20:00.920 --> 1:20:04.360\n That's more about like hardware engineering.\n\n1:20:04.360 --> 1:20:07.240\n I think more and more there is like this,\n\n1:20:07.240 --> 1:20:10.160\n a lot of like systems are designed,\n\n1:20:10.160 --> 1:20:11.400\n basically taking into account\n\n1:20:11.400 --> 1:20:12.520\n the machine learning needs, right?\n\n1:20:12.520 --> 1:20:14.760\n So because whenever you're doing this kind of\n\n1:20:14.760 --> 1:20:17.040\n distributed training, there is a lot of intercommunication\n\n1:20:17.040 --> 1:20:17.880\n between nodes.\n\n1:20:17.880 --> 1:20:20.680\n So like gradients or the model parameters are being passed.\n\n1:20:20.680 --> 1:20:22.840\n So you really want to minimize communication costs\n\n1:20:22.840 --> 1:20:25.280\n when you really want to scale these models up.\n\n1:20:25.280 --> 1:20:29.240\n You want basically to be able to do as much,\n\n1:20:29.240 --> 1:20:31.520\n like as limited amount of communication as possible.\n\n1:20:31.520 --> 1:20:33.320\n So currently like a dominant paradigm\n\n1:20:33.320 --> 1:20:35.160\n is synchronized sort of training.\n\n1:20:35.160 --> 1:20:38.520\n So essentially after every sort of gradient step,\n\n1:20:38.520 --> 1:20:41.240\n all you basically have like a synchronization step\n\n1:20:41.240 --> 1:20:43.440\n between all the sort of compute chips\n\n1:20:43.440 --> 1:20:44.800\n that you're going on with.\n\n1:20:45.720 --> 1:20:47.880\n I think asynchronous training was popular,\n\n1:20:47.880 --> 1:20:50.440\n but it doesn't seem to perform as well.\n\n1:20:50.440 --> 1:20:53.400\n But in general, I think that's sort of the,\n\n1:20:53.400 --> 1:20:55.320\n I guess it's outside my scope as well.\n\n1:20:55.320 --> 1:21:00.000\n But the main thing is like minimize the amount of\n\n1:21:00.000 --> 1:21:01.960\n synchronization steps that you have.\n\n1:21:01.960 --> 1:21:04.680\n That has been the key takeaway, at least in my experience.\n\n1:21:04.680 --> 1:21:06.600\n The others I have no idea about, how to design the chip.\n\n1:21:06.600 --> 1:21:11.200\n Yeah, there's very few things that I see Jim Keller's eyes\n\n1:21:11.200 --> 1:21:14.200\n light up as much as talking about giant computers doing\n\n1:21:15.360 --> 1:21:18.040\n like that fast communication that you're talking to well\n\n1:21:18.040 --> 1:21:21.240\n when they're training machine learning systems.\n\n1:21:21.240 --> 1:21:26.240\n What is VSSL, V I S S L, the PyTorch based SSL library?\n\n1:21:27.880 --> 1:21:30.120\n What are the use cases that you might have?\n\n1:21:30.120 --> 1:21:33.040\n VSSL basically was born out of a lot of us at Facebook\n\n1:21:33.040 --> 1:21:35.120\n are doing the self supervised learning research.\n\n1:21:35.120 --> 1:21:38.800\n So it's a common framework in which we have like a lot of\n\n1:21:38.800 --> 1:21:41.720\n self supervised learning methods implemented for vision.\n\n1:21:41.720 --> 1:21:45.920\n It's also, it has in itself like a benchmark of tasks\n\n1:21:45.920 --> 1:21:48.800\n that you can evaluate the self supervised representations on.\n\n1:21:48.800 --> 1:21:51.640\n So the use case for it is basically for anyone who's either\n\n1:21:51.640 --> 1:21:53.760\n trying to evaluate their self supervised model\n\n1:21:53.760 --> 1:21:56.000\n or train their self supervised model,\n\n1:21:56.000 --> 1:21:57.800\n or a researcher who's trying to build\n\n1:21:57.800 --> 1:21:59.240\n a new self supervised technique.\n\n1:21:59.240 --> 1:22:01.520\n So it's basically supposed to be all of these things.\n\n1:22:01.520 --> 1:22:04.480\n So as a researcher before VSSL, for example,\n\n1:22:04.480 --> 1:22:06.960\n or like when we started doing this work fairly seriously\n\n1:22:06.960 --> 1:22:09.960\n at Facebook, it was very hard for us to go and implement\n\n1:22:09.960 --> 1:22:11.880\n every self supervised learning model,\n\n1:22:11.880 --> 1:22:14.560\n test it out in a like sort of consistent manner.\n\n1:22:14.560 --> 1:22:16.440\n The experimental setup was very different\n\n1:22:16.440 --> 1:22:18.160\n across different groups.\n\n1:22:18.160 --> 1:22:20.440\n Even when someone said that they were reporting\n\n1:22:20.440 --> 1:22:23.200\n image net accuracy, it could mean lots of different things.\n\n1:22:23.200 --> 1:22:25.400\n So with VSSL, we tried to really sort of standardize that\n\n1:22:25.400 --> 1:22:26.400\n as much as possible.\n\n1:22:26.400 --> 1:22:28.280\n And there was a paper like we did in 2019\n\n1:22:28.280 --> 1:22:29.800\n just about benchmarking.\n\n1:22:29.800 --> 1:22:32.880\n And so VSSL basically builds upon a lot of this kind of work\n\n1:22:32.880 --> 1:22:35.160\n that we did about like benchmarking.\n\n1:22:35.160 --> 1:22:37.200\n And then every time we try to like,\n\n1:22:37.200 --> 1:22:39.000\n we come up with a self supervised learning method,\n\n1:22:39.000 --> 1:22:41.240\n a lot of us try to push that into VSSL as well,\n\n1:22:41.240 --> 1:22:43.480\n just so that it basically is like the central piece\n\n1:22:43.480 --> 1:22:46.400\n where a lot of these methods can reside.\n\n1:22:46.400 --> 1:22:49.240\n Just out of curiosity, people may be,\n\n1:22:49.240 --> 1:22:52.040\n so certainly outside of Facebook, but just researchers,\n\n1:22:52.040 --> 1:22:54.960\n or just even people that know how to program in Python\n\n1:22:54.960 --> 1:22:58.680\n and know how to use PyTorch, what would be the use case?\n\n1:22:58.680 --> 1:23:01.360\n What would be a fun thing to play around with VSSL on?\n\n1:23:01.360 --> 1:23:04.320\n Like what's a fun thing to play around\n\n1:23:04.320 --> 1:23:07.960\n with self supervised learning on, would you say?\n\n1:23:07.960 --> 1:23:09.800\n Is there a good Hello World program?\n\n1:23:09.800 --> 1:23:14.640\n Like is it always about big size that's important to have,\n\n1:23:14.640 --> 1:23:18.880\n or is there fun little smaller case playgrounds\n\n1:23:18.880 --> 1:23:19.760\n to play around with?\n\n1:23:19.760 --> 1:23:22.440\n So we're trying to like push something towards that.\n\n1:23:22.440 --> 1:23:24.360\n I think there are a few setups out there,\n\n1:23:24.360 --> 1:23:26.840\n but nothing like super standard on the smaller scale.\n\n1:23:26.840 --> 1:23:29.320\n I mean, ImageNet in itself is actually pretty big also.\n\n1:23:29.320 --> 1:23:31.440\n So that is not something\n\n1:23:31.440 --> 1:23:33.520\n which is like feasible for a lot of people.\n\n1:23:33.520 --> 1:23:34.880\n But we are trying to like push up\n\n1:23:34.880 --> 1:23:36.400\n with like smaller sort of use cases.\n\n1:23:36.400 --> 1:23:39.000\n The thing is, at a smaller scale,\n\n1:23:39.000 --> 1:23:40.320\n a lot of the observations\n\n1:23:40.320 --> 1:23:41.800\n or a lot of the algorithms that work\n\n1:23:41.800 --> 1:23:43.760\n don't necessarily translate into the medium\n\n1:23:43.760 --> 1:23:45.000\n or the larger scale.\n\n1:23:45.000 --> 1:23:46.160\n So it's really tricky to come up\n\n1:23:46.160 --> 1:23:47.480\n with a good small scale setup\n\n1:23:47.480 --> 1:23:49.160\n where a lot of your empirical observations\n\n1:23:49.160 --> 1:23:51.560\n will really translate to the other setup.\n\n1:23:51.560 --> 1:23:53.280\n So it's been really challenging.\n\n1:23:53.280 --> 1:23:54.920\n I've been trying to do that for a little bit as well\n\n1:23:54.920 --> 1:23:56.880\n because it does take time to train stuff on ImageNet.\n\n1:23:56.880 --> 1:23:59.880\n It does take time to train on like more images,\n\n1:23:59.880 --> 1:24:02.240\n but pretty much every time I've tried to do that,\n\n1:24:02.240 --> 1:24:03.080\n it's been unsuccessful\n\n1:24:03.080 --> 1:24:04.480\n because all the observations I draw\n\n1:24:04.480 --> 1:24:07.440\n from my set of experiments on a smaller data set\n\n1:24:07.440 --> 1:24:09.440\n don't translate into ImageNet\n\n1:24:09.440 --> 1:24:11.760\n or like don't translate into another sort of data set.\n\n1:24:11.760 --> 1:24:14.240\n So it's been hard for us to figure this one out,\n\n1:24:14.240 --> 1:24:15.760\n but it's an important problem.\n\n1:24:15.760 --> 1:24:17.960\n So there's this really interesting idea\n\n1:24:17.960 --> 1:24:20.840\n of learning across multiple modalities.\n\n1:24:20.840 --> 1:24:25.840\n You have a CVPR 2021 best paper candidate\n\n1:24:26.400 --> 1:24:29.280\n titled audio visual instance discrimination\n\n1:24:29.280 --> 1:24:31.440\n with cross modal agreement.\n\n1:24:31.440 --> 1:24:33.880\n What are the key results, insights in this paper\n\n1:24:33.880 --> 1:24:35.240\n and what can you say in general\n\n1:24:35.240 --> 1:24:37.640\n about the promise and power of multimodal learning?\n\n1:24:37.640 --> 1:24:40.000\n For this paper, it actually came as a little bit\n\n1:24:40.000 --> 1:24:41.960\n of a shock to me at how well it worked.\n\n1:24:41.960 --> 1:24:44.160\n So I can describe what the problem set up was.\n\n1:24:44.160 --> 1:24:46.560\n So it's been used in the past by lots of folks\n\n1:24:46.560 --> 1:24:48.400\n like for example, Andrew Owens from MIT,\n\n1:24:48.400 --> 1:24:49.960\n Alyosha Efros from Berkeley,\n\n1:24:49.960 --> 1:24:51.160\n Andrew Zisserman from Oxford.\n\n1:24:51.160 --> 1:24:52.200\n So a lot of these people have been\n\n1:24:52.200 --> 1:24:53.840\n sort of showing results in this.\n\n1:24:53.840 --> 1:24:55.520\n Of course, I was aware of this result,\n\n1:24:55.520 --> 1:24:58.600\n but I wasn't really sure how well it would work in practice\n\n1:24:58.600 --> 1:25:00.600\n for like other sort of downstream tasks.\n\n1:25:00.600 --> 1:25:02.440\n So the results kept getting better.\n\n1:25:02.440 --> 1:25:04.200\n And I wasn't sure if like a lot of our insights\n\n1:25:04.200 --> 1:25:05.920\n from self supervised learning would translate\n\n1:25:05.920 --> 1:25:08.320\n into this multimodal learning problem.\n\n1:25:08.320 --> 1:25:11.880\n So multimodal learning is when you have like,\n\n1:25:12.880 --> 1:25:14.280\n when you have multiple modalities.\n\n1:25:14.280 --> 1:25:15.680\n That's not even cool.\n\n1:25:15.680 --> 1:25:19.400\n Okay, so the particular modalities\n\n1:25:19.400 --> 1:25:22.040\n that we worked on in this work were audio and video.\n\n1:25:22.040 --> 1:25:23.920\n So the idea was basically, if you have a video,\n\n1:25:23.920 --> 1:25:25.880\n you have its corresponding audio track.\n\n1:25:25.880 --> 1:25:27.560\n And you want to use both of these signals,\n\n1:25:27.560 --> 1:25:29.280\n the audio signal and the video signal\n\n1:25:29.280 --> 1:25:31.280\n to learn a good representation for video\n\n1:25:31.280 --> 1:25:32.720\n and good representation for audio.\n\n1:25:32.720 --> 1:25:33.720\n Like this podcast.\n\n1:25:33.720 --> 1:25:35.480\n Like this podcast, exactly.\n\n1:25:35.480 --> 1:25:38.160\n So what we did in this work was basically train\n\n1:25:38.160 --> 1:25:39.400\n two different neural networks,\n\n1:25:39.400 --> 1:25:41.960\n one on the video signal, one on the audio signal.\n\n1:25:41.960 --> 1:25:43.800\n And what we wanted is basically the features\n\n1:25:43.800 --> 1:25:45.400\n that we get from both of these neural networks\n\n1:25:45.400 --> 1:25:46.800\n should be similar.\n\n1:25:46.800 --> 1:25:48.720\n So it should basically be able to produce\n\n1:25:48.720 --> 1:25:51.120\n the same kinds of features from the video\n\n1:25:51.120 --> 1:25:53.240\n and the same kinds of features from the audio.\n\n1:25:53.240 --> 1:25:54.280\n Now, why is this useful?\n\n1:25:54.280 --> 1:25:56.680\n Well, for a lot of these objects that we have,\n\n1:25:56.680 --> 1:25:58.280\n there is a characteristic sound, right?\n\n1:25:58.280 --> 1:25:59.520\n So trains, when they go by,\n\n1:25:59.520 --> 1:26:00.760\n they make a particular kind of sound.\n\n1:26:00.760 --> 1:26:02.480\n Boats make a particular kind of sound.\n\n1:26:02.480 --> 1:26:03.840\n People, when they're jumping around,\n\n1:26:03.840 --> 1:26:06.240\n will like shout, whatever.\n\n1:26:06.240 --> 1:26:07.280\n Bananas don't make a sound.\n\n1:26:07.280 --> 1:26:09.400\n So where you can't learn anything about bananas there.\n\n1:26:09.400 --> 1:26:11.640\n Or when humans mentioned bananas.\n\n1:26:11.640 --> 1:26:13.520\n Well, yes, when they say the word banana, then.\n\n1:26:13.520 --> 1:26:15.080\n So you can't trust basically anything\n\n1:26:15.080 --> 1:26:17.120\n that comes out of a human's mouth as a source,\n\n1:26:17.120 --> 1:26:19.040\n that source of audio is useless.\n\n1:26:19.040 --> 1:26:20.640\n The typical use case is basically like,\n\n1:26:20.640 --> 1:26:22.440\n for example, someone playing a musical instrument.\n\n1:26:22.440 --> 1:26:24.720\n So guitars have a particular kind of sound and so on.\n\n1:26:24.720 --> 1:26:27.120\n So because a lot of these things are correlated,\n\n1:26:27.120 --> 1:26:28.480\n the idea in multimodal learning\n\n1:26:28.480 --> 1:26:30.160\n is to take these two kinds of modalities,\n\n1:26:30.160 --> 1:26:33.160\n video and audio, and learn a common embedding space,\n\n1:26:33.160 --> 1:26:35.240\n a common feature space where both of these\n\n1:26:35.240 --> 1:26:38.560\n related modalities can basically be close together.\n\n1:26:38.560 --> 1:26:40.600\n And again, you use contrastive learning for this.\n\n1:26:40.600 --> 1:26:43.360\n So in contrastive learning, basically the video\n\n1:26:43.360 --> 1:26:45.520\n and the corresponding audio are positives.\n\n1:26:45.520 --> 1:26:48.200\n And you can take any other video or any other audio\n\n1:26:48.200 --> 1:26:49.840\n and that becomes a negative.\n\n1:26:49.840 --> 1:26:51.000\n And so basically that's it.\n\n1:26:51.000 --> 1:26:53.720\n It's just a simple application of contrastive learning.\n\n1:26:53.720 --> 1:26:55.920\n The main sort of finding from this work for us\n\n1:26:56.840 --> 1:26:58.680\n was basically that you can actually learn\n\n1:26:58.680 --> 1:27:00.760\n very, very powerful feature representations,\n\n1:27:00.760 --> 1:27:02.840\n very, very powerful video representations.\n\n1:27:02.840 --> 1:27:05.400\n So you can learn the sort of video network\n\n1:27:05.400 --> 1:27:07.480\n that we ended up learning can actually be used\n\n1:27:07.480 --> 1:27:11.000\n for downstream, for example, recognizing human actions\n\n1:27:11.000 --> 1:27:14.440\n or recognizing different types of sounds, for example.\n\n1:27:14.440 --> 1:27:17.160\n So this was sort of the key finding.\n\n1:27:17.160 --> 1:27:20.200\n Can you give kind of an example of a human action\n\n1:27:20.200 --> 1:27:23.400\n or like just so we can build up intuition\n\n1:27:23.400 --> 1:27:24.360\n of what kind of thing?\n\n1:27:24.360 --> 1:27:26.880\n Right, so there is this data set called kinetics,\n\n1:27:26.880 --> 1:27:28.640\n for example, which has like 400 different types\n\n1:27:28.640 --> 1:27:29.480\n of human actions.\n\n1:27:29.480 --> 1:27:32.880\n So people jumping, people doing different kinds of sports\n\n1:27:32.880 --> 1:27:34.240\n or different types of swimming.\n\n1:27:34.240 --> 1:27:37.600\n So like different strokes and swimming, golf and so on.\n\n1:27:37.600 --> 1:27:39.640\n So there are like just different types of actions\n\n1:27:39.640 --> 1:27:40.560\n right there.\n\n1:27:40.560 --> 1:27:42.600\n And the point is this kind of video network\n\n1:27:42.600 --> 1:27:44.360\n that you learn in a self supervised way\n\n1:27:44.360 --> 1:27:46.920\n can be used very easily to kind of recognize\n\n1:27:46.920 --> 1:27:48.880\n these different types of actions.\n\n1:27:48.880 --> 1:27:50.440\n It can also be used for recognizing\n\n1:27:50.440 --> 1:27:51.760\n different types of objects.\n\n1:27:53.120 --> 1:27:54.760\n And what we did is we tried to visualize\n\n1:27:54.760 --> 1:27:56.080\n whether the network can figure out\n\n1:27:56.080 --> 1:27:57.880\n where the sound is coming from.\n\n1:27:57.880 --> 1:27:59.840\n So basically, give it a video\n\n1:27:59.840 --> 1:28:03.000\n and basically play say of a person just strumming a guitar,\n\n1:28:03.000 --> 1:28:04.760\n but of course, there is no audio in this.\n\n1:28:04.760 --> 1:28:07.160\n And now you give it this sound of a guitar.\n\n1:28:07.160 --> 1:28:08.880\n And you ask like basically try to visualize\n\n1:28:08.880 --> 1:28:12.520\n where the network thinks the sound is coming from.\n\n1:28:12.520 --> 1:28:14.560\n And that can kind of basically draw like\n\n1:28:14.560 --> 1:28:15.400\n when you visualize it,\n\n1:28:15.400 --> 1:28:17.480\n you can see that it's basically focusing on the guitar.\n\n1:28:17.480 --> 1:28:18.320\n Yeah, that's surreal.\n\n1:28:18.320 --> 1:28:20.160\n And the same thing, for example,\n\n1:28:20.160 --> 1:28:21.480\n for certain people's voices,\n\n1:28:21.480 --> 1:28:22.920\n like famous celebrities voices,\n\n1:28:22.920 --> 1:28:26.040\n it can actually figure out where their mouth is.\n\n1:28:26.040 --> 1:28:28.600\n So it can actually distinguish different people's voices,\n\n1:28:28.600 --> 1:28:30.480\n for example, a little bit as well.\n\n1:28:30.480 --> 1:28:33.680\n Without that ever being annotated in any way.\n\n1:28:33.680 --> 1:28:35.520\n Right, so this is all what it had discovered.\n\n1:28:35.520 --> 1:28:38.200\n We never pointed out that this is a guitar\n\n1:28:38.200 --> 1:28:40.080\n and this is the kind of sound it produces.\n\n1:28:40.080 --> 1:28:41.520\n It can actually naturally figure that out\n\n1:28:41.520 --> 1:28:44.200\n because it's seen so many correlations of this sound\n\n1:28:44.200 --> 1:28:46.680\n coming with this kind of like an object\n\n1:28:46.680 --> 1:28:49.040\n that it basically learns to associate this sound\n\n1:28:49.040 --> 1:28:50.000\n with this kind of an object.\n\n1:28:50.000 --> 1:28:52.760\n Yeah, that's really fascinating, right?\n\n1:28:52.760 --> 1:28:53.600\n That's really interesting.\n\n1:28:53.600 --> 1:28:55.200\n So the idea with this kind of network\n\n1:28:55.200 --> 1:28:57.920\n is then you then fine tune it for a particular task.\n\n1:28:57.920 --> 1:29:01.880\n So this is forming like a really good knowledge base\n\n1:29:01.880 --> 1:29:04.320\n within a neural network based on which you could then\n\n1:29:04.320 --> 1:29:07.720\n the train a little bit more to accomplish a specific task.\n\n1:29:07.720 --> 1:29:11.680\n Well, so you don't need a lot of videos of humans\n\n1:29:11.680 --> 1:29:12.800\n doing actions annotated.\n\n1:29:12.800 --> 1:29:16.040\n You can just use a few of them to basically get your.\n\n1:29:16.040 --> 1:29:18.520\n How much insight do you draw from the fact\n\n1:29:18.520 --> 1:29:22.560\n that it can figure out where the sound is coming from?\n\n1:29:23.440 --> 1:29:26.160\n I'm trying to see, so that's kind of very,\n\n1:29:26.160 --> 1:29:28.520\n it's very CVPR beautiful, right?\n\n1:29:28.520 --> 1:29:30.000\n It's a cool little insight.\n\n1:29:30.000 --> 1:29:33.000\n I wonder how profound that is.\n\n1:29:33.000 --> 1:29:38.000\n Does it speak to the idea that multiple modalities\n\n1:29:39.320 --> 1:29:44.120\n are somehow much bigger than the sum of their parts?\n\n1:29:44.120 --> 1:29:48.000\n Or is it really, really useful to have multiple modalities?\n\n1:29:48.000 --> 1:29:50.640\n Or is it just that cool thing that there's parts\n\n1:29:50.640 --> 1:29:55.640\n of our world that can be revealed like effectively\n\n1:29:57.320 --> 1:29:58.400\n through multiple modalities,\n\n1:29:58.400 --> 1:30:01.200\n but most of it is really all about vision\n\n1:30:01.200 --> 1:30:03.880\n or about one of the modalities.\n\n1:30:03.880 --> 1:30:07.760\n I would say a little tending more towards the second part.\n\n1:30:07.760 --> 1:30:10.680\n So most of it can be sort of figured out with one modality,\n\n1:30:10.680 --> 1:30:13.160\n but having an extra modality always helps you.\n\n1:30:13.160 --> 1:30:14.560\n So in this case, for example,\n\n1:30:14.560 --> 1:30:17.720\n like one thing is when you're,\n\n1:30:17.720 --> 1:30:19.400\n if you observe someone cutting something\n\n1:30:19.400 --> 1:30:21.960\n and you don't have any sort of sound there,\n\n1:30:21.960 --> 1:30:25.080\n whether it's an apple or whether it's an onion,\n\n1:30:25.080 --> 1:30:26.720\n it's very hard to figure that out.\n\n1:30:26.720 --> 1:30:28.240\n But if you hear someone cutting it,\n\n1:30:28.240 --> 1:30:30.760\n it's very easy to figure it out because apples and onions\n\n1:30:30.760 --> 1:30:33.560\n make a very different kind of characteristics\n\n1:30:33.560 --> 1:30:34.840\n on when they're cut.\n\n1:30:34.840 --> 1:30:36.880\n So you really figure this out based on audio,\n\n1:30:36.880 --> 1:30:38.240\n it's much easier.\n\n1:30:38.240 --> 1:30:40.040\n So your life will become much easier\n\n1:30:40.040 --> 1:30:42.280\n when you have access to different kinds of modalities.\n\n1:30:42.280 --> 1:30:45.040\n And the other thing is, so I like to relate it in this way,\n\n1:30:45.040 --> 1:30:46.360\n it may be like completely wrong,\n\n1:30:46.360 --> 1:30:49.320\n but the distributional hypothesis in NLP,\n\n1:30:49.320 --> 1:30:53.040\n where context basically gives kind of meaning to that word,\n\n1:30:53.040 --> 1:30:55.040\n sound kind of does that too.\n\n1:30:55.040 --> 1:30:57.160\n So if you have the same sound,\n\n1:30:57.160 --> 1:30:59.840\n so that's the same context across different videos,\n\n1:30:59.840 --> 1:31:03.000\n you're very likely to be observing the same kind of concept.\n\n1:31:03.000 --> 1:31:04.280\n So that's the kind of reason\n\n1:31:04.280 --> 1:31:06.440\n why it figures out the guitar thing, right?\n\n1:31:06.440 --> 1:31:09.760\n It observed the same sound across multiple different videos\n\n1:31:09.760 --> 1:31:11.880\n and it figures out maybe this is the common factor\n\n1:31:11.880 --> 1:31:13.240\n that's actually doing it.\n\n1:31:13.240 --> 1:31:17.440\n I wonder, I used to have this argument with my dad a bunch\n\n1:31:17.440 --> 1:31:19.760\n for creating general intelligence,\n\n1:31:19.760 --> 1:31:22.840\n whether smell is an important,\n\n1:31:22.840 --> 1:31:25.480\n like if that's important sensory information,\n\n1:31:25.480 --> 1:31:27.560\n mostly we're talking about like falling in love\n\n1:31:27.560 --> 1:31:30.000\n with an AI system and for him,\n\n1:31:30.000 --> 1:31:31.440\n smell and touch are important.\n\n1:31:31.440 --> 1:31:33.880\n And I was arguing that it's not at all.\n\n1:31:33.880 --> 1:31:35.320\n It's important, it's nice and everything,\n\n1:31:35.320 --> 1:31:38.400\n but like you can fall in love with just language really,\n\n1:31:38.400 --> 1:31:41.400\n but a voice is very powerful and vision is next\n\n1:31:41.400 --> 1:31:43.880\n and smell is not that important.\n\n1:31:43.880 --> 1:31:46.880\n Can I ask you about this process of active learning?\n\n1:31:46.880 --> 1:31:49.200\n You mentioned interactivity.\n\n1:31:49.200 --> 1:31:50.040\n Right.\n\n1:31:50.040 --> 1:31:52.920\n Is there some value\n\n1:31:52.920 --> 1:31:57.040\n within the self supervised learning context\n\n1:31:57.040 --> 1:32:02.040\n to select parts of the data in intelligent ways\n\n1:32:02.280 --> 1:32:06.880\n such that they would most benefit the learning process?\n\n1:32:06.880 --> 1:32:07.720\n So I think so.\n\n1:32:07.720 --> 1:32:10.320\n I mean, I know I'm talking to an active learning fan here,\n\n1:32:10.320 --> 1:32:12.640\n so of course I know the answer.\n\n1:32:12.640 --> 1:32:14.000\n First you were talking bananas\n\n1:32:14.000 --> 1:32:15.600\n and now you're talking about active learning.\n\n1:32:15.600 --> 1:32:16.720\n I love it.\n\n1:32:16.720 --> 1:32:18.800\n I think Yannakun told me that active learning\n\n1:32:18.800 --> 1:32:20.480\n is not that interesting.\n\n1:32:20.480 --> 1:32:24.400\n I think back then I didn't want to argue with him too much,\n\n1:32:24.400 --> 1:32:26.040\n but when we talk again,\n\n1:32:26.040 --> 1:32:28.400\n we're gonna spend three hours arguing about active learning.\n\n1:32:28.400 --> 1:32:32.760\n My sense was you can go extremely far with active learning,\n\n1:32:32.760 --> 1:32:34.920\n perhaps farther than anything else.\n\n1:32:34.920 --> 1:32:37.960\n Like to me, there's this kind of intuition\n\n1:32:37.960 --> 1:32:40.840\n that similar to data augmentation,\n\n1:32:40.840 --> 1:32:44.160\n you can get a lot from the data,\n\n1:32:45.280 --> 1:32:50.280\n from intelligent optimized usage of the data.\n\n1:32:50.480 --> 1:32:53.200\n I'm trying to speak generally in such a way\n\n1:32:53.200 --> 1:32:55.280\n that includes data augmentation\n\n1:32:55.280 --> 1:32:57.040\n and active learning,\n\n1:32:57.040 --> 1:32:59.880\n that there's something about maybe interactive exploration\n\n1:32:59.880 --> 1:33:03.640\n of the data that at least is part\n\n1:33:03.640 --> 1:33:07.160\n of the solution to intelligence, like an important part.\n\n1:33:07.160 --> 1:33:08.200\n I don't know what your thoughts are\n\n1:33:08.200 --> 1:33:09.320\n on active learning in general.\n\n1:33:09.320 --> 1:33:10.840\n I actually really like active learning.\n\n1:33:10.840 --> 1:33:14.200\n So back in the day we did this largely ignored CVPR paper\n\n1:33:14.200 --> 1:33:16.520\n called learning by asking questions.\n\n1:33:16.520 --> 1:33:18.240\n So the idea was basically you would train an agent\n\n1:33:18.240 --> 1:33:20.120\n that would ask a question about the image.\n\n1:33:20.120 --> 1:33:21.520\n It would get an answer\n\n1:33:21.520 --> 1:33:23.360\n and basically then it would update itself.\n\n1:33:23.360 --> 1:33:24.360\n It would see the next image.\n\n1:33:24.360 --> 1:33:26.800\n It would decide what's the next hardest question\n\n1:33:26.800 --> 1:33:28.760\n that I can ask to learn the most.\n\n1:33:28.760 --> 1:33:31.320\n And the idea was basically because it was being smart\n\n1:33:31.320 --> 1:33:33.480\n about the kinds of questions it was asking,\n\n1:33:33.480 --> 1:33:35.080\n it would learn in fewer samples.\n\n1:33:35.080 --> 1:33:37.880\n It would be more efficient at using data.\n\n1:33:37.880 --> 1:33:39.400\n And we did find to some extent\n\n1:33:39.400 --> 1:33:42.040\n that it was actually better than randomly asking questions.\n\n1:33:42.040 --> 1:33:43.480\n Kind of weird thing about active learning\n\n1:33:43.480 --> 1:33:45.160\n is it's also a chicken and egg problem\n\n1:33:45.160 --> 1:33:47.120\n because when you look at an image,\n\n1:33:47.120 --> 1:33:48.640\n to ask a good question about the image,\n\n1:33:48.640 --> 1:33:50.880\n you need to understand something about the image.\n\n1:33:50.880 --> 1:33:53.440\n You can't ask a completely arbitrarily random question.\n\n1:33:53.440 --> 1:33:55.480\n It may not even apply to that particular image.\n\n1:33:55.480 --> 1:33:57.600\n So there is some amount of understanding or knowledge\n\n1:33:57.600 --> 1:33:59.160\n that basically keeps getting built\n\n1:33:59.160 --> 1:34:01.280\n when you're doing active learning.\n\n1:34:01.280 --> 1:34:04.560\n So I think active learning by itself is really good.\n\n1:34:04.560 --> 1:34:07.240\n And the main thing we need to figure out is basically\n\n1:34:07.240 --> 1:34:09.600\n how do we come up with a technique\n\n1:34:09.600 --> 1:34:13.320\n to first model what the model knows\n\n1:34:13.320 --> 1:34:16.000\n and also model what the model does not know.\n\n1:34:16.000 --> 1:34:18.360\n I think that's the sort of beauty of it.\n\n1:34:18.360 --> 1:34:20.480\n Because when you know that there are certain things\n\n1:34:20.480 --> 1:34:22.120\n that you don't know anything about,\n\n1:34:22.120 --> 1:34:23.640\n asking a question about those concepts\n\n1:34:23.640 --> 1:34:26.480\n is actually going to bring you the most value.\n\n1:34:26.480 --> 1:34:28.360\n And I think that's the sort of key challenge.\n\n1:34:28.360 --> 1:34:29.960\n Now, self supervised learning by itself,\n\n1:34:29.960 --> 1:34:31.480\n like selecting data for it and so on,\n\n1:34:31.480 --> 1:34:32.640\n that's actually really useful.\n\n1:34:32.640 --> 1:34:33.960\n But I think that's a very narrow view\n\n1:34:33.960 --> 1:34:35.080\n of looking at active learning.\n\n1:34:35.080 --> 1:34:36.360\n If you look at it more broadly,\n\n1:34:36.360 --> 1:34:40.040\n it is basically about if the model has a knowledge\n\n1:34:40.040 --> 1:34:41.400\n about N concepts,\n\n1:34:41.400 --> 1:34:43.840\n and it is weak basically about certain things.\n\n1:34:43.840 --> 1:34:45.280\n So it needs to ask questions\n\n1:34:45.280 --> 1:34:46.880\n either to discover new concepts\n\n1:34:46.880 --> 1:34:49.200\n or to basically increase its knowledge\n\n1:34:49.200 --> 1:34:50.400\n about these N concepts.\n\n1:34:50.400 --> 1:34:53.200\n So at that level, it's a very powerful technique.\n\n1:34:53.200 --> 1:34:56.520\n I actually do think it's going to be really useful.\n\n1:34:56.520 --> 1:34:59.040\n Even in like simple things such as like data labeling,\n\n1:34:59.040 --> 1:35:00.240\n it's super useful.\n\n1:35:00.240 --> 1:35:02.920\n So here is like one simple way\n\n1:35:02.920 --> 1:35:04.280\n that you can use active learning.\n\n1:35:04.280 --> 1:35:06.880\n For example, you have your self supervised model,\n\n1:35:06.880 --> 1:35:08.760\n which is very good at predicting similarities\n\n1:35:08.760 --> 1:35:10.760\n and dissimilarities between things.\n\n1:35:10.760 --> 1:35:14.600\n And so if you label a picture as basically say a banana,\n\n1:35:15.480 --> 1:35:17.720\n now you know that all the images\n\n1:35:17.720 --> 1:35:19.200\n that are very similar to this image\n\n1:35:19.200 --> 1:35:21.480\n are also likely to contain bananas.\n\n1:35:21.480 --> 1:35:24.240\n So probably when you want to understand\n\n1:35:24.240 --> 1:35:25.160\n what else is a banana,\n\n1:35:25.160 --> 1:35:26.880\n you're not going to use these other images.\n\n1:35:26.880 --> 1:35:28.160\n You're actually going to use an image\n\n1:35:28.160 --> 1:35:31.120\n that is not completely dissimilar,\n\n1:35:31.120 --> 1:35:32.320\n but somewhere in between,\n\n1:35:32.320 --> 1:35:33.840\n which is not super similar to this image,\n\n1:35:33.840 --> 1:35:35.640\n but not super dissimilar either.\n\n1:35:35.640 --> 1:35:37.120\n And that's going to tell you a lot more\n\n1:35:37.120 --> 1:35:39.520\n about what this concept of a banana is.\n\n1:35:39.520 --> 1:35:41.840\n So that's kind of a heuristic.\n\n1:35:41.840 --> 1:35:46.840\n I wonder if it's possible to also learn ways\n\n1:35:46.840 --> 1:35:50.640\n to discover the most likely,\n\n1:35:50.640 --> 1:35:52.880\n the most beneficial image.\n\n1:35:52.880 --> 1:35:54.920\n So like, so not just looking a thing\n\n1:35:54.920 --> 1:35:58.360\n that's somewhat similar to a banana,\n\n1:35:58.360 --> 1:35:59.920\n but not exactly similar,\n\n1:35:59.920 --> 1:36:03.480\n but have some kind of more complicated learning system,\n\n1:36:03.480 --> 1:36:07.000\n like learned discovering mechanism\n\n1:36:07.000 --> 1:36:09.360\n that tells you what image to look for.\n\n1:36:09.360 --> 1:36:14.240\n Like how, yeah, like actually in a self supervised way,\n\n1:36:14.240 --> 1:36:17.160\n learning strictly a function that says,\n\n1:36:17.160 --> 1:36:20.440\n is this image going to be very useful to me\n\n1:36:20.440 --> 1:36:22.000\n given what I currently know?\n\n1:36:22.000 --> 1:36:23.880\n I think there's a lot of synergy there.\n\n1:36:23.880 --> 1:36:26.600\n It's just, I think, yeah, it's going to be explored.\n\n1:36:27.520 --> 1:36:29.240\n I think very much related to that.\n\n1:36:29.240 --> 1:36:32.280\n I kind of think of what Tesla Autopilot is doing\n\n1:36:33.480 --> 1:36:36.720\n currently as kind of active learning.\n\n1:36:36.720 --> 1:36:39.120\n There's something that Andre Capati and their team\n\n1:36:39.120 --> 1:36:41.440\n are calling a data engine.\n\n1:36:41.440 --> 1:36:45.640\n So you're basically deploying a bunch of instantiations\n\n1:36:45.640 --> 1:36:47.920\n of a neural network into the wild,\n\n1:36:47.920 --> 1:36:50.640\n and they're collecting a bunch of edge cases\n\n1:36:50.640 --> 1:36:53.920\n that are then sent back for annotation for particular,\n\n1:36:53.920 --> 1:36:56.680\n and edge cases as defined as near failure\n\n1:36:56.680 --> 1:36:59.960\n or some weirdness on a particular task\n\n1:36:59.960 --> 1:37:01.400\n that's then sent back.\n\n1:37:01.400 --> 1:37:04.000\n It's that not exactly a banana,\n\n1:37:04.000 --> 1:37:07.200\n but almost the banana cases sent back for annotation.\n\n1:37:07.200 --> 1:37:09.200\n And then there's this loop that keeps going\n\n1:37:09.200 --> 1:37:11.600\n and you keep retraining and retraining.\n\n1:37:11.600 --> 1:37:13.280\n And the active learning step there,\n\n1:37:13.280 --> 1:37:14.800\n or whatever you want to call it,\n\n1:37:14.800 --> 1:37:19.120\n is the cars themselves that are sending you back the data.\n\n1:37:19.120 --> 1:37:20.760\n Like, what the hell happened here?\n\n1:37:20.760 --> 1:37:22.840\n This was weird.\n\n1:37:22.840 --> 1:37:26.440\n What are your thoughts about that sort of deployment\n\n1:37:26.440 --> 1:37:28.240\n of neural networks in the wild?\n\n1:37:28.240 --> 1:37:31.360\n Another way to ask a question from first is your thoughts.\n\n1:37:31.360 --> 1:37:33.840\n And maybe if you want to comment,\n\n1:37:33.840 --> 1:37:36.960\n is there applications for autonomous driving,\n\n1:37:36.960 --> 1:37:40.160\n like computer vision based autonomous driving,\n\n1:37:40.160 --> 1:37:42.040\n applications of self supervised learning\n\n1:37:42.040 --> 1:37:46.080\n in the context of computer vision based autonomous driving?\n\n1:37:47.520 --> 1:37:48.360\n So I think so.\n\n1:37:48.360 --> 1:37:49.560\n I think for self supervised learning\n\n1:37:49.560 --> 1:37:50.800\n to be used in autonomous driving,\n\n1:37:50.800 --> 1:37:51.800\n there are lots of opportunities.\n\n1:37:51.800 --> 1:37:54.880\n I mean, just like pure consistency in predictions\n\n1:37:54.880 --> 1:37:55.840\n is one way, right?\n\n1:37:55.840 --> 1:38:00.280\n So because you have this nice sequence of data\n\n1:38:00.280 --> 1:38:02.320\n that is coming in, a video stream of it,\n\n1:38:02.320 --> 1:38:04.040\n associated of course with the actions\n\n1:38:04.040 --> 1:38:05.240\n that say the car took,\n\n1:38:05.240 --> 1:38:07.640\n you can form a very nice predictive model\n\n1:38:07.640 --> 1:38:08.480\n of what's happening.\n\n1:38:08.480 --> 1:38:11.400\n So for example, like all the way,\n\n1:38:11.400 --> 1:38:14.440\n like one way possibly in which how they're figuring out\n\n1:38:14.440 --> 1:38:15.880\n what data to get labeled is basically\n\n1:38:15.880 --> 1:38:17.440\n through prediction uncertainty, right?\n\n1:38:17.440 --> 1:38:20.360\n So you predict that the car was going to turn right.\n\n1:38:20.360 --> 1:38:21.840\n So this was the action that was going to happen,\n\n1:38:21.840 --> 1:38:23.080\n say in the shadow mode.\n\n1:38:23.080 --> 1:38:24.640\n And now the driver turned left.\n\n1:38:24.640 --> 1:38:27.160\n And this is a really big surprise.\n\n1:38:27.160 --> 1:38:30.120\n So basically by forming these good predictive models,\n\n1:38:30.120 --> 1:38:32.840\n you are, I mean, these are kind of self supervised models.\n\n1:38:32.840 --> 1:38:34.600\n Prediction models are basically being trained\n\n1:38:34.600 --> 1:38:36.800\n just by looking at what's going to happen next\n\n1:38:36.800 --> 1:38:38.960\n and asking them to predict what's going to happen next.\n\n1:38:38.960 --> 1:38:40.720\n So I would say this is really like one use\n\n1:38:40.720 --> 1:38:42.320\n of self supervised learning.\n\n1:38:42.320 --> 1:38:43.440\n It's a predictive model\n\n1:38:43.440 --> 1:38:44.680\n and you're learning a predictive model\n\n1:38:44.680 --> 1:38:46.880\n basically just by looking at what data you have.\n\n1:38:46.880 --> 1:38:49.600\n Is there something about that active learning context\n\n1:38:49.600 --> 1:38:53.000\n that you find insights from?\n\n1:38:53.000 --> 1:38:54.760\n Like that kind of deployment of the system,\n\n1:38:54.760 --> 1:38:59.120\n seeing cases where it doesn't perform as you expected\n\n1:38:59.120 --> 1:39:01.000\n and then retraining the system based on that?\n\n1:39:01.000 --> 1:39:03.600\n I think that, I mean, that really resonates with me.\n\n1:39:03.600 --> 1:39:05.560\n It's super smart to do it that way.\n\n1:39:05.560 --> 1:39:08.520\n Because I mean, the thing is with any kind\n\n1:39:08.520 --> 1:39:11.160\n of like practical system, like autonomous driving,\n\n1:39:11.160 --> 1:39:13.040\n there are those edge cases that are the things\n\n1:39:13.040 --> 1:39:14.520\n that are actually the problem, right?\n\n1:39:14.520 --> 1:39:17.440\n I mean, highway driving or like freeway driving\n\n1:39:17.440 --> 1:39:19.120\n has basically been like,\n\n1:39:19.120 --> 1:39:21.120\n there has been a lot of success in that particular part\n\n1:39:21.120 --> 1:39:22.840\n of autonomous driving for a long time.\n\n1:39:22.840 --> 1:39:25.560\n I would say like since the eighties or something.\n\n1:39:25.560 --> 1:39:28.000\n Now the point is all these failure cases\n\n1:39:28.000 --> 1:39:30.600\n are the sort of reason why autonomous driving\n\n1:39:30.600 --> 1:39:33.800\n hasn't become like super, super mainstream and available\n\n1:39:33.800 --> 1:39:35.640\n like in every possible car right now.\n\n1:39:35.640 --> 1:39:38.200\n And so basically by really scaling this problem out\n\n1:39:38.200 --> 1:39:40.440\n by really trying to get all of these edge cases out\n\n1:39:40.440 --> 1:39:41.880\n as quickly as possible,\n\n1:39:41.880 --> 1:39:43.920\n and then just like using those to improve your model,\n\n1:39:43.920 --> 1:39:45.640\n that's super smart.\n\n1:39:45.640 --> 1:39:47.120\n And prediction uncertainty to do that\n\n1:39:47.120 --> 1:39:49.800\n is like one really nice way of doing it.\n\n1:39:49.800 --> 1:39:52.040\n Let me put you on the spot.\n\n1:39:52.040 --> 1:39:55.240\n So we mentioned offline Jitendra,\n\n1:39:55.240 --> 1:39:58.240\n he thinks that the Tesla computer vision approach\n\n1:39:58.240 --> 1:40:00.800\n or really any approach for autonomous driving\n\n1:40:00.800 --> 1:40:02.680\n is very far away.\n\n1:40:02.680 --> 1:40:05.440\n How many years away,\n\n1:40:05.440 --> 1:40:06.960\n if you have to bet all your money on it,\n\n1:40:06.960 --> 1:40:09.600\n are we to solving autonomous driving\n\n1:40:09.600 --> 1:40:12.000\n with this kind of computer vision only\n\n1:40:12.000 --> 1:40:13.600\n machine learning based approach?\n\n1:40:13.600 --> 1:40:15.400\n Okay, so what does solving autonomous driving mean?\n\n1:40:15.400 --> 1:40:17.200\n Does it mean solving it in the US?\n\n1:40:17.200 --> 1:40:18.480\n Does it mean solving it in India?\n\n1:40:18.480 --> 1:40:19.320\n Because I can tell you\n\n1:40:19.320 --> 1:40:21.200\n that very different types of driving happening.\n\n1:40:21.200 --> 1:40:23.800\n Not India, not Russia.\n\n1:40:23.800 --> 1:40:26.200\n In the United States, autonomous,\n\n1:40:26.200 --> 1:40:31.200\n so what solving means is when the car says it has control,\n\n1:40:31.880 --> 1:40:34.040\n it is fully liable.\n\n1:40:34.040 --> 1:40:37.800\n You can go to sleep, it's driving by itself.\n\n1:40:37.800 --> 1:40:39.720\n So this is highway and city driving,\n\n1:40:39.720 --> 1:40:42.280\n but not everywhere, but mostly everywhere.\n\n1:40:42.280 --> 1:40:45.040\n And it's, let's say significantly better,\n\n1:40:45.040 --> 1:40:50.040\n like say five times less accidents than humans.\n\n1:40:50.480 --> 1:40:53.960\n Sufficiently safer such that the public feels\n\n1:40:53.960 --> 1:40:57.960\n like that transition is enticing beneficial\n\n1:40:57.960 --> 1:40:59.480\n both for our safety and financial\n\n1:40:59.480 --> 1:41:01.040\n and all those kinds of things.\n\n1:41:01.040 --> 1:41:02.240\n Okay, so first disclaimer,\n\n1:41:02.240 --> 1:41:04.200\n I'm not an expert in autonomous driving.\n\n1:41:04.200 --> 1:41:05.920\n So let me put it out there.\n\n1:41:05.920 --> 1:41:08.320\n I would say like at least five to 10 years.\n\n1:41:09.360 --> 1:41:11.760\n This would be my guess from now.\n\n1:41:12.920 --> 1:41:14.640\n Yeah, I'm actually very impressed.\n\n1:41:14.640 --> 1:41:16.760\n Like when I sat in a friend's Tesla recently\n\n1:41:16.760 --> 1:41:20.600\n and of course, like looking on that screen,\n\n1:41:20.600 --> 1:41:22.800\n it basically shows all the detections and everything.\n\n1:41:22.800 --> 1:41:24.640\n The car is doing as you're driving by\n\n1:41:24.640 --> 1:41:26.880\n and that's super distracting for me as a person\n\n1:41:26.880 --> 1:41:29.440\n because all I keep looking at is like the bounding boxes\n\n1:41:29.440 --> 1:41:31.760\n in the cars it's tracking and it's really impressive.\n\n1:41:31.760 --> 1:41:34.280\n Like especially when it's raining and it's able to do that,\n\n1:41:34.280 --> 1:41:36.000\n that was the most impressive part for me.\n\n1:41:36.000 --> 1:41:38.520\n It's actually able to get through rain and do that.\n\n1:41:38.520 --> 1:41:41.720\n And one of the reasons why like a lot of us believed\n\n1:41:41.720 --> 1:41:44.040\n and I would put myself in that category\n\n1:41:44.040 --> 1:41:47.680\n is LIDAR based sort of technology for autonomous driving\n\n1:41:47.680 --> 1:41:48.720\n was the key driver, right?\n\n1:41:48.720 --> 1:41:50.960\n So Waymo was using it for the longest time.\n\n1:41:50.960 --> 1:41:53.280\n And Tesla then decided to go this completely other route\n\n1:41:53.280 --> 1:41:55.760\n that we are not going to even use LIDAR.\n\n1:41:55.760 --> 1:41:58.720\n So their initial system I think was camera and radar based\n\n1:41:58.720 --> 1:41:59.640\n and now they're actually moving\n\n1:41:59.640 --> 1:42:02.000\n to a completely like vision based system.\n\n1:42:02.000 --> 1:42:04.640\n And so that was just like, it sounded completely crazy.\n\n1:42:04.640 --> 1:42:07.000\n Like LIDAR is very useful in cases\n\n1:42:07.000 --> 1:42:09.240\n where you have low visibility.\n\n1:42:09.240 --> 1:42:11.720\n Of course it comes with its own set of complications.\n\n1:42:11.720 --> 1:42:15.160\n But now to see that happen in like on a live Tesla\n\n1:42:15.160 --> 1:42:16.960\n that basically just proves everyone wrong\n\n1:42:16.960 --> 1:42:18.120\n I would say in a way.\n\n1:42:18.120 --> 1:42:20.520\n And that's just working really well.\n\n1:42:20.520 --> 1:42:22.720\n I think there were also like a lot of advancements\n\n1:42:22.720 --> 1:42:23.920\n in camera technology.\n\n1:42:23.920 --> 1:42:26.280\n Now there were like, I know at CMU when I was there\n\n1:42:26.280 --> 1:42:27.960\n there was a particular kind of camera\n\n1:42:27.960 --> 1:42:30.040\n that had been developed that was really good\n\n1:42:30.040 --> 1:42:32.760\n at basically low visibility setting.\n\n1:42:32.760 --> 1:42:34.400\n So like lots of snow and lots of rain\n\n1:42:34.400 --> 1:42:37.640\n it could actually still have a very reasonable visibility.\n\n1:42:37.640 --> 1:42:39.360\n And I think there are lots of these kinds of innovations\n\n1:42:39.360 --> 1:42:40.960\n that will happen on the sensor side itself\n\n1:42:40.960 --> 1:42:42.840\n which is actually going to make this very easy\n\n1:42:42.840 --> 1:42:43.840\n in the future.\n\n1:42:43.840 --> 1:42:46.080\n And so maybe that's actually why I'm more optimistic\n\n1:42:46.080 --> 1:42:49.000\n about vision based self, like autonomous driving.\n\n1:42:49.000 --> 1:42:51.960\n I was going to call it self supervised driving, but.\n\n1:42:51.960 --> 1:42:53.520\n Vision based autonomous driving.\n\n1:42:53.520 --> 1:42:55.480\n That's the reason I'm quite optimistic about it\n\n1:42:55.480 --> 1:42:56.640\n because I think there are going to be lots\n\n1:42:56.640 --> 1:42:58.960\n of these advances on the sensor side itself.\n\n1:42:58.960 --> 1:43:00.720\n So acquiring this data\n\n1:43:00.720 --> 1:43:02.640\n we're actually going to get much better about it.\n\n1:43:02.640 --> 1:43:05.080\n And then of course, once we're able to scale out\n\n1:43:05.080 --> 1:43:06.800\n and get all of these edge cases in\n\n1:43:06.800 --> 1:43:08.720\n as like Andre described\n\n1:43:08.720 --> 1:43:11.720\n I think that's going to make us go very far away.\n\n1:43:11.720 --> 1:43:13.560\n Yeah, so it's funny.\n\n1:43:13.560 --> 1:43:16.280\n I'm very much with you on the five to 10 years\n\n1:43:16.280 --> 1:43:17.840\n maybe 10 years\n\n1:43:17.840 --> 1:43:21.760\n but you made it, I'm not sure how you made it sound\n\n1:43:21.760 --> 1:43:23.640\n but for some people that seem\n\n1:43:23.640 --> 1:43:25.360\n that might seem like really far away.\n\n1:43:25.360 --> 1:43:30.360\n And then for other people, it might seem like very close.\n\n1:43:30.440 --> 1:43:32.320\n There's a lot of fundamental questions\n\n1:43:32.320 --> 1:43:36.880\n about how much game theory is in this whole thing.\n\n1:43:36.880 --> 1:43:41.160\n So like, how much is this simply a collision avoidance\n\n1:43:41.160 --> 1:43:45.200\n problem and how much of it is you still interacting\n\n1:43:45.200 --> 1:43:46.960\n with other humans in the scene\n\n1:43:46.960 --> 1:43:48.800\n and you're trying to create an experience\n\n1:43:48.800 --> 1:43:49.640\n that's compelling.\n\n1:43:49.640 --> 1:43:53.080\n So you want to get from point A to point B quickly\n\n1:43:53.080 --> 1:43:55.280\n you want to navigate the scene in a safe way\n\n1:43:55.280 --> 1:43:58.480\n but you also want to show some level of aggression\n\n1:43:58.480 --> 1:44:02.000\n because well, certainly this is why you're screwed in India\n\n1:44:02.000 --> 1:44:03.320\n because you have to show aggression.\n\n1:44:03.320 --> 1:44:04.840\n Or Jersey or New Jersey.\n\n1:44:04.840 --> 1:44:05.680\n Or Jersey, right.\n\n1:44:05.680 --> 1:44:10.680\n So like, or New York or basically any major city\n\n1:44:11.200 --> 1:44:13.240\n but I think it's probably Elon\n\n1:44:13.240 --> 1:44:14.800\n that I talked the most about this\n\n1:44:14.800 --> 1:44:17.720\n which is a surprise to the level of which\n\n1:44:17.720 --> 1:44:20.080\n they're not considering human beings\n\n1:44:20.080 --> 1:44:22.960\n as a huge problem in this, as a source of problem.\n\n1:44:22.960 --> 1:44:27.960\n Like the driving is fundamentally a robot on robot\n\n1:44:29.000 --> 1:44:31.160\n versus the environment problem\n\n1:44:31.160 --> 1:44:33.960\n versus like you can just consider humans\n\n1:44:33.960 --> 1:44:35.160\n not part of the problem.\n\n1:44:35.160 --> 1:44:38.840\n I used to think humans are almost certainly\n\n1:44:38.840 --> 1:44:41.200\n have to be modeled really well.\n\n1:44:41.200 --> 1:44:44.360\n Pedestrians and cyclists and humans inside other cars\n\n1:44:44.360 --> 1:44:46.320\n you have to have like mental models for them.\n\n1:44:46.320 --> 1:44:48.280\n You cannot just see it as objects\n\n1:44:48.280 --> 1:44:50.360\n but more and more it's like the\n\n1:44:51.400 --> 1:44:53.720\n it's the same kind of intuition breaking thing\n\n1:44:53.720 --> 1:44:57.000\n that's self supervised learning does, which is\n\n1:44:57.000 --> 1:44:58.840\n well maybe through the learning\n\n1:44:58.840 --> 1:45:03.840\n you'll get all the human like human information you need.\n\n1:45:04.080 --> 1:45:04.920\n Right?\n\n1:45:04.920 --> 1:45:07.760\n Like maybe you'll get it just with enough data.\n\n1:45:07.760 --> 1:45:09.680\n You don't need to have explicit good models\n\n1:45:09.680 --> 1:45:10.800\n of human behavior.\n\n1:45:10.800 --> 1:45:12.120\n Maybe you get it through the data.\n\n1:45:12.120 --> 1:45:14.640\n So, I mean my skepticism also just knowing\n\n1:45:14.640 --> 1:45:16.360\n a lot of automotive companies\n\n1:45:16.360 --> 1:45:18.600\n and how difficult it is to be innovative.\n\n1:45:18.600 --> 1:45:22.560\n I was skeptical that they would be able at scale\n\n1:45:22.560 --> 1:45:27.400\n to convert the driving scene across the world\n\n1:45:27.400 --> 1:45:30.640\n into digital form such that you can create\n\n1:45:30.640 --> 1:45:33.160\n this data engine at scale.\n\n1:45:33.160 --> 1:45:36.640\n And the fact that Tesla is at least getting there\n\n1:45:36.640 --> 1:45:41.640\n or are already there makes me think that\n\n1:45:41.640 --> 1:45:43.680\n it's now starting to be coupled\n\n1:45:43.680 --> 1:45:47.600\n to this self supervised learning vision\n\n1:45:47.600 --> 1:45:49.840\n which is like if that's gonna work\n\n1:45:49.840 --> 1:45:52.920\n if through purely this process you can get really far\n\n1:45:52.920 --> 1:45:54.880\n then maybe you can solve driving that way.\n\n1:45:54.880 --> 1:45:55.720\n I don't know.\n\n1:45:55.720 --> 1:46:00.000\n I tend to believe we don't give enough credit\n\n1:46:00.000 --> 1:46:05.000\n to the how amazing humans are both at driving\n\n1:46:05.920 --> 1:46:09.360\n and at supervising autonomous systems.\n\n1:46:09.360 --> 1:46:13.200\n And also we don't, this is, I wish we were.\n\n1:46:13.200 --> 1:46:17.120\n I wish there was much more driver sensing inside Teslas\n\n1:46:17.120 --> 1:46:21.200\n and much deeper consideration of human factors\n\n1:46:21.200 --> 1:46:24.680\n like understanding psychology and drowsiness\n\n1:46:24.680 --> 1:46:26.200\n and all those kinds of things\n\n1:46:26.200 --> 1:46:28.720\n when the car does more and more of the work.\n\n1:46:28.720 --> 1:46:32.960\n How to keep utilizing the little human supervision\n\n1:46:32.960 --> 1:46:35.080\n that are needed to keep this whole thing safe.\n\n1:46:35.080 --> 1:46:38.440\n I mean it's a fascinating dance of human robot interaction.\n\n1:46:38.440 --> 1:46:42.120\n To me autonomous driving for a long time\n\n1:46:42.120 --> 1:46:45.040\n is a human robot interaction problem.\n\n1:46:45.040 --> 1:46:48.040\n It is not a robotics problem or computer vision problem.\n\n1:46:48.040 --> 1:46:50.000\n Like you have to have a human in the loop.\n\n1:46:50.000 --> 1:46:53.320\n But so which is why I think it's 10 years plus.\n\n1:46:53.320 --> 1:46:56.280\n But I do think there'll be a bunch of cities and contexts\n\n1:46:56.280 --> 1:47:01.280\n where geo restricted it will work really, really damn well.\n\n1:47:02.360 --> 1:47:05.000\n So I think for me that gets five if I'm being optimistic\n\n1:47:05.000 --> 1:47:07.360\n and it's going to be five for a lot of cases\n\n1:47:07.360 --> 1:47:09.200\n and 10 plus, yeah, I agree with you.\n\n1:47:09.200 --> 1:47:13.120\n 10 plus basically if we want to recover most of the,\n\n1:47:13.120 --> 1:47:15.240\n say, contiguous United States or something.\n\n1:47:15.240 --> 1:47:16.080\n Oh, interesting.\n\n1:47:16.080 --> 1:47:20.280\n So my optimistic is five and pessimistic is 30.\n\n1:47:20.280 --> 1:47:21.120\n 30.\n\n1:47:21.120 --> 1:47:22.480\n I have a long tail on this one.\n\n1:47:22.480 --> 1:47:24.440\n I haven't watched enough driving videos.\n\n1:47:24.440 --> 1:47:29.160\n I've watched enough pedestrians to think like we may be,\n\n1:47:29.160 --> 1:47:31.680\n like there's a small part of me still, not a small,\n\n1:47:31.680 --> 1:47:34.360\n like a pretty big part of me that thinks\n\n1:47:34.360 --> 1:47:37.520\n we will have to build AGI to solve driving.\n\n1:47:37.520 --> 1:47:38.440\n Oh, well.\n\n1:47:38.440 --> 1:47:39.640\n Like there's something to me,\n\n1:47:39.640 --> 1:47:41.800\n like because humans are part of the picture,\n\n1:47:41.800 --> 1:47:44.000\n deeply part of the picture,\n\n1:47:44.000 --> 1:47:46.080\n and also human society is part of the picture\n\n1:47:46.080 --> 1:47:47.920\n in that human life is at stake.\n\n1:47:47.920 --> 1:47:50.840\n Anytime a robot kills a human,\n\n1:47:50.840 --> 1:47:54.280\n it's not clear to me that that's not a problem\n\n1:47:54.280 --> 1:47:56.360\n that machine learning will also have to solve.\n\n1:47:56.360 --> 1:47:59.400\n Like it has to, you have to integrate that\n\n1:47:59.400 --> 1:48:00.240\n into the whole thing.\n\n1:48:00.240 --> 1:48:03.280\n Just like Facebook or social networks,\n\n1:48:03.280 --> 1:48:04.600\n one thing is to say how to make\n\n1:48:04.600 --> 1:48:06.720\n a really good recommender system.\n\n1:48:06.720 --> 1:48:08.640\n And then the other thing is to integrate\n\n1:48:08.640 --> 1:48:10.240\n into that recommender system,\n\n1:48:10.240 --> 1:48:12.080\n all the journalists that will write articles\n\n1:48:12.080 --> 1:48:13.880\n about that recommender system.\n\n1:48:13.880 --> 1:48:15.880\n Like you have to consider the society\n\n1:48:15.880 --> 1:48:18.400\n within which the AI system operates.\n\n1:48:18.400 --> 1:48:21.000\n And in order to, and like politicians too,\n\n1:48:21.000 --> 1:48:24.200\n this is the regulatory stuff for autonomous driving.\n\n1:48:24.200 --> 1:48:26.720\n It's kind of fascinating that the more successful\n\n1:48:26.720 --> 1:48:28.720\n your AI system becomes,\n\n1:48:28.720 --> 1:48:31.600\n the more it gets integrated in society\n\n1:48:31.600 --> 1:48:33.560\n and the more precious politicians\n\n1:48:33.560 --> 1:48:36.000\n and the public and the clickbait journalists\n\n1:48:36.000 --> 1:48:38.040\n and all the different fascinating forces\n\n1:48:38.040 --> 1:48:40.360\n of our society start acting on it.\n\n1:48:40.360 --> 1:48:42.240\n And then it's no longer how good you are\n\n1:48:42.240 --> 1:48:43.960\n at doing the initial task.\n\n1:48:43.960 --> 1:48:47.000\n It's also how good you are at navigating human nature,\n\n1:48:47.000 --> 1:48:49.920\n which is a fascinating space.\n\n1:48:49.920 --> 1:48:52.600\n What do you think are the limits of deep learning?\n\n1:48:52.600 --> 1:48:54.800\n If you allow me, we'll zoom out a little bit\n\n1:48:54.800 --> 1:48:58.120\n into the big question of artificial intelligence.\n\n1:48:58.120 --> 1:49:02.080\n You said dark matter of intelligence is self supervised\n\n1:49:02.080 --> 1:49:04.320\n learning, but there could be more.\n\n1:49:04.320 --> 1:49:07.760\n What do you think the limits of self supervised learning\n\n1:49:07.760 --> 1:49:10.720\n and just learning in general, deep learning are?\n\n1:49:10.720 --> 1:49:12.680\n I think like for deep learning in particular,\n\n1:49:12.680 --> 1:49:14.640\n because self supervised learning is I would say\n\n1:49:14.640 --> 1:49:16.800\n a little bit more vague right now.\n\n1:49:16.800 --> 1:49:18.680\n So I wouldn't, like for something that's so vague,\n\n1:49:18.680 --> 1:49:21.960\n it's hard to predict what its limits are going to be.\n\n1:49:21.960 --> 1:49:25.240\n But like I said, I think anywhere you want to interact\n\n1:49:25.240 --> 1:49:27.920\n with human self supervised learning kind of hits a boundary\n\n1:49:27.920 --> 1:49:29.960\n very quickly because you need to have an interface\n\n1:49:29.960 --> 1:49:31.600\n to be able to communicate with the human.\n\n1:49:31.600 --> 1:49:35.040\n So really like if you have just like vacuous concepts\n\n1:49:35.040 --> 1:49:37.360\n or like just like nebulous concepts discovered\n\n1:49:37.360 --> 1:49:39.920\n by a network, it's very hard to communicate those\n\n1:49:39.920 --> 1:49:41.760\n with the human without like inserting some kind\n\n1:49:41.760 --> 1:49:44.560\n of human knowledge or some kind of like human bias there.\n\n1:49:45.600 --> 1:49:47.040\n In general, I think for deep learning,\n\n1:49:47.040 --> 1:49:50.680\n the biggest challenge is just like data efficiency.\n\n1:49:50.680 --> 1:49:52.600\n Even with self supervised learning,\n\n1:49:52.600 --> 1:49:54.920\n even with anything else, if you just see\n\n1:49:54.920 --> 1:49:59.280\n a single concept once, like one image of like,\n\n1:49:59.280 --> 1:50:01.200\n I don't know, whatever you want to call it,\n\n1:50:01.200 --> 1:50:03.840\n like any concept, it's really hard for these methods\n\n1:50:03.840 --> 1:50:07.040\n to generalize by looking at just one or two samples\n\n1:50:07.040 --> 1:50:09.760\n of things and that has been a real challenge.\n\n1:50:09.760 --> 1:50:11.680\n I think that's actually why like these edge cases,\n\n1:50:11.680 --> 1:50:14.520\n for example, for Tesla are actually that important.\n\n1:50:14.520 --> 1:50:18.040\n Because if you see just one instance of the car failing\n\n1:50:18.040 --> 1:50:20.280\n and if you just annotate that and you get that\n\n1:50:20.280 --> 1:50:23.560\n into your data set, you have like very limited guarantee\n\n1:50:23.560 --> 1:50:25.160\n that it's not going to happen again.\n\n1:50:25.160 --> 1:50:26.720\n And you're actually going to be able to recognize\n\n1:50:26.720 --> 1:50:28.640\n this kind of instance in a very different scenario.\n\n1:50:28.640 --> 1:50:31.400\n So like when it was snowing, so you got that thing labeled\n\n1:50:31.400 --> 1:50:33.240\n when it was snowing, but now when it's raining,\n\n1:50:33.240 --> 1:50:34.640\n you're actually not able to get it.\n\n1:50:34.640 --> 1:50:36.600\n Or you basically have the same scenario\n\n1:50:36.600 --> 1:50:37.440\n in a different part of the world.\n\n1:50:37.440 --> 1:50:39.120\n So the lighting was different or so on.\n\n1:50:39.120 --> 1:50:41.000\n So it's just really hard for these models,\n\n1:50:41.000 --> 1:50:42.720\n like deep learning especially to do that.\n\n1:50:42.720 --> 1:50:43.560\n What's your intuition?\n\n1:50:43.560 --> 1:50:47.800\n How do we solve handwritten digit recognition problem\n\n1:50:47.800 --> 1:50:51.200\n when we only have one example for each number?\n\n1:50:51.200 --> 1:50:54.720\n It feels like humans are using something like learning.\n\n1:50:54.720 --> 1:50:55.560\n Right.\n\n1:50:55.560 --> 1:50:59.240\n I think we are good at transferring knowledge a little bit.\n\n1:50:59.240 --> 1:51:02.640\n We are just better at like for a lot of these problems\n\n1:51:02.640 --> 1:51:04.840\n where we are generalizing from a single sample\n\n1:51:04.840 --> 1:51:06.960\n or recognizing from a single sample,\n\n1:51:06.960 --> 1:51:08.760\n we are using a lot of our own domain knowledge\n\n1:51:08.760 --> 1:51:10.320\n and a lot of our like inductive bias\n\n1:51:10.320 --> 1:51:12.280\n into that one sample to generalize it.\n\n1:51:12.280 --> 1:51:15.320\n So I've never seen you write the number nine, for example.\n\n1:51:15.320 --> 1:51:17.440\n And if you were to write it, I would still get it.\n\n1:51:17.440 --> 1:51:19.280\n And if you were to write a different kind of alphabet\n\n1:51:19.280 --> 1:51:20.840\n and like write it in two different ways,\n\n1:51:20.840 --> 1:51:22.360\n I would still probably be able to figure out\n\n1:51:22.360 --> 1:51:24.720\n that these are the same two characters.\n\n1:51:24.720 --> 1:51:26.320\n It's just that I have been very used\n\n1:51:26.320 --> 1:51:29.080\n to seeing handwritten digits in my life.\n\n1:51:29.080 --> 1:51:31.360\n The other sort of problem with any deep learning system\n\n1:51:31.360 --> 1:51:33.080\n or any kind of machine learning system is like,\n\n1:51:33.080 --> 1:51:34.200\n it's guarantees, right?\n\n1:51:34.200 --> 1:51:35.880\n There are no guarantees for it.\n\n1:51:35.880 --> 1:51:38.200\n Now you can argue that humans also don't have any guarantees.\n\n1:51:38.200 --> 1:51:41.160\n Like there is no guarantee that I can recognize a cat\n\n1:51:41.160 --> 1:51:42.280\n in every scenario.\n\n1:51:42.280 --> 1:51:43.920\n I'm sure there are going to be lots of cats\n\n1:51:43.920 --> 1:51:45.720\n that I don't recognize, lots of scenarios\n\n1:51:45.720 --> 1:51:48.120\n in which I don't recognize cats in general.\n\n1:51:48.120 --> 1:51:52.840\n But I think from just a sort of application perspective,\n\n1:51:52.840 --> 1:51:54.760\n you do need guarantees, right?\n\n1:51:54.760 --> 1:51:56.960\n We call these things algorithms.\n\n1:51:56.960 --> 1:51:59.080\n Now algorithms, like traditional CS algorithms\n\n1:51:59.080 --> 1:51:59.960\n have guarantees.\n\n1:51:59.960 --> 1:52:01.480\n Sorting is a guarantee.\n\n1:52:01.480 --> 1:52:05.600\n If you were to call sort on a particular array of numbers,\n\n1:52:05.600 --> 1:52:07.640\n you are guaranteed that it's going to be sorted.\n\n1:52:07.640 --> 1:52:09.320\n Otherwise it's a bug.\n\n1:52:09.320 --> 1:52:10.160\n Now for machine learning,\n\n1:52:10.160 --> 1:52:12.440\n it's very hard to characterize this.\n\n1:52:12.440 --> 1:52:15.440\n We know for a fact that a cat recognition model\n\n1:52:15.440 --> 1:52:17.040\n is not going to recognize cats,\n\n1:52:17.040 --> 1:52:19.720\n every cat in the world in every circumstance.\n\n1:52:19.720 --> 1:52:22.040\n I think most people would agree with that statement,\n\n1:52:22.040 --> 1:52:23.600\n but we are still okay with it.\n\n1:52:23.600 --> 1:52:25.400\n We still don't call this as a bug.\n\n1:52:25.400 --> 1:52:26.720\n Whereas in traditional computer science\n\n1:52:26.720 --> 1:52:27.840\n or traditional science,\n\n1:52:27.840 --> 1:52:29.960\n like if you have this kind of failure case existing,\n\n1:52:29.960 --> 1:52:33.160\n then you think of it as like something is wrong.\n\n1:52:33.160 --> 1:52:34.520\n I think there is this sort of notion\n\n1:52:34.520 --> 1:52:37.000\n of nebulous correctness for machine learning.\n\n1:52:37.000 --> 1:52:38.840\n And that's something we just need to be very comfortable\n\n1:52:38.840 --> 1:52:39.680\n with.\n\n1:52:39.680 --> 1:52:40.520\n And for deep learning,\n\n1:52:40.520 --> 1:52:42.680\n or like for a lot of these machine learning algorithms,\n\n1:52:42.680 --> 1:52:44.680\n it's not clear how do we characterize\n\n1:52:44.680 --> 1:52:46.320\n this notion of correctness.\n\n1:52:46.320 --> 1:52:48.120\n I think limitation in our understanding,\n\n1:52:48.120 --> 1:52:51.160\n or at least a limitation in our phrasing of this.\n\n1:52:51.160 --> 1:52:53.080\n And if we were to come up with better ways\n\n1:52:53.080 --> 1:52:55.040\n to understand this limitation,\n\n1:52:55.040 --> 1:52:57.160\n then it would actually help us a lot.\n\n1:52:57.160 --> 1:52:58.840\n Do you think there's a distinction\n\n1:52:58.840 --> 1:53:01.800\n between the concept of learning\n\n1:53:01.800 --> 1:53:03.360\n and the concept of reasoning?\n\n1:53:04.240 --> 1:53:09.240\n Do you think it's possible for neural networks to reason?\n\n1:53:10.280 --> 1:53:11.680\n So I think of it slightly differently.\n\n1:53:11.680 --> 1:53:14.520\n So for me, learning is whenever\n\n1:53:14.520 --> 1:53:16.040\n I can like make a snap judgment.\n\n1:53:16.040 --> 1:53:17.200\n So if you show me a picture of a dog,\n\n1:53:17.200 --> 1:53:18.880\n I can immediately say it's a dog.\n\n1:53:18.880 --> 1:53:20.680\n But if you give me like a puzzle,\n\n1:53:20.680 --> 1:53:23.480\n like whatever a Goldsberg machine\n\n1:53:23.480 --> 1:53:24.960\n of like things going to happen,\n\n1:53:24.960 --> 1:53:26.440\n then I have to reason because I've never,\n\n1:53:26.440 --> 1:53:27.600\n it's a very complicated setup.\n\n1:53:27.600 --> 1:53:29.280\n I've never seen that particular setup.\n\n1:53:29.280 --> 1:53:32.200\n And I really need to draw and like imagine in my head\n\n1:53:32.200 --> 1:53:34.640\n what's going to happen to figure it out.\n\n1:53:34.640 --> 1:53:36.840\n So I think, yes, neural networks are really good\n\n1:53:36.840 --> 1:53:41.160\n at recognition, but they're not very good at reasoning.\n\n1:53:41.160 --> 1:53:44.120\n Because they have seen something before\n\n1:53:44.120 --> 1:53:46.360\n or seen something similar before, they're very good\n\n1:53:46.360 --> 1:53:48.240\n at making those sort of snap judgments.\n\n1:53:48.240 --> 1:53:50.680\n But if you were to give them a very complicated thing\n\n1:53:50.680 --> 1:53:52.480\n that they've not seen before,\n\n1:53:52.480 --> 1:53:55.320\n they have very limited ability right now\n\n1:53:55.320 --> 1:53:56.560\n to compose different things.\n\n1:53:56.560 --> 1:53:58.240\n Like, oh, I've seen this particular part before.\n\n1:53:58.240 --> 1:54:00.040\n I've seen this particular part before.\n\n1:54:00.040 --> 1:54:01.400\n And now probably like this is how\n\n1:54:01.400 --> 1:54:02.920\n they're going to work in tandem.\n\n1:54:02.920 --> 1:54:04.160\n It's very hard for them to come up\n\n1:54:04.160 --> 1:54:05.200\n with these kinds of things.\n\n1:54:05.200 --> 1:54:08.800\n Well, there's a certain aspect to reasoning\n\n1:54:08.800 --> 1:54:11.880\n that you can maybe convert into the process of programming.\n\n1:54:11.880 --> 1:54:14.320\n And so there's the whole field of program synthesis\n\n1:54:14.320 --> 1:54:17.240\n and people have been applying machine learning\n\n1:54:17.240 --> 1:54:18.920\n to the problem of program synthesis.\n\n1:54:18.920 --> 1:54:22.680\n And the question is, can they, the step of composition,\n\n1:54:22.680 --> 1:54:24.200\n why can't that be learned?\n\n1:54:25.280 --> 1:54:29.400\n You know, this step of like building things on top of you,\n\n1:54:29.400 --> 1:54:33.200\n like little intuitions, concepts on top of each other,\n\n1:54:33.200 --> 1:54:35.280\n can that be learnable?\n\n1:54:35.280 --> 1:54:36.800\n What's your intuition there?\n\n1:54:36.800 --> 1:54:39.440\n Or like, I guess similar set of techniques,\n\n1:54:39.440 --> 1:54:42.040\n do you think that will be applicable?\n\n1:54:42.040 --> 1:54:44.640\n So I think it is, of course, it is learnable\n\n1:54:44.640 --> 1:54:47.080\n because like we are prime examples of machines\n\n1:54:47.080 --> 1:54:49.480\n that have like, or individuals that have learned this, right?\n\n1:54:49.480 --> 1:54:51.080\n Like humans have learned this.\n\n1:54:51.080 --> 1:54:52.760\n So it is, of course, it is a technique\n\n1:54:52.760 --> 1:54:54.200\n that is very easy to learn.\n\n1:54:55.840 --> 1:54:58.400\n I think where we are kind of hitting a wall\n\n1:54:58.400 --> 1:55:00.480\n basically with like current machine learning\n\n1:55:00.480 --> 1:55:03.400\n is the fact that when the network learns\n\n1:55:03.400 --> 1:55:04.640\n all of this information,\n\n1:55:04.640 --> 1:55:07.480\n we basically are not able to figure out\n\n1:55:07.480 --> 1:55:10.640\n how well it's going to generalize to an unseen thing.\n\n1:55:10.640 --> 1:55:13.880\n And we have no, like a priori, no way of characterizing that.\n\n1:55:15.040 --> 1:55:17.640\n And I think that's basically telling us a lot about,\n\n1:55:18.480 --> 1:55:20.720\n like a lot about the fact that we really don't know\n\n1:55:20.720 --> 1:55:22.760\n what this model has learned and how well it's basically,\n\n1:55:22.760 --> 1:55:25.120\n because we don't know how well it's going to transfer.\n\n1:55:25.120 --> 1:55:28.080\n There's also a sense in which it feels like\n\n1:55:28.080 --> 1:55:33.080\n we humans may not be aware of how much like background,\n\n1:55:34.400 --> 1:55:36.760\n how good our background model is,\n\n1:55:36.760 --> 1:55:39.880\n how much knowledge we just have slowly building\n\n1:55:39.880 --> 1:55:41.400\n on top of each other.\n\n1:55:41.400 --> 1:55:42.480\n It feels like neural networks\n\n1:55:42.480 --> 1:55:43.840\n are constantly throwing stuff out.\n\n1:55:43.840 --> 1:55:45.360\n Like you'll do some incredible thing\n\n1:55:45.360 --> 1:55:49.040\n where you're learning a particular task in computer vision,\n\n1:55:49.040 --> 1:55:51.240\n you celebrate your state of the art successes\n\n1:55:51.240 --> 1:55:52.720\n and you throw that out.\n\n1:55:52.720 --> 1:55:54.240\n Like, it feels like it's,\n\n1:55:54.240 --> 1:55:56.720\n you're never using stuff you've learned\n\n1:55:56.720 --> 1:56:00.080\n for your future successes in other domains.\n\n1:56:00.080 --> 1:56:03.240\n And humans are obviously doing that exceptionally well,\n\n1:56:03.240 --> 1:56:05.840\n still throwing stuff away in their mind,\n\n1:56:05.840 --> 1:56:07.840\n but keeping certain kernels of truth.\n\n1:56:07.840 --> 1:56:09.200\n Right, so I think we're like,\n\n1:56:09.200 --> 1:56:11.080\n continual learning is sort of the paradigm\n\n1:56:11.080 --> 1:56:11.920\n for this in machine learning.\n\n1:56:11.920 --> 1:56:15.160\n And I don't think it's a very well explored paradigm.\n\n1:56:15.160 --> 1:56:17.440\n We have like things in deep learning, for example,\n\n1:56:17.440 --> 1:56:20.160\n catastrophic forgetting is like one of the standard things.\n\n1:56:20.160 --> 1:56:23.120\n The thing basically being that if you teach a network\n\n1:56:23.120 --> 1:56:24.760\n like to recognize dogs,\n\n1:56:24.760 --> 1:56:27.400\n and now you teach that same network to recognize cats,\n\n1:56:27.400 --> 1:56:29.040\n it basically forgets how to recognize dogs.\n\n1:56:29.040 --> 1:56:30.800\n So it forgets very quickly.\n\n1:56:30.800 --> 1:56:32.520\n I mean, and whereas a human,\n\n1:56:32.520 --> 1:56:34.560\n if you were to teach someone to recognize dogs\n\n1:56:34.560 --> 1:56:35.880\n and then to recognize cats,\n\n1:56:35.880 --> 1:56:38.440\n they don't forget immediately how to recognize these dogs.\n\n1:56:38.440 --> 1:56:40.640\n I think that's basically sort of what you're trying to get.\n\n1:56:40.640 --> 1:56:42.400\n Yeah, I just, I wonder if like\n\n1:56:42.400 --> 1:56:44.720\n the long term memory mechanisms\n\n1:56:44.720 --> 1:56:47.080\n or the mechanisms that store not just memories,\n\n1:56:47.080 --> 1:56:52.080\n but concepts that allow you to the reason\n\n1:56:54.240 --> 1:56:57.200\n and compose concepts,\n\n1:56:57.200 --> 1:56:59.000\n if those things will look very different\n\n1:56:59.000 --> 1:56:59.880\n than neural networks,\n\n1:56:59.880 --> 1:57:02.320\n or if you can do that within a single neural network\n\n1:57:02.320 --> 1:57:06.040\n with some particular sort of architecture quirks,\n\n1:57:06.040 --> 1:57:07.720\n that seems to be a really open problem.\n\n1:57:07.720 --> 1:57:09.440\n And of course I go up and down on that\n\n1:57:09.440 --> 1:57:14.440\n because there's something so compelling to the symbolic AI\n\n1:57:14.840 --> 1:57:19.840\n or to the ideas of logic based sort of expert systems.\n\n1:57:20.320 --> 1:57:22.440\n You have like human interpretable facts\n\n1:57:22.440 --> 1:57:24.080\n that built on top of each other.\n\n1:57:24.080 --> 1:57:27.800\n It's really annoying like with self supervised learning\n\n1:57:27.800 --> 1:57:31.120\n that the AI is not very explainable.\n\n1:57:31.120 --> 1:57:33.360\n Like you can't like understand\n\n1:57:33.360 --> 1:57:35.520\n all the beautiful things it has learned.\n\n1:57:35.520 --> 1:57:38.400\n You can't ask it like questions,\n\n1:57:38.400 --> 1:57:40.960\n but then again, maybe that's a stupid thing\n\n1:57:40.960 --> 1:57:42.440\n for us humans to want.\n\n1:57:42.440 --> 1:57:45.240\n Right, I think whenever we try to like understand it,\n\n1:57:45.240 --> 1:57:47.840\n we are putting our own subjective human bias into it.\n\n1:57:47.840 --> 1:57:48.680\n Yeah.\n\n1:57:48.680 --> 1:57:50.000\n And I think that's the sort of problem\n\n1:57:50.000 --> 1:57:51.000\n with self supervised learning,\n\n1:57:51.000 --> 1:57:54.280\n the goal is that it should learn naturally from the data.\n\n1:57:54.280 --> 1:57:55.520\n So now if you try to understand it,\n\n1:57:55.520 --> 1:57:58.640\n you are using your own preconceived notions\n\n1:57:58.640 --> 1:58:00.600\n of what this model has learned.\n\n1:58:00.600 --> 1:58:02.480\n And that's the problem.\n\n1:58:03.480 --> 1:58:04.640\n High level question.\n\n1:58:04.640 --> 1:58:07.920\n What do you think it takes to build a system\n\n1:58:07.920 --> 1:58:10.520\n with superhuman, maybe let's say human level\n\n1:58:10.520 --> 1:58:13.520\n or superhuman level general intelligence?\n\n1:58:13.520 --> 1:58:15.560\n We've already kind of started talking about this,\n\n1:58:15.560 --> 1:58:17.760\n but what's your intuition?\n\n1:58:17.760 --> 1:58:20.760\n Like, does this thing have to have a body?\n\n1:58:20.760 --> 1:58:23.920\n Does it have to interact richly with the world?\n\n1:58:25.400 --> 1:58:27.920\n Does it have to have some more human elements\n\n1:58:27.920 --> 1:58:30.480\n like self awareness?\n\n1:58:30.480 --> 1:58:32.240\n I think emotion.\n\n1:58:32.240 --> 1:58:35.720\n I think emotion is something which is like,\n\n1:58:35.720 --> 1:58:37.520\n it's not really attributed typically\n\n1:58:37.520 --> 1:58:38.440\n in standard machine learning.\n\n1:58:38.440 --> 1:58:39.560\n It's not something we think about,\n\n1:58:39.560 --> 1:58:41.040\n like there is NLP, there is vision,\n\n1:58:41.040 --> 1:58:42.560\n there is no like emotion.\n\n1:58:42.560 --> 1:58:44.600\n Emotion is never a part of all of this.\n\n1:58:44.600 --> 1:58:47.080\n And that just seems a little bit weird to me.\n\n1:58:47.080 --> 1:58:50.320\n I think the reason basically being that there is surprise\n\n1:58:50.320 --> 1:58:53.800\n and like, basically emotion is like one of the reasons\n\n1:58:53.800 --> 1:58:55.800\n emotions arise is like what happens\n\n1:58:55.800 --> 1:58:57.120\n and what do you expect to happen, right?\n\n1:58:57.120 --> 1:58:59.440\n There is like a mismatch between these things.\n\n1:58:59.440 --> 1:59:01.080\n And so that gives rise to like,\n\n1:59:01.080 --> 1:59:03.520\n I can either be surprised or I can be saddened\n\n1:59:03.520 --> 1:59:05.320\n or I can be happy and all of this.\n\n1:59:05.320 --> 1:59:07.960\n And so this basically indicates\n\n1:59:07.960 --> 1:59:10.160\n that I already have a predictive model in my head\n\n1:59:10.160 --> 1:59:11.840\n and something that I predicted or something\n\n1:59:11.840 --> 1:59:13.720\n that I thought was likely to happen.\n\n1:59:13.720 --> 1:59:15.120\n And then there was something that I observed\n\n1:59:15.120 --> 1:59:16.720\n that happened that there was a disconnect\n\n1:59:16.720 --> 1:59:18.280\n between these two things.\n\n1:59:18.280 --> 1:59:21.840\n And that basically is like maybe one of the reasons\n\n1:59:21.840 --> 1:59:24.280\n like you have a lot of emotions.\n\n1:59:24.280 --> 1:59:26.880\n Yeah, I think, so I talk to people a lot about them\n\n1:59:26.880 --> 1:59:29.120\n like Lisa Feldman Barrett.\n\n1:59:29.120 --> 1:59:31.720\n I think that's an interesting concept of emotion\n\n1:59:31.720 --> 1:59:36.720\n but I have a sense that emotion primarily\n\n1:59:36.880 --> 1:59:38.080\n in the way we think about it,\n\n1:59:38.080 --> 1:59:40.320\n which is the display of emotion\n\n1:59:40.320 --> 1:59:43.800\n is a communication mechanism between humans.\n\n1:59:43.800 --> 1:59:48.240\n So it's a part of basically human to human interaction,\n\n1:59:48.240 --> 1:59:50.200\n an important part, but just the part.\n\n1:59:50.200 --> 1:59:55.040\n So it's like, I would throw it into the full mix\n\n1:59:55.040 --> 1:59:58.040\n of communication.\n\n1:59:58.040 --> 2:00:01.240\n And to me, communication can be done with objects\n\n2:00:01.240 --> 2:00:04.360\n that don't look at all like humans.\n\n2:00:04.360 --> 2:00:05.440\n Okay.\n\n2:00:05.440 --> 2:00:07.560\n I've seen our ability to anthropomorphize\n\n2:00:07.560 --> 2:00:10.680\n our ability to connect with things that look like a Roomba\n\n2:00:10.680 --> 2:00:12.000\n our ability to connect.\n\n2:00:12.000 --> 2:00:14.720\n First of all, let's talk about other biological systems\n\n2:00:14.720 --> 2:00:17.440\n like dogs, our ability to love things\n\n2:00:17.440 --> 2:00:19.400\n that are very different than humans.\n\n2:00:19.400 --> 2:00:20.960\n But they do display emotion, right?\n\n2:00:20.960 --> 2:00:23.200\n I mean, dogs do display emotion.\n\n2:00:23.200 --> 2:00:25.320\n So they don't have to be anthropomorphic\n\n2:00:25.320 --> 2:00:27.600\n for them to like display the kind of emotions\n\n2:00:27.600 --> 2:00:28.440\n that we don't.\n\n2:00:28.440 --> 2:00:29.280\n Exactly.\n\n2:00:29.280 --> 2:00:32.160\n So, I mean, but then the word emotion starts to lose.\n\n2:00:33.920 --> 2:00:36.280\n So then we have to be, I guess specific, but yeah.\n\n2:00:36.280 --> 2:00:39.520\n So have rich flavorful communication.\n\n2:00:39.520 --> 2:00:40.360\n Communication, yeah.\n\n2:00:40.360 --> 2:00:43.000\n Yeah, so like, yes, it's full of emotion.\n\n2:00:43.000 --> 2:00:48.000\n It's full of wit and humor and moods\n\n2:00:49.080 --> 2:00:50.280\n and all those kinds of things, yeah.\n\n2:00:50.280 --> 2:00:53.720\n So you're talking about like flavor.\n\n2:00:53.720 --> 2:00:54.560\n Flavor, yeah.\n\n2:00:54.560 --> 2:00:55.400\n Okay, let's call it that.\n\n2:00:55.400 --> 2:00:57.240\n So there's content and then there is flavor\n\n2:00:57.240 --> 2:00:58.440\n and I'm talking about the flavor.\n\n2:00:58.440 --> 2:01:00.280\n Do you think it needs to have a body?\n\n2:01:00.280 --> 2:01:02.840\n Do you think like to interact with the physical world?\n\n2:01:02.840 --> 2:01:04.640\n Do you think you can understand the physical world\n\n2:01:04.640 --> 2:01:07.080\n without being able to directly interact with it?\n\n2:01:07.080 --> 2:01:08.440\n I don't think so, yeah.\n\n2:01:08.440 --> 2:01:10.720\n I think at some point we will need to bite the bullet\n\n2:01:10.720 --> 2:01:12.680\n and actually interact with the physical,\n\n2:01:12.680 --> 2:01:15.880\n as much as I like working on like passive computer vision\n\n2:01:15.880 --> 2:01:17.280\n where I just like sit in my arm chair\n\n2:01:17.280 --> 2:01:19.040\n and look at videos and learn.\n\n2:01:19.040 --> 2:01:22.760\n I do think that we will need to have some kind of embodiment\n\n2:01:22.760 --> 2:01:24.600\n or some kind of interaction\n\n2:01:24.600 --> 2:01:26.960\n to figure out things about the world.\n\n2:01:26.960 --> 2:01:28.640\n What about consciousness?\n\n2:01:28.640 --> 2:01:32.320\n Do you think, how often do you think about consciousness\n\n2:01:32.320 --> 2:01:34.320\n when you think about your work?\n\n2:01:34.320 --> 2:01:35.280\n You could think of it\n\n2:01:35.280 --> 2:01:37.760\n as the more simple thing of self awareness,\n\n2:01:38.640 --> 2:01:43.640\n of being aware that you are a perceiving,\n\n2:01:43.880 --> 2:01:46.840\n sensing, acting thing in this world.\n\n2:01:46.840 --> 2:01:50.320\n Or you can think about the bigger version of that,\n\n2:01:50.320 --> 2:01:51.640\n which is consciousness,\n\n2:01:51.640 --> 2:01:56.640\n which is having it feel like something to be that entity,\n\n2:01:57.200 --> 2:01:59.560\n the subjective experience of being in this world.\n\n2:01:59.560 --> 2:02:01.440\n So I think of self awareness a little bit more\n\n2:02:01.440 --> 2:02:03.400\n than like the broader goal of it,\n\n2:02:03.400 --> 2:02:06.120\n because I think self awareness is pretty critical\n\n2:02:06.120 --> 2:02:09.280\n for like any kind of like any kind of AGI\n\n2:02:09.280 --> 2:02:10.680\n or whatever you want to call it that we build,\n\n2:02:10.680 --> 2:02:13.960\n because it needs to contextualize what it is\n\n2:02:13.960 --> 2:02:15.600\n and what role it's playing\n\n2:02:15.600 --> 2:02:17.960\n with respect to all the other things that exist around it.\n\n2:02:17.960 --> 2:02:19.680\n I think that requires self awareness.\n\n2:02:19.680 --> 2:02:23.520\n It needs to understand that it's an autonomous car, right?\n\n2:02:23.520 --> 2:02:24.920\n And what does that mean?\n\n2:02:24.920 --> 2:02:26.240\n What are its limitations?\n\n2:02:26.240 --> 2:02:29.080\n What are the things that it is supposed to do and so on?\n\n2:02:29.080 --> 2:02:30.760\n What is its role in some way?\n\n2:02:30.760 --> 2:02:34.240\n Or, I mean, these are the kinds of things\n\n2:02:34.240 --> 2:02:36.880\n that we kind of expect from it, I would say.\n\n2:02:36.880 --> 2:02:39.360\n And so that's the level of self awareness\n\n2:02:39.360 --> 2:02:42.200\n that's, I would say, basically required at least,\n\n2:02:42.200 --> 2:02:44.280\n if not more than that.\n\n2:02:44.280 --> 2:02:46.440\n Yeah, I tend to, on the emotion side,\n\n2:02:46.440 --> 2:02:48.360\n believe that it has to have,\n\n2:02:48.360 --> 2:02:52.560\n it has to be able to display consciousness.\n\n2:02:52.560 --> 2:02:54.360\n Display consciousness, what do you mean by that?\n\n2:02:54.360 --> 2:02:57.600\n Meaning like for us humans to connect with each other\n\n2:02:57.600 --> 2:03:01.680\n or to connect with other living entities,\n\n2:03:01.680 --> 2:03:04.200\n I think we need to feel,\n\n2:03:04.200 --> 2:03:06.840\n like in order for us to truly feel\n\n2:03:06.840 --> 2:03:09.400\n like that there's another being there,\n\n2:03:09.400 --> 2:03:11.440\n we have to believe that they're conscious.\n\n2:03:11.440 --> 2:03:14.960\n And so we won't ever connect with something\n\n2:03:14.960 --> 2:03:17.320\n that doesn't have elements of consciousness.\n\n2:03:17.320 --> 2:03:21.560\n Now I tend to think that that's easier to achieve\n\n2:03:21.560 --> 2:03:23.080\n than it may sound,\n\n2:03:23.080 --> 2:03:25.720\n because we anthropomorphize stuff so hard.\n\n2:03:25.720 --> 2:03:28.760\n Like you have a mug that just like has wheels\n\n2:03:28.760 --> 2:03:31.920\n and like rotates every once in a while and makes a sound.\n\n2:03:31.920 --> 2:03:34.320\n I think a couple of days in,\n\n2:03:34.320 --> 2:03:39.320\n especially if you don't hang out with humans,\n\n2:03:39.520 --> 2:03:42.200\n you might start to believe that mug on wheels is conscious.\n\n2:03:42.200 --> 2:03:44.840\n So I think we anthropomorphize pretty effectively\n\n2:03:44.840 --> 2:03:46.040\n as human beings.\n\n2:03:46.040 --> 2:03:49.240\n But I do think that it's in the same bucket\n\n2:03:49.240 --> 2:03:50.920\n that we'll call emotion,\n\n2:03:50.920 --> 2:03:54.720\n that show that you're,\n\n2:03:54.720 --> 2:03:57.400\n I think of consciousness as the capacity to suffer.\n\n2:03:58.320 --> 2:04:02.400\n And if you're an entity that's able to feel things\n\n2:04:02.400 --> 2:04:05.560\n in the world and to communicate that to others,\n\n2:04:06.640 --> 2:04:08.520\n I think that's a really powerful way\n\n2:04:08.520 --> 2:04:10.880\n to interact with humans.\n\n2:04:10.880 --> 2:04:13.200\n And in order to create an AGI system,\n\n2:04:13.200 --> 2:04:18.000\n I believe you should be able to richly interact with humans.\n\n2:04:18.000 --> 2:04:21.120\n Like humans would need to want to interact with you.\n\n2:04:21.120 --> 2:04:22.200\n Like it can't be like,\n\n2:04:22.200 --> 2:04:27.200\n it's the self supervised learning versus like,\n\n2:04:27.400 --> 2:04:29.280\n like the robot shouldn't have to pay you\n\n2:04:29.280 --> 2:04:30.400\n to interact with me.\n\n2:04:30.400 --> 2:04:33.600\n So like it should be a natural fun thing.\n\n2:04:33.600 --> 2:04:36.080\n And then you're going to scale up significantly\n\n2:04:36.080 --> 2:04:39.080\n how much interaction it gets.\n\n2:04:39.080 --> 2:04:40.840\n It's the Alexa prize,\n\n2:04:40.840 --> 2:04:43.400\n which they were trying to get me to be a judge\n\n2:04:43.400 --> 2:04:44.680\n on their contest.\n\n2:04:44.680 --> 2:04:46.040\n Let's see if I want to do that.\n\n2:04:46.040 --> 2:04:50.560\n But their challenge is to talk to you,\n\n2:04:50.560 --> 2:04:53.960\n make the human sufficiently interested\n\n2:04:53.960 --> 2:04:56.160\n that the human keeps talking for 20 minutes.\n\n2:04:56.160 --> 2:04:57.000\n To Alexa?\n\n2:04:57.000 --> 2:04:58.600\n To Alexa, yeah.\n\n2:04:58.600 --> 2:05:00.240\n And right now they're not even close to that\n\n2:05:00.240 --> 2:05:02.560\n because it just gets so boring when you're like,\n\n2:05:02.560 --> 2:05:04.280\n when the intelligence is not there,\n\n2:05:04.280 --> 2:05:06.920\n it gets very not interesting to talk to it.\n\n2:05:06.920 --> 2:05:08.960\n And so the robot needs to be interesting.\n\n2:05:08.960 --> 2:05:10.440\n And one of the ways it can be interesting\n\n2:05:10.440 --> 2:05:14.680\n is display the capacity to love, to suffer.\n\n2:05:14.680 --> 2:05:17.480\n And I would say that essentially means\n\n2:05:17.480 --> 2:05:20.920\n the capacity to display consciousness.\n\n2:05:20.920 --> 2:05:25.160\n Like it is an entity, much like a human being.\n\n2:05:25.160 --> 2:05:27.320\n Of course, what that really means,\n\n2:05:27.320 --> 2:05:30.520\n I don't know if that's fundamentally a robotics problem\n\n2:05:30.520 --> 2:05:33.040\n or some kind of problem that we're not yet even aware.\n\n2:05:33.040 --> 2:05:36.040\n Like if it is truly a hard problem of consciousness,\n\n2:05:36.040 --> 2:05:38.600\n I tend to maybe optimistically think it's a,\n\n2:05:38.600 --> 2:05:42.640\n we can pretty effectively fake it till we make it.\n\n2:05:42.640 --> 2:05:46.400\n So we can display a lot of human like elements for a while.\n\n2:05:46.400 --> 2:05:49.080\n And that will be sufficient to form\n\n2:05:49.080 --> 2:05:50.920\n really close connections with humans.\n\n2:05:52.000 --> 2:05:53.720\n What's used the most beautiful idea\n\n2:05:53.720 --> 2:05:55.840\n in self supervised learning?\n\n2:05:55.840 --> 2:05:59.040\n Like when you sit back with, I don't know,\n\n2:05:59.040 --> 2:06:03.200\n with a glass of wine and an armchair\n\n2:06:03.200 --> 2:06:06.080\n and just at a fireplace,\n\n2:06:06.080 --> 2:06:08.720\n just thinking how beautiful this world that you get\n\n2:06:08.720 --> 2:06:10.560\n to explore is, what do you think\n\n2:06:10.560 --> 2:06:12.820\n is the especially beautiful idea?\n\n2:06:13.800 --> 2:06:16.480\n The fact that like object level,\n\n2:06:16.480 --> 2:06:19.960\n what objects are and some notion of objectness emerges\n\n2:06:19.960 --> 2:06:22.800\n from these models by just like self supervised learning.\n\n2:06:23.680 --> 2:06:28.680\n So for example, like one of the things like the dyno paper\n\n2:06:28.920 --> 2:06:33.040\n that I was a part of at Facebook is the object sort\n\n2:06:33.040 --> 2:06:35.600\n of boundaries emerge from these representations.\n\n2:06:35.600 --> 2:06:38.060\n So if you have like a dog running in the field,\n\n2:06:38.060 --> 2:06:39.440\n the boundaries around the dog,\n\n2:06:39.440 --> 2:06:42.320\n the network is basically able to figure out\n\n2:06:42.320 --> 2:06:45.520\n what the boundaries of this dog are automatically.\n\n2:06:45.520 --> 2:06:47.040\n And it was never trained to do that.\n\n2:06:47.040 --> 2:06:50.160\n It was never trained to, no one taught it\n\n2:06:50.160 --> 2:06:52.680\n that this is a dog and these pixels belong to a dog.\n\n2:06:52.680 --> 2:06:55.000\n It's able to group these things together automatically.\n\n2:06:55.000 --> 2:06:56.160\n So that's one.\n\n2:06:56.160 --> 2:07:00.000\n I think in general, that entire notion that this dumb idea\n\n2:07:00.000 --> 2:07:01.960\n that you take like these two crops of an image\n\n2:07:01.960 --> 2:07:04.120\n and then you say that the features should be similar,\n\n2:07:04.120 --> 2:07:06.040\n that has resulted in something like this,\n\n2:07:06.040 --> 2:07:07.920\n like the model is able to figure out\n\n2:07:07.920 --> 2:07:10.320\n what the dog pixels are and so on.\n\n2:07:10.320 --> 2:07:12.120\n That just seems like so surprising.\n\n2:07:13.440 --> 2:07:16.200\n And I mean, I don't think a lot of us even understand\n\n2:07:16.200 --> 2:07:18.120\n how that is happening really.\n\n2:07:18.120 --> 2:07:20.800\n And it's something we are taking for granted,\n\n2:07:20.800 --> 2:07:23.120\n maybe like a lot in terms of how we're setting up\n\n2:07:23.120 --> 2:07:24.920\n these algorithms, but it's just,\n\n2:07:24.920 --> 2:07:26.780\n it's a very beautiful and powerful idea.\n\n2:07:26.780 --> 2:07:30.240\n So it's really fundamentally telling us something about\n\n2:07:30.240 --> 2:07:32.440\n that there is so much signal in the pixels\n\n2:07:32.440 --> 2:07:34.120\n that we can be super dumb about it.\n\n2:07:34.120 --> 2:07:35.200\n How about how we are setting up\n\n2:07:35.200 --> 2:07:37.080\n the self sequencing problem.\n\n2:07:37.080 --> 2:07:39.600\n And despite being like super dumb about it,\n\n2:07:39.600 --> 2:07:41.640\n we'll actually get very good,\n\n2:07:41.640 --> 2:07:44.000\n like we'll actually get something that is able to do\n\n2:07:44.000 --> 2:07:45.720\n very like surprising things.\n\n2:07:45.720 --> 2:07:48.280\n I wonder if there's other like objectness\n\n2:07:48.280 --> 2:07:50.380\n of other concepts that can emerge.\n\n2:07:51.600 --> 2:07:53.600\n I don't know if you follow Francois Chollet,\n\n2:07:53.600 --> 2:07:56.600\n he had the competition for intelligence\n\n2:07:56.600 --> 2:07:59.560\n that basically it's kind of like an IQ test,\n\n2:07:59.560 --> 2:08:02.400\n but for machines, but for an IQ test,\n\n2:08:02.400 --> 2:08:05.360\n you have to have a few concepts that you want to apply.\n\n2:08:05.360 --> 2:08:07.800\n One of them is objectness.\n\n2:08:07.800 --> 2:08:11.520\n I wonder if those concepts can emerge\n\n2:08:11.520 --> 2:08:14.760\n through self supervised learning on billions of images.\n\n2:08:14.760 --> 2:08:16.320\n I think something like object permanence\n\n2:08:16.320 --> 2:08:17.440\n can definitely emerge, right?\n\n2:08:17.440 --> 2:08:20.240\n So that's like a fundamental concept which we have,\n\n2:08:20.240 --> 2:08:21.480\n maybe not through images, through video,\n\n2:08:21.480 --> 2:08:25.160\n but that's another concept that should be emerging from it\n\n2:08:25.160 --> 2:08:26.760\n because it's not something that,\n\n2:08:26.760 --> 2:08:29.120\n like if we don't teach humans that this isn't,\n\n2:08:29.120 --> 2:08:31.520\n this is like about this concept of object permanence,\n\n2:08:31.520 --> 2:08:32.500\n it actually emerges.\n\n2:08:32.500 --> 2:08:34.100\n And the same thing for like animals, like dogs,\n\n2:08:34.100 --> 2:08:36.360\n I think actually permanence automatically\n\n2:08:36.360 --> 2:08:38.080\n is something that they are born with.\n\n2:08:38.080 --> 2:08:40.320\n So I think it should emerge from the data.\n\n2:08:40.320 --> 2:08:42.440\n It should emerge basically very quickly.\n\n2:08:42.440 --> 2:08:45.880\n I wonder if ideas like symmetry, rotation,\n\n2:08:45.880 --> 2:08:47.920\n these kinds of things might emerge.\n\n2:08:47.920 --> 2:08:50.360\n So I think rotation, probably yes.\n\n2:08:50.360 --> 2:08:51.640\n Yeah, rotation, yes.\n\n2:08:51.640 --> 2:08:55.200\n I mean, there's some constraints in the architecture itself,\n\n2:08:55.200 --> 2:08:59.240\n but it's interesting if all of them could be,\n\n2:08:59.240 --> 2:09:04.240\n like counting was another one, being able to kind of\n\n2:09:04.280 --> 2:09:06.240\n understand that there's multiple objects\n\n2:09:06.240 --> 2:09:09.040\n of the same kind in the image and be able to count them.\n\n2:09:10.040 --> 2:09:11.560\n I wonder if all of that could be,\n\n2:09:11.560 --> 2:09:14.360\n if constructed correctly, they can emerge\n\n2:09:14.360 --> 2:09:16.480\n because then you can transfer those concepts\n\n2:09:16.480 --> 2:09:20.680\n to then interpret images at a deeper level.\n\n2:09:20.680 --> 2:09:21.520\n Right.\n\n2:09:21.520 --> 2:09:24.680\n Counting, I do believe, I mean, it should be possible.\n\n2:09:24.680 --> 2:09:25.920\n You don't know like yet,\n\n2:09:25.920 --> 2:09:29.720\n but I do think it's not that far in the realm of possibility.\n\n2:09:29.720 --> 2:09:30.560\n Yeah, that'd be interesting\n\n2:09:30.560 --> 2:09:33.240\n if using self supervised learning on images\n\n2:09:33.240 --> 2:09:36.520\n can then be applied to then solving those kinds of IQ tests,\n\n2:09:36.520 --> 2:09:38.840\n which seem currently to be kind of impossible.\n\n2:09:40.440 --> 2:09:43.320\n What idea do you believe might be true\n\n2:09:43.320 --> 2:09:46.600\n that most people think is not true\n\n2:09:46.600 --> 2:09:48.560\n or don't agree with you on?\n\n2:09:48.560 --> 2:09:50.040\n Is there something like that?\n\n2:09:50.040 --> 2:09:52.400\n So this is going to be a little controversial,\n\n2:09:52.400 --> 2:09:53.500\n but okay, sure.\n\n2:09:53.500 --> 2:09:55.340\n I don't believe in simulation.\n\n2:09:55.340 --> 2:09:58.840\n Like actually using simulation to do things very much.\n\n2:09:58.840 --> 2:10:01.040\n Just to clarify, because this is a podcast\n\n2:10:01.040 --> 2:10:03.600\n where you talk about, are we living in a simulation often?\n\n2:10:03.600 --> 2:10:08.000\n You're referring to using simulation to construct worlds\n\n2:10:08.000 --> 2:10:10.320\n that you then leverage for machine learning.\n\n2:10:10.320 --> 2:10:11.160\n Right, yeah.\n\n2:10:11.160 --> 2:10:13.080\n For example, like one example would be like\n\n2:10:13.080 --> 2:10:15.520\n to train an autonomous car driving system.\n\n2:10:15.520 --> 2:10:17.400\n You basically first build a simulator,\n\n2:10:17.400 --> 2:10:19.840\n which builds like the environment of the world.\n\n2:10:19.840 --> 2:10:22.680\n And then you basically have a lot of like,\n\n2:10:22.680 --> 2:10:25.320\n you train your machine learning system in that.\n\n2:10:25.320 --> 2:10:27.560\n So I believe it is possible,\n\n2:10:27.560 --> 2:10:30.920\n but I think it's a really expensive way of doing things.\n\n2:10:30.920 --> 2:10:33.760\n And at the end of it, you do need the real world.\n\n2:10:33.760 --> 2:10:35.520\n So I'm not sure.\n\n2:10:35.520 --> 2:10:36.920\n So maybe for certain settings,\n\n2:10:36.920 --> 2:10:38.880\n like maybe the payout is so large,\n\n2:10:38.880 --> 2:10:40.880\n like for autonomous driving, the payout is so large\n\n2:10:40.880 --> 2:10:43.360\n that you can actually invest that much money to build it.\n\n2:10:43.360 --> 2:10:45.480\n But I think as a general sort of principle,\n\n2:10:45.480 --> 2:10:47.040\n it does not apply to a lot of concepts.\n\n2:10:47.040 --> 2:10:49.720\n You can't really build simulations of everything.\n\n2:10:49.720 --> 2:10:51.520\n Not only because like one, it's expensive,\n\n2:10:51.520 --> 2:10:54.800\n because second, it's also not possible for a lot of things.\n\n2:10:54.800 --> 2:10:59.400\n So in general, like there's a lot of work\n\n2:10:59.400 --> 2:11:02.120\n on like using synthetic data and like synthetic simulators.\n\n2:11:02.120 --> 2:11:05.840\n I generally am not very, like I don't believe in that.\n\n2:11:05.840 --> 2:11:09.040\n So you're saying it's very challenging visually,\n\n2:11:09.040 --> 2:11:11.960\n like to correctly like simulate the visual,\n\n2:11:11.960 --> 2:11:13.600\n like the lighting, all those kinds of things.\n\n2:11:13.600 --> 2:11:15.680\n I mean, all these companies that you have, right?\n\n2:11:15.680 --> 2:11:17.880\n So like Pixar and like whatever,\n\n2:11:17.880 --> 2:11:19.840\n all these companies are,\n\n2:11:19.840 --> 2:11:21.540\n all this like computer graphics stuff\n\n2:11:21.540 --> 2:11:22.920\n is really about accurately,\n\n2:11:22.920 --> 2:11:26.120\n a lot of them is about like accurately trying to figure out\n\n2:11:26.120 --> 2:11:28.760\n how the lighting is and like how things reflect off\n\n2:11:28.760 --> 2:11:30.440\n of one another and so on,\n\n2:11:30.440 --> 2:11:32.280\n and like how sparkly things look and so on.\n\n2:11:32.280 --> 2:11:34.040\n So it's a very hard problem.\n\n2:11:34.040 --> 2:11:37.200\n So do we really need to solve that first\n\n2:11:37.200 --> 2:11:39.440\n to be able to like do computer vision?\n\n2:11:39.440 --> 2:11:40.640\n Probably not.\n\n2:11:40.640 --> 2:11:43.920\n And for me, in the context of autonomous driving,\n\n2:11:44.800 --> 2:11:48.040\n it's very tempting to be able to use simulation, right?\n\n2:11:48.040 --> 2:11:50.560\n Because it's a safety critical application,\n\n2:11:50.560 --> 2:11:54.960\n but the other limitation of simulation that perhaps\n\n2:11:54.960 --> 2:11:58.440\n is a bigger one than the visual limitation\n\n2:11:58.440 --> 2:12:00.840\n is the behavior of objects.\n\n2:12:00.840 --> 2:12:03.920\n So you're ultimately interested in edge cases.\n\n2:12:03.920 --> 2:12:05.000\n And the question is,\n\n2:12:05.000 --> 2:12:08.800\n how well can you generate edge cases in simulation,\n\n2:12:08.800 --> 2:12:11.080\n especially with human behavior?\n\n2:12:11.080 --> 2:12:13.480\n I think another problem is like for autonomous driving,\n\n2:12:13.480 --> 2:12:15.260\n it's a constantly changing world.\n\n2:12:15.260 --> 2:12:18.600\n So say autonomous driving like in 10 years from now,\n\n2:12:18.600 --> 2:12:20.800\n like there are lots of autonomous cars,\n\n2:12:20.800 --> 2:12:22.440\n but they're still going to be humans.\n\n2:12:22.440 --> 2:12:25.240\n So now there are 50% of the agents say, which are humans,\n\n2:12:25.240 --> 2:12:26.880\n 50% of the agents that are autonomous,\n\n2:12:26.880 --> 2:12:28.600\n like car driving agents.\n\n2:12:28.600 --> 2:12:30.120\n So now the mixture has changed.\n\n2:12:30.120 --> 2:12:32.360\n So now the kinds of behaviors that you actually expect\n\n2:12:32.360 --> 2:12:35.200\n from the other agents or other cars on the road\n\n2:12:35.200 --> 2:12:36.760\n are actually going to be very different.\n\n2:12:36.760 --> 2:12:39.120\n And as the proportion of the number of autonomous cars\n\n2:12:39.120 --> 2:12:40.480\n to humans keeps changing,\n\n2:12:40.480 --> 2:12:42.640\n this behavior will actually change a lot.\n\n2:12:42.640 --> 2:12:44.520\n So now if you were to build a simulator based on\n\n2:12:44.520 --> 2:12:46.480\n just like right now to build them today,\n\n2:12:46.480 --> 2:12:48.440\n you don't have that many autonomous cars on the road.\n\n2:12:48.440 --> 2:12:50.560\n So you would try to like make all of the other agents\n\n2:12:50.560 --> 2:12:52.920\n in that simulator behave as humans,\n\n2:12:52.920 --> 2:12:55.760\n but that's not really going to hold true 10, 15, 20,\n\n2:12:55.760 --> 2:12:57.400\n 30 years from now.\n\n2:12:57.400 --> 2:12:59.280\n Do you think we're living in a simulation?\n\n2:12:59.280 --> 2:13:00.120\n No.\n\n2:13:01.520 --> 2:13:02.840\n How hard is it?\n\n2:13:02.840 --> 2:13:04.880\n This is why I think it's an interesting question.\n\n2:13:04.880 --> 2:13:07.780\n How hard is it to build a video game,\n\n2:13:07.780 --> 2:13:12.660\n like virtual reality game where it is so real,\n\n2:13:12.660 --> 2:13:15.840\n forget like ultra realistic to where\n\n2:13:15.840 --> 2:13:17.400\n you can't tell the difference,\n\n2:13:17.400 --> 2:13:20.860\n but like it's so nice that you just want to stay there.\n\n2:13:20.860 --> 2:13:24.960\n You just want to stay there and you don't want to come back.\n\n2:13:24.960 --> 2:13:29.380\n Do you think that's doable within our lifetime?\n\n2:13:29.380 --> 2:13:31.700\n Within our lifetime, probably.\n\n2:13:31.700 --> 2:13:32.540\n Yeah.\n\n2:13:32.540 --> 2:13:33.880\n I eat healthy, I live long.\n\n2:13:33.880 --> 2:13:38.360\n Does that make you sad that there'll be like\n\n2:13:39.400 --> 2:13:44.280\n like population of kids that basically spend 95%,\n\n2:13:44.280 --> 2:13:47.520\n 99% of their time in a virtual world?\n\n2:13:50.120 --> 2:13:51.920\n Very, very hard question to answer.\n\n2:13:53.380 --> 2:13:55.760\n For certain people, it might be something\n\n2:13:55.760 --> 2:13:58.160\n that they really derive a lot of value out of,\n\n2:13:58.160 --> 2:14:00.760\n derive a lot of enjoyment and like happiness out of,\n\n2:14:00.760 --> 2:14:03.140\n and maybe the real world wasn't giving them that.\n\n2:14:03.140 --> 2:14:03.980\n That's why they did that.\n\n2:14:03.980 --> 2:14:05.960\n So maybe it is good for certain people.\n\n2:14:05.960 --> 2:14:09.400\n So ultimately, if it maximizes happiness,\n\n2:14:09.400 --> 2:14:10.240\n Right, I think if.\n\n2:14:10.240 --> 2:14:11.060\n Or we could judge.\n\n2:14:11.060 --> 2:14:12.780\n Yeah, I think if it's making people happy,\n\n2:14:12.780 --> 2:14:14.440\n maybe it's okay.\n\n2:14:14.440 --> 2:14:16.780\n Again, I think this is a very hard question.\n\n2:14:18.320 --> 2:14:22.600\n So like you've been a part of a lot of amazing papers.\n\n2:14:23.520 --> 2:14:25.640\n What advice would you give to somebody\n\n2:14:25.640 --> 2:14:28.060\n on what it takes to write a good paper?\n\n2:14:29.220 --> 2:14:31.020\n Grad students writing papers now,\n\n2:14:31.020 --> 2:14:34.540\n is there common things that you've learned along the way\n\n2:14:34.540 --> 2:14:35.760\n that you think it takes,\n\n2:14:35.760 --> 2:14:39.020\n both for a good idea and a good paper?\n\n2:14:39.020 --> 2:14:44.020\n Right, so I think both of these have picked up\n\n2:14:44.140 --> 2:14:46.580\n from like lots of people I've worked with in the past.\n\n2:14:46.580 --> 2:14:48.740\n So one of them is picking the right problem\n\n2:14:48.740 --> 2:14:51.100\n to work on in research is as important\n\n2:14:51.100 --> 2:14:53.720\n as like finding the solution to it.\n\n2:14:53.720 --> 2:14:56.220\n So I mean, there are multiple reasons for this.\n\n2:14:56.220 --> 2:14:59.000\n So one is that there are certain problems\n\n2:14:59.000 --> 2:15:02.380\n that can actually be solved in a particular timeframe.\n\n2:15:02.380 --> 2:15:06.420\n So now say you want to work on finding the meaning of life.\n\n2:15:06.420 --> 2:15:07.460\n This is a great problem.\n\n2:15:07.460 --> 2:15:09.460\n I think most people will agree with that.\n\n2:15:09.460 --> 2:15:12.260\n But do you believe that your talents\n\n2:15:12.260 --> 2:15:13.860\n and like the energy that you'll spend on it\n\n2:15:13.860 --> 2:15:17.300\n will make some kind of meaningful progress\n\n2:15:17.300 --> 2:15:18.860\n in your lifetime?\n\n2:15:18.860 --> 2:15:21.020\n If you are optimistic about it, then go ahead.\n\n2:15:21.020 --> 2:15:22.140\n That's why I started this podcast.\n\n2:15:22.140 --> 2:15:24.080\n I keep asking people about the meaning of life.\n\n2:15:24.080 --> 2:15:27.460\n I'm hoping by episode like 2.20, I'll figure it out.\n\n2:15:27.460 --> 2:15:30.300\n Oh, not too many episodes to go.\n\n2:15:30.300 --> 2:15:31.780\n All right, cool.\n\n2:15:31.780 --> 2:15:33.820\n Maybe today, I don't know, but you're right.\n\n2:15:33.820 --> 2:15:36.300\n So that seems intractable at the moment.\n\n2:15:36.300 --> 2:15:39.060\n Right, so I think it's just the fact of like,\n\n2:15:39.060 --> 2:15:41.100\n if you're starting a PhD, for example,\n\n2:15:41.100 --> 2:15:43.020\n what is one problem that you want to focus on\n\n2:15:43.020 --> 2:15:45.740\n that you do think is interesting enough,\n\n2:15:45.740 --> 2:15:47.800\n and you will be able to make a reasonable amount\n\n2:15:47.800 --> 2:15:50.540\n of headway into it that you think you'll be doing a PhD for?\n\n2:15:50.540 --> 2:15:53.100\n So in that kind of a timeframe.\n\n2:15:53.100 --> 2:15:53.920\n So that's one.\n\n2:15:53.920 --> 2:15:54.780\n Of course, there's the second part,\n\n2:15:54.780 --> 2:15:56.380\n which is what excites you genuinely.\n\n2:15:56.380 --> 2:15:57.620\n So you shouldn't just pick problems\n\n2:15:57.620 --> 2:15:59.020\n that you are not excited about,\n\n2:15:59.020 --> 2:16:01.860\n because as a grad student or as a researcher,\n\n2:16:01.860 --> 2:16:03.220\n you really need to be passionate about it\n\n2:16:03.220 --> 2:16:04.580\n to continue doing that,\n\n2:16:04.580 --> 2:16:05.740\n because there are so many other things\n\n2:16:05.740 --> 2:16:07.100\n that you could be doing in life.\n\n2:16:07.100 --> 2:16:08.260\n So you really need to believe in that\n\n2:16:08.260 --> 2:16:10.740\n to be able to do that for that long.\n\n2:16:10.740 --> 2:16:12.660\n In terms of papers, I think the one thing\n\n2:16:12.660 --> 2:16:13.660\n that I've learned is,\n\n2:16:15.580 --> 2:16:17.780\n like in the past, whenever I used to write things,\n\n2:16:17.780 --> 2:16:18.940\n and even now, whenever I do that,\n\n2:16:18.940 --> 2:16:21.420\n I try to cram in a lot of things into the paper,\n\n2:16:21.420 --> 2:16:22.820\n whereas what really matters\n\n2:16:22.820 --> 2:16:25.760\n is just pushing one simple idea, that's it.\n\n2:16:25.760 --> 2:16:29.980\n That's all because the paper is going to be like,\n\n2:16:29.980 --> 2:16:32.180\n whatever, eight or nine pages.\n\n2:16:32.180 --> 2:16:34.240\n If you keep cramming in lots of ideas,\n\n2:16:34.240 --> 2:16:36.240\n it's really hard for the single thing\n\n2:16:36.240 --> 2:16:38.020\n that you believe in to stand out.\n\n2:16:38.020 --> 2:16:40.900\n So if you really try to just focus,\n\n2:16:40.900 --> 2:16:41.940\n especially in terms of writing,\n\n2:16:41.940 --> 2:16:43.820\n really try to focus on one particular idea\n\n2:16:43.820 --> 2:16:46.220\n and articulate it out in multiple different ways,\n\n2:16:46.220 --> 2:16:49.020\n it's far more valuable to the reader as well,\n\n2:16:49.020 --> 2:16:51.600\n and basically to the reader, of course,\n\n2:16:51.600 --> 2:16:53.100\n because they get to,\n\n2:16:53.100 --> 2:16:54.420\n they know that this particular idea\n\n2:16:54.420 --> 2:16:56.140\n is associated with this paper,\n\n2:16:56.140 --> 2:16:59.260\n and also for you, because you have,\n\n2:16:59.260 --> 2:17:01.080\n when you write about a particular idea in different ways,\n\n2:17:01.080 --> 2:17:02.700\n you think about it more deeply.\n\n2:17:02.700 --> 2:17:06.020\n So as a grad student, I used to always wait to it,\n\n2:17:06.020 --> 2:17:08.700\n maybe in the last week or whatever, to write the paper,\n\n2:17:08.700 --> 2:17:10.280\n because I used to always believe\n\n2:17:10.280 --> 2:17:11.380\n that doing the experiments\n\n2:17:11.380 --> 2:17:13.860\n was actually the bigger part of research than writing.\n\n2:17:13.860 --> 2:17:15.260\n And my advisor always told me\n\n2:17:15.260 --> 2:17:16.660\n that you should start writing very early on,\n\n2:17:16.660 --> 2:17:17.900\n and I thought, oh, it doesn't matter,\n\n2:17:17.900 --> 2:17:19.700\n I don't know what he's talking about.\n\n2:17:19.700 --> 2:17:22.020\n But I think more and more I realized that's the case.\n\n2:17:22.020 --> 2:17:24.060\n Whenever I write something that I'm doing,\n\n2:17:24.060 --> 2:17:26.440\n I actually think much better about it.\n\n2:17:26.440 --> 2:17:28.820\n And so if you start writing early on,\n\n2:17:28.820 --> 2:17:31.220\n you actually, I think, get better ideas,\n\n2:17:31.220 --> 2:17:33.820\n or at least you figure out holes in your theory,\n\n2:17:33.820 --> 2:17:36.260\n or particular experiments that you should run\n\n2:17:36.260 --> 2:17:38.740\n to plug those holes, and so on.\n\n2:17:38.740 --> 2:17:40.340\n Yeah, I'm continually surprised\n\n2:17:40.340 --> 2:17:43.620\n how many really good papers throughout history\n\n2:17:43.620 --> 2:17:47.440\n are quite short and quite simple.\n\n2:17:48.340 --> 2:17:50.180\n And there's a lesson to that.\n\n2:17:50.180 --> 2:17:52.620\n If you want to dream about writing a paper\n\n2:17:52.620 --> 2:17:54.180\n that changes the world,\n\n2:17:54.180 --> 2:17:58.120\n and you wanna go by example, they're usually simple.\n\n2:17:58.120 --> 2:18:01.280\n And that's, it's not cramming,\n\n2:18:01.280 --> 2:18:06.280\n or it's focusing on one idea, and thinking deeply.\n\n2:18:07.200 --> 2:18:10.340\n And you're right that the writing process itself\n\n2:18:10.340 --> 2:18:12.280\n reveals the idea.\n\n2:18:12.280 --> 2:18:15.320\n It challenges you to really think about what is the idea\n\n2:18:15.320 --> 2:18:18.020\n that explains it, the thread that ties it all together.\n\n2:18:19.040 --> 2:18:21.540\n And so a lot of famous researchers I know\n\n2:18:21.540 --> 2:18:24.760\n actually would start off, like, first they were,\n\n2:18:24.760 --> 2:18:27.000\n even before the experiments were in,\n\n2:18:27.000 --> 2:18:28.360\n a lot of them would actually start\n\n2:18:28.360 --> 2:18:30.400\n with writing the introduction of the paper,\n\n2:18:30.400 --> 2:18:32.160\n with zero experiments in.\n\n2:18:32.160 --> 2:18:33.800\n Because that at least helps them figure out\n\n2:18:33.800 --> 2:18:35.800\n what they're trying to solve,\n\n2:18:35.800 --> 2:18:38.660\n and how it fits in the context of things right now.\n\n2:18:38.660 --> 2:18:40.680\n And that would really guide their entire research.\n\n2:18:40.680 --> 2:18:42.360\n So a lot of them would actually first write in intros\n\n2:18:42.360 --> 2:18:43.560\n with zero experiments in,\n\n2:18:43.560 --> 2:18:46.040\n and that's how they would start projects.\n\n2:18:46.040 --> 2:18:48.240\n Some basic questions about people maybe\n\n2:18:49.800 --> 2:18:51.960\n that are more like beginners in this field.\n\n2:18:51.960 --> 2:18:54.080\n What's the best programming language to learn\n\n2:18:54.080 --> 2:18:56.600\n if you're interested in machine learning?\n\n2:18:56.600 --> 2:18:57.440\n I would say Python,\n\n2:18:57.440 --> 2:19:00.320\n just because it's the easiest one to learn.\n\n2:19:00.320 --> 2:19:03.160\n And also a lot of like programming\n\n2:19:03.160 --> 2:19:05.000\n and machine learning happens in Python.\n\n2:19:05.000 --> 2:19:07.600\n So if you don't know any other programming language,\n\n2:19:07.600 --> 2:19:09.560\n Python is actually going to get you a long way.\n\n2:19:09.560 --> 2:19:11.680\n Yeah, it seems like sort of a,\n\n2:19:11.680 --> 2:19:14.000\n it's a toss up question because it seems like Python\n\n2:19:14.000 --> 2:19:16.800\n is so much dominating the space now.\n\n2:19:16.800 --> 2:19:18.520\n But I wonder if there's an interesting alternative.\n\n2:19:18.520 --> 2:19:19.960\n Obviously there's like Swift,\n\n2:19:19.960 --> 2:19:22.740\n and there's a lot of interesting alternatives popping up,\n\n2:19:22.740 --> 2:19:23.960\n even JavaScript.\n\n2:19:23.960 --> 2:19:28.880\n So I, or are more like for the data science applications.\n\n2:19:28.880 --> 2:19:31.240\n But it seems like Python more and more\n\n2:19:31.240 --> 2:19:34.160\n is actually being used to teach like introduction\n\n2:19:34.160 --> 2:19:35.880\n to programming at universities.\n\n2:19:35.880 --> 2:19:38.300\n So it just combines everything very nicely.\n\n2:19:39.840 --> 2:19:41.840\n Even harder question.\n\n2:19:41.840 --> 2:19:46.120\n What are the pros and cons of PyTorch versus TensorFlow?\n\n2:19:46.120 --> 2:19:46.960\n I see.\n\n2:19:48.440 --> 2:19:49.280\n Okay.\n\n2:19:49.280 --> 2:19:51.360\n You can go with no comment.\n\n2:19:51.360 --> 2:19:53.400\n So a disclaimer to this is that the last time\n\n2:19:53.400 --> 2:19:56.400\n I used TensorFlow was probably like four years ago.\n\n2:19:56.400 --> 2:19:58.160\n And so it was right when it had come out\n\n2:19:58.160 --> 2:20:02.660\n because so I started on like deep learning in 2014 or so,\n\n2:20:02.660 --> 2:20:06.480\n and the dominant sort of framework for us then\n\n2:20:06.480 --> 2:20:09.040\n for vision was Cafe, which was out of Berkeley.\n\n2:20:09.040 --> 2:20:11.240\n And we used Cafe a lot, it was really nice.\n\n2:20:12.120 --> 2:20:13.360\n And then TensorFlow came in,\n\n2:20:13.360 --> 2:20:15.080\n which was basically like Python first.\n\n2:20:15.080 --> 2:20:17.040\n So Cafe was mainly C++,\n\n2:20:17.040 --> 2:20:19.040\n and it had like very loose kind of Python binding.\n\n2:20:19.040 --> 2:20:21.320\n So Python wasn't really the first language you would use.\n\n2:20:21.320 --> 2:20:24.680\n You would really use either MATLAB or C++\n\n2:20:24.680 --> 2:20:28.240\n like get stuff done in like Cafe.\n\n2:20:28.240 --> 2:20:30.920\n And then Python of course became popular a little bit later.\n\n2:20:30.920 --> 2:20:32.620\n So TensorFlow was basically around that time.\n\n2:20:32.620 --> 2:20:35.240\n So 2015, 2016 is when I last used it.\n\n2:20:36.120 --> 2:20:37.200\n It's been a while.\n\n2:20:37.200 --> 2:20:40.600\n And then what, did you use Torch or did you?\n\n2:20:40.600 --> 2:20:44.040\n So then I moved to LuaTorch, which was the torch in Lua.\n\n2:20:44.040 --> 2:20:46.780\n And then in 2017, I think basically pretty much\n\n2:20:46.780 --> 2:20:48.420\n to PyTorch completely.\n\n2:20:48.420 --> 2:20:49.260\n Oh, interesting.\n\n2:20:49.260 --> 2:20:50.520\n So you went to Lua, cool.\n\n2:20:50.520 --> 2:20:51.480\n Yeah.\n\n2:20:51.480 --> 2:20:54.200\n Huh, so you were there before it was cool.\n\n2:20:54.200 --> 2:20:56.320\n Yeah, I mean, so LuaTorch was really good\n\n2:20:56.320 --> 2:20:59.000\n because it actually allowed you\n\n2:20:59.000 --> 2:21:01.340\n to do a lot of different kinds of things.\n\n2:21:01.340 --> 2:21:03.880\n So which Cafe was very rigid in terms of its structure.\n\n2:21:03.880 --> 2:21:06.800\n Like you would create a neural network once and that's it.\n\n2:21:06.800 --> 2:21:09.320\n Whereas if you wanted like very dynamic graphs and so on,\n\n2:21:09.320 --> 2:21:10.200\n it was very hard to do that.\n\n2:21:10.200 --> 2:21:11.600\n And LuaTorch was much more friendly\n\n2:21:11.600 --> 2:21:13.560\n for all of these things.\n\n2:21:13.560 --> 2:21:15.600\n Okay, so in terms of PyTorch and TensorFlow,\n\n2:21:15.600 --> 2:21:17.280\n my personal bias is PyTorch\n\n2:21:17.280 --> 2:21:19.080\n just because I've been using it longer\n\n2:21:19.080 --> 2:21:20.780\n and I'm more familiar with it.\n\n2:21:20.780 --> 2:21:23.560\n And also that PyTorch is much easier to debug\n\n2:21:23.560 --> 2:21:26.300\n is what I find because it's imperative in nature\n\n2:21:26.300 --> 2:21:28.620\n compared to like TensorFlow, which is not imperative.\n\n2:21:28.620 --> 2:21:30.480\n But that's telling you a lot that basically\n\n2:21:30.480 --> 2:21:33.320\n the imperative design is sort of a way\n\n2:21:33.320 --> 2:21:35.240\n in which a lot of people are taught programming\n\n2:21:35.240 --> 2:21:38.160\n and that's what actually makes debugging easier for them.\n\n2:21:38.160 --> 2:21:40.480\n So like I learned programming in C, C++.\n\n2:21:40.480 --> 2:21:44.040\n And so for me, imperative way of programming is more natural.\n\n2:21:44.040 --> 2:21:45.280\n Do you think it's good to have\n\n2:21:45.280 --> 2:21:48.480\n kind of these two communities, this kind of competition?\n\n2:21:48.480 --> 2:21:50.680\n I think PyTorch is kind of more and more\n\n2:21:50.680 --> 2:21:52.520\n becoming dominant in the research community,\n\n2:21:52.520 --> 2:21:54.600\n but TensorFlow is still very popular\n\n2:21:54.600 --> 2:21:57.920\n in the more sort of application machine learning community.\n\n2:21:57.920 --> 2:21:59.640\n So do you think it's good to have\n\n2:21:59.640 --> 2:22:02.080\n that kind of split in code bases?\n\n2:22:02.080 --> 2:22:06.560\n Or so like the benefit there is the competition challenges\n\n2:22:06.560 --> 2:22:09.980\n the library developers to step up to a game.\n\n2:22:09.980 --> 2:22:12.720\n But the downside is there's these code bases\n\n2:22:12.720 --> 2:22:15.180\n that are in different libraries.\n\n2:22:15.180 --> 2:22:17.080\n Right, so I think the downside is that,\n\n2:22:17.080 --> 2:22:18.480\n I mean, for a lot of research code\n\n2:22:18.480 --> 2:22:19.640\n that's released in one framework\n\n2:22:19.640 --> 2:22:20.600\n and if you're using the other one,\n\n2:22:20.600 --> 2:22:23.800\n it's really hard to like really build on top of it.\n\n2:22:23.800 --> 2:22:25.800\n But thankfully the open source community\n\n2:22:25.800 --> 2:22:27.080\n in machine learning is amazing.\n\n2:22:27.080 --> 2:22:30.840\n So whenever like something pops up in TensorFlow,\n\n2:22:30.840 --> 2:22:33.200\n you wait a few days and someone who's like super sharp\n\n2:22:33.200 --> 2:22:35.340\n will actually come and translate that particular code\n\n2:22:35.340 --> 2:22:38.380\n based into PyTorch and basically have figured that\n\n2:22:38.380 --> 2:22:39.700\n all the nooks and crannies out.\n\n2:22:39.700 --> 2:22:41.800\n So the open source community is amazing\n\n2:22:41.800 --> 2:22:44.280\n and they really like figure out this gap.\n\n2:22:44.280 --> 2:22:47.560\n So I think in terms of like having these two frameworks\n\n2:22:47.560 --> 2:22:49.720\n or multiple, I think of course there are different use cases\n\n2:22:49.720 --> 2:22:51.080\n so there are going to be benefits\n\n2:22:51.080 --> 2:22:52.840\n to using one or the other framework.\n\n2:22:52.840 --> 2:22:54.720\n And like you said, I think competition is just healthy\n\n2:22:54.720 --> 2:22:57.360\n because both of these frameworks keep\n\n2:22:57.360 --> 2:22:59.060\n or like all of these frameworks really sort of\n\n2:22:59.060 --> 2:23:00.120\n keep learning from each other\n\n2:23:00.120 --> 2:23:01.640\n and keep incorporating different things\n\n2:23:01.640 --> 2:23:03.760\n to just make them better and better.\n\n2:23:03.760 --> 2:23:06.320\n What advice would you have for someone\n\n2:23:06.320 --> 2:23:09.680\n new to machine learning, you know,\n\n2:23:09.680 --> 2:23:11.520\n maybe just started or haven't even started\n\n2:23:11.520 --> 2:23:14.880\n but are curious about it and who want to get in the field?\n\n2:23:14.880 --> 2:23:16.620\n Don't be afraid to get your hands dirty.\n\n2:23:16.620 --> 2:23:17.640\n I think that's the main thing.\n\n2:23:17.640 --> 2:23:19.120\n So if something doesn't work,\n\n2:23:19.120 --> 2:23:22.200\n like really drill into why things are not working.\n\n2:23:22.200 --> 2:23:24.520\n Can you elaborate what your hands dirty means?\n\n2:23:24.520 --> 2:23:27.540\n Right, so for example, like if an algorithm,\n\n2:23:27.540 --> 2:23:29.720\n if you try to train the network and it's not converging,\n\n2:23:29.720 --> 2:23:32.240\n whatever, rather than trying to like Google the answer\n\n2:23:32.240 --> 2:23:33.400\n or trying to do something,\n\n2:23:33.400 --> 2:23:36.320\n like really spend those like five, eight, 10, 15, 20,\n\n2:23:36.320 --> 2:23:37.560\n whatever number of hours really trying\n\n2:23:37.560 --> 2:23:39.000\n to figure it out yourself.\n\n2:23:39.000 --> 2:23:41.320\n Because in that process, you'll actually learn a lot more.\n\n2:23:41.320 --> 2:23:42.520\n Yeah.\n\n2:23:42.520 --> 2:23:44.600\n Googling is of course like a good way to solve it\n\n2:23:44.600 --> 2:23:45.960\n when you need a quick answer.\n\n2:23:45.960 --> 2:23:48.120\n But I think initially, especially like when you're starting\n\n2:23:48.120 --> 2:23:51.840\n out, it's much nicer to like figure things out by yourself.\n\n2:23:51.840 --> 2:23:52.960\n And I just say that from experience\n\n2:23:52.960 --> 2:23:54.280\n because like when I started out,\n\n2:23:54.280 --> 2:23:55.480\n there were not a lot of resources.\n\n2:23:55.480 --> 2:23:57.880\n So we would like in the lab, a lot of us,\n\n2:23:57.880 --> 2:23:59.680\n like we would look up to senior students\n\n2:23:59.680 --> 2:24:01.360\n and then the senior students were of course busy\n\n2:24:01.360 --> 2:24:03.080\n and they would be like, hey, why don't you go figure it out?\n\n2:24:03.080 --> 2:24:04.320\n Because I just don't have the time.\n\n2:24:04.320 --> 2:24:06.480\n I'm working on my dissertation or whatever.\n\n2:24:06.480 --> 2:24:07.640\n I'll find a PhD students.\n\n2:24:07.640 --> 2:24:08.760\n And so then we would sit down\n\n2:24:08.760 --> 2:24:10.480\n and like just try to figure it out.\n\n2:24:10.480 --> 2:24:12.440\n And that I think really helped me.\n\n2:24:12.440 --> 2:24:15.040\n That has really helped me figure a lot of things out.\n\n2:24:15.040 --> 2:24:18.720\n I think in general, if I were to generalize that,\n\n2:24:18.720 --> 2:24:22.720\n I feel like persevering through any kind of struggle\n\n2:24:22.720 --> 2:24:25.640\n on a thing you care about is good.\n\n2:24:25.640 --> 2:24:27.960\n So you're basically, you try to make it seem\n\n2:24:27.960 --> 2:24:30.840\n like it's good to spend time debugging,\n\n2:24:30.840 --> 2:24:33.680\n but really any kind of struggle, whatever form that takes,\n\n2:24:33.680 --> 2:24:36.080\n it could be just Googling a lot.\n\n2:24:36.080 --> 2:24:38.720\n Just basically anything, just sticking with it\n\n2:24:38.720 --> 2:24:41.000\n and going through the hard thing that could take a form\n\n2:24:41.000 --> 2:24:43.200\n of implementing stuff from scratch.\n\n2:24:43.200 --> 2:24:45.600\n It could take the form of re implementing\n\n2:24:45.600 --> 2:24:46.520\n with different libraries\n\n2:24:46.520 --> 2:24:48.320\n or different programming languages.\n\n2:24:49.320 --> 2:24:50.560\n It could take a lot of different forms,\n\n2:24:50.560 --> 2:24:53.520\n but struggle is good for the soul.\n\n2:24:53.520 --> 2:24:55.800\n So like in Pittsburgh, where I did my PhD,\n\n2:24:55.800 --> 2:24:58.360\n the thing was it used to snow a lot.\n\n2:24:58.360 --> 2:25:00.800\n And so when it was snowed, you really couldn't do much.\n\n2:25:00.800 --> 2:25:02.880\n So the thing that a lot of people said\n\n2:25:02.880 --> 2:25:05.320\n was snow builds character.\n\n2:25:05.320 --> 2:25:07.480\n Because when it's snowing, you can't do anything else.\n\n2:25:07.480 --> 2:25:09.040\n You focus on work.\n\n2:25:09.040 --> 2:25:10.800\n Do you have advice in general for people,\n\n2:25:10.800 --> 2:25:13.400\n you've already exceptionally successful, you're young,\n\n2:25:13.400 --> 2:25:15.760\n but do you have advice for young people starting out\n\n2:25:15.760 --> 2:25:18.160\n in college or maybe in high school?\n\n2:25:18.160 --> 2:25:21.040\n Advice for their career, advice for their life,\n\n2:25:21.040 --> 2:25:25.760\n how to pave a successful path in career and life?\n\n2:25:25.760 --> 2:25:27.640\n I would say just be hungry.\n\n2:25:27.640 --> 2:25:29.680\n Always be hungry for what you want.\n\n2:25:29.680 --> 2:25:33.280\n And I think I've been inspired by a lot of people\n\n2:25:33.280 --> 2:25:36.720\n who are just driven and who really go for what they want,\n\n2:25:36.720 --> 2:25:39.440\n no matter what, like you shouldn't want it,\n\n2:25:39.440 --> 2:25:40.480\n you should need it.\n\n2:25:40.480 --> 2:25:41.480\n So if you need something,\n\n2:25:41.480 --> 2:25:44.360\n you basically go towards the ends to make it work.\n\n2:25:44.360 --> 2:25:47.840\n How do you know when you come across a thing\n\n2:25:47.840 --> 2:25:50.280\n that's like you need?\n\n2:25:51.120 --> 2:25:53.080\n I think there's not going to be any single thing\n\n2:25:53.080 --> 2:25:53.920\n that you're going to need.\n\n2:25:53.920 --> 2:25:54.920\n There are going to be different types of things\n\n2:25:54.920 --> 2:25:56.600\n that you need, but whenever you need something,\n\n2:25:56.600 --> 2:25:57.920\n you just go push for it.\n\n2:25:57.920 --> 2:26:00.040\n And of course, once you, you may not get it,\n\n2:26:00.040 --> 2:26:01.960\n or you may find that this was not even the thing\n\n2:26:01.960 --> 2:26:03.640\n that you were looking for, it might be a different thing.\n\n2:26:03.640 --> 2:26:06.240\n But the point is like you're pushing through things\n\n2:26:06.240 --> 2:26:08.960\n and that actually brings a lot of skills\n\n2:26:08.960 --> 2:26:12.880\n and builds a certain kind of attitude\n\n2:26:12.880 --> 2:26:15.680\n which will probably help you get the other thing\n\n2:26:15.680 --> 2:26:18.080\n once you figure out what's really the thing that you want.\n\n2:26:18.080 --> 2:26:20.480\n Yeah, I think a lot of people are,\n\n2:26:20.480 --> 2:26:22.520\n I've noticed, kind of afraid of that\n\n2:26:22.520 --> 2:26:24.880\n is because one, it's a fear of commitment.\n\n2:26:24.880 --> 2:26:26.880\n And two, there's so many amazing things in this world,\n\n2:26:26.880 --> 2:26:28.120\n you almost don't want to miss out\n\n2:26:28.120 --> 2:26:29.440\n on all the other amazing things\n\n2:26:29.440 --> 2:26:31.080\n by committing to this one thing.\n\n2:26:31.080 --> 2:26:32.720\n So I think a lot of it has to do with just\n\n2:26:32.720 --> 2:26:37.720\n allowing yourself to notice that thing\n\n2:26:37.920 --> 2:26:41.560\n and just go all the way with it.\n\n2:26:41.560 --> 2:26:43.240\n I mean, I also like failure, right?\n\n2:26:43.240 --> 2:26:47.280\n So I know this is like super cheesy that failure\n\n2:26:47.280 --> 2:26:49.760\n is something that you should be prepared for and so on,\n\n2:26:49.760 --> 2:26:52.520\n but I do think, I mean, especially in research,\n\n2:26:52.520 --> 2:26:54.400\n for example, failure is something that happens\n\n2:26:54.400 --> 2:26:58.160\n almost every day is like experiments failing\n\n2:26:58.160 --> 2:26:59.080\n and not working.\n\n2:26:59.080 --> 2:27:02.240\n And so you really need to be so used to it.\n\n2:27:02.240 --> 2:27:03.880\n You need to have a thick skin,\n\n2:27:03.880 --> 2:27:06.280\n but, and only basically through,\n\n2:27:06.280 --> 2:27:07.880\n like when you get through it is when you find\n\n2:27:07.880 --> 2:27:09.560\n the one thing that's actually working.\n\n2:27:09.560 --> 2:27:11.840\n So Thomas Edison was like one person like that, right?\n\n2:27:11.840 --> 2:27:13.680\n So I really, like when I was a kid,\n\n2:27:13.680 --> 2:27:17.040\n I used to really read about how he found like the filament,\n\n2:27:17.040 --> 2:27:18.680\n the light bulb filament.\n\n2:27:18.680 --> 2:27:20.560\n And then he, I think his thing was like,\n\n2:27:20.560 --> 2:27:23.120\n he tried 990 things that didn't work\n\n2:27:23.120 --> 2:27:24.320\n or something of the sort.\n\n2:27:24.320 --> 2:27:26.920\n And then they asked him like, so what did you learn?\n\n2:27:26.920 --> 2:27:28.480\n Because all of these were failed experiments.\n\n2:27:28.480 --> 2:27:31.600\n And then he says, oh, these 990 things don't work.\n\n2:27:31.600 --> 2:27:32.440\n And I know that.\n\n2:27:32.440 --> 2:27:33.280\n Did you know that?\n\n2:27:33.280 --> 2:27:35.960\n I mean, that's really inspiring.\n\n2:27:35.960 --> 2:27:38.480\n So you spent a few years on this earth\n\n2:27:38.480 --> 2:27:43.480\n performing a self supervised kind of learning process.\n\n2:27:43.960 --> 2:27:46.400\n Have you figured out the meaning of life yet?\n\n2:27:46.400 --> 2:27:47.720\n I told you I'm doing this podcast\n\n2:27:47.720 --> 2:27:49.120\n to try to get the answer.\n\n2:27:49.120 --> 2:27:50.720\n I'm hoping you could tell me,\n\n2:27:50.720 --> 2:27:52.920\n what do you think the meaning of it all is?\n\n2:27:54.320 --> 2:27:55.800\n I don't think I figured this out.\n\n2:27:55.800 --> 2:27:57.120\n No, I have no idea.\n\n2:27:57.120 --> 2:28:02.120\n Do you think AI will help us figure it out\n\n2:28:02.560 --> 2:28:03.880\n or do you think there's no answer?\n\n2:28:03.880 --> 2:28:05.480\n The whole point is to keep searching.\n\n2:28:05.480 --> 2:28:08.800\n I think, yeah, I think it's an endless sort of quest for us.\n\n2:28:08.800 --> 2:28:10.560\n I don't think AI will help us there.\n\n2:28:10.560 --> 2:28:13.600\n This is like a very hard, hard, hard question\n\n2:28:13.600 --> 2:28:15.440\n which so many humans have tried to answer.\n\n2:28:15.440 --> 2:28:16.400\n Well, that's the interesting thing\n\n2:28:16.400 --> 2:28:18.600\n about the difference between AI and humans.\n\n2:28:19.560 --> 2:28:21.880\n Humans don't seem to know what the hell they're doing.\n\n2:28:21.880 --> 2:28:23.720\n And AI is almost always operating\n\n2:28:23.720 --> 2:28:27.440\n under well defined objective functions.\n\n2:28:28.360 --> 2:28:33.360\n And I wonder whether our lack of ability\n\n2:28:33.680 --> 2:28:37.240\n to define good longterm objective functions\n\n2:28:37.240 --> 2:28:40.400\n or introspect what is the objective function\n\n2:28:40.400 --> 2:28:44.400\n under which we operate, if that's a feature or a bug.\n\n2:28:44.400 --> 2:28:45.240\n I would say it's a feature\n\n2:28:45.240 --> 2:28:47.440\n because then everyone actually has very different kinds\n\n2:28:47.440 --> 2:28:49.360\n of objective functions that they're optimizing\n\n2:28:49.360 --> 2:28:51.320\n and those objective functions evolve\n\n2:28:51.320 --> 2:28:53.400\n and change dramatically through the course\n\n2:28:53.400 --> 2:28:54.240\n of their life.\n\n2:28:54.240 --> 2:28:56.000\n That's actually what makes us interesting, right?\n\n2:28:56.000 --> 2:28:58.040\n If otherwise, like if everyone was doing\n\n2:28:58.040 --> 2:29:00.560\n the exact same thing, that would be pretty boring.\n\n2:29:00.560 --> 2:29:02.600\n We do want like people with different kinds\n\n2:29:02.600 --> 2:29:06.160\n of perspectives, also people evolve continuously.\n\n2:29:06.160 --> 2:29:09.320\n That's like, I would say the biggest feature of being human.\n\n2:29:09.320 --> 2:29:11.160\n And then we get to like the ones that die\n\n2:29:11.160 --> 2:29:12.560\n because they do something stupid.\n\n2:29:12.560 --> 2:29:15.440\n We get to watch that, see it and learn from it.\n\n2:29:15.440 --> 2:29:20.360\n And as a species, we take that lesson\n\n2:29:20.360 --> 2:29:22.600\n and become better and better\n\n2:29:22.600 --> 2:29:24.280\n because of all the dumb people in the world\n\n2:29:24.280 --> 2:29:28.160\n that died doing something wild and beautiful.\n\n2:29:29.080 --> 2:29:31.840\n Ishan, thank you so much for this incredible conversation.\n\n2:29:31.840 --> 2:29:36.840\n We did a depth first search through the space\n\n2:29:37.080 --> 2:29:41.640\n of machine learning and it was fun and fascinating.\n\n2:29:41.640 --> 2:29:43.920\n So it's really an honor to meet you\n\n2:29:43.920 --> 2:29:45.760\n and it was a really awesome conversation.\n\n2:29:45.760 --> 2:29:48.200\n Thanks for coming down today and talking with me.\n\n2:29:48.200 --> 2:29:50.240\n Thanks Lex, I mean, I've listened to you.\n\n2:29:50.240 --> 2:29:52.400\n I told you it was unreal for me to actually meet you\n\n2:29:52.400 --> 2:29:55.000\n in person and I'm so happy to be here, thank you.\n\n2:29:55.000 --> 2:29:55.840\n Thanks man.\n\n2:29:56.680 --> 2:29:58.200\n Thanks for listening to this conversation\n\n2:29:58.200 --> 2:30:01.280\n with Ishan Misra and thank you to Onnit,\n\n2:30:01.280 --> 2:30:05.280\n The Information, Grammarly and Athletic Greens.\n\n2:30:05.280 --> 2:30:08.560\n Check them out in the description to support this podcast.\n\n2:30:08.560 --> 2:30:10.440\n And now let me leave you with some words\n\n2:30:10.440 --> 2:30:12.480\n from Arthur C. Clarke.\n\n2:30:12.480 --> 2:30:14.920\n Any sufficiently advanced technology\n\n2:30:14.920 --> 2:30:18.120\n is indistinguishable from magic.\n\n2:30:18.120 --> 2:30:23.120\n Thank you for listening and hope to see you next time.\n\n"
}