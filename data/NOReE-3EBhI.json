{
  "title": "Erik Brynjolfsson: Economics of AI, Social Networks, and Technology | Lex Fridman Podcast #141",
  "id": "NOReE-3EBhI",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:03.200\n The following is a conversation with Erik Brynjolfsson.\n\n00:03.200 --> 00:05.800\n He's an economics professor at Stanford\n\n00:05.800 --> 00:09.360\n and the director of Stanford's Digital Economy Lab.\n\n00:09.360 --> 00:13.440\n Previously, he was a long, long time professor at MIT\n\n00:13.440 --> 00:15.160\n where he did groundbreaking work\n\n00:15.160 --> 00:17.720\n on the economics of information.\n\n00:17.720 --> 00:19.760\n He's the author of many books,\n\n00:19.760 --> 00:21.960\n including The Second Machine Age\n\n00:21.960 --> 00:24.520\n and Machine Platform Crowd,\n\n00:24.520 --> 00:27.520\n coauthored with Andrew McAfee.\n\n00:27.520 --> 00:29.080\n Quick mention of each sponsor,\n\n00:29.080 --> 00:31.560\n followed by some thoughts related to the episode.\n\n00:31.560 --> 00:34.040\n Ventura Watches, the maker of classy,\n\n00:34.040 --> 00:35.960\n well performing watches.\n\n00:35.960 --> 00:39.720\n Four Sigmatic, the maker of delicious mushroom coffee.\n\n00:39.720 --> 00:42.760\n ExpressVPN, the VPN I've used for many years\n\n00:42.760 --> 00:44.920\n to protect my privacy on the internet.\n\n00:44.920 --> 00:48.920\n And CashApp, the app I use to send money to friends.\n\n00:48.920 --> 00:50.840\n Please check out these sponsors in the description\n\n00:50.840 --> 00:54.480\n to get a discount and to support this podcast.\n\n00:54.480 --> 00:56.800\n As a side note, let me say that the impact\n\n00:56.800 --> 00:59.120\n of artificial intelligence and automation\n\n00:59.120 --> 01:01.640\n on our economy and our world\n\n01:01.640 --> 01:04.360\n is something worth thinking deeply about.\n\n01:04.360 --> 01:06.280\n Like with many topics that are linked\n\n01:06.280 --> 01:09.040\n to predicting the future evolution of technology,\n\n01:09.040 --> 01:12.560\n it is often too easy to fall into one of two camps.\n\n01:12.560 --> 01:14.680\n The fear mongering camp\n\n01:14.680 --> 01:18.160\n or the technological utopianism camp.\n\n01:18.160 --> 01:21.480\n As always, the future will land us somewhere in between.\n\n01:21.480 --> 01:24.240\n I prefer to wear two hats in these discussions\n\n01:24.240 --> 01:26.400\n and alternate between them often.\n\n01:26.400 --> 01:29.360\n The hat of a pragmatic engineer\n\n01:29.360 --> 01:31.760\n and the hat of a futurist.\n\n01:31.760 --> 01:34.920\n This is probably a good time to mention Andrew Yang,\n\n01:34.920 --> 01:37.920\n the presidential candidate who has been\n\n01:37.920 --> 01:41.040\n one of the high profile thinkers on this topic.\n\n01:41.040 --> 01:42.680\n And I'm sure I will speak with him\n\n01:42.680 --> 01:44.600\n on this podcast eventually.\n\n01:44.600 --> 01:48.520\n A conversation with Andrew has been on the table many times.\n\n01:48.520 --> 01:50.440\n Our schedules just haven't aligned,\n\n01:50.440 --> 01:54.280\n especially because I have a strongly held to preference\n\n01:54.280 --> 01:58.040\n for long form, two, three, four hours or more,\n\n01:58.040 --> 02:00.000\n and in person.\n\n02:00.000 --> 02:02.880\n I work hard to not compromise on this.\n\n02:02.880 --> 02:04.800\n Trust me, it's not easy.\n\n02:04.800 --> 02:07.080\n Even more so in the times of COVID,\n\n02:07.080 --> 02:09.640\n which requires getting tested nonstop,\n\n02:09.640 --> 02:12.440\n staying isolated and doing a lot of costly\n\n02:12.440 --> 02:15.760\n and uncomfortable things that minimize risk for the guest.\n\n02:15.760 --> 02:17.760\n The reason I do this is because to me,\n\n02:17.760 --> 02:20.720\n something is lost in remote conversation.\n\n02:20.720 --> 02:23.360\n That something, that magic,\n\n02:23.360 --> 02:25.120\n I think is worth the effort,\n\n02:25.120 --> 02:29.360\n even if it ultimately leads to a failed conversation.\n\n02:29.360 --> 02:31.280\n This is how I approach life,\n\n02:31.280 --> 02:35.840\n treasuring the possibility of a rare moment of magic.\n\n02:35.840 --> 02:38.240\n I'm willing to go to the ends of the world\n\n02:38.240 --> 02:39.640\n for just such a moment.\n\n02:40.680 --> 02:43.080\n If you enjoy this thing, subscribe on YouTube,\n\n02:43.080 --> 02:45.320\n review it with five stars on Apple Podcast,\n\n02:45.320 --> 02:47.960\n follow on Spotify, support on Patreon,\n\n02:47.960 --> 02:51.080\n connect with me on Twitter at Lex Friedman.\n\n02:51.080 --> 02:55.120\n And now here's my conversation with Erik Brynjolfsson.\n\n02:56.080 --> 02:59.800\n You posted a quote on Twitter by Albert Bartlett\n\n02:59.800 --> 03:03.240\n saying that the greatest shortcoming of the human race\n\n03:03.240 --> 03:06.320\n is our inability to understand the exponential function.\n\n03:07.760 --> 03:09.680\n Why would you say the exponential growth\n\n03:09.680 --> 03:11.080\n is important to understand?\n\n03:12.120 --> 03:15.040\n Yeah, that quote, I remember posting that.\n\n03:15.040 --> 03:17.720\n It's actually a reprise of something Andy McAfee and I said\n\n03:17.720 --> 03:19.240\n in the second machine age,\n\n03:19.240 --> 03:21.240\n but I posted it in early March\n\n03:21.240 --> 03:23.680\n when COVID was really just beginning to take off\n\n03:23.680 --> 03:25.600\n and I was really scared.\n\n03:25.600 --> 03:28.080\n There were actually only a couple dozen cases,\n\n03:28.080 --> 03:29.800\n maybe less at that time,\n\n03:29.800 --> 03:32.040\n but they were doubling every like two or three days\n\n03:32.040 --> 03:35.280\n and I could see, oh my God, this is gonna be a catastrophe\n\n03:35.280 --> 03:36.840\n and it's gonna happen soon,\n\n03:36.840 --> 03:38.760\n but nobody was taking it very seriously\n\n03:38.760 --> 03:40.760\n or not a lot of people were taking it very seriously.\n\n03:40.760 --> 03:45.000\n In fact, I remember I did my last in person conference\n\n03:45.000 --> 03:47.680\n that week, I was flying back from Las Vegas\n\n03:47.680 --> 03:50.640\n and I was the only person on the plane wearing a mask\n\n03:50.640 --> 03:52.160\n and the flight attendant came over to me.\n\n03:52.160 --> 03:53.080\n She looked very concerned.\n\n03:53.080 --> 03:54.240\n She kind of put her hands on my shoulder.\n\n03:54.240 --> 03:56.440\n She was touching me all over, which I wasn't thrilled about\n\n03:56.440 --> 03:59.280\n and she goes, do you have some kind of anxiety disorder?\n\n03:59.280 --> 04:00.360\n Are you okay?\n\n04:00.360 --> 04:02.760\n And I was like, no, it's because of COVID.\n\n04:02.760 --> 04:03.920\n This is early March.\n\n04:03.920 --> 04:06.720\n Early March, but I was worried\n\n04:06.720 --> 04:10.680\n because I knew I could see or I suspected, I guess,\n\n04:10.680 --> 04:13.200\n that that doubling would continue and it did\n\n04:13.200 --> 04:17.000\n and pretty soon we had thousands of times more cases.\n\n04:17.000 --> 04:18.480\n Most of the time when I use that quote,\n\n04:18.480 --> 04:21.120\n I try to, it's motivated by more optimistic things\n\n04:21.120 --> 04:23.040\n like Moore's law and the wonders\n\n04:23.040 --> 04:25.400\n of having more computer power,\n\n04:25.400 --> 04:28.480\n but in either case, it can be very counterintuitive.\n\n04:28.480 --> 04:31.440\n I mean, if you walk for 10 minutes,\n\n04:31.440 --> 04:32.920\n you get about 10 times as far away\n\n04:32.920 --> 04:34.720\n as if you walk for one minute.\n\n04:34.720 --> 04:35.920\n That's the way our physical world works.\n\n04:35.920 --> 04:38.040\n That's the way our brains are wired,\n\n04:38.040 --> 04:41.480\n but if something doubles for 10 times as long,\n\n04:41.480 --> 04:43.080\n you don't get 10 times as much.\n\n04:43.080 --> 04:45.240\n You get a thousand times as much\n\n04:45.240 --> 04:47.080\n and after 20, it's a billion.\n\n04:47.080 --> 04:50.720\n After 30, it's a, no, sorry, after 20, it's a million.\n\n04:50.720 --> 04:53.400\n After 30, it's a billion.\n\n04:53.400 --> 04:54.480\n And pretty soon after that,\n\n04:54.480 --> 04:57.600\n it just gets to these numbers that you can barely grasp.\n\n04:57.600 --> 05:00.640\n Our world is becoming more and more exponential,\n\n05:00.640 --> 05:03.440\n mainly because of digital technologies.\n\n05:03.440 --> 05:06.200\n So more and more often our intuitions are out of whack\n\n05:06.200 --> 05:10.760\n and that can be good in the case of things creating wonders,\n\n05:10.760 --> 05:13.520\n but it can be dangerous in the case of viruses\n\n05:13.520 --> 05:14.440\n and other things.\n\n05:14.440 --> 05:16.280\n Do you think it generally applies,\n\n05:16.280 --> 05:18.120\n like is there spaces where it does apply\n\n05:18.120 --> 05:19.320\n and where it doesn't?\n\n05:19.320 --> 05:21.600\n How are we supposed to build an intuition\n\n05:21.600 --> 05:25.160\n about in which aspects of our society\n\n05:25.160 --> 05:27.280\n does exponential growth apply?\n\n05:27.280 --> 05:29.480\n Well, you can learn the math,\n\n05:29.480 --> 05:32.000\n but the truth is our brains, I think,\n\n05:32.000 --> 05:35.280\n tend to learn more from experiences.\n\n05:35.280 --> 05:37.440\n So we just start seeing it more and more often.\n\n05:37.440 --> 05:39.320\n So hanging around Silicon Valley,\n\n05:39.320 --> 05:41.560\n hanging around AI and computer researchers,\n\n05:41.560 --> 05:44.800\n I see this kind of exponential growth a lot more frequently\n\n05:44.800 --> 05:46.760\n and I'm getting used to it, but I still make mistakes.\n\n05:46.760 --> 05:48.680\n I still underestimate some of the progress\n\n05:48.680 --> 05:50.920\n in just talking to someone about GPT3\n\n05:50.920 --> 05:54.040\n and how rapidly natural language has improved.\n\n05:54.040 --> 05:58.280\n But I think that as the world becomes more exponential,\n\n05:58.280 --> 06:01.760\n we'll all start experiencing it more frequently.\n\n06:01.760 --> 06:05.360\n The danger is that we may make some mistakes in the meantime\n\n06:05.360 --> 06:07.680\n using our old kind of caveman intuitions\n\n06:07.680 --> 06:09.280\n about how the world works.\n\n06:09.280 --> 06:11.760\n Well, the weird thing is it always kind of looks linear\n\n06:11.760 --> 06:12.640\n in the moment.\n\n06:12.640 --> 06:16.920\n Like it's hard to feel,\n\n06:16.920 --> 06:19.920\n it's hard to like introspect\n\n06:19.920 --> 06:22.960\n and really acknowledge how much has changed\n\n06:22.960 --> 06:26.320\n in just a couple of years or five years or 10 years\n\n06:26.320 --> 06:27.200\n with the internet.\n\n06:27.200 --> 06:29.600\n If we just look at advancements of AI\n\n06:29.600 --> 06:31.680\n or even just social media,\n\n06:31.680 --> 06:33.680\n all the various technologies\n\n06:33.680 --> 06:36.240\n that go into the digital umbrella,\n\n06:36.240 --> 06:39.440\n it feels pretty calm and normal and gradual.\n\n06:39.440 --> 06:40.760\n Well, a lot of stuff,\n\n06:40.760 --> 06:42.000\n I think there are parts of the world,\n\n06:42.000 --> 06:44.240\n most of the world that is not exponential.\n\n06:45.600 --> 06:47.000\n The way humans learn,\n\n06:47.000 --> 06:49.200\n the way organizations change,\n\n06:49.200 --> 06:52.000\n the way our whole institutions adapt and evolve,\n\n06:52.000 --> 06:54.520\n those don't improve at exponential paces.\n\n06:54.520 --> 06:56.320\n And that leads to a mismatch oftentimes\n\n06:56.320 --> 06:58.920\n between these exponentially improving technologies\n\n06:58.920 --> 07:00.440\n or let's say changing technologies\n\n07:00.440 --> 07:03.040\n because some of them are exponentially more dangerous\n\n07:03.040 --> 07:06.720\n and our intuitions and our human skills\n\n07:06.720 --> 07:11.720\n and our institutions that just don't change very fast at all.\n\n07:11.720 --> 07:13.760\n And that mismatch I think is at the root\n\n07:13.760 --> 07:15.520\n of a lot of the problems in our society,\n\n07:15.520 --> 07:18.160\n the growing inequality\n\n07:18.160 --> 07:22.560\n and other dysfunctions in our political\n\n07:22.560 --> 07:24.240\n and economic systems.\n\n07:24.240 --> 07:28.240\n So one guy that talks about exponential functions\n\n07:28.240 --> 07:29.440\n a lot is Elon Musk.\n\n07:29.440 --> 07:32.040\n He seems to internalize this kind of way\n\n07:32.040 --> 07:34.680\n of exponential thinking.\n\n07:34.680 --> 07:36.320\n He calls it first principles thinking,\n\n07:36.320 --> 07:39.480\n sort of the kind of going to the basics,\n\n07:39.480 --> 07:41.560\n asking the question,\n\n07:41.560 --> 07:43.800\n like what were the assumptions of the past?\n\n07:43.800 --> 07:46.720\n How can we throw them out the window?\n\n07:46.720 --> 07:49.160\n How can we do this 10X much more efficiently\n\n07:49.160 --> 07:51.360\n and constantly practicing that process?\n\n07:51.360 --> 07:54.200\n And also using that kind of thinking\n\n07:54.200 --> 08:01.200\n to estimate sort of when, you know, create deadlines\n\n08:01.200 --> 08:04.040\n and estimate when you'll be able to deliver\n\n08:04.040 --> 08:06.520\n on some of these technologies.\n\n08:06.520 --> 08:09.560\n Now, it often gets him in trouble\n\n08:09.560 --> 08:12.520\n because he overestimates,\n\n08:12.520 --> 08:17.360\n like he doesn't meet the initial estimates of the deadlines,\n\n08:17.360 --> 08:22.360\n but he seems to deliver late but deliver.\n\n08:22.360 --> 08:25.080\n And which is kind of interesting.\n\n08:25.080 --> 08:26.920\n Like, what are your thoughts about this whole thing?\n\n08:26.920 --> 08:28.840\n I think we can all learn from Elon.\n\n08:28.840 --> 08:30.120\n I think going to first principles,\n\n08:30.120 --> 08:32.840\n I talked about two ways of getting more of a grip\n\n08:32.840 --> 08:34.560\n on the exponential function.\n\n08:34.560 --> 08:36.280\n And one of them just comes from first principles.\n\n08:36.280 --> 08:37.800\n You know, if you understand the math of it,\n\n08:37.800 --> 08:39.040\n you can see what's gonna happen.\n\n08:39.040 --> 08:41.040\n And even if it seems counterintuitive\n\n08:41.040 --> 08:42.960\n that a couple of dozen of COVID cases\n\n08:42.960 --> 08:46.200\n can become thousands or tens or hundreds of thousands\n\n08:46.200 --> 08:48.080\n of them in a month,\n\n08:48.080 --> 08:51.200\n it makes sense once you just do the math.\n\n08:51.200 --> 08:53.680\n And I think Elon tries to do that a lot.\n\n08:53.680 --> 08:55.120\n You know, in fairness, I think he also benefits\n\n08:55.120 --> 08:56.960\n from hanging out in Silicon Valley\n\n08:56.960 --> 09:00.600\n and he's experienced it in a lot of different applications.\n\n09:00.600 --> 09:04.080\n So, you know, it's not as much of a shock to him anymore,\n\n09:04.080 --> 09:06.280\n but that's something we can all learn from.\n\n09:07.160 --> 09:10.400\n In my own life, I remember one of my first experiences\n\n09:10.400 --> 09:12.960\n really seeing it was when I was a grad student\n\n09:12.960 --> 09:17.560\n and my advisor asked me to plot the growth of computer power\n\n09:17.560 --> 09:20.000\n in the US economy in different industries.\n\n09:20.000 --> 09:21.560\n And there are all these, you know,\n\n09:21.560 --> 09:23.000\n exponentially growing curves.\n\n09:23.000 --> 09:24.560\n And I was like, holy shit, look at this.\n\n09:24.560 --> 09:26.880\n In each industry, it was just taking off.\n\n09:26.880 --> 09:29.240\n And, you know, you didn't have to be a rocket scientist\n\n09:29.240 --> 09:30.640\n to extend that and say, wow,\n\n09:30.640 --> 09:33.600\n this means that this was in the late 80s and early 90s\n\n09:33.600 --> 09:35.880\n that, you know, if it goes anything like that,\n\n09:35.880 --> 09:38.160\n we're gonna have orders of magnitude more computer power\n\n09:38.160 --> 09:39.480\n than we did at that time.\n\n09:39.480 --> 09:41.320\n And of course we do.\n\n09:41.320 --> 09:43.680\n So, you know, when people look at Moore's law,\n\n09:45.040 --> 09:46.880\n they often talk about it as just,\n\n09:46.880 --> 09:49.240\n so the exponential function is actually\n\n09:49.240 --> 09:51.360\n a stack of S curves.\n\n09:51.360 --> 09:56.360\n So basically it's you milk or whatever,\n\n09:57.240 --> 10:01.240\n take the most advantage of a particular little revolution\n\n10:01.240 --> 10:03.000\n and then you search for another revolution.\n\n10:03.000 --> 10:06.720\n And it's basically revolutions stack on top of revolutions.\n\n10:06.720 --> 10:08.960\n Do you have any intuition about how the head humans\n\n10:08.960 --> 10:12.280\n keep finding ways to revolutionize things?\n\n10:12.280 --> 10:14.200\n Well, first, let me just unpack that first point\n\n10:14.200 --> 10:17.160\n that I talked about exponential curves,\n\n10:17.160 --> 10:20.280\n but no exponential curve continues forever.\n\n10:21.480 --> 10:24.960\n It's been said that if anything can't go on forever,\n\n10:24.960 --> 10:26.680\n eventually it will stop.\n\n10:26.680 --> 10:29.840\n And, and it's very profound, but it's,\n\n10:29.840 --> 10:32.440\n it seems that a lot of people don't appreciate\n\n10:32.440 --> 10:33.960\n that half of it as well either.\n\n10:33.960 --> 10:36.560\n And that's why all exponential functions eventually turn\n\n10:36.560 --> 10:39.800\n into some kind of S curve or stop in some other way,\n\n10:39.800 --> 10:41.240\n maybe catastrophically.\n\n10:41.240 --> 10:42.800\n And that's a cap with COVID as well.\n\n10:42.800 --> 10:44.560\n I mean, it was, it went up and then it sort of, you know,\n\n10:44.560 --> 10:47.640\n at some point it starts saturating the pool of people\n\n10:47.640 --> 10:49.040\n to be infected.\n\n10:49.040 --> 10:51.320\n There's a standard epidemiological model\n\n10:51.320 --> 10:52.960\n that's based on that.\n\n10:52.960 --> 10:55.040\n And it's beginning to happen with Moore's law\n\n10:55.040 --> 10:56.920\n or different generations of computer power.\n\n10:56.920 --> 10:59.280\n It happens with all exponential curves.\n\n10:59.280 --> 11:01.040\n The remarkable thing is you elude,\n\n11:01.040 --> 11:03.560\n the second part of your question is that we've been able\n\n11:03.560 --> 11:06.840\n to come up with a new S curve on top of the previous one\n\n11:06.840 --> 11:10.640\n and do that generation after generation with new materials,\n\n11:10.640 --> 11:14.520\n new processes, and just extend it further and further.\n\n11:15.680 --> 11:17.400\n I don't think anyone has a really good theory\n\n11:17.400 --> 11:21.400\n about why we've been so successful in doing that.\n\n11:21.400 --> 11:23.160\n It's great that we have been,\n\n11:23.160 --> 11:26.400\n and I hope it continues for some time,\n\n11:26.400 --> 11:31.400\n but it's, you know, one beginning of a theory\n\n11:31.480 --> 11:34.440\n is that there's huge incentives when other parts\n\n11:34.440 --> 11:36.880\n of the system are going on that clock speed\n\n11:36.880 --> 11:39.280\n of doubling every two to three years.\n\n11:39.280 --> 11:42.120\n If there's one component of it that's not keeping up,\n\n11:42.120 --> 11:44.800\n then the economic incentives become really large\n\n11:44.800 --> 11:46.200\n to improve that one part.\n\n11:46.200 --> 11:49.720\n It becomes a bottleneck and anyone who can do improvements\n\n11:49.720 --> 11:51.640\n in that part can reap huge returns\n\n11:51.640 --> 11:54.000\n so that the resources automatically get focused\n\n11:54.000 --> 11:56.560\n on whatever part of the system isn't keeping up.\n\n11:56.560 --> 11:59.680\n Do you think some version of the Moore's law will continue?\n\n11:59.680 --> 12:01.360\n Some version, yes, it is.\n\n12:01.360 --> 12:04.560\n I mean, one version that has become more important\n\n12:04.560 --> 12:06.280\n is something I call Coomey's law,\n\n12:06.280 --> 12:08.440\n which is named after John Coomey,\n\n12:08.440 --> 12:10.280\n who I should mention was also my college roommate,\n\n12:10.280 --> 12:14.360\n but he identified the fact that energy consumption\n\n12:14.360 --> 12:17.280\n has been declining by a factor of two.\n\n12:17.280 --> 12:18.960\n And for most of us, that's more important.\n\n12:18.960 --> 12:21.320\n The new iPhones came out today as we're recording this.\n\n12:21.320 --> 12:23.120\n I'm not sure when you're gonna make it available.\n\n12:23.120 --> 12:24.920\n Very soon after this, yeah.\n\n12:24.920 --> 12:29.920\n And for most of us, having the iPhone be twice as fast,\n\n12:30.000 --> 12:33.160\n it's nice, but having the battery lifelonger,\n\n12:33.160 --> 12:35.440\n that would be much more valuable.\n\n12:35.440 --> 12:38.200\n And the fact that a lot of the progress in chips now\n\n12:38.200 --> 12:42.800\n is reducing energy consumption is probably more important\n\n12:42.800 --> 12:46.040\n for many applications than just the raw speed.\n\n12:46.040 --> 12:47.480\n Other dimensions of Moore's law\n\n12:47.480 --> 12:50.120\n are in AI and machine learning.\n\n12:51.560 --> 12:55.160\n Those tend to be very parallelizable functions,\n\n12:55.160 --> 12:58.440\n especially deep neural nets.\n\n12:58.440 --> 13:01.280\n And so instead of having one chip,\n\n13:01.280 --> 13:05.000\n you can have multiple chips or you can have a GPU,\n\n13:05.000 --> 13:07.000\n graphic processing unit that goes faster.\n\n13:07.000 --> 13:09.600\n Now, special chips designed for machine learning\n\n13:09.600 --> 13:11.160\n like tensor processing units,\n\n13:11.160 --> 13:13.000\n each time you switch, there's another 10X\n\n13:13.000 --> 13:16.720\n or 100X improvement above and beyond Moore's law.\n\n13:16.720 --> 13:18.800\n So I think that the raw silicon\n\n13:18.800 --> 13:20.720\n isn't improving as much as it used to,\n\n13:20.720 --> 13:23.880\n but these other dimensions are becoming important,\n\n13:23.880 --> 13:26.240\n more important, and we're seeing progress in them.\n\n13:26.240 --> 13:28.200\n I don't know if you've seen the work by OpenAI\n\n13:28.200 --> 13:31.880\n where they show the exponential improvement\n\n13:31.880 --> 13:34.320\n of the training of neural networks\n\n13:34.320 --> 13:36.920\n just literally in the techniques used.\n\n13:36.920 --> 13:40.520\n So that's almost like the algorithm.\n\n13:40.520 --> 13:43.640\n It's fascinating to think like, can I actually continue?\n\n13:43.640 --> 13:45.160\n I was figuring out more and more tricks\n\n13:45.160 --> 13:47.000\n on how to train networks faster and faster.\n\n13:47.000 --> 13:49.520\n The progress has been staggering.\n\n13:49.520 --> 13:51.720\n If you look at image recognition, as you mentioned,\n\n13:51.720 --> 13:53.440\n I think it's a function of at least three things\n\n13:53.440 --> 13:54.520\n that are coming together.\n\n13:54.520 --> 13:56.560\n One, we just talked about faster chips,\n\n13:56.560 --> 14:00.400\n not just Moore's law, but GPUs, TPUs and other technologies.\n\n14:00.400 --> 14:02.760\n The second is just a lot more data.\n\n14:02.760 --> 14:05.600\n I mean, we are awash in digital data today\n\n14:05.600 --> 14:08.080\n in a way we weren't 20 years ago.\n\n14:08.080 --> 14:09.960\n Photography, I'm old enough to remember,\n\n14:09.960 --> 14:12.800\n it used to be chemical, and now everything is digital.\n\n14:12.800 --> 14:16.440\n I took probably 50 digital photos yesterday.\n\n14:16.440 --> 14:17.880\n I wouldn't have done that if it was chemical.\n\n14:17.880 --> 14:20.680\n And we have the internet of things\n\n14:20.680 --> 14:22.920\n and all sorts of other types of data.\n\n14:22.920 --> 14:24.120\n When we walk around with our phone,\n\n14:24.120 --> 14:27.240\n it's just broadcasting a huge amounts of digital data\n\n14:27.240 --> 14:29.240\n that can be used as training sets.\n\n14:29.240 --> 14:34.240\n And then last but not least, as they mentioned at OpenAI,\n\n14:34.240 --> 14:37.160\n there've been significant improvements in the techniques.\n\n14:37.160 --> 14:39.240\n The core idea of deep neural nets\n\n14:39.240 --> 14:41.160\n has been around for a few decades,\n\n14:41.160 --> 14:44.200\n but the advances in making it work more efficiently\n\n14:44.200 --> 14:48.160\n have also improved a couple of orders of magnitude or more.\n\n14:48.160 --> 14:49.640\n So you multiply together,\n\n14:49.640 --> 14:52.480\n a hundred fold improvement in computer power,\n\n14:52.480 --> 14:55.320\n a hundred fold or more improvement in data,\n\n14:55.320 --> 14:59.000\n a hundred fold improvement in techniques\n\n14:59.000 --> 15:00.560\n of software and algorithms,\n\n15:00.560 --> 15:03.840\n and soon you're getting into a million fold improvements.\n\n15:03.840 --> 15:07.280\n So somebody brought this up, this idea with GPT3 that,\n\n15:09.920 --> 15:11.960\n so it's trained in a self supervised way\n\n15:11.960 --> 15:13.960\n on basically internet data.\n\n15:15.080 --> 15:18.920\n And that's one of the, I've seen arguments made\n\n15:18.920 --> 15:21.120\n and they seem to be pretty convincing\n\n15:21.120 --> 15:23.440\n that the bottleneck there is going to be\n\n15:23.440 --> 15:25.640\n how much data there is on the internet,\n\n15:25.640 --> 15:29.120\n which is a fascinating idea that it literally\n\n15:29.120 --> 15:33.280\n will just run out of human generated data to train on.\n\n15:33.280 --> 15:35.720\n Right, I know we make it to the point where it's consumed\n\n15:35.720 --> 15:37.480\n basically all of human knowledge\n\n15:37.480 --> 15:39.120\n or all digitized human knowledge, yeah.\n\n15:39.120 --> 15:40.880\n And that will be the bottleneck.\n\n15:40.880 --> 15:44.520\n But the interesting thing with bottlenecks\n\n15:44.520 --> 15:47.360\n is people often use bottlenecks\n\n15:47.360 --> 15:49.920\n as a way to argue against exponential growth.\n\n15:49.920 --> 15:51.400\n They say, well, there's no way\n\n15:51.400 --> 15:53.840\n you can overcome this bottleneck,\n\n15:53.840 --> 15:56.960\n but we seem to somehow keep coming up in new ways\n\n15:56.960 --> 15:59.200\n to like overcome whatever bottlenecks\n\n15:59.200 --> 16:01.920\n the critics come up with, which is fascinating.\n\n16:01.920 --> 16:04.600\n I don't know how you overcome the data bottleneck,\n\n16:04.600 --> 16:07.000\n but probably more efficient training algorithms.\n\n16:07.000 --> 16:08.240\n Yeah, well, you already mentioned that,\n\n16:08.240 --> 16:10.240\n that these training algorithms are getting much better\n\n16:10.240 --> 16:12.440\n at using smaller amounts of data.\n\n16:12.440 --> 16:15.880\n We also are just capturing a lot more data than we used to,\n\n16:15.880 --> 16:18.880\n especially in China, but all around us.\n\n16:18.880 --> 16:20.680\n So those are both important.\n\n16:20.680 --> 16:24.160\n In some applications, you can simulate the data,\n\n16:24.160 --> 16:28.920\n video games, some of the self driving car systems\n\n16:28.920 --> 16:32.520\n are simulating driving, and of course,\n\n16:32.520 --> 16:34.240\n that has some risks and weaknesses,\n\n16:34.240 --> 16:38.080\n but you can also, if you want to exhaust\n\n16:38.080 --> 16:39.840\n all the different ways you could beat a video game,\n\n16:39.840 --> 16:42.280\n you could just simulate all the options.\n\n16:42.280 --> 16:44.920\n Can we take a step in that direction of autonomous vehicles?\n\n16:44.920 --> 16:47.720\n Next, you're talking to the CTO of Waymo tomorrow.\n\n16:48.640 --> 16:52.400\n And obviously, I'm talking to Elon again in a couple of weeks.\n\n16:53.920 --> 16:57.040\n What's your thoughts on autonomous vehicles?\n\n16:57.040 --> 17:01.800\n Like where do we stand as a problem\n\n17:01.800 --> 17:04.480\n that has the potential of revolutionizing the world?\n\n17:04.480 --> 17:06.760\n Well, I'm really excited about that,\n\n17:06.760 --> 17:09.000\n but it's become much clearer\n\n17:09.000 --> 17:10.680\n that the original way that I thought about it,\n\n17:10.680 --> 17:11.520\n most people thought about like,\n\n17:11.520 --> 17:13.440\n you know, will we have a self driving car or not\n\n17:13.440 --> 17:15.640\n is way too simple.\n\n17:15.640 --> 17:17.400\n The better way to think about it\n\n17:17.400 --> 17:19.360\n is that there's a whole continuum\n\n17:19.360 --> 17:22.320\n of how much driving and assisting the car can do.\n\n17:22.320 --> 17:24.760\n I noticed that you're right next door\n\n17:24.760 --> 17:25.960\n to the Toyota Research Institute.\n\n17:25.960 --> 17:27.600\n That is a total accident.\n\n17:27.600 --> 17:29.320\n I love the TRI folks, but yeah.\n\n17:29.320 --> 17:30.800\n Have you talked to Gil Pratt?\n\n17:30.800 --> 17:34.080\n Yeah, we're supposed to talk.\n\n17:34.080 --> 17:34.960\n It's kind of hilarious.\n\n17:34.960 --> 17:35.800\n So there's kind of the,\n\n17:35.800 --> 17:38.720\n I think it's a good counterpart to say what Elon is doing.\n\n17:38.720 --> 17:40.040\n And hopefully they can be frank\n\n17:40.040 --> 17:41.440\n in what they think about each other,\n\n17:41.440 --> 17:43.920\n because I've heard both of them talk about it.\n\n17:43.920 --> 17:45.400\n But they're much more, you know,\n\n17:45.400 --> 17:47.440\n this is an assistive, a guardian angel\n\n17:47.440 --> 17:50.440\n that watches over you as opposed to try to do everything.\n\n17:50.440 --> 17:53.320\n I think there's some things like driving on a highway,\n\n17:53.320 --> 17:55.320\n you know, from LA to Phoenix,\n\n17:55.320 --> 17:58.120\n where it's mostly good weather, straight roads.\n\n17:58.120 --> 18:01.240\n That's close to a solved problem, let's face it.\n\n18:01.240 --> 18:02.560\n In other situations, you know,\n\n18:02.560 --> 18:04.640\n driving through the snow in Boston\n\n18:04.640 --> 18:06.160\n where the roads are kind of crazy.\n\n18:06.160 --> 18:08.200\n And most importantly, you have to make a lot of judgments\n\n18:08.200 --> 18:09.440\n about what the other driver is gonna do\n\n18:09.440 --> 18:11.680\n at these intersections that aren't really right angles\n\n18:11.680 --> 18:13.400\n and aren't very well described.\n\n18:13.400 --> 18:15.320\n It's more like game theory.\n\n18:15.320 --> 18:17.080\n That's a much harder problem\n\n18:17.080 --> 18:20.600\n and requires understanding human motivations.\n\n18:22.080 --> 18:24.320\n So there's a continuum there of some places\n\n18:24.320 --> 18:27.560\n where the cars will work very well\n\n18:27.560 --> 18:30.920\n and others where it could probably take decades.\n\n18:30.920 --> 18:33.480\n What do you think about the Waymo?\n\n18:33.480 --> 18:36.000\n So you mentioned two companies\n\n18:36.000 --> 18:38.040\n that actually have cars on the road.\n\n18:38.040 --> 18:40.640\n There's the Waymo approach that it's more like\n\n18:40.640 --> 18:42.800\n we're not going to release anything until it's perfect\n\n18:42.800 --> 18:45.320\n and we're gonna be very strict\n\n18:45.320 --> 18:47.680\n about the streets that we travel on,\n\n18:47.680 --> 18:49.160\n but it better be perfect.\n\n18:49.160 --> 18:50.200\n Yeah.\n\n18:50.200 --> 18:53.920\n Well, I'm smart enough to be humble\n\n18:53.920 --> 18:55.000\n and not try to get between.\n\n18:55.000 --> 18:56.600\n I know there's very bright people\n\n18:56.600 --> 18:57.440\n on both sides of the argument.\n\n18:57.440 --> 19:00.000\n I've talked to them and they make convincing arguments to me\n\n19:00.000 --> 19:04.000\n about how careful they need to be and the social acceptance.\n\n19:04.000 --> 19:07.400\n Some people thought that when the first few people died\n\n19:07.400 --> 19:09.840\n from self driving cars, that would shut down the industry,\n\n19:09.840 --> 19:11.800\n but it was more of a blip actually.\n\n19:11.800 --> 19:14.440\n And, you know, so that was interesting.\n\n19:14.440 --> 19:16.040\n Of course, there's still a concern\n\n19:16.040 --> 19:20.560\n that if there could be setbacks, if we do this wrong,\n\n19:20.560 --> 19:22.600\n you know, your listeners may be familiar\n\n19:22.600 --> 19:24.160\n with the different levels of self driving,\n\n19:24.160 --> 19:26.800\n you know, level one, two, three, four, five.\n\n19:26.800 --> 19:29.640\n I think Andrew Ng has convinced me that this idea\n\n19:29.640 --> 19:32.600\n of really focusing on level four,\n\n19:32.600 --> 19:35.000\n where you only go in areas that are well mapped\n\n19:35.000 --> 19:37.480\n rather than just going out in the wild\n\n19:37.480 --> 19:39.720\n is the way things are gonna evolve.\n\n19:39.720 --> 19:42.600\n But you can just keep expanding those areas\n\n19:42.600 --> 19:44.040\n where you've mapped things really well,\n\n19:44.040 --> 19:45.040\n where you really understand them\n\n19:45.040 --> 19:47.960\n and eventually all become kind of interconnected.\n\n19:47.960 --> 19:51.160\n And that could be a kind of another way of progressing\n\n19:51.160 --> 19:55.240\n to make it more feasible over time.\n\n19:55.240 --> 19:57.400\n I mean, that's kind of like the Waymo approach,\n\n19:57.400 --> 19:59.520\n which is they just now released,\n\n19:59.520 --> 20:01.960\n I think just like a day or two ago,\n\n20:01.960 --> 20:05.480\n a public, like anyone from the public\n\n20:05.480 --> 20:10.480\n in the Phoenix, Arizona to, you know,\n\n20:12.160 --> 20:14.360\n you can get a ride in a Waymo car\n\n20:14.360 --> 20:16.120\n with no person, no driver.\n\n20:16.120 --> 20:17.640\n Oh, they've taken away the safety driver?\n\n20:17.640 --> 20:21.080\n Oh yeah, for a while now there's been no safety driver.\n\n20:21.080 --> 20:22.760\n Okay, because I mean, I've been following that one\n\n20:22.760 --> 20:24.840\n in particular, but I thought it was kind of funny\n\n20:24.840 --> 20:26.960\n about a year ago when they had the safety driver\n\n20:26.960 --> 20:28.400\n and then they added a second safety driver\n\n20:28.400 --> 20:30.880\n because the first safety driver would fall asleep.\n\n20:30.880 --> 20:32.120\n It's like, I'm not sure they're going\n\n20:32.120 --> 20:33.400\n in the right direction with that.\n\n20:33.400 --> 20:38.200\n No, they've Waymo in particular\n\n20:38.200 --> 20:39.480\n done a really good job of that.\n\n20:39.480 --> 20:44.360\n They actually have a very interesting infrastructure\n\n20:44.360 --> 20:47.480\n of remote like observation.\n\n20:47.480 --> 20:49.840\n So they're not controlling the vehicles remotely,\n\n20:49.840 --> 20:52.440\n but they're able to, it's like a customer service.\n\n20:52.440 --> 20:55.160\n They can anytime tune into the car.\n\n20:55.160 --> 20:58.160\n I bet they can probably remotely control it as well,\n\n20:58.160 --> 21:00.920\n but that's officially not the function that they use.\n\n21:00.920 --> 21:02.760\n Yeah, I can see that being really,\n\n21:02.760 --> 21:06.280\n because I think the thing that's proven harder\n\n21:06.280 --> 21:08.040\n than maybe some of the early people expected\n\n21:08.040 --> 21:10.840\n was there's a long tail of weird exceptions.\n\n21:10.840 --> 21:15.440\n So you can deal with 90, 99, 99.99% of the cases,\n\n21:15.440 --> 21:17.720\n but then there's something that just never been seen before\n\n21:17.720 --> 21:18.840\n in the training data.\n\n21:18.840 --> 21:21.080\n And humans more or less can work around that.\n\n21:21.080 --> 21:22.840\n Although let me be clear and note,\n\n21:22.840 --> 21:25.640\n there are about 30,000 human fatalities\n\n21:25.640 --> 21:28.360\n just in the United States and maybe a million worldwide.\n\n21:28.360 --> 21:30.000\n So they're far from perfect.\n\n21:30.000 --> 21:33.440\n But I think people have higher expectations of machines.\n\n21:33.440 --> 21:36.280\n They wouldn't tolerate that level of death\n\n21:36.280 --> 21:40.000\n and damage from a machine.\n\n21:40.000 --> 21:41.800\n And so we have to do a lot better\n\n21:41.800 --> 21:43.480\n at dealing with those edge cases.\n\n21:43.480 --> 21:46.960\n And also the tricky thing that if I have a criticism\n\n21:46.960 --> 21:50.520\n for the Waymo folks, there's such a huge focus on safety\n\n21:51.800 --> 21:55.160\n where people don't talk enough about creating products\n\n21:55.160 --> 21:57.240\n that people, that customers love,\n\n21:57.240 --> 21:59.800\n that human beings love using.\n\n22:00.680 --> 22:03.320\n It's very easy to create a thing that's safe\n\n22:03.320 --> 22:06.880\n at the extremes, but then nobody wants to get into it.\n\n22:06.880 --> 22:09.640\n Yeah, well, back to Elon, I think one of,\n\n22:09.640 --> 22:11.440\n part of his genius was with the electric cars.\n\n22:11.440 --> 22:13.960\n Before he came along, electric cars were all kind of\n\n22:13.960 --> 22:15.680\n underpowered, really light,\n\n22:15.680 --> 22:20.680\n and there were sort of wimpy cars that weren't fun.\n\n22:20.680 --> 22:23.640\n And the first thing he did was he made a roadster\n\n22:23.640 --> 22:27.440\n that went zero to 60 faster than just about any other car\n\n22:27.440 --> 22:28.480\n and went the other end.\n\n22:28.480 --> 22:30.800\n And I think that was a really wise marketing move\n\n22:30.800 --> 22:33.160\n as well as a wise technology move.\n\n22:33.160 --> 22:34.840\n Yeah, it's difficult to figure out\n\n22:34.840 --> 22:37.960\n what the right marketing move is for AI systems.\n\n22:37.960 --> 22:42.960\n That's always been, I think it requires guts and risk taking\n\n22:42.960 --> 22:46.320\n which is what Elon practices.\n\n22:46.320 --> 22:50.480\n I mean, to the chagrin of perhaps investors or whatever,\n\n22:50.480 --> 22:54.320\n but it also requires rethinking what you're doing.\n\n22:54.320 --> 22:57.520\n I think way too many people are unimaginative,\n\n22:57.520 --> 22:59.760\n intellectually lazy, and when they take AI,\n\n22:59.760 --> 23:01.640\n they basically say, what are we doing now?\n\n23:01.640 --> 23:04.000\n How can we make a machine do the same thing?\n\n23:04.000 --> 23:06.720\n Maybe we'll save some costs, we'll have less labor.\n\n23:06.720 --> 23:08.560\n And yeah, it's not necessarily the worst thing\n\n23:08.560 --> 23:10.640\n in the world to do, but it's really not leading\n\n23:10.640 --> 23:12.840\n to a quantum change in the way you do things.\n\n23:12.840 --> 23:16.640\n When Jeff Bezos said, hey, we're gonna use the internet\n\n23:16.640 --> 23:19.320\n to change how bookstores work and we're gonna use technology,\n\n23:19.320 --> 23:22.680\n he didn't go and say, okay, let's put a robot cashier\n\n23:22.680 --> 23:25.640\n where the human cashier is and leave everything else alone.\n\n23:25.640 --> 23:28.360\n That would have been a very lame way to automate a bookstore.\n\n23:28.360 --> 23:31.400\n He's like went from soup to nuts and let's just rethink it.\n\n23:31.400 --> 23:33.040\n We get rid of the physical bookstore.\n\n23:33.040 --> 23:34.520\n We have a warehouse, we have delivery,\n\n23:34.520 --> 23:36.360\n we have people order on a screen\n\n23:36.360 --> 23:38.480\n and everything was reinvented.\n\n23:38.480 --> 23:39.720\n And that's been the story\n\n23:39.720 --> 23:43.560\n of these general purpose technologies all through history.\n\n23:43.560 --> 23:46.320\n And in my books, I write about like electricity\n\n23:46.320 --> 23:50.360\n and how for 30 years, there was almost no productivity gain\n\n23:50.360 --> 23:53.040\n from the electrification of factories a century ago.\n\n23:53.040 --> 23:54.160\n Now it's not because electricity\n\n23:54.160 --> 23:55.800\n is a wimpy useless technology.\n\n23:55.800 --> 23:57.560\n We all know how awesome electricity is.\n\n23:57.560 --> 23:58.400\n It's cause at first,\n\n23:58.400 --> 24:00.560\n they really didn't rethink the factories.\n\n24:00.560 --> 24:02.160\n It was only after they reinvented them\n\n24:02.160 --> 24:04.040\n and we describe how in the book,\n\n24:04.040 --> 24:05.920\n then you suddenly got a doubling and tripling\n\n24:05.920 --> 24:07.560\n of productivity growth.\n\n24:07.560 --> 24:09.920\n But it's the combination of the technology\n\n24:09.920 --> 24:12.960\n with the new business models, new business organization.\n\n24:12.960 --> 24:14.000\n That just takes a long time\n\n24:14.000 --> 24:16.920\n and it takes more creativity than most people have.\n\n24:16.920 --> 24:19.000\n Can you maybe linger on electricity?\n\n24:19.000 --> 24:20.080\n Cause that's a fun one.\n\n24:20.080 --> 24:22.480\n Yeah, well, sure, I'll tell you what happened.\n\n24:22.480 --> 24:25.760\n Before electricity, there were basically steam engines\n\n24:25.760 --> 24:28.400\n or sometimes water wheels and to power the machinery,\n\n24:28.400 --> 24:30.560\n you had to have pulleys and crankshafts\n\n24:30.560 --> 24:32.120\n and you really can't make them too long\n\n24:32.120 --> 24:34.040\n cause they'll break the torsion.\n\n24:34.040 --> 24:35.960\n So all the equipment was kind of clustered\n\n24:35.960 --> 24:37.960\n around this one giant steam engine.\n\n24:37.960 --> 24:39.480\n You can't make small steam engines either\n\n24:39.480 --> 24:40.800\n cause of thermodynamics.\n\n24:40.800 --> 24:42.360\n So you have one giant steam engine,\n\n24:42.360 --> 24:44.320\n all the equipment clustered around it, multi story.\n\n24:44.320 --> 24:46.080\n They have it vertical to minimize the distance\n\n24:46.080 --> 24:47.760\n as well as horizontal.\n\n24:47.760 --> 24:48.960\n And then when they did electricity,\n\n24:48.960 --> 24:50.080\n they took out the steam engine.\n\n24:50.080 --> 24:51.360\n They got the biggest electric motor\n\n24:51.360 --> 24:54.200\n they could buy from General Electric or someone like that.\n\n24:54.200 --> 24:56.400\n And nothing much else changed.\n\n24:57.920 --> 25:00.760\n It took until a generation of managers retired\n\n25:00.760 --> 25:03.200\n or died three years later,\n\n25:03.200 --> 25:04.440\n that people started thinking,\n\n25:04.440 --> 25:05.840\n wait, we don't have to do it that way.\n\n25:05.840 --> 25:09.480\n You can make electric motors, big, small, medium.\n\n25:09.480 --> 25:11.480\n You can put one with each piece of equipment.\n\n25:11.480 --> 25:12.320\n There's this big debate\n\n25:12.320 --> 25:13.360\n if you read the management literature\n\n25:13.360 --> 25:16.160\n between what they call a group drive versus unit drive\n\n25:16.160 --> 25:18.960\n where every machine would have its own motor.\n\n25:18.960 --> 25:21.040\n Well, once they did that, once they went to unit drive,\n\n25:21.040 --> 25:23.040\n those guys won the debate.\n\n25:23.040 --> 25:25.080\n Then you started having a new kind of factory\n\n25:25.080 --> 25:29.400\n which is sometimes spread out over acres, single story\n\n25:29.400 --> 25:31.360\n and each piece of equipment has its own motor.\n\n25:31.360 --> 25:33.360\n And most importantly, they weren't laid out based on\n\n25:33.360 --> 25:35.000\n who needed the most power.\n\n25:35.000 --> 25:37.600\n They were laid out based on\n\n25:37.600 --> 25:40.000\n what is the workflow of materials?\n\n25:40.000 --> 25:41.720\n Assembly line, let's have it go from this machine\n\n25:41.720 --> 25:43.240\n to that machine, to that machine.\n\n25:43.240 --> 25:46.040\n Once they rethought the factory that way,\n\n25:46.040 --> 25:47.680\n huge increases in productivity.\n\n25:47.680 --> 25:48.520\n It was just staggering.\n\n25:48.520 --> 25:50.080\n People like Paul David have documented this\n\n25:50.080 --> 25:51.760\n in their research papers.\n\n25:51.760 --> 25:55.920\n And I think that that is a lesson you see over and over.\n\n25:55.920 --> 25:58.560\n It happened when the steam engine changed manual production.\n\n25:58.560 --> 26:00.240\n It's happened with the computerization.\n\n26:00.240 --> 26:03.600\n People like Michael Hammer said, don't automate, obliterate.\n\n26:03.600 --> 26:08.440\n In each case, the big gains only came once\n\n26:08.440 --> 26:10.400\n smart entrepreneurs and managers\n\n26:10.400 --> 26:13.040\n basically reinvented their industries.\n\n26:13.040 --> 26:14.680\n I mean, one other interesting point about all that\n\n26:14.680 --> 26:17.920\n is that during that reinvention period,\n\n26:18.920 --> 26:22.200\n you often actually not only don't see productivity growth,\n\n26:22.200 --> 26:24.320\n you can actually see a slipping back.\n\n26:24.320 --> 26:26.560\n Measured productivity actually falls.\n\n26:26.560 --> 26:29.040\n I just wrote a paper with Chad Severson and Daniel Rock\n\n26:29.040 --> 26:31.520\n called the productivity J curve,\n\n26:31.520 --> 26:33.840\n which basically shows that in a lot of these cases,\n\n26:33.840 --> 26:36.520\n you have a downward dip before it goes up.\n\n26:36.520 --> 26:38.320\n And that downward dip is when everyone's trying\n\n26:38.320 --> 26:40.400\n to like reinvent things.\n\n26:40.400 --> 26:43.120\n And you could say that they're creating knowledge\n\n26:43.120 --> 26:44.640\n and intangible assets,\n\n26:44.640 --> 26:46.760\n but that doesn't show up on anyone's balance sheet.\n\n26:46.760 --> 26:48.320\n It doesn't show up in GDP.\n\n26:48.320 --> 26:50.080\n So it's as if they're doing nothing.\n\n26:50.080 --> 26:52.480\n Like take self driving cars, we were just talking about it.\n\n26:52.480 --> 26:55.040\n There have been hundreds of billions of dollars\n\n26:55.040 --> 26:57.880\n spent developing self driving cars.\n\n26:57.880 --> 27:02.360\n And basically no chauffeur has lost his job, no taxi driver.\n\n27:02.360 --> 27:03.200\n I guess I got to check out the ones that.\n\n27:03.200 --> 27:04.440\n It's a big J curve.\n\n27:04.440 --> 27:06.080\n Yeah, so there's a bunch of spending\n\n27:06.080 --> 27:08.120\n and no real consumer benefit.\n\n27:08.120 --> 27:11.440\n Now they're doing that in the belief,\n\n27:11.440 --> 27:13.240\n I think the justified belief\n\n27:13.240 --> 27:15.160\n that they will get the upward part of the J curve\n\n27:15.160 --> 27:16.920\n and there will be some big returns,\n\n27:16.920 --> 27:19.320\n but in the short run, you're not seeing it.\n\n27:19.320 --> 27:21.840\n That's happening with a lot of other AI technologies,\n\n27:21.840 --> 27:22.680\n just as it happened\n\n27:22.680 --> 27:25.040\n with earlier general purpose technologies.\n\n27:25.040 --> 27:25.880\n And it's one of the reasons\n\n27:25.880 --> 27:29.280\n we're having relatively low productivity growth lately.\n\n27:29.280 --> 27:31.400\n As an economist, one of the things that disappoints me\n\n27:31.400 --> 27:34.440\n is that as eye popping as these technologies are,\n\n27:34.440 --> 27:35.360\n you and I are both excited\n\n27:35.360 --> 27:36.880\n about some of the things they can do.\n\n27:36.880 --> 27:40.280\n The economic productivity statistics are kind of dismal.\n\n27:40.280 --> 27:42.200\n We actually, believe it or not,\n\n27:42.200 --> 27:44.560\n have had lower productivity growth\n\n27:44.560 --> 27:47.000\n in the past about 15 years\n\n27:47.000 --> 27:48.840\n than we did in the previous 15 years,\n\n27:48.840 --> 27:51.360\n in the 90s and early 2000s.\n\n27:51.360 --> 27:53.200\n And so that's not what you would have expected\n\n27:53.200 --> 27:55.520\n if these technologies were that much better.\n\n27:55.520 --> 27:59.400\n But I think we're in kind of a long J curve there.\n\n27:59.400 --> 28:00.560\n Personally, I'm optimistic.\n\n28:00.560 --> 28:02.120\n We'll start seeing the upward tick,\n\n28:02.120 --> 28:04.520\n maybe as soon as next year.\n\n28:04.520 --> 28:08.520\n But the past decade has been a bit disappointing\n\n28:08.520 --> 28:10.000\n if you thought there's a one to one relationship\n\n28:10.000 --> 28:12.760\n between cool technology and higher productivity.\n\n28:12.760 --> 28:15.240\n Well, what would you place your biggest hope\n\n28:15.240 --> 28:17.240\n for productivity increases on?\n\n28:17.240 --> 28:19.880\n Because you kind of said at a high level AI,\n\n28:19.880 --> 28:22.840\n but if I were to think about\n\n28:22.840 --> 28:27.840\n what has been so revolutionary in the last 10 years,\n\n28:28.200 --> 28:32.240\n I would 15 years and thinking about the internet,\n\n28:32.240 --> 28:34.320\n I would say things like,\n\n28:35.800 --> 28:37.160\n hopefully I'm not saying anything ridiculous,\n\n28:37.160 --> 28:41.040\n but everything from Wikipedia to Twitter.\n\n28:41.040 --> 28:43.600\n So like these kind of websites,\n\n28:43.600 --> 28:46.080\n not so much AI,\n\n28:46.080 --> 28:48.160\n but like I would expect to see some kind\n\n28:48.160 --> 28:50.520\n of big productivity increases\n\n28:50.520 --> 28:54.160\n from just the connectivity between people\n\n28:54.160 --> 28:58.040\n and the access to more information.\n\n28:58.040 --> 29:00.040\n Yeah, well, so that's another area\n\n29:00.040 --> 29:01.840\n I've done quite a bit of research on actually,\n\n29:01.840 --> 29:06.840\n is these free goods like Wikipedia, Facebook, Twitter, Zoom.\n\n29:06.840 --> 29:08.120\n We're actually doing this in person,\n\n29:08.120 --> 29:11.120\n but almost everything else I do these days is online.\n\n29:12.400 --> 29:13.760\n The interesting thing about all those\n\n29:13.760 --> 29:17.720\n is most of them have a price of zero.\n\n29:18.880 --> 29:19.960\n What do you pay for Wikipedia?\n\n29:19.960 --> 29:21.680\n Maybe like a little bit for the electrons\n\n29:21.680 --> 29:22.960\n to come to your house.\n\n29:22.960 --> 29:24.080\n Basically zero, right?\n\n29:25.600 --> 29:28.040\n Take a small pause and say, I donate to Wikipedia.\n\n29:28.040 --> 29:28.880\n Often you should too.\n\n29:28.880 --> 29:30.080\n It's good for you, yeah.\n\n29:30.080 --> 29:32.480\n So, but what does that do mean for GDP?\n\n29:32.480 --> 29:36.120\n GDP is based on the price and quantity\n\n29:36.120 --> 29:37.760\n of all the goods, things bought and sold.\n\n29:37.760 --> 29:39.560\n If something has zero price,\n\n29:39.560 --> 29:42.280\n you know how much it contributes to GDP?\n\n29:42.280 --> 29:44.520\n To a first approximation, zero.\n\n29:44.520 --> 29:47.560\n So these digital goods that we're getting more and more of,\n\n29:47.560 --> 29:50.240\n we're spending more and more hours a day\n\n29:50.240 --> 29:52.080\n consuming stuff off of screens,\n\n29:52.080 --> 29:53.440\n little screens, big screens,\n\n29:54.520 --> 29:56.160\n that doesn't get priced into GDP.\n\n29:56.160 --> 29:58.640\n It's like they don't exist.\n\n29:58.640 --> 30:00.000\n That doesn't mean they don't create value.\n\n30:00.000 --> 30:03.360\n I get a lot of value from watching cat videos\n\n30:03.360 --> 30:06.160\n and reading Wikipedia articles and listening to podcasts,\n\n30:06.160 --> 30:08.440\n even if I don't pay for them.\n\n30:08.440 --> 30:10.440\n So we've got a mismatch there.\n\n30:10.440 --> 30:12.520\n Now, in fairness, economists,\n\n30:12.520 --> 30:15.320\n since Simon Kuznets invented GDP and productivity,\n\n30:15.320 --> 30:17.760\n all those statistics back in the 1930s,\n\n30:17.760 --> 30:19.680\n he recognized, he in fact said,\n\n30:19.680 --> 30:21.520\n this is not a measure of wellbeing.\n\n30:21.520 --> 30:23.200\n This is not a measure of welfare.\n\n30:23.200 --> 30:25.120\n It's a measure of production.\n\n30:25.120 --> 30:28.960\n But almost everybody has kind of forgotten\n\n30:28.960 --> 30:31.000\n that he said that and they just use it.\n\n30:31.000 --> 30:32.200\n It's like, how well off are we?\n\n30:32.200 --> 30:33.240\n What was GDP last year?\n\n30:33.240 --> 30:35.800\n It was 2.3% growth or whatever.\n\n30:35.800 --> 30:39.440\n That is how much physical production,\n\n30:39.440 --> 30:42.360\n but it's not the value we're getting.\n\n30:42.360 --> 30:43.840\n We need a new set of statistics\n\n30:43.840 --> 30:45.280\n and I'm working with some colleagues.\n\n30:45.280 --> 30:48.360\n Avi Collis and others to develop something\n\n30:48.360 --> 30:50.440\n we call GDP dash B.\n\n30:50.440 --> 30:55.440\n GDP B measures the benefits you get, not the cost.\n\n30:55.440 --> 31:00.360\n If you get benefit from Zoom or Wikipedia or Facebook,\n\n31:00.360 --> 31:02.520\n then that gets counted in GDP B,\n\n31:02.520 --> 31:04.560\n even if you pay zero for it.\n\n31:04.560 --> 31:07.360\n So, you know, back to your original point,\n\n31:07.360 --> 31:10.480\n I think there is a lot of gain over the past decade\n\n31:10.480 --> 31:15.280\n in these digital goods that doesn't show up in GDP,\n\n31:15.280 --> 31:16.440\n doesn't show up in productivity.\n\n31:16.440 --> 31:17.920\n By the way, productivity is just defined\n\n31:17.920 --> 31:20.080\n as GDP divided by hours worked.\n\n31:20.080 --> 31:22.080\n So if you mismeasure GDP,\n\n31:22.080 --> 31:25.360\n you mismeasure productivity by the exact same amount.\n\n31:25.360 --> 31:26.480\n That's something we need to fix.\n\n31:26.480 --> 31:28.440\n I'm working with the statistical agencies\n\n31:28.440 --> 31:30.200\n to come up with a new set of metrics.\n\n31:30.200 --> 31:32.200\n And, you know, over the coming years,\n\n31:32.200 --> 31:34.480\n I think we'll see, we're not gonna do away with GDP.\n\n31:34.480 --> 31:37.240\n It's very useful, but we'll see a parallel set of accounts\n\n31:37.240 --> 31:38.400\n that measure the benefits.\n\n31:38.400 --> 31:41.080\n How difficult is it to get that B in the GDP B?\n\n31:41.080 --> 31:41.920\n It's pretty hard.\n\n31:41.920 --> 31:44.360\n I mean, one of the reasons it hasn't been done before\n\n31:44.360 --> 31:46.720\n is that, you know, you can measure it,\n\n31:46.720 --> 31:49.000\n the cash register, what people pay for stuff,\n\n31:49.000 --> 31:51.040\n but how do you measure what they would have paid,\n\n31:51.040 --> 31:52.040\n like what the value is?\n\n31:52.040 --> 31:54.040\n That's a lot harder, you know?\n\n31:54.040 --> 31:56.040\n How much is Wikipedia worth to you?\n\n31:56.040 --> 31:57.480\n That's what we have to answer.\n\n31:57.480 --> 32:00.720\n And to do that, what we do is we can use online experiments.\n\n32:00.720 --> 32:03.120\n We do massive online choice experiments.\n\n32:03.120 --> 32:05.720\n We ask hundreds of thousands, now millions of people\n\n32:05.720 --> 32:07.760\n to do lots of sort of A, B tests.\n\n32:07.760 --> 32:09.080\n How much would I have to pay you\n\n32:09.080 --> 32:10.840\n to give up Wikipedia for a month?\n\n32:10.840 --> 32:14.120\n How much would I have to pay you to stop using your phone?\n\n32:14.120 --> 32:15.960\n And in some cases, it's hypothetical.\n\n32:15.960 --> 32:17.520\n In other cases, we actually enforce it,\n\n32:17.520 --> 32:18.920\n which is kind of expensive.\n\n32:18.920 --> 32:22.440\n Like we pay somebody $30 to stop using Facebook\n\n32:22.440 --> 32:23.440\n and we see if they'll do it.\n\n32:23.440 --> 32:26.280\n And some people will give it up for $10.\n\n32:26.280 --> 32:28.880\n Some people won't give it up even if you give them $100.\n\n32:28.880 --> 32:31.080\n And then you get a whole demand curve.\n\n32:31.080 --> 32:33.600\n You get to see what all the different prices are\n\n32:33.600 --> 32:36.000\n and how much value different people get.\n\n32:36.000 --> 32:36.880\n And not surprisingly,\n\n32:36.880 --> 32:38.240\n different people have different values.\n\n32:38.240 --> 32:41.520\n We find that women tend to value Facebook more than men.\n\n32:41.520 --> 32:43.200\n Old people tend to value it a little bit more\n\n32:43.200 --> 32:44.040\n than young people.\n\n32:44.040 --> 32:44.880\n That was interesting.\n\n32:44.880 --> 32:46.600\n I think young people maybe know about other networks\n\n32:46.600 --> 32:50.280\n that I don't know the name of that are better than Facebook.\n\n32:50.280 --> 32:53.480\n And so you get to see these patterns,\n\n32:53.480 --> 32:55.440\n but every person's individual.\n\n32:55.440 --> 32:57.280\n And then if you add up all those numbers,\n\n32:57.280 --> 33:00.040\n you start getting an estimate of the value.\n\n33:00.040 --> 33:01.240\n Okay, first of all, that's brilliant.\n\n33:01.240 --> 33:05.720\n Is this a work that will soon eventually be published?\n\n33:05.720 --> 33:07.040\n Yeah, well, there's a version of it\n\n33:07.040 --> 33:09.520\n in the Proceedings of the National Academy of Sciences\n\n33:09.520 --> 33:11.880\n about I think we call it massive online choice experiments.\n\n33:11.880 --> 33:14.920\n I should remember the title, but it's on my website.\n\n33:14.920 --> 33:17.520\n So yeah, we have some more papers coming out on it,\n\n33:17.520 --> 33:20.160\n but the first one is already out.\n\n33:20.160 --> 33:22.320\n You know, it's kind of a fascinating mystery\n\n33:22.320 --> 33:24.360\n that Twitter, Facebook,\n\n33:24.360 --> 33:26.800\n like all these social networks are free.\n\n33:26.800 --> 33:31.440\n And it seems like almost none of them except for YouTube\n\n33:31.440 --> 33:35.200\n have experimented with removing ads for money.\n\n33:35.200 --> 33:37.160\n Can you like, do you understand that\n\n33:37.160 --> 33:39.800\n from both economics and the product perspective?\n\n33:39.800 --> 33:41.000\n Yeah, it's something that, you know,\n\n33:41.000 --> 33:43.240\n so I teach a course on digital business models.\n\n33:43.240 --> 33:45.800\n So I used to at MIT, at Stanford, I'm not quite sure.\n\n33:45.800 --> 33:47.360\n I'm not teaching until next spring.\n\n33:47.360 --> 33:50.040\n I'm still thinking what my course is gonna be.\n\n33:50.040 --> 33:52.200\n But there are a lot of different business models.\n\n33:52.200 --> 33:54.880\n And when you have something that has zero marginal cost,\n\n33:54.880 --> 33:56.400\n there's a lot of forces,\n\n33:56.400 --> 33:57.880\n especially if there's any kind of competition\n\n33:57.880 --> 33:59.960\n that push prices down to zero.\n\n33:59.960 --> 34:03.360\n But you can have ad supported systems,\n\n34:03.360 --> 34:05.520\n you can bundle things together.\n\n34:05.520 --> 34:07.360\n You can have volunteer, you mentioned Wikipedia,\n\n34:07.360 --> 34:08.760\n there's donations.\n\n34:08.760 --> 34:11.120\n And I think economists underestimate\n\n34:11.120 --> 34:14.560\n the power of volunteerism and donations.\n\n34:14.560 --> 34:16.040\n Your national public radio.\n\n34:16.040 --> 34:18.560\n Actually, how do you, this podcast, how is this,\n\n34:18.560 --> 34:19.480\n what's the revenue model?\n\n34:19.480 --> 34:22.240\n There's sponsors at the beginning.\n\n34:22.240 --> 34:24.640\n And then, and people, the funny thing is,\n\n34:24.640 --> 34:26.640\n I tell people they can, it's very,\n\n34:26.640 --> 34:27.880\n I tell them the timestamp.\n\n34:27.880 --> 34:30.960\n So if you wanna skip the sponsors, you're free.\n\n34:30.960 --> 34:33.560\n But it's funny that a bunch of people,\n\n34:33.560 --> 34:36.200\n so I read the advertisement\n\n34:36.200 --> 34:38.400\n and then a bunch of people enjoy reading it.\n\n34:38.400 --> 34:39.240\n And it's.\n\n34:39.240 --> 34:40.080\n Well, they may learn something from it.\n\n34:40.080 --> 34:42.920\n And also from the advertiser's perspective,\n\n34:42.920 --> 34:45.400\n those are people who are actually interested.\n\n34:45.400 --> 34:46.960\n I mean, the example I sometimes get is like,\n\n34:46.960 --> 34:49.840\n I bought a car recently and all of a sudden,\n\n34:49.840 --> 34:52.400\n all the car ads were like interesting to me.\n\n34:52.400 --> 34:53.240\n Exactly.\n\n34:53.240 --> 34:54.360\n And then like, now that I have the car,\n\n34:54.360 --> 34:56.280\n like I sort of zone out on, but that's fine.\n\n34:56.280 --> 34:58.720\n The car companies, they don't really wanna be advertising\n\n34:58.720 --> 35:01.320\n to me if I'm not gonna buy their product.\n\n35:01.320 --> 35:03.560\n So there are a lot of these different revenue models\n\n35:03.560 --> 35:06.880\n and it's a little complicated,\n\n35:06.880 --> 35:08.000\n but the economic theory has to do\n\n35:08.000 --> 35:09.480\n with what the shape of the demand curve is,\n\n35:09.480 --> 35:13.160\n when it's better to monetize it with charging people\n\n35:13.160 --> 35:15.640\n versus when you're better off doing advertising.\n\n35:15.640 --> 35:18.280\n I mean, in short, when the demand curve\n\n35:18.280 --> 35:20.600\n is relatively flat and wide,\n\n35:20.600 --> 35:22.760\n like generic news and things like that,\n\n35:22.760 --> 35:25.920\n then you tend to do better with advertising.\n\n35:25.920 --> 35:28.840\n If it's a good that's only useful to a small number\n\n35:28.840 --> 35:30.320\n of people, but they're willing to pay a lot,\n\n35:30.320 --> 35:32.720\n they have a very high value for it,\n\n35:32.720 --> 35:34.560\n then advertising isn't gonna work as well\n\n35:34.560 --> 35:36.080\n and you're better off charging for it.\n\n35:36.080 --> 35:38.080\n Both of them have some inefficiencies.\n\n35:38.080 --> 35:39.480\n And then when you get into targeting\n\n35:39.480 --> 35:40.600\n and you get into these other revenue models,\n\n35:40.600 --> 35:41.960\n it gets more complicated,\n\n35:41.960 --> 35:45.320\n but there's some economic theory on it.\n\n35:45.320 --> 35:47.560\n I also think to be frank,\n\n35:47.560 --> 35:49.560\n there's just a lot of experimentation that's needed\n\n35:49.560 --> 35:53.200\n because sometimes things are a little counterintuitive,\n\n35:53.200 --> 35:55.160\n especially when you get into what are called\n\n35:55.160 --> 35:57.640\n two sided networks or platform effects,\n\n35:57.640 --> 36:01.840\n where you may grow the market on one side\n\n36:01.840 --> 36:04.120\n and harvest the revenue on the other side.\n\n36:04.120 --> 36:06.080\n Facebook tries to get more and more users\n\n36:06.080 --> 36:08.960\n and then they harvest the revenue from advertising.\n\n36:08.960 --> 36:12.040\n So that's another way of kind of thinking about it.\n\n36:12.040 --> 36:14.400\n Is it strange to you that they haven't experimented?\n\n36:14.400 --> 36:15.360\n Well, they are experimenting.\n\n36:15.360 --> 36:17.600\n So they are doing some experiments\n\n36:17.600 --> 36:20.360\n about what the willingness is for people to pay.\n\n36:22.040 --> 36:23.560\n I think that when they do the math,\n\n36:23.560 --> 36:26.400\n it's gonna work out that they still are better off\n\n36:26.400 --> 36:29.440\n with an advertising driven model, but...\n\n36:29.440 --> 36:30.400\n What about a mix?\n\n36:30.400 --> 36:32.400\n Like this is what YouTube is, right?\n\n36:32.400 --> 36:36.360\n It's you allow the person to decide,\n\n36:36.360 --> 36:39.360\n the customer to decide exactly which model they prefer.\n\n36:39.360 --> 36:40.920\n No, that can work really well.\n\n36:40.920 --> 36:41.760\n And newspapers, of course,\n\n36:41.760 --> 36:42.760\n have known this for a long time.\n\n36:42.760 --> 36:44.560\n The Wall Street Journal, the New York Times,\n\n36:44.560 --> 36:45.840\n they have subscription revenue.\n\n36:45.840 --> 36:48.080\n They also have advertising revenue.\n\n36:48.080 --> 36:52.200\n And that can definitely work.\n\n36:52.200 --> 36:54.080\n Online, it's a lot easier to have a dial\n\n36:54.080 --> 36:55.240\n that's much more personalized\n\n36:55.240 --> 36:57.720\n and everybody can kind of roll their own mix.\n\n36:57.720 --> 37:00.320\n And I could imagine having a little slider\n\n37:00.320 --> 37:05.040\n about how much advertising you want or are willing to take.\n\n37:05.040 --> 37:07.400\n And if it's done right and it's incentive compatible,\n\n37:07.400 --> 37:10.960\n it could be a win win where both the content provider\n\n37:10.960 --> 37:12.560\n and the consumer are better off\n\n37:12.560 --> 37:14.480\n than they would have been before.\n\n37:14.480 --> 37:17.960\n Yeah, the done right part is a really good point.\n\n37:17.960 --> 37:19.600\n Like with the Jeff Bezos\n\n37:19.600 --> 37:22.000\n and the single click purchase on Amazon,\n\n37:22.000 --> 37:23.880\n the frictionless effort there,\n\n37:23.880 --> 37:25.760\n if I could just rant for a second\n\n37:25.760 --> 37:27.240\n about the Wall Street Journal,\n\n37:27.240 --> 37:29.280\n all the newspapers you mentioned,\n\n37:29.280 --> 37:34.280\n is I have to click so many times to subscribe to them\n\n37:34.800 --> 37:37.400\n that I literally don't subscribe\n\n37:37.400 --> 37:39.520\n just because of the number of times I have to click.\n\n37:39.520 --> 37:40.360\n I'm totally with you.\n\n37:40.360 --> 37:44.560\n I don't understand why so many companies make it so hard.\n\n37:44.560 --> 37:47.240\n I mean, another example is when you buy a new iPhone\n\n37:47.240 --> 37:48.900\n or a new computer, whatever,\n\n37:48.900 --> 37:51.440\n I feel like, okay, I'm gonna lose an afternoon\n\n37:51.440 --> 37:53.800\n just like loading up and getting all my stuff back.\n\n37:53.800 --> 37:56.080\n And for a lot of us,\n\n37:56.080 --> 37:58.600\n that's more of a deterrent than the price.\n\n37:58.600 --> 38:01.800\n And if they could make it painless,\n\n38:01.800 --> 38:03.680\n we'd give them a lot more money.\n\n38:03.680 --> 38:06.440\n So I'm hoping somebody listening is working\n\n38:06.440 --> 38:10.000\n on making it more painless for us to buy your products.\n\n38:10.000 --> 38:12.280\n If we could just like linger a little bit\n\n38:12.280 --> 38:13.680\n on the social network thing,\n\n38:13.680 --> 38:18.200\n because there's this Netflix social dilemma.\n\n38:18.200 --> 38:19.280\n Yeah, no, I saw that.\n\n38:19.280 --> 38:22.620\n And Tristan Harris and company, yeah.\n\n38:24.000 --> 38:27.100\n And people's data,\n\n38:29.500 --> 38:31.560\n it's really sensitive and social networks\n\n38:31.560 --> 38:36.560\n are at the core arguably of many of societal like tension\n\n38:37.440 --> 38:39.640\n and some of the most important things happening in society.\n\n38:39.640 --> 38:42.040\n So it feels like it's important to get this right,\n\n38:42.040 --> 38:43.960\n both from a business model perspective\n\n38:43.960 --> 38:46.340\n and just like a trust perspective.\n\n38:46.340 --> 38:49.840\n I still gotta, I mean, it just still feels like,\n\n38:49.840 --> 38:52.140\n I know there's experimentation going on.\n\n38:52.140 --> 38:54.740\n It still feels like everyone is afraid\n\n38:54.740 --> 38:57.520\n to try different business models, like really try.\n\n38:57.520 --> 38:59.600\n Well, I'm worried that people are afraid\n\n38:59.600 --> 39:01.220\n to try different business models.\n\n39:01.220 --> 39:03.480\n I'm also worried that some of the business models\n\n39:03.480 --> 39:06.280\n may lead them to bad choices.\n\n39:06.280 --> 39:10.980\n And Danny Kahneman talks about system one and system two,\n\n39:10.980 --> 39:12.280\n sort of like a reptilian brain\n\n39:12.280 --> 39:14.360\n that reacts quickly to what we see,\n\n39:14.360 --> 39:16.160\n see something interesting, we click on it,\n\n39:16.160 --> 39:20.800\n we retweet it versus our system two,\n\n39:20.800 --> 39:24.080\n our frontal cortex that's supposed to be more careful\n\n39:24.080 --> 39:26.240\n and rational that really doesn't make\n\n39:26.240 --> 39:27.800\n as many decisions as it should.\n\n39:28.840 --> 39:32.680\n I think there's a tendency for a lot of these social networks\n\n39:32.680 --> 39:37.680\n to really exploit system one, our quick instant reaction,\n\n39:37.680 --> 39:40.960\n make it so we just click on stuff and pass it on\n\n39:40.960 --> 39:42.320\n and not really think carefully about it.\n\n39:42.320 --> 39:45.160\n And that system, it tends to be driven\n\n39:45.160 --> 39:50.160\n by sex, violence, disgust, anger, fear,\n\n39:51.320 --> 39:53.800\n these relatively primitive kinds of emotions.\n\n39:53.800 --> 39:55.960\n Maybe they're important for a lot of purposes,\n\n39:55.960 --> 39:58.920\n but they're not a great way to organize a society.\n\n39:58.920 --> 40:01.920\n And most importantly, when you think about this huge,\n\n40:01.920 --> 40:04.320\n amazing information infrastructure we've had\n\n40:04.320 --> 40:08.000\n that's connected billions of brains across the globe,\n\n40:08.000 --> 40:09.640\n not just so we can all access information,\n\n40:09.640 --> 40:12.640\n but we can all contribute to it and share it.\n\n40:12.640 --> 40:14.100\n Arguably the most important thing\n\n40:14.100 --> 40:19.100\n that that network should do is favor truth over falsehoods.\n\n40:19.360 --> 40:21.640\n And the way it's been designed,\n\n40:21.640 --> 40:24.660\n not necessarily intentionally, is exactly the opposite.\n\n40:24.660 --> 40:29.440\n My MIT colleagues are all, and Deb Roy and others at MIT,\n\n40:29.440 --> 40:31.760\n did a terrific paper in the cover of Science.\n\n40:31.760 --> 40:33.460\n And they documented what we all feared,\n\n40:33.460 --> 40:37.740\n which is that lies spread faster than truth\n\n40:37.740 --> 40:39.760\n on social networks.\n\n40:39.760 --> 40:42.760\n They looked at a bunch of tweets and retweets,\n\n40:42.760 --> 40:44.560\n and they found that false information\n\n40:44.560 --> 40:48.960\n was more likely to spread further, faster, to more people.\n\n40:48.960 --> 40:49.920\n And why was that?\n\n40:49.920 --> 40:53.360\n It's not because people like lies.\n\n40:53.360 --> 40:55.880\n It's because people like things that are shocking,\n\n40:55.880 --> 40:57.840\n amazing, can you believe this?\n\n40:57.840 --> 41:00.280\n Something that is not mundane,\n\n41:00.280 --> 41:02.460\n not something that everybody else already knew.\n\n41:02.460 --> 41:05.400\n And what are the most unbelievable things?\n\n41:05.400 --> 41:07.320\n Well, lies.\n\n41:07.320 --> 41:09.820\n And so if you wanna find something unbelievable,\n\n41:09.820 --> 41:10.660\n it's a lot easier to do that\n\n41:10.660 --> 41:12.440\n if you're not constrained by the truth.\n\n41:12.440 --> 41:15.640\n So they found that the emotional valence\n\n41:15.640 --> 41:17.960\n of false information was just much higher.\n\n41:17.960 --> 41:19.680\n It was more likely to be shocking,\n\n41:19.680 --> 41:21.680\n and therefore more likely to be spread.\n\n41:22.880 --> 41:24.040\n Another interesting thing was that\n\n41:24.040 --> 41:26.560\n that wasn't necessarily driven by the algorithms.\n\n41:27.580 --> 41:29.680\n I know that there is some evidence,\n\n41:29.680 --> 41:32.400\n Zeynep Tufekci and others have pointed out on YouTube,\n\n41:32.400 --> 41:34.680\n some of the algorithms unintentionally were tuned\n\n41:34.680 --> 41:37.840\n to amplify more extremist content.\n\n41:37.840 --> 41:42.420\n But in the study of Twitter that Sinan and Deb and others did,\n\n41:42.420 --> 41:44.480\n they found that even if you took out all the bots\n\n41:44.480 --> 41:47.880\n and all the automated tweets,\n\n41:47.880 --> 41:50.760\n you still had lies spreading significantly faster.\n\n41:50.760 --> 41:52.560\n It's just the problems with ourselves\n\n41:52.560 --> 41:57.080\n that we just can't resist passing on the salacious content.\n\n41:58.480 --> 41:59.920\n But I also blame the platforms\n\n41:59.920 --> 42:03.160\n because there's different ways you can design a platform.\n\n42:03.160 --> 42:05.400\n You can design a platform in a way\n\n42:05.400 --> 42:07.280\n that makes it easy to spread lies\n\n42:07.280 --> 42:09.520\n and to retweet and spread things on,\n\n42:09.520 --> 42:11.520\n or you can kind of put some friction on that\n\n42:11.520 --> 42:13.960\n and try to favor truth.\n\n42:13.960 --> 42:15.520\n I had dinner with Jimmy Wales once,\n\n42:15.520 --> 42:19.720\n the guy who helped found Wikipedia.\n\n42:19.720 --> 42:22.580\n And he convinced me that, look,\n\n42:22.580 --> 42:24.500\n you can make some design choices,\n\n42:24.500 --> 42:26.360\n whether it's at Facebook, at Twitter,\n\n42:26.360 --> 42:29.240\n at Wikipedia, or Reddit, whatever,\n\n42:29.240 --> 42:31.440\n and depending on how you make those choices,\n\n42:32.340 --> 42:35.080\n you're more likely or less likely to have false news.\n\n42:35.080 --> 42:37.160\n Create a little bit of friction, like you said.\n\n42:37.160 --> 42:38.000\n Yeah.\n\n42:38.000 --> 42:39.560\n You know, that's the, and so if I'm...\n\n42:39.560 --> 42:41.560\n It could be friction, it could be speeding the truth,\n\n42:41.560 --> 42:44.400\n either way, but, and I don't totally understand...\n\n42:44.400 --> 42:45.520\n Speeding the truth, I love it.\n\n42:45.520 --> 42:47.040\n Yeah, yeah.\n\n42:47.040 --> 42:48.900\n Amplifying it and giving it more credit.\n\n42:48.900 --> 42:52.520\n And in academia, which is far, far from perfect,\n\n42:52.520 --> 42:55.640\n but when someone has an important discovery,\n\n42:55.640 --> 42:56.880\n it tends to get more cited\n\n42:56.880 --> 42:58.160\n and people kind of look to it more\n\n42:58.160 --> 43:00.760\n and sort of, it tends to get amplified a little bit.\n\n43:00.760 --> 43:03.320\n So you could try to do that too.\n\n43:03.320 --> 43:04.680\n I don't know what the silver bullet is,\n\n43:04.680 --> 43:07.440\n but the meta point is that if we spend time\n\n43:07.440 --> 43:10.800\n thinking about it, we can amplify truth over falsehoods.\n\n43:10.800 --> 43:14.920\n And I'm disappointed in the heads of these social networks\n\n43:14.920 --> 43:16.680\n that they haven't been as successful\n\n43:16.680 --> 43:19.540\n or maybe haven't tried as hard to amplify truth.\n\n43:19.540 --> 43:21.560\n And part of it, going back to what we said earlier,\n\n43:21.560 --> 43:25.140\n is these revenue models may push them\n\n43:25.140 --> 43:29.880\n more towards growing fast, spreading information rapidly,\n\n43:29.880 --> 43:31.440\n getting lots of users,\n\n43:31.440 --> 43:34.560\n which isn't the same thing as finding truth.\n\n43:34.560 --> 43:38.840\n Yeah, I mean, implicit in what you're saying now\n\n43:38.840 --> 43:42.240\n is a hopeful message that with platforms,\n\n43:42.240 --> 43:47.240\n we can take a step towards a greater\n\n43:47.440 --> 43:51.120\n and greater popularity of truth.\n\n43:51.120 --> 43:54.000\n But the more cynical view is that\n\n43:54.000 --> 43:56.800\n what the last few years have revealed\n\n43:56.800 --> 43:59.020\n is that there's a lot of money to be made\n\n43:59.020 --> 44:03.120\n in dismantling even the idea of truth,\n\n44:03.120 --> 44:05.000\n that nothing is true.\n\n44:05.000 --> 44:07.020\n And as a thought experiment,\n\n44:07.020 --> 44:09.320\n I've been thinking about if it's possible\n\n44:09.320 --> 44:11.200\n that our future will have,\n\n44:11.200 --> 44:14.360\n like the idea of truth is something we won't even have.\n\n44:14.360 --> 44:17.800\n Do you think it's possible in the future\n\n44:17.800 --> 44:20.980\n that everything is on the table in terms of truth,\n\n44:20.980 --> 44:24.720\n and we're just swimming in this kind of digital economy\n\n44:24.720 --> 44:29.720\n where ideas are just little toys\n\n44:29.720 --> 44:33.080\n that are not at all connected to reality?\n\n44:33.080 --> 44:35.760\n Yeah, I think that's definitely possible.\n\n44:35.760 --> 44:37.960\n I'm not a technological determinist,\n\n44:37.960 --> 44:40.280\n so I don't think that's inevitable.\n\n44:40.280 --> 44:42.300\n I don't think it's inevitable that it doesn't happen.\n\n44:42.300 --> 44:43.960\n I mean, the thing that I've come away with\n\n44:43.960 --> 44:45.320\n every time I do these studies,\n\n44:45.320 --> 44:47.200\n and I emphasize it in my books and elsewhere,\n\n44:47.200 --> 44:50.040\n is that technology doesn't shape our destiny,\n\n44:50.040 --> 44:51.680\n we shape our destiny.\n\n44:51.680 --> 44:54.640\n So just by us having this conversation,\n\n44:54.640 --> 44:58.440\n I hope that your audience is gonna take it upon themselves\n\n44:58.440 --> 44:59.880\n as they design their products,\n\n44:59.880 --> 45:01.280\n and they think about, they use products,\n\n45:01.280 --> 45:02.760\n as they manage companies,\n\n45:02.760 --> 45:05.300\n how can they make conscious decisions\n\n45:05.300 --> 45:08.840\n to favor truth over falsehoods,\n\n45:08.840 --> 45:10.880\n favor the better kinds of societies,\n\n45:10.880 --> 45:13.720\n and not abdicate and say, well, we just build the tools.\n\n45:13.720 --> 45:15.600\n I think there was a saying that,\n\n45:16.940 --> 45:18.300\n was it the German scientist\n\n45:18.300 --> 45:23.000\n when they were working on the missiles in late World War II?\n\n45:23.000 --> 45:25.680\n They said, well, our job is to make the missiles go up.\n\n45:25.680 --> 45:28.400\n Where they come down, that's someone else's department.\n\n45:28.400 --> 45:31.840\n And that's obviously not the, I think it's obvious,\n\n45:31.840 --> 45:32.840\n that's not the right attitude\n\n45:32.840 --> 45:33.980\n that technologists should have,\n\n45:33.980 --> 45:35.680\n that engineers should have.\n\n45:35.680 --> 45:36.920\n They should be very conscious\n\n45:36.920 --> 45:38.800\n about what the implications are.\n\n45:38.800 --> 45:40.600\n And if we think carefully about it,\n\n45:40.600 --> 45:42.920\n we can avoid the kind of world that you just described,\n\n45:42.920 --> 45:45.040\n where truth is all relative.\n\n45:45.040 --> 45:47.840\n There are going to be people who benefit from a world\n\n45:47.840 --> 45:51.320\n of where people don't check facts,\n\n45:51.320 --> 45:52.680\n and where truth is relative,\n\n45:52.680 --> 45:57.680\n and popularity or fame or money is orthogonal to truth.\n\n45:59.880 --> 46:01.880\n But one of the reasons I suspect\n\n46:01.880 --> 46:04.540\n that we've had so much progress over the past few hundred\n\n46:04.540 --> 46:07.600\n years is the invention of the scientific method,\n\n46:07.600 --> 46:10.200\n which is a really powerful tool or meta tool\n\n46:10.200 --> 46:15.200\n for finding truth and favoring things that are true\n\n46:15.400 --> 46:16.600\n versus things that are false.\n\n46:16.600 --> 46:18.560\n If they don't pass the scientific method,\n\n46:18.560 --> 46:20.640\n they're less likely to be true.\n\n46:20.640 --> 46:25.560\n And that has, the societies and the people\n\n46:25.560 --> 46:27.760\n and the organizations that embrace that\n\n46:27.760 --> 46:30.520\n have done a lot better than the ones who haven't.\n\n46:30.520 --> 46:32.800\n And so I'm hoping that people keep that in mind\n\n46:32.800 --> 46:35.460\n and continue to try to embrace not just the truth,\n\n46:35.460 --> 46:37.640\n but methods that lead to the truth.\n\n46:37.640 --> 46:40.520\n So maybe on a more personal question,\n\n46:41.400 --> 46:45.480\n if one were to try to build a competitor to Twitter,\n\n46:45.480 --> 46:47.360\n what would you advise?\n\n46:47.360 --> 46:52.360\n Is there, I mean, the bigger, the meta question,\n\n46:53.360 --> 46:55.680\n is that the right way to improve systems?\n\n46:55.680 --> 46:59.380\n Yeah, no, I think that the underlying premise\n\n46:59.380 --> 47:01.380\n behind Twitter and all these networks is amazing,\n\n47:01.380 --> 47:02.800\n that we can communicate with each other.\n\n47:02.800 --> 47:04.000\n And I use it a lot.\n\n47:04.000 --> 47:05.920\n There's a subpart of Twitter called Econ Twitter,\n\n47:05.920 --> 47:08.640\n where we economists tweet to each other\n\n47:08.640 --> 47:10.560\n and talk about new papers.\n\n47:10.560 --> 47:11.960\n Something came out in the NBER,\n\n47:11.960 --> 47:13.320\n the National Bureau of Economic Research,\n\n47:13.320 --> 47:14.160\n and we share about it.\n\n47:14.160 --> 47:15.360\n People critique it.\n\n47:15.360 --> 47:16.880\n I think it's been a godsend\n\n47:16.880 --> 47:20.040\n because it's really sped up the scientific process,\n\n47:20.040 --> 47:21.880\n if you can call economic scientific.\n\n47:21.880 --> 47:23.560\n Does it get divisive in that little?\n\n47:23.560 --> 47:24.500\n Sometimes, yeah, sure.\n\n47:24.500 --> 47:25.340\n Sometimes it does.\n\n47:25.340 --> 47:28.360\n It can also be done in nasty ways and there's the bad parts.\n\n47:28.360 --> 47:29.680\n But the good parts are great\n\n47:29.680 --> 47:31.640\n because you just speed up that clock speed\n\n47:31.640 --> 47:33.320\n of learning about things.\n\n47:33.320 --> 47:35.480\n Instead of like in the old, old days,\n\n47:35.480 --> 47:36.800\n waiting to read it in a journal,\n\n47:36.800 --> 47:39.520\n or the not so old days when you'd see it posted\n\n47:39.520 --> 47:41.600\n on a website and you'd read it.\n\n47:41.600 --> 47:44.000\n Now on Twitter, people will distill it down\n\n47:44.000 --> 47:47.160\n and it's a real art to getting to the essence of things.\n\n47:47.160 --> 47:49.080\n So that's been great.\n\n47:49.080 --> 47:52.320\n But it certainly, we all know that Twitter\n\n47:52.320 --> 47:55.560\n can be a cesspool of misinformation.\n\n47:55.560 --> 47:57.360\n And like I just said,\n\n47:57.360 --> 48:00.240\n unfortunately misinformation tends to spread faster\n\n48:00.240 --> 48:02.320\n on Twitter than truth.\n\n48:02.320 --> 48:03.160\n And there are a lot of people\n\n48:03.160 --> 48:04.200\n who are very vulnerable to it.\n\n48:04.200 --> 48:06.000\n I'm sure I've been fooled at times.\n\n48:06.000 --> 48:09.120\n There are agents, whether from Russia\n\n48:09.120 --> 48:11.680\n or from political groups or others\n\n48:11.680 --> 48:15.640\n that explicitly create efforts at misinformation\n\n48:15.640 --> 48:17.900\n and efforts at getting people to hate each other.\n\n48:17.900 --> 48:19.720\n Or even more important lately I've discovered\n\n48:19.720 --> 48:21.200\n is nut picking.\n\n48:21.200 --> 48:22.320\n You know the idea of nut picking?\n\n48:22.320 --> 48:23.160\n No, what's that?\n\n48:23.160 --> 48:24.320\n It's a good term.\n\n48:24.320 --> 48:27.800\n Nut picking is when you find like an extreme nut case\n\n48:27.800 --> 48:30.700\n on the other side and then you amplify them\n\n48:30.700 --> 48:34.000\n and make it seem like that's typical of the other side.\n\n48:34.000 --> 48:35.480\n So you're not literally lying.\n\n48:35.480 --> 48:37.760\n You're taking some idiot, you know,\n\n48:37.760 --> 48:39.920\n renting on the subway or just, you know,\n\n48:39.920 --> 48:42.800\n whether they're in the KKK or Antifa or whatever,\n\n48:42.800 --> 48:44.360\n they're just, and you,\n\n48:44.360 --> 48:46.040\n normally nobody would pay attention to this guy.\n\n48:46.040 --> 48:48.080\n Like 12 people would see him and it'd be the end.\n\n48:48.080 --> 48:51.120\n Instead with video or whatever,\n\n48:51.120 --> 48:54.520\n you get tens of millions of people say it.\n\n48:54.520 --> 48:56.320\n And I've seen this, you know, I look at it,\n\n48:56.320 --> 48:57.160\n I'm like, I get angry.\n\n48:57.160 --> 48:58.280\n I'm like, I can't believe that person\n\n48:58.280 --> 48:59.720\n did something so terrible.\n\n48:59.720 --> 49:02.880\n Let me tell all my friends about this terrible person.\n\n49:02.880 --> 49:06.640\n And it's a great way to generate division.\n\n49:06.640 --> 49:10.520\n I talked to a friend who studied Russian misinformation\n\n49:10.520 --> 49:13.840\n campaigns, and they're very clever about literally\n\n49:13.840 --> 49:15.880\n being on both sides of some of these debates.\n\n49:15.880 --> 49:18.620\n They would have some people pretend to be part of BLM.\n\n49:18.620 --> 49:21.040\n Some people pretend to be white nationalists\n\n49:21.040 --> 49:22.960\n and they would be throwing epithets at each other,\n\n49:22.960 --> 49:25.100\n saying crazy things at each other.\n\n49:25.100 --> 49:26.600\n And they're literally playing both sides of it,\n\n49:26.600 --> 49:28.600\n but their goal wasn't for one or the other to win.\n\n49:28.600 --> 49:30.120\n It was for everybody to get behaving\n\n49:30.120 --> 49:32.000\n and distrusting everyone else.\n\n49:32.000 --> 49:34.500\n So these tools can definitely be used for that.\n\n49:34.500 --> 49:36.580\n And they are being used for that.\n\n49:36.580 --> 49:39.680\n It's been super destructive for our democracy\n\n49:39.680 --> 49:41.080\n and our society.\n\n49:41.080 --> 49:43.540\n And the people who run these platforms,\n\n49:43.540 --> 49:46.100\n I think have a social responsibility,\n\n49:46.100 --> 49:48.680\n a moral and ethical, personal responsibility\n\n49:48.680 --> 49:51.800\n to do a better job and to shut that stuff down.\n\n49:51.800 --> 49:52.960\n Well, I don't know if you can shut it down,\n\n49:52.960 --> 49:55.800\n but to design them in a way that, you know,\n\n49:55.800 --> 49:58.620\n as I said earlier, favors truth over falsehoods\n\n49:58.620 --> 50:01.320\n and favors positive types of\n\n50:03.200 --> 50:06.060\n communication versus destructive ones.\n\n50:06.060 --> 50:09.600\n And just like you said, it's also on us.\n\n50:09.600 --> 50:12.400\n I try to be all about love and compassion,\n\n50:12.400 --> 50:13.280\n empathy on Twitter.\n\n50:13.280 --> 50:14.820\n I mean, one of the things,\n\n50:14.820 --> 50:16.600\n nut picking is a fascinating term.\n\n50:16.600 --> 50:18.940\n One of the things that people do,\n\n50:18.940 --> 50:21.800\n that's I think even more dangerous\n\n50:21.800 --> 50:26.760\n is nut picking applied to individual statements\n\n50:26.760 --> 50:28.440\n of good people.\n\n50:28.440 --> 50:32.180\n So basically worst case analysis in computer science\n\n50:32.180 --> 50:35.360\n is taking sometimes out of context,\n\n50:35.360 --> 50:37.040\n but sometimes in context,\n\n50:38.480 --> 50:42.320\n a statement, one statement by a person,\n\n50:42.320 --> 50:43.740\n like I've been, because I've been reading\n\n50:43.740 --> 50:45.360\n The Rise and Fall of the Third Reich,\n\n50:45.360 --> 50:48.960\n I often talk about Hitler on this podcast with folks\n\n50:48.960 --> 50:50.640\n and it is so easy.\n\n50:50.640 --> 50:52.060\n That's really dangerous.\n\n50:52.060 --> 50:54.560\n But I'm all leaning in, I'm 100%.\n\n50:54.560 --> 50:56.960\n Because, well, it's actually a safer place\n\n50:56.960 --> 50:59.200\n than people realize because it's history\n\n50:59.200 --> 51:04.120\n and history in long form is actually very fascinating\n\n51:04.120 --> 51:06.300\n to think about and it's,\n\n51:06.300 --> 51:09.600\n but I could see how that could be taken\n\n51:09.600 --> 51:11.320\n totally out of context and it's very worrying.\n\n51:11.320 --> 51:12.800\n You know, these digital infrastructures,\n\n51:12.800 --> 51:14.040\n not just they disseminate things,\n\n51:14.040 --> 51:14.880\n but they're sort of permanent.\n\n51:14.880 --> 51:16.540\n So anything you say at some point,\n\n51:16.540 --> 51:18.160\n someone can go back and find something you said\n\n51:18.160 --> 51:21.080\n three years ago, perhaps jokingly, perhaps not,\n\n51:21.080 --> 51:22.800\n maybe you're just wrong and you made them, you know,\n\n51:22.800 --> 51:25.600\n and like that becomes, they can use that to define you\n\n51:25.600 --> 51:26.840\n if they have ill intent.\n\n51:26.840 --> 51:29.080\n And we all need to be a little more forgiving.\n\n51:29.080 --> 51:32.240\n I mean, somewhere in my 20s, I told myself,\n\n51:32.240 --> 51:33.820\n I was going through all my different friends\n\n51:33.820 --> 51:37.300\n and I was like, you know, every one of them\n\n51:37.300 --> 51:39.400\n has at least like one nutty opinion.\n\n51:39.400 --> 51:42.040\n And I was like, there's like nobody\n\n51:42.040 --> 51:44.160\n who's like completely, except me, of course,\n\n51:44.160 --> 51:45.700\n but I'm sure they thought that about me too.\n\n51:45.700 --> 51:47.760\n And so you just kind of like learned\n\n51:47.760 --> 51:49.420\n to be a little bit tolerant that like, okay,\n\n51:49.420 --> 51:51.140\n there's just, you know.\n\n51:51.140 --> 51:55.240\n Yeah, I wonder who the responsibility lays on there.\n\n51:55.240 --> 51:59.680\n Like, I think ultimately it's about leadership.\n\n51:59.680 --> 52:02.760\n Like the previous president, Barack Obama,\n\n52:02.760 --> 52:06.040\n has been, I think, quite eloquent\n\n52:06.040 --> 52:07.680\n at walking this very difficult line\n\n52:07.680 --> 52:10.640\n of talking about cancel culture, but it's a difficult,\n\n52:10.640 --> 52:12.160\n it takes skill.\n\n52:12.160 --> 52:13.800\n Because you say the wrong thing\n\n52:13.800 --> 52:15.320\n and you piss off a lot of people.\n\n52:15.320 --> 52:17.440\n And so you have to do it well.\n\n52:17.440 --> 52:20.000\n But then also the platform of the technology is,\n\n52:21.220 --> 52:23.600\n should slow down, create friction,\n\n52:23.600 --> 52:26.440\n and spreading this kind of nut picking in all its forms.\n\n52:26.440 --> 52:27.280\n Absolutely.\n\n52:27.280 --> 52:29.780\n No, and your point that we have to like learn over time,\n\n52:29.780 --> 52:30.620\n how to manage it.\n\n52:30.620 --> 52:31.800\n I mean, we can't put it all on the platform\n\n52:31.800 --> 52:33.240\n and say, you guys design it.\n\n52:33.240 --> 52:35.200\n Because if we're idiots about using it,\n\n52:35.200 --> 52:38.480\n nobody can design a platform that withstands that.\n\n52:38.480 --> 52:41.720\n And every new technology people learn its dangers.\n\n52:41.720 --> 52:43.960\n You know, when someone invented fire,\n\n52:43.960 --> 52:44.960\n it's great cooking and everything,\n\n52:44.960 --> 52:46.160\n but then somebody burned themself.\n\n52:46.160 --> 52:48.200\n And then you had to like learn how to like avoid,\n\n52:48.200 --> 52:50.640\n maybe somebody invented a fire extinguisher later.\n\n52:50.640 --> 52:52.840\n So you kind of like figure out ways\n\n52:52.840 --> 52:54.640\n of working around these technologies.\n\n52:54.640 --> 52:57.440\n Someone invented seat belts, et cetera.\n\n52:57.440 --> 52:58.640\n And that's certainly true\n\n52:58.640 --> 53:00.620\n with all the new digital technologies\n\n53:00.620 --> 53:02.320\n that we have to figure out,\n\n53:02.320 --> 53:05.280\n not just technologies that protect us,\n\n53:05.280 --> 53:08.640\n but ways of using them that emphasize\n\n53:08.640 --> 53:11.520\n that are more likely to be successful than dangerous.\n\n53:11.520 --> 53:12.560\n So you've written quite a bit\n\n53:12.560 --> 53:16.020\n about how artificial intelligence might change our world.\n\n53:19.000 --> 53:21.240\n How do you think if we look forward,\n\n53:21.240 --> 53:23.200\n again, it's impossible to predict the future,\n\n53:23.200 --> 53:26.440\n but if we look at trends from the past\n\n53:26.440 --> 53:28.200\n and we tried to predict what's gonna happen\n\n53:28.200 --> 53:29.720\n in the rest of the 21st century,\n\n53:29.720 --> 53:31.840\n how do you think AI will change our world?\n\n53:33.080 --> 53:34.200\n That's a big question.\n\n53:34.200 --> 53:37.440\n You know, I'm mostly a techno optimist.\n\n53:37.440 --> 53:38.660\n I'm not at the extreme, you know,\n\n53:38.660 --> 53:41.080\n the singularity is near end of the spectrum,\n\n53:41.080 --> 53:44.560\n but I do think that we're likely in\n\n53:44.560 --> 53:47.480\n for some significantly improved living standards,\n\n53:47.480 --> 53:49.260\n some really important progress,\n\n53:49.260 --> 53:51.240\n even just the technologies that are already kind of like\n\n53:51.240 --> 53:53.080\n in the can that haven't diffused.\n\n53:53.080 --> 53:54.880\n You know, when I talked earlier about the J curve,\n\n53:54.880 --> 53:58.760\n it could take 10, 20, 30 years for an existing technology\n\n53:58.760 --> 54:00.780\n to have the kind of profound effects.\n\n54:00.780 --> 54:03.760\n And when I look at whether it's, you know,\n\n54:03.760 --> 54:07.840\n vision systems, voice recognition, problem solving systems,\n\n54:07.840 --> 54:09.400\n even if nothing new got invented,\n\n54:09.400 --> 54:11.800\n we would have a few decades of progress.\n\n54:11.800 --> 54:13.440\n So I'm excited about that.\n\n54:13.440 --> 54:16.840\n And I think that's gonna lead to us being wealthier,\n\n54:16.840 --> 54:17.800\n healthier, I mean,\n\n54:17.800 --> 54:19.520\n the healthcare is probably one of the applications\n\n54:19.520 --> 54:21.320\n that I'm most excited about.\n\n54:22.520 --> 54:23.760\n So that's good news.\n\n54:23.760 --> 54:26.760\n I don't think we're gonna have the end of work anytime soon.\n\n54:26.760 --> 54:30.960\n There's just too many things that machines still can't do.\n\n54:30.960 --> 54:32.000\n When I look around the world\n\n54:32.000 --> 54:34.640\n and think of whether it's childcare or healthcare,\n\n54:34.640 --> 54:37.740\n cleaning the environment, interacting with people,\n\n54:37.740 --> 54:40.900\n scientific work, artistic creativity,\n\n54:40.900 --> 54:42.560\n these are things that for now,\n\n54:42.560 --> 54:45.640\n machines aren't able to do nearly as well as humans,\n\n54:45.640 --> 54:47.160\n even just something as mundane as, you know,\n\n54:47.160 --> 54:48.720\n folding laundry or whatever.\n\n54:48.720 --> 54:52.920\n And many of these, I think are gonna be years or decades\n\n54:52.920 --> 54:54.720\n before machines catch up.\n\n54:54.720 --> 54:56.120\n You know, I may be surprised on some of them,\n\n54:56.120 --> 54:58.760\n but overall, I think there's plenty of work\n\n54:58.760 --> 54:59.760\n for humans to do.\n\n54:59.760 --> 55:01.320\n There's plenty of problems in society\n\n55:01.320 --> 55:02.560\n that need the human touch.\n\n55:02.560 --> 55:04.180\n So we'll have to repurpose.\n\n55:04.180 --> 55:07.880\n We'll have to, as machines are able to do some tasks,\n\n55:07.880 --> 55:11.040\n people are gonna have to reskill and move into other areas.\n\n55:11.040 --> 55:12.740\n And that's probably what's gonna be going on\n\n55:12.740 --> 55:16.240\n for the next, you know, 10, 20, 30 years or more,\n\n55:16.240 --> 55:18.920\n kind of big restructuring of society.\n\n55:18.920 --> 55:22.420\n We'll get wealthier and people will have to do new skills.\n\n55:22.420 --> 55:24.360\n Now, if you turn the dial further, I don't know,\n\n55:24.360 --> 55:26.960\n 50 or a hundred years into the future,\n\n55:26.960 --> 55:29.640\n then, you know, maybe all bets are off.\n\n55:29.640 --> 55:32.880\n Then it's possible that machines will be able to do\n\n55:32.880 --> 55:34.240\n most of what people do.\n\n55:34.240 --> 55:37.360\n You know, say one or 200 years, I think it's even likely.\n\n55:37.360 --> 55:38.400\n And at that point,\n\n55:38.400 --> 55:41.040\n then we're more in the sort of abundance economy.\n\n55:41.040 --> 55:44.040\n Then we're in a world where there's really little\n\n55:44.040 --> 55:48.000\n for the humans can do economically better than machines,\n\n55:48.000 --> 55:49.900\n other than be human.\n\n55:49.900 --> 55:53.640\n And, you know, that will take a transition as well,\n\n55:53.640 --> 55:56.480\n kind of more of a transition of how we get meaning in life\n\n55:56.480 --> 55:58.220\n and what our values are.\n\n55:58.220 --> 56:00.400\n But shame on us if we screw that up.\n\n56:00.400 --> 56:02.720\n I mean, that should be like great, great news.\n\n56:02.720 --> 56:04.520\n And it kind of saddens me that some people see that\n\n56:04.520 --> 56:05.540\n as like a big problem.\n\n56:05.540 --> 56:07.640\n I think that would be, should be wonderful\n\n56:07.640 --> 56:10.420\n if people have all the health and material things\n\n56:10.420 --> 56:14.180\n that they need and can focus on loving each other\n\n56:14.180 --> 56:16.840\n and discussing philosophy and playing\n\n56:16.840 --> 56:19.440\n and doing all the other things that don't require work.\n\n56:19.440 --> 56:23.960\n Do you think you'd be surprised to see what the 20,\n\n56:23.960 --> 56:27.420\n if we were to travel in time, 100 years into the future,\n\n56:27.420 --> 56:29.560\n do you think you'll be able to,\n\n56:29.560 --> 56:32.300\n like if I gave you a month to like talk to people,\n\n56:32.300 --> 56:34.120\n no, like let's say a week,\n\n56:34.120 --> 56:37.800\n would you be able to understand what the hell's going on?\n\n56:37.800 --> 56:39.200\n You mean if I was there for a week?\n\n56:39.200 --> 56:40.840\n Yeah, if you were there for a week.\n\n56:40.840 --> 56:42.120\n A hundred years in the future?\n\n56:42.120 --> 56:43.000\n Yeah.\n\n56:43.000 --> 56:46.600\n So like, so I'll give you one thought experiment is like,\n\n56:46.600 --> 56:49.640\n isn't it possible that we're all living in virtual reality\n\n56:49.640 --> 56:50.480\n by then?\n\n56:50.480 --> 56:52.620\n Yeah, no, I think that's very possible.\n\n56:52.620 --> 56:54.640\n I've played around with some of those VR headsets\n\n56:54.640 --> 56:55.480\n and they're not great,\n\n56:55.480 --> 57:00.480\n but I mean the average person spends many waking hours\n\n57:00.960 --> 57:03.320\n staring at screens right now.\n\n57:03.320 --> 57:05.720\n They're kind of low res compared to what they could be\n\n57:05.720 --> 57:10.680\n in 30 or 50 years, but certainly games\n\n57:10.680 --> 57:15.360\n and why not any other interactions could be done with VR?\n\n57:15.360 --> 57:16.320\n And that would be a pretty different world\n\n57:16.320 --> 57:19.520\n and we'd all, in some ways be as rich as we wanted.\n\n57:19.520 --> 57:21.360\n We could have castles and we could be traveling\n\n57:21.360 --> 57:25.960\n anywhere we want and it could obviously be multisensory.\n\n57:25.960 --> 57:29.560\n So that would be possible and of course there's people,\n\n57:30.880 --> 57:33.360\n you've had Elon Musk on and others, there are people,\n\n57:33.360 --> 57:35.380\n Nick Bostrom makes the simulation argument\n\n57:35.380 --> 57:36.760\n that maybe we're already there.\n\n57:36.760 --> 57:37.720\n We're already there.\n\n57:37.720 --> 57:41.200\n So, but in general, or do you not even think about\n\n57:41.200 --> 57:45.080\n in this kind of way, you're self critically thinking,\n\n57:45.080 --> 57:48.560\n how good are you as an economist at predicting\n\n57:48.560 --> 57:50.340\n what the future looks like?\n\n57:50.340 --> 57:51.180\n Do you have a?\n\n57:51.180 --> 57:52.000\n Well, it starts getting, I mean,\n\n57:52.000 --> 57:55.960\n I feel reasonably comfortable the next five, 10, 20 years\n\n57:55.960 --> 57:58.720\n in terms of that path.\n\n57:58.720 --> 58:01.720\n When you start getting truly superhuman\n\n58:01.720 --> 58:06.000\n artificial intelligence, kind of by definition,\n\n58:06.000 --> 58:07.040\n be able to think of a lot of things\n\n58:07.040 --> 58:09.080\n that I couldn't have thought of and create a world\n\n58:09.080 --> 58:10.960\n that I couldn't even imagine.\n\n58:10.960 --> 58:15.240\n And so I'm not sure I can predict what that world\n\n58:15.240 --> 58:16.520\n is going to be like.\n\n58:16.520 --> 58:19.840\n One thing that AI researchers, AI safety researchers\n\n58:19.840 --> 58:22.540\n worry about is what's called the alignment problem.\n\n58:22.540 --> 58:25.080\n When an AI is that powerful,\n\n58:25.080 --> 58:27.960\n then they can do all sorts of things.\n\n58:27.960 --> 58:30.560\n And you really hope that their values\n\n58:30.560 --> 58:32.440\n are aligned with our values.\n\n58:32.440 --> 58:34.480\n And it's even tricky to finding what our values are.\n\n58:34.480 --> 58:37.220\n I mean, first off, we all have different values.\n\n58:37.220 --> 58:40.440\n And secondly, maybe if we were smarter,\n\n58:40.440 --> 58:41.620\n we would have better values.\n\n58:41.620 --> 58:44.200\n Like, I like to think that we have better values\n\n58:44.200 --> 58:49.200\n than we did in 1860 and, or in the year 200 BC\n\n58:50.320 --> 58:51.360\n on a lot of dimensions,\n\n58:51.360 --> 58:53.440\n things that we consider barbaric today.\n\n58:53.440 --> 58:56.080\n And it may be that if I thought about it more deeply,\n\n58:56.080 --> 58:57.400\n I would also be morally evolved.\n\n58:57.400 --> 59:00.120\n Maybe I'd be a vegetarian or do other things\n\n59:00.120 --> 59:02.980\n that right now, whether my future self\n\n59:02.980 --> 59:05.240\n would consider kind of immoral.\n\n59:05.240 --> 59:07.740\n So that's a tricky problem,\n\n59:07.740 --> 59:11.120\n getting the AI to do what we want,\n\n59:11.120 --> 59:12.960\n assuming it's even a friendly AI.\n\n59:12.960 --> 59:14.780\n I mean, I should probably mention\n\n59:14.780 --> 59:17.100\n there's a nontrivial other branch\n\n59:17.100 --> 59:18.720\n where we destroy ourselves, right?\n\n59:18.720 --> 59:22.040\n I mean, there's a lot of exponentially improving\n\n59:22.040 --> 59:26.640\n technologies that could be ferociously destructive,\n\n59:26.640 --> 59:29.480\n whether it's in nanotechnology or biotech\n\n59:29.480 --> 59:34.280\n and weaponized viruses, AI and other things that.\n\n59:34.280 --> 59:35.120\n nuclear weapons.\n\n59:35.120 --> 59:36.240\n Nuclear weapons, of course.\n\n59:36.240 --> 59:37.320\n The old school technology.\n\n59:37.320 --> 59:42.040\n Yeah, good old nuclear weapons that could be devastating\n\n59:42.040 --> 59:45.240\n or even existential and new things yet to be invented.\n\n59:45.240 --> 59:50.240\n So that's a branch that I think is pretty significant.\n\n59:52.200 --> 59:54.260\n And there are those who think that one of the reasons\n\n59:54.260 --> 59:57.480\n we haven't been contacted by other civilizations, right?\n\n59:57.480 --> 1:00:01.560\n Is that once you get to a certain level of complexity\n\n1:00:01.560 --> 1:00:04.640\n in technology, there's just too many ways to go wrong.\n\n1:00:04.640 --> 1:00:06.200\n There's a lot of ways to blow yourself up.\n\n1:00:06.200 --> 1:00:09.640\n And people, or I should say species,\n\n1:00:09.640 --> 1:00:12.520\n end up falling into one of those traps.\n\n1:00:12.520 --> 1:00:13.580\n The great filter.\n\n1:00:13.580 --> 1:00:14.960\n The great filter.\n\n1:00:14.960 --> 1:00:16.720\n I mean, there's an optimistic view of that.\n\n1:00:16.720 --> 1:00:19.380\n If there is literally no intelligent life out there\n\n1:00:19.380 --> 1:00:22.340\n in the universe, or at least in our galaxy,\n\n1:00:22.340 --> 1:00:25.140\n that means that we've passed at least one\n\n1:00:25.140 --> 1:00:27.840\n of the great filters or some of the great filters\n\n1:00:27.840 --> 1:00:30.040\n that we survived.\n\n1:00:30.040 --> 1:00:32.240\n Yeah, no, I think Robin Hansen has a good way of,\n\n1:00:32.240 --> 1:00:33.920\n maybe others have a good way of thinking about this,\n\n1:00:33.920 --> 1:00:38.920\n that if there are no other intelligence creatures out there\n\n1:00:38.920 --> 1:00:40.640\n that we've been able to detect,\n\n1:00:40.640 --> 1:00:43.440\n one possibility is that there's a filter ahead of us.\n\n1:00:43.440 --> 1:00:44.780\n And when you get a little more advanced,\n\n1:00:44.780 --> 1:00:47.600\n maybe in a hundred or a thousand or 10,000 years,\n\n1:00:47.600 --> 1:00:50.560\n things just get destroyed for some reason.\n\n1:00:50.560 --> 1:00:53.000\n The other one is the great filters behind us.\n\n1:00:53.000 --> 1:00:57.700\n That'll be good, is that most planets don't even evolve life\n\n1:00:57.700 --> 1:00:58.920\n or if they don't evolve life,\n\n1:00:58.920 --> 1:01:00.280\n they don't evolve intelligent life.\n\n1:01:00.280 --> 1:01:02.040\n Maybe we've gotten past that.\n\n1:01:02.040 --> 1:01:03.960\n And so now maybe we're on the good side\n\n1:01:03.960 --> 1:01:05.680\n of the great filter.\n\n1:01:05.680 --> 1:01:10.480\n So if we sort of rewind back and look at the thing\n\n1:01:10.480 --> 1:01:12.760\n where we could say something a little bit more comfortably\n\n1:01:12.760 --> 1:01:14.460\n at five years and 10 years out,\n\n1:01:15.860 --> 1:01:20.200\n you've written about jobs\n\n1:01:20.200 --> 1:01:24.680\n and the impact on sort of our economy and the jobs\n\n1:01:24.680 --> 1:01:28.240\n in terms of artificial intelligence that it might have.\n\n1:01:28.240 --> 1:01:30.560\n It's a fascinating question of what kind of jobs are safe,\n\n1:01:30.560 --> 1:01:32.520\n what kind of jobs are not.\n\n1:01:32.520 --> 1:01:34.560\n Can you maybe speak to your intuition\n\n1:01:34.560 --> 1:01:38.320\n about how we should think about AI changing\n\n1:01:38.320 --> 1:01:39.940\n the landscape of work?\n\n1:01:39.940 --> 1:01:40.880\n Sure, absolutely.\n\n1:01:40.880 --> 1:01:42.600\n Well, this is a really important question\n\n1:01:42.600 --> 1:01:43.900\n because I think we're very far\n\n1:01:43.900 --> 1:01:45.720\n from artificial general intelligence,\n\n1:01:45.720 --> 1:01:48.120\n which is AI that can just do the full breadth\n\n1:01:48.120 --> 1:01:49.520\n of what humans can do.\n\n1:01:49.520 --> 1:01:52.980\n But we do have human level or superhuman level\n\n1:01:52.980 --> 1:01:56.800\n narrow intelligence, narrow artificial intelligence.\n\n1:01:56.800 --> 1:01:59.880\n And obviously my calculator can do math a lot better\n\n1:01:59.880 --> 1:02:00.720\n than I can.\n\n1:02:00.720 --> 1:02:01.560\n And there's a lot of other things\n\n1:02:01.560 --> 1:02:03.160\n that machines can do better than I can.\n\n1:02:03.160 --> 1:02:04.440\n So which is which?\n\n1:02:04.440 --> 1:02:06.860\n We actually set out to address that question\n\n1:02:06.860 --> 1:02:08.160\n with Tom Mitchell.\n\n1:02:08.160 --> 1:02:12.160\n I wrote a paper called what can machine learning do\n\n1:02:12.160 --> 1:02:13.440\n that was in science.\n\n1:02:13.440 --> 1:02:16.840\n And we went and interviewed a whole bunch of AI experts\n\n1:02:16.840 --> 1:02:20.440\n and kind of synthesized what they thought machine learning\n\n1:02:20.440 --> 1:02:22.220\n was good at and wasn't good at.\n\n1:02:22.220 --> 1:02:25.540\n And we came up with what we called a rubric,\n\n1:02:25.540 --> 1:02:28.160\n basically a set of questions you can ask about any task\n\n1:02:28.160 --> 1:02:30.960\n that will tell you whether it's likely to score high or low\n\n1:02:30.960 --> 1:02:33.720\n on suitability for machine learning.\n\n1:02:33.720 --> 1:02:34.760\n And then we've applied that\n\n1:02:34.760 --> 1:02:36.940\n to a bunch of tasks in the economy.\n\n1:02:36.940 --> 1:02:39.080\n In fact, there's a data set of all the tasks\n\n1:02:39.080 --> 1:02:41.600\n in the US economy, believe it or not, it's called ONET.\n\n1:02:41.600 --> 1:02:43.120\n The US government put it together,\n\n1:02:43.120 --> 1:02:45.000\n part of the Bureau of Labor Statistics.\n\n1:02:45.000 --> 1:02:48.680\n They divide the economy into about 970 occupations\n\n1:02:48.680 --> 1:02:52.140\n like bus driver, economist, primary school teacher,\n\n1:02:52.140 --> 1:02:54.800\n radiologist, and then for each one of them,\n\n1:02:54.800 --> 1:02:57.580\n they describe which tasks need to be done.\n\n1:02:57.580 --> 1:03:00.720\n Like for radiologists, there are 27 distinct tasks.\n\n1:03:00.720 --> 1:03:02.160\n So we went through all those tasks\n\n1:03:02.160 --> 1:03:04.960\n to see whether or not a machine could do them.\n\n1:03:04.960 --> 1:03:06.680\n And what we found interestingly was...\n\n1:03:06.680 --> 1:03:08.880\n Brilliant study by the way, that's so awesome.\n\n1:03:08.880 --> 1:03:10.240\n Yeah, thank you.\n\n1:03:10.240 --> 1:03:13.760\n So what we found was that there was no occupation\n\n1:03:13.760 --> 1:03:16.240\n in our data set where machine learning just ran the table\n\n1:03:16.240 --> 1:03:17.520\n and did everything.\n\n1:03:17.520 --> 1:03:18.980\n And there was almost no occupation\n\n1:03:18.980 --> 1:03:19.900\n where machine learning didn't have\n\n1:03:19.900 --> 1:03:22.120\n like a significant ability to do things.\n\n1:03:22.120 --> 1:03:24.360\n Like take radiology, a lot of people I hear saying,\n\n1:03:24.360 --> 1:03:26.680\n you know, it's the end of radiology.\n\n1:03:26.680 --> 1:03:29.880\n And one of the 27 tasks is read medical images.\n\n1:03:29.880 --> 1:03:31.960\n Really important one, like it's kind of a core job.\n\n1:03:31.960 --> 1:03:34.640\n And machines have basically gotten as good\n\n1:03:34.640 --> 1:03:35.880\n or better than radiologists.\n\n1:03:35.880 --> 1:03:38.360\n There was just an article in Nature last week,\n\n1:03:38.360 --> 1:03:41.160\n but they've been publishing them for the past few years\n\n1:03:42.440 --> 1:03:46.480\n showing that machine learning can do as well as humans\n\n1:03:46.480 --> 1:03:49.600\n on many kinds of diagnostic imaging tasks.\n\n1:03:49.600 --> 1:03:51.120\n But other things that radiologists do,\n\n1:03:51.120 --> 1:03:54.440\n they sometimes administer conscious sedation.\n\n1:03:54.440 --> 1:03:55.940\n They sometimes do physical exams.\n\n1:03:55.940 --> 1:03:57.320\n They have to synthesize the results\n\n1:03:57.320 --> 1:04:01.680\n and explain it to the other doctors or to the patients.\n\n1:04:01.680 --> 1:04:02.520\n In all those categories,\n\n1:04:02.520 --> 1:04:05.560\n machine learning isn't really up to snuff yet.\n\n1:04:05.560 --> 1:04:09.300\n So that job, we're gonna see a lot of restructuring.\n\n1:04:09.300 --> 1:04:11.400\n Parts of the job, they'll hand over to machines.\n\n1:04:11.400 --> 1:04:13.160\n Others, humans will do more of.\n\n1:04:13.160 --> 1:04:15.080\n That's been more or less the pattern all of them.\n\n1:04:15.080 --> 1:04:17.080\n So, you know, to oversimplify a bit,\n\n1:04:17.080 --> 1:04:19.080\n we're gonna see a lot of restructuring,\n\n1:04:19.080 --> 1:04:20.400\n reorganization of work.\n\n1:04:20.400 --> 1:04:22.300\n And it's real gonna be a great time.\n\n1:04:22.300 --> 1:04:24.720\n It is a great time for smart entrepreneurs and managers\n\n1:04:24.720 --> 1:04:27.280\n to do that reinvention of work.\n\n1:04:27.280 --> 1:04:29.420\n I'm not gonna see mass unemployment.\n\n1:04:30.600 --> 1:04:33.120\n To get more specifically to your question,\n\n1:04:33.120 --> 1:04:36.560\n the kinds of tasks that machines tend to be good at\n\n1:04:36.560 --> 1:04:39.040\n are a lot of routine problem solving,\n\n1:04:39.040 --> 1:04:42.560\n mapping inputs X into outputs Y.\n\n1:04:42.560 --> 1:04:44.840\n If you have a lot of data on the Xs and the Ys,\n\n1:04:44.840 --> 1:04:45.680\n the inputs and the outputs,\n\n1:04:45.680 --> 1:04:48.520\n you can do that kind of mapping and find the relationships.\n\n1:04:48.520 --> 1:04:50.660\n They tend to not be very good at,\n\n1:04:50.660 --> 1:04:53.680\n even now, fine motor control and dexterity.\n\n1:04:53.680 --> 1:04:58.680\n Emotional intelligence and human interactions\n\n1:04:58.960 --> 1:05:01.700\n and thinking outside the box, creative work.\n\n1:05:01.700 --> 1:05:03.220\n If you give it a well structured task,\n\n1:05:03.220 --> 1:05:05.040\n machines can be very good at it.\n\n1:05:05.040 --> 1:05:08.680\n But even asking the right questions, that's hard.\n\n1:05:08.680 --> 1:05:10.680\n There's a quote that Andrew McAfee and I use\n\n1:05:10.680 --> 1:05:12.980\n in our book, Second Machine Age.\n\n1:05:12.980 --> 1:05:16.840\n Apparently Pablo Picasso was shown an early computer\n\n1:05:16.840 --> 1:05:18.460\n and he came away kind of unimpressed.\n\n1:05:18.460 --> 1:05:20.660\n He goes, well, I don't see all the fusses.\n\n1:05:20.660 --> 1:05:23.900\n All that does is answer questions.\n\n1:05:23.900 --> 1:05:26.740\n And to him, the interesting thing was asking the questions.\n\n1:05:26.740 --> 1:05:31.260\n Yeah, try to replace me, GPT3, I dare you.\n\n1:05:31.260 --> 1:05:33.160\n Although some people think I'm a robot.\n\n1:05:33.160 --> 1:05:35.360\n You have this cool plot that shows,\n\n1:05:37.020 --> 1:05:39.640\n I just remember where economists land,\n\n1:05:39.640 --> 1:05:43.380\n where I think the X axis is the income.\n\n1:05:43.380 --> 1:05:46.220\n And then the Y axis is, I guess,\n\n1:05:46.220 --> 1:05:49.380\n aggregating the information of how replaceable the job is.\n\n1:05:49.380 --> 1:05:50.780\n Or I think there's an index.\n\n1:05:50.780 --> 1:05:51.620\n There's a suitability for machine learning index.\n\n1:05:51.620 --> 1:05:52.460\n Exactly.\n\n1:05:52.460 --> 1:05:55.300\n So we have all 970 occupations on that chart.\n\n1:05:55.300 --> 1:05:56.500\n It's a cool plot.\n\n1:05:56.500 --> 1:05:59.200\n And there's scatters in all four corners\n\n1:05:59.200 --> 1:06:01.040\n have some occupations.\n\n1:06:01.040 --> 1:06:02.700\n But there is a definite pattern,\n\n1:06:02.700 --> 1:06:05.660\n which is the lower wage occupations tend to have more tasks\n\n1:06:05.660 --> 1:06:07.960\n that are suitable for machine learning, like cashiers.\n\n1:06:07.960 --> 1:06:10.400\n I mean, anyone who's gone to a supermarket or CVS\n\n1:06:10.400 --> 1:06:12.380\n knows that they not only read barcodes,\n\n1:06:12.380 --> 1:06:14.520\n but they can recognize an apple and an orange\n\n1:06:14.520 --> 1:06:19.520\n and a lot of things cashiers, humans used to be needed for.\n\n1:06:19.520 --> 1:06:21.020\n At the other end of the spectrum,\n\n1:06:21.020 --> 1:06:23.580\n there are some jobs like airline pilot\n\n1:06:23.580 --> 1:06:26.640\n that are among the highest paid in our economy,\n\n1:06:26.640 --> 1:06:28.780\n but also a lot of them are suitable for machine learning.\n\n1:06:28.780 --> 1:06:30.940\n A lot of those tasks are.\n\n1:06:30.940 --> 1:06:32.500\n And then, yeah, you mentioned economists.\n\n1:06:32.500 --> 1:06:33.820\n I couldn't help peeking at those\n\n1:06:33.820 --> 1:06:36.100\n and they're paid a fair amount,\n\n1:06:36.100 --> 1:06:39.120\n maybe not as much as some of us think they should be.\n\n1:06:39.120 --> 1:06:43.620\n But they have some tasks that are suitable\n\n1:06:43.620 --> 1:06:45.540\n for machine learning, but for now at least,\n\n1:06:45.540 --> 1:06:47.180\n most of the tasks of economists\n\n1:06:47.180 --> 1:06:48.540\n didn't end up being in that category.\n\n1:06:48.540 --> 1:06:50.640\n And I should say, I didn't like create that data.\n\n1:06:50.640 --> 1:06:54.480\n We just took the analysis and that's what came out of it.\n\n1:06:54.480 --> 1:06:57.320\n And over time, that scatter plot will be updated\n\n1:06:57.320 --> 1:06:59.940\n as the technology improves.\n\n1:06:59.940 --> 1:07:02.860\n But it was just interesting to see the pattern there.\n\n1:07:02.860 --> 1:07:05.140\n And it is a little troubling in so far\n\n1:07:05.140 --> 1:07:08.100\n as if you just take the technology as it is today,\n\n1:07:08.100 --> 1:07:10.520\n it's likely to worsen income inequality\n\n1:07:10.520 --> 1:07:12.260\n on a lot of dimensions.\n\n1:07:12.260 --> 1:07:16.480\n So on this topic of the effect of AI\n\n1:07:16.480 --> 1:07:21.060\n on our landscape of work,\n\n1:07:21.060 --> 1:07:23.660\n one of the people that have been speaking about it\n\n1:07:23.660 --> 1:07:25.800\n in the public domain, public discourse\n\n1:07:25.800 --> 1:07:28.100\n is the presidential candidate, Andrew Yang.\n\n1:07:28.100 --> 1:07:29.040\n Yeah.\n\n1:07:29.040 --> 1:07:31.900\n What are your thoughts about Andrew?\n\n1:07:31.900 --> 1:07:34.340\n What are your thoughts about UBI,\n\n1:07:34.340 --> 1:07:36.700\n that universal basic income\n\n1:07:36.700 --> 1:07:39.100\n that he made one of the core ideas,\n\n1:07:39.100 --> 1:07:40.780\n by the way, he has like hundreds of ideas\n\n1:07:40.780 --> 1:07:44.020\n about like everything, it's kind of interesting.\n\n1:07:44.020 --> 1:07:45.380\n But what are your thoughts about him\n\n1:07:45.380 --> 1:07:46.740\n and what are your thoughts about UBI?\n\n1:07:46.740 --> 1:07:51.740\n Let me answer the question about his broader approach first.\n\n1:07:52.060 --> 1:07:52.900\n I mean, I just love that.\n\n1:07:52.900 --> 1:07:56.460\n He's really thoughtful, analytical.\n\n1:07:56.460 --> 1:07:58.220\n I agree with his values.\n\n1:07:58.220 --> 1:07:59.420\n So that's awesome.\n\n1:07:59.420 --> 1:08:02.220\n And he read my book and mentions it sometimes,\n\n1:08:02.220 --> 1:08:03.820\n so it makes me even more excited.\n\n1:08:04.820 --> 1:08:07.660\n And the thing that he really made the centerpiece\n\n1:08:07.660 --> 1:08:09.940\n of his campaign was UBI.\n\n1:08:09.940 --> 1:08:13.260\n And I was originally kind of a fan of it.\n\n1:08:13.260 --> 1:08:15.980\n And then as I studied it more, I became less of a fan,\n\n1:08:15.980 --> 1:08:17.420\n although I'm beginning to come back a little bit.\n\n1:08:17.420 --> 1:08:19.300\n So let me tell you a little bit of my evolution.\n\n1:08:19.300 --> 1:08:23.060\n As an economist, we have, by looking at the problem\n\n1:08:23.060 --> 1:08:25.180\n of people not having enough income and the simplest thing\n\n1:08:25.180 --> 1:08:26.860\n is, well, why don't we write them a check?\n\n1:08:26.860 --> 1:08:28.040\n Problem solved.\n\n1:08:28.040 --> 1:08:30.460\n But then I talked to my sociologist friends\n\n1:08:30.460 --> 1:08:34.420\n and they really convinced me that just writing a check\n\n1:08:34.420 --> 1:08:36.940\n doesn't really get at the core values.\n\n1:08:36.940 --> 1:08:40.660\n Voltaire once said that work solves three great ills,\n\n1:08:40.660 --> 1:08:43.380\n boredom, vice, and need.\n\n1:08:43.380 --> 1:08:46.680\n And you can deal with the need thing by writing a check,\n\n1:08:46.680 --> 1:08:49.300\n but people need a sense of meaning,\n\n1:08:49.300 --> 1:08:50.820\n they need something to do.\n\n1:08:50.820 --> 1:08:55.820\n And when, say, steel workers or coal miners lost their jobs\n\n1:08:57.980 --> 1:09:02.980\n and were just given checks, alcoholism, depression, divorce,\n\n1:09:03.820 --> 1:09:06.540\n all those social indicators, drug use, all went way up.\n\n1:09:06.540 --> 1:09:08.020\n People just weren't happy\n\n1:09:08.020 --> 1:09:10.420\n just sitting around collecting a check.\n\n1:09:11.380 --> 1:09:13.220\n Maybe it's part of the way they were raised.\n\n1:09:13.220 --> 1:09:14.740\n Maybe it's something innate in people\n\n1:09:14.740 --> 1:09:17.220\n that they need to feel wanted and needed.\n\n1:09:17.220 --> 1:09:19.540\n So it's not as simple as just writing people a check.\n\n1:09:19.540 --> 1:09:23.980\n You need to also give them a way to have a sense of purpose.\n\n1:09:23.980 --> 1:09:25.380\n And that was important to me.\n\n1:09:25.380 --> 1:09:28.740\n And the second thing is that, as I mentioned earlier,\n\n1:09:28.740 --> 1:09:31.160\n we are far from the end of work.\n\n1:09:31.160 --> 1:09:32.800\n I don't buy the idea that there's just like\n\n1:09:32.800 --> 1:09:34.140\n not enough work to be done.\n\n1:09:34.140 --> 1:09:37.100\n I see like our cities need to be cleaned up.\n\n1:09:37.100 --> 1:09:39.580\n And robots can't do most of that.\n\n1:09:39.580 --> 1:09:40.780\n We need to have better childcare.\n\n1:09:40.780 --> 1:09:41.640\n We need better healthcare.\n\n1:09:41.640 --> 1:09:44.940\n We need to take care of people who are mentally ill or older.\n\n1:09:44.940 --> 1:09:46.500\n We need to repair our roads.\n\n1:09:46.500 --> 1:09:49.940\n There's so much work that require at least partly,\n\n1:09:49.940 --> 1:09:52.300\n maybe entirely a human component.\n\n1:09:52.300 --> 1:09:54.660\n So rather than like write all these people off,\n\n1:09:54.660 --> 1:09:58.240\n let's find a way to repurpose them and keep them engaged.\n\n1:09:58.240 --> 1:10:03.240\n Now that said, I would like to see more buying power\n\n1:10:04.640 --> 1:10:06.400\n from people who are sort of at the bottom end\n\n1:10:06.400 --> 1:10:07.320\n of the spectrum.\n\n1:10:07.320 --> 1:10:12.320\n The economy has been designed and evolved in a way\n\n1:10:12.540 --> 1:10:15.600\n that's I think very unfair to a lot of hardworking people.\n\n1:10:15.600 --> 1:10:18.100\n I see super hardworking people who aren't really seeing\n\n1:10:18.100 --> 1:10:20.720\n their wages grow over the past 20, 30 years,\n\n1:10:20.720 --> 1:10:24.080\n while some other people who have been super smart\n\n1:10:24.080 --> 1:10:29.080\n and or super lucky have made billions\n\n1:10:29.480 --> 1:10:30.920\n or hundreds of billions.\n\n1:10:30.920 --> 1:10:33.800\n And I don't think they need those hundreds of billions\n\n1:10:33.800 --> 1:10:35.740\n to have the right incentives to invent things.\n\n1:10:35.740 --> 1:10:38.560\n I think if you talk to almost any of them as I have,\n\n1:10:39.440 --> 1:10:42.440\n they don't think that they need an extra $10 billion\n\n1:10:42.440 --> 1:10:43.560\n to do what they're doing.\n\n1:10:43.560 --> 1:10:48.120\n Most of them probably would love to do it for only a billion\n\n1:10:48.120 --> 1:10:49.360\n or maybe for nothing.\n\n1:10:49.360 --> 1:10:50.800\n For nothing, many of them, yeah.\n\n1:10:50.800 --> 1:10:54.200\n I mean, an interesting point to make is,\n\n1:10:54.200 --> 1:10:56.640\n do we think that Bill Gates would have founded Microsoft\n\n1:10:56.640 --> 1:10:58.720\n if tax rates were 70%?\n\n1:10:58.720 --> 1:11:01.380\n Well, we know he would have because they were tax rates\n\n1:11:01.380 --> 1:11:03.680\n of 70% when he founded it.\n\n1:11:03.680 --> 1:11:06.200\n So I don't think that's as big a deterrent\n\n1:11:06.200 --> 1:11:09.100\n and we could provide more buying power to people.\n\n1:11:09.100 --> 1:11:12.800\n My own favorite tool is the Earned Income Tax Credit,\n\n1:11:12.800 --> 1:11:16.240\n which is basically a way of supplementing income\n\n1:11:16.240 --> 1:11:18.160\n of people who have jobs and giving employers\n\n1:11:18.160 --> 1:11:20.300\n an incentive to hire even more people.\n\n1:11:20.300 --> 1:11:22.400\n The minimum wage can discourage employment,\n\n1:11:22.400 --> 1:11:25.160\n but the Earned Income Tax Credit encourages employment\n\n1:11:25.160 --> 1:11:27.960\n by supplementing people's wages.\n\n1:11:27.960 --> 1:11:31.760\n If the employer can only afford to pay them $10 for a task,\n\n1:11:32.680 --> 1:11:35.200\n the rest of us kick in another five or $10\n\n1:11:35.200 --> 1:11:37.640\n and bring their wages up to 15 or 20 total.\n\n1:11:37.640 --> 1:11:39.360\n And then they have more buying power.\n\n1:11:39.360 --> 1:11:42.320\n Then entrepreneurs are thinking, how can we cater to them?\n\n1:11:42.320 --> 1:11:44.080\n How can we make products for them?\n\n1:11:44.080 --> 1:11:47.220\n And it becomes a self reinforcing system\n\n1:11:47.220 --> 1:11:49.840\n where people are better off.\n\n1:11:49.840 --> 1:11:51.840\n Ian Drang and I had a good discussion\n\n1:11:51.840 --> 1:11:55.940\n where he suggested instead of a universal basic income,\n\n1:11:55.940 --> 1:11:59.080\n he suggested, or instead of an unconditional basic income,\n\n1:11:59.080 --> 1:12:00.600\n how about a conditional basic income\n\n1:12:00.600 --> 1:12:03.040\n where the condition is you learn some new skills,\n\n1:12:03.040 --> 1:12:05.040\n we need to reskill our workforce.\n\n1:12:05.040 --> 1:12:09.120\n So let's make it easier for people to find ways\n\n1:12:09.120 --> 1:12:11.280\n to get those skills and get rewarded for doing them.\n\n1:12:11.280 --> 1:12:13.080\n And that's kind of a neat idea as well.\n\n1:12:13.080 --> 1:12:13.900\n That's really interesting.\n\n1:12:13.900 --> 1:12:16.160\n So, I mean, one of the questions,\n\n1:12:16.160 --> 1:12:19.680\n one of the dreams of UBI is that you provide\n\n1:12:19.680 --> 1:12:24.280\n some little safety net while you retrain,\n\n1:12:24.280 --> 1:12:26.040\n while you learn a new skill.\n\n1:12:26.040 --> 1:12:28.360\n But like, I think, I guess you're speaking\n\n1:12:28.360 --> 1:12:31.280\n to the intuition that that doesn't always,\n\n1:12:31.280 --> 1:12:33.760\n like there needs to be some incentive to reskill,\n\n1:12:33.760 --> 1:12:35.280\n to train, to learn a new thing.\n\n1:12:35.280 --> 1:12:36.120\n I think it helps.\n\n1:12:36.120 --> 1:12:37.960\n I mean, there are lots of self motivated people,\n\n1:12:37.960 --> 1:12:40.600\n but there are also people that maybe need a little guidance\n\n1:12:40.600 --> 1:12:44.960\n or help and I think it's a really hard question\n\n1:12:44.960 --> 1:12:48.280\n for someone who is losing a job in one area to know\n\n1:12:48.280 --> 1:12:50.600\n what is the new area I should be learning skills in.\n\n1:12:50.600 --> 1:12:52.600\n And we could provide a much better set of tools\n\n1:12:52.600 --> 1:12:54.480\n and platforms that maps it.\n\n1:12:54.480 --> 1:12:56.400\n Okay, here's a set of skills you already have.\n\n1:12:56.400 --> 1:12:58.120\n Here's something that's in demand.\n\n1:12:58.120 --> 1:13:00.440\n Let's create a path for you to go from where you are\n\n1:13:00.440 --> 1:13:02.240\n to where you need to be.\n\n1:13:03.120 --> 1:13:07.080\n So I'm a total, how do I put it nicely about myself?\n\n1:13:07.080 --> 1:13:09.640\n I'm totally clueless about the economy.\n\n1:13:09.640 --> 1:13:12.760\n It's not totally true, but pretty good approximation.\n\n1:13:12.760 --> 1:13:17.160\n If you were to try to fix our tax system\n\n1:13:20.480 --> 1:13:23.240\n and, or maybe from another side,\n\n1:13:23.240 --> 1:13:26.680\n if there's fundamental problems in taxation\n\n1:13:26.680 --> 1:13:29.720\n or some fundamental problems about our economy,\n\n1:13:29.720 --> 1:13:31.320\n what would you try to fix?\n\n1:13:31.320 --> 1:13:33.440\n What would you try to speak to?\n\n1:13:33.440 --> 1:13:36.320\n You know, I definitely think our whole tax system,\n\n1:13:36.320 --> 1:13:40.080\n our political and economic system has gotten more\n\n1:13:40.080 --> 1:13:43.520\n and more screwed up over the past 20, 30 years.\n\n1:13:43.520 --> 1:13:46.520\n I don't think it's that hard to make headway\n\n1:13:46.520 --> 1:13:47.360\n in improving it.\n\n1:13:47.360 --> 1:13:49.880\n I don't think we need to totally reinvent stuff.\n\n1:13:49.880 --> 1:13:52.400\n A lot of it is what I've been elsewhere with Andy\n\n1:13:52.400 --> 1:13:54.680\n and others called economics 101.\n\n1:13:54.680 --> 1:13:56.400\n You know, there's just some basic principles\n\n1:13:56.400 --> 1:14:00.640\n that have worked really well in the 20th century\n\n1:14:00.640 --> 1:14:01.880\n that we sort of forgot, you know,\n\n1:14:01.880 --> 1:14:03.960\n in terms of investing in education,\n\n1:14:03.960 --> 1:14:07.560\n investing in infrastructure, welcoming immigrants,\n\n1:14:07.560 --> 1:14:12.560\n having a tax system that was more progressive and fair.\n\n1:14:13.280 --> 1:14:16.560\n At one point, tax rates were on top incomes\n\n1:14:16.560 --> 1:14:18.080\n were significantly higher.\n\n1:14:18.080 --> 1:14:19.880\n And they've come down a lot to the point where\n\n1:14:19.880 --> 1:14:21.440\n in many cases they're lower now\n\n1:14:21.440 --> 1:14:23.440\n than they are for poorer people.\n\n1:14:24.760 --> 1:14:27.960\n So, and we could do things like earned income tax credit\n\n1:14:27.960 --> 1:14:29.240\n to get a little more wonky.\n\n1:14:29.240 --> 1:14:31.440\n I'd like to see more Pigouvian taxes.\n\n1:14:31.440 --> 1:14:35.720\n What that means is you tax things that are bad\n\n1:14:35.720 --> 1:14:36.960\n instead of things that are good.\n\n1:14:36.960 --> 1:14:40.640\n So right now we tax labor, we tax capital\n\n1:14:40.640 --> 1:14:42.200\n and which is unfortunate\n\n1:14:42.200 --> 1:14:44.080\n because one of the basic principles of economics\n\n1:14:44.080 --> 1:14:46.400\n if you tax something, you tend to get less of it.\n\n1:14:46.400 --> 1:14:48.800\n So, you know, right now there's still work to be done\n\n1:14:48.800 --> 1:14:51.220\n and still capital to be invested in.\n\n1:14:51.220 --> 1:14:54.600\n But instead we should be taxing things like pollution\n\n1:14:54.600 --> 1:14:55.980\n and congestion.\n\n1:14:57.200 --> 1:15:00.000\n And if we did that, we would have less pollution.\n\n1:15:00.000 --> 1:15:02.120\n So a carbon tax is, you know,\n\n1:15:02.120 --> 1:15:04.120\n almost every economist would say it's a no brainer\n\n1:15:04.120 --> 1:15:07.560\n whether they're Republican or Democrat,\n\n1:15:07.560 --> 1:15:09.680\n Greg Mankiw who is head of George Bush's\n\n1:15:09.680 --> 1:15:13.000\n Council of Economic Advisers or Dick Schmollensie\n\n1:15:13.000 --> 1:15:16.080\n who is another Republican economist agree.\n\n1:15:16.080 --> 1:15:21.080\n And of course a lot of Democratic economists agree as well.\n\n1:15:21.600 --> 1:15:22.800\n If we taxed carbon,\n\n1:15:22.800 --> 1:15:26.040\n we could raise hundreds of billions of dollars.\n\n1:15:26.040 --> 1:15:28.600\n We could take that money and redistribute it\n\n1:15:28.600 --> 1:15:31.200\n through an earned income tax credit or other things\n\n1:15:31.200 --> 1:15:35.280\n so that overall our tax system would become more progressive.\n\n1:15:35.280 --> 1:15:36.960\n We could tax congestion.\n\n1:15:36.960 --> 1:15:39.040\n One of the things that kills me as an economist\n\n1:15:39.040 --> 1:15:41.080\n is every time I sit in a traffic jam,\n\n1:15:41.080 --> 1:15:43.280\n I know that it's completely unnecessary.\n\n1:15:43.280 --> 1:15:44.840\n This is complete wasted time.\n\n1:15:44.840 --> 1:15:47.560\n You just visualize the cost and productivity.\n\n1:15:47.560 --> 1:15:51.260\n Exactly, because they are taking costs for me\n\n1:15:51.260 --> 1:15:52.700\n and all the people around me.\n\n1:15:52.700 --> 1:15:54.840\n And if they charged a congestion tax,\n\n1:15:54.840 --> 1:15:57.080\n they would take that same amount of money\n\n1:15:57.080 --> 1:15:59.720\n and people would, it would streamline the roads.\n\n1:15:59.720 --> 1:16:01.640\n Like when you're in Singapore, the traffic just flows\n\n1:16:01.640 --> 1:16:02.640\n because they have a congestion tax.\n\n1:16:02.640 --> 1:16:03.640\n They listened to economists.\n\n1:16:03.640 --> 1:16:06.480\n They invited me and others to go talk to them.\n\n1:16:06.480 --> 1:16:09.240\n And then I'd still be paying,\n\n1:16:09.240 --> 1:16:11.740\n I'd be paying a congestion tax instead of paying in my time,\n\n1:16:11.740 --> 1:16:14.240\n but that money would now be available for healthcare,\n\n1:16:14.240 --> 1:16:15.520\n be available for infrastructure,\n\n1:16:15.520 --> 1:16:16.880\n or be available just to give to people\n\n1:16:16.880 --> 1:16:18.660\n so they could buy food or whatever.\n\n1:16:18.660 --> 1:16:22.280\n So it's just, it saddens me when you sit,\n\n1:16:22.280 --> 1:16:23.320\n when you're sitting in a traffic jam,\n\n1:16:23.320 --> 1:16:25.060\n it's like taxing me and then taking that money\n\n1:16:25.060 --> 1:16:27.820\n and dumping it in the ocean, just like destroying it.\n\n1:16:27.820 --> 1:16:29.500\n So there are a lot of things like that\n\n1:16:29.500 --> 1:16:32.520\n that economists, and I'm not,\n\n1:16:32.520 --> 1:16:33.940\n I'm not like doing anything radical here.\n\n1:16:33.940 --> 1:16:36.680\n Most, you know, good economists would,\n\n1:16:36.680 --> 1:16:39.440\n I probably agree with me point by point on these things.\n\n1:16:39.440 --> 1:16:41.000\n And we could do those things\n\n1:16:41.000 --> 1:16:43.760\n and our whole economy would become much more efficient.\n\n1:16:43.760 --> 1:16:47.000\n It'd become fairer, invest in R&D and research,\n\n1:16:47.000 --> 1:16:50.060\n which is close to a free lunch is what we have.\n\n1:16:50.060 --> 1:16:53.160\n My erstwhile MIT colleague, Bob Solla,\n\n1:16:53.160 --> 1:16:57.360\n got the Nobel Prize, not yesterday, but 30 years ago,\n\n1:16:57.360 --> 1:17:00.560\n for describing that most improvements\n\n1:17:00.560 --> 1:17:02.880\n in living standards come from tech progress.\n\n1:17:02.880 --> 1:17:04.560\n And Paul Romer later got a Nobel Prize\n\n1:17:04.560 --> 1:17:08.040\n for noting that investments in R&D and human capital\n\n1:17:08.040 --> 1:17:11.040\n can speed the rate of tech progress.\n\n1:17:11.040 --> 1:17:14.680\n So if we do that, then we'll be healthier and wealthier.\n\n1:17:14.680 --> 1:17:16.200\n Yeah, from an economics perspective,\n\n1:17:16.200 --> 1:17:18.440\n I remember taking an undergrad econ,\n\n1:17:18.440 --> 1:17:20.380\n you mentioned econ 101.\n\n1:17:20.380 --> 1:17:23.660\n It seemed from all the plots I saw\n\n1:17:23.660 --> 1:17:28.660\n that R&D is an obvious, as close to free lunch as we have,\n\n1:17:29.040 --> 1:17:32.340\n it seemed like obvious that we should do more research.\n\n1:17:32.340 --> 1:17:33.180\n It is.\n\n1:17:33.180 --> 1:17:36.620\n Like what, what, like, there's no.\n\n1:17:36.620 --> 1:17:38.000\n Well, we should do basic research.\n\n1:17:38.000 --> 1:17:39.440\n I mean, so let me just be clear.\n\n1:17:39.440 --> 1:17:41.420\n It'd be great if everybody did more research\n\n1:17:41.420 --> 1:17:42.260\n and I would make this issue\n\n1:17:42.260 --> 1:17:46.080\n between applied development versus basic research.\n\n1:17:46.080 --> 1:17:48.120\n So applied development, like, you know,\n\n1:17:48.120 --> 1:17:52.120\n how do we get this self driving car, you know,\n\n1:17:52.120 --> 1:17:53.960\n feature to work better in the Tesla?\n\n1:17:53.960 --> 1:17:55.240\n That's great for private companies\n\n1:17:55.240 --> 1:17:57.080\n because they can capture the value from that.\n\n1:17:57.080 --> 1:17:59.700\n If they make a better self driving car system,\n\n1:17:59.700 --> 1:18:02.240\n they can sell cars that are more valuable\n\n1:18:02.240 --> 1:18:03.080\n and then make money.\n\n1:18:03.080 --> 1:18:05.720\n So there's an incentive that there's not a big problem there\n\n1:18:05.720 --> 1:18:08.200\n and smart companies, Amazon, Tesla,\n\n1:18:08.200 --> 1:18:09.440\n and others are investing in it.\n\n1:18:09.440 --> 1:18:11.260\n The problem is with basic research,\n\n1:18:11.260 --> 1:18:14.420\n like coming up with core basic ideas,\n\n1:18:14.420 --> 1:18:16.120\n whether it's in nuclear fusion\n\n1:18:16.120 --> 1:18:19.000\n or artificial intelligence or biotech.\n\n1:18:19.000 --> 1:18:21.640\n There, if someone invents something,\n\n1:18:21.640 --> 1:18:23.920\n it's very hard for them to capture the benefits from it.\n\n1:18:23.920 --> 1:18:26.740\n It's shared by everybody, which is great in a way,\n\n1:18:26.740 --> 1:18:28.640\n but it means that they're not gonna have the incentives\n\n1:18:28.640 --> 1:18:30.680\n to put as much effort into it.\n\n1:18:30.680 --> 1:18:32.960\n There you need, it's a classic public good.\n\n1:18:32.960 --> 1:18:35.120\n There you need the government to be involved in it.\n\n1:18:35.120 --> 1:18:39.360\n And the US government used to be investing much more in R&D,\n\n1:18:39.360 --> 1:18:42.940\n but we have slashed that part of the government\n\n1:18:42.940 --> 1:18:46.900\n really foolishly and we're all poorer,\n\n1:18:46.900 --> 1:18:48.440\n significantly poorer as a result.\n\n1:18:48.440 --> 1:18:50.000\n Growth rates are down.\n\n1:18:50.000 --> 1:18:51.680\n We're not having the kind of scientific progress\n\n1:18:51.680 --> 1:18:53.260\n we used to have.\n\n1:18:53.260 --> 1:18:57.800\n It's been sort of a short term eating the seed corn,\n\n1:18:57.800 --> 1:19:00.120\n whatever metaphor you wanna use\n\n1:19:00.120 --> 1:19:03.320\n where people grab some money, put it in their pockets today,\n\n1:19:03.320 --> 1:19:07.120\n but five, 10, 20 years later,\n\n1:19:07.120 --> 1:19:10.140\n they're a lot poorer than they otherwise would have been.\n\n1:19:10.140 --> 1:19:12.320\n So we're living through a pandemic right now,\n\n1:19:12.320 --> 1:19:14.800\n globally in the United States.\n\n1:19:16.580 --> 1:19:18.840\n From an economics perspective,\n\n1:19:18.840 --> 1:19:23.040\n how do you think this pandemic will change the world?\n\n1:19:23.040 --> 1:19:24.640\n It's been remarkable.\n\n1:19:24.640 --> 1:19:27.760\n And it's horrible how many people have suffered,\n\n1:19:27.760 --> 1:19:31.240\n the amount of death, the economic destruction.\n\n1:19:31.240 --> 1:19:34.300\n It's also striking just the amount of change in work\n\n1:19:34.300 --> 1:19:35.840\n that I've seen.\n\n1:19:35.840 --> 1:19:38.440\n In the last 20 weeks, I've seen more change\n\n1:19:38.440 --> 1:19:41.200\n than there were in the previous 20 years.\n\n1:19:41.200 --> 1:19:42.400\n There's been nothing like it\n\n1:19:42.400 --> 1:19:44.700\n since probably the World War II mobilization\n\n1:19:44.700 --> 1:19:47.040\n in terms of reorganizing our economy.\n\n1:19:47.040 --> 1:19:50.200\n The most obvious one is the shift to remote work.\n\n1:19:50.200 --> 1:19:54.280\n And I and many other people stopped going into the office\n\n1:19:54.280 --> 1:19:56.160\n and teaching my students in person.\n\n1:19:56.160 --> 1:19:57.760\n I did a study on this with a bunch of colleagues\n\n1:19:57.760 --> 1:19:59.180\n at MIT and elsewhere.\n\n1:19:59.180 --> 1:20:02.440\n And what we found was that before the pandemic,\n\n1:20:02.440 --> 1:20:05.400\n in the beginning of 2020, about one in six,\n\n1:20:05.400 --> 1:20:08.660\n a little over 15% of Americans were working remotely.\n\n1:20:09.840 --> 1:20:13.560\n When the pandemic hit, that grew steadily and hit 50%,\n\n1:20:13.560 --> 1:20:16.080\n roughly half of Americans working at home.\n\n1:20:16.080 --> 1:20:17.840\n So a complete transformation.\n\n1:20:17.840 --> 1:20:19.160\n And of course, it wasn't even,\n\n1:20:19.160 --> 1:20:20.520\n it wasn't like everybody did it.\n\n1:20:20.520 --> 1:20:22.760\n If you're an information worker, professional,\n\n1:20:22.760 --> 1:20:24.400\n if you work mainly with data,\n\n1:20:24.400 --> 1:20:26.880\n then you're much more likely to work at home.\n\n1:20:26.880 --> 1:20:28.800\n If you're a manufacturing worker,\n\n1:20:28.800 --> 1:20:32.320\n working with other people or physical things,\n\n1:20:32.320 --> 1:20:34.520\n then it wasn't so easy to work at home.\n\n1:20:34.520 --> 1:20:36.480\n And instead, those people were much more likely\n\n1:20:36.480 --> 1:20:39.280\n to become laid off or unemployed.\n\n1:20:39.280 --> 1:20:41.840\n So it's been something that's had very disparate effects\n\n1:20:41.840 --> 1:20:44.520\n on different parts of the workforce.\n\n1:20:44.520 --> 1:20:46.720\n Do you think it's gonna be sticky in a sense\n\n1:20:46.720 --> 1:20:51.060\n that after vaccine comes out and the economy reopens,\n\n1:20:51.060 --> 1:20:54.300\n do you think remote work will continue?\n\n1:20:55.180 --> 1:20:57.080\n That's a great question.\n\n1:20:57.080 --> 1:20:59.360\n My hypothesis is yes, a lot of it will.\n\n1:20:59.360 --> 1:21:00.800\n Of course, some of it will go back,\n\n1:21:00.800 --> 1:21:03.480\n but a surprising amount of it will stay.\n\n1:21:03.480 --> 1:21:06.620\n I personally, for instance, I moved my seminars,\n\n1:21:06.620 --> 1:21:08.840\n my academic seminars to Zoom,\n\n1:21:08.840 --> 1:21:10.800\n and I was surprised how well it worked.\n\n1:21:10.800 --> 1:21:11.640\n So it works?\n\n1:21:11.640 --> 1:21:13.600\n Yeah, I mean, obviously we were able to reach\n\n1:21:13.600 --> 1:21:14.760\n a much broader audience.\n\n1:21:14.760 --> 1:21:16.600\n So we have people tuning in from Europe\n\n1:21:16.600 --> 1:21:18.520\n and other countries,\n\n1:21:18.520 --> 1:21:20.320\n just all over the United States for that matter.\n\n1:21:20.320 --> 1:21:21.760\n I also actually found that it would,\n\n1:21:21.760 --> 1:21:23.520\n in many ways, is more egalitarian.\n\n1:21:23.520 --> 1:21:25.920\n We use the chat feature and other tools,\n\n1:21:25.920 --> 1:21:27.600\n and grad students and others who might've been\n\n1:21:27.600 --> 1:21:29.400\n a little shy about speaking up,\n\n1:21:29.400 --> 1:21:32.680\n we now kind of have more of ability for lots of voices.\n\n1:21:32.680 --> 1:21:34.360\n And they're answering each other's questions,\n\n1:21:34.360 --> 1:21:35.960\n so you kind of get parallel.\n\n1:21:35.960 --> 1:21:39.040\n Like if someone had some question about some of the data\n\n1:21:39.040 --> 1:21:40.660\n or a reference or whatever,\n\n1:21:40.660 --> 1:21:42.480\n then someone else in the chat would answer it.\n\n1:21:42.480 --> 1:21:44.480\n And the whole thing just became like a higher bandwidth,\n\n1:21:44.480 --> 1:21:46.600\n higher quality thing.\n\n1:21:46.600 --> 1:21:48.440\n So I thought that was kind of interesting.\n\n1:21:48.440 --> 1:21:51.280\n I think a lot of people are discovering that these tools\n\n1:21:51.280 --> 1:21:54.480\n that thanks to technologists have been developed\n\n1:21:54.480 --> 1:21:56.440\n over the past decade,\n\n1:21:56.440 --> 1:21:57.920\n they're a lot more powerful than we thought.\n\n1:21:57.920 --> 1:22:00.120\n I mean, all the terrible things we've seen with COVID\n\n1:22:00.120 --> 1:22:03.400\n and the real failure of many of our institutions\n\n1:22:03.400 --> 1:22:04.960\n that I thought would work better.\n\n1:22:04.960 --> 1:22:09.420\n One area that's been a bright spot is our technologies.\n\n1:22:09.420 --> 1:22:11.840\n Bandwidth has held up pretty well,\n\n1:22:11.840 --> 1:22:14.200\n and all of our email and other tools\n\n1:22:14.200 --> 1:22:18.000\n have just scaled up kind of gracefully.\n\n1:22:18.000 --> 1:22:20.280\n So that's been a plus.\n\n1:22:20.280 --> 1:22:21.680\n Economists call this question\n\n1:22:21.680 --> 1:22:23.920\n of whether it'll go back a hysteresis.\n\n1:22:23.920 --> 1:22:25.880\n The question is like when you boil an egg\n\n1:22:25.880 --> 1:22:29.020\n after it gets cold again, it stays hard.\n\n1:22:29.020 --> 1:22:30.860\n And I think that we're gonna have a fair amount\n\n1:22:30.860 --> 1:22:32.160\n of hysteresis in the economy.\n\n1:22:32.160 --> 1:22:33.440\n We're gonna move to this new,\n\n1:22:33.440 --> 1:22:35.520\n we have moved to a new remote work system,\n\n1:22:35.520 --> 1:22:37.260\n and it's not gonna snap all the way back\n\n1:22:37.260 --> 1:22:38.720\n to where it was before.\n\n1:22:38.720 --> 1:22:43.720\n One of the things that worries me is that the people\n\n1:22:44.160 --> 1:22:49.160\n with lots of followers on Twitter and people with voices,\n\n1:22:51.380 --> 1:22:56.380\n people that can, voices that can be magnified by reporters\n\n1:22:56.380 --> 1:22:57.900\n and all that kind of stuff are the people\n\n1:22:57.900 --> 1:22:59.240\n that fall into this category\n\n1:22:59.240 --> 1:23:01.600\n that we were referring to just now\n\n1:23:01.600 --> 1:23:03.000\n where they can still function\n\n1:23:03.000 --> 1:23:06.240\n and be successful with remote work.\n\n1:23:06.240 --> 1:23:11.240\n And then there is a kind of quiet suffering\n\n1:23:11.240 --> 1:23:14.800\n of what feels like millions of people\n\n1:23:14.800 --> 1:23:19.800\n whose jobs are disturbed profoundly by this pandemic,\n\n1:23:21.200 --> 1:23:23.400\n but they don't have many followers on Twitter.\n\n1:23:26.320 --> 1:23:31.320\n What do we, and again, I apologize,\n\n1:23:31.840 --> 1:23:35.840\n but I've been reading the rise and fall of the Third Reich\n\n1:23:35.840 --> 1:23:38.080\n and there's a connection to the depression\n\n1:23:38.080 --> 1:23:39.580\n on the American side.\n\n1:23:39.580 --> 1:23:42.320\n There's a deep, complicated connection\n\n1:23:42.320 --> 1:23:46.400\n to how suffering can turn into forces\n\n1:23:46.400 --> 1:23:51.400\n that potentially change the world in destructive ways.\n\n1:23:51.960 --> 1:23:53.840\n So like it's something I worry about is like,\n\n1:23:53.840 --> 1:23:56.600\n what is this suffering going to materialize itself\n\n1:23:56.600 --> 1:23:58.080\n in five, 10 years?\n\n1:23:58.080 --> 1:24:01.020\n Is that something you worry about, think about?\n\n1:24:01.020 --> 1:24:03.320\n It's like the center of what I worry about.\n\n1:24:03.320 --> 1:24:05.400\n And let me break it down to two parts.\n\n1:24:05.400 --> 1:24:07.280\n There's a moral and ethical aspect to it.\n\n1:24:07.280 --> 1:24:09.340\n We need to relieve this suffering.\n\n1:24:09.340 --> 1:24:13.280\n I mean, I'm sure the values of, I think most Americans,\n\n1:24:13.280 --> 1:24:15.000\n we like to see shared prosperity\n\n1:24:15.000 --> 1:24:16.620\n or most people on the planet.\n\n1:24:16.620 --> 1:24:20.220\n And we would like to see people not falling behind\n\n1:24:20.220 --> 1:24:23.080\n and they have fallen behind, not just due to COVID,\n\n1:24:23.080 --> 1:24:25.760\n but in the previous couple of decades,\n\n1:24:25.760 --> 1:24:27.920\n median income has barely moved,\n\n1:24:27.920 --> 1:24:29.900\n depending on how you measure it.\n\n1:24:29.900 --> 1:24:33.360\n And the incomes of the top 1% have skyrocketed.\n\n1:24:33.360 --> 1:24:36.460\n And part of that is due to the ways technology has been used.\n\n1:24:36.460 --> 1:24:38.840\n Part of this been due to, frankly, our political system\n\n1:24:38.840 --> 1:24:43.680\n has continually shifted more wealth into those people\n\n1:24:43.680 --> 1:24:45.120\n who have the powerful interest.\n\n1:24:45.120 --> 1:24:48.720\n So there's just, I think, a moral imperative\n\n1:24:48.720 --> 1:24:49.800\n to do a better job.\n\n1:24:49.800 --> 1:24:51.900\n And ultimately, we're all gonna be wealthier\n\n1:24:51.900 --> 1:24:53.320\n if more people can contribute,\n\n1:24:53.320 --> 1:24:55.040\n more people have the wherewithal.\n\n1:24:55.040 --> 1:24:58.640\n But the second thing is that there's a real political risk.\n\n1:24:58.640 --> 1:24:59.960\n I'm not a political scientist,\n\n1:24:59.960 --> 1:25:02.560\n but you don't have to be one, I think,\n\n1:25:02.560 --> 1:25:05.660\n to see how a lot of people are really upset\n\n1:25:05.660 --> 1:25:07.380\n with they're getting a raw deal\n\n1:25:07.380 --> 1:25:12.380\n and they want to smash the system in different ways,\n\n1:25:13.680 --> 1:25:15.960\n in 2016 and 2018.\n\n1:25:15.960 --> 1:25:18.280\n And now I think there are a lot of people\n\n1:25:18.280 --> 1:25:19.600\n who are looking at the political system\n\n1:25:19.600 --> 1:25:21.120\n and they feel like it's not working for them\n\n1:25:21.120 --> 1:25:23.740\n and they just wanna do something radical.\n\n1:25:24.720 --> 1:25:28.140\n Unfortunately, demagogues have harnessed that\n\n1:25:28.140 --> 1:25:33.140\n in a way that is pretty destructive to the country.\n\n1:25:33.140 --> 1:25:36.260\n And an analogy I see is what happened with trade.\n\n1:25:37.240 --> 1:25:39.440\n Almost every economist thinks that free trade\n\n1:25:39.440 --> 1:25:42.440\n is a good thing, that when two people voluntarily exchange\n\n1:25:42.440 --> 1:25:44.940\n almost by definition, they're both better off\n\n1:25:44.940 --> 1:25:45.920\n if it's voluntary.\n\n1:25:47.320 --> 1:25:49.800\n And so generally, trade is a good thing.\n\n1:25:49.800 --> 1:25:52.480\n But they also recognize that trade can lead\n\n1:25:52.480 --> 1:25:56.260\n to uneven effects, that there can be winners and losers\n\n1:25:56.260 --> 1:25:59.280\n in some of the people who didn't have the skills\n\n1:25:59.280 --> 1:26:02.880\n to compete with somebody else or didn't have other assets.\n\n1:26:02.880 --> 1:26:04.920\n And so trade can shift prices\n\n1:26:04.920 --> 1:26:07.500\n in ways that are averse to some people.\n\n1:26:08.460 --> 1:26:11.340\n So there's a formula that economists have,\n\n1:26:11.340 --> 1:26:13.440\n which is that you have free trade,\n\n1:26:13.440 --> 1:26:15.920\n but then you compensate the people who are hurt\n\n1:26:15.920 --> 1:26:18.400\n and free trade makes the pie bigger.\n\n1:26:18.400 --> 1:26:19.460\n And since the pie is bigger,\n\n1:26:19.460 --> 1:26:21.920\n it's possible for everyone to be better off.\n\n1:26:21.920 --> 1:26:23.200\n You can make the winners better off,\n\n1:26:23.200 --> 1:26:25.440\n but you can also compensate those who don't win.\n\n1:26:25.440 --> 1:26:28.460\n And so they end up being better off as well.\n\n1:26:28.460 --> 1:26:33.160\n What happened was that we didn't fulfill that promise.\n\n1:26:33.160 --> 1:26:36.040\n We did have some more increased free trade\n\n1:26:36.040 --> 1:26:39.480\n in the 80s and 90s, but we didn't compensate the people\n\n1:26:39.480 --> 1:26:40.640\n who were hurt.\n\n1:26:40.640 --> 1:26:43.800\n And so they felt like the people in power\n\n1:26:43.800 --> 1:26:45.900\n reneged on the bargain, and I think they did.\n\n1:26:45.900 --> 1:26:48.760\n And so then there's a backlash against trade.\n\n1:26:48.760 --> 1:26:50.840\n And now both political parties,\n\n1:26:50.840 --> 1:26:53.640\n but especially Trump and company,\n\n1:26:53.640 --> 1:26:58.200\n have really pushed back against free trade.\n\n1:26:58.200 --> 1:27:00.680\n Ultimately, that's bad for the country.\n\n1:27:00.680 --> 1:27:02.720\n Ultimately, that's bad for living standards.\n\n1:27:02.720 --> 1:27:04.400\n But in a way I can understand\n\n1:27:04.400 --> 1:27:06.200\n that people felt they were betrayed.\n\n1:27:07.080 --> 1:27:10.680\n Technology has a lot of similar characteristics.\n\n1:27:10.680 --> 1:27:14.920\n Technology can make us all better off.\n\n1:27:14.920 --> 1:27:16.120\n It makes the pie bigger.\n\n1:27:16.120 --> 1:27:18.920\n It creates wealth and health, but it can also be uneven.\n\n1:27:18.920 --> 1:27:21.280\n Not everyone automatically benefits.\n\n1:27:21.280 --> 1:27:22.880\n It's possible for some people,\n\n1:27:22.880 --> 1:27:25.080\n even a majority of people to get left behind\n\n1:27:25.080 --> 1:27:27.200\n while a small group benefits.\n\n1:27:28.200 --> 1:27:29.560\n What most economists would say,\n\n1:27:29.560 --> 1:27:30.880\n well, let's make the pie bigger,\n\n1:27:30.880 --> 1:27:33.000\n but let's make sure we adjust the system\n\n1:27:33.000 --> 1:27:35.200\n so we compensate the people who are hurt.\n\n1:27:35.200 --> 1:27:36.920\n And since the pie is bigger,\n\n1:27:36.920 --> 1:27:38.000\n we can make the rich richer,\n\n1:27:38.000 --> 1:27:39.200\n we can make the middle class richer,\n\n1:27:39.200 --> 1:27:40.980\n we can make the poor richer.\n\n1:27:40.980 --> 1:27:43.640\n Mathematically, everyone could be better off.\n\n1:27:43.640 --> 1:27:45.400\n But again, we're not doing that.\n\n1:27:45.400 --> 1:27:48.940\n And again, people are saying this isn't working for us.\n\n1:27:48.940 --> 1:27:52.540\n And again, instead of fixing the distribution,\n\n1:27:52.540 --> 1:27:54.280\n a lot of people are beginning to say,\n\n1:27:54.280 --> 1:27:57.280\n hey, technology sucks, we've got to stop it.\n\n1:27:57.280 --> 1:27:59.040\n Let's throw rocks at the Google bus.\n\n1:27:59.040 --> 1:27:59.980\n Let's blow it up.\n\n1:27:59.980 --> 1:28:01.240\n Let's blow it up.\n\n1:28:01.240 --> 1:28:04.760\n And there were the Luddites almost exactly 200 years ago\n\n1:28:04.760 --> 1:28:08.040\n who smashed the looms and the spinning machines\n\n1:28:08.040 --> 1:28:11.320\n because they felt like those machines weren't helping them.\n\n1:28:11.320 --> 1:28:12.720\n We have a real imperative,\n\n1:28:12.720 --> 1:28:14.700\n not just to do the morally right thing,\n\n1:28:14.700 --> 1:28:17.520\n but to do the thing that is gonna save the country,\n\n1:28:17.520 --> 1:28:19.440\n which is make sure that we create\n\n1:28:19.440 --> 1:28:22.680\n not just prosperity, but shared prosperity.\n\n1:28:22.680 --> 1:28:27.600\n So you've been at MIT for over 30 years, I think.\n\n1:28:27.600 --> 1:28:28.440\n Don't tell anyone how old I am.\n\n1:28:28.440 --> 1:28:30.280\n Yeah, no, that's true, that's true.\n\n1:28:30.280 --> 1:28:34.000\n And you're now moved to Stanford.\n\n1:28:34.000 --> 1:28:35.680\n I'm gonna try not to say anything\n\n1:28:37.240 --> 1:28:38.880\n about how great MIT is.\n\n1:28:39.760 --> 1:28:41.520\n What's that move been like?\n\n1:28:41.520 --> 1:28:44.960\n What, it's East Coast to West Coast?\n\n1:28:44.960 --> 1:28:46.160\n Well, MIT is great.\n\n1:28:46.160 --> 1:28:48.080\n MIT has been very good to me.\n\n1:28:48.080 --> 1:28:49.560\n It continues to be very good to me.\n\n1:28:49.560 --> 1:28:51.440\n It's an amazing place.\n\n1:28:51.440 --> 1:28:53.200\n I continue to have so many amazing friends\n\n1:28:53.200 --> 1:28:54.600\n and colleagues there.\n\n1:28:54.600 --> 1:28:56.120\n I'm very fortunate to have been able\n\n1:28:56.120 --> 1:28:58.480\n to spend a lot of time at MIT.\n\n1:28:58.480 --> 1:29:00.200\n Stanford's also amazing.\n\n1:29:00.200 --> 1:29:01.980\n And part of what attracted me out here\n\n1:29:01.980 --> 1:29:04.960\n was not just the weather, but also Silicon Valley,\n\n1:29:04.960 --> 1:29:07.360\n let's face it, is really more of the epicenter\n\n1:29:07.360 --> 1:29:09.000\n of the technological revolution.\n\n1:29:09.000 --> 1:29:10.400\n And I wanna be close to the people\n\n1:29:10.400 --> 1:29:12.320\n who are inventing AI and elsewhere.\n\n1:29:12.320 --> 1:29:14.920\n A lot of it is being invested at MIT for that matter\n\n1:29:14.920 --> 1:29:18.940\n in Europe and China and elsewhere, in Nia.\n\n1:29:18.940 --> 1:29:23.800\n But being a little closer to some of the key technologists\n\n1:29:23.800 --> 1:29:25.920\n was something that was important to me.\n\n1:29:25.920 --> 1:29:28.600\n And it may be shallow,\n\n1:29:28.600 --> 1:29:30.180\n but I also do enjoy the good weather.\n\n1:29:30.180 --> 1:29:33.120\n And I felt a little ripped off\n\n1:29:33.120 --> 1:29:35.040\n when I came here a couple of months ago.\n\n1:29:35.040 --> 1:29:36.640\n And immediately there are the fires\n\n1:29:36.640 --> 1:29:39.840\n and my eyes were burning, the sky was orange\n\n1:29:39.840 --> 1:29:41.320\n and there's the heat waves.\n\n1:29:41.320 --> 1:29:44.460\n And so it wasn't exactly what I've been promised,\n\n1:29:44.460 --> 1:29:47.960\n but fingers crossed it'll get back to better.\n\n1:29:47.960 --> 1:29:50.720\n But maybe on a brief aside,\n\n1:29:50.720 --> 1:29:52.720\n there's been some criticism of academia\n\n1:29:52.720 --> 1:29:55.760\n and universities and different avenues.\n\n1:29:55.760 --> 1:30:00.760\n And I, as a person who's gotten to enjoy universities\n\n1:30:00.760 --> 1:30:04.380\n from the pure playground of ideas that it can be,\n\n1:30:06.380 --> 1:30:08.840\n always kind of try to find the words\n\n1:30:08.840 --> 1:30:13.160\n to tell people that these are magical places.\n\n1:30:13.160 --> 1:30:17.000\n Is there something that you can speak to\n\n1:30:17.000 --> 1:30:22.000\n that is beautiful or powerful about universities?\n\n1:30:22.440 --> 1:30:23.280\n Well, sure.\n\n1:30:23.280 --> 1:30:24.500\n I mean, first off, I mean,\n\n1:30:24.500 --> 1:30:26.660\n economists have this concept called revealed preference.\n\n1:30:26.660 --> 1:30:28.300\n You can ask people what they say\n\n1:30:28.300 --> 1:30:29.940\n or you can watch what they do.\n\n1:30:29.940 --> 1:30:33.960\n And so obviously by reveal preferences, I love academia.\n\n1:30:33.960 --> 1:30:35.540\n I could be doing lots of other things,\n\n1:30:35.540 --> 1:30:37.600\n but it's something I enjoy a lot.\n\n1:30:37.600 --> 1:30:39.640\n And I think the word magical is exactly right.\n\n1:30:39.640 --> 1:30:41.480\n At least it is for me.\n\n1:30:41.480 --> 1:30:43.120\n I do what I love, you know,\n\n1:30:43.120 --> 1:30:44.320\n hopefully my Dean won't be listening,\n\n1:30:44.320 --> 1:30:45.640\n but I would do this for free.\n\n1:30:45.640 --> 1:30:49.060\n You know, it's just what I like to do.\n\n1:30:49.060 --> 1:30:50.160\n I like to do research.\n\n1:30:50.160 --> 1:30:51.840\n I love to have conversations like this with you\n\n1:30:51.840 --> 1:30:53.740\n and with my students, with my fellow colleagues.\n\n1:30:53.740 --> 1:30:55.760\n I love being around the smartest people I can find\n\n1:30:55.760 --> 1:30:57.220\n and learning something from them\n\n1:30:57.220 --> 1:30:58.640\n and having them challenge me.\n\n1:30:58.640 --> 1:31:02.480\n And that just gives me joy.\n\n1:31:02.480 --> 1:31:05.500\n And every day I find something new and exciting to work on.\n\n1:31:05.500 --> 1:31:08.040\n And a university environment is really filled\n\n1:31:08.040 --> 1:31:09.820\n with other people who feel that way.\n\n1:31:09.820 --> 1:31:12.960\n And so I feel very fortunate to be part of it.\n\n1:31:12.960 --> 1:31:14.840\n And I'm lucky that I'm in a society\n\n1:31:14.840 --> 1:31:16.200\n where I can actually get paid for it\n\n1:31:16.200 --> 1:31:17.240\n and put food on the table\n\n1:31:17.240 --> 1:31:19.260\n while doing the stuff that I really love.\n\n1:31:19.260 --> 1:31:21.560\n And I hope someday everybody can have jobs\n\n1:31:21.560 --> 1:31:22.800\n that are like that.\n\n1:31:22.800 --> 1:31:25.340\n And I appreciate that it's not necessarily easy\n\n1:31:25.340 --> 1:31:27.400\n for everybody to have a job that they both love\n\n1:31:27.400 --> 1:31:29.400\n and also they get paid for.\n\n1:31:30.660 --> 1:31:34.000\n So there are things that don't go well in academia,\n\n1:31:34.000 --> 1:31:36.000\n but by and large, I think it's a kind of, you know,\n\n1:31:36.000 --> 1:31:37.960\n kinder, gentler version of a lot of the world.\n\n1:31:37.960 --> 1:31:41.280\n You know, we sort of cut each other a little slack\n\n1:31:41.280 --> 1:31:45.800\n on things like, you know, on just a lot of things.\n\n1:31:45.800 --> 1:31:48.320\n You know, of course there's harsh debates\n\n1:31:48.320 --> 1:31:49.900\n and discussions about things\n\n1:31:49.900 --> 1:31:52.060\n and some petty politics here and there.\n\n1:31:52.060 --> 1:31:53.520\n I personally, I try to stay away\n\n1:31:53.520 --> 1:31:55.600\n from most of that sort of politics.\n\n1:31:55.600 --> 1:31:56.560\n It's not my thing.\n\n1:31:56.560 --> 1:31:58.320\n And so it doesn't affect me most of the time,\n\n1:31:58.320 --> 1:32:00.480\n sometimes a little bit, maybe.\n\n1:32:00.480 --> 1:32:03.200\n But, you know, being able to pull together something,\n\n1:32:03.200 --> 1:32:04.860\n we have the digital economy lab.\n\n1:32:04.860 --> 1:32:07.480\n We've got all these brilliant grad students\n\n1:32:07.480 --> 1:32:09.280\n and undergraduates and postdocs\n\n1:32:09.280 --> 1:32:12.320\n that are just doing stuff that I learned from.\n\n1:32:12.320 --> 1:32:14.760\n And every one of them has some aspect\n\n1:32:14.760 --> 1:32:16.640\n of what they're doing that's just,\n\n1:32:16.640 --> 1:32:17.600\n I couldn't even understand.\n\n1:32:17.600 --> 1:32:19.340\n It's like way, way more brilliant.\n\n1:32:19.340 --> 1:32:23.040\n And that's really, to me, actually I really enjoy that,\n\n1:32:23.040 --> 1:32:25.120\n being in a room with lots of other smart people.\n\n1:32:25.120 --> 1:32:29.440\n And Stanford has made it very easy to attract,\n\n1:32:29.440 --> 1:32:31.260\n you know, those people.\n\n1:32:31.260 --> 1:32:33.680\n I just, you know, say I'm gonna do a seminar, whatever,\n\n1:32:33.680 --> 1:32:36.820\n and the people come, they come and wanna work with me.\n\n1:32:36.820 --> 1:32:38.880\n We get funding, we get data sets,\n\n1:32:38.880 --> 1:32:41.440\n and it's come together real nicely.\n\n1:32:41.440 --> 1:32:44.220\n And the rest is just fun.\n\n1:32:44.220 --> 1:32:45.840\n It's fun, yeah.\n\n1:32:45.840 --> 1:32:47.480\n And we feel like we're working on important problems,\n\n1:32:47.480 --> 1:32:50.320\n you know, and we're doing things that, you know,\n\n1:32:50.320 --> 1:32:53.680\n I think are first order in terms of what's important\n\n1:32:53.680 --> 1:32:56.320\n in the world, and that's very satisfying to me.\n\n1:32:56.320 --> 1:32:58.080\n Maybe a bit of a fun question.\n\n1:32:58.080 --> 1:33:02.180\n What three books, technical, fiction, philosophical,\n\n1:33:02.180 --> 1:33:07.180\n you've enjoyed, had a big, big impact in your life?\n\n1:33:07.380 --> 1:33:09.980\n Well, I guess I go back to like my teen years,\n\n1:33:09.980 --> 1:33:12.300\n and, you know, I read Sid Arthur,\n\n1:33:12.300 --> 1:33:13.420\n which is a philosophical book,\n\n1:33:13.420 --> 1:33:15.260\n and kind of helps keep me centered.\n\n1:33:15.260 --> 1:33:16.180\n By Herman Hesse.\n\n1:33:16.180 --> 1:33:17.340\n Yeah, by Herman Hesse, exactly.\n\n1:33:17.340 --> 1:33:20.380\n Don't get too wrapped up in material things\n\n1:33:20.380 --> 1:33:21.980\n or other things, and just sort of, you know,\n\n1:33:21.980 --> 1:33:24.780\n try to find peace on things.\n\n1:33:24.780 --> 1:33:26.340\n A book that actually influenced me a lot\n\n1:33:26.340 --> 1:33:27.620\n in terms of my career was called\n\n1:33:27.620 --> 1:33:30.460\n The Worldly Philosophers by Robert Halbrenner.\n\n1:33:30.460 --> 1:33:31.660\n It's actually about economists.\n\n1:33:31.660 --> 1:33:33.500\n It goes through a series of different,\n\n1:33:33.500 --> 1:33:34.900\n it's written in a very lively form,\n\n1:33:34.900 --> 1:33:36.220\n and it probably sounds boring,\n\n1:33:36.220 --> 1:33:38.820\n but it did describe whether it's Adam Smith\n\n1:33:38.820 --> 1:33:40.820\n or Karl Marx or John Maynard Keynes,\n\n1:33:40.820 --> 1:33:43.340\n and each of them sort of what their key insights were,\n\n1:33:43.340 --> 1:33:45.340\n but also kind of their personalities,\n\n1:33:45.340 --> 1:33:46.520\n and I think that's one of the reasons\n\n1:33:46.520 --> 1:33:50.600\n I became an economist was just understanding\n\n1:33:50.600 --> 1:33:53.100\n how they grapple with the big questions of the world.\n\n1:33:53.100 --> 1:33:56.340\n So would you recommend it as a good whirlwind overview\n\n1:33:56.340 --> 1:33:57.540\n of the history of economics?\n\n1:33:57.540 --> 1:33:59.060\n Yeah, yeah, I think that's exactly right.\n\n1:33:59.060 --> 1:34:00.940\n It kind of takes you through the different things,\n\n1:34:00.940 --> 1:34:04.020\n and so you can understand how they reach,\n\n1:34:04.020 --> 1:34:06.380\n thinking some of the strengths and weaknesses.\n\n1:34:06.380 --> 1:34:07.900\n I mean, it probably is a little out of date now.\n\n1:34:07.900 --> 1:34:08.980\n It needs to be updated a bit,\n\n1:34:08.980 --> 1:34:10.380\n but you could at least look through\n\n1:34:10.380 --> 1:34:12.940\n the first couple hundred years of economics,\n\n1:34:12.940 --> 1:34:15.020\n which is not a bad place to start.\n\n1:34:15.020 --> 1:34:17.580\n More recently, I mean, a book I really enjoyed\n\n1:34:17.580 --> 1:34:20.260\n is by my friend and colleague, Max Tegmark,\n\n1:34:20.260 --> 1:34:21.340\n called Life 3.0.\n\n1:34:21.340 --> 1:34:23.260\n You should have him on your podcast if you haven't already.\n\n1:34:23.260 --> 1:34:25.460\n He was episode number one.\n\n1:34:25.460 --> 1:34:26.500\n Oh my God.\n\n1:34:26.500 --> 1:34:30.220\n And he's back, he'll be back, he'll be back soon.\n\n1:34:30.220 --> 1:34:31.460\n Yeah, no, he's terrific.\n\n1:34:31.460 --> 1:34:33.460\n I love the way his brain works,\n\n1:34:33.460 --> 1:34:35.780\n and he makes you think about profound things.\n\n1:34:35.780 --> 1:34:38.540\n He's got such a joyful approach to life,\n\n1:34:38.540 --> 1:34:41.060\n and so that's been a great book,\n\n1:34:41.060 --> 1:34:43.180\n and I learn a lot from it, I think everybody,\n\n1:34:43.180 --> 1:34:45.580\n but he explains it in a way, even though he's so brilliant,\n\n1:34:45.580 --> 1:34:48.280\n that everyone can understand, that I can understand.\n\n1:34:50.020 --> 1:34:52.920\n That's three, but let me mention maybe one or two others.\n\n1:34:52.920 --> 1:34:55.340\n I mean, I recently read More From Less\n\n1:34:55.340 --> 1:34:58.620\n by my sometimes coauthor, Andrew McAfee.\n\n1:34:58.620 --> 1:35:01.940\n It made me optimistic about how we can continue\n\n1:35:01.940 --> 1:35:04.580\n to have rising living standards\n\n1:35:04.580 --> 1:35:06.140\n while living more lightly on the planet.\n\n1:35:06.140 --> 1:35:07.860\n In fact, because of higher living standards,\n\n1:35:07.860 --> 1:35:09.140\n because of technology,\n\n1:35:09.140 --> 1:35:11.500\n because of digitization that I mentioned,\n\n1:35:11.500 --> 1:35:13.580\n we don't have to have as big an impact on the planet,\n\n1:35:13.580 --> 1:35:15.740\n and that's a great story to tell,\n\n1:35:15.740 --> 1:35:17.440\n and he documents it very carefully.\n\n1:35:19.740 --> 1:35:21.380\n You know, a personal kind of self help book\n\n1:35:21.380 --> 1:35:24.140\n that I found kind of useful, People, is Atomic Habits.\n\n1:35:24.140 --> 1:35:26.180\n I think it's, what's his name, James Clear.\n\n1:35:26.180 --> 1:35:27.540\n Yeah, James Clear.\n\n1:35:27.540 --> 1:35:29.100\n He's just, yeah, it's a good name,\n\n1:35:29.100 --> 1:35:30.460\n because he writes very clearly,\n\n1:35:30.460 --> 1:35:33.620\n and you know, most of the sentences I read in that book,\n\n1:35:33.620 --> 1:35:34.500\n I was like, yeah, I know that,\n\n1:35:34.500 --> 1:35:37.220\n but it just really helps to have somebody like remind you\n\n1:35:37.220 --> 1:35:40.860\n and tell you and kind of just reinforce it, and it's helpful.\n\n1:35:40.860 --> 1:35:45.020\n So build habits in your life that you hope to have,\n\n1:35:45.020 --> 1:35:46.140\n that have a positive impact,\n\n1:35:46.140 --> 1:35:48.100\n and don't have to make it big things.\n\n1:35:48.100 --> 1:35:49.220\n It could be just tiny little.\n\n1:35:49.220 --> 1:35:50.720\n Exactly, I mean, the word atomic,\n\n1:35:50.720 --> 1:35:52.540\n it's a little bit of a pun, I think he says.\n\n1:35:52.540 --> 1:35:54.020\n You know, one, atomic means they're really small.\n\n1:35:54.020 --> 1:35:56.860\n You take these little things, but also like atomic power,\n\n1:35:56.860 --> 1:35:59.460\n can have like, you know, big impact.\n\n1:35:59.460 --> 1:36:00.460\n That's funny, yeah.\n\n1:36:01.460 --> 1:36:04.180\n The biggest ridiculous question,\n\n1:36:04.180 --> 1:36:06.860\n especially to ask an economist, but also a human being,\n\n1:36:06.860 --> 1:36:08.260\n what's the meaning of life?\n\n1:36:08.260 --> 1:36:11.460\n I hope you've gotten the answer to that from somebody else.\n\n1:36:11.460 --> 1:36:14.740\n I think we're all still working on that one, but what is it?\n\n1:36:14.740 --> 1:36:18.120\n You know, I actually learned a lot from my son, Luke,\n\n1:36:18.120 --> 1:36:22.100\n and he's 19 now, but he's always loved philosophy,\n\n1:36:22.100 --> 1:36:24.900\n and he reads way more sophisticated philosophy than I do.\n\n1:36:24.900 --> 1:36:25.860\n I went and took him to Oxford,\n\n1:36:25.860 --> 1:36:27.060\n and he spent the whole time like pulling\n\n1:36:27.060 --> 1:36:29.020\n all these obscure books down and reading them.\n\n1:36:29.020 --> 1:36:32.600\n And a couple of years ago, we had this argument,\n\n1:36:32.600 --> 1:36:34.500\n and he was trying to convince me that hedonism\n\n1:36:34.500 --> 1:36:37.480\n was the ultimate, you know, meaning of life,\n\n1:36:37.480 --> 1:36:40.380\n just pleasure seeking, and...\n\n1:36:40.380 --> 1:36:41.580\n Well, how old was he at the time?\n\n1:36:41.580 --> 1:36:42.420\n 17, so...\n\n1:36:42.420 --> 1:36:43.260\n Okay.\n\n1:36:43.260 --> 1:36:46.700\n But he made a really good like intellectual argument\n\n1:36:46.700 --> 1:36:47.540\n for it too, and you know,\n\n1:36:47.540 --> 1:36:50.180\n but you know, it just didn't strike me as right.\n\n1:36:50.180 --> 1:36:54.540\n And I think that, you know, while I am kind of a utilitarian,\n\n1:36:54.540 --> 1:36:55.940\n like, you know, I do think we should do the grace,\n\n1:36:55.940 --> 1:36:58.740\n good for the grace number, that's just too shallow.\n\n1:36:58.740 --> 1:37:02.820\n And I think I've convinced myself that real happiness\n\n1:37:02.820 --> 1:37:04.260\n doesn't come from seeking pleasure.\n\n1:37:04.260 --> 1:37:05.700\n It's kind of a little, it's ironic.\n\n1:37:05.700 --> 1:37:07.700\n Like if you really focus on being happy,\n\n1:37:07.700 --> 1:37:09.740\n I think it doesn't work.\n\n1:37:09.740 --> 1:37:12.420\n You gotta like be doing something bigger.\n\n1:37:12.420 --> 1:37:14.900\n I think the analogy I sometimes use is, you know,\n\n1:37:14.900 --> 1:37:17.580\n when you look at a dim star in the sky,\n\n1:37:17.580 --> 1:37:19.460\n if you look right at it, it kind of disappears,\n\n1:37:19.460 --> 1:37:20.740\n but you have to look a little to the side,\n\n1:37:20.740 --> 1:37:23.180\n and then the parts of your retina\n\n1:37:23.180 --> 1:37:24.940\n that are better at absorbing light,\n\n1:37:24.940 --> 1:37:26.340\n you know, can pick it up better.\n\n1:37:26.340 --> 1:37:27.420\n It's the same thing with happiness.\n\n1:37:27.420 --> 1:37:32.420\n I think you need to sort of find something, other goal,\n\n1:37:32.500 --> 1:37:33.980\n something, some meaning in life,\n\n1:37:33.980 --> 1:37:36.180\n and that ultimately makes you happier\n\n1:37:36.180 --> 1:37:39.060\n than if you go squarely at just pleasure.\n\n1:37:39.060 --> 1:37:42.260\n And so for me, you know, the kind of research I do\n\n1:37:42.260 --> 1:37:44.220\n that I think is trying to change the world,\n\n1:37:44.220 --> 1:37:46.140\n make the world a better place,\n\n1:37:46.140 --> 1:37:47.980\n and I'm not like an evolutionary psychologist,\n\n1:37:47.980 --> 1:37:50.860\n but my guess is that our brains are wired,\n\n1:37:50.860 --> 1:37:53.860\n not just for pleasure, but we're social animals,\n\n1:37:53.860 --> 1:37:57.220\n and we're wired to like help others.\n\n1:37:57.220 --> 1:37:58.860\n And ultimately, you know,\n\n1:37:58.860 --> 1:38:02.060\n that's something that's really deeply rooted in our psyche.\n\n1:38:02.060 --> 1:38:04.500\n And if we do help others, if we do,\n\n1:38:04.500 --> 1:38:06.660\n or at least feel like we're helping others,\n\n1:38:06.660 --> 1:38:08.220\n you know, our reward systems kick in,\n\n1:38:08.220 --> 1:38:10.460\n and we end up being more deeply satisfied\n\n1:38:10.460 --> 1:38:13.620\n than if we just do something selfish and shallow.\n\n1:38:13.620 --> 1:38:14.460\n Beautifully put.\n\n1:38:14.460 --> 1:38:16.980\n I don't think there's a better way to end it, Eric.\n\n1:38:16.980 --> 1:38:20.500\n You were one of the people when I first showed up at MIT,\n\n1:38:20.500 --> 1:38:22.420\n that made me proud to be at MIT.\n\n1:38:22.420 --> 1:38:24.540\n So it's so sad that you're now at Stanford,\n\n1:38:24.540 --> 1:38:28.980\n but I'm sure you'll do wonderful things at Stanford as well.\n\n1:38:28.980 --> 1:38:30.900\n I can't wait till future books,\n\n1:38:30.900 --> 1:38:32.260\n and people should definitely read your other books.\n\n1:38:32.260 --> 1:38:33.180\n Well, thank you so much.\n\n1:38:33.180 --> 1:38:35.580\n And I think we're all part of the invisible college,\n\n1:38:35.580 --> 1:38:36.420\n as we call it.\n\n1:38:36.420 --> 1:38:38.700\n You know, we're all part of this intellectual\n\n1:38:38.700 --> 1:38:41.660\n and human community where we all can learn from each other.\n\n1:38:41.660 --> 1:38:43.100\n It doesn't really matter physically\n\n1:38:43.100 --> 1:38:44.860\n where we are so much anymore.\n\n1:38:44.860 --> 1:38:45.700\n Beautiful.\n\n1:38:45.700 --> 1:38:46.540\n Thanks for talking today.\n\n1:38:46.540 --> 1:38:48.060\n My pleasure.\n\n1:38:48.060 --> 1:38:49.460\n Thanks for listening to this conversation\n\n1:38:49.460 --> 1:38:50.860\n with Eric Brynjolfsson.\n\n1:38:50.860 --> 1:38:52.620\n And thank you to our sponsors.\n\n1:38:52.620 --> 1:38:55.060\n Vincero Watches, the maker of classy,\n\n1:38:55.060 --> 1:38:56.860\n well performing watches.\n\n1:38:56.860 --> 1:39:00.060\n Fort Sigmatic, the maker of delicious mushroom coffee.\n\n1:39:00.060 --> 1:39:03.140\n ExpressVPN, the VPN I've used for many years\n\n1:39:03.140 --> 1:39:05.260\n to protect my privacy on the internet.\n\n1:39:05.260 --> 1:39:09.140\n And CashApp, the app I use to send money to friends.\n\n1:39:09.140 --> 1:39:11.180\n Please check out these sponsors in the description\n\n1:39:11.180 --> 1:39:14.900\n to get a discount and to support this podcast.\n\n1:39:14.900 --> 1:39:17.280\n If you enjoy this thing, subscribe on YouTube.\n\n1:39:17.280 --> 1:39:19.500\n Review it with five stars on Apple Podcast,\n\n1:39:19.500 --> 1:39:22.100\n follow on Spotify, support on Patreon,\n\n1:39:22.100 --> 1:39:25.380\n or connect with me on Twitter at Lex Friedman.\n\n1:39:25.380 --> 1:39:27.700\n And now, let me leave you with some words\n\n1:39:27.700 --> 1:39:29.980\n from Albert Einstein.\n\n1:39:29.980 --> 1:39:32.860\n It has become appallingly obvious\n\n1:39:32.860 --> 1:39:36.600\n that our technology has exceeded our humanity.\n\n1:39:36.600 --> 1:39:49.100\n Thank you for listening and hope to see you next time.\n\n"
}