{
  "title": "Steven Pinker: AI in the Age of Reason | Lex Fridman Podcast #3",
  "id": "epQxfSp-rdU",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:05.280\n You've studied the human mind, cognition, language, vision, evolution, psychology,\n\n00:05.280 --> 00:11.120\n from child to adult, from the level of individual to the level of our entire civilization.\n\n00:11.680 --> 00:14.880\n So I feel like I can start with a simple multiple choice question.\n\n00:16.240 --> 00:21.840\n What is the meaning of life? Is it A. to attain knowledge as Plato said,\n\n00:22.400 --> 00:27.920\n B. to attain power as Nietzsche said, C. to escape death as Ernest Becker said,\n\n00:27.920 --> 00:32.640\n D. to propagate our genes as Darwin and others have said,\n\n00:33.200 --> 00:36.400\n E. there is no meaning as the nihilists have said,\n\n00:37.520 --> 00:42.960\n F. knowing the meaning of life is beyond our cognitive capabilities as Stephen Pinker said,\n\n00:42.960 --> 00:47.360\n based on my interpretation 20 years ago, and G. none of the above.\n\n00:48.160 --> 00:51.200\n I'd say A. comes closest, but I would amend that to\n\n00:51.200 --> 00:58.640\n C. to attaining not only knowledge but fulfillment more generally, that is life, health, stimulation,\n\n01:00.640 --> 01:06.080\n access to the living cultural and social world.\n\n01:06.080 --> 01:10.800\n Now this is our meaning of life. It's not the meaning of life if you were to ask our genes.\n\n01:12.160 --> 01:17.680\n Their meaning is to propagate copies of themselves, but that is distinct from the\n\n01:17.680 --> 01:21.600\n meaning that the brain that they lead to sets for itself.\n\n01:22.400 --> 01:27.840\n So to you knowledge is a small subset or a large subset?\n\n01:27.840 --> 01:34.000\n It's a large subset, but it's not the entirety of human striving because we also want to\n\n01:35.200 --> 01:39.600\n interact with people. We want to experience beauty. We want to experience the richness\n\n01:39.600 --> 01:47.840\n of the natural world, but understanding what makes the universe tick is way up there.\n\n01:47.840 --> 01:54.000\n For some of us more than others, certainly for me that's one of the top five.\n\n01:54.560 --> 02:00.080\n So is that a fundamental aspect? Are you just describing your own preference or is this a\n\n02:00.080 --> 02:05.920\n fundamental aspect of human nature is to seek knowledge? In your latest book you talk about\n\n02:05.920 --> 02:11.760\n the power, the usefulness of rationality and reason and so on. Is that a fundamental\n\n02:11.760 --> 02:16.160\n nature of human beings or is it something we should just strive for?\n\n02:17.040 --> 02:23.920\n Both. We're capable of striving for it because it is one of the things that make us what we are,\n\n02:23.920 --> 02:32.320\n homo sapiens, wise men. We are unusual among animals in the degree to which we acquire\n\n02:32.320 --> 02:41.600\n knowledge and use it to survive. We make tools. We strike agreements via language. We extract\n\n02:41.600 --> 02:47.760\n poisons. We predict the behavior of animals. We try to get at the workings of plants.\n\n02:47.760 --> 02:52.640\n And when I say we, I don't just mean we in the modern West, but we as a species everywhere,\n\n02:52.640 --> 02:58.160\n which is how we've managed to occupy every niche on the planet, how we've managed to drive other\n\n02:58.160 --> 03:06.480\n animals to extinction. And the refinement of reason in pursuit of human wellbeing, of health,\n\n03:06.480 --> 03:13.680\n happiness, social richness, cultural richness is our main challenge in the present. That is\n\n03:14.480 --> 03:19.280\n using our intellect, using our knowledge to figure out how the world works, how we work\n\n03:19.280 --> 03:25.200\n in order to make discoveries and strike agreements that make us all better off in the long run.\n\n03:25.200 --> 03:31.840\n Right. And you do that almost undeniably and in a data driven way in your recent book,\n\n03:31.840 --> 03:36.400\n but I'd like to focus on the artificial intelligence aspect of things and not just\n\n03:36.400 --> 03:41.920\n artificial intelligence, but natural intelligence too. So 20 years ago in a book you've written on\n\n03:41.920 --> 03:49.520\n how the mind works, you conjecture again, am I right to interpret things? You can correct me\n\n03:49.520 --> 03:54.400\n if I'm wrong, but you conjecture that human thought in the brain may be a result of a\n\n03:54.400 --> 04:00.560\n massive network of highly interconnected neurons. So from this interconnectivity emerges thought\n\n04:01.280 --> 04:05.600\n compared to artificial neural networks, which we use for machine learning today,\n\n04:06.160 --> 04:12.640\n is there something fundamentally more complex, mysterious, even magical about the biological\n\n04:12.640 --> 04:19.440\n neural networks versus the ones we've been starting to use over the past 60 years and\n\n04:19.440 --> 04:24.720\n have become to success in the past 10? There is something a little bit mysterious\n\n04:24.720 --> 04:31.600\n about the human neural networks, which is that each one of us who is a neural network knows that\n\n04:31.600 --> 04:36.960\n we ourselves are conscious. Conscious not in the sense of registering our surroundings or even\n\n04:36.960 --> 04:42.720\n registering our internal state, but in having subjective first person, present tense experience.\n\n04:42.720 --> 04:49.840\n That is when I see red, it's not just different from green, but there's a redness to it that I\n\n04:49.840 --> 04:54.960\n feel. Whether an artificial system would experience that or not, I don't know and I don't think I can\n\n04:54.960 --> 05:00.480\n know. That's why it's mysterious. If we had a perfectly lifelike robot that was behaviorally\n\n05:00.480 --> 05:06.800\n indistinguishable from a human, would we attribute consciousness to it or ought we to attribute\n\n05:06.800 --> 05:12.160\n consciousness to it? And that's something that it's very hard to know. But putting that aside,\n\n05:12.160 --> 05:19.040\n putting aside that largely philosophical question, the question is, is there some difference between\n\n05:19.040 --> 05:23.760\n the human neural network and the ones that we're building in artificial intelligence will mean\n\n05:23.760 --> 05:30.000\n that we're on the current trajectory, not going to reach the point where we've got a lifelike\n\n05:30.000 --> 05:35.120\n robot indistinguishable from a human because the way their so called neural networks are organized\n\n05:35.120 --> 05:39.760\n are different from the way ours are organized. I think there's overlap, but I think there are\n\n05:39.760 --> 05:48.800\n some big differences that current neural networks, current so called deep learning systems are in\n\n05:48.800 --> 05:53.840\n reality not all that deep. That is, they are very good at extracting high order statistical\n\n05:53.840 --> 06:00.640\n regularities, but most of the systems don't have a semantic level, a level of actual understanding\n\n06:00.640 --> 06:07.520\n of who did what to whom, why, where, how things work, what causes what else. Do you think that\n\n06:07.520 --> 06:11.840\n kind of thing can emerge as it does? So artificial neural networks are much smaller, the number of\n\n06:11.840 --> 06:17.760\n connections and so on than the current human biological networks, but do you think sort of\n\n06:18.320 --> 06:22.800\n to go to consciousness or to go to this higher level semantic reasoning about things, do you\n\n06:22.800 --> 06:29.760\n think that can emerge with just a larger network with a more richly weirdly interconnected network?\n\n06:29.760 --> 06:33.280\n Separate it in consciousness because consciousness is even a matter of complexity.\n\n06:33.280 --> 06:34.320\n A really weird one.\n\n06:34.320 --> 06:38.400\n Yeah, you could sensibly ask the question of whether shrimp are conscious, for example,\n\n06:38.400 --> 06:43.840\n they're not terribly complex, but maybe they feel pain. So let's just put that part of it aside.\n\n06:44.400 --> 06:52.480\n But I think sheer size of a neural network is not enough to give it structure and knowledge,\n\n06:52.480 --> 06:59.520\n but if it's suitably engineered, then why not? That is, we're neural networks, natural selection\n\n06:59.520 --> 07:04.880\n did a kind of equivalent of engineering of our brains. So I don't think there's anything mysterious\n\n07:04.880 --> 07:11.760\n in the sense that no system made out of silicon could ever do what a human brain can do. I think\n\n07:11.760 --> 07:17.360\n it's possible in principle. Whether it'll ever happen depends not only on how clever we are\n\n07:17.360 --> 07:22.240\n in engineering these systems, but whether we even want to, whether that's even a sensible goal.\n\n07:22.240 --> 07:29.920\n That is, you can ask the question, is there any locomotion system that is as good as a human?\n\n07:29.920 --> 07:33.920\n Well, we kind of want to do better than a human ultimately in terms of legged locomotion.\n\n07:35.520 --> 07:39.760\n There's no reason that humans should be our benchmark. They're tools that might be better\n\n07:39.760 --> 07:49.760\n in some ways. It may be that we can't duplicate a natural system because at some point it's so much\n\n07:49.760 --> 07:54.560\n cheaper to use a natural system that we're not going to invest more brainpower and resources.\n\n07:54.560 --> 08:00.480\n So for example, we don't really have an exact substitute for wood. We still build houses out\n\n08:00.480 --> 08:05.040\n of wood. We still build furniture out of wood. We like the look. We like the feel. It has certain\n\n08:05.040 --> 08:10.400\n properties that synthetics don't. It's not that there's anything magical or mysterious about wood.\n\n08:11.120 --> 08:17.600\n It's just that the extra steps of duplicating everything about wood is something we just haven't\n\n08:17.600 --> 08:21.600\n bothered because we have wood. Likewise, say cotton. I'm wearing cotton clothing now. It feels\n\n08:21.600 --> 08:29.760\n much better than polyester. It's not that cotton has something magic in it. It's not that we couldn't\n\n08:29.760 --> 08:35.920\n ever synthesize something exactly like cotton, but at some point it's just not worth it. We've got\n\n08:35.920 --> 08:42.000\n cotton. Likewise, in the case of human intelligence, the goal of making an artificial system that is\n\n08:42.000 --> 08:47.840\n exactly like the human brain is a goal that we probably know is going to pursue to the bitter\n\n08:47.840 --> 08:53.120\n end, I suspect, because if you want tools that do things better than humans, you're not going to\n\n08:53.120 --> 08:58.240\n care whether it does something like humans. So for example, diagnosing cancer or predicting the\n\n08:58.240 --> 09:05.280\n weather, why set humans as your benchmark? But in general, I suspect you also believe\n\n09:05.840 --> 09:10.480\n that even if the human should not be a benchmark and we don't want to imitate humans in their\n\n09:10.480 --> 09:15.520\n system, there's a lot to be learned about how to create an artificial intelligence system by\n\n09:15.520 --> 09:22.480\n studying the human. Yeah, I think that's right. In the same way that to build flying machines,\n\n09:22.480 --> 09:27.760\n we want to understand the laws of aerodynamics, including birds, but not mimic the birds,\n\n09:27.760 --> 09:35.440\n but they're the same laws. You have a view on AI, artificial intelligence, and safety\n\n09:35.440 --> 09:47.040\n that, from my perspective, is refreshingly rational or perhaps more importantly, has elements\n\n09:47.040 --> 09:53.040\n of positivity to it, which I think can be inspiring and empowering as opposed to paralyzing.\n\n09:53.600 --> 09:59.840\n For many people, including AI researchers, the eventual existential threat of AI is obvious,\n\n09:59.840 --> 10:05.840\n not only possible, but obvious. And for many others, including AI researchers, the threat\n\n10:05.840 --> 10:14.640\n is not obvious. So Elon Musk is famously in the highly concerned about AI camp, saying things like\n\n10:14.640 --> 10:21.200\n AI is far more dangerous than nuclear weapons, and that AI will likely destroy human civilization.\n\n10:21.200 --> 10:29.360\n Human civilization. So in February, he said that if Elon was really serious about AI, the threat\n\n10:29.360 --> 10:34.960\n of AI, he would stop building self driving cars that he's doing very successfully as part of Tesla.\n\n10:35.760 --> 10:40.800\n Then he said, wow, if even Pinker doesn't understand the difference between narrow AI,\n\n10:40.800 --> 10:47.280\n like a car and general AI, when the latter literally has a million times more compute power\n\n10:47.280 --> 10:54.160\n and an open ended utility function, humanity is in deep trouble. So first, what did you mean by\n\n10:54.160 --> 10:59.200\n the statement about Elon Musk should stop building self driving cars if he's deeply concerned?\n\n11:00.080 --> 11:03.520\n Not the last time that Elon Musk has fired off an intemperate tweet.\n\n11:04.320 --> 11:07.680\n Well, we live in a world where Twitter has power.\n\n11:07.680 --> 11:16.640\n Yes. Yeah, I think there are two kinds of existential threat that have been discussed\n\n11:16.640 --> 11:19.760\n in connection with artificial intelligence, and I think that they're both incoherent.\n\n11:20.480 --> 11:29.520\n One of them is a vague fear of AI takeover, that just as we subjugated animals and less technologically\n\n11:29.520 --> 11:34.640\n advanced peoples, so if we build something that's more advanced than us, it will inevitably turn us\n\n11:34.640 --> 11:42.320\n into pets or slaves or domesticated animal equivalents. I think this confuses intelligence\n\n11:42.320 --> 11:49.200\n with a will to power, that it so happens that in the intelligence system we are most familiar with,\n\n11:49.200 --> 11:54.160\n namely homo sapiens, we are products of natural selection, which is a competitive process,\n\n11:54.160 --> 11:59.600\n and so bundled together with our problem solving capacity are a number of nasty traits like\n\n12:00.320 --> 12:08.720\n dominance and exploitation and maximization of power and glory and resources and influence.\n\n12:08.720 --> 12:13.120\n There's no reason to think that sheer problem solving capability will set that as one of its\n\n12:13.120 --> 12:18.720\n goals. Its goals will be whatever we set its goals as, and as long as someone isn't building a\n\n12:18.720 --> 12:24.320\n megalomaniacal artificial intelligence, then there's no reason to think that it would naturally\n\n12:24.320 --> 12:28.960\n evolve in that direction. Now, you might say, well, what if we gave it the goal of maximizing\n\n12:28.960 --> 12:34.880\n its own power source? That's a pretty stupid goal to give an autonomous system. You don't give it\n\n12:34.880 --> 12:40.720\n that goal. I mean, that's just self evidently idiotic. So if you look at the history of the\n\n12:40.720 --> 12:45.120\n world, there's been a lot of opportunities where engineers could instill in a system\n\n12:45.120 --> 12:49.440\n destructive power and they choose not to because that's the natural process of engineering.\n\n12:49.440 --> 12:53.120\n Well, except for weapons. I mean, if you're building a weapon, its goal is to destroy people,\n\n12:53.680 --> 12:58.560\n and so I think there are good reasons to not build certain kinds of weapons. I think building\n\n12:58.560 --> 13:06.480\n nuclear weapons was a massive mistake. You do. So maybe pause on that because that is one of\n\n13:06.480 --> 13:12.800\n the serious threats. Do you think that it was a mistake in a sense that it should have been\n\n13:12.800 --> 13:19.200\n stopped early on? Or do you think it's just an unfortunate event of invention that this was\n\n13:19.200 --> 13:23.280\n invented? Do you think it's possible to stop? I guess is the question. It's hard to rewind the\n\n13:23.280 --> 13:28.320\n clock because of course it was invented in the context of World War II and the fear that the\n\n13:28.320 --> 13:35.120\n Nazis might develop one first. Then once it was initiated for that reason, it was hard to turn\n\n13:35.120 --> 13:42.080\n off, especially since winning the war against the Japanese and the Nazis was such an overwhelming\n\n13:42.080 --> 13:47.120\n goal of every responsible person that there's just nothing that people wouldn't have done then\n\n13:47.120 --> 13:52.560\n to ensure victory. It's quite possible if World War II hadn't happened that nuclear weapons\n\n13:52.560 --> 13:57.440\n wouldn't have been invented. We can't know, but I don't think it was by any means a necessity,\n\n13:57.440 --> 14:02.720\n any more than some of the other weapon systems that were envisioned but never implemented,\n\n14:02.720 --> 14:09.360\n like planes that would disperse poison gas over cities like crop dusters or systems to try to\n\n14:10.560 --> 14:16.000\n create earthquakes and tsunamis in enemy countries, to weaponize the weather,\n\n14:16.000 --> 14:21.120\n weaponize solar flares, all kinds of crazy schemes that we thought the better of.\n\n14:21.120 --> 14:25.840\n I think analogies between nuclear weapons and artificial intelligence are fundamentally\n\n14:25.840 --> 14:30.560\n misguided because the whole point of nuclear weapons is to destroy things. The point of\n\n14:30.560 --> 14:36.080\n artificial intelligence is not to destroy things. So the analogy is misleading.\n\n14:36.080 --> 14:39.920\n So there's two artificial intelligence you mentioned. The first one I guess is highly\n\n14:39.920 --> 14:42.080\n intelligent or power hungry.\n\n14:42.080 --> 14:46.800\n Yeah, it's a system that we design ourselves where we give it the goals. Goals are external to\n\n14:46.800 --> 14:55.200\n the means to attain the goals. If we don't design an artificially intelligent system to\n\n14:55.200 --> 15:00.800\n maximize dominance, then it won't maximize dominance. It's just that we're so familiar\n\n15:00.800 --> 15:06.320\n with homo sapiens where these two traits come bundled together, particularly in men,\n\n15:06.320 --> 15:14.480\n that we are apt to confuse high intelligence with a will to power, but that's just an error.\n\n15:15.520 --> 15:21.440\n The other fear is that will be collateral damage that will give artificial intelligence a goal\n\n15:21.440 --> 15:27.440\n like make paper clips and it will pursue that goal so brilliantly that before we can stop it,\n\n15:27.440 --> 15:32.800\n it turns us into paper clips. We'll give it the goal of curing cancer and it will turn us into\n\n15:32.800 --> 15:38.720\n guinea pigs for lethal experiments or give it the goal of world peace and its conception of world\n\n15:38.720 --> 15:43.680\n peace is no people, therefore no fighting and so it will kill us all. Now I think these are utterly\n\n15:43.680 --> 15:49.040\n fanciful. In fact, I think they're actually self defeating. They first of all assume that we're\n\n15:49.040 --> 15:53.600\n going to be so brilliant that we can design an artificial intelligence that can cure cancer,\n\n15:53.600 --> 15:59.600\n but so stupid that we don't specify what we mean by curing cancer in enough detail that it won't\n\n15:59.600 --> 16:06.240\n kill us in the process and it assumes that the system will be so smart that it can cure cancer,\n\n16:06.240 --> 16:11.920\n but so idiotic that it can't figure out that what we mean by curing cancer is not killing everyone.\n\n16:12.880 --> 16:18.320\n I think that the collateral damage scenario, the value alignment problem is also based on\n\n16:18.320 --> 16:23.200\n a misconception. So one of the challenges, of course, we don't know how to build either system\n\n16:23.200 --> 16:27.440\n currently or are we even close to knowing? Of course, those things can change overnight,\n\n16:27.440 --> 16:33.840\n but at this time, theorizing about it is very challenging in either direction. So that's\n\n16:33.840 --> 16:39.600\n probably at the core of the problem is without that ability to reason about the real engineering\n\n16:39.600 --> 16:45.120\n things here at hand is your imagination runs away with things. Exactly. But let me sort of ask,\n\n16:45.120 --> 16:52.320\n what do you think was the motivation, the thought process of Elon Musk? I build autonomous vehicles,\n\n16:52.320 --> 16:57.680\n I study autonomous vehicles, I study Tesla autopilot. I think it is one of the greatest\n\n16:57.680 --> 17:04.400\n currently large scale application of artificial intelligence in the world. It has potentially a\n\n17:04.400 --> 17:10.880\n very positive impact on society. So how does a person who's creating this very good quote unquote\n\n17:10.880 --> 17:19.280\n narrow AI system also seem to be so concerned about this other general AI? What do you think\n\n17:19.280 --> 17:23.040\n is the motivation there? What do you think is the thing? Well, you probably have to ask him,\n\n17:23.040 --> 17:31.520\n but there, and he is notoriously flamboyant, impulsive to the, as we have just seen,\n\n17:31.520 --> 17:37.360\n to the detriment of his own goals of the health of the company. So I don't know what's going on\n\n17:37.360 --> 17:42.560\n in his mind. You probably have to ask him, but I don't think the, and I don't think the distinction\n\n17:42.560 --> 17:50.240\n between special purpose AI and so called general AI is relevant that in the same way that special\n\n17:50.240 --> 17:56.400\n purpose AI is not going to do anything conceivable in order to attain a goal. All engineering systems\n\n17:57.760 --> 18:02.800\n are designed to trade off across multiple goals. When we build cars in the first place,\n\n18:02.800 --> 18:08.880\n we didn't forget to install brakes because the goal of a car is to go fast. It occurred to people,\n\n18:08.880 --> 18:13.920\n yes, you want it to go fast, but not always. So you would build in brakes too. Likewise,\n\n18:13.920 --> 18:20.320\n if a car is going to be autonomous and program it to take the shortest route to the airport,\n\n18:20.320 --> 18:24.560\n it's not going to take the diagonal and mow down people and trees and fences because that's the\n\n18:24.560 --> 18:29.120\n shortest route. That's not what we mean by the shortest route when we program it. And that's just\n\n18:29.120 --> 18:36.000\n what an intelligence system is by definition. It takes into account multiple constraints.\n\n18:36.000 --> 18:41.280\n The same is true, in fact, even more true of so called general intelligence. That is,\n\n18:41.280 --> 18:48.080\n if it's genuinely intelligent, it's not going to pursue some goal singlemindedly, omitting every\n\n18:48.080 --> 18:54.400\n other consideration and collateral effect. That's not artificial and general intelligence. That's\n\n18:54.400 --> 19:01.120\n artificial stupidity. I agree with you, by the way, on the promise of autonomous vehicles for\n\n19:01.120 --> 19:06.160\n improving human welfare. I think it's spectacular. And I'm surprised at how little press coverage\n\n19:06.160 --> 19:11.040\n notes that in the United States alone, something like 40,000 people die every year on the highways,\n\n19:11.680 --> 19:17.680\n vastly more than are killed by terrorists. And we spent a trillion dollars on a war to combat\n\n19:18.240 --> 19:24.160\n deaths by terrorism, about half a dozen a year. Whereas year in, year out, 40,000 people are\n\n19:24.160 --> 19:29.920\n massacred on the highways, which could be brought down to very close to zero. So I'm with you on\n\n19:29.920 --> 19:34.800\n the humanitarian benefit. Let me just mention that as a person who's building these cars,\n\n19:34.800 --> 19:39.360\n it is a little bit offensive to me to say that engineers would be clueless enough not to engineer\n\n19:39.360 --> 19:45.280\n safety into systems. I often stay up at night thinking about those 40,000 people that are dying.\n\n19:45.280 --> 19:50.800\n And everything I tried to engineer is to save those people's lives. So every new invention that\n\n19:50.800 --> 19:59.280\n I'm super excited about, in all the deep learning literature and CVPR conferences and NIPS, everything\n\n19:59.280 --> 20:08.240\n I'm super excited about is all grounded in making it safe and help people. So I just don't see how\n\n20:08.240 --> 20:13.280\n that trajectory can all of a sudden slip into a situation where intelligence will be highly\n\n20:13.280 --> 20:17.920\n negative. You and I certainly agree on that. And I think that's only the beginning of the\n\n20:17.920 --> 20:24.320\n potential humanitarian benefits of artificial intelligence. There's been enormous attention to\n\n20:24.320 --> 20:28.720\n what are we going to do with the people whose jobs are made obsolete by artificial intelligence,\n\n20:28.720 --> 20:32.560\n but very little attention given to the fact that the jobs that are going to be made obsolete are\n\n20:32.560 --> 20:38.960\n horrible jobs. The fact that people aren't going to be picking crops and making beds and driving\n\n20:38.960 --> 20:45.520\n trucks and mining coal, these are soul deadening jobs. And we have a whole literature sympathizing\n\n20:45.520 --> 20:53.040\n with the people stuck in these menial, mind deadening, dangerous jobs. If we can eliminate\n\n20:53.040 --> 20:58.480\n them, this is a fantastic boon to humanity. Now granted, you solve one problem and there's another\n\n20:58.480 --> 21:05.840\n one, namely, how do we get these people a decent income? But if we're smart enough to invent machines\n\n21:05.840 --> 21:12.400\n that can make beds and put away dishes and handle hospital patients, I think we're smart enough to\n\n21:12.400 --> 21:19.200\n figure out how to redistribute income to apportion some of the vast economic savings to the human\n\n21:19.200 --> 21:26.000\n beings who will no longer be needed to make beds. Okay. Sam Harris says that it's obvious that\n\n21:26.000 --> 21:30.960\n eventually AI will be an existential risk. He's one of the people who says it's obvious.\n\n21:31.760 --> 21:38.640\n We don't know when the claim goes, but eventually it's obvious. And because we don't know when,\n\n21:38.640 --> 21:45.680\n we should worry about it now. This is a very interesting argument in my eyes. So how do we\n\n21:45.680 --> 21:51.120\n think about timescale? How do we think about existential threats when we don't really, we know\n\n21:51.120 --> 21:58.160\n so little about the threat, unlike nuclear weapons perhaps, about this particular threat, that it\n\n21:58.160 --> 22:04.560\n could happen tomorrow, right? So, but very likely it won't. Very likely it'd be a hundred years away.\n\n22:04.560 --> 22:11.440\n So how do we ignore it? How do we talk about it? Do we worry about it? How do we think about those?\n\n22:12.480 --> 22:13.040\n What is it?\n\n22:13.840 --> 22:18.560\n A threat that we can imagine. It's within the limits of our imagination,\n\n22:18.560 --> 22:23.360\n but not within our limits of understanding to accurately predict it.\n\n22:24.320 --> 22:26.880\n But what is the it that we're afraid of?\n\n22:26.880 --> 22:30.320\n Sorry. AI being the existential threat.\n\n22:30.320 --> 22:35.120\n AI. How? Like enslaving us or turning us into paperclips?\n\n22:35.120 --> 22:38.720\n I think the most compelling from the Sam Harris perspective would be the paperclip situation.\n\n22:39.520 --> 22:43.440\n Yeah. I mean, I just think it's totally fanciful. I mean, that is don't build a system.\n\n22:43.440 --> 22:50.080\n Don't give a, don't, first of all, the code of engineering is you don't implement a system with\n\n22:50.080 --> 22:55.040\n massive control before testing it. Now, perhaps the culture of engineering will radically change.\n\n22:55.040 --> 23:00.320\n Then I would worry, but I don't see any signs that engineers will suddenly do idiotic things,\n\n23:00.320 --> 23:06.400\n like put a electric power plant in control of a system that they haven't tested first.\n\n23:07.360 --> 23:14.720\n Or all of these scenarios, not only imagine almost a magically powered intelligence,\n\n23:15.600 --> 23:20.240\n including things like cure cancer, which is probably an incoherent goal because there's\n\n23:20.240 --> 23:25.600\n so many different kinds of cancer or bring about world peace. I mean, how do you even specify that\n\n23:25.600 --> 23:31.360\n as a goal? But the scenarios also imagine some degree of control of every molecule in the\n\n23:31.360 --> 23:38.400\n universe, which not only is itself unlikely, but we would not start to connect these systems to\n\n23:39.120 --> 23:45.680\n infrastructure without testing as we would any kind of engineering system.\n\n23:45.680 --> 23:53.920\n Now, maybe some engineers will be irresponsible and we need legal and regulatory and legal\n\n23:53.920 --> 23:59.440\n responsibility implemented so that engineers don't do things that are stupid by their own standards.\n\n24:00.640 --> 24:08.560\n But the, I've never seen enough of a plausible scenario of existential threat to devote large\n\n24:08.560 --> 24:14.240\n amounts of brain power to, to forestall it. So you believe in the sort of the power on\n\n24:14.240 --> 24:19.520\n mass of the engineering of reason, as you argue in your latest book of Reason and Science, to sort of\n\n24:20.400 --> 24:26.400\n be the very thing that guides the development of new technology so it's safe and also keeps us safe.\n\n24:28.000 --> 24:34.560\n You know, granted the same culture of safety that currently is part of the engineering mindset for\n\n24:34.560 --> 24:40.480\n airplanes, for example. So yeah, I don't think that that should be thrown out the window and\n\n24:40.480 --> 24:45.520\n that untested all powerful systems should be suddenly implemented, but there's no reason to\n\n24:45.520 --> 24:50.400\n think they are. And in fact, if you look at the progress of artificial intelligence, it's been,\n\n24:50.400 --> 24:54.160\n you know, it's been impressive, especially in the last 10 years or so, but the idea that suddenly\n\n24:54.160 --> 25:00.080\n there'll be a step function that all of a sudden before we know it, it will be all powerful,\n\n25:00.080 --> 25:06.800\n that there'll be some kind of recursive self improvement, some kind of fume is also fanciful.\n\n25:06.800 --> 25:13.200\n We, certainly by the technology that we, that we're now impresses us, such as deep learning,\n\n25:13.200 --> 25:18.320\n where you train something on hundreds of thousands or millions of examples,\n\n25:18.320 --> 25:25.200\n they're not hundreds of thousands of problems of which curing cancer is a typical example.\n\n25:26.000 --> 25:31.520\n And so the kind of techniques that have allowed AI to increase in the last five years are not the\n\n25:31.520 --> 25:40.320\n kind that are going to lead to this fantasy of exponential sudden self improvement. I think it's\n\n25:40.320 --> 25:45.440\n kind of a magical thinking. It's not based on our understanding of how AI actually works.\n\n25:45.440 --> 25:51.040\n Now give me a chance here. So you said fanciful, magical thinking. In his TED talk,\n\n25:51.040 --> 25:55.760\n Sam Harris says that thinking about AI killing all human civilization is somehow fun,\n\n25:55.760 --> 26:00.400\n intellectually. Now I have to say as a scientist engineer, I don't find it fun,\n\n26:00.400 --> 26:08.560\n but when I'm having beer with my non AI friends, there is indeed something fun and appealing about\n\n26:08.560 --> 26:14.640\n it. Like talking about an episode of Black Mirror, considering if a large meteor is headed towards\n\n26:14.640 --> 26:20.640\n Earth, we were just told a large meteor is headed towards Earth, something like this. And can you\n\n26:20.640 --> 26:24.560\n relate to this sense of fun? And do you understand the psychology of it?\n\n26:24.560 --> 26:32.800\n Yes. Good question. I personally don't find it fun. I find it kind of actually a waste of time\n\n26:32.800 --> 26:39.760\n because there are genuine threats that we ought to be thinking about like pandemics, like cyber\n\n26:39.760 --> 26:46.160\n security vulnerabilities, like the possibility of nuclear war and certainly climate change.\n\n26:46.160 --> 26:54.320\n You know, this is enough to fill many conversations. And I think Sam did put his\n\n26:54.320 --> 27:00.240\n finger on something, namely that there is a community, sometimes called the rationality\n\n27:00.240 --> 27:07.280\n community, that delights in using its brainpower to come up with scenarios that would not occur\n\n27:07.280 --> 27:14.560\n to mere mortals, to less cerebral people. So there is a kind of intellectual thrill in finding new\n\n27:14.560 --> 27:19.280\n things to worry about that no one has worried about yet. I actually think, though, that it's\n\n27:19.840 --> 27:25.440\n not only is it a kind of fun that doesn't give me particular pleasure, but I think there can be a\n\n27:25.440 --> 27:32.400\n pernicious side to it, namely that you overcome people with such dread, such fatalism, that there\n\n27:32.400 --> 27:39.200\n are so many ways to die, to annihilate our civilization, that we may as well enjoy life\n\n27:39.200 --> 27:42.880\n while we can. There's nothing we can do about it. If climate change doesn't do us in, then runaway\n\n27:42.880 --> 27:52.480\n robots will. So let's enjoy ourselves now. We've got to prioritize. We have to look at threats that\n\n27:52.480 --> 27:58.160\n are close to certainty, such as climate change, and distinguish those from ones that are merely\n\n27:58.160 --> 28:05.280\n imaginable but with infinitesimal probabilities. And we have to take into account people's worry\n\n28:05.280 --> 28:12.480\n budget. You can't worry about everything. And if you sow dread and fear and terror and fatalism,\n\n28:12.480 --> 28:17.280\n it can lead to a kind of numbness. Well, these problems are overwhelming, and the engineers are\n\n28:17.280 --> 28:25.760\n just going to kill us all. So let's either destroy the entire infrastructure of science, technology,\n\n28:26.640 --> 28:32.560\n or let's just enjoy life while we can. So there's a certain line of worry, which I'm worried about\n\n28:32.560 --> 28:36.800\n a lot of things in engineering. There's a certain line of worry when you cross, you're allowed to\n\n28:36.800 --> 28:43.360\n cross, that it becomes paralyzing fear as opposed to productive fear. And that's kind of what\n\n28:44.240 --> 28:50.160\n you're highlighting. Exactly right. And we've seen some, we know that human effort is not\n\n28:50.160 --> 28:58.160\n well calibrated against risk in that because a basic tenet of cognitive psychology is that\n\n28:58.160 --> 29:05.680\n perception of risk and hence perception of fear is driven by imaginability, not by data. And so we\n\n29:05.680 --> 29:11.360\n misallocate vast amounts of resources to avoiding terrorism, which kills on average about six\n\n29:11.360 --> 29:18.640\n Americans a year with one exception of 9 11. We invade countries, we invent entire new departments\n\n29:18.640 --> 29:25.120\n of government with massive, massive expenditure of resources and lives to defend ourselves against\n\n29:25.120 --> 29:34.720\n a trivial risk. Whereas guaranteed risks, one of them you mentioned traffic fatalities and even\n\n29:34.720 --> 29:45.200\n risks that are not here, but are plausible enough to worry about like pandemics, like nuclear war,\n\n29:45.920 --> 29:51.440\n receive far too little attention. In presidential debates, there's no discussion of how to minimize\n\n29:51.440 --> 29:58.240\n the risk of nuclear war. Lots of discussion of terrorism, for example. And so I think it's\n\n29:58.240 --> 30:08.080\n essential to calibrate our budget of fear, worry, concern, planning to the actual probability of\n\n30:08.080 --> 30:15.840\n harm. Yep. So let me ask this question. So speaking of imaginability, you said it's important to think\n\n30:15.840 --> 30:23.360\n about reason and one of my favorite people who likes to dip into the outskirts of reason through\n\n30:23.840 --> 30:32.000\n fascinating exploration of his imagination is Joe Rogan. Oh yes. So who has through reason used to\n\n30:32.000 --> 30:37.280\n believe a lot of conspiracies and through reason has stripped away a lot of his beliefs in that\n\n30:37.280 --> 30:43.120\n way. So it's fascinating actually to watch him through rationality kind of throw away the ideas\n\n30:43.120 --> 30:50.320\n of Bigfoot and 9 11. I'm not sure exactly. Kim Trails. I don't know what he believes in. Yes.\n\n30:50.320 --> 30:55.520\n Okay. But he no longer believed in. No, that's right. No, he's become a real force for good.\n\n30:55.520 --> 31:00.240\n Yep. So you were on the Joe Rogan podcast in February and had a fascinating conversation,\n\n31:00.240 --> 31:05.920\n but as far as I remember, didn't talk much about artificial intelligence. I will be on his podcast\n\n31:05.920 --> 31:11.520\n in a couple of weeks. Joe is very much concerned about existential threat of AI. I'm not sure if\n\n31:11.520 --> 31:17.040\n you're, this is why I was hoping that you would get into that topic. And in this way,\n\n31:17.040 --> 31:21.840\n he represents quite a lot of people who look at the topic of AI from 10,000 foot level.\n\n31:22.480 --> 31:29.040\n So as an exercise of communication, you said it's important to be rational and reason\n\n31:29.040 --> 31:34.080\n about these things. Let me ask, if you were to coach me as an AI researcher about how to speak\n\n31:34.080 --> 31:40.640\n to Joe and the general public about AI, what would you advise? Well, the short answer would be to\n\n31:40.640 --> 31:45.200\n read the sections that I wrote in enlightenment now about AI, but a longer reason would be I\n\n31:45.200 --> 31:50.480\n think to emphasize, and I think you're very well positioned as an engineer to remind people about\n\n31:50.480 --> 31:57.040\n the culture of engineering, that it really is safety oriented, that another discussion in\n\n31:57.040 --> 32:04.560\n enlightenment now, I plot rates of accidental death from various causes, plane crashes, car\n\n32:04.560 --> 32:12.640\n crashes, occupational accidents, even death by lightning strikes. And they all plummet because\n\n32:12.640 --> 32:18.320\n the culture of engineering is how do you squeeze out the lethal risks, death by fire, death by\n\n32:18.320 --> 32:24.320\n drowning, death by asphyxiation, all of them drastically declined because of advances in\n\n32:24.320 --> 32:29.840\n engineering that I got to say, I did not appreciate until I saw those graphs. And it is because\n\n32:29.840 --> 32:37.520\n exactly, people like you who stay up at night thinking, oh my God, is what I'm inventing likely\n\n32:37.520 --> 32:43.760\n to hurt people and to deploy ingenuity to prevent that from happening. Now, I'm not an engineer,\n\n32:43.760 --> 32:48.240\n although I spent 22 years at MIT, so I know something about the culture of engineering.\n\n32:48.240 --> 32:53.680\n My understanding is that this is the way you think if you're an engineer. And it's essential\n\n32:53.680 --> 32:59.200\n that that culture not be suddenly switched off when it comes to artificial intelligence. So,\n\n32:59.200 --> 33:02.560\n I mean, that could be a problem, but is there any reason to think it would be switched off?\n\n33:02.560 --> 33:06.960\n I don't think so. And one, there's not enough engineers speaking up for this\n\n33:06.960 --> 33:13.440\n way, for the excitement, for the positive view of human nature, what you're trying to create\n\n33:13.440 --> 33:18.240\n is positivity. Like everything we try to invent is trying to do good for the world.\n\n33:18.240 --> 33:23.600\n But let me ask you about the psychology of negativity. It seems just objectively,\n\n33:23.600 --> 33:28.480\n not considering the topic, it seems that being negative about the future makes you sound smarter\n\n33:28.480 --> 33:33.520\n than being positive about the future, irregardless of topic. Am I correct in this observation? And\n\n33:34.320 --> 33:39.280\n if so, why do you think that is? Yeah, I think there is that phenomenon that,\n\n33:40.080 --> 33:44.240\n as Tom Lehrer, the satirist said, always predict the worst and you'll be hailed as a prophet.\n\n33:45.360 --> 33:52.240\n It may be part of our overall negativity bias. We are as a species more attuned to the negative\n\n33:52.240 --> 33:59.920\n than the positive. We dread losses more than we enjoy gains. And that might open up a space for\n\n34:02.480 --> 34:06.560\n prophets to remind us of harms and risks and losses that we may have overlooked.\n\n34:07.680 --> 34:15.040\n So I think there is that asymmetry. So you've written some of my favorite books\n\n34:16.080 --> 34:21.600\n all over the place. So starting from Enlightenment Now to The Better Ages of Our Nature,\n\n34:21.600 --> 34:28.560\n Blank Slate, How the Mind Works, the one about language, Language Instinct. Bill Gates,\n\n34:29.200 --> 34:35.680\n big fan too, said of your most recent book that it's my new favorite book of all time.\n\n34:37.440 --> 34:43.840\n So for you as an author, what was a book early on in your life that had a profound impact on the\n\n34:43.840 --> 34:49.040\n way you saw the world? Certainly this book, Enlightenment Now, was influenced by David\n\n34:49.040 --> 34:55.920\n Deutsch's The Beginning of Infinity, a rather deep reflection on knowledge and the power of\n\n34:55.920 --> 35:02.320\n knowledge to improve the human condition. And with bits of wisdom such as that problems are\n\n35:02.320 --> 35:07.440\n inevitable but problems are solvable given the right knowledge and that solutions create new\n\n35:07.440 --> 35:11.920\n problems that have to be solved in their turn. That's I think a kind of wisdom about the human\n\n35:11.920 --> 35:16.080\n condition that influenced the writing of this book. There are some books that are excellent\n\n35:16.080 --> 35:22.080\n but obscure, some of which I have on a page on my website. I read a book called The History of Force,\n\n35:22.800 --> 35:27.840\n self published by a political scientist named James Payne on the historical decline of violence\n\n35:27.840 --> 35:30.800\n and that was one of the inspirations for The Better Angels of Our Nature.\n\n35:33.600 --> 35:37.520\n What about early on? If you look back when you were maybe a teenager?\n\n35:38.160 --> 35:43.040\n I loved a book called One, Two, Three, Infinity. When I was a young adult I read that book by\n\n35:43.040 --> 35:48.960\n George Gamow, the physicist, which had very accessible and humorous explanations of\n\n35:48.960 --> 35:59.360\n relativity, of number theory, of dimensionality, high multiple dimensional spaces in a way that I\n\n35:59.360 --> 36:06.080\n think is still delightful 70 years after it was published. I like the Time Life Science series.\n\n36:06.080 --> 36:11.280\n These are books that would arrive every month that my mother subscribed to, each one on a different\n\n36:11.280 --> 36:17.280\n topic. One would be on electricity, one would be on forests, one would be on evolution and then one\n\n36:17.280 --> 36:24.240\n was on the mind. I was just intrigued that there could be a science of mind and that book I would\n\n36:24.240 --> 36:28.480\n cite as an influence as well. Then later on... That's when you fell in love with the idea of\n\n36:28.480 --> 36:35.040\n studying the mind? Was that the thing that grabbed you? It was one of the things I would say. I read\n\n36:35.040 --> 36:41.360\n as a college student the book Reflections on Language by Noam Chomsky. I spent most of his\n\n36:41.360 --> 36:47.440\n career here at MIT. Richard Dawkins, two books, The Blind Watchmaker and The Selfish Gene,\n\n36:47.440 --> 36:54.240\n were enormously influential, mainly for the content but also for the writing style, the\n\n36:55.040 --> 37:02.480\n ability to explain abstract concepts in lively prose. Stephen Jay Gould's first collection,\n\n37:02.480 --> 37:10.240\n Ever Since Darwin, also an excellent example of lively writing. George Miller, a psychologist that\n\n37:10.240 --> 37:15.600\n most psychologists are familiar with, came up with the idea that human memory has a capacity of\n\n37:16.160 --> 37:20.640\n seven plus or minus two chunks. That's probably his biggest claim to fame. But he wrote a couple\n\n37:20.640 --> 37:25.840\n of books on language and communication that I read as an undergraduate. Again, beautifully written\n\n37:25.840 --> 37:31.920\n and intellectually deep. Wonderful. Stephen, thank you so much for taking the time today.\n\n37:31.920 --> 37:33.920\n My pleasure. Thanks a lot, Lex.\n\n"
}