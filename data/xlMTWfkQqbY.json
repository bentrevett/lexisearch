{
  "title": "Daphne Koller: Biomedicine and Machine Learning | Lex Fridman Podcast #93",
  "id": "xlMTWfkQqbY",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:03.300\n The following is a conversation with Daphne Koller,\n\n00:03.300 --> 00:06.260\n a professor of computer science at Stanford University,\n\n00:06.260 --> 00:08.980\n a cofounder of Coursera with Andrew Ng,\n\n00:08.980 --> 00:11.880\n and founder and CEO of Incitro,\n\n00:11.880 --> 00:13.380\n a company at the intersection\n\n00:13.380 --> 00:15.940\n of machine learning and biomedicine.\n\n00:15.940 --> 00:17.820\n We're now in the exciting early days\n\n00:17.820 --> 00:20.580\n of using the data driven methods of machine learning\n\n00:20.580 --> 00:22.580\n to help discover and develop new drugs\n\n00:22.580 --> 00:24.420\n and treatments at scale.\n\n00:24.420 --> 00:27.780\n Daphne and Incitro are leading the way on this\n\n00:27.780 --> 00:29.660\n with breakthroughs that may ripple\n\n00:29.660 --> 00:31.620\n through all fields of medicine,\n\n00:31.620 --> 00:34.260\n including ones most critical for helping\n\n00:34.260 --> 00:36.360\n with the current coronavirus pandemic.\n\n00:37.220 --> 00:38.660\n This conversation was recorded\n\n00:38.660 --> 00:41.300\n before the COVID 19 outbreak.\n\n00:41.300 --> 00:43.540\n For everyone feeling the medical, psychological,\n\n00:43.540 --> 00:45.620\n and financial burden of this crisis,\n\n00:45.620 --> 00:47.700\n I'm sending love your way.\n\n00:47.700 --> 00:51.740\n Stay strong, we're in this together, we'll beat this thing.\n\n00:51.740 --> 00:54.260\n This is the Artificial Intelligence Podcast.\n\n00:54.260 --> 00:56.380\n If you enjoy it, subscribe on YouTube,\n\n00:56.380 --> 00:58.720\n review it with five stars on Apple Podcast,\n\n00:58.720 --> 01:00.100\n support it on Patreon,\n\n01:00.100 --> 01:02.060\n or simply connect with me on Twitter\n\n01:02.060 --> 01:05.940\n at Lex Friedman, spelled F R I D M A N.\n\n01:05.940 --> 01:08.100\n As usual, I'll do a few minutes of ads now\n\n01:08.100 --> 01:09.420\n and never any ads in the middle\n\n01:09.420 --> 01:11.740\n that can break the flow of this conversation.\n\n01:11.740 --> 01:13.060\n I hope that works for you\n\n01:13.060 --> 01:15.940\n and doesn't hurt the listening experience.\n\n01:15.940 --> 01:17.940\n This show is presented by Cash App,\n\n01:17.940 --> 01:20.280\n the number one finance app in the app store.\n\n01:20.280 --> 01:23.420\n When you get it, use code LEXPODCAST.\n\n01:23.420 --> 01:25.620\n Cash App lets you send money to friends,\n\n01:25.620 --> 01:27.900\n buy Bitcoin, and invest in the stock market\n\n01:27.900 --> 01:30.220\n with as little as one dollar.\n\n01:30.220 --> 01:31.700\n Since Cash App allows you to send\n\n01:31.700 --> 01:33.420\n and receive money digitally,\n\n01:33.420 --> 01:36.900\n peer to peer, and security in all digital transactions\n\n01:36.900 --> 01:38.120\n is very important,\n\n01:38.120 --> 01:41.380\n let me mention the PCI data security standard\n\n01:41.380 --> 01:43.900\n that Cash App is compliant with.\n\n01:43.900 --> 01:46.780\n I'm a big fan of standards for safety and security.\n\n01:46.780 --> 01:49.520\n PCI DSS is a good example of that,\n\n01:49.520 --> 01:51.140\n where a bunch of competitors got together\n\n01:51.140 --> 01:53.860\n and agreed that there needs to be a global standard\n\n01:53.860 --> 01:56.020\n around the security of transactions.\n\n01:56.020 --> 01:58.420\n Now we just need to do the same for autonomous vehicles\n\n01:58.420 --> 02:00.620\n and AI systems in general.\n\n02:00.620 --> 02:03.260\n So again, if you get Cash App from the App Store\n\n02:03.260 --> 02:07.060\n or Google Play and use the code LEXPODCAST,\n\n02:07.060 --> 02:11.220\n you get $10 and Cash App will also donate $10 to FIRST,\n\n02:11.220 --> 02:14.100\n an organization that is helping to advance robotics\n\n02:14.100 --> 02:17.700\n and STEM education for young people around the world.\n\n02:17.700 --> 02:21.460\n And now here's my conversation with Daphne Koller.\n\n02:22.420 --> 02:25.040\n So you cofounded Coursera and made a huge impact\n\n02:25.040 --> 02:26.660\n in the global education of AI.\n\n02:26.660 --> 02:29.700\n And after five years in August, 2016,\n\n02:29.700 --> 02:33.040\n wrote a blog post saying that you're stepping away\n\n02:33.040 --> 02:34.460\n and wrote, quote,\n\n02:34.460 --> 02:37.500\n it is time for me to turn to another critical challenge,\n\n02:37.500 --> 02:38.940\n the development of machine learning\n\n02:38.940 --> 02:41.700\n and its applications to improving human health.\n\n02:41.700 --> 02:45.140\n So let me ask two far out philosophical questions.\n\n02:45.140 --> 02:48.020\n One, do you think we'll one day find cures\n\n02:48.020 --> 02:50.760\n for all major diseases known today?\n\n02:50.760 --> 02:53.560\n And two, do you think we'll one day figure out\n\n02:53.560 --> 02:55.980\n a way to extend the human lifespan,\n\n02:55.980 --> 02:57.880\n perhaps to the point of immortality?\n\n02:59.460 --> 03:01.780\n So one day is a very long time\n\n03:01.780 --> 03:04.260\n and I don't like to make predictions\n\n03:04.260 --> 03:07.300\n of the type we will never be able to do X\n\n03:07.300 --> 03:12.300\n because I think that's a smacks of hubris.\n\n03:12.740 --> 03:16.140\n It seems that never in the entire eternity\n\n03:16.140 --> 03:19.380\n of human existence will we be able to solve a problem.\n\n03:19.380 --> 03:24.260\n That being said, curing disease is very hard\n\n03:24.260 --> 03:28.540\n because oftentimes by the time you discover the disease,\n\n03:28.540 --> 03:30.560\n a lot of damage has already been done.\n\n03:30.560 --> 03:34.980\n And so to assume that we would be able to cure disease\n\n03:34.980 --> 03:37.620\n at that stage assumes that we would come up with ways\n\n03:37.620 --> 03:41.940\n of basically regenerating entire parts of the human body\n\n03:41.940 --> 03:45.340\n in the way that actually returns it to its original state.\n\n03:45.340 --> 03:47.420\n And that's a very challenging problem.\n\n03:47.420 --> 03:49.420\n We have cured very few diseases.\n\n03:49.420 --> 03:51.460\n We've been able to provide treatment\n\n03:51.460 --> 03:52.940\n for an increasingly large number,\n\n03:52.940 --> 03:54.700\n but the number of things that you could actually define\n\n03:54.700 --> 03:57.620\n to be cures is actually not that large.\n\n03:59.440 --> 04:02.540\n So I think that there's a lot of work\n\n04:02.540 --> 04:05.660\n that would need to happen before one could legitimately say\n\n04:05.660 --> 04:08.820\n that we have cured even a reasonable number,\n\n04:08.820 --> 04:10.460\n far less all diseases.\n\n04:10.460 --> 04:12.780\n On the scale of zero to 100,\n\n04:12.780 --> 04:15.580\n where are we in understanding the fundamental mechanisms\n\n04:15.580 --> 04:18.140\n of all of major diseases?\n\n04:18.140 --> 04:19.260\n What's your sense?\n\n04:19.260 --> 04:21.080\n So from the computer science perspective\n\n04:21.080 --> 04:24.160\n that you've entered the world of health,\n\n04:24.160 --> 04:25.740\n how far along are we?\n\n04:26.740 --> 04:29.520\n I think it depends on which disease.\n\n04:29.520 --> 04:31.780\n I mean, there are ones where I would say\n\n04:31.780 --> 04:33.420\n we're maybe not quite at a hundred\n\n04:33.420 --> 04:35.580\n because biology is really complicated\n\n04:35.580 --> 04:38.960\n and there's always new things that we uncover\n\n04:38.960 --> 04:41.220\n that people didn't even realize existed.\n\n04:43.040 --> 04:44.420\n But I would say there's diseases\n\n04:44.420 --> 04:48.060\n where we might be in the 70s or 80s,\n\n04:48.060 --> 04:51.340\n and then there's diseases in which I would say\n\n04:51.340 --> 04:55.220\n with probably the majority where we're really close to zero.\n\n04:55.220 --> 04:57.980\n Would Alzheimer's and schizophrenia\n\n04:57.980 --> 05:02.980\n and type two diabetes fall closer to zero or to the 80?\n\n05:04.340 --> 05:09.340\n I think Alzheimer's is probably closer to zero than to 80.\n\n05:11.060 --> 05:12.660\n There are hypotheses,\n\n05:12.660 --> 05:17.300\n but I don't think those hypotheses have as of yet\n\n05:17.300 --> 05:21.980\n been sufficiently validated that we believe them to be true.\n\n05:21.980 --> 05:23.780\n And there is an increasing number of people\n\n05:23.780 --> 05:25.900\n who believe that the traditional hypotheses\n\n05:25.900 --> 05:28.020\n might not really explain what's going on.\n\n05:28.020 --> 05:31.700\n I would also say that Alzheimer's and schizophrenia\n\n05:31.700 --> 05:35.300\n and even type two diabetes are not really one disease.\n\n05:35.300 --> 05:39.380\n They're almost certainly a heterogeneous collection\n\n05:39.380 --> 05:43.700\n of mechanisms that manifest in clinically similar ways.\n\n05:43.700 --> 05:46.640\n So in the same way that we now understand\n\n05:46.640 --> 05:48.900\n that breast cancer is really not one disease,\n\n05:48.900 --> 05:53.420\n it is multitude of cellular mechanisms,\n\n05:53.420 --> 05:55.160\n all of which ultimately translate\n\n05:55.160 --> 05:59.340\n to uncontrolled proliferation, but it's not one disease.\n\n05:59.340 --> 06:01.140\n The same is almost undoubtedly true\n\n06:01.140 --> 06:02.900\n for those other diseases as well.\n\n06:02.900 --> 06:05.780\n And that understanding that needs to precede\n\n06:05.780 --> 06:08.460\n any understanding of the specific mechanisms\n\n06:08.460 --> 06:10.100\n of any of those other diseases.\n\n06:10.100 --> 06:11.580\n Now, in schizophrenia, I would say\n\n06:11.580 --> 06:15.220\n we're almost certainly closer to zero than to anything else.\n\n06:15.220 --> 06:18.260\n Type two diabetes is a bit of a mix.\n\n06:18.260 --> 06:21.380\n There are clear mechanisms that are implicated\n\n06:21.380 --> 06:22.980\n that I think have been validated\n\n06:22.980 --> 06:25.260\n that have to do with insulin resistance and such,\n\n06:25.260 --> 06:28.500\n but there's almost certainly there as well\n\n06:28.500 --> 06:31.300\n many mechanisms that we have not yet understood.\n\n06:31.300 --> 06:34.420\n You've also thought and worked a little bit\n\n06:34.420 --> 06:35.860\n on the longevity side.\n\n06:35.860 --> 06:40.260\n Do you see the disease and longevity as overlapping\n\n06:40.260 --> 06:45.260\n completely, partially, or not at all as efforts?\n\n06:45.260 --> 06:48.620\n Those mechanisms are certainly overlapping.\n\n06:48.620 --> 06:51.940\n There's a well known phenomenon that says\n\n06:51.940 --> 06:56.820\n that for most diseases, other than childhood diseases,\n\n06:56.820 --> 07:01.300\n the risk for contracting that disease\n\n07:01.300 --> 07:03.260\n increases exponentially year on year,\n\n07:03.260 --> 07:05.700\n every year from the time you're about 40.\n\n07:05.700 --> 07:09.100\n So obviously there's a connection between those two things.\n\n07:10.380 --> 07:12.420\n That's not to say that they're identical.\n\n07:12.420 --> 07:14.980\n There's clearly aging that happens\n\n07:14.980 --> 07:18.740\n that is not really associated with any specific disease.\n\n07:18.740 --> 07:22.300\n And there's also diseases and mechanisms of disease\n\n07:22.300 --> 07:25.660\n that are not specifically related to aging.\n\n07:25.660 --> 07:29.140\n So I think overlap is where we're at.\n\n07:29.140 --> 07:30.420\n Okay.\n\n07:30.420 --> 07:32.620\n It is a little unfortunate that we get older\n\n07:32.620 --> 07:34.180\n and it seems that there's some correlation\n\n07:34.180 --> 07:39.060\n with the occurrence of diseases\n\n07:39.060 --> 07:40.780\n or the fact that we get older.\n\n07:40.780 --> 07:43.100\n And both are quite sad.\n\n07:43.100 --> 07:46.700\n I mean, there's processes that happen as cells age\n\n07:46.700 --> 07:49.580\n that I think are contributing to disease.\n\n07:49.580 --> 07:52.780\n Some of those have to do with DNA damage\n\n07:52.780 --> 07:54.980\n that accumulates as cells divide\n\n07:54.980 --> 07:59.620\n where the repair mechanisms don't fully correct for those.\n\n07:59.620 --> 08:03.660\n There are accumulations of proteins\n\n08:03.660 --> 08:06.340\n that are misfolded and potentially aggregate\n\n08:06.340 --> 08:08.540\n and those too contribute to disease\n\n08:08.540 --> 08:10.540\n and will contribute to inflammation.\n\n08:10.540 --> 08:14.020\n There's a multitude of mechanisms that have been uncovered\n\n08:14.020 --> 08:17.100\n that are sort of wear and tear at the cellular level\n\n08:17.100 --> 08:19.940\n that contribute to disease processes\n\n08:21.780 --> 08:24.860\n and I'm sure there's many that we don't yet understand.\n\n08:24.860 --> 08:27.340\n On a small tangent and perhaps philosophical,\n\n08:30.220 --> 08:32.340\n the fact that things get older\n\n08:32.340 --> 08:36.580\n and the fact that things die is a very powerful feature\n\n08:36.580 --> 08:38.900\n for the growth of new things.\n\n08:38.900 --> 08:41.380\n It's a learning, it's a kind of learning mechanism.\n\n08:41.380 --> 08:43.700\n So it's both tragic and beautiful.\n\n08:44.660 --> 08:49.660\n So do you, so in trying to fight disease\n\n08:52.140 --> 08:53.900\n and trying to fight aging,\n\n08:55.260 --> 08:58.940\n do you think about sort of the useful fact of our mortality\n\n08:58.940 --> 09:02.660\n or would you, like if you were, could be immortal,\n\n09:02.660 --> 09:04.260\n would you choose to be immortal?\n\n09:07.140 --> 09:10.860\n Again, I think immortal is a very long time\n\n09:10.860 --> 09:15.860\n and I don't know that that would necessarily be something\n\n09:16.020 --> 09:17.900\n that I would want to aspire to\n\n09:17.900 --> 09:22.900\n but I think all of us aspire to an increased health span,\n\n09:24.180 --> 09:27.620\n I would say, which is an increased amount of time\n\n09:27.620 --> 09:29.860\n where you're healthy and active\n\n09:29.860 --> 09:33.300\n and feel as you did when you were 20\n\n09:33.300 --> 09:35.860\n and we're nowhere close to that.\n\n09:36.780 --> 09:41.780\n People deteriorate physically and mentally over time\n\n09:41.820 --> 09:43.660\n and that is a very sad phenomenon.\n\n09:43.660 --> 09:47.300\n So I think a wonderful aspiration would be\n\n09:47.300 --> 09:52.300\n if we could all live to the biblical 120 maybe\n\n09:52.340 --> 09:53.740\n in perfect health.\n\n09:53.740 --> 09:54.820\n In high quality of life.\n\n09:54.820 --> 09:55.860\n High quality of life.\n\n09:55.860 --> 09:57.780\n I think that would be an amazing goal\n\n09:57.780 --> 09:59.300\n for us to achieve as a society\n\n09:59.300 --> 10:03.660\n now is the right age 120 or 100 or 150.\n\n10:03.660 --> 10:05.740\n I think that's up for debate\n\n10:05.740 --> 10:07.660\n but I think an increased health span\n\n10:07.660 --> 10:09.020\n is a really worthy goal.\n\n10:10.100 --> 10:14.700\n And anyway, in a grand time of the age of the universe,\n\n10:14.700 --> 10:16.580\n it's all pretty short.\n\n10:16.580 --> 10:18.460\n So from the perspective,\n\n10:18.460 --> 10:20.980\n you've done obviously a lot of incredible work\n\n10:20.980 --> 10:22.060\n in machine learning.\n\n10:22.060 --> 10:25.140\n So what role do you think data and machine learning\n\n10:25.140 --> 10:29.300\n play in this goal of trying to understand diseases\n\n10:29.300 --> 10:31.820\n and trying to eradicate diseases?\n\n10:32.940 --> 10:35.180\n Up until now, I don't think it's played\n\n10:35.180 --> 10:37.860\n very much of a significant role\n\n10:37.860 --> 10:42.420\n because largely the data sets that one really needed\n\n10:42.420 --> 10:47.300\n to enable a powerful machine learning methods,\n\n10:47.300 --> 10:49.620\n those data sets haven't really existed.\n\n10:49.620 --> 10:50.940\n There's been dribs and drabs\n\n10:50.940 --> 10:53.300\n and some interesting machine learning\n\n10:53.300 --> 10:55.700\n that has been applied, I would say machine learning\n\n10:55.700 --> 10:57.660\n slash data science,\n\n10:57.660 --> 11:00.180\n but the last few years are starting to change that.\n\n11:00.180 --> 11:05.180\n So we now see an increase in some large data sets\n\n11:06.300 --> 11:11.300\n but equally importantly, an increase in technologies\n\n11:11.340 --> 11:14.700\n that are able to produce data at scale.\n\n11:14.700 --> 11:19.340\n It's not typically the case that people have deliberately\n\n11:19.340 --> 11:21.420\n proactively used those tools\n\n11:21.420 --> 11:24.180\n for the purpose of generating data for machine learning.\n\n11:24.180 --> 11:26.540\n They, to the extent that those techniques\n\n11:26.540 --> 11:28.540\n have been used for data production,\n\n11:28.540 --> 11:29.860\n they've been used for data production\n\n11:29.860 --> 11:31.300\n to drive scientific discovery\n\n11:31.300 --> 11:34.420\n and the machine learning came as a sort of byproduct\n\n11:34.420 --> 11:36.900\n second stage of, oh, you know, now we have a data set,\n\n11:36.900 --> 11:38.260\n let's do machine learning on that\n\n11:38.260 --> 11:41.820\n rather than a more simplistic data analysis method.\n\n11:41.820 --> 11:44.420\n But what we are doing in Citro\n\n11:44.420 --> 11:46.780\n is actually flipping that around and saying,\n\n11:46.780 --> 11:50.300\n here's this incredible repertoire of methods\n\n11:50.300 --> 11:54.580\n that bioengineers, cell biologists have come up with,\n\n11:54.580 --> 11:57.420\n let's see if we can put them together in brand new ways\n\n11:57.420 --> 12:00.260\n with the goal of creating data sets\n\n12:00.260 --> 12:03.380\n that machine learning can really be applied on productively\n\n12:03.380 --> 12:06.580\n to create powerful predictive models\n\n12:06.580 --> 12:08.460\n that can help us address fundamental problems\n\n12:08.460 --> 12:09.420\n in human health.\n\n12:09.420 --> 12:14.420\n So really focus to get, make data the primary focus\n\n12:14.500 --> 12:16.460\n and the primary goal and find,\n\n12:16.460 --> 12:18.900\n use the mechanisms of biology and chemistry\n\n12:18.900 --> 12:23.340\n to create the kinds of data set\n\n12:23.340 --> 12:25.700\n that could allow machine learning to benefit the most.\n\n12:25.700 --> 12:27.580\n I wouldn't put it in those terms\n\n12:27.580 --> 12:30.460\n because that says that data is the end goal.\n\n12:30.460 --> 12:32.140\n Data is the means.\n\n12:32.140 --> 12:35.740\n So for us, the end goal is helping address challenges\n\n12:35.740 --> 12:39.980\n in human health and the method that we've elected to do that\n\n12:39.980 --> 12:44.140\n is to apply machine learning to build predictive models\n\n12:44.140 --> 12:45.980\n and machine learning, in my opinion,\n\n12:45.980 --> 12:48.820\n can only be really successfully applied\n\n12:48.820 --> 12:50.700\n especially the more powerful models\n\n12:50.700 --> 12:53.540\n if you give it data that is of sufficient scale\n\n12:53.540 --> 12:54.540\n and sufficient quality.\n\n12:54.540 --> 12:58.580\n So how do you create those data sets\n\n12:58.580 --> 13:03.580\n so as to drive the ability to generate predictive models\n\n13:03.700 --> 13:05.740\n which subsequently help improve human health?\n\n13:05.740 --> 13:08.700\n So before we dive into the details of that,\n\n13:08.700 --> 13:13.700\n let me take a step back and ask when and where\n\n13:13.820 --> 13:16.780\n was your interest in human health born?\n\n13:16.780 --> 13:19.900\n Are there moments, events, perhaps if I may ask,\n\n13:19.900 --> 13:23.060\n tragedies in your own life that catalyzes passion\n\n13:23.060 --> 13:26.580\n or was it the broader desire to help humankind?\n\n13:26.580 --> 13:29.180\n So I would say it's a bit of both.\n\n13:29.180 --> 13:32.620\n So on, I mean, my interest in human health\n\n13:32.620 --> 13:37.780\n actually dates back to the early 2000s\n\n13:37.780 --> 13:42.780\n when a lot of my peers in machine learning\n\n13:43.940 --> 13:45.500\n and I were using data sets\n\n13:45.500 --> 13:47.420\n that frankly were not very inspiring.\n\n13:47.420 --> 13:49.820\n Some of us old timers still remember\n\n13:49.820 --> 13:52.300\n the quote unquote 20 news groups data set\n\n13:52.300 --> 13:55.740\n where this was literally a bunch of texts\n\n13:55.740 --> 13:57.100\n from 20 news groups,\n\n13:57.100 --> 13:59.260\n a concept that doesn't really even exist anymore.\n\n13:59.260 --> 14:01.660\n And the question was, can you classify\n\n14:01.660 --> 14:06.660\n which news group a particular bag of words came from?\n\n14:06.780 --> 14:08.700\n And it wasn't very interesting.\n\n14:08.700 --> 14:12.460\n The data sets at the time on the biology side\n\n14:12.460 --> 14:14.020\n were much more interesting,\n\n14:14.020 --> 14:15.540\n both from a technical and also from\n\n14:15.540 --> 14:17.540\n an aspirational perspective.\n\n14:17.540 --> 14:18.860\n They were still pretty small,\n\n14:18.860 --> 14:20.740\n but they were better than 20 news groups.\n\n14:20.740 --> 14:25.620\n And so I started out, I think just by wanting\n\n14:25.620 --> 14:27.860\n to do something that was more, I don't know,\n\n14:27.860 --> 14:30.780\n societally useful and technically interesting.\n\n14:30.780 --> 14:34.420\n And then over time became more and more interested\n\n14:34.420 --> 14:39.420\n in the biology and the human health aspects for themselves\n\n14:40.220 --> 14:43.460\n and began to work even sometimes on papers\n\n14:43.460 --> 14:45.140\n that were just in biology\n\n14:45.140 --> 14:48.460\n without having a significant machine learning component.\n\n14:48.460 --> 14:52.740\n I think my interest in drug discovery\n\n14:52.740 --> 14:57.740\n is partly due to an incident I had with\n\n14:58.580 --> 15:02.580\n when my father sadly passed away about 12 years ago.\n\n15:02.580 --> 15:07.060\n He had an autoimmune disease that settled in his lungs\n\n15:08.900 --> 15:11.460\n and the doctors basically said,\n\n15:11.460 --> 15:13.380\n well, there's only one thing that we could do,\n\n15:13.380 --> 15:15.020\n which is give him prednisone.\n\n15:15.020 --> 15:17.780\n At some point, I remember a doctor even came and said,\n\n15:17.780 --> 15:19.620\n hey, let's do a lung biopsy to figure out\n\n15:19.620 --> 15:20.940\n which autoimmune disease he has.\n\n15:20.940 --> 15:23.180\n And I said, would that be helpful?\n\n15:23.180 --> 15:24.020\n Would that change treatment?\n\n15:24.020 --> 15:25.500\n He said, no, there's only prednisone.\n\n15:25.500 --> 15:27.180\n That's the only thing we can give him.\n\n15:27.180 --> 15:29.900\n And I had friends who were rheumatologists who said\n\n15:29.900 --> 15:32.060\n the FDA would never approve prednisone today\n\n15:32.060 --> 15:37.060\n because the ratio of side effects to benefit\n\n15:37.260 --> 15:39.580\n is probably not large enough.\n\n15:39.580 --> 15:44.580\n Today, we're in a state where there's probably four or five,\n\n15:44.860 --> 15:48.740\n maybe even more, well, it depends for which autoimmune disease,\n\n15:48.740 --> 15:52.940\n but there are multiple drugs that can help people\n\n15:52.940 --> 15:53.980\n with autoimmune disease,\n\n15:53.980 --> 15:56.740\n many of which didn't exist 12 years ago.\n\n15:56.740 --> 16:00.380\n And I think we're at a golden time in some ways\n\n16:00.380 --> 16:05.380\n in drug discovery where there's the ability to create drugs\n\n16:05.380 --> 16:10.380\n that are much more safe and much more effective\n\n16:10.580 --> 16:13.060\n than we've ever been able to before.\n\n16:13.060 --> 16:16.340\n And what's lacking is enough understanding\n\n16:16.340 --> 16:21.340\n of biology and mechanism to know where to aim that engine.\n\n16:22.300 --> 16:25.380\n And I think that's where machine learning can help.\n\n16:25.380 --> 16:29.900\n So in 2018, you started and now lead a company in Citro,\n\n16:29.900 --> 16:32.580\n which is, like you mentioned,\n\n16:32.580 --> 16:34.740\n perhaps the focus is drug discovery\n\n16:34.740 --> 16:38.140\n and the utilization of machine learning for drug discovery.\n\n16:38.140 --> 16:40.620\n So you mentioned that, quote,\n\n16:40.620 --> 16:42.100\n we're really interested in creating\n\n16:42.100 --> 16:45.580\n what you might call a disease in a dish model,\n\n16:45.580 --> 16:47.380\n disease in a dish models,\n\n16:47.380 --> 16:49.180\n places where diseases are complex,\n\n16:49.180 --> 16:52.220\n where we really haven't had a good model system,\n\n16:52.220 --> 16:55.020\n where typical animal models that have been used for years,\n\n16:55.020 --> 16:58.860\n including testing on mice, just aren't very effective.\n\n16:58.860 --> 17:02.640\n So can you try to describe what is an animal model\n\n17:02.640 --> 17:05.340\n and what is a disease in a dish model?\n\n17:05.340 --> 17:06.260\n Sure.\n\n17:06.260 --> 17:09.300\n So an animal model for disease\n\n17:09.300 --> 17:13.000\n is where you create effectively,\n\n17:13.860 --> 17:14.900\n it's what it sounds like.\n\n17:14.900 --> 17:19.300\n It's oftentimes a mouse where we have introduced\n\n17:19.300 --> 17:22.780\n some external perturbation that creates the disease\n\n17:22.780 --> 17:26.300\n and then we cure that disease.\n\n17:26.300 --> 17:28.740\n And the hope is that by doing that,\n\n17:28.740 --> 17:31.340\n we will cure a similar disease in the human.\n\n17:31.340 --> 17:33.500\n The problem is that oftentimes\n\n17:33.500 --> 17:36.900\n the way in which we generate the disease in the animal\n\n17:36.900 --> 17:38.560\n has nothing to do with how that disease\n\n17:38.560 --> 17:40.900\n actually comes about in a human.\n\n17:40.900 --> 17:44.420\n It's what you might think of as a copy of the phenotype,\n\n17:44.420 --> 17:46.740\n a copy of the clinical outcome,\n\n17:46.740 --> 17:48.740\n but the mechanisms are quite different.\n\n17:48.740 --> 17:52.120\n And so curing the disease in the animal,\n\n17:52.120 --> 17:54.880\n which in most cases doesn't happen naturally,\n\n17:54.880 --> 17:57.180\n mice don't get Alzheimer's, they don't get diabetes,\n\n17:57.180 --> 17:58.700\n they don't get atherosclerosis,\n\n17:58.700 --> 18:01.280\n they don't get autism or schizophrenia.\n\n18:02.580 --> 18:05.700\n Those cures don't translate over\n\n18:05.700 --> 18:08.140\n to what happens in the human.\n\n18:08.140 --> 18:10.860\n And that's where most drugs fails\n\n18:10.860 --> 18:13.700\n just because the findings that we had in the mouse\n\n18:13.700 --> 18:15.060\n don't translate to a human.\n\n18:16.660 --> 18:20.860\n The disease in the dish models is a fairly new approach.\n\n18:20.860 --> 18:24.140\n It's been enabled by technologies\n\n18:24.140 --> 18:28.420\n that have not existed for more than five to 10 years.\n\n18:28.420 --> 18:32.780\n So for instance, the ability for us to take a cell\n\n18:32.780 --> 18:35.540\n from any one of us, you or me,\n\n18:35.540 --> 18:39.960\n revert that say skin cell to what's called stem cell status,\n\n18:39.960 --> 18:44.740\n which is what's called the pluripotent cell\n\n18:44.740 --> 18:46.600\n that can then be differentiated\n\n18:46.600 --> 18:47.860\n into different types of cells.\n\n18:47.860 --> 18:49.800\n So from that pluripotent cell,\n\n18:49.800 --> 18:54.300\n one can create a Lex neuron or a Lex cardiomyocyte\n\n18:54.300 --> 18:57.760\n or a Lex hepatocyte that has your genetics,\n\n18:57.760 --> 19:00.020\n but that right cell type.\n\n19:00.020 --> 19:04.780\n And so if there's a genetic burden of disease\n\n19:04.780 --> 19:07.180\n that would manifest in that particular cell type,\n\n19:07.180 --> 19:10.300\n you might be able to see it by looking at those cells\n\n19:10.300 --> 19:13.380\n and saying, oh, that's what potentially sick cells\n\n19:13.380 --> 19:15.620\n look like versus healthy cells\n\n19:15.620 --> 19:20.620\n and then explore what kind of interventions\n\n19:20.740 --> 19:24.860\n might revert the unhealthy looking cell to a healthy cell.\n\n19:24.860 --> 19:27.740\n Now, of course, curing cells is not the same\n\n19:27.740 --> 19:28.880\n as curing people.\n\n19:29.820 --> 19:33.220\n And so there's still potentially a translatability gap,\n\n19:33.220 --> 19:38.220\n but at least for diseases that are driven,\n\n19:38.500 --> 19:41.980\n say by human genetics and where the human genetics\n\n19:41.980 --> 19:43.780\n is what drives the cellular phenotype,\n\n19:43.780 --> 19:47.960\n there is some reason to hope that if we revert those cells\n\n19:47.960 --> 19:49.600\n in which the disease begins\n\n19:49.600 --> 19:52.180\n and where the disease is driven by genetics\n\n19:52.180 --> 19:55.260\n and we can revert that cell back to a healthy state,\n\n19:55.260 --> 19:58.140\n maybe that will help also revert\n\n19:58.140 --> 20:00.860\n the more global clinical phenotype.\n\n20:00.860 --> 20:02.740\n So that's really what we're hoping to do.\n\n20:02.740 --> 20:06.020\n That step, that backward step, I was reading about it,\n\n20:06.020 --> 20:08.300\n the Yamanaka factor.\n\n20:08.300 --> 20:09.700\n Yes.\n\n20:09.700 --> 20:12.280\n So it's like that reverse step back to stem cells.\n\n20:12.280 --> 20:13.120\n Yes.\n\n20:13.120 --> 20:14.180\n Seems like magic.\n\n20:14.180 --> 20:15.740\n It is.\n\n20:15.740 --> 20:17.660\n Honestly, before that happened,\n\n20:17.660 --> 20:20.120\n I think very few people would have predicted\n\n20:20.120 --> 20:21.700\n that to be possible.\n\n20:21.700 --> 20:22.540\n It's amazing.\n\n20:22.540 --> 20:25.180\n Can you maybe elaborate, is it actually possible?\n\n20:25.180 --> 20:27.300\n Like where, like how stable?\n\n20:27.300 --> 20:29.380\n So this result was maybe like,\n\n20:29.380 --> 20:30.580\n I don't know how many years ago,\n\n20:30.580 --> 20:32.700\n maybe 10 years ago was first demonstrated,\n\n20:32.700 --> 20:33.860\n something like that.\n\n20:33.860 --> 20:35.520\n Is this, how hard is this?\n\n20:35.520 --> 20:37.500\n Like how noisy is this backward step?\n\n20:37.500 --> 20:39.460\n It seems quite incredible and cool.\n\n20:39.460 --> 20:42.220\n It is, it is incredible and cool.\n\n20:42.220 --> 20:46.420\n It was much more, I think finicky and bespoke\n\n20:46.420 --> 20:49.980\n at the early stages when the discovery was first made.\n\n20:49.980 --> 20:54.500\n But at this point, it's become almost industrialized.\n\n20:54.500 --> 20:59.440\n There are what's called contract research organizations,\n\n20:59.440 --> 21:02.300\n vendors that will take a sample from a human\n\n21:02.300 --> 21:04.460\n and revert it back to stem cell status.\n\n21:04.460 --> 21:07.120\n And it works a very good fraction of the time.\n\n21:07.120 --> 21:10.360\n Now there are people who will ask,\n\n21:10.360 --> 21:12.060\n I think good questions.\n\n21:12.060 --> 21:15.340\n Is this really truly a stem cell or does it remember\n\n21:15.340 --> 21:17.860\n certain aspects of what,\n\n21:17.860 --> 21:22.500\n of changes that were made in the human beyond the genetics?\n\n21:22.500 --> 21:24.660\n It's passed as a skin cell, yeah.\n\n21:24.660 --> 21:26.740\n It's passed as a skin cell or it's passed\n\n21:26.740 --> 21:29.920\n in terms of exposures to different environmental factors\n\n21:29.920 --> 21:30.920\n and so on.\n\n21:30.920 --> 21:33.300\n So I think the consensus right now\n\n21:33.300 --> 21:36.420\n is that these are not always perfect\n\n21:36.420 --> 21:40.020\n and there is little bits and pieces of memory sometimes,\n\n21:40.020 --> 21:43.560\n but by and large, these are actually pretty good.\n\n21:44.780 --> 21:47.260\n So one of the key things,\n\n21:47.260 --> 21:48.740\n well, maybe you can correct me,\n\n21:48.740 --> 21:50.900\n but one of the useful things for machine learning\n\n21:50.900 --> 21:54.180\n is size, scale of data.\n\n21:54.180 --> 21:59.100\n How easy it is to do these kinds of reversals to stem cells\n\n21:59.100 --> 22:02.360\n and then disease in a dish models at scale.\n\n22:02.360 --> 22:05.320\n Is that a huge challenge or not?\n\n22:06.180 --> 22:11.180\n So the reversal is not as of this point\n\n22:11.660 --> 22:14.220\n something that can be done at the scale\n\n22:14.220 --> 22:18.540\n of tens of thousands or hundreds of thousands.\n\n22:18.540 --> 22:22.260\n I think total number of stem cells or IPS cells\n\n22:22.260 --> 22:25.260\n that are what's called induced pluripotent stem cells\n\n22:25.260 --> 22:30.220\n in the world I think is somewhere between five and 10,000\n\n22:30.220 --> 22:31.460\n last I looked.\n\n22:31.460 --> 22:34.460\n Now again, that might not count things that exist\n\n22:34.460 --> 22:36.260\n in this or that academic center\n\n22:36.260 --> 22:37.860\n and they may add up to a bit more,\n\n22:37.860 --> 22:40.060\n but that's about the range.\n\n22:40.060 --> 22:42.180\n So it's not something that you could at this point\n\n22:42.180 --> 22:45.540\n generate IPS cells from a million people,\n\n22:45.540 --> 22:47.900\n but maybe you don't need to\n\n22:47.900 --> 22:51.820\n because maybe that background is enough\n\n22:51.820 --> 22:56.140\n because it can also be now perturbed in different ways.\n\n22:56.140 --> 23:00.100\n And some people have done really interesting experiments\n\n23:00.100 --> 23:05.100\n in for instance, taking cells from a healthy human\n\n23:05.660 --> 23:08.540\n and then introducing a mutation into it\n\n23:08.540 --> 23:11.860\n using one of the other miracle technologies\n\n23:11.860 --> 23:13.820\n that's emerged in the last decade\n\n23:13.820 --> 23:16.140\n which is CRISPR gene editing\n\n23:16.140 --> 23:19.660\n and introduced a mutation that is known to be pathogenic.\n\n23:19.660 --> 23:22.420\n And so you can now look at the healthy cells\n\n23:22.420 --> 23:24.740\n and the unhealthy cells, the one with the mutation\n\n23:24.740 --> 23:26.100\n and do a one on one comparison\n\n23:26.100 --> 23:28.380\n where everything else is held constant.\n\n23:28.380 --> 23:31.820\n And so you could really start to understand specifically\n\n23:31.820 --> 23:34.380\n what the mutation does at the cellular level.\n\n23:34.380 --> 23:37.700\n So the IPS cells are a great starting point\n\n23:37.700 --> 23:39.820\n and obviously more diversity is better\n\n23:39.820 --> 23:42.380\n because you also wanna capture ethnic background\n\n23:42.380 --> 23:43.580\n and how that affects things,\n\n23:43.580 --> 23:46.780\n but maybe you don't need one from every single patient\n\n23:46.780 --> 23:48.100\n with every single type of disease\n\n23:48.100 --> 23:50.260\n because we have other tools at our disposal.\n\n23:50.260 --> 23:52.580\n Well, how much difference is there between people\n\n23:52.580 --> 23:54.940\n I mentioned ethnic background in terms of IPS cells?\n\n23:54.940 --> 23:59.380\n So we're all like, it seems like these magical cells\n\n23:59.380 --> 24:01.860\n that can do to create anything\n\n24:01.860 --> 24:04.020\n between different populations, different people.\n\n24:04.020 --> 24:07.020\n Is there a lot of variability between cell cells?\n\n24:07.020 --> 24:09.580\n Well, first of all, there's the variability,\n\n24:09.580 --> 24:10.980\n that's driven simply by the fact\n\n24:10.980 --> 24:13.420\n that genetically we're different.\n\n24:13.420 --> 24:15.820\n So a stem cell that's derived from my genotype\n\n24:15.820 --> 24:18.340\n is gonna be different from a stem cell\n\n24:18.340 --> 24:20.540\n that's derived from your genotype.\n\n24:20.540 --> 24:23.700\n There's also some differences that have more to do with\n\n24:23.700 --> 24:27.260\n for whatever reason, some people's stem cells\n\n24:27.260 --> 24:29.860\n differentiate better than other people's stem cells.\n\n24:29.860 --> 24:31.500\n We don't entirely understand why.\n\n24:31.500 --> 24:34.180\n So there's certainly some differences there as well,\n\n24:34.180 --> 24:35.460\n but the fundamental difference\n\n24:35.460 --> 24:38.740\n and the one that we really care about and is a positive\n\n24:38.740 --> 24:43.220\n is that the fact that the genetics are different\n\n24:43.220 --> 24:45.980\n and therefore recapitulate my disease burden\n\n24:45.980 --> 24:47.780\n versus your disease burden.\n\n24:47.780 --> 24:49.260\n What's a disease burden?\n\n24:49.260 --> 24:52.300\n Well, a disease burden is just if you think,\n\n24:52.300 --> 24:55.060\n I mean, it's not a well defined mathematical term,\n\n24:55.060 --> 24:58.260\n although there are mathematical formulations of it.\n\n24:58.260 --> 25:01.500\n If you think about the fact that some of us are more likely\n\n25:01.500 --> 25:03.460\n to get a certain disease than others\n\n25:03.460 --> 25:07.300\n because we have more variations in our genome\n\n25:07.300 --> 25:09.500\n that are causative of the disease,\n\n25:09.500 --> 25:12.620\n maybe fewer that are protective of the disease.\n\n25:12.620 --> 25:14.860\n People have quantified that\n\n25:14.860 --> 25:17.860\n using what are called polygenic risk scores,\n\n25:17.860 --> 25:20.820\n which look at all of the variations\n\n25:20.820 --> 25:23.620\n in an individual person's genome\n\n25:23.620 --> 25:26.620\n and add them all up in terms of how much risk they confer\n\n25:26.620 --> 25:27.820\n for a particular disease.\n\n25:27.820 --> 25:30.540\n And then they've put people on a spectrum\n\n25:30.540 --> 25:32.540\n of their disease risk.\n\n25:32.540 --> 25:36.500\n And for certain diseases where we've been sufficiently\n\n25:36.500 --> 25:38.740\n powered to really understand the connection\n\n25:38.740 --> 25:41.580\n between the many, many small variations\n\n25:41.580 --> 25:44.940\n that give rise to an increased disease risk,\n\n25:44.940 --> 25:47.060\n there's some pretty significant differences\n\n25:47.060 --> 25:49.300\n in terms of the risk between the people,\n\n25:49.300 --> 25:52.060\n say at the highest decile of this polygenic risk score\n\n25:52.060 --> 25:53.500\n and the people at the lowest decile.\n\n25:53.500 --> 25:58.500\n Sometimes those differences are factor of 10 or 12 higher.\n\n25:58.940 --> 26:03.940\n So there's definitely a lot that our genetics\n\n26:03.940 --> 26:07.100\n contributes to disease risk, even if it's not\n\n26:07.100 --> 26:09.100\n by any stretch the full explanation.\n\n26:09.100 --> 26:10.500\n And from a machine learning perspective,\n\n26:10.500 --> 26:12.020\n there's signal there.\n\n26:12.020 --> 26:14.780\n There is definitely signal in the genetics\n\n26:14.780 --> 26:19.100\n and there's even more signal, we believe,\n\n26:19.100 --> 26:21.540\n in looking at the cells that are derived\n\n26:21.540 --> 26:25.540\n from those different genetics because in principle,\n\n26:25.540 --> 26:28.660\n you could say all the signal is there at the genetics level.\n\n26:28.660 --> 26:30.180\n So we don't need to look at the cells,\n\n26:30.180 --> 26:34.100\n but our understanding of the biology is so limited at this\n\n26:34.100 --> 26:37.100\n point than seeing what actually happens at the cellular\n\n26:37.100 --> 26:41.780\n level is a heck of a lot closer to the human clinical outcome\n\n26:41.780 --> 26:44.620\n than looking at the genetics directly.\n\n26:44.620 --> 26:47.180\n And so we can learn a lot more from it\n\n26:47.180 --> 26:49.420\n than we could by looking at genetics alone.\n\n26:49.420 --> 26:51.660\n So just to get a sense, I don't know if it's easy to do,\n\n26:51.660 --> 26:54.220\n but what kind of data is useful\n\n26:54.220 --> 26:56.220\n in this disease in a dish model?\n\n26:56.220 --> 26:59.940\n Like what's the source of raw data information?\n\n26:59.940 --> 27:03.900\n And also from my outsider's perspective,\n\n27:03.900 --> 27:08.620\n so biology and cells are squishy things.\n\n27:08.620 --> 27:13.620\n And then how do you connect the computer to that?\n\n27:15.620 --> 27:17.780\n Which sensory mechanisms, I guess.\n\n27:17.780 --> 27:20.660\n So that's another one of those revolutions\n\n27:20.660 --> 27:22.540\n that have happened in the last 10 years\n\n27:22.540 --> 27:27.540\n in that our ability to measure cells very quantitatively\n\n27:27.540 --> 27:30.020\n has also dramatically increased.\n\n27:30.020 --> 27:35.020\n So back when I started doing biology in the late 90s,\n\n27:35.260 --> 27:40.260\n early 2000s, that was the initial era\n\n27:40.820 --> 27:42.500\n where we started to measure biology\n\n27:42.500 --> 27:46.420\n in really quantitative ways using things like microarrays,\n\n27:46.420 --> 27:50.580\n where you would measure in a single experiment\n\n27:50.580 --> 27:53.820\n the activity level, what's called expression level\n\n27:53.820 --> 27:56.980\n of every gene in the genome in that sample.\n\n27:56.980 --> 28:00.340\n And that ability is what actually allowed us\n\n28:00.340 --> 28:04.180\n to even understand that there are molecular subtypes\n\n28:04.180 --> 28:06.820\n of diseases like cancer, where up until that point,\n\n28:06.820 --> 28:09.220\n it's like, oh, you have breast cancer.\n\n28:09.220 --> 28:13.180\n But then when we looked at the molecular data,\n\n28:13.180 --> 28:14.940\n it was clear that there's different subtypes\n\n28:14.940 --> 28:17.460\n of breast cancer that at the level of gene activity\n\n28:17.460 --> 28:19.380\n look completely different to each other.\n\n28:20.660 --> 28:23.100\n So that was the beginning of this process.\n\n28:23.100 --> 28:26.900\n Now we have the ability to measure individual cells\n\n28:26.900 --> 28:28.860\n in terms of their gene activity\n\n28:28.860 --> 28:31.340\n using what's called single cell RNA sequencing,\n\n28:31.340 --> 28:35.020\n which basically sequences the RNA,\n\n28:35.020 --> 28:37.980\n which is that activity level of different genes\n\n28:39.300 --> 28:40.980\n for every gene in the genome.\n\n28:40.980 --> 28:42.700\n And you could do that at single cell level.\n\n28:42.700 --> 28:45.380\n So that's an incredibly powerful way of measuring cells.\n\n28:45.380 --> 28:47.860\n I mean, you literally count the number of transcripts.\n\n28:47.860 --> 28:50.020\n So it really turns that squishy thing\n\n28:50.020 --> 28:51.820\n into something that's digital.\n\n28:51.820 --> 28:55.100\n Another tremendous data source that's emerged\n\n28:55.100 --> 28:57.860\n in the last few years is microscopy\n\n28:57.860 --> 29:00.580\n and specifically even super resolution microscopy,\n\n29:00.580 --> 29:03.460\n where you could use digital reconstruction\n\n29:03.460 --> 29:06.460\n to look at subcellular structures,\n\n29:06.460 --> 29:08.380\n sometimes even things that are below\n\n29:08.380 --> 29:10.540\n the diffraction limit of light\n\n29:10.540 --> 29:13.340\n by doing a sophisticated reconstruction.\n\n29:13.340 --> 29:16.500\n And again, that gives you a tremendous amount of information\n\n29:16.500 --> 29:18.420\n at the subcellular level.\n\n29:18.420 --> 29:22.860\n There's now more and more ways that amazing scientists\n\n29:22.860 --> 29:27.540\n out there are developing for getting new types\n\n29:27.540 --> 29:30.820\n of information from even single cells.\n\n29:30.820 --> 29:35.500\n And so that is a way of turning those squishy things\n\n29:35.500 --> 29:37.260\n into digital data.\n\n29:37.260 --> 29:38.660\n Into beautiful data sets.\n\n29:38.660 --> 29:42.540\n But so that data set then with machine learning tools\n\n29:42.540 --> 29:45.820\n allows you to maybe understand the developmental,\n\n29:45.820 --> 29:49.900\n like the mechanism of a particular disease.\n\n29:49.900 --> 29:54.300\n And if it's possible to sort of at a high level describe,\n\n29:54.300 --> 29:59.300\n how does that help lead to a drug discovery\n\n30:01.180 --> 30:05.380\n that can help prevent, reverse that mechanism?\n\n30:05.380 --> 30:08.180\n So I think there's different ways in which this data\n\n30:08.180 --> 30:10.420\n could potentially be used.\n\n30:10.420 --> 30:13.820\n Some people use it for scientific discovery\n\n30:13.820 --> 30:17.060\n and say, oh, look, we see this phenotype\n\n30:17.060 --> 30:20.060\n at the cellular level.\n\n30:20.060 --> 30:22.940\n So let's try and work our way backwards\n\n30:22.940 --> 30:26.100\n and think which genes might be involved in pathways\n\n30:26.100 --> 30:27.060\n that give rise to that.\n\n30:27.060 --> 30:32.060\n So that's a very sort of analytical method\n\n30:32.380 --> 30:35.140\n to sort of work our way backwards\n\n30:35.140 --> 30:37.580\n using our understanding of known biology.\n\n30:38.500 --> 30:41.540\n Some people use it in a somewhat more,\n\n30:44.100 --> 30:46.580\n sort of forward, if that was a backward,\n\n30:46.580 --> 30:48.140\n this would be forward, which is to say,\n\n30:48.140 --> 30:50.260\n okay, if I can perturb this gene,\n\n30:51.140 --> 30:54.060\n does it show a phenotype that is similar\n\n30:54.060 --> 30:56.020\n to what I see in disease patients?\n\n30:56.020 --> 30:58.980\n And so maybe that gene is actually causal of the disease.\n\n30:58.980 --> 31:00.180\n So that's a different way.\n\n31:00.180 --> 31:01.580\n And then there's what we do,\n\n31:01.580 --> 31:06.260\n which is basically to take that very large collection\n\n31:06.260 --> 31:10.660\n of data and use machine learning to uncover the patterns\n\n31:10.660 --> 31:12.340\n that emerge from it.\n\n31:12.340 --> 31:14.900\n So for instance, what are those subtypes\n\n31:14.900 --> 31:18.620\n that might be similar at the human clinical outcome,\n\n31:18.620 --> 31:21.740\n but quite distinct when you look at the molecular data?\n\n31:21.740 --> 31:25.140\n And then if we can identify such a subtype,\n\n31:25.140 --> 31:27.980\n are there interventions that if I apply it\n\n31:27.980 --> 31:32.060\n to cells that come from this subtype of the disease\n\n31:32.060 --> 31:34.140\n and you apply that intervention,\n\n31:34.140 --> 31:38.820\n it could be a drug or it could be a CRISPR gene intervention,\n\n31:38.820 --> 31:41.340\n does it revert the disease state\n\n31:41.340 --> 31:42.980\n to something that looks more like normal,\n\n31:42.980 --> 31:44.100\n happy, healthy cells?\n\n31:44.100 --> 31:46.900\n And so hopefully if you see that,\n\n31:46.900 --> 31:50.380\n that gives you a certain hope\n\n31:50.380 --> 31:53.100\n that that intervention will also have\n\n31:53.100 --> 31:55.100\n a meaningful clinical benefit to people.\n\n31:55.100 --> 31:56.580\n And there's obviously a bunch of things\n\n31:56.580 --> 31:58.740\n that you would wanna do after that to validate that,\n\n31:58.740 --> 32:03.740\n but it's a very different and much less hypothesis driven way\n\n32:03.900 --> 32:06.100\n of uncovering new potential interventions\n\n32:06.100 --> 32:10.100\n and might give rise to things that are not the same things\n\n32:10.100 --> 32:12.460\n that everyone else is already looking at.\n\n32:12.460 --> 32:16.780\n That's, I don't know, I'm just like to psychoanalyze\n\n32:16.780 --> 32:18.700\n my own feeling about our discussion currently.\n\n32:18.700 --> 32:21.500\n It's so exciting to talk about sort of a machine,\n\n32:21.500 --> 32:23.780\n fundamentally, well, something that's been turned\n\n32:23.780 --> 32:25.900\n into a machine learning problem\n\n32:25.900 --> 32:29.140\n and that says can have so much real world impact.\n\n32:29.140 --> 32:30.340\n That's how I feel too.\n\n32:30.340 --> 32:32.260\n That's kind of exciting because I'm so,\n\n32:32.260 --> 32:35.740\n most of my day is spent with data sets\n\n32:35.740 --> 32:37.900\n that I guess closer to the news groups.\n\n32:39.060 --> 32:41.980\n So this is a kind of, it just feels good to talk about.\n\n32:41.980 --> 32:45.340\n In fact, I almost don't wanna talk about machine learning.\n\n32:45.340 --> 32:47.460\n I wanna talk about the fundamentals of the data set,\n\n32:47.460 --> 32:50.420\n which is an exciting place to be.\n\n32:50.420 --> 32:51.740\n I agree with you.\n\n32:51.740 --> 32:53.740\n It's what gets me up in the morning.\n\n32:53.740 --> 32:57.140\n It's also what attracts a lot of the people\n\n32:57.140 --> 32:59.140\n who work at InCetro to InCetro\n\n32:59.140 --> 33:01.660\n because I think all of the,\n\n33:01.660 --> 33:03.220\n certainly all of our machine learning people\n\n33:03.220 --> 33:08.220\n are outstanding and could go get a job selling ads online\n\n33:08.220 --> 33:12.500\n or doing eCommerce or even self driving cars.\n\n33:12.500 --> 33:17.500\n But I think they would want, they come to us\n\n33:17.860 --> 33:20.020\n because they want to work on something\n\n33:20.020 --> 33:22.380\n that has more of an aspirational nature\n\n33:22.380 --> 33:24.740\n and can really benefit humanity.\n\n33:24.740 --> 33:28.300\n What, with these approaches, what do you hope,\n\n33:28.300 --> 33:31.140\n what kind of diseases can be helped?\n\n33:31.140 --> 33:33.940\n We mentioned Alzheimer's, schizophrenia, type 2 diabetes.\n\n33:33.940 --> 33:36.540\n Can you just describe the various kinds of diseases\n\n33:36.540 --> 33:38.580\n that this approach can help?\n\n33:38.580 --> 33:39.620\n Well, we don't know.\n\n33:39.620 --> 33:43.900\n And I try and be very cautious about making promises\n\n33:43.900 --> 33:46.620\n about some things that, oh, we will cure X.\n\n33:46.620 --> 33:48.060\n People make that promise.\n\n33:48.060 --> 33:52.700\n And I think it's, I tried to first deliver and then promise\n\n33:52.700 --> 33:54.460\n as opposed to the other way around.\n\n33:54.460 --> 33:57.340\n There are characteristics of a disease\n\n33:57.340 --> 34:00.580\n that make it more likely that this type of approach\n\n34:00.580 --> 34:02.700\n can potentially be helpful.\n\n34:02.700 --> 34:04.580\n So for instance, diseases that have\n\n34:04.580 --> 34:08.820\n a very strong genetic basis are ones\n\n34:08.820 --> 34:10.940\n that are more likely to manifest\n\n34:10.940 --> 34:12.820\n in a stem cell derived model.\n\n34:13.860 --> 34:16.300\n We would want the cellular models\n\n34:16.300 --> 34:19.940\n to be relatively reproducible and robust\n\n34:19.940 --> 34:24.940\n so that you could actually get enough of those cells\n\n34:25.380 --> 34:29.580\n and in a way that isn't very highly variable and noisy.\n\n34:30.740 --> 34:34.140\n You would want the disease to be relatively contained\n\n34:34.140 --> 34:36.700\n in one or a small number of cell types\n\n34:36.700 --> 34:40.020\n that you could actually create in an in vitro,\n\n34:40.020 --> 34:40.980\n in a dish setting.\n\n34:40.980 --> 34:43.460\n Whereas if it's something that's really broad and systemic\n\n34:43.460 --> 34:45.540\n and involves multiple cells\n\n34:45.540 --> 34:48.460\n that are in very distal parts of your body,\n\n34:48.460 --> 34:50.980\n putting that all in the dish is really challenging.\n\n34:50.980 --> 34:53.740\n So we want to focus on the ones\n\n34:53.740 --> 34:56.980\n that are most likely to be successful today\n\n34:56.980 --> 35:01.980\n with the hope, I think, that really smart bioengineers\n\n35:01.980 --> 35:04.900\n out there are developing better and better systems\n\n35:04.900 --> 35:07.900\n all the time so that diseases that might not be tractable\n\n35:07.900 --> 35:11.220\n today might be tractable in three years.\n\n35:11.220 --> 35:14.340\n So for instance, five years ago,\n\n35:14.340 --> 35:16.140\n these stem cell derived models didn't really exist.\n\n35:16.140 --> 35:18.540\n People were doing most of the work in cancer cells\n\n35:18.540 --> 35:21.660\n and cancer cells are very, very poor models\n\n35:21.660 --> 35:24.300\n of most human biology because they're,\n\n35:24.300 --> 35:25.820\n A, they were cancer to begin with\n\n35:25.820 --> 35:30.140\n and B, as you passage them and they proliferate in a dish,\n\n35:30.140 --> 35:32.660\n they become, because of the genomic instability,\n\n35:32.660 --> 35:35.700\n even less similar to human biology.\n\n35:35.700 --> 35:38.060\n Now we have these stem cell derived models.\n\n35:39.340 --> 35:42.620\n We have the capability to reasonably robustly,\n\n35:42.620 --> 35:45.820\n not quite at the right scale yet, but close,\n\n35:45.820 --> 35:47.940\n to derive what's called organoids,\n\n35:47.940 --> 35:52.940\n which are these teeny little sort of multicellular organ,\n\n35:54.820 --> 35:56.660\n sort of models of an organ system.\n\n35:56.660 --> 35:59.300\n So there's cerebral organoids and liver organoids\n\n35:59.300 --> 36:01.620\n and kidney organoids and.\n\n36:01.620 --> 36:03.460\n Yeah, brain organoids.\n\n36:03.460 --> 36:04.300\n That's organoids.\n\n36:04.300 --> 36:05.500\n It's possibly the coolest thing I've ever seen.\n\n36:05.500 --> 36:07.500\n Is that not like the coolest thing?\n\n36:07.500 --> 36:08.380\n Yeah.\n\n36:08.380 --> 36:09.940\n And then I think on the horizon,\n\n36:09.940 --> 36:11.780\n we're starting to see things like connecting\n\n36:11.780 --> 36:13.900\n these organoids to each other\n\n36:13.900 --> 36:15.140\n so that you could actually start,\n\n36:15.140 --> 36:17.620\n and there's some really cool papers that start to do that\n\n36:17.620 --> 36:19.020\n where you can actually start to say,\n\n36:19.020 --> 36:22.180\n okay, can we do multi organ system stuff?\n\n36:22.180 --> 36:23.500\n There's many challenges to that.\n\n36:23.500 --> 36:27.780\n It's not easy by any stretch, but it might,\n\n36:27.780 --> 36:29.460\n I'm sure people will figure it out.\n\n36:29.460 --> 36:31.580\n And in three years or five years,\n\n36:31.580 --> 36:34.020\n there will be disease models that we could make\n\n36:34.020 --> 36:35.420\n for things that we can't make today.\n\n36:35.420 --> 36:38.700\n Yeah, and this conversation would seem almost outdated\n\n36:38.700 --> 36:40.460\n with the kind of scale that could be achieved\n\n36:40.460 --> 36:41.300\n in like three years.\n\n36:41.300 --> 36:42.140\n I hope so.\n\n36:42.140 --> 36:42.980\n That's the hope.\n\n36:42.980 --> 36:43.820\n That would be so cool.\n\n36:43.820 --> 36:48.060\n So you've cofounded Coursera with Andrew Ng\n\n36:48.060 --> 36:50.380\n and were part of the whole MOOC revolution.\n\n36:51.380 --> 36:53.900\n So to jump topics a little bit,\n\n36:53.900 --> 36:57.900\n can you maybe tell the origin story of the history,\n\n36:57.900 --> 37:00.900\n the origin story of MOOCs, of Coursera,\n\n37:00.900 --> 37:05.900\n and in general, your teaching to huge audiences\n\n37:07.100 --> 37:12.100\n on a very sort of impactful topic of AI in general?\n\n37:12.100 --> 37:15.860\n So I think the origin story of MOOCs\n\n37:15.860 --> 37:17.940\n emanates from a number of efforts\n\n37:17.940 --> 37:20.580\n that occurred at Stanford University\n\n37:20.580 --> 37:25.420\n around the late 2000s\n\n37:25.420 --> 37:28.580\n where different individuals within Stanford,\n\n37:28.580 --> 37:31.500\n myself included, were getting really excited\n\n37:31.500 --> 37:35.220\n about the opportunities of using online technologies\n\n37:35.220 --> 37:38.980\n as a way of achieving both improved quality of teaching\n\n37:38.980 --> 37:40.940\n and also improved scale.\n\n37:40.940 --> 37:44.420\n And so Andrew, for instance,\n\n37:44.420 --> 37:48.820\n led the Stanford Engineering Everywhere,\n\n37:48.820 --> 37:51.660\n which was sort of an attempt to take 10 Stanford courses\n\n37:51.660 --> 37:55.980\n and put them online just as video lectures.\n\n37:55.980 --> 38:00.620\n I led an effort within Stanford to take some of the courses\n\n38:00.620 --> 38:04.380\n and really create a very different teaching model\n\n38:04.380 --> 38:07.340\n that broke those up into smaller units\n\n38:07.340 --> 38:11.060\n and had some of those embedded interactions and so on,\n\n38:11.060 --> 38:14.620\n which got a lot of support from university leaders\n\n38:14.620 --> 38:17.380\n because they felt like it was potentially a way\n\n38:17.380 --> 38:19.580\n of improving the quality of instruction at Stanford\n\n38:19.580 --> 38:22.980\n by moving to what's now called the flipped classroom model.\n\n38:22.980 --> 38:26.620\n And so those efforts eventually sort of started\n\n38:26.620 --> 38:28.020\n to interplay with each other\n\n38:28.020 --> 38:30.940\n and created a tremendous sense of excitement and energy\n\n38:30.940 --> 38:32.780\n within the Stanford community\n\n38:32.780 --> 38:36.380\n about the potential of online teaching\n\n38:36.380 --> 38:39.260\n and led in the fall of 2011\n\n38:39.260 --> 38:42.460\n to the launch of the first Stanford MOOCs.\n\n38:43.740 --> 38:46.420\n By the way, MOOCs, it's probably impossible\n\n38:46.420 --> 38:49.020\n that people don't know, but it's, I guess, massive.\n\n38:49.020 --> 38:51.900\n Open online courses. Open online courses.\n\n38:51.900 --> 38:54.300\n We did not come up with the acronym.\n\n38:54.300 --> 38:57.020\n I'm not particularly fond of the acronym,\n\n38:57.020 --> 38:58.460\n but it is what it is. It is what it is.\n\n38:58.460 --> 39:01.380\n Big bang is not a great term for the start of the universe,\n\n39:01.380 --> 39:03.540\n but it is what it is. Probably so.\n\n39:05.220 --> 39:10.220\n So anyway, so those courses launched in the fall of 2011,\n\n39:10.900 --> 39:13.780\n and there were, within a matter of weeks,\n\n39:13.780 --> 39:17.940\n with no real publicity campaign, just a New York Times article\n\n39:17.940 --> 39:22.660\n that went viral, about 100,000 students or more\n\n39:22.660 --> 39:24.580\n in each of those courses.\n\n39:24.580 --> 39:29.180\n And I remember this conversation that Andrew and I had.\n\n39:29.180 --> 39:33.420\n We were just like, wow, there's this real need here.\n\n39:33.420 --> 39:36.220\n And I think we both felt like, sure,\n\n39:36.220 --> 39:39.820\n we were accomplished academics and we could go back\n\n39:39.820 --> 39:42.620\n and go back to our labs, write more papers.\n\n39:42.620 --> 39:45.860\n But if we did that, then this wouldn't happen.\n\n39:45.860 --> 39:48.700\n And it seemed too important not to happen.\n\n39:48.700 --> 39:51.620\n And so we spent a fair bit of time debating,\n\n39:51.620 --> 39:55.300\n do we wanna do this as a Stanford effort,\n\n39:55.300 --> 39:56.860\n kind of building on what we'd started?\n\n39:56.860 --> 39:59.340\n Do we wanna do this as a for profit company?\n\n39:59.340 --> 40:00.780\n Do we wanna do this as a nonprofit?\n\n40:00.780 --> 40:03.940\n And we decided ultimately to do it as we did with Coursera.\n\n40:04.900 --> 40:09.900\n And so, you know, we started really operating\n\n40:09.900 --> 40:13.380\n as a company at the beginning of 2012.\n\n40:13.380 --> 40:15.340\n And the rest is history.\n\n40:15.340 --> 40:18.380\n But how did you, was that really surprising to you?\n\n40:19.580 --> 40:23.300\n How did you at that time and at this time\n\n40:23.300 --> 40:27.580\n make sense of this need for sort of global education\n\n40:27.580 --> 40:29.380\n you mentioned that you felt that, wow,\n\n40:29.380 --> 40:33.260\n the popularity indicates that there's a hunger\n\n40:33.260 --> 40:37.620\n for sort of globalization of learning.\n\n40:37.620 --> 40:42.620\n I think there is a hunger for learning that,\n\n40:43.620 --> 40:45.100\n you know, globalization is part of it,\n\n40:45.100 --> 40:47.140\n but I think it's just a hunger for learning.\n\n40:47.140 --> 40:50.420\n The world has changed in the last 50 years.\n\n40:50.420 --> 40:54.820\n It used to be that you finished college, you got a job,\n\n40:54.820 --> 40:57.020\n by and large, the skills that you learned in college\n\n40:57.020 --> 40:59.700\n were pretty much what got you through\n\n40:59.700 --> 41:01.380\n the rest of your job history.\n\n41:01.380 --> 41:02.940\n And yeah, you learn some stuff,\n\n41:02.940 --> 41:05.500\n but it wasn't a dramatic change.\n\n41:05.500 --> 41:09.420\n Today, we're in a world where the skills that you need\n\n41:09.420 --> 41:11.260\n for a lot of jobs, they didn't even exist\n\n41:11.260 --> 41:12.500\n when you went to college.\n\n41:12.500 --> 41:14.540\n And the jobs, and many of the jobs that existed\n\n41:14.540 --> 41:18.620\n when you went to college don't even exist today or are dying.\n\n41:18.620 --> 41:22.580\n So part of that is due to AI, but not only.\n\n41:22.580 --> 41:27.300\n And we need to find a way of keeping people,\n\n41:27.300 --> 41:29.900\n giving people access to the skills that they need today.\n\n41:29.900 --> 41:32.020\n And I think that's really what's driving\n\n41:32.020 --> 41:33.900\n a lot of this hunger.\n\n41:33.900 --> 41:37.020\n So I think if we even take a step back,\n\n41:37.020 --> 41:39.940\n for you, all of this started in trying to think\n\n41:39.940 --> 41:43.140\n of new ways to teach or to,\n\n41:43.140 --> 41:47.100\n new ways to sort of organize the material\n\n41:47.100 --> 41:48.380\n and present the material in a way\n\n41:48.380 --> 41:51.380\n that would help the education process, the pedagogy, yeah.\n\n41:51.380 --> 41:56.380\n So what have you learned about effective education\n\n41:56.380 --> 41:57.540\n from this process of playing,\n\n41:57.540 --> 42:00.580\n of experimenting with different ideas?\n\n42:00.580 --> 42:03.940\n So we learned a number of things.\n\n42:03.940 --> 42:06.620\n Some of which I think could translate back\n\n42:06.620 --> 42:08.380\n and have translated back effectively\n\n42:08.380 --> 42:09.900\n to how people teach on campus.\n\n42:09.900 --> 42:11.700\n And some of which I think are more specific\n\n42:11.700 --> 42:13.820\n to people who learn online,\n\n42:13.820 --> 42:18.820\n more sort of people who learn as part of their daily life.\n\n42:18.900 --> 42:20.980\n So we learned, for instance, very quickly\n\n42:20.980 --> 42:23.180\n that short is better.\n\n42:23.180 --> 42:26.820\n So people who are especially in the workforce\n\n42:26.820 --> 42:30.020\n can't do a 15 week semester long course.\n\n42:30.020 --> 42:32.500\n They just can't fit that into their lives.\n\n42:32.500 --> 42:35.540\n Sure, can you describe the shortness of what?\n\n42:35.540 --> 42:39.060\n The entirety, so every aspect,\n\n42:39.060 --> 42:41.980\n so the little lecture, the lecture's short,\n\n42:41.980 --> 42:43.020\n the course is short.\n\n42:43.020 --> 42:43.860\n Both.\n\n42:43.860 --> 42:47.820\n We started out, the first online education efforts\n\n42:47.820 --> 42:50.620\n were actually MIT's OpenCourseWare initiatives.\n\n42:50.620 --> 42:55.620\n And that was recording of classroom lectures and,\n\n42:55.860 --> 42:57.620\n Hour and a half or something like that, yeah.\n\n42:57.620 --> 43:00.380\n And that didn't really work very well.\n\n43:00.380 --> 43:01.540\n I mean, some people benefit.\n\n43:01.540 --> 43:03.140\n I mean, of course they did,\n\n43:03.140 --> 43:06.700\n but it's not really a very palatable experience\n\n43:06.700 --> 43:11.220\n for someone who has a job and three kids\n\n43:11.220 --> 43:13.980\n and they need to run errands and such.\n\n43:13.980 --> 43:17.900\n They can't fit 15 weeks into their life\n\n43:17.900 --> 43:20.700\n and the hour and a half is really hard.\n\n43:20.700 --> 43:22.940\n So we learned very quickly.\n\n43:22.940 --> 43:26.540\n I mean, we started out with short video modules\n\n43:26.540 --> 43:28.180\n and over time we made them shorter\n\n43:28.180 --> 43:31.660\n because we realized that 15 minutes was still too long.\n\n43:31.660 --> 43:33.860\n If you wanna fit in when you're waiting in line\n\n43:33.860 --> 43:35.500\n for your kid's doctor's appointment,\n\n43:35.500 --> 43:37.220\n it's better if it's five to seven.\n\n43:38.620 --> 43:42.540\n We learned that 15 week courses don't work\n\n43:42.540 --> 43:44.820\n and you really wanna break this up into shorter units\n\n43:44.820 --> 43:46.820\n so that there is a natural completion point,\n\n43:46.820 --> 43:48.660\n gives people a sense of they're really close\n\n43:48.660 --> 43:50.420\n to finishing something meaningful.\n\n43:50.420 --> 43:53.580\n They can always come back and take part two and part three.\n\n43:53.580 --> 43:56.500\n We also learned that compressing the content works\n\n43:56.500 --> 44:00.340\n really well because if some people that pace works well\n\n44:00.340 --> 44:03.260\n and for others, they can always rewind and watch again.\n\n44:03.260 --> 44:05.340\n And so people have the ability\n\n44:05.340 --> 44:06.980\n to then learn at their own pace.\n\n44:06.980 --> 44:11.740\n And so that flexibility, the brevity and the flexibility\n\n44:11.740 --> 44:15.420\n are both things that we found to be very important.\n\n44:15.420 --> 44:18.780\n We learned that engagement during the content is important\n\n44:18.780 --> 44:20.620\n and the quicker you give people feedback,\n\n44:20.620 --> 44:22.540\n the more likely they are to be engaged.\n\n44:22.540 --> 44:24.540\n Hence the introduction of these,\n\n44:24.540 --> 44:27.740\n which we actually was an intuition that I had going in\n\n44:27.740 --> 44:30.900\n and was then validated using data\n\n44:30.900 --> 44:34.300\n that introducing some of these sort of little micro quizzes\n\n44:34.300 --> 44:36.500\n into the lectures really helps.\n\n44:36.500 --> 44:39.420\n Self graded as automatically graded assessments\n\n44:39.420 --> 44:41.900\n really helped too because it gives people feedback.\n\n44:41.900 --> 44:43.180\n See, there you are.\n\n44:43.180 --> 44:45.620\n So all of these are valuable.\n\n44:45.620 --> 44:47.260\n And then we learned a bunch of other things too.\n\n44:47.260 --> 44:49.420\n We did some really interesting experiments, for instance,\n\n44:49.420 --> 44:54.180\n on gender bias and how having a female role model\n\n44:54.180 --> 44:59.180\n as an instructor can change the balance of men to women\n\n44:59.340 --> 45:02.020\n in terms of, especially in STEM courses.\n\n45:02.020 --> 45:04.820\n And you could do that online by doing AB testing\n\n45:04.820 --> 45:07.740\n in ways that would be really difficult to go on campus.\n\n45:07.740 --> 45:09.140\n Oh, that's exciting.\n\n45:09.140 --> 45:11.540\n But so the shortness, the compression,\n\n45:11.540 --> 45:15.700\n I mean, that's actually, so that probably is true\n\n45:15.700 --> 45:20.700\n for all good editing is always just compressing the content,\n\n45:20.980 --> 45:21.940\n making it shorter.\n\n45:21.940 --> 45:24.860\n So that puts a lot of burden on the creator of the,\n\n45:24.860 --> 45:28.660\n the instructor and the creator of the educational content.\n\n45:28.660 --> 45:31.260\n Probably most lectures at MIT or Stanford\n\n45:31.260 --> 45:34.340\n could be five times shorter\n\n45:34.340 --> 45:37.580\n if the preparation was put enough.\n\n45:37.580 --> 45:41.660\n So maybe people might disagree with that,\n\n45:41.660 --> 45:45.340\n but like the Christmas, the clarity that a lot of the,\n\n45:45.340 --> 45:50.140\n like Coursera delivers is, how much effort does that take?\n\n45:50.140 --> 45:54.100\n So first of all, let me say that it's not clear\n\n45:54.100 --> 45:57.380\n that that crispness would work as effectively\n\n45:57.380 --> 45:58.900\n in a face to face setting\n\n45:58.900 --> 46:02.420\n because people need time to absorb the material.\n\n46:02.420 --> 46:04.740\n And so you need to at least pause\n\n46:04.740 --> 46:07.300\n and give people a chance to reflect and maybe practice.\n\n46:07.300 --> 46:09.500\n And that's what MOOCs do is that they give you\n\n46:09.500 --> 46:11.780\n these chunks of content and then ask you\n\n46:11.780 --> 46:13.420\n to practice with it.\n\n46:13.420 --> 46:16.300\n And that's where I think some of the newer pedagogy\n\n46:16.300 --> 46:19.180\n that people are adopting in face to face teaching\n\n46:19.180 --> 46:21.580\n that have to do with interactive learning and such\n\n46:21.580 --> 46:23.460\n can be really helpful.\n\n46:23.460 --> 46:26.620\n But both those approaches,\n\n46:26.620 --> 46:29.380\n whether you're doing that type of methodology\n\n46:29.380 --> 46:32.820\n in online teaching or in that flipped classroom,\n\n46:32.820 --> 46:34.500\n interactive teaching.\n\n46:34.500 --> 46:37.180\n What's that, sorry to pause, what's flipped classroom?\n\n46:37.180 --> 46:41.540\n Flipped classroom is a way in which online content\n\n46:41.540 --> 46:45.060\n is used to supplement face to face teaching\n\n46:45.060 --> 46:47.220\n where people watch the videos perhaps\n\n46:47.220 --> 46:49.860\n and do some of the exercises before coming to class.\n\n46:49.860 --> 46:51.180\n And then when they come to class,\n\n46:51.180 --> 46:53.580\n it's actually to do much deeper problem solving\n\n46:53.580 --> 46:54.980\n oftentimes in a group.\n\n46:56.100 --> 47:00.460\n But any one of those different pedagogies\n\n47:00.460 --> 47:03.500\n that are beyond just standing there and droning on\n\n47:03.500 --> 47:06.300\n in front of the classroom for an hour and 15 minutes\n\n47:06.300 --> 47:09.260\n require a heck of a lot more preparation.\n\n47:09.260 --> 47:13.660\n And so it's one of the challenges I think that people have\n\n47:13.660 --> 47:15.740\n that we had when trying to convince instructors\n\n47:15.740 --> 47:16.700\n to teach on Coursera.\n\n47:16.700 --> 47:20.380\n And it's part of the challenges that pedagogy experts\n\n47:20.380 --> 47:22.060\n on campus have in trying to get faculty\n\n47:22.060 --> 47:23.740\n to teach differently is that it's actually harder\n\n47:23.740 --> 47:26.380\n to teach that way than it is to stand there and drone.\n\n47:27.860 --> 47:32.420\n Do you think MOOCs will replace in person education\n\n47:32.420 --> 47:37.420\n or become the majority of in person of education\n\n47:37.420 --> 47:41.380\n of the way people learn in the future?\n\n47:41.380 --> 47:43.260\n Again, the future could be very far away,\n\n47:43.260 --> 47:46.020\n but where's the trend going do you think?\n\n47:46.020 --> 47:50.140\n So I think it's a nuanced and complicated answer.\n\n47:50.140 --> 47:55.140\n I don't think MOOCs will replace face to face teaching.\n\n47:55.780 --> 48:00.300\n I think learning is in many cases a social experience.\n\n48:00.300 --> 48:05.300\n And even at Coursera, we had people who naturally formed\n\n48:05.300 --> 48:07.780\n study groups, even when they didn't have to,\n\n48:07.780 --> 48:10.300\n to just come and talk to each other.\n\n48:10.300 --> 48:14.420\n And we found that that actually benefited their learning\n\n48:14.420 --> 48:15.780\n in very important ways.\n\n48:15.780 --> 48:19.660\n So there was more success among learners\n\n48:19.660 --> 48:22.620\n who had those study groups than among ones who didn't.\n\n48:22.620 --> 48:23.860\n So I don't think it's just gonna,\n\n48:23.860 --> 48:26.060\n oh, we're all gonna just suddenly learn online\n\n48:26.060 --> 48:28.940\n with a computer and no one else in the same way\n\n48:28.940 --> 48:33.180\n that recorded music has not replaced live concerts.\n\n48:33.180 --> 48:38.180\n But I do think that especially when you are thinking\n\n48:38.940 --> 48:42.740\n about continuing education, the stuff that people get\n\n48:42.740 --> 48:44.700\n when they're traditional,\n\n48:44.700 --> 48:47.780\n whatever high school, college education is done,\n\n48:47.780 --> 48:52.500\n and they yet have to maintain their level of expertise\n\n48:52.500 --> 48:54.620\n and skills in a rapidly changing world,\n\n48:54.620 --> 48:58.180\n I think people will consume more and more educational content\n\n48:58.180 --> 49:01.380\n in this online format because going back to school\n\n49:01.380 --> 49:04.860\n for formal education is not an option for most people.\n\n49:04.860 --> 49:07.380\n Briefly, it might be a difficult question to ask,\n\n49:07.380 --> 49:09.940\n but there's a lot of people fascinated\n\n49:09.940 --> 49:12.820\n by artificial intelligence, by machine learning,\n\n49:12.820 --> 49:13.940\n by deep learning.\n\n49:13.940 --> 49:18.140\n Is there a recommendation for the next year\n\n49:18.140 --> 49:21.340\n or for a lifelong journey of somebody interested in this?\n\n49:21.340 --> 49:23.700\n How do they begin?\n\n49:23.700 --> 49:27.220\n How do they enter that learning journey?\n\n49:27.220 --> 49:30.900\n I think the important thing is first to just get started.\n\n49:30.900 --> 49:35.900\n And there's plenty of online content that one can get\n\n49:36.580 --> 49:40.460\n for both the core foundations of mathematics\n\n49:40.460 --> 49:42.260\n and statistics and programming.\n\n49:42.260 --> 49:44.580\n And then from there to machine learning,\n\n49:44.580 --> 49:47.100\n I would encourage people not to skip\n\n49:47.100 --> 49:48.700\n to quickly pass the foundations\n\n49:48.700 --> 49:51.060\n because I find that there's a lot of people\n\n49:51.060 --> 49:53.740\n who learn machine learning, whether it's online\n\n49:53.740 --> 49:56.180\n or on campus without getting those foundations.\n\n49:56.180 --> 50:00.020\n And they basically just turn the crank on existing models\n\n50:00.020 --> 50:03.540\n in ways that A, don't allow for a lot of innovation\n\n50:03.540 --> 50:07.700\n and an adjustment to the problem at hand,\n\n50:07.700 --> 50:09.660\n but also B, are sometimes just wrong\n\n50:09.660 --> 50:12.900\n and they don't even realize that their application is wrong\n\n50:12.900 --> 50:15.940\n because there's artifacts that they haven't fully understood.\n\n50:15.940 --> 50:17.860\n So I think the foundations,\n\n50:17.860 --> 50:19.860\n machine learning is an important step.\n\n50:19.860 --> 50:24.860\n And then actually start solving problems,\n\n50:24.860 --> 50:27.620\n try and find someone to solve them with\n\n50:27.620 --> 50:28.980\n because especially at the beginning,\n\n50:28.980 --> 50:31.580\n it's useful to have someone to bounce ideas off\n\n50:31.580 --> 50:33.220\n and fix mistakes that you make\n\n50:33.220 --> 50:35.980\n and you can fix mistakes that they make,\n\n50:35.980 --> 50:40.540\n but then just find practical problems,\n\n50:40.540 --> 50:43.300\n whether it's in your workplace or if you don't have that,\n\n50:43.300 --> 50:46.100\n Kaggle competitions or such are a really great place\n\n50:46.100 --> 50:50.860\n to find interesting problems and just practice.\n\n50:50.860 --> 50:52.340\n Practice.\n\n50:52.340 --> 50:54.540\n Perhaps a bit of a romanticized question,\n\n50:54.540 --> 50:59.340\n but what idea in deep learning do you find,\n\n50:59.340 --> 51:02.220\n have you found in your journey the most beautiful\n\n51:02.220 --> 51:04.140\n or surprising or interesting?\n\n51:07.660 --> 51:09.420\n Perhaps not just deep learning,\n\n51:09.420 --> 51:12.620\n but AI in general, statistics.\n\n51:14.940 --> 51:16.540\n I'm gonna answer with two things.\n\n51:19.100 --> 51:23.100\n One would be the foundational concept of end to end training,\n\n51:23.100 --> 51:26.940\n which is that you start from the raw data\n\n51:26.940 --> 51:31.940\n and you train something that is not like a single piece,\n\n51:32.980 --> 51:37.980\n but rather towards the actual goal that you're looking to.\n\n51:38.980 --> 51:40.820\n From the raw data to the outcome,\n\n51:40.820 --> 51:43.580\n like no details in between.\n\n51:43.580 --> 51:45.460\n Well, not no details, but the fact that you,\n\n51:45.460 --> 51:47.540\n I mean, you could certainly introduce building blocks\n\n51:47.540 --> 51:50.260\n that were trained towards other tasks.\n\n51:50.260 --> 51:53.060\n I'm actually coming to that in my second half of the answer,\n\n51:53.060 --> 51:57.740\n but it doesn't have to be like a single monolithic blob\n\n51:57.740 --> 51:58.580\n in the middle.\n\n51:58.580 --> 52:00.220\n Actually, I think that's not ideal,\n\n52:00.220 --> 52:02.620\n but rather the fact that at the end of the day,\n\n52:02.620 --> 52:04.780\n you can actually train something that goes all the way\n\n52:04.780 --> 52:06.900\n from the beginning to the end.\n\n52:06.900 --> 52:09.140\n And the other one that I find really compelling\n\n52:09.140 --> 52:13.180\n is the notion of learning a representation\n\n52:13.180 --> 52:18.180\n that in its turn, even if it was trained to another task,\n\n52:18.180 --> 52:23.180\n can potentially be used as a much more rapid starting point\n\n52:24.260 --> 52:26.700\n to solving a different task.\n\n52:26.700 --> 52:29.500\n And that's, I think, reminiscent\n\n52:29.500 --> 52:32.300\n of what makes people successful learners.\n\n52:32.300 --> 52:35.460\n It's something that is relatively new\n\n52:35.460 --> 52:36.540\n in the machine learning space.\n\n52:36.540 --> 52:38.700\n I think it's underutilized even relative\n\n52:38.700 --> 52:41.460\n to today's capabilities, but more and more\n\n52:41.460 --> 52:45.220\n of how do we learn sort of reusable representation?\n\n52:45.220 --> 52:49.700\n And so end to end and transfer learning.\n\n52:49.700 --> 52:51.140\n Yeah.\n\n52:51.140 --> 52:53.660\n Is it surprising to you that neural networks\n\n52:53.660 --> 52:56.980\n are able to, in many cases, do these things?\n\n52:56.980 --> 53:01.980\n Is it maybe taken back to when you first would dive deep\n\n53:02.260 --> 53:05.460\n into neural networks or in general, even today,\n\n53:05.460 --> 53:07.860\n is it surprising that neural networks work at all\n\n53:07.860 --> 53:12.860\n and work wonderfully to do this kind of raw end to end\n\n53:12.860 --> 53:16.380\n and end to end learning and even transfer learning?\n\n53:16.380 --> 53:21.380\n I think I was surprised by how well\n\n53:22.540 --> 53:25.780\n when you have large enough amounts of data,\n\n53:26.820 --> 53:31.820\n it's possible to find a meaningful representation\n\n53:32.940 --> 53:36.060\n in what is an exceedingly high dimensional space.\n\n53:36.060 --> 53:39.300\n And so I find that to be really exciting\n\n53:39.300 --> 53:41.620\n and people are still working on the math for that.\n\n53:41.620 --> 53:43.580\n There's more papers on that every year.\n\n53:43.580 --> 53:46.220\n And I think it would be really cool\n\n53:46.220 --> 53:51.220\n if we figured that out, but that to me was a surprise\n\n53:52.220 --> 53:55.420\n because in the early days when I was starting my way\n\n53:55.420 --> 53:58.700\n in machine learning and the data sets were rather small,\n\n53:58.700 --> 54:02.780\n I think we believed, I believed that you needed\n\n54:02.780 --> 54:05.500\n to have a much more constrained\n\n54:05.500 --> 54:08.620\n and knowledge rich search space\n\n54:08.620 --> 54:11.860\n to really make, to really get to a meaningful answer.\n\n54:11.860 --> 54:13.860\n And I think it was true at the time.\n\n54:13.860 --> 54:18.220\n What I think is still a question\n\n54:18.220 --> 54:23.180\n is will a completely knowledge free approach\n\n54:23.180 --> 54:26.020\n where there's no prior knowledge going\n\n54:26.020 --> 54:28.980\n into the construction of the model,\n\n54:28.980 --> 54:31.620\n is that gonna be the solution or not?\n\n54:31.620 --> 54:34.180\n It's not actually the solution today\n\n54:34.180 --> 54:38.940\n in the sense that the architecture of a convolutional\n\n54:38.940 --> 54:41.500\n neural network that's used for images\n\n54:41.500 --> 54:43.260\n is actually quite different\n\n54:43.260 --> 54:46.580\n to the type of network that's used for language\n\n54:46.580 --> 54:50.220\n and yet different from the one that's used for speech\n\n54:50.220 --> 54:52.500\n or biology or any other application.\n\n54:52.500 --> 54:55.860\n There's still some insight that goes\n\n54:55.860 --> 54:58.180\n into the structure of the network\n\n54:58.180 --> 55:00.820\n to get the right performance.\n\n55:00.820 --> 55:01.660\n Will you be able to come up\n\n55:01.660 --> 55:03.220\n with a universal learning machine?\n\n55:03.220 --> 55:05.100\n I don't know.\n\n55:05.100 --> 55:07.300\n I wonder if there's always has to be some insight\n\n55:07.300 --> 55:10.300\n injected somewhere or whether it can converge.\n\n55:10.300 --> 55:13.580\n So you've done a lot of interesting work\n\n55:13.580 --> 55:16.340\n with probabilistic graphical models in general,\n\n55:16.340 --> 55:18.420\n Bayesian deep learning and so on.\n\n55:18.420 --> 55:21.060\n Can you maybe speak high level,\n\n55:21.060 --> 55:25.500\n how can learning systems deal with uncertainty?\n\n55:25.500 --> 55:28.940\n One of the limitations I think of a lot\n\n55:28.940 --> 55:33.780\n of machine learning models is that\n\n55:33.780 --> 55:35.780\n they come up with an answer\n\n55:35.780 --> 55:40.780\n and you don't know how much you can believe that answer.\n\n55:40.860 --> 55:45.860\n And oftentimes the answer is actually\n\n55:47.740 --> 55:50.580\n quite poorly calibrated relative to its uncertainties.\n\n55:50.580 --> 55:55.500\n Even if you look at where the confidence\n\n55:55.500 --> 55:58.980\n that comes out of say the neural network at the end,\n\n55:58.980 --> 56:01.820\n and you ask how much more likely\n\n56:01.820 --> 56:04.820\n is an answer of 0.8 versus 0.9,\n\n56:04.820 --> 56:07.700\n it's not really in any way calibrated\n\n56:07.700 --> 56:12.340\n to the actual reliability of that network\n\n56:12.340 --> 56:13.180\n and how true it is.\n\n56:13.180 --> 56:16.780\n And the further away you move from the training data,\n\n56:16.780 --> 56:20.700\n the more, not only the more wrong the network is,\n\n56:20.700 --> 56:22.580\n often it's more wrong and more confident\n\n56:22.580 --> 56:24.380\n in its wrong answer.\n\n56:24.380 --> 56:29.340\n And that is a serious issue in a lot of application areas.\n\n56:29.340 --> 56:30.380\n So when you think for instance,\n\n56:30.380 --> 56:33.340\n about medical diagnosis as being maybe an epitome\n\n56:33.340 --> 56:35.700\n of how problematic this can be,\n\n56:35.700 --> 56:37.700\n if you were training your network\n\n56:37.700 --> 56:40.180\n on a certain set of patients\n\n56:40.180 --> 56:41.540\n and a certain patient population,\n\n56:41.540 --> 56:44.620\n and I have a patient that is an outlier\n\n56:44.620 --> 56:46.780\n and there's no human that looks at this,\n\n56:46.780 --> 56:49.100\n and that patient is put into a neural network\n\n56:49.100 --> 56:50.340\n and your network not only gives\n\n56:50.340 --> 56:51.940\n a completely incorrect diagnosis,\n\n56:51.940 --> 56:53.980\n but is supremely confident\n\n56:53.980 --> 56:56.340\n in its wrong answer, you could kill people.\n\n56:56.340 --> 57:01.340\n So I think creating more of an understanding\n\n57:01.940 --> 57:05.540\n of how do you produce networks\n\n57:05.540 --> 57:09.060\n that are calibrated in their uncertainty\n\n57:09.060 --> 57:10.940\n and can also say, you know what, I give up.\n\n57:10.940 --> 57:14.580\n I don't know what to say about this particular data instance\n\n57:14.580 --> 57:16.340\n because I've never seen something\n\n57:16.340 --> 57:18.140\n that's sufficiently like it before.\n\n57:18.140 --> 57:20.540\n I think it's going to be really important\n\n57:20.540 --> 57:23.060\n in mission critical applications,\n\n57:23.060 --> 57:25.380\n especially ones where human life is at stake\n\n57:25.380 --> 57:28.300\n and that includes medical applications,\n\n57:28.300 --> 57:31.180\n but it also includes automated driving\n\n57:31.180 --> 57:33.300\n because you'd want the network to be able to say,\n\n57:33.300 --> 57:36.020\n you know what, I have no idea what this blob is\n\n57:36.020 --> 57:37.140\n that I'm seeing in the middle of the road.\n\n57:37.140 --> 57:38.380\n So I'm just going to stop\n\n57:38.380 --> 57:41.540\n because I don't want to potentially run over a pedestrian\n\n57:41.540 --> 57:42.820\n that I don't recognize.\n\n57:42.820 --> 57:47.540\n Is there good mechanisms, ideas of how to allow\n\n57:47.540 --> 57:52.260\n learning systems to provide that uncertainty\n\n57:52.260 --> 57:54.060\n along with their predictions?\n\n57:54.060 --> 57:57.180\n Certainly people have come up with mechanisms\n\n57:57.180 --> 58:00.700\n that involve Bayesian deep learning,\n\n58:00.700 --> 58:04.460\n deep learning that involves Gaussian processes.\n\n58:04.460 --> 58:07.660\n I mean, there's a slew of different approaches\n\n58:07.660 --> 58:09.180\n that people have come up with.\n\n58:09.180 --> 58:13.660\n There's methods that use ensembles of networks\n\n58:13.660 --> 58:15.260\n trained with different subsets of data\n\n58:15.260 --> 58:17.620\n or different random starting points.\n\n58:17.620 --> 58:20.260\n Those are actually sometimes surprisingly good\n\n58:20.260 --> 58:24.020\n at creating a sort of set of how confident\n\n58:24.020 --> 58:26.580\n or not you are in your answer.\n\n58:26.580 --> 58:28.980\n It's very much an area of open research.\n\n58:30.020 --> 58:33.660\n Let's cautiously venture back into the land of philosophy\n\n58:33.660 --> 58:37.660\n and speaking of AI systems providing uncertainty,\n\n58:37.660 --> 58:41.140\n somebody like Stuart Russell believes\n\n58:41.140 --> 58:43.420\n that as we create more and more intelligence systems,\n\n58:43.420 --> 58:46.820\n it's really important for them to be full of self doubt\n\n58:46.820 --> 58:51.820\n because if they're given more and more power,\n\n58:51.940 --> 58:54.820\n we want the way to maintain human control\n\n58:54.820 --> 58:57.900\n over AI systems or human supervision, which is true.\n\n58:57.900 --> 58:59.500\n Like you just mentioned with autonomous vehicles,\n\n58:59.500 --> 59:02.420\n it's really important to get human supervision\n\n59:02.420 --> 59:05.940\n when the car is not sure because if it's really confident\n\n59:05.940 --> 59:07.860\n in cases when it can get in trouble,\n\n59:07.860 --> 59:09.380\n it's gonna be really problematic.\n\n59:09.380 --> 59:12.980\n So let me ask about sort of the questions of AGI\n\n59:12.980 --> 59:14.860\n and human level intelligence.\n\n59:14.860 --> 59:17.180\n I mean, we've talked about curing diseases,\n\n59:18.780 --> 59:20.180\n which is sort of fundamental thing\n\n59:20.180 --> 59:21.780\n we can have an impact today,\n\n59:21.780 --> 59:26.180\n but AI people also dream of both understanding\n\n59:26.180 --> 59:29.220\n and creating intelligence.\n\n59:29.220 --> 59:30.420\n Is that something you think about?\n\n59:30.420 --> 59:32.780\n Is that something you dream about?\n\n59:32.780 --> 59:36.980\n Is that something you think is within our reach\n\n59:36.980 --> 59:39.660\n to be thinking about as computer scientists?\n\n59:39.660 --> 59:43.500\n Well, boy, let me tease apart different parts\n\n59:43.500 --> 59:45.180\n of that question.\n\n59:45.180 --> 59:46.420\n The worst question.\n\n59:46.420 --> 59:50.940\n Yeah, it's a multi part question.\n\n59:50.940 --> 59:55.940\n So let me start with the feasibility of AGI.\n\n59:57.500 --> 1:00:01.500\n Then I'll talk about the timelines a little bit\n\n1:00:01.500 --> 1:00:05.980\n and then talk about, well, what controls does one need\n\n1:00:05.980 --> 1:00:10.540\n when thinking about protections in the AI space?\n\n1:00:10.540 --> 1:00:15.540\n So, I think AGI obviously is a longstanding dream\n\n1:00:17.180 --> 1:00:21.300\n that even our early pioneers in the space had,\n\n1:00:21.300 --> 1:00:23.460\n the Turing test and so on\n\n1:00:23.460 --> 1:00:27.580\n are the earliest discussions of that.\n\n1:00:27.580 --> 1:00:32.580\n We're obviously closer than we were 70 or so years ago,\n\n1:00:32.580 --> 1:00:36.420\n but I think it's still very far away.\n\n1:00:37.420 --> 1:00:40.900\n I think machine learning algorithms today\n\n1:00:40.900 --> 1:00:45.900\n are really exquisitely good pattern recognizers\n\n1:00:46.180 --> 1:00:49.420\n in very specific problem domains\n\n1:00:49.420 --> 1:00:51.540\n where they have seen enough training data\n\n1:00:51.540 --> 1:00:53.740\n to make good predictions.\n\n1:00:53.740 --> 1:00:57.860\n You take a machine learning algorithm\n\n1:00:57.860 --> 1:01:00.660\n and you move it to a slightly different version\n\n1:01:00.660 --> 1:01:03.780\n of even that same problem, far less one that's different\n\n1:01:03.780 --> 1:01:06.980\n and it will just completely choke.\n\n1:01:06.980 --> 1:01:11.620\n So I think we're nowhere close to the versatility\n\n1:01:11.620 --> 1:01:15.620\n and flexibility of even a human toddler\n\n1:01:15.620 --> 1:01:19.740\n in terms of their ability to context switch\n\n1:01:19.740 --> 1:01:20.740\n and solve different problems\n\n1:01:20.740 --> 1:01:24.340\n using a single knowledge base, single brain.\n\n1:01:24.340 --> 1:01:28.820\n So am I desperately worried about\n\n1:01:28.820 --> 1:01:33.540\n the machines taking over the universe\n\n1:01:33.540 --> 1:01:35.500\n and starting to kill people\n\n1:01:35.500 --> 1:01:37.380\n because they want to have more power?\n\n1:01:37.380 --> 1:01:38.460\n I don't think so.\n\n1:01:38.460 --> 1:01:40.460\n Well, so to pause on that,\n\n1:01:40.460 --> 1:01:43.620\n so you kind of intuited that super intelligence\n\n1:01:43.620 --> 1:01:46.300\n is a very difficult thing to achieve.\n\n1:01:46.300 --> 1:01:47.140\n Even intelligence.\n\n1:01:47.140 --> 1:01:48.180\n Intelligence, intelligence.\n\n1:01:48.180 --> 1:01:50.500\n Super intelligence, we're not even close to intelligence.\n\n1:01:50.500 --> 1:01:53.380\n Even just the greater abilities of generalization\n\n1:01:53.380 --> 1:01:55.180\n of our current systems.\n\n1:01:55.180 --> 1:01:59.180\n But we haven't answered all the parts\n\n1:01:59.180 --> 1:02:00.020\n and we'll take another.\n\n1:02:00.020 --> 1:02:00.860\n I'm getting to the second part.\n\n1:02:00.860 --> 1:02:04.340\n Okay, but maybe another tangent you can also pick up\n\n1:02:04.340 --> 1:02:08.140\n is can we get in trouble with much dumber systems?\n\n1:02:08.140 --> 1:02:11.300\n Yes, and that is exactly where I was going.\n\n1:02:11.300 --> 1:02:16.140\n So just to wrap up on the threats of AGI,\n\n1:02:16.140 --> 1:02:21.140\n I think that it seems to me a little early today\n\n1:02:21.140 --> 1:02:26.140\n to figure out protections against a human level\n\n1:02:26.220 --> 1:02:28.620\n or superhuman level intelligence\n\n1:02:28.620 --> 1:02:31.580\n where we don't even see the skeleton\n\n1:02:31.580 --> 1:02:33.140\n of what that would look like.\n\n1:02:33.140 --> 1:02:35.740\n So it seems that it's very speculative\n\n1:02:35.740 --> 1:02:39.820\n on how to protect against that.\n\n1:02:39.820 --> 1:02:43.940\n But we can definitely and have gotten into trouble\n\n1:02:43.940 --> 1:02:45.980\n on much dumber systems.\n\n1:02:45.980 --> 1:02:48.340\n And a lot of that has to do with the fact\n\n1:02:48.340 --> 1:02:52.300\n that the systems that we're building are increasingly\n\n1:02:52.300 --> 1:02:57.300\n complex, increasingly poorly understood.\n\n1:02:57.380 --> 1:03:01.420\n And there's ripple effects that are unpredictable\n\n1:03:01.420 --> 1:03:06.420\n in changing little things that can have dramatic consequences\n\n1:03:06.420 --> 1:03:08.460\n on the outcome.\n\n1:03:08.460 --> 1:03:11.620\n And by the way, that's not unique to artificial intelligence.\n\n1:03:11.620 --> 1:03:13.820\n I think artificial intelligence exacerbates that,\n\n1:03:13.820 --> 1:03:15.100\n brings it to a new level.\n\n1:03:15.100 --> 1:03:18.420\n But heck, our electric grid is really complicated.\n\n1:03:18.420 --> 1:03:20.820\n The software that runs our financial markets\n\n1:03:20.820 --> 1:03:22.540\n is really complicated.\n\n1:03:22.540 --> 1:03:25.820\n And we've seen those ripple effects translate\n\n1:03:25.820 --> 1:03:28.540\n to dramatic negative consequences,\n\n1:03:28.540 --> 1:03:32.820\n like for instance, financial crashes that have to do\n\n1:03:32.820 --> 1:03:35.020\n with feedback loops that we didn't anticipate.\n\n1:03:35.020 --> 1:03:38.460\n So I think that's an issue that we need to be thoughtful\n\n1:03:38.460 --> 1:03:40.580\n about in many places,\n\n1:03:41.940 --> 1:03:44.300\n artificial intelligence being one of them.\n\n1:03:44.300 --> 1:03:49.300\n And I think it's really important that people are thinking\n\n1:03:49.660 --> 1:03:54.380\n about ways in which we can have better interpretability\n\n1:03:54.380 --> 1:03:59.140\n of systems, better tests for, for instance,\n\n1:03:59.140 --> 1:04:01.900\n measuring the extent to which a machine learning system\n\n1:04:01.900 --> 1:04:04.860\n that was trained in one set of circumstances,\n\n1:04:04.860 --> 1:04:07.340\n how well does it actually work\n\n1:04:07.340 --> 1:04:09.540\n in a very different set of circumstances\n\n1:04:09.540 --> 1:04:12.340\n where you might say, for instance,\n\n1:04:12.340 --> 1:04:14.740\n well, I'm not gonna be able to test my automated vehicle\n\n1:04:14.740 --> 1:04:17.860\n in every possible city, village,\n\n1:04:18.980 --> 1:04:20.780\n weather condition and so on.\n\n1:04:20.780 --> 1:04:23.740\n But if you trained it on this set of conditions\n\n1:04:23.740 --> 1:04:27.340\n and then tested it on 50 or a hundred others\n\n1:04:27.340 --> 1:04:29.140\n that were quite different from the ones\n\n1:04:29.140 --> 1:04:31.980\n that you trained it on and it worked,\n\n1:04:31.980 --> 1:04:34.100\n then that gives you confidence that the next 50\n\n1:04:34.100 --> 1:04:36.100\n that you didn't test it on might also work.\n\n1:04:36.100 --> 1:04:39.020\n So effectively it's testing for generalizability.\n\n1:04:39.020 --> 1:04:41.300\n So I think there's ways that we should be\n\n1:04:41.300 --> 1:04:45.900\n constantly thinking about to validate the robustness\n\n1:04:45.900 --> 1:04:47.500\n of our systems.\n\n1:04:47.500 --> 1:04:50.980\n I think it's very different from the let's make sure\n\n1:04:50.980 --> 1:04:53.260\n robots don't take over the world.\n\n1:04:53.260 --> 1:04:57.020\n And then the other place where I think we have a threat,\n\n1:04:57.020 --> 1:04:59.420\n which is also important for us to think about\n\n1:04:59.420 --> 1:05:03.180\n is the extent to which technology can be abused.\n\n1:05:03.180 --> 1:05:06.540\n So like any really powerful technology,\n\n1:05:06.540 --> 1:05:10.900\n machine learning can be very much used badly\n\n1:05:10.900 --> 1:05:12.700\n as well as to good.\n\n1:05:12.700 --> 1:05:15.580\n And that goes back to many other technologies\n\n1:05:15.580 --> 1:05:19.140\n that have come up with when people invented\n\n1:05:19.140 --> 1:05:22.140\n projectile missiles and it turned into guns\n\n1:05:22.140 --> 1:05:24.660\n and people invented nuclear power\n\n1:05:24.660 --> 1:05:26.420\n and it turned into nuclear bombs.\n\n1:05:26.420 --> 1:05:30.340\n And I think honestly, I would say that to me,\n\n1:05:30.340 --> 1:05:33.500\n gene editing and CRISPR is at least as dangerous\n\n1:05:33.500 --> 1:05:38.500\n as technology if used badly than as machine learning.\n\n1:05:39.780 --> 1:05:43.860\n You could create really nasty viruses and such\n\n1:05:43.860 --> 1:05:48.860\n using gene editing that you would be really careful about.\n\n1:05:51.900 --> 1:05:56.700\n So anyway, that's something that we need\n\n1:05:56.700 --> 1:05:59.620\n to be really thoughtful about whenever we have\n\n1:05:59.620 --> 1:06:02.500\n any really powerful new technology.\n\n1:06:02.500 --> 1:06:04.140\n Yeah, and in the case of machine learning\n\n1:06:04.140 --> 1:06:06.820\n is adversarial machine learning.\n\n1:06:06.820 --> 1:06:09.140\n So all the kinds of attacks like security almost threats\n\n1:06:09.140 --> 1:06:10.540\n and there's a social engineering\n\n1:06:10.540 --> 1:06:12.100\n with machine learning algorithms.\n\n1:06:12.100 --> 1:06:15.900\n And there's face recognition and big brother is watching you\n\n1:06:15.900 --> 1:06:20.900\n and there's the killer drones that can potentially go\n\n1:06:20.980 --> 1:06:25.220\n and targeted execution of people in a different country.\n\n1:06:27.180 --> 1:06:29.620\n One can argue that bombs are not necessarily\n\n1:06:29.620 --> 1:06:34.020\n that much better, but people wanna kill someone,\n\n1:06:34.020 --> 1:06:35.740\n they'll find a way to do it.\n\n1:06:35.740 --> 1:06:39.060\n So in general, if you look at trends in the data,\n\n1:06:39.060 --> 1:06:41.100\n there's less wars, there's less violence,\n\n1:06:41.100 --> 1:06:42.940\n there's more human rights.\n\n1:06:42.940 --> 1:06:47.940\n So we've been doing overall quite good as a human species.\n\n1:06:48.340 --> 1:06:49.180\n Are you optimistic?\n\n1:06:49.180 --> 1:06:50.620\n Surprisingly sometimes.\n\n1:06:50.620 --> 1:06:52.740\n Are you optimistic?\n\n1:06:52.740 --> 1:06:55.540\n Maybe another way to ask is do you think most people\n\n1:06:55.540 --> 1:07:00.540\n are good and fundamentally we tend towards a better world,\n\n1:07:03.140 --> 1:07:05.460\n which is underlying the question,\n\n1:07:05.460 --> 1:07:09.180\n will machine learning with gene editing\n\n1:07:09.180 --> 1:07:12.140\n ultimately land us somewhere good?\n\n1:07:12.140 --> 1:07:13.420\n Are you optimistic?\n\n1:07:15.860 --> 1:07:19.140\n I think by and large, I'm optimistic.\n\n1:07:19.140 --> 1:07:24.140\n I think that most people mean well,\n\n1:07:24.140 --> 1:07:28.140\n that doesn't mean that most people are altruistic do gooders,\n\n1:07:28.140 --> 1:07:31.020\n but I think most people mean well,\n\n1:07:31.020 --> 1:07:34.980\n but I think it's also really important for us as a society\n\n1:07:34.980 --> 1:07:39.980\n to create social norms where doing good\n\n1:07:40.820 --> 1:07:45.820\n and being perceived well by our peers\n\n1:07:47.140 --> 1:07:49.780\n are positively correlated.\n\n1:07:49.780 --> 1:07:54.060\n I mean, it's very easy to create dysfunctional norms\n\n1:07:54.060 --> 1:07:55.620\n in emotional societies.\n\n1:07:55.620 --> 1:07:58.540\n There's certainly multiple psychological experiments\n\n1:07:58.540 --> 1:08:02.420\n as well as sadly real world events\n\n1:08:02.420 --> 1:08:05.300\n where people have devolved to a world\n\n1:08:05.300 --> 1:08:09.340\n where being perceived well by your peers\n\n1:08:09.340 --> 1:08:12.700\n is correlated with really atrocious,\n\n1:08:14.100 --> 1:08:16.860\n often genocidal behaviors.\n\n1:08:17.820 --> 1:08:19.500\n So we really want to make sure\n\n1:08:19.500 --> 1:08:21.740\n that we maintain a set of social norms\n\n1:08:21.740 --> 1:08:25.660\n where people know that to be a successful member of society,\n\n1:08:25.660 --> 1:08:27.500\n you want to be doing good.\n\n1:08:27.500 --> 1:08:31.420\n And one of the things that I sometimes worry about\n\n1:08:31.420 --> 1:08:35.420\n is that some societies don't seem to necessarily\n\n1:08:35.420 --> 1:08:38.340\n be moving in the forward direction in that regard\n\n1:08:38.340 --> 1:08:40.420\n where it's not necessarily the case\n\n1:08:43.620 --> 1:08:45.100\n that being a good person\n\n1:08:45.100 --> 1:08:47.980\n is what makes you be perceived well by your peers.\n\n1:08:47.980 --> 1:08:49.700\n And I think that's a really important thing\n\n1:08:49.700 --> 1:08:51.300\n for us as a society to remember.\n\n1:08:51.300 --> 1:08:55.940\n It's really easy to degenerate back into a universe\n\n1:08:55.940 --> 1:09:00.540\n where it's okay to do really bad stuff\n\n1:09:00.540 --> 1:09:03.340\n and still have your peers think you're amazing.\n\n1:09:04.980 --> 1:09:08.180\n It's fun to ask a world class computer scientist\n\n1:09:08.180 --> 1:09:11.380\n and engineer a ridiculously philosophical question\n\n1:09:11.380 --> 1:09:13.460\n like what is the meaning of life?\n\n1:09:13.460 --> 1:09:17.500\n Let me ask, what gives your life meaning?\n\n1:09:17.500 --> 1:09:22.180\n Or what is the source of fulfillment, happiness,\n\n1:09:22.180 --> 1:09:23.900\n joy, purpose?\n\n1:09:26.540 --> 1:09:31.540\n When we were starting Coursera in the fall of 2011,\n\n1:09:32.980 --> 1:09:36.900\n that was right around the time that Steve Jobs passed away.\n\n1:09:37.740 --> 1:09:41.020\n And so the media was full of various famous quotes\n\n1:09:41.020 --> 1:09:45.500\n that he uttered and one of them that really stuck with me\n\n1:09:45.500 --> 1:09:48.780\n because it resonated with stuff that I'd been feeling\n\n1:09:48.780 --> 1:09:52.380\n for even years before that is that our goal in life\n\n1:09:52.380 --> 1:09:55.100\n should be to make a dent in the universe.\n\n1:09:55.100 --> 1:10:00.100\n So I think that to me, what gives my life meaning\n\n1:10:00.620 --> 1:10:05.620\n is that I would hope that when I am lying there\n\n1:10:05.900 --> 1:10:09.660\n on my deathbed and looking at what I'd done in my life\n\n1:10:09.660 --> 1:10:15.660\n that I can point to ways in which I have left the world\n\n1:10:15.860 --> 1:10:20.460\n a better place than it was when I entered it.\n\n1:10:20.460 --> 1:10:23.620\n This is something I tell my kids all the time\n\n1:10:23.620 --> 1:10:27.260\n because I also think that the burden of that\n\n1:10:27.260 --> 1:10:31.420\n is much greater for those of us who were born to privilege.\n\n1:10:31.420 --> 1:10:34.380\n And in some ways I was, I mean, I wasn't born super wealthy\n\n1:10:34.380 --> 1:10:37.900\n or anything like that, but I grew up in an educated family\n\n1:10:37.900 --> 1:10:40.860\n with parents who loved me and took care of me\n\n1:10:40.860 --> 1:10:43.060\n and I had a chance at a great education\n\n1:10:43.060 --> 1:10:46.620\n and I always had enough to eat.\n\n1:10:46.620 --> 1:10:48.900\n So I was in many ways born to privilege\n\n1:10:48.900 --> 1:10:51.940\n more than the vast majority of humanity.\n\n1:10:51.940 --> 1:10:55.940\n And my kids I think are even more so born to privilege\n\n1:10:55.940 --> 1:10:57.940\n than I was fortunate enough to be.\n\n1:10:57.940 --> 1:11:01.020\n And I think it's really important that especially\n\n1:11:01.020 --> 1:11:03.900\n for those of us who have that opportunity\n\n1:11:03.900 --> 1:11:07.420\n that we use our lives to make the world a better place.\n\n1:11:07.420 --> 1:11:09.620\n I don't think there's a better way to end it.\n\n1:11:09.620 --> 1:11:11.620\n Daphne, it was an honor to talk to you.\n\n1:11:11.620 --> 1:11:12.620\n Thank you so much for talking today.\n\n1:11:12.620 --> 1:11:13.460\n Thank you.\n\n1:11:14.420 --> 1:11:15.900\n Thanks for listening to this conversation\n\n1:11:15.900 --> 1:11:17.780\n with Daphne Koller and thank you\n\n1:11:17.780 --> 1:11:19.900\n to our presenting sponsor, Cash App.\n\n1:11:19.900 --> 1:11:21.660\n Please consider supporting the podcast\n\n1:11:21.660 --> 1:11:26.180\n by downloading Cash App and using code LEXPodcast.\n\n1:11:26.180 --> 1:11:28.620\n If you enjoy this podcast, subscribe on YouTube,\n\n1:11:28.620 --> 1:11:31.060\n review it with five stars on Apple Podcast,\n\n1:11:31.060 --> 1:11:33.340\n support it on Patreon, or simply connect with me\n\n1:11:33.340 --> 1:11:36.260\n on Twitter at LEXFREEDMAN.\n\n1:11:36.260 --> 1:11:39.820\n And now let me leave you with some words from Hippocrates,\n\n1:11:39.820 --> 1:11:41.900\n a physician from ancient Greece\n\n1:11:41.900 --> 1:11:44.340\n who's considered to be the father of medicine.\n\n1:11:45.340 --> 1:11:48.340\n Wherever the art of medicine is loved,\n\n1:11:48.340 --> 1:11:50.780\n there's also a love of humanity.\n\n1:11:50.780 --> 1:12:05.780\n Thank you for listening and hope to see you next time.\n\n"
}