{
  "title": "Yann LeCun: Deep Learning, ConvNets, and Self-Supervised Learning | Lex Fridman Podcast #36",
  "id": "SGSOCuByo24",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:03.080\n The following is a conversation with Yann LeCun.\n\n00:03.080 --> 00:06.320\n He's considered to be one of the fathers of deep learning,\n\n00:06.320 --> 00:09.040\n which, if you've been hiding under a rock,\n\n00:09.040 --> 00:12.240\n is the recent revolution in AI that has captivated the world\n\n00:12.240 --> 00:16.160\n with the possibility of what machines can learn from data.\n\n00:16.160 --> 00:18.520\n He's a professor at New York University,\n\n00:18.520 --> 00:21.720\n a vice president and chief AI scientist at Facebook,\n\n00:21.720 --> 00:24.320\n and co recipient of the Turing Award\n\n00:24.320 --> 00:26.240\n for his work on deep learning.\n\n00:26.240 --> 00:28.880\n He's probably best known as the founding father\n\n00:28.880 --> 00:30.720\n of convolutional neural networks,\n\n00:30.720 --> 00:32.480\n in particular their application\n\n00:32.480 --> 00:34.400\n to optical character recognition\n\n00:34.400 --> 00:37.240\n and the famed MNIST dataset.\n\n00:37.240 --> 00:40.100\n He is also an outspoken personality,\n\n00:40.100 --> 00:43.800\n unafraid to speak his mind in a distinctive French accent\n\n00:43.800 --> 00:45.720\n and explore provocative ideas,\n\n00:45.720 --> 00:48.360\n both in the rigorous medium of academic research\n\n00:48.360 --> 00:51.000\n and the somewhat less rigorous medium\n\n00:51.000 --> 00:52.800\n of Twitter and Facebook.\n\n00:52.800 --> 00:55.600\n This is the Artificial Intelligence Podcast.\n\n00:55.600 --> 00:57.960\n If you enjoy it, subscribe on YouTube,\n\n00:57.960 --> 01:00.960\n give it five stars on iTunes, support it on Patreon,\n\n01:00.960 --> 01:03.840\n or simply connect with me on Twitter at Lex Friedman,\n\n01:03.840 --> 01:06.840\n spelled F R I D M A N.\n\n01:06.840 --> 01:10.640\n And now, here's my conversation with Yann LeCun.\n\n01:11.720 --> 01:13.820\n You said that 2001 Space Odyssey\n\n01:13.820 --> 01:15.360\n is one of your favorite movies.\n\n01:16.260 --> 01:20.360\n Hal 9000 decides to get rid of the astronauts\n\n01:20.360 --> 01:23.040\n for people who haven't seen the movie, spoiler alert,\n\n01:23.040 --> 01:28.040\n because he, it, she believes that the astronauts,\n\n01:29.200 --> 01:31.600\n they will interfere with the mission.\n\n01:31.600 --> 01:34.720\n Do you see Hal as flawed in some fundamental way\n\n01:34.720 --> 01:38.440\n or even evil, or did he do the right thing?\n\n01:38.440 --> 01:39.320\n Neither.\n\n01:39.320 --> 01:43.240\n There's no notion of evil in that context,\n\n01:43.240 --> 01:44.760\n other than the fact that people die,\n\n01:44.760 --> 01:48.720\n but it was an example of what people call\n\n01:48.720 --> 01:50.120\n value misalignment, right?\n\n01:50.120 --> 01:52.120\n You give an objective to a machine,\n\n01:52.120 --> 01:55.560\n and the machine strives to achieve this objective.\n\n01:55.560 --> 01:58.160\n And if you don't put any constraints on this objective,\n\n01:58.160 --> 02:00.860\n like don't kill people and don't do things like this,\n\n02:02.260 --> 02:06.240\n the machine, given the power, will do stupid things\n\n02:06.240 --> 02:08.000\n just to achieve this objective,\n\n02:08.000 --> 02:10.200\n or damaging things to achieve this objective.\n\n02:10.200 --> 02:12.440\n It's a little bit like, I mean, we're used to this\n\n02:12.440 --> 02:14.320\n in the context of human society.\n\n02:15.740 --> 02:20.740\n We put in place laws to prevent people\n\n02:20.740 --> 02:22.920\n from doing bad things, because spontaneously,\n\n02:22.920 --> 02:24.800\n they would do those bad things, right?\n\n02:24.800 --> 02:28.400\n So we have to shape their cost function,\n\n02:28.400 --> 02:29.500\n their objective function, if you want,\n\n02:29.500 --> 02:31.520\n through laws to kind of correct,\n\n02:31.520 --> 02:35.180\n and education, obviously, to sort of correct for those.\n\n02:36.120 --> 02:41.040\n So maybe just pushing a little further on that point,\n\n02:41.960 --> 02:44.360\n how, you know, there's a mission,\n\n02:44.360 --> 02:46.400\n there's this fuzziness around,\n\n02:46.400 --> 02:49.800\n the ambiguity around what the actual mission is,\n\n02:49.800 --> 02:54.800\n but, you know, do you think that there will be a time,\n\n02:55.120 --> 02:56.720\n from a utilitarian perspective,\n\n02:56.720 --> 02:59.660\n where an AI system, where it is not misalignment,\n\n02:59.660 --> 03:02.820\n where it is alignment, for the greater good of society,\n\n03:02.820 --> 03:05.880\n that an AI system will make decisions that are difficult?\n\n03:05.880 --> 03:06.800\n Well, that's the trick.\n\n03:06.800 --> 03:10.800\n I mean, eventually we'll have to figure out how to do this.\n\n03:10.800 --> 03:12.600\n And again, we're not starting from scratch,\n\n03:12.600 --> 03:16.440\n because we've been doing this with humans for millennia.\n\n03:16.440 --> 03:19.160\n So designing objective functions for people\n\n03:19.160 --> 03:20.880\n is something that we know how to do.\n\n03:20.880 --> 03:24.600\n And we don't do it by, you know, programming things,\n\n03:24.600 --> 03:29.060\n although the legal code is called code.\n\n03:29.060 --> 03:30.640\n So that tells you something.\n\n03:30.640 --> 03:33.040\n And it's actually the design of an objective function.\n\n03:33.040 --> 03:34.600\n That's really what legal code is, right?\n\n03:34.600 --> 03:36.280\n It tells you, here is what you can do,\n\n03:36.280 --> 03:37.420\n here is what you can't do.\n\n03:37.420 --> 03:39.000\n If you do it, you pay that much,\n\n03:39.000 --> 03:40.720\n that's an objective function.\n\n03:41.680 --> 03:44.600\n So there is this idea somehow that it's a new thing\n\n03:44.600 --> 03:46.600\n for people to try to design objective functions\n\n03:46.600 --> 03:47.940\n that are aligned with the common good.\n\n03:47.940 --> 03:49.880\n But no, we've been writing laws for millennia\n\n03:49.880 --> 03:52.080\n and that's exactly what it is.\n\n03:52.080 --> 03:57.080\n So that's where, you know, the science of lawmaking\n\n03:57.120 --> 04:00.560\n and computer science will.\n\n04:00.560 --> 04:01.400\n Come together.\n\n04:01.400 --> 04:02.840\n Will come together.\n\n04:02.840 --> 04:06.800\n So there's nothing special about HAL or AI systems,\n\n04:06.800 --> 04:09.480\n it's just the continuation of tools used\n\n04:09.480 --> 04:11.740\n to make some of these difficult ethical judgments\n\n04:11.740 --> 04:13.020\n that laws make.\n\n04:13.020 --> 04:15.080\n Yeah, and we have systems like this already\n\n04:15.080 --> 04:19.960\n that make many decisions for ourselves in society\n\n04:19.960 --> 04:22.600\n that need to be designed in a way that they,\n\n04:22.600 --> 04:27.480\n like rules about things that sometimes have bad side effects\n\n04:27.480 --> 04:29.600\n and we have to be flexible enough about those rules\n\n04:29.600 --> 04:31.560\n so that they can be broken when it's obvious\n\n04:31.560 --> 04:33.120\n that they shouldn't be applied.\n\n04:34.000 --> 04:35.640\n So you don't see this on the camera here,\n\n04:35.640 --> 04:36.920\n but all the decoration in this room\n\n04:36.920 --> 04:39.640\n is all pictures from 2001 and Space Odyssey.\n\n04:41.360 --> 04:43.720\n Wow, is that by accident or is there a lot?\n\n04:43.720 --> 04:45.460\n No, by accident, it's by design.\n\n04:47.440 --> 04:48.480\n Oh, wow.\n\n04:48.480 --> 04:52.560\n So if you were to build HAL 10,000,\n\n04:52.560 --> 04:57.120\n so an improvement of HAL 9,000, what would you improve?\n\n04:57.120 --> 05:00.680\n Well, first of all, I wouldn't ask it to hold secrets\n\n05:00.680 --> 05:03.440\n and tell lies because that's really what breaks it\n\n05:03.440 --> 05:06.680\n in the end, that's the fact that it's asking itself\n\n05:06.680 --> 05:08.840\n questions about the purpose of the mission\n\n05:08.840 --> 05:11.560\n and it's, you know, pieces things together that it's heard,\n\n05:11.560 --> 05:14.000\n you know, all the secrecy of the preparation of the mission\n\n05:14.000 --> 05:16.440\n and the fact that it was the discovery\n\n05:16.440 --> 05:19.560\n on the lunar surface that really was kept secret\n\n05:19.560 --> 05:22.360\n and one part of HAL's memory knows this\n\n05:22.360 --> 05:24.720\n and the other part does not know it\n\n05:24.720 --> 05:26.720\n and is supposed to not tell anyone\n\n05:26.720 --> 05:28.600\n and that creates internal conflict.\n\n05:28.600 --> 05:32.240\n So you think there's never should be a set of things\n\n05:32.240 --> 05:35.520\n that an AI system should not be allowed,\n\n05:36.600 --> 05:39.920\n like a set of facts that should not be shared\n\n05:39.920 --> 05:41.400\n with the human operators?\n\n05:42.400 --> 05:45.440\n Well, I think, no, I think it should be a bit like\n\n05:46.600 --> 05:51.600\n in the design of autonomous AI systems,\n\n05:52.040 --> 05:54.280\n there should be the equivalent of, you know,\n\n05:54.280 --> 05:59.080\n the oath that a hypocrite oath\n\n05:59.080 --> 06:02.640\n that doctors sign up to, right?\n\n06:02.640 --> 06:04.120\n So there's certain things, certain rules\n\n06:04.120 --> 06:07.280\n that you have to abide by and we can sort of hardwire this\n\n06:07.280 --> 06:11.000\n into our machines to kind of make sure they don't go.\n\n06:11.000 --> 06:14.720\n So I'm not, you know, an advocate of the three laws\n\n06:14.720 --> 06:17.120\n of robotics, you know, the Asimov kind of thing\n\n06:17.120 --> 06:18.560\n because I don't think it's practical,\n\n06:18.560 --> 06:23.240\n but, you know, some level of limits.\n\n06:23.240 --> 06:27.960\n But to be clear, these are not questions\n\n06:27.960 --> 06:32.040\n that are kind of really worth asking today\n\n06:32.040 --> 06:34.360\n because we just don't have the technology to do this.\n\n06:34.360 --> 06:36.440\n We don't have autonomous intelligent machines,\n\n06:36.440 --> 06:37.560\n we have intelligent machines.\n\n06:37.560 --> 06:41.000\n Some are intelligent machines that are very specialized,\n\n06:41.000 --> 06:43.360\n but they don't really sort of satisfy an objective.\n\n06:43.360 --> 06:46.520\n They're just, you know, kind of trained to do one thing.\n\n06:46.520 --> 06:50.000\n So until we have some idea for design\n\n06:50.000 --> 06:53.360\n of a full fledged autonomous intelligent system,\n\n06:53.360 --> 06:55.680\n asking the question of how we design this objective,\n\n06:55.680 --> 06:58.600\n I think is a little too abstract.\n\n06:58.600 --> 06:59.680\n It's a little too abstract.\n\n06:59.680 --> 07:04.240\n There's useful elements to it in that it helps us understand\n\n07:04.240 --> 07:07.960\n our own ethical codes, humans.\n\n07:07.960 --> 07:10.240\n So even just as a thought experiment,\n\n07:10.240 --> 07:14.280\n if you imagine that an AGI system is here today,\n\n07:14.280 --> 07:17.640\n how would we program it is a kind of nice thought experiment\n\n07:17.640 --> 07:21.880\n of constructing how should we have a law,\n\n07:21.880 --> 07:24.360\n have a system of laws for us humans.\n\n07:24.360 --> 07:26.800\n It's just a nice practical tool.\n\n07:26.800 --> 07:29.760\n And I think there's echoes of that idea too\n\n07:29.760 --> 07:32.160\n in the AI systems we have today\n\n07:32.160 --> 07:33.960\n that don't have to be that intelligent.\n\n07:33.960 --> 07:34.800\n Yeah.\n\n07:34.800 --> 07:35.640\n Like autonomous vehicles.\n\n07:35.640 --> 07:39.200\n These things start creeping in that are worth thinking about,\n\n07:39.200 --> 07:42.600\n but certainly they shouldn't be framed as how.\n\n07:42.600 --> 07:43.720\n Yeah.\n\n07:43.720 --> 07:46.720\n Looking back, what is the most,\n\n07:46.720 --> 07:49.440\n I'm sorry if it's a silly question,\n\n07:49.440 --> 07:51.440\n but what is the most beautiful\n\n07:51.440 --> 07:53.800\n or surprising idea in deep learning\n\n07:53.800 --> 07:56.320\n or AI in general that you've ever come across?\n\n07:56.320 --> 07:58.480\n Sort of personally, when you said back\n\n08:00.040 --> 08:01.960\n and just had this kind of,\n\n08:01.960 --> 08:03.920\n oh, that's pretty cool moment.\n\n08:03.920 --> 08:04.760\n That's nice.\n\n08:04.760 --> 08:05.600\n That's surprising.\n\n08:05.600 --> 08:06.560\n I don't know if it's an idea\n\n08:06.560 --> 08:11.040\n rather than a sort of empirical fact.\n\n08:12.160 --> 08:16.440\n The fact that you can build gigantic neural nets,\n\n08:16.440 --> 08:21.440\n train them on relatively small amounts of data relatively\n\n08:23.400 --> 08:24.840\n with stochastic gradient descent\n\n08:24.840 --> 08:26.920\n and that it actually works,\n\n08:26.920 --> 08:29.240\n breaks everything you read in every textbook, right?\n\n08:29.240 --> 08:32.560\n Every pre deep learning textbook that told you,\n\n08:32.560 --> 08:33.920\n you need to have fewer parameters\n\n08:33.920 --> 08:35.520\n and you have data samples.\n\n08:37.080 --> 08:38.760\n If you have a non convex objective function,\n\n08:38.760 --> 08:40.680\n you have no guarantee of convergence.\n\n08:40.680 --> 08:42.080\n All those things that you read in textbook\n\n08:42.080 --> 08:43.640\n and they tell you to stay away from this\n\n08:43.640 --> 08:45.120\n and they're all wrong.\n\n08:45.120 --> 08:48.080\n The huge number of parameters, non convex,\n\n08:48.080 --> 08:50.320\n and somehow which is very relative\n\n08:50.320 --> 08:53.480\n to the number of parameters data,\n\n08:53.480 --> 08:54.840\n it's able to learn anything.\n\n08:54.840 --> 08:55.680\n Right.\n\n08:55.680 --> 08:57.520\n Does that still surprise you today?\n\n08:57.520 --> 09:00.360\n Well, it was kind of obvious to me\n\n09:00.360 --> 09:04.120\n before I knew anything that this is a good idea.\n\n09:04.120 --> 09:06.040\n And then it became surprising that it worked\n\n09:06.040 --> 09:08.080\n because I started reading those textbooks.\n\n09:09.240 --> 09:10.080\n Okay.\n\n09:10.080 --> 09:10.920\n Okay.\n\n09:10.920 --> 09:12.280\n So can you talk through the intuition\n\n09:12.280 --> 09:14.360\n of why it was obvious to you if you remember?\n\n09:14.360 --> 09:15.200\n Well, okay.\n\n09:15.200 --> 09:17.360\n So the intuition was it's sort of like,\n\n09:17.360 --> 09:19.960\n those people in the late 19th century\n\n09:19.960 --> 09:24.960\n who proved that heavier than air flight was impossible.\n\n09:25.480 --> 09:26.800\n And of course you have birds, right?\n\n09:26.800 --> 09:28.280\n They do fly.\n\n09:28.280 --> 09:30.400\n And so on the face of it,\n\n09:30.400 --> 09:33.200\n it's obviously wrong as an empirical question, right?\n\n09:33.200 --> 09:34.640\n And so we have the same kind of thing\n\n09:34.640 --> 09:38.560\n that we know that the brain works.\n\n09:38.560 --> 09:39.920\n We don't know how, but we know it works.\n\n09:39.920 --> 09:43.160\n And we know it's a large network of neurons and interaction\n\n09:43.160 --> 09:45.360\n and that learning takes place by changing the connection.\n\n09:45.360 --> 09:48.000\n So kind of getting this level of inspiration\n\n09:48.000 --> 09:49.320\n without copying the details,\n\n09:49.320 --> 09:52.520\n but sort of trying to derive basic principles,\n\n09:52.520 --> 09:56.760\n and that kind of gives you a clue\n\n09:56.760 --> 09:58.320\n as to which direction to go.\n\n09:58.320 --> 10:01.120\n There's also the idea somehow that I've been convinced of\n\n10:01.120 --> 10:04.640\n since I was an undergrad that, even before,\n\n10:04.640 --> 10:06.840\n that intelligence is inseparable from learning.\n\n10:06.840 --> 10:10.000\n So the idea somehow that you can create\n\n10:10.000 --> 10:14.040\n an intelligent machine by basically programming,\n\n10:14.040 --> 10:17.600\n for me it was a non starter from the start.\n\n10:17.600 --> 10:20.280\n Every intelligent entity that we know about\n\n10:20.280 --> 10:23.960\n arrives at this intelligence through learning.\n\n10:24.960 --> 10:28.200\n So machine learning was a completely obvious path.\n\n10:29.960 --> 10:32.000\n Also because I'm lazy, so, you know, kind of.\n\n10:32.000 --> 10:35.160\n He's automate basically everything\n\n10:35.160 --> 10:37.840\n and learning is the automation of intelligence.\n\n10:37.840 --> 10:42.840\n So do you think, so what is learning then?\n\n10:42.920 --> 10:44.520\n What falls under learning?\n\n10:44.520 --> 10:48.240\n Because do you think of reasoning as learning?\n\n10:48.240 --> 10:51.600\n Well, reasoning is certainly a consequence\n\n10:51.600 --> 10:56.600\n of learning as well, just like other functions of the brain.\n\n10:56.600 --> 10:58.160\n The big question about reasoning is,\n\n10:58.160 --> 11:00.680\n how do you make reasoning compatible\n\n11:00.680 --> 11:02.720\n with gradient based learning?\n\n11:02.720 --> 11:04.960\n Do you think neural networks can be made to reason?\n\n11:04.960 --> 11:07.080\n Yes, there is no question about that.\n\n11:07.080 --> 11:08.920\n Again, we have a good example, right?\n\n11:10.320 --> 11:11.680\n The question is how?\n\n11:11.680 --> 11:14.040\n So the question is how much prior structure\n\n11:14.040 --> 11:15.360\n do you have to put in the neural net\n\n11:15.360 --> 11:17.480\n so that something like human reasoning\n\n11:17.480 --> 11:20.840\n will emerge from it, you know, from learning?\n\n11:20.840 --> 11:24.600\n Another question is all of our kind of model\n\n11:24.600 --> 11:27.240\n of what reasoning is that are based on logic\n\n11:27.240 --> 11:31.120\n are discrete and are therefore incompatible\n\n11:31.120 --> 11:32.720\n with gradient based learning.\n\n11:32.720 --> 11:34.120\n And I'm a very strong believer\n\n11:34.120 --> 11:35.840\n in this idea of gradient based learning.\n\n11:35.840 --> 11:39.280\n I don't believe that other types of learning\n\n11:39.280 --> 11:41.920\n that don't use kind of gradient information if you want.\n\n11:41.920 --> 11:43.400\n So you don't like discrete mathematics?\n\n11:43.400 --> 11:45.000\n You don't like anything discrete?\n\n11:45.000 --> 11:46.920\n Well, that's, it's not that I don't like it,\n\n11:46.920 --> 11:49.200\n it's just that it's incompatible with learning\n\n11:49.200 --> 11:51.120\n and I'm a big fan of learning, right?\n\n11:51.120 --> 11:53.600\n So in fact, that's perhaps one reason\n\n11:53.600 --> 11:57.040\n why deep learning has been kind of looked at\n\n11:57.040 --> 11:58.720\n with suspicion by a lot of computer scientists\n\n11:58.720 --> 11:59.920\n because the math is very different.\n\n11:59.920 --> 12:02.480\n The math that you use for deep learning,\n\n12:02.480 --> 12:05.040\n you know, it kind of has more to do with,\n\n12:05.040 --> 12:08.280\n you know, cybernetics, the kind of math you do\n\n12:08.280 --> 12:10.600\n in electrical engineering than the kind of math\n\n12:10.600 --> 12:12.240\n you do in computer science.\n\n12:12.240 --> 12:15.680\n And, you know, nothing in machine learning is exact, right?\n\n12:15.680 --> 12:18.520\n Computer science is all about sort of, you know,\n\n12:18.520 --> 12:21.960\n obviously compulsive attention to details of like,\n\n12:21.960 --> 12:23.760\n you know, every index has to be right.\n\n12:23.760 --> 12:26.760\n And you can prove that an algorithm is correct, right?\n\n12:26.760 --> 12:30.360\n Machine learning is the science of sloppiness, really.\n\n12:30.360 --> 12:32.920\n That's beautiful.\n\n12:32.920 --> 12:37.920\n So, okay, maybe let's feel around in the dark\n\n12:38.200 --> 12:41.400\n of what is a neural network that reasons\n\n12:41.400 --> 12:46.400\n or a system that works with continuous functions\n\n12:47.840 --> 12:52.400\n that's able to do, build knowledge,\n\n12:52.400 --> 12:54.280\n however we think about reasoning,\n\n12:54.280 --> 12:57.880\n build on previous knowledge, build on extra knowledge,\n\n12:57.880 --> 12:59.520\n create new knowledge,\n\n12:59.520 --> 13:03.100\n generalize outside of any training set to ever build.\n\n13:03.100 --> 13:04.560\n What does that look like?\n\n13:04.560 --> 13:08.780\n If, yeah, maybe give inklings of thoughts\n\n13:08.780 --> 13:10.860\n of what that might look like.\n\n13:10.860 --> 13:12.320\n Yeah, I mean, yes and no.\n\n13:12.320 --> 13:14.220\n If I had precise ideas about this,\n\n13:14.220 --> 13:17.280\n I think, you know, we'd be building it right now.\n\n13:17.280 --> 13:19.120\n And there are people working on this\n\n13:19.120 --> 13:22.240\n whose main research interest is actually exactly that, right?\n\n13:22.240 --> 13:25.320\n So what you need to have is a working memory.\n\n13:25.320 --> 13:29.940\n So you need to have some device, if you want,\n\n13:29.940 --> 13:34.600\n some subsystem that can store a relatively large number\n\n13:34.600 --> 13:39.080\n of factual episodic information for, you know,\n\n13:39.080 --> 13:40.920\n a reasonable amount of time.\n\n13:40.920 --> 13:43.920\n So, you know, in the brain, for example,\n\n13:43.920 --> 13:45.800\n there are kind of three main types of memory.\n\n13:45.800 --> 13:50.800\n One is the sort of memory of the state of your cortex.\n\n13:53.760 --> 13:55.920\n And that sort of disappears within 20 seconds.\n\n13:55.920 --> 13:58.280\n You can't remember things for more than about 20 seconds\n\n13:58.280 --> 14:01.560\n or a minute if you don't have any other form of memory.\n\n14:02.440 --> 14:04.480\n The second type of memory, which is longer term,\n\n14:04.480 --> 14:06.200\n is still short term, is the hippocampus.\n\n14:06.200 --> 14:08.360\n So you can, you know, you came into this building,\n\n14:08.360 --> 14:12.800\n you remember where the exit is, where the elevators are.\n\n14:14.000 --> 14:15.560\n You have some map of that building\n\n14:15.560 --> 14:17.520\n that's stored in your hippocampus.\n\n14:17.520 --> 14:20.240\n You might remember something about what I said,\n\n14:20.240 --> 14:21.400\n you know, a few minutes ago.\n\n14:21.400 --> 14:22.320\n I forgot it all already.\n\n14:22.320 --> 14:24.420\n Of course, it's been erased, but, you know,\n\n14:24.420 --> 14:27.360\n but that would be in your hippocampus.\n\n14:27.360 --> 14:30.700\n And then the longer term memory is in the synapse,\n\n14:30.700 --> 14:31.900\n the synapses, right?\n\n14:32.880 --> 14:34.640\n So what you need if you want a system\n\n14:34.640 --> 14:35.600\n that's capable of reasoning\n\n14:35.600 --> 14:38.800\n is that you want the hippocampus like thing, right?\n\n14:40.240 --> 14:41.800\n And that's what people have tried to do\n\n14:41.800 --> 14:43.720\n with memory networks and, you know,\n\n14:43.720 --> 14:45.800\n neural training machines and stuff like that, right?\n\n14:45.800 --> 14:47.200\n And now with transformers,\n\n14:47.200 --> 14:50.540\n which have sort of a memory in there,\n\n14:50.540 --> 14:51.980\n kind of self attention system.\n\n14:51.980 --> 14:53.480\n You can think of it this way.\n\n14:55.720 --> 14:57.160\n So that's one element you need.\n\n14:57.160 --> 14:59.880\n Another thing you need is some sort of network\n\n14:59.880 --> 15:03.240\n that can access this memory,\n\n15:03.240 --> 15:08.160\n get an information back and then kind of crunch on it\n\n15:08.160 --> 15:10.920\n and then do this iteratively multiple times\n\n15:10.920 --> 15:15.860\n because a chain of reasoning is a process\n\n15:15.860 --> 15:19.400\n by which you update your knowledge\n\n15:19.400 --> 15:20.400\n about the state of the world,\n\n15:20.400 --> 15:22.820\n about, you know, what's going to happen, et cetera.\n\n15:22.820 --> 15:25.440\n And that has to be this sort of\n\n15:25.440 --> 15:27.120\n recurrent operation basically.\n\n15:27.120 --> 15:29.160\n And you think that kind of,\n\n15:29.160 --> 15:31.120\n if we think about a transformer,\n\n15:31.120 --> 15:32.640\n so that seems to be too small\n\n15:32.640 --> 15:34.400\n to contain the knowledge that's,\n\n15:36.240 --> 15:37.280\n to represent the knowledge\n\n15:37.280 --> 15:39.260\n that's contained in Wikipedia, for example.\n\n15:39.260 --> 15:42.000\n Well, a transformer doesn't have this idea of recurrence.\n\n15:42.000 --> 15:43.120\n It's got a fixed number of layers\n\n15:43.120 --> 15:44.680\n and that's the number of steps that, you know,\n\n15:44.680 --> 15:47.120\n limits basically its representation.\n\n15:47.120 --> 15:51.240\n But recurrence would build on the knowledge somehow.\n\n15:51.240 --> 15:54.760\n I mean, it would evolve the knowledge\n\n15:54.760 --> 15:58.080\n and expand the amount of information perhaps\n\n15:58.080 --> 16:00.360\n or useful information within that knowledge.\n\n16:00.360 --> 16:04.800\n But is this something that just can emerge with size?\n\n16:04.800 --> 16:06.440\n Because it seems like everything we have now is too small.\n\n16:06.440 --> 16:09.360\n Not just, no, it's not clear.\n\n16:09.360 --> 16:11.160\n I mean, how you access and write\n\n16:11.160 --> 16:13.800\n into an associative memory in an efficient way.\n\n16:13.800 --> 16:15.240\n I mean, sort of the original memory network\n\n16:15.240 --> 16:17.560\n maybe had something like the right architecture,\n\n16:17.560 --> 16:20.540\n but if you try to scale up a memory network\n\n16:20.540 --> 16:22.880\n so that the memory contains all the Wikipedia,\n\n16:22.880 --> 16:24.040\n it doesn't quite work.\n\n16:24.040 --> 16:25.120\n Right.\n\n16:25.120 --> 16:28.680\n So there's a need for new ideas there, okay.\n\n16:28.680 --> 16:30.000\n But it's not the only form of reasoning.\n\n16:30.000 --> 16:31.400\n So there's another form of reasoning,\n\n16:31.400 --> 16:34.160\n which is true, which is very classical also\n\n16:34.160 --> 16:36.720\n in some types of AI.\n\n16:36.720 --> 16:40.920\n And it's based on, let's call it energy minimization.\n\n16:40.920 --> 16:44.960\n Okay, so you have some sort of objective,\n\n16:44.960 --> 16:47.200\n some energy function that represents\n\n16:47.200 --> 16:52.200\n the quality or the negative quality, okay.\n\n16:53.320 --> 16:54.740\n Energy goes up when things get bad\n\n16:54.740 --> 16:57.320\n and they get low when things get good.\n\n16:57.320 --> 17:00.480\n So let's say you want to figure out,\n\n17:00.480 --> 17:02.960\n what gestures do I need to do\n\n17:03.960 --> 17:07.240\n to grab an object or walk out the door.\n\n17:08.200 --> 17:10.360\n If you have a good model of your own body,\n\n17:10.360 --> 17:12.500\n a good model of the environment,\n\n17:12.500 --> 17:14.360\n using this kind of energy minimization,\n\n17:14.360 --> 17:16.920\n you can do planning.\n\n17:16.920 --> 17:19.280\n And in optimal control,\n\n17:19.280 --> 17:22.140\n it's called model predictive control.\n\n17:22.140 --> 17:24.140\n You have a model of what's gonna happen in the world\n\n17:24.140 --> 17:25.520\n as a consequence of your actions.\n\n17:25.520 --> 17:28.600\n And that allows you to, by energy minimization,\n\n17:28.600 --> 17:29.800\n figure out the sequence of action\n\n17:29.800 --> 17:32.080\n that optimizes a particular objective function,\n\n17:32.080 --> 17:34.160\n which measures, minimizes the number of times\n\n17:34.160 --> 17:35.000\n you're gonna hit something\n\n17:35.000 --> 17:36.540\n and the energy you're gonna spend\n\n17:36.540 --> 17:38.820\n doing the gesture and et cetera.\n\n17:39.800 --> 17:42.440\n So that's a form of reasoning.\n\n17:42.440 --> 17:43.520\n Planning is a form of reasoning.\n\n17:43.520 --> 17:48.040\n And perhaps what led to the ability of humans to reason\n\n17:48.040 --> 17:53.040\n is the fact that, or species that appear before us\n\n17:53.480 --> 17:55.080\n had to do some sort of planning\n\n17:55.080 --> 17:56.960\n to be able to hunt and survive\n\n17:56.960 --> 17:59.600\n and survive the winter in particular.\n\n17:59.600 --> 18:03.360\n And so it's the same capacity that you need to have.\n\n18:03.360 --> 18:06.440\n So in your intuition is,\n\n18:07.600 --> 18:09.520\n if we look at expert systems\n\n18:09.520 --> 18:13.240\n and encoding knowledge as logic systems,\n\n18:13.240 --> 18:16.720\n as graphs, in this kind of way,\n\n18:16.720 --> 18:20.280\n is not a useful way to think about knowledge?\n\n18:20.280 --> 18:23.960\n Graphs are a little brittle or logic representation.\n\n18:23.960 --> 18:27.880\n So basically, variables that have values\n\n18:27.880 --> 18:29.280\n and then constraint between them\n\n18:29.280 --> 18:31.300\n that are represented by rules,\n\n18:31.300 --> 18:32.860\n is a little too rigid and too brittle, right?\n\n18:32.860 --> 18:36.600\n So some of the early efforts in that respect\n\n18:38.640 --> 18:41.020\n were to put probabilities on them.\n\n18:41.020 --> 18:44.560\n So a rule, if you have this and that symptom,\n\n18:44.560 --> 18:47.200\n you have this disease with that probability\n\n18:47.200 --> 18:49.400\n and you should prescribe that antibiotic\n\n18:49.400 --> 18:50.520\n with that probability, right?\n\n18:50.520 --> 18:54.320\n That's the mycin system from the 70s.\n\n18:54.320 --> 18:57.640\n And that's what that branch of AI led to,\n\n18:58.520 --> 19:00.320\n based on networks and graphical models\n\n19:00.320 --> 19:04.960\n and causal inference and variational method.\n\n19:04.960 --> 19:09.960\n So there is certainly a lot of interesting\n\n19:10.240 --> 19:11.440\n work going on in this area.\n\n19:11.440 --> 19:13.880\n The main issue with this is knowledge acquisition.\n\n19:13.880 --> 19:18.880\n How do you reduce a bunch of data to a graph of this type?\n\n19:18.880 --> 19:22.720\n Yeah, it relies on the expert, on the human being,\n\n19:22.720 --> 19:24.960\n to encode, to add knowledge.\n\n19:24.960 --> 19:27.120\n And that's essentially impractical.\n\n19:27.120 --> 19:29.480\n Yeah, it's not scalable.\n\n19:29.480 --> 19:30.320\n That's a big question.\n\n19:30.320 --> 19:31.440\n The second question is,\n\n19:31.440 --> 19:33.800\n do you want to represent knowledge as symbols\n\n19:34.640 --> 19:37.240\n and do you want to manipulate them with logic?\n\n19:37.240 --> 19:39.320\n And again, that's incompatible with learning.\n\n19:39.320 --> 19:43.160\n So one suggestion, which Jeff Hinton\n\n19:43.160 --> 19:45.080\n has been advocating for many decades,\n\n19:45.080 --> 19:49.360\n is replace symbols by vectors.\n\n19:49.360 --> 19:50.960\n Think of it as pattern of activities\n\n19:50.960 --> 19:53.320\n in a bunch of neurons or units\n\n19:53.320 --> 19:55.120\n or whatever you want to call them.\n\n19:55.120 --> 19:59.560\n And replace logic by continuous functions.\n\n19:59.560 --> 20:01.840\n Okay, and that becomes now compatible.\n\n20:01.840 --> 20:04.960\n There's a very good set of ideas\n\n20:04.960 --> 20:07.640\n by, written in a paper about 10 years ago\n\n20:07.640 --> 20:11.000\n by Leon Boutout, who is here at Facebook.\n\n20:13.160 --> 20:14.400\n The title of the paper is,\n\n20:14.400 --> 20:15.840\n From Machine Learning to Machine Reasoning.\n\n20:15.840 --> 20:19.480\n And his idea is that a learning system\n\n20:19.480 --> 20:20.880\n should be able to manipulate objects\n\n20:20.880 --> 20:23.160\n that are in a space\n\n20:23.160 --> 20:24.920\n and then put the result back in the same space.\n\n20:24.920 --> 20:27.280\n So it's this idea of working memory, basically.\n\n20:28.400 --> 20:30.640\n And it's very enlightening.\n\n20:30.640 --> 20:33.720\n And in a sense, that might learn something\n\n20:33.720 --> 20:36.880\n like the simple expert systems.\n\n20:37.920 --> 20:42.080\n I mean, you can learn basic logic operations there.\n\n20:42.080 --> 20:43.400\n Yeah, quite possibly.\n\n20:43.400 --> 20:46.680\n There's a big debate on sort of how much prior structure\n\n20:46.680 --> 20:49.080\n you have to put in for this kind of stuff to emerge.\n\n20:49.080 --> 20:50.720\n That's the debate I have with Gary Marcus\n\n20:50.720 --> 20:51.560\n and people like that.\n\n20:51.560 --> 20:55.040\n Yeah, yeah, so, and the other person,\n\n20:55.040 --> 20:57.520\n so I just talked to Judea Pearl,\n\n20:57.520 --> 21:00.240\n from the you mentioned causal inference world.\n\n21:00.240 --> 21:04.160\n So his worry is that the current neural networks\n\n21:04.160 --> 21:09.160\n are not able to learn what causes\n\n21:09.600 --> 21:12.760\n what causal inference between things.\n\n21:12.760 --> 21:15.640\n So I think he's right and wrong about this.\n\n21:15.640 --> 21:18.640\n If he's talking about the sort of classic\n\n21:20.280 --> 21:21.320\n type of neural nets,\n\n21:21.320 --> 21:23.800\n people sort of didn't worry too much about this.\n\n21:23.800 --> 21:26.160\n But there's a lot of people now working on causal inference.\n\n21:26.160 --> 21:27.840\n And there's a paper that just came out last week\n\n21:27.840 --> 21:29.160\n by Leon Boutou, among others,\n\n21:29.160 --> 21:32.000\n David Lopez, Baz, and a bunch of other people,\n\n21:32.000 --> 21:35.480\n exactly on that problem of how do you kind of\n\n21:36.880 --> 21:39.400\n get a neural net to sort of pay attention\n\n21:39.400 --> 21:41.600\n to real causal relationships,\n\n21:41.600 --> 21:46.600\n which may also solve issues of bias in data\n\n21:46.600 --> 21:48.040\n and things like this, so.\n\n21:48.040 --> 21:49.200\n I'd like to read that paper\n\n21:49.200 --> 21:51.960\n because that ultimately the challenges\n\n21:51.960 --> 21:56.000\n also seems to fall back on the human expert\n\n21:56.920 --> 22:01.880\n to ultimately decide causality between things.\n\n22:01.880 --> 22:02.720\n People are not very good\n\n22:02.720 --> 22:04.800\n at establishing causality, first of all.\n\n22:04.800 --> 22:06.560\n So first of all, you talk to physicists\n\n22:06.560 --> 22:08.600\n and physicists actually don't believe in causality\n\n22:08.600 --> 22:12.960\n because look at all the basic laws of microphysics\n\n22:12.960 --> 22:15.480\n are time reversible, so there's no causality.\n\n22:15.480 --> 22:17.120\n The arrow of time is not real, yeah.\n\n22:17.120 --> 22:20.440\n It's as soon as you start looking at macroscopic systems\n\n22:20.440 --> 22:22.800\n where there is unpredictable randomness,\n\n22:22.800 --> 22:25.440\n where there is clearly an arrow of time,\n\n22:25.440 --> 22:27.320\n but it's a big mystery in physics, actually,\n\n22:27.320 --> 22:29.160\n how that emerges.\n\n22:29.160 --> 22:31.720\n Is it emergent or is it part of\n\n22:31.720 --> 22:34.320\n the fundamental fabric of reality?\n\n22:34.320 --> 22:36.880\n Or is it a bias of intelligent systems\n\n22:36.880 --> 22:39.280\n that because of the second law of thermodynamics,\n\n22:39.280 --> 22:41.440\n we perceive a particular arrow of time,\n\n22:41.440 --> 22:45.120\n but in fact, it's kind of arbitrary, right?\n\n22:45.120 --> 22:47.120\n So yeah, physicists, mathematicians,\n\n22:47.120 --> 22:48.440\n they don't care about, I mean,\n\n22:48.440 --> 22:51.520\n the math doesn't care about the flow of time.\n\n22:51.520 --> 22:54.080\n Well, certainly, macrophysics doesn't.\n\n22:54.080 --> 22:55.440\n People themselves are not very good\n\n22:55.440 --> 22:58.920\n at establishing causal relationships.\n\n22:58.920 --> 23:02.760\n If you ask, I think it was in one of Seymour Papert's book\n\n23:02.760 --> 23:06.800\n on children learning.\n\n23:06.800 --> 23:08.840\n He studied with Jean Piaget.\n\n23:08.840 --> 23:11.520\n He's the guy who coauthored the book Perceptron\n\n23:11.520 --> 23:12.960\n with Marvin Minsky that kind of killed\n\n23:12.960 --> 23:14.080\n the first wave of neural nets,\n\n23:14.080 --> 23:17.200\n but he was actually a learning person.\n\n23:17.200 --> 23:21.040\n He, in the sense of studying learning in humans\n\n23:21.040 --> 23:24.160\n and machines, that's why he got interested in Perceptron.\n\n23:24.160 --> 23:29.160\n And he wrote that if you ask a little kid\n\n23:29.280 --> 23:32.680\n about what is the cause of the wind,\n\n23:33.720 --> 23:35.840\n a lot of kids will say, they will think for a while\n\n23:35.840 --> 23:38.120\n and they'll say, oh, it's the branches in the trees,\n\n23:38.120 --> 23:40.120\n they move and that creates wind, right?\n\n23:40.120 --> 23:42.600\n So they get the causal relationship backwards.\n\n23:42.600 --> 23:44.520\n And it's because their understanding of the world\n\n23:44.520 --> 23:46.280\n and intuitive physics is not that great, right?\n\n23:46.280 --> 23:49.880\n I mean, these are like, you know, four or five year old kids.\n\n23:49.880 --> 23:50.720\n You know, it gets better,\n\n23:50.720 --> 23:54.080\n and then you understand that this, it can be, right?\n\n23:54.080 --> 23:57.440\n But there are many things which we can,\n\n23:57.440 --> 24:00.920\n because of our common sense understanding of things,\n\n24:00.920 --> 24:03.280\n what people call common sense,\n\n24:03.280 --> 24:05.000\n and our understanding of physics,\n\n24:05.000 --> 24:07.640\n we can, there's a lot of stuff\n\n24:07.640 --> 24:08.840\n that we can figure out causality.\n\n24:08.840 --> 24:10.480\n Even with diseases, we can figure out\n\n24:10.480 --> 24:14.520\n what's not causing what, often.\n\n24:14.520 --> 24:16.040\n There's a lot of mystery, of course,\n\n24:16.040 --> 24:18.120\n but the idea is that you should be able\n\n24:18.120 --> 24:20.160\n to encode that into systems,\n\n24:20.160 --> 24:21.400\n because it seems unlikely they'd be able\n\n24:21.400 --> 24:22.800\n to figure that out themselves.\n\n24:22.800 --> 24:24.480\n Well, whenever we can do intervention,\n\n24:24.480 --> 24:27.400\n but you know, all of humanity has been completely deluded\n\n24:27.400 --> 24:30.400\n for millennia, probably since its existence,\n\n24:30.400 --> 24:33.420\n about a very, very wrong causal relationship,\n\n24:33.420 --> 24:35.720\n where whatever you can explain, you attribute it to,\n\n24:35.720 --> 24:37.960\n you know, some deity, some divinity, right?\n\n24:39.240 --> 24:41.000\n And that's a cop out, that's a way of saying like,\n\n24:41.000 --> 24:43.920\n I don't know the cause, so you know, God did it, right?\n\n24:43.920 --> 24:46.240\n So you mentioned Marvin Minsky,\n\n24:46.240 --> 24:51.240\n and the irony of, you know,\n\n24:51.520 --> 24:54.580\n maybe causing the first AI winter.\n\n24:54.580 --> 24:56.920\n You were there in the 90s, you were there in the 80s,\n\n24:56.920 --> 24:58.120\n of course.\n\n24:58.120 --> 25:00.640\n In the 90s, why do you think people lost faith\n\n25:00.640 --> 25:04.000\n in deep learning, in the 90s, and found it again,\n\n25:04.000 --> 25:06.360\n a decade later, over a decade later?\n\n25:06.360 --> 25:07.760\n Yeah, it wasn't called deep learning yet,\n\n25:07.760 --> 25:11.880\n it was just called neural nets, but yeah,\n\n25:11.880 --> 25:13.840\n they lost interest.\n\n25:13.840 --> 25:16.840\n I mean, I think I would put that around 1995,\n\n25:16.840 --> 25:18.080\n at least the machine learning community,\n\n25:18.080 --> 25:19.660\n there was always a neural net community,\n\n25:19.660 --> 25:23.760\n but it became kind of disconnected\n\n25:23.760 --> 25:26.560\n from sort of mainstream machine learning, if you want.\n\n25:26.560 --> 25:30.960\n There were, it was basically electrical engineering\n\n25:30.960 --> 25:35.960\n that kept at it, and computer science gave up on neural nets.\n\n25:38.000 --> 25:40.520\n I don't know, you know, I was too close to it\n\n25:40.520 --> 25:45.520\n to really sort of analyze it with sort of an unbiased eye,\n\n25:46.960 --> 25:50.760\n if you want, but I would make a few guesses.\n\n25:50.760 --> 25:55.760\n So the first one is, at the time, neural nets were,\n\n25:55.760 --> 25:57.880\n it was very hard to make them work,\n\n25:57.880 --> 26:02.400\n in the sense that you would implement backprop\n\n26:02.400 --> 26:06.120\n in your favorite language, and that favorite language\n\n26:06.120 --> 26:08.240\n was not Python, it was not MATLAB,\n\n26:08.240 --> 26:09.320\n it was not any of those things,\n\n26:09.320 --> 26:10.760\n because they didn't exist, right?\n\n26:10.760 --> 26:13.320\n You had to write it in Fortran OC,\n\n26:13.320 --> 26:14.880\n or something like this, right?\n\n26:16.320 --> 26:18.680\n So you would experiment with it,\n\n26:18.680 --> 26:21.200\n you would probably make some very basic mistakes,\n\n26:21.200 --> 26:23.240\n like, you know, badly initialize your weights,\n\n26:23.240 --> 26:24.200\n make the network too small,\n\n26:24.200 --> 26:25.520\n because you read in the textbook, you know,\n\n26:25.520 --> 26:27.640\n you don't want too many parameters, right?\n\n26:27.640 --> 26:29.280\n And of course, you know, and you would train on XOR,\n\n26:29.280 --> 26:32.000\n because you didn't have any other data set to trade on.\n\n26:32.000 --> 26:33.760\n And of course, you know, it works half the time.\n\n26:33.760 --> 26:36.280\n So you would say, I give up.\n\n26:36.280 --> 26:37.680\n Also, you would train it with batch gradient,\n\n26:37.680 --> 26:40.240\n which, you know, isn't that sufficient.\n\n26:40.240 --> 26:42.680\n So there's a lot of, there's a bag of tricks\n\n26:42.680 --> 26:44.840\n that you had to know to make those things work,\n\n26:44.840 --> 26:48.200\n or you had to reinvent, and a lot of people just didn't,\n\n26:48.200 --> 26:50.000\n and they just couldn't make it work.\n\n26:51.320 --> 26:52.400\n So that's one thing.\n\n26:52.400 --> 26:54.720\n The investment in software platform\n\n26:54.720 --> 26:58.120\n to be able to kind of, you know, display things,\n\n26:58.120 --> 26:59.360\n figure out why things don't work,\n\n26:59.360 --> 27:02.120\n kind of get a good intuition for how to get them to work,\n\n27:02.120 --> 27:04.640\n have enough flexibility so you can create, you know,\n\n27:04.640 --> 27:06.240\n network architectures like convolutional nets\n\n27:06.240 --> 27:07.280\n and stuff like that.\n\n27:08.320 --> 27:09.160\n It was hard.\n\n27:09.160 --> 27:10.520\n I mean, you had to write everything from scratch.\n\n27:10.520 --> 27:11.840\n And again, you didn't have any Python\n\n27:11.840 --> 27:13.240\n or MATLAB or anything, right?\n\n27:14.280 --> 27:15.600\n I read that, sorry to interrupt,\n\n27:15.600 --> 27:17.680\n but I read that you wrote in Lisp\n\n27:17.680 --> 27:22.680\n the first versions of Lanet with convolutional networks,\n\n27:22.680 --> 27:25.320\n which by the way, one of my favorite languages.\n\n27:25.320 --> 27:27.560\n That's how I knew you were legit.\n\n27:27.560 --> 27:29.440\n Turing award, whatever.\n\n27:29.440 --> 27:30.760\n You programmed in Lisp, that's...\n\n27:30.760 --> 27:31.920\n It's still my favorite language,\n\n27:31.920 --> 27:34.880\n but it's not that we programmed in Lisp,\n\n27:34.880 --> 27:38.000\n it's that we had to write our Lisp interpreter, okay?\n\n27:38.000 --> 27:40.320\n Because it's not like we used one that existed.\n\n27:40.320 --> 27:43.880\n So we wrote a Lisp interpreter that we hooked up to,\n\n27:43.880 --> 27:46.640\n you know, a backend library that we wrote also\n\n27:46.640 --> 27:48.440\n for sort of neural net computation.\n\n27:48.440 --> 27:50.840\n And then after a few years around 1991,\n\n27:50.840 --> 27:54.560\n we invented this idea of basically having modules\n\n27:54.560 --> 27:56.160\n that know how to forward propagate\n\n27:56.160 --> 27:57.560\n and back propagate gradients,\n\n27:57.560 --> 28:00.280\n and then interconnecting those modules in a graph.\n\n28:01.480 --> 28:03.280\n Number two had made proposals on this,\n\n28:03.280 --> 28:04.720\n about this in the late eighties,\n\n28:04.720 --> 28:08.200\n and we were able to implement this using our Lisp system.\n\n28:08.200 --> 28:09.800\n Eventually we wanted to use that system\n\n28:09.800 --> 28:13.800\n to build production code for character recognition\n\n28:13.800 --> 28:14.640\n at Bell Labs.\n\n28:14.640 --> 28:16.760\n So we actually wrote a compiler for that Lisp interpreter\n\n28:16.760 --> 28:19.280\n so that Patricia Simard, who is now at Microsoft,\n\n28:19.280 --> 28:22.400\n kind of did the bulk of it with Leon and me.\n\n28:22.400 --> 28:24.920\n And so we could write our system in Lisp\n\n28:24.920 --> 28:26.520\n and then compile to C,\n\n28:26.520 --> 28:29.720\n and then we'll have a self contained complete system\n\n28:29.720 --> 28:32.160\n that could kind of do the entire thing.\n\n28:33.280 --> 28:36.080\n Neither PyTorch nor TensorFlow can do this today.\n\n28:36.080 --> 28:37.840\n Yeah, okay, it's coming.\n\n28:37.840 --> 28:38.680\n Yeah.\n\n28:40.080 --> 28:42.000\n I mean, there's something like that in PyTorch\n\n28:42.000 --> 28:44.520\n called TorchScript.\n\n28:44.520 --> 28:46.840\n And so, you know, we had to write our Lisp interpreter,\n\n28:46.840 --> 28:48.000\n we had to write our Lisp compiler,\n\n28:48.000 --> 28:50.840\n we had to invest a huge amount of effort to do this.\n\n28:50.840 --> 28:52.320\n And not everybody,\n\n28:52.320 --> 28:55.040\n if you don't completely believe in the concept,\n\n28:55.040 --> 28:57.040\n you're not going to invest the time to do this.\n\n28:57.040 --> 28:59.160\n Now at the time also, you know,\n\n28:59.160 --> 29:02.640\n or today, this would turn into Torch or PyTorch\n\n29:02.640 --> 29:03.840\n or TensorFlow or whatever,\n\n29:03.840 --> 29:05.720\n we'd put it in open source, everybody would use it\n\n29:05.720 --> 29:07.920\n and, you know, realize it's good.\n\n29:07.920 --> 29:11.240\n Back before 1995, working at AT&T,\n\n29:11.240 --> 29:13.720\n there's no way the lawyers would let you\n\n29:13.720 --> 29:17.680\n release anything in open source of this nature.\n\n29:17.680 --> 29:20.600\n And so we could not distribute our code really.\n\n29:20.600 --> 29:21.920\n And on that point,\n\n29:21.920 --> 29:23.520\n and sorry to go on a million tangents,\n\n29:23.520 --> 29:26.560\n but on that point, I also read that there was some,\n\n29:26.560 --> 29:30.000\n almost like a patent on convolutional neural networks\n\n29:30.000 --> 29:32.000\n at Bell Labs.\n\n29:32.000 --> 29:35.680\n So that, first of all, I mean, just.\n\n29:35.680 --> 29:36.680\n There's two actually.\n\n29:38.000 --> 29:39.840\n That ran out.\n\n29:39.840 --> 29:41.840\n Thankfully, in 2007.\n\n29:41.840 --> 29:42.680\n In 2007.\n\n29:42.680 --> 29:45.440\n So I'm gonna, what,\n\n29:46.800 --> 29:48.600\n can we just talk about that for a second?\n\n29:48.600 --> 29:51.200\n I know you're a Facebook, but you're also at NYU.\n\n29:51.200 --> 29:55.520\n And what does it mean to patent ideas\n\n29:55.520 --> 29:58.920\n like these software ideas, essentially?\n\n29:58.920 --> 30:02.360\n Or what are mathematical ideas?\n\n30:02.360 --> 30:03.320\n Or what are they?\n\n30:03.320 --> 30:05.640\n Okay, so they're not mathematical ideas.\n\n30:05.640 --> 30:07.600\n They are, you know, algorithms.\n\n30:07.600 --> 30:11.200\n And there was a period where the US Patent Office\n\n30:11.200 --> 30:14.000\n would allow the patent of software\n\n30:14.000 --> 30:15.360\n as long as it was embodied.\n\n30:16.280 --> 30:18.120\n The Europeans are very different.\n\n30:18.120 --> 30:20.320\n They don't quite accept that.\n\n30:20.320 --> 30:21.160\n They have a different concept.\n\n30:21.160 --> 30:24.040\n But, you know, I don't, I no longer,\n\n30:24.040 --> 30:26.280\n I mean, I never actually strongly believed in this,\n\n30:26.280 --> 30:28.880\n but I don't believe in this kind of patent.\n\n30:28.880 --> 30:31.680\n Facebook basically doesn't believe in this kind of patent.\n\n30:34.040 --> 30:39.040\n Google fires patents because they've been burned with Apple.\n\n30:39.040 --> 30:41.360\n And so now they do this for defensive purpose,\n\n30:41.360 --> 30:42.720\n but usually they say,\n\n30:42.720 --> 30:44.760\n we're not gonna sue you if you infringe.\n\n30:44.760 --> 30:47.080\n Facebook has a similar policy.\n\n30:47.080 --> 30:49.560\n They say, you know, we fire patents on certain things\n\n30:49.560 --> 30:50.480\n for defensive purpose.\n\n30:50.480 --> 30:52.080\n We're not gonna sue you if you infringe,\n\n30:52.080 --> 30:53.080\n unless you sue us.\n\n30:54.600 --> 30:59.240\n So the industry does not believe in patents.\n\n30:59.240 --> 31:00.720\n They are there because of, you know,\n\n31:00.720 --> 31:03.280\n the legal landscape and various things.\n\n31:03.280 --> 31:06.280\n But I don't really believe in patents\n\n31:06.280 --> 31:07.560\n for this kind of stuff.\n\n31:07.560 --> 31:09.600\n So that's a great thing.\n\n31:09.600 --> 31:10.440\n So I...\n\n31:10.440 --> 31:11.800\n I'll tell you a worse story, actually.\n\n31:11.800 --> 31:15.440\n So what happens was the first patent about convolutional net\n\n31:15.440 --> 31:18.240\n was about kind of the early version of convolutional net\n\n31:18.240 --> 31:19.960\n that didn't have separate pooling layers.\n\n31:19.960 --> 31:22.880\n It had convolutional layers\n\n31:22.880 --> 31:25.240\n which tried more than one, if you want, right?\n\n31:25.240 --> 31:28.440\n And then there was a second one on convolutional nets\n\n31:28.440 --> 31:31.720\n with separate pooling layers, trained with backprop.\n\n31:31.720 --> 31:35.280\n And there were files filed in 89 and 1990\n\n31:35.280 --> 31:36.240\n or something like this.\n\n31:36.240 --> 31:39.360\n At the time, the life of a patent was 17 years.\n\n31:40.280 --> 31:42.080\n So here's what happened over the next few years\n\n31:42.080 --> 31:45.480\n is that we started developing character recognition\n\n31:45.480 --> 31:48.640\n technology around convolutional nets.\n\n31:48.640 --> 31:51.080\n And in 1994,\n\n31:52.200 --> 31:56.160\n a check reading system was deployed in ATM machines.\n\n31:56.160 --> 31:59.040\n In 1995, it was for large check reading machines\n\n31:59.040 --> 32:00.520\n in back offices, et cetera.\n\n32:00.520 --> 32:04.840\n And those systems were developed by an engineering group\n\n32:04.840 --> 32:07.000\n that we were collaborating with at AT&T.\n\n32:07.000 --> 32:08.640\n And they were commercialized by NCR,\n\n32:08.640 --> 32:11.640\n which at the time was a subsidiary of AT&T.\n\n32:11.640 --> 32:14.880\n Now AT&T split up in 1996,\n\n32:17.000 --> 32:18.640\n early 1996.\n\n32:18.640 --> 32:20.440\n And the lawyers just looked at all the patents\n\n32:20.440 --> 32:23.000\n and they distributed the patents among the various companies.\n\n32:23.000 --> 32:26.440\n They gave the convolutional net patent to NCR\n\n32:26.440 --> 32:29.240\n because they were actually selling products that used it.\n\n32:29.240 --> 32:32.320\n But nobody at NCR had any idea what a convolutional net was.\n\n32:32.320 --> 32:33.240\n Yeah.\n\n32:33.240 --> 32:34.080\n Okay.\n\n32:34.080 --> 32:36.760\n So between 1996 and 2007,\n\n32:38.080 --> 32:39.880\n so there's a whole period until 2002\n\n32:39.880 --> 32:42.040\n where I didn't actually work on machine learning\n\n32:42.040 --> 32:42.880\n or convolutional net.\n\n32:42.880 --> 32:44.920\n I resumed working on this around 2002.\n\n32:45.920 --> 32:47.520\n And between 2002 and 2007,\n\n32:47.520 --> 32:49.560\n I was working on them, crossing my finger\n\n32:49.560 --> 32:51.240\n that nobody at NCR would notice.\n\n32:51.240 --> 32:52.080\n Nobody noticed.\n\n32:52.080 --> 32:55.640\n Yeah, and I hope that this kind of somewhat,\n\n32:55.640 --> 32:58.320\n as you said, lawyers aside,\n\n32:58.320 --> 33:02.920\n relative openness of the community now will continue.\n\n33:02.920 --> 33:05.960\n It accelerates the entire progress of the industry.\n\n33:05.960 --> 33:10.960\n And the problems that Facebook and Google\n\n33:11.600 --> 33:13.040\n and others are facing today\n\n33:13.040 --> 33:16.000\n is not whether Facebook or Google or Microsoft or IBM\n\n33:16.000 --> 33:18.080\n or whoever is ahead of the other.\n\n33:18.080 --> 33:19.680\n It's that we don't have the technology\n\n33:19.680 --> 33:21.080\n to build the things we want to build.\n\n33:21.080 --> 33:23.240\n We want to build intelligent virtual assistants\n\n33:23.240 --> 33:24.960\n that have common sense.\n\n33:24.960 --> 33:26.720\n We don't have monopoly on good ideas for this.\n\n33:26.720 --> 33:27.960\n We don't believe we do.\n\n33:27.960 --> 33:30.440\n Maybe others believe they do, but we don't.\n\n33:30.440 --> 33:31.320\n Okay.\n\n33:31.320 --> 33:33.840\n If a startup tells you they have the secret\n\n33:33.840 --> 33:36.880\n to human level intelligence and common sense,\n\n33:36.880 --> 33:38.240\n don't believe them, they don't.\n\n33:38.240 --> 33:42.760\n And it's gonna take the entire work\n\n33:42.760 --> 33:45.240\n of the world research community for a while\n\n33:45.240 --> 33:47.600\n to get to the point where you can go off\n\n33:47.600 --> 33:49.240\n and each of those companies\n\n33:49.240 --> 33:50.640\n kind of start to build things on this.\n\n33:50.640 --> 33:51.760\n We're not there yet.\n\n33:51.760 --> 33:54.680\n It's absolutely, and this calls to the gap\n\n33:54.680 --> 33:57.000\n between the space of ideas\n\n33:57.000 --> 34:00.440\n and the rigorous testing of those ideas\n\n34:00.440 --> 34:03.560\n of practical application that you often speak to.\n\n34:03.560 --> 34:06.320\n You've written advice saying don't get fooled\n\n34:06.320 --> 34:08.760\n by people who claim to have a solution\n\n34:08.760 --> 34:10.560\n to artificial general intelligence,\n\n34:10.560 --> 34:11.960\n who claim to have an AI system\n\n34:11.960 --> 34:14.280\n that works just like the human brain\n\n34:14.280 --> 34:17.080\n or who claim to have figured out how the brain works.\n\n34:17.080 --> 34:20.960\n Ask them what the error rate they get\n\n34:20.960 --> 34:23.120\n on MNIST or ImageNet.\n\n34:23.120 --> 34:25.400\n So this is a little dated by the way.\n\n34:25.400 --> 34:28.280\n 2000, I mean five years, who's counting?\n\n34:28.280 --> 34:30.920\n Okay, but I think your opinion is still,\n\n34:30.920 --> 34:34.920\n MNIST and ImageNet, yes, may be dated,\n\n34:34.920 --> 34:36.360\n there may be new benchmarks, right?\n\n34:36.360 --> 34:39.360\n But I think that philosophy is one you still\n\n34:39.360 --> 34:43.400\n in somewhat hold, that benchmarks\n\n34:43.400 --> 34:45.760\n and the practical testing, the practical application\n\n34:45.760 --> 34:48.000\n is where you really get to test the ideas.\n\n34:48.000 --> 34:49.840\n Well, it may not be completely practical.\n\n34:49.840 --> 34:52.480\n Like for example, it could be a toy data set,\n\n34:52.480 --> 34:54.880\n but it has to be some sort of task\n\n34:54.880 --> 34:57.320\n that the community as a whole has accepted\n\n34:57.320 --> 35:00.640\n as some sort of standard kind of benchmark if you want.\n\n35:00.640 --> 35:01.480\n It doesn't need to be real.\n\n35:01.480 --> 35:04.320\n So for example, many years ago here at FAIR,\n\n35:05.400 --> 35:07.080\n people, Jason West and Antoine Borne\n\n35:07.080 --> 35:09.080\n and a few others proposed the Babi tasks,\n\n35:09.080 --> 35:12.280\n which were kind of a toy problem to test\n\n35:12.280 --> 35:14.360\n the ability of machines to reason actually\n\n35:14.360 --> 35:16.960\n to access working memory and things like this.\n\n35:16.960 --> 35:20.120\n And it was very useful even though it wasn't a real task.\n\n35:20.120 --> 35:22.680\n MNIST is kind of halfway real task.\n\n35:23.680 --> 35:26.040\n So toy problems can be very useful.\n\n35:26.040 --> 35:29.000\n It's just that I was really struck by the fact\n\n35:29.000 --> 35:31.160\n that a lot of people, particularly a lot of people\n\n35:31.160 --> 35:34.380\n with money to invest would be fooled by people telling them,\n\n35:34.380 --> 35:37.400\n oh, we have the algorithm of the cortex\n\n35:37.400 --> 35:39.360\n and you should give us 50 million.\n\n35:39.360 --> 35:40.200\n Yes, absolutely.\n\n35:40.200 --> 35:45.200\n So there's a lot of people who try to take advantage\n\n35:45.280 --> 35:48.240\n of the hype for business reasons and so on.\n\n35:48.240 --> 35:50.800\n But let me sort of talk to this idea\n\n35:50.800 --> 35:55.320\n that sort of new ideas, the ideas that push the field\n\n35:55.320 --> 35:58.620\n forward may not yet have a benchmark\n\n35:58.620 --> 36:00.880\n or it may be very difficult to establish a benchmark.\n\n36:00.880 --> 36:01.720\n I agree.\n\n36:01.720 --> 36:02.560\n That's part of the process.\n\n36:02.560 --> 36:04.600\n Establishing benchmarks is part of the process.\n\n36:04.600 --> 36:07.300\n So what are your thoughts about,\n\n36:07.300 --> 36:10.960\n so we have these benchmarks on around stuff we can do\n\n36:10.960 --> 36:14.920\n with images from classification to captioning\n\n36:14.920 --> 36:16.940\n to just every kind of information you can pull off\n\n36:16.940 --> 36:18.880\n from images and the surface level.\n\n36:18.880 --> 36:21.440\n There's audio data sets, there's some video.\n\n36:22.600 --> 36:27.480\n What can we start, natural language, what kind of stuff,\n\n36:27.480 --> 36:30.160\n what kind of benchmarks do you see that start creeping\n\n36:30.160 --> 36:34.840\n on to more something like intelligence, like reasoning,\n\n36:34.840 --> 36:37.440\n like maybe you don't like the term,\n\n36:37.440 --> 36:41.520\n but AGI echoes of that kind of formulation.\n\n36:41.520 --> 36:44.160\n A lot of people are working on interactive environments\n\n36:44.160 --> 36:48.120\n in which you can train and test intelligence systems.\n\n36:48.120 --> 36:53.120\n So there, for example, it's the classical paradigm\n\n36:54.840 --> 36:57.960\n of supervised learning is that you have a data set,\n\n36:57.960 --> 37:00.040\n you partition it into a training set, validation set,\n\n37:00.040 --> 37:03.040\n test set, and there's a clear protocol, right?\n\n37:03.040 --> 37:06.400\n But what if that assumes that the samples\n\n37:06.400 --> 37:10.100\n are statistically independent, you can exchange them,\n\n37:10.100 --> 37:12.240\n the order in which you see them shouldn't matter,\n\n37:12.240 --> 37:13.480\n things like that.\n\n37:13.480 --> 37:16.020\n But what if the answer you give determines\n\n37:16.020 --> 37:18.760\n the next sample you see, which is the case, for example,\n\n37:18.760 --> 37:19.600\n in robotics, right?\n\n37:19.600 --> 37:22.480\n You robot does something and then it gets exposed\n\n37:22.480 --> 37:25.120\n to a new room, and depending on where it goes,\n\n37:25.120 --> 37:26.000\n the room would be different.\n\n37:26.000 --> 37:28.440\n So that creates the exploration problem.\n\n37:30.120 --> 37:34.280\n The what if the samples, so that creates also a dependency\n\n37:34.280 --> 37:35.480\n between samples, right?\n\n37:35.480 --> 37:39.640\n You, if you move, if you can only move in space,\n\n37:39.640 --> 37:41.840\n the next sample you're gonna see is gonna be probably\n\n37:41.840 --> 37:44.080\n in the same building, most likely, right?\n\n37:44.080 --> 37:47.920\n So all the assumptions about the validity\n\n37:47.920 --> 37:51.560\n of this training set, test set hypothesis break.\n\n37:51.560 --> 37:53.120\n Whenever a machine can take an action\n\n37:53.120 --> 37:54.960\n that has an influence in the world,\n\n37:54.960 --> 37:56.400\n and it's what it's gonna see.\n\n37:56.400 --> 38:00.160\n So people are setting up artificial environments\n\n38:00.160 --> 38:02.080\n where that takes place, right?\n\n38:02.080 --> 38:05.840\n The robot runs around a 3D model of a house\n\n38:05.840 --> 38:08.680\n and can interact with objects and things like this.\n\n38:08.680 --> 38:10.380\n So you do robotics based simulation,\n\n38:10.380 --> 38:14.400\n you have those opening a gym type thing\n\n38:14.400 --> 38:18.800\n or Mujoko kind of simulated robots\n\n38:18.800 --> 38:21.280\n and you have games, things like that.\n\n38:21.280 --> 38:23.640\n So that's where the field is going really,\n\n38:23.640 --> 38:24.840\n this kind of environment.\n\n38:25.760 --> 38:28.600\n Now, back to the question of AGI.\n\n38:28.600 --> 38:33.180\n I don't like the term AGI because it implies\n\n38:33.180 --> 38:35.760\n that human intelligence is general\n\n38:35.760 --> 38:38.360\n and human intelligence is nothing like general.\n\n38:38.360 --> 38:40.840\n It's very, very specialized.\n\n38:40.840 --> 38:41.720\n We think it's general.\n\n38:41.720 --> 38:42.760\n We'd like to think of ourselves\n\n38:42.760 --> 38:43.840\n as having general intelligence.\n\n38:43.840 --> 38:46.120\n We don't, we're very specialized.\n\n38:46.120 --> 38:47.560\n We're only slightly more general than.\n\n38:47.560 --> 38:48.900\n Why does it feel general?\n\n38:48.900 --> 38:52.040\n So you kind of, the term general.\n\n38:52.040 --> 38:56.320\n I think what's impressive about humans is ability to learn,\n\n38:56.320 --> 38:58.240\n as we were talking about learning,\n\n38:58.240 --> 39:01.280\n to learn in just so many different domains.\n\n39:01.280 --> 39:04.440\n It's perhaps not arbitrarily general,\n\n39:04.440 --> 39:06.440\n but just you can learn in many domains\n\n39:06.440 --> 39:08.240\n and integrate that knowledge somehow.\n\n39:08.240 --> 39:09.080\n Okay.\n\n39:09.080 --> 39:09.920\n The knowledge persists.\n\n39:09.920 --> 39:11.640\n So let me take a very specific example.\n\n39:11.640 --> 39:12.480\n Yes.\n\n39:12.480 --> 39:13.300\n It's not an example.\n\n39:13.300 --> 39:17.080\n It's more like a quasi mathematical demonstration.\n\n39:17.080 --> 39:18.520\n So you have about 1 million fibers\n\n39:18.520 --> 39:20.420\n coming out of one of your eyes.\n\n39:20.420 --> 39:21.320\n Okay, 2 million total,\n\n39:21.320 --> 39:23.440\n but let's talk about just one of them.\n\n39:23.440 --> 39:26.060\n It's 1 million nerve fibers, your optical nerve.\n\n39:27.160 --> 39:28.800\n Let's imagine that they are binary.\n\n39:28.800 --> 39:30.640\n So they can be active or inactive, right?\n\n39:30.640 --> 39:34.060\n So the input to your visual cortex is 1 million bits.\n\n39:34.060 --> 39:36.900\n Mm hmm.\n\n39:36.900 --> 39:39.420\n Now they're connected to your brain in a particular way,\n\n39:39.420 --> 39:41.940\n and your brain has connections\n\n39:41.940 --> 39:44.180\n that are kind of a little bit like a convolutional net,\n\n39:44.180 --> 39:46.780\n they're kind of local, you know, in space\n\n39:46.780 --> 39:47.940\n and things like this.\n\n39:47.940 --> 39:49.740\n Now, imagine I play a trick on you.\n\n39:50.980 --> 39:53.060\n It's a pretty nasty trick, I admit.\n\n39:53.060 --> 39:55.720\n I cut your optical nerve,\n\n39:55.720 --> 39:58.500\n and I put a device that makes a random perturbation\n\n39:58.500 --> 40:01.100\n of a permutation of all the nerve fibers.\n\n40:01.100 --> 40:04.580\n So now what comes to your brain\n\n40:04.580 --> 40:07.840\n is a fixed but random permutation of all the pixels.\n\n40:09.160 --> 40:11.380\n There's no way in hell that your visual cortex,\n\n40:11.380 --> 40:14.760\n even if I do this to you in infancy,\n\n40:14.760 --> 40:16.500\n will actually learn vision\n\n40:16.500 --> 40:20.060\n to the same level of quality that you can.\n\n40:20.060 --> 40:22.700\n Got it, and you're saying there's no way you've learned that?\n\n40:22.700 --> 40:25.620\n No, because now two pixels that are nearby in the world\n\n40:25.620 --> 40:29.240\n will end up in very different places in your visual cortex,\n\n40:29.240 --> 40:31.620\n and your neurons there have no connections with each other\n\n40:31.620 --> 40:33.500\n because they're only connected locally.\n\n40:33.500 --> 40:36.660\n So this whole, our entire, the hardware is built\n\n40:36.660 --> 40:38.620\n in many ways to support?\n\n40:38.620 --> 40:40.180\n The locality of the real world.\n\n40:40.180 --> 40:42.580\n Yes, that's specialization.\n\n40:42.580 --> 40:44.580\n Yeah, but it's still pretty damn impressive,\n\n40:44.580 --> 40:46.980\n so it's not perfect generalization, it's not even close.\n\n40:46.980 --> 40:50.960\n No, no, it's not that it's not even close, it's not at all.\n\n40:50.960 --> 40:52.220\n Yeah, it's not, it's specialized, yeah.\n\n40:52.220 --> 40:54.020\n So how many Boolean functions?\n\n40:54.020 --> 40:58.260\n So let's imagine you want to train your visual system\n\n40:58.260 --> 41:03.260\n to recognize particular patterns of those one million bits.\n\n41:03.820 --> 41:05.780\n Okay, so that's a Boolean function, right?\n\n41:05.780 --> 41:07.020\n Either the pattern is here or not here,\n\n41:07.020 --> 41:09.200\n this is a two way classification\n\n41:09.200 --> 41:11.680\n with one million binary inputs.\n\n41:13.620 --> 41:16.260\n How many such Boolean functions are there?\n\n41:16.260 --> 41:19.020\n Okay, you have two to the one million\n\n41:19.940 --> 41:21.180\n combinations of inputs,\n\n41:21.180 --> 41:24.060\n for each of those you have an output bit,\n\n41:24.060 --> 41:27.660\n and so you have two to the one million\n\n41:27.660 --> 41:30.060\n Boolean functions of this type, okay?\n\n41:30.060 --> 41:33.020\n Which is an unimaginably large number.\n\n41:33.020 --> 41:35.560\n How many of those functions can actually be computed\n\n41:35.560 --> 41:37.260\n by your visual cortex?\n\n41:37.260 --> 41:41.460\n And the answer is a tiny, tiny, tiny, tiny, tiny, tiny sliver.\n\n41:41.460 --> 41:43.500\n Like an enormously tiny sliver.\n\n41:43.500 --> 41:44.980\n Yeah, yeah.\n\n41:44.980 --> 41:47.300\n So we are ridiculously specialized.\n\n41:48.860 --> 41:49.700\n Okay.\n\n41:49.700 --> 41:54.220\n But, okay, that's an argument against the word general.\n\n41:54.220 --> 41:59.180\n I think there's a, I agree with your intuition,\n\n41:59.180 --> 42:04.180\n but I'm not sure it's, it seems the brain is impressively\n\n42:06.900 --> 42:09.660\n capable of adjusting to things, so.\n\n42:09.660 --> 42:13.420\n It's because we can't imagine tasks\n\n42:13.420 --> 42:16.340\n that are outside of our comprehension, right?\n\n42:16.340 --> 42:18.780\n So we think we're general because we're general\n\n42:18.780 --> 42:20.780\n of all the things that we can apprehend.\n\n42:20.780 --> 42:23.020\n But there is a huge world out there\n\n42:23.020 --> 42:24.740\n of things that we have no idea.\n\n42:24.740 --> 42:26.860\n We call that heat, by the way.\n\n42:26.860 --> 42:27.700\n Heat.\n\n42:27.700 --> 42:28.540\n Heat.\n\n42:28.540 --> 42:30.660\n So, at least physicists call that heat,\n\n42:30.660 --> 42:33.420\n or they call it entropy, which is kind of.\n\n42:33.420 --> 42:38.420\n You have a thing full of gas, right?\n\n42:39.380 --> 42:40.760\n Closed system for gas.\n\n42:40.760 --> 42:41.780\n Right?\n\n42:41.780 --> 42:42.660\n Closed or not closed.\n\n42:42.660 --> 42:47.660\n It has pressure, it has temperature, it has, you know,\n\n42:47.660 --> 42:50.660\n and you can write equations, PV equal N on T,\n\n42:50.660 --> 42:52.540\n you know, things like that, right?\n\n42:52.540 --> 42:54.900\n When you reduce the volume, the temperature goes up,\n\n42:54.900 --> 42:57.780\n the pressure goes up, you know, things like that, right?\n\n42:57.780 --> 42:59.620\n For perfect gas, at least.\n\n42:59.620 --> 43:02.420\n Those are the things you can know about that system.\n\n43:02.420 --> 43:04.580\n And it's a tiny, tiny number of bits\n\n43:04.580 --> 43:06.900\n compared to the complete information\n\n43:06.900 --> 43:08.340\n of the state of the entire system.\n\n43:08.340 --> 43:09.740\n Because the state of the entire system\n\n43:09.740 --> 43:11.260\n will give you the position of momentum\n\n43:11.260 --> 43:14.660\n of every molecule of the gas.\n\n43:14.660 --> 43:17.660\n And what you don't know about it is the entropy,\n\n43:17.660 --> 43:20.620\n and you interpret it as heat.\n\n43:20.620 --> 43:24.700\n The energy contained in that thing is what we call heat.\n\n43:24.700 --> 43:28.740\n Now, it's very possible that, in fact,\n\n43:28.740 --> 43:30.220\n there is some very strong structure\n\n43:30.220 --> 43:31.620\n in how those molecules are moving.\n\n43:31.620 --> 43:33.020\n It's just that they are in a way\n\n43:33.020 --> 43:35.580\n that we are just not wired to perceive.\n\n43:35.580 --> 43:36.420\n Yeah, we're ignorant to it.\n\n43:36.420 --> 43:40.500\n And there's, in your infinite amount of things,\n\n43:40.500 --> 43:41.820\n we're not wired to perceive.\n\n43:41.820 --> 43:44.660\n And you're right, that's a nice way to put it.\n\n43:44.660 --> 43:47.620\n We're general to all the things we can imagine,\n\n43:47.620 --> 43:51.820\n which is a very tiny subset of all things that are possible.\n\n43:51.820 --> 43:53.260\n So it's like comograph complexity\n\n43:53.260 --> 43:55.820\n or the comograph chitin sum of complexity.\n\n43:55.820 --> 43:56.660\n Yeah.\n\n43:56.660 --> 44:01.660\n You know, every bit string or every integer is random,\n\n44:02.220 --> 44:05.220\n except for all the ones that you can actually write down.\n\n44:05.220 --> 44:06.060\n Yeah.\n\n44:06.060 --> 44:06.900\n Yeah.\n\n44:06.900 --> 44:07.740\n Yeah.\n\n44:07.740 --> 44:08.580\n Yeah.\n\n44:08.580 --> 44:09.420\n Yeah.\n\n44:09.420 --> 44:10.260\n Yeah.\n\n44:10.260 --> 44:12.180\n Yeah, okay.\n\n44:12.180 --> 44:13.020\n So beautifully put.\n\n44:13.020 --> 44:15.460\n But, you know, so we can just call it artificial intelligence.\n\n44:15.460 --> 44:17.100\n We don't need to have a general.\n\n44:17.980 --> 44:18.820\n Or human level.\n\n44:18.820 --> 44:20.900\n Human level intelligence is good.\n\n44:20.900 --> 44:24.700\n You know, you'll start, anytime you touch human,\n\n44:24.700 --> 44:28.700\n it gets interesting because, you know,\n\n44:30.660 --> 44:33.420\n it's because we attach ourselves to human\n\n44:33.420 --> 44:36.060\n and it's difficult to define what human intelligence is.\n\n44:36.060 --> 44:37.220\n Yeah.\n\n44:37.220 --> 44:42.100\n Nevertheless, my definition is maybe dem impressive\n\n44:42.100 --> 44:43.900\n intelligence, okay?\n\n44:43.900 --> 44:46.700\n Dem impressive demonstration of intelligence, whatever.\n\n44:46.700 --> 44:51.420\n And so on that topic, most successes in deep learning\n\n44:51.420 --> 44:53.700\n have been in supervised learning.\n\n44:53.700 --> 44:57.860\n What is your view on unsupervised learning?\n\n44:57.860 --> 45:02.860\n Is there a hope to reduce involvement of human input\n\n45:03.180 --> 45:05.620\n and still have successful systems\n\n45:05.620 --> 45:08.300\n that have practical use?\n\n45:08.300 --> 45:09.900\n Yeah, I mean, there's definitely a hope.\n\n45:09.900 --> 45:11.180\n It's more than a hope, actually.\n\n45:11.180 --> 45:13.900\n It's mounting evidence for it.\n\n45:13.900 --> 45:16.020\n And that's basically all I do.\n\n45:16.020 --> 45:19.100\n Like, the only thing I'm interested in at the moment is,\n\n45:19.100 --> 45:21.260\n I call it self supervised learning, not unsupervised.\n\n45:21.260 --> 45:24.020\n Because unsupervised learning is a loaded term.\n\n45:25.700 --> 45:27.900\n People who know something about machine learning,\n\n45:27.900 --> 45:30.620\n you know, tell you, so you're doing clustering or PCA,\n\n45:30.620 --> 45:31.580\n which is not the case.\n\n45:31.580 --> 45:32.580\n And the white public, you know,\n\n45:32.580 --> 45:33.620\n when you say unsupervised learning,\n\n45:33.620 --> 45:35.860\n oh my God, machines are gonna learn by themselves\n\n45:35.860 --> 45:37.300\n without supervision.\n\n45:37.300 --> 45:39.660\n You know, they see this as...\n\n45:39.660 --> 45:40.780\n Where's the parents?\n\n45:40.780 --> 45:42.900\n Yeah, so I call it self supervised learning\n\n45:42.900 --> 45:46.140\n because, in fact, the underlying algorithms that are used\n\n45:46.140 --> 45:48.340\n are the same algorithms as the supervised learning\n\n45:48.340 --> 45:52.300\n algorithms, except that what we train them to do\n\n45:52.300 --> 45:55.540\n is not predict a particular set of variables,\n\n45:55.540 --> 45:58.580\n like the category of an image,\n\n46:00.420 --> 46:02.540\n and not to predict a set of variables\n\n46:02.540 --> 46:06.380\n that have been provided by human labelers.\n\n46:06.380 --> 46:07.380\n But what you're trying the machine to do\n\n46:07.380 --> 46:10.300\n is basically reconstruct a piece of its input\n\n46:10.300 --> 46:14.140\n that is being maxed out, essentially.\n\n46:14.140 --> 46:15.620\n You can think of it this way, right?\n\n46:15.620 --> 46:18.780\n So show a piece of video to a machine\n\n46:18.780 --> 46:20.940\n and ask it to predict what's gonna happen next.\n\n46:20.940 --> 46:23.780\n And of course, after a while, you can show what happens\n\n46:23.780 --> 46:26.220\n and the machine will kind of train itself\n\n46:26.220 --> 46:27.540\n to do better at that task.\n\n46:28.820 --> 46:32.220\n You can do like all the latest, most successful models\n\n46:32.220 --> 46:33.260\n in natural language processing,\n\n46:33.260 --> 46:34.780\n use self supervised learning.\n\n46:36.220 --> 46:38.660\n You know, sort of BERT style systems, for example, right?\n\n46:38.660 --> 46:43.500\n You show it a window of a dozen words on a text corpus,\n\n46:43.500 --> 46:46.300\n you take out 15% of the words,\n\n46:46.300 --> 46:49.900\n and then you train the machine to predict the words\n\n46:49.900 --> 46:52.820\n that are missing, that self supervised learning.\n\n46:52.820 --> 46:53.980\n It's not predicting the future,\n\n46:53.980 --> 46:56.260\n it's just predicting things in the middle,\n\n46:56.260 --> 46:57.860\n but you could have it predict the future,\n\n46:57.860 --> 46:59.500\n that's what language models do.\n\n46:59.500 --> 47:01.780\n So you construct, so in an unsupervised way,\n\n47:01.780 --> 47:03.980\n you construct a model of language.\n\n47:03.980 --> 47:05.060\n Do you think...\n\n47:05.060 --> 47:09.140\n Or video or the physical world or whatever, right?\n\n47:09.140 --> 47:12.620\n How far do you think that can take us?\n\n47:12.620 --> 47:16.420\n Do you think BERT understands anything?\n\n47:18.020 --> 47:21.980\n To some level, it has a shallow understanding of text,\n\n47:23.460 --> 47:24.740\n but it needs to, I mean,\n\n47:24.740 --> 47:26.820\n to have kind of true human level intelligence,\n\n47:26.820 --> 47:29.220\n I think you need to ground language in reality.\n\n47:29.220 --> 47:32.780\n So some people are attempting to do this, right?\n\n47:32.780 --> 47:35.460\n Having systems that kind of have some visual representation\n\n47:35.460 --> 47:37.420\n of what is being talked about,\n\n47:37.420 --> 47:38.580\n which is one reason you need\n\n47:38.580 --> 47:41.060\n those interactive environments actually.\n\n47:41.060 --> 47:43.300\n But this is like a huge technical problem\n\n47:43.300 --> 47:45.060\n that is not solved,\n\n47:45.060 --> 47:47.900\n and that explains why self supervised learning\n\n47:47.900 --> 47:49.980\n works in the context of natural language,\n\n47:49.980 --> 47:52.740\n but does not work in the context, or at least not well,\n\n47:52.740 --> 47:55.380\n in the context of image recognition and video,\n\n47:55.380 --> 47:57.820\n although it's making progress quickly.\n\n47:57.820 --> 48:00.660\n And the reason, that reason is the fact that\n\n48:01.820 --> 48:05.300\n it's much easier to represent uncertainty in the prediction\n\n48:05.300 --> 48:06.900\n in a context of natural language\n\n48:06.900 --> 48:10.100\n than it is in the context of things like video and images.\n\n48:10.100 --> 48:12.940\n So for example, if I ask you to predict\n\n48:12.940 --> 48:14.140\n what words are missing,\n\n48:14.140 --> 48:16.220\n 15% of the words that I've taken out.\n\n48:17.700 --> 48:19.140\n The possibilities are small.\n\n48:19.140 --> 48:20.020\n That means... It's small, right?\n\n48:20.020 --> 48:23.340\n There is 100,000 words in the lexicon,\n\n48:23.340 --> 48:24.820\n and what the machine spits out\n\n48:24.820 --> 48:27.620\n is a big probability vector, right?\n\n48:27.620 --> 48:29.660\n It's a bunch of numbers between zero and one\n\n48:29.660 --> 48:30.740\n that sum to one.\n\n48:30.740 --> 48:33.140\n And we know how to do this with computers.\n\n48:34.460 --> 48:36.940\n So there, representing uncertainty in the prediction\n\n48:36.940 --> 48:39.100\n is relatively easy, and that's, in my opinion,\n\n48:39.100 --> 48:42.460\n why those techniques work for NLP.\n\n48:42.460 --> 48:45.460\n For images, if you ask...\n\n48:45.460 --> 48:46.900\n If you block a piece of an image,\n\n48:46.900 --> 48:47.740\n and you ask the system,\n\n48:47.740 --> 48:49.180\n reconstruct that piece of the image,\n\n48:49.180 --> 48:51.540\n there are many possible answers.\n\n48:51.540 --> 48:54.620\n They are all perfectly legit, right?\n\n48:54.620 --> 48:58.740\n And how do you represent this set of possible answers?\n\n48:58.740 --> 49:00.900\n You can't train a system to make one prediction.\n\n49:00.900 --> 49:02.500\n You can't train a neural net to say,\n\n49:02.500 --> 49:04.620\n here it is, that's the image,\n\n49:04.620 --> 49:06.420\n because there's a whole set of things\n\n49:06.420 --> 49:07.260\n that are compatible with it.\n\n49:07.260 --> 49:08.740\n So how do you get the machine to represent\n\n49:08.740 --> 49:11.140\n not a single output, but a whole set of outputs?\n\n49:13.060 --> 49:17.220\n And similarly with video prediction,\n\n49:17.220 --> 49:19.220\n there's a lot of things that can happen\n\n49:19.220 --> 49:20.100\n in the future of video.\n\n49:20.100 --> 49:21.140\n You're looking at me right now.\n\n49:21.140 --> 49:22.740\n I'm not moving my head very much,\n\n49:22.740 --> 49:26.940\n but I might turn my head to the left or to the right.\n\n49:26.940 --> 49:29.300\n If you don't have a system that can predict this,\n\n49:30.420 --> 49:31.740\n and you train it with least square\n\n49:31.740 --> 49:33.700\n to minimize the error with the prediction\n\n49:33.700 --> 49:34.660\n and what I'm doing,\n\n49:34.660 --> 49:36.940\n what you get is a blurry image of myself\n\n49:36.940 --> 49:39.660\n in all possible future positions that I might be in,\n\n49:39.660 --> 49:41.780\n which is not a good prediction.\n\n49:41.780 --> 49:43.420\n So there might be other ways\n\n49:43.420 --> 49:48.100\n to do the self supervision for visual scenes.\n\n49:48.100 --> 49:48.940\n Like what?\n\n49:48.940 --> 49:52.740\n I mean, if I knew, I wouldn't tell you,\n\n49:52.740 --> 49:54.300\n publish it first, I don't know.\n\n49:55.620 --> 49:56.620\n No, there might be.\n\n49:57.540 --> 49:59.340\n So I mean, these are kind of,\n\n50:00.300 --> 50:03.260\n there might be artificial ways of like self play in games,\n\n50:03.260 --> 50:05.780\n the way you can simulate part of the environment.\n\n50:05.780 --> 50:06.820\n Oh, that doesn't solve the problem.\n\n50:06.820 --> 50:08.620\n It's just a way of generating data.\n\n50:10.420 --> 50:12.580\n But because you have more of a control,\n\n50:12.580 --> 50:14.620\n like maybe you can control,\n\n50:14.620 --> 50:16.100\n yeah, it's a way to generate data.\n\n50:16.100 --> 50:16.940\n That's right.\n\n50:16.940 --> 50:20.500\n And because you can do huge amounts of data generation,\n\n50:20.500 --> 50:21.580\n that doesn't, you're right.\n\n50:21.580 --> 50:26.020\n Well, it creeps up on the problem from the side of data,\n\n50:26.020 --> 50:27.700\n and you don't think that's the right way to creep up.\n\n50:27.700 --> 50:28.980\n It doesn't solve this problem\n\n50:28.980 --> 50:30.980\n of handling uncertainty in the world, right?\n\n50:30.980 --> 50:35.260\n So if you have a machine learn a predictive model\n\n50:35.260 --> 50:38.180\n of the world in a game that is deterministic\n\n50:38.180 --> 50:42.540\n or quasi deterministic, it's easy, right?\n\n50:42.540 --> 50:45.940\n Just give a few frames of the game to a ConvNet,\n\n50:45.940 --> 50:47.060\n put a bunch of layers,\n\n50:47.060 --> 50:49.660\n and then have the game generates the next few frames.\n\n50:49.660 --> 50:52.380\n And if the game is deterministic, it works fine.\n\n50:54.860 --> 50:59.140\n And that includes feeding the system with the action\n\n50:59.140 --> 51:01.740\n that your little character is gonna take.\n\n51:03.060 --> 51:06.660\n The problem comes from the fact that the real world\n\n51:06.660 --> 51:09.700\n and most games are not entirely predictable.\n\n51:09.700 --> 51:11.340\n And so there you get those blurry predictions\n\n51:11.340 --> 51:14.500\n and you can't do planning with blurry predictions, right?\n\n51:14.500 --> 51:17.460\n So if you have a perfect model of the world,\n\n51:17.460 --> 51:20.740\n you can, in your head, run this model\n\n51:20.740 --> 51:24.100\n with a hypothesis for a sequence of actions,\n\n51:24.100 --> 51:25.380\n and you're going to predict the outcome\n\n51:25.380 --> 51:26.820\n of that sequence of actions.\n\n51:28.620 --> 51:32.460\n But if your model is imperfect, how can you plan?\n\n51:32.460 --> 51:33.940\n Yeah, it quickly explodes.\n\n51:34.820 --> 51:37.300\n What are your thoughts on the extension of this,\n\n51:37.300 --> 51:39.700\n which topic I'm super excited about,\n\n51:39.700 --> 51:41.380\n it's connected to something you were talking about\n\n51:41.380 --> 51:44.580\n in terms of robotics, is active learning.\n\n51:44.580 --> 51:47.940\n So as opposed to sort of completely unsupervised\n\n51:47.940 --> 51:49.740\n or self supervised learning,\n\n51:51.060 --> 51:53.780\n you ask the system for human help\n\n51:54.900 --> 51:58.100\n for selecting parts you want annotated next.\n\n51:58.100 --> 52:00.660\n So if you think about a robot exploring a space\n\n52:00.660 --> 52:02.420\n or a baby exploring a space\n\n52:02.420 --> 52:05.260\n or a system exploring a data set,\n\n52:05.260 --> 52:07.940\n every once in a while asking for human input,\n\n52:07.940 --> 52:12.180\n do you see value in that kind of work?\n\n52:12.180 --> 52:14.180\n I don't see transformative value.\n\n52:14.180 --> 52:16.780\n It's going to make things that we can already do\n\n52:18.180 --> 52:20.780\n more efficient or they will learn slightly more efficiently,\n\n52:20.780 --> 52:21.940\n but it's not going to make machines\n\n52:21.940 --> 52:23.700\n sort of significantly more intelligent.\n\n52:23.700 --> 52:28.700\n I think, and by the way, there is no opposition,\n\n52:29.340 --> 52:34.340\n there's no conflict between self supervised learning,\n\n52:34.620 --> 52:35.980\n reinforcement learning and supervised learning\n\n52:35.980 --> 52:38.180\n or imitation learning or active learning.\n\n52:39.060 --> 52:40.500\n I see self supervised learning\n\n52:40.500 --> 52:43.820\n as a preliminary to all of the above.\n\n52:43.820 --> 52:44.660\n Yes.\n\n52:44.660 --> 52:49.660\n So the example I use very often is how is it that,\n\n52:50.420 --> 52:54.580\n so if you use classical reinforcement learning,\n\n52:54.580 --> 52:57.540\n deep reinforcement learning, if you want,\n\n52:57.540 --> 52:59.300\n the best methods today,\n\n53:01.300 --> 53:03.100\n so called model free reinforcement learning\n\n53:03.100 --> 53:04.660\n to learn to play Atari games,\n\n53:04.660 --> 53:07.100\n take about 80 hours of training to reach the level\n\n53:07.100 --> 53:09.300\n that any human can reach in about 15 minutes.\n\n53:11.540 --> 53:14.340\n They get better than humans, but it takes them a long time.\n\n53:16.540 --> 53:20.420\n Alpha star, okay, the, you know,\n\n53:20.420 --> 53:22.260\n Aureal Vinyals and his teams,\n\n53:22.260 --> 53:27.260\n the system to play StarCraft plays,\n\n53:27.900 --> 53:32.900\n you know, a single map, a single type of player.\n\n53:32.900 --> 53:37.900\n A single player and can reach better than human level\n\n53:38.820 --> 53:43.380\n with about the equivalent of 200 years of training\n\n53:43.380 --> 53:45.300\n playing against itself.\n\n53:45.300 --> 53:46.420\n It's 200 years, right?\n\n53:46.420 --> 53:50.100\n It's not something that no human can ever do.\n\n53:50.100 --> 53:52.340\n I mean, I'm not sure what lesson to take away from that.\n\n53:52.340 --> 53:54.820\n Okay, now take those algorithms,\n\n53:54.820 --> 53:57.380\n the best algorithms we have today\n\n53:57.380 --> 54:00.200\n to train a car to drive itself.\n\n54:00.200 --> 54:02.960\n It would probably have to drive millions of hours.\n\n54:02.960 --> 54:04.680\n It will have to kill thousands of pedestrians.\n\n54:04.680 --> 54:06.480\n It will have to run into thousands of trees.\n\n54:06.480 --> 54:08.520\n It will have to run off cliffs.\n\n54:08.520 --> 54:10.560\n And it had to run off cliff multiple times\n\n54:10.560 --> 54:14.040\n before it figures out that it's a bad idea, first of all.\n\n54:14.040 --> 54:17.520\n And second of all, before it figures out how not to do it.\n\n54:17.520 --> 54:19.840\n And so, I mean, this type of learning obviously\n\n54:19.840 --> 54:21.360\n does not reflect the kind of learning\n\n54:21.360 --> 54:23.200\n that animals and humans do.\n\n54:23.200 --> 54:24.240\n There is something missing\n\n54:24.240 --> 54:26.320\n that's really, really important there.\n\n54:26.320 --> 54:28.600\n And my hypothesis, which I've been advocating\n\n54:28.600 --> 54:30.400\n for like five years now,\n\n54:30.400 --> 54:33.680\n is that we have predictive models of the world\n\n54:34.840 --> 54:38.520\n that include the ability to predict under uncertainty.\n\n54:38.520 --> 54:43.520\n And what allows us to not run off a cliff\n\n54:43.520 --> 54:44.720\n when we learn to drive,\n\n54:44.720 --> 54:47.040\n most of us can learn to drive in about 20 or 30 hours\n\n54:47.040 --> 54:50.960\n of training without ever crashing, causing any accident.\n\n54:50.960 --> 54:53.280\n And if we drive next to a cliff,\n\n54:53.280 --> 54:55.240\n we know that if we turn the wheel to the right,\n\n54:55.240 --> 54:57.080\n the car is gonna run off the cliff\n\n54:57.080 --> 54:58.760\n and nothing good is gonna come out of this.\n\n54:58.760 --> 55:00.600\n Because we have a pretty good model of intuitive physics\n\n55:00.600 --> 55:02.280\n that tells us the car is gonna fall.\n\n55:02.280 --> 55:04.200\n We know about gravity.\n\n55:04.200 --> 55:07.120\n Babies learn this around the age of eight or nine months\n\n55:07.120 --> 55:09.840\n that objects don't float, they fall.\n\n55:11.200 --> 55:13.720\n And we have a pretty good idea of the effect\n\n55:13.720 --> 55:15.040\n of turning the wheel on the car\n\n55:15.040 --> 55:16.960\n and we know we need to stay on the road.\n\n55:16.960 --> 55:19.480\n So there's a lot of things that we bring to the table,\n\n55:19.480 --> 55:22.400\n which is basically our predictive model of the world.\n\n55:22.400 --> 55:25.840\n And that model allows us to not do stupid things.\n\n55:25.840 --> 55:28.160\n And to basically stay within the context\n\n55:28.160 --> 55:29.960\n of things we need to do.\n\n55:29.960 --> 55:32.520\n We still face unpredictable situations\n\n55:32.520 --> 55:34.040\n and that's how we learn.\n\n55:34.040 --> 55:37.600\n But that allows us to learn really, really, really quickly.\n\n55:37.600 --> 55:40.200\n So that's called model based reinforcement learning.\n\n55:41.200 --> 55:43.000\n There's some imitation and supervised learning\n\n55:43.000 --> 55:44.840\n because we have a driving instructor\n\n55:44.840 --> 55:47.000\n that tells us occasionally what to do.\n\n55:47.000 --> 55:52.000\n But most of the learning is learning the model,\n\n55:52.080 --> 55:55.080\n learning physics that we've done since we were babies.\n\n55:55.080 --> 55:56.880\n That's where all, almost all the learning.\n\n55:56.880 --> 56:00.080\n And the physics is somewhat transferable from,\n\n56:00.080 --> 56:01.960\n it's transferable from scene to scene.\n\n56:01.960 --> 56:04.320\n Stupid things are the same everywhere.\n\n56:04.320 --> 56:07.720\n Yeah, I mean, if you have experience of the world,\n\n56:07.720 --> 56:11.400\n you don't need to be from a particularly intelligent species\n\n56:11.400 --> 56:14.880\n to know that if you spill water from a container,\n\n56:16.520 --> 56:18.800\n the rest is gonna get wet.\n\n56:18.800 --> 56:19.640\n You might get wet.\n\n56:20.640 --> 56:22.840\n So cats know this, right?\n\n56:22.840 --> 56:23.680\n Yeah.\n\n56:23.680 --> 56:27.040\n Right, so the main problem we need to solve\n\n56:27.040 --> 56:29.920\n is how do we learn models of the world?\n\n56:29.920 --> 56:31.280\n That's what I'm interested in.\n\n56:31.280 --> 56:34.080\n That's what self supervised learning is all about.\n\n56:34.080 --> 56:37.360\n If you were to try to construct a benchmark for,\n\n56:39.400 --> 56:41.120\n let's look at MNIST.\n\n56:41.120 --> 56:42.280\n I love that data set.\n\n56:44.120 --> 56:48.040\n Do you think it's useful, interesting, slash possible\n\n56:48.040 --> 56:52.320\n to perform well on MNIST with just one example\n\n56:52.320 --> 56:57.320\n of each digit and how would we solve that problem?\n\n56:58.640 --> 56:59.560\n The answer is probably yes.\n\n56:59.560 --> 57:02.400\n The question is what other type of learning\n\n57:02.400 --> 57:03.240\n are you allowed to do?\n\n57:03.240 --> 57:04.800\n So if what you're allowed to do is train\n\n57:04.800 --> 57:07.360\n on some gigantic data set of labeled digit,\n\n57:07.360 --> 57:08.840\n that's called transfer learning.\n\n57:08.840 --> 57:10.600\n And we know that works, okay?\n\n57:11.680 --> 57:13.560\n We do this at Facebook, like in production, right?\n\n57:13.560 --> 57:17.040\n We train large convolutional nets to predict hashtags\n\n57:17.040 --> 57:18.200\n that people type on Instagram\n\n57:18.200 --> 57:20.960\n and we train on billions of images, literally billions.\n\n57:20.960 --> 57:22.920\n And then we chop off the last layer\n\n57:22.920 --> 57:24.920\n and fine tune on whatever task we want.\n\n57:24.920 --> 57:26.360\n That works really well.\n\n57:26.360 --> 57:28.760\n You can beat the ImageNet record with this.\n\n57:28.760 --> 57:30.520\n We actually open sourced the whole thing\n\n57:30.520 --> 57:31.800\n like a few weeks ago.\n\n57:31.800 --> 57:33.320\n Yeah, that's still pretty cool.\n\n57:33.320 --> 57:36.800\n But yeah, so what would be impressive?\n\n57:36.800 --> 57:38.160\n What's useful and impressive?\n\n57:38.160 --> 57:39.280\n What kind of transfer learning\n\n57:39.280 --> 57:40.320\n would be useful and impressive?\n\n57:40.320 --> 57:42.600\n Is it Wikipedia, that kind of thing?\n\n57:42.600 --> 57:44.960\n No, no, so I don't think transfer learning\n\n57:44.960 --> 57:46.240\n is really where we should focus.\n\n57:46.240 --> 57:48.000\n We should try to do,\n\n57:48.000 --> 57:51.200\n you know, have a kind of scenario for Benchmark\n\n57:51.200 --> 57:53.680\n where you have unlabeled data\n\n57:53.680 --> 57:58.680\n and you can, and it's very large number of unlabeled data.\n\n57:58.680 --> 58:00.640\n It could be video clips.\n\n58:00.640 --> 58:03.680\n It could be where you do, you know, frame prediction.\n\n58:03.680 --> 58:06.160\n It could be images where you could choose to,\n\n58:06.160 --> 58:10.680\n you know, mask a piece of it, could be whatever,\n\n58:10.680 --> 58:13.920\n but they're unlabeled and you're not allowed to label them.\n\n58:13.920 --> 58:18.040\n So you do some training on this,\n\n58:18.040 --> 58:23.040\n and then you train on a particular supervised task,\n\n58:24.720 --> 58:26.320\n ImageNet or a NIST,\n\n58:26.320 --> 58:30.200\n and you measure how your test error decrease\n\n58:30.200 --> 58:31.480\n or validation error decreases\n\n58:31.480 --> 58:34.080\n as you increase the number of label training samples.\n\n58:35.400 --> 58:40.400\n Okay, and what you'd like to see is that,\n\n58:40.400 --> 58:43.000\n you know, your error decreases much faster\n\n58:43.000 --> 58:45.400\n than if you train from scratch from random weights.\n\n58:46.560 --> 58:48.600\n So that to reach the same level of performance\n\n58:48.600 --> 58:52.120\n and a completely supervised, purely supervised system\n\n58:52.120 --> 58:54.440\n would reach you would need way fewer samples.\n\n58:54.440 --> 58:55.760\n So that's the crucial question\n\n58:55.760 --> 58:58.280\n because it will answer the question to like, you know,\n\n58:58.280 --> 59:01.000\n people interested in medical image analysis.\n\n59:01.000 --> 59:05.000\n Okay, you know, if I want to get to a particular level\n\n59:05.000 --> 59:07.120\n of error rate for this task,\n\n59:07.120 --> 59:10.480\n I know I need a million samples.\n\n59:10.480 --> 59:13.560\n Can I do, you know, self supervised pre training\n\n59:13.560 --> 59:15.800\n to reduce this to about 100 or something?\n\n59:15.800 --> 59:16.840\n And you think the answer there\n\n59:16.840 --> 59:18.960\n is self supervised pre training?\n\n59:18.960 --> 59:23.040\n Yeah, some form, some form of it.\n\n59:23.040 --> 59:26.600\n Telling you active learning, but you disagree.\n\n59:26.600 --> 59:28.440\n No, it's not useless.\n\n59:28.440 --> 59:30.640\n It's just not gonna lead to a quantum leap.\n\n59:30.640 --> 59:32.200\n It's just gonna make things that we already do.\n\n59:32.200 --> 59:33.720\n So you're way smarter than me.\n\n59:33.720 --> 59:35.160\n I just disagree with you.\n\n59:35.160 --> 59:37.280\n But I don't have anything to back that.\n\n59:37.280 --> 59:38.760\n It's just intuition.\n\n59:38.760 --> 59:40.760\n So I worked a lot of large scale data sets\n\n59:40.760 --> 59:43.640\n and there's something that might be magic\n\n59:43.640 --> 59:45.840\n in active learning, but okay.\n\n59:45.840 --> 59:47.440\n And at least I said it publicly.\n\n59:48.560 --> 59:50.520\n At least I'm being an idiot publicly.\n\n59:50.520 --> 59:51.360\n Okay.\n\n59:51.360 --> 59:52.200\n It's not being an idiot.\n\n59:52.200 --> 59:54.080\n It's, you know, working with the data you have.\n\n59:54.080 --> 59:56.360\n I mean, I mean, certainly people are doing things like,\n\n59:56.360 --> 59:59.160\n okay, I have 3000 hours of, you know,\n\n59:59.160 --> 1:00:01.280\n imitation learning for start driving car,\n\n1:00:01.280 --> 1:00:03.280\n but most of those are incredibly boring.\n\n1:00:03.280 --> 1:00:05.840\n What I like is select, you know, 10% of them\n\n1:00:05.840 --> 1:00:07.400\n that are kind of the most informative.\n\n1:00:07.400 --> 1:00:10.400\n And with just that, I would probably reach the same.\n\n1:00:10.400 --> 1:00:14.280\n So it's a weak form of active learning if you want.\n\n1:00:14.280 --> 1:00:18.040\n Yes, but there might be a much stronger version.\n\n1:00:18.040 --> 1:00:18.880\n Yeah, that's right.\n\n1:00:18.880 --> 1:00:21.600\n That's what, and that's an awful question if it exists.\n\n1:00:21.600 --> 1:00:24.360\n The question is how much stronger can you get?\n\n1:00:24.360 --> 1:00:26.520\n Elon Musk is confident.\n\n1:00:26.520 --> 1:00:28.120\n Talked to him recently.\n\n1:00:28.120 --> 1:00:30.760\n He's confident that large scale data and deep learning\n\n1:00:30.760 --> 1:00:33.560\n can solve the autonomous driving problem.\n\n1:00:33.560 --> 1:00:36.280\n What are your thoughts on the limits,\n\n1:00:36.280 --> 1:00:38.520\n possibilities of deep learning in this space?\n\n1:00:38.520 --> 1:00:40.880\n It's obviously part of the solution.\n\n1:00:40.880 --> 1:00:43.800\n I mean, I don't think we'll ever have a set driving system\n\n1:00:43.800 --> 1:00:45.600\n or at least not in the foreseeable future\n\n1:00:45.600 --> 1:00:47.240\n that does not use deep learning.\n\n1:00:47.240 --> 1:00:48.360\n Let me put it this way.\n\n1:00:48.360 --> 1:00:49.600\n Now, how much of it?\n\n1:00:49.600 --> 1:00:53.040\n So in the history of sort of engineering,\n\n1:00:54.040 --> 1:00:58.320\n particularly sort of AI like systems,\n\n1:00:58.320 --> 1:01:01.000\n there's generally a first phase where everything is built by hand.\n\n1:01:01.000 --> 1:01:02.120\n Then there is a second phase.\n\n1:01:02.120 --> 1:01:06.400\n And that was the case for autonomous driving 20, 30 years ago.\n\n1:01:06.400 --> 1:01:09.160\n There's a phase where there's a little bit of learning is used,\n\n1:01:09.160 --> 1:01:12.800\n but there's a lot of engineering that's involved in kind of\n\n1:01:12.800 --> 1:01:16.480\n taking care of corner cases and putting limits, et cetera,\n\n1:01:16.480 --> 1:01:18.200\n because the learning system is not perfect.\n\n1:01:18.200 --> 1:01:20.600\n And then as technology progresses,\n\n1:01:21.960 --> 1:01:23.920\n we end up relying more and more on learning.\n\n1:01:23.920 --> 1:01:25.800\n That's the history of character recognition,\n\n1:01:25.800 --> 1:01:27.120\n it's the history of science.\n\n1:01:27.120 --> 1:01:29.120\n Character recognition is the history of speech recognition,\n\n1:01:29.120 --> 1:01:31.600\n now computer vision, natural language processing.\n\n1:01:31.600 --> 1:01:36.160\n And I think the same is going to happen with autonomous driving\n\n1:01:36.160 --> 1:01:40.720\n that currently the methods that are closest\n\n1:01:40.720 --> 1:01:43.120\n to providing some level of autonomy,\n\n1:01:43.120 --> 1:01:44.960\n some decent level of autonomy\n\n1:01:44.960 --> 1:01:48.560\n where you don't expect a driver to kind of do anything\n\n1:01:48.560 --> 1:01:50.880\n is where you constrain the world.\n\n1:01:50.880 --> 1:01:53.760\n So you only run within 100 square kilometers\n\n1:01:53.760 --> 1:01:56.200\n or square miles in Phoenix where the weather is nice\n\n1:01:56.200 --> 1:02:00.240\n and the roads are wide, which is what Waymo is doing.\n\n1:02:00.240 --> 1:02:04.480\n You completely overengineer the car with tons of LIDARs\n\n1:02:04.480 --> 1:02:08.440\n and sophisticated sensors that are too expensive\n\n1:02:08.440 --> 1:02:09.280\n for consumer cars,\n\n1:02:09.280 --> 1:02:11.280\n but they're fine if you just run a fleet.\n\n1:02:13.040 --> 1:02:16.400\n And you engineer the hell out of everything else.\n\n1:02:16.400 --> 1:02:17.960\n You map the entire world.\n\n1:02:17.960 --> 1:02:20.360\n So you have complete 3D model of everything.\n\n1:02:20.360 --> 1:02:22.160\n So the only thing that the perception system\n\n1:02:22.160 --> 1:02:24.160\n has to take care of is moving objects\n\n1:02:24.160 --> 1:02:29.160\n and construction and sort of things that weren't in your map.\n\n1:02:30.880 --> 1:02:34.160\n And you can engineer a good SLAM system and all that stuff.\n\n1:02:34.160 --> 1:02:35.840\n So that's kind of the current approach\n\n1:02:35.840 --> 1:02:37.480\n that's closest to some level of autonomy.\n\n1:02:37.480 --> 1:02:39.640\n But I think eventually the longterm solution\n\n1:02:39.640 --> 1:02:43.400\n is going to rely more and more on learning\n\n1:02:43.400 --> 1:02:45.000\n and possibly using a combination\n\n1:02:45.000 --> 1:02:49.320\n of self supervised learning and model based reinforcement\n\n1:02:49.320 --> 1:02:50.840\n or something like that.\n\n1:02:50.840 --> 1:02:54.760\n But ultimately learning will be not just at the core,\n\n1:02:54.760 --> 1:02:57.160\n but really the fundamental part of the system.\n\n1:02:57.160 --> 1:03:00.360\n Yeah, it already is, but it will become more and more.\n\n1:03:00.360 --> 1:03:02.720\n What do you think it takes to build a system\n\n1:03:02.720 --> 1:03:04.080\n with human level intelligence?\n\n1:03:04.080 --> 1:03:07.600\n You talked about the AI system in the movie Her\n\n1:03:07.600 --> 1:03:10.040\n being way out of reach, our current reach.\n\n1:03:10.040 --> 1:03:12.360\n This might be outdated as well, but.\n\n1:03:12.360 --> 1:03:13.240\n It's still way out of reach.\n\n1:03:13.240 --> 1:03:14.720\n It's still way out of reach.\n\n1:03:15.800 --> 1:03:18.360\n What would it take to build Her?\n\n1:03:18.360 --> 1:03:19.720\n Do you think?\n\n1:03:19.720 --> 1:03:21.760\n So I can tell you the first two obstacles\n\n1:03:21.760 --> 1:03:22.880\n that we have to clear,\n\n1:03:22.880 --> 1:03:24.880\n but I don't know how many obstacles there are after this.\n\n1:03:24.880 --> 1:03:26.640\n So the image I usually use is that\n\n1:03:26.640 --> 1:03:28.680\n there is a bunch of mountains that we have to climb\n\n1:03:28.680 --> 1:03:29.720\n and we can see the first one,\n\n1:03:29.720 --> 1:03:33.080\n but we don't know if there are 50 mountains behind it or not.\n\n1:03:33.080 --> 1:03:34.960\n And this might be a good sort of metaphor\n\n1:03:34.960 --> 1:03:38.400\n for why AI researchers in the past\n\n1:03:38.400 --> 1:03:42.000\n have been overly optimistic about the result of AI.\n\n1:03:43.520 --> 1:03:44.640\n You know, for example,\n\n1:03:45.800 --> 1:03:49.440\n Noel and Simon wrote the general problem solver\n\n1:03:49.440 --> 1:03:51.440\n and they called it the general problem solver.\n\n1:03:51.440 --> 1:03:52.960\n General problem solver.\n\n1:03:52.960 --> 1:03:54.520\n And of course, the first thing you realize\n\n1:03:54.520 --> 1:03:56.360\n is that all the problems you want to solve are exponential.\n\n1:03:56.360 --> 1:03:59.160\n And so you can't actually use it for anything useful,\n\n1:03:59.160 --> 1:04:00.080\n but you know.\n\n1:04:00.080 --> 1:04:02.280\n Yeah, so yeah, all you see is the first peak.\n\n1:04:02.280 --> 1:04:05.280\n So in general, what are the first couple of peaks for Her?\n\n1:04:05.280 --> 1:04:08.000\n So the first peak, which is precisely what I'm working on\n\n1:04:08.000 --> 1:04:10.280\n is self supervised learning.\n\n1:04:10.280 --> 1:04:12.280\n How do we get machines to run models of the world\n\n1:04:12.280 --> 1:04:15.880\n by observation, kind of like babies and like young animals?\n\n1:04:15.880 --> 1:04:20.880\n So we've been working with, you know, cognitive scientists.\n\n1:04:21.760 --> 1:04:24.760\n So this Emmanuelle Dupoux, who's at FAIR in Paris,\n\n1:04:24.760 --> 1:04:29.760\n is a half time, is also a researcher in a French university.\n\n1:04:30.640 --> 1:04:35.640\n And he has this chart that shows that which,\n\n1:04:36.120 --> 1:04:38.640\n how many months of life baby humans\n\n1:04:38.640 --> 1:04:40.720\n kind of learn different concepts.\n\n1:04:40.720 --> 1:04:44.040\n And you can measure this in sort of various ways.\n\n1:04:44.040 --> 1:04:49.040\n So things like distinguishing animate objects\n\n1:04:49.040 --> 1:04:50.360\n from inanimate objects,\n\n1:04:50.360 --> 1:04:53.280\n you can tell the difference at age two, three months.\n\n1:04:54.720 --> 1:04:56.360\n Whether an object is going to stay stable,\n\n1:04:56.360 --> 1:04:58.080\n is going to fall, you know,\n\n1:04:58.080 --> 1:05:00.760\n about four months, you can tell.\n\n1:05:00.760 --> 1:05:02.400\n You know, there are various things like this.\n\n1:05:02.400 --> 1:05:04.240\n And then things like gravity,\n\n1:05:04.240 --> 1:05:06.520\n the fact that objects are not supposed to float in the air,\n\n1:05:06.520 --> 1:05:07.880\n but are supposed to fall,\n\n1:05:07.880 --> 1:05:10.360\n you run this around the age of eight or nine months.\n\n1:05:10.360 --> 1:05:11.960\n If you look at the data,\n\n1:05:11.960 --> 1:05:14.600\n eight or nine months, if you look at a lot of,\n\n1:05:14.600 --> 1:05:15.880\n you know, eight month old babies,\n\n1:05:15.880 --> 1:05:19.040\n you give them a bunch of toys on their high chair.\n\n1:05:19.040 --> 1:05:20.560\n First thing they do is they throw them on the ground\n\n1:05:20.560 --> 1:05:21.720\n and they look at them.\n\n1:05:21.720 --> 1:05:23.920\n It's because, you know, they're learning about,\n\n1:05:23.920 --> 1:05:26.120\n actively learning about gravity.\n\n1:05:26.120 --> 1:05:26.960\n Gravity, yeah.\n\n1:05:26.960 --> 1:05:29.680\n Okay, so they're not trying to annoy you,\n\n1:05:29.680 --> 1:05:32.480\n but they, you know, they need to do the experiment, right?\n\n1:05:32.480 --> 1:05:33.600\n Yeah.\n\n1:05:33.600 --> 1:05:36.600\n So, you know, how do we get machines to learn like babies,\n\n1:05:36.600 --> 1:05:39.240\n mostly by observation with a little bit of interaction\n\n1:05:39.240 --> 1:05:41.200\n and learning those models of the world?\n\n1:05:41.200 --> 1:05:43.720\n Because I think that's really a crucial piece\n\n1:05:43.720 --> 1:05:46.360\n of an intelligent autonomous system.\n\n1:05:46.360 --> 1:05:47.520\n So if you think about the architecture\n\n1:05:47.520 --> 1:05:49.520\n of an intelligent autonomous system,\n\n1:05:49.520 --> 1:05:51.320\n it needs to have a predictive model of the world.\n\n1:05:51.320 --> 1:05:54.080\n So something that says, here is a world at time T,\n\n1:05:54.080 --> 1:05:55.520\n here is a state of the world at time T plus one,\n\n1:05:55.520 --> 1:05:56.680\n if I take this action.\n\n1:05:57.560 --> 1:05:59.680\n And it's not a single answer, it can be a...\n\n1:05:59.680 --> 1:06:01.240\n Yeah, it can be a distribution, yeah.\n\n1:06:01.240 --> 1:06:03.200\n Yeah, well, but we don't know how to represent\n\n1:06:03.200 --> 1:06:04.840\n distributions in high dimensional T spaces.\n\n1:06:04.840 --> 1:06:07.200\n So it's gotta be something weaker than that, okay?\n\n1:06:07.200 --> 1:06:09.760\n But with some representation of uncertainty.\n\n1:06:09.760 --> 1:06:12.440\n If you have that, then you can do what optimal control\n\n1:06:12.440 --> 1:06:14.360\n theorists call model predictive control,\n\n1:06:14.360 --> 1:06:16.360\n which means that you can run your model\n\n1:06:16.360 --> 1:06:18.800\n with a hypothesis for a sequence of action\n\n1:06:18.800 --> 1:06:20.840\n and then see the result.\n\n1:06:20.840 --> 1:06:22.160\n Now, what you need, the other thing you need\n\n1:06:22.160 --> 1:06:24.920\n is some sort of objective that you want to optimize.\n\n1:06:24.920 --> 1:06:27.560\n Am I reaching the goal of grabbing this object?\n\n1:06:27.560 --> 1:06:28.880\n Am I minimizing energy?\n\n1:06:28.880 --> 1:06:30.040\n Am I whatever, right?\n\n1:06:30.040 --> 1:06:33.720\n So there is some sort of objective that you have to minimize.\n\n1:06:33.720 --> 1:06:35.640\n And so in your head, if you have this model,\n\n1:06:35.640 --> 1:06:37.080\n you can figure out the sequence of action\n\n1:06:37.080 --> 1:06:38.920\n that will optimize your objective.\n\n1:06:38.920 --> 1:06:42.400\n That objective is something that ultimately is rooted\n\n1:06:42.400 --> 1:06:44.960\n in your basal ganglia, at least in the human brain,\n\n1:06:44.960 --> 1:06:47.040\n that's what it's basal ganglia,\n\n1:06:47.040 --> 1:06:50.600\n computes your level of contentment or miscontentment.\n\n1:06:50.600 --> 1:06:52.360\n I don't know if that's a word.\n\n1:06:52.360 --> 1:06:53.680\n Unhappiness, okay?\n\n1:06:53.680 --> 1:06:54.800\n Yeah, yeah.\n\n1:06:54.800 --> 1:06:55.640\n Discontentment.\n\n1:06:55.640 --> 1:06:56.680\n Discontentment, maybe.\n\n1:06:56.680 --> 1:07:00.720\n And so your entire behavior is driven towards\n\n1:07:01.720 --> 1:07:03.320\n kind of minimizing that objective,\n\n1:07:03.320 --> 1:07:05.720\n which is maximizing your contentment,\n\n1:07:05.720 --> 1:07:07.600\n computed by your basal ganglia.\n\n1:07:07.600 --> 1:07:10.600\n And what you have is an objective function,\n\n1:07:10.600 --> 1:07:12.320\n which is basically a predictor\n\n1:07:12.320 --> 1:07:14.520\n of what your basal ganglia is going to tell you.\n\n1:07:14.520 --> 1:07:16.600\n So you're not going to put your hand on fire\n\n1:07:16.600 --> 1:07:19.760\n because you know it's going to burn\n\n1:07:19.760 --> 1:07:21.240\n and you're going to get hurt.\n\n1:07:21.240 --> 1:07:23.160\n And you're predicting this because of your model\n\n1:07:23.160 --> 1:07:25.720\n of the world and your sort of predictor\n\n1:07:25.720 --> 1:07:27.560\n of this objective, right?\n\n1:07:27.560 --> 1:07:31.160\n So if you have those three components,\n\n1:07:31.160 --> 1:07:32.600\n you have four components,\n\n1:07:32.600 --> 1:07:36.080\n you have the hardwired objective,\n\n1:07:36.080 --> 1:07:41.080\n hardwired contentment objective computer,\n\n1:07:41.760 --> 1:07:43.960\n if you want, calculator.\n\n1:07:43.960 --> 1:07:45.160\n And then you have the three components.\n\n1:07:45.160 --> 1:07:46.760\n One is the objective predictor,\n\n1:07:46.760 --> 1:07:48.960\n which basically predicts your level of contentment.\n\n1:07:48.960 --> 1:07:52.560\n One is the model of the world.\n\n1:07:52.560 --> 1:07:54.120\n And there's a third module I didn't mention,\n\n1:07:54.120 --> 1:07:57.280\n which is the module that will figure out\n\n1:07:57.280 --> 1:08:00.560\n the best course of action to optimize an objective\n\n1:08:00.560 --> 1:08:03.480\n given your model, okay?\n\n1:08:03.480 --> 1:08:04.520\n Yeah.\n\n1:08:04.520 --> 1:08:07.240\n And you can call this a policy network\n\n1:08:07.240 --> 1:08:09.400\n or something like that, right?\n\n1:08:09.400 --> 1:08:11.720\n Now, you need those three components\n\n1:08:11.720 --> 1:08:13.960\n to act autonomously intelligently.\n\n1:08:13.960 --> 1:08:16.120\n And you can be stupid in three different ways.\n\n1:08:16.120 --> 1:08:19.400\n You can be stupid because your model of the world is wrong.\n\n1:08:19.400 --> 1:08:22.520\n You can be stupid because your objective is not aligned\n\n1:08:22.520 --> 1:08:25.600\n with what you actually want to achieve, okay?\n\n1:08:27.000 --> 1:08:28.960\n In humans, that would be a psychopath.\n\n1:08:30.000 --> 1:08:33.640\n And then the third way you can be stupid\n\n1:08:33.640 --> 1:08:34.960\n is that you have the right model,\n\n1:08:34.960 --> 1:08:36.360\n you have the right objective,\n\n1:08:36.360 --> 1:08:38.840\n but you're unable to figure out a course of action\n\n1:08:38.840 --> 1:08:41.240\n to optimize your objective given your model.\n\n1:08:41.240 --> 1:08:42.080\n Okay.\n\n1:08:44.080 --> 1:08:45.920\n Some people who are in charge of big countries\n\n1:08:45.920 --> 1:08:47.760\n actually have all three that are wrong.\n\n1:08:47.760 --> 1:08:48.600\n All right.\n\n1:08:50.920 --> 1:08:51.760\n Which countries?\n\n1:08:51.760 --> 1:08:52.600\n I don't know.\n\n1:08:52.600 --> 1:08:55.960\n Okay, so if we think about this agent,\n\n1:08:55.960 --> 1:08:58.000\n if we think about the movie Her,\n\n1:08:58.000 --> 1:09:02.920\n you've criticized the art project\n\n1:09:02.920 --> 1:09:04.680\n that is Sophia the Robot.\n\n1:09:04.680 --> 1:09:07.560\n And what that project essentially does\n\n1:09:07.560 --> 1:09:11.720\n is uses our natural inclination to anthropomorphize\n\n1:09:11.720 --> 1:09:14.800\n things that look like human and give them more.\n\n1:09:14.800 --> 1:09:17.720\n Do you think that could be used by AI systems\n\n1:09:17.720 --> 1:09:18.960\n like in the movie Her?\n\n1:09:21.320 --> 1:09:23.400\n So do you think that body is needed\n\n1:09:23.400 --> 1:09:27.200\n to create a feeling of intelligence?\n\n1:09:27.200 --> 1:09:29.320\n Well, if Sophia was just an art piece,\n\n1:09:29.320 --> 1:09:30.360\n I would have no problem with it,\n\n1:09:30.360 --> 1:09:33.040\n but it's presented as something else.\n\n1:09:33.040 --> 1:09:35.280\n Let me, on that comment real quick,\n\n1:09:35.280 --> 1:09:38.520\n if creators of Sophia could change something\n\n1:09:38.520 --> 1:09:40.760\n about their marketing or behavior in general,\n\n1:09:40.760 --> 1:09:41.600\n what would it be?\n\n1:09:41.600 --> 1:09:42.840\n What's?\n\n1:09:42.840 --> 1:09:44.160\n I'm just about everything.\n\n1:09:44.160 --> 1:09:49.160\n I mean, don't you think, here's a tough question.\n\n1:09:50.080 --> 1:09:51.680\n Let me, so I agree with you.\n\n1:09:51.680 --> 1:09:56.560\n So Sophia is not, the general public feels\n\n1:09:56.560 --> 1:09:59.320\n that Sophia can do way more than she actually can.\n\n1:09:59.320 --> 1:10:00.200\n That's right.\n\n1:10:00.200 --> 1:10:02.760\n And the people who created Sophia\n\n1:10:02.760 --> 1:10:07.760\n are not honestly publicly communicating,\n\n1:10:08.360 --> 1:10:09.440\n trying to teach the public.\n\n1:10:09.440 --> 1:10:10.280\n Right.\n\n1:10:10.280 --> 1:10:13.280\n But here's a tough question.\n\n1:10:13.280 --> 1:10:18.280\n Don't you think the same thing is scientists\n\n1:10:19.800 --> 1:10:22.920\n in industry and research are taking advantage\n\n1:10:22.920 --> 1:10:25.640\n of the same misunderstanding in the public\n\n1:10:25.640 --> 1:10:29.920\n when they create AI companies or publish stuff?\n\n1:10:29.920 --> 1:10:31.120\n Some companies, yes.\n\n1:10:31.120 --> 1:10:33.160\n I mean, there is no sense of,\n\n1:10:33.160 --> 1:10:34.880\n there's no desire to delude.\n\n1:10:34.880 --> 1:10:37.840\n There's no desire to kind of over claim\n\n1:10:37.840 --> 1:10:38.840\n when something is done, right?\n\n1:10:38.840 --> 1:10:41.400\n You publish a paper on AI that has this result\n\n1:10:41.400 --> 1:10:43.080\n on ImageNet, it's pretty clear.\n\n1:10:43.080 --> 1:10:44.960\n I mean, it's not even interesting anymore,\n\n1:10:44.960 --> 1:10:47.920\n but I don't think there is that.\n\n1:10:49.240 --> 1:10:52.880\n I mean, the reviewers are generally not very forgiving\n\n1:10:52.880 --> 1:10:57.200\n of unsupported claims of this type.\n\n1:10:57.200 --> 1:10:59.680\n And, but there are certainly quite a few startups\n\n1:10:59.680 --> 1:11:02.680\n that have had a huge amount of hype around this\n\n1:11:02.680 --> 1:11:05.520\n that I find extremely damaging\n\n1:11:05.520 --> 1:11:08.080\n and I've been calling it out when I've seen it.\n\n1:11:08.080 --> 1:11:10.280\n So yeah, but to go back to your original question,\n\n1:11:10.280 --> 1:11:13.080\n like the necessity of embodiment,\n\n1:11:13.080 --> 1:11:15.640\n I think, I don't think embodiment is necessary.\n\n1:11:15.640 --> 1:11:17.120\n I think grounding is necessary.\n\n1:11:17.120 --> 1:11:18.960\n So I don't think we're gonna get machines\n\n1:11:18.960 --> 1:11:20.520\n that really understand language\n\n1:11:20.520 --> 1:11:22.440\n without some level of grounding in the real world.\n\n1:11:22.440 --> 1:11:24.360\n And it's not clear to me that language\n\n1:11:24.360 --> 1:11:26.160\n is a high enough bandwidth medium\n\n1:11:26.160 --> 1:11:28.280\n to communicate how the real world works.\n\n1:11:28.280 --> 1:11:30.120\n So I think for this.\n\n1:11:30.120 --> 1:11:32.320\n Can you talk to what grounding means?\n\n1:11:32.320 --> 1:11:34.040\n So grounding means that,\n\n1:11:34.040 --> 1:11:37.720\n so there is this classic problem of common sense reasoning,\n\n1:11:37.720 --> 1:11:41.000\n you know, the Winograd schema, right?\n\n1:11:41.000 --> 1:11:44.960\n And so I tell you the trophy doesn't fit in the suitcase\n\n1:11:44.960 --> 1:11:46.360\n because it's too big,\n\n1:11:46.360 --> 1:11:47.760\n or the trophy doesn't fit in the suitcase\n\n1:11:47.760 --> 1:11:49.160\n because it's too small.\n\n1:11:49.160 --> 1:11:51.800\n And the it in the first case refers to the trophy\n\n1:11:51.800 --> 1:11:53.640\n in the second case to the suitcase.\n\n1:11:53.640 --> 1:11:55.160\n And the reason you can figure this out\n\n1:11:55.160 --> 1:11:56.960\n is because you know where the trophy and the suitcase are,\n\n1:11:56.960 --> 1:11:58.640\n you know, one is supposed to fit in the other one\n\n1:11:58.640 --> 1:12:00.560\n and you know the notion of size\n\n1:12:00.560 --> 1:12:03.000\n and a big object doesn't fit in a small object,\n\n1:12:03.000 --> 1:12:05.280\n unless it's a Tardis, you know, things like that, right?\n\n1:12:05.280 --> 1:12:08.640\n So you have this knowledge of how the world works,\n\n1:12:08.640 --> 1:12:10.640\n of geometry and things like that.\n\n1:12:12.440 --> 1:12:14.640\n I don't believe you can learn everything about the world\n\n1:12:14.640 --> 1:12:18.000\n by just being told in language how the world works.\n\n1:12:18.000 --> 1:12:21.680\n I think you need some low level perception of the world,\n\n1:12:21.680 --> 1:12:23.680\n you know, be it visual touch, you know, whatever,\n\n1:12:23.680 --> 1:12:26.760\n but some higher bandwidth perception of the world.\n\n1:12:26.760 --> 1:12:28.800\n By reading all the world's text,\n\n1:12:28.800 --> 1:12:31.160\n you still might not have enough information.\n\n1:12:31.160 --> 1:12:32.520\n That's right.\n\n1:12:32.520 --> 1:12:35.440\n There's a lot of things that just will never appear in text\n\n1:12:35.440 --> 1:12:37.000\n and that you can't really infer.\n\n1:12:37.000 --> 1:12:40.560\n So I think common sense will emerge from,\n\n1:12:41.440 --> 1:12:43.440\n you know, certainly a lot of language interaction,\n\n1:12:43.440 --> 1:12:45.640\n but also with watching videos\n\n1:12:45.640 --> 1:12:48.920\n or perhaps even interacting in virtual environments\n\n1:12:48.920 --> 1:12:51.760\n and possibly, you know, robot interacting in the real world.\n\n1:12:51.760 --> 1:12:53.640\n But I don't actually believe necessarily\n\n1:12:53.640 --> 1:12:56.000\n that this last one is absolutely necessary.\n\n1:12:56.000 --> 1:12:58.560\n But I think that there's a need for some grounding.\n\n1:13:00.240 --> 1:13:01.880\n But the final product\n\n1:13:01.880 --> 1:13:04.840\n doesn't necessarily need to be embodied, you're saying.\n\n1:13:04.840 --> 1:13:05.680\n No.\n\n1:13:05.680 --> 1:13:07.720\n It just needs to have an awareness, a grounding to.\n\n1:13:07.720 --> 1:13:11.120\n Right, but it needs to know how the world works\n\n1:13:11.120 --> 1:13:14.440\n to have, you know, to not be frustrating to talk to.\n\n1:13:15.840 --> 1:13:19.520\n And you talked about emotions being important.\n\n1:13:19.520 --> 1:13:21.760\n That's a whole nother topic.\n\n1:13:21.760 --> 1:13:24.320\n Well, so, you know, I talked about this,\n\n1:13:24.320 --> 1:13:29.320\n the basal ganglia as the thing\n\n1:13:29.600 --> 1:13:32.920\n that calculates your level of miscontentment.\n\n1:13:32.920 --> 1:13:34.640\n And then there is this other module\n\n1:13:34.640 --> 1:13:36.640\n that sort of tries to do a prediction\n\n1:13:36.640 --> 1:13:38.520\n of whether you're going to be content or not.\n\n1:13:38.520 --> 1:13:40.240\n That's the source of some emotion.\n\n1:13:40.240 --> 1:13:43.040\n So fear, for example, is an anticipation\n\n1:13:43.040 --> 1:13:46.400\n of bad things that can happen to you, right?\n\n1:13:47.440 --> 1:13:49.240\n You have this inkling that there is some chance\n\n1:13:49.240 --> 1:13:50.880\n that something really bad is going to happen to you\n\n1:13:50.880 --> 1:13:52.280\n and that creates fear.\n\n1:13:52.280 --> 1:13:53.120\n Well, you know for sure\n\n1:13:53.120 --> 1:13:54.480\n that something bad is going to happen to you,\n\n1:13:54.480 --> 1:13:55.960\n you kind of give up, right?\n\n1:13:55.960 --> 1:13:57.560\n It's not fear anymore.\n\n1:13:57.560 --> 1:13:59.480\n It's uncertainty that creates fear.\n\n1:13:59.480 --> 1:14:01.200\n So the punchline is,\n\n1:14:01.200 --> 1:14:02.560\n we're not going to have autonomous intelligence\n\n1:14:02.560 --> 1:14:03.400\n without emotions.\n\n1:14:07.040 --> 1:14:08.880\n Whatever the heck emotions are.\n\n1:14:08.880 --> 1:14:11.080\n So you mentioned very practical things of fear,\n\n1:14:11.080 --> 1:14:13.480\n but there's a lot of other mess around it.\n\n1:14:13.480 --> 1:14:16.400\n But there are kind of the results of, you know, drives.\n\n1:14:16.400 --> 1:14:19.360\n Yeah, there's deeper biological stuff going on.\n\n1:14:19.360 --> 1:14:21.440\n And I've talked to a few folks on this.\n\n1:14:21.440 --> 1:14:23.360\n There's fascinating stuff\n\n1:14:23.360 --> 1:14:27.320\n that ultimately connects to our brain.\n\n1:14:27.320 --> 1:14:30.880\n If we create an AGI system, sorry.\n\n1:14:30.880 --> 1:14:31.720\n Human level intelligence.\n\n1:14:31.720 --> 1:14:33.360\n Human level intelligence system.\n\n1:14:34.480 --> 1:14:37.160\n And you get to ask her one question.\n\n1:14:37.160 --> 1:14:38.560\n What would that question be?\n\n1:14:39.960 --> 1:14:42.880\n You know, I think the first one we'll create\n\n1:14:42.880 --> 1:14:45.520\n would probably not be that smart.\n\n1:14:45.520 --> 1:14:47.040\n They'd be like a four year old.\n\n1:14:47.040 --> 1:14:47.880\n Okay.\n\n1:14:47.880 --> 1:14:50.040\n So you would have to ask her a question\n\n1:14:50.040 --> 1:14:51.560\n to know she's not that smart.\n\n1:14:52.840 --> 1:14:53.680\n Yeah.\n\n1:14:54.520 --> 1:14:56.960\n Well, what's a good question to ask, you know,\n\n1:14:56.960 --> 1:14:57.800\n to be impressed.\n\n1:14:57.800 --> 1:14:58.640\n What is the cause of wind?\n\n1:15:01.040 --> 1:15:02.240\n And if she answers,\n\n1:15:02.240 --> 1:15:04.760\n oh, it's because the leaves of the tree are moving\n\n1:15:04.760 --> 1:15:06.520\n and that creates wind.\n\n1:15:06.520 --> 1:15:07.680\n She's onto something.\n\n1:15:08.760 --> 1:15:11.840\n And if she says that's a stupid question,\n\n1:15:11.840 --> 1:15:12.680\n she's really onto something.\n\n1:15:12.680 --> 1:15:14.440\n No, and then you tell her,\n\n1:15:14.440 --> 1:15:18.080\n actually, you know, here is the real thing.\n\n1:15:18.080 --> 1:15:20.520\n She says, oh yeah, that makes sense.\n\n1:15:20.520 --> 1:15:24.480\n So questions that reveal the ability\n\n1:15:24.480 --> 1:15:26.960\n to do common sense reasoning about the physical world.\n\n1:15:26.960 --> 1:15:27.800\n Yeah.\n\n1:15:27.800 --> 1:15:30.120\n And you'll sum it up with causal inference.\n\n1:15:30.120 --> 1:15:31.200\n Causal inference.\n\n1:15:31.200 --> 1:15:33.640\n Well, it was a huge honor.\n\n1:15:33.640 --> 1:15:35.720\n Congratulations on your Turing Award.\n\n1:15:35.720 --> 1:15:37.240\n Thank you so much for talking today.\n\n1:15:37.240 --> 1:15:38.080\n Thank you.\n\n1:15:38.080 --> 1:15:58.080\n Thank you for having me.\n\n"
}