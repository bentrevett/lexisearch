{
  "title": "Anca Dragan: Human-Robot Interaction and Reward Engineering | Lex Fridman Podcast #81",
  "id": "iOCfIFBBpVY",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:03.880\n The following is a conversation with Anca Drogon,\n\n00:03.880 --> 00:08.160\n a professor at Berkeley working on human robot interaction,\n\n00:08.160 --> 00:10.760\n algorithms that look beyond the robot's function\n\n00:10.760 --> 00:13.920\n in isolation and generate robot behavior\n\n00:13.920 --> 00:15.960\n that accounts for interaction\n\n00:15.960 --> 00:18.080\n and coordination with human beings.\n\n00:18.080 --> 00:22.360\n She also consults at Waymo, the autonomous vehicle company,\n\n00:22.360 --> 00:23.560\n but in this conversation,\n\n00:23.560 --> 00:27.120\n she is 100% wearing her Berkeley hat.\n\n00:27.120 --> 00:30.600\n She is one of the most brilliant and fun roboticists\n\n00:30.600 --> 00:32.480\n in the world to talk with.\n\n00:32.480 --> 00:36.320\n I had a tough and crazy day leading up to this conversation,\n\n00:36.320 --> 00:41.320\n so I was a bit tired, even more so than usual,\n\n00:41.440 --> 00:44.160\n but almost immediately as she walked in,\n\n00:44.160 --> 00:46.320\n her energy, passion, and excitement\n\n00:46.320 --> 00:48.880\n for human robot interaction was contagious.\n\n00:48.880 --> 00:52.840\n So I had a lot of fun and really enjoyed this conversation.\n\n00:52.840 --> 00:55.560\n This is the Artificial Intelligence Podcast.\n\n00:55.560 --> 00:57.880\n If you enjoy it, subscribe on YouTube,\n\n00:57.880 --> 01:00.320\n review it with five stars on Apple Podcast,\n\n01:00.320 --> 01:01.680\n support it on Patreon,\n\n01:01.680 --> 01:05.160\n or simply connect with me on Twitter at Lex Friedman,\n\n01:05.160 --> 01:08.160\n spelled F R I D M A N.\n\n01:08.160 --> 01:11.000\n As usual, I'll do one or two minutes of ads now\n\n01:11.000 --> 01:12.560\n and never any ads in the middle\n\n01:12.560 --> 01:14.800\n that can break the flow of the conversation.\n\n01:14.800 --> 01:16.240\n I hope that works for you\n\n01:16.240 --> 01:18.580\n and doesn't hurt the listening experience.\n\n01:20.440 --> 01:22.720\n This show is presented by Cash App,\n\n01:22.720 --> 01:25.520\n the number one finance app in the App Store.\n\n01:25.520 --> 01:29.320\n When you get it, use code LEXPODCAST.\n\n01:29.320 --> 01:31.360\n Cash App lets you send money to friends,\n\n01:31.360 --> 01:33.880\n buy Bitcoin, and invest in the stock market\n\n01:33.880 --> 01:36.000\n with as little as one dollar.\n\n01:36.840 --> 01:39.200\n Since Cash App does fractional share trading,\n\n01:39.200 --> 01:41.700\n let me mention that the order execution algorithm\n\n01:41.700 --> 01:43.360\n that works behind the scenes\n\n01:43.360 --> 01:45.960\n to create the abstraction of fractional orders\n\n01:45.960 --> 01:48.180\n is an algorithmic marvel.\n\n01:48.180 --> 01:50.500\n So big props to the Cash App engineers\n\n01:50.500 --> 01:53.240\n for solving a hard problem that in the end\n\n01:53.240 --> 01:56.120\n provides an easy interface that takes a step up\n\n01:56.120 --> 01:59.320\n to the next layer of abstraction over the stock market,\n\n01:59.320 --> 02:02.060\n making trading more accessible for new investors\n\n02:02.060 --> 02:04.820\n and diversification much easier.\n\n02:05.860 --> 02:08.240\n So again, if you get Cash App from the App Store\n\n02:08.240 --> 02:11.880\n or Google Play and use the code LEXPODCAST,\n\n02:11.880 --> 02:15.920\n you get $10 and Cash App will also donate $10 to FIRST,\n\n02:15.920 --> 02:18.520\n an organization that is helping to advance robotics\n\n02:18.520 --> 02:22.280\n and STEM education for young people around the world.\n\n02:22.280 --> 02:25.880\n And now, here's my conversation with Anca Drogon.\n\n02:26.800 --> 02:29.880\n When did you first fall in love with robotics?\n\n02:29.880 --> 02:34.200\n I think it was a very gradual process\n\n02:34.200 --> 02:37.040\n and it was somewhat accidental actually\n\n02:37.040 --> 02:41.160\n because I first started getting into programming\n\n02:41.160 --> 02:43.200\n when I was a kid and then into math\n\n02:43.200 --> 02:46.280\n and then I decided computer science\n\n02:46.280 --> 02:47.840\n was the thing I was gonna do\n\n02:47.840 --> 02:50.160\n and then in college I got into AI\n\n02:50.160 --> 02:52.480\n and then I applied to the Robotics Institute\n\n02:52.480 --> 02:56.080\n at Carnegie Mellon and I was coming from this little school\n\n02:56.080 --> 02:59.000\n in Germany that nobody had heard of\n\n02:59.000 --> 03:01.800\n but I had spent an exchange semester at Carnegie Mellon\n\n03:01.800 --> 03:04.040\n so I had letters from Carnegie Mellon.\n\n03:04.040 --> 03:06.880\n So that was the only, you know, MIT said no,\n\n03:06.880 --> 03:09.200\n Berkeley said no, Stanford said no.\n\n03:09.200 --> 03:11.100\n That was the only place I got into\n\n03:11.100 --> 03:13.200\n so I went there to the Robotics Institute\n\n03:13.200 --> 03:16.240\n and I thought that robotics is a really cool way\n\n03:16.240 --> 03:20.000\n to actually apply the stuff that I knew and loved\n\n03:20.000 --> 03:23.240\n to like optimization so that's how I got into robotics.\n\n03:23.240 --> 03:25.800\n I have a better story how I got into cars\n\n03:25.800 --> 03:30.800\n which is I used to do mostly manipulation in my PhD\n\n03:31.600 --> 03:34.800\n but now I do kind of a bit of everything application wise\n\n03:34.800 --> 03:38.960\n including cars and I got into cars\n\n03:38.960 --> 03:42.180\n because I was here in Berkeley\n\n03:42.180 --> 03:46.400\n while I was a PhD student still for RSS 2014,\n\n03:46.400 --> 03:50.380\n Peter Bill organized it and he arranged for,\n\n03:50.380 --> 03:52.840\n it was Google at the time to give us rides\n\n03:52.840 --> 03:56.400\n in self driving cars and I was in a robot\n\n03:56.400 --> 04:00.660\n and it was just making decision after decision,\n\n04:00.660 --> 04:03.400\n the right call and it was so amazing.\n\n04:03.400 --> 04:05.560\n So it was a whole different experience, right?\n\n04:05.560 --> 04:07.880\n Just I mean manipulation is so hard you can't do anything\n\n04:07.880 --> 04:08.720\n and there it was.\n\n04:08.720 --> 04:11.200\n Was it the most magical robot you've ever met?\n\n04:11.200 --> 04:14.940\n So like for me to meet a Google self driving car\n\n04:14.940 --> 04:17.600\n for the first time was like a transformative moment.\n\n04:18.480 --> 04:19.960\n Like I had two moments like that,\n\n04:19.960 --> 04:22.480\n that and Spot Mini, I don't know if you met Spot Mini\n\n04:22.480 --> 04:24.160\n from Boston Dynamics.\n\n04:24.160 --> 04:27.200\n I felt like I fell in love or something\n\n04:27.200 --> 04:30.840\n like it, cause I know how a Spot Mini works, right?\n\n04:30.840 --> 04:34.000\n It's just, I mean there's nothing truly special,\n\n04:34.000 --> 04:38.440\n it's great engineering work but the anthropomorphism\n\n04:38.440 --> 04:41.440\n that went on into my brain that came to life\n\n04:41.440 --> 04:45.880\n like it had a little arm and it looked at me,\n\n04:45.880 --> 04:47.640\n he, she looked at me, I don't know,\n\n04:47.640 --> 04:48.960\n there's a magical connection there\n\n04:48.960 --> 04:52.480\n and it made me realize, wow, robots can be so much more\n\n04:52.480 --> 04:54.240\n than things that manipulate objects.\n\n04:54.240 --> 04:56.920\n They can be things that have a human connection.\n\n04:56.920 --> 05:01.100\n Do you have, was the self driving car the moment like,\n\n05:01.100 --> 05:04.680\n was there a robot that truly sort of inspired you?\n\n05:04.680 --> 05:08.240\n That was, I remember that experience very viscerally,\n\n05:08.240 --> 05:11.600\n riding in that car and being just wowed.\n\n05:11.600 --> 05:16.040\n I had the, they gave us a sticker that said,\n\n05:16.040 --> 05:17.520\n I rode in a self driving car\n\n05:17.520 --> 05:20.880\n and it had this cute little firefly on and,\n\n05:20.880 --> 05:21.720\n or logo or something like that.\n\n05:21.720 --> 05:23.680\n Oh, that was like the smaller one, like the firefly.\n\n05:23.680 --> 05:25.640\n Yeah, the really cute one, yeah.\n\n05:25.640 --> 05:30.140\n And I put it on my laptop and I had that for years\n\n05:30.140 --> 05:33.120\n until I finally changed my laptop out and you know.\n\n05:33.120 --> 05:36.320\n What about if we walk back, you mentioned optimization,\n\n05:36.320 --> 05:40.760\n like what beautiful ideas inspired you in math,\n\n05:40.760 --> 05:42.680\n computer science early on?\n\n05:42.680 --> 05:44.560\n Like why get into this field?\n\n05:44.560 --> 05:47.460\n It seems like a cold and boring field of math.\n\n05:47.460 --> 05:49.080\n Like what was exciting to you about it?\n\n05:49.080 --> 05:52.460\n The thing is I liked math from very early on,\n\n05:52.460 --> 05:56.720\n from fifth grade is when I got into the math Olympiad\n\n05:56.720 --> 05:57.540\n and all of that.\n\n05:57.540 --> 05:58.600\n Oh, you competed too?\n\n05:58.600 --> 06:01.440\n Yeah, this, it Romania is like our national sport too,\n\n06:01.440 --> 06:02.840\n you gotta understand.\n\n06:02.840 --> 06:05.800\n So I got into that fairly early\n\n06:05.800 --> 06:10.240\n and it was a little, maybe too just theory\n\n06:10.240 --> 06:13.000\n with no kind of, I didn't kind of had a,\n\n06:13.000 --> 06:15.040\n didn't really have a goal.\n\n06:15.040 --> 06:17.600\n And other than understanding, which was cool,\n\n06:17.600 --> 06:19.360\n I always liked learning and understanding,\n\n06:19.360 --> 06:20.240\n but there was no, okay,\n\n06:20.240 --> 06:22.280\n what am I applying this understanding to?\n\n06:22.280 --> 06:23.880\n And so I think that's how I got into,\n\n06:23.880 --> 06:25.400\n more heavily into computer science\n\n06:25.400 --> 06:29.280\n because it was kind of math meets something\n\n06:29.280 --> 06:31.360\n you can do tangibly in the world.\n\n06:31.360 --> 06:34.520\n Do you remember like the first program you've written?\n\n06:34.520 --> 06:37.360\n Okay, the first program I've written with,\n\n06:37.360 --> 06:41.520\n I kind of do, it was in Cubasic in fourth grade.\n\n06:42.600 --> 06:43.440\n Wow.\n\n06:43.440 --> 06:46.680\n And it was drawing like a circle.\n\n06:46.680 --> 06:47.520\n Graphics.\n\n06:47.520 --> 06:50.840\n Yeah, that was, I don't know how to do that anymore,\n\n06:51.720 --> 06:52.880\n but in fourth grade,\n\n06:52.880 --> 06:54.200\n that's the first thing that they taught me.\n\n06:54.200 --> 06:56.320\n I was like, you could take a special,\n\n06:56.320 --> 06:57.600\n I wouldn't say it was an extracurricular,\n\n06:57.600 --> 06:59.040\n it's in the sense an extracurricular,\n\n06:59.040 --> 07:03.340\n so you could sign up for dance or music or programming.\n\n07:03.340 --> 07:04.700\n And I did the programming thing\n\n07:04.700 --> 07:06.960\n and my mom was like, what, why?\n\n07:07.840 --> 07:08.880\n Did you compete in programming?\n\n07:08.880 --> 07:12.040\n Like these days, Romania probably,\n\n07:12.040 --> 07:12.980\n that's like a big thing.\n\n07:12.980 --> 07:15.400\n There's a programming competition.\n\n07:15.400 --> 07:17.120\n Was that, did that touch you at all?\n\n07:17.120 --> 07:21.360\n I did a little bit of the computer science Olympian,\n\n07:21.360 --> 07:24.720\n but not as seriously as I did the math Olympian.\n\n07:24.720 --> 07:25.760\n So it was programming.\n\n07:25.760 --> 07:26.720\n Yeah, it's basically,\n\n07:26.720 --> 07:27.720\n here's a hard math problem,\n\n07:27.720 --> 07:29.480\n solve it with a computer is kind of the deal.\n\n07:29.480 --> 07:30.720\n Yeah, it's more like algorithm.\n\n07:30.720 --> 07:32.640\n Exactly, it's always algorithmic.\n\n07:32.640 --> 07:36.720\n So again, you kind of mentioned the Google self driving car,\n\n07:36.720 --> 07:39.920\n but outside of that,\n\n07:39.920 --> 07:44.000\n what's like who or what is your favorite robot,\n\n07:44.000 --> 07:46.520\n real or fictional that like captivated\n\n07:46.520 --> 07:48.360\n your imagination throughout?\n\n07:48.360 --> 07:49.900\n I mean, I guess you kind of alluded\n\n07:49.900 --> 07:51.440\n to the Google self drive,\n\n07:51.440 --> 07:53.620\n the Firefly was a magical moment,\n\n07:53.620 --> 07:54.880\n but is there something else?\n\n07:54.880 --> 07:56.220\n It wasn't the Firefly there,\n\n07:56.220 --> 07:58.000\n I think there was the Lexus by the way.\n\n07:58.000 --> 07:59.660\n This was back then.\n\n07:59.660 --> 08:02.720\n But yeah, so good question.\n\n08:02.720 --> 08:07.720\n Okay, my favorite fictional robot is WALLI.\n\n08:08.800 --> 08:13.800\n And I love how amazingly expressive it is.\n\n08:15.000 --> 08:16.040\n I'm personally thinks a little bit\n\n08:16.040 --> 08:18.400\n about expressive motion kinds of things you're saying with,\n\n08:18.400 --> 08:20.800\n you can do this and it's a head and it's the manipulator\n\n08:20.800 --> 08:22.840\n and what does it all mean?\n\n08:22.840 --> 08:24.040\n I like to think about that stuff.\n\n08:24.040 --> 08:26.160\n I love Pixar, I love animation.\n\n08:26.160 --> 08:28.680\n WALLI has two big eyes, I think, or no?\n\n08:28.680 --> 08:33.680\n Yeah, it has these cameras and they move.\n\n08:34.600 --> 08:38.860\n So yeah, it goes and then it's super cute.\n\n08:38.860 --> 08:41.480\n Yeah, the way it moves is just so expressive,\n\n08:41.480 --> 08:43.280\n the timing of that motion,\n\n08:43.280 --> 08:44.760\n what it's doing with its arms\n\n08:44.760 --> 08:48.280\n and what it's doing with these lenses is amazing.\n\n08:48.280 --> 08:53.280\n And so I've really liked that from the start.\n\n08:53.360 --> 08:56.440\n And then on top of that, sometimes I share this,\n\n08:56.440 --> 08:58.120\n it's a personal story I share with people\n\n08:58.120 --> 09:01.160\n or when I teach about AI or whatnot.\n\n09:01.160 --> 09:06.080\n My husband proposed to me by building a WALLI\n\n09:07.040 --> 09:09.700\n and he actuated it.\n\n09:09.700 --> 09:13.520\n So it's seven degrees of freedom, including the lens thing.\n\n09:13.520 --> 09:17.960\n And it kind of came in and it had the,\n\n09:17.960 --> 09:21.880\n he made it have like the belly box opening thing.\n\n09:21.880 --> 09:23.520\n So it just did that.\n\n09:23.520 --> 09:27.600\n And then it spewed out this box made out of Legos\n\n09:27.600 --> 09:31.200\n that open slowly and then bam, yeah.\n\n09:31.200 --> 09:34.360\n Yeah, it was quite, it set a bar.\n\n09:34.360 --> 09:37.620\n That could be like the most impressive thing I've ever heard.\n\n09:37.620 --> 09:39.080\n Okay.\n\n09:39.080 --> 09:40.980\n That was special connection to WALLI, long story short.\n\n09:40.980 --> 09:43.760\n I like WALLI because I like animation and I like robots\n\n09:43.760 --> 09:46.920\n and I like the fact that this was,\n\n09:46.920 --> 09:49.880\n we still have this robot to this day.\n\n09:49.880 --> 09:50.920\n How hard is that problem,\n\n09:50.920 --> 09:54.260\n do you think of the expressivity of robots?\n\n09:54.260 --> 09:59.000\n Like with the Boston Dynamics, I never talked to those folks\n\n09:59.000 --> 10:00.360\n about this particular element.\n\n10:00.360 --> 10:02.120\n I've talked to them a lot,\n\n10:02.120 --> 10:05.320\n but it seems to be like almost an accidental side effect\n\n10:05.320 --> 10:07.480\n for them that they weren't,\n\n10:07.480 --> 10:08.720\n I don't know if they're faking it.\n\n10:08.720 --> 10:11.740\n They weren't trying to, okay.\n\n10:11.740 --> 10:14.240\n They do say that the gripper,\n\n10:14.240 --> 10:16.620\n it was not intended to be a face.\n\n10:17.920 --> 10:20.400\n I don't know if that's a honest statement,\n\n10:20.400 --> 10:21.720\n but I think they're legitimate.\n\n10:21.720 --> 10:25.720\n Probably yes. And so do we automatically just\n\n10:25.720 --> 10:29.320\n anthropomorphize anything we can see about a robot?\n\n10:29.320 --> 10:30.720\n So like the question is,\n\n10:30.720 --> 10:33.680\n how hard is it to create a WALLI type robot\n\n10:33.680 --> 10:35.360\n that connects so deeply with us humans?\n\n10:35.360 --> 10:36.760\n What do you think?\n\n10:36.760 --> 10:37.880\n It's really hard, right?\n\n10:37.880 --> 10:39.980\n So it depends on what setting.\n\n10:39.980 --> 10:44.980\n So if you wanna do it in this very particular narrow setting\n\n10:45.760 --> 10:48.200\n where it does only one thing and it's expressive,\n\n10:48.200 --> 10:50.120\n then you can get an animator, you know,\n\n10:50.120 --> 10:52.100\n you can have Pixar on call come in,\n\n10:52.100 --> 10:53.520\n design some trajectories.\n\n10:53.520 --> 10:56.040\n There was a, Anki had a robot called Cosmo\n\n10:56.040 --> 10:58.360\n where they put in some of these animations.\n\n10:58.360 --> 11:00.520\n That part is easy, right?\n\n11:00.520 --> 11:04.320\n The hard part is doing it not via these\n\n11:04.320 --> 11:06.480\n kind of handcrafted behaviors,\n\n11:06.480 --> 11:09.820\n but doing it generally autonomously.\n\n11:09.820 --> 11:12.040\n Like I want robots, I don't work on,\n\n11:12.040 --> 11:14.680\n just to clarify, I don't, I used to work a lot on this.\n\n11:14.680 --> 11:17.360\n I don't work on that quite as much these days,\n\n11:17.360 --> 11:21.720\n but the notion of having robots that, you know,\n\n11:21.720 --> 11:24.320\n when they pick something up and put it in a place,\n\n11:24.320 --> 11:28.160\n they can do that with various forms of style,\n\n11:28.160 --> 11:30.200\n or you can say, well, this robot is, you know,\n\n11:30.200 --> 11:32.000\n succeeding at this task and is confident\n\n11:32.000 --> 11:34.080\n versus it's hesitant versus, you know,\n\n11:34.080 --> 11:35.920\n maybe it's happy or it's, you know,\n\n11:35.920 --> 11:38.800\n disappointed about something, some failure that it had.\n\n11:38.800 --> 11:41.320\n I think that when robots move,\n\n11:42.880 --> 11:46.840\n they can communicate so much about internal states\n\n11:46.840 --> 11:49.800\n or perceived internal states that they have.\n\n11:49.800 --> 11:53.320\n And I think that's really useful\n\n11:53.320 --> 11:55.520\n and an element that we'll want in the future\n\n11:55.520 --> 11:58.080\n because I was reading this article\n\n11:58.080 --> 12:02.260\n about how kids are,\n\n12:04.120 --> 12:07.360\n kids are being rude to Alexa\n\n12:07.360 --> 12:09.680\n because they can be rude to it\n\n12:09.680 --> 12:11.560\n and it doesn't really get angry, right?\n\n12:11.560 --> 12:15.200\n It doesn't reply in any way, it just says the same thing.\n\n12:15.200 --> 12:17.560\n So I think there's, at least for that,\n\n12:17.560 --> 12:20.040\n for the correct development of children,\n\n12:20.040 --> 12:21.480\n it's important that these things,\n\n12:21.480 --> 12:22.920\n you kind of react differently.\n\n12:22.920 --> 12:24.600\n I also think, you know, you walk in your home\n\n12:24.600 --> 12:27.160\n and you have a personal robot and if you're really pissed,\n\n12:27.160 --> 12:28.880\n presumably the robot should kind of behave\n\n12:28.880 --> 12:31.320\n slightly differently than when you're super happy\n\n12:31.320 --> 12:36.020\n and excited, but it's really hard because it's,\n\n12:36.020 --> 12:38.720\n I don't know, you know, the way I would think about it\n\n12:38.720 --> 12:40.840\n and the way I thought about it when it came to\n\n12:40.840 --> 12:44.080\n expressing goals or intentions for robots,\n\n12:44.080 --> 12:47.440\n it's, well, what's really happening is that\n\n12:47.440 --> 12:51.520\n instead of doing robotics where you have your state\n\n12:51.520 --> 12:55.600\n and you have your action space and you have your space,\n\n12:55.600 --> 12:57.840\n the reward function that you're trying to optimize,\n\n12:57.840 --> 13:00.560\n now you kind of have to expand the notion of state\n\n13:00.560 --> 13:02.780\n to include this human internal state.\n\n13:02.780 --> 13:05.920\n What is the person actually perceiving?\n\n13:05.920 --> 13:08.600\n What do they think about the robots?\n\n13:08.600 --> 13:10.160\n Something or rather,\n\n13:10.160 --> 13:12.760\n and then you have to optimize in that system.\n\n13:12.760 --> 13:14.120\n And so that means that you have to understand\n\n13:14.120 --> 13:17.960\n how your motion, your actions end up sort of influencing\n\n13:17.960 --> 13:20.980\n the observer's kind of perception of you.\n\n13:20.980 --> 13:25.040\n And it's very hard to write math about that.\n\n13:25.040 --> 13:27.140\n Right, so when you start to think about\n\n13:27.140 --> 13:29.760\n incorporating the human into the state model,\n\n13:31.560 --> 13:33.680\n apologize for the philosophical question,\n\n13:33.680 --> 13:36.440\n but how complicated are human beings, do you think?\n\n13:36.440 --> 13:40.740\n Like, can they be reduced to a kind of\n\n13:40.740 --> 13:43.740\n almost like an object that moves\n\n13:43.740 --> 13:46.160\n and maybe has some basic intents?\n\n13:46.160 --> 13:50.060\n Or is there something, do we have to model things like mood\n\n13:50.060 --> 13:52.780\n and general aggressiveness and time?\n\n13:52.780 --> 13:54.980\n I mean, all these kinds of human qualities\n\n13:54.980 --> 13:58.780\n or like game theoretic qualities, like what's your sense?\n\n13:58.780 --> 14:00.140\n How complicated is...\n\n14:00.140 --> 14:03.340\n How hard is the problem of human robot interaction?\n\n14:03.340 --> 14:05.260\n Yeah, should we talk about\n\n14:05.260 --> 14:07.780\n what the problem of human robot interaction is?\n\n14:07.780 --> 14:10.860\n Yeah, what is human robot interaction?\n\n14:10.860 --> 14:12.300\n And then talk about how that, yeah.\n\n14:12.300 --> 14:15.020\n So, and by the way, I'm gonna talk about\n\n14:15.020 --> 14:19.060\n this very particular view of human robot interaction, right?\n\n14:19.060 --> 14:21.620\n Which is not so much on the social side\n\n14:21.620 --> 14:24.540\n or on the side of how do you have a good conversation\n\n14:24.540 --> 14:26.780\n with the robot, what should the robot's appearance be?\n\n14:26.780 --> 14:29.220\n It turns out that if you make robots taller versus shorter,\n\n14:29.220 --> 14:31.900\n this has an effect on how people act with them.\n\n14:31.900 --> 14:34.660\n So I'm not talking about that.\n\n14:34.660 --> 14:36.260\n But I'm talking about this very kind of narrow thing,\n\n14:36.260 --> 14:39.900\n which is you take, if you wanna take a task\n\n14:39.900 --> 14:42.860\n that a robot can do in isolation,\n\n14:42.860 --> 14:46.580\n in a lab out there in the world, but in isolation,\n\n14:46.580 --> 14:49.740\n and now you're asking what does it mean for the robot\n\n14:49.740 --> 14:52.580\n to be able to do this task for,\n\n14:52.580 --> 14:54.300\n presumably what its actually end goal is,\n\n14:54.300 --> 14:55.900\n which is to help some person.\n\n14:56.740 --> 15:01.740\n That ends up changing the problem in two ways.\n\n15:02.940 --> 15:04.700\n The first way it changes the problem is that\n\n15:04.700 --> 15:08.580\n the robot is no longer the single agent acting.\n\n15:08.580 --> 15:10.980\n That you have humans who also take actions\n\n15:10.980 --> 15:12.140\n in that same space.\n\n15:12.140 --> 15:15.300\n Cars navigating around people, robots around an office,\n\n15:15.300 --> 15:18.580\n navigating around the people in that office.\n\n15:18.580 --> 15:20.900\n If I send the robot over there in the cafeteria\n\n15:20.900 --> 15:23.580\n to get me a coffee, then there's probably other people\n\n15:23.580 --> 15:25.340\n reaching for stuff in the same space.\n\n15:25.340 --> 15:28.580\n And so now you have your robot and you're in charge\n\n15:28.580 --> 15:30.580\n of the actions that the robot is taking.\n\n15:30.580 --> 15:33.500\n Then you have these people who are also making decisions\n\n15:33.500 --> 15:36.260\n and taking actions in that same space.\n\n15:36.260 --> 15:39.140\n And even if, you know, the robot knows what it should do\n\n15:39.140 --> 15:42.740\n and all of that, just coexisting with these people, right?\n\n15:42.740 --> 15:45.340\n Kind of getting the actions to gel well,\n\n15:45.340 --> 15:47.100\n to mesh well together.\n\n15:47.100 --> 15:50.500\n That's sort of the kind of problem number one.\n\n15:50.500 --> 15:51.660\n And then there's problem number two,\n\n15:51.660 --> 15:56.660\n which is, goes back to this notion of if I'm a programmer,\n\n15:58.220 --> 16:00.900\n I can specify some objective for the robot\n\n16:00.900 --> 16:03.820\n to go off and optimize and specify the task.\n\n16:03.820 --> 16:07.340\n But if I put the robot in your home,\n\n16:07.340 --> 16:11.420\n presumably you might have your own opinions about,\n\n16:11.420 --> 16:12.860\n well, okay, I want my house clean,\n\n16:12.860 --> 16:14.060\n but how do I want it cleaned?\n\n16:14.060 --> 16:16.340\n And how should robot move, how close to me it should come\n\n16:16.340 --> 16:17.340\n and all of that.\n\n16:17.340 --> 16:20.380\n And so I think those are the two differences that you have.\n\n16:20.380 --> 16:24.940\n You're acting around people and what you should be\n\n16:24.940 --> 16:27.500\n optimizing for should satisfy the preferences\n\n16:27.500 --> 16:30.860\n of that end user, not of your programmer who programmed you.\n\n16:30.860 --> 16:33.780\n Yeah, and the preferences thing is tricky.\n\n16:33.780 --> 16:35.700\n So figuring out those preferences,\n\n16:35.700 --> 16:38.340\n be able to interactively adjust\n\n16:38.340 --> 16:39.860\n to understand what the human is doing.\n\n16:39.860 --> 16:42.260\n So really it boils down to understand the humans\n\n16:42.260 --> 16:45.860\n in order to interact with them and in order to please them.\n\n16:45.860 --> 16:47.100\n Right.\n\n16:47.100 --> 16:48.420\n So why is this hard?\n\n16:48.420 --> 16:51.100\n Yeah, why is understanding humans hard?\n\n16:51.100 --> 16:56.100\n So I think there's two tasks about understanding humans\n\n16:57.980 --> 16:59.940\n that in my mind are very, very similar,\n\n16:59.940 --> 17:00.980\n but not everyone agrees.\n\n17:00.980 --> 17:04.460\n So there's the task of being able to just anticipate\n\n17:04.460 --> 17:05.740\n what people will do.\n\n17:05.740 --> 17:07.620\n We all know that cars need to do this, right?\n\n17:07.620 --> 17:10.580\n We all know that, well, if I navigate around some people,\n\n17:10.580 --> 17:12.580\n the robot has to get some notion of,\n\n17:12.580 --> 17:15.500\n okay, where is this person gonna be?\n\n17:15.500 --> 17:17.340\n So that's kind of the prediction side.\n\n17:17.340 --> 17:19.260\n And then there's what you were saying,\n\n17:19.260 --> 17:21.060\n satisfying the preferences, right?\n\n17:21.060 --> 17:22.820\n So adapting to the person's preferences,\n\n17:22.820 --> 17:24.500\n knowing what to optimize for,\n\n17:24.500 --> 17:25.900\n which is more this inference side,\n\n17:25.900 --> 17:28.820\n this what does this person want?\n\n17:28.820 --> 17:31.580\n What is their intent? What are their preferences?\n\n17:31.580 --> 17:35.100\n And to me, those kind of go together\n\n17:35.100 --> 17:39.700\n because I think that at the very least,\n\n17:39.700 --> 17:42.980\n if you can understand, if you can look at human behavior\n\n17:42.980 --> 17:45.500\n and understand what it is that they want,\n\n17:45.500 --> 17:47.380\n then that's sort of the key enabler\n\n17:47.380 --> 17:50.660\n to being able to anticipate what they'll do in the future.\n\n17:50.660 --> 17:53.580\n Because I think that we're not arbitrary.\n\n17:53.580 --> 17:55.380\n We make these decisions that we make,\n\n17:55.380 --> 17:56.940\n we act in the way we do\n\n17:56.940 --> 17:59.340\n because we're trying to achieve certain things.\n\n17:59.340 --> 18:01.540\n And so I think that's the relationship between them.\n\n18:01.540 --> 18:05.540\n Now, how complicated do these models need to be\n\n18:05.540 --> 18:10.140\n in order to be able to understand what people want?\n\n18:10.140 --> 18:15.140\n So we've gotten a long way in robotics\n\n18:15.180 --> 18:17.540\n with something called inverse reinforcement learning,\n\n18:17.540 --> 18:19.500\n which is the notion of if someone acts,\n\n18:19.500 --> 18:22.100\n demonstrates how they want the thing done.\n\n18:22.100 --> 18:24.220\n What is inverse reinforcement learning?\n\n18:24.220 --> 18:25.220\n You just briefly said it.\n\n18:25.220 --> 18:30.220\n Right, so it's the problem of take human behavior\n\n18:30.220 --> 18:33.260\n and infer reward function from this.\n\n18:33.260 --> 18:34.500\n So figure out what it is\n\n18:34.500 --> 18:37.420\n that that behavior is optimal with respect to.\n\n18:37.420 --> 18:38.700\n And it's a great way to think\n\n18:38.700 --> 18:40.260\n about learning human preferences\n\n18:40.260 --> 18:45.260\n in the sense of you have a car and the person can drive it\n\n18:45.300 --> 18:46.900\n and then you can say, well, okay,\n\n18:46.900 --> 18:50.660\n I can actually learn what the person is optimizing for.\n\n18:51.940 --> 18:53.460\n I can learn their driving style,\n\n18:53.460 --> 18:55.620\n or you can have people demonstrate\n\n18:55.620 --> 18:57.300\n how they want the house clean.\n\n18:57.300 --> 18:59.820\n And then you can say, okay, this is,\n\n18:59.820 --> 19:02.980\n I'm getting the trade offs that they're making.\n\n19:02.980 --> 19:06.140\n I'm getting the preferences that they want out of this.\n\n19:06.140 --> 19:10.300\n And so we've been successful in robotics somewhat with this.\n\n19:10.300 --> 19:15.020\n And it's based on a very simple model of human behavior.\n\n19:15.020 --> 19:16.340\n It was remarkably simple,\n\n19:16.340 --> 19:18.660\n which is that human behavior is optimal\n\n19:18.660 --> 19:22.020\n with respect to whatever it is that people want, right?\n\n19:22.020 --> 19:23.100\n So you make that assumption\n\n19:23.100 --> 19:24.380\n and now you can kind of inverse through.\n\n19:24.380 --> 19:25.900\n That's why it's called inverse,\n\n19:25.900 --> 19:27.220\n well, really optimal control,\n\n19:27.220 --> 19:30.540\n but also inverse reinforcement learning.\n\n19:30.540 --> 19:35.540\n So this is based on utility maximization in economics.\n\n19:36.460 --> 19:39.500\n Back in the forties, von Neumann and Morgenstern\n\n19:39.500 --> 19:43.020\n were like, okay, people are making choices\n\n19:43.020 --> 19:45.740\n by maximizing utility, go.\n\n19:45.740 --> 19:48.380\n And then in the late fifties,\n\n19:48.380 --> 19:52.460\n we had Luce and Shepherd come in and say,\n\n19:52.460 --> 19:57.460\n people are a little bit noisy and approximate in that process.\n\n19:57.860 --> 20:01.580\n So they might choose something kind of stochastically\n\n20:01.580 --> 20:03.940\n with probability proportional to\n\n20:03.940 --> 20:07.060\n how much utility something has.\n\n20:07.060 --> 20:09.620\n So there's a bit of noise in there.\n\n20:09.620 --> 20:11.740\n This has translated into robotics\n\n20:11.740 --> 20:14.180\n and something that we call Boltzmann rationality.\n\n20:14.180 --> 20:15.700\n So it's a kind of an evolution\n\n20:15.700 --> 20:16.780\n of inverse reinforcement learning\n\n20:16.780 --> 20:19.620\n that accounts for human noise.\n\n20:19.620 --> 20:21.980\n And we've had some success with that too,\n\n20:21.980 --> 20:23.860\n for these tasks where it turns out\n\n20:23.860 --> 20:28.340\n people act noisily enough that you can't just do vanilla,\n\n20:28.340 --> 20:29.900\n the vanilla version.\n\n20:29.900 --> 20:31.020\n You can account for noise\n\n20:31.020 --> 20:36.020\n and still infer what they seem to want based on this.\n\n20:36.460 --> 20:39.940\n Then now we're hitting tasks where that's not enough.\n\n20:39.940 --> 20:41.260\n And because...\n\n20:41.260 --> 20:43.620\n What are examples of spatial tasks?\n\n20:43.620 --> 20:45.900\n So imagine you're trying to control some robot,\n\n20:45.900 --> 20:47.820\n that's fairly complicated.\n\n20:47.820 --> 20:49.220\n You're trying to control a robot arm\n\n20:49.220 --> 20:52.580\n because maybe you're a patient with a motor impairment\n\n20:52.580 --> 20:53.860\n and you have this wheelchair mounted arm\n\n20:53.860 --> 20:56.260\n and you're trying to control it around.\n\n20:56.260 --> 21:00.700\n Or one task that we've looked at with Sergei is,\n\n21:00.700 --> 21:02.860\n and our students did, is a lunar lander.\n\n21:02.860 --> 21:05.060\n So I don't know if you know this Atari game,\n\n21:05.060 --> 21:06.820\n it's called Lunar Lander.\n\n21:06.820 --> 21:07.660\n It's really hard.\n\n21:07.660 --> 21:09.740\n People really suck at landing the thing.\n\n21:09.740 --> 21:11.860\n Mostly they just crash it left and right.\n\n21:11.860 --> 21:14.300\n Okay, so this is the kind of task we imagine\n\n21:14.300 --> 21:16.980\n you're trying to provide some assistance\n\n21:16.980 --> 21:20.180\n to a person operating such a robot\n\n21:20.180 --> 21:21.980\n where you want the kind of the autonomy to kick in,\n\n21:21.980 --> 21:23.460\n figure out what it is that you're trying to do\n\n21:23.460 --> 21:24.860\n and help you do it.\n\n21:25.900 --> 21:30.700\n It's really hard to do that for, say, Lunar Lander\n\n21:30.700 --> 21:32.940\n because people are all over the place.\n\n21:32.940 --> 21:36.700\n And so they seem much more noisy than really irrational.\n\n21:36.700 --> 21:37.900\n That's an example of a task\n\n21:37.900 --> 21:40.220\n where these models are kind of failing us.\n\n21:41.220 --> 21:43.500\n And it's not surprising because\n\n21:43.500 --> 21:47.020\n we're talking about the 40s, utility, late 50s,\n\n21:47.020 --> 21:48.900\n sort of noisy.\n\n21:48.900 --> 21:52.340\n Then the 70s came and behavioral economics\n\n21:52.340 --> 21:54.620\n started being a thing where people were like,\n\n21:54.620 --> 21:58.140\n no, no, no, no, no, people are not rational.\n\n21:58.140 --> 22:03.140\n People are messy and emotional and irrational\n\n22:03.300 --> 22:05.340\n and have all sorts of heuristics\n\n22:05.340 --> 22:06.980\n that might be domain specific.\n\n22:06.980 --> 22:08.580\n And they're just a mess.\n\n22:08.580 --> 22:09.420\n The mess.\n\n22:09.420 --> 22:13.180\n So what does my robot do to understand\n\n22:13.180 --> 22:14.740\n what you want?\n\n22:14.740 --> 22:18.020\n And it's a very, it's very, that's why it's complicated.\n\n22:18.020 --> 22:19.580\n It's, you know, for the most part,\n\n22:19.580 --> 22:23.300\n we get away with pretty simple models until we don't.\n\n22:23.300 --> 22:25.540\n And then the question is, what do you do then?\n\n22:26.580 --> 22:30.180\n And I had days when I wanted to, you know,\n\n22:30.180 --> 22:32.780\n pack my bags and go home and switch jobs\n\n22:32.780 --> 22:35.020\n because it's just, it feels really daunting\n\n22:35.020 --> 22:37.300\n to make sense of human behavior enough\n\n22:37.300 --> 22:40.540\n that you can reliably understand what people want,\n\n22:40.540 --> 22:41.380\n especially as, you know,\n\n22:41.380 --> 22:44.940\n robot capabilities will continue to get developed.\n\n22:44.940 --> 22:47.180\n You'll get these systems that are more and more capable\n\n22:47.180 --> 22:48.060\n of all sorts of things.\n\n22:48.060 --> 22:49.140\n And then you really want to make sure\n\n22:49.140 --> 22:51.500\n that you're telling them the right thing to do.\n\n22:51.500 --> 22:52.620\n What is that thing?\n\n22:52.620 --> 22:55.140\n Well, read it in human behavior.\n\n22:56.100 --> 22:58.460\n So if I just sat here quietly\n\n22:58.460 --> 23:00.380\n and tried to understand something about you\n\n23:00.380 --> 23:02.140\n by listening to you talk,\n\n23:02.140 --> 23:06.140\n it would be harder than if I got to say something\n\n23:06.140 --> 23:08.780\n and ask you and interact and control.\n\n23:08.780 --> 23:13.140\n Can you, can the robot help its understanding of the human\n\n23:13.140 --> 23:18.140\n by influencing the behavior by actually acting?\n\n23:18.540 --> 23:19.780\n Yeah, absolutely.\n\n23:19.780 --> 23:23.660\n So one of the things that's been exciting to me lately\n\n23:23.660 --> 23:28.660\n is this notion that when you try to,\n\n23:28.780 --> 23:31.940\n that when you try to think of the robotics problem as,\n\n23:31.940 --> 23:34.500\n okay, I have a robot and it needs to optimize\n\n23:34.500 --> 23:37.540\n for whatever it is that a person wants it to optimize\n\n23:37.540 --> 23:39.740\n as opposed to maybe what a programmer said.\n\n23:40.700 --> 23:44.700\n That problem we think of as a human robot\n\n23:44.700 --> 23:49.140\n collaboration problem in which both agents get to act\n\n23:49.140 --> 23:52.300\n in which the robot knows less than the human\n\n23:52.300 --> 23:54.660\n because the human actually has access to,\n\n23:54.660 --> 23:57.220\n you know, at least implicitly to what it is that they want.\n\n23:57.220 --> 24:00.660\n They can't write it down, but they can talk about it.\n\n24:00.660 --> 24:02.300\n They can give all sorts of signals.\n\n24:02.300 --> 24:04.460\n They can demonstrate and,\n\n24:04.460 --> 24:06.540\n but the robot doesn't need to sit there\n\n24:06.540 --> 24:08.780\n and passively observe human behavior\n\n24:08.780 --> 24:10.100\n and try to make sense of it.\n\n24:10.100 --> 24:11.900\n The robot can act too.\n\n24:11.900 --> 24:15.380\n And so there's these information gathering actions\n\n24:15.380 --> 24:19.020\n that the robot can take to sort of solicit responses\n\n24:19.020 --> 24:21.060\n that are actually informative.\n\n24:21.060 --> 24:22.980\n So for instance, this is not for the purpose\n\n24:22.980 --> 24:25.580\n of assisting people, but with kind of back to coordinating\n\n24:25.580 --> 24:27.420\n with people in cars and all of that.\n\n24:27.420 --> 24:31.860\n One thing that Dorsa did was,\n\n24:31.860 --> 24:34.260\n so we were looking at cars being able to navigate\n\n24:34.260 --> 24:39.260\n around people and you might not know exactly\n\n24:39.500 --> 24:41.860\n the driving style of a particular individual\n\n24:41.860 --> 24:43.020\n that's next to you,\n\n24:43.020 --> 24:45.260\n but you wanna change lanes in front of them.\n\n24:45.260 --> 24:48.780\n Navigating around other humans inside cars.\n\n24:48.780 --> 24:50.940\n Yeah, good, good clarification question.\n\n24:50.940 --> 24:55.860\n So you have an autonomous car and it's trying to navigate\n\n24:55.860 --> 24:58.980\n the road around human driven vehicles.\n\n24:58.980 --> 25:01.620\n Similar things ideas apply to pedestrians as well,\n\n25:01.620 --> 25:03.900\n but let's just take human driven vehicles.\n\n25:03.900 --> 25:06.220\n So now you're trying to change a lane.\n\n25:06.220 --> 25:10.460\n Well, you could be trying to infer the driving style\n\n25:10.460 --> 25:12.180\n of this person next to you.\n\n25:12.180 --> 25:13.780\n You'd like to know if they're in particular,\n\n25:13.780 --> 25:15.940\n if they're sort of aggressive or defensive,\n\n25:15.940 --> 25:18.020\n if they're gonna let you kind of go in\n\n25:18.020 --> 25:20.300\n or if they're gonna not.\n\n25:20.300 --> 25:24.340\n And it's very difficult to just,\n\n25:25.900 --> 25:27.940\n if you think that if you wanna hedge your bets\n\n25:27.940 --> 25:30.340\n and say, ah, maybe they're actually pretty aggressive,\n\n25:30.340 --> 25:31.580\n I shouldn't try this.\n\n25:31.580 --> 25:33.420\n You kind of end up driving next to them\n\n25:33.420 --> 25:34.860\n and driving next to them, right?\n\n25:34.860 --> 25:36.460\n And then you don't know\n\n25:36.460 --> 25:39.380\n because you're not actually getting the observations\n\n25:39.380 --> 25:40.220\n that you're getting away.\n\n25:40.220 --> 25:42.620\n Someone drives when they're next to you\n\n25:42.620 --> 25:44.420\n and they just need to go straight.\n\n25:44.420 --> 25:45.260\n It's kind of the same\n\n25:45.260 --> 25:47.460\n regardless if they're aggressive or defensive.\n\n25:47.460 --> 25:51.020\n And so you need to enable the robot\n\n25:51.020 --> 25:54.220\n to reason about how it might actually be able\n\n25:54.220 --> 25:57.020\n to gather information by changing the actions\n\n25:57.020 --> 25:58.140\n that it's taking.\n\n25:58.140 --> 25:59.940\n And then the robot comes up with these cool things\n\n25:59.940 --> 26:02.580\n where it kind of nudges towards you\n\n26:02.580 --> 26:05.260\n and then sees if you're gonna slow down or not.\n\n26:05.260 --> 26:06.260\n Then if you slow down,\n\n26:06.260 --> 26:07.940\n it sort of updates its model of you\n\n26:07.940 --> 26:11.340\n and says, oh, okay, you're more on the defensive side.\n\n26:11.340 --> 26:12.740\n So now I can actually like.\n\n26:12.740 --> 26:14.340\n That's a fascinating dance.\n\n26:14.340 --> 26:18.100\n That's so cool that you could use your own actions\n\n26:18.100 --> 26:19.380\n to gather information.\n\n26:19.380 --> 26:22.380\n That feels like a totally open,\n\n26:22.380 --> 26:24.380\n exciting new world of robotics.\n\n26:24.380 --> 26:26.100\n I mean, how many people are even thinking\n\n26:26.100 --> 26:28.660\n about that kind of thing?\n\n26:28.660 --> 26:30.260\n A handful of us, I'd say.\n\n26:30.260 --> 26:33.380\n It's rare because it's actually leveraging human.\n\n26:33.380 --> 26:34.620\n I mean, most roboticists,\n\n26:34.620 --> 26:38.220\n I've talked to a lot of colleagues and so on,\n\n26:38.220 --> 26:42.980\n are kind of, being honest, kind of afraid of humans.\n\n26:42.980 --> 26:45.460\n Because they're messy and complicated, right?\n\n26:45.460 --> 26:46.700\n I understand.\n\n26:47.900 --> 26:49.820\n Going back to what we were talking about earlier,\n\n26:49.820 --> 26:52.500\n right now we're kind of in this dilemma of, okay,\n\n26:52.500 --> 26:54.020\n there are tasks that we can just assume\n\n26:54.020 --> 26:55.700\n people are approximately rational for\n\n26:55.700 --> 26:57.140\n and we can figure out what they want.\n\n26:57.140 --> 26:57.980\n We can figure out their goals.\n\n26:57.980 --> 26:59.740\n We can figure out their driving styles, whatever.\n\n26:59.740 --> 27:00.580\n Cool.\n\n27:00.580 --> 27:02.860\n There are these tasks that we can't.\n\n27:02.860 --> 27:03.980\n So what do we do, right?\n\n27:03.980 --> 27:06.060\n Do we pack our bags and go home?\n\n27:06.060 --> 27:11.060\n And this one, I've had a little bit of hope recently.\n\n27:12.340 --> 27:13.740\n And I'm kind of doubting myself\n\n27:13.740 --> 27:15.500\n because what do I know that, you know,\n\n27:15.500 --> 27:19.620\n 50 years of behavioral economics hasn't figured out.\n\n27:19.620 --> 27:21.500\n But maybe it's not really in contradiction\n\n27:21.500 --> 27:23.940\n with the way that field is headed.\n\n27:23.940 --> 27:27.980\n But basically one thing that we've been thinking about is,\n\n27:27.980 --> 27:30.180\n instead of kind of giving up and saying\n\n27:30.180 --> 27:32.020\n people are too crazy and irrational\n\n27:32.020 --> 27:33.500\n for us to make sense of them,\n\n27:34.460 --> 27:39.380\n maybe we can give them a bit the benefit of the doubt.\n\n27:39.380 --> 27:41.420\n And maybe we can think of them\n\n27:41.420 --> 27:43.980\n as actually being relatively rational,\n\n27:43.980 --> 27:48.980\n but just under different assumptions about the world,\n\n27:48.980 --> 27:51.580\n about how the world works, about, you know,\n\n27:51.580 --> 27:54.100\n they don't have, when we think about rationality,\n\n27:54.100 --> 27:56.500\n implicit assumption is, oh, they're rational,\n\n27:56.500 --> 27:58.580\n and they're all the same assumptions and constraints\n\n27:58.580 --> 27:59.940\n as the robot, right?\n\n27:59.940 --> 28:01.820\n What, if this is the state of the world,\n\n28:01.820 --> 28:02.740\n that's what they know.\n\n28:02.740 --> 28:05.140\n This is the transition function, that's what they know.\n\n28:05.140 --> 28:07.380\n This is the horizon, that's what they know.\n\n28:07.380 --> 28:11.060\n But maybe the kind of this difference,\n\n28:11.060 --> 28:13.820\n the way, the reason they can seem a little messy\n\n28:13.820 --> 28:16.500\n and hectic, especially to robots,\n\n28:16.500 --> 28:20.060\n is that perhaps they just make different assumptions\n\n28:20.060 --> 28:21.660\n or have different beliefs.\n\n28:21.660 --> 28:24.820\n Yeah, I mean, that's another fascinating idea\n\n28:24.820 --> 28:29.060\n that this, our kind of anecdotal desire\n\n28:29.060 --> 28:31.060\n to say that humans are irrational,\n\n28:31.060 --> 28:33.300\n perhaps grounded in behavioral economics,\n\n28:33.300 --> 28:36.420\n is that we just don't understand the constraints\n\n28:36.420 --> 28:38.300\n and the rewards under which they operate.\n\n28:38.300 --> 28:40.980\n And so our goal shouldn't be to throw our hands up\n\n28:40.980 --> 28:42.420\n and say they're irrational,\n\n28:42.420 --> 28:44.940\n it's to say, let's try to understand\n\n28:44.940 --> 28:46.420\n what are the constraints.\n\n28:46.420 --> 28:48.420\n What it is that they must be assuming\n\n28:48.420 --> 28:51.140\n that makes this behavior make sense.\n\n28:51.140 --> 28:52.620\n Good life lesson, right?\n\n28:52.620 --> 28:53.460\n Good life lesson.\n\n28:53.460 --> 28:55.580\n That's true, it's just outside of robotics.\n\n28:55.580 --> 28:58.500\n That's just good to, that's communicating with humans.\n\n28:58.500 --> 29:00.780\n That's just a good assume\n\n29:00.780 --> 29:03.340\n that you just don't, sort of empathy, right?\n\n29:03.340 --> 29:04.420\n It's a...\n\n29:04.420 --> 29:06.020\n This is maybe there's something you're missing\n\n29:06.020 --> 29:08.580\n and it's, you know, it especially happens to robots\n\n29:08.580 --> 29:10.220\n cause they're kind of dumb and they don't know things.\n\n29:10.220 --> 29:12.740\n And oftentimes people are sort of supra rational\n\n29:12.740 --> 29:14.300\n and that they actually know a lot of things\n\n29:14.300 --> 29:15.420\n that robots don't.\n\n29:15.420 --> 29:17.860\n Sometimes like with the lunar lander,\n\n29:17.860 --> 29:20.540\n the robot, you know, knows much more.\n\n29:20.540 --> 29:23.980\n So it turns out that if you try to say,\n\n29:23.980 --> 29:26.940\n look, maybe people are operating this thing\n\n29:26.940 --> 29:31.100\n but assuming a much more simplified physics model\n\n29:31.100 --> 29:33.900\n cause they don't get the complexity of this kind of craft\n\n29:33.900 --> 29:36.100\n or the robot arm with seven degrees of freedom\n\n29:36.100 --> 29:38.420\n with these inertias and whatever.\n\n29:38.420 --> 29:41.580\n So maybe they have this intuitive physics model\n\n29:41.580 --> 29:44.260\n which is not, you know, this notion of intuitive physics\n\n29:44.260 --> 29:46.620\n is something that you studied actually in cognitive science\n\n29:46.620 --> 29:49.900\n was like Josh Denenbaum, Tom Griffith's work on this stuff.\n\n29:49.900 --> 29:54.700\n And what we found is that you can actually try\n\n29:54.700 --> 29:58.420\n to figure out what physics model\n\n29:58.420 --> 30:01.380\n kind of best explains human actions.\n\n30:01.380 --> 30:06.380\n And then you can use that to sort of correct what it is\n\n30:06.460 --> 30:08.820\n that they're commanding the craft to do.\n\n30:08.820 --> 30:11.420\n So they might, you know, be sending the craft somewhere\n\n30:11.420 --> 30:13.340\n but instead of executing that action,\n\n30:13.340 --> 30:15.260\n you can sort of take a step back and say,\n\n30:15.260 --> 30:16.900\n according to their intuitive,\n\n30:16.900 --> 30:20.100\n if the world worked according to their intuitive physics\n\n30:20.100 --> 30:23.620\n model, where do they think that the craft is going?\n\n30:23.620 --> 30:26.020\n Where are they trying to send it to?\n\n30:26.020 --> 30:28.620\n And then you can use the real physics, right?\n\n30:28.620 --> 30:30.220\n The inverse of that to actually figure out\n\n30:30.220 --> 30:31.540\n what you should do so that you do that\n\n30:31.540 --> 30:33.380\n instead of where they were actually sending you\n\n30:33.380 --> 30:34.820\n in the real world.\n\n30:34.820 --> 30:38.300\n And I kid you not at work people land the damn thing\n\n30:38.300 --> 30:42.460\n and you know, in between the two flags and all that.\n\n30:42.460 --> 30:45.180\n So it's not conclusive in any way\n\n30:45.180 --> 30:47.300\n but I'd say it's evidence that yeah,\n\n30:47.300 --> 30:50.420\n maybe we're kind of underestimating humans in some ways\n\n30:50.420 --> 30:51.620\n when we're giving up and saying,\n\n30:51.620 --> 30:53.220\n yeah, they're just crazy noisy.\n\n30:53.220 --> 30:56.300\n So then you try to explicitly try to model\n\n30:56.300 --> 30:58.140\n the kind of worldview that they have.\n\n30:58.140 --> 30:59.620\n That they have, that's right.\n\n30:59.620 --> 31:00.460\n That's right.\n\n31:00.460 --> 31:02.260\n And it's not too, I mean,\n\n31:02.260 --> 31:03.620\n there's things in behavior economics too\n\n31:03.620 --> 31:06.940\n that for instance have touched upon the planning horizon.\n\n31:06.940 --> 31:09.900\n So there's this idea that there's bounded rationality\n\n31:09.900 --> 31:11.380\n essentially and the idea that, well,\n\n31:11.380 --> 31:13.660\n maybe we work under computational constraints.\n\n31:13.660 --> 31:17.020\n And I think kind of our view recently has been\n\n31:17.020 --> 31:19.740\n take the Bellman update in AI\n\n31:19.740 --> 31:22.580\n and just break it in all sorts of ways by saying state,\n\n31:22.580 --> 31:25.020\n no, no, no, the person doesn't get to see the real state.\n\n31:25.020 --> 31:26.540\n Maybe they're estimating somehow.\n\n31:26.540 --> 31:28.860\n Transition function, no, no, no, no, no.\n\n31:28.860 --> 31:31.580\n Even the actual reward evaluation,\n\n31:31.580 --> 31:32.740\n maybe they're still learning\n\n31:32.740 --> 31:34.860\n about what it is that they want.\n\n31:34.860 --> 31:37.740\n Like, you know, when you watch Netflix\n\n31:37.740 --> 31:39.420\n and you know, you have all the things\n\n31:39.420 --> 31:41.700\n and then you have to pick something,\n\n31:41.700 --> 31:46.180\n imagine that, you know, the AI system interpreted\n\n31:46.180 --> 31:48.860\n that choice as this is the thing you prefer to see.\n\n31:48.860 --> 31:49.700\n Like, how are you going to know?\n\n31:49.700 --> 31:51.340\n You're still trying to figure out what you like,\n\n31:51.340 --> 31:52.620\n what you don't like, et cetera.\n\n31:52.620 --> 31:55.540\n So I think it's important to also account for that.\n\n31:55.540 --> 31:56.780\n So it's not irrationality,\n\n31:56.780 --> 31:58.100\n because they're doing the right thing\n\n31:58.100 --> 31:59.980\n under the things that they know.\n\n31:59.980 --> 32:01.300\n Yeah, that's brilliant.\n\n32:01.300 --> 32:03.260\n You mentioned recommender systems.\n\n32:03.260 --> 32:05.340\n What kind of, and we were talking\n\n32:05.340 --> 32:07.140\n about human robot interaction,\n\n32:07.140 --> 32:10.820\n what kind of problem spaces are you thinking about?\n\n32:10.820 --> 32:14.900\n So is it robots, like wheeled robots\n\n32:14.900 --> 32:16.020\n with autonomous vehicles?\n\n32:16.020 --> 32:18.580\n Is it object manipulation?\n\n32:18.580 --> 32:19.460\n Like when you think\n\n32:19.460 --> 32:21.940\n about human robot interaction in your mind,\n\n32:21.940 --> 32:24.460\n and maybe I'm sure you can speak\n\n32:24.460 --> 32:27.820\n for the entire community of human robot interaction.\n\n32:27.820 --> 32:30.540\n But like, what are the problems of interest here?\n\n32:30.540 --> 32:34.500\n And does it, you know, I kind of think\n\n32:34.500 --> 32:40.860\n of open domain dialogue as human robot interaction,\n\n32:40.860 --> 32:43.060\n and that happens not in the physical space,\n\n32:43.060 --> 32:46.380\n but it could just happen in the virtual space.\n\n32:46.380 --> 32:49.580\n So where's the boundaries of this field for you\n\n32:49.580 --> 32:50.780\n when you're thinking about the things\n\n32:50.780 --> 32:51.860\n we've been talking about?\n\n32:51.860 --> 33:00.740\n Yeah, so I try to find kind of underlying,\n\n33:00.740 --> 33:02.500\n I don't know what to even call them.\n\n33:02.500 --> 33:05.060\n I try to work on, you know, I might call what I do,\n\n33:05.060 --> 33:07.620\n the kind of working on the foundations\n\n33:07.620 --> 33:09.580\n of algorithmic human robot interaction\n\n33:09.580 --> 33:12.780\n and trying to make contributions there.\n\n33:12.780 --> 33:15.940\n And it's important to me that whatever we do\n\n33:15.940 --> 33:19.340\n is actually somewhat domain agnostic when it comes to,\n\n33:19.340 --> 33:23.980\n is it about, you know, autonomous cars\n\n33:23.980 --> 33:27.780\n or is it about quadrotors or is it about,\n\n33:27.780 --> 33:30.780\n is this sort of the same underlying principles apply?\n\n33:30.780 --> 33:31.660\n Of course, when you're trying to get\n\n33:31.660 --> 33:32.900\n a particular domain to work,\n\n33:32.900 --> 33:34.260\n you usually have to do some extra work\n\n33:34.260 --> 33:36.580\n to adapt that to that particular domain.\n\n33:36.580 --> 33:40.020\n But these things that we were talking about around,\n\n33:40.020 --> 33:42.420\n well, you know, how do you model humans?\n\n33:42.420 --> 33:44.260\n It turns out that a lot of systems need\n\n33:44.260 --> 33:47.260\n to core benefit from a better understanding\n\n33:47.260 --> 33:50.940\n of how human behavior relates to what people want\n\n33:50.940 --> 33:53.540\n and need to predict human behavior,\n\n33:53.540 --> 33:56.420\n physical robots of all sorts and beyond that.\n\n33:56.420 --> 33:58.540\n And so I used to do manipulation.\n\n33:58.540 --> 34:00.620\n I used to be, you know, picking up stuff\n\n34:00.620 --> 34:03.340\n and then I was picking up stuff with people around.\n\n34:03.340 --> 34:05.940\n And now it's sort of very broad\n\n34:05.940 --> 34:07.820\n when it comes to the application level,\n\n34:07.820 --> 34:11.140\n but in a sense, very focused on, okay,\n\n34:11.140 --> 34:14.060\n how does the problem need to change?\n\n34:14.060 --> 34:15.860\n How do the algorithms need to change\n\n34:15.860 --> 34:19.980\n when we're not doing a robot by itself?\n\n34:19.980 --> 34:21.380\n You know, emptying the dishwasher,\n\n34:21.380 --> 34:23.780\n but we're stepping outside of that.\n\n34:23.780 --> 34:26.820\n I thought that popped into my head just now.\n\n34:26.820 --> 34:27.860\n On the game theoretic side,\n\n34:27.860 --> 34:29.900\n I think you said this really interesting idea\n\n34:29.900 --> 34:33.300\n of using actions to gain more information.\n\n34:33.300 --> 34:37.780\n But if we think of sort of game theory,\n\n34:39.780 --> 34:43.420\n the humans that are interacting with you,\n\n34:43.420 --> 34:44.540\n with you, the robot?\n\n34:44.540 --> 34:46.420\n Wow, I'm thinking the identity of the robot.\n\n34:46.420 --> 34:47.460\n Yeah, I do that all the time.\n\n34:47.460 --> 34:52.460\n Yeah, is they also have a world model of you\n\n34:55.540 --> 34:57.420\n and you can manipulate that.\n\n34:57.420 --> 34:59.340\n I mean, if we look at autonomous vehicles,\n\n34:59.340 --> 35:01.420\n people have a certain viewpoint.\n\n35:01.420 --> 35:06.260\n You said with the kids, people see Alexa in a certain way.\n\n35:07.260 --> 35:10.860\n Is there some value in trying to also optimize\n\n35:10.860 --> 35:13.540\n how people see you as a robot?\n\n35:15.100 --> 35:20.100\n Or is that a little too far away from the specifics\n\n35:20.140 --> 35:21.620\n of what we can solve right now?\n\n35:21.620 --> 35:24.340\n So, well, both, right?\n\n35:24.340 --> 35:26.300\n So it's really interesting.\n\n35:26.300 --> 35:30.940\n And we've seen a little bit of progress on this problem,\n\n35:30.940 --> 35:32.340\n on pieces of this problem.\n\n35:32.340 --> 35:36.220\n So you can, again, it kind of comes down\n\n35:36.220 --> 35:38.260\n to how complicated does the human model need to be?\n\n35:38.260 --> 35:42.300\n But in one piece of work that we were looking at,\n\n35:42.300 --> 35:46.180\n we just said, okay, there's these parameters\n\n35:46.180 --> 35:47.900\n that are internal to the robot\n\n35:47.900 --> 35:51.620\n and what the robot is about to do,\n\n35:51.620 --> 35:52.700\n or maybe what objective,\n\n35:52.700 --> 35:55.260\n what driving style the robot has or something like that.\n\n35:55.260 --> 35:58.180\n And what we're gonna do is we're gonna set up a system\n\n35:58.180 --> 36:00.300\n where part of the state is the person's belief\n\n36:00.300 --> 36:02.300\n over those parameters.\n\n36:02.300 --> 36:05.180\n And now when the robot acts,\n\n36:05.180 --> 36:07.580\n that the person gets new evidence\n\n36:07.580 --> 36:10.700\n about this robot internal state.\n\n36:10.700 --> 36:13.700\n And so they're updating their mental model of the robot.\n\n36:13.700 --> 36:16.940\n So if they see a car that sort of cuts someone off,\n\n36:16.940 --> 36:18.340\n they're like, oh, that's an aggressive car.\n\n36:18.340 --> 36:19.180\n They know more.\n\n36:20.700 --> 36:24.100\n If they see sort of a robot head towards a particular door,\n\n36:24.100 --> 36:25.500\n they're like, oh yeah, the robot's trying to get\n\n36:25.500 --> 36:26.340\n to that door.\n\n36:26.340 --> 36:27.980\n So this thing that we have to do with humans\n\n36:27.980 --> 36:31.060\n to try and understand their goals and intentions,\n\n36:31.060 --> 36:34.460\n humans are inevitably gonna do that to robots.\n\n36:34.460 --> 36:36.500\n And then that raises this interesting question\n\n36:36.500 --> 36:38.860\n that you asked, which is, can we do something about that?\n\n36:38.860 --> 36:40.220\n This is gonna happen inevitably,\n\n36:40.220 --> 36:42.060\n but we can sort of be more confusing\n\n36:42.060 --> 36:44.100\n or less confusing to people.\n\n36:44.100 --> 36:45.580\n And it turns out you can optimize\n\n36:45.580 --> 36:48.980\n for being more informative and less confusing\n\n36:48.980 --> 36:51.820\n if you have an understanding of how your actions\n\n36:51.820 --> 36:53.540\n are being interpreted by the human,\n\n36:53.540 --> 36:56.740\n and how they're using these actions to update their belief.\n\n36:56.740 --> 36:59.700\n And honestly, all we did is just Bayes rule.\n\n36:59.700 --> 37:02.980\n Basically, okay, the person has a belief,\n\n37:02.980 --> 37:04.820\n they see an action, they make some assumptions\n\n37:04.820 --> 37:06.420\n about how the robot generates its actions,\n\n37:06.420 --> 37:07.740\n presumably as being rational,\n\n37:07.740 --> 37:09.180\n because robots are rational.\n\n37:09.180 --> 37:11.340\n It's reasonable to assume that about them.\n\n37:11.340 --> 37:16.340\n And then they incorporate that new piece of evidence\n\n37:17.300 --> 37:19.380\n in the Bayesian sense in their belief,\n\n37:19.380 --> 37:20.700\n and they obtain a posterior.\n\n37:20.700 --> 37:23.020\n And now the robot is trying to figure out\n\n37:23.020 --> 37:25.180\n what actions to take such that it steers\n\n37:25.180 --> 37:27.420\n the person's belief to put as much probability mass\n\n37:27.420 --> 37:31.260\n as possible on the correct parameters.\n\n37:31.260 --> 37:33.940\n So that's kind of a mathematical formalization of that.\n\n37:33.940 --> 37:38.300\n But my worry, and I don't know if you wanna go there\n\n37:38.300 --> 37:43.300\n with me, but I talk about this quite a bit.\n\n37:44.140 --> 37:49.140\n The kids talking to Alexa disrespectfully worries me.\n\n37:49.500 --> 37:52.260\n I worry in general about human nature.\n\n37:52.260 --> 37:54.820\n Like I said, I grew up in Soviet Union, World War II,\n\n37:54.820 --> 37:58.180\n I'm a Jew too, so with the Holocaust and everything.\n\n37:58.180 --> 38:02.540\n I just worry about how we humans sometimes treat the other,\n\n38:02.540 --> 38:05.100\n the group that we call the other, whatever it is.\n\n38:05.100 --> 38:07.300\n Through human history, the group that's the other\n\n38:07.300 --> 38:09.580\n has been changed faces.\n\n38:09.580 --> 38:13.900\n But it seems like the robot will be the other, the other,\n\n38:13.900 --> 38:15.700\n the next other.\n\n38:15.700 --> 38:19.420\n And one thing is it feels to me\n\n38:19.420 --> 38:22.220\n that robots don't get no respect.\n\n38:22.220 --> 38:23.420\n They get shoved around.\n\n38:23.420 --> 38:27.180\n Shoved around, and is there, one, at the shallow level,\n\n38:27.180 --> 38:29.740\n for a better experience, it seems that robots\n\n38:29.740 --> 38:31.540\n need to talk back a little bit.\n\n38:31.540 --> 38:35.460\n Like my intuition says, I mean, most companies\n\n38:35.460 --> 38:38.420\n from sort of Roomba, autonomous vehicle companies\n\n38:38.420 --> 38:41.500\n might not be so happy with the idea that a robot\n\n38:41.500 --> 38:43.660\n has a little bit of an attitude.\n\n38:43.660 --> 38:46.760\n But I feel, it feels to me that that's necessary\n\n38:46.760 --> 38:48.300\n to create a compelling experience.\n\n38:48.300 --> 38:50.640\n Like we humans don't seem to respect anything\n\n38:50.640 --> 38:52.980\n that doesn't give us some attitude.\n\n38:52.980 --> 38:57.980\n That, or like a mix of mystery and attitude and anger\n\n38:58.940 --> 39:03.940\n and that threatens us subtly, maybe passive aggressively.\n\n39:03.940 --> 39:04.780\n I don't know.\n\n39:04.780 --> 39:08.200\n It seems like we humans, yeah, need that.\n\n39:08.200 --> 39:10.100\n Do you, what are your, is there something,\n\n39:10.100 --> 39:11.900\n you have thoughts on this?\n\n39:11.900 --> 39:13.100\n All right, I'll give you two thoughts on this.\n\n39:13.100 --> 39:13.940\n Okay, sure.\n\n39:13.940 --> 39:18.940\n One is, one is, it's, we respond to, you know,\n\n39:18.940 --> 39:22.980\n someone being assertive, but we also respond\n\n39:24.220 --> 39:26.020\n to someone being vulnerable.\n\n39:26.020 --> 39:28.220\n So I think robots, my first thought is that\n\n39:28.220 --> 39:31.460\n robots get shoved around and bullied a lot\n\n39:31.460 --> 39:32.860\n because they're sort of, you know, tempting\n\n39:32.860 --> 39:34.100\n and they're sort of showing off\n\n39:34.100 --> 39:35.700\n or they appear to be showing off.\n\n39:35.700 --> 39:38.700\n And so I think going back to these things\n\n39:38.700 --> 39:39.940\n we were talking about in the beginning\n\n39:39.940 --> 39:43.940\n of making robots a little more, a little more expressive,\n\n39:43.940 --> 39:46.880\n a little bit more like, eh, that wasn't cool to do.\n\n39:46.880 --> 39:49.900\n And now I'm bummed, right?\n\n39:49.900 --> 39:51.500\n I think that that can actually help\n\n39:51.500 --> 39:53.420\n because people can't help but anthropomorphize\n\n39:53.420 --> 39:54.260\n and respond to that.\n\n39:54.260 --> 39:56.860\n Even that though, the emotion being communicated\n\n39:56.860 --> 39:58.740\n is not in any way a real thing.\n\n39:58.740 --> 40:00.220\n And people know that it's not a real thing\n\n40:00.220 --> 40:01.860\n because they know it's just a machine.\n\n40:01.860 --> 40:04.500\n We're still interpreting, you know, we watch,\n\n40:04.500 --> 40:07.100\n there's this famous psychology experiment\n\n40:07.100 --> 40:11.020\n with little triangles and kind of dots on a screen\n\n40:11.020 --> 40:12.860\n and a triangle is chasing the square\n\n40:12.860 --> 40:15.860\n and you get really angry at the darn triangle\n\n40:15.860 --> 40:18.500\n because why is it not leaving the square alone?\n\n40:18.500 --> 40:20.100\n So that's, yeah, we can't help.\n\n40:20.100 --> 40:21.460\n So that was the first thought.\n\n40:21.460 --> 40:25.500\n The vulnerability, that's really interesting that,\n\n40:25.500 --> 40:30.500\n I think of like being, pushing back, being assertive\n\n40:31.620 --> 40:33.680\n as the only mechanism of getting,\n\n40:33.680 --> 40:36.300\n of forming a connection, of getting respect,\n\n40:36.300 --> 40:37.920\n but perhaps vulnerability,\n\n40:37.920 --> 40:40.100\n perhaps there's other mechanisms that are less threatening.\n\n40:40.100 --> 40:40.940\n Yeah.\n\n40:40.940 --> 40:41.760\n Is there?\n\n40:41.760 --> 40:43.980\n Well, I think, well, a little bit, yes,\n\n40:43.980 --> 40:47.220\n but then this other thing that we can think about is,\n\n40:47.220 --> 40:48.380\n it goes back to what you were saying,\n\n40:48.380 --> 40:50.640\n that interaction is really game theoretic, right?\n\n40:50.640 --> 40:52.780\n So the moment you're taking actions in a space,\n\n40:52.780 --> 40:55.380\n the humans are taking actions in that same space,\n\n40:55.380 --> 40:58.060\n but you have your own objective, which is, you know,\n\n40:58.060 --> 40:59.640\n you're a car, you need to get your passenger\n\n40:59.640 --> 41:00.900\n to the destination.\n\n41:00.900 --> 41:03.740\n And then the human nearby has their own objective,\n\n41:03.740 --> 41:07.060\n which somewhat overlaps with you, but not entirely.\n\n41:07.060 --> 41:09.180\n You're not interested in getting into an accident\n\n41:09.180 --> 41:11.580\n with each other, but you have different destinations\n\n41:11.580 --> 41:13.000\n and you wanna get home faster\n\n41:13.000 --> 41:14.620\n and they wanna get home faster.\n\n41:14.620 --> 41:17.580\n And that's a general sum game at that point.\n\n41:17.580 --> 41:20.540\n And so that's, I think that's what,\n\n41:22.220 --> 41:25.620\n treating it as such is kind of a way we can step outside\n\n41:25.620 --> 41:29.580\n of this kind of mode that,\n\n41:29.580 --> 41:32.180\n where you try to anticipate what people do\n\n41:32.180 --> 41:35.260\n and you don't realize you have any influence over it\n\n41:35.260 --> 41:37.180\n while still protecting yourself\n\n41:37.180 --> 41:40.540\n because you're understanding that people also understand\n\n41:40.540 --> 41:42.660\n that they can influence you.\n\n41:42.660 --> 41:45.540\n And it's just kind of back and forth is this negotiation,\n\n41:45.540 --> 41:49.160\n which is really talking about different equilibria\n\n41:49.160 --> 41:50.500\n of a game.\n\n41:50.500 --> 41:53.140\n The very basic way to solve coordination\n\n41:53.140 --> 41:55.860\n is to just make predictions about what people will do\n\n41:55.860 --> 41:57.780\n and then stay out of their way.\n\n41:57.780 --> 41:59.860\n And that's hard for the reasons we talked about,\n\n41:59.860 --> 42:02.820\n which is how you have to understand people's intentions\n\n42:02.820 --> 42:05.320\n implicitly, explicitly, who knows,\n\n42:05.320 --> 42:07.140\n but somehow you have to get enough of an understanding\n\n42:07.140 --> 42:09.640\n of that to be able to anticipate what happens next.\n\n42:10.900 --> 42:11.980\n And so that's challenging.\n\n42:11.980 --> 42:13.900\n But then it's further challenged by the fact\n\n42:13.900 --> 42:17.620\n that people change what they do based on what you do\n\n42:17.620 --> 42:21.240\n because they don't plan in isolation either, right?\n\n42:21.240 --> 42:25.020\n So when you see cars trying to merge on a highway\n\n42:25.020 --> 42:27.940\n and not succeeding, one of the reasons this can be\n\n42:27.940 --> 42:32.940\n is because they look at traffic that keeps coming,\n\n42:33.180 --> 42:35.940\n they predict what these people are planning on doing,\n\n42:35.940 --> 42:37.720\n which is to just keep going,\n\n42:37.720 --> 42:39.260\n and then they stay out of the way\n\n42:39.260 --> 42:42.260\n because there's no feasible plan, right?\n\n42:42.260 --> 42:44.640\n Any plan would actually intersect\n\n42:44.640 --> 42:46.780\n with one of these other people.\n\n42:46.780 --> 42:49.380\n So that's bad, so you get stuck there.\n\n42:49.380 --> 42:53.820\n So now kind of if you start thinking about it as no, no, no,\n\n42:53.820 --> 42:58.220\n actually these people change what they do\n\n42:58.220 --> 42:59.900\n depending on what the car does.\n\n42:59.900 --> 43:03.700\n Like if the car actually tries to kind of inch itself forward,\n\n43:03.700 --> 43:07.220\n they might actually slow down and let the car in.\n\n43:07.220 --> 43:10.620\n And now taking advantage of that,\n\n43:10.620 --> 43:13.600\n well, that's kind of the next level.\n\n43:13.600 --> 43:16.260\n We call this like this underactuated system idea\n\n43:16.260 --> 43:18.700\n where it's kind of underactuated system robotics,\n\n43:18.700 --> 43:22.100\n but it's kind of, you're influenced\n\n43:22.100 --> 43:23.300\n these other degrees of freedom,\n\n43:23.300 --> 43:25.740\n but you don't get to decide what they do.\n\n43:25.740 --> 43:28.480\n I've somewhere seen you mention it,\n\n43:28.480 --> 43:32.020\n the human element in this picture as underactuated.\n\n43:32.020 --> 43:35.220\n So you understand underactuated robotics\n\n43:35.220 --> 43:40.220\n is that you can't fully control the system.\n\n43:41.340 --> 43:43.420\n You can't go in arbitrary directions\n\n43:43.420 --> 43:44.860\n in the configuration space.\n\n43:44.860 --> 43:46.360\n Under your control.\n\n43:46.360 --> 43:48.860\n Yeah, it's a very simple way of underactuation\n\n43:48.860 --> 43:51.060\n where basically there's literally these degrees of freedom\n\n43:51.060 --> 43:52.020\n that you can control,\n\n43:52.020 --> 43:53.500\n and these degrees of freedom that you can't,\n\n43:53.500 --> 43:54.340\n but you influence them.\n\n43:54.340 --> 43:55.900\n And I think that's the important part\n\n43:55.900 --> 43:59.460\n is that they don't do whatever, regardless of what you do,\n\n43:59.460 --> 44:02.300\n that what you do influences what they end up doing.\n\n44:02.300 --> 44:05.460\n I just also like the poetry of calling human robot\n\n44:05.460 --> 44:09.420\n interaction an underactuated robotics problem.\n\n44:09.420 --> 44:11.900\n And you also mentioned sort of nudging.\n\n44:11.900 --> 44:14.260\n It seems that they're, I don't know.\n\n44:14.260 --> 44:16.620\n I think about this a lot in the case of pedestrians\n\n44:16.620 --> 44:18.720\n I've collected hundreds of hours of videos.\n\n44:18.720 --> 44:21.100\n I like to just watch pedestrians.\n\n44:21.100 --> 44:22.860\n And it seems that.\n\n44:22.860 --> 44:24.300\n It's a funny hobby.\n\n44:24.300 --> 44:25.740\n Yeah, it's weird.\n\n44:25.740 --> 44:27.220\n Cause I learn a lot.\n\n44:27.220 --> 44:28.620\n I learned a lot about myself,\n\n44:28.620 --> 44:32.940\n about our human behavior, from watching pedestrians,\n\n44:32.940 --> 44:35.280\n watching people in their environment.\n\n44:35.280 --> 44:37.900\n Basically crossing the street\n\n44:37.900 --> 44:40.360\n is like you're putting your life on the line.\n\n44:41.660 --> 44:44.540\n I don't know, tens of millions of time in America every day\n\n44:44.540 --> 44:48.940\n is people are just like playing this weird game of chicken\n\n44:48.940 --> 44:49.980\n when they cross the street,\n\n44:49.980 --> 44:51.940\n especially when there's some ambiguity\n\n44:51.940 --> 44:54.340\n about the right of way.\n\n44:54.340 --> 44:56.660\n That has to do either with the rules of the road\n\n44:56.660 --> 44:59.860\n or with the general personality of the intersection\n\n44:59.860 --> 45:02.340\n based on the time of day and so on.\n\n45:02.340 --> 45:04.100\n And this nudging idea,\n\n45:05.660 --> 45:07.340\n it seems that people don't even nudge.\n\n45:07.340 --> 45:10.340\n They just aggressively take, make a decision.\n\n45:10.340 --> 45:14.080\n Somebody, there's a runner that gave me this advice.\n\n45:14.080 --> 45:16.620\n I sometimes run in the street,\n\n45:17.740 --> 45:18.860\n not in the street, on the sidewalk.\n\n45:18.860 --> 45:22.260\n And he said that if you don't make eye contact with people\n\n45:22.260 --> 45:25.700\n when you're running, they will all move out of your way.\n\n45:25.700 --> 45:27.500\n It's called civil inattention.\n\n45:27.500 --> 45:29.220\n Civil inattention, that's a thing.\n\n45:29.220 --> 45:32.020\n Oh wow, I need to look this up, but it works.\n\n45:32.020 --> 45:32.860\n What is that?\n\n45:32.860 --> 45:37.860\n My sense was if you communicate like confidence\n\n45:37.860 --> 45:41.260\n in your actions that you're unlikely to deviate\n\n45:41.260 --> 45:43.100\n from the action that you're following,\n\n45:43.100 --> 45:44.940\n that's a really powerful signal to others\n\n45:44.940 --> 45:47.180\n that they need to plan around your actions.\n\n45:47.180 --> 45:50.380\n As opposed to nudging where you're sort of hesitantly,\n\n45:50.380 --> 45:53.300\n then the hesitation might communicate\n\n45:53.300 --> 45:56.340\n that you're still in the dance and the game\n\n45:56.340 --> 45:59.460\n that they can influence with their own actions.\n\n45:59.460 --> 46:03.220\n I've recently had a conversation with Jim Keller,\n\n46:03.220 --> 46:08.220\n who's a sort of this legendary chip architect,\n\n46:08.260 --> 46:12.260\n but he also led the autopilot team for a while.\n\n46:12.260 --> 46:16.820\n And his intuition that driving is fundamentally\n\n46:16.820 --> 46:18.860\n still like a ballistics problem.\n\n46:18.860 --> 46:22.220\n Like you can ignore the human element\n\n46:22.220 --> 46:24.040\n that is just not hitting things.\n\n46:24.040 --> 46:26.580\n And you can kind of learn the right dynamics\n\n46:26.580 --> 46:29.700\n required to do the merger and all those kinds of things.\n\n46:29.700 --> 46:32.660\n And then my sense is, and I don't know if I can provide\n\n46:32.660 --> 46:34.980\n sort of definitive proof of this,\n\n46:34.980 --> 46:38.060\n but my sense is like an order of magnitude\n\n46:38.060 --> 46:41.540\n are more difficult when humans are involved.\n\n46:41.540 --> 46:46.540\n Like it's not simply object collision avoidance problem.\n\n46:48.100 --> 46:49.260\n Where does your intuition,\n\n46:49.260 --> 46:51.020\n of course, nobody knows the right answer here,\n\n46:51.020 --> 46:54.380\n but where does your intuition fall on the difficulty,\n\n46:54.380 --> 46:57.060\n fundamental difficulty of the driving problem\n\n46:57.060 --> 46:58.780\n when humans are involved?\n\n46:58.780 --> 47:00.360\n Yeah, good question.\n\n47:00.360 --> 47:01.980\n I have many opinions on this.\n\n47:03.260 --> 47:07.260\n Imagine downtown San Francisco.\n\n47:07.260 --> 47:10.740\n Yeah, it's crazy, busy, everything.\n\n47:10.740 --> 47:12.800\n Okay, now take all the humans out.\n\n47:12.800 --> 47:15.660\n No pedestrians, no human driven vehicles,\n\n47:15.660 --> 47:18.700\n no cyclists, no people on little electric scooters\n\n47:18.700 --> 47:19.960\n zipping around, nothing.\n\n47:19.960 --> 47:21.960\n I think we're done.\n\n47:21.960 --> 47:23.800\n I think driving at that point is done.\n\n47:23.800 --> 47:25.000\n We're done.\n\n47:25.000 --> 47:27.720\n There's nothing really that still needs\n\n47:27.720 --> 47:28.880\n to be solved about that.\n\n47:28.880 --> 47:30.600\n Well, let's pause there.\n\n47:30.600 --> 47:34.240\n I think I agree with you and I think a lot of people\n\n47:34.240 --> 47:37.400\n that will hear will agree with that,\n\n47:37.400 --> 47:41.640\n but we need to sort of internalize that idea.\n\n47:41.640 --> 47:42.920\n So what's the problem there?\n\n47:42.920 --> 47:45.280\n Cause we might not quite yet be done with that.\n\n47:45.280 --> 47:46.860\n Cause a lot of people kind of focus\n\n47:46.860 --> 47:48.200\n on the perception problem.\n\n47:48.200 --> 47:52.840\n A lot of people kind of map autonomous driving\n\n47:52.840 --> 47:55.720\n into how close are we to solving,\n\n47:55.720 --> 47:57.920\n being able to detect all the, you know,\n\n47:57.920 --> 48:01.560\n the drivable area, the objects in the scene.\n\n48:02.600 --> 48:06.160\n Do you see that as a, how hard is that problem?\n\n48:07.440 --> 48:09.640\n So your intuition there behind your statement\n\n48:09.640 --> 48:11.520\n was we might have not solved it yet,\n\n48:11.520 --> 48:14.520\n but we're close to solving basically the perception problem.\n\n48:14.520 --> 48:17.120\n I think the perception problem, I mean,\n\n48:17.120 --> 48:19.360\n and by the way, a bunch of years ago,\n\n48:19.360 --> 48:21.520\n this would not have been true.\n\n48:21.520 --> 48:24.600\n And a lot of issues in the space were coming\n\n48:24.600 --> 48:27.040\n from the fact that, oh, we don't really, you know,\n\n48:27.040 --> 48:29.360\n we don't know what's where.\n\n48:29.360 --> 48:33.760\n But I think it's fairly safe to say that at this point,\n\n48:33.760 --> 48:35.840\n although you could always improve on things\n\n48:35.840 --> 48:38.880\n and all of that, you can drive through downtown San Francisco\n\n48:38.880 --> 48:40.400\n if there are no people around.\n\n48:40.400 --> 48:42.520\n There's no really perception issues\n\n48:42.520 --> 48:44.920\n standing in your way there.\n\n48:44.920 --> 48:47.400\n I think perception is hard, but yeah, it's, we've made\n\n48:47.400 --> 48:49.160\n a lot of progress on the perception,\n\n48:49.160 --> 48:50.920\n so I had to undermine the difficulty of the problem.\n\n48:50.920 --> 48:53.480\n I think everything about robotics is really difficult,\n\n48:53.480 --> 48:57.160\n of course, I think that, you know, the planning problem,\n\n48:57.160 --> 48:59.480\n the control problem, all very difficult,\n\n48:59.480 --> 49:03.520\n but I think what's, what makes it really kind of, yeah.\n\n49:03.520 --> 49:05.440\n It might be, I mean, you know,\n\n49:05.440 --> 49:07.000\n and I picked downtown San Francisco,\n\n49:07.000 --> 49:11.560\n it's adapting to, well, now it's snowing,\n\n49:11.560 --> 49:14.080\n now it's no longer snowing, now it's slippery in this way,\n\n49:14.080 --> 49:16.600\n now it's the dynamics part could,\n\n49:16.600 --> 49:21.600\n I could imagine being still somewhat challenging, but.\n\n49:24.080 --> 49:26.000\n No, the thing that I think worries us,\n\n49:26.000 --> 49:27.680\n and our intuition's not good there,\n\n49:27.680 --> 49:31.560\n is the perception problem at the edge cases.\n\n49:31.560 --> 49:35.320\n Sort of downtown San Francisco, the nice thing,\n\n49:35.320 --> 49:39.760\n it's not actually, it may not be a good example because.\n\n49:39.760 --> 49:41.360\n Because you know what you're getting from,\n\n49:41.360 --> 49:43.200\n well, there's like crazy construction zones\n\n49:43.200 --> 49:44.480\n and all of that. Yeah, but the thing is,\n\n49:44.480 --> 49:46.200\n you're traveling at slow speeds,\n\n49:46.200 --> 49:47.840\n so like it doesn't feel dangerous.\n\n49:47.840 --> 49:51.040\n To me, what feels dangerous is highway speeds,\n\n49:51.040 --> 49:54.600\n when everything is, to us humans, super clear.\n\n49:54.600 --> 49:57.120\n Yeah, I'm assuming LiDAR here, by the way.\n\n49:57.120 --> 49:59.760\n I think it's kind of irresponsible to not use LiDAR.\n\n49:59.760 --> 50:01.360\n That's just my personal opinion.\n\n50:02.440 --> 50:04.600\n That's, I mean, depending on your use case,\n\n50:04.600 --> 50:07.480\n but I think like, you know, if you have the opportunity\n\n50:07.480 --> 50:11.000\n to use LiDAR, in a lot of cases, you might not.\n\n50:11.000 --> 50:13.640\n Good, your intuition makes more sense now.\n\n50:13.640 --> 50:15.200\n So you don't think vision.\n\n50:15.200 --> 50:18.040\n I really just don't know enough to say,\n\n50:18.040 --> 50:21.440\n well, vision alone, what, you know, what's like,\n\n50:21.440 --> 50:24.160\n there's a lot of, how many cameras do you have?\n\n50:24.160 --> 50:25.680\n Is it, how are you using them?\n\n50:25.680 --> 50:26.680\n I don't know. There's details.\n\n50:26.680 --> 50:28.400\n There's all, there's all sorts of details.\n\n50:28.400 --> 50:30.120\n I imagine there's stuff that's really hard\n\n50:30.120 --> 50:33.800\n to actually see, you know, how do you deal with glare,\n\n50:33.800 --> 50:34.640\n exactly what you were saying,\n\n50:34.640 --> 50:37.680\n stuff that people would see that you don't.\n\n50:37.680 --> 50:40.640\n I think I have, more of my intuition comes from systems\n\n50:40.640 --> 50:44.240\n that can actually use LiDAR as well.\n\n50:44.240 --> 50:45.800\n Yeah, and until we know for sure,\n\n50:45.800 --> 50:48.000\n it makes sense to be using LiDAR.\n\n50:48.000 --> 50:50.040\n That's kind of the safety focus.\n\n50:50.040 --> 50:52.240\n But then the sort of the,\n\n50:52.240 --> 50:55.880\n I also sympathize with the Elon Musk statement\n\n50:55.880 --> 50:57.880\n of LiDAR is a crutch.\n\n50:57.880 --> 51:02.880\n It's a fun notion to think that the things that work today\n\n51:04.600 --> 51:08.040\n is a crutch for the invention of the things\n\n51:08.040 --> 51:09.960\n that will work tomorrow, right?\n\n51:09.960 --> 51:14.960\n Like it, it's kind of true in the sense that if,\n\n51:15.520 --> 51:17.320\n you know, we want to stick to the comfort zone,\n\n51:17.320 --> 51:19.440\n you see this in academic and research settings\n\n51:19.440 --> 51:22.360\n all the time, the things that work force you\n\n51:22.360 --> 51:25.400\n to not explore outside, think outside the box.\n\n51:25.400 --> 51:26.840\n I mean, that happens all the time.\n\n51:26.840 --> 51:29.080\n The problem is in the safety critical systems,\n\n51:29.080 --> 51:32.120\n you kind of want to stick with the things that work.\n\n51:32.120 --> 51:34.920\n So it's an interesting and difficult trade off\n\n51:34.920 --> 51:38.400\n in the case of real world sort of safety critical\n\n51:38.400 --> 51:43.400\n robotic systems, but so your intuition is,\n\n51:44.960 --> 51:48.080\n just to clarify, how, I mean,\n\n51:48.080 --> 51:51.320\n how hard is this human element for,\n\n51:51.320 --> 51:52.760\n like how hard is driving\n\n51:52.760 --> 51:55.120\n when this human element is involved?\n\n51:55.120 --> 52:00.040\n Are we years, decades away from solving it?\n\n52:00.040 --> 52:03.880\n But perhaps actually the year isn't the thing I'm asking.\n\n52:03.880 --> 52:05.480\n It doesn't matter what the timeline is,\n\n52:05.480 --> 52:09.240\n but do you think we're, how many breakthroughs\n\n52:09.240 --> 52:12.320\n are we away from in solving\n\n52:12.320 --> 52:13.640\n the human robotic interaction problem\n\n52:13.640 --> 52:15.640\n to get this, to get this right?\n\n52:15.640 --> 52:20.520\n I think it, in a sense, it really depends.\n\n52:20.520 --> 52:24.040\n I think that, you know, we were talking about how,\n\n52:24.040 --> 52:25.160\n well, look, it's really hard\n\n52:25.160 --> 52:27.080\n because anticipate what people do is hard.\n\n52:27.080 --> 52:30.360\n And on top of that, playing the game is hard.\n\n52:30.360 --> 52:35.360\n But I think we sort of have the fundamental,\n\n52:35.960 --> 52:38.680\n some of the fundamental understanding for that.\n\n52:38.680 --> 52:41.080\n And then you already see that these systems\n\n52:41.080 --> 52:45.000\n are being deployed in the real world,\n\n52:45.000 --> 52:47.720\n you know, even driverless.\n\n52:47.720 --> 52:50.840\n Like there's, I think now a few companies\n\n52:50.840 --> 52:55.840\n that don't have a driver in the car in some small areas.\n\n52:55.840 --> 52:59.640\n I got a chance to, I went to Phoenix and I,\n\n52:59.640 --> 53:03.560\n I shot a video with Waymo and I needed to get\n\n53:03.560 --> 53:04.640\n that video out.\n\n53:04.640 --> 53:06.640\n People have been giving me slack,\n\n53:06.640 --> 53:09.280\n but there's incredible engineering work being done there.\n\n53:09.280 --> 53:11.160\n And it's one of those other seminal moments\n\n53:11.160 --> 53:13.920\n for me in my life to be able to, it sounds silly,\n\n53:13.920 --> 53:17.640\n but to be able to drive without a ride, sorry,\n\n53:17.640 --> 53:19.360\n without a driver in the seat.\n\n53:19.360 --> 53:22.360\n I mean, that was an incredible robotics.\n\n53:22.360 --> 53:27.360\n I was driven by a robot without being able to take over,\n\n53:27.840 --> 53:31.200\n without being able to take the steering wheel.\n\n53:31.200 --> 53:33.520\n That's a magical, that's a magical moment.\n\n53:33.520 --> 53:35.560\n So in that regard, in those domains,\n\n53:35.560 --> 53:39.960\n at least for like Waymo, they're solving that human,\n\n53:39.960 --> 53:43.520\n there's, I mean, they're going, I mean, it felt fast\n\n53:43.520 --> 53:45.600\n because you're like freaking out at first.\n\n53:45.600 --> 53:47.440\n That was, this is my first experience,\n\n53:47.440 --> 53:49.080\n but it's going like the speed limit, right?\n\n53:49.080 --> 53:51.200\n 30, 40, whatever it is.\n\n53:51.200 --> 53:53.840\n And there's humans and it deals with them quite well.\n\n53:53.840 --> 53:57.000\n It detects them, it negotiates the intersections,\n\n53:57.000 --> 53:58.240\n the left turns and all of that.\n\n53:58.240 --> 54:01.240\n So at least in those domains, it's solving them.\n\n54:01.240 --> 54:05.060\n The open question for me is like, how quickly can we expand?\n\n54:06.000 --> 54:08.760\n You know, that's the, you know,\n\n54:08.760 --> 54:10.080\n outside of the weather conditions,\n\n54:10.080 --> 54:11.040\n all of those kinds of things,\n\n54:11.040 --> 54:14.560\n how quickly can we expand to like cities like San Francisco?\n\n54:14.560 --> 54:17.120\n Yeah, and I wouldn't say that it's just, you know,\n\n54:17.120 --> 54:20.280\n now it's just pure engineering and it's probably the,\n\n54:20.280 --> 54:22.080\n I mean, and by the way,\n\n54:22.080 --> 54:26.360\n I'm speaking kind of very generally here as hypothesizing,\n\n54:26.360 --> 54:31.260\n but I think that there are successes\n\n54:31.260 --> 54:34.400\n and yet no one is everywhere out there.\n\n54:34.400 --> 54:38.880\n So that seems to suggest that things can be expanded\n\n54:38.880 --> 54:41.680\n and can be scaled and we know how to do a lot of things,\n\n54:41.680 --> 54:44.080\n but there's still probably, you know,\n\n54:44.080 --> 54:46.760\n new algorithms or modified algorithms\n\n54:46.760 --> 54:49.240\n that you still need to put in there\n\n54:49.240 --> 54:53.440\n as you learn more and more about new challenges\n\n54:53.440 --> 54:55.760\n that you get faced with.\n\n54:55.760 --> 54:58.280\n How much of this problem do you think can be learned\n\n54:58.280 --> 54:59.120\n through end to end?\n\n54:59.120 --> 55:00.680\n Is it the success of machine learning\n\n55:00.680 --> 55:02.760\n and reinforcement learning?\n\n55:02.760 --> 55:05.280\n How much of it can be learned from sort of data\n\n55:05.280 --> 55:07.040\n from scratch and how much,\n\n55:07.040 --> 55:10.540\n which most of the success of autonomous vehicle systems\n\n55:10.540 --> 55:14.400\n have a lot of heuristics and rule based stuff on top,\n\n55:14.400 --> 55:19.320\n like human expertise injected forced into the system\n\n55:19.320 --> 55:20.840\n to make it work.\n\n55:20.840 --> 55:22.000\n What's your sense?\n\n55:22.000 --> 55:26.120\n How much, what will be the role of learning\n\n55:26.120 --> 55:28.160\n in the near term and long term?\n\n55:28.160 --> 55:33.160\n I think on the one hand that learning is inevitable here,\n\n55:36.000 --> 55:37.400\n right?\n\n55:37.400 --> 55:39.720\n I think on the other hand that when people characterize\n\n55:39.720 --> 55:42.080\n the problem as it's a bunch of rules\n\n55:42.080 --> 55:44.400\n that some people wrote down,\n\n55:44.400 --> 55:49.400\n versus it's an end to end RL system or imitation learning,\n\n55:49.640 --> 55:52.400\n then maybe there's kind of something missing\n\n55:53.480 --> 55:57.080\n from maybe that's more.\n\n55:57.080 --> 56:02.080\n So for instance, I think a very, very useful tool\n\n56:02.840 --> 56:04.360\n in this sort of problem,\n\n56:04.360 --> 56:07.360\n both in how to generate the car's behavior\n\n56:07.360 --> 56:11.720\n and robots in general and how to model human beings\n\n56:11.720 --> 56:15.000\n is actually planning, search optimization, right?\n\n56:15.000 --> 56:18.280\n So robotics is the sequential decision making problem.\n\n56:18.280 --> 56:23.280\n And when a robot can figure out on its own\n\n56:26.360 --> 56:28.960\n how to achieve its goal without hitting stuff\n\n56:28.960 --> 56:30.040\n and all that stuff, right?\n\n56:30.040 --> 56:33.080\n All the good stuff for motion planning 101,\n\n56:33.080 --> 56:36.280\n I think of that as very much AI,\n\n56:36.280 --> 56:38.120\n not this is some rule or something.\n\n56:38.120 --> 56:40.360\n There's nothing rule based around that, right?\n\n56:40.360 --> 56:42.000\n It's just you're searching through a space\n\n56:42.000 --> 56:43.720\n and figuring out are you optimizing through a space\n\n56:43.720 --> 56:46.360\n and figure out what seems to be the right thing to do.\n\n56:47.320 --> 56:49.880\n And I think it's hard to just do that\n\n56:49.880 --> 56:52.520\n because you need to learn models of the world.\n\n56:52.520 --> 56:55.720\n And I think it's hard to just do the learning part\n\n56:55.720 --> 56:58.800\n where you don't bother with any of that,\n\n56:58.800 --> 57:01.720\n because then you're saying, well, I could do imitation,\n\n57:01.720 --> 57:04.640\n but then when I go off distribution, I'm really screwed.\n\n57:04.640 --> 57:08.320\n Or you can say, I can do reinforcement learning,\n\n57:08.320 --> 57:09.840\n which adds a lot of robustness,\n\n57:09.840 --> 57:12.640\n but then you have to do either reinforcement learning\n\n57:12.640 --> 57:15.320\n in the real world, which sounds a little challenging\n\n57:15.320 --> 57:18.400\n or that trial and error, you know,\n\n57:18.400 --> 57:21.080\n or you have to do reinforcement learning in simulation.\n\n57:21.080 --> 57:23.080\n And then that means, well, guess what?\n\n57:23.080 --> 57:27.280\n You need to model things, at least to model people,\n\n57:27.280 --> 57:31.560\n model the world enough that whatever policy you get of that\n\n57:31.560 --> 57:34.920\n is actually fine to roll out in the world\n\n57:34.920 --> 57:36.480\n and do some additional learning there.\n\n57:36.480 --> 57:40.920\n So. Do you think simulation, by the way, just a quick tangent\n\n57:40.920 --> 57:44.280\n has a role in the human robot interaction space?\n\n57:44.280 --> 57:46.320\n Like, is it useful?\n\n57:46.320 --> 57:48.480\n It seems like humans, everything we've been talking about\n\n57:48.480 --> 57:51.400\n are difficult to model and simulate.\n\n57:51.400 --> 57:53.640\n Do you think simulation has a role in this space?\n\n57:53.640 --> 57:54.480\n I do.\n\n57:54.480 --> 57:58.840\n I think so because you can take models\n\n57:58.840 --> 58:03.840\n and train with them ahead of time, for instance.\n\n58:04.040 --> 58:06.080\n You can.\n\n58:06.080 --> 58:07.640\n But the models, sorry to interrupt,\n\n58:07.640 --> 58:10.480\n the models are sort of human constructed or learned?\n\n58:10.480 --> 58:14.880\n I think they have to be a combination\n\n58:14.880 --> 58:19.880\n because if you get some human data and then you say,\n\n58:20.520 --> 58:22.960\n this is how, this is gonna be my model of the person.\n\n58:22.960 --> 58:24.440\n What are for simulation and training\n\n58:24.440 --> 58:25.800\n or for just deployment time?\n\n58:25.800 --> 58:27.200\n And that's what I'm planning with\n\n58:27.200 --> 58:29.120\n as my model of how people work.\n\n58:29.120 --> 58:31.640\n Regardless, if you take some data\n\n58:33.440 --> 58:35.280\n and you don't assume anything else and you just say,\n\n58:35.280 --> 58:39.200\n okay, this is some data that I've collected.\n\n58:39.200 --> 58:42.600\n Let me fit a policy to how people work based on that.\n\n58:42.600 --> 58:45.120\n What tends to happen is you collected some data\n\n58:45.120 --> 58:50.120\n and some distribution, and then now your robot\n\n58:50.400 --> 58:52.960\n sort of computes a best response to that, right?\n\n58:52.960 --> 58:54.480\n It's sort of like, what should I do\n\n58:54.480 --> 58:56.280\n if this is how people work?\n\n58:56.280 --> 58:58.600\n And easily goes off of distribution\n\n58:58.600 --> 59:01.040\n where that model that you've built of the human\n\n59:01.040 --> 59:03.480\n completely sucks because out of distribution,\n\n59:03.480 --> 59:05.120\n you have no idea, right?\n\n59:05.120 --> 59:07.880\n If you think of all the possible policies\n\n59:07.880 --> 59:10.960\n and then you take only the ones that are consistent\n\n59:10.960 --> 59:13.040\n with the human data that you've observed,\n\n59:13.040 --> 59:15.880\n that still leads a lot of, a lot of things could happen\n\n59:15.880 --> 59:18.680\n outside of that distribution where you're confident\n\n59:18.680 --> 59:19.840\n then you know what's going on.\n\n59:19.840 --> 59:22.640\n By the way, that's, I mean, I've gotten used\n\n59:22.640 --> 59:25.360\n to this terminology of not a distribution,\n\n59:25.360 --> 59:29.000\n but it's such a machine learning terminology\n\n59:29.000 --> 59:30.800\n because it kind of assumes,\n\n59:30.800 --> 59:35.800\n so distribution is referring to the data\n\n59:36.040 --> 59:36.880\n that you've seen.\n\n59:36.880 --> 59:38.040\n The set of states that you encounter\n\n59:38.040 --> 59:39.400\n at training time. They've encountered so far\n\n59:39.400 --> 59:40.720\n at training time. Yeah.\n\n59:40.720 --> 59:43.960\n But it kind of also implies that there's a nice\n\n59:43.960 --> 59:47.440\n like statistical model that represents that data.\n\n59:47.440 --> 59:50.120\n So out of distribution feels like, I don't know,\n\n59:50.120 --> 59:54.400\n it raises to me philosophical questions\n\n59:54.400 --> 59:58.640\n of how we humans reason out of distribution,\n\n59:58.640 --> 1:00:01.600\n reason about things that are completely,\n\n1:00:01.600 --> 1:00:03.240\n we haven't seen before.\n\n1:00:03.240 --> 1:00:05.760\n And so, and what we're talking about here is\n\n1:00:05.760 --> 1:00:09.160\n how do we reason about what other people do\n\n1:00:09.160 --> 1:00:11.480\n in situations where we haven't seen them?\n\n1:00:11.480 --> 1:00:14.880\n And somehow we just magically navigate that.\n\n1:00:14.880 --> 1:00:18.000\n I can anticipate what will happen in situations\n\n1:00:18.000 --> 1:00:21.640\n that are even novel in many ways.\n\n1:00:21.640 --> 1:00:22.960\n And I have a pretty good intuition for,\n\n1:00:22.960 --> 1:00:24.520\n I don't always get it right, but you know,\n\n1:00:24.520 --> 1:00:26.520\n and I might be a little uncertain and so on.\n\n1:00:26.520 --> 1:00:31.520\n But I think it's this that if you just rely on data,\n\n1:00:33.240 --> 1:00:36.000\n you know, there's just too many possibilities,\n\n1:00:36.000 --> 1:00:37.960\n there's too many policies out there that fit the data.\n\n1:00:37.960 --> 1:00:39.320\n And by the way, it's not just state,\n\n1:00:39.320 --> 1:00:40.640\n it's really kind of history of state,\n\n1:00:40.640 --> 1:00:41.840\n cause to really be able to anticipate\n\n1:00:41.840 --> 1:00:43.080\n what the person will do,\n\n1:00:43.080 --> 1:00:45.200\n it kind of depends on what they've been doing so far,\n\n1:00:45.200 --> 1:00:47.840\n cause that's the information you need to kind of,\n\n1:00:47.840 --> 1:00:49.560\n at least implicitly sort of say,\n\n1:00:49.560 --> 1:00:51.320\n oh, this is the kind of person that this is,\n\n1:00:51.320 --> 1:00:53.080\n this is probably what they're trying to do.\n\n1:00:53.080 --> 1:00:55.200\n So anyway, it's like you're trying to map history of states\n\n1:00:55.200 --> 1:00:56.640\n to actions, there's many mappings.\n\n1:00:56.640 --> 1:00:59.840\n And history meaning like the last few seconds\n\n1:00:59.840 --> 1:01:02.520\n or the last few minutes or the last few months.\n\n1:01:02.520 --> 1:01:04.680\n Who knows, who knows how much you need, right?\n\n1:01:04.680 --> 1:01:07.280\n In terms of if your state is really like the positions\n\n1:01:07.280 --> 1:01:09.680\n of everything or whatnot and velocities,\n\n1:01:09.680 --> 1:01:10.520\n who knows how much you need.\n\n1:01:10.520 --> 1:01:14.680\n And then there's so many mappings.\n\n1:01:14.680 --> 1:01:16.560\n And so now you're talking about\n\n1:01:16.560 --> 1:01:17.960\n how do you regularize that space?\n\n1:01:17.960 --> 1:01:21.440\n What priors do you impose or what's the inductive bias?\n\n1:01:21.440 --> 1:01:23.600\n So, you know, there's all very related things\n\n1:01:23.600 --> 1:01:25.800\n to think about it.\n\n1:01:25.800 --> 1:01:28.880\n Basically, what are assumptions that we should be making\n\n1:01:29.800 --> 1:01:32.600\n such that these models actually generalize\n\n1:01:32.600 --> 1:01:34.560\n outside of the data that we've seen?\n\n1:01:35.560 --> 1:01:37.800\n And now you're talking about, well, I don't know,\n\n1:01:37.800 --> 1:01:38.640\n what can you assume?\n\n1:01:38.640 --> 1:01:40.840\n Maybe you can assume that people like actually\n\n1:01:40.840 --> 1:01:43.800\n have intentions and that's what drives their actions.\n\n1:01:43.800 --> 1:01:46.560\n Maybe that's, you know, the right thing to do\n\n1:01:46.560 --> 1:01:49.600\n when you haven't seen data very nearby\n\n1:01:49.600 --> 1:01:51.000\n that tells you otherwise.\n\n1:01:51.000 --> 1:01:53.360\n I don't know, it's a very open question.\n\n1:01:53.360 --> 1:01:55.600\n Do you think sort of that one of the dreams\n\n1:01:55.600 --> 1:01:58.200\n of artificial intelligence was to solve\n\n1:01:58.200 --> 1:02:01.160\n common sense reasoning, whatever the heck that means.\n\n1:02:02.640 --> 1:02:04.960\n Do you think something like common sense reasoning\n\n1:02:04.960 --> 1:02:09.040\n has to be solved in part to be able to solve this dance\n\n1:02:09.040 --> 1:02:12.280\n of human robot interaction, the driving space\n\n1:02:12.280 --> 1:02:14.960\n or human robot interaction in general?\n\n1:02:14.960 --> 1:02:16.880\n Do you have to be able to reason about these kinds\n\n1:02:16.880 --> 1:02:21.880\n of common sense concepts of physics,\n\n1:02:21.880 --> 1:02:26.880\n of, you know, all the things we've been talking about\n\n1:02:27.640 --> 1:02:30.640\n humans, I don't even know how to express them with words,\n\n1:02:30.640 --> 1:02:34.680\n but the basics of human behavior, a fear of death.\n\n1:02:34.680 --> 1:02:38.080\n So like, to me, it's really important to encode\n\n1:02:38.080 --> 1:02:41.920\n in some kind of sense, maybe not, maybe it's implicit,\n\n1:02:41.920 --> 1:02:44.760\n but it feels that it's important to explicitly encode\n\n1:02:44.760 --> 1:02:48.200\n the fear of death, that people don't wanna die.\n\n1:02:48.200 --> 1:02:53.200\n Because it seems silly, but like the game of chicken\n\n1:02:56.880 --> 1:02:59.800\n that involves with the pedestrian crossing the street\n\n1:02:59.800 --> 1:03:03.000\n is playing with the idea of mortality.\n\n1:03:03.000 --> 1:03:04.240\n Like we really don't wanna die.\n\n1:03:04.240 --> 1:03:06.080\n It's not just like a negative reward.\n\n1:03:07.000 --> 1:03:10.040\n I don't know, it just feels like all these human concepts\n\n1:03:10.040 --> 1:03:11.760\n have to be encoded.\n\n1:03:11.760 --> 1:03:14.320\n Do you share that sense or is this a lot simpler\n\n1:03:14.320 --> 1:03:15.840\n than I'm making out to be?\n\n1:03:15.840 --> 1:03:17.080\n I think it might be simpler.\n\n1:03:17.080 --> 1:03:18.840\n And I'm the person who likes to complicate things.\n\n1:03:18.840 --> 1:03:21.120\n I think it might be simpler than that.\n\n1:03:21.120 --> 1:03:24.200\n Because it turns out, for instance,\n\n1:03:24.200 --> 1:03:29.200\n if you say model people in the very,\n\n1:03:29.560 --> 1:03:31.720\n I'll call it traditional, I don't know if it's fair\n\n1:03:31.720 --> 1:03:33.040\n to look at it as a traditional way,\n\n1:03:33.040 --> 1:03:35.360\n but you know, calling people as,\n\n1:03:35.360 --> 1:03:37.880\n okay, they're rational somehow,\n\n1:03:37.880 --> 1:03:40.080\n the utilitarian perspective.\n\n1:03:40.080 --> 1:03:45.080\n Well, in that, once you say that,\n\n1:03:45.080 --> 1:03:48.960\n you automatically capture that they have an incentive\n\n1:03:48.960 --> 1:03:50.960\n to keep on being.\n\n1:03:50.960 --> 1:03:53.720\n You know, Stuart likes to say,\n\n1:03:53.720 --> 1:03:55.840\n you can't fetch the coffee if you're dead.\n\n1:03:56.960 --> 1:03:58.320\n Stuart Russell, by the way.\n\n1:03:59.960 --> 1:04:01.320\n That's a good line.\n\n1:04:01.320 --> 1:04:05.600\n So when you're sort of treating agents\n\n1:04:05.600 --> 1:04:10.240\n as having these objectives, these incentives,\n\n1:04:10.240 --> 1:04:14.880\n humans or artificial, you're kind of implicitly modeling\n\n1:04:14.880 --> 1:04:16.960\n that they'd like to stick around\n\n1:04:16.960 --> 1:04:20.160\n so that they can accomplish those goals.\n\n1:04:20.160 --> 1:04:22.760\n So I think in a sense,\n\n1:04:22.760 --> 1:04:24.200\n maybe that's what draws me so much\n\n1:04:24.200 --> 1:04:25.520\n to the rationality framework,\n\n1:04:25.520 --> 1:04:26.800\n even though it's so broken,\n\n1:04:26.800 --> 1:04:30.680\n we've been able to, it's been such a useful perspective.\n\n1:04:30.680 --> 1:04:32.200\n And like we were talking about earlier,\n\n1:04:32.200 --> 1:04:33.040\n what's the alternative?\n\n1:04:33.040 --> 1:04:34.360\n I give up and go home or, you know,\n\n1:04:34.360 --> 1:04:36.040\n I just use complete black boxes,\n\n1:04:36.040 --> 1:04:37.960\n but then I don't know what to assume out of distribution\n\n1:04:37.960 --> 1:04:40.040\n that come back to this.\n\n1:04:40.040 --> 1:04:42.600\n It's just, it's been a very fruitful way\n\n1:04:42.600 --> 1:04:43.960\n to think about the problem\n\n1:04:43.960 --> 1:04:47.240\n in a very more positive way, right?\n\n1:04:47.240 --> 1:04:49.080\n People aren't just crazy.\n\n1:04:49.080 --> 1:04:51.440\n Maybe they make more sense than we think.\n\n1:04:51.440 --> 1:04:55.640\n But I think we also have to somehow be ready for it\n\n1:04:55.640 --> 1:04:58.200\n to be wrong, be able to detect\n\n1:04:58.200 --> 1:05:00.440\n when these assumptions aren't holding,\n\n1:05:00.440 --> 1:05:02.880\n be all of that stuff.\n\n1:05:02.880 --> 1:05:06.640\n Let me ask sort of another small side of this\n\n1:05:06.640 --> 1:05:07.800\n that we've been talking about\n\n1:05:07.800 --> 1:05:09.920\n the pure autonomous driving problem,\n\n1:05:09.920 --> 1:05:13.720\n but there's also relatively successful systems\n\n1:05:13.720 --> 1:05:17.360\n already deployed out there in what you may call\n\n1:05:17.360 --> 1:05:20.680\n like level two autonomy or semi autonomous vehicles,\n\n1:05:20.680 --> 1:05:22.560\n whether that's Tesla Autopilot,\n\n1:05:23.400 --> 1:05:27.480\n work quite a bit with Cadillac SuperGuru system,\n\n1:05:27.480 --> 1:05:31.320\n which has a driver facing camera that detects your state.\n\n1:05:31.320 --> 1:05:35.400\n There's a bunch of basically lane centering systems.\n\n1:05:35.400 --> 1:05:40.400\n What's your sense about this kind of way of dealing\n\n1:05:41.160 --> 1:05:43.160\n with the human robot interaction problem\n\n1:05:43.160 --> 1:05:45.280\n by having a really dumb robot\n\n1:05:46.400 --> 1:05:50.280\n and relying on the human to help the robot out\n\n1:05:50.280 --> 1:05:51.840\n to keep them both alive?\n\n1:05:53.000 --> 1:05:57.400\n Is that from the research perspective,\n\n1:05:57.400 --> 1:05:59.280\n how difficult is that problem?\n\n1:05:59.280 --> 1:06:02.240\n And from a practical deployment perspective,\n\n1:06:02.240 --> 1:06:05.960\n is that a fruitful way to approach\n\n1:06:05.960 --> 1:06:08.080\n this human robot interaction problem?\n\n1:06:08.080 --> 1:06:12.120\n I think what we have to be careful about there\n\n1:06:12.120 --> 1:06:16.240\n is to not, it seems like some of these systems,\n\n1:06:16.240 --> 1:06:19.880\n not all are making this underlying assumption\n\n1:06:19.880 --> 1:06:24.880\n that if, so I'm a driver and I'm now really not driving,\n\n1:06:25.560 --> 1:06:28.920\n but supervising and my job is to intervene, right?\n\n1:06:28.920 --> 1:06:31.280\n And so we have to be careful with this assumption\n\n1:06:31.280 --> 1:06:35.680\n that when I'm, if I'm supervising,\n\n1:06:36.640 --> 1:06:41.640\n I will be just as safe as when I'm driving.\n\n1:06:41.640 --> 1:06:46.640\n That I will, if I wouldn't get into some kind of accident,\n\n1:06:46.840 --> 1:06:50.880\n if I'm driving, I will be able to avoid that accident\n\n1:06:50.880 --> 1:06:52.240\n when I'm supervising too.\n\n1:06:52.240 --> 1:06:55.120\n And I think I'm concerned about this assumption\n\n1:06:55.120 --> 1:06:56.840\n from a few perspectives.\n\n1:06:56.840 --> 1:06:58.440\n So from a technical perspective,\n\n1:06:58.440 --> 1:07:01.400\n it's that when you let something kind of take control\n\n1:07:01.400 --> 1:07:03.800\n and do its thing, and it depends on what that thing is,\n\n1:07:03.800 --> 1:07:05.480\n obviously, and how much it's taking control\n\n1:07:05.480 --> 1:07:07.920\n and how, what things are you trusting it to do.\n\n1:07:07.920 --> 1:07:11.880\n But if you let it do its thing and take control,\n\n1:07:11.880 --> 1:07:15.080\n it will go to what we might call off policy\n\n1:07:15.080 --> 1:07:16.800\n from the person's perspective state.\n\n1:07:16.800 --> 1:07:18.440\n So states that the person wouldn't actually\n\n1:07:18.440 --> 1:07:20.880\n find themselves in if they were the ones driving.\n\n1:07:22.000 --> 1:07:24.120\n And the assumption that the person functions\n\n1:07:24.120 --> 1:07:26.280\n just as well there as they function in the states\n\n1:07:26.280 --> 1:07:28.080\n that they would normally encounter\n\n1:07:28.080 --> 1:07:30.040\n is a little questionable.\n\n1:07:30.040 --> 1:07:34.400\n Now, another part is the kind of the human factor side\n\n1:07:34.400 --> 1:07:38.320\n of this, which is that I don't know about you,\n\n1:07:38.320 --> 1:07:42.120\n but I think I definitely feel like I'm experiencing things\n\n1:07:42.120 --> 1:07:45.320\n very differently when I'm actively engaged in the task\n\n1:07:45.320 --> 1:07:47.000\n versus when I'm a passive observer.\n\n1:07:47.000 --> 1:07:49.400\n Like even if I try to stay engaged, right?\n\n1:07:49.400 --> 1:07:51.120\n It's very different than when I'm actually\n\n1:07:51.120 --> 1:07:53.560\n actively making decisions.\n\n1:07:53.560 --> 1:07:55.480\n And you see this in life in general.\n\n1:07:55.480 --> 1:07:58.360\n Like you see students who are actively trying\n\n1:07:58.360 --> 1:08:00.920\n to come up with the answer, learn this thing better\n\n1:08:00.920 --> 1:08:03.000\n than when they're passively told the answer.\n\n1:08:03.000 --> 1:08:04.360\n I think that's somewhat related.\n\n1:08:04.360 --> 1:08:06.680\n And I think people have studied this in human factors\n\n1:08:06.680 --> 1:08:07.600\n for airplanes.\n\n1:08:07.600 --> 1:08:10.200\n And I think it's actually fairly established\n\n1:08:10.200 --> 1:08:12.160\n that these two are not the same.\n\n1:08:12.160 --> 1:08:13.000\n So.\n\n1:08:13.000 --> 1:08:14.960\n On that point, because I've gotten a huge amount\n\n1:08:14.960 --> 1:08:17.120\n of heat on this and I stand by it.\n\n1:08:17.120 --> 1:08:17.960\n Okay.\n\n1:08:18.960 --> 1:08:22.000\n Because I know the human factors community well\n\n1:08:22.000 --> 1:08:24.040\n and the work here is really strong.\n\n1:08:24.040 --> 1:08:27.040\n And there's many decades of work showing exactly\n\n1:08:27.040 --> 1:08:28.280\n what you're saying.\n\n1:08:28.280 --> 1:08:30.920\n Nevertheless, I've been continuously surprised\n\n1:08:30.920 --> 1:08:33.800\n that much of the predictions of that work has been wrong\n\n1:08:33.800 --> 1:08:35.360\n in what I've seen.\n\n1:08:35.360 --> 1:08:37.000\n So what we have to do,\n\n1:08:37.880 --> 1:08:40.320\n I still agree with everything you said,\n\n1:08:40.320 --> 1:08:45.320\n but we have to be a little bit more open minded.\n\n1:08:45.640 --> 1:08:49.480\n So the, I'll tell you, there's a few surprising things\n\n1:08:49.480 --> 1:08:52.960\n that supervise, like everything you said to the word\n\n1:08:52.960 --> 1:08:54.840\n is actually exactly correct.\n\n1:08:54.840 --> 1:08:57.880\n But it doesn't say, what you didn't say\n\n1:08:57.880 --> 1:09:00.160\n is that these systems are,\n\n1:09:00.160 --> 1:09:02.480\n you said you can't assume a bunch of things,\n\n1:09:02.480 --> 1:09:06.680\n but we don't know if these systems are fundamentally unsafe.\n\n1:09:06.680 --> 1:09:07.920\n That's still unknown.\n\n1:09:08.800 --> 1:09:11.040\n There's a lot of interesting things,\n\n1:09:11.040 --> 1:09:15.880\n like I'm surprised by the fact, not the fact,\n\n1:09:15.880 --> 1:09:18.840\n that what seems to be anecdotally from,\n\n1:09:18.840 --> 1:09:21.160\n well, from large data collection that we've done,\n\n1:09:21.160 --> 1:09:23.960\n but also from just talking to a lot of people,\n\n1:09:23.960 --> 1:09:27.120\n when in the supervisory role of semi autonomous systems\n\n1:09:27.120 --> 1:09:29.480\n that are sufficiently dumb, at least,\n\n1:09:29.480 --> 1:09:33.560\n which is, that might be the key element,\n\n1:09:33.560 --> 1:09:35.200\n is the systems have to be dumb.\n\n1:09:35.200 --> 1:09:38.680\n The people are actually more energized as observers.\n\n1:09:38.680 --> 1:09:40.600\n So they're actually better,\n\n1:09:40.600 --> 1:09:43.400\n they're better at observing the situation.\n\n1:09:43.400 --> 1:09:46.520\n So there might be cases in systems,\n\n1:09:46.520 --> 1:09:48.320\n if you get the interaction right,\n\n1:09:48.320 --> 1:09:50.880\n where you, as a supervisor,\n\n1:09:50.880 --> 1:09:53.600\n will do a better job with the system together.\n\n1:09:53.600 --> 1:09:56.760\n I agree, I think that is actually really possible.\n\n1:09:56.760 --> 1:10:00.080\n I guess mainly I'm pointing out that if you do it naively,\n\n1:10:00.080 --> 1:10:02.160\n you're implicitly assuming something,\n\n1:10:02.160 --> 1:10:04.480\n that assumption might actually really be wrong.\n\n1:10:04.480 --> 1:10:07.760\n But I do think that if you explicitly think about\n\n1:10:09.120 --> 1:10:10.720\n what the agent should do\n\n1:10:10.720 --> 1:10:13.480\n so that the person still stays engaged.\n\n1:10:13.480 --> 1:10:16.400\n What the, so that you essentially empower the person\n\n1:10:16.400 --> 1:10:17.560\n to do more than they could,\n\n1:10:17.560 --> 1:10:19.080\n that's really the goal, right?\n\n1:10:19.080 --> 1:10:20.280\n Is you still have a driver,\n\n1:10:20.280 --> 1:10:25.320\n so you wanna empower them to be so much better\n\n1:10:25.320 --> 1:10:27.040\n than they would be by themselves.\n\n1:10:27.040 --> 1:10:29.760\n And that's different, it's a very different mindset\n\n1:10:29.760 --> 1:10:33.160\n than I want them to basically not drive, right?\n\n1:10:33.160 --> 1:10:38.160\n And, but be ready to sort of take over.\n\n1:10:40.320 --> 1:10:42.360\n So one of the interesting things we've been talking about\n\n1:10:42.360 --> 1:10:47.000\n is the rewards, that they seem to be fundamental too,\n\n1:10:47.000 --> 1:10:49.200\n the way robots behaves.\n\n1:10:49.200 --> 1:10:52.440\n So broadly speaking,\n\n1:10:52.440 --> 1:10:54.320\n we've been talking about utility functions and so on,\n\n1:10:54.320 --> 1:10:56.960\n but could you comment on how do we approach\n\n1:10:56.960 --> 1:10:59.640\n the design of reward functions?\n\n1:10:59.640 --> 1:11:02.600\n Like, how do we come up with good reward functions?\n\n1:11:02.600 --> 1:11:05.160\n Well, really good question,\n\n1:11:05.160 --> 1:11:08.640\n because the answer is we don't.\n\n1:11:10.880 --> 1:11:13.560\n This was, you know, I used to think,\n\n1:11:13.560 --> 1:11:16.480\n I used to think about how,\n\n1:11:16.480 --> 1:11:18.920\n well, it's actually really hard to specify rewards\n\n1:11:18.920 --> 1:11:22.960\n for interaction because it's really supposed to be\n\n1:11:22.960 --> 1:11:25.040\n what the people want, and then you really, you know,\n\n1:11:25.040 --> 1:11:26.600\n we talked about how you have to customize\n\n1:11:26.600 --> 1:11:30.720\n what you wanna do to the end user.\n\n1:11:30.720 --> 1:11:35.720\n But I kind of realized that even if you take\n\n1:11:36.080 --> 1:11:38.000\n the interactive component away,\n\n1:11:39.200 --> 1:11:42.680\n it's still really hard to design reward functions.\n\n1:11:42.680 --> 1:11:43.800\n So what do I mean by that?\n\n1:11:43.800 --> 1:11:47.360\n I mean, if we assume this sort of AI paradigm\n\n1:11:47.360 --> 1:11:51.080\n in which there's an agent and his job is to optimize\n\n1:11:51.080 --> 1:11:55.640\n some objectives, some reward, utility, loss, whatever, cost,\n\n1:11:58.280 --> 1:12:00.280\n if you write it out, maybe it's a set,\n\n1:12:00.280 --> 1:12:02.520\n depending on the situation or whatever it is,\n\n1:12:03.680 --> 1:12:06.960\n if you write that out and then you deploy the agent,\n\n1:12:06.960 --> 1:12:10.240\n you'd wanna make sure that whatever you specified\n\n1:12:10.240 --> 1:12:14.840\n incentivizes the behavior you want from the agent\n\n1:12:14.840 --> 1:12:18.640\n in any situation that the agent will be faced with, right?\n\n1:12:18.640 --> 1:12:22.080\n So I do motion planning on my robot arm,\n\n1:12:22.080 --> 1:12:25.920\n I specify some cost function like, you know,\n\n1:12:25.920 --> 1:12:28.080\n this is how far away you should try to stay,\n\n1:12:28.080 --> 1:12:29.560\n so much it matters to stay away from people,\n\n1:12:29.560 --> 1:12:31.800\n and this is how much it matters to be able to be efficient\n\n1:12:31.800 --> 1:12:33.920\n and blah, blah, blah, right?\n\n1:12:33.920 --> 1:12:36.560\n I need to make sure that whatever I specified,\n\n1:12:36.560 --> 1:12:39.160\n those constraints or trade offs or whatever they are,\n\n1:12:40.160 --> 1:12:43.360\n that when the robot goes and solves that problem\n\n1:12:43.360 --> 1:12:45.120\n in every new situation,\n\n1:12:45.120 --> 1:12:47.920\n that behavior is the behavior that I wanna see.\n\n1:12:47.920 --> 1:12:50.160\n And what I've been finding is\n\n1:12:50.160 --> 1:12:52.320\n that we have no idea how to do that.\n\n1:12:52.320 --> 1:12:56.520\n Basically, what I can do is I can sample,\n\n1:12:56.520 --> 1:12:58.160\n I can think of some situations\n\n1:12:58.160 --> 1:13:01.160\n that I think are representative of what the robot will face,\n\n1:13:02.240 --> 1:13:07.240\n and I can tune and add and tune some reward function\n\n1:13:08.320 --> 1:13:11.560\n until the optimal behavior is what I want\n\n1:13:11.560 --> 1:13:13.280\n on those situations,\n\n1:13:13.280 --> 1:13:15.800\n which first of all is super frustrating\n\n1:13:15.800 --> 1:13:19.040\n because, you know, through the miracle of AI,\n\n1:13:19.040 --> 1:13:21.360\n we've taken, we don't have to specify rules\n\n1:13:21.360 --> 1:13:22.880\n for behavior anymore, right?\n\n1:13:22.880 --> 1:13:24.520\n The, who were saying before,\n\n1:13:24.520 --> 1:13:27.000\n the robot comes up with the right thing to do,\n\n1:13:27.000 --> 1:13:28.520\n you plug in this situation,\n\n1:13:28.520 --> 1:13:31.640\n it optimizes right in that situation, it optimizes,\n\n1:13:31.640 --> 1:13:34.680\n but you have to spend still a lot of time\n\n1:13:34.680 --> 1:13:37.200\n on actually defining what it is\n\n1:13:37.200 --> 1:13:39.000\n that that criteria should be,\n\n1:13:39.000 --> 1:13:40.040\n making sure you didn't forget\n\n1:13:40.040 --> 1:13:42.400\n about 50 bazillion things that are important\n\n1:13:42.400 --> 1:13:44.640\n and how they all should be combining together\n\n1:13:44.640 --> 1:13:46.800\n to tell the robot what's good and what's bad\n\n1:13:46.800 --> 1:13:48.840\n and how good and how bad.\n\n1:13:48.840 --> 1:13:53.840\n And so I think this is a lesson that I don't know,\n\n1:13:55.360 --> 1:13:59.120\n kind of, I guess I close my eyes to it for a while\n\n1:13:59.120 --> 1:14:00.240\n cause I've been, you know,\n\n1:14:00.240 --> 1:14:02.520\n tuning cost functions for 10 years now,\n\n1:14:03.640 --> 1:14:07.120\n but it's really strikes me that,\n\n1:14:07.120 --> 1:14:09.600\n yeah, we've moved the tuning\n\n1:14:09.600 --> 1:14:13.240\n and the like designing of features or whatever\n\n1:14:13.240 --> 1:14:18.240\n from the behavior side into the reward side.\n\n1:14:19.720 --> 1:14:22.040\n And yes, I agree that there's way less of it,\n\n1:14:22.040 --> 1:14:24.000\n but it still seems really hard\n\n1:14:24.000 --> 1:14:26.960\n to anticipate any possible situation\n\n1:14:26.960 --> 1:14:30.240\n and make sure you specify a reward function\n\n1:14:30.240 --> 1:14:32.800\n that when optimized will work well\n\n1:14:32.800 --> 1:14:35.160\n in every possible situation.\n\n1:14:35.160 --> 1:14:38.600\n So you're kind of referring to unintended consequences\n\n1:14:38.600 --> 1:14:42.120\n or just in general, any kind of suboptimal behavior\n\n1:14:42.120 --> 1:14:44.840\n that emerges outside of the things you said,\n\n1:14:44.840 --> 1:14:46.520\n out of distribution.\n\n1:14:46.520 --> 1:14:49.720\n Suboptimal behavior that is, you know, actually optimal.\n\n1:14:49.720 --> 1:14:51.640\n I mean, this, I guess the idea of unintended consequences,\n\n1:14:51.640 --> 1:14:53.720\n you know, it's optimal respect to what you specified,\n\n1:14:53.720 --> 1:14:55.480\n but it's not what you want.\n\n1:14:55.480 --> 1:14:57.560\n And there's a difference between those.\n\n1:14:57.560 --> 1:14:59.880\n But that's not fundamentally a robotics problem, right?\n\n1:14:59.880 --> 1:15:01.320\n That's a human problem.\n\n1:15:01.320 --> 1:15:03.440\n So like. That's the thing, right?\n\n1:15:03.440 --> 1:15:05.280\n So there's this thing called Goodhart's law,\n\n1:15:05.280 --> 1:15:07.920\n which is you set a metric for an organization\n\n1:15:07.920 --> 1:15:10.880\n and the moment it becomes a target\n\n1:15:10.880 --> 1:15:13.040\n that people actually optimize for,\n\n1:15:13.040 --> 1:15:15.000\n it's no longer a good metric.\n\n1:15:15.000 --> 1:15:15.840\n What's it called?\n\n1:15:15.840 --> 1:15:16.680\n Goodhart's law.\n\n1:15:16.680 --> 1:15:17.520\n Goodhart's law.\n\n1:15:17.520 --> 1:15:20.120\n So the moment you specify a metric,\n\n1:15:20.120 --> 1:15:21.600\n it stops doing its job.\n\n1:15:21.600 --> 1:15:24.000\n Yeah, it stops doing its job.\n\n1:15:24.000 --> 1:15:25.120\n So there's, yeah, there's such a thing\n\n1:15:25.120 --> 1:15:27.400\n as optimizing for things and, you know,\n\n1:15:27.400 --> 1:15:32.200\n failing to think ahead of time\n\n1:15:32.200 --> 1:15:35.600\n of all the possible things that might be important.\n\n1:15:35.600 --> 1:15:38.080\n And so that's, so that's interesting\n\n1:15:38.080 --> 1:15:41.560\n because Historia works a lot on reward learning\n\n1:15:41.560 --> 1:15:44.000\n from the perspective of customizing to the end user,\n\n1:15:44.000 --> 1:15:48.040\n but it really seems like it's not just the interaction\n\n1:15:48.040 --> 1:15:50.880\n with the end user that's a problem of the human\n\n1:15:50.880 --> 1:15:52.320\n and the robot collaborating\n\n1:15:52.320 --> 1:15:55.160\n so that the robot can do what the human wants, right?\n\n1:15:55.160 --> 1:15:57.280\n This kind of back and forth, the robot probing,\n\n1:15:57.280 --> 1:16:00.200\n the person being informative, all of that stuff\n\n1:16:00.200 --> 1:16:04.400\n might be actually just as applicable\n\n1:16:04.400 --> 1:16:07.440\n to this kind of maybe new form of human robot interaction,\n\n1:16:07.440 --> 1:16:10.760\n which is the interaction between the robot\n\n1:16:10.760 --> 1:16:14.280\n and the expert programmer, roboticist designer\n\n1:16:14.280 --> 1:16:16.240\n in charge of actually specifying\n\n1:16:16.240 --> 1:16:18.360\n what the heck the robot should do,\n\n1:16:18.360 --> 1:16:20.200\n specifying the task for the robot.\n\n1:16:20.200 --> 1:16:21.040\n That's fascinating.\n\n1:16:21.040 --> 1:16:23.800\n That's so cool, like collaborating on the reward design.\n\n1:16:23.800 --> 1:16:26.200\n Right, collaborating on the reward design.\n\n1:16:26.200 --> 1:16:28.080\n And so what does it mean, right?\n\n1:16:28.080 --> 1:16:29.840\n What does it, when we think about the problem,\n\n1:16:29.840 --> 1:16:34.400\n not as someone specifies all of your job is to optimize,\n\n1:16:34.400 --> 1:16:37.600\n and we start thinking about you're in this interaction\n\n1:16:37.600 --> 1:16:39.280\n and this collaboration.\n\n1:16:39.280 --> 1:16:42.440\n And the first thing that comes up is\n\n1:16:42.440 --> 1:16:46.360\n when the person specifies a reward, it's not, you know,\n\n1:16:46.360 --> 1:16:48.720\n gospel, it's not like the letter of the law.\n\n1:16:48.720 --> 1:16:52.080\n It's not the definition of the reward function\n\n1:16:52.080 --> 1:16:53.320\n you should be optimizing,\n\n1:16:53.320 --> 1:16:54.840\n because they're doing their best,\n\n1:16:54.840 --> 1:16:57.120\n but they're not some magic perfect oracle.\n\n1:16:57.120 --> 1:16:58.720\n And the sooner we start understanding that,\n\n1:16:58.720 --> 1:17:02.360\n I think the sooner we'll get to more robust robots\n\n1:17:02.360 --> 1:17:06.400\n that function better in different situations.\n\n1:17:06.400 --> 1:17:08.480\n And then you have kind of say, okay, well,\n\n1:17:08.480 --> 1:17:12.680\n it's almost like robots are over learning,\n\n1:17:12.680 --> 1:17:16.760\n over putting too much weight on the reward specified\n\n1:17:16.760 --> 1:17:21.120\n by definition, and maybe leaving a lot of other information\n\n1:17:21.120 --> 1:17:23.280\n on the table, like what are other things we could do\n\n1:17:23.280 --> 1:17:25.480\n to actually communicate to the robot\n\n1:17:25.480 --> 1:17:28.280\n about what we want them to do besides attempting\n\n1:17:28.280 --> 1:17:29.600\n to specify a reward function.\n\n1:17:29.600 --> 1:17:31.760\n Yeah, you have this awesome,\n\n1:17:31.760 --> 1:17:34.760\n and again, I love the poetry of it, of leaked information.\n\n1:17:34.760 --> 1:17:38.680\n So you mentioned humans leak information\n\n1:17:38.680 --> 1:17:40.880\n about what they want, you know,\n\n1:17:40.880 --> 1:17:44.960\n leak reward signal for the robot.\n\n1:17:44.960 --> 1:17:47.680\n So how do we detect these leaks?\n\n1:17:47.680 --> 1:17:48.520\n What is that?\n\n1:17:48.520 --> 1:17:49.960\n Yeah, what are these leaks?\n\n1:17:49.960 --> 1:17:51.840\n Whether it just, I don't know,\n\n1:17:51.840 --> 1:17:54.040\n those were just recently saw it, read it,\n\n1:17:54.040 --> 1:17:55.200\n I don't know where from you,\n\n1:17:55.200 --> 1:17:58.640\n and it's gonna stick with me for a while for some reason,\n\n1:17:58.640 --> 1:18:00.920\n because it's not explicitly expressed.\n\n1:18:00.920 --> 1:18:04.520\n It kind of leaks indirectly from our behavior.\n\n1:18:04.520 --> 1:18:06.160\n From what we do, yeah, absolutely.\n\n1:18:06.160 --> 1:18:11.160\n So I think maybe some surprising bits, right?\n\n1:18:11.320 --> 1:18:14.760\n So we were talking before about, I'm a robot arm,\n\n1:18:14.760 --> 1:18:18.200\n it needs to move around people, carry stuff,\n\n1:18:18.200 --> 1:18:20.520\n put stuff away, all of that.\n\n1:18:20.520 --> 1:18:25.080\n And now imagine that, you know,\n\n1:18:25.080 --> 1:18:27.160\n the robot has some initial objective\n\n1:18:27.160 --> 1:18:28.960\n that the programmer gave it\n\n1:18:28.960 --> 1:18:30.680\n so they can do all these things functionally.\n\n1:18:30.680 --> 1:18:32.240\n It's capable of doing that.\n\n1:18:32.240 --> 1:18:35.800\n And now I noticed that it's doing something\n\n1:18:35.800 --> 1:18:39.480\n and maybe it's coming too close to me, right?\n\n1:18:39.480 --> 1:18:40.520\n And maybe I'm the designer,\n\n1:18:40.520 --> 1:18:43.840\n maybe I'm the end user and this robot is now in my home.\n\n1:18:43.840 --> 1:18:46.040\n And I push it away.\n\n1:18:47.800 --> 1:18:49.320\n So I push away because, you know,\n\n1:18:49.320 --> 1:18:52.360\n it's a reaction to what the robot is currently doing.\n\n1:18:52.360 --> 1:18:55.800\n And this is what we call physical human robot interaction.\n\n1:18:55.800 --> 1:18:58.440\n And now there's a lot of interesting work\n\n1:18:58.440 --> 1:19:00.640\n on how the heck do you respond to physical human\n\n1:19:00.640 --> 1:19:01.480\n robot interaction?\n\n1:19:01.480 --> 1:19:03.520\n What should the robot do if such an event occurs?\n\n1:19:03.520 --> 1:19:05.000\n And there's sort of different schools of thought.\n\n1:19:05.000 --> 1:19:07.040\n Well, you know, you can sort of treat it\n\n1:19:07.040 --> 1:19:08.280\n the control theoretic way and say,\n\n1:19:08.280 --> 1:19:11.160\n this is a disturbance that you must reject.\n\n1:19:11.160 --> 1:19:15.880\n You can sort of treat it more kind of heuristically\n\n1:19:15.880 --> 1:19:18.040\n and say, I'm gonna go into some like gravity compensation\n\n1:19:18.040 --> 1:19:19.800\n mode so that I'm easily maneuverable around.\n\n1:19:19.800 --> 1:19:22.280\n I'm gonna go in the direction that the person pushed me.\n\n1:19:22.280 --> 1:19:27.280\n And to us, part of realization has been\n\n1:19:27.280 --> 1:19:30.480\n that that is signal that communicates about the reward.\n\n1:19:30.480 --> 1:19:34.560\n Because if my robot was moving in an optimal way\n\n1:19:34.560 --> 1:19:37.760\n and I intervened, that means that I disagree\n\n1:19:37.760 --> 1:19:40.240\n with his notion of optimality, right?\n\n1:19:40.240 --> 1:19:43.560\n Whatever it thinks is optimal is not actually optimal.\n\n1:19:43.560 --> 1:19:45.960\n And sort of optimization problems aside,\n\n1:19:45.960 --> 1:19:47.400\n that means that the cost function,\n\n1:19:47.400 --> 1:19:51.400\n the reward function is incorrect,\n\n1:19:51.400 --> 1:19:53.560\n or at least is not what I want it to be.\n\n1:19:53.560 --> 1:19:58.440\n How difficult is that signal to interpret\n\n1:19:58.440 --> 1:19:59.400\n and make actionable?\n\n1:19:59.400 --> 1:20:00.800\n So like, cause this connects\n\n1:20:00.800 --> 1:20:02.120\n to our autonomous vehicle discussion\n\n1:20:02.120 --> 1:20:03.960\n where they're in the semi autonomous vehicle\n\n1:20:03.960 --> 1:20:06.480\n or autonomous vehicle when a safety driver\n\n1:20:06.480 --> 1:20:08.480\n disengages the car, like,\n\n1:20:08.480 --> 1:20:11.840\n but they could have disengaged it for a million reasons.\n\n1:20:11.840 --> 1:20:15.080\n Yeah, so that's true.\n\n1:20:15.080 --> 1:20:19.840\n Again, it comes back to, can you structure a little bit\n\n1:20:19.840 --> 1:20:22.040\n your assumptions about how human behavior\n\n1:20:22.040 --> 1:20:24.240\n relates to what they want?\n\n1:20:24.240 --> 1:20:26.320\n And you can, one thing that we've done is\n\n1:20:26.320 --> 1:20:29.480\n literally just treated this external torque\n\n1:20:29.480 --> 1:20:32.960\n that they applied as, when you take that\n\n1:20:32.960 --> 1:20:34.800\n and you add it with what the torque\n\n1:20:34.800 --> 1:20:36.600\n the robot was already applying,\n\n1:20:36.600 --> 1:20:39.680\n that overall action is probably relatively optimal\n\n1:20:39.680 --> 1:20:41.800\n in respect to whatever it is that the person wants.\n\n1:20:41.800 --> 1:20:43.040\n And then that gives you information\n\n1:20:43.040 --> 1:20:44.320\n about what it is that they want.\n\n1:20:44.320 --> 1:20:45.680\n So you can learn that people want you\n\n1:20:45.680 --> 1:20:47.600\n to stay further away from them.\n\n1:20:47.600 --> 1:20:49.760\n Now you're right that there might be many things\n\n1:20:49.760 --> 1:20:51.360\n that explain just that one signal\n\n1:20:51.360 --> 1:20:53.360\n and that you might need much more data than that\n\n1:20:53.360 --> 1:20:55.480\n for the person to be able to shape\n\n1:20:55.480 --> 1:20:57.200\n your reward function over time.\n\n1:20:58.640 --> 1:21:00.880\n You can also do this info gathering stuff\n\n1:21:00.880 --> 1:21:01.760\n that we were talking about.\n\n1:21:01.760 --> 1:21:03.280\n Not that we've done that in that context,\n\n1:21:03.280 --> 1:21:04.800\n just to clarify, but it's definitely something\n\n1:21:04.800 --> 1:21:09.080\n we thought about where you can have the robot\n\n1:21:09.080 --> 1:21:11.040\n start acting in a way, like if there's\n\n1:21:11.040 --> 1:21:13.400\n a bunch of different explanations, right?\n\n1:21:13.400 --> 1:21:16.360\n It moves in a way where it sees if you correct it\n\n1:21:16.360 --> 1:21:17.600\n in some other way or not,\n\n1:21:17.600 --> 1:21:19.920\n and then kind of actually plans its motion\n\n1:21:19.920 --> 1:21:21.760\n so that it can disambiguate\n\n1:21:21.760 --> 1:21:23.920\n and collect information about what you want.\n\n1:21:24.880 --> 1:21:26.000\n Anyway, so that's one way,\n\n1:21:26.000 --> 1:21:27.440\n that's kind of sort of leaked information,\n\n1:21:27.440 --> 1:21:29.280\n maybe even more subtle leaked information\n\n1:21:29.280 --> 1:21:32.760\n is if I just press the E stop, right?\n\n1:21:32.760 --> 1:21:34.040\n I just, I'm doing it out of panic\n\n1:21:34.040 --> 1:21:36.280\n because the robot is about to do something bad.\n\n1:21:36.280 --> 1:21:38.480\n There's again, information there, right?\n\n1:21:38.480 --> 1:21:40.800\n Okay, the robot should definitely stop,\n\n1:21:40.800 --> 1:21:42.560\n but it should also figure out\n\n1:21:42.560 --> 1:21:45.240\n that whatever it was about to do was not good.\n\n1:21:45.240 --> 1:21:46.720\n And in fact, it was so not good\n\n1:21:46.720 --> 1:21:48.920\n that stopping and remaining stopped for a while\n\n1:21:48.920 --> 1:21:51.080\n was a better trajectory for it\n\n1:21:51.080 --> 1:21:52.760\n than whatever it is that it was about to do.\n\n1:21:52.760 --> 1:21:54.800\n And that again is information about\n\n1:21:54.800 --> 1:21:57.560\n what are my preferences, what do I want?\n\n1:21:57.560 --> 1:22:02.560\n Speaking of E stops, what are your expert opinions\n\n1:22:03.600 --> 1:22:07.280\n on the three laws of robotics from Isaac Asimov\n\n1:22:08.160 --> 1:22:11.280\n that don't harm humans, obey orders, protect yourself?\n\n1:22:11.280 --> 1:22:13.320\n I mean, it's such a silly notion,\n\n1:22:13.320 --> 1:22:15.400\n but I speak to so many people these days,\n\n1:22:15.400 --> 1:22:17.040\n just regular folks, just, I don't know,\n\n1:22:17.040 --> 1:22:19.360\n my parents and so on about robotics.\n\n1:22:19.360 --> 1:22:21.920\n And they kind of operate in that space of,\n\n1:22:23.440 --> 1:22:25.800\n you know, imagining our future with robots\n\n1:22:25.800 --> 1:22:28.440\n and thinking what are the ethical,\n\n1:22:28.440 --> 1:22:31.520\n how do we get that dance right?\n\n1:22:31.520 --> 1:22:34.040\n I know the three laws might be a silly notion,\n\n1:22:34.040 --> 1:22:35.560\n but do you think about like\n\n1:22:35.560 --> 1:22:39.000\n what universal reward functions that might be\n\n1:22:39.000 --> 1:22:44.000\n that we should enforce on the robots of the future?\n\n1:22:44.000 --> 1:22:48.160\n Or is that a little too far out and it doesn't,\n\n1:22:48.160 --> 1:22:51.240\n or is the mechanism that you just described,\n\n1:22:51.240 --> 1:22:52.680\n it shouldn't be three laws,\n\n1:22:52.680 --> 1:22:55.160\n it should be constantly adjusting kind of thing.\n\n1:22:55.160 --> 1:22:57.840\n I think it should constantly be adjusting kind of thing.\n\n1:22:57.840 --> 1:23:00.080\n You know, the issue with the laws is,\n\n1:23:01.000 --> 1:23:02.600\n I don't even, you know, they're words\n\n1:23:02.600 --> 1:23:04.600\n and I have to write math\n\n1:23:04.600 --> 1:23:06.240\n and have to translate them into math.\n\n1:23:06.240 --> 1:23:07.280\n What does it mean to?\n\n1:23:07.280 --> 1:23:08.200\n What does harm mean?\n\n1:23:08.200 --> 1:23:11.920\n What is, it's not math.\n\n1:23:11.920 --> 1:23:12.880\n Obey what, right?\n\n1:23:12.880 --> 1:23:14.720\n Cause we just talked about how\n\n1:23:14.720 --> 1:23:17.040\n you try to say what you want,\n\n1:23:17.040 --> 1:23:19.880\n but you don't always get it right.\n\n1:23:19.880 --> 1:23:22.520\n And you want these machines to do what you want,\n\n1:23:22.520 --> 1:23:24.560\n not necessarily exactly what you literally,\n\n1:23:24.560 --> 1:23:26.600\n so you don't want them to take you literally.\n\n1:23:26.600 --> 1:23:31.600\n You wanna take what you say and interpret it in context.\n\n1:23:31.600 --> 1:23:33.520\n And that's what we do with the specified rewards.\n\n1:23:33.520 --> 1:23:36.720\n We don't take them literally anymore from the designer.\n\n1:23:36.720 --> 1:23:39.680\n We, not we as a community, we as, you know,\n\n1:23:39.680 --> 1:23:44.160\n some members of my group, we,\n\n1:23:44.160 --> 1:23:46.360\n and some of our collaborators like Peter Beal\n\n1:23:46.360 --> 1:23:50.160\n and Stuart Russell, we sort of say,\n\n1:23:50.160 --> 1:23:52.400\n okay, the designer specified this thing,\n\n1:23:53.320 --> 1:23:55.640\n but I'm gonna interpret it not as,\n\n1:23:55.640 --> 1:23:57.160\n this is the universal reward function\n\n1:23:57.160 --> 1:23:59.520\n that I shall always optimize always and forever,\n\n1:23:59.520 --> 1:24:04.520\n but as this is good evidence about what the person wants.\n\n1:24:05.440 --> 1:24:07.400\n And I should interpret that evidence\n\n1:24:07.400 --> 1:24:11.000\n in the context of these situations that it was specified for.\n\n1:24:11.000 --> 1:24:12.840\n Cause ultimately that's what the designer thought about.\n\n1:24:12.840 --> 1:24:14.280\n That's what they had in mind.\n\n1:24:14.280 --> 1:24:16.800\n And really them specifying reward function\n\n1:24:16.800 --> 1:24:18.960\n that works for me in all these situations\n\n1:24:18.960 --> 1:24:22.120\n is really kind of telling me that whatever behavior\n\n1:24:22.120 --> 1:24:24.040\n that incentivizes must be good behavior\n\n1:24:24.040 --> 1:24:25.960\n with respect to the thing\n\n1:24:25.960 --> 1:24:28.120\n that I should actually be optimizing for.\n\n1:24:28.120 --> 1:24:30.320\n And so now the robot kind of has uncertainty\n\n1:24:30.320 --> 1:24:32.320\n about what it is that it should be,\n\n1:24:32.320 --> 1:24:34.320\n what its reward function is.\n\n1:24:34.320 --> 1:24:36.320\n And then there's all these additional signals\n\n1:24:36.320 --> 1:24:39.160\n that we've been finding that it can kind of continually\n\n1:24:39.160 --> 1:24:41.800\n learn from and adapt its understanding of what people want.\n\n1:24:41.800 --> 1:24:44.880\n Every time the person corrects it, maybe they demonstrate,\n\n1:24:44.880 --> 1:24:48.440\n maybe they stop, hopefully not, right?\n\n1:24:48.440 --> 1:24:53.440\n One really, really crazy one is the environment itself.\n\n1:24:54.920 --> 1:24:58.960\n Like our world, you don't, it's not, you know,\n\n1:24:58.960 --> 1:25:01.600\n you observe our world and the state of it.\n\n1:25:01.600 --> 1:25:03.600\n And it's not that you're seeing behavior\n\n1:25:03.600 --> 1:25:05.280\n and you're saying, oh, people are making decisions\n\n1:25:05.280 --> 1:25:07.160\n that are rational, blah, blah, blah.\n\n1:25:07.160 --> 1:25:12.160\n It's, but our world is something that we've been acting with\n\n1:25:12.240 --> 1:25:14.240\n according to our preferences.\n\n1:25:14.240 --> 1:25:15.680\n So I have this example where like,\n\n1:25:15.680 --> 1:25:18.880\n the robot walks into my home and my shoes are laid down\n\n1:25:18.880 --> 1:25:21.120\n on the floor kind of in a line, right?\n\n1:25:21.120 --> 1:25:23.320\n It took effort to do that.\n\n1:25:23.320 --> 1:25:27.480\n So even though the robot doesn't see me doing this,\n\n1:25:27.480 --> 1:25:29.920\n you know, actually aligning the shoes,\n\n1:25:29.920 --> 1:25:31.560\n it should still be able to figure out\n\n1:25:31.560 --> 1:25:33.240\n that I want the shoes aligned\n\n1:25:33.240 --> 1:25:35.920\n because there's no way for them to have magically,\n\n1:25:35.920 --> 1:25:39.040\n you know, be instantiated themselves in that way.\n\n1:25:39.040 --> 1:25:43.720\n Someone must have actually taken the time to do that.\n\n1:25:43.720 --> 1:25:44.680\n So it must be important.\n\n1:25:44.680 --> 1:25:46.920\n So the environment actually tells, the environment is.\n\n1:25:46.920 --> 1:25:48.040\n Leaks information.\n\n1:25:48.040 --> 1:25:48.880\n It leaks information.\n\n1:25:48.880 --> 1:25:50.680\n I mean, the environment is the way it is\n\n1:25:50.680 --> 1:25:52.880\n because humans somehow manipulated it.\n\n1:25:52.880 --> 1:25:55.760\n So you have to kind of reverse engineer the narrative\n\n1:25:55.760 --> 1:25:57.800\n that happened to create the environment as it is\n\n1:25:57.800 --> 1:26:00.640\n and that leaks the preference information.\n\n1:26:00.640 --> 1:26:03.160\n Yeah, and you have to be careful, right?\n\n1:26:03.160 --> 1:26:06.720\n Because people don't have the bandwidth to do everything.\n\n1:26:06.720 --> 1:26:08.120\n So just because, you know, my house is messy\n\n1:26:08.120 --> 1:26:10.840\n doesn't mean that I want it to be messy, right?\n\n1:26:10.840 --> 1:26:14.440\n But that just, you know, I didn't put the effort into that.\n\n1:26:14.440 --> 1:26:16.280\n I put the effort into something else.\n\n1:26:16.280 --> 1:26:17.440\n So the robot should figure out,\n\n1:26:17.440 --> 1:26:19.200\n well, that something else was more important,\n\n1:26:19.200 --> 1:26:20.400\n but it doesn't mean that, you know,\n\n1:26:20.400 --> 1:26:21.640\n the house being messy is not.\n\n1:26:21.640 --> 1:26:24.560\n So it's a little subtle, but yeah, we really think of it.\n\n1:26:24.560 --> 1:26:26.800\n The state itself is kind of like a choice\n\n1:26:26.800 --> 1:26:31.800\n that people implicitly made about how they want their world.\n\n1:26:31.800 --> 1:26:34.920\n What book or books, technical or fiction or philosophical,\n\n1:26:34.920 --> 1:26:39.560\n when you like look back, you know, life had a big impact,\n\n1:26:39.560 --> 1:26:42.600\n maybe it was a turning point, it was inspiring in some way.\n\n1:26:42.600 --> 1:26:45.600\n Maybe we're talking about some silly book\n\n1:26:45.600 --> 1:26:48.520\n that nobody in their right mind would want to read.\n\n1:26:48.520 --> 1:26:51.560\n Or maybe it's a book that you would recommend\n\n1:26:51.560 --> 1:26:52.480\n to others to read.\n\n1:26:52.480 --> 1:26:56.120\n Or maybe those could be two different recommendations\n\n1:26:56.120 --> 1:27:00.520\n of books that could be useful for people on their journey.\n\n1:27:00.520 --> 1:27:03.520\n When I was in, it's kind of a personal story.\n\n1:27:03.520 --> 1:27:05.520\n When I was in 12th grade,\n\n1:27:05.520 --> 1:27:10.520\n I got my hands on a PDF copy in Romania\n\n1:27:10.520 --> 1:27:14.520\n of Russell Norvig, AI modern approach.\n\n1:27:14.520 --> 1:27:16.520\n I didn't know anything about AI at that point.\n\n1:27:16.520 --> 1:27:19.520\n I was, you know, I had watched the movie,\n\n1:27:19.520 --> 1:27:22.520\n The Matrix was my exposure.\n\n1:27:22.520 --> 1:27:28.520\n And so I started going through this thing\n\n1:27:28.520 --> 1:27:31.520\n and, you know, you were asking in the beginning,\n\n1:27:31.520 --> 1:27:35.520\n what are, you know, it's math and it's algorithms,\n\n1:27:35.520 --> 1:27:36.520\n what's interesting.\n\n1:27:36.520 --> 1:27:38.520\n It was so captivating.\n\n1:27:38.520 --> 1:27:41.520\n This notion that you could just have a goal\n\n1:27:41.520 --> 1:27:44.520\n and figure out your way through\n\n1:27:44.520 --> 1:27:47.520\n kind of a messy, complicated situation.\n\n1:27:47.520 --> 1:27:50.520\n So what sequence of decisions you should make\n\n1:27:50.520 --> 1:27:53.520\n to autonomously to achieve that goal.\n\n1:27:53.520 --> 1:27:55.520\n That was so cool.\n\n1:27:55.520 --> 1:28:00.520\n I'm, you know, I'm biased, but that's a cool book to look at.\n\n1:28:00.520 --> 1:28:03.520\n You can convert, you know, the goal of intelligence,\n\n1:28:03.520 --> 1:28:06.520\n the process of intelligence and mechanize it.\n\n1:28:06.520 --> 1:28:07.520\n I had the same experience.\n\n1:28:07.520 --> 1:28:09.520\n I was really interested in psychiatry\n\n1:28:09.520 --> 1:28:11.520\n and trying to understand human behavior.\n\n1:28:11.520 --> 1:28:14.520\n And then AI modern approach is like, wait,\n\n1:28:14.520 --> 1:28:15.520\n you can just reduce it all to.\n\n1:28:15.520 --> 1:28:18.520\n You can write math about human behavior, right?\n\n1:28:18.520 --> 1:28:19.520\n Yeah.\n\n1:28:19.520 --> 1:28:21.520\n So that's, and I think that stuck with me\n\n1:28:21.520 --> 1:28:25.520\n because, you know, a lot of what I do, a lot of what we do\n\n1:28:25.520 --> 1:28:28.520\n in my lab is write math about human behavior,\n\n1:28:28.520 --> 1:28:31.520\n combine it with data and learning, put it all together,\n\n1:28:31.520 --> 1:28:33.520\n give it to robots to plan with, and, you know,\n\n1:28:33.520 --> 1:28:37.520\n hope that instead of writing rules for the robots,\n\n1:28:37.520 --> 1:28:39.520\n writing heuristics, designing behavior,\n\n1:28:39.520 --> 1:28:42.520\n they can actually autonomously come up with the right thing\n\n1:28:42.520 --> 1:28:43.520\n to do around people.\n\n1:28:43.520 --> 1:28:46.520\n That's kind of our, you know, that's our signature move.\n\n1:28:46.520 --> 1:28:49.520\n We wrote some math and then instead of kind of hand crafting\n\n1:28:49.520 --> 1:28:52.520\n this and that and that and the robot figuring stuff out\n\n1:28:52.520 --> 1:28:53.520\n and isn't that cool.\n\n1:28:53.520 --> 1:28:56.520\n And I think that is the same enthusiasm that I got from\n\n1:28:56.520 --> 1:28:59.520\n the robot figured out how to reach that goal in that graph.\n\n1:28:59.520 --> 1:29:02.520\n Isn't that cool?\n\n1:29:02.520 --> 1:29:05.520\n So apologize for the romanticized questions,\n\n1:29:05.520 --> 1:29:07.520\n but, and the silly ones,\n\n1:29:07.520 --> 1:29:11.520\n if a doctor gave you five years to live,\n\n1:29:11.520 --> 1:29:15.520\n sort of emphasizing the finiteness of our existence,\n\n1:29:15.520 --> 1:29:20.520\n what would you try to accomplish?\n\n1:29:20.520 --> 1:29:22.520\n It's like my biggest nightmare, by the way.\n\n1:29:22.520 --> 1:29:24.520\n I really like living.\n\n1:29:24.520 --> 1:29:28.520\n So I'm actually, I really don't like the idea of being told\n\n1:29:28.520 --> 1:29:30.520\n that I'm going to die.\n\n1:29:30.520 --> 1:29:32.520\n Sorry to linger on that for a second.\n\n1:29:32.520 --> 1:29:36.520\n Do you, I mean, do you meditate or ponder on your mortality\n\n1:29:36.520 --> 1:29:38.520\n or human, the fact that this thing ends,\n\n1:29:38.520 --> 1:29:41.520\n it seems to be a fundamental feature.\n\n1:29:41.520 --> 1:29:44.520\n Do you think of it as a feature or a bug too?\n\n1:29:44.520 --> 1:29:47.520\n Is it, you said you don't like the idea of dying,\n\n1:29:47.520 --> 1:29:50.520\n but if I were to give you a choice of living forever,\n\n1:29:50.520 --> 1:29:52.520\n like you're not allowed to die.\n\n1:29:52.520 --> 1:29:54.520\n Now I'll say that I want to live forever,\n\n1:29:54.520 --> 1:29:55.520\n but I watched this show.\n\n1:29:55.520 --> 1:29:56.520\n It's very silly.\n\n1:29:56.520 --> 1:29:59.520\n It's called The Good Place and they reflect a lot on this.\n\n1:29:59.520 --> 1:30:00.520\n And you know, the,\n\n1:30:00.520 --> 1:30:03.520\n the moral of the story is that you have to make the afterlife\n\n1:30:03.520 --> 1:30:05.520\n be a finite too.\n\n1:30:05.520 --> 1:30:08.520\n Cause otherwise people just kind of, it's like Wally.\n\n1:30:08.520 --> 1:30:10.520\n It's like, ah, whatever.\n\n1:30:10.520 --> 1:30:13.520\n So, so I think the finiteness helps, but,\n\n1:30:13.520 --> 1:30:16.520\n but yeah, it's just, you know, I don't, I don't,\n\n1:30:16.520 --> 1:30:18.520\n I'm not a religious person.\n\n1:30:18.520 --> 1:30:21.520\n I don't think that there's something after.\n\n1:30:21.520 --> 1:30:25.520\n And so I think it just ends and you stop existing.\n\n1:30:25.520 --> 1:30:26.520\n And I really like existing.\n\n1:30:26.520 --> 1:30:31.520\n It's just, it's such a great privilege to exist that,\n\n1:30:31.520 --> 1:30:35.520\n that yeah, it's just, I think that's the scary part.\n\n1:30:35.520 --> 1:30:40.520\n I still think that we like existing so much because it ends.\n\n1:30:40.520 --> 1:30:41.520\n And that's so sad.\n\n1:30:41.520 --> 1:30:43.520\n Like it's so sad to me every time.\n\n1:30:43.520 --> 1:30:46.520\n Like I find almost everything about this life beautiful.\n\n1:30:46.520 --> 1:30:49.520\n Like the silliest, most mundane things are just beautiful.\n\n1:30:49.520 --> 1:30:52.520\n And I think I'm cognizant of the fact that I find it beautiful\n\n1:30:52.520 --> 1:30:55.520\n because it ends like it.\n\n1:30:55.520 --> 1:30:57.520\n And it's so, I don't know.\n\n1:30:57.520 --> 1:30:59.520\n I don't know how to feel about that.\n\n1:30:59.520 --> 1:31:03.520\n I also feel like there's a lesson in there for robotics\n\n1:31:03.520 --> 1:31:10.520\n and AI that is not like the finiteness of things seems\n\n1:31:10.520 --> 1:31:13.520\n to be a fundamental nature of human existence.\n\n1:31:13.520 --> 1:31:16.520\n I think some people sort of accuse me of just being Russian\n\n1:31:16.520 --> 1:31:19.520\n and melancholic and romantic or something,\n\n1:31:19.520 --> 1:31:24.520\n but that seems to be a fundamental nature of our existence\n\n1:31:24.520 --> 1:31:28.520\n that should be incorporated in our reward functions.\n\n1:31:28.520 --> 1:31:34.520\n But anyway, if you were speaking of reward functions,\n\n1:31:34.520 --> 1:31:38.520\n if you only had five years, what would you try to accomplish?\n\n1:31:38.520 --> 1:31:41.520\n This is the thing.\n\n1:31:41.520 --> 1:31:45.520\n I'm thinking about this question and have a pretty joyous moment\n\n1:31:45.520 --> 1:31:49.520\n because I don't know that I would change much.\n\n1:31:49.520 --> 1:31:55.520\n I'm trying to make some contributions to how we understand\n\n1:31:55.520 --> 1:31:57.520\n human AI interaction.\n\n1:31:57.520 --> 1:32:00.520\n I don't think I would change that.\n\n1:32:00.520 --> 1:32:04.520\n Maybe I'll take more trips to the Caribbean or something,\n\n1:32:04.520 --> 1:32:08.520\n but I tried some of that already from time to time.\n\n1:32:08.520 --> 1:32:13.520\n So, yeah, I try to do the things that bring me joy\n\n1:32:13.520 --> 1:32:17.520\n and thinking about these things bring me joy is the Marie Kondo thing.\n\n1:32:17.520 --> 1:32:19.520\n Don't do stuff that doesn't spark joy.\n\n1:32:19.520 --> 1:32:22.520\n For the most part, I do things that spark joy.\n\n1:32:22.520 --> 1:32:25.520\n Maybe I'll do less service in the department or something.\n\n1:32:25.520 --> 1:32:30.520\n I'm not dealing with admissions anymore.\n\n1:32:30.520 --> 1:32:36.520\n But no, I think I have amazing colleagues and amazing students\n\n1:32:36.520 --> 1:32:40.520\n and amazing family and friends and spending time in some balance\n\n1:32:40.520 --> 1:32:44.520\n with all of them is what I do and that's what I'm doing already.\n\n1:32:44.520 --> 1:32:47.520\n So, I don't know that I would really change anything.\n\n1:32:47.520 --> 1:32:52.520\n So, on the spirit of positiveness, what small act of kindness,\n\n1:32:52.520 --> 1:32:57.520\n if one pops to mind, were you once shown that you will never forget?\n\n1:32:57.520 --> 1:33:08.520\n When I was in high school, my friends, my classmates did some tutoring.\n\n1:33:08.520 --> 1:33:11.520\n We were gearing up for our baccalaureate exam\n\n1:33:11.520 --> 1:33:15.520\n and they did some tutoring on, well, some on math, some on whatever.\n\n1:33:15.520 --> 1:33:19.520\n I was comfortable enough with some of those subjects,\n\n1:33:19.520 --> 1:33:22.520\n but physics was something that I hadn't focused on in a while.\n\n1:33:22.520 --> 1:33:28.520\n And so, they were all working with this one teacher\n\n1:33:28.520 --> 1:33:31.520\n and I started working with that teacher.\n\n1:33:31.520 --> 1:33:33.520\n Her name is Nicole Beccano.\n\n1:33:33.520 --> 1:33:39.520\n And she was the one who kind of opened up this whole world for me\n\n1:33:39.520 --> 1:33:44.520\n because she sort of told me that I should take the SATs\n\n1:33:44.520 --> 1:33:51.520\n and apply to go to college abroad and do better on my English and all of that.\n\n1:33:51.520 --> 1:33:55.520\n And when it came to, well, financially I couldn't,\n\n1:33:55.520 --> 1:33:58.520\n my parents couldn't really afford to do all these things,\n\n1:33:58.520 --> 1:34:01.520\n she started tutoring me on physics for free\n\n1:34:01.520 --> 1:34:06.520\n and on top of that sitting down with me to kind of train me for SATs\n\n1:34:06.520 --> 1:34:09.520\n and all that jazz that she had experience with.\n\n1:34:09.520 --> 1:34:15.520\n Wow. And obviously that has taken you to be here today,\n\n1:34:15.520 --> 1:34:17.520\n sort of one of the world experts in robotics.\n\n1:34:17.520 --> 1:34:24.520\n It's funny those little... For no reason really.\n\n1:34:24.520 --> 1:34:27.520\n Just out of karma.\n\n1:34:27.520 --> 1:34:29.520\n Wanting to support someone, yeah.\n\n1:34:29.520 --> 1:34:33.520\n Yeah. So, we talked a ton about reward functions.\n\n1:34:33.520 --> 1:34:37.520\n Let me talk about the most ridiculous big question.\n\n1:34:37.520 --> 1:34:39.520\n What is the meaning of life?\n\n1:34:39.520 --> 1:34:42.520\n What's the reward function under which we humans operate?\n\n1:34:42.520 --> 1:34:47.520\n Like what, maybe to your life, maybe broader to human life in general,\n\n1:34:47.520 --> 1:34:51.520\n what do you think...\n\n1:34:51.520 --> 1:34:57.520\n What gives life fulfillment, purpose, happiness, meaning?\n\n1:34:57.520 --> 1:34:59.520\n You can't even ask that question with a straight face.\n\n1:34:59.520 --> 1:35:00.520\n That's how ridiculous this is.\n\n1:35:00.520 --> 1:35:01.520\n I can't, I can't.\n\n1:35:01.520 --> 1:35:05.520\n Okay. So, you know...\n\n1:35:05.520 --> 1:35:09.520\n You're going to try to answer it anyway, aren't you?\n\n1:35:09.520 --> 1:35:13.520\n So, I was in a planetarium once.\n\n1:35:13.520 --> 1:35:14.520\n Yes.\n\n1:35:14.520 --> 1:35:18.520\n And, you know, they show you the thing and then they zoom out and zoom out\n\n1:35:18.520 --> 1:35:20.520\n and this whole, like, you're a speck of dust kind of thing.\n\n1:35:20.520 --> 1:35:23.520\n I think I was conceptualizing that we're kind of, you know, what are humans?\n\n1:35:23.520 --> 1:35:26.520\n We're just on this little planet, whatever.\n\n1:35:26.520 --> 1:35:29.520\n We don't matter much in the grand scheme of things.\n\n1:35:29.520 --> 1:35:35.520\n And then my mind got really blown because they talked about this multiverse theory\n\n1:35:35.520 --> 1:35:38.520\n where they kind of zoomed out and were like, this is our universe.\n\n1:35:38.520 --> 1:35:42.520\n And then, like, there's a bazillion other ones and they just pop in and out of existence.\n\n1:35:42.520 --> 1:35:48.520\n So, like, our whole thing that we can't even fathom how big it is was like a blimp that went in and out.\n\n1:35:48.520 --> 1:35:51.520\n And at that point, I was like, okay, like, I'm done.\n\n1:35:51.520 --> 1:35:54.520\n This is not, there is no meaning.\n\n1:35:54.520 --> 1:35:59.520\n And clearly what we should be doing is try to impact whatever local thing we can impact,\n\n1:35:59.520 --> 1:36:05.520\n our communities, leave a little bit behind there, our friends, our family, our local communities,\n\n1:36:05.520 --> 1:36:13.520\n and just try to be there for other humans because I just, everything beyond that seems ridiculous.\n\n1:36:13.520 --> 1:36:16.520\n I mean, are you, like, how do you make sense of these multiverses?\n\n1:36:16.520 --> 1:36:21.520\n Like, are you inspired by the immensity of it?\n\n1:36:21.520 --> 1:36:34.520\n Do you, I mean, is there, like, is it amazing to you or is it almost paralyzing in the mystery of it?\n\n1:36:34.520 --> 1:36:35.520\n It's frustrating.\n\n1:36:35.520 --> 1:36:41.520\n I'm frustrated by my inability to comprehend.\n\n1:36:41.520 --> 1:36:43.520\n It just feels very frustrating.\n\n1:36:43.520 --> 1:36:48.520\n It's like there's some stuff that, you know, we should time, blah, blah, blah, that we should really be understanding.\n\n1:36:48.520 --> 1:36:50.520\n And I definitely don't understand it.\n\n1:36:50.520 --> 1:36:56.520\n But, you know, the amazing physicists of the world have a much better understanding than me.\n\n1:36:56.520 --> 1:36:58.520\n But it still seems epsilon in the grand scheme of things.\n\n1:36:58.520 --> 1:37:00.520\n So, it's very frustrating.\n\n1:37:00.520 --> 1:37:06.520\n It just, it sort of feels like our brain don't have some fundamental capacity yet, well, yet or ever.\n\n1:37:06.520 --> 1:37:07.520\n I don't know.\n\n1:37:07.520 --> 1:37:12.520\n Well, that's one of the dreams of artificial intelligence is to create systems that will aid,\n\n1:37:12.520 --> 1:37:19.520\n expand our cognitive capacity in order to understand, build the theory of everything with the physics\n\n1:37:19.520 --> 1:37:24.520\n and understand what the heck these multiverses are.\n\n1:37:24.520 --> 1:37:32.520\n So, I think there's no better way to end it than talking about the meaning of life and the fundamental nature of the universe and the multiverses.\n\n1:37:32.520 --> 1:37:33.520\n And the multiverse.\n\n1:37:33.520 --> 1:37:35.520\n So, Anca, it is a huge honor.\n\n1:37:35.520 --> 1:37:38.520\n One of my favorite conversations I've had.\n\n1:37:38.520 --> 1:37:40.520\n I really, really appreciate your time.\n\n1:37:40.520 --> 1:37:41.520\n Thank you for talking today.\n\n1:37:41.520 --> 1:37:42.520\n Thank you for coming.\n\n1:37:42.520 --> 1:37:44.520\n Come back again.\n\n1:37:44.520 --> 1:37:47.520\n Thanks for listening to this conversation with Anca Dragan.\n\n1:37:47.520 --> 1:37:50.520\n And thank you to our presenting sponsor, Cash App.\n\n1:37:50.520 --> 1:37:56.520\n Please consider supporting the podcast by downloading Cash App and using code LexPodcast.\n\n1:37:56.520 --> 1:38:01.520\n If you enjoy this podcast, subscribe on YouTube, review it with 5 stars on Apple Podcast,\n\n1:38:01.520 --> 1:38:07.520\n support it on Patreon, or simply connect with me on Twitter at LexFriedman.\n\n1:38:07.520 --> 1:38:12.520\n And now, let me leave you with some words from Isaac Asimov.\n\n1:38:12.520 --> 1:38:15.520\n Your assumptions are your windows in the world.\n\n1:38:15.520 --> 1:38:20.520\n Scrub them off every once in a while or the light won't come in.\n\n1:38:20.520 --> 1:38:46.520\n Thank you for listening and hope to see you next time.\n\n"
}