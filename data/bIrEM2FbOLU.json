{
  "title": "Greg Brockman: OpenAI and AGI | Lex Fridman Podcast #17",
  "id": "bIrEM2FbOLU",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:02.880\n The following is a conversation with Greg Brockman.\n\n00:02.880 --> 00:05.360\n He's the cofounder and CTO of OpenAI,\n\n00:05.360 --> 00:07.440\n a world class research organization\n\n00:07.440 --> 00:10.840\n developing ideas in AI with a goal of eventually\n\n00:10.840 --> 00:14.200\n creating a safe and friendly artificial general\n\n00:14.200 --> 00:18.800\n intelligence, one that benefits and empowers humanity.\n\n00:18.800 --> 00:23.080\n OpenAI is not only a source of publications, algorithms, tools,\n\n00:23.080 --> 00:24.480\n and data sets.\n\n00:24.480 --> 00:28.160\n Their mission is a catalyst for an important public discourse\n\n00:28.160 --> 00:32.720\n about our future with both narrow and general intelligence\n\n00:32.720 --> 00:34.040\n systems.\n\n00:34.040 --> 00:36.660\n This conversation is part of the Artificial Intelligence\n\n00:36.660 --> 00:39.560\n podcast at MIT and beyond.\n\n00:39.560 --> 00:42.760\n If you enjoy it, subscribe on YouTube, iTunes,\n\n00:42.760 --> 00:45.680\n or simply connect with me on Twitter at Lex Friedman,\n\n00:45.680 --> 00:50.240\n spelled F R I D. And now, here's my conversation\n\n00:50.240 --> 00:52.800\n with Greg Brockman.\n\n00:52.800 --> 00:54.440\n So in high school, and right after you\n\n00:54.440 --> 00:56.680\n wrote a draft of a chemistry textbook,\n\n00:56.680 --> 00:59.080\n saw that that covers everything from basic structure\n\n00:59.080 --> 01:01.400\n of the atom to quantum mechanics.\n\n01:01.400 --> 01:04.360\n So it's clear you have an intuition and a passion\n\n01:04.360 --> 01:09.880\n for both the physical world with chemistry and now robotics\n\n01:09.880 --> 01:14.200\n to the digital world with AI, deep learning, reinforcement\n\n01:14.200 --> 01:15.400\n learning, so on.\n\n01:15.400 --> 01:17.360\n Do you see the physical world and the digital world\n\n01:17.360 --> 01:18.640\n as different?\n\n01:18.640 --> 01:20.520\n And what do you think is the gap?\n\n01:20.520 --> 01:23.320\n A lot of it actually boils down to iteration speed.\n\n01:23.320 --> 01:25.240\n I think that a lot of what really motivates me\n\n01:25.240 --> 01:26.520\n is building things.\n\n01:26.520 --> 01:28.960\n I think about mathematics, for example,\n\n01:28.960 --> 01:30.880\n where you think really hard about a problem.\n\n01:30.880 --> 01:31.680\n You understand it.\n\n01:31.680 --> 01:33.460\n You write it down in this very obscure form\n\n01:33.460 --> 01:34.560\n that we call a proof.\n\n01:34.560 --> 01:37.600\n But then, this is in humanity's library.\n\n01:37.600 --> 01:38.440\n It's there forever.\n\n01:38.440 --> 01:40.520\n This is some truth that we've discovered.\n\n01:40.520 --> 01:43.040\n Maybe only five people in your field will ever read it.\n\n01:43.040 --> 01:45.400\n But somehow, you've kind of moved humanity forward.\n\n01:45.400 --> 01:46.900\n And so I actually used to really think\n\n01:46.900 --> 01:48.600\n that I was going to be a mathematician.\n\n01:48.600 --> 01:51.000\n And then I actually started writing this chemistry\n\n01:51.000 --> 01:51.600\n textbook.\n\n01:51.600 --> 01:53.600\n One of my friends told me, you'll never publish it\n\n01:53.600 --> 01:54.840\n because you don't have a PhD.\n\n01:54.840 --> 01:57.960\n So instead, I decided to build a website\n\n01:57.960 --> 01:59.920\n and try to promote my ideas that way.\n\n01:59.920 --> 02:01.440\n And then I discovered programming.\n\n02:01.440 --> 02:05.280\n And in programming, you think hard about a problem.\n\n02:05.280 --> 02:06.040\n You understand it.\n\n02:06.040 --> 02:08.000\n You write it down in a very obscure form\n\n02:08.000 --> 02:10.000\n that we call a program.\n\n02:10.000 --> 02:12.200\n But then once again, it's in humanity's library.\n\n02:12.200 --> 02:14.080\n And anyone can get the benefit from it.\n\n02:14.080 --> 02:15.540\n And the scalability is massive.\n\n02:15.540 --> 02:17.540\n And so I think that the thing that really appeals\n\n02:17.540 --> 02:19.420\n to me about the digital world is that you\n\n02:19.420 --> 02:21.920\n can have this insane leverage.\n\n02:21.920 --> 02:24.320\n A single individual with an idea is\n\n02:24.320 --> 02:25.800\n able to affect the entire planet.\n\n02:25.800 --> 02:27.400\n And that's something I think is really\n\n02:27.400 --> 02:30.160\n hard to do if you're moving around physical atoms.\n\n02:30.160 --> 02:32.400\n But you said mathematics.\n\n02:32.400 --> 02:36.800\n So if you look at the wet thing over here, our mind,\n\n02:36.800 --> 02:39.760\n do you ultimately see it as just math,\n\n02:39.760 --> 02:41.720\n as just information processing?\n\n02:41.720 --> 02:44.320\n Or is there some other magic, as you've seen,\n\n02:44.320 --> 02:47.000\n if you've seen through biology and chemistry and so on?\n\n02:47.000 --> 02:48.640\n Yeah, I think it's really interesting to think about\n\n02:48.640 --> 02:50.920\n humans as just information processing systems.\n\n02:50.920 --> 02:52.560\n And that seems like it's actually\n\n02:52.560 --> 02:57.160\n a pretty good way of describing a lot of how the world works\n\n02:57.160 --> 03:00.640\n or a lot of what we're capable of, to think that, again,\n\n03:00.640 --> 03:02.760\n if you just look at technological innovations\n\n03:02.760 --> 03:05.480\n over time, that in some ways, the most transformative\n\n03:05.480 --> 03:07.720\n innovation that we've had has been the computer.\n\n03:07.720 --> 03:10.520\n In some ways, the internet, that what has the internet done?\n\n03:10.520 --> 03:12.720\n The internet is not about these physical cables.\n\n03:12.720 --> 03:14.520\n It's about the fact that I am suddenly\n\n03:14.520 --> 03:16.520\n able to instantly communicate with any other human\n\n03:16.520 --> 03:17.640\n on the planet.\n\n03:17.640 --> 03:19.640\n I'm able to retrieve any piece of knowledge\n\n03:19.640 --> 03:22.640\n that in some ways the human race has ever had,\n\n03:22.640 --> 03:26.040\n and that those are these insane transformations.\n\n03:26.040 --> 03:29.320\n Do you see our society as a whole, the collective,\n\n03:29.320 --> 03:32.240\n as another extension of the intelligence of the human being?\n\n03:32.240 --> 03:33.400\n So if you look at the human being\n\n03:33.400 --> 03:35.040\n as an information processing system,\n\n03:35.040 --> 03:36.880\n you mentioned the internet, the networking.\n\n03:36.880 --> 03:39.320\n Do you see us all together as a civilization\n\n03:39.320 --> 03:41.640\n as a kind of intelligent system?\n\n03:41.640 --> 03:42.840\n Yeah, I think this is actually\n\n03:42.840 --> 03:44.840\n a really interesting perspective to take\n\n03:44.840 --> 03:46.680\n and to think about, that you sort of have\n\n03:46.680 --> 03:49.480\n this collective intelligence of all of society,\n\n03:49.480 --> 03:51.640\n the economy itself is this superhuman machine\n\n03:51.640 --> 03:54.400\n that is optimizing something, right?\n\n03:54.400 --> 03:57.960\n And in some ways, a company has a will of its own, right?\n\n03:57.960 --> 03:59.040\n That you have all these individuals\n\n03:59.040 --> 04:00.800\n who are all pursuing their own individual goals\n\n04:00.800 --> 04:01.960\n and thinking really hard\n\n04:01.960 --> 04:03.600\n and thinking about the right things to do,\n\n04:03.600 --> 04:05.320\n but somehow the company does something\n\n04:05.320 --> 04:07.880\n that is this emergent thing\n\n04:07.880 --> 04:10.600\n and that is a really useful abstraction.\n\n04:10.600 --> 04:12.400\n And so I think that in some ways,\n\n04:12.400 --> 04:14.840\n we think of ourselves as the most intelligent things\n\n04:14.840 --> 04:17.440\n on the planet and the most powerful things on the planet,\n\n04:17.440 --> 04:19.280\n but there are things that are bigger than us\n\n04:19.280 --> 04:21.400\n that are the systems that we all contribute to.\n\n04:21.400 --> 04:24.960\n And so I think actually, it's interesting to think about\n\n04:24.960 --> 04:27.400\n if you've read Isaac Asimov's foundation, right?\n\n04:27.400 --> 04:30.000\n That there's this concept of psychohistory in there,\n\n04:30.000 --> 04:31.000\n which is effectively this,\n\n04:31.000 --> 04:33.880\n that if you have trillions or quadrillions of beings,\n\n04:33.880 --> 04:36.520\n then maybe you could actually predict what that being,\n\n04:36.520 --> 04:39.040\n that huge macro being will do\n\n04:39.040 --> 04:42.320\n and almost independent of what the individuals want.\n\n04:42.320 --> 04:44.200\n And I actually have a second angle on this\n\n04:44.200 --> 04:45.040\n that I think is interesting,\n\n04:45.040 --> 04:48.360\n which is thinking about technological determinism.\n\n04:48.360 --> 04:51.240\n One thing that I actually think a lot about with OpenAI,\n\n04:51.240 --> 04:53.320\n right, is that we're kind of coming on\n\n04:53.320 --> 04:55.840\n to this insanely transformational technology\n\n04:55.840 --> 04:57.320\n of general intelligence, right,\n\n04:57.320 --> 04:58.760\n that will happen at some point.\n\n04:58.760 --> 05:01.520\n And there's a question of how can you take actions\n\n05:01.520 --> 05:04.840\n that will actually steer it to go better rather than worse.\n\n05:04.840 --> 05:06.520\n And that I think one question you need to ask\n\n05:06.520 --> 05:09.280\n is as a scientist, as an inventor, as a creator,\n\n05:09.280 --> 05:11.680\n what impact can you have in general, right?\n\n05:11.680 --> 05:12.840\n You look at things like the telephone\n\n05:12.840 --> 05:14.800\n invented by two people on the same day.\n\n05:14.800 --> 05:15.920\n Like, what does that mean?\n\n05:15.920 --> 05:18.080\n Like, what does that mean about the shape of innovation?\n\n05:18.080 --> 05:19.240\n And I think that what's going on\n\n05:19.240 --> 05:21.680\n is everyone's building on the shoulders of the same giants.\n\n05:21.680 --> 05:23.800\n And so you can kind of, you can't really hope\n\n05:23.800 --> 05:25.680\n to create something no one else ever would.\n\n05:25.680 --> 05:27.000\n You know, if Einstein wasn't born,\n\n05:27.000 --> 05:29.160\n someone else would have come up with relativity.\n\n05:29.160 --> 05:30.960\n You know, he changed the timeline a bit, right,\n\n05:30.960 --> 05:32.960\n that maybe it would have taken another 20 years,\n\n05:32.960 --> 05:34.560\n but it wouldn't be that fundamentally humanity\n\n05:34.560 --> 05:37.320\n would never discover these fundamental truths.\n\n05:37.320 --> 05:40.400\n So there's some kind of invisible momentum\n\n05:40.400 --> 05:45.360\n that some people like Einstein or OpenAI is plugging into\n\n05:45.360 --> 05:47.760\n that anybody else can also plug into\n\n05:47.760 --> 05:50.760\n and ultimately that wave takes us into a certain direction.\n\n05:50.760 --> 05:51.840\n That's what he means by digital.\n\n05:51.840 --> 05:52.800\n That's right, that's right.\n\n05:52.800 --> 05:54.160\n And you know, this kind of seems to play out\n\n05:54.160 --> 05:55.680\n in a bunch of different ways,\n\n05:55.680 --> 05:58.000\n that there's some exponential that is being written\n\n05:58.000 --> 06:00.600\n and that the exponential itself, which one it is, changes.\n\n06:00.600 --> 06:02.400\n Think about Moore's Law, an entire industry\n\n06:02.400 --> 06:04.760\n set its clock to it for 50 years.\n\n06:04.760 --> 06:06.160\n Like, how can that be, right?\n\n06:06.160 --> 06:07.320\n How is that possible?\n\n06:07.320 --> 06:09.240\n And yet somehow it happened.\n\n06:09.240 --> 06:12.160\n And so I think you can't hope to ever invent something\n\n06:12.160 --> 06:13.280\n that no one else will.\n\n06:13.280 --> 06:15.280\n Maybe you can change the timeline a little bit.\n\n06:15.280 --> 06:17.360\n But if you really want to make a difference,\n\n06:17.360 --> 06:19.360\n I think that the thing that you really have to do,\n\n06:19.360 --> 06:21.280\n the only real degree of freedom you have\n\n06:21.280 --> 06:23.000\n is to set the initial conditions\n\n06:23.000 --> 06:24.880\n under which a technology is born.\n\n06:24.880 --> 06:26.640\n And so you think about the internet, right?\n\n06:26.640 --> 06:27.800\n That there are lots of other competitors\n\n06:27.800 --> 06:29.360\n trying to build similar things.\n\n06:29.360 --> 06:30.720\n And the internet won.\n\n06:30.720 --> 06:33.200\n And that the initial conditions\n\n06:33.200 --> 06:34.640\n were that it was created by this group\n\n06:34.640 --> 06:37.240\n that really valued people being able to be,\n\n06:38.200 --> 06:39.080\n anyone being able to plug in\n\n06:39.080 --> 06:42.440\n this very academic mindset of being open and connected.\n\n06:42.440 --> 06:44.320\n And I think that the internet for the next 40 years\n\n06:44.320 --> 06:46.280\n really played out that way.\n\n06:46.280 --> 06:48.400\n You know, maybe today things are starting\n\n06:48.400 --> 06:49.840\n to shift in a different direction.\n\n06:49.840 --> 06:51.120\n But I think that those initial conditions\n\n06:51.120 --> 06:52.720\n were really important to determine\n\n06:52.720 --> 06:55.040\n the next 40 years worth of progress.\n\n06:55.040 --> 06:56.440\n That's really beautifully put.\n\n06:56.440 --> 06:58.800\n So another example that I think about,\n\n06:58.800 --> 07:00.800\n you know, I recently looked at it.\n\n07:00.800 --> 07:03.800\n I looked at Wikipedia, the formation of Wikipedia.\n\n07:03.800 --> 07:05.520\n And I wondered what the internet would be like\n\n07:05.520 --> 07:07.720\n if Wikipedia had ads.\n\n07:07.720 --> 07:09.600\n You know, there's an interesting argument\n\n07:09.600 --> 07:12.600\n that why they chose not to make it,\n\n07:12.600 --> 07:14.240\n put advertisement on Wikipedia.\n\n07:14.240 --> 07:17.760\n I think Wikipedia's one of the greatest resources\n\n07:17.760 --> 07:18.880\n we have on the internet.\n\n07:18.880 --> 07:21.200\n It's extremely surprising how well it works\n\n07:21.200 --> 07:22.920\n and how well it was able to aggregate\n\n07:22.920 --> 07:24.960\n all this kind of good information.\n\n07:24.960 --> 07:27.280\n And essentially the creator of Wikipedia,\n\n07:27.280 --> 07:29.320\n I don't know, there's probably some debates there,\n\n07:29.320 --> 07:31.160\n but set the initial conditions.\n\n07:31.160 --> 07:33.220\n And now it carried itself forward.\n\n07:33.220 --> 07:34.060\n That's really interesting.\n\n07:34.060 --> 07:36.480\n So the way you're thinking about AGI\n\n07:36.480 --> 07:38.440\n or artificial intelligence is you're focused\n\n07:38.440 --> 07:41.160\n on setting the initial conditions for the progress.\n\n07:41.160 --> 07:42.280\n That's right.\n\n07:42.280 --> 07:43.120\n That's powerful.\n\n07:43.120 --> 07:45.520\n Okay, so looking to the future,\n\n07:45.520 --> 07:48.120\n if you create an AGI system,\n\n07:48.120 --> 07:51.560\n like one that can ace the Turing test, natural language,\n\n07:51.560 --> 07:54.760\n what do you think would be the interactions\n\n07:54.760 --> 07:55.840\n you would have with it?\n\n07:55.840 --> 07:57.720\n What do you think are the questions you would ask?\n\n07:57.720 --> 08:00.520\n Like what would be the first question you would ask?\n\n08:00.520 --> 08:01.800\n It, her, him.\n\n08:01.800 --> 08:02.640\n That's right.\n\n08:02.640 --> 08:03.920\n I think that at that point,\n\n08:03.920 --> 08:05.920\n if you've really built a powerful system\n\n08:05.920 --> 08:08.480\n that is capable of shaping the future of humanity,\n\n08:08.480 --> 08:10.240\n the first question that you really should ask\n\n08:10.240 --> 08:12.280\n is how do we make sure that this plays out well?\n\n08:12.280 --> 08:13.960\n And so that's actually the first question\n\n08:13.960 --> 08:17.600\n that I would ask a powerful AGI system is.\n\n08:17.600 --> 08:19.160\n So you wouldn't ask your colleague,\n\n08:19.160 --> 08:20.760\n you wouldn't ask like Ilya,\n\n08:20.760 --> 08:22.280\n you would ask the AGI system.\n\n08:22.280 --> 08:24.600\n Oh, we've already had the conversation with Ilya, right?\n\n08:24.600 --> 08:25.720\n And everyone here.\n\n08:25.720 --> 08:27.460\n And so you want as many perspectives\n\n08:27.460 --> 08:29.680\n and a piece of wisdom as you can\n\n08:29.680 --> 08:31.200\n for answering this question.\n\n08:31.200 --> 08:32.440\n So I don't think you necessarily defer\n\n08:32.440 --> 08:35.440\n to whatever your powerful system tells you,\n\n08:35.440 --> 08:37.080\n but you use it as one input\n\n08:37.080 --> 08:39.240\n to try to figure out what to do.\n\n08:39.240 --> 08:41.800\n But, and I guess fundamentally what it really comes down to\n\n08:41.800 --> 08:43.960\n is if you built something really powerful\n\n08:43.960 --> 08:45.280\n and you think about, for example,\n\n08:45.280 --> 08:47.640\n the creation of shortly after\n\n08:47.640 --> 08:48.880\n the creation of nuclear weapons, right?\n\n08:48.880 --> 08:51.100\n The most important question in the world was\n\n08:51.100 --> 08:52.800\n what's the world order going to be like?\n\n08:52.800 --> 08:54.900\n How do we set ourselves up in a place\n\n08:54.900 --> 08:58.320\n where we're going to be able to survive as a species?\n\n08:58.320 --> 09:00.640\n With AGI, I think the question is slightly different, right?\n\n09:00.640 --> 09:02.720\n That there is a question of how do we make sure\n\n09:02.720 --> 09:04.440\n that we don't get the negative effects,\n\n09:04.440 --> 09:06.240\n but there's also the positive side, right?\n\n09:06.240 --> 09:09.760\n You imagine that, like what won't AGI be like?\n\n09:09.760 --> 09:11.240\n Like what will it be capable of?\n\n09:11.240 --> 09:13.520\n And I think that one of the core reasons\n\n09:13.520 --> 09:15.760\n that an AGI can be powerful and transformative\n\n09:15.760 --> 09:18.900\n is actually due to technological development, right?\n\n09:18.900 --> 09:21.440\n If you have something that's capable as a human\n\n09:21.440 --> 09:23.880\n and that it's much more scalable,\n\n09:23.880 --> 09:25.880\n that you absolutely want that thing\n\n09:25.880 --> 09:27.640\n to go read the whole scientific literature\n\n09:27.640 --> 09:29.820\n and think about how to create cures for all the diseases,\n\n09:29.820 --> 09:30.660\n right?\n\n09:30.660 --> 09:31.480\n You want it to think about how to go\n\n09:31.480 --> 09:34.500\n and build technologies to help us create material abundance\n\n09:34.500 --> 09:37.320\n and to figure out societal problems\n\n09:37.320 --> 09:38.160\n that we have trouble with.\n\n09:38.160 --> 09:40.000\n Like how are we supposed to clean up the environment?\n\n09:40.000 --> 09:42.840\n And maybe you want this to go and invent\n\n09:42.840 --> 09:44.120\n a bunch of little robots that will go out\n\n09:44.120 --> 09:47.280\n and be biodegradable and turn ocean debris\n\n09:47.280 --> 09:49.660\n into harmless molecules.\n\n09:49.660 --> 09:54.040\n And I think that that positive side\n\n09:54.040 --> 09:55.720\n is something that I think people miss\n\n09:55.720 --> 09:58.160\n sometimes when thinking about what an AGI will be like.\n\n09:58.160 --> 10:00.280\n And so I think that if you have a system\n\n10:00.280 --> 10:01.640\n that's capable of all of that,\n\n10:01.640 --> 10:03.960\n you absolutely want its advice about how do I make sure\n\n10:03.960 --> 10:07.600\n that we're using your capabilities\n\n10:07.600 --> 10:09.220\n in a positive way for humanity.\n\n10:09.220 --> 10:11.440\n So what do you think about that psychology\n\n10:11.440 --> 10:14.800\n that looks at all the different possible trajectories\n\n10:14.800 --> 10:17.520\n of an AGI system, many of which,\n\n10:17.520 --> 10:19.960\n perhaps the majority of which are positive,\n\n10:19.960 --> 10:23.340\n and nevertheless focuses on the negative trajectories?\n\n10:23.340 --> 10:24.720\n I mean, you get to interact with folks,\n\n10:24.720 --> 10:28.860\n you get to think about this, maybe within yourself as well.\n\n10:28.860 --> 10:30.560\n You look at Sam Harris and so on.\n\n10:30.560 --> 10:32.760\n It seems to be, sorry to put it this way,\n\n10:32.760 --> 10:34.560\n but almost more fun to think about\n\n10:34.560 --> 10:36.780\n the negative possibilities.\n\n10:36.780 --> 10:39.560\n Whatever that's deep in our psychology,\n\n10:39.560 --> 10:40.840\n what do you think about that?\n\n10:40.840 --> 10:41.920\n And how do we deal with it?\n\n10:41.920 --> 10:44.400\n Because we want AI to help us.\n\n10:44.400 --> 10:47.360\n So I think there's kind of two problems\n\n10:47.360 --> 10:49.960\n entailed in that question.\n\n10:49.960 --> 10:52.360\n The first is more of the question of\n\n10:52.360 --> 10:54.620\n how can you even picture what a world\n\n10:54.620 --> 10:56.600\n with a new technology will be like?\n\n10:56.600 --> 10:57.840\n Now imagine we're in 1950,\n\n10:57.840 --> 11:00.040\n and I'm trying to describe Uber to someone.\n\n11:02.840 --> 11:05.340\n Apps and the internet.\n\n11:05.340 --> 11:08.920\n Yeah, I mean, that's going to be extremely complicated.\n\n11:08.920 --> 11:10.160\n But it's imaginable.\n\n11:10.160 --> 11:11.880\n It's imaginable, right?\n\n11:11.880 --> 11:15.280\n And now imagine being in 1950 and predicting Uber, right?\n\n11:15.280 --> 11:17.680\n And you need to describe the internet,\n\n11:17.680 --> 11:18.720\n you need to describe GPS,\n\n11:18.720 --> 11:20.480\n you need to describe the fact that\n\n11:20.480 --> 11:23.920\n everyone's going to have this phone in their pocket.\n\n11:23.920 --> 11:26.160\n And so I think that just the first truth\n\n11:26.160 --> 11:28.040\n is that it is hard to picture\n\n11:28.040 --> 11:31.160\n how a transformative technology will play out in the world.\n\n11:31.160 --> 11:32.760\n We've seen that before with technologies\n\n11:32.760 --> 11:35.560\n that are far less transformative than AGI will be.\n\n11:35.560 --> 11:37.780\n And so I think that one piece is that\n\n11:37.780 --> 11:39.560\n it's just even hard to imagine\n\n11:39.560 --> 11:41.640\n and to really put yourself in a world\n\n11:41.640 --> 11:44.640\n where you can predict what that positive vision\n\n11:44.640 --> 11:45.800\n would be like.\n\n11:46.920 --> 11:49.520\n And I think the second thing is that\n\n11:49.520 --> 11:54.280\n I think it is always easier to support the negative side\n\n11:54.280 --> 11:55.120\n than the positive side.\n\n11:55.120 --> 11:57.120\n It's always easier to destroy than create.\n\n11:58.160 --> 12:00.760\n And less in a physical sense\n\n12:00.760 --> 12:03.080\n and more just in an intellectual sense, right?\n\n12:03.080 --> 12:05.680\n Because I think that with creating something,\n\n12:05.680 --> 12:07.400\n you need to just get a bunch of things right.\n\n12:07.400 --> 12:10.280\n And to destroy, you just need to get one thing wrong.\n\n12:10.280 --> 12:12.040\n And so I think that what that means\n\n12:12.040 --> 12:14.240\n is that I think a lot of people's thinking dead ends\n\n12:14.240 --> 12:16.920\n as soon as they see the negative story.\n\n12:16.920 --> 12:20.360\n But that being said, I actually have some hope, right?\n\n12:20.360 --> 12:23.160\n I think that the positive vision\n\n12:23.160 --> 12:26.000\n is something that I think can be,\n\n12:26.000 --> 12:27.560\n is something that we can talk about.\n\n12:27.560 --> 12:30.240\n And I think that just simply saying this fact of,\n\n12:30.240 --> 12:32.000\n yeah, there's positive, there's negatives,\n\n12:32.000 --> 12:33.600\n everyone likes to dwell on the negative.\n\n12:33.600 --> 12:35.400\n People actually respond well to that message and say,\n\n12:35.400 --> 12:37.040\n huh, you're right, there's a part of this\n\n12:37.040 --> 12:39.640\n that we're not talking about, not thinking about.\n\n12:39.640 --> 12:42.280\n And that's actually something that's I think really\n\n12:42.280 --> 12:46.640\n been a key part of how we think about AGI at OpenAI.\n\n12:46.640 --> 12:48.640\n You can kind of look at it as like, okay,\n\n12:48.640 --> 12:51.040\n OpenAI talks about the fact that there are risks\n\n12:51.040 --> 12:53.720\n and yet they're trying to build this system.\n\n12:53.720 --> 12:56.120\n How do you square those two facts?\n\n12:56.120 --> 12:59.160\n So do you share the intuition that some people have,\n\n12:59.160 --> 13:02.720\n I mean from Sam Harris to even Elon Musk himself,\n\n13:02.720 --> 13:06.640\n that it's tricky as you develop AGI\n\n13:06.640 --> 13:10.440\n to keep it from slipping into the existential threats,\n\n13:10.440 --> 13:11.800\n into the negative?\n\n13:11.800 --> 13:14.840\n What's your intuition about how hard is it\n\n13:14.840 --> 13:19.680\n to keep AI development on the positive track?\n\n13:19.680 --> 13:20.760\n What's your intuition there?\n\n13:20.760 --> 13:22.280\n To answer that question, you can really look\n\n13:22.280 --> 13:24.000\n at how we structure OpenAI.\n\n13:24.000 --> 13:25.880\n So we really have three main arms.\n\n13:25.880 --> 13:28.000\n We have capabilities, which is actually doing\n\n13:28.000 --> 13:29.880\n the technical work and pushing forward\n\n13:29.880 --> 13:31.200\n what these systems can do.\n\n13:31.200 --> 13:35.160\n There's safety, which is working on technical mechanisms\n\n13:35.160 --> 13:36.920\n to ensure that the systems we build\n\n13:36.920 --> 13:38.480\n are aligned with human values.\n\n13:38.480 --> 13:40.680\n And then there's policy, which is making sure\n\n13:40.680 --> 13:42.040\n that we have governance mechanisms,\n\n13:42.040 --> 13:45.280\n answering that question of, well, whose values?\n\n13:45.280 --> 13:47.400\n And so I think that the technical safety one\n\n13:47.400 --> 13:50.480\n is the one that people kind of talk about the most, right?\n\n13:50.480 --> 13:53.840\n You talk about, like think about all of the dystopic AI\n\n13:53.840 --> 13:55.800\n movies, a lot of that is about not having\n\n13:55.800 --> 13:57.560\n good technical safety in place.\n\n13:57.560 --> 13:59.840\n And what we've been finding is that,\n\n13:59.840 --> 14:01.360\n you know, I think that actually a lot of people\n\n14:01.360 --> 14:02.680\n look at the technical safety problem\n\n14:02.680 --> 14:05.400\n and think it's just intractable, right?\n\n14:05.400 --> 14:07.840\n This question of what do humans want?\n\n14:07.840 --> 14:09.160\n How am I supposed to write that down?\n\n14:09.160 --> 14:11.200\n Can I even write down what I want?\n\n14:11.200 --> 14:12.040\n No way.\n\n14:13.040 --> 14:14.840\n And then they stop there.\n\n14:14.840 --> 14:16.880\n But the thing is, we've already built systems\n\n14:16.880 --> 14:20.920\n that are able to learn things that humans can't specify.\n\n14:20.920 --> 14:22.920\n You know, even the rules for how to recognize\n\n14:22.920 --> 14:24.960\n if there's a cat or a dog in an image.\n\n14:24.960 --> 14:26.520\n Turns out it's intractable to write that down,\n\n14:26.520 --> 14:28.440\n and yet we're able to learn it.\n\n14:28.440 --> 14:31.040\n And that what we're seeing with systems we build at OpenAI,\n\n14:31.040 --> 14:33.800\n and they're still in early proof of concept stage,\n\n14:33.800 --> 14:36.320\n is that you are able to learn human preferences.\n\n14:36.320 --> 14:38.960\n You're able to learn what humans want from data.\n\n14:38.960 --> 14:40.400\n And so that's kind of the core focus\n\n14:40.400 --> 14:41.760\n for our technical safety team,\n\n14:41.760 --> 14:43.800\n and I think that there actually,\n\n14:43.800 --> 14:45.680\n we've had some pretty encouraging updates\n\n14:45.680 --> 14:48.040\n in terms of what we've been able to make work.\n\n14:48.040 --> 14:51.680\n So you have an intuition and a hope that from data,\n\n14:51.680 --> 14:53.640\n you know, looking at the value alignment problem,\n\n14:53.640 --> 14:57.080\n from data we can build systems that align\n\n14:57.080 --> 15:00.640\n with the collective better angels of our nature.\n\n15:00.640 --> 15:04.640\n So align with the ethics and the morals of human beings.\n\n15:04.640 --> 15:05.920\n To even say this in a different way,\n\n15:05.920 --> 15:08.600\n I mean, think about how do we align humans, right?\n\n15:08.600 --> 15:10.440\n Think about like a human baby can grow up\n\n15:10.440 --> 15:12.920\n to be an evil person or a great person.\n\n15:12.920 --> 15:15.240\n And a lot of that is from learning from data, right?\n\n15:15.240 --> 15:17.760\n That you have some feedback as a child is growing up,\n\n15:17.760 --> 15:19.200\n they get to see positive examples.\n\n15:19.200 --> 15:22.000\n And so I think that just like,\n\n15:22.000 --> 15:25.400\n that the only example we have of a general intelligence\n\n15:25.400 --> 15:28.040\n that is able to learn from data\n\n15:28.040 --> 15:31.440\n to align with human values and to learn values,\n\n15:31.440 --> 15:32.880\n I think we shouldn't be surprised\n\n15:32.880 --> 15:36.000\n that we can do the same sorts of techniques\n\n15:36.000 --> 15:37.440\n or whether the same sort of techniques\n\n15:37.440 --> 15:41.080\n end up being how we solve value alignment for AGI's.\n\n15:41.080 --> 15:42.720\n So let's go even higher.\n\n15:42.720 --> 15:44.800\n I don't know if you've read the book, Sapiens,\n\n15:44.800 --> 15:48.280\n but there's an idea that, you know,\n\n15:48.280 --> 15:49.960\n that as a collective, as us human beings,\n\n15:49.960 --> 15:54.720\n we kind of develop together ideas that we hold.\n\n15:54.720 --> 15:57.880\n There's no, in that context, objective truth.\n\n15:57.880 --> 15:59.960\n We just kind of all agree to certain ideas\n\n15:59.960 --> 16:01.400\n and hold them as a collective.\n\n16:01.400 --> 16:03.440\n Did you have a sense that there is,\n\n16:03.440 --> 16:05.320\n in the world of good and evil,\n\n16:05.320 --> 16:07.520\n do you have a sense that to the first approximation,\n\n16:07.520 --> 16:10.240\n there are some things that are good\n\n16:10.240 --> 16:14.520\n and that you could teach systems to behave to be good?\n\n16:14.520 --> 16:18.280\n So I think that this actually blends into our third team,\n\n16:18.280 --> 16:19.880\n right, which is the policy team.\n\n16:19.880 --> 16:22.360\n And this is the one, the aspect I think people\n\n16:22.360 --> 16:25.280\n really talk about way less than they should, right?\n\n16:25.280 --> 16:27.640\n Because imagine that we build super powerful systems\n\n16:27.640 --> 16:29.720\n that we've managed to figure out all the mechanisms\n\n16:29.720 --> 16:32.800\n for these things to do whatever the operator wants.\n\n16:32.800 --> 16:34.480\n The most important question becomes,\n\n16:34.480 --> 16:36.720\n who's the operator, what do they want,\n\n16:36.720 --> 16:39.360\n and how is that going to affect everyone else, right?\n\n16:39.360 --> 16:43.080\n And I think that this question of what is good,\n\n16:43.080 --> 16:44.720\n what are those values, I mean,\n\n16:44.720 --> 16:46.600\n I think you don't even have to go to those,\n\n16:46.600 --> 16:48.400\n those very grand existential places\n\n16:48.400 --> 16:50.920\n to start to realize how hard this problem is.\n\n16:50.920 --> 16:52.880\n You just look at different countries\n\n16:52.880 --> 16:54.520\n and cultures across the world,\n\n16:54.520 --> 16:57.120\n and that there's a very different conception\n\n16:57.120 --> 17:01.920\n of how the world works and what kinds of ways\n\n17:01.920 --> 17:03.400\n that society wants to operate.\n\n17:03.400 --> 17:07.000\n And so I think that the really core question\n\n17:07.000 --> 17:09.560\n is actually very concrete,\n\n17:09.560 --> 17:10.980\n and I think it's not a question\n\n17:10.980 --> 17:12.720\n that we have ready answers to, right?\n\n17:12.720 --> 17:14.720\n It's how do you have a world\n\n17:14.720 --> 17:17.280\n where all of the different countries that we have,\n\n17:17.280 --> 17:19.760\n United States, China, Russia,\n\n17:19.760 --> 17:22.760\n and the hundreds of other countries out there\n\n17:22.760 --> 17:26.620\n are able to continue to not just operate\n\n17:26.620 --> 17:28.440\n in the way that they see fit,\n\n17:28.440 --> 17:31.320\n but in the world that emerges\n\n17:32.560 --> 17:34.640\n where you have these very powerful systems\n\n17:36.080 --> 17:37.820\n operating alongside humans,\n\n17:37.820 --> 17:39.820\n ends up being something that empowers humans more,\n\n17:39.820 --> 17:44.140\n that makes human existence be a more meaningful thing,\n\n17:44.140 --> 17:46.440\n and that people are happier and wealthier,\n\n17:46.440 --> 17:49.040\n and able to live more fulfilling lives.\n\n17:49.040 --> 17:51.600\n It's not an obvious thing for how to design that world\n\n17:51.600 --> 17:53.640\n once you have that very powerful system.\n\n17:53.640 --> 17:55.860\n So if we take a little step back,\n\n17:55.860 --> 17:58.260\n and we're having a fascinating conversation,\n\n17:58.260 --> 18:01.920\n and OpenAI is in many ways a tech leader in the world,\n\n18:01.920 --> 18:03.240\n and yet we're thinking about\n\n18:03.240 --> 18:05.480\n these big existential questions,\n\n18:05.480 --> 18:07.060\n which is fascinating, really important.\n\n18:07.060 --> 18:09.200\n I think you're a leader in that space,\n\n18:09.200 --> 18:10.880\n and that's a really important space\n\n18:10.880 --> 18:13.120\n of just thinking how AI affects society\n\n18:13.120 --> 18:14.400\n in a big picture view.\n\n18:14.400 --> 18:17.360\n So Oscar Wilde said, we're all in the gutter,\n\n18:17.360 --> 18:19.040\n but some of us are looking at the stars,\n\n18:19.040 --> 18:22.360\n and I think OpenAI has a charter\n\n18:22.360 --> 18:24.640\n that looks to the stars, I would say,\n\n18:24.640 --> 18:26.920\n to create intelligence, to create general intelligence,\n\n18:26.920 --> 18:29.480\n make it beneficial, safe, and collaborative.\n\n18:29.480 --> 18:33.720\n So can you tell me how that came about,\n\n18:33.720 --> 18:36.360\n how a mission like that and the path\n\n18:36.360 --> 18:39.160\n to creating a mission like that at OpenAI was founded?\n\n18:39.160 --> 18:41.680\n Yeah, so I think that in some ways\n\n18:41.680 --> 18:45.160\n it really boils down to taking a look at the landscape.\n\n18:45.160 --> 18:47.060\n So if you think about the history of AI,\n\n18:47.060 --> 18:49.960\n that basically for the past 60 or 70 years,\n\n18:49.960 --> 18:51.680\n people have thought about this goal\n\n18:51.680 --> 18:54.000\n of what could happen if you could automate\n\n18:54.000 --> 18:55.680\n human intellectual labor.\n\n18:56.700 --> 18:58.280\n Imagine you could build a computer system\n\n18:58.280 --> 19:00.560\n that could do that, what becomes possible?\n\n19:00.560 --> 19:02.440\n We have a lot of sci fi that tells stories\n\n19:02.440 --> 19:04.960\n of various dystopias, and increasingly you have movies\n\n19:04.960 --> 19:06.520\n like Her that tell you a little bit about,\n\n19:06.520 --> 19:09.480\n maybe more of a little bit utopic vision.\n\n19:09.480 --> 19:12.560\n You think about the impacts that we've seen\n\n19:12.560 --> 19:16.280\n from being able to have bicycles for our minds\n\n19:16.280 --> 19:20.360\n and computers, and I think that the impact\n\n19:20.360 --> 19:23.480\n of computers and the internet has just far outstripped\n\n19:23.480 --> 19:26.200\n what anyone really could have predicted.\n\n19:26.200 --> 19:27.420\n And so I think that it's very clear\n\n19:27.420 --> 19:29.360\n that if you can build an AGI,\n\n19:29.360 --> 19:31.600\n it will be the most transformative technology\n\n19:31.600 --> 19:33.000\n that humans will ever create.\n\n19:34.640 --> 19:36.840\n And so what it boils down to then is a question of,\n\n19:36.840 --> 19:39.400\n well, is there a path, is there hope,\n\n19:39.400 --> 19:41.480\n is there a way to build such a system?\n\n19:41.480 --> 19:43.620\n And I think that for 60 or 70 years,\n\n19:43.620 --> 19:47.280\n that people got excited and that ended up\n\n19:47.280 --> 19:49.440\n not being able to deliver on the hopes\n\n19:49.440 --> 19:51.400\n that people had pinned on them.\n\n19:51.400 --> 19:54.880\n And I think that then, that after two winters\n\n19:54.880 --> 19:58.320\n of AI development, that people I think kind of\n\n19:58.320 --> 20:00.520\n almost stopped daring to dream, right?\n\n20:00.520 --> 20:03.240\n That really talking about AGI or thinking about AGI\n\n20:03.240 --> 20:05.640\n became almost this taboo in the community.\n\n20:06.600 --> 20:08.660\n But I actually think that people took the wrong lesson\n\n20:08.660 --> 20:10.080\n from AI history.\n\n20:10.080 --> 20:12.360\n And if you look back, starting in 1959\n\n20:12.360 --> 20:14.240\n is when the Perceptron was released.\n\n20:14.240 --> 20:17.680\n And this is basically one of the earliest neural networks.\n\n20:17.680 --> 20:19.220\n It was released to what was perceived\n\n20:19.220 --> 20:20.820\n as this massive overhype.\n\n20:20.820 --> 20:22.320\n So in the New York Times in 1959,\n\n20:22.320 --> 20:26.380\n you have this article saying that the Perceptron\n\n20:26.380 --> 20:29.160\n will one day recognize people, call out their names,\n\n20:29.160 --> 20:31.440\n instantly translate speech between languages.\n\n20:31.440 --> 20:33.800\n And people at the time looked at this and said,\n\n20:33.800 --> 20:36.080\n this is, your system can't do any of that.\n\n20:36.080 --> 20:38.060\n And basically spent 10 years trying to discredit\n\n20:38.060 --> 20:40.600\n the whole Perceptron direction and succeeded.\n\n20:40.600 --> 20:41.800\n And all the funding dried up.\n\n20:41.800 --> 20:44.960\n And people kind of went in other directions.\n\n20:44.960 --> 20:46.900\n And in the 80s, there was this resurgence.\n\n20:46.900 --> 20:49.280\n And I'd always heard that the resurgence in the 80s\n\n20:49.280 --> 20:51.480\n was due to the invention of backpropagation\n\n20:51.480 --> 20:53.680\n and these algorithms that got people excited.\n\n20:53.680 --> 20:55.720\n But actually the causality was due to people\n\n20:55.720 --> 20:57.140\n building larger computers.\n\n20:57.140 --> 20:59.080\n That you can find these articles from the 80s\n\n20:59.080 --> 21:01.720\n saying that the democratization of computing power\n\n21:01.720 --> 21:02.660\n suddenly meant that you could run\n\n21:02.660 --> 21:04.000\n these larger neural networks.\n\n21:04.000 --> 21:06.280\n And then people started to do all these amazing things.\n\n21:06.280 --> 21:08.000\n Backpropagation algorithm was invented.\n\n21:08.000 --> 21:10.100\n And the neural nets people were running\n\n21:10.100 --> 21:13.040\n were these tiny little 20 neuron neural nets.\n\n21:13.040 --> 21:15.160\n What are you supposed to learn with 20 neurons?\n\n21:15.160 --> 21:18.640\n And so of course, they weren't able to get great results.\n\n21:18.640 --> 21:21.940\n And it really wasn't until 2012 that this approach,\n\n21:21.940 --> 21:24.680\n that's almost the most simple, natural approach\n\n21:24.680 --> 21:27.720\n that people had come up with in the 50s,\n\n21:27.720 --> 21:30.320\n in some ways even in the 40s before there were computers,\n\n21:30.320 --> 21:32.120\n with the Pitts\u2013McCullough neuron,\n\n21:32.120 --> 21:37.120\n suddenly this became the best way of solving problems.\n\n21:37.460 --> 21:39.260\n And I think there are three core properties\n\n21:39.260 --> 21:42.080\n that deep learning has that I think\n\n21:42.080 --> 21:44.100\n are very worth paying attention to.\n\n21:44.100 --> 21:45.900\n The first is generality.\n\n21:45.900 --> 21:48.700\n We have a very small number of deep learning tools.\n\n21:48.700 --> 21:52.340\n SGD, deep neural net, maybe some RL.\n\n21:53.180 --> 21:55.580\n And it solves this huge variety of problems.\n\n21:55.580 --> 21:57.220\n Speech recognition, machine translation,\n\n21:57.220 --> 22:00.980\n game playing, all of these problems, small set of tools.\n\n22:00.980 --> 22:02.740\n So there's the generality.\n\n22:02.740 --> 22:04.980\n There's a second piece, which is the competence.\n\n22:04.980 --> 22:07.020\n You want to solve any of those problems?\n\n22:07.020 --> 22:10.620\n Throw up 40 years worth of normal computer vision research,\n\n22:10.620 --> 22:11.780\n replace it with a deep neural net,\n\n22:11.780 --> 22:13.580\n it's going to work better.\n\n22:13.580 --> 22:16.860\n And there's a third piece, which is the scalability.\n\n22:16.860 --> 22:18.680\n One thing that has been shown time and time again\n\n22:18.680 --> 22:21.740\n is that if you have a larger neural network,\n\n22:21.740 --> 22:25.120\n throw more compute, more data at it, it will work better.\n\n22:25.120 --> 22:28.860\n Those three properties together feel like essential parts\n\n22:28.860 --> 22:30.820\n of building a general intelligence.\n\n22:30.820 --> 22:33.800\n Now it doesn't just mean that if we scale up what we have,\n\n22:33.800 --> 22:35.180\n that we will have an AGI, right?\n\n22:35.180 --> 22:36.780\n There are clearly missing pieces.\n\n22:36.780 --> 22:38.020\n There are missing ideas.\n\n22:38.020 --> 22:40.000\n We need to have answers for reasoning.\n\n22:40.000 --> 22:44.780\n But I think that the core here is that for the first time,\n\n22:44.780 --> 22:47.940\n it feels that we have a paradigm that gives us hope\n\n22:47.940 --> 22:50.580\n that general intelligence can be achievable.\n\n22:50.580 --> 22:52.140\n And so as soon as you believe that,\n\n22:52.140 --> 22:54.460\n everything else comes into focus, right?\n\n22:54.460 --> 22:56.580\n If you imagine that you may be able to,\n\n22:56.580 --> 22:59.820\n and you know that the timeline I think remains uncertain,\n\n22:59.820 --> 23:02.220\n but I think that certainly within our lifetimes\n\n23:02.220 --> 23:04.660\n and possibly within a much shorter period of time\n\n23:04.660 --> 23:06.580\n than people would expect,\n\n23:06.580 --> 23:09.340\n if you can really build the most transformative technology\n\n23:09.340 --> 23:10.660\n that will ever exist,\n\n23:10.660 --> 23:12.620\n you stop thinking about yourself so much, right?\n\n23:12.620 --> 23:14.220\n You start thinking about just like,\n\n23:14.220 --> 23:16.440\n how do you have a world where this goes well?\n\n23:16.440 --> 23:18.180\n And that you need to think about the practicalities\n\n23:18.180 --> 23:19.540\n of how do you build an organization\n\n23:19.540 --> 23:22.020\n and get together a bunch of people and resources\n\n23:22.020 --> 23:25.140\n and to make sure that people feel motivated\n\n23:25.140 --> 23:26.780\n and ready to do it.\n\n23:26.780 --> 23:29.260\n But I think that then you start thinking about,\n\n23:29.260 --> 23:30.580\n well, what if we succeed?\n\n23:30.580 --> 23:32.740\n And how do we make sure that when we succeed,\n\n23:32.740 --> 23:34.020\n that the world is actually the place\n\n23:34.020 --> 23:36.780\n that we want ourselves to exist in?\n\n23:36.780 --> 23:39.500\n And almost in the Rawlsian Veil sense of the word.\n\n23:39.500 --> 23:42.340\n And so that's kind of the broader landscape.\n\n23:42.340 --> 23:45.140\n And OpenAI was really formed in 2015\n\n23:45.140 --> 23:50.140\n with that high level picture of AGI might be possible\n\n23:50.140 --> 23:51.380\n sooner than people think,\n\n23:51.380 --> 23:54.420\n and that we need to try to do our best\n\n23:54.420 --> 23:55.820\n to make sure it's going to go well.\n\n23:55.820 --> 23:57.740\n And then we spent the next couple of years\n\n23:57.740 --> 23:59.180\n really trying to figure out what does that mean?\n\n23:59.180 --> 24:00.500\n How do we do it?\n\n24:00.500 --> 24:03.060\n And I think that typically with a company,\n\n24:03.060 --> 24:06.460\n you start out very small, see you in a co founder,\n\n24:06.460 --> 24:07.900\n and you build a product, you get some users,\n\n24:07.900 --> 24:09.540\n you get a product market fit.\n\n24:09.540 --> 24:11.620\n Then at some point you raise some money,\n\n24:11.620 --> 24:14.940\n you hire people, you scale, and then down the road,\n\n24:14.940 --> 24:16.420\n then the big companies realize you exist\n\n24:16.420 --> 24:17.420\n and try to kill you.\n\n24:17.420 --> 24:19.860\n And for OpenAI, it was basically everything\n\n24:19.860 --> 24:21.260\n in exactly the opposite order.\n\n24:21.260 --> 24:26.260\n Let me just pause for a second, you said a lot of things.\n\n24:26.260 --> 24:29.740\n And let me just admire the jarring aspect\n\n24:29.740 --> 24:33.740\n of what OpenAI stands for, which is daring to dream.\n\n24:33.740 --> 24:35.620\n I mean, you said it's pretty powerful.\n\n24:35.620 --> 24:38.620\n It caught me off guard because I think that's very true.\n\n24:38.620 --> 24:43.620\n The step of just daring to dream about the possibilities\n\n24:43.620 --> 24:47.180\n of creating intelligence in a positive, in a safe way,\n\n24:47.180 --> 24:50.700\n but just even creating intelligence is a very powerful\n\n24:50.700 --> 24:55.700\n is a much needed refreshing catalyst for the AI community.\n\n24:57.460 --> 24:58.860\n So that's the starting point.\n\n24:58.860 --> 25:02.900\n Okay, so then formation of OpenAI, what's that?\n\n25:02.900 --> 25:05.740\n I would just say that when we were starting OpenAI,\n\n25:05.740 --> 25:07.820\n that kind of the first question that we had is,\n\n25:07.820 --> 25:10.380\n is it too late to start a lab\n\n25:10.380 --> 25:12.060\n with a bunch of the best people?\n\n25:12.060 --> 25:13.220\n Right, is that even possible? Wow, okay.\n\n25:13.220 --> 25:14.540\n That was an actual question?\n\n25:14.540 --> 25:17.340\n That was the core question of,\n\n25:17.340 --> 25:19.380\n we had this dinner in July of 2015,\n\n25:19.380 --> 25:21.220\n and that was really what we spent the whole time\n\n25:21.220 --> 25:22.300\n talking about.\n\n25:22.300 --> 25:26.780\n And, you know, because you think about kind of where AI was\n\n25:26.780 --> 25:30.180\n is that it had transitioned from being an academic pursuit\n\n25:30.180 --> 25:32.220\n to an industrial pursuit.\n\n25:32.220 --> 25:34.220\n And so a lot of the best people were in these big\n\n25:34.220 --> 25:36.980\n research labs and that we wanted to start our own one\n\n25:36.980 --> 25:40.540\n that no matter how much resources we could accumulate\n\n25:40.540 --> 25:43.500\n would be pale in comparison to the big tech companies.\n\n25:43.500 --> 25:44.700\n And we knew that.\n\n25:44.700 --> 25:47.020\n And it was a question of, are we going to be actually\n\n25:47.020 --> 25:48.700\n able to get this thing off the ground?\n\n25:48.700 --> 25:49.740\n You need critical mass.\n\n25:49.740 --> 25:52.100\n You can't just do you and a cofounder build a product.\n\n25:52.100 --> 25:55.580\n You really need to have a group of five to 10 people.\n\n25:55.580 --> 25:59.460\n And we kind of concluded it wasn't obviously impossible.\n\n25:59.460 --> 26:00.820\n So it seemed worth trying.\n\n26:02.220 --> 26:04.780\n Well, you're also a dreamer, so who knows, right?\n\n26:04.780 --> 26:05.620\n That's right.\n\n26:05.620 --> 26:10.460\n Okay, so speaking of that, competing with the big players,\n\n26:11.460 --> 26:14.020\n let's talk about some of the tricky things\n\n26:14.020 --> 26:17.420\n as you think through this process of growing,\n\n26:17.420 --> 26:20.060\n of seeing how you can develop these systems\n\n26:20.060 --> 26:22.580\n at a scale that competes.\n\n26:22.580 --> 26:25.660\n So you recently formed OpenAI LP,\n\n26:26.540 --> 26:30.780\n a new cap profit company that now carries the name OpenAI.\n\n26:30.780 --> 26:33.260\n So OpenAI is now this official company.\n\n26:33.260 --> 26:36.500\n The original nonprofit company still exists\n\n26:36.500 --> 26:39.740\n and carries the OpenAI nonprofit name.\n\n26:39.740 --> 26:41.940\n So can you explain what this company is,\n\n26:41.940 --> 26:44.220\n what the purpose of this creation is,\n\n26:44.220 --> 26:48.740\n and how did you arrive at the decision to create it?\n\n26:48.740 --> 26:53.220\n OpenAI, the whole entity and OpenAI LP as a vehicle\n\n26:53.220 --> 26:55.500\n is trying to accomplish the mission\n\n26:55.500 --> 26:57.460\n of ensuring that artificial general intelligence\n\n26:57.460 --> 26:58.740\n benefits everyone.\n\n26:58.740 --> 27:00.180\n And the main way that we're trying to do that\n\n27:00.180 --> 27:02.500\n is by actually trying to build general intelligence\n\n27:02.500 --> 27:04.140\n ourselves and make sure the benefits\n\n27:04.140 --> 27:05.860\n are distributed to the world.\n\n27:05.860 --> 27:07.100\n That's the primary way.\n\n27:07.100 --> 27:09.540\n We're also fine if someone else does this, right?\n\n27:09.540 --> 27:10.580\n Doesn't have to be us.\n\n27:10.580 --> 27:12.540\n If someone else is going to build an AGI\n\n27:12.540 --> 27:14.740\n and make sure that the benefits don't get locked up\n\n27:14.740 --> 27:18.100\n in one company or with one set of people,\n\n27:19.220 --> 27:21.100\n like we're actually fine with that.\n\n27:21.100 --> 27:25.340\n And so those ideas are baked into our charter,\n\n27:25.340 --> 27:28.340\n which is kind of the foundational document\n\n27:28.340 --> 27:31.820\n that describes kind of our values and how we operate.\n\n27:32.780 --> 27:36.300\n But it's also really baked into the structure of OpenAI LP.\n\n27:36.300 --> 27:37.900\n And so the way that we've set up OpenAI LP\n\n27:37.900 --> 27:42.100\n is that in the case where we succeed, right?\n\n27:42.100 --> 27:45.260\n If we actually build what we're trying to build,\n\n27:45.260 --> 27:47.260\n then investors are able to get a return,\n\n27:48.300 --> 27:50.300\n but that return is something that is capped.\n\n27:50.300 --> 27:52.940\n And so if you think of AGI in terms of the value\n\n27:52.940 --> 27:54.100\n that you could really create,\n\n27:54.100 --> 27:56.260\n you're talking about the most transformative technology\n\n27:56.260 --> 27:58.780\n ever created, it's going to create orders of magnitude\n\n27:58.780 --> 28:01.820\n more value than any existing company.\n\n28:01.820 --> 28:05.900\n And that all of that value will be owned by the world,\n\n28:05.900 --> 28:07.820\n like legally titled to the nonprofit\n\n28:07.820 --> 28:09.500\n to fulfill that mission.\n\n28:09.500 --> 28:12.740\n And so that's the structure.\n\n28:12.740 --> 28:15.140\n So the mission is a powerful one,\n\n28:15.140 --> 28:18.860\n and it's one that I think most people would agree with.\n\n28:18.860 --> 28:22.900\n It's how we would hope AI progresses.\n\n28:22.900 --> 28:25.340\n And so how do you tie yourself to that mission?\n\n28:25.340 --> 28:29.180\n How do you make sure you do not deviate from that mission,\n\n28:29.180 --> 28:34.180\n that other incentives that are profit driven\n\n28:35.260 --> 28:36.740\n don't interfere with the mission?\n\n28:36.740 --> 28:39.540\n So this was actually a really core question for us\n\n28:39.540 --> 28:40.900\n for the past couple of years,\n\n28:40.900 --> 28:43.540\n because I'd say that like the way that our history went\n\n28:43.540 --> 28:44.920\n was that for the first year,\n\n28:44.920 --> 28:46.200\n we were getting off the ground, right?\n\n28:46.200 --> 28:47.900\n We had this high level picture,\n\n28:47.900 --> 28:51.860\n but we didn't know exactly how we wanted to accomplish it.\n\n28:51.860 --> 28:55.020\n And really two years ago is when we first started realizing\n\n28:55.020 --> 28:56.140\n in order to build AGI,\n\n28:56.140 --> 28:58.700\n we're just going to need to raise way more money\n\n28:58.700 --> 29:00.180\n than we can as a nonprofit.\n\n29:00.180 --> 29:02.820\n And we're talking many billions of dollars.\n\n29:02.820 --> 29:06.860\n And so the first question is how are you supposed to do that\n\n29:06.860 --> 29:08.700\n and stay true to this mission?\n\n29:08.700 --> 29:10.580\n And we looked at every legal structure out there\n\n29:10.580 --> 29:11.940\n and concluded none of them were quite right\n\n29:11.940 --> 29:13.380\n for what we wanted to do.\n\n29:13.380 --> 29:14.580\n And I guess it shouldn't be too surprising\n\n29:14.580 --> 29:16.920\n if you're gonna do some like crazy unprecedented technology\n\n29:16.920 --> 29:17.980\n that you're gonna have to come with\n\n29:17.980 --> 29:20.340\n some crazy unprecedented structure to do it in.\n\n29:20.340 --> 29:25.340\n And a lot of our conversation was with people at OpenAI,\n\n29:26.140 --> 29:27.220\n the people who really joined\n\n29:27.220 --> 29:29.100\n because they believe so much in this mission\n\n29:29.100 --> 29:31.260\n and thinking about how do we actually\n\n29:31.260 --> 29:33.020\n raise the resources to do it\n\n29:33.020 --> 29:35.900\n and also stay true to what we stand for.\n\n29:35.900 --> 29:37.940\n And the place you gotta start is to really align\n\n29:37.940 --> 29:39.540\n on what is it that we stand for, right?\n\n29:39.540 --> 29:40.500\n What are those values?\n\n29:40.500 --> 29:41.820\n What's really important to us?\n\n29:41.820 --> 29:43.740\n And so I'd say that we spent about a year\n\n29:43.740 --> 29:46.220\n really compiling the OpenAI charter\n\n29:46.220 --> 29:47.540\n and that determines,\n\n29:47.540 --> 29:50.220\n and if you even look at the first line item in there,\n\n29:50.220 --> 29:52.340\n it says that, look, we expect we're gonna have to marshal\n\n29:52.340 --> 29:53.740\n huge amounts of resources,\n\n29:53.740 --> 29:55.720\n but we're going to make sure that we minimize\n\n29:55.720 --> 29:57.620\n conflict of interest with the mission.\n\n29:57.620 --> 30:00.700\n And that kind of aligning on all of those pieces\n\n30:00.700 --> 30:04.180\n was the most important step towards figuring out\n\n30:04.180 --> 30:06.020\n how do we structure a company\n\n30:06.020 --> 30:08.200\n that can actually raise the resources\n\n30:08.200 --> 30:10.300\n to do what we need to do.\n\n30:10.300 --> 30:14.740\n I imagine OpenAI, the decision to create OpenAI LP\n\n30:14.740 --> 30:16.340\n was a really difficult one.\n\n30:16.340 --> 30:17.900\n And there was a lot of discussions,\n\n30:17.900 --> 30:19.600\n as you mentioned, for a year,\n\n30:19.600 --> 30:22.740\n and there was different ideas,\n\n30:22.740 --> 30:25.120\n perhaps detractors within OpenAI,\n\n30:26.100 --> 30:28.900\n sort of different paths that you could have taken.\n\n30:28.900 --> 30:30.220\n What were those concerns?\n\n30:30.220 --> 30:32.020\n What were the different paths considered?\n\n30:32.020 --> 30:34.100\n What was that process of making that decision like?\n\n30:34.100 --> 30:37.720\n Yep, so if you look actually at the OpenAI charter,\n\n30:37.720 --> 30:40.900\n there's almost two paths embedded within it.\n\n30:40.900 --> 30:44.900\n There is, we are primarily trying to build AGI ourselves,\n\n30:44.900 --> 30:47.340\n but we're also okay if someone else does it.\n\n30:47.340 --> 30:49.020\n And this is a weird thing for a company.\n\n30:49.020 --> 30:51.140\n It's really interesting, actually.\n\n30:51.140 --> 30:53.260\n There is an element of competition\n\n30:53.260 --> 30:56.660\n that you do wanna be the one that does it,\n\n30:56.660 --> 30:59.020\n but at the same time, you're okay if somebody else doesn't.\n\n30:59.020 --> 31:01.380\n We'll talk about that a little bit, that trade off,\n\n31:01.380 --> 31:02.940\n that dance that's really interesting.\n\n31:02.940 --> 31:04.600\n And I think this was the core tension\n\n31:04.600 --> 31:06.380\n as we were designing OpenAI LP,\n\n31:06.380 --> 31:08.260\n and really the OpenAI strategy,\n\n31:08.260 --> 31:11.080\n is how do you make sure that both you have a shot\n\n31:11.080 --> 31:12.660\n at being a primary actor,\n\n31:12.660 --> 31:15.820\n which really requires building an organization,\n\n31:15.820 --> 31:17.700\n raising massive resources,\n\n31:17.700 --> 31:19.420\n and really having the will to go\n\n31:19.420 --> 31:22.040\n and execute on some really, really hard vision, right?\n\n31:22.040 --> 31:23.800\n You need to really sign up for a long period\n\n31:23.800 --> 31:27.160\n to go and take on a lot of pain and a lot of risk.\n\n31:27.160 --> 31:30.420\n And to do that, normally you just import\n\n31:30.420 --> 31:31.780\n the startup mindset, right?\n\n31:31.780 --> 31:32.820\n And that you think about, okay,\n\n31:32.820 --> 31:34.300\n like how do we out execute everyone?\n\n31:34.300 --> 31:36.220\n You have this very competitive angle.\n\n31:36.220 --> 31:38.180\n But you also have the second angle of saying that,\n\n31:38.180 --> 31:41.660\n well, the true mission isn't for OpenAI to build AGI.\n\n31:41.660 --> 31:45.140\n The true mission is for AGI to go well for humanity.\n\n31:45.140 --> 31:48.140\n And so how do you take all of those first actions\n\n31:48.140 --> 31:51.380\n and make sure you don't close the door on outcomes\n\n31:51.380 --> 31:54.460\n that would actually be positive and fulfill the mission?\n\n31:54.460 --> 31:56.700\n And so I think it's a very delicate balance, right?\n\n31:56.700 --> 31:59.620\n And I think that going 100% one direction or the other\n\n31:59.620 --> 32:01.340\n is clearly not the correct answer.\n\n32:01.340 --> 32:03.700\n And so I think that even in terms of just how we talk\n\n32:03.700 --> 32:05.440\n about OpenAI and think about it,\n\n32:05.440 --> 32:07.980\n there's just like one thing that's always in the back\n\n32:07.980 --> 32:11.260\n of my mind is to make sure that we're not just saying\n\n32:11.260 --> 32:14.020\n OpenAI's goal is to build AGI, right?\n\n32:14.020 --> 32:15.580\n That it's actually much broader than that, right?\n\n32:15.580 --> 32:18.260\n That first of all, it's not just AGI,\n\n32:18.260 --> 32:20.340\n it's safe AGI that's very important.\n\n32:20.340 --> 32:23.100\n But secondly, our goal isn't to be the ones to build it.\n\n32:23.100 --> 32:24.700\n Our goal is to make sure it goes well for the world.\n\n32:24.700 --> 32:26.100\n And so I think that figuring out\n\n32:26.100 --> 32:27.900\n how do you balance all of those\n\n32:27.900 --> 32:30.220\n and to get people to really come to the table\n\n32:30.220 --> 32:35.220\n and compile a single document that encompasses all of that\n\n32:36.340 --> 32:37.540\n wasn't trivial.\n\n32:37.540 --> 32:41.640\n So part of the challenge here is your mission is,\n\n32:41.640 --> 32:44.220\n I would say, beautiful, empowering,\n\n32:44.220 --> 32:47.500\n and a beacon of hope for people in the research community\n\n32:47.500 --> 32:49.180\n and just people thinking about AI.\n\n32:49.180 --> 32:53.140\n So your decisions are scrutinized more than,\n\n32:53.140 --> 32:55.900\n I think, a regular profit driven company.\n\n32:55.900 --> 32:57.380\n Do you feel the burden of this\n\n32:57.380 --> 32:58.540\n in the creation of the charter\n\n32:58.540 --> 33:00.160\n and just in the way you operate?\n\n33:00.160 --> 33:01.000\n Yes.\n\n33:01.000 --> 33:05.900\n So why do you lean into the burden\n\n33:07.020 --> 33:08.660\n by creating such a charter?\n\n33:08.660 --> 33:10.420\n Why not keep it quiet?\n\n33:10.420 --> 33:12.900\n I mean, it just boils down to the mission, right?\n\n33:12.900 --> 33:15.180\n Like I'm here and everyone else is here\n\n33:15.180 --> 33:17.380\n because we think this is the most important mission.\n\n33:17.380 --> 33:18.980\n Dare to dream.\n\n33:18.980 --> 33:23.340\n All right, so do you think you can be good for the world\n\n33:23.340 --> 33:25.980\n or create an AGI system that's good\n\n33:25.980 --> 33:28.320\n when you're a for profit company?\n\n33:28.320 --> 33:30.660\n From my perspective, I don't understand\n\n33:30.660 --> 33:35.660\n why profit interferes with positive impact on society.\n\n33:37.620 --> 33:40.740\n I don't understand why Google,\n\n33:40.740 --> 33:42.900\n that makes most of its money from ads,\n\n33:42.900 --> 33:45.020\n can't also do good for the world\n\n33:45.020 --> 33:47.500\n or other companies, Facebook, anything.\n\n33:47.500 --> 33:50.220\n I don't understand why those have to interfere.\n\n33:50.220 --> 33:55.100\n You know, profit isn't the thing, in my view,\n\n33:55.100 --> 33:57.200\n that affects the impact of a company.\n\n33:57.200 --> 34:00.340\n What affects the impact of the company is the charter,\n\n34:00.340 --> 34:04.140\n is the culture, is the people inside,\n\n34:04.140 --> 34:07.100\n and profit is the thing that just fuels those people.\n\n34:07.100 --> 34:08.740\n So what are your views there?\n\n34:08.740 --> 34:10.900\n Yeah, so I think that's a really good question\n\n34:10.900 --> 34:14.180\n and there's some real longstanding debates\n\n34:14.180 --> 34:16.460\n in human society that are wrapped up in it.\n\n34:16.460 --> 34:18.640\n The way that I think about it is just think about\n\n34:18.640 --> 34:21.500\n what are the most impactful non profits in the world?\n\n34:23.980 --> 34:26.780\n What are the most impactful for profits in the world?\n\n34:26.780 --> 34:29.260\n Right, it's much easier to list the for profits.\n\n34:29.260 --> 34:32.420\n That's right, and I think that there's some real truth here\n\n34:32.420 --> 34:34.600\n that the system that we set up,\n\n34:34.600 --> 34:38.320\n the system for kind of how today's world is organized,\n\n34:38.320 --> 34:41.300\n is one that really allows for huge impact.\n\n34:41.300 --> 34:45.140\n And that kind of part of that is that you need to be,\n\n34:45.140 --> 34:48.060\n that for profits are self sustaining\n\n34:48.060 --> 34:51.180\n and able to kind of build on their own momentum.\n\n34:51.180 --> 34:53.060\n And I think that's a really powerful thing.\n\n34:53.060 --> 34:55.860\n It's something that when it turns out\n\n34:55.860 --> 34:57.900\n that we haven't set the guardrails correctly,\n\n34:57.900 --> 34:58.820\n causes problems, right?\n\n34:58.820 --> 35:01.600\n Think about logging companies that go into forest,\n\n35:01.600 --> 35:04.680\n the rainforest, that's really bad, we don't want that.\n\n35:04.680 --> 35:06.500\n And it's actually really interesting to me\n\n35:06.500 --> 35:08.940\n that kind of this question of how do you get\n\n35:08.940 --> 35:11.380\n positive benefits out of a for profit company,\n\n35:11.380 --> 35:13.020\n it's actually very similar to how do you get\n\n35:13.020 --> 35:15.800\n positive benefits out of an AGI, right?\n\n35:15.800 --> 35:17.980\n That you have this like very powerful system,\n\n35:17.980 --> 35:19.700\n it's more powerful than any human,\n\n35:19.700 --> 35:21.860\n and is kind of autonomous in some ways,\n\n35:21.860 --> 35:23.740\n it's superhuman in a lot of axes,\n\n35:23.740 --> 35:25.420\n and somehow you have to set the guardrails\n\n35:25.420 --> 35:26.820\n to get good things to happen.\n\n35:26.820 --> 35:29.380\n But when you do, the benefits are massive.\n\n35:29.380 --> 35:32.500\n And so I think that when I think about\n\n35:32.500 --> 35:34.420\n nonprofit versus for profit,\n\n35:34.420 --> 35:36.760\n I think just not enough happens in nonprofits,\n\n35:36.760 --> 35:39.180\n they're very pure, but it's just kind of,\n\n35:39.180 --> 35:40.860\n it's just hard to do things there.\n\n35:40.860 --> 35:43.980\n In for profits in some ways, like too much happens,\n\n35:43.980 --> 35:46.460\n but if kind of shaped in the right way,\n\n35:46.460 --> 35:47.820\n it can actually be very positive.\n\n35:47.820 --> 35:52.100\n And so with OpenAI LP, we're picking a road in between.\n\n35:52.100 --> 35:54.820\n Now the thing that I think is really important to recognize\n\n35:54.820 --> 35:57.140\n is that the way that we think about OpenAI LP\n\n35:57.140 --> 36:00.420\n is that in the world where AGI actually happens, right,\n\n36:00.420 --> 36:01.660\n in a world where we are successful,\n\n36:01.660 --> 36:03.760\n we build the most transformative technology ever,\n\n36:03.760 --> 36:06.580\n the amount of value we're gonna create will be astronomical.\n\n36:07.580 --> 36:12.580\n And so then in that case, that the cap that we have\n\n36:12.760 --> 36:15.540\n will be a small fraction of the value we create,\n\n36:15.540 --> 36:17.800\n and the amount of value that goes back to investors\n\n36:17.800 --> 36:20.020\n and employees looks pretty similar to what would happen\n\n36:20.020 --> 36:21.720\n in a pretty successful startup.\n\n36:23.780 --> 36:26.580\n And that's really the case that we're optimizing for, right?\n\n36:26.580 --> 36:28.600\n That we're thinking about in the success case,\n\n36:28.600 --> 36:32.220\n making sure that the value we create doesn't get locked up.\n\n36:32.220 --> 36:34.980\n And I expect that in other for profit companies\n\n36:34.980 --> 36:37.860\n that it's possible to do something like that.\n\n36:37.860 --> 36:39.780\n I think it's not obvious how to do it, right?\n\n36:39.780 --> 36:41.500\n I think that as a for profit company,\n\n36:41.500 --> 36:44.300\n you have a lot of fiduciary duty to your shareholders\n\n36:44.300 --> 36:45.700\n and that there are certain decisions\n\n36:45.700 --> 36:47.560\n that you just cannot make.\n\n36:47.560 --> 36:49.140\n In our structure, we've set it up\n\n36:49.140 --> 36:52.500\n so that we have a fiduciary duty to the charter,\n\n36:52.500 --> 36:54.460\n that we always get to make the decision\n\n36:54.460 --> 36:57.460\n that is right for the charter rather than,\n\n36:57.460 --> 37:00.700\n even if it comes at the expense of our own stakeholders.\n\n37:00.700 --> 37:03.420\n And so I think that when I think about\n\n37:03.420 --> 37:04.380\n what's really important,\n\n37:04.380 --> 37:06.300\n it's not really about nonprofit versus for profit,\n\n37:06.300 --> 37:09.620\n it's really a question of if you build AGI\n\n37:09.620 --> 37:13.100\n and you kind of, humanity's now in this new age,\n\n37:13.100 --> 37:15.780\n who benefits, whose lives are better?\n\n37:15.780 --> 37:17.180\n And I think that what's really important\n\n37:17.180 --> 37:20.340\n is to have an answer that is everyone.\n\n37:20.340 --> 37:23.380\n Yeah, which is one of the core aspects of the charter.\n\n37:23.380 --> 37:26.540\n So one concern people have, not just with OpenAI,\n\n37:26.540 --> 37:28.420\n but with Google, Facebook, Amazon,\n\n37:28.420 --> 37:33.420\n anybody really that's creating impact at scale\n\n37:35.020 --> 37:37.680\n is how do we avoid, as your charter says,\n\n37:37.680 --> 37:40.100\n avoid enabling the use of AI or AGI\n\n37:40.100 --> 37:43.660\n to unduly concentrate power?\n\n37:43.660 --> 37:45.940\n Why would not a company like OpenAI\n\n37:45.940 --> 37:48.660\n keep all the power of an AGI system to itself?\n\n37:48.660 --> 37:49.540\n The charter.\n\n37:49.540 --> 37:50.380\n The charter.\n\n37:50.380 --> 37:52.020\n So how does the charter\n\n37:53.140 --> 37:57.260\n actualize itself in day to day?\n\n37:57.260 --> 38:00.580\n So I think that first, to zoom out,\n\n38:00.580 --> 38:01.860\n that the way that we structure the company\n\n38:01.860 --> 38:05.560\n is so that the power for sort of dictating the actions\n\n38:05.560 --> 38:08.600\n that OpenAI takes ultimately rests with the board,\n\n38:08.600 --> 38:11.020\n the board of the nonprofit.\n\n38:11.020 --> 38:12.300\n And the board is set up in certain ways\n\n38:12.300 --> 38:14.260\n with certain restrictions that you can read about\n\n38:14.260 --> 38:16.300\n in the OpenAI LP blog post.\n\n38:16.300 --> 38:19.220\n But effectively the board is the governing body\n\n38:19.220 --> 38:21.260\n for OpenAI LP.\n\n38:21.260 --> 38:24.440\n And the board has a duty to fulfill the mission\n\n38:24.440 --> 38:26.420\n of the nonprofit.\n\n38:26.420 --> 38:28.820\n And so that's kind of how we tie,\n\n38:28.820 --> 38:30.980\n how we thread all these things together.\n\n38:30.980 --> 38:32.900\n Now there's a question of, so day to day,\n\n38:32.900 --> 38:34.820\n how do people, the individuals,\n\n38:34.820 --> 38:36.980\n who in some ways are the most empowered ones, right?\n\n38:36.980 --> 38:38.820\n Now the board sort of gets to call the shots\n\n38:38.820 --> 38:40.540\n at the high level, but the people\n\n38:40.540 --> 38:43.140\n who are actually executing are the employees, right?\n\n38:43.140 --> 38:44.820\n People here on a day to day basis\n\n38:44.820 --> 38:47.660\n who have the keys to the technical whole kingdom.\n\n38:48.940 --> 38:51.700\n And there I think that the answer looks a lot like,\n\n38:51.700 --> 38:55.080\n well, how does any company's values get actualized, right?\n\n38:55.080 --> 38:56.680\n And I think that a lot of that comes down to\n\n38:56.680 --> 38:58.120\n that you need people who are here\n\n38:58.120 --> 39:01.300\n because they really believe in that mission\n\n39:01.300 --> 39:02.780\n and they believe in the charter\n\n39:02.780 --> 39:05.420\n and that they are willing to take actions\n\n39:05.420 --> 39:07.060\n that maybe are worse for them,\n\n39:07.060 --> 39:08.580\n but are better for the charter.\n\n39:08.580 --> 39:11.420\n And that's something that's really baked into the culture.\n\n39:11.420 --> 39:13.180\n And honestly, I think it's, you know,\n\n39:13.180 --> 39:14.540\n I think that that's one of the things\n\n39:14.540 --> 39:18.140\n that we really have to work to preserve as time goes on.\n\n39:18.140 --> 39:19.740\n And that's a really important part\n\n39:19.740 --> 39:21.620\n of how we think about hiring people\n\n39:21.620 --> 39:23.020\n and bringing people into OpenAI.\n\n39:23.020 --> 39:25.280\n So there's people here, there's people here\n\n39:25.280 --> 39:30.280\n who could speak up and say, like, hold on a second,\n\n39:30.820 --> 39:34.540\n this is totally against what we stand for, culture wise.\n\n39:34.540 --> 39:35.380\n Yeah, yeah, for sure.\n\n39:35.380 --> 39:37.060\n I mean, I think that we actually have,\n\n39:37.060 --> 39:38.720\n I think that's like a pretty important part\n\n39:38.720 --> 39:41.900\n of how we operate and how we have,\n\n39:41.900 --> 39:44.180\n even again with designing the charter\n\n39:44.180 --> 39:46.700\n and designing OpenAI LP in the first place,\n\n39:46.700 --> 39:48.740\n that there has been a lot of conversation\n\n39:48.740 --> 39:50.500\n with employees here and a lot of times\n\n39:50.500 --> 39:52.400\n where employees said, wait a second,\n\n39:52.400 --> 39:53.940\n this seems like it's going in the wrong direction\n\n39:53.940 --> 39:55.140\n and let's talk about it.\n\n39:55.140 --> 39:57.380\n And so I think one thing that's I think a really,\n\n39:57.380 --> 39:58.900\n and you know, here's actually one thing\n\n39:58.900 --> 40:02.140\n that I think is very unique about us as a small company,\n\n40:02.140 --> 40:04.400\n is that if you're at a massive tech giant,\n\n40:04.400 --> 40:05.720\n that's a little bit hard for someone\n\n40:05.720 --> 40:08.140\n who's a line employee to go and talk to the CEO\n\n40:08.140 --> 40:10.900\n and say, I think that we're doing this wrong.\n\n40:10.900 --> 40:13.060\n And you know, you'll get companies like Google\n\n40:13.060 --> 40:15.740\n that have had some collective action from employees\n\n40:15.740 --> 40:19.420\n to make ethical change around things like Maven.\n\n40:19.420 --> 40:20.700\n And so maybe there are mechanisms\n\n40:20.700 --> 40:22.260\n at other companies that work.\n\n40:22.260 --> 40:24.500\n But here, super easy for anyone to pull me aside,\n\n40:24.500 --> 40:26.340\n to pull Sam aside, to pull Ilya aside,\n\n40:26.340 --> 40:27.780\n and people do it all the time.\n\n40:27.780 --> 40:29.820\n One of the interesting things in the charter\n\n40:29.820 --> 40:31.660\n is this idea that it'd be great\n\n40:31.660 --> 40:34.260\n if you could try to describe or untangle\n\n40:34.260 --> 40:36.460\n switching from competition to collaboration\n\n40:36.460 --> 40:38.820\n in late stage AGI development.\n\n40:38.820 --> 40:39.780\n It's really interesting,\n\n40:39.780 --> 40:42.180\n this dance between competition and collaboration.\n\n40:42.180 --> 40:43.420\n How do you think about that?\n\n40:43.420 --> 40:45.020\n Yeah, assuming that you can actually do\n\n40:45.020 --> 40:47.060\n the technical side of AGI development,\n\n40:47.060 --> 40:48.980\n I think there's going to be two key problems\n\n40:48.980 --> 40:50.460\n with figuring out how do you actually deploy it,\n\n40:50.460 --> 40:51.540\n make it go well.\n\n40:51.540 --> 40:53.180\n The first one of these is the run up\n\n40:53.180 --> 40:56.380\n to building the first AGI.\n\n40:56.380 --> 40:58.940\n You look at how self driving cars are being developed,\n\n40:58.940 --> 41:00.700\n and it's a competitive race.\n\n41:00.700 --> 41:02.580\n And the thing that always happens in competitive race\n\n41:02.580 --> 41:04.200\n is that you have huge amounts of pressure\n\n41:04.200 --> 41:06.700\n to get rid of safety.\n\n41:06.700 --> 41:08.940\n And so that's one thing we're very concerned about,\n\n41:08.940 --> 41:12.020\n is that people, multiple teams figuring out\n\n41:12.020 --> 41:13.620\n we can actually get there,\n\n41:13.620 --> 41:16.740\n but if we took the slower path\n\n41:16.740 --> 41:20.300\n that is more guaranteed to be safe, we will lose.\n\n41:20.300 --> 41:22.380\n And so we're going to take the fast path.\n\n41:22.380 --> 41:25.520\n And so the more that we can both ourselves\n\n41:25.520 --> 41:27.300\n be in a position where we don't generate\n\n41:27.300 --> 41:29.040\n that competitive race, where we say,\n\n41:29.040 --> 41:31.540\n if the race is being run and that someone else\n\n41:31.540 --> 41:33.340\n is further ahead than we are,\n\n41:33.340 --> 41:35.640\n we're not going to try to leapfrog.\n\n41:35.640 --> 41:37.220\n We're going to actually work with them, right?\n\n41:37.220 --> 41:38.840\n We will help them succeed.\n\n41:38.840 --> 41:40.460\n As long as what they're trying to do\n\n41:40.460 --> 41:42.940\n is to fulfill our mission, then we're good.\n\n41:42.940 --> 41:44.860\n We don't have to build AGI ourselves.\n\n41:44.860 --> 41:47.100\n And I think that's a really important commitment from us,\n\n41:47.100 --> 41:49.100\n but it can't just be unilateral, right?\n\n41:49.100 --> 41:51.420\n I think that it's really important that other players\n\n41:51.420 --> 41:53.140\n who are serious about building AGI\n\n41:53.140 --> 41:54.700\n make similar commitments, right?\n\n41:54.700 --> 41:57.820\n I think that, again, to the extent that everyone believes\n\n41:57.820 --> 42:00.060\n that AGI should be something to benefit everyone,\n\n42:00.060 --> 42:01.220\n then it actually really shouldn't matter\n\n42:01.220 --> 42:02.460\n which company builds it.\n\n42:02.460 --> 42:04.140\n And we should all be concerned about the case\n\n42:04.140 --> 42:06.060\n where we just race so hard to get there\n\n42:06.060 --> 42:07.620\n that something goes wrong.\n\n42:07.620 --> 42:09.580\n So what role do you think government,\n\n42:10.540 --> 42:13.820\n our favorite entity, has in setting policy and rules\n\n42:13.820 --> 42:18.300\n about this domain, from research to the development\n\n42:18.300 --> 42:22.900\n to early stage to late stage AI and AGI development?\n\n42:22.900 --> 42:25.660\n So I think that, first of all,\n\n42:25.660 --> 42:28.100\n it's really important that government's in there, right?\n\n42:28.100 --> 42:29.820\n In some way, shape, or form.\n\n42:29.820 --> 42:30.940\n At the end of the day, we're talking about\n\n42:30.940 --> 42:35.140\n building technology that will shape how the world operates,\n\n42:35.140 --> 42:37.300\n and that there needs to be government\n\n42:37.300 --> 42:39.100\n as part of that answer.\n\n42:39.100 --> 42:42.220\n And so that's why we've done a number\n\n42:42.220 --> 42:43.660\n of different congressional testimonies,\n\n42:43.660 --> 42:46.300\n we interact with a number of different lawmakers,\n\n42:46.300 --> 42:50.060\n and that right now, a lot of our message to them\n\n42:50.060 --> 42:54.380\n is that it's not the time for regulation,\n\n42:54.380 --> 42:56.420\n it is the time for measurement, right?\n\n42:56.420 --> 42:59.100\n That our main policy recommendation is that people,\n\n42:59.100 --> 43:00.700\n and the government does this all the time\n\n43:00.700 --> 43:04.900\n with bodies like NIST, spend time trying to figure out\n\n43:04.900 --> 43:07.940\n just where the technology is, how fast it's moving,\n\n43:07.940 --> 43:11.220\n and can really become literate and up to speed\n\n43:11.220 --> 43:13.500\n with respect to what to expect.\n\n43:13.500 --> 43:15.260\n So I think that today, the answer really\n\n43:15.260 --> 43:19.260\n is about measurement, and I think that there will be a time\n\n43:19.260 --> 43:21.740\n and place where that will change.\n\n43:21.740 --> 43:23.820\n And I think it's a little bit hard to predict\n\n43:23.820 --> 43:27.140\n exactly what exactly that trajectory should look like.\n\n43:27.140 --> 43:31.060\n So there will be a point at which regulation,\n\n43:31.060 --> 43:34.220\n federal in the United States, the government steps in\n\n43:34.220 --> 43:39.220\n and helps be the, I don't wanna say the adult in the room,\n\n43:39.500 --> 43:42.420\n to make sure that there is strict rules,\n\n43:42.420 --> 43:45.260\n maybe conservative rules that nobody can cross.\n\n43:45.260 --> 43:47.440\n Well, I think there's kind of maybe two angles to it.\n\n43:47.440 --> 43:49.820\n So today, with narrow AI applications\n\n43:49.820 --> 43:51.980\n that I think there are already existing bodies\n\n43:51.980 --> 43:53.980\n that are responsible and should be responsible\n\n43:53.980 --> 43:55.880\n for regulation, you think about, for example,\n\n43:55.880 --> 43:59.480\n with self driving cars, that you want the national highway.\n\n44:00.340 --> 44:01.180\n Netsa.\n\n44:01.180 --> 44:02.980\n Yeah, exactly, to be regulating that.\n\n44:02.980 --> 44:04.980\n That makes sense, right, that basically what we're saying\n\n44:04.980 --> 44:08.160\n is that we're going to have these technological systems\n\n44:08.160 --> 44:10.640\n that are going to be performing applications\n\n44:10.640 --> 44:12.740\n that humans already do, great.\n\n44:12.740 --> 44:14.820\n We already have ways of thinking about standards\n\n44:14.820 --> 44:16.140\n and safety for those.\n\n44:16.140 --> 44:18.860\n So I think actually empowering those regulators today\n\n44:18.860 --> 44:20.020\n is also pretty important.\n\n44:20.020 --> 44:24.780\n And then I think for AGI, that there's going to be a point\n\n44:24.780 --> 44:26.000\n where we'll have better answers.\n\n44:26.000 --> 44:27.580\n And I think that maybe a similar approach\n\n44:27.580 --> 44:30.500\n of first measurement and start thinking about\n\n44:30.500 --> 44:31.620\n what the rules should be.\n\n44:31.620 --> 44:32.580\n I think it's really important\n\n44:32.580 --> 44:36.260\n that we don't prematurely squash progress.\n\n44:36.260 --> 44:40.140\n I think it's very easy to kind of smother a budding field.\n\n44:40.140 --> 44:42.120\n And I think that's something to really avoid.\n\n44:42.120 --> 44:43.740\n But I don't think that the right way of doing it\n\n44:43.740 --> 44:46.900\n is to say, let's just try to blaze ahead\n\n44:46.900 --> 44:50.260\n and not involve all these other stakeholders.\n\n44:50.260 --> 44:55.260\n So you recently released a paper on GPT2 language modeling,\n\n44:58.820 --> 45:02.020\n but did not release the full model\n\n45:02.020 --> 45:04.380\n because you had concerns about the possible\n\n45:04.380 --> 45:07.480\n negative effects of the availability of such model.\n\n45:07.480 --> 45:10.700\n It's outside of just that decision,\n\n45:10.700 --> 45:14.340\n it's super interesting because of the discussion\n\n45:14.340 --> 45:16.980\n at a societal level, the discourse it creates.\n\n45:16.980 --> 45:19.260\n So it's fascinating in that aspect.\n\n45:19.260 --> 45:22.860\n But if you think that's the specifics here at first,\n\n45:22.860 --> 45:25.860\n what are some negative effects that you envisioned?\n\n45:25.860 --> 45:28.540\n And of course, what are some of the positive effects?\n\n45:28.540 --> 45:30.780\n Yeah, so again, I think to zoom out,\n\n45:30.780 --> 45:33.980\n the way that we thought about GPT2\n\n45:33.980 --> 45:35.760\n is that with language modeling,\n\n45:35.760 --> 45:38.520\n we are clearly on a trajectory right now\n\n45:38.520 --> 45:40.860\n where we scale up our models\n\n45:40.860 --> 45:44.440\n and we get qualitatively better performance.\n\n45:44.440 --> 45:47.340\n GPT2 itself was actually just a scale up\n\n45:47.340 --> 45:50.660\n of a model that we've released in the previous June.\n\n45:50.660 --> 45:52.860\n We just ran it at much larger scale\n\n45:52.860 --> 45:54.300\n and we got these results where\n\n45:54.300 --> 45:57.020\n suddenly starting to write coherent pros,\n\n45:57.020 --> 46:00.020\n which was not something we'd seen previously.\n\n46:00.020 --> 46:01.300\n And what are we doing now?\n\n46:01.300 --> 46:05.740\n Well, we're gonna scale up GPT2 by 10x, by 100x, by 1000x,\n\n46:05.740 --> 46:07.820\n and we don't know what we're gonna get.\n\n46:07.820 --> 46:10.080\n And so it's very clear that the model\n\n46:10.080 --> 46:12.820\n that we released last June,\n\n46:12.820 --> 46:16.420\n I think it's kind of like, it's a good academic toy.\n\n46:16.420 --> 46:18.900\n It's not something that we think is something\n\n46:18.900 --> 46:20.420\n that can really have negative applications\n\n46:20.420 --> 46:21.660\n or to the extent that it can,\n\n46:21.660 --> 46:24.340\n that the positive of people being able to play with it\n\n46:24.340 --> 46:28.260\n is far outweighs the possible harms.\n\n46:28.260 --> 46:32.580\n You fast forward to not GPT2, but GPT20,\n\n46:32.580 --> 46:34.680\n and you think about what that's gonna be like.\n\n46:34.680 --> 46:38.180\n And I think that the capabilities are going to be substantive.\n\n46:38.180 --> 46:41.100\n And so there needs to be a point in between the two\n\n46:41.100 --> 46:43.460\n where you say, this is something\n\n46:43.460 --> 46:45.140\n where we are drawing the line\n\n46:45.140 --> 46:47.940\n and that we need to start thinking about the safety aspects.\n\n46:47.940 --> 46:50.140\n And I think for GPT2, we could have gone either way.\n\n46:50.140 --> 46:52.700\n And in fact, when we had conversations internally\n\n46:52.700 --> 46:54.740\n that we had a bunch of pros and cons,\n\n46:54.740 --> 46:58.140\n and it wasn't clear which one outweighed the other.\n\n46:58.140 --> 46:59.940\n And I think that when we announced that,\n\n46:59.940 --> 47:02.140\n hey, we decide not to release this model,\n\n47:02.140 --> 47:03.560\n then there was a bunch of conversation\n\n47:03.560 --> 47:04.420\n where various people said,\n\n47:04.420 --> 47:06.340\n it's so obvious that you should have just released it.\n\n47:06.340 --> 47:07.180\n There are other people said,\n\n47:07.180 --> 47:08.820\n it's so obvious you should not have released it.\n\n47:08.820 --> 47:10.940\n And I think that that almost definitionally means\n\n47:10.940 --> 47:13.580\n that holding it back was the correct decision.\n\n47:13.580 --> 47:15.900\n Right, if it's not obvious\n\n47:15.900 --> 47:17.620\n whether something is beneficial or not,\n\n47:17.620 --> 47:19.700\n you should probably default to caution.\n\n47:19.700 --> 47:22.420\n And so I think that the overall landscape\n\n47:22.420 --> 47:23.700\n for how we think about it\n\n47:23.700 --> 47:25.900\n is that this decision could have gone either way.\n\n47:25.900 --> 47:27.940\n There are great arguments in both directions,\n\n47:27.940 --> 47:30.060\n but for future models down the road\n\n47:30.060 --> 47:32.300\n and possibly sooner than you'd expect,\n\n47:32.300 --> 47:33.460\n because scaling these things up\n\n47:33.460 --> 47:35.660\n doesn't actually take that long,\n\n47:35.660 --> 47:37.900\n those ones you're definitely not going to want\n\n47:37.900 --> 47:39.560\n to release into the wild.\n\n47:39.560 --> 47:42.600\n And so I think that we almost view this as a test case\n\n47:42.600 --> 47:45.140\n and to see, can we even design,\n\n47:45.140 --> 47:46.580\n you know, how do you have a society\n\n47:46.580 --> 47:47.940\n or how do you have a system\n\n47:47.940 --> 47:49.220\n that goes from having no concept\n\n47:49.220 --> 47:50.500\n of responsible disclosure,\n\n47:50.500 --> 47:53.400\n where the mere idea of not releasing something\n\n47:53.400 --> 47:55.940\n for safety reasons is unfamiliar\n\n47:55.940 --> 47:58.680\n to a world where you say, okay, we have a powerful model,\n\n47:58.680 --> 47:59.660\n let's at least think about it,\n\n47:59.660 --> 48:01.220\n let's go through some process.\n\n48:01.220 --> 48:02.660\n And you think about the security community,\n\n48:02.660 --> 48:03.860\n it took them a long time\n\n48:03.860 --> 48:05.660\n to design responsible disclosure, right?\n\n48:05.660 --> 48:07.160\n You know, you think about this question of,\n\n48:07.160 --> 48:08.740\n well, I have a security exploit,\n\n48:08.740 --> 48:09.720\n I send it to the company,\n\n48:09.720 --> 48:11.980\n the company is like, tries to prosecute me\n\n48:11.980 --> 48:16.020\n or just sit, just ignores it, what do I do, right?\n\n48:16.020 --> 48:17.300\n And so, you know, the alternatives of,\n\n48:17.300 --> 48:19.060\n oh, I just always publish your exploits,\n\n48:19.060 --> 48:20.180\n that doesn't seem good either, right?\n\n48:20.180 --> 48:21.580\n And so it really took a long time\n\n48:21.580 --> 48:25.300\n and took this, it was bigger than any individual, right?\n\n48:25.300 --> 48:27.060\n It's really about building a whole community\n\n48:27.060 --> 48:28.740\n that believe that, okay, we'll have this process\n\n48:28.740 --> 48:30.140\n where you send it to the company, you know,\n\n48:30.140 --> 48:31.660\n if they don't act in a certain time,\n\n48:31.660 --> 48:34.420\n then you can go public and you're not a bad person,\n\n48:34.420 --> 48:36.220\n you've done the right thing.\n\n48:36.220 --> 48:38.620\n And I think that in AI,\n\n48:38.620 --> 48:41.380\n part of the response at GPT2 just proves\n\n48:41.380 --> 48:43.280\n that we don't have any concept of this.\n\n48:44.140 --> 48:47.060\n So that's the high level picture.\n\n48:47.060 --> 48:48.660\n And so I think that,\n\n48:48.660 --> 48:51.220\n I think this was a really important move to make\n\n48:51.220 --> 48:53.980\n and we could have maybe delayed it for GPT3,\n\n48:53.980 --> 48:56.020\n but I'm really glad we did it for GPT2.\n\n48:56.020 --> 48:57.740\n And so now you look at GPT2 itself\n\n48:57.740 --> 48:59.420\n and you think about the substance of, okay,\n\n48:59.420 --> 49:01.300\n what are potential negative applications?\n\n49:01.300 --> 49:04.100\n So you have this model that's been trained on the internet,\n\n49:04.100 --> 49:05.340\n which, you know, it's also going to be\n\n49:05.340 --> 49:06.500\n a bunch of very biased data,\n\n49:06.500 --> 49:09.580\n a bunch of, you know, very offensive content in there,\n\n49:09.580 --> 49:13.180\n and you can ask it to generate content for you\n\n49:13.180 --> 49:14.540\n on basically any topic, right?\n\n49:14.540 --> 49:16.700\n You just give it a prompt and it'll just start writing\n\n49:16.700 --> 49:19.060\n and it writes content like you see on the internet,\n\n49:19.060 --> 49:21.820\n you know, even down to like saying advertisement\n\n49:21.820 --> 49:24.140\n in the middle of some of its generations.\n\n49:24.140 --> 49:26.140\n And you think about the possibilities\n\n49:26.140 --> 49:29.220\n for generating fake news or abusive content.\n\n49:29.220 --> 49:30.300\n And, you know, it's interesting seeing\n\n49:30.300 --> 49:31.820\n what people have done with, you know,\n\n49:31.820 --> 49:34.340\n we released a smaller version of GPT2\n\n49:34.340 --> 49:37.460\n and the people have done things like try to generate,\n\n49:37.460 --> 49:40.700\n you know, take my own Facebook message history\n\n49:40.700 --> 49:43.340\n and generate more Facebook messages like me\n\n49:43.340 --> 49:47.340\n and people generating fake politician content\n\n49:47.340 --> 49:49.500\n or, you know, there's a bunch of things there\n\n49:49.500 --> 49:51.860\n where you at least have to think,\n\n49:51.860 --> 49:53.740\n is this going to be good for the world?\n\n49:54.700 --> 49:56.300\n There's the flip side, which is I think\n\n49:56.300 --> 49:57.780\n that there's a lot of awesome applications\n\n49:57.780 --> 49:59.340\n that we really want to see,\n\n49:59.340 --> 50:02.380\n like creative applications in terms of\n\n50:02.380 --> 50:05.340\n if you have sci fi authors that can work with this tool\n\n50:05.340 --> 50:08.580\n and come up with cool ideas, like that seems awesome\n\n50:08.580 --> 50:11.340\n if we can write better sci fi through the use of these tools\n\n50:11.340 --> 50:13.020\n and we've actually had a bunch of people write into us\n\n50:13.020 --> 50:16.060\n asking, hey, can we use it for, you know,\n\n50:16.060 --> 50:18.300\n a variety of different creative applications?\n\n50:18.300 --> 50:21.780\n So the positive are actually pretty easy to imagine.\n\n50:21.780 --> 50:26.780\n They're, you know, the usual NLP applications\n\n50:26.820 --> 50:30.860\n are really interesting, but let's go there.\n\n50:30.860 --> 50:32.860\n It's kind of interesting to think about a world\n\n50:32.860 --> 50:37.860\n where, look at Twitter, where not just fake news,\n\n50:37.860 --> 50:42.860\n but smarter and smarter bots being able to spread\n\n50:42.980 --> 50:47.300\n in an interesting, complex, networking way information\n\n50:47.300 --> 50:50.700\n that just floods out us regular human beings\n\n50:50.700 --> 50:52.780\n with our original thoughts.\n\n50:52.780 --> 50:57.780\n So what are your views of this world with GPT20, right?\n\n51:00.180 --> 51:01.220\n How do we think about it?\n\n51:01.220 --> 51:03.540\n Again, it's like one of those things about in the 50s\n\n51:03.540 --> 51:08.540\n trying to describe the internet or the smartphone.\n\n51:08.700 --> 51:09.940\n What do you think about that world,\n\n51:09.940 --> 51:11.400\n the nature of information?\n\n51:12.900 --> 51:16.780\n One possibility is that we'll always try to design systems\n\n51:16.780 --> 51:19.660\n that identify robot versus human\n\n51:19.660 --> 51:23.340\n and we'll do so successfully and so we'll authenticate\n\n51:23.340 --> 51:25.700\n that we're still human and the other world is that\n\n51:25.700 --> 51:29.020\n we just accept the fact that we're swimming in a sea\n\n51:29.020 --> 51:32.220\n of fake news and just learn to swim there.\n\n51:32.220 --> 51:37.220\n Well, have you ever seen the popular meme of robot\n\n51:39.860 --> 51:42.020\n with a physical arm and pen clicking the\n\n51:42.020 --> 51:43.460\n I'm not a robot button?\n\n51:43.460 --> 51:44.300\n Yeah.\n\n51:44.300 --> 51:48.620\n I think the truth is that really trying to distinguish\n\n51:48.620 --> 51:52.200\n between robot and human is a losing battle.\n\n51:52.200 --> 51:53.860\n Ultimately, you think it's a losing battle?\n\n51:53.860 --> 51:55.560\n I think it's a losing battle ultimately, right?\n\n51:55.560 --> 51:57.820\n I think that that is, in terms of the content,\n\n51:57.820 --> 51:59.380\n in terms of the actions that you can take.\n\n51:59.380 --> 52:01.220\n I mean, think about how captures have gone, right?\n\n52:01.220 --> 52:02.980\n The captures used to be a very nice, simple,\n\n52:02.980 --> 52:06.340\n you just have this image, all of our OCR is terrible,\n\n52:06.340 --> 52:08.900\n you put a couple of artifacts in it,\n\n52:08.900 --> 52:11.500\n humans are gonna be able to tell what it is.\n\n52:11.500 --> 52:13.300\n An AI system wouldn't be able to.\n\n52:13.300 --> 52:15.740\n Today, I could barely do captures.\n\n52:15.740 --> 52:18.380\n And I think that this is just kind of where we're going.\n\n52:18.380 --> 52:20.420\n I think captures were a moment in time thing\n\n52:20.420 --> 52:22.500\n and as AI systems become more powerful,\n\n52:22.500 --> 52:25.500\n that there being human capabilities that can be measured\n\n52:25.500 --> 52:28.900\n in a very easy, automated way that AIs\n\n52:28.900 --> 52:30.180\n will not be capable of.\n\n52:30.180 --> 52:31.140\n I think that's just like,\n\n52:31.140 --> 52:34.180\n it's just an increasingly hard technical battle.\n\n52:34.180 --> 52:36.260\n But it's not that all hope is lost, right?\n\n52:36.260 --> 52:40.360\n You think about how do we already authenticate ourselves,\n\n52:40.360 --> 52:43.460\n right, that we have systems, we have social security numbers\n\n52:43.460 --> 52:47.700\n if you're in the US or you have ways of identifying\n\n52:47.700 --> 52:50.180\n individual people and having real world identity\n\n52:50.180 --> 52:53.060\n tied to digital identity seems like a step\n\n52:53.060 --> 52:56.220\n towards authenticating the source of content\n\n52:56.220 --> 52:58.260\n rather than the content itself.\n\n52:58.260 --> 52:59.980\n Now, there are problems with that.\n\n52:59.980 --> 53:02.340\n How can you have privacy and anonymity\n\n53:02.340 --> 53:05.460\n in a world where the only content you can really trust is,\n\n53:05.460 --> 53:06.580\n or the only way you can trust content\n\n53:06.580 --> 53:08.560\n is by looking at where it comes from?\n\n53:08.560 --> 53:11.420\n And so I think that building out good reputation networks\n\n53:11.420 --> 53:14.060\n may be one possible solution.\n\n53:14.060 --> 53:17.700\n But yeah, I think that this question is not an obvious one.\n\n53:17.700 --> 53:20.220\n And I think that we, maybe sooner than we think,\n\n53:20.220 --> 53:23.820\n will be in a world where today I often will read a tweet\n\n53:23.820 --> 53:25.980\n and be like, hmm, do I feel like a real human wrote this?\n\n53:25.980 --> 53:27.560\n Or do I feel like this is genuine?\n\n53:27.560 --> 53:30.180\n I feel like I can kind of judge the content a little bit.\n\n53:30.180 --> 53:32.640\n And I think in the future, it just won't be the case.\n\n53:32.640 --> 53:36.900\n You look at, for example, the FCC comments on net neutrality.\n\n53:36.900 --> 53:39.900\n It came out later that millions of those were auto generated\n\n53:39.900 --> 53:41.660\n and that the researchers were able to do\n\n53:41.660 --> 53:44.040\n various statistical techniques to do that.\n\n53:44.040 --> 53:45.100\n What do you do in a world\n\n53:45.100 --> 53:47.720\n where those statistical techniques don't exist?\n\n53:47.720 --> 53:49.180\n It's just impossible to tell the difference\n\n53:49.180 --> 53:50.660\n between humans and AIs.\n\n53:50.660 --> 53:53.980\n And in fact, the most persuasive arguments\n\n53:53.980 --> 53:56.620\n are written by AI.\n\n53:56.620 --> 53:58.660\n All that stuff, it's not sci fi anymore.\n\n53:58.660 --> 54:00.580\n You look at GPT2 making a great argument\n\n54:00.580 --> 54:02.580\n for why recycling is bad for the world.\n\n54:02.580 --> 54:04.460\n You gotta read that and be like, huh, you're right.\n\n54:04.460 --> 54:06.540\n We are addressing just the symptoms.\n\n54:06.540 --> 54:08.140\n Yeah, that's quite interesting.\n\n54:08.140 --> 54:11.380\n I mean, ultimately it boils down to the physical world\n\n54:11.380 --> 54:13.720\n being the last frontier of proving,\n\n54:13.720 --> 54:16.100\n so you said like basically networks of people,\n\n54:16.100 --> 54:19.420\n humans vouching for humans in the physical world.\n\n54:19.420 --> 54:22.980\n And somehow the authentication ends there.\n\n54:22.980 --> 54:24.560\n I mean, if I had to ask you,\n\n54:25.560 --> 54:28.180\n I mean, you're way too eloquent for a human.\n\n54:28.180 --> 54:31.260\n So if I had to ask you to authenticate,\n\n54:31.260 --> 54:33.180\n like prove how do I know you're not a robot\n\n54:33.180 --> 54:34.940\n and how do you know I'm not a robot?\n\n54:34.940 --> 54:35.780\n Yeah.\n\n54:35.780 --> 54:40.540\n I think that's so far where in this space,\n\n54:40.540 --> 54:42.140\n this conversation we just had,\n\n54:42.140 --> 54:44.020\n the physical movements we did,\n\n54:44.020 --> 54:47.060\n is the biggest gap between us and AI systems\n\n54:47.060 --> 54:49.380\n is the physical manipulation.\n\n54:49.380 --> 54:51.300\n So maybe that's the last frontier.\n\n54:51.300 --> 54:55.020\n Well, here's another question is why is,\n\n54:55.020 --> 54:57.300\n why is solving this problem important, right?\n\n54:57.300 --> 54:59.100\n Like what aspects are really important to us?\n\n54:59.100 --> 55:01.220\n And I think that probably where we'll end up\n\n55:01.220 --> 55:03.620\n is we'll hone in on what do we really want\n\n55:03.620 --> 55:06.420\n out of knowing if we're talking to a human.\n\n55:06.420 --> 55:09.460\n And I think that, again, this comes down to identity.\n\n55:09.460 --> 55:11.780\n And so I think that the internet of the future,\n\n55:11.780 --> 55:14.900\n I expect to be one that will have lots of agents out there\n\n55:14.900 --> 55:16.380\n that will interact with you.\n\n55:16.380 --> 55:19.260\n But I think that the question of is this\n\n55:19.260 --> 55:21.580\n flesh, real flesh and blood human\n\n55:21.580 --> 55:23.860\n or is this an automated system,\n\n55:23.860 --> 55:25.820\n may actually just be less important.\n\n55:25.820 --> 55:27.420\n Let's actually go there.\n\n55:27.420 --> 55:32.420\n It's GPT2 is impressive and let's look at GPT20.\n\n55:32.500 --> 55:37.500\n Why is it so bad that all my friends are GPT20?\n\n55:37.500 --> 55:42.500\n Why is it so important on the internet,\n\n55:43.300 --> 55:47.340\n do you think, to interact with only human beings?\n\n55:47.340 --> 55:50.620\n Why can't we live in a world where ideas can come\n\n55:50.620 --> 55:52.940\n from models trained on human data?\n\n55:52.940 --> 55:54.820\n Yeah, I think this is actually\n\n55:54.820 --> 55:55.700\n a really interesting question.\n\n55:55.700 --> 55:58.100\n This comes back to the how do you even picture a world\n\n55:58.100 --> 55:59.580\n with some new technology?\n\n55:59.580 --> 56:02.060\n And I think that one thing that I think is important\n\n56:02.060 --> 56:04.780\n is, you know, let's say honesty.\n\n56:04.780 --> 56:07.820\n And I think that if you have almost in the Turing test\n\n56:07.820 --> 56:12.420\n style sense of technology, you have AIs that are pretending\n\n56:12.420 --> 56:14.100\n to be humans and deceiving you.\n\n56:14.100 --> 56:17.300\n I think that feels like a bad thing, right?\n\n56:17.300 --> 56:19.460\n I think that it's really important that we feel like\n\n56:19.460 --> 56:20.980\n we're in control of our environment, right?\n\n56:20.980 --> 56:23.140\n That we understand who we're interacting with.\n\n56:23.140 --> 56:27.060\n And if it's an AI or a human, that's not something\n\n56:27.060 --> 56:28.420\n that we're being deceived about.\n\n56:28.420 --> 56:31.220\n But I think that the flip side of can I have as meaningful\n\n56:31.220 --> 56:33.980\n of an interaction with an AI as I can with a human?\n\n56:33.980 --> 56:36.620\n Well, I actually think here you can turn to sci fi.\n\n56:36.620 --> 56:39.380\n And her I think is a great example of asking\n\n56:39.380 --> 56:40.860\n this very question, right?\n\n56:40.860 --> 56:42.940\n One thing I really love about her is it really starts out\n\n56:42.940 --> 56:44.660\n almost by asking how meaningful\n\n56:44.660 --> 56:47.020\n are human virtual relationships, right?\n\n56:47.020 --> 56:50.940\n And then you have a human who has a relationship with an AI\n\n56:50.940 --> 56:54.100\n and that you really start to be drawn into that, right?\n\n56:54.100 --> 56:56.700\n That all of your emotional buttons get triggered\n\n56:56.700 --> 56:58.260\n in the same way as if there was a real human\n\n56:58.260 --> 57:00.180\n that was on the other side of that phone.\n\n57:00.180 --> 57:03.540\n And so I think that this is one way of thinking about it\n\n57:03.540 --> 57:06.900\n is that I think that we can have meaningful interactions\n\n57:06.900 --> 57:09.500\n and that if there's a funny joke,\n\n57:09.500 --> 57:10.580\n some sense it doesn't really matter\n\n57:10.580 --> 57:12.660\n if it was written by a human or an AI.\n\n57:12.660 --> 57:14.660\n But what you don't want and why I think\n\n57:14.660 --> 57:17.100\n we should really draw hard lines is deception.\n\n57:17.100 --> 57:19.340\n And I think that as long as we're in a world\n\n57:19.340 --> 57:22.420\n where why do we build AI systems at all, right?\n\n57:22.420 --> 57:24.740\n The reason we want to build them is to enhance human lives,\n\n57:24.740 --> 57:26.420\n to make humans be able to do more things,\n\n57:26.420 --> 57:28.820\n to have humans feel more fulfilled.\n\n57:28.820 --> 57:32.940\n And if we can build AI systems that do that, sign me up.\n\n57:32.940 --> 57:34.980\n So the process of language modeling,\n\n57:36.860 --> 57:38.540\n how far do you think it'd take us?\n\n57:38.540 --> 57:40.420\n Let's look at movie Her.\n\n57:40.420 --> 57:44.780\n Do you think a dialogue, natural language conversation\n\n57:44.780 --> 57:47.580\n is formulated by the Turing test, for example,\n\n57:47.580 --> 57:50.180\n do you think that process could be achieved\n\n57:50.180 --> 57:52.900\n through this kind of unsupervised language modeling?\n\n57:52.900 --> 57:56.700\n So I think the Turing test in its real form\n\n57:56.700 --> 57:58.420\n isn't just about language, right?\n\n57:58.420 --> 58:00.420\n It's really about reasoning too, right?\n\n58:00.420 --> 58:01.660\n To really pass the Turing test,\n\n58:01.660 --> 58:03.660\n I should be able to teach calculus\n\n58:03.660 --> 58:05.340\n to whoever's on the other side\n\n58:05.340 --> 58:07.300\n and have it really understand calculus\n\n58:07.300 --> 58:11.100\n and be able to go and solve new calculus problems.\n\n58:11.100 --> 58:13.780\n And so I think that to really solve the Turing test,\n\n58:13.780 --> 58:16.220\n we need more than what we're seeing with language models.\n\n58:16.220 --> 58:18.500\n We need some way of plugging in reasoning.\n\n58:18.500 --> 58:22.180\n Now, how different will that be from what we already do?\n\n58:22.180 --> 58:23.660\n That's an open question, right?\n\n58:23.660 --> 58:25.260\n Might be that we need some sequence\n\n58:25.260 --> 58:26.980\n of totally radical new ideas,\n\n58:26.980 --> 58:29.340\n or it might be that we just need to kind of shape\n\n58:29.340 --> 58:31.700\n our existing systems in a slightly different way.\n\n58:32.740 --> 58:35.020\n But I think that in terms of how far language modeling\n\n58:35.020 --> 58:37.260\n will go, it's already gone way further\n\n58:37.260 --> 58:39.460\n than many people would have expected, right?\n\n58:39.460 --> 58:40.700\n I think that things like,\n\n58:40.700 --> 58:42.420\n and I think there's a lot of really interesting angles\n\n58:42.420 --> 58:45.660\n to poke in terms of how much does GPT2\n\n58:45.660 --> 58:47.620\n understand physical world?\n\n58:47.620 --> 58:52.060\n Like, you read a little bit about fire underwater in GPT2.\n\n58:52.060 --> 58:53.900\n So it's like, okay, maybe it doesn't quite understand\n\n58:53.900 --> 58:56.660\n what these things are, but at the same time,\n\n58:56.660 --> 58:58.780\n I think that you also see various things\n\n58:58.780 --> 59:00.340\n like smoke coming from flame,\n\n59:00.340 --> 59:02.380\n and a bunch of these things that GPT2,\n\n59:02.380 --> 59:04.580\n it has no body, it has no physical experience,\n\n59:04.580 --> 59:06.980\n it's just statically read data.\n\n59:06.980 --> 59:11.980\n And I think that the answer is like, we don't know yet.\n\n59:13.140 --> 59:15.020\n These questions, though, we're starting to be able\n\n59:15.020 --> 59:17.300\n to actually ask them to physical systems,\n\n59:17.300 --> 59:19.580\n to real systems that exist, and that's very exciting.\n\n59:19.580 --> 59:20.860\n Do you think, what's your intuition?\n\n59:20.860 --> 59:23.700\n Do you think if you just scale language modeling,\n\n59:25.220 --> 59:27.420\n like significantly scale,\n\n59:27.420 --> 59:30.980\n that reasoning can emerge from the same exact mechanisms?\n\n59:30.980 --> 59:34.580\n I think it's unlikely that if we just scale GPT2\n\n59:34.580 --> 59:38.260\n that we'll have reasoning in the full fledged way.\n\n59:38.260 --> 59:39.420\n And I think that there's like,\n\n59:39.420 --> 59:41.180\n the type signature's a little bit wrong, right?\n\n59:41.180 --> 59:44.220\n That like, there's something we do with,\n\n59:44.220 --> 59:45.460\n that we call thinking, right?\n\n59:45.460 --> 59:47.300\n Where we spend a lot of compute,\n\n59:47.300 --> 59:48.820\n like a variable amount of compute,\n\n59:48.820 --> 59:50.340\n to get to better answers, right?\n\n59:50.340 --> 59:52.700\n I think a little bit harder, I get a better answer.\n\n59:52.700 --> 59:54.860\n And that that kind of type signature\n\n59:54.860 --> 59:58.620\n isn't quite encoded in a GPT, right?\n\n59:58.620 --> 1:00:01.580\n GPT will kind of like, it's been a long time,\n\n1:00:01.580 --> 1:00:03.340\n and it's like evolutionary history,\n\n1:00:03.340 --> 1:00:04.380\n baking in all this information,\n\n1:00:04.380 --> 1:00:06.700\n getting very, very good at this predictive process.\n\n1:00:06.700 --> 1:00:10.020\n And then at runtime, I just kind of do one forward pass,\n\n1:00:10.020 --> 1:00:12.940\n and I'm able to generate stuff.\n\n1:00:12.940 --> 1:00:15.260\n And so, you know, there might be small tweaks\n\n1:00:15.260 --> 1:00:17.700\n to what we do in order to get the type signature, right?\n\n1:00:17.700 --> 1:00:19.140\n For example, well, you know,\n\n1:00:19.140 --> 1:00:20.700\n it's not really one forward pass, right?\n\n1:00:20.700 --> 1:00:22.300\n You know, you generate symbol by symbol,\n\n1:00:22.300 --> 1:00:24.340\n and so maybe you generate like a whole sequence\n\n1:00:24.340 --> 1:00:26.540\n of thoughts, and you only keep like the last bit\n\n1:00:26.540 --> 1:00:27.860\n or something.\n\n1:00:27.860 --> 1:00:29.500\n But I think that at the very least,\n\n1:00:29.500 --> 1:00:31.820\n I would expect you have to make changes like that.\n\n1:00:31.820 --> 1:00:35.220\n Yeah, just exactly how we, you said, think,\n\n1:00:35.220 --> 1:00:38.060\n is the process of generating thought by thought\n\n1:00:38.060 --> 1:00:40.060\n in the same kind of way, like you said,\n\n1:00:40.060 --> 1:00:43.220\n keep the last bit, the thing that we converge towards.\n\n1:00:43.220 --> 1:00:44.700\n Yep.\n\n1:00:44.700 --> 1:00:46.980\n And I think there's another piece which is interesting,\n\n1:00:46.980 --> 1:00:49.940\n which is this out of distribution generalization, right?\n\n1:00:49.940 --> 1:00:52.300\n That like thinking somehow lets us do that, right?\n\n1:00:52.300 --> 1:00:54.780\n That we haven't experienced a thing, and yet somehow\n\n1:00:54.780 --> 1:00:57.780\n we just kind of keep refining our mental model of it.\n\n1:00:57.780 --> 1:01:00.340\n This is, again, something that feels tied\n\n1:01:00.340 --> 1:01:04.620\n to whatever reasoning is, and maybe it's a small tweak\n\n1:01:04.620 --> 1:01:06.380\n to what we do, maybe it's many ideas,\n\n1:01:06.380 --> 1:01:07.820\n and we'll take as many decades.\n\n1:01:07.820 --> 1:01:09.660\n Yeah, so the assumption there,\n\n1:01:10.940 --> 1:01:12.980\n generalization out of distribution,\n\n1:01:12.980 --> 1:01:16.620\n is that it's possible to create new ideas.\n\n1:01:16.620 --> 1:01:17.460\n Mm hmm.\n\n1:01:17.460 --> 1:01:19.780\n You know, it's possible that nobody's ever created\n\n1:01:19.780 --> 1:01:24.780\n any new ideas, and then with scaling GPT2 to GPT20,\n\n1:01:25.340 --> 1:01:30.340\n you would essentially generalize to all possible thoughts\n\n1:01:30.340 --> 1:01:31.780\n that us humans could have.\n\n1:01:31.780 --> 1:01:33.180\n I mean.\n\n1:01:33.180 --> 1:01:34.180\n Just to play devil's advocate.\n\n1:01:34.180 --> 1:01:37.260\n Right, right, right, I mean, how many new story ideas\n\n1:01:37.260 --> 1:01:39.060\n have we come up with since Shakespeare, right?\n\n1:01:39.060 --> 1:01:40.100\n Yeah, exactly.\n\n1:01:40.100 --> 1:01:44.620\n It's just all different forms of love and drama and so on.\n\n1:01:44.620 --> 1:01:45.740\n Okay.\n\n1:01:45.740 --> 1:01:47.460\n Not sure if you read Bitter Lesson,\n\n1:01:47.460 --> 1:01:49.340\n a recent blog post by Rich Sutton.\n\n1:01:49.340 --> 1:01:50.820\n Yep, I have.\n\n1:01:50.820 --> 1:01:54.380\n He basically says something that echoes some of the ideas\n\n1:01:54.380 --> 1:01:56.780\n that you've been talking about, which is,\n\n1:01:56.780 --> 1:01:58.980\n he says the biggest lesson that can be read\n\n1:01:58.980 --> 1:02:01.980\n from 70 years of AI research is that general methods\n\n1:02:01.980 --> 1:02:05.900\n that leverage computation are ultimately going to,\n\n1:02:05.900 --> 1:02:07.820\n ultimately win out.\n\n1:02:07.820 --> 1:02:08.860\n Do you agree with this?\n\n1:02:08.860 --> 1:02:12.780\n So basically, and OpenAI in general,\n\n1:02:12.780 --> 1:02:15.780\n but the ideas you're exploring about coming up with methods,\n\n1:02:15.780 --> 1:02:20.060\n whether it's GPT2 modeling or whether it's OpenAI 5\n\n1:02:20.060 --> 1:02:23.940\n playing Dota, or a general method is better\n\n1:02:23.940 --> 1:02:26.940\n than a more fine tuned, expert tuned method.\n\n1:02:29.700 --> 1:02:32.140\n Yeah, so I think that, well one thing that I think\n\n1:02:32.140 --> 1:02:33.740\n was really interesting about the reaction\n\n1:02:33.740 --> 1:02:36.380\n to that blog post was that a lot of people have read this\n\n1:02:36.380 --> 1:02:39.380\n as saying that compute is all that matters.\n\n1:02:39.380 --> 1:02:41.300\n And that's a very threatening idea, right?\n\n1:02:41.300 --> 1:02:43.500\n And I don't think it's a true idea either.\n\n1:02:43.500 --> 1:02:45.740\n Right, it's very clear that we have algorithmic ideas\n\n1:02:45.740 --> 1:02:47.820\n that have been very important for making progress\n\n1:02:47.820 --> 1:02:49.460\n and to really build AGI.\n\n1:02:49.460 --> 1:02:52.060\n You wanna push as far as you can on the computational scale\n\n1:02:52.060 --> 1:02:55.500\n and you wanna push as far as you can on human ingenuity.\n\n1:02:55.500 --> 1:02:56.980\n And so I think you need both.\n\n1:02:56.980 --> 1:02:58.260\n But I think the way that you phrased the question\n\n1:02:58.260 --> 1:02:59.580\n is actually very good, right?\n\n1:02:59.580 --> 1:03:02.140\n That it's really about what kind of ideas\n\n1:03:02.140 --> 1:03:03.940\n should we be striving for?\n\n1:03:03.940 --> 1:03:07.540\n And absolutely, if you can find a scalable idea,\n\n1:03:07.540 --> 1:03:09.780\n you pour more compute into it, you pour more data into it,\n\n1:03:09.780 --> 1:03:13.740\n it gets better, like that's the real holy grail.\n\n1:03:13.740 --> 1:03:16.580\n And so I think that the answer to the question,\n\n1:03:16.580 --> 1:03:19.900\n I think, is yes, that that's really how we think about it\n\n1:03:19.900 --> 1:03:22.700\n and that part of why we're excited about the power\n\n1:03:22.700 --> 1:03:25.260\n of deep learning, the potential for building AGI\n\n1:03:25.260 --> 1:03:27.540\n is because we look at the systems that exist\n\n1:03:27.540 --> 1:03:29.700\n in the most successful AI systems\n\n1:03:29.700 --> 1:03:32.620\n and we realize that you scale those up,\n\n1:03:32.620 --> 1:03:33.940\n they're gonna work better.\n\n1:03:33.940 --> 1:03:35.780\n And I think that that scalability\n\n1:03:35.780 --> 1:03:37.020\n is something that really gives us hope\n\n1:03:37.020 --> 1:03:39.540\n for being able to build transformative systems.\n\n1:03:39.540 --> 1:03:43.780\n So I'll tell you, this is partially an emotional,\n\n1:03:43.780 --> 1:03:45.660\n a response that people often have,\n\n1:03:45.660 --> 1:03:49.700\n if compute is so important for state of the art performance,\n\n1:03:49.700 --> 1:03:51.780\n individual developers, maybe a 13 year old\n\n1:03:51.780 --> 1:03:54.420\n sitting somewhere in Kansas or something like that,\n\n1:03:54.420 --> 1:03:56.940\n they're sitting, they might not even have a GPU\n\n1:03:56.940 --> 1:03:59.980\n or may have a single GPU, a 1080 or something like that,\n\n1:03:59.980 --> 1:04:02.580\n and there's this feeling like, well,\n\n1:04:02.580 --> 1:04:05.700\n how can I possibly compete or contribute\n\n1:04:05.700 --> 1:04:09.780\n to this world of AI if scale is so important?\n\n1:04:09.780 --> 1:04:12.460\n So if you can comment on that and in general,\n\n1:04:12.460 --> 1:04:14.780\n do you think we need to also in the future\n\n1:04:14.780 --> 1:04:19.780\n focus on democratizing compute resources more\n\n1:04:19.980 --> 1:04:22.620\n or as much as we democratize the algorithms?\n\n1:04:22.620 --> 1:04:23.900\n Well, so the way that I think about it\n\n1:04:23.900 --> 1:04:28.820\n is that there's this space of possible progress, right?\n\n1:04:28.820 --> 1:04:30.860\n There's a space of ideas and sort of systems\n\n1:04:30.860 --> 1:04:32.900\n that will work that will move us forward\n\n1:04:32.900 --> 1:04:34.780\n and there's a portion of that space\n\n1:04:34.780 --> 1:04:37.020\n and to some extent, an increasingly significant portion\n\n1:04:37.020 --> 1:04:38.780\n of that space that does just require\n\n1:04:38.780 --> 1:04:40.980\n massive compute resources.\n\n1:04:40.980 --> 1:04:44.660\n And for that, I think that the answer is kind of clear\n\n1:04:44.660 --> 1:04:47.860\n and that part of why we have the structure that we do\n\n1:04:47.860 --> 1:04:49.580\n is because we think it's really important\n\n1:04:49.580 --> 1:04:51.660\n to be pushing the scale and to be building\n\n1:04:51.660 --> 1:04:53.740\n these large clusters and systems.\n\n1:04:53.740 --> 1:04:55.820\n But there's another portion of the space\n\n1:04:55.820 --> 1:04:57.780\n that isn't about the large scale compute\n\n1:04:57.780 --> 1:04:59.900\n that are these ideas that, and again,\n\n1:04:59.900 --> 1:05:02.140\n I think that for the ideas to really be impactful\n\n1:05:02.140 --> 1:05:04.140\n and really shine, that they should be ideas\n\n1:05:04.140 --> 1:05:06.580\n that if you scale them up, would work way better\n\n1:05:06.580 --> 1:05:08.740\n than they do at small scale.\n\n1:05:08.740 --> 1:05:10.420\n But that you can discover them\n\n1:05:10.420 --> 1:05:12.700\n without massive computational resources.\n\n1:05:12.700 --> 1:05:15.140\n And if you look at the history of recent developments,\n\n1:05:15.140 --> 1:05:17.620\n you think about things like the GAN or the VAE,\n\n1:05:17.620 --> 1:05:20.860\n that these are ones that I think you could come up with them\n\n1:05:20.860 --> 1:05:22.660\n without having, and in practice,\n\n1:05:22.660 --> 1:05:24.460\n people did come up with them without having\n\n1:05:24.460 --> 1:05:26.500\n massive, massive computational resources.\n\n1:05:26.500 --> 1:05:27.900\n Right, I just talked to Ian Goodfellow,\n\n1:05:27.900 --> 1:05:31.500\n but the thing is the initial GAN\n\n1:05:31.500 --> 1:05:34.140\n produced pretty terrible results, right?\n\n1:05:34.140 --> 1:05:36.220\n So only because it was in a very specific,\n\n1:05:36.220 --> 1:05:38.220\n it was only because they're smart enough\n\n1:05:38.220 --> 1:05:39.940\n to know that this is quite surprising\n\n1:05:39.940 --> 1:05:43.100\n it can generate anything that they know.\n\n1:05:43.100 --> 1:05:45.980\n Do you see a world, or is that too optimistic and dreamer\n\n1:05:45.980 --> 1:05:49.700\n like to imagine that the compute resources\n\n1:05:49.700 --> 1:05:52.180\n are something that's owned by governments\n\n1:05:52.180 --> 1:05:55.020\n and provided as utility?\n\n1:05:55.020 --> 1:05:57.100\n Actually, to some extent, this question reminds me\n\n1:05:57.100 --> 1:06:01.140\n of a blog post from one of my former professors at Harvard,\n\n1:06:01.140 --> 1:06:03.740\n this guy Matt Welsh, who was a systems professor.\n\n1:06:03.740 --> 1:06:05.300\n I remember sitting in his tenure talk, right,\n\n1:06:05.300 --> 1:06:08.780\n and that he had literally just gotten tenure.\n\n1:06:08.780 --> 1:06:10.940\n He went to Google for the summer\n\n1:06:10.940 --> 1:06:15.660\n and then decided he wasn't going back to academia, right?\n\n1:06:15.660 --> 1:06:18.340\n And kind of in his blog post, he makes this point that,\n\n1:06:18.340 --> 1:06:20.780\n look, as a systems researcher,\n\n1:06:20.780 --> 1:06:23.180\n that I come up with these cool system ideas, right,\n\n1:06:23.180 --> 1:06:25.060\n and I kind of build a little proof of concept,\n\n1:06:25.060 --> 1:06:27.060\n and the best thing I can hope for\n\n1:06:27.060 --> 1:06:30.100\n is that the people at Google or Yahoo,\n\n1:06:30.100 --> 1:06:31.580\n which was around at the time,\n\n1:06:31.580 --> 1:06:35.380\n will implement it and actually make it work at scale, right?\n\n1:06:35.380 --> 1:06:36.580\n That's like the dream for me, right?\n\n1:06:36.580 --> 1:06:37.420\n I build the little thing,\n\n1:06:37.420 --> 1:06:39.980\n and they turn it into the big thing that's actually working.\n\n1:06:39.980 --> 1:06:43.340\n And for him, he said, I'm done with that.\n\n1:06:43.340 --> 1:06:45.740\n I want to be the person who's actually doing building\n\n1:06:45.740 --> 1:06:47.300\n and deploying.\n\n1:06:47.300 --> 1:06:49.540\n And I think that there's a similar dichotomy here, right?\n\n1:06:49.540 --> 1:06:53.340\n I think that there are people who really actually find value,\n\n1:06:53.340 --> 1:06:55.180\n and I think it is a valuable thing to do\n\n1:06:55.180 --> 1:06:57.420\n to be the person who produces those ideas, right,\n\n1:06:57.420 --> 1:06:58.820\n who builds the proof of concept.\n\n1:06:58.820 --> 1:07:00.540\n And yeah, you don't get to generate\n\n1:07:00.540 --> 1:07:02.740\n the coolest possible GAN images,\n\n1:07:02.740 --> 1:07:04.460\n but you invented the GAN, right?\n\n1:07:04.460 --> 1:07:07.540\n And so there's a real trade off there,\n\n1:07:07.540 --> 1:07:09.020\n and I think that that's a very personal choice,\n\n1:07:09.020 --> 1:07:10.820\n but I think there's value in both sides.\n\n1:07:10.820 --> 1:07:15.820\n So do you think creating AGI or some new models,\n\n1:07:18.260 --> 1:07:20.460\n we would see echoes of the brilliance\n\n1:07:20.460 --> 1:07:22.260\n even at the prototype level?\n\n1:07:22.260 --> 1:07:24.900\n So you would be able to develop those ideas without scale,\n\n1:07:24.900 --> 1:07:27.300\n the initial seeds.\n\n1:07:27.300 --> 1:07:28.980\n So take a look at, you know,\n\n1:07:28.980 --> 1:07:31.740\n I always like to look at examples that exist, right?\n\n1:07:31.740 --> 1:07:32.700\n Look at real precedent.\n\n1:07:32.700 --> 1:07:37.020\n And so take a look at the June 2018 model that we released,\n\n1:07:37.020 --> 1:07:39.180\n that we scaled up to turn into GPT2.\n\n1:07:39.180 --> 1:07:41.260\n And you can see that at small scale,\n\n1:07:41.260 --> 1:07:42.780\n it set some records, right?\n\n1:07:42.780 --> 1:07:44.820\n This was the original GPT.\n\n1:07:44.820 --> 1:07:46.820\n We actually had some cool generations.\n\n1:07:46.820 --> 1:07:49.820\n They weren't nearly as amazing and really stunning\n\n1:07:49.820 --> 1:07:51.980\n as the GPT2 ones, but it was promising.\n\n1:07:51.980 --> 1:07:53.020\n It was interesting.\n\n1:07:53.020 --> 1:07:54.500\n And so I think it is the case\n\n1:07:54.500 --> 1:07:56.100\n that with a lot of these ideas,\n\n1:07:56.100 --> 1:07:58.260\n that you see promise at small scale.\n\n1:07:58.260 --> 1:08:00.820\n But there is an asterisk here, a very big asterisk,\n\n1:08:00.820 --> 1:08:05.220\n which is sometimes we see behaviors that emerge\n\n1:08:05.220 --> 1:08:07.260\n that are qualitatively different\n\n1:08:07.260 --> 1:08:09.060\n from anything we saw at small scale.\n\n1:08:09.060 --> 1:08:12.580\n And that the original inventor of whatever algorithm\n\n1:08:12.580 --> 1:08:15.500\n looks at and says, I didn't think it could do that.\n\n1:08:15.500 --> 1:08:17.420\n This is what we saw in Dota, right?\n\n1:08:17.420 --> 1:08:19.340\n So PPO was created by John Shulman,\n\n1:08:19.340 --> 1:08:20.540\n who's a researcher here.\n\n1:08:20.540 --> 1:08:24.660\n And with Dota, we basically just ran PPO\n\n1:08:24.660 --> 1:08:26.540\n at massive, massive scale.\n\n1:08:26.540 --> 1:08:29.100\n And there's some tweaks in order to make it work,\n\n1:08:29.100 --> 1:08:31.540\n but fundamentally, it's PPO at the core.\n\n1:08:31.540 --> 1:08:35.300\n And we were able to get this long term planning,\n\n1:08:35.300 --> 1:08:38.700\n these behaviors to really play out on a time scale\n\n1:08:38.700 --> 1:08:40.780\n that we just thought was not possible.\n\n1:08:40.780 --> 1:08:42.700\n And John looked at that and was like,\n\n1:08:42.700 --> 1:08:44.220\n I didn't think it could do that.\n\n1:08:44.220 --> 1:08:45.460\n That's what happens when you're at three orders\n\n1:08:45.460 --> 1:08:48.380\n of magnitude more scale than you tested at.\n\n1:08:48.380 --> 1:08:50.580\n Yeah, but it still has the same flavors of,\n\n1:08:50.580 --> 1:08:55.580\n you know, at least echoes of the expected billions.\n\n1:08:55.980 --> 1:08:59.020\n Although I suspect with GPT scaled more and more,\n\n1:08:59.020 --> 1:09:01.780\n you might get surprising things.\n\n1:09:01.780 --> 1:09:04.740\n So yeah, you're right, it's interesting.\n\n1:09:04.740 --> 1:09:07.940\n It's difficult to see how far an idea will go\n\n1:09:07.940 --> 1:09:09.300\n when it's scaled.\n\n1:09:09.300 --> 1:09:11.020\n It's an open question.\n\n1:09:11.020 --> 1:09:13.060\n Well, so to that point with Dota and PPO,\n\n1:09:13.060 --> 1:09:14.980\n like, I mean, here's a very concrete one, right?\n\n1:09:14.980 --> 1:09:16.620\n It's like, it's actually one thing\n\n1:09:16.620 --> 1:09:17.700\n that's very surprising about Dota\n\n1:09:17.700 --> 1:09:20.340\n that I think people don't really pay that much attention to\n\n1:09:20.340 --> 1:09:22.380\n is the decree of generalization\n\n1:09:22.380 --> 1:09:24.580\n out of distribution that happens, right?\n\n1:09:24.580 --> 1:09:27.860\n That you have this AI that's trained against other bots\n\n1:09:27.860 --> 1:09:30.340\n for its entirety, the entirety of its existence.\n\n1:09:30.340 --> 1:09:31.460\n Sorry to take a step back.\n\n1:09:31.460 --> 1:09:36.460\n Can you talk through, you know, a story of Dota,\n\n1:09:37.260 --> 1:09:42.060\n a story of leading up to opening I5 and that past,\n\n1:09:42.060 --> 1:09:43.900\n and what was the process of self play\n\n1:09:43.900 --> 1:09:45.420\n and so on of training on this?\n\n1:09:45.420 --> 1:09:46.260\n Yeah, yeah, yeah.\n\n1:09:46.260 --> 1:09:47.100\n So with Dota.\n\n1:09:47.100 --> 1:09:47.940\n What is Dota?\n\n1:09:47.940 --> 1:09:50.020\n Yeah, Dota is a complex video game\n\n1:09:50.020 --> 1:09:52.700\n and we started trying to solve Dota\n\n1:09:52.700 --> 1:09:55.660\n because we felt like this was a step towards the real world\n\n1:09:55.660 --> 1:09:58.020\n relative to other games like chess or Go, right?\n\n1:09:58.020 --> 1:09:59.180\n Those very cerebral games\n\n1:09:59.180 --> 1:10:00.500\n where you just kind of have this board,\n\n1:10:00.500 --> 1:10:01.900\n very discreet moves.\n\n1:10:01.900 --> 1:10:04.060\n Dota starts to be much more continuous time\n\n1:10:04.060 --> 1:10:06.220\n that you have this huge variety of different actions\n\n1:10:06.220 --> 1:10:07.660\n that you have a 45 minute game\n\n1:10:07.660 --> 1:10:09.380\n with all these different units\n\n1:10:09.380 --> 1:10:11.820\n and it's got a lot of messiness to it\n\n1:10:11.820 --> 1:10:14.500\n that really hasn't been captured by previous games.\n\n1:10:14.500 --> 1:10:17.340\n And famously, all of the hard coded bots for Dota\n\n1:10:17.340 --> 1:10:18.380\n were terrible, right?\n\n1:10:18.380 --> 1:10:19.940\n It's just impossible to write anything good for it\n\n1:10:19.940 --> 1:10:21.260\n because it's so complex.\n\n1:10:21.260 --> 1:10:23.300\n And so this seemed like a really good place\n\n1:10:23.300 --> 1:10:25.260\n to push what's the state of the art\n\n1:10:25.260 --> 1:10:26.860\n in reinforcement learning.\n\n1:10:26.860 --> 1:10:28.380\n And so we started by focusing\n\n1:10:28.380 --> 1:10:29.980\n on the one versus one version of the game\n\n1:10:29.980 --> 1:10:32.380\n and we're able to solve that.\n\n1:10:32.380 --> 1:10:33.900\n We're able to beat the world champions\n\n1:10:33.900 --> 1:10:38.900\n and the skill curve was this crazy exponential, right?\n\n1:10:38.980 --> 1:10:41.020\n And it was like constantly we were just scaling up\n\n1:10:41.020 --> 1:10:42.260\n that we were fixing bugs\n\n1:10:42.260 --> 1:10:44.340\n and that you look at the skill curve\n\n1:10:44.340 --> 1:10:46.660\n and it was really a very, very smooth one.\n\n1:10:46.660 --> 1:10:47.500\n This is actually really interesting\n\n1:10:47.500 --> 1:10:50.020\n to see how that human iteration loop\n\n1:10:50.020 --> 1:10:52.740\n yielded very steady exponential progress.\n\n1:10:52.740 --> 1:10:55.220\n And to one side note, first of all,\n\n1:10:55.220 --> 1:10:57.140\n it's an exceptionally popular video game.\n\n1:10:57.140 --> 1:11:00.300\n The side effect is that there's a lot of incredible\n\n1:11:00.300 --> 1:11:01.960\n human experts at that video game.\n\n1:11:01.960 --> 1:11:05.260\n So the benchmark that you're trying to reach is very high.\n\n1:11:05.260 --> 1:11:07.900\n And the other, can you talk about the approach\n\n1:11:07.900 --> 1:11:10.140\n that was used initially and throughout\n\n1:11:10.140 --> 1:11:12.100\n training these agents to play this game?\n\n1:11:12.100 --> 1:11:14.420\n Yep, and so the approach that we used is self play.\n\n1:11:14.420 --> 1:11:17.380\n And so you have two agents that don't know anything.\n\n1:11:17.380 --> 1:11:18.700\n They battle each other,\n\n1:11:18.700 --> 1:11:20.820\n they discover something a little bit good\n\n1:11:20.820 --> 1:11:22.060\n and now they both know it.\n\n1:11:22.060 --> 1:11:23.400\n And they just get better and better and better\n\n1:11:23.400 --> 1:11:24.540\n without bound.\n\n1:11:24.540 --> 1:11:27.100\n And that's a really powerful idea, right?\n\n1:11:27.100 --> 1:11:30.180\n That we then went from the one versus one version\n\n1:11:30.180 --> 1:11:32.460\n of the game and scaled up to five versus five, right?\n\n1:11:32.460 --> 1:11:34.340\n So you think about kind of like with basketball\n\n1:11:34.340 --> 1:11:35.500\n where you have this like team sport\n\n1:11:35.500 --> 1:11:37.700\n and you need to do all this coordination\n\n1:11:37.700 --> 1:11:40.940\n and we were able to push the same idea,\n\n1:11:40.940 --> 1:11:45.940\n the same self play to really get to the professional level\n\n1:11:45.940 --> 1:11:48.980\n at the full five versus five version of the game.\n\n1:11:48.980 --> 1:11:52.460\n And the things I think are really interesting here\n\n1:11:52.460 --> 1:11:54.820\n is that these agents, in some ways,\n\n1:11:54.820 --> 1:11:56.820\n they're almost like an insect like intelligence, right?\n\n1:11:56.820 --> 1:11:58.720\n Where they have a lot in common\n\n1:11:58.720 --> 1:12:00.180\n with how an insect is trained, right?\n\n1:12:00.180 --> 1:12:01.840\n An insect kind of lives in this environment\n\n1:12:01.840 --> 1:12:04.980\n for a very long time or the ancestors of this insect\n\n1:12:04.980 --> 1:12:05.900\n have been around for a long time\n\n1:12:05.900 --> 1:12:09.740\n and had a lot of experience that gets baked into this agent.\n\n1:12:09.740 --> 1:12:12.780\n And it's not really smart in the sense of a human, right?\n\n1:12:12.780 --> 1:12:14.620\n It's not able to go and learn calculus,\n\n1:12:14.620 --> 1:12:16.980\n but it's able to navigate its environment extremely well.\n\n1:12:16.980 --> 1:12:18.460\n And it's able to handle unexpected things\n\n1:12:18.460 --> 1:12:22.060\n in the environment that it's never seen before pretty well.\n\n1:12:22.060 --> 1:12:24.780\n And we see the same sort of thing with our Dota bots, right?\n\n1:12:24.780 --> 1:12:26.740\n That they're able to, within this game,\n\n1:12:26.740 --> 1:12:28.460\n they're able to play against humans,\n\n1:12:28.460 --> 1:12:29.980\n which is something that never existed\n\n1:12:29.980 --> 1:12:31.380\n in its evolutionary environment,\n\n1:12:31.380 --> 1:12:34.340\n totally different play styles from humans versus the bots.\n\n1:12:34.340 --> 1:12:37.220\n And yet it's able to handle it extremely well.\n\n1:12:37.220 --> 1:12:40.420\n And that's something that I think was very surprising to us,\n\n1:12:40.420 --> 1:12:43.460\n was something that doesn't really emerge\n\n1:12:43.460 --> 1:12:47.260\n from what we've seen with PPO at smaller scale, right?\n\n1:12:47.260 --> 1:12:49.780\n And the kind of scale we're running this stuff at was,\n\n1:12:49.780 --> 1:12:51.980\n I could say like 100,000 CPU cores\n\n1:12:51.980 --> 1:12:54.140\n running with like hundreds of GPUs.\n\n1:12:54.140 --> 1:12:57.580\n It was probably about something like hundreds\n\n1:12:57.580 --> 1:13:01.300\n of years of experience going into this bot\n\n1:13:01.300 --> 1:13:03.860\n every single real day.\n\n1:13:03.860 --> 1:13:06.280\n And so that scale is massive\n\n1:13:06.280 --> 1:13:08.500\n and we start to see very different kinds of behaviors\n\n1:13:08.500 --> 1:13:10.820\n out of the algorithms that we all know and love.\n\n1:13:10.820 --> 1:13:15.260\n Dota, you mentioned, beat the world expert one v one.\n\n1:13:15.260 --> 1:13:20.260\n And then you weren't able to win five v five this year.\n\n1:13:20.820 --> 1:13:21.660\n Yeah.\n\n1:13:21.660 --> 1:13:24.180\n At the best players in the world.\n\n1:13:24.180 --> 1:13:26.700\n So what's the comeback story?\n\n1:13:26.700 --> 1:13:27.740\n First of all, talk through that.\n\n1:13:27.740 --> 1:13:29.540\n That was an exceptionally exciting event.\n\n1:13:29.540 --> 1:13:33.260\n And what's the following months and this year look like?\n\n1:13:33.260 --> 1:13:35.340\n Yeah, yeah, so one thing that's interesting\n\n1:13:35.340 --> 1:13:37.700\n is that we lose all the time.\n\n1:13:38.700 --> 1:13:39.540\n Because we play.\n\n1:13:39.540 --> 1:13:40.380\n Who's we here?\n\n1:13:40.380 --> 1:13:41.820\n The Dota team at OpenAI.\n\n1:13:41.820 --> 1:13:44.260\n We play the bot against better players\n\n1:13:44.260 --> 1:13:45.920\n than our system all the time.\n\n1:13:45.920 --> 1:13:47.500\n Or at least we used to, right?\n\n1:13:47.500 --> 1:13:50.200\n Like the first time we lost publicly\n\n1:13:50.200 --> 1:13:52.340\n was we went up on stage at the international\n\n1:13:52.340 --> 1:13:54.740\n and we played against some of the best teams in the world\n\n1:13:54.740 --> 1:13:56.440\n and we ended up losing both games,\n\n1:13:56.440 --> 1:13:58.660\n but we gave them a run for their money, right?\n\n1:13:58.660 --> 1:14:01.540\n That both games were kind of 30 minutes, 25 minutes\n\n1:14:01.540 --> 1:14:03.260\n and they went back and forth, back and forth,\n\n1:14:03.260 --> 1:14:04.180\n back and forth.\n\n1:14:04.180 --> 1:14:06.020\n And so I think that really shows\n\n1:14:06.020 --> 1:14:08.140\n that we're at the professional level\n\n1:14:08.140 --> 1:14:09.780\n and that kind of looking at those games,\n\n1:14:09.780 --> 1:14:12.420\n we think that the coin could have gone a different direction\n\n1:14:12.420 --> 1:14:14.140\n and we could have had some wins.\n\n1:14:14.140 --> 1:14:16.140\n That was actually very encouraging for us.\n\n1:14:16.140 --> 1:14:18.380\n And it's interesting because the international\n\n1:14:18.380 --> 1:14:19.860\n was at a fixed time, right?\n\n1:14:19.860 --> 1:14:22.900\n So we knew exactly what day we were going to be playing\n\n1:14:22.900 --> 1:14:25.660\n and we pushed as far as we could, as fast as we could.\n\n1:14:25.660 --> 1:14:28.160\n Two weeks later, we had a bot that had an 80% win rate\n\n1:14:28.160 --> 1:14:30.260\n versus the one that played at TI.\n\n1:14:30.260 --> 1:14:32.460\n So the march of progress, you should think of it\n\n1:14:32.460 --> 1:14:34.920\n as a snapshot rather than as an end state.\n\n1:14:34.920 --> 1:14:39.180\n And so in fact, we'll be announcing our finals pretty soon.\n\n1:14:39.180 --> 1:14:41.980\n I actually think that we'll announce our final match\n\n1:14:42.900 --> 1:14:45.340\n prior to this podcast being released.\n\n1:14:45.340 --> 1:14:49.900\n So we'll be playing against the world champions.\n\n1:14:49.900 --> 1:14:52.700\n And for us, it's really less about,\n\n1:14:52.700 --> 1:14:55.460\n like the way that we think about what's upcoming\n\n1:14:55.460 --> 1:14:59.180\n is the final milestone, the final competitive milestone\n\n1:14:59.180 --> 1:15:00.460\n for the project, right?\n\n1:15:00.460 --> 1:15:02.220\n That our goal in all of this\n\n1:15:02.220 --> 1:15:05.340\n isn't really about beating humans at Dota.\n\n1:15:05.340 --> 1:15:06.980\n Our goal is to push the state of the art\n\n1:15:06.980 --> 1:15:08.020\n in reinforcement learning.\n\n1:15:08.020 --> 1:15:09.100\n And we've done that, right?\n\n1:15:09.100 --> 1:15:10.820\n And we've actually learned a lot from our system\n\n1:15:10.820 --> 1:15:13.940\n and that we have, I think, a lot of exciting next steps\n\n1:15:13.940 --> 1:15:14.860\n that we want to take.\n\n1:15:14.860 --> 1:15:17.480\n And so kind of as a final showcase of what we built,\n\n1:15:17.480 --> 1:15:18.900\n we're going to do this match.\n\n1:15:18.900 --> 1:15:21.380\n But for us, it's not really the success or failure\n\n1:15:21.380 --> 1:15:24.480\n to see do we have the coin flip go in our direction\n\n1:15:24.480 --> 1:15:25.940\n or against.\n\n1:15:25.940 --> 1:15:28.860\n Where do you see the field of deep learning\n\n1:15:28.860 --> 1:15:31.620\n heading in the next few years?\n\n1:15:31.620 --> 1:15:35.620\n Where do you see the work and reinforcement learning\n\n1:15:35.620 --> 1:15:40.620\n perhaps heading, and more specifically with OpenAI,\n\n1:15:41.220 --> 1:15:44.460\n all the exciting projects that you're working on,\n\n1:15:44.460 --> 1:15:46.460\n what does 2019 hold for you?\n\n1:15:46.460 --> 1:15:47.420\n Massive scale.\n\n1:15:47.420 --> 1:15:48.260\n Scale.\n\n1:15:48.260 --> 1:15:49.900\n I will put an asterisk on that and just say,\n\n1:15:49.900 --> 1:15:52.340\n I think that it's about ideas plus scale.\n\n1:15:52.340 --> 1:15:53.180\n You need both.\n\n1:15:53.180 --> 1:15:55.060\n So that's a really good point.\n\n1:15:55.060 --> 1:15:58.620\n So the question, in terms of ideas,\n\n1:15:58.620 --> 1:16:00.620\n you have a lot of projects\n\n1:16:00.620 --> 1:16:04.380\n that are exploring different areas of intelligence.\n\n1:16:04.380 --> 1:16:07.660\n And the question is, when you think of scale,\n\n1:16:07.660 --> 1:16:09.820\n do you think about growing the scale\n\n1:16:09.820 --> 1:16:10.940\n of those individual projects\n\n1:16:10.940 --> 1:16:13.260\n or do you think about adding new projects?\n\n1:16:13.260 --> 1:16:16.060\n And sorry to, and if you're thinking about\n\n1:16:16.060 --> 1:16:19.020\n adding new projects, or if you look at the past,\n\n1:16:19.020 --> 1:16:21.380\n what's the process of coming up with new projects\n\n1:16:21.380 --> 1:16:22.220\n and new ideas?\n\n1:16:22.220 --> 1:16:23.060\n Yep.\n\n1:16:23.060 --> 1:16:25.380\n So we really have a life cycle of project here.\n\n1:16:25.380 --> 1:16:27.040\n So we start with a few people\n\n1:16:27.040 --> 1:16:28.560\n just working on a small scale idea.\n\n1:16:28.560 --> 1:16:30.700\n And language is actually a very good example of this.\n\n1:16:30.700 --> 1:16:32.620\n That it was really one person here\n\n1:16:32.620 --> 1:16:35.020\n who was pushing on language for a long time.\n\n1:16:35.020 --> 1:16:36.820\n I mean, then you get signs of life, right?\n\n1:16:36.820 --> 1:16:38.860\n And so this is like, let's say,\n\n1:16:38.860 --> 1:16:42.740\n with the original GPT, we had something that was interesting\n\n1:16:42.740 --> 1:16:44.940\n and we said, okay, it's time to scale this, right?\n\n1:16:44.940 --> 1:16:46.100\n It's time to put more people on it,\n\n1:16:46.100 --> 1:16:48.160\n put more computational resources behind it.\n\n1:16:48.160 --> 1:16:51.660\n And then we just kind of keep pushing and keep pushing.\n\n1:16:51.660 --> 1:16:52.700\n And the end state is something\n\n1:16:52.700 --> 1:16:54.420\n that looks like Dota or robotics,\n\n1:16:54.420 --> 1:16:57.220\n where you have a large team of 10 or 15 people\n\n1:16:57.220 --> 1:16:59.300\n that are running things at very large scale\n\n1:16:59.300 --> 1:17:02.300\n and that you're able to really have material engineering\n\n1:17:02.300 --> 1:17:06.640\n and sort of machine learning science coming together\n\n1:17:06.640 --> 1:17:10.380\n to make systems that work and get material results\n\n1:17:10.380 --> 1:17:12.380\n that just would have been impossible otherwise.\n\n1:17:12.380 --> 1:17:13.740\n So we do that whole life cycle.\n\n1:17:13.740 --> 1:17:16.780\n We've done it a number of times, typically end to end.\n\n1:17:16.780 --> 1:17:20.540\n It's probably two years or so to do it.\n\n1:17:20.540 --> 1:17:21.900\n The organization has been around for three years,\n\n1:17:21.900 --> 1:17:23.140\n so maybe we'll find that we also have\n\n1:17:23.140 --> 1:17:27.800\n longer life cycle projects, but we'll work up to those.\n\n1:17:29.740 --> 1:17:31.580\n So one team that we were actually just starting,\n\n1:17:31.580 --> 1:17:33.400\n Ilya and I are kicking off a new team\n\n1:17:33.400 --> 1:17:34.620\n called the Reasoning Team,\n\n1:17:34.620 --> 1:17:36.420\n and that this is to really try to tackle\n\n1:17:36.420 --> 1:17:38.700\n how do you get neural networks to reason?\n\n1:17:38.700 --> 1:17:42.700\n And we think that this will be a long term project.\n\n1:17:42.700 --> 1:17:44.720\n It's one that we're very excited about.\n\n1:17:44.720 --> 1:17:47.540\n In terms of reasoning, super exciting topic,\n\n1:17:48.400 --> 1:17:53.400\n what kind of benchmarks, what kind of tests of reasoning\n\n1:17:54.180 --> 1:17:55.280\n do you envision?\n\n1:17:55.280 --> 1:17:58.980\n What would, if you sat back with whatever drink\n\n1:17:58.980 --> 1:18:01.220\n and you would be impressed that this system\n\n1:18:01.220 --> 1:18:03.900\n is able to do something, what would that look like?\n\n1:18:03.900 --> 1:18:04.860\n Theorem proving.\n\n1:18:04.860 --> 1:18:06.460\n Theorem proving.\n\n1:18:06.460 --> 1:18:10.540\n So some kind of logic, and especially mathematical logic.\n\n1:18:10.540 --> 1:18:11.380\n I think so.\n\n1:18:11.380 --> 1:18:14.180\n I think that there's other problems that are dual\n\n1:18:14.180 --> 1:18:15.980\n to theorem proving in particular.\n\n1:18:15.980 --> 1:18:18.500\n You think about programming, you think about\n\n1:18:18.500 --> 1:18:21.260\n even security analysis of code,\n\n1:18:21.260 --> 1:18:23.720\n that these all kind of capture the same sorts\n\n1:18:23.720 --> 1:18:26.200\n of core reasoning and being able to do\n\n1:18:26.200 --> 1:18:28.360\n some out of distribution generalization.\n\n1:18:28.360 --> 1:18:32.600\n So it would be quite exciting if OpenAI Reasoning Team\n\n1:18:32.600 --> 1:18:34.720\n was able to prove that P equals NP.\n\n1:18:34.720 --> 1:18:36.040\n That would be very nice.\n\n1:18:36.040 --> 1:18:38.560\n It would be very, very, very exciting, especially.\n\n1:18:38.560 --> 1:18:39.760\n If it turns out that P equals NP,\n\n1:18:39.760 --> 1:18:41.060\n that'll be interesting too.\n\n1:18:41.060 --> 1:18:46.060\n It would be ironic and humorous.\n\n1:18:47.560 --> 1:18:49.880\n So what problem stands out to you\n\n1:18:49.880 --> 1:18:53.960\n as the most exciting and challenging and impactful\n\n1:18:53.960 --> 1:18:56.380\n to the work for us as a community in general\n\n1:18:56.380 --> 1:18:58.520\n and for OpenAI this year?\n\n1:18:58.520 --> 1:18:59.600\n You mentioned reasoning.\n\n1:18:59.600 --> 1:19:01.440\n I think that's a heck of a problem.\n\n1:19:01.440 --> 1:19:02.880\n Yeah, so I think reasoning's an important one.\n\n1:19:02.880 --> 1:19:05.840\n I think it's gonna be hard to get good results in 2019.\n\n1:19:05.840 --> 1:19:08.760\n Again, just like we think about the life cycle, takes time.\n\n1:19:08.760 --> 1:19:11.040\n I think for 2019, language modeling seems to be\n\n1:19:11.040 --> 1:19:12.640\n kind of on that ramp.\n\n1:19:12.640 --> 1:19:14.960\n It's at the point that we have a technique that works.\n\n1:19:14.960 --> 1:19:18.080\n We wanna scale 100x, 1,000x, see what happens.\n\n1:19:18.080 --> 1:19:19.040\n Awesome.\n\n1:19:19.040 --> 1:19:21.600\n Do you think we're living in a simulation?\n\n1:19:21.600 --> 1:19:24.840\n I think it's hard to have a real opinion about it.\n\n1:19:24.840 --> 1:19:26.320\n It's actually interesting.\n\n1:19:26.320 --> 1:19:29.520\n I separate out things that I think can have like,\n\n1:19:29.520 --> 1:19:32.680\n yield materially different predictions about the world\n\n1:19:32.680 --> 1:19:35.880\n from ones that are just kind of fun to speculate about.\n\n1:19:35.880 --> 1:19:37.960\n I kind of view simulation as more like,\n\n1:19:37.960 --> 1:19:40.320\n is there a flying teapot between Mars and Jupiter?\n\n1:19:40.320 --> 1:19:44.000\n Like, maybe, but it's a little bit hard to know\n\n1:19:44.000 --> 1:19:45.120\n what that would mean for my life.\n\n1:19:45.120 --> 1:19:47.000\n So there is something actionable.\n\n1:19:47.000 --> 1:19:50.760\n So some of the best work OpenAI has done\n\n1:19:50.760 --> 1:19:52.780\n is in the field of reinforcement learning.\n\n1:19:52.780 --> 1:19:56.620\n And some of the success of reinforcement learning\n\n1:19:56.620 --> 1:19:58.520\n come from being able to simulate\n\n1:19:58.520 --> 1:20:00.120\n the problem you're trying to solve.\n\n1:20:00.120 --> 1:20:03.680\n So do you have a hope for reinforcement,\n\n1:20:03.680 --> 1:20:05.320\n for the future of reinforcement learning\n\n1:20:05.320 --> 1:20:07.080\n and for the future of simulation?\n\n1:20:07.080 --> 1:20:09.120\n Like whether it's, we're talking about autonomous vehicles\n\n1:20:09.120 --> 1:20:10.920\n or any kind of system.\n\n1:20:10.920 --> 1:20:13.560\n Do you see that scaling to where we'll be able\n\n1:20:13.560 --> 1:20:16.440\n to simulate systems and hence,\n\n1:20:16.440 --> 1:20:19.400\n be able to create a simulator that echoes our real world\n\n1:20:19.400 --> 1:20:21.620\n and proving once and for all,\n\n1:20:21.620 --> 1:20:22.680\n even though you're denying it,\n\n1:20:22.680 --> 1:20:25.080\n that we're living in a simulation?\n\n1:20:25.080 --> 1:20:26.500\n I feel like it's two separate questions, right?\n\n1:20:26.500 --> 1:20:28.400\n So kind of at the core there of like,\n\n1:20:28.400 --> 1:20:31.240\n can we use simulation for self driving cars?\n\n1:20:31.240 --> 1:20:33.860\n Take a look at our robotic system, Dactyl, right?\n\n1:20:33.860 --> 1:20:37.000\n That was trained in simulation using the Dota system,\n\n1:20:37.000 --> 1:20:40.480\n in fact, and it transfers to a physical robot.\n\n1:20:40.480 --> 1:20:42.320\n And I think everyone looks at our Dota system,\n\n1:20:42.320 --> 1:20:43.560\n they're like, okay, it's just a game.\n\n1:20:43.560 --> 1:20:45.260\n How are you ever gonna escape to the real world?\n\n1:20:45.260 --> 1:20:47.480\n And the answer is, well, we did it with a physical robot\n\n1:20:47.480 --> 1:20:48.720\n that no one could program.\n\n1:20:48.720 --> 1:20:50.240\n And so I think the answer is simulation\n\n1:20:50.240 --> 1:20:52.080\n goes a lot further than you think\n\n1:20:52.080 --> 1:20:54.240\n if you apply the right techniques to it.\n\n1:20:54.240 --> 1:20:55.480\n Now, there's a question of,\n\n1:20:55.480 --> 1:20:57.520\n are the beings in that simulation gonna wake up\n\n1:20:57.520 --> 1:20:59.620\n and have consciousness?\n\n1:20:59.620 --> 1:21:02.380\n I think that one seems a lot harder to, again,\n\n1:21:02.380 --> 1:21:03.220\n reason about.\n\n1:21:03.220 --> 1:21:05.400\n I think that you really should think about\n\n1:21:05.400 --> 1:21:07.940\n where exactly does human consciousness come from\n\n1:21:07.940 --> 1:21:09.160\n in our own self awareness?\n\n1:21:09.160 --> 1:21:11.920\n And is it just that once you have a complicated enough\n\n1:21:11.920 --> 1:21:13.220\n neural net, you have to worry about\n\n1:21:13.220 --> 1:21:14.800\n the agents feeling pain?\n\n1:21:15.840 --> 1:21:19.440\n And I think there's interesting speculation to do there,\n\n1:21:19.440 --> 1:21:23.120\n but again, I think it's a little bit hard to know for sure.\n\n1:21:23.120 --> 1:21:25.040\n Well, let me just keep with the speculation.\n\n1:21:25.040 --> 1:21:28.640\n Do you think to create intelligence, general intelligence,\n\n1:21:28.640 --> 1:21:33.180\n you need, one, consciousness, and two, a body?\n\n1:21:33.180 --> 1:21:35.040\n Do you think any of those elements are needed,\n\n1:21:35.040 --> 1:21:38.480\n or is intelligence something that's orthogonal to those?\n\n1:21:38.480 --> 1:21:41.920\n I'll stick to the non grand answer first, right?\n\n1:21:41.920 --> 1:21:44.360\n So the non grand answer is just to look at,\n\n1:21:44.360 --> 1:21:45.800\n what are we already making work?\n\n1:21:45.800 --> 1:21:47.800\n You look at GPT2, a lot of people would have said\n\n1:21:47.800 --> 1:21:49.480\n that to even get these kinds of results,\n\n1:21:49.480 --> 1:21:51.080\n you need real world experience.\n\n1:21:51.080 --> 1:21:52.560\n You need a body, you need grounding.\n\n1:21:52.560 --> 1:21:55.060\n How are you supposed to reason about any of these things?\n\n1:21:55.060 --> 1:21:56.500\n How are you supposed to like even kind of know\n\n1:21:56.500 --> 1:21:58.040\n about smoke and fire and those things\n\n1:21:58.040 --> 1:21:59.740\n if you've never experienced them?\n\n1:21:59.740 --> 1:22:03.000\n And GPT2 shows that you can actually go way further\n\n1:22:03.000 --> 1:22:05.940\n than that kind of reasoning would predict.\n\n1:22:06.880 --> 1:22:10.600\n So I think that in terms of, do we need consciousness?\n\n1:22:10.600 --> 1:22:11.840\n Do we need a body?\n\n1:22:11.840 --> 1:22:13.400\n It seems the answer is probably not, right?\n\n1:22:13.400 --> 1:22:15.100\n That we could probably just continue to push\n\n1:22:15.100 --> 1:22:16.140\n kind of the systems we have.\n\n1:22:16.140 --> 1:22:18.280\n They already feel general.\n\n1:22:18.280 --> 1:22:20.560\n They're not as competent or as general\n\n1:22:20.560 --> 1:22:23.000\n or able to learn as quickly as an AGI would,\n\n1:22:23.000 --> 1:22:27.420\n but they're at least like kind of proto AGI in some way,\n\n1:22:27.420 --> 1:22:29.800\n and they don't need any of those things.\n\n1:22:29.800 --> 1:22:31.960\n Now let's move to the grand answer,\n\n1:22:31.960 --> 1:22:36.520\n which is, are our neural nets conscious already?\n\n1:22:36.520 --> 1:22:37.440\n Would we ever know?\n\n1:22:37.440 --> 1:22:38.920\n How can we tell, right?\n\n1:22:38.920 --> 1:22:41.640\n And here's where the speculation starts to become\n\n1:22:43.040 --> 1:22:44.920\n at least interesting or fun\n\n1:22:44.920 --> 1:22:46.520\n and maybe a little bit disturbing\n\n1:22:46.520 --> 1:22:48.080\n depending on where you take it.\n\n1:22:48.080 --> 1:22:51.280\n But it certainly seems that when we think about animals,\n\n1:22:51.280 --> 1:22:53.280\n that there's some continuum of consciousness.\n\n1:22:53.280 --> 1:22:57.120\n You know, my cat I think is conscious in some way, right?\n\n1:22:57.120 --> 1:22:58.200\n Not as conscious as a human.\n\n1:22:58.200 --> 1:23:00.080\n And you could imagine that you could build\n\n1:23:00.080 --> 1:23:01.220\n a little consciousness meter, right?\n\n1:23:01.220 --> 1:23:03.080\n You point at a cat, it gives you a little reading.\n\n1:23:03.080 --> 1:23:05.480\n Point at a human, it gives you much bigger reading.\n\n1:23:06.400 --> 1:23:08.120\n What would happen if you pointed one of those\n\n1:23:08.120 --> 1:23:09.960\n at a donor neural net?\n\n1:23:09.960 --> 1:23:12.180\n And if you're training in this massive simulation,\n\n1:23:12.180 --> 1:23:13.680\n do the neural nets feel pain?\n\n1:23:13.680 --> 1:23:16.960\n You know, it becomes pretty hard to know\n\n1:23:16.960 --> 1:23:18.840\n that the answer is no.\n\n1:23:18.840 --> 1:23:21.660\n And it becomes pretty hard to really think about\n\n1:23:21.660 --> 1:23:24.340\n what that would mean if the answer were yes.\n\n1:23:25.440 --> 1:23:27.600\n And it's very possible, you know, for example,\n\n1:23:27.600 --> 1:23:29.600\n you could imagine that maybe the reason\n\n1:23:29.600 --> 1:23:31.560\n that humans have consciousness\n\n1:23:31.560 --> 1:23:35.160\n is because it's a convenient computational shortcut, right?\n\n1:23:35.160 --> 1:23:37.120\n If you think about it, if you have a being\n\n1:23:37.120 --> 1:23:38.360\n that wants to avoid pain,\n\n1:23:38.360 --> 1:23:40.960\n which seems pretty important to survive in this environment\n\n1:23:40.960 --> 1:23:43.800\n and wants to like, you know, eat food,\n\n1:23:43.800 --> 1:23:45.640\n then that maybe the best way of doing it\n\n1:23:45.640 --> 1:23:47.240\n is to have a being that's conscious, right?\n\n1:23:47.240 --> 1:23:49.640\n That, you know, in order to succeed in the environment,\n\n1:23:49.640 --> 1:23:51.200\n you need to have those properties\n\n1:23:51.200 --> 1:23:52.760\n and how are you supposed to implement them\n\n1:23:52.760 --> 1:23:55.440\n and maybe this consciousness's way of doing that.\n\n1:23:55.440 --> 1:23:57.920\n If that's true, then actually maybe we should expect\n\n1:23:57.920 --> 1:24:00.060\n that really competent reinforcement learning agents\n\n1:24:00.060 --> 1:24:02.120\n will also have consciousness.\n\n1:24:02.120 --> 1:24:03.360\n But you know, that's a big if.\n\n1:24:03.360 --> 1:24:04.880\n And I think there are a lot of other arguments\n\n1:24:04.880 --> 1:24:06.760\n they can make in other directions.\n\n1:24:06.760 --> 1:24:08.520\n I think that's a really interesting idea\n\n1:24:08.520 --> 1:24:11.520\n that even GPT2 has some degree of consciousness.\n\n1:24:11.520 --> 1:24:14.320\n That's something, it's actually not as crazy\n\n1:24:14.320 --> 1:24:16.640\n to think about, it's useful to think about\n\n1:24:16.640 --> 1:24:18.320\n as we think about what it means\n\n1:24:18.320 --> 1:24:21.160\n to create intelligence of a dog, intelligence of a cat,\n\n1:24:22.240 --> 1:24:24.480\n and the intelligence of a human.\n\n1:24:24.480 --> 1:24:26.360\n So last question, do you think\n\n1:24:27.880 --> 1:24:32.040\n we will ever fall in love, like in the movie Her,\n\n1:24:32.040 --> 1:24:34.480\n with an artificial intelligence system\n\n1:24:34.480 --> 1:24:36.300\n or an artificial intelligence system\n\n1:24:36.300 --> 1:24:38.640\n falling in love with a human?\n\n1:24:38.640 --> 1:24:40.280\n I hope so.\n\n1:24:40.280 --> 1:24:43.760\n If there's any better way to end it is on love.\n\n1:24:43.760 --> 1:24:45.680\n So Greg, thanks so much for talking today.\n\n1:24:45.680 --> 1:25:06.680\n Thank you for having me.\n\n"
}