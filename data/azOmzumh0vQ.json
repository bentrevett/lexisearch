{
  "title": "Yoshua Bengio: Deep Learning | Lex Fridman Podcast #4",
  "id": "azOmzumh0vQ",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:04.320\n What difference between biological neural networks and artificial neural networks\n\n00:04.320 --> 00:07.600\n is most mysterious, captivating, and profound for you?\n\n00:11.120 --> 00:15.280\n First of all, there's so much we don't know about biological neural networks,\n\n00:15.280 --> 00:21.840\n and that's very mysterious and captivating because maybe it holds the key to improving\n\n00:21.840 --> 00:29.680\n artificial neural networks. One of the things I studied recently is something\n\n00:29.680 --> 00:36.160\n that we don't know how biological neural networks do but would be really useful for artificial ones\n\n00:37.120 --> 00:45.040\n is the ability to do credit assignment through very long time spans. There are things that\n\n00:46.560 --> 00:50.400\n we can in principle do with artificial neural nets, but it's not very convenient and it's\n\n00:50.400 --> 00:55.920\n not biologically plausible. And this mismatch, I think this kind of mismatch\n\n00:55.920 --> 01:02.560\n may be an interesting thing to study to, A, understand better how brains might do these\n\n01:02.560 --> 01:08.000\n things because we don't have good corresponding theories with artificial neural nets, and B,\n\n01:09.200 --> 01:18.320\n maybe provide new ideas that we could explore about things that brain do differently and that\n\n01:18.320 --> 01:23.680\n we could incorporate in artificial neural nets. So let's break credit assignment up a little bit.\n\n01:23.680 --> 01:30.320\n Yes. So what, it's a beautifully technical term, but it could incorporate so many things. So is it\n\n01:30.320 --> 01:37.760\n more on the RNN memory side, that thinking like that, or is it something about knowledge, building\n\n01:37.760 --> 01:44.800\n up common sense knowledge over time? Or is it more in the reinforcement learning sense that you're\n\n01:44.800 --> 01:50.080\n picking up rewards over time for a particular, to achieve a certain kind of goal? So I was thinking\n\n01:50.080 --> 01:59.440\n more about the first two meanings whereby we store all kinds of memories, episodic memories\n\n01:59.440 --> 02:10.560\n in our brain, which we can access later in order to help us both infer causes of things that we\n\n02:10.560 --> 02:20.640\n are observing now and assign credit to decisions or interpretations we came up with a while ago\n\n02:20.640 --> 02:29.280\n when those memories were stored. And then we can change the way we would have reacted or interpreted\n\n02:29.280 --> 02:33.760\n things in the past, and now that's credit assignment used for learning.\n\n02:33.760 --> 02:43.600\n So in which way do you think artificial neural networks, the current LSTM, the current architectures\n\n02:43.600 --> 02:50.320\n are not able to capture the, presumably you're thinking of very long term?\n\n02:50.320 --> 02:58.560\n Yes. So current, the current nets are doing a fairly good jobs for sequences with dozens or\n\n02:58.560 --> 03:04.960\n say hundreds of time steps. And then it gets harder and harder and depending on what you have\n\n03:04.960 --> 03:11.920\n to remember and so on, as you consider longer durations. Whereas humans seem to be able to\n\n03:12.480 --> 03:16.960\n do credit assignment through essentially arbitrary times, like I could remember something I did last\n\n03:16.960 --> 03:23.840\n year. And then now because I see some new evidence, I'm going to change my mind about the way I was\n\n03:23.840 --> 03:28.720\n thinking last year. And hopefully not do the same mistake again.\n\n03:30.720 --> 03:36.080\n I think a big part of that is probably forgetting. You're only remembering the really important\n\n03:36.080 --> 03:38.480\n things. It's very efficient forgetting.\n\n03:40.000 --> 03:46.160\n Yes. So there's a selection of what we remember. And I think there are really cool connection to\n\n03:46.160 --> 03:52.080\n higher level cognition here regarding consciousness, deciding and emotions,\n\n03:52.080 --> 03:59.200\n so deciding what comes to consciousness and what gets stored in memory, which are not trivial either.\n\n04:00.720 --> 04:07.120\n So you've been at the forefront there all along, showing some of the amazing things that neural\n\n04:07.120 --> 04:12.640\n networks, deep neural networks can do in the field of artificial intelligence is just broadly\n\n04:12.640 --> 04:19.120\n in all kinds of applications. But we can talk about that forever. But what, in your view,\n\n04:19.120 --> 04:23.920\n because we're thinking towards the future, is the weakest aspect of the way deep neural networks\n\n04:23.920 --> 04:28.160\n represent the world? What is that? What is in your view is missing?\n\n04:29.200 --> 04:38.240\n So current state of the art neural nets trained on large quantities of images or texts\n\n04:38.240 --> 04:44.240\n have some level of understanding of, you know, what explains those data sets, but it's very\n\n04:45.360 --> 04:53.360\n basic, it's it's very low level. And it's not nearly as robust and abstract and general\n\n04:54.160 --> 05:02.400\n as our understanding. Okay, so that doesn't tell us how to fix things. But I think it encourages\n\n05:02.400 --> 05:13.200\n us to think about how we can maybe train our neural nets differently, so that they would\n\n05:14.240 --> 05:20.400\n focus, for example, on causal explanation, something that we don't do currently with neural\n\n05:20.400 --> 05:27.440\n net training. Also, one thing I'll talk about in my talk this afternoon is the fact that\n\n05:27.440 --> 05:33.680\n instead of learning separately from images and videos on one hand and from texts on the other\n\n05:33.680 --> 05:42.000\n hand, we need to do a better job of jointly learning about language and about the world\n\n05:42.000 --> 05:50.160\n to which it refers. So that, you know, both sides can help each other. We need to have good world\n\n05:50.160 --> 05:57.360\n models in our neural nets for them to really understand sentences, which talk about what's\n\n05:57.360 --> 06:06.400\n going on in the world. And I think we need language input to help provide clues about\n\n06:06.400 --> 06:13.600\n what high level concepts like semantic concepts should be represented at the top levels of our\n\n06:13.600 --> 06:21.920\n neural nets. In fact, there is evidence that the purely unsupervised learning of representations\n\n06:21.920 --> 06:28.960\n doesn't give rise to high level representations that are as powerful as the ones we're getting\n\n06:28.960 --> 06:35.040\n from supervised learning. And so the clues we're getting just with the labels, not even sentences,\n\n06:35.680 --> 06:42.400\n is already very, very high level. And I think that's a very important thing to keep in mind.\n\n06:42.400 --> 06:49.520\n It's already very powerful. Do you think that's an architecture challenge or is it a data set challenge?\n\n06:49.520 --> 06:59.360\n Neither. I'm tempted to just end it there. Can you elaborate slightly?\n\n07:02.880 --> 07:06.800\n Of course, data sets and architectures are something you want to always play with. But\n\n07:06.800 --> 07:13.040\n I think the crucial thing is more the training objectives, the training frameworks. For example,\n\n07:13.040 --> 07:20.240\n going from passive observation of data to more active agents, which\n\n07:22.320 --> 07:27.280\n learn by intervening in the world, the relationships between causes and effects,\n\n07:27.280 --> 07:36.640\n the sort of objective functions, which could be important to allow the highest level explanations\n\n07:36.640 --> 07:43.840\n to rise from the learning, which I don't think we have now, the kinds of objective functions,\n\n07:43.840 --> 07:50.400\n which could be used to reward exploration, the right kind of exploration. So these kinds of\n\n07:50.400 --> 07:57.200\n questions are neither in the data set nor in the architecture, but more in how we learn,\n\n07:57.200 --> 08:04.240\n under what objectives and so on. Yeah, I've heard you mention in several contexts, the idea of sort\n\n08:04.240 --> 08:08.880\n of the way children learn, they interact with objects in the world. And it seems fascinating\n\n08:08.880 --> 08:15.520\n because in some sense, except with some cases in reinforcement learning, that idea\n\n08:15.520 --> 08:20.720\n is not part of the learning process in artificial neural networks. So it's almost like,\n\n08:21.360 --> 08:29.120\n do you envision something like an objective function saying, you know what, if you\n\n08:29.680 --> 08:36.400\n poke this object in this kind of way, it would be really helpful for me to further learn.\n\n08:36.400 --> 08:37.040\n Right, right.\n\n08:37.040 --> 08:40.320\n Sort of almost guiding some aspect of the learning.\n\n08:40.320 --> 08:43.600\n Right, right, right. So I was talking to Rebecca Sacks just a few minutes ago,\n\n08:43.600 --> 08:52.080\n and she was talking about lots and lots of evidence from infants seem to clearly pick\n\n08:52.960 --> 09:03.040\n what interests them in a directed way. And so they're not passive learners, they focus their\n\n09:03.040 --> 09:10.480\n attention on aspects of the world, which are most interesting, surprising in a non trivial way.\n\n09:10.480 --> 09:14.000\n That makes them change their theories of the world.\n\n09:16.000 --> 09:25.280\n So that's a fascinating view of the future progress. But on a more maybe boring question,\n\n09:26.080 --> 09:33.760\n do you think going deeper and larger, so do you think just increasing the size of the things that\n\n09:33.760 --> 09:38.800\n have been increasing a lot in the past few years, is going to be a big thing?\n\n09:38.800 --> 09:43.760\n I think increasing the size of the things that have been increasing a lot in the past few years\n\n09:44.320 --> 09:51.840\n will also make significant progress. So some of the representational issues that you mentioned,\n\n09:51.840 --> 09:54.880\n they're kind of shallow, in some sense.\n\n09:54.880 --> 09:58.400\n Oh, shallow in the sense of abstraction.\n\n09:58.400 --> 10:00.800\n In the sense of abstraction, they're not getting some...\n\n10:00.800 --> 10:06.880\n I don't think that having more depth in the network in the sense of instead of 100 layers,\n\n10:06.880 --> 10:11.680\n you're going to have more layers. I don't think so. Is that obvious to you?\n\n10:11.680 --> 10:19.200\n Yes. What is clear to me is that engineers and companies and labs and grad students will continue\n\n10:19.200 --> 10:25.600\n to tune architectures and explore all kinds of tweaks to make the current state of the art\n\n10:25.600 --> 10:31.440\n slightly ever slightly better. But I don't think that's going to be nearly enough. I think we need\n\n10:31.440 --> 10:39.920\n changes in the way that we're considering learning to achieve the goal that these learners actually\n\n10:39.920 --> 10:45.840\n understand in a deep way the environment in which they are, you know, observing and acting.\n\n10:46.640 --> 10:52.080\n But I guess I was trying to ask a question that's more interesting than just more layers.\n\n10:53.200 --> 11:00.800\n It's basically, once you figure out a way to learn through interacting, how many parameters\n\n11:00.800 --> 11:07.760\n it takes to store that information. So I think our brain is quite bigger than most neural networks.\n\n11:07.760 --> 11:13.120\n Right, right. Oh, I see what you mean. Oh, I'm with you there. So I agree that in order to\n\n11:14.240 --> 11:19.760\n build neural nets with the kind of broad knowledge of the world that typical adult humans have,\n\n11:20.960 --> 11:24.880\n probably the kind of computing power we have now is going to be insufficient.\n\n11:25.600 --> 11:30.320\n So the good news is there are hardware companies building neural net chips. And so\n\n11:30.320 --> 11:37.520\n it's going to get better. However, the good news in a way, which is also a bad news,\n\n11:37.520 --> 11:46.960\n is that even our state of the art, deep learning methods fail to learn models that understand\n\n11:46.960 --> 11:50.480\n even very simple environments, like some grid worlds that we have built.\n\n11:52.000 --> 11:56.080\n Even these fairly simple environments, I mean, of course, if you train them with enough examples,\n\n11:56.080 --> 12:02.640\n eventually they get it. But it's just like, instead of what humans might need just\n\n12:03.440 --> 12:09.200\n dozens of examples, these things will need millions for very, very, very simple tasks.\n\n12:10.000 --> 12:16.640\n And so I think there's an opportunity for academics who don't have the kind of computing\n\n12:16.640 --> 12:23.440\n power that, say, Google has to do really important and exciting research to advance\n\n12:23.440 --> 12:30.960\n the state of the art in training frameworks, learning models, agent learning in even simple\n\n12:30.960 --> 12:37.200\n environments that are synthetic, that seem trivial, but yet current machine learning fails on.\n\n12:38.240 --> 12:43.760\n We talked about priors and common sense knowledge. It seems like\n\n12:43.760 --> 12:52.160\n we humans take a lot of knowledge for granted. So what's your view of these priors of forming\n\n12:52.160 --> 12:58.880\n this broad view of the world, this accumulation of information and how we can teach neural networks\n\n12:58.880 --> 13:05.520\n or learning systems to pick that knowledge up? So knowledge, for a while, the artificial\n\n13:05.520 --> 13:14.320\n intelligence was maybe in the 80s, like there's a time where knowledge representation, knowledge,\n\n13:14.320 --> 13:22.240\n acquisition, expert systems, I mean, the symbolic AI was a view, was an interesting problem set to\n\n13:22.240 --> 13:27.680\n solve and it was kind of put on hold a little bit, it seems like. Because it doesn't work.\n\n13:27.680 --> 13:34.960\n It doesn't work. That's right. But that's right. But the goals of that remain important.\n\n13:34.960 --> 13:39.760\n Yes. Remain important. And how do you think those goals can be addressed?\n\n13:39.760 --> 13:47.600\n Right. So first of all, I believe that one reason why the classical expert systems approach failed\n\n13:48.400 --> 13:54.000\n is because a lot of the knowledge we have, so you talked about common sense intuition,\n\n13:56.320 --> 14:01.680\n there's a lot of knowledge like this, which is not consciously accessible.\n\n14:01.680 --> 14:05.440\n There are lots of decisions we're taking that we can't really explain, even if sometimes we make\n\n14:05.440 --> 14:15.600\n up a story. And that knowledge is also necessary for machines to take good decisions. And that\n\n14:15.600 --> 14:22.400\n knowledge is hard to codify in expert systems, rule based systems and classical AI formalism.\n\n14:22.960 --> 14:29.520\n And there are other issues, of course, with the old AI, like not really good ways of handling\n\n14:29.520 --> 14:37.040\n uncertainty, I would say something more subtle, which we understand better now, but I think still\n\n14:37.040 --> 14:43.360\n isn't enough in the minds of people. There's something really powerful that comes from\n\n14:43.920 --> 14:49.280\n distributed representations, the thing that really makes neural nets work so well.\n\n14:49.280 --> 14:58.640\n And it's hard to replicate that kind of power in a symbolic world. The knowledge in expert systems\n\n14:58.640 --> 15:04.960\n and so on is nicely decomposed into like a bunch of rules. Whereas if you think about a neural net,\n\n15:04.960 --> 15:10.960\n it's the opposite. You have this big blob of parameters which work intensely together to\n\n15:10.960 --> 15:16.960\n represent everything the network knows. And it's not sufficiently factorized. It's not\n\n15:16.960 --> 15:23.520\n sufficiently factorized. And so I think this is one of the weaknesses of current neural nets,\n\n15:24.240 --> 15:32.320\n that we have to take lessons from classical AI in order to bring in another kind of compositionality,\n\n15:32.320 --> 15:38.800\n which is common in language, for example, and in these rules, but that isn't so native to neural\n\n15:38.800 --> 15:48.400\n nets. And on that line of thinking, disentangled representations. Yes. So let me connect with\n\n15:48.400 --> 15:55.280\n disentangled representations, if you might, if you don't mind. So for many years, I've thought,\n\n15:55.280 --> 16:00.560\n and I still believe that it's really important that we come up with learning algorithms,\n\n16:00.560 --> 16:06.400\n either unsupervised or supervised, but reinforcement, whatever, that build representations\n\n16:06.400 --> 16:13.360\n in which the important factors, hopefully causal factors are nicely separated and easy to pick up\n\n16:13.360 --> 16:18.480\n from the representation. So that's the idea of disentangled representations. It says transform\n\n16:18.480 --> 16:25.120\n the data into a space where everything becomes easy. We can maybe just learn with linear models\n\n16:25.120 --> 16:30.960\n about the things we care about. And I still think this is important, but I think this is missing out\n\n16:30.960 --> 16:37.280\n on a very important ingredient, which classical AI systems can remind us of.\n\n16:38.080 --> 16:41.920\n So let's say we have these disentangled representations. You still need to learn about\n\n16:43.440 --> 16:47.200\n the relationships between the variables, those high level semantic variables. They're not going\n\n16:47.200 --> 16:52.000\n to be independent. I mean, this is like too much of an assumption. They're going to have some\n\n16:52.000 --> 16:56.320\n interesting relationships that allow to predict things in the future, to explain what happened\n\n16:56.320 --> 17:01.600\n in the past. The kind of knowledge about those relationships in a classical AI system\n\n17:01.600 --> 17:06.000\n is encoded in the rules. Like a rule is just like a little piece of knowledge that says,\n\n17:06.000 --> 17:10.960\n oh, I have these two, three, four variables that are linked in this interesting way,\n\n17:10.960 --> 17:14.800\n then I can say something about one or two of them given a couple of others, right?\n\n17:14.800 --> 17:22.160\n In addition to disentangling the elements of the representation, which are like the variables\n\n17:22.160 --> 17:31.840\n in a rule based system, you also need to disentangle the mechanisms that relate those\n\n17:31.840 --> 17:37.200\n variables to each other. So like the rules. So the rules are neatly separated. Like each rule is,\n\n17:37.200 --> 17:43.360\n you know, living on its own. And when I change a rule because I'm learning, it doesn't need to\n\n17:43.360 --> 17:48.720\n break other rules. Whereas current neural nets, for example, are very sensitive to what's called\n\n17:48.720 --> 17:53.520\n catastrophic forgetting, where after I've learned some things and then I learn new things,\n\n17:54.080 --> 17:59.280\n they can destroy the old things that I had learned, right? If the knowledge was better\n\n17:59.280 --> 18:05.520\n factorized and separated, disentangled, then you would avoid a lot of that.\n\n18:06.560 --> 18:09.680\n Now, you can't do this in the sensory domain.\n\n18:10.320 --> 18:13.120\n What do you mean by sensory domain?\n\n18:13.120 --> 18:18.640\n Like in pixel space. But my idea is that when you project the data in the right semantic space,\n\n18:18.640 --> 18:25.040\n it becomes possible to now represent this extra knowledge beyond the transformation from inputs\n\n18:25.040 --> 18:30.000\n to representations, which is how representations act on each other and predict the future and so on\n\n18:31.120 --> 18:37.680\n in a way that can be neatly disentangled. So now it's the rules that are disentangled from each\n\n18:37.680 --> 18:40.400\n other and not just the variables that are disentangled from each other.\n\n18:40.400 --> 18:45.200\n And you draw a distinction between semantic space and pixel, like does there need to be\n\n18:45.200 --> 18:46.560\n an architectural difference?\n\n18:46.560 --> 18:51.280\n Well, yeah. So there's the sensory space like pixels, which where everything is entangled.\n\n18:52.080 --> 18:57.680\n The information, like the variables are completely interdependent in very complicated ways.\n\n18:58.160 --> 19:03.520\n And also computation, like it's not just the variables, it's also how they are related to\n\n19:03.520 --> 19:10.240\n each other is all intertwined. But I'm hypothesizing that in the right high level\n\n19:11.280 --> 19:16.720\n representation space, both the variables and how they relate to each other can be\n\n19:16.720 --> 19:20.240\n disentangled. And that will provide a lot of generalization power.\n\n19:20.800 --> 19:22.240\n Generalization power.\n\n19:22.240 --> 19:22.720\n Yes.\n\n19:22.720 --> 19:29.280\n Distribution of the test set is assumed to be the same as the distribution of the training set.\n\n19:29.280 --> 19:35.600\n Right. This is where current machine learning is too weak. It doesn't tell us anything,\n\n19:35.600 --> 19:40.080\n is not able to tell us anything about how our neural nets, say, are going to generalize to\n\n19:40.080 --> 19:45.120\n a new distribution. And, you know, people may think, well, but there's nothing we can say\n\n19:45.120 --> 19:50.880\n if we don't know what the new distribution will be. The truth is humans are able to generalize\n\n19:50.880 --> 19:51.760\n to new distributions.\n\n19:52.560 --> 19:54.000\n Yeah. How are we able to do that?\n\n19:54.000 --> 19:57.920\n Yeah. Because there is something, these new distributions, even though they could look\n\n19:57.920 --> 20:02.240\n very different from the training distributions, they have things in common. So let me give you\n\n20:02.240 --> 20:07.920\n a concrete example. You read a science fiction novel. The science fiction novel, maybe, you\n\n20:07.920 --> 20:15.200\n know, brings you in some other planet where things look very different on the surface,\n\n20:15.200 --> 20:20.000\n but it's still the same laws of physics. And so you can read the book and you understand\n\n20:20.000 --> 20:27.360\n what's going on. So the distribution is very different. But because you can transport\n\n20:27.360 --> 20:33.120\n a lot of the knowledge you had from Earth about the underlying cause and effect relationships\n\n20:33.120 --> 20:38.720\n and physical mechanisms and all that, and maybe even social interactions, you can now\n\n20:38.720 --> 20:42.160\n make sense of what is going on on this planet where, like, visually, for example,\n\n20:42.160 --> 20:43.280\n things are totally different.\n\n20:45.280 --> 20:50.800\n Taking that analogy further and distorting it, let's enter a science fiction world of,\n\n20:50.800 --> 20:59.840\n say, Space Odyssey, 2001, with Hal. Or maybe, which is probably one of my favorite AI movies.\n\n20:59.840 --> 21:00.480\n Me too.\n\n21:00.480 --> 21:05.360\n And then there's another one that a lot of people love that may be a little bit outside\n\n21:05.360 --> 21:10.000\n of the AI community is Ex Machina. I don't know if you've seen it.\n\n21:10.000 --> 21:10.480\n Yes. Yes.\n\n21:11.600 --> 21:16.000\n By the way, what are your views on that movie? Are you able to enjoy it?\n\n21:16.000 --> 21:21.120\n Are there things I like and things I hate?\n\n21:21.120 --> 21:26.800\n So you could talk about that in the context of a question I want to ask, which is, there's\n\n21:26.800 --> 21:32.800\n quite a large community of people from different backgrounds, often outside of AI, who are concerned\n\n21:32.800 --> 21:37.600\n about existential threat of artificial intelligence. You've seen this community\n\n21:37.600 --> 21:42.160\n develop over time. You've seen you have a perspective. So what do you think is the best\n\n21:42.160 --> 21:48.320\n way to talk about AI safety, to think about it, to have discourse about it within AI community\n\n21:48.320 --> 21:54.560\n and outside and grounded in the fact that Ex Machina is one of the main sources of information\n\n21:54.560 --> 21:56.560\n for the general public about AI?\n\n21:56.560 --> 22:02.240\n So I think you're putting it right. There's a big difference between the sort of discussion\n\n22:02.240 --> 22:07.600\n we ought to have within the AI community and the sort of discussion that really matter\n\n22:07.600 --> 22:17.120\n in the general public. So I think the picture of Terminator and AI loose and killing people\n\n22:17.120 --> 22:24.560\n and super intelligence that's going to destroy us, whatever we try, isn't really so useful\n\n22:24.560 --> 22:30.000\n for the public discussion. Because for the public discussion, the things I believe really\n\n22:30.000 --> 22:37.200\n matter are the short term and medium term, very likely negative impacts of AI on society,\n\n22:37.200 --> 22:43.280\n whether it's from security, like, you know, big brother scenarios with face recognition\n\n22:43.280 --> 22:50.000\n or killer robots, or the impact on the job market, or concentration of power and discrimination,\n\n22:50.000 --> 22:57.760\n all kinds of social issues, which could actually, some of them could really threaten democracy,\n\n22:57.760 --> 22:58.800\n for example.\n\n22:58.800 --> 23:04.000\n Just to clarify, when you said killer robots, you mean autonomous weapon, weapon systems.\n\n23:04.000 --> 23:06.320\n Yes, I don't mean that's right.\n\n23:06.320 --> 23:13.040\n So I think these short and medium term concerns should be important parts of the public debate.\n\n23:13.040 --> 23:24.080\n Now, existential risk, for me is a very unlikely consideration, but still worth academic investigation\n\n23:24.640 --> 23:30.080\n in the same way that you could say, should we study what could happen if meteorite, you\n\n23:30.080 --> 23:33.920\n know, came to earth and destroyed it. So I think it's very unlikely that this is going\n\n23:33.920 --> 23:43.040\n to happen in or happen in a reasonable future. The sort of scenario of an AI getting loose\n\n23:43.040 --> 23:46.560\n goes against my understanding of at least current machine learning and current neural\n\n23:46.560 --> 23:51.120\n nets and so on. It's not plausible to me. But of course, I don't have a crystal ball\n\n23:51.120 --> 23:55.520\n and who knows what AI will be in 50 years from now. So I think it is worth that scientists\n\n23:55.520 --> 23:59.680\n study those problems. It's just not a pressing question as far as I'm concerned.\n\n23:59.680 --> 24:05.840\n So before I continue down that line, I have a few questions there. But what do you like\n\n24:05.840 --> 24:09.840\n and not like about Ex Machina as a movie? Because I actually watched it for the second\n\n24:09.840 --> 24:15.600\n time and enjoyed it. I hated it the first time, and I enjoyed it quite a bit more the\n\n24:15.600 --> 24:23.440\n second time when I sort of learned to accept certain pieces of it, see it as a concept\n\n24:23.440 --> 24:26.320\n movie. What was your experience? What were your thoughts?\n\n24:26.320 --> 24:36.080\n So the negative is the picture it paints of science is totally wrong. Science in general\n\n24:36.080 --> 24:44.160\n and AI in particular. Science is not happening in some hidden place by some, you know, really\n\n24:44.160 --> 24:52.160\n smart guy, one person. This is totally unrealistic. This is not how it happens. Even a team of\n\n24:52.160 --> 24:59.840\n people in some isolated place will not make it. Science moves by small steps, thanks to\n\n24:59.840 --> 25:10.480\n the collaboration and community of a large number of people interacting. And all the\n\n25:10.480 --> 25:14.560\n scientists who are expert in their field kind of know what is going on, even in the industrial\n\n25:14.560 --> 25:21.920\n labs. It's information flows and leaks and so on. And the spirit of it is very different\n\n25:21.920 --> 25:25.600\n from the way science is painted in this movie.\n\n25:25.600 --> 25:32.400\n Yeah, let me ask on that point. It's been the case to this point that kind of even if\n\n25:32.400 --> 25:36.800\n the research happens inside Google or Facebook, inside companies, it still kind of comes out,\n\n25:36.800 --> 25:41.680\n ideas come out. Do you think that will always be the case with AI? Is it possible to bottle\n\n25:41.680 --> 25:47.360\n ideas to the point where there's a set of breakthroughs that go completely undiscovered\n\n25:47.360 --> 25:52.240\n by the general research community? Do you think that's even possible?\n\n25:52.240 --> 25:59.520\n It's possible, but it's unlikely. It's not how it is done now. It's not how I can foresee\n\n25:59.520 --> 26:09.520\n it in the foreseeable future. But of course, I don't have a crystal ball and science is\n\n26:09.520 --> 26:14.960\n a crystal ball. And so who knows? This is science fiction after all.\n\n26:14.960 --> 26:21.440\n I think it's ominous that the lights went off during that discussion.\n\n26:21.440 --> 26:25.320\n So the problem, again, there's one thing is the movie and you could imagine all kinds\n\n26:25.320 --> 26:30.320\n of science fiction. The problem for me, maybe similar to the question about existential\n\n26:30.320 --> 26:39.440\n risk, is that this kind of movie paints such a wrong picture of what is the actual science\n\n26:39.440 --> 26:45.640\n and how it's going on that it can have unfortunate effects on people's understanding of current\n\n26:45.640 --> 26:50.800\n science. And so that's kind of sad.\n\n26:50.800 --> 26:58.440\n There's an important principle in research, which is diversity. So in other words, research\n\n26:58.440 --> 27:03.720\n is exploration. Research is exploration in the space of ideas. And different people will\n\n27:03.720 --> 27:09.520\n focus on different directions. And this is not just good, it's essential. So I'm totally\n\n27:09.520 --> 27:16.440\n fine with people exploring directions that are contrary to mine or look orthogonal to\n\n27:16.440 --> 27:24.920\n mine. I am more than fine. I think it's important. I and my friends don't claim we have universal\n\n27:24.920 --> 27:29.560\n truth about what will, especially about what will happen in the future. Now that being\n\n27:29.560 --> 27:36.560\n said, we have our intuitions and then we act accordingly according to where we think we\n\n27:36.560 --> 27:42.480\n can be most useful and where society has the most to gain or to lose. We should have those\n\n27:42.480 --> 27:49.800\n debates and not end up in a society where there's only one voice and one way of thinking\n\n27:49.800 --> 27:53.520\n and research money is spread out.\n\n27:53.520 --> 27:59.040\n So disagreement is a sign of good research, good science.\n\n27:59.040 --> 28:00.040\n Yes.\n\n28:00.040 --> 28:08.600\n The idea of bias in the human sense of bias. How do you think about instilling in machine\n\n28:08.600 --> 28:15.240\n learning something that's aligned with human values in terms of bias? We intuitively as\n\n28:15.240 --> 28:21.160\n human beings have a concept of what bias means, of what fundamental respect for other human\n\n28:21.160 --> 28:26.760\n beings means. But how do we instill that into machine learning systems, do you think?\n\n28:26.760 --> 28:32.360\n So I think there are short term things that are already happening and then there are long\n\n28:32.360 --> 28:38.360\n term things that we need to do. In the short term, there are techniques that have been\n\n28:38.360 --> 28:44.200\n proposed and I think will continue to be improved and maybe alternatives will come up to take\n\n28:44.200 --> 28:50.120\n data sets in which we know there is bias, we can measure it. Pretty much any data set\n\n28:50.120 --> 28:55.520\n where humans are being observed taking decisions will have some sort of bias, discrimination\n\n28:55.520 --> 28:59.000\n against particular groups and so on.\n\n28:59.000 --> 29:04.240\n And we can use machine learning techniques to try to build predictors, classifiers that\n\n29:04.240 --> 29:11.600\n are going to be less biased. We can do it, for example, using adversarial methods to\n\n29:11.600 --> 29:18.360\n make our systems less sensitive to these variables we should not be sensitive to.\n\n29:18.360 --> 29:23.520\n So these are clear, well defined ways of trying to address the problem. Maybe they have weaknesses\n\n29:23.520 --> 29:28.840\n and more research is needed and so on. But I think in fact they are sufficiently mature\n\n29:28.840 --> 29:35.240\n that governments should start regulating companies where it matters, say like insurance companies,\n\n29:35.240 --> 29:40.480\n so that they use those techniques. Because those techniques will probably reduce the\n\n29:40.480 --> 29:46.440\n bias but at a cost. For example, maybe their predictions will be less accurate and so companies\n\n29:46.440 --> 29:48.560\n will not do it until you force them.\n\n29:48.560 --> 29:56.040\n All right, so this is short term. Long term, I'm really interested in thinking how we can\n\n29:56.040 --> 30:01.560\n instill moral values into computers. Obviously, this is not something we'll achieve in the\n\n30:01.560 --> 30:08.120\n next five or 10 years. How can we, you know, there's already work in detecting emotions,\n\n30:08.120 --> 30:19.880\n for example, in images, in sounds, in texts, and also studying how different agents interacting\n\n30:19.880 --> 30:28.200\n in different ways may correspond to patterns of, say, injustice, which could trigger anger.\n\n30:28.200 --> 30:37.840\n So these are things we can do in the medium term and eventually train computers to model,\n\n30:37.840 --> 30:46.960\n for example, how humans react emotionally. I would say the simplest thing is unfair situations\n\n30:46.960 --> 30:52.680\n which trigger anger. This is one of the most basic emotions that we share with other animals.\n\n30:52.680 --> 30:57.160\n I think it's quite feasible within the next few years that we can build systems that can\n\n30:57.160 --> 31:01.980\n detect these kinds of things to the extent, unfortunately, that they understand enough\n\n31:01.980 --> 31:08.240\n about the world around us, which is a long time away. But maybe we can initially do this\n\n31:08.240 --> 31:14.840\n in virtual environments. So you can imagine a video game where agents interact in some\n\n31:14.840 --> 31:21.640\n ways and then some situations trigger an emotion. I think we could train machines to detect\n\n31:21.640 --> 31:27.400\n those situations and predict that the particular emotion will likely be felt if a human was\n\n31:27.400 --> 31:29.460\n playing one of the characters.\n\n31:29.460 --> 31:35.720\n You have shown excitement and done a lot of excellent work with unsupervised learning.\n\n31:35.720 --> 31:39.840\n But there's been a lot of success on the supervised learning side.\n\n31:39.840 --> 31:40.840\n Yes, yes.\n\n31:40.840 --> 31:46.680\n And one of the things I'm really passionate about is how humans and robots work together.\n\n31:46.680 --> 31:52.800\n And in the context of supervised learning, that means the process of annotation. Do you\n\n31:52.800 --> 32:00.080\n think about the problem of annotation put in a more interesting way as humans teaching\n\n32:00.080 --> 32:01.080\n machines?\n\n32:01.080 --> 32:02.080\n Yes.\n\n32:02.080 --> 32:03.080\n Is there?\n\n32:03.080 --> 32:09.560\n Yes. I think it's an important subject. Reducing it to annotation may be useful for somebody\n\n32:09.560 --> 32:16.300\n building a system tomorrow. But longer term, the process of teaching, I think, is something\n\n32:16.300 --> 32:19.960\n that deserves a lot more attention from the machine learning community. So there are people\n\n32:19.960 --> 32:24.560\n who have coined the term machine teaching. So what are good strategies for teaching a\n\n32:24.560 --> 32:33.160\n learning agent? And can we design and train a system that is going to be a good teacher?\n\n32:33.160 --> 32:42.200\n So in my group, we have a project called BBI or BBI game, where there is a game or scenario\n\n32:42.200 --> 32:48.480\n where there's a learning agent and a teaching agent. Presumably, the teaching agent would\n\n32:48.480 --> 32:57.960\n eventually be a human. But we're not there yet. And the role of the teacher is to use\n\n32:57.960 --> 33:04.840\n its knowledge of the environment, which it can acquire using whatever way brute force\n\n33:04.840 --> 33:10.760\n to help the learner learn as quickly as possible. So the learner is going to try to learn by\n\n33:10.760 --> 33:19.920\n itself, maybe using some exploration and whatever. But the teacher can choose, can have an influence\n\n33:19.920 --> 33:27.160\n on the interaction with the learner, so as to guide the learner, maybe teach it the things\n\n33:27.160 --> 33:30.840\n that the learner has most trouble with, or just add the boundary between what it knows\n\n33:30.840 --> 33:36.180\n and doesn't know, and so on. So there's a tradition of these kind of ideas from other\n\n33:36.180 --> 33:45.320\n fields and like tutorial systems, for example, and AI. And of course, people in the humanities\n\n33:45.320 --> 33:48.240\n have been thinking about these questions. But I think it's time that machine learning\n\n33:48.240 --> 33:55.440\n people look at this, because in the future, we'll have more and more human machine interaction\n\n33:55.440 --> 34:01.040\n with the human in the loop. And I think understanding how to make this work better, all the problems\n\n34:01.040 --> 34:06.160\n around that are very interesting and not sufficiently addressed. You've done a lot of work with\n\n34:06.160 --> 34:14.000\n language, too. What aspect of the traditionally formulated Turing test, a test of natural\n\n34:14.000 --> 34:19.520\n language understanding and generation in your eyes is the most difficult of conversation?\n\n34:19.520 --> 34:25.640\n What in your eyes is the hardest part of conversation to solve for machines? So I would say it's\n\n34:25.640 --> 34:32.300\n everything having to do with the non linguistic knowledge, which implicitly you need in order\n\n34:32.300 --> 34:37.680\n to make sense of sentences, things like the Winograd schema. So these sentences that are\n\n34:37.680 --> 34:43.720\n semantically ambiguous. In other words, you need to understand enough about the world\n\n34:43.720 --> 34:49.280\n in order to really interpret properly those sentences. I think these are interesting challenges\n\n34:49.280 --> 34:57.300\n for machine learning, because they point in the direction of building systems that both\n\n34:57.300 --> 35:03.760\n understand how the world works and this causal relationships in the world and associate that\n\n35:03.760 --> 35:12.080\n knowledge with how to express it in language, either for reading or writing.\n\n35:12.080 --> 35:13.080\n You speak French?\n\n35:13.080 --> 35:14.760\n Yes, it's my mother tongue.\n\n35:14.760 --> 35:20.400\n It's one of the romance languages. Do you think passing the Turing test and all the\n\n35:20.400 --> 35:24.320\n underlying challenges we just mentioned depend on language? Do you think it might be easier\n\n35:24.320 --> 35:28.920\n in French than it is in English, or is independent of language?\n\n35:28.920 --> 35:37.600\n I think it's independent of language. I would like to build systems that can use the same\n\n35:37.600 --> 35:46.720\n principles, the same learning mechanisms to learn from human agents, whatever their language.\n\n35:46.720 --> 35:53.560\n Well, certainly us humans can talk more beautifully and smoothly in poetry, some Russian originally.\n\n35:53.560 --> 36:02.600\n I know poetry in Russian is maybe easier to convey complex ideas than it is in English.\n\n36:02.600 --> 36:09.480\n But maybe I'm showing my bias and some people could say that about French. But of course,\n\n36:09.480 --> 36:15.880\n the goal ultimately is our human brain is able to utilize any kind of those languages\n\n36:15.880 --> 36:18.280\n to use them as tools to convey meaning.\n\n36:18.280 --> 36:22.040\n Yeah, of course, there are differences between languages, and maybe some are slightly better\n\n36:22.040 --> 36:26.120\n at some things, but in the grand scheme of things, where we're trying to understand how\n\n36:26.120 --> 36:32.040\n the brain works and language and so on, I think these differences are minute.\n\n36:32.040 --> 36:38.880\n So you've lived perhaps through an AI winter of sorts?\n\n36:38.880 --> 36:39.920\n Yes.\n\n36:39.920 --> 36:44.740\n How did you stay warm and continue your research?\n\n36:44.740 --> 36:45.740\n Stay warm with friends.\n\n36:45.740 --> 36:51.160\n With friends. Okay, so it's important to have friends. And what have you learned from the\n\n36:51.160 --> 36:53.600\n experience?\n\n36:53.600 --> 37:02.040\n Listen to your inner voice. Don't, you know, be trying to just please the crowds and the\n\n37:02.040 --> 37:10.320\n fashion. And if you have a strong intuition about something that is not contradicted by\n\n37:10.320 --> 37:17.280\n actual evidence, go for it. I mean, it could be contradicted by people.\n\n37:17.280 --> 37:20.600\n Not your own instinct of based on everything you've learned?\n\n37:20.600 --> 37:28.320\n Of course, you have to adapt your beliefs when your experiments contradict those beliefs.\n\n37:28.320 --> 37:35.000\n But you have to stick to your beliefs. Otherwise, it's what allowed me to go through those years.\n\n37:35.000 --> 37:42.040\n It's what allowed me to persist in directions that, you know, took time, whatever other\n\n37:42.040 --> 37:48.040\n people think, took time to mature and bring fruits.\n\n37:48.040 --> 37:54.520\n So history of AI is marked with these, of course, it's marked with technical breakthroughs,\n\n37:54.520 --> 38:00.980\n but it's also marked with these seminal events that capture the imagination of the community.\n\n38:00.980 --> 38:06.400\n Most recent, I would say, AlphaGo beating the world champion human Go player was one\n\n38:06.400 --> 38:12.360\n of those moments. What do you think the next such moment might be?\n\n38:12.360 --> 38:22.600\n Okay, so first of all, I think that these so called seminal events are overrated. As\n\n38:22.600 --> 38:30.200\n I said, science really moves by small steps. Now what happens is you make one more small\n\n38:30.200 --> 38:39.480\n step and it's like the drop that, you know, that fills the bucket and then you have drastic\n\n38:39.480 --> 38:43.920\n consequences because now you're able to do something you were not able to do before.\n\n38:43.920 --> 38:49.720\n Or now, say, the cost of building some device or solving a problem becomes cheaper than\n\n38:49.720 --> 38:53.900\n what existed and you have a new market that opens up, right? So especially in the world\n\n38:53.900 --> 39:03.760\n of commerce and applications, the impact of a small scientific progress could be huge.\n\n39:03.760 --> 39:07.800\n But in the science itself, I think it's very, very gradual.\n\n39:07.800 --> 39:13.160\n And where are these steps being taken now? So there's unsupervised learning.\n\n39:13.160 --> 39:23.380\n So if I look at one trend that I like in my community, so for example, at Milan, my institute,\n\n39:23.380 --> 39:31.840\n what are the two hardest topics? GANs and reinforcement learning. Even though in Montreal\n\n39:31.840 --> 39:37.020\n in particular, reinforcement learning was something pretty much absent just two or three\n\n39:37.020 --> 39:44.280\n years ago. So there's really a big interest from students and there's a big interest from\n\n39:44.280 --> 39:51.560\n people like me. So I would say this is something where we're going to see more progress, even\n\n39:51.560 --> 39:58.680\n though it hasn't yet provided much in terms of actual industrial fallout. Like even though\n\n39:58.680 --> 40:03.360\n there's AlphaGo, there's no, like Google is not making money on this right now. But I\n\n40:03.360 --> 40:08.960\n think over the long term, this is really, really important for many reasons.\n\n40:08.960 --> 40:13.840\n So in other words, I would say reinforcement learning may be more generally agent learning\n\n40:13.840 --> 40:17.520\n because it doesn't have to be with rewards. It could be in all kinds of ways that an agent\n\n40:17.520 --> 40:20.720\n is learning about its environment.\n\n40:20.720 --> 40:28.840\n Now reinforcement learning you're excited about, do you think GANs could provide something,\n\n40:28.840 --> 40:38.880\n at the moment? Well, GANs or other generative models, I believe, will be crucial ingredients\n\n40:38.880 --> 40:45.480\n in building agents that can understand the world. A lot of the successes in reinforcement\n\n40:45.480 --> 40:51.160\n learning in the past has been with policy gradient, where you just learn a policy, you\n\n40:51.160 --> 40:55.760\n don't actually learn a model of the world. But there are lots of issues with that. And\n\n40:55.760 --> 41:00.880\n we don't know how to do model based RL right now. But I think this is where we have to\n\n41:00.880 --> 41:09.340\n go in order to build models that can generalize faster and better like to new distributions\n\n41:09.340 --> 41:16.120\n that capture to some extent, at least the underlying causal mechanisms in the world.\n\n41:16.120 --> 41:21.480\n Last question. What made you fall in love with artificial intelligence? If you look\n\n41:21.480 --> 41:28.880\n back, what was the first moment in your life when you were fascinated by either the human\n\n41:28.880 --> 41:31.360\n mind or the artificial mind?\n\n41:31.360 --> 41:35.520\n You know, when I was an adolescent, I was reading a lot. And then I started reading\n\n41:35.520 --> 41:36.520\n science fiction.\n\n41:36.520 --> 41:37.520\n There you go.\n\n41:37.520 --> 41:46.520\n That's it. That's where I got hooked. And then, you know, I had one of the first personal\n\n41:46.520 --> 41:52.680\n computers and I got hooked in programming. And so it just, you know,\n\n41:52.680 --> 41:54.800\n Start with fiction and then make it a reality.\n\n41:54.800 --> 41:55.800\n That's right.\n\n41:55.800 --> 41:57.560\n Yoshua, thank you so much for talking to me.\n\n41:57.560 --> 42:18.160\n My pleasure.\n\n"
}