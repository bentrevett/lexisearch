{
  "title": "Jitendra Malik: Computer Vision | Lex Fridman Podcast #110",
  "id": "LRYkH-fAVGE",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:05.280\n The following is a conversation with Jitendra Malik, a professor at Berkeley and one of\n\n00:05.280 --> 00:10.080\n the seminal figures in the field of computer vision, the kind before the deep learning\n\n00:10.080 --> 00:13.940\n revolution and the kind after.\n\n00:13.940 --> 00:21.040\n He has been cited over 180,000 times and has mentored many world class researchers in computer\n\n00:21.040 --> 00:22.880\n science.\n\n00:22.880 --> 00:24.540\n Quick summary of the ads.\n\n00:24.540 --> 00:31.520\n Two sponsors, one new one which is BetterHelp and an old goodie ExpressVPN.\n\n00:31.520 --> 00:37.240\n Please consider supporting this podcast by going to betterhelp.com slash lex and signing\n\n00:37.240 --> 00:40.960\n up at expressvpn.com slash lexpod.\n\n00:40.960 --> 00:45.600\n Click the links, buy the stuff, it really is the best way to support this podcast and\n\n00:45.600 --> 00:47.300\n the journey I'm on.\n\n00:47.300 --> 00:52.400\n If you enjoy this thing, subscribe on YouTube, review it with 5 stars on Apple Podcast, support\n\n00:52.400 --> 00:57.880\n it on Patreon, or connect with me on Twitter at Lex Friedman, however the heck you spell\n\n00:57.880 --> 00:58.880\n that.\n\n00:58.880 --> 01:02.920\n As usual, I'll do a few minutes of ads now and never any ads in the middle that can break\n\n01:02.920 --> 01:05.240\n the flow of the conversation.\n\n01:05.240 --> 01:11.640\n This show is sponsored by BetterHelp, spelled H E L P help.\n\n01:11.640 --> 01:15.120\n Check it out at betterhelp.com slash lex.\n\n01:15.120 --> 01:19.440\n They figure out what you need and match you with a licensed professional therapist in\n\n01:19.440 --> 01:21.480\n under 48 hours.\n\n01:21.480 --> 01:26.480\n It's not a crisis line, it's not self help, it's professional counseling done securely\n\n01:26.480 --> 01:27.480\n online.\n\n01:27.480 --> 01:33.360\n I'm a bit from the David Goggins line of creatures, as you may know, and so have some\n\n01:33.360 --> 01:40.240\n demons to contend with, usually on long runs or all nights working, forever and possibly\n\n01:40.240 --> 01:42.060\n full of self doubt.\n\n01:42.060 --> 01:47.180\n It may be because I'm Russian, but I think suffering is essential for creation.\n\n01:47.180 --> 01:52.040\n But I also think you can suffer beautifully, in a way that doesn't destroy you.\n\n01:52.040 --> 01:56.440\n For most people, I think a good therapist can help in this, so it's at least worth a\n\n01:56.440 --> 01:57.440\n try.\n\n01:57.440 --> 02:03.340\n Check out their reviews, they're good, it's easy, private, affordable, available worldwide.\n\n02:03.340 --> 02:09.780\n You can communicate by text, any time, and schedule weekly audio and video sessions.\n\n02:09.780 --> 02:15.440\n I highly recommend that you check them out at betterhelp.com slash lex.\n\n02:15.440 --> 02:19.520\n This show is also sponsored by ExpressVPN.\n\n02:19.520 --> 02:26.080\n Get it at expressvpn.com slash lexpod to support this podcast and to get an extra three months\n\n02:26.080 --> 02:28.520\n free on a one year package.\n\n02:28.520 --> 02:32.640\n I've been using ExpressVPN for many years, I love it.\n\n02:32.640 --> 02:36.080\n I think ExpressVPN is the best VPN out there.\n\n02:36.080 --> 02:39.160\n They told me to say it, but it happens to be true.\n\n02:39.160 --> 02:45.520\n It doesn't log your data, it's crazy fast, and is easy to use, literally just one big,\n\n02:45.520 --> 02:47.360\n sexy power on button.\n\n02:47.360 --> 02:51.480\n Again, for obvious reasons, it's really important that they don't log your data.\n\n02:51.480 --> 02:57.120\n It works on Linux and everywhere else too, but really, why use anything else?\n\n02:57.120 --> 03:02.280\n Shout out to my favorite flavor of Linux, Ubuntu Mate 2004.\n\n03:02.280 --> 03:09.120\n Once again, get it at expressvpn.com slash lexpod to support this podcast and to get\n\n03:09.120 --> 03:13.200\n an extra three months free on a one year package.\n\n03:13.200 --> 03:18.140\n And now, here's my conversation with Jitendra Malik.\n\n03:18.140 --> 03:25.640\n In 1966, Seymour Papert at MIT wrote up a proposal called the Summer Vision Project\n\n03:25.640 --> 03:31.360\n to be given, as far as we know, to 10 students to work on and solve that summer.\n\n03:31.360 --> 03:37.080\n So that proposal outlined many of the computer vision tasks we still work on today.\n\n03:37.080 --> 03:43.040\n Why do you think we underestimate, and perhaps we did underestimate and perhaps still underestimate\n\n03:43.040 --> 03:46.420\n how hard computer vision is?\n\n03:46.420 --> 03:51.040\n Because most of what we do in vision, we do unconsciously or subconsciously.\n\n03:51.040 --> 03:52.040\n In human vision.\n\n03:52.040 --> 03:53.080\n In human vision.\n\n03:53.080 --> 03:58.400\n So that gives us this, that effortlessness gives us the sense that, oh, this must be\n\n03:58.400 --> 04:02.040\n very easy to implement on a computer.\n\n04:02.040 --> 04:09.480\n Now, this is why the early researchers in AI got it so wrong.\n\n04:09.480 --> 04:17.560\n However, if you go into neuroscience or psychology of human vision, then the complexity becomes\n\n04:17.560 --> 04:19.100\n very clear.\n\n04:19.100 --> 04:26.640\n The fact is that a very large part of the cerebral cortex is devoted to visual processing.\n\n04:26.640 --> 04:29.520\n And this is true in other primates as well.\n\n04:29.520 --> 04:35.960\n So once we looked at it from a neuroscience or psychology perspective, it becomes quite\n\n04:35.960 --> 04:39.680\n clear that the problem is very challenging and it will take some time.\n\n04:39.680 --> 04:43.800\n You said the higher level parts are the harder parts?\n\n04:43.800 --> 04:52.940\n I think vision appears to be easy because most of what visual processing is subconscious\n\n04:52.940 --> 04:55.680\n or unconscious.\n\n04:55.680 --> 05:03.940\n So we underestimate the difficulty, whereas when you are like proving a mathematical theorem\n\n05:03.940 --> 05:08.580\n or playing chess, the difficulty is much more evident.\n\n05:08.580 --> 05:15.320\n So because it is your conscious brain, which is processing various aspects of the problem\n\n05:15.320 --> 05:21.960\n solving behavior, whereas in vision, all this is happening, but it's not in your awareness,\n\n05:21.960 --> 05:25.840\n it's in your, it's operating below that.\n\n05:25.840 --> 05:27.880\n But it's, it still seems strange.\n\n05:27.880 --> 05:35.320\n Yes, that's true, but it seems strange that as computer vision researchers, for example,\n\n05:35.320 --> 05:41.020\n the community broadly is time and time again makes the mistake of thinking the problem\n\n05:41.020 --> 05:43.880\n is easier than it is, or maybe it's not a mistake.\n\n05:43.880 --> 05:48.160\n We'll talk a little bit about autonomous driving, for example, how hard of a vision task that\n\n05:48.160 --> 05:56.120\n is, it, do you think, I mean, what, is it just human nature or is there something fundamental\n\n05:56.120 --> 06:01.000\n to the vision problem that we, we underestimate?\n\n06:01.000 --> 06:05.400\n We're still not able to be cognizant of how hard the problem is.\n\n06:05.400 --> 06:11.800\n Yeah, I think in the early days it could have been excused because in the early days, all\n\n06:11.800 --> 06:15.520\n aspects of AI were regarded as too easy.\n\n06:15.520 --> 06:19.920\n But I think today it is much less excusable.\n\n06:19.920 --> 06:27.800\n And I think why people fall for this is because of what I call the fallacy of the successful\n\n06:27.800 --> 06:30.320\n first step.\n\n06:30.320 --> 06:37.840\n There are many problems in vision where getting 50% of the solution you can get in one minute,\n\n06:37.840 --> 06:47.720\n getting to 90% can take you a day, getting to 99% may take you five years, and 99.99%\n\n06:47.720 --> 06:49.720\n may be not in your lifetime.\n\n06:49.720 --> 06:52.640\n I wonder if that's a unique division.\n\n06:52.640 --> 06:58.040\n It seems that language, people are not so confident about, so natural language processing,\n\n06:58.040 --> 07:04.200\n people are a little bit more cautious about our ability to, to solve that problem.\n\n07:04.200 --> 07:10.640\n I think for language, people intuit that we have to be able to do natural language understanding.\n\n07:10.640 --> 07:18.400\n For vision, it seems that we're not cognizant or we don't think about how much understanding\n\n07:18.400 --> 07:19.400\n is required.\n\n07:19.400 --> 07:21.520\n It's probably still an open problem.\n\n07:21.520 --> 07:27.440\n But in your sense, how much understanding is required to solve vision?\n\n07:27.440 --> 07:34.720\n Like this, put another way, how much something called common sense reasoning is required\n\n07:34.720 --> 07:39.080\n to really be able to interpret even static scenes?\n\n07:39.080 --> 07:40.080\n Yeah.\n\n07:40.080 --> 07:47.760\n So vision operates at all levels and there are parts which can be solved with what we\n\n07:47.760 --> 07:50.800\n could call maybe peripheral processing.\n\n07:50.800 --> 07:57.320\n So in the human vision literature, there used to be these terms, sensation, perception and\n\n07:57.320 --> 08:04.320\n cognition, which roughly speaking referred to like the front end of processing, middle\n\n08:04.320 --> 08:08.220\n stages of processing and higher level of processing.\n\n08:08.220 --> 08:13.680\n And I think they made a big deal out of, out of this and they wanted to study only perception\n\n08:13.680 --> 08:19.240\n and then dismiss certain, certain problems as being quote cognitive.\n\n08:19.240 --> 08:23.200\n But really I think these are artificial divides.\n\n08:23.200 --> 08:28.560\n The problem is continuous at all levels and there are challenges at all levels.\n\n08:28.560 --> 08:34.120\n The techniques that we have today, they work better at the lower and mid levels of the\n\n08:34.120 --> 08:35.120\n problem.\n\n08:35.120 --> 08:39.960\n I think the higher levels of the problem, quote the cognitive levels of the problem\n\n08:39.960 --> 08:46.480\n are there and we, in many real applications, we have to confront them.\n\n08:46.480 --> 08:51.520\n Now how much that is necessary will depend on the application.\n\n08:51.520 --> 08:55.280\n For some problems it doesn't matter, for some problems it matters a lot.\n\n08:55.280 --> 09:04.960\n So I am, for example, a pessimist on fully autonomous driving in the near future.\n\n09:04.960 --> 09:13.880\n And the reason is because I think there will be that 0.01% of the cases where quite sophisticated\n\n09:13.880 --> 09:16.120\n cognitive reasoning is called for.\n\n09:16.120 --> 09:23.720\n However, there are tasks where you can, first of all, they are much more, they are robust.\n\n09:23.720 --> 09:28.440\n So in the sense that error rates, error is not so much of a problem.\n\n09:28.440 --> 09:34.840\n For example, let's say we are, you're doing image search, you're trying to get images\n\n09:34.840 --> 09:41.900\n based on some, some, some description, some visual description.\n\n09:41.900 --> 09:43.840\n We are very tolerant of errors there, right?\n\n09:43.840 --> 09:49.360\n I mean, when Google image search gives you some images back and a few of them are wrong,\n\n09:49.360 --> 09:50.360\n it's okay.\n\n09:50.360 --> 09:51.360\n It doesn't hurt anybody.\n\n09:51.360 --> 09:54.720\n There is no, there's not a matter of life and death.\n\n09:54.720 --> 10:02.600\n But making mistakes when you are driving at 60 miles per hour and you could potentially\n\n10:02.600 --> 10:06.160\n kill somebody is much more important.\n\n10:06.160 --> 10:11.220\n So just for the, for the fun of it, since you mentioned, let's go there briefly about\n\n10:11.220 --> 10:12.880\n autonomous vehicles.\n\n10:12.880 --> 10:19.200\n So one of the companies in the space, Tesla, is with Andre Karpathy and Elon Musk are working\n\n10:19.200 --> 10:26.400\n on a system called Autopilot, which is primarily a vision based system with eight cameras and\n\n10:26.400 --> 10:30.560\n basically a single neural network, a multitask neural network.\n\n10:30.560 --> 10:35.680\n They call it HydroNet, multiple heads, so it does multiple tasks, but is forming the\n\n10:35.680 --> 10:38.800\n same representation at the core.\n\n10:38.800 --> 10:47.120\n Do you think driving can be converted in this way to purely a vision problem and then solved\n\n10:47.120 --> 10:53.720\n with learning or even more specifically in the current approach, what do you think about\n\n10:53.720 --> 10:57.120\n what Tesla Autopilot team is doing?\n\n10:57.120 --> 11:02.800\n So the way I think about it is that there are certainly subsets of the visual based\n\n11:02.800 --> 11:05.480\n driving problem, which are quite solvable.\n\n11:05.480 --> 11:11.960\n So for example, driving in freeway conditions is quite a solvable problem.\n\n11:11.960 --> 11:18.600\n I think there were demonstrations of that going back to the 1980s by someone called\n\n11:18.600 --> 11:22.080\n Ernst Tickmans in Munich.\n\n11:22.080 --> 11:27.200\n In the 90s, there were approaches from Carnegie Mellon, there were approaches from our team\n\n11:27.200 --> 11:28.780\n at Berkeley.\n\n11:28.780 --> 11:33.200\n In the 2000s, there were approaches from Stanford and so on.\n\n11:33.200 --> 11:38.560\n So autonomous driving in certain settings is very doable.\n\n11:38.560 --> 11:45.440\n The challenge is to have an autopilot work under all kinds of driving conditions.\n\n11:45.440 --> 11:51.280\n At that point, it's not just a question of vision or perception, but really also of control\n\n11:51.280 --> 11:54.200\n and dealing with all the edge cases.\n\n11:54.200 --> 11:59.160\n So where do you think most of the difficult cases, to me, even the highway driving is\n\n11:59.160 --> 12:08.000\n an open problem because it applies the same 50, 90, 95, 99 rule where the first step,\n\n12:08.000 --> 12:12.080\n the fallacy of the first step, I forget how you put it, we fall victim to.\n\n12:12.080 --> 12:17.120\n I think even highway driving has a lot of elements because to solve autonomous driving,\n\n12:17.120 --> 12:22.920\n you have to completely relinquish the help of a human being.\n\n12:22.920 --> 12:26.640\n You're always in control so that you're really going to feel the edge cases.\n\n12:26.640 --> 12:29.480\n So I think even highway driving is really difficult.\n\n12:29.480 --> 12:35.440\n But in terms of the general driving task, do you think vision is the fundamental problem\n\n12:35.440 --> 12:44.800\n or is it also your action, the interaction with the environment, the ability to...\n\n12:44.800 --> 12:48.720\n And then the middle ground, I don't know if you put that under vision, which is trying\n\n12:48.720 --> 12:54.720\n to predict the behavior of others, which is a little bit in the world of understanding\n\n12:54.720 --> 13:00.640\n the scene, but it's also trying to form a model of the actors in the scene and predict\n\n13:00.640 --> 13:01.640\n their behavior.\n\n13:01.640 --> 13:02.640\n Yeah.\n\n13:02.640 --> 13:08.320\n I include that in vision because to me, perception blends into cognition and building predictive\n\n13:08.320 --> 13:13.520\n models of other agents in the world, which could be other agents, could be people, other\n\n13:13.520 --> 13:15.520\n agents could be other cars.\n\n13:15.520 --> 13:22.720\n That is part of the task of perception because perception always has to not tell us what\n\n13:22.720 --> 13:26.480\n is now, but what will happen because what's now is boring.\n\n13:26.480 --> 13:27.480\n It's done.\n\n13:27.480 --> 13:28.480\n It's over with.\n\n13:28.480 --> 13:29.480\n Okay?\n\n13:29.480 --> 13:30.480\n Yeah.\n\n13:30.480 --> 13:33.520\n We care about the future because we act in the future.\n\n13:33.520 --> 13:39.020\n And we care about the past in as much as it informs what's going to happen in the future.\n\n13:39.020 --> 13:45.920\n So I think we have to build predictive models of behaviors of people and those can get quite\n\n13:45.920 --> 13:48.020\n complicated.\n\n13:48.020 --> 13:59.760\n So I mean, I've seen examples of this in actually, I mean, I own a Tesla and it has various safety\n\n13:59.760 --> 14:01.720\n features built in.\n\n14:01.720 --> 14:09.920\n And what I see are these examples where let's say there is some a skateboarder, I mean,\n\n14:09.920 --> 14:16.160\n and I don't want to be too critical because obviously these systems are always being improved\n\n14:16.160 --> 14:23.680\n and any specific criticism I have, maybe the system six months from now will not have that\n\n14:23.680 --> 14:25.800\n particular failure mode.\n\n14:25.800 --> 14:36.680\n So it had the wrong response and it's because it couldn't predict what this skateboarder\n\n14:36.680 --> 14:38.360\n was going to do.\n\n14:38.360 --> 14:39.360\n Okay?\n\n14:39.360 --> 14:45.120\n And because it really required that higher level cognitive understanding of what skateboarders\n\n14:45.120 --> 14:48.760\n typically do as opposed to a normal pedestrian.\n\n14:48.760 --> 14:53.640\n So what might have been the correct behavior for a pedestrian, a typical behavior for pedestrian\n\n14:53.640 --> 14:59.040\n was not the typical behavior for a skateboarder, right?\n\n14:59.040 --> 15:00.040\n Yeah.\n\n15:00.040 --> 15:07.600\n And so therefore to do a good job there, you need to have enough data where you have pedestrians,\n\n15:07.600 --> 15:14.720\n you also have skateboarders, you've seen enough skateboarders to see what kinds of patterns\n\n15:14.720 --> 15:16.560\n of behavior they have.\n\n15:16.560 --> 15:21.660\n So it is in principle with enough data, that problem could be solved.\n\n15:21.660 --> 15:29.960\n But I think our current systems, computer vision systems, they need far, far more data\n\n15:29.960 --> 15:33.760\n than humans do for learning those same capabilities.\n\n15:33.760 --> 15:38.100\n So say that there is going to be a system that solves autonomous driving.\n\n15:38.100 --> 15:43.480\n Do you think it will look similar to what we have today, but have a lot more data, perhaps\n\n15:43.480 --> 15:48.800\n more compute, but the fundamental architecture is involved, like neural, well, in the case\n\n15:48.800 --> 15:52.280\n of Tesla autopilot is neural networks.\n\n15:52.280 --> 15:57.160\n Do you think it will look similar in that regard and we'll just have more data?\n\n15:57.160 --> 16:01.880\n That's a scientific hypothesis as to which way is it going to go.\n\n16:01.880 --> 16:05.420\n I will tell you what I would bet on.\n\n16:05.420 --> 16:14.200\n So and this is my general philosophical position on how these learning systems have been.\n\n16:14.200 --> 16:20.860\n What we have found currently very effective in computer vision in the deep learning paradigm\n\n16:20.860 --> 16:27.800\n is sort of tabula rasa learning and tabula rasa learning in a supervised way with lots\n\n16:27.800 --> 16:28.800\n and lots of...\n\n16:28.800 --> 16:29.800\n What's tabula rasa learning?\n\n16:29.800 --> 16:35.340\n Tabula rasa in the sense that blank slate, we just have the system, which is given a\n\n16:35.340 --> 16:39.960\n series of experiences in this setting and then it learns there.\n\n16:39.960 --> 16:44.700\n Now if let's think about human driving, it is not tabula rasa learning.\n\n16:44.700 --> 16:55.240\n So at the age of 16 in high school, a teenager goes into driver ed class, right?\n\n16:55.240 --> 17:02.040\n And now at that point they learn, but at the age of 16, they are already visual geniuses\n\n17:02.040 --> 17:07.720\n because from zero to 16, they have built a certain repertoire of vision.\n\n17:07.720 --> 17:13.520\n In fact, most of it has probably been achieved by age two, right?\n\n17:13.520 --> 17:18.160\n In this period of age up to age two, they know that the world is three dimensional.\n\n17:18.160 --> 17:22.360\n They know how objects look like from different perspectives.\n\n17:22.360 --> 17:24.720\n They know about occlusion.\n\n17:24.720 --> 17:29.760\n They know about common dynamics of humans and other bodies.\n\n17:29.760 --> 17:32.200\n They have some notion of intuitive physics.\n\n17:32.200 --> 17:38.820\n So they built that up from their observations and interactions in early childhood and of\n\n17:38.820 --> 17:44.020\n course reinforced through their growing up to age 16.\n\n17:44.020 --> 17:49.400\n So then at age 16, when they go into driver ed, what are they learning?\n\n17:49.400 --> 17:52.360\n They're not learning afresh the visual world.\n\n17:52.360 --> 17:54.800\n They have a mastery of the visual world.\n\n17:54.800 --> 17:58.520\n What they are learning is control, okay?\n\n17:58.520 --> 18:04.000\n They're learning how to be smooth about control, about steering and brakes and so forth.\n\n18:04.000 --> 18:08.000\n They're learning a sense of typical traffic situations.\n\n18:08.000 --> 18:17.840\n Now that education process can be quite short because they are coming in as visual geniuses.\n\n18:17.840 --> 18:23.440\n And of course in their future, they're going to encounter situations which are very novel,\n\n18:23.440 --> 18:24.440\n right?\n\n18:24.440 --> 18:29.720\n So during my driver ed class, I may not have had to deal with a skateboarder.\n\n18:29.720 --> 18:37.640\n I may not have had to deal with a truck driving in front of me where the back opens up and\n\n18:37.640 --> 18:42.260\n some junk gets dropped from the truck and I have to deal with it, right?\n\n18:42.260 --> 18:47.480\n But I can deal with this as a driver even though I did not encounter this in my driver\n\n18:47.480 --> 18:48.840\n ed class.\n\n18:48.840 --> 18:52.880\n And the reason I can deal with it is because I have all this general visual knowledge and\n\n18:52.880 --> 18:55.120\n expertise.\n\n18:55.120 --> 19:02.440\n And do you think the learning mechanisms we have today can do that kind of long term accumulation\n\n19:02.440 --> 19:03.800\n of knowledge?\n\n19:03.800 --> 19:11.400\n Or do we have to do some kind of, you know, the work that led up to expert systems with\n\n19:11.400 --> 19:17.720\n knowledge representation, you know, the broader field of artificial intelligence worked on\n\n19:17.720 --> 19:20.240\n this kind of accumulation of knowledge.\n\n19:20.240 --> 19:22.040\n Do you think neural networks can do the same?\n\n19:22.040 --> 19:29.960\n I think I don't see any in principle problem with neural networks doing it, but I think\n\n19:29.960 --> 19:33.760\n the learning techniques would need to evolve significantly.\n\n19:33.760 --> 19:41.520\n So the current learning techniques that we have are supervised learning.\n\n19:41.520 --> 19:47.520\n You're given lots of examples, x, y, y pairs and you learn the functional mapping between\n\n19:47.520 --> 19:48.520\n them.\n\n19:48.520 --> 19:52.360\n I think that human learning is far richer than that.\n\n19:52.360 --> 19:54.760\n It includes many different components.\n\n19:54.760 --> 20:05.560\n There is a child explores the world and sees, for example, a child takes an object and manipulates\n\n20:05.560 --> 20:12.760\n it in his hand and therefore gets to see the object from different points of view.\n\n20:12.760 --> 20:14.820\n And the child has commanded the movement.\n\n20:14.820 --> 20:21.000\n So that's a kind of learning data, but the learning data has been arranged by the child.\n\n20:21.000 --> 20:23.600\n And this is a very rich kind of data.\n\n20:23.600 --> 20:30.540\n The child can do various experiments with the world.\n\n20:30.540 --> 20:36.700\n So there are many aspects of sort of human learning, and these have been studied in child\n\n20:36.700 --> 20:39.600\n development by psychologists.\n\n20:39.600 --> 20:45.120\n And what they tell us is that supervised learning is a very small part of it.\n\n20:45.120 --> 20:48.920\n There are many different aspects of learning.\n\n20:48.920 --> 20:57.220\n And what we would need to do is to develop models of all of these and then train our\n\n20:57.220 --> 21:02.480\n systems with that kind of a protocol.\n\n21:02.480 --> 21:07.800\n So new methods of learning, some of which might imitate the human brain, but you also\n\n21:07.800 --> 21:13.640\n in your talks have mentioned sort of the compute side of things, in terms of the difference\n\n21:13.640 --> 21:19.440\n in the human brain or referencing Moravec, Hans Moravec.\n\n21:19.440 --> 21:25.360\n So do you think there's something interesting, valuable to consider about the difference\n\n21:25.360 --> 21:32.000\n in the computational power of the human brain versus the computers of today in terms of\n\n21:32.000 --> 21:34.360\n instructions per second?\n\n21:34.360 --> 21:41.880\n Yes, so if we go back, so this is a point I've been making for 20 years now.\n\n21:41.880 --> 21:47.240\n And I think once upon a time, the way I used to argue this was that we just didn't have\n\n21:47.240 --> 21:49.160\n the computing power of the human brain.\n\n21:49.160 --> 21:53.480\n Our computers were not quite there.\n\n21:53.480 --> 22:03.200\n And I mean, there is a well known trade off, which we know that neurons are slow compared\n\n22:03.200 --> 22:09.720\n to transistors, but we have a lot of them and they have a very high connectivity.\n\n22:09.720 --> 22:18.240\n Whereas in silicon, you have much faster devices, transistors switch at the order of nanoseconds,\n\n22:18.240 --> 22:21.780\n but the connectivity is usually smaller.\n\n22:21.780 --> 22:27.640\n At this point in time, I mean, we are now talking about 2020, we do have, if you consider\n\n22:27.640 --> 22:31.840\n the latest GPUs and so on, amazing computing power.\n\n22:31.840 --> 22:39.200\n And if we look back at Hans Moravec type of calculations, which he did in the 1990s, we\n\n22:39.200 --> 22:44.800\n may be there today in terms of computing power comparable to the brain, but it's not in the\n\n22:44.800 --> 22:49.960\n of the same style, it's of a very different style.\n\n22:49.960 --> 22:55.560\n So I mean, for example, the style of computing that we have in our GPUs is far, far more\n\n22:55.560 --> 23:02.920\n power hungry than the style of computing that is there in the human brain or other biological\n\n23:02.920 --> 23:03.920\n entities.\n\n23:03.920 --> 23:04.920\n Yeah.\n\n23:04.920 --> 23:11.040\n And that the efficiency part is, we're going to have to solve that in order to build actual\n\n23:11.040 --> 23:15.160\n real world systems of large scale.\n\n23:15.160 --> 23:19.520\n Let me ask sort of the high level question, taking a step back.\n\n23:19.520 --> 23:24.400\n How would you articulate the general problem of computer vision?\n\n23:24.400 --> 23:25.560\n Does such a thing exist?\n\n23:25.560 --> 23:30.220\n So if you look at the computer vision conferences and the work that's been going on, it's often\n\n23:30.220 --> 23:36.280\n separated into different little segments, breaking the problem of vision apart into\n\n23:36.280 --> 23:44.640\n whether segmentation, 3D reconstruction, object detection, I don't know, image capturing,\n\n23:44.640 --> 23:45.640\n whatever.\n\n23:45.640 --> 23:46.840\n There's benchmarks for each.\n\n23:46.840 --> 23:52.340\n But if you were to sort of philosophically say, what is the big problem of computer vision?\n\n23:52.340 --> 23:54.640\n Does such a thing exist?\n\n23:54.640 --> 23:57.400\n Yes, but it's not in isolation.\n\n23:57.400 --> 24:09.840\n So for all intelligence tasks, I always go back to sort of biology or humans.\n\n24:09.840 --> 24:15.800\n And if we think about vision or perception in that setting, we realize that perception\n\n24:15.800 --> 24:18.480\n is always to guide action.\n\n24:18.480 --> 24:25.040\n Action for a biological system does not give any benefits unless it is coupled with action.\n\n24:25.040 --> 24:30.920\n So we can go back and think about the first multicellular animals, which arose in the\n\n24:30.920 --> 24:35.040\n Cambrian era, you know, 500 million years ago.\n\n24:35.040 --> 24:40.840\n And these animals could move and they could see in some way.\n\n24:40.840 --> 24:43.600\n And the two activities helped each other.\n\n24:43.600 --> 24:47.720\n Because how does movement help?\n\n24:47.720 --> 24:52.240\n Movement helps that because you can get food in different places.\n\n24:52.240 --> 24:54.420\n But you need to know where to go.\n\n24:54.420 --> 25:00.580\n And that's really about perception or seeing, I mean, vision is perhaps the single most\n\n25:00.580 --> 25:02.760\n perception sense.\n\n25:02.760 --> 25:06.040\n But all the others are equally are also important.\n\n25:06.040 --> 25:10.160\n So perception and action kind of go together.\n\n25:10.160 --> 25:17.700\n So earlier, it was in these very simple feedback loops, which were about finding food or avoid\n\n25:17.700 --> 25:24.360\n avoiding becoming food if there's a predator running, trying to, you know, eat you up,\n\n25:24.360 --> 25:25.360\n and so forth.\n\n25:25.360 --> 25:30.160\n So we must, at the fundamental level, connect perception to action.\n\n25:30.160 --> 25:37.400\n Then as we evolved, perception became more and more sophisticated because it served many\n\n25:37.400 --> 25:39.800\n more purposes.\n\n25:39.800 --> 25:46.520\n And so today we have what seems like a fairly general purpose capability, which can look\n\n25:46.520 --> 25:53.520\n at the external world and build a model of the external world inside the head.\n\n25:53.520 --> 25:55.040\n We do have that capability.\n\n25:55.040 --> 25:56.960\n That model is not perfect.\n\n25:56.960 --> 26:01.440\n And psychologists have great fun in pointing out the ways in which the model in your head\n\n26:01.440 --> 26:05.240\n is not a perfect model of the external world.\n\n26:05.240 --> 26:11.460\n They create various illusions to show the ways in which it is imperfect.\n\n26:11.460 --> 26:17.840\n But it's amazing how far it has come from a very simple perception action loop that\n\n26:17.840 --> 26:23.840\n you exist in, you know, an animal 500 million years ago.\n\n26:23.840 --> 26:29.760\n Once we have this, these very sophisticated visual systems, we can then impose a structure\n\n26:29.760 --> 26:30.760\n on them.\n\n26:30.760 --> 26:36.500\n It's we as scientists who are imposing that structure, where we have chosen to characterize\n\n26:36.500 --> 26:43.040\n this part of the system as this quote, module of object detection or quote, this module\n\n26:43.040 --> 26:45.120\n of 3D reconstruction.\n\n26:45.120 --> 26:55.400\n What's going on is really all of these processes are running simultaneously and they are running\n\n26:55.400 --> 27:01.000\n simultaneously because originally their purpose was in fact to help guide action.\n\n27:01.000 --> 27:08.080\n So as a guiding general statement of a problem, do you think we can say that the general problem\n\n27:08.080 --> 27:14.680\n of computer vision, you said in humans, it was tied to action.\n\n27:14.680 --> 27:20.880\n Do you think we should also say that ultimately the goal, the problem of computer vision is\n\n27:20.880 --> 27:27.080\n to sense the world in a way that helps you act in the world?\n\n27:27.080 --> 27:28.080\n Yes.\n\n27:28.080 --> 27:32.960\n I think that's the most fundamental, that's the most fundamental purpose.\n\n27:32.960 --> 27:37.320\n We have by now hyper evolved.\n\n27:37.320 --> 27:42.000\n So we have this visual system which can be used for other things.\n\n27:42.000 --> 27:46.940\n For example, judging the aesthetic value of a painting.\n\n27:46.940 --> 27:49.300\n And this is not guiding action.\n\n27:49.300 --> 27:54.240\n Maybe it's guiding action in terms of how much money you will put in your auction bid,\n\n27:54.240 --> 27:56.020\n but that's a bit stretched.\n\n27:56.020 --> 28:06.120\n But the basics are in fact in terms of action, but we evolved really this hyper, we have\n\n28:06.120 --> 28:08.160\n hyper evolved our visual system.\n\n28:08.160 --> 28:13.640\n Actually just to, sorry to interrupt, but perhaps it is fundamentally about action.\n\n28:13.640 --> 28:20.940\n You kind of jokingly said about spending, but perhaps the capitalistic drive that drives\n\n28:20.940 --> 28:25.600\n a lot of the development in this world is about the exchange of money and the fundamental\n\n28:25.600 --> 28:26.600\n action is money.\n\n28:26.600 --> 28:30.840\n If you watch Netflix, if you enjoy watching movies, you're using your perception system\n\n28:30.840 --> 28:36.780\n to interpret the movie, ultimately your enjoyment of that movie means you'll subscribe to Netflix.\n\n28:36.780 --> 28:44.680\n So the action is this extra layer that we've developed in modern society perhaps is fundamentally\n\n28:44.680 --> 28:47.760\n tied to the action of spending money.\n\n28:47.760 --> 28:54.200\n Well certainly with respect to interactions with firms.\n\n28:54.200 --> 29:01.960\n So in this homo economicus role, when you're interacting with firms, it does become that.\n\n29:01.960 --> 29:02.960\n What else is there?\n\n29:02.960 --> 29:07.800\n And that was a rhetorical question.\n\n29:07.800 --> 29:16.200\n So to linger on the division between the static and the dynamic, so much of the work in computer\n\n29:16.200 --> 29:20.560\n vision, so many of the breakthroughs that you've been a part of have been in the static\n\n29:20.560 --> 29:24.560\n world and looking at static images.\n\n29:24.560 --> 29:29.000\n And then you've also worked on starting, but it's a much smaller degree, the community\n\n29:29.000 --> 29:32.880\n is looking at dynamic, at video, at dynamic scenes.\n\n29:32.880 --> 29:38.840\n And then there is robotic vision, which is dynamic, but also where you actually have\n\n29:38.840 --> 29:43.620\n a robot in the physical world interacting based on that vision.\n\n29:43.620 --> 29:49.840\n Which problem is harder?\n\n29:49.840 --> 29:53.960\n The trivial first answer is, well, of course one image is harder.\n\n29:53.960 --> 30:03.400\n But if you look at a deeper question there, are we, what's the term, cutting ourselves\n\n30:03.400 --> 30:08.200\n at the knees or like making the problem harder by focusing on images?\n\n30:08.200 --> 30:09.200\n That's a fair question.\n\n30:09.200 --> 30:20.800\n I think sometimes we can simplify a problem so much that we essentially lose part of the\n\n30:20.800 --> 30:24.640\n juice that could enable us to solve the problem.\n\n30:24.640 --> 30:29.600\n And one could reasonably argue that to some extent this happens when we go from video\n\n30:29.600 --> 30:31.400\n to single images.\n\n30:31.400 --> 30:39.920\n Now historically you have to consider the limits imposed by the computation capabilities\n\n30:39.920 --> 30:41.040\n we had.\n\n30:41.040 --> 30:50.780\n So many of the choices made in the computer vision community through the 70s, 80s, 90s\n\n30:50.780 --> 30:59.720\n can be understood as choices which were forced upon us by the fact that we just didn't have\n\n30:59.720 --> 31:01.760\n enough access to enough compute.\n\n31:01.760 --> 31:04.360\n Not enough memory, not enough hardware.\n\n31:04.360 --> 31:05.360\n Exactly.\n\n31:05.360 --> 31:08.240\n Not enough compute, not enough storage.\n\n31:08.240 --> 31:09.480\n So think of these choices.\n\n31:09.480 --> 31:14.280\n So one of the choices is focusing on single images rather than video.\n\n31:14.280 --> 31:15.280\n Okay.\n\n31:15.280 --> 31:16.760\n Clear question.\n\n31:16.760 --> 31:19.400\n Storage and compute.\n\n31:19.400 --> 31:24.960\n We had to focus on, we used to detect edges and throw away the image.\n\n31:24.960 --> 31:25.960\n Right?\n\n31:25.960 --> 31:31.120\n So we would have an image which I say 256 by 256 pixels and instead of keeping around\n\n31:31.120 --> 31:37.360\n the grayscale value, what we did was we detected edges, find the places where the brightness\n\n31:37.360 --> 31:42.040\n changes a lot and then throw away the rest.\n\n31:42.040 --> 31:47.640\n So this was a major compression device and the hope was that this makes it that you can\n\n31:47.640 --> 31:53.480\n still work with it and the logic was humans can interpret a line drawing.\n\n31:53.480 --> 31:58.240\n And yes, and this will save us computation.\n\n31:58.240 --> 32:00.920\n So many of the choices were dictated by that.\n\n32:00.920 --> 32:07.240\n I think today we are no longer detecting edges, right?\n\n32:07.240 --> 32:10.840\n We process images with ConvNets because we don't need to.\n\n32:10.840 --> 32:14.040\n We don't have those computer restrictions anymore.\n\n32:14.040 --> 32:19.880\n Now video is still understudied because video compute is still quite challenging if you\n\n32:19.880 --> 32:22.320\n are a university researcher.\n\n32:22.320 --> 32:29.080\n I think video computing is not so challenging if you are at Google or Facebook or Amazon.\n\n32:29.080 --> 32:30.080\n Still super challenging.\n\n32:30.080 --> 32:35.480\n I just spoke with the VP of engineering at Google, head of the YouTube search and discovery\n\n32:35.480 --> 32:38.480\n and they still struggle doing stuff on video.\n\n32:38.480 --> 32:44.360\n It's very difficult except using techniques that are essentially the techniques you used\n\n32:44.360 --> 32:45.500\n in the 90s.\n\n32:45.500 --> 32:48.680\n Some very basic computer vision techniques.\n\n32:48.680 --> 32:51.540\n No, that's when you want to do things at scale.\n\n32:51.540 --> 32:56.920\n So if you want to operate at the scale of all the content of YouTube, it's very challenging\n\n32:56.920 --> 32:59.440\n and there are similar issues with Facebook.\n\n32:59.440 --> 33:05.840\n But as a researcher, you have more opportunities.\n\n33:05.840 --> 33:11.240\n You can train large networks with relatively large video data sets.\n\n33:11.240 --> 33:17.160\n So I think that this is part of the reason why we have so emphasized static images.\n\n33:17.160 --> 33:22.800\n I think that this is changing and over the next few years, I see a lot more progress\n\n33:22.800 --> 33:25.240\n happening in video.\n\n33:25.240 --> 33:32.560\n So I have this generic statement that to me, video recognition feels like 10 years behind\n\n33:32.560 --> 33:37.840\n object recognition and you can quantify that because you can take some of the challenging\n\n33:37.840 --> 33:45.280\n video data sets and their performance on action classification is like say 30%, which is kind\n\n33:45.280 --> 33:51.840\n of what we used to have around 2009 in object detection.\n\n33:51.840 --> 33:58.160\n It's like about 10 years behind and whether it'll take 10 years to catch up is a different\n\n33:58.160 --> 33:59.160\n question.\n\n33:59.160 --> 34:01.360\n Hopefully, it will take less than that.\n\n34:01.360 --> 34:08.600\n Let me ask a similar question I've already asked, but once again, so for dynamic scenes,\n\n34:08.600 --> 34:17.280\n do you think some kind of injection of knowledge bases and reasoning is required to help improve\n\n34:17.280 --> 34:20.400\n like action recognition?\n\n34:20.400 --> 34:28.800\n Like if we saw the general action recognition problem, what do you think the solution would\n\n34:28.800 --> 34:31.120\n look like as another way to put it?\n\n34:31.120 --> 34:39.720\n So I completely agree that knowledge is called for and that knowledge can be quite sophisticated.\n\n34:39.720 --> 34:44.960\n So the way I would say it is that perception blends into cognition and cognition brings\n\n34:44.960 --> 34:54.040\n in issues of memory and this notion of a schema from psychology, which is, let me use the\n\n34:54.040 --> 34:58.780\n classic example, which is you go to a restaurant, right?\n\n34:58.780 --> 35:03.580\n Now there are things that happen in a certain order, you walk in, somebody takes you to\n\n35:03.580 --> 35:13.240\n a table, waiter comes, gives you a menu, takes the order, food arrives, eventually bill arrives,\n\n35:13.240 --> 35:15.160\n et cetera, et cetera.\n\n35:15.160 --> 35:19.840\n This is a classic example of AI from the 1970s.\n\n35:19.840 --> 35:26.080\n It was called, there was the term frames and scripts and schemas, these are all quite similar\n\n35:26.080 --> 35:27.080\n ideas.\n\n35:27.080 --> 35:34.280\n Okay, and in the 70s, the way the AI of the time dealt with it was by hand coding this.\n\n35:34.280 --> 35:40.440\n So they hand coded in this notion of a script and the various stages and the actors and\n\n35:40.440 --> 35:45.440\n so on and so forth, and use that to interpret, for example, language.\n\n35:45.440 --> 35:52.840\n I mean, if there's a description of a story involving some people eating at a restaurant,\n\n35:52.840 --> 35:58.440\n there are all these inferences you can make because you know what happens typically at\n\n35:58.440 --> 36:00.240\n a restaurant.\n\n36:00.240 --> 36:06.120\n So I think this kind of knowledge is absolutely essential.\n\n36:06.120 --> 36:12.320\n So I think that when we are going to do long form video understanding, we are going to\n\n36:12.320 --> 36:13.400\n need to do this.\n\n36:13.400 --> 36:19.360\n I think the kinds of technology that we have right now with 3D convolutions over a couple\n\n36:19.360 --> 36:26.080\n of seconds of clip or video, it's very much tailored towards short term video understanding,\n\n36:26.080 --> 36:28.440\n not that long term understanding.\n\n36:28.440 --> 36:35.760\n Long term understanding requires this notion of schemas that I talked about, perhaps some\n\n36:35.760 --> 36:43.120\n notions of goals, intentionality, functionality, and so on and so forth.\n\n36:43.120 --> 36:46.040\n Now, how will we bring that in?\n\n36:46.040 --> 36:51.760\n So we could either revert back to the 70s and say, OK, I'm going to hand code in a script\n\n36:51.760 --> 36:56.280\n or we might try to learn it.\n\n36:56.280 --> 37:03.560\n So I tend to believe that we have to find learning ways of doing this because I think\n\n37:03.560 --> 37:06.880\n learning ways land up being more robust.\n\n37:06.880 --> 37:12.440\n And there must be a learning version of the story because children acquire a lot of this\n\n37:12.440 --> 37:16.640\n knowledge by sort of just observation.\n\n37:16.640 --> 37:24.320\n So at no moment in a child's life does it's possible, but I think it's not so typical\n\n37:24.320 --> 37:29.560\n that somebody that a mother coaches a child through all the stages of what happens in\n\n37:29.560 --> 37:30.560\n a restaurant.\n\n37:30.560 --> 37:36.480\n They just go as a family, they go to the restaurant, they eat, come back, and the child goes through\n\n37:36.480 --> 37:41.560\n ten such experiences and the child has got a schema of what happens when you go to a\n\n37:41.560 --> 37:42.720\n restaurant.\n\n37:42.720 --> 37:48.040\n So we somehow need to provide that capability to our systems.\n\n37:48.040 --> 37:53.880\n You mentioned the following line from the end of the Alan Turing paper, Computing Machinery\n\n37:53.880 --> 37:59.680\n and Intelligence, that many people, like you said, many people know and very few have read\n\n37:59.680 --> 38:03.960\n where he proposes the Turing test.\n\n38:03.960 --> 38:06.960\n This is how you know because it's towards the end of the paper.\n\n38:06.960 --> 38:10.940\n Instead of trying to produce a program to simulate the adult mind, why not rather try\n\n38:10.940 --> 38:14.440\n to produce one which simulates the child's?\n\n38:14.440 --> 38:17.280\n So that's a really interesting point.\n\n38:17.280 --> 38:24.520\n If I think about the benchmarks we have before us, the tests of our computer vision systems,\n\n38:24.520 --> 38:28.340\n they're often kind of trying to get to the adult.\n\n38:28.340 --> 38:31.160\n So what kind of benchmarks should we have?\n\n38:31.160 --> 38:37.400\n What kind of tests for computer vision do you think we should have that mimic the child's\n\n38:37.400 --> 38:38.400\n in computer vision?\n\n38:38.400 --> 38:42.880\n I think we should have those and we don't have those today.\n\n38:42.880 --> 38:50.240\n And I think the part of the challenge is that we should really be collecting data of the\n\n38:50.240 --> 38:55.180\n type that the child experiences.\n\n38:55.180 --> 38:59.400\n So that gets into issues of privacy and so on and so forth.\n\n38:59.400 --> 39:05.080\n But there are attempts in this direction to sort of try to collect the kind of data that\n\n39:05.080 --> 39:08.600\n a child encounters growing up.\n\n39:08.600 --> 39:11.200\n So what's the child's linguistic environment?\n\n39:11.200 --> 39:13.580\n What's the child's visual environment?\n\n39:13.580 --> 39:20.800\n So if we could collect that kind of data and then develop learning schemes based on that\n\n39:20.800 --> 39:25.160\n data, that would be one way to do it.\n\n39:25.160 --> 39:28.880\n I think that's a very promising direction myself.\n\n39:28.880 --> 39:33.920\n There might be people who would argue that we could just short circuit this in some way\n\n39:33.920 --> 39:44.440\n and sometimes we have imitated, we have had success by not imitating nature in detail.\n\n39:44.440 --> 39:47.520\n So the usual example is airplanes, right?\n\n39:47.520 --> 39:51.940\n We don't build flapping wings.\n\n39:51.940 --> 39:57.160\n So yes, that's one of the points of debate.\n\n39:57.160 --> 40:05.120\n In my mind, I would bet on this learning like a child approach.\n\n40:05.120 --> 40:11.400\n So one of the fundamental aspects of learning like a child is the interactivity.\n\n40:11.400 --> 40:14.200\n So the child gets to play with the data set it's learning from.\n\n40:14.200 --> 40:15.200\n Yes.\n\n40:15.200 --> 40:16.200\n So it gets to select.\n\n40:16.200 --> 40:19.600\n I mean, you can call that active learning.\n\n40:19.600 --> 40:23.660\n In the machine learning world, you can call it a lot of terms.\n\n40:23.660 --> 40:27.600\n What are your thoughts about this whole space of being able to play with the data set or\n\n40:27.600 --> 40:29.320\n select what you're learning?\n\n40:29.320 --> 40:30.320\n Yeah.\n\n40:30.320 --> 40:38.720\n So I think that I believe in that and I think that we could achieve it in two ways and I\n\n40:38.720 --> 40:40.800\n think we should use both.\n\n40:40.800 --> 40:45.560\n So one is actually real robotics, right?\n\n40:45.560 --> 40:52.880\n So real physical embodiments of agents who are interacting with the world and they have\n\n40:52.880 --> 40:59.440\n a physical body with dynamics and mass and moment of inertia and friction and all the\n\n40:59.440 --> 41:08.400\n rest and you learn your body, the robot learns its body by doing a series of actions.\n\n41:08.400 --> 41:11.640\n The second is that simulation environments.\n\n41:11.640 --> 41:17.000\n So I think simulation environments are getting much, much better.\n\n41:17.000 --> 41:24.880\n In my life in Facebook AI research, our group has worked on something called Habitat, which\n\n41:24.880 --> 41:34.560\n is a simulation environment, which is a visually photorealistic environment of places like\n\n41:34.560 --> 41:39.680\n houses or interiors of various urban spaces and so forth.\n\n41:39.680 --> 41:45.000\n And as you move, you get a picture, which is a pretty accurate picture.\n\n41:45.000 --> 41:53.880\n So now you can imagine that subsequent generations of these simulators will be accurate, not\n\n41:53.880 --> 42:01.600\n just visually, but with respect to forces and masses and haptic interactions and so\n\n42:01.600 --> 42:03.560\n on.\n\n42:03.560 --> 42:07.520\n And then we have that environment to play with.\n\n42:07.520 --> 42:16.280\n I think, let me state one reason why I think being able to act in the world is important.\n\n42:16.280 --> 42:23.000\n I think that this is one way to break the correlation versus causation barrier.\n\n42:23.000 --> 42:27.160\n So this is something which is of a great deal of interest these days.\n\n42:27.160 --> 42:34.660\n I mean, people like Judea Pearl have talked a lot about that we are neglecting causality\n\n42:34.660 --> 42:42.740\n and he describes the entire set of successes of deep learning as just curve fitting, right?\n\n42:42.740 --> 42:45.240\n But I don't quite agree about it.\n\n42:45.240 --> 42:46.240\n He's a troublemaker.\n\n42:46.240 --> 42:47.240\n He is.\n\n42:47.240 --> 42:54.520\n But causality is important, but causality is not like a single silver bullet.\n\n42:54.520 --> 42:56.160\n It's not like one single principle.\n\n42:56.160 --> 42:58.660\n There are many different aspects here.\n\n42:58.660 --> 43:05.120\n And one of the ways in which, one of our most reliable ways of establishing causal links\n\n43:05.120 --> 43:11.600\n and this is the way, for example, the medical community does this is randomized control\n\n43:11.600 --> 43:12.840\n trials.\n\n43:12.840 --> 43:18.440\n So you have, you pick some situation and now in some situation you perform an action and\n\n43:18.440 --> 43:22.600\n for certain others you don't, right?\n\n43:22.600 --> 43:23.800\n So you have a controlled experiment.\n\n43:23.800 --> 43:28.880\n Well, the child is in fact performing controlled experiments all the time, right?\n\n43:28.880 --> 43:29.880\n Right.\n\n43:29.880 --> 43:30.880\n Okay.\n\n43:30.880 --> 43:31.880\n Small scale.\n\n43:31.880 --> 43:32.880\n In a small scale.\n\n43:32.880 --> 43:41.240\n But that is a way that the child gets to build and refine its causal models of the world.\n\n43:41.240 --> 43:47.000\n And my colleague Alison Gopnik has, together with a couple of authors, coauthors, has this\n\n43:47.000 --> 43:50.820\n book called The Scientist in the Crib, referring to the children.\n\n43:50.820 --> 43:57.720\n So I like, the part that I like about that is the scientist wants to do, wants to build\n\n43:57.720 --> 44:01.820\n causal models and the scientist does control experiments.\n\n44:01.820 --> 44:03.800\n And I think the child is doing that.\n\n44:03.800 --> 44:10.240\n So to enable that, we will need to have these active experiments.\n\n44:10.240 --> 44:14.640\n And I think this could be done, some in the real world and some in simulation.\n\n44:14.640 --> 44:16.840\n So you have hope for simulation.\n\n44:16.840 --> 44:18.120\n I have hope for simulation.\n\n44:18.120 --> 44:22.960\n That's an exciting possibility if we can get to not just photorealistic, but what's that\n\n44:22.960 --> 44:27.720\n called life realistic simulation.\n\n44:27.720 --> 44:35.800\n So you don't see any fundamental blocks to why we can't eventually simulate the principles\n\n44:35.800 --> 44:39.440\n of what it means to exist in the world as a physical scientist.\n\n44:39.440 --> 44:43.960\n I don't see any fundamental problems that, I mean, and look, the computer graphics community\n\n44:43.960 --> 44:45.440\n has come a long way.\n\n44:45.440 --> 44:50.600\n So in the early days, back going back to the eighties and nineties, they were focusing\n\n44:50.600 --> 44:52.760\n on visual realism, right?\n\n44:52.760 --> 44:58.080\n And then they could do the easy stuff, but they couldn't do stuff like hair or fur and\n\n44:58.080 --> 44:59.080\n so on.\n\n44:59.080 --> 45:01.280\n Okay, well, they managed to do that.\n\n45:01.280 --> 45:04.440\n Then they couldn't do physical actions, right?\n\n45:04.440 --> 45:09.120\n Like there's a bowl of glass and it falls down and it shatters, but then they could\n\n45:09.120 --> 45:13.920\n start to do pretty realistic models of that and so on and so forth.\n\n45:13.920 --> 45:19.920\n So the graphics people have shown that they can do this forward direction, not just for\n\n45:19.920 --> 45:23.880\n optical interactions, but also for physical interactions.\n\n45:23.880 --> 45:30.000\n So I think, of course, some of that is very compute intensive, but I think by and by we\n\n45:30.000 --> 45:35.860\n will find ways of making our models ever more realistic.\n\n45:35.860 --> 45:40.600\n You break vision apart into, in one of your presentations, early vision, static scene\n\n45:40.600 --> 45:44.320\n understanding, dynamic scene understanding, and raise a few interesting questions.\n\n45:44.320 --> 45:50.360\n I thought I could just throw some at you to see if you want to talk about them.\n\n45:50.360 --> 45:58.360\n So early vision, so it's, what is it that you said, sensation, perception and cognition.\n\n45:58.360 --> 46:00.720\n So is this a sensation?\n\n46:00.720 --> 46:01.720\n Yes.\n\n46:01.720 --> 46:05.720\n What can we learn from image statistics that we don't already know?\n\n46:05.720 --> 46:15.560\n So at the lowest level, what can we make from just the statistics, the basics, or the variations\n\n46:15.560 --> 46:18.480\n in the rock pixels, the textures and so on?\n\n46:18.480 --> 46:19.480\n Yeah.\n\n46:19.480 --> 46:28.960\n So what we seem to have learned is that there's a lot of redundancy in these images and as\n\n46:28.960 --> 46:35.000\n a result, we are able to do a lot of compression and this compression is very important in\n\n46:35.000 --> 46:36.960\n biological settings, right?\n\n46:36.960 --> 46:42.560\n So you might have 10 to the 8 photoreceptors and only 10 to the 6 fibers in the optic nerve.\n\n46:42.560 --> 46:46.880\n So you have to do this compression by a factor of 100 is to 1.\n\n46:46.880 --> 46:54.760\n And so there are analogs of that which are happening in our neural net, artificial neural\n\n46:54.760 --> 46:55.760\n network.\n\n46:55.760 --> 46:56.760\n That's the early layers.\n\n46:56.760 --> 47:01.520\n So you think there's a lot of compression that can be done in the beginning.\n\n47:01.520 --> 47:02.520\n Just the statistics.\n\n47:02.520 --> 47:03.520\n Yeah.\n\n47:03.520 --> 47:05.640\n So how successful is image compression?\n\n47:05.640 --> 47:06.640\n How much?\n\n47:06.640 --> 47:14.160\n Well, I mean, the way to think about it is just how successful is image compression,\n\n47:14.160 --> 47:15.160\n right?\n\n47:15.160 --> 47:23.160\n And that's been done with older technologies, but it can be done with, there are several\n\n47:23.160 --> 47:29.160\n companies which are trying to use sort of these more advanced neural network type techniques\n\n47:29.160 --> 47:34.360\n for compression, both for static images as well as for video.\n\n47:34.360 --> 47:41.880\n One of my former students has a company which is trying to do stuff like this.\n\n47:41.880 --> 47:47.480\n And I think that they are showing quite interesting results.\n\n47:47.480 --> 47:52.560\n And I think that that's all the success of, that's really about image statistics and\n\n47:52.560 --> 47:53.560\n video statistics.\n\n47:53.560 --> 47:59.120\n But that's still not doing compression of the kind, when I see a picture of a cat, all\n\n47:59.120 --> 48:02.480\n I have to say is it's a cat, that's another semantic kind of compression.\n\n48:02.480 --> 48:03.480\n Yeah.\n\n48:03.480 --> 48:04.800\n So this is at the lower level, right?\n\n48:04.800 --> 48:10.280\n So we are, as I said, yeah, that's focusing on low level statistics.\n\n48:10.280 --> 48:17.880\n So to linger on that for a little bit, you mentioned how far can bottom up image segmentation\n\n48:17.880 --> 48:18.880\n go.\n\n48:18.880 --> 48:24.680\n You know, what you mentioned that the central question for scene understanding is the interplay\n\n48:24.680 --> 48:26.880\n of bottom up and top down information.\n\n48:26.880 --> 48:29.980\n Maybe this is a good time to elaborate on that.\n\n48:29.980 --> 48:37.400\n Maybe define what is bottom up, what is top down in the context of computer vision.\n\n48:37.400 --> 48:38.400\n Right.\n\n48:38.400 --> 48:45.160\n So today what we have are very interesting systems because they work completely bottom\n\n48:45.160 --> 48:46.160\n up.\n\n48:46.160 --> 48:47.920\n What does bottom up mean, sorry?\n\n48:47.920 --> 48:52.160\n So bottom up means, in this case means a feed forward neural network.\n\n48:52.160 --> 48:57.020\n So starting from the raw pixels, yeah, they start from the raw pixels and they end up\n\n48:57.020 --> 49:00.600\n with some, something like cat or not a cat, right?\n\n49:00.600 --> 49:04.600\n So our systems are running totally feed forward.\n\n49:04.600 --> 49:07.560\n They're trained in a very top down way.\n\n49:07.560 --> 49:11.560\n So they're trained by saying, okay, this is a cat, there's a cat, there's a dog, there's\n\n49:11.560 --> 49:14.440\n a zebra, et cetera.\n\n49:14.440 --> 49:18.560\n And I'm not happy with either of these choices fully.\n\n49:18.560 --> 49:24.960\n We have gone into, because we have completely separated these processes, right?\n\n49:24.960 --> 49:34.160\n So there's a, so I would like the process, so what do we know compared to biology?\n\n49:34.160 --> 49:42.500\n So in biology, what we know is that the processes in at test time, at runtime, those processes\n\n49:42.500 --> 49:46.340\n are not purely feed forward, but they involve feedback.\n\n49:46.340 --> 49:50.080\n So and they involve much shallower neural networks.\n\n49:50.080 --> 49:55.880\n So the kinds of neural networks we are using in computer vision, say a ResNet 50 has 50\n\n49:55.880 --> 49:56.880\n layers.\n\n49:56.880 --> 50:02.800\n Well in the brain, in the visual cortex going from the retina to IT, maybe we have like\n\n50:02.800 --> 50:04.240\n seven, right?\n\n50:04.240 --> 50:08.080\n So they're far shallower, but we have the possibility of feedback.\n\n50:08.080 --> 50:11.000\n So there are backward connections.\n\n50:11.000 --> 50:18.240\n And this might enable us to deal with the more ambiguous stimuli, for example.\n\n50:18.240 --> 50:26.480\n So the biological solution seems to involve feedback, the solution in artificial vision\n\n50:26.480 --> 50:30.760\n seems to be just feed forward, but with a much deeper network.\n\n50:30.760 --> 50:35.500\n And the two are functionally equivalent because if you have a feedback network, which just\n\n50:35.500 --> 50:40.440\n has like three rounds of feedback, you can just unroll it and make it three times the\n\n50:40.440 --> 50:44.520\n depth and create it in a totally feed forward way.\n\n50:44.520 --> 50:49.800\n So this is something which, I mean, we have written some papers on this theme, but I really\n\n50:49.800 --> 50:55.720\n feel that this should, this theme should be pursued further.\n\n50:55.720 --> 50:57.440\n Some kind of occurrence mechanism.\n\n50:57.440 --> 50:58.440\n Yeah.\n\n50:58.440 --> 50:59.440\n Okay.\n\n50:59.440 --> 51:07.440\n The other, so that's, so I want to have a little bit more top down in the, at test time.\n\n51:07.440 --> 51:08.440\n Okay.\n\n51:08.440 --> 51:13.800\n And then at training time, we make use of a lot of top down knowledge right now.\n\n51:13.800 --> 51:19.320\n So basically to learn to segment an object, we have to have all these examples of this\n\n51:19.320 --> 51:22.840\n is the boundary of a cat, and this is the boundary of a chair, and this is the boundary\n\n51:22.840 --> 51:24.640\n of a horse and so on.\n\n51:24.640 --> 51:27.960\n And this is too much top down knowledge.\n\n51:27.960 --> 51:30.400\n How do humans do this?\n\n51:30.400 --> 51:36.680\n We manage to, we manage with far less supervision and we do it in a sort of bottom up way because\n\n51:36.680 --> 51:44.540\n for example, we are looking at a video stream and the horse moves and that enables me to\n\n51:44.540 --> 51:47.360\n say that all these pixels are together.\n\n51:47.360 --> 51:53.180\n So the Gestalt psychologist used to call this the principle of common fate.\n\n51:53.180 --> 51:58.160\n So there was a bottom up process by which we were able to segment out these objects\n\n51:58.160 --> 52:04.540\n and we have totally focused on this top down training signal.\n\n52:04.540 --> 52:10.280\n So in my view, we have currently solved it in machine vision, this top down bottom up\n\n52:10.280 --> 52:17.680\n interaction, but I don't find the solution fully satisfactory and I would rather have\n\n52:17.680 --> 52:20.200\n a bit of both at both stages.\n\n52:20.200 --> 52:25.440\n For all computer vision problems, not just segmentation.\n\n52:25.440 --> 52:30.360\n And the question that you can ask is, so for me, I'm inspired a lot by human vision and\n\n52:30.360 --> 52:31.880\n I care about that.\n\n52:31.880 --> 52:35.560\n You could be just a hard boiled engineer and not give a damn.\n\n52:35.560 --> 52:41.960\n So to you, I would then argue that you would need far less training data if you could make\n\n52:41.960 --> 52:45.920\n my research agenda fruitful.\n\n52:45.920 --> 52:54.120\n Okay, so then maybe taking a step into segmentation, static scene understanding.\n\n52:54.120 --> 52:57.400\n What is the interaction between segmentation and recognition?\n\n52:57.400 --> 53:00.800\n You mentioned the movement of objects.\n\n53:00.800 --> 53:07.680\n So for people who don't know computer vision, segmentation is this weird activity that computer\n\n53:07.680 --> 53:15.220\n vision folks have all agreed is very important of drawing outlines around objects versus\n\n53:15.220 --> 53:21.920\n a bounding box and then classifying that object.\n\n53:21.920 --> 53:23.660\n What's the value of segmentation?\n\n53:23.660 --> 53:27.320\n What is it as a problem in computer vision?\n\n53:27.320 --> 53:31.720\n How is it fundamentally different from detection recognition and the other problems?\n\n53:31.720 --> 53:41.760\n Yeah, so I think, so segmentation enables us to say that some set of pixels are an object\n\n53:41.760 --> 53:47.120\n without necessarily even being able to name that object or knowing properties of that\n\n53:47.120 --> 53:48.120\n object.\n\n53:48.120 --> 53:55.000\n Oh, so you mean segmentation purely as the act of separating an object.\n\n53:55.000 --> 53:56.000\n From its background.\n\n53:56.000 --> 54:01.120\n It's a job that's united in some way from its background.\n\n54:01.120 --> 54:05.760\n Yeah, so entitification, if you will, making an entity out of it.\n\n54:05.760 --> 54:09.280\n Entitification, beautifully termed.\n\n54:09.280 --> 54:17.820\n So I think that we have that capability and that enables us to, as we are growing up,\n\n54:17.820 --> 54:23.760\n to acquire names of objects with very little supervision.\n\n54:23.760 --> 54:28.720\n So suppose the child, let's posit that the child has this ability to separate out objects\n\n54:28.720 --> 54:30.080\n in the world.\n\n54:30.080 --> 54:42.160\n Then when the mother says, pick up your bottle or the cat's behaving funny today, the word\n\n54:42.160 --> 54:47.740\n cat suggests some object and then the child sort of does the mapping, right?\n\n54:47.740 --> 54:55.000\n The mother doesn't have to teach specific object labels by pointing to them.\n\n54:55.000 --> 55:01.600\n Weak supervision works in the context that you have the ability to create objects.\n\n55:01.600 --> 55:07.800\n So I think that, so to me, that's a very fundamental capability.\n\n55:07.800 --> 55:13.180\n There are applications where this is very important, for example, medical diagnosis.\n\n55:13.180 --> 55:20.180\n So in medical diagnosis, you have some brain scan, I mean, this is some work that we did\n\n55:20.180 --> 55:26.960\n in my group where you have CT scans of people who have had traumatic brain injury and what\n\n55:26.960 --> 55:32.680\n the radiologist needs to do is to precisely delineate various places where there might\n\n55:32.680 --> 55:39.840\n be bleeds, for example, and there are clear needs like that.\n\n55:39.840 --> 55:46.360\n So there are certainly very practical applications of computer vision where segmentation is necessary,\n\n55:46.360 --> 55:54.980\n but philosophically segmentation enables the task of recognition to proceed with much weaker\n\n55:54.980 --> 55:58.000\n supervision than we require today.\n\n55:58.000 --> 56:03.960\n And you think of segmentation as this kind of task that takes on a visual scene and breaks\n\n56:03.960 --> 56:11.840\n it apart into interesting entities that might be useful for whatever the task is.\n\n56:11.840 --> 56:12.840\n Yeah.\n\n56:12.840 --> 56:14.760\n And it is not semantics free.\n\n56:14.760 --> 56:22.080\n So I think, I mean, it blends into, it involves perception and cognition.\n\n56:22.080 --> 56:28.440\n It is not, I think the mistake that we used to make in the early days of computer vision\n\n56:28.440 --> 56:32.520\n was to treat it as a purely bottom up perceptual task.\n\n56:32.520 --> 56:41.000\n It is not just that because we do revise our notion of segmentation with more experience,\n\n56:41.000 --> 56:42.000\n right?\n\n56:42.000 --> 56:47.320\n Because for example, there are objects which are nonrigid like animals or humans.\n\n56:47.320 --> 56:53.280\n And I think understanding that all the pixels of a human are one entity is actually quite\n\n56:53.280 --> 56:59.400\n a challenge because the parts of the human, they can move independently and the human\n\n56:59.400 --> 57:02.800\n wears clothes, so they might be differently colored.\n\n57:02.800 --> 57:05.600\n So it's all sort of a challenge.\n\n57:05.600 --> 57:12.280\n You mentioned the three R's of computer vision are recognition, reconstruction and reorganization.\n\n57:12.280 --> 57:15.760\n Can you describe these three R's and how they interact?\n\n57:15.760 --> 57:16.840\n Yeah.\n\n57:16.840 --> 57:24.240\n So recognition is the easiest one because that's what I think people generally think\n\n57:24.240 --> 57:30.520\n of as computer vision achieving these days, which is labels.\n\n57:30.520 --> 57:31.600\n So is this a cat?\n\n57:31.600 --> 57:32.640\n Is this a dog?\n\n57:32.640 --> 57:35.160\n Is this a chihuahua?\n\n57:35.160 --> 57:41.080\n I mean, you know, it could be very fine grained like, you know, specific breed of a dog or\n\n57:41.080 --> 57:47.080\n a specific species of bird, or it could be very abstract like animal.\n\n57:47.080 --> 57:51.880\n But given a part of an image or a whole image, say put a label on it.\n\n57:51.880 --> 57:52.880\n Yeah.\n\n57:52.880 --> 57:54.440\n That's recognition.\n\n57:54.440 --> 58:03.440\n Reconstruction is essentially, you can think of it as inverse graphics.\n\n58:03.440 --> 58:07.160\n I mean, that's one way to think about it.\n\n58:07.160 --> 58:14.760\n So graphics is you have some internal computer representation and you have a computer representation\n\n58:14.760 --> 58:17.440\n of some objects arranged in a scene.\n\n58:17.440 --> 58:22.080\n And what you do is you produce a picture, you produce the pixels corresponding to a\n\n58:22.080 --> 58:24.560\n rendering of that scene.\n\n58:24.560 --> 58:28.840\n So let's do the inverse of this.\n\n58:28.840 --> 58:38.480\n We are given an image and we try to, we say, oh, this image arises from some objects in\n\n58:38.480 --> 58:41.960\n a scene looked at with a camera from this viewpoint.\n\n58:41.960 --> 58:47.520\n And we might have more information about the objects like their shape, maybe their textures,\n\n58:47.520 --> 58:51.720\n maybe, you know, color, et cetera, et cetera.\n\n58:51.720 --> 58:53.320\n So that's the reconstruction problem.\n\n58:53.320 --> 59:00.200\n In a way, you are in your head creating a model of the external world.\n\n59:00.200 --> 59:01.200\n Right.\n\n59:01.200 --> 59:02.200\n Okay.\n\n59:02.200 --> 59:09.240\n Reorganization is to do with essentially finding these entities.\n\n59:09.240 --> 59:15.600\n So it's organization, the word organization implies structure.\n\n59:15.600 --> 59:22.760\n So that in perception, in psychology, we use the term perceptual organization.\n\n59:22.760 --> 59:30.980\n That the world is not just, an image is not just seen as, is not internally represented\n\n59:30.980 --> 59:34.800\n as just a collection of pixels, but we make these entities.\n\n59:34.800 --> 59:38.120\n We create these entities, objects, whatever you want to call it.\n\n59:38.120 --> 59:42.400\n And the relationship between the entities as well, or is it purely about the entities?\n\n59:42.400 --> 59:47.160\n It could be about the relationships, but mainly we focus on the fact that there are entities.\n\n59:47.160 --> 59:48.160\n Okay.\n\n59:48.160 --> 59:52.440\n So I'm trying to pinpoint what the organization means.\n\n59:52.440 --> 1:00:02.120\n So organization is that instead of like a uniform grid, we have this structure of objects.\n\n1:00:02.120 --> 1:00:05.400\n So the segmentation is the small part of that.\n\n1:00:05.400 --> 1:00:09.000\n So segmentation gets us going towards that.\n\n1:00:09.000 --> 1:00:10.120\n Yeah.\n\n1:00:10.120 --> 1:00:13.560\n And you kind of have this triangle where they all interact together.\n\n1:00:13.560 --> 1:00:14.560\n Yes.\n\n1:00:14.560 --> 1:00:23.560\n So how do you see that interaction in sort of reorganization is yes, finding the entities\n\n1:00:23.560 --> 1:00:25.200\n in the world.\n\n1:00:25.200 --> 1:00:32.720\n The recognition is labeling those entities and then reconstruction is what filling in\n\n1:00:32.720 --> 1:00:33.720\n the gaps.\n\n1:00:33.720 --> 1:00:43.280\n Well, for example, see, impute some 3D objects corresponding to each of these entities.\n\n1:00:43.280 --> 1:00:44.280\n That would be part of it.\n\n1:00:44.280 --> 1:00:48.400\n So adding more information that's not there in the raw data.\n\n1:00:48.400 --> 1:00:49.400\n Correct.\n\n1:00:49.400 --> 1:00:58.260\n I mean, I started pushing this kind of a view in the, around 2010 or something like that.\n\n1:00:58.260 --> 1:01:06.360\n Because at that time in computer vision, the distinction that people were just working\n\n1:01:06.360 --> 1:01:11.360\n on many different problems, but they treated each of them as a separate isolated problem\n\n1:01:11.360 --> 1:01:13.880\n with each with its own data set.\n\n1:01:13.880 --> 1:01:17.040\n And then you try to solve that and get good numbers on it.\n\n1:01:17.040 --> 1:01:23.840\n So I wasn't, I didn't like that approach because I wanted to see the connection between these.\n\n1:01:23.840 --> 1:01:30.640\n And if people divided up vision into, into various modules, the way they would do it\n\n1:01:30.640 --> 1:01:36.720\n is as low level, mid level and high level vision corresponding roughly to the psychologist's\n\n1:01:36.720 --> 1:01:40.180\n notion of sensation, perception and cognition.\n\n1:01:40.180 --> 1:01:45.160\n And I didn't, that didn't map to tasks that people cared about.\n\n1:01:45.160 --> 1:01:46.160\n Okay.\n\n1:01:46.160 --> 1:01:52.380\n So therefore I tried to promote this particular framework as a way of considering the problems\n\n1:01:52.380 --> 1:01:58.180\n that people in computer vision were actually working on and trying to be more explicit\n\n1:01:58.180 --> 1:02:02.440\n about the fact that they actually are connected to each other.\n\n1:02:02.440 --> 1:02:07.400\n And I was at that time just doing this on the basis of information flow.\n\n1:02:07.400 --> 1:02:17.180\n Now it turns out in the last five years or so in the post, the deep learning revolution\n\n1:02:17.180 --> 1:02:25.000\n that this, this architecture has turned out to be very conducive to that.\n\n1:02:25.000 --> 1:02:33.040\n Because basically in these neural networks, we are trying to build multiple representations.\n\n1:02:33.040 --> 1:02:37.280\n They can be multiple output heads sharing common representations.\n\n1:02:37.280 --> 1:02:46.240\n So in a certain sense today, given the reality of what solutions people have to this, I do\n\n1:02:46.240 --> 1:02:48.320\n not need to preach this anymore.\n\n1:02:48.320 --> 1:02:50.720\n It is, it is just there.\n\n1:02:50.720 --> 1:02:52.600\n It's part of the sedation space.\n\n1:02:52.600 --> 1:03:02.280\n So speaking of neural networks, how much of this problem of computer vision of reorganization\n\n1:03:02.280 --> 1:03:09.280\n recognition can be reconstruction?\n\n1:03:09.280 --> 1:03:12.800\n How much of it can be learned end to end, do you think?\n\n1:03:12.800 --> 1:03:17.160\n Sort of set it and forget it.\n\n1:03:17.160 --> 1:03:23.160\n Just plug and play, have a giant data set, multiple, perhaps multimodal, and then just\n\n1:03:23.160 --> 1:03:25.680\n learn the entirety of it.\n\n1:03:25.680 --> 1:03:31.440\n Well, so I think that currently what that end to end learning means nowadays is end\n\n1:03:31.440 --> 1:03:34.360\n to end supervised learning.\n\n1:03:34.360 --> 1:03:38.360\n And that I would argue is too narrow a view of the problem.\n\n1:03:38.360 --> 1:03:46.440\n I like this child development view, this lifelong learning view, one where there are certain\n\n1:03:46.440 --> 1:03:51.720\n capabilities that are built up and then there are certain capabilities which are built up\n\n1:03:51.720 --> 1:03:53.320\n on top of that.\n\n1:03:53.320 --> 1:03:58.700\n So that's what I believe in.\n\n1:03:58.700 --> 1:04:13.080\n So I think end to end learning in the supervised setting for a very precise task to me is kind\n\n1:04:13.080 --> 1:04:17.560\n of is sort of a limited view of the learning process.\n\n1:04:17.560 --> 1:04:18.660\n Got it.\n\n1:04:18.660 --> 1:04:25.500\n So if we think about beyond purely supervised, looking back to children, you mentioned six\n\n1:04:25.500 --> 1:04:33.400\n lessons that we can learn from children of be multimodal, be incremental, be physical,\n\n1:04:33.400 --> 1:04:36.520\n explore, be social, use language.\n\n1:04:36.520 --> 1:04:42.280\n Can you speak to these, perhaps picking one that you find most fundamental to our time\n\n1:04:42.280 --> 1:04:43.280\n today?\n\n1:04:43.280 --> 1:04:44.280\n Yeah.\n\n1:04:44.280 --> 1:04:50.120\n So I mean, I should say to give a due credit, this is from a paper by Smith and Gasser.\n\n1:04:50.120 --> 1:05:00.000\n And it reflects essentially, I would say common wisdom among child development people.\n\n1:05:00.000 --> 1:05:07.040\n It's just that this is not common wisdom among people in computer vision and AI and machine\n\n1:05:07.040 --> 1:05:08.040\n learning.\n\n1:05:08.040 --> 1:05:15.920\n So I view my role as trying to bridge the two worlds.\n\n1:05:15.920 --> 1:05:18.960\n So let's take an example of a multimodal.\n\n1:05:18.960 --> 1:05:20.160\n I like that.\n\n1:05:20.160 --> 1:05:28.840\n So multimodal, a canonical example is a child interacting with an object.\n\n1:05:28.840 --> 1:05:32.600\n So then the child holds a ball and plays with it.\n\n1:05:32.600 --> 1:05:35.720\n So at that point, it's getting a touch signal.\n\n1:05:35.720 --> 1:05:44.120\n So the touch signal is getting the notion of 3D shape, but it is sparse.\n\n1:05:44.120 --> 1:05:48.320\n And then the child is also seeing a visual signal.\n\n1:05:48.320 --> 1:05:52.640\n And these two, so imagine these are two in totally different spaces.\n\n1:05:52.640 --> 1:05:59.660\n So one is the space of receptors on the skin of the fingers and the thumb and the palm.\n\n1:05:59.660 --> 1:06:06.460\n And then these map onto these neuronal fibers are getting activated somewhere.\n\n1:06:06.460 --> 1:06:10.360\n These lead to some activation in somatosensory cortex.\n\n1:06:10.360 --> 1:06:15.800\n I mean, a similar thing will happen if we have a robot hand.\n\n1:06:15.800 --> 1:06:20.440\n And then we have the pixels corresponding to the visual view, but we know that they\n\n1:06:20.440 --> 1:06:24.440\n correspond to the same object.\n\n1:06:24.440 --> 1:06:28.920\n So that's a very, very strong cross calibration signal.\n\n1:06:28.920 --> 1:06:32.520\n And it is self supervisory, which is beautiful.\n\n1:06:32.520 --> 1:06:34.000\n There's nobody assigning a label.\n\n1:06:34.000 --> 1:06:37.880\n The mother doesn't have to come and assign a label.\n\n1:06:37.880 --> 1:06:42.760\n The child doesn't even have to know that this object is called a ball.\n\n1:06:42.760 --> 1:06:49.600\n That the child is learning something about the three dimensional world from this signal.\n\n1:06:49.600 --> 1:06:54.880\n I think tactile and visual, there is some work on, there is a lot of work currently\n\n1:06:54.880 --> 1:06:57.960\n on audio and visual.\n\n1:06:57.960 --> 1:07:02.600\n And audio visual, so there is some event that happens in the world and that event has a\n\n1:07:02.600 --> 1:07:07.200\n visual signature and it has a auditory signature.\n\n1:07:07.200 --> 1:07:12.020\n So there is this glass bowl on the table and it falls and breaks and I hear the smashing\n\n1:07:12.020 --> 1:07:14.200\n sound and I see the pieces of glass.\n\n1:07:14.200 --> 1:07:19.520\n Okay, I've built that connection between the two, right?\n\n1:07:19.520 --> 1:07:24.280\n We have people, I mean, this has become a hot topic in computer vision in the last couple\n\n1:07:24.280 --> 1:07:26.120\n of years.\n\n1:07:26.120 --> 1:07:32.560\n There are problems like separating out multiple speakers, right?\n\n1:07:32.560 --> 1:07:35.460\n Which was a classic problem in auditions.\n\n1:07:35.460 --> 1:07:40.680\n They call this the problem of source separation or the cocktail party effect and so on.\n\n1:07:40.680 --> 1:07:47.560\n But just try to do it visually when you also have, it becomes so much easier and so much\n\n1:07:47.560 --> 1:07:50.640\n more useful.\n\n1:07:50.640 --> 1:07:56.680\n So the multimodal, I mean, there's so much more signal with multimodal and you can use\n\n1:07:56.680 --> 1:08:00.240\n that for some kind of weak supervision as well.\n\n1:08:00.240 --> 1:08:03.220\n Yes, because they are occurring at the same time in time.\n\n1:08:03.220 --> 1:08:06.220\n So you have time which links the two, right?\n\n1:08:06.220 --> 1:08:10.840\n So at a certain moment, T1, you've got a certain signal in the auditory domain and a certain\n\n1:08:10.840 --> 1:08:14.520\n signal in the visual domain, but they must be causally related.\n\n1:08:14.520 --> 1:08:16.640\n Yeah, that's an exciting area.\n\n1:08:16.640 --> 1:08:17.640\n Not well studied yet.\n\n1:08:17.640 --> 1:08:25.540\n Yeah, I mean, we have a little bit of work at this, but so much more needs to be done.\n\n1:08:25.540 --> 1:08:28.220\n So this is a good example.\n\n1:08:28.220 --> 1:08:34.040\n Be physical, that's to do with like the one thing we talked about earlier that there's\n\n1:08:34.040 --> 1:08:36.560\n a embodied world.\n\n1:08:36.560 --> 1:08:39.440\n To mention language, use language.\n\n1:08:39.440 --> 1:08:44.160\n So Noam Chomsky believes that language may be at the core of cognition, at the core of\n\n1:08:44.160 --> 1:08:46.480\n everything in the human mind.\n\n1:08:46.480 --> 1:08:50.760\n What is the connection between language and vision to you?\n\n1:08:50.760 --> 1:08:51.920\n What's more fundamental?\n\n1:08:51.920 --> 1:08:53.440\n Are they neighbors?\n\n1:08:53.440 --> 1:08:58.000\n Is one the parent and the child, the chicken and the egg?\n\n1:08:58.000 --> 1:08:59.000\n Oh, it's very clear.\n\n1:08:59.000 --> 1:09:00.560\n It is vision, which is the parent.\n\n1:09:00.560 --> 1:09:07.680\n Which is the fundamental ability, okay.\n\n1:09:07.680 --> 1:09:11.640\n It comes before you think vision is more fundamental than language.\n\n1:09:11.640 --> 1:09:12.640\n Correct.\n\n1:09:12.640 --> 1:09:18.240\n And you can think of it either in phylogeny or in ontogeny.\n\n1:09:18.240 --> 1:09:22.320\n So phylogeny means if you look at evolutionary time, right?\n\n1:09:22.320 --> 1:09:27.160\n So we have vision that developed 500 million years ago, okay.\n\n1:09:27.160 --> 1:09:33.040\n Then something like when we get to maybe like five million years ago, you have the first\n\n1:09:33.040 --> 1:09:34.400\n bipedal primate.\n\n1:09:34.400 --> 1:09:38.920\n So when we started to walk, then the hands became free.\n\n1:09:38.920 --> 1:09:45.160\n And so then manipulation, the ability to manipulate objects and build tools and so on and so forth.\n\n1:09:45.160 --> 1:09:47.520\n So you said 500,000 years ago?\n\n1:09:47.520 --> 1:09:48.520\n No, sorry.\n\n1:09:48.520 --> 1:09:56.720\n The first multicellular animals, which you can say had some intelligence arose 500 million\n\n1:09:56.720 --> 1:09:57.720\n years ago.\n\n1:09:57.720 --> 1:09:58.720\n Million.\n\n1:09:58.720 --> 1:09:59.720\n Okay.\n\n1:09:59.720 --> 1:10:05.680\n And now let's fast forward to say the last seven million years, which is the development\n\n1:10:05.680 --> 1:10:10.560\n of the hominid line, right, where from the other primates, we have the branch which leads\n\n1:10:10.560 --> 1:10:12.840\n on to modern humans.\n\n1:10:12.840 --> 1:10:21.680\n Now there are many of these hominids, but the ones which, you know, people talk about\n\n1:10:21.680 --> 1:10:25.080\n Lucy because that's like a skeleton from three million years ago.\n\n1:10:25.080 --> 1:10:28.600\n And we know that Lucy walked, okay.\n\n1:10:28.600 --> 1:10:34.360\n So at this stage you have that the hand is free for manipulating objects and then the\n\n1:10:34.360 --> 1:10:43.520\n ability to manipulate objects, build tools and the brain size grew in this era.\n\n1:10:43.520 --> 1:10:46.140\n So okay, so now you have manipulation.\n\n1:10:46.140 --> 1:10:49.660\n Now we don't know exactly when language arose.\n\n1:10:49.660 --> 1:10:50.660\n But after that.\n\n1:10:50.660 --> 1:10:57.760\n Because no apes have, I mean, so I mean Chomsky is correct in that, that it is a uniquely\n\n1:10:57.760 --> 1:11:04.440\n human capability and we primates, other primates don't have that.\n\n1:11:04.440 --> 1:11:12.040\n But so it developed somewhere in this era, but it developed, I would, I mean, argue that\n\n1:11:12.040 --> 1:11:19.520\n it probably developed after we had this stage of humans, I mean, the human species already\n\n1:11:19.520 --> 1:11:25.440\n able to manipulate and hands free much bigger brain size.\n\n1:11:25.440 --> 1:11:31.720\n And for that, there's a lot of vision has already had, had to have developed.\n\n1:11:31.720 --> 1:11:35.800\n So the sensation and the perception may be some of the cognition.\n\n1:11:35.800 --> 1:11:36.800\n Yeah.\n\n1:11:36.800 --> 1:11:45.800\n So we, we, we, so those, so, so that vision, so the world, so there, so, so these ancestors\n\n1:11:45.800 --> 1:11:53.360\n of ours, you know, three, four million years ago, they had, they had special intelligence.\n\n1:11:53.360 --> 1:11:56.240\n So they knew that the world consists of objects.\n\n1:11:56.240 --> 1:11:59.720\n They knew that the objects were in certain relationships to each other.\n\n1:11:59.720 --> 1:12:05.280\n They had observed causal interactions among objects.\n\n1:12:05.280 --> 1:12:06.500\n They could move in space.\n\n1:12:06.500 --> 1:12:09.000\n So they had space and time and all of that.\n\n1:12:09.000 --> 1:12:13.120\n So language builds on that substrate.\n\n1:12:13.120 --> 1:12:19.800\n So language has a lot of, I mean, I mean, the none, all human languages have constructs\n\n1:12:19.800 --> 1:12:22.840\n which depend on a notion of space and time.\n\n1:12:22.840 --> 1:12:26.920\n Where did that notion of space and time come from?\n\n1:12:26.920 --> 1:12:30.960\n It had to come from perception and action in the world we live in.\n\n1:12:30.960 --> 1:12:31.960\n Yeah.\n\n1:12:31.960 --> 1:12:33.560\n Well, you've referred to the spatial intelligence.\n\n1:12:33.560 --> 1:12:34.560\n Yeah.\n\n1:12:34.560 --> 1:12:35.560\n Yeah.\n\n1:12:35.560 --> 1:12:42.960\n So to linger a little bit, we'll mention Turing and his mention of, we should learn from\n\n1:12:42.960 --> 1:12:43.960\n children.\n\n1:12:43.960 --> 1:12:49.360\n Nevertheless, language is the fundamental piece of the test of intelligence that Turing\n\n1:12:49.360 --> 1:12:50.360\n proposed.\n\n1:12:50.360 --> 1:12:51.360\n Yes.\n\n1:12:51.360 --> 1:12:53.840\n What do you think is a good test of intelligence?\n\n1:12:53.840 --> 1:12:56.480\n Are you, what would impress the heck out of you?\n\n1:12:56.480 --> 1:13:02.800\n Is it fundamentally natural language or is there something in vision?\n\n1:13:02.800 --> 1:13:10.160\n I think, I wouldn't, I don't think we should have created a single test of intelligence.\n\n1:13:10.160 --> 1:13:17.200\n So just like I don't believe in IQ as a single number, I think generally there can be many\n\n1:13:17.200 --> 1:13:21.920\n capabilities which are correlated perhaps.\n\n1:13:21.920 --> 1:13:28.920\n So I think that there will be, there will be accomplishments which are visual accomplishments,\n\n1:13:28.920 --> 1:13:36.000\n accomplishments which are accomplishments in manipulation or robotics, and then accomplishments\n\n1:13:36.000 --> 1:13:37.000\n in language.\n\n1:13:37.000 --> 1:13:40.400\n But I do believe that language will be the hardest nut to crack.\n\n1:13:40.400 --> 1:13:41.400\n Really?\n\n1:13:41.400 --> 1:13:42.400\n Yeah.\n\n1:13:42.400 --> 1:13:46.840\n So what's harder, to pass the spirit of the Turing test or like whatever formulation will\n\n1:13:46.840 --> 1:13:52.000\n make it natural language, convincingly a natural language, like somebody you would want to\n\n1:13:52.000 --> 1:13:59.340\n have a beer with, hang out and have a chat with, or the general natural scene understanding?\n\n1:13:59.340 --> 1:14:01.440\n You think language is the tougher problem?\n\n1:14:01.440 --> 1:14:09.080\n I think, I'm not a fan of the, I think, I think Turing test, that Turing as he proposed\n\n1:14:09.080 --> 1:14:13.840\n the test in 1950 was trying to solve a certain problem.\n\n1:14:13.840 --> 1:14:14.840\n Yeah, imitation.\n\n1:14:14.840 --> 1:14:15.840\n Yeah.\n\n1:14:15.840 --> 1:14:18.240\n And, and I think it made a lot of sense then.\n\n1:14:18.240 --> 1:14:26.720\n Where we are today, 70 years later, I think, I think we should not worry about that.\n\n1:14:26.720 --> 1:14:34.620\n I think the Turing test is no longer the right way to channel research in AI, because that,\n\n1:14:34.620 --> 1:14:39.720\n it takes us down this path of this chat bot, which can fool us for five minutes or whatever.\n\n1:14:39.720 --> 1:14:40.720\n Okay.\n\n1:14:40.720 --> 1:14:44.400\n I think I would rather have a list of 10 different tasks.\n\n1:14:44.400 --> 1:14:50.720\n I mean, I think there are tasks which, there are tasks in the manipulation domain, tasks\n\n1:14:50.720 --> 1:14:58.120\n in navigation, tasks in visual scene understanding, tasks in reading a story and answering questions\n\n1:14:58.120 --> 1:14:59.120\n based on that.\n\n1:14:59.120 --> 1:15:05.520\n I mean, so my favorite language understanding task would be, you know, reading a novel and\n\n1:15:05.520 --> 1:15:08.560\n being able to answer arbitrary questions from it.\n\n1:15:08.560 --> 1:15:09.560\n Okay.\n\n1:15:09.560 --> 1:15:10.560\n Right.\n\n1:15:10.560 --> 1:15:15.800\n I think that to me, and this is not an exhaustive list by any means.\n\n1:15:15.800 --> 1:15:21.120\n So I would, I think that that's what we, where we need to be going to.\n\n1:15:21.120 --> 1:15:26.120\n And each of these, on each of these axes, there's a fair amount of work to be done.\n\n1:15:26.120 --> 1:15:31.240\n So on the visual understanding side, in this intelligence Olympics that we've set up, what's\n\n1:15:31.240 --> 1:15:39.840\n a good test for one of many of visual scene understanding?\n\n1:15:39.840 --> 1:15:41.320\n Do you think such benchmarks exist?\n\n1:15:41.320 --> 1:15:42.320\n Sorry to interrupt.\n\n1:15:42.320 --> 1:15:43.680\n No, there aren't any.\n\n1:15:43.680 --> 1:15:50.920\n I think, I think essentially to me, a really good aid to the blind.\n\n1:15:50.920 --> 1:15:57.160\n So suppose there was a blind person and I needed to assist the blind person.\n\n1:15:57.160 --> 1:16:05.840\n So ultimately, like we said, vision that aids in the action in a survival in this world,\n\n1:16:05.840 --> 1:16:09.000\n maybe in the simulated world.\n\n1:16:09.000 --> 1:16:15.280\n Maybe easier to measure performance in a simulated world, what we are ultimately after is performance\n\n1:16:15.280 --> 1:16:17.680\n in the real world.\n\n1:16:17.680 --> 1:16:23.920\n So David Hilbert in 1900 proposed 23 open problems in mathematics, some of which are\n\n1:16:23.920 --> 1:16:29.400\n still unsolved, most important, famous of which is probably the Riemann hypothesis.\n\n1:16:29.400 --> 1:16:33.240\n You've thought about and presented about the Hilbert problems of computer vision.\n\n1:16:33.240 --> 1:16:38.960\n So let me ask, what do you today, I don't know when the last year you presented that\n\n1:16:38.960 --> 1:16:44.000\n in 2015, but versions of it, you're kind of the face and the spokesperson for computer\n\n1:16:44.000 --> 1:16:45.000\n vision.\n\n1:16:45.000 --> 1:16:51.840\n It's your job to state what the open problems are for the field.\n\n1:16:51.840 --> 1:16:56.560\n So what today are the Hilbert problems of computer vision, do you think?\n\n1:16:56.560 --> 1:17:05.760\n Let me pick one which I regard as clearly unsolved, which is what I would call long\n\n1:17:05.760 --> 1:17:08.280\n form video understanding.\n\n1:17:08.280 --> 1:17:20.840\n So we have a video clip and we want to understand the behavior in there in terms of agents,\n\n1:17:20.840 --> 1:17:30.600\n their goals, intentionality and make predictions about what might happen.\n\n1:17:30.600 --> 1:17:37.120\n So that kind of understanding which goes away from atomic visual action.\n\n1:17:37.120 --> 1:17:41.800\n So in the short range, the question is, are you sitting, are you standing, are you catching\n\n1:17:41.800 --> 1:17:44.080\n a ball?\n\n1:17:44.080 --> 1:17:50.400\n That we can do now, or even if we can't do it fully accurately, if we can do it at 50%,\n\n1:17:50.400 --> 1:17:54.000\n maybe next year we'll do it at 65% and so forth.\n\n1:17:54.000 --> 1:18:01.800\n But I think the long range video understanding, I don't think we can do today.\n\n1:18:01.800 --> 1:18:06.920\n And it blends into cognition, that's the reason why it's challenging.\n\n1:18:06.920 --> 1:18:11.280\n So you have to track, you have to understand the entities, you have to understand the entities,\n\n1:18:11.280 --> 1:18:16.960\n you have to track them and you have to have some kind of model of their behavior.\n\n1:18:16.960 --> 1:18:17.960\n Correct.\n\n1:18:17.960 --> 1:18:24.080\n And their behavior might be, these are agents, so they are not just like passive objects,\n\n1:18:24.080 --> 1:18:29.760\n but they're agents, so therefore they would exhibit goal directed behavior.\n\n1:18:29.760 --> 1:18:32.580\n Okay, so this is one area.\n\n1:18:32.580 --> 1:18:37.120\n Then I will talk about understanding the world in 3D.\n\n1:18:37.120 --> 1:18:43.020\n This may seem paradoxical because in a way we have been able to do 3D understanding even\n\n1:18:43.020 --> 1:18:45.840\n like 30 years ago, right?\n\n1:18:45.840 --> 1:18:51.600\n But I don't think we currently have the richness of 3D understanding in our computer vision\n\n1:18:51.600 --> 1:18:55.440\n system that we would like.\n\n1:18:55.440 --> 1:18:57.560\n So let me elaborate on that a bit.\n\n1:18:57.560 --> 1:19:03.340\n So currently we have two kinds of techniques which are not fully unified.\n\n1:19:03.340 --> 1:19:08.080\n So they are the kinds of techniques from multi view geometry that you have multiple pictures\n\n1:19:08.080 --> 1:19:14.660\n of a scene and you do a reconstruction using stereoscopic vision or structure from motion.\n\n1:19:14.660 --> 1:19:21.520\n But these techniques do not, they totally fail if you just have a single view because\n\n1:19:21.520 --> 1:19:25.680\n they are relying on this multiple view geometry.\n\n1:19:25.680 --> 1:19:30.240\n Okay, then we have some techniques that we have developed in the computer vision community\n\n1:19:30.240 --> 1:19:34.440\n which try to guess 3D from single views.\n\n1:19:34.440 --> 1:19:41.780\n And these techniques are based on supervised learning and they are based on having a training\n\n1:19:41.780 --> 1:19:46.020\n time 3D models of objects available.\n\n1:19:46.020 --> 1:19:50.080\n And this is completely unnatural supervision, right?\n\n1:19:50.080 --> 1:19:54.000\n That's not, CAD models are not injected into your brain.\n\n1:19:54.000 --> 1:19:56.120\n Okay, so what would I like?\n\n1:19:56.120 --> 1:20:06.360\n What I would like would be a kind of learning as you move around the world notion of 3D.\n\n1:20:06.360 --> 1:20:19.200\n So we have our succession of visual experiences and from those we, so as part of that I might\n\n1:20:19.200 --> 1:20:24.880\n see a chair from different viewpoints or a table from different viewpoints and so on.\n\n1:20:24.880 --> 1:20:31.320\n Now as part that enables me to build some internal representation.\n\n1:20:31.320 --> 1:20:37.260\n And then next time I just see a single photograph and it may not even be of that chair, it's\n\n1:20:37.260 --> 1:20:38.960\n of some other chair.\n\n1:20:38.960 --> 1:20:42.040\n And I have a guess of what it's 3D shape is like.\n\n1:20:42.040 --> 1:20:45.680\n So you're almost learning the CAD model, kind of.\n\n1:20:45.680 --> 1:20:46.680\n Yeah, implicitly.\n\n1:20:46.680 --> 1:20:47.680\n Implicitly.\n\n1:20:47.680 --> 1:20:52.600\n I mean, the CAD model need not be in the same form as used by computer graphics programs.\n\n1:20:52.600 --> 1:20:53.880\n Hidden in the representation.\n\n1:20:53.880 --> 1:20:58.240\n It's hidden in the representation, the ability to predict new views.\n\n1:20:58.240 --> 1:21:04.320\n And what I would see if I went to such and such position.\n\n1:21:04.320 --> 1:21:14.360\n By the way, on a small tangent on that, are you okay or comfortable with neural networks\n\n1:21:14.360 --> 1:21:19.200\n that do achieve visual understanding that do, for example, achieve this kind of 3D understanding\n\n1:21:19.200 --> 1:21:27.600\n and you don't know how they, you're not able to interest, you're not able to visualize\n\n1:21:27.600 --> 1:21:31.120\n or understand or interact with the representation.\n\n1:21:31.120 --> 1:21:34.960\n So the fact that they're not or may not be explainable.\n\n1:21:34.960 --> 1:21:38.400\n Yeah, I think that's fine.\n\n1:21:38.400 --> 1:21:44.540\n To me that is, so let me put some caveats on that.\n\n1:21:44.540 --> 1:21:46.460\n So it depends on the setting.\n\n1:21:46.460 --> 1:21:55.600\n So first of all, I think the humans are not explainable.\n\n1:21:55.600 --> 1:21:57.120\n So that's a really good point.\n\n1:21:57.120 --> 1:22:02.680\n So we, one human to another human is not fully explainable.\n\n1:22:02.680 --> 1:22:10.880\n I think there are settings where explainability matters and these might be, for example, questions\n\n1:22:10.880 --> 1:22:13.520\n on medical diagnosis.\n\n1:22:13.520 --> 1:22:19.400\n So I'm in a setting where maybe the doctor, maybe a computer program has made a certain\n\n1:22:19.400 --> 1:22:25.840\n diagnosis and then depending on the diagnosis, perhaps I should have treatment A or treatment\n\n1:22:25.840 --> 1:22:28.120\n B, right?\n\n1:22:28.120 --> 1:22:38.720\n So now is the computer program's diagnosis based on data, which was data collected off\n\n1:22:38.720 --> 1:22:45.500\n for American males who are in their 30s and 40s and maybe not so relevant to me.\n\n1:22:45.500 --> 1:22:48.560\n Maybe it is relevant, you know, et cetera, et cetera.\n\n1:22:48.560 --> 1:22:53.560\n I mean, in medical diagnosis, we have major issues to do with the reference class.\n\n1:22:53.560 --> 1:22:58.680\n So we may have acquired statistics from one group of people and applying it to a different\n\n1:22:58.680 --> 1:23:02.880\n group of people who may not share all the same characteristics.\n\n1:23:02.880 --> 1:23:07.600\n The data might have, there might be error bars in the prediction.\n\n1:23:07.600 --> 1:23:14.120\n So that prediction should really be taken with a huge grain of salt.\n\n1:23:14.120 --> 1:23:20.400\n But this has an impact on what treatments should be picked, right?\n\n1:23:20.400 --> 1:23:26.800\n So there are settings where I want to know more than just, this is the answer.\n\n1:23:26.800 --> 1:23:33.840\n But what I acknowledge is that, so in that sense, explainability and interpretability\n\n1:23:33.840 --> 1:23:34.840\n may matter.\n\n1:23:34.840 --> 1:23:40.840\n It's about giving error bounds and a better sense of the quality of the decision.\n\n1:23:40.840 --> 1:23:50.000\n Where I'm willing to sacrifice interpretability is that I believe that there can be systems\n\n1:23:50.000 --> 1:23:56.200\n which can be highly performant, but which are internally black boxes.\n\n1:23:56.200 --> 1:23:57.880\n And that seems to be where it's headed.\n\n1:23:57.880 --> 1:24:04.200\n Some of the best performing systems are essentially black boxes, fundamentally by their construction.\n\n1:24:04.200 --> 1:24:06.360\n You and I are black boxes to each other.\n\n1:24:06.360 --> 1:24:07.360\n Yeah.\n\n1:24:07.360 --> 1:24:13.960\n So the nice thing about the black boxes we are is, so we ourselves are black boxes, but\n\n1:24:13.960 --> 1:24:20.720\n we're also, those of us who are charming are able to convince others, like explain the\n\n1:24:20.720 --> 1:24:25.440\n black, what's going on inside the black box with narratives of stories.\n\n1:24:25.440 --> 1:24:31.480\n So in some sense, neural networks don't have to actually explain what's going on inside.\n\n1:24:31.480 --> 1:24:37.080\n They just have to come up with stories, real or fake that convince you that they know what's\n\n1:24:37.080 --> 1:24:38.560\n going on.\n\n1:24:38.560 --> 1:24:39.880\n And I'm sure we can do that.\n\n1:24:39.880 --> 1:24:45.080\n We can create those stories, neural networks can create those stories.\n\n1:24:45.080 --> 1:24:46.080\n Yeah.\n\n1:24:46.080 --> 1:24:50.040\n And the transformer will be involved.\n\n1:24:50.040 --> 1:24:56.520\n Do you think we will ever build a system of human level or superhuman level intelligence?\n\n1:24:56.520 --> 1:25:01.680\n We've kind of defined what it takes to try to approach that, but do you think that's\n\n1:25:01.680 --> 1:25:02.680\n within our reach?\n\n1:25:02.680 --> 1:25:07.480\n The thing that we thought we could do, what Turing thought actually we could do by year\n\n1:25:07.480 --> 1:25:09.480\n 2000, right?\n\n1:25:09.480 --> 1:25:11.200\n What do you think we'll ever be able to do?\n\n1:25:11.200 --> 1:25:12.880\n So I think there are two answers here.\n\n1:25:12.880 --> 1:25:18.240\n One question, one answer is in principle, can we do this at some time?\n\n1:25:18.240 --> 1:25:20.560\n And my answer is yes.\n\n1:25:20.560 --> 1:25:23.640\n The second answer is a pragmatic one.\n\n1:25:23.640 --> 1:25:27.840\n Do you think we will be able to do it in the next 20 years or whatever?\n\n1:25:27.840 --> 1:25:30.400\n And to that my answer is no.\n\n1:25:30.400 --> 1:25:34.680\n So of course that's a wild guess.\n\n1:25:34.680 --> 1:25:40.800\n I think that, you know, Donald Rumsfeld is not a favorite person of mine, but one of\n\n1:25:40.800 --> 1:25:48.280\n his lines was very good, which is about known unknowns and unknown unknowns.\n\n1:25:48.280 --> 1:25:55.040\n So in the business we are in, there are known unknowns and we have unknown unknowns.\n\n1:25:55.040 --> 1:26:04.800\n So I think with respect to a lot of what's the case in vision and robotics, I feel like\n\n1:26:04.800 --> 1:26:06.960\n we have known unknowns.\n\n1:26:06.960 --> 1:26:13.520\n So I have a sense of where we need to go and what the problems that need to be solved are.\n\n1:26:13.520 --> 1:26:21.320\n I feel with respect to natural language, understanding and high level cognition, it's not just known\n\n1:26:21.320 --> 1:26:24.200\n unknowns, but also unknown unknowns.\n\n1:26:24.200 --> 1:26:30.920\n So it is very difficult to put any kind of a timeframe to that.\n\n1:26:30.920 --> 1:26:36.360\n Do you think some of the unknown unknowns might be positive in that they'll surprise\n\n1:26:36.360 --> 1:26:38.720\n us and make the job much easier?\n\n1:26:38.720 --> 1:26:40.120\n So fundamental breakthroughs?\n\n1:26:40.120 --> 1:26:45.680\n I think that is possible because certainly I have been very positively surprised by how\n\n1:26:45.680 --> 1:26:53.880\n effective these deep learning systems have been because I certainly would not have believed\n\n1:26:53.880 --> 1:26:57.640\n that in 2010.\n\n1:26:57.640 --> 1:27:06.160\n I think what we knew from the mathematical theory was that convex optimization works.\n\n1:27:06.160 --> 1:27:11.200\n When there's a single global optima, then these gradient descent techniques would work.\n\n1:27:11.200 --> 1:27:16.240\n Now these are nonlinear systems with non convex systems.\n\n1:27:16.240 --> 1:27:18.680\n Huge number of variables, so over parametrized.\n\n1:27:18.680 --> 1:27:26.680\n And the people who used to play with them a lot, the ones who are totally immersed in\n\n1:27:26.680 --> 1:27:33.920\n the lore and the black magic, they knew that they worked well, even though they were...\n\n1:27:33.920 --> 1:27:34.920\n Really?\n\n1:27:34.920 --> 1:27:35.920\n I thought like everybody...\n\n1:27:35.920 --> 1:27:43.200\n No, the claim that I hear from my friends like Yann LeCun and so forth is that they\n\n1:27:43.200 --> 1:27:45.960\n feel that they were comfortable with them.\n\n1:27:45.960 --> 1:27:50.920\n But the community as a whole was certainly not.\n\n1:27:50.920 --> 1:27:59.820\n And I think to me that was the surprise that they actually worked robustly for a wide range\n\n1:27:59.820 --> 1:28:04.960\n of problems from a wide range of initializations and so on.\n\n1:28:04.960 --> 1:28:13.720\n And so that was certainly more rapid progress than we expected.\n\n1:28:13.720 --> 1:28:19.520\n But then there are certainly lots of times, in fact, most of the history of AI is when\n\n1:28:19.520 --> 1:28:24.060\n we have made less progress at a slower rate than we expected.\n\n1:28:24.060 --> 1:28:27.360\n So we just keep going.\n\n1:28:27.360 --> 1:28:39.600\n I think what I regard as really unwarranted are these fears of AGI in 10 years and 20\n\n1:28:39.600 --> 1:28:44.880\n years and that kind of stuff, because that's based on completely unrealistic models of\n\n1:28:44.880 --> 1:28:48.800\n how rapidly we will make progress in this field.\n\n1:28:48.800 --> 1:28:54.680\n So I agree with you, but I've also gotten the chance to interact with very smart people\n\n1:28:54.680 --> 1:28:57.840\n who really worry about existential threats of AI.\n\n1:28:57.840 --> 1:29:04.080\n And I, as an open minded person, am sort of taking it in.\n\n1:29:04.080 --> 1:29:12.920\n Do you think if AI systems in some way, the unknown unknowns, not super intelligent AI,\n\n1:29:12.920 --> 1:29:18.080\n but in ways we don't quite understand the nature of super intelligence, will have a\n\n1:29:18.080 --> 1:29:20.280\n detrimental effect on society?\n\n1:29:20.280 --> 1:29:25.920\n Do you think this is something we should be worried about or we need to first allow the\n\n1:29:25.920 --> 1:29:29.800\n unknown unknowns to become known unknowns?\n\n1:29:29.800 --> 1:29:32.960\n I think we need to be worried about AI today.\n\n1:29:32.960 --> 1:29:38.240\n I think that it is not just a worry we need to have when we get that AGI.\n\n1:29:38.240 --> 1:29:43.360\n I think that AI is being used in many systems today.\n\n1:29:43.360 --> 1:29:49.800\n And there might be settings, for example, when it causes biases or decisions which could\n\n1:29:49.800 --> 1:29:50.800\n be harmful.\n\n1:29:50.800 --> 1:29:55.400\n I mean, decisions which could be unfair to some people or it could be a self driving\n\n1:29:55.400 --> 1:29:57.740\n cars which kills a pedestrian.\n\n1:29:57.740 --> 1:30:02.000\n So AI systems are being deployed today, right?\n\n1:30:02.000 --> 1:30:05.440\n And they're being deployed in many different settings, maybe in medical diagnosis, maybe\n\n1:30:05.440 --> 1:30:10.000\n in a self driving car, maybe in selecting applicants for an interview.\n\n1:30:10.000 --> 1:30:18.320\n So I would argue that when these systems make mistakes, there are consequences.\n\n1:30:18.320 --> 1:30:22.760\n And we are in a certain sense responsible for those consequences.\n\n1:30:22.760 --> 1:30:27.040\n So I would argue that this is a continuous effort.\n\n1:30:27.040 --> 1:30:32.440\n It is we and this is something that in a way is not so surprising.\n\n1:30:32.440 --> 1:30:40.000\n It's about all engineering and scientific progress which great power comes great responsibility.\n\n1:30:40.000 --> 1:30:44.300\n So as these systems are deployed, we have to worry about them and it's a continuous\n\n1:30:44.300 --> 1:30:45.300\n problem.\n\n1:30:45.300 --> 1:30:51.680\n I don't think of it as something which will suddenly happen on some day in 2079 for which\n\n1:30:51.680 --> 1:30:54.880\n I need to design some clever trick.\n\n1:30:54.880 --> 1:31:00.800\n I'm saying that these problems exist today and we need to be continuously on the lookout\n\n1:31:00.800 --> 1:31:06.840\n for worrying about safety, biases, risks, right?\n\n1:31:06.840 --> 1:31:11.600\n I mean, the self driving car kills a pedestrian and they have, right?\n\n1:31:11.600 --> 1:31:16.080\n I mean, this Uber incident in Arizona, right?\n\n1:31:16.080 --> 1:31:17.760\n It has happened, right?\n\n1:31:17.760 --> 1:31:18.760\n This is not about AGI.\n\n1:31:18.760 --> 1:31:23.880\n In fact, it's about a very dumb intelligence which is still killing people.\n\n1:31:23.880 --> 1:31:28.480\n The worry people have with AGI is the scale.\n\n1:31:28.480 --> 1:31:34.840\n But I think you're 100% right is like the thing that worries me about AI today and it's\n\n1:31:34.840 --> 1:31:39.320\n happening in a huge scale is recommender systems, recommendation systems.\n\n1:31:39.320 --> 1:31:47.600\n So if you look at Twitter or Facebook or YouTube, they're controlling the ideas that we have\n\n1:31:47.600 --> 1:31:50.560\n access to, the news and so on.\n\n1:31:50.560 --> 1:31:55.480\n And that's a fundamental machine learning algorithm behind each of these recommendations.\n\n1:31:55.480 --> 1:32:00.840\n And they, I mean, my life would not be the same without these sources of information.\n\n1:32:00.840 --> 1:32:07.180\n I'm a totally new human being and the ideas that I know are very much because of the internet,\n\n1:32:07.180 --> 1:32:09.680\n because of the algorithm that recommend those ideas.\n\n1:32:09.680 --> 1:32:16.880\n And so as they get smarter and smarter, I mean, that is the AGI is that's the algorithm\n\n1:32:16.880 --> 1:32:23.480\n that's recommending the next YouTube video you should watch has control of millions of\n\n1:32:23.480 --> 1:32:30.160\n billions of people that that algorithm is already super intelligent and has complete\n\n1:32:30.160 --> 1:32:35.160\n control of the population, not a complete, but very strong control.\n\n1:32:35.160 --> 1:32:39.920\n For now we can turn off YouTube, we can just go have a normal life outside of that.\n\n1:32:39.920 --> 1:32:46.760\n But the more and more that gets into our life, it's that algorithm we start depending on\n\n1:32:46.760 --> 1:32:49.040\n it in the different companies that are working on the algorithm.\n\n1:32:49.040 --> 1:32:53.000\n So I think it's, you're right, it's already there.\n\n1:32:53.000 --> 1:32:59.760\n And YouTube in particular is using computer vision, doing their hardest to try to understand\n\n1:32:59.760 --> 1:33:05.680\n the content of videos so they could be able to connect videos with the people who would\n\n1:33:05.680 --> 1:33:08.080\n benefit from those videos the most.\n\n1:33:08.080 --> 1:33:12.860\n And so that development could go in a bunch of different directions, some of which might\n\n1:33:12.860 --> 1:33:14.820\n be harmful.\n\n1:33:14.820 --> 1:33:19.720\n So yeah, you're right, the threats of AI are here already and we should be thinking about\n\n1:33:19.720 --> 1:33:20.720\n them.\n\n1:33:20.720 --> 1:33:29.200\n On a philosophical notion, if you could, personal perhaps, if you could relive a moment in\n\n1:33:29.200 --> 1:33:36.280\n your life outside of family because it made you truly happy or it was a profound moment\n\n1:33:36.280 --> 1:33:44.160\n that impacted the direction of your life, what moment would you go to?\n\n1:33:44.160 --> 1:33:49.240\n I don't think of single moments, but I look over the long haul.\n\n1:33:49.240 --> 1:33:58.840\n I feel that I've been very lucky because I feel that, I think that in scientific research,\n\n1:33:58.840 --> 1:34:03.720\n a lot of it is about being at the right place at the right time.\n\n1:34:03.720 --> 1:34:10.680\n And you can work on problems at a time when they're just too premature.\n\n1:34:10.680 --> 1:34:18.440\n You butt your head against them and nothing happens because the prerequisites for success\n\n1:34:18.440 --> 1:34:19.840\n are not there.\n\n1:34:19.840 --> 1:34:25.500\n And then there are times when you are in a field which is all pretty mature and you can\n\n1:34:25.500 --> 1:34:30.020\n only solve curlicues upon curlicues.\n\n1:34:30.020 --> 1:34:36.920\n I've been lucky to have been in this field which for 34 years, well actually 34 years\n\n1:34:36.920 --> 1:34:44.600\n as a professor at Berkeley, so longer than that, which when I started in it was just\n\n1:34:44.600 --> 1:34:53.600\n like some little crazy, absolutely useless field which couldn't really do anything to\n\n1:34:53.600 --> 1:35:01.200\n a time when it's really, really solving a lot of practical problems, has offered a lot\n\n1:35:01.200 --> 1:35:08.580\n of tools for scientific research because computer vision is impactful for images in biology\n\n1:35:08.580 --> 1:35:12.160\n or astronomy and so on and so forth.\n\n1:35:12.160 --> 1:35:18.180\n And we have, so we have made great scientific progress which has had real practical impact\n\n1:35:18.180 --> 1:35:19.400\n in the world.\n\n1:35:19.400 --> 1:35:28.360\n And I feel lucky that I got in at a time when the field was very young and at a time when\n\n1:35:28.360 --> 1:35:34.120\n it is, it's now mature but not fully mature.\n\n1:35:34.120 --> 1:35:35.600\n It's mature but not done.\n\n1:35:35.600 --> 1:35:39.040\n I mean, it's really still in a productive phase.\n\n1:35:39.040 --> 1:35:45.680\n Yeah, I think people 500 years from now would laugh at you calling this field mature.\n\n1:35:45.680 --> 1:35:46.680\n That is very possible.\n\n1:35:46.680 --> 1:35:47.680\n Yeah.\n\n1:35:47.680 --> 1:35:53.860\n So, but you're also, lest I forget to mention, you've also mentored some of the biggest names\n\n1:35:53.860 --> 1:35:59.200\n of computer vision, computer science and AI today.\n\n1:35:59.200 --> 1:36:04.560\n So many questions I could ask, but really is what, what is it, how did you do it?\n\n1:36:04.560 --> 1:36:06.760\n What does it take to be a good mentor?\n\n1:36:06.760 --> 1:36:09.200\n What does it take to be a good guide?\n\n1:36:09.200 --> 1:36:17.640\n Yeah, I think what I feel, I've been lucky to have had very, very smart and hardworking\n\n1:36:17.640 --> 1:36:18.920\n and creative students.\n\n1:36:18.920 --> 1:36:25.600\n I think some part of the credit just belongs to being at Berkeley.\n\n1:36:25.600 --> 1:36:32.880\n Those of us who are at top universities are blessed because we have very, very smart and\n\n1:36:32.880 --> 1:36:37.040\n capable students coming on, knocking on our door.\n\n1:36:37.040 --> 1:36:40.440\n So I have to be humble enough to acknowledge that.\n\n1:36:40.440 --> 1:36:41.960\n But what have I added?\n\n1:36:41.960 --> 1:36:44.160\n I think I have added something.\n\n1:36:44.160 --> 1:36:52.360\n What I have added is, I think what I've always tried to teach them is a sense of picking\n\n1:36:52.360 --> 1:36:54.760\n the right problems.\n\n1:36:54.760 --> 1:37:04.240\n So I think that in science, in the short run, success is always based on technical competence.\n\n1:37:04.240 --> 1:37:09.080\n You're, you know, you're quick with math or you are whatever.\n\n1:37:09.080 --> 1:37:15.640\n I mean, there's certain technical capabilities which make for short range progress.\n\n1:37:15.640 --> 1:37:21.280\n Long range progress is really determined by asking the right questions and focusing on\n\n1:37:21.280 --> 1:37:23.280\n the right problems.\n\n1:37:23.280 --> 1:37:31.320\n And I feel that what I've been able to bring to the table in terms of advising these students\n\n1:37:31.320 --> 1:37:38.760\n is some sense of taste of what are good problems, what are problems that are worth attacking\n\n1:37:38.760 --> 1:37:41.680\n now as opposed to waiting 10 years.\n\n1:37:41.680 --> 1:37:42.720\n What's a good problem?\n\n1:37:42.720 --> 1:37:47.320\n If you could summarize, is that possible to even summarize, like what's your sense of\n\n1:37:47.320 --> 1:37:48.320\n a good problem?\n\n1:37:48.320 --> 1:37:55.400\n I think, I think I have a sense of what is a good problem, which is there is a British\n\n1:37:55.400 --> 1:38:02.920\n scientist, in fact, he won a Nobel Prize, Peter Medover, who has a book on this.\n\n1:38:02.920 --> 1:38:08.440\n And basically he calls it, research is the art of the soluble.\n\n1:38:08.440 --> 1:38:18.440\n So we need to sort of find problems which are not yet solved, but which are approachable.\n\n1:38:18.440 --> 1:38:25.080\n And he sort of refers to this sense that there is this problem which isn't quite solved yet,\n\n1:38:25.080 --> 1:38:26.760\n but it has a soft underbelly.\n\n1:38:26.760 --> 1:38:32.800\n There is some place where you can, you know, spear the beast.\n\n1:38:32.800 --> 1:38:39.160\n And having that intuition that this problem is ripe is a good thing because otherwise\n\n1:38:39.160 --> 1:38:42.400\n you can just beat your head and not make progress.\n\n1:38:42.400 --> 1:38:45.840\n So I think that is important.\n\n1:38:45.840 --> 1:38:52.080\n So if I have that and if I can convey that to students, it's not just that they do great\n\n1:38:52.080 --> 1:38:56.320\n research while they're working with me, but that they continue to do great research.\n\n1:38:56.320 --> 1:39:01.200\n So in a sense, I'm proud of my students and their achievements and their great research\n\n1:39:01.200 --> 1:39:05.760\n even 20 years after they've ceased being my student.\n\n1:39:05.760 --> 1:39:11.440\n So it's in part developing, helping them develop that sense that a problem is not yet solved,\n\n1:39:11.440 --> 1:39:12.440\n but it's solvable.\n\n1:39:12.440 --> 1:39:13.440\n Correct.\n\n1:39:13.440 --> 1:39:21.600\n The other thing which I have, which I think I bring to the table, is a certain intellectual\n\n1:39:21.600 --> 1:39:22.600\n breadth.\n\n1:39:22.600 --> 1:39:29.320\n I've spent a fair amount of time studying psychology, neuroscience, relevant areas of\n\n1:39:29.320 --> 1:39:31.320\n applied math and so forth.\n\n1:39:31.320 --> 1:39:40.480\n So I can probably help them see some connections to disparate things, which they might not\n\n1:39:40.480 --> 1:39:42.960\n have otherwise.\n\n1:39:42.960 --> 1:39:50.440\n So the smart students coming into Berkeley can be very deep, they can think very deeply,\n\n1:39:50.440 --> 1:39:58.520\n meaning very hard down one particular path, but where I could help them is the shallow\n\n1:39:58.520 --> 1:40:08.560\n breadth, but they would have the narrow depth, but that's of some value.\n\n1:40:08.560 --> 1:40:14.760\n Well, it was beautifully refreshing just to hear you naturally jump to psychology back\n\n1:40:14.760 --> 1:40:18.520\n to computer science in this conversation back and forth.\n\n1:40:18.520 --> 1:40:23.680\n That's actually a rare quality and I think it's certainly for students empowering to\n\n1:40:23.680 --> 1:40:25.600\n think about problems in a new way.\n\n1:40:25.600 --> 1:40:29.440\n So for that and for many other reasons, I really enjoyed this conversation.\n\n1:40:29.440 --> 1:40:30.440\n Thank you so much.\n\n1:40:30.440 --> 1:40:31.440\n It was a huge honor.\n\n1:40:31.440 --> 1:40:32.440\n Thanks for talking to me.\n\n1:40:32.440 --> 1:40:34.320\n It's been my pleasure.\n\n1:40:34.320 --> 1:40:39.840\n Thanks for listening to this conversation with Jitendra Malik and thank you to our sponsors,\n\n1:40:39.840 --> 1:40:43.120\n BetterHelp and ExpressVPN.\n\n1:40:43.120 --> 1:40:49.480\n Please consider supporting this podcast by going to betterhelp.com slash Lex and signing\n\n1:40:49.480 --> 1:40:52.940\n up at expressvpn.com slash LexPod.\n\n1:40:52.940 --> 1:40:55.440\n Click the links, buy the stuff.\n\n1:40:55.440 --> 1:41:00.720\n That's how they know I sent you and it really is the best way to support this podcast and\n\n1:41:00.720 --> 1:41:02.360\n the journey I'm on.\n\n1:41:02.360 --> 1:41:07.520\n If you enjoy this thing, subscribe on YouTube, review it with five stars on Apple podcast,\n\n1:41:07.520 --> 1:41:12.280\n support it on Patreon or connect with me on Twitter at Lex Friedman.\n\n1:41:12.280 --> 1:41:13.360\n Don't ask me how to spell that.\n\n1:41:13.360 --> 1:41:15.720\n I don't remember it myself.\n\n1:41:15.720 --> 1:41:22.120\n And now let me leave you with some words from Prince Mishkin in The Idiot by Dostoevsky.\n\n1:41:22.120 --> 1:41:24.760\n Beauty will save the world.\n\n1:41:24.760 --> 1:41:27.520\n Thank you for listening and hope to see you next time.\n\n"
}