{
  "title": "Ray Kurzweil: Singularity, Superintelligence, and Immortality | Lex Fridman Podcast #321",
  "id": "ykY69lSpDdo",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:02.800\n By the time he gets to 2045,\n\n00:02.800 --> 00:05.320\n we'll be able to multiply our intelligence\n\n00:05.320 --> 00:07.680\n many millions fold.\n\n00:07.680 --> 00:10.840\n And it's just very hard to imagine what that will be like.\n\n00:13.560 --> 00:16.840\n The following is a conversation with Ray Kurzweil,\n\n00:16.840 --> 00:19.480\n author, inventor, and futurist,\n\n00:19.480 --> 00:22.280\n who has an optimistic view of our future\n\n00:22.280 --> 00:24.320\n as a human civilization,\n\n00:24.320 --> 00:27.280\n predicting that exponentially improving technologies\n\n00:27.280 --> 00:29.880\n will take us to a point of a singularity\n\n00:29.880 --> 00:33.480\n beyond which superintelligent artificial intelligence\n\n00:33.480 --> 00:38.380\n will transform our world in nearly unimaginable ways.\n\n00:38.380 --> 00:41.280\n 18 years ago, in the book Singularity is Near,\n\n00:41.280 --> 00:44.000\n he predicted that the onset of the singularity\n\n00:44.000 --> 00:47.360\n will happen in the year 2045.\n\n00:47.360 --> 00:50.800\n He still holds to this prediction and estimate.\n\n00:50.800 --> 00:53.440\n In fact, he's working on a new book on this topic\n\n00:53.440 --> 00:55.640\n that will hopefully be out next year.\n\n00:56.520 --> 00:58.320\n This is the Lex Friedman podcast.\n\n00:58.320 --> 01:00.360\n To support it, please check out our sponsors\n\n01:00.360 --> 01:01.640\n in the description.\n\n01:01.640 --> 01:05.360\n And now, dear friends, here's Ray Kurzweil.\n\n01:06.360 --> 01:10.960\n In your 2005 book titled The Singularity is Near,\n\n01:10.960 --> 01:15.400\n you predicted that the singularity will happen in 2045.\n\n01:15.400 --> 01:18.460\n So now, 18 years later, do you still estimate\n\n01:18.460 --> 01:22.480\n that the singularity will happen on 2045?\n\n01:22.480 --> 01:24.960\n And maybe first, what is the singularity,\n\n01:24.960 --> 01:27.760\n the technological singularity, and when will it happen?\n\n01:27.760 --> 01:31.640\n Singularity is where computers really change our view\n\n01:31.640 --> 01:35.840\n of what's important and change who we are.\n\n01:35.840 --> 01:39.560\n But we're getting close to some salient things\n\n01:39.560 --> 01:42.800\n that will change who we are.\n\n01:42.800 --> 01:45.680\n A key thing is 2029,\n\n01:45.680 --> 01:49.060\n when computers will pass the Turing test.\n\n01:50.120 --> 01:51.520\n And there's also some controversy\n\n01:51.520 --> 01:53.680\n whether the Turing test is valid.\n\n01:53.680 --> 01:55.080\n I believe it is.\n\n01:55.080 --> 01:57.920\n Most people do believe that,\n\n01:57.920 --> 01:59.680\n but there's some controversy about that.\n\n01:59.680 --> 02:04.680\n But Stanford got very alarmed at my prediction about 2029.\n\n02:06.520 --> 02:10.520\n I made this in 1999 in my book.\n\n02:10.520 --> 02:12.120\n The Age of Spiritual Machines.\n\n02:12.120 --> 02:12.960\n Right.\n\n02:12.960 --> 02:15.600\n And then you repeated the prediction in 2005.\n\n02:15.600 --> 02:16.600\n In 2005.\n\n02:16.600 --> 02:17.520\n Yeah.\n\n02:17.520 --> 02:19.480\n So they held an international conference,\n\n02:19.480 --> 02:20.800\n you might have been aware of it,\n\n02:20.800 --> 02:25.800\n of AI experts in 1999 to assess this view.\n\n02:26.620 --> 02:29.580\n So people gave different predictions,\n\n02:29.580 --> 02:30.840\n and they took a poll.\n\n02:30.840 --> 02:34.240\n It was really the first time that AI experts worldwide\n\n02:34.240 --> 02:36.620\n were polled on this prediction.\n\n02:37.720 --> 02:39.980\n And the average poll was 100 years.\n\n02:41.420 --> 02:44.320\n 20% believed it would never happen.\n\n02:44.320 --> 02:48.120\n And that was the view in 1999.\n\n02:48.120 --> 02:50.640\n 80% believed it would happen,\n\n02:50.640 --> 02:53.200\n but not within their lifetimes.\n\n02:53.200 --> 02:55.840\n There's been so many advances in AI\n\n02:56.920 --> 03:01.840\n that the poll of AI experts has come down over the years.\n\n03:01.840 --> 03:05.440\n So a year ago, something called Meticulous,\n\n03:05.440 --> 03:07.080\n which you may be aware of,\n\n03:07.080 --> 03:11.560\n assesses different types of experts on the future.\n\n03:11.560 --> 03:16.440\n They again assessed what AI experts then felt.\n\n03:16.440 --> 03:18.940\n And they were saying 2042.\n\n03:18.940 --> 03:20.440\n For the Turing test.\n\n03:20.440 --> 03:22.440\n For the Turing test.\n\n03:22.440 --> 03:23.560\n So it's coming down.\n\n03:23.560 --> 03:26.320\n And I was still saying 2029.\n\n03:26.320 --> 03:30.200\n A few weeks ago, they again did another poll,\n\n03:30.200 --> 03:31.660\n and it was 2030.\n\n03:32.960 --> 03:37.920\n So AI experts now basically agree with me.\n\n03:37.920 --> 03:41.220\n I haven't changed at all, I've stayed with 2029.\n\n03:42.820 --> 03:44.520\n And AI experts now agree with me,\n\n03:44.520 --> 03:46.840\n but they didn't agree at first.\n\n03:46.840 --> 03:50.120\n So Alan Turing formulated the Turing test,\n\n03:50.120 --> 03:50.960\n and...\n\n03:50.960 --> 03:54.480\n Right, now, what he said was very little about it.\n\n03:54.480 --> 03:55.920\n I mean, the 1950 paper\n\n03:55.920 --> 03:58.060\n where he had articulated the Turing test,\n\n03:59.440 --> 04:04.440\n there's like a few lines that talk about the Turing test.\n\n04:06.840 --> 04:11.840\n And it really wasn't very clear how to administer it.\n\n04:12.040 --> 04:16.520\n And he said if they did it in like 15 minutes,\n\n04:16.520 --> 04:17.680\n that would be sufficient,\n\n04:17.680 --> 04:20.600\n which I don't really think is the case.\n\n04:20.600 --> 04:22.960\n These large language models now,\n\n04:22.960 --> 04:25.560\n some people are convinced by it already.\n\n04:25.560 --> 04:28.440\n I mean, you can talk to it and have a conversation with it.\n\n04:28.440 --> 04:30.340\n You can actually talk to it for hours.\n\n04:31.760 --> 04:35.360\n So it requires a little more depth.\n\n04:35.360 --> 04:38.120\n There's some problems with large language models\n\n04:38.120 --> 04:39.620\n which we can talk about.\n\n04:41.840 --> 04:46.460\n But some people are convinced by the Turing test.\n\n04:46.460 --> 04:50.160\n Now, if somebody passes the Turing test,\n\n04:50.160 --> 04:52.160\n what are the implications of that?\n\n04:52.160 --> 04:53.720\n Does that mean that they're sentient,\n\n04:53.720 --> 04:56.280\n that they're conscious or not?\n\n04:56.280 --> 05:00.880\n It's not necessarily clear what the implications are.\n\n05:00.880 --> 05:05.880\n Anyway, I believe 2029, that's six, seven years from now,\n\n05:07.640 --> 05:10.360\n we'll have something that passes the Turing test\n\n05:10.360 --> 05:12.480\n and a valid Turing test,\n\n05:12.480 --> 05:15.320\n meaning it goes for hours, not just a few minutes.\n\n05:15.320 --> 05:16.600\n Can you speak to that a little bit?\n\n05:16.600 --> 05:21.160\n What is your formulation of the Turing test?\n\n05:21.160 --> 05:23.180\n You've proposed a very difficult version\n\n05:23.180 --> 05:25.420\n of the Turing test, so what does that look like?\n\n05:25.420 --> 05:28.560\n Basically, it's just to assess it over several hours\n\n05:30.760 --> 05:35.760\n and also have a human judge that's fairly sophisticated\n\n05:36.440 --> 05:39.220\n on what computers can do and can't do.\n\n05:40.800 --> 05:43.800\n If you take somebody who's not that sophisticated\n\n05:43.800 --> 05:47.240\n or even an average engineer,\n\n05:48.360 --> 05:52.080\n they may not really assess various aspects of it.\n\n05:52.080 --> 05:55.680\n So you really want the human to challenge the system.\n\n05:55.680 --> 05:57.040\n Exactly, exactly.\n\n05:57.040 --> 05:58.520\n On its ability to do things\n\n05:58.520 --> 06:00.800\n like common sense reasoning, perhaps.\n\n06:00.800 --> 06:04.680\n That's actually a key problem with large language models.\n\n06:04.680 --> 06:08.080\n They don't do these kinds of tests\n\n06:08.080 --> 06:13.080\n that would involve assessing chains of reasoning,\n\n06:17.400 --> 06:18.960\n but you can lose track of that.\n\n06:18.960 --> 06:20.200\n If you talk to them,\n\n06:20.200 --> 06:22.760\n they actually can talk to you pretty well\n\n06:22.760 --> 06:24.840\n and you can be convinced by it,\n\n06:24.840 --> 06:27.400\n but it's somebody that would really convince you\n\n06:27.400 --> 06:32.200\n that it's a human, whatever that takes.\n\n06:32.200 --> 06:34.800\n Maybe it would take days or weeks,\n\n06:34.800 --> 06:38.720\n but it would really convince you that it's human.\n\n06:40.880 --> 06:45.320\n Large language models can appear that way.\n\n06:45.320 --> 06:49.760\n You can read conversations and they appear pretty good.\n\n06:49.760 --> 06:52.260\n There are some problems with it.\n\n06:52.260 --> 06:54.140\n It doesn't do math very well.\n\n06:55.000 --> 06:58.160\n You can ask how many legs did 10 elephants have\n\n06:58.160 --> 07:00.020\n and they'll tell you, well, okay,\n\n07:00.020 --> 07:01.440\n each elephant has four legs\n\n07:01.440 --> 07:03.700\n and it's 10 elephants, so it's 40 legs.\n\n07:03.700 --> 07:05.840\n And you go, okay, that's pretty good.\n\n07:05.840 --> 07:07.960\n How many legs do 11 elephants have?\n\n07:07.960 --> 07:11.520\n And they don't seem to understand the question.\n\n07:11.520 --> 07:14.160\n Do all humans understand that question?\n\n07:14.160 --> 07:15.880\n No, that's the key thing.\n\n07:15.880 --> 07:19.440\n I mean, how advanced a human do you want it to be?\n\n07:19.440 --> 07:21.360\n But we do expect a human\n\n07:21.360 --> 07:23.980\n to be able to do multi chain reasoning,\n\n07:24.840 --> 07:26.320\n to be able to take a few facts\n\n07:26.320 --> 07:29.840\n and put them together, not perfectly.\n\n07:29.840 --> 07:32.800\n And we see that in a lot of polls\n\n07:32.800 --> 07:35.540\n that people don't do that perfectly at all.\n\n07:39.220 --> 07:42.020\n So it's not very well defined,\n\n07:42.020 --> 07:44.320\n but it's something where it really would convince you\n\n07:44.320 --> 07:45.600\n that it's a human.\n\n07:45.600 --> 07:48.840\n Is your intuition that large language models\n\n07:48.840 --> 07:52.320\n will not be solely the kind of system\n\n07:52.320 --> 07:55.600\n that passes the Turing test in 2029?\n\n07:55.600 --> 07:56.800\n Do we need something else?\n\n07:56.800 --> 07:58.720\n No, I think it will be a large language model,\n\n07:58.720 --> 08:02.960\n but they have to go beyond what they're doing now.\n\n08:02.960 --> 08:04.400\n I think we're getting there.\n\n08:05.760 --> 08:09.240\n And another key issue is if somebody\n\n08:09.240 --> 08:12.200\n actually passes the Turing test validly,\n\n08:12.200 --> 08:13.640\n I would believe they're conscious.\n\n08:13.640 --> 08:15.000\n And then not everybody would say that.\n\n08:15.000 --> 08:17.440\n It's okay, we can pass the Turing test,\n\n08:17.440 --> 08:20.080\n but we don't really believe that it's conscious.\n\n08:20.080 --> 08:21.480\n That's a whole nother issue.\n\n08:23.120 --> 08:24.920\n But if it really passes the Turing test,\n\n08:24.920 --> 08:26.720\n I would believe that it's conscious.\n\n08:26.720 --> 08:31.720\n But I don't believe that of large language models today.\n\n08:32.760 --> 08:35.520\n If it appears to be conscious,\n\n08:35.520 --> 08:38.240\n that's as good as being conscious, at least for you,\n\n08:38.240 --> 08:40.700\n in some sense.\n\n08:40.700 --> 08:45.300\n I mean, consciousness is not something that's scientific.\n\n08:46.640 --> 08:48.880\n I mean, I believe you're conscious,\n\n08:49.760 --> 08:51.100\n but it's really just a belief,\n\n08:51.100 --> 08:52.800\n and we believe that about other humans\n\n08:52.800 --> 08:57.400\n that at least appear to be conscious.\n\n08:57.400 --> 09:00.460\n When you go outside of shared human assumption,\n\n09:01.720 --> 09:03.640\n like are animals conscious?\n\n09:04.520 --> 09:06.200\n Some people believe they're not conscious.\n\n09:06.200 --> 09:08.680\n Some people believe they are conscious.\n\n09:08.680 --> 09:13.680\n And would a machine that acts just like a human be conscious?\n\n09:14.520 --> 09:16.200\n I mean, I believe it would be.\n\n09:17.040 --> 09:20.800\n But that's really a philosophical belief.\n\n09:20.800 --> 09:22.720\n You can't prove it.\n\n09:22.720 --> 09:25.480\n I can't take an entity and prove that it's conscious.\n\n09:25.480 --> 09:27.280\n There's nothing that you can do\n\n09:27.280 --> 09:30.360\n that would indicate that.\n\n09:30.360 --> 09:32.780\n It's like saying a piece of art is beautiful.\n\n09:32.780 --> 09:35.000\n You can say it.\n\n09:35.000 --> 09:38.200\n Multiple people can experience a piece of art as beautiful,\n\n09:39.300 --> 09:41.320\n but you can't prove it.\n\n09:41.320 --> 09:44.840\n But it's also an extremely important issue.\n\n09:44.840 --> 09:47.040\n I mean, imagine if you had something\n\n09:47.040 --> 09:49.140\n where nobody's conscious.\n\n09:49.140 --> 09:52.660\n The world may as well not exist.\n\n09:55.660 --> 10:00.060\n And so some people, like say Marvin Minsky,\n\n10:02.620 --> 10:05.940\n said, well, consciousness is not logical,\n\n10:05.940 --> 10:08.380\n it's not scientific, and therefore we should dismiss it,\n\n10:08.380 --> 10:13.380\n and any talk about consciousness is just not to be believed.\n\n10:15.500 --> 10:18.500\n But when he actually engaged with somebody\n\n10:18.500 --> 10:20.660\n who was conscious, he actually acted\n\n10:20.660 --> 10:22.620\n as if they were conscious.\n\n10:22.620 --> 10:24.260\n He didn't ignore that.\n\n10:24.260 --> 10:26.860\n He acted as if consciousness does matter.\n\n10:26.860 --> 10:28.180\n Exactly.\n\n10:28.180 --> 10:30.500\n Whereas he said it didn't matter.\n\n10:30.500 --> 10:31.780\n Well, that's Marvin Minsky.\n\n10:31.780 --> 10:32.620\n Yeah.\n\n10:32.620 --> 10:34.060\n He's full of contradictions.\n\n10:34.060 --> 10:37.660\n But that's true of a lot of people as well.\n\n10:37.660 --> 10:39.620\n But to you, consciousness matters.\n\n10:39.620 --> 10:42.160\n But to me, it's very important.\n\n10:42.160 --> 10:45.640\n But I would say it's not a scientific issue.\n\n10:45.640 --> 10:49.240\n It's a philosophical issue.\n\n10:49.240 --> 10:50.720\n And people have different views.\n\n10:50.720 --> 10:52.800\n Some people believe that anything\n\n10:52.800 --> 10:54.520\n that makes a decision is conscious.\n\n10:54.520 --> 10:56.760\n So your light switch is conscious.\n\n10:56.760 --> 10:59.400\n Its level of consciousness is low,\n\n10:59.400 --> 11:03.400\n not very interesting, but that's a consciousness.\n\n11:05.120 --> 11:09.120\n So a computer that makes a more interesting decision\n\n11:09.120 --> 11:10.440\n is still not at human levels,\n\n11:10.440 --> 11:12.560\n but it's also conscious and at a higher level\n\n11:12.560 --> 11:13.720\n than your light switch.\n\n11:13.720 --> 11:15.980\n So that's one view.\n\n11:17.360 --> 11:20.080\n There's many different views of what consciousness is.\n\n11:20.080 --> 11:22.880\n So if a system passes the Turing test,\n\n11:24.600 --> 11:29.600\n it's not scientific, but in issues of philosophy,\n\n11:30.000 --> 11:32.600\n things like ethics start to enter the picture.\n\n11:32.600 --> 11:35.500\n Do you think there would be,\n\n11:35.500 --> 11:38.920\n we would start contending as a human species\n\n11:39.920 --> 11:42.840\n about the ethics of turning off such a machine?\n\n11:42.840 --> 11:46.480\n Yeah, I mean, that's definitely come up.\n\n11:47.400 --> 11:49.600\n Hasn't come up in reality yet.\n\n11:49.600 --> 11:50.560\n Yet.\n\n11:50.560 --> 11:52.400\n But I'm talking about 2029.\n\n11:52.400 --> 11:54.180\n It's not that many years from now.\n\n11:56.080 --> 11:58.480\n So what are our obligations to it?\n\n11:59.960 --> 12:03.240\n It has a different, I mean, a computer that's conscious,\n\n12:03.240 --> 12:08.240\n it has a little bit different connotations than a human.\n\n12:08.240 --> 12:13.240\n We have a continuous consciousness.\n\n12:15.600 --> 12:19.640\n We're in an entity that does not last forever.\n\n12:22.080 --> 12:27.080\n Now, actually, a significant portion of humans still exist\n\n12:27.400 --> 12:29.240\n and are therefore still conscious.\n\n12:31.760 --> 12:34.880\n But anybody who is over a certain age\n\n12:34.880 --> 12:37.160\n doesn't exist anymore.\n\n12:37.160 --> 12:40.320\n That wouldn't be true of a computer program.\n\n12:40.320 --> 12:42.000\n You could completely turn it off\n\n12:42.000 --> 12:46.120\n and a copy of it could be stored and you could recreate it.\n\n12:46.120 --> 12:49.920\n And so it has a different type of validity.\n\n12:51.160 --> 12:52.920\n You could actually take it back in time.\n\n12:52.920 --> 12:55.840\n You could eliminate its memory and have it go over again.\n\n12:55.840 --> 12:59.800\n I mean, it has a different kind of connotation\n\n12:59.800 --> 13:01.800\n than humans do.\n\n13:01.800 --> 13:04.400\n Well, perhaps it can do the same thing with humans.\n\n13:04.400 --> 13:06.880\n It's just that we don't know how to do that yet.\n\n13:06.880 --> 13:09.400\n It's possible that we figure out all of these things\n\n13:09.400 --> 13:10.780\n on the machine first.\n\n13:12.320 --> 13:15.480\n But that doesn't mean the machine isn't conscious.\n\n13:15.480 --> 13:17.640\n I mean, if you look at the way people react,\n\n13:17.640 --> 13:22.640\n say, 3CPO or other machines that are conscious in movies,\n\n13:25.000 --> 13:26.740\n they don't actually present how it's conscious,\n\n13:26.740 --> 13:30.120\n but we see that they are a machine\n\n13:30.120 --> 13:33.280\n and people will believe that they are conscious\n\n13:33.280 --> 13:34.640\n and they'll actually worry about it\n\n13:34.640 --> 13:37.480\n if they get into trouble and so on.\n\n13:37.480 --> 13:40.840\n So 2029 is going to be the first year\n\n13:40.840 --> 13:43.440\n when a major thing happens.\n\n13:43.440 --> 13:44.280\n Right.\n\n13:44.280 --> 13:46.520\n And that will shake our civilization\n\n13:46.520 --> 13:50.280\n to start to consider the role of AI in this world.\n\n13:50.280 --> 13:51.120\n Yes and no.\n\n13:51.120 --> 13:54.560\n I mean, this one guy at Google claimed\n\n13:54.560 --> 13:58.440\n that the machine was conscious.\n\n13:58.440 --> 14:00.160\n But that's just one person.\n\n14:00.160 --> 14:01.000\n Right.\n\n14:01.000 --> 14:03.080\n When it starts to happen to scale.\n\n14:03.080 --> 14:06.320\n Well, that's exactly right because most people\n\n14:06.320 --> 14:07.760\n have not taken that position.\n\n14:07.760 --> 14:08.940\n I don't take that position.\n\n14:08.940 --> 14:13.940\n I mean, I've used different things like this\n\n14:17.240 --> 14:20.500\n and they don't appear to me to be conscious.\n\n14:20.500 --> 14:22.840\n As we eliminate various problems\n\n14:22.840 --> 14:25.840\n of these large language models,\n\n14:26.960 --> 14:30.480\n more and more people will accept that they're conscious.\n\n14:30.480 --> 14:35.480\n So when we get to 2029, I think a large fraction\n\n14:35.760 --> 14:37.960\n of people will believe that they're conscious.\n\n14:39.080 --> 14:41.040\n So it's not gonna happen all at once.\n\n14:42.440 --> 14:44.360\n I believe it will actually happen gradually\n\n14:44.360 --> 14:46.240\n and it's already started to happen.\n\n14:47.280 --> 14:52.280\n And so that takes us one step closer to the singularity.\n\n14:52.360 --> 14:55.560\n Another step then is in the 2030s\n\n14:55.560 --> 14:59.800\n when we can actually connect our neocortex,\n\n14:59.800 --> 15:04.800\n which is where we do our thinking, to computers.\n\n15:04.880 --> 15:09.280\n And I mean, just as this actually gains a lot\n\n15:09.280 --> 15:12.200\n to being connected to computers\n\n15:12.200 --> 15:15.360\n that will amplify its abilities,\n\n15:15.360 --> 15:17.400\n I mean, if this did not have any connection,\n\n15:17.400 --> 15:19.360\n it would be pretty stupid.\n\n15:19.360 --> 15:21.860\n It could not answer any of your questions.\n\n15:21.860 --> 15:24.400\n If you're just listening to this, by the way,\n\n15:24.400 --> 15:29.400\n Ray's holding up the all powerful smartphone.\n\n15:29.400 --> 15:32.480\n So we're gonna do that directly from our brains.\n\n15:33.520 --> 15:35.040\n I mean, these are pretty good.\n\n15:35.040 --> 15:37.720\n These already have amplified our intelligence.\n\n15:37.720 --> 15:40.040\n I'm already much smarter than I would otherwise be\n\n15:40.040 --> 15:41.480\n if I didn't have this.\n\n15:42.600 --> 15:44.240\n Because I remember my first book,\n\n15:44.240 --> 15:45.920\n The Age of Intelligent Machines,\n\n15:49.060 --> 15:52.080\n there was no way to get information from computers.\n\n15:52.080 --> 15:55.400\n I actually would go to a library, find a book,\n\n15:55.400 --> 15:58.440\n find the page that had an information I wanted,\n\n15:58.440 --> 15:59.920\n and I'd go to the copier,\n\n15:59.920 --> 16:04.360\n and my most significant information tool\n\n16:04.360 --> 16:08.480\n was a roll of quarters where I could feed the copier.\n\n16:08.480 --> 16:11.400\n So we're already greatly advanced\n\n16:11.400 --> 16:13.280\n that we have these things.\n\n16:13.280 --> 16:15.460\n There's a few problems with it.\n\n16:15.460 --> 16:17.280\n First of all, I constantly put it down,\n\n16:17.280 --> 16:19.680\n and I don't remember where I put it.\n\n16:19.680 --> 16:21.220\n I've actually never lost it.\n\n16:21.220 --> 16:26.080\n But you have to find it, and then you have to turn it on.\n\n16:26.080 --> 16:28.160\n So there's a certain amount of steps.\n\n16:28.160 --> 16:30.100\n It would actually be quite useful\n\n16:30.100 --> 16:33.440\n if someone would just listen to your conversation\n\n16:33.440 --> 16:38.440\n and say, oh, that's so and so actress,\n\n16:38.920 --> 16:41.160\n and tell you what you're talking about.\n\n16:41.160 --> 16:43.160\n So going from active to passive,\n\n16:43.160 --> 16:46.240\n where it just permeates your whole life.\n\n16:46.240 --> 16:47.280\n Yeah, exactly.\n\n16:47.280 --> 16:49.560\n The way your brain does when you're awake.\n\n16:49.560 --> 16:51.220\n Your brain is always there.\n\n16:51.220 --> 16:52.060\n Right.\n\n16:52.060 --> 16:53.800\n That's something that could actually\n\n16:53.800 --> 16:55.840\n just about be done today,\n\n16:55.840 --> 16:57.400\n where we'd listen to your conversation,\n\n16:57.400 --> 16:58.600\n understand what you're saying,\n\n16:58.600 --> 17:01.840\n understand what you're not missing,\n\n17:01.840 --> 17:03.600\n and give you that information.\n\n17:04.520 --> 17:07.300\n But another step is to actually go inside your brain.\n\n17:09.720 --> 17:12.740\n And there are some prototypes\n\n17:12.740 --> 17:15.280\n where you can connect your brain.\n\n17:15.280 --> 17:17.040\n They actually don't have the amount\n\n17:17.040 --> 17:19.160\n of bandwidth that we need.\n\n17:19.160 --> 17:21.940\n They can work, but they work fairly slowly.\n\n17:21.940 --> 17:26.160\n So if it actually would connect to your neocortex,\n\n17:26.160 --> 17:30.180\n and the neocortex, which I describe\n\n17:30.180 --> 17:31.740\n in How to Create a Mind,\n\n17:33.000 --> 17:34.820\n the neocortex is actually,\n\n17:36.700 --> 17:38.180\n it has different levels,\n\n17:38.180 --> 17:39.980\n and as you go up the levels,\n\n17:39.980 --> 17:41.780\n it's kind of like a pyramid.\n\n17:41.780 --> 17:44.340\n The top level is fairly small,\n\n17:44.340 --> 17:46.540\n and that's the level where you wanna connect\n\n17:47.820 --> 17:50.140\n these brain extenders.\n\n17:50.140 --> 17:55.140\n And so I believe that will happen in the 2030s.\n\n17:58.100 --> 18:01.580\n So just the way this is greatly amplified\n\n18:01.580 --> 18:03.480\n by being connected to the cloud,\n\n18:04.420 --> 18:07.420\n we can connect our own brain to the cloud,\n\n18:07.420 --> 18:12.420\n and just do what we can do by using this machine.\n\n18:14.260 --> 18:15.660\n Do you think it would look like\n\n18:15.660 --> 18:18.920\n the brain computer interface of like Neuralink?\n\n18:18.920 --> 18:19.760\n So would it be?\n\n18:19.760 --> 18:22.500\n Well, Neuralink, it's an attempt to do that.\n\n18:22.500 --> 18:24.920\n It doesn't have the bandwidth that we need.\n\n18:26.300 --> 18:27.660\n Yet, right?\n\n18:27.660 --> 18:29.240\n Right, but I think,\n\n18:30.320 --> 18:31.980\n I mean, they're gonna get permission for this\n\n18:31.980 --> 18:33.160\n because there are a lot of people\n\n18:33.160 --> 18:36.660\n who absolutely need it because they can't communicate.\n\n18:36.660 --> 18:38.420\n I know a couple people like that\n\n18:38.420 --> 18:41.220\n who have ideas and they cannot,\n\n18:42.660 --> 18:44.580\n they cannot move their muscles and so on.\n\n18:44.580 --> 18:45.800\n They can't communicate.\n\n18:45.800 --> 18:50.800\n And so for them, this would be very valuable,\n\n18:52.040 --> 18:53.320\n but we could all use it.\n\n18:54.820 --> 18:56.600\n Basically, it'd be,\n\n18:59.000 --> 19:02.520\n turn us into something that would be like we have a phone,\n\n19:02.520 --> 19:05.120\n but it would be in our minds.\n\n19:05.120 --> 19:07.360\n It would be kind of instantaneous.\n\n19:07.360 --> 19:09.440\n And maybe communication between two people\n\n19:09.440 --> 19:14.080\n would not require this low bandwidth mechanism of language.\n\n19:14.080 --> 19:15.640\n Yes, exactly.\n\n19:15.640 --> 19:17.280\n We don't know what that would be,\n\n19:17.280 --> 19:22.280\n although we do know that computers can share information\n\n19:22.400 --> 19:24.640\n like language instantly.\n\n19:24.640 --> 19:28.880\n They can share many, many books in a second.\n\n19:28.880 --> 19:31.200\n So we could do that as well.\n\n19:31.200 --> 19:34.240\n If you look at what our brain does,\n\n19:34.240 --> 19:39.120\n it actually can manipulate different parameters.\n\n19:39.120 --> 19:44.120\n So we talk about these large language models.\n\n19:46.560 --> 19:48.320\n I mean, I had written that\n\n19:51.520 --> 19:55.000\n it requires a certain amount of information\n\n19:55.000 --> 19:57.520\n in order to be effective\n\n19:58.600 --> 20:01.920\n and that we would not see AI really being effective\n\n20:01.920 --> 20:03.480\n until it got to that level.\n\n20:04.400 --> 20:06.400\n And we had large language models\n\n20:06.400 --> 20:09.600\n that were like 10 billion bytes, didn't work very well.\n\n20:09.600 --> 20:11.680\n They finally got to a hundred billion bytes\n\n20:11.680 --> 20:13.440\n and now they work fairly well.\n\n20:13.440 --> 20:16.280\n And now we're going to a trillion bytes.\n\n20:16.280 --> 20:21.280\n If you say lambda has a hundred billion bytes,\n\n20:22.520 --> 20:23.520\n what does that mean?\n\n20:23.520 --> 20:27.160\n Well, what if you had something that had one byte,\n\n20:27.160 --> 20:30.000\n one parameter, maybe you wanna tell\n\n20:30.000 --> 20:33.960\n whether or not something's an elephant or not.\n\n20:33.960 --> 20:37.680\n And so you put in something that would detect its trunk.\n\n20:37.680 --> 20:39.160\n If it has a trunk, it's an elephant.\n\n20:39.160 --> 20:41.680\n If it doesn't have a trunk, it's not an elephant.\n\n20:41.680 --> 20:44.400\n That would work fairly well.\n\n20:44.400 --> 20:46.320\n There's a few problems with it.\n\n20:47.440 --> 20:49.720\n And it really wouldn't be able to tell what a trunk is,\n\n20:49.720 --> 20:50.640\n but anyway.\n\n20:50.640 --> 20:54.120\n And maybe other things other than elephants have trunks,\n\n20:54.120 --> 20:55.560\n you might get really confused.\n\n20:55.560 --> 20:56.960\n Yeah, exactly.\n\n20:56.960 --> 20:58.800\n I'm not sure which animals have trunks,\n\n20:58.800 --> 21:02.400\n but how do you define a trunk?\n\n21:02.400 --> 21:04.000\n But yeah, that's one parameter.\n\n21:04.960 --> 21:06.400\n You can do okay.\n\n21:06.400 --> 21:08.760\n So these things have a hundred billion parameters.\n\n21:08.760 --> 21:12.200\n So they're able to deal with very complex issues.\n\n21:12.200 --> 21:14.000\n All kinds of trunks.\n\n21:14.000 --> 21:16.240\n Human beings actually have a little bit more than that,\n\n21:16.240 --> 21:17.960\n but they're getting to the point\n\n21:17.960 --> 21:20.000\n where they can emulate humans.\n\n21:22.400 --> 21:27.400\n If we were able to connect this to our neocortex,\n\n21:27.400 --> 21:32.400\n we would basically add more of these abilities\n\n21:33.400 --> 21:35.400\n to make distinctions,\n\n21:35.400 --> 21:37.600\n and it could ultimately be much smarter\n\n21:37.600 --> 21:39.680\n and also be attached to information\n\n21:39.680 --> 21:42.240\n that we feel is reliable.\n\n21:43.800 --> 21:45.240\n So that's where we're headed.\n\n21:45.240 --> 21:49.120\n So you think that there will be a merger in the 30s,\n\n21:49.120 --> 21:50.880\n an increasing amount of merging\n\n21:50.880 --> 21:55.880\n between the human brain and the AI brain?\n\n21:55.880 --> 21:57.640\n Exactly.\n\n21:57.640 --> 22:02.280\n And the AI brain is really an emulation of human beings.\n\n22:02.280 --> 22:04.480\n I mean, that's why we're creating them,\n\n22:04.480 --> 22:07.200\n because human beings act the same way,\n\n22:07.200 --> 22:09.600\n and this is basically to amplify them.\n\n22:09.600 --> 22:11.600\n I mean, this amplifies our brain.\n\n22:13.800 --> 22:15.560\n It's a little bit clumsy to interact with,\n\n22:15.560 --> 22:20.560\n but it definitely is way beyond what we had 15 years ago.\n\n22:21.840 --> 22:23.520\n But the implementation becomes different,\n\n22:23.520 --> 22:26.680\n just like a bird versus the airplane,\n\n22:26.680 --> 22:30.600\n even though the AI brain is an emulation,\n\n22:30.600 --> 22:34.360\n it starts adding features we might not otherwise have,\n\n22:34.360 --> 22:36.280\n like ability to consume a huge amount\n\n22:36.280 --> 22:38.520\n of information quickly,\n\n22:38.520 --> 22:43.080\n like look up thousands of Wikipedia articles in one take.\n\n22:43.080 --> 22:44.200\n Exactly.\n\n22:44.200 --> 22:46.320\n I mean, we can get, for example,\n\n22:46.320 --> 22:48.120\n issues like simulated biology,\n\n22:48.120 --> 22:53.120\n where it can simulate many different things at once.\n\n22:56.760 --> 22:59.600\n We already had one example of simulated biology,\n\n22:59.600 --> 23:01.480\n which is the Moderna vaccine.\n\n23:04.560 --> 23:06.600\n And that's gonna be now\n\n23:06.600 --> 23:10.200\n the way in which we create medications.\n\n23:11.160 --> 23:13.000\n But they were able to simulate\n\n23:13.000 --> 23:17.760\n what each example of an mRNA would do to a human being,\n\n23:17.760 --> 23:21.400\n and they were able to simulate that quite reliably.\n\n23:21.400 --> 23:23.960\n And we actually simulated billions\n\n23:23.960 --> 23:27.040\n of different mRNA sequences,\n\n23:27.040 --> 23:29.000\n and they found the ones that were the best,\n\n23:29.000 --> 23:31.040\n and they created the vaccine.\n\n23:31.040 --> 23:34.120\n And they did, and talked about doing that quickly,\n\n23:34.120 --> 23:36.280\n they did that in two days.\n\n23:36.280 --> 23:37.880\n Now, how long would a human being take\n\n23:37.880 --> 23:41.000\n to simulate billions of different mRNA sequences?\n\n23:41.000 --> 23:42.840\n I don't know that we could do it at all,\n\n23:42.840 --> 23:45.800\n but it would take many years.\n\n23:45.800 --> 23:50.000\n They did it in two days, and one of the reasons\n\n23:50.000 --> 23:52.800\n that people didn't like vaccines\n\n23:53.720 --> 23:55.520\n is because it was done too quickly,\n\n23:55.520 --> 23:57.000\n it was done too fast.\n\n23:58.200 --> 24:01.280\n And they actually included the time it took to test it out,\n\n24:01.280 --> 24:03.600\n which was 10 months, so they figured,\n\n24:03.600 --> 24:06.320\n okay, it took 10 months to create this.\n\n24:06.320 --> 24:08.080\n Actually, it took us two days.\n\n24:09.080 --> 24:11.880\n And we also will be able to ultimately do the tests\n\n24:11.880 --> 24:14.200\n in a few days as well.\n\n24:14.200 --> 24:16.600\n Oh, because we can simulate how the body will respond to it.\n\n24:16.600 --> 24:19.120\n Yeah, that's a little bit more complicated\n\n24:19.120 --> 24:22.920\n because the body has a lot of different elements,\n\n24:22.920 --> 24:25.400\n and we have to simulate all of that,\n\n24:25.400 --> 24:27.520\n but that's coming as well.\n\n24:27.520 --> 24:30.240\n So ultimately, we could create it in a few days\n\n24:30.240 --> 24:32.840\n and then test it in a few days, and it would be done.\n\n24:34.040 --> 24:35.960\n And we can do that with every type\n\n24:35.960 --> 24:40.240\n of medical insufficiency that we have.\n\n24:40.240 --> 24:45.240\n So curing all diseases, improving certain functions\n\n24:46.680 --> 24:51.680\n of the body, supplements, drugs for recreation,\n\n24:53.120 --> 24:56.080\n for health, for performance, for productivity,\n\n24:56.080 --> 24:56.920\n all that kind of stuff.\n\n24:56.920 --> 24:58.040\n Well, that's where we're headed,\n\n24:58.040 --> 25:00.640\n because I mean, right now we have a very inefficient way\n\n25:00.640 --> 25:02.520\n of creating these new medications.\n\n25:04.560 --> 25:07.120\n But we've already shown it, and the Moderna vaccine\n\n25:07.120 --> 25:11.400\n is actually the best of the vaccines we've had,\n\n25:12.520 --> 25:14.880\n and it literally took two days to create.\n\n25:16.280 --> 25:17.200\n And we'll get to the point\n\n25:17.200 --> 25:20.160\n where we can test it out also quickly.\n\n25:20.160 --> 25:22.280\n Are you impressed by AlphaFold\n\n25:22.280 --> 25:25.760\n and the solution to the protein folding,\n\n25:25.760 --> 25:30.040\n which essentially is simulating, modeling\n\n25:30.040 --> 25:33.080\n this primitive building block of life,\n\n25:33.080 --> 25:36.080\n which is a protein, and its 3D shape?\n\n25:36.080 --> 25:39.040\n It's pretty remarkable that they can actually predict\n\n25:39.040 --> 25:42.000\n what the 3D shape of these things are,\n\n25:42.000 --> 25:45.920\n but they did it with the same type of neural net\n\n25:45.920 --> 25:50.920\n that won, for example, the Go test.\n\n25:51.240 --> 25:52.720\n So it's all the same.\n\n25:52.720 --> 25:53.560\n It's all the same.\n\n25:53.560 --> 25:54.400\n All the same approaches.\n\n25:54.400 --> 25:57.080\n They took that same thing and just changed the rules\n\n25:57.080 --> 26:01.640\n to chess, and within a couple of days,\n\n26:01.640 --> 26:03.960\n it now played a master level of chess\n\n26:03.960 --> 26:06.280\n greater than any human being.\n\n26:09.160 --> 26:12.040\n And the same thing then worked for AlphaFold,\n\n26:13.480 --> 26:14.800\n which no human had done.\n\n26:14.800 --> 26:16.800\n I mean, human beings could do,\n\n26:16.800 --> 26:20.600\n the best humans could maybe do 15, 20%\n\n26:22.640 --> 26:25.840\n of figuring out what the shape would be.\n\n26:25.840 --> 26:30.840\n And after a few takes, it ultimately did just about 100%.\n\n26:30.840 --> 26:32.560\n 100%.\n\n26:32.560 --> 26:35.600\n Do you still think the singularity will happen in 2045?\n\n26:37.560 --> 26:38.960\n And what does that look like?\n\n26:40.760 --> 26:45.760\n Once we can amplify our brain with computers directly,\n\n26:46.600 --> 26:48.080\n which will happen in the 2030s,\n\n26:48.080 --> 26:49.800\n that's gonna keep growing.\n\n26:49.800 --> 26:51.040\n That's another whole theme,\n\n26:51.040 --> 26:54.960\n which is the exponential growth of computing power.\n\n26:54.960 --> 26:57.520\n Yeah, so looking at price performance of computation\n\n26:57.520 --> 26:59.800\n from 1939 to 2021.\n\n26:59.800 --> 27:02.920\n Right, so that starts with the very first computer\n\n27:02.920 --> 27:05.600\n actually created by a German during World War II.\n\n27:06.560 --> 27:09.440\n You might have thought that that might be significant,\n\n27:09.440 --> 27:12.880\n but actually the Germans didn't think computers\n\n27:12.880 --> 27:16.640\n were significant, and they completely rejected it.\n\n27:16.640 --> 27:20.360\n The second one is also the ZUSA 2.\n\n27:20.360 --> 27:22.240\n And by the way, we're looking at a plot\n\n27:22.240 --> 27:27.240\n with the X axis being the year from 1935 to 2025.\n\n27:27.240 --> 27:30.920\n And on the Y axis in log scale\n\n27:30.920 --> 27:34.680\n is computation per second per constant dollar.\n\n27:34.680 --> 27:36.800\n So dollar normalized inflation.\n\n27:37.720 --> 27:40.200\n And it's growing linearly on the log scale,\n\n27:40.200 --> 27:41.880\n which means it's growing exponentially.\n\n27:41.880 --> 27:44.480\n The third one was the British computer,\n\n27:44.480 --> 27:47.720\n which the Allies did take very seriously.\n\n27:47.720 --> 27:51.780\n And it cracked the German code\n\n27:51.780 --> 27:55.520\n and enables the British to win the Battle of Britain,\n\n27:55.520 --> 27:57.720\n which otherwise absolutely would not have happened\n\n27:57.720 --> 28:00.780\n if they hadn't cracked the code using that computer.\n\n28:02.120 --> 28:03.640\n But that's an exponential graph.\n\n28:03.640 --> 28:07.300\n So a straight line on that graph is exponential growth.\n\n28:07.300 --> 28:11.600\n And you see 80 years of exponential growth.\n\n28:11.600 --> 28:15.200\n And I would say about every five years,\n\n28:15.200 --> 28:18.280\n and this happened shortly before the pandemic,\n\n28:18.280 --> 28:20.680\n people saying, well, they call it Moore's law,\n\n28:20.680 --> 28:25.480\n which is not the correct, because that's not all intel.\n\n28:25.480 --> 28:29.680\n In fact, this started decades before intel was even created.\n\n28:29.680 --> 28:34.140\n It wasn't with transistors formed into a grid.\n\n28:34.140 --> 28:37.280\n So it's not just transistor count or transistor size.\n\n28:37.280 --> 28:42.280\n Right, it started with relays, then went to vacuum tubes,\n\n28:43.200 --> 28:46.720\n then went to individual transistors,\n\n28:46.720 --> 28:48.820\n and then to integrated circuits.\n\n28:51.080 --> 28:54.000\n And integrated circuits actually starts\n\n28:54.000 --> 28:55.640\n like in the middle of this graph.\n\n28:56.520 --> 28:58.760\n And it has nothing to do with intel.\n\n28:58.760 --> 29:02.880\n Intel actually was a key part of this.\n\n29:02.880 --> 29:07.180\n But a few years ago, they stopped making the fastest chips.\n\n29:08.960 --> 29:12.800\n But if you take the fastest chip of any technology\n\n29:12.800 --> 29:16.640\n in that year, you get this kind of graph.\n\n29:16.640 --> 29:19.760\n And it's definitely continuing for 80 years.\n\n29:19.760 --> 29:23.880\n So you don't think Moore's law, broadly defined, is dead.\n\n29:24.840 --> 29:29.280\n It's been declared dead multiple times throughout this process.\n\n29:29.280 --> 29:31.400\n I don't like the term Moore's law,\n\n29:31.400 --> 29:34.740\n because it has nothing to do with Moore or with intel.\n\n29:34.740 --> 29:39.740\n But yes, the exponential growth of computing is continuing.\n\n29:41.600 --> 29:42.920\n It has never stopped.\n\n29:42.920 --> 29:43.960\n From various sources.\n\n29:43.960 --> 29:45.880\n I mean, it went through World War II,\n\n29:45.880 --> 29:49.120\n it went through global recessions.\n\n29:49.120 --> 29:50.680\n It's just continuing.\n\n29:53.480 --> 29:58.100\n And if you continue that out, along with software gains,\n\n29:58.100 --> 29:59.720\n which is a whole nother issue,\n\n30:01.560 --> 30:02.960\n and they really multiply,\n\n30:02.960 --> 30:04.400\n whatever you get from software gains,\n\n30:04.400 --> 30:07.920\n you multiply by the computer gains,\n\n30:07.920 --> 30:10.080\n you get faster and faster speed.\n\n30:10.940 --> 30:14.320\n This is actually the fastest computer models\n\n30:14.320 --> 30:15.840\n that have been created.\n\n30:15.840 --> 30:19.480\n And that actually expands roughly twice a year.\n\n30:19.480 --> 30:22.840\n Like, every six months it expands by two.\n\n30:22.840 --> 30:27.840\n So we're looking at a plot from 2010 to 2022.\n\n30:28.360 --> 30:31.400\n On the x axis is the publication date of the model,\n\n30:31.400 --> 30:34.240\n and perhaps sometimes the actual paper associated with it.\n\n30:34.240 --> 30:39.240\n And on the y axis is training, compute, and flops.\n\n30:40.240 --> 30:43.880\n And so basically this is looking at the increase\n\n30:43.880 --> 30:46.360\n in the, not transistors,\n\n30:46.360 --> 30:51.360\n but the computational power of neural networks.\n\n30:51.500 --> 30:55.120\n Yes, the computational power that created these models.\n\n30:55.120 --> 30:57.560\n And that's doubled every six months.\n\n30:57.560 --> 31:00.360\n Which is even faster than transistor division.\n\n31:00.360 --> 31:01.200\n Yeah.\n\n31:02.120 --> 31:06.880\n Now actually, since it goes faster than the amount of cost,\n\n31:06.880 --> 31:10.840\n this has actually become a greater investment\n\n31:10.840 --> 31:12.260\n to create these.\n\n31:12.260 --> 31:16.600\n But at any rate, by the time we get to 2045,\n\n31:16.600 --> 31:19.120\n we'll be able to multiply our intelligence\n\n31:19.120 --> 31:21.480\n many millions fold.\n\n31:21.480 --> 31:25.040\n And it's just very hard to imagine what that will be like.\n\n31:25.040 --> 31:28.400\n And that's the singularity where we can't even imagine.\n\n31:28.400 --> 31:30.480\n Right, that's why we call it the singularity.\n\n31:30.480 --> 31:32.760\n Because the singularity in physics,\n\n31:32.760 --> 31:35.120\n something gets sucked into its singularity\n\n31:35.120 --> 31:37.780\n and you can't tell what's going on in there\n\n31:37.780 --> 31:40.440\n because no information can get out of it.\n\n31:40.440 --> 31:42.120\n There's various problems with that,\n\n31:42.120 --> 31:44.080\n but that's the idea.\n\n31:44.080 --> 31:48.960\n It's too much beyond what we can imagine.\n\n31:48.960 --> 31:51.080\n Do you think it's possible we don't notice\n\n31:52.120 --> 31:55.280\n that what the singularity actually feels like\n\n31:56.360 --> 31:58.240\n is we just live through it\n\n31:59.640 --> 32:04.640\n with exponentially increasing cognitive capabilities\n\n32:05.400 --> 32:08.440\n and we almost, because everything's moving so quickly,\n\n32:09.560 --> 32:11.800\n aren't really able to introspect\n\n32:11.800 --> 32:13.680\n that our life has changed.\n\n32:13.680 --> 32:17.480\n Yeah, but I mean, we will have that much greater capacity\n\n32:17.480 --> 32:20.760\n to understand things, so we should be able to look back.\n\n32:20.760 --> 32:23.240\n Looking at history, understand history.\n\n32:23.240 --> 32:26.880\n But we will need people, basically like you and me,\n\n32:26.880 --> 32:29.240\n to actually think about these things.\n\n32:29.240 --> 32:30.680\n But we might be distracted\n\n32:30.680 --> 32:34.160\n by all the other sources of entertainment and fun\n\n32:34.160 --> 32:39.160\n because the exponential power of intellect is growing,\n\n32:39.160 --> 32:41.640\n but also there'll be a lot of fun.\n\n32:41.640 --> 32:46.160\n The amount of ways you can have, you know.\n\n32:46.160 --> 32:48.440\n I mean, we already have a lot of fun with computer games\n\n32:48.440 --> 32:51.400\n and so on that are really quite remarkable.\n\n32:51.400 --> 32:54.760\n What do you think about the digital world,\n\n32:54.760 --> 32:57.640\n the metaverse, virtual reality?\n\n32:57.640 --> 32:59.120\n Will that have a component in this\n\n32:59.120 --> 33:01.840\n or will most of our advancement be in physical reality?\n\n33:01.840 --> 33:04.480\n Well, that's a little bit like Second Life,\n\n33:04.480 --> 33:06.880\n although the Second Life actually didn't work very well\n\n33:06.880 --> 33:09.320\n because it couldn't actually handle too many people.\n\n33:09.320 --> 33:14.200\n And I don't think the metaverse has come to being.\n\n33:14.200 --> 33:16.040\n I think there will be something like that.\n\n33:16.040 --> 33:21.040\n It won't necessarily be from that one company.\n\n33:21.040 --> 33:23.480\n I mean, there's gonna be competitors.\n\n33:23.480 --> 33:26.480\n But yes, we're gonna live increasingly online,\n\n33:26.480 --> 33:28.960\n and particularly if our brains are online.\n\n33:28.960 --> 33:31.320\n I mean, how could we not be online?\n\n33:31.320 --> 33:34.680\n Do you think it's possible that given this merger with AI,\n\n33:34.680 --> 33:39.040\n most of our meaningful interactions\n\n33:39.040 --> 33:43.880\n will be in this virtual world most of our life?\n\n33:43.880 --> 33:46.320\n We fall in love, we make friends,\n\n33:46.320 --> 33:49.480\n we come up with ideas, we do collaborations, we have fun.\n\n33:49.480 --> 33:51.720\n I actually know somebody who's marrying somebody\n\n33:51.720 --> 33:53.000\n that they never met.\n\n33:54.720 --> 33:57.760\n I think they just met her briefly before the wedding,\n\n33:57.760 --> 34:01.560\n but she actually fell in love with this other person,\n\n34:01.560 --> 34:04.160\n never having met them.\n\n34:06.280 --> 34:10.360\n And I think the love is real, so.\n\n34:10.360 --> 34:11.520\n That's a beautiful story,\n\n34:11.520 --> 34:15.080\n but do you think that story is one that might be experienced\n\n34:15.080 --> 34:18.480\n as opposed to by hundreds of thousands of people,\n\n34:18.480 --> 34:22.280\n but instead by hundreds of millions of people?\n\n34:22.280 --> 34:23.880\n I mean, it really gives you appreciation\n\n34:23.880 --> 34:27.240\n for these virtual ways of communicating.\n\n34:28.520 --> 34:30.080\n And if anybody can do it,\n\n34:30.080 --> 34:33.560\n then it's really not such a freak story.\n\n34:34.720 --> 34:37.440\n So I think more and more people will do that.\n\n34:37.440 --> 34:38.720\n But that's turning our back\n\n34:38.720 --> 34:41.920\n on our entire history of evolution.\n\n34:41.920 --> 34:45.520\n The old days, we used to fall in love by holding hands\n\n34:45.520 --> 34:49.360\n and sitting by the fire, that kind of stuff.\n\n34:49.360 --> 34:50.720\n Here, you're playing.\n\n34:50.720 --> 34:54.640\n Actually, I have five patents on where you can hold hands,\n\n34:54.640 --> 34:57.040\n even if you're separated.\n\n34:57.040 --> 34:58.640\n Great.\n\n34:58.640 --> 35:01.960\n So the touch, the sense, it's all just senses.\n\n35:01.960 --> 35:03.040\n It's all just replicated.\n\n35:03.040 --> 35:04.720\n Yeah, I mean, touch is,\n\n35:04.720 --> 35:07.160\n it's not just that you're touching someone or not.\n\n35:07.160 --> 35:10.480\n There's a whole way of doing it, and it's very subtle.\n\n35:11.520 --> 35:15.200\n But ultimately, we can emulate all of that.\n\n35:17.480 --> 35:19.800\n Are you excited by that future?\n\n35:19.800 --> 35:21.560\n Do you worry about that future?\n\n35:23.480 --> 35:25.120\n I have certain worries about the future,\n\n35:25.120 --> 35:27.600\n but not virtual touch.\n\n35:27.600 --> 35:31.520\n Well, I agree with you.\n\n35:31.520 --> 35:33.480\n You described six stages\n\n35:33.480 --> 35:36.600\n in the evolution of information processing in the universe,\n\n35:36.600 --> 35:39.440\n as you started to describe.\n\n35:39.440 --> 35:42.760\n Can you maybe talk through some of those stages\n\n35:42.760 --> 35:46.320\n from the physics and chemistry to DNA and brains,\n\n35:46.320 --> 35:48.800\n and then to the very end,\n\n35:48.800 --> 35:52.000\n to the very beautiful end of this process?\n\n35:52.000 --> 35:54.120\n It actually gets more rapid.\n\n35:54.120 --> 35:57.720\n So physics and chemistry, that's how we started.\n\n35:59.720 --> 36:02.120\n So the very beginning of the universe.\n\n36:02.120 --> 36:05.920\n We had lots of electrons and various things traveling around.\n\n36:07.240 --> 36:11.480\n And that took actually many billions of years,\n\n36:11.480 --> 36:14.840\n kind of jumping ahead here to kind of\n\n36:14.840 --> 36:16.840\n some of the last stages where we have things\n\n36:16.840 --> 36:19.240\n like love and creativity.\n\n36:19.240 --> 36:21.760\n It's really quite remarkable that that happens.\n\n36:21.760 --> 36:26.760\n But finally, physics and chemistry created biology and DNA.\n\n36:29.920 --> 36:33.440\n And now you had actually one type of molecule\n\n36:33.440 --> 36:37.000\n that described the cutting edge of this process.\n\n36:38.680 --> 36:42.600\n And we go from physics and chemistry to biology.\n\n36:44.440 --> 36:47.080\n And finally, biology created brains.\n\n36:48.120 --> 36:51.440\n I mean, not everything that's created by biology\n\n36:51.440 --> 36:56.440\n has a brain, but eventually brains came along.\n\n36:56.440 --> 36:58.840\n And all of this is happening faster and faster.\n\n36:58.840 --> 37:00.320\n Yeah.\n\n37:00.320 --> 37:04.480\n It created increasingly complex organisms.\n\n37:04.480 --> 37:08.400\n Another key thing is actually not just brains,\n\n37:08.400 --> 37:09.960\n but our thumb.\n\n37:12.880 --> 37:15.880\n Because there's a lot of animals\n\n37:15.880 --> 37:18.080\n with brains even bigger than humans.\n\n37:18.080 --> 37:21.480\n I mean, elephants have a bigger brain.\n\n37:21.480 --> 37:22.920\n Whales have a bigger brain.\n\n37:24.160 --> 37:26.080\n But they've not created technology\n\n37:27.080 --> 37:28.680\n because they don't have a thumb.\n\n37:29.800 --> 37:32.280\n So that's one of the really key elements\n\n37:32.280 --> 37:34.120\n in the evolution of humans.\n\n37:34.120 --> 37:37.920\n This physical manipulator device\n\n37:37.920 --> 37:41.320\n that's useful for puzzle solving in the physical reality.\n\n37:41.320 --> 37:43.600\n So I could think, I could look at a tree and go,\n\n37:43.600 --> 37:46.240\n oh, I could actually trip that branch down\n\n37:46.240 --> 37:49.920\n and eliminate the leaves and carve a tip on it\n\n37:49.920 --> 37:51.800\n and I would create technology.\n\n37:53.000 --> 37:56.640\n And you can't do that if you don't have a thumb.\n\n37:56.640 --> 37:57.480\n Yeah.\n\n37:59.160 --> 38:04.160\n So thumbs then created technology\n\n38:04.480 --> 38:08.040\n and technology also had a memory.\n\n38:08.040 --> 38:10.000\n And now those memories are competing\n\n38:10.000 --> 38:15.000\n with the scale and scope of human beings.\n\n38:15.000 --> 38:17.040\n And ultimately we'll go beyond it.\n\n38:18.160 --> 38:22.440\n And then we're gonna merge human technology\n\n38:22.440 --> 38:27.440\n with human intelligence\n\n38:27.520 --> 38:30.960\n and understand how human intelligence works,\n\n38:30.960 --> 38:33.280\n which I think we already do.\n\n38:33.280 --> 38:37.920\n And we're putting that into our human technology.\n\n38:39.360 --> 38:43.120\n So create the technology inspired by our own intelligence\n\n38:43.120 --> 38:45.360\n and then that technology supersedes us\n\n38:45.360 --> 38:47.200\n in terms of its capabilities.\n\n38:47.200 --> 38:48.640\n And we ride along.\n\n38:48.640 --> 38:50.400\n Or do you ultimately see it as...\n\n38:50.400 --> 38:52.720\n And we ride along, but a lot of people don't see that.\n\n38:52.720 --> 38:56.200\n They say, well, you've got humans and you've got machines\n\n38:56.200 --> 38:59.280\n and there's no way we can ultimately compete with humans.\n\n39:00.280 --> 39:02.200\n And you can already see that.\n\n39:02.200 --> 39:07.000\n Lee Soudal, who's like the best Go player in the world,\n\n39:07.000 --> 39:09.040\n says he's not gonna play Go anymore.\n\n39:10.000 --> 39:12.880\n Because playing Go for a human,\n\n39:12.880 --> 39:14.920\n that was like the ultimate in intelligence\n\n39:14.920 --> 39:16.600\n because no one else could do that.\n\n39:18.200 --> 39:22.400\n But now a machine can actually go way beyond him.\n\n39:22.400 --> 39:25.080\n And so he says, well, there's no point playing it anymore.\n\n39:25.080 --> 39:28.880\n That may be more true for games than it is for life.\n\n39:30.080 --> 39:31.360\n I think there's a lot of benefit\n\n39:31.360 --> 39:34.440\n to working together with AI in regular life.\n\n39:34.440 --> 39:37.920\n So if you were to put a probability on it,\n\n39:37.920 --> 39:41.240\n is it more likely that we merge with AI\n\n39:41.240 --> 39:43.560\n or AI replaces us?\n\n39:43.560 --> 39:47.400\n A lot of people just think computers come along\n\n39:47.400 --> 39:48.320\n and they compete with them.\n\n39:48.320 --> 39:50.880\n We can't really compete and that's the end of it.\n\n39:52.320 --> 39:57.200\n As opposed to them increasing our abilities.\n\n39:57.200 --> 39:59.760\n And if you look at most technology,\n\n39:59.760 --> 40:01.920\n it increases our abilities.\n\n40:04.480 --> 40:06.240\n I mean, look at the history of work.\n\n40:07.920 --> 40:11.120\n Look at what people did 100 years ago.\n\n40:11.120 --> 40:13.000\n Does any of that exist anymore?\n\n40:13.000 --> 40:16.560\n People, I mean, if you were to predict\n\n40:16.560 --> 40:19.440\n that all of these jobs would go away\n\n40:19.440 --> 40:21.000\n and would be done by machines,\n\n40:21.000 --> 40:22.760\n people would say, well, there's gonna be,\n\n40:22.760 --> 40:24.040\n no one's gonna have jobs\n\n40:24.040 --> 40:26.800\n and it's gonna be massive unemployment.\n\n40:29.480 --> 40:32.480\n But I show in this book that's coming out\n\n40:34.120 --> 40:36.760\n the amount of people that are working,\n\n40:36.760 --> 40:41.640\n even as a percentage of the population has gone way up.\n\n40:41.640 --> 40:46.160\n We're looking at the x axis year from 1774 to 2024\n\n40:46.160 --> 40:49.520\n and on the y axis, personal income per capita\n\n40:49.520 --> 40:52.720\n in constant dollars and it's growing super linearly.\n\n40:52.720 --> 40:57.720\n I mean, it's 2021 constant dollars and it's gone way up.\n\n40:58.040 --> 40:59.840\n That's not what you would predict\n\n41:00.760 --> 41:01.920\n given that we would predict\n\n41:01.920 --> 41:03.760\n that all these jobs would go away.\n\n41:03.760 --> 41:07.000\n But the reason it's gone up is because\n\n41:07.000 --> 41:09.880\n we've basically enhanced our own capabilities\n\n41:09.880 --> 41:11.280\n by using these machines\n\n41:11.280 --> 41:13.400\n as opposed to them just competing with us.\n\n41:14.280 --> 41:16.280\n That's a key way in which we're gonna be able\n\n41:16.280 --> 41:18.680\n to become far smarter than we are now\n\n41:18.680 --> 41:23.200\n by increasing the number of different parameters\n\n41:23.200 --> 41:25.640\n we can consider in making a decision.\n\n41:26.480 --> 41:28.640\n I was very fortunate, I am very fortunate\n\n41:28.640 --> 41:31.480\n to be able to get a glimpse preview\n\n41:31.480 --> 41:36.480\n of your upcoming book, Singularity is Nearer.\n\n41:37.320 --> 41:41.920\n And one of the themes outside of just discussing\n\n41:41.920 --> 41:44.760\n the increasing exponential growth of technology,\n\n41:44.760 --> 41:48.480\n one of the themes is that things are getting better\n\n41:48.480 --> 41:50.800\n in all aspects of life.\n\n41:50.800 --> 41:53.720\n And you talked just about this.\n\n41:53.720 --> 41:55.640\n So one of the things you're saying is with jobs.\n\n41:55.640 --> 41:57.840\n So let me just ask about that.\n\n41:57.840 --> 42:01.040\n There is a big concern that automation,\n\n42:01.040 --> 42:06.040\n especially powerful AI, will get rid of jobs.\n\n42:06.400 --> 42:07.880\n There are people who lose jobs.\n\n42:07.880 --> 42:10.960\n And as you were saying, the sense is\n\n42:10.960 --> 42:14.000\n throughout the history of the 20th century,\n\n42:14.000 --> 42:16.640\n automation did not do that ultimately.\n\n42:16.640 --> 42:20.600\n And so the question is, will this time be different?\n\n42:20.600 --> 42:22.560\n Right, that is the question.\n\n42:22.560 --> 42:24.480\n Will this time be different?\n\n42:24.480 --> 42:26.360\n And it really has to do with how quickly\n\n42:26.360 --> 42:29.120\n we can merge with this type of intelligence.\n\n42:29.120 --> 42:34.120\n Whether Lambda or GPT3 is out there,\n\n42:34.920 --> 42:38.640\n and maybe it's overcome some of its key problems,\n\n42:40.200 --> 42:43.480\n and we really haven't enhanced human intelligence,\n\n42:43.480 --> 42:45.640\n that might be a negative scenario.\n\n42:49.600 --> 42:53.160\n But I mean, that's why we create technologies,\n\n42:53.160 --> 42:54.640\n to enhance ourselves.\n\n42:56.280 --> 42:58.800\n And I believe we will be enhanced\n\n42:58.800 --> 43:00.720\n when I'm just going to sit here with\n\n43:03.000 --> 43:08.000\n 300 million modules in our neocortex.\n\n43:09.040 --> 43:11.080\n We're going to be able to go beyond that.\n\n43:14.000 --> 43:18.360\n Because that's useful, but we can multiply that by 10,\n\n43:19.640 --> 43:22.240\n 100, 1,000, a million.\n\n43:22.240 --> 43:27.240\n And you might think, well, what's the point of doing that?\n\n43:30.240 --> 43:33.920\n It's like asking somebody that's never heard music,\n\n43:33.920 --> 43:36.600\n well, what's the value of music?\n\n43:36.600 --> 43:39.920\n I mean, you can't appreciate it until you've created it.\n\n43:41.360 --> 43:45.320\n There's some worry that there'll be a wealth disparity.\n\n43:46.880 --> 43:50.240\n Class or wealth disparity, only the rich people\n\n43:50.240 --> 43:53.120\n will be, basically, the rich people\n\n43:53.120 --> 43:55.520\n will first have access to this kind of thing,\n\n43:55.520 --> 43:58.000\n and then because of this kind of thing,\n\n43:58.000 --> 43:59.480\n because the ability to merge\n\n43:59.480 --> 44:02.680\n will get richer exponentially faster.\n\n44:02.680 --> 44:05.120\n And I say that's just like cell phones.\n\n44:06.280 --> 44:08.080\n I mean, there's like four billion cell phones\n\n44:08.080 --> 44:10.320\n in the world today.\n\n44:10.320 --> 44:13.320\n In fact, when cell phones first came out,\n\n44:13.320 --> 44:14.840\n you had to be fairly wealthy.\n\n44:14.840 --> 44:17.520\n They weren't very inexpensive.\n\n44:17.520 --> 44:20.160\n So you had to have some wealth in order to afford them.\n\n44:20.160 --> 44:22.760\n Yeah, there were these big, sexy phones.\n\n44:22.760 --> 44:24.080\n And they didn't work very well.\n\n44:24.080 --> 44:25.640\n They did almost nothing.\n\n44:26.480 --> 44:31.240\n So you can only afford these things if you're wealthy\n\n44:31.240 --> 44:34.040\n at a point where they really don't work very well.\n\n44:35.760 --> 44:39.880\n So achieving scale and making it inexpensive\n\n44:39.880 --> 44:42.240\n is part of making the thing work well.\n\n44:42.240 --> 44:43.560\n Exactly.\n\n44:43.560 --> 44:46.980\n So these are not totally cheap, but they're pretty cheap.\n\n44:46.980 --> 44:51.980\n I mean, you can get them for a few hundred dollars.\n\n44:52.140 --> 44:55.400\n Especially given the kind of things it provides for you.\n\n44:55.400 --> 44:57.100\n There's a lot of people in the third world\n\n44:57.100 --> 45:00.380\n that have very little, but they have a smartphone.\n\n45:00.380 --> 45:01.980\n Yeah, absolutely.\n\n45:01.980 --> 45:03.820\n And the same will be true with AI.\n\n45:03.820 --> 45:07.640\n I mean, I see homeless people have their own cell phones.\n\n45:07.640 --> 45:12.120\n Yeah, so your sense is any kind of advanced technology\n\n45:12.120 --> 45:13.740\n will take the same trajectory.\n\n45:13.740 --> 45:17.700\n Right, it ultimately becomes cheap and will be affordable.\n\n45:19.180 --> 45:21.060\n I probably would not be the first person\n\n45:21.060 --> 45:26.060\n to put something in my brain to connect to computers\n\n45:28.220 --> 45:30.240\n because I think it will have limitations.\n\n45:30.240 --> 45:33.180\n But once it's really perfected,\n\n45:34.140 --> 45:36.420\n and at that point it'll be pretty inexpensive,\n\n45:36.420 --> 45:38.260\n I think it'll be pretty affordable.\n\n45:39.660 --> 45:43.080\n So in which other ways, as you outline your book,\n\n45:43.080 --> 45:44.460\n is life getting better?\n\n45:44.460 --> 45:45.340\n Because I think...\n\n45:45.340 --> 45:49.220\n Well, I mean, I have 50 charts in there\n\n45:49.220 --> 45:51.780\n where everything is getting better.\n\n45:51.780 --> 45:54.140\n I think there's a kind of cynicism about,\n\n45:55.500 --> 45:58.020\n like even if you look at extreme poverty, for example.\n\n45:58.020 --> 46:00.860\n For example, this is actually a poll\n\n46:00.860 --> 46:05.500\n taken on extreme poverty, and people were asked,\n\n46:05.500 --> 46:08.320\n has poverty gotten better or worse?\n\n46:08.320 --> 46:11.180\n And the options are increased by 50%,\n\n46:11.180 --> 46:13.940\n increased by 25%, remain the same,\n\n46:13.940 --> 46:16.740\n decreased by 25%, decreased by 50%.\n\n46:16.740 --> 46:18.780\n If you're watching this or listening to this,\n\n46:18.780 --> 46:21.500\n try to vote for yourself.\n\n46:21.500 --> 46:24.200\n 70% thought it had gotten worse,\n\n46:24.200 --> 46:27.100\n and that's the general impression.\n\n46:27.100 --> 46:31.600\n 88% thought it had gotten worse or remained the same.\n\n46:32.500 --> 46:35.700\n Only 1% thought it decreased by 50%,\n\n46:35.700 --> 46:37.620\n and that is the answer.\n\n46:37.620 --> 46:39.540\n It actually decreased by 50%.\n\n46:39.540 --> 46:43.660\n So only 1% of people got the right optimistic estimate\n\n46:43.660 --> 46:45.260\n of how poverty is.\n\n46:45.260 --> 46:47.520\n Right, and this is the reality,\n\n46:47.520 --> 46:51.140\n and it's true of almost everything you look at.\n\n46:51.140 --> 46:54.780\n You don't wanna go back 100 years or 50 years.\n\n46:54.780 --> 46:56.980\n Things were quite miserable then,\n\n46:56.980 --> 47:01.020\n but we tend not to remember that.\n\n47:01.020 --> 47:05.340\n So literacy rate increasing over the past few centuries\n\n47:05.340 --> 47:07.940\n across all the different nations,\n\n47:07.940 --> 47:11.880\n nearly to 100% across many of the nations in the world.\n\n47:11.880 --> 47:12.820\n It's gone way up.\n\n47:12.820 --> 47:15.620\n Average years of education have gone way up.\n\n47:15.620 --> 47:18.560\n Life expectancy is also increasing.\n\n47:18.560 --> 47:23.560\n Life expectancy was 48 in 1900.\n\n47:24.380 --> 47:26.400\n And it's over 80 now.\n\n47:26.400 --> 47:28.140\n And it's gonna continue to go up,\n\n47:28.140 --> 47:30.940\n particularly as we get into more advanced stages\n\n47:30.940 --> 47:33.380\n of simulated biology.\n\n47:33.380 --> 47:35.580\n For life expectancy, these trends are the same\n\n47:35.580 --> 47:37.940\n for at birth, age one, age five, age 10,\n\n47:37.940 --> 47:40.340\n so it's not just the infant mortality.\n\n47:40.340 --> 47:42.620\n And I have 50 more graphs in the book\n\n47:42.620 --> 47:44.580\n about all kinds of things.\n\n47:46.120 --> 47:48.340\n Even spread of democracy,\n\n47:48.340 --> 47:52.740\n which might bring up some sort of controversial issues,\n\n47:52.740 --> 47:55.140\n it still has gone way up.\n\n47:55.140 --> 47:57.260\n Well, that one has gone way up,\n\n47:57.260 --> 47:59.500\n but that one is a bumpy road, right?\n\n47:59.500 --> 48:03.220\n Exactly, and somebody might represent democracy\n\n48:03.220 --> 48:08.220\n and go backwards, but we basically had no democracies\n\n48:08.460 --> 48:10.980\n before the creation of the United States,\n\n48:10.980 --> 48:13.860\n which was a little over two centuries ago,\n\n48:13.860 --> 48:16.460\n which in the scale of human history isn't that long.\n\n48:17.460 --> 48:19.860\n Do you think superintelligence systems will help\n\n48:21.460 --> 48:22.580\n with democracy?\n\n48:23.620 --> 48:25.020\n So what is democracy?\n\n48:25.020 --> 48:29.660\n Democracy is giving a voice to the populace\n\n48:29.660 --> 48:33.700\n and having their ideas, having their beliefs,\n\n48:33.700 --> 48:38.180\n having their views represented.\n\n48:38.180 --> 48:39.440\n Well, I hope so.\n\n48:41.260 --> 48:44.060\n I mean, we've seen social networks\n\n48:44.060 --> 48:47.720\n can spread conspiracy theories,\n\n48:49.180 --> 48:51.340\n which have been quite negative,\n\n48:51.340 --> 48:55.500\n being, for example, being against any kind of stuff\n\n48:55.500 --> 48:58.340\n that would help your health.\n\n48:58.340 --> 49:01.520\n So those kinds of ideas have,\n\n49:03.100 --> 49:06.540\n on social media, what you notice is they increase\n\n49:06.540 --> 49:10.340\n engagement, so dramatic division increases engagement.\n\n49:10.340 --> 49:13.460\n Do you worry about AI systems that will learn\n\n49:13.460 --> 49:15.140\n to maximize that division?\n\n49:17.020 --> 49:20.360\n I mean, I do have some concerns about this,\n\n49:22.040 --> 49:25.740\n and I have a chapter in the book about the perils\n\n49:25.740 --> 49:30.740\n of advanced AI, spreading misinformation\n\n49:32.380 --> 49:34.080\n on social networks is one of them,\n\n49:34.080 --> 49:36.780\n but there are many others.\n\n49:36.780 --> 49:38.780\n What's the one that worries you the most\n\n49:40.640 --> 49:42.900\n that we should think about to try to avoid?\n\n49:47.260 --> 49:49.000\n Well, it's hard to choose.\n\n49:50.820 --> 49:55.340\n We do have the nuclear power that evolved\n\n49:55.340 --> 49:57.660\n when I was a child, I remember,\n\n49:57.660 --> 50:02.660\n and we would actually do these drills against a nuclear war.\n\n50:03.580 --> 50:07.620\n We'd get under our desks and put our hands behind our heads\n\n50:07.620 --> 50:10.120\n to protect us from a nuclear war.\n\n50:11.140 --> 50:13.300\n Seems to work, we're still around, so.\n\n50:15.540 --> 50:17.060\n You're protected.\n\n50:17.060 --> 50:20.080\n But that's still a concern.\n\n50:20.080 --> 50:22.860\n And there are key dangerous situations\n\n50:22.860 --> 50:26.260\n that can take place in biology.\n\n50:27.140 --> 50:32.140\n Someone could create a virus that's very,\n\n50:33.340 --> 50:36.340\n I mean, we have viruses that are hard to spread,\n\n50:40.560 --> 50:42.800\n and they can be very dangerous,\n\n50:42.800 --> 50:46.160\n and we have viruses that are easy to spread,\n\n50:46.160 --> 50:47.800\n but they're not so dangerous.\n\n50:47.800 --> 50:51.600\n Somebody could create something\n\n50:51.600 --> 50:55.580\n that would be very easy to spread and very dangerous,\n\n50:55.580 --> 50:57.400\n and be very hard to stop.\n\n50:58.960 --> 51:02.040\n It could be something that would spread\n\n51:02.040 --> 51:04.640\n without people noticing, because people could get it,\n\n51:04.640 --> 51:08.320\n they'd have no symptoms, and then everybody would get it,\n\n51:08.320 --> 51:11.800\n and then symptoms would occur maybe a month later.\n\n51:11.800 --> 51:16.800\n So I mean, and that actually doesn't occur normally,\n\n51:18.760 --> 51:23.760\n because if we were to have a problem with that,\n\n51:24.680 --> 51:26.920\n we wouldn't exist.\n\n51:26.920 --> 51:30.720\n So the fact that humans exist means that we don't have\n\n51:30.720 --> 51:35.040\n viruses that can spread easily and kill us,\n\n51:35.040 --> 51:37.540\n because otherwise we wouldn't exist.\n\n51:37.540 --> 51:39.080\n Yeah, viruses don't wanna do that.\n\n51:39.080 --> 51:44.080\n They want to spread and keep the host alive somewhat.\n\n51:44.080 --> 51:47.240\n So you can describe various dangers with biology.\n\n51:48.620 --> 51:53.520\n Also nanotechnology, which we actually haven't experienced\n\n51:53.520 --> 51:56.040\n yet, but there are people that are creating nanotechnology,\n\n51:56.040 --> 51:57.960\n and I describe that in the book.\n\n51:57.960 --> 52:00.880\n Now you're excited by the possibilities of nanotechnology,\n\n52:00.880 --> 52:04.920\n of nanobots, of being able to do things inside our body,\n\n52:04.920 --> 52:07.520\n inside our mind, that's going to help.\n\n52:07.520 --> 52:10.880\n What's exciting, what's terrifying about nanobots?\n\n52:10.880 --> 52:13.920\n What's exciting is that that's a way to communicate\n\n52:13.920 --> 52:18.920\n with our neocortex, because each neocortex is pretty small\n\n52:19.000 --> 52:22.360\n and you need a small entity that can actually get in there\n\n52:22.360 --> 52:25.440\n and establish a communication channel.\n\n52:25.440 --> 52:30.320\n And that's gonna really be necessary to connect our brains\n\n52:30.320 --> 52:35.320\n to AI within ourselves, because otherwise it would be hard\n\n52:35.320 --> 52:38.720\n for us to compete with it.\n\n52:38.720 --> 52:40.240\n In a high bandwidth way.\n\n52:40.240 --> 52:41.760\n Yeah, yeah.\n\n52:41.760 --> 52:45.720\n And that's key, actually, because a lot of the things\n\n52:45.720 --> 52:48.980\n like Neuralink are really not high bandwidth yet.\n\n52:49.880 --> 52:52.680\n So nanobots is the way you achieve high bandwidth.\n\n52:52.680 --> 52:55.880\n How much intelligence would those nanobots have?\n\n52:55.880 --> 53:00.320\n Yeah, they don't need a lot, just enough to basically\n\n53:00.320 --> 53:04.400\n establish a communication channel to one nanobot.\n\n53:04.400 --> 53:06.720\n So it's primarily about communication.\n\n53:06.720 --> 53:07.560\n Yeah.\n\n53:07.560 --> 53:09.880\n Between external computing devices\n\n53:09.880 --> 53:14.120\n and our biological thinking machine.\n\n53:15.020 --> 53:17.040\n What worries you about nanobots?\n\n53:17.040 --> 53:19.840\n Is it similar to with the viruses?\n\n53:19.840 --> 53:22.720\n Well, I mean, it's the great goo challenge.\n\n53:22.720 --> 53:23.560\n Yes.\n\n53:24.920 --> 53:29.920\n If you had a nanobot that wanted to create\n\n53:29.920 --> 53:34.920\n any kind of entity and repeat itself,\n\n53:37.520 --> 53:41.520\n and was able to operate in a natural environment,\n\n53:41.520 --> 53:45.240\n it could turn everything into that entity\n\n53:45.240 --> 53:50.240\n and basically destroy all biological life.\n\n53:52.000 --> 53:54.600\n So you mentioned nuclear weapons.\n\n53:54.600 --> 53:55.440\n Yeah.\n\n53:55.440 --> 54:00.440\n I'd love to hear your opinion about the 21st century\n\n54:01.840 --> 54:05.320\n and whether you think we might destroy ourselves.\n\n54:05.320 --> 54:08.840\n And maybe your opinion, if it has changed\n\n54:08.840 --> 54:11.760\n by looking at what's going on in Ukraine,\n\n54:11.760 --> 54:16.760\n that we could have a hot war with nuclear powers involved\n\n54:18.980 --> 54:23.320\n and the tensions building and the seeming forgetting\n\n54:23.320 --> 54:27.460\n of how terrifying and destructive nuclear weapons are.\n\n54:29.340 --> 54:32.940\n Do you think humans might destroy ourselves\n\n54:32.940 --> 54:36.220\n in the 21st century, and if we do, how?\n\n54:36.220 --> 54:37.500\n And how do we avoid it?\n\n54:38.540 --> 54:41.100\n I don't think that's gonna happen\n\n54:41.100 --> 54:45.180\n despite the terrors of that war.\n\n54:45.180 --> 54:50.180\n It is a possibility, but I mean, I don't.\n\n54:50.420 --> 54:52.700\n It's unlikely in your mind.\n\n54:52.700 --> 54:55.380\n Yeah, even with the tensions we've had\n\n54:55.380 --> 54:59.860\n with this one nuclear power plant that's been taken over,\n\n55:02.340 --> 55:07.340\n it's very tense, but I don't actually see a lot of people\n\n55:07.580 --> 55:10.220\n worrying that that's gonna happen.\n\n55:10.220 --> 55:11.900\n I think we'll avoid that.\n\n55:11.900 --> 55:15.940\n We had two nuclear bombs go off in 45,\n\n55:15.940 --> 55:20.860\n so now we're 77 years later.\n\n55:20.860 --> 55:22.400\n Yeah, we're doing pretty good.\n\n55:22.400 --> 55:27.100\n We've never had another one go off through anger.\n\n55:27.100 --> 55:31.020\n People forget the lessons of history.\n\n55:31.020 --> 55:33.540\n Well, yeah, I mean, I am worried about it.\n\n55:33.540 --> 55:37.460\n I mean, that is definitely a challenge.\n\n55:37.460 --> 55:40.620\n But you believe that we'll make it out\n\n55:40.620 --> 55:44.600\n and ultimately superintelligent AI will help us make it out\n\n55:44.600 --> 55:47.680\n as opposed to destroy us.\n\n55:47.680 --> 55:52.420\n I think so, but we do have to be mindful of these dangers.\n\n55:52.420 --> 55:56.340\n And there are other dangers besides nuclear weapons, so.\n\n55:56.340 --> 56:01.060\n So to get back to merging with AI,\n\n56:01.060 --> 56:03.740\n will we be able to upload our mind in a computer\n\n56:06.020 --> 56:09.380\n in a way where we might even transcend\n\n56:09.380 --> 56:11.620\n the constraints of our bodies?\n\n56:11.620 --> 56:15.300\n So copy our mind into a computer and leave the body behind?\n\n56:15.300 --> 56:20.300\n Let me describe one thing I've already done with my father.\n\n56:21.060 --> 56:22.200\n That's a great story.\n\n56:23.700 --> 56:26.660\n So we created a technology, this is public,\n\n56:26.660 --> 56:30.140\n came out, I think, six years ago,\n\n56:30.140 --> 56:33.740\n where you could ask any question\n\n56:33.740 --> 56:35.220\n and the release product,\n\n56:35.220 --> 56:37.620\n which I think is still on the market,\n\n56:37.620 --> 56:40.900\n it would read 200,000 books.\n\n56:40.900 --> 56:45.900\n And then find the one sentence in 200,000 books\n\n56:46.140 --> 56:48.220\n that best answered your question.\n\n56:49.860 --> 56:51.180\n And it's actually quite interesting.\n\n56:51.180 --> 56:52.740\n You can ask all kinds of questions\n\n56:52.740 --> 56:56.240\n and you get the best answer in 200,000 books.\n\n56:57.580 --> 56:59.940\n But I was also able to take it\n\n56:59.940 --> 57:03.180\n and not go through 200,000 books,\n\n57:03.180 --> 57:07.060\n but go through a book that I put together,\n\n57:07.060 --> 57:10.980\n which is basically everything my father had written.\n\n57:10.980 --> 57:14.660\n So everything he had written, I had gathered,\n\n57:14.660 --> 57:16.100\n and we created a book,\n\n57:17.100 --> 57:20.220\n everything that Frederick Herzog had written.\n\n57:20.220 --> 57:23.380\n Now, I didn't think this actually would work that well\n\n57:23.380 --> 57:28.380\n because stuff he had written was stuff about how to lay out.\n\n57:30.900 --> 57:35.900\n I mean, he directed choral groups\n\n57:35.900 --> 57:39.220\n and music groups,\n\n57:39.220 --> 57:44.180\n and he would be laying out how the people should,\n\n57:44.180 --> 57:49.180\n where they should sit and how to fund this\n\n57:49.660 --> 57:52.220\n and all kinds of things\n\n57:52.220 --> 57:55.140\n that really didn't seem that interesting.\n\n57:57.620 --> 57:59.900\n And yet, when you ask a question,\n\n57:59.900 --> 58:00.820\n it would go through it\n\n58:00.820 --> 58:04.760\n and it would actually give you a very good answer.\n\n58:04.760 --> 58:07.860\n So I said, well, who's the most interesting composer?\n\n58:07.860 --> 58:09.500\n And he said, well, definitely Brahms.\n\n58:09.500 --> 58:13.220\n And he would go on about how Brahms was fabulous\n\n58:13.220 --> 58:17.140\n and talk about the importance of music education.\n\n58:18.020 --> 58:21.140\n So you could have essentially a question and answer,\n\n58:21.140 --> 58:21.980\n a conversation with him.\n\n58:21.980 --> 58:23.020\n You could have a conversation with him,\n\n58:23.020 --> 58:25.940\n which was actually more interesting than talking to him\n\n58:25.940 --> 58:27.020\n because if you talked to him,\n\n58:27.020 --> 58:30.080\n he'd be concerned about how they're gonna lay out\n\n58:30.080 --> 58:34.220\n this property to give a choral group.\n\n58:34.220 --> 58:36.060\n He'd be concerned about the day to day\n\n58:36.060 --> 58:37.300\n versus the big questions.\n\n58:37.300 --> 58:39.060\n Exactly, yeah.\n\n58:39.060 --> 58:41.620\n And you did ask about the meaning of life\n\n58:41.620 --> 58:43.260\n and he answered, love.\n\n58:43.260 --> 58:44.100\n Yeah.\n\n58:46.460 --> 58:47.300\n Do you miss him?\n\n58:49.180 --> 58:50.260\n Yes, I do.\n\n58:52.940 --> 58:57.940\n Yeah, you get used to missing somebody after 52 years,\n\n58:58.540 --> 59:02.700\n and I didn't really have intelligent conversations with him\n\n59:02.700 --> 59:04.500\n until later in life.\n\n59:06.940 --> 59:08.780\n In the last few years, he was sick,\n\n59:08.780 --> 59:10.140\n which meant he was home a lot\n\n59:10.140 --> 59:11.900\n and I was actually able to talk to him\n\n59:11.900 --> 59:15.780\n about different things like music and other things.\n\n59:15.780 --> 59:19.820\n And so I miss that very much.\n\n59:19.820 --> 59:22.220\n What did you learn about life from your father?\n\n59:25.180 --> 59:27.820\n What part of him is with you now?\n\n59:29.020 --> 59:31.580\n He was devoted to music.\n\n59:31.580 --> 59:33.860\n And when he would create something to music,\n\n59:33.860 --> 59:35.460\n it put him in a different world.\n\n59:37.540 --> 59:39.680\n Otherwise, he was very shy.\n\n59:42.520 --> 59:43.780\n And if people got together,\n\n59:43.780 --> 59:47.020\n he tended not to interact with people\n\n59:47.020 --> 59:48.580\n just because of his shyness.\n\n59:49.780 --> 59:54.440\n But when he created music, he was like a different person.\n\n59:55.340 --> 59:56.540\n Do you have that in you?\n\n59:56.540 --> 59:59.840\n That kind of light that shines?\n\n59:59.840 --> 1:00:04.840\n I mean, I got involved with technology at like age five.\n\n1:00:06.620 --> 1:00:07.620\n And you fell in love with it\n\n1:00:07.620 --> 1:00:09.380\n in the same way he did with music?\n\n1:00:09.380 --> 1:00:11.300\n Yeah, yeah.\n\n1:00:11.300 --> 1:00:15.900\n I remember this actually happened with my grandmother.\n\n1:00:16.940 --> 1:00:20.060\n She had a manual typewriter\n\n1:00:20.060 --> 1:00:23.060\n and she wrote a book, One Life Is Not Enough,\n\n1:00:23.060 --> 1:00:26.240\n which actually a good title for a book I might write,\n\n1:00:26.240 --> 1:00:30.260\n but it was about a school she had created.\n\n1:00:30.260 --> 1:00:33.800\n Well, actually her mother created it.\n\n1:00:33.800 --> 1:00:38.300\n So my mother's mother's mother created the school in 1868.\n\n1:00:38.300 --> 1:00:40.620\n And it was the first school in Europe\n\n1:00:40.620 --> 1:00:42.660\n that provided higher education for girls.\n\n1:00:42.660 --> 1:00:44.420\n It went through 14th grade.\n\n1:00:45.620 --> 1:00:48.260\n If you were a girl and you were lucky enough\n\n1:00:48.260 --> 1:00:50.700\n to get an education at all,\n\n1:00:50.700 --> 1:00:52.940\n it would go through like ninth grade.\n\n1:00:52.940 --> 1:00:56.660\n And many people didn't have any education as a girl.\n\n1:00:56.660 --> 1:00:58.320\n This went through 14th grade.\n\n1:01:00.620 --> 1:01:04.060\n Her mother created it, she took it over,\n\n1:01:04.060 --> 1:01:09.060\n and the book was about the history of the school\n\n1:01:09.420 --> 1:01:11.120\n and her involvement with it.\n\n1:01:12.980 --> 1:01:14.020\n When she presented it to me,\n\n1:01:14.020 --> 1:01:19.020\n I was not so interested in the story of the school,\n\n1:01:19.020 --> 1:01:24.020\n but I was totally amazed with this manual typewriter.\n\n1:01:25.400 --> 1:01:27.860\n I mean, here is something you could put a blank piece\n\n1:01:27.860 --> 1:01:31.080\n of paper into and you could turn it into something\n\n1:01:31.080 --> 1:01:33.780\n that looked like it came from a book.\n\n1:01:33.780 --> 1:01:34.620\n And you can actually type on it\n\n1:01:34.620 --> 1:01:36.440\n and it looked like it came from a book.\n\n1:01:36.440 --> 1:01:38.180\n It was just amazing to me.\n\n1:01:39.120 --> 1:01:41.740\n And I could see actually how it worked.\n\n1:01:42.720 --> 1:01:44.840\n And I was also interested in magic.\n\n1:01:44.840 --> 1:01:49.840\n But in magic, if somebody actually knows how it works,\n\n1:01:50.420 --> 1:01:52.540\n the magic goes away.\n\n1:01:52.540 --> 1:01:53.780\n The magic doesn't stay there\n\n1:01:53.780 --> 1:01:56.600\n if you actually understand how it works.\n\n1:01:56.600 --> 1:01:57.820\n But here was technology.\n\n1:01:57.820 --> 1:02:01.020\n I didn't have that word when I was five or six.\n\n1:02:01.020 --> 1:02:02.660\n And the magic was still there for you?\n\n1:02:02.660 --> 1:02:05.740\n The magic was still there, even if you knew how it worked.\n\n1:02:06.900 --> 1:02:08.780\n So I became totally interested in this\n\n1:02:08.780 --> 1:02:12.580\n and then went around, collected little pieces\n\n1:02:12.580 --> 1:02:17.580\n of mechanical objects from bicycles, from broken radios.\n\n1:02:17.580 --> 1:02:19.340\n I would go through the neighborhood.\n\n1:02:20.500 --> 1:02:23.700\n This was an era where you would allow five or six year olds\n\n1:02:23.700 --> 1:02:26.340\n to run through the neighborhood and do this.\n\n1:02:26.340 --> 1:02:27.740\n We don't do that anymore.\n\n1:02:27.740 --> 1:02:30.700\n But I didn't know how to put them together.\n\n1:02:30.700 --> 1:02:32.220\n I said, if I could just figure out\n\n1:02:32.220 --> 1:02:36.140\n how to put these things together, I could solve any problem.\n\n1:02:37.340 --> 1:02:41.660\n And I actually remember talking to these very old girls.\n\n1:02:41.660 --> 1:02:42.780\n I think they were 10.\n\n1:02:45.340 --> 1:02:48.340\n And telling them, if I could just figure this out,\n\n1:02:48.340 --> 1:02:50.120\n we could fly, we could do anything.\n\n1:02:50.120 --> 1:02:53.580\n And they said, well, you have quite an imagination.\n\n1:02:56.220 --> 1:03:00.780\n And then when I was in third grade,\n\n1:03:00.780 --> 1:03:02.860\n so I was like eight,\n\n1:03:02.860 --> 1:03:05.900\n created like a virtual reality theater\n\n1:03:05.900 --> 1:03:07.780\n where people could come on stage\n\n1:03:07.780 --> 1:03:09.900\n and they could move their arms.\n\n1:03:09.900 --> 1:03:13.540\n And all of it was controlled through one control box.\n\n1:03:13.540 --> 1:03:15.800\n It was all done with mechanical technology.\n\n1:03:16.980 --> 1:03:19.800\n And it was a big hit in my third grade class.\n\n1:03:21.100 --> 1:03:22.980\n And then I went on to do things\n\n1:03:22.980 --> 1:03:24.900\n in junior high school science fairs\n\n1:03:24.900 --> 1:03:27.660\n and high school science fairs.\n\n1:03:27.660 --> 1:03:30.720\n I won the Westinghouse Science Talent Search.\n\n1:03:30.720 --> 1:03:33.940\n So I mean, I became committed to technology\n\n1:03:33.940 --> 1:03:37.460\n when I was five or six years old.\n\n1:03:37.460 --> 1:03:42.460\n You've talked about how you use lucid dreaming to think,\n\n1:03:43.100 --> 1:03:45.900\n to come up with ideas as a source of creativity.\n\n1:03:45.900 --> 1:03:49.360\n Because you maybe talk through that,\n\n1:03:49.360 --> 1:03:52.020\n maybe the process of how to,\n\n1:03:52.020 --> 1:03:54.060\n you've invented a lot of things.\n\n1:03:54.060 --> 1:03:55.620\n You've came up and thought through\n\n1:03:55.620 --> 1:03:57.180\n some very interesting ideas.\n\n1:03:58.100 --> 1:03:59.520\n What advice would you give,\n\n1:03:59.520 --> 1:04:03.420\n or can you speak to the process of thinking,\n\n1:04:03.420 --> 1:04:07.100\n of how to think, how to think creatively?\n\n1:04:07.100 --> 1:04:10.460\n Well, I mean, sometimes I will think through in a dream\n\n1:04:10.460 --> 1:04:12.340\n and try to interpret that.\n\n1:04:12.340 --> 1:04:17.340\n But I think the key issue that I would tell younger people\n\n1:04:22.220 --> 1:04:25.080\n is to put yourself in the position\n\n1:04:25.080 --> 1:04:28.860\n that what you're trying to create already exists.\n\n1:04:30.660 --> 1:04:33.580\n And then you're explaining, like...\n\n1:04:34.980 --> 1:04:35.820\n How it works.\n\n1:04:35.820 --> 1:04:38.220\n Exactly.\n\n1:04:38.220 --> 1:04:39.220\n That's really interesting.\n\n1:04:39.220 --> 1:04:42.780\n You paint a world that you would like to exist,\n\n1:04:42.780 --> 1:04:45.940\n you think it exists, and reverse engineer that.\n\n1:04:45.940 --> 1:04:47.900\n And then you actually imagine you're giving a speech\n\n1:04:47.900 --> 1:04:50.140\n about how you created this.\n\n1:04:50.140 --> 1:04:51.780\n Well, you'd have to then work backwards\n\n1:04:51.780 --> 1:04:56.780\n as to how you would create it in order to make it work.\n\n1:04:57.320 --> 1:04:58.160\n That's brilliant.\n\n1:04:58.160 --> 1:05:01.420\n And that requires some imagination too,\n\n1:05:01.420 --> 1:05:03.140\n some first principles thinking.\n\n1:05:03.140 --> 1:05:06.040\n You have to visualize that world.\n\n1:05:06.040 --> 1:05:07.720\n That's really interesting.\n\n1:05:07.720 --> 1:05:10.600\n And generally, when I talk about things\n\n1:05:10.600 --> 1:05:13.160\n we're trying to invent, I would use the present tense\n\n1:05:13.160 --> 1:05:14.700\n as if it already exists.\n\n1:05:15.880 --> 1:05:18.280\n Not just to give myself that confidence,\n\n1:05:18.280 --> 1:05:20.280\n but everybody else who's working on it.\n\n1:05:21.840 --> 1:05:26.640\n We just have to kind of do all the steps\n\n1:05:26.640 --> 1:05:31.040\n in order to make it actual.\n\n1:05:31.040 --> 1:05:33.500\n How much of a good idea is about timing?\n\n1:05:35.400 --> 1:05:37.040\n How much is it about your genius\n\n1:05:37.040 --> 1:05:40.000\n versus that its time has come?\n\n1:05:41.620 --> 1:05:42.920\n Timing's very important.\n\n1:05:42.920 --> 1:05:46.200\n I mean, that's really why I got into futurism.\n\n1:05:46.200 --> 1:05:50.500\n I didn't, I wasn't inherently a futurist.\n\n1:05:50.500 --> 1:05:52.620\n That was not really my goal.\n\n1:05:54.320 --> 1:05:57.400\n It's really to figure out when things are feasible.\n\n1:05:57.400 --> 1:06:00.800\n We see that now with large scale models.\n\n1:06:01.680 --> 1:06:06.400\n The very large scale models like GPT3,\n\n1:06:06.400 --> 1:06:08.200\n it started two years ago.\n\n1:06:09.600 --> 1:06:11.160\n Four years ago, it wasn't feasible.\n\n1:06:11.160 --> 1:06:16.160\n In fact, they did create GPT2, which didn't work.\n\n1:06:18.800 --> 1:06:22.360\n So it required a certain amount of timing\n\n1:06:22.360 --> 1:06:24.200\n having to do with this exponential growth\n\n1:06:24.200 --> 1:06:27.400\n of computing power.\n\n1:06:27.400 --> 1:06:31.240\n So futurism in some sense is a study of timing,\n\n1:06:31.240 --> 1:06:34.400\n trying to understand how the world will evolve\n\n1:06:34.400 --> 1:06:38.320\n and when will the capacity for certain ideas emerge.\n\n1:06:38.320 --> 1:06:40.040\n And that's become a thing in itself\n\n1:06:40.040 --> 1:06:42.560\n and to try to time things in the future.\n\n1:06:43.960 --> 1:06:48.960\n But really its original purpose was to time my products.\n\n1:06:48.960 --> 1:06:53.800\n I mean, I did OCR in the 1970s\n\n1:06:55.480 --> 1:07:00.480\n because OCR doesn't require a lot of computation.\n\n1:07:01.440 --> 1:07:02.760\n Optical character recognition.\n\n1:07:02.760 --> 1:07:06.560\n Yeah, so we were able to do that in the 70s\n\n1:07:06.560 --> 1:07:11.000\n and I waited till the 80s to address speech recognition\n\n1:07:11.000 --> 1:07:14.480\n since that requires more computation.\n\n1:07:14.480 --> 1:07:16.000\n So you were thinking through timing\n\n1:07:16.000 --> 1:07:17.480\n when you're developing those things.\n\n1:07:17.480 --> 1:07:18.320\n Yeah.\n\n1:07:18.320 --> 1:07:19.880\n Time come.\n\n1:07:19.880 --> 1:07:21.400\n Yeah.\n\n1:07:21.400 --> 1:07:24.360\n And that's how you've developed that brain power\n\n1:07:24.360 --> 1:07:26.720\n to start to think in a futurist sense\n\n1:07:26.720 --> 1:07:31.040\n when how will the world look like in 2045\n\n1:07:31.040 --> 1:07:33.640\n and work backwards and how it gets there.\n\n1:07:33.640 --> 1:07:35.360\n But that has to become a thing in itself\n\n1:07:35.360 --> 1:07:40.360\n because looking at what things will be like in the future\n\n1:07:40.360 --> 1:07:47.360\n and the future reflects such dramatic changes in how humans will live\n\n1:07:48.680 --> 1:07:51.240\n that was worth communicating also.\n\n1:07:51.240 --> 1:07:56.360\n So you developed that muscle of predicting the future\n\n1:07:56.360 --> 1:07:58.280\n and then applied broadly\n\n1:07:58.280 --> 1:08:02.280\n and started to discuss how it changes the world of technology,\n\n1:08:02.280 --> 1:08:06.800\n how it changes the world of human life on earth.\n\n1:08:06.800 --> 1:08:09.000\n In Danielle, one of your books,\n\n1:08:09.000 --> 1:08:11.600\n you write about someone who has the courage\n\n1:08:11.600 --> 1:08:15.040\n to question assumptions that limit human imagination\n\n1:08:15.040 --> 1:08:16.640\n to solve problems.\n\n1:08:16.640 --> 1:08:18.560\n And you also give advice\n\n1:08:18.560 --> 1:08:22.760\n on how each of us can have this kind of courage.\n\n1:08:22.760 --> 1:08:24.520\n Well, it's good that you picked that quote\n\n1:08:24.520 --> 1:08:27.480\n because I think that does symbolize what Danielle is about.\n\n1:08:27.480 --> 1:08:28.760\n Courage.\n\n1:08:28.760 --> 1:08:30.760\n So how can each of us have that courage\n\n1:08:30.760 --> 1:08:33.760\n to question assumptions?\n\n1:08:33.760 --> 1:08:38.600\n I mean, we see that when people can go beyond\n\n1:08:38.600 --> 1:08:43.600\n the current realm and create something that's new.\n\n1:08:43.880 --> 1:08:45.520\n I mean, take Uber, for example.\n\n1:08:45.520 --> 1:08:48.120\n Before that existed, you never thought\n\n1:08:48.120 --> 1:08:49.960\n that that would be feasible\n\n1:08:49.960 --> 1:08:53.200\n and it did require changes in the way people work.\n\n1:08:54.520 --> 1:08:57.840\n Is there practical advice as you give in the book\n\n1:08:57.840 --> 1:09:02.040\n about what each of us can do to be a Danielle?\n\n1:09:04.880 --> 1:09:06.880\n Well, she looks at the situation\n\n1:09:06.880 --> 1:09:11.880\n and tries to imagine how she can overcome various obstacles\n\n1:09:15.840 --> 1:09:17.960\n and then she goes for it.\n\n1:09:17.960 --> 1:09:19.680\n And she's a very good communicator\n\n1:09:19.680 --> 1:09:24.680\n so she can communicate these ideas to other people.\n\n1:09:25.080 --> 1:09:27.640\n And there's practical advice of learning to program\n\n1:09:27.640 --> 1:09:32.000\n and recording your life and things of this nature.\n\n1:09:32.000 --> 1:09:33.240\n Become a physicist.\n\n1:09:33.240 --> 1:09:36.880\n So you list a bunch of different suggestions\n\n1:09:36.880 --> 1:09:39.120\n of how to throw yourself into this world.\n\n1:09:39.120 --> 1:09:42.200\n Yeah, I mean, it's kind of an idea\n\n1:09:42.200 --> 1:09:46.160\n how young people can actually change the world\n\n1:09:46.160 --> 1:09:51.160\n by learning all of these different skills.\n\n1:09:52.440 --> 1:09:54.760\n And at the core of that is the belief\n\n1:09:54.760 --> 1:09:56.760\n that you can change the world.\n\n1:09:57.840 --> 1:10:00.480\n That your mind, your body can change the world.\n\n1:10:00.480 --> 1:10:02.760\n Yeah, that's right.\n\n1:10:02.760 --> 1:10:05.160\n And not letting anyone else tell you otherwise.\n\n1:10:06.640 --> 1:10:08.920\n That's really good, exactly.\n\n1:10:08.920 --> 1:10:13.440\n When we upload the story you told about your dad\n\n1:10:13.440 --> 1:10:15.280\n and having a conversation with him,\n\n1:10:16.160 --> 1:10:19.960\n we're talking about uploading your mind to the computer.\n\n1:10:21.720 --> 1:10:23.160\n Do you think we'll have a future\n\n1:10:23.160 --> 1:10:25.640\n with something you call afterlife?\n\n1:10:25.640 --> 1:10:29.840\n We'll have avatars that mimic increasingly better and better\n\n1:10:29.840 --> 1:10:33.520\n our behavior, our appearance, all that kind of stuff.\n\n1:10:33.520 --> 1:10:36.800\n Even those that are perhaps no longer with us.\n\n1:10:36.800 --> 1:10:41.800\n Yes, I mean, we need some information about them.\n\n1:10:42.840 --> 1:10:44.520\n I mean, think about my father.\n\n1:10:45.640 --> 1:10:47.040\n I have what he wrote.\n\n1:10:48.080 --> 1:10:50.480\n Now, he didn't have a word processor,\n\n1:10:50.480 --> 1:10:53.680\n so he didn't actually write that much.\n\n1:10:53.680 --> 1:10:56.000\n And our memories of him aren't perfect.\n\n1:10:56.000 --> 1:10:59.840\n So how do you even know if you've created something\n\n1:10:59.840 --> 1:11:00.840\n that's satisfactory?\n\n1:11:00.840 --> 1:11:04.920\n Now, you could do a Frederick Kurzweil Turing test.\n\n1:11:04.920 --> 1:11:07.080\n It seems like Frederick Kurzweil to me.\n\n1:11:07.920 --> 1:11:10.280\n But the people who remember him, like me,\n\n1:11:11.240 --> 1:11:14.400\n don't have a perfect memory.\n\n1:11:14.400 --> 1:11:16.320\n Is there such a thing as a perfect memory?\n\n1:11:16.320 --> 1:11:21.320\n Maybe the whole point is for him to make you feel\n\n1:11:24.760 --> 1:11:25.600\n a certain way.\n\n1:11:25.600 --> 1:11:28.400\n Yeah, well, I think that would be the goal.\n\n1:11:28.400 --> 1:11:30.360\n And that's the connection we have with loved ones.\n\n1:11:30.360 --> 1:11:35.120\n It's not really based on very strict definition of truth.\n\n1:11:35.120 --> 1:11:37.560\n It's more about the experiences we share.\n\n1:11:37.560 --> 1:11:39.880\n And they get morphed through memory.\n\n1:11:39.880 --> 1:11:41.800\n But ultimately, they make us smile.\n\n1:11:41.800 --> 1:11:44.440\n I think we definitely can do that.\n\n1:11:44.440 --> 1:11:46.800\n And that would be very worthwhile.\n\n1:11:46.800 --> 1:11:49.960\n So do you think we'll have a world of replicants?\n\n1:11:49.960 --> 1:11:51.280\n Of copies?\n\n1:11:51.280 --> 1:11:53.800\n There'll be a bunch of Ray Kurzweils.\n\n1:11:53.800 --> 1:11:55.320\n Like, I could hang out with one.\n\n1:11:55.320 --> 1:11:58.200\n I can download it for five bucks\n\n1:11:58.200 --> 1:12:00.040\n and have a best friend, Ray.\n\n1:12:01.680 --> 1:12:04.820\n And you, the original copy, wouldn't even know about it.\n\n1:12:07.160 --> 1:12:10.040\n Is that, do you think that world is,\n\n1:12:11.440 --> 1:12:13.360\n first of all, do you think that world is feasible?\n\n1:12:13.360 --> 1:12:16.320\n And do you think there's ethical challenges there?\n\n1:12:16.320 --> 1:12:18.080\n Like, how would you feel about me hanging out\n\n1:12:18.080 --> 1:12:20.480\n with Ray Kurzweil and you not knowing about it?\n\n1:12:20.480 --> 1:12:25.480\n It doesn't strike me as a problem.\n\n1:12:28.080 --> 1:12:30.240\n Which you, the original?\n\n1:12:30.240 --> 1:12:34.240\n Would you strike, would that cause a problem for you?\n\n1:12:34.240 --> 1:12:37.480\n No, I would really very much enjoy it.\n\n1:12:37.480 --> 1:12:38.760\n No, not just hang out with me,\n\n1:12:38.760 --> 1:12:43.760\n but if somebody hang out with you, a replicant of you.\n\n1:12:43.840 --> 1:12:46.800\n Well, I think I would start, it sounds exciting,\n\n1:12:46.800 --> 1:12:50.640\n but then what if they start doing better than me\n\n1:12:51.560 --> 1:12:54.060\n and take over my friend group?\n\n1:12:55.000 --> 1:13:00.000\n And then, because they may be an imperfect copy\n\n1:13:02.280 --> 1:13:05.320\n or there may be more social, all these kinds of things,\n\n1:13:05.320 --> 1:13:07.640\n and then I become like the old version\n\n1:13:07.640 --> 1:13:10.240\n that's not nearly as exciting.\n\n1:13:10.240 --> 1:13:12.360\n Maybe they're a copy of the best version of me\n\n1:13:12.360 --> 1:13:13.200\n on a good day.\n\n1:13:13.200 --> 1:13:16.880\n Yeah, but if you hang out with a replicant of me\n\n1:13:18.020 --> 1:13:20.200\n and that turned out to be successful,\n\n1:13:20.200 --> 1:13:23.520\n I'd feel proud of that person because it was based on me.\n\n1:13:24.960 --> 1:13:29.960\n So it's, but it is a kind of death of this version of you.\n\n1:13:32.420 --> 1:13:33.960\n Well, not necessarily.\n\n1:13:33.960 --> 1:13:36.360\n I mean, you can still be alive, right?\n\n1:13:36.360 --> 1:13:38.560\n But, and you would be proud, okay,\n\n1:13:38.560 --> 1:13:40.280\n so it's like having kids and you're proud\n\n1:13:40.280 --> 1:13:42.720\n that they've done even more than you were able to do.\n\n1:13:42.720 --> 1:13:43.560\n Yeah, exactly.\n\n1:13:48.280 --> 1:13:50.040\n It does bring up new issues,\n\n1:13:50.040 --> 1:13:53.880\n but it seems like an opportunity.\n\n1:13:55.120 --> 1:13:57.840\n Well, that replicant should probably have the same rights\n\n1:13:57.840 --> 1:13:58.680\n as you do.\n\n1:13:59.680 --> 1:14:02.720\n Well, that gets into a whole issue\n\n1:14:05.420 --> 1:14:07.400\n because when a replicant occurs,\n\n1:14:07.400 --> 1:14:10.320\n they're not necessarily gonna have your rights.\n\n1:14:10.320 --> 1:14:11.680\n And if a replicant occurs,\n\n1:14:11.680 --> 1:14:13.480\n if it's somebody who's already dead,\n\n1:14:14.680 --> 1:14:17.880\n do they have all the obligations\n\n1:14:17.880 --> 1:14:21.160\n and that the original person had?\n\n1:14:21.160 --> 1:14:23.400\n Do they have all the agreements that they had?\n\n1:14:25.840 --> 1:14:30.200\n I think you're gonna have to have laws that say yes.\n\n1:14:30.200 --> 1:14:33.260\n There has to be, if you wanna create a replicant,\n\n1:14:33.260 --> 1:14:35.720\n they have to have all the same rights as human rights.\n\n1:14:35.720 --> 1:14:37.080\n Well, you don't know.\n\n1:14:37.080 --> 1:14:38.400\n Someone can create a replicant and say,\n\n1:14:38.400 --> 1:14:39.240\n well, it's a replicant,\n\n1:14:39.240 --> 1:14:40.920\n but I didn't bother getting their rights.\n\n1:14:40.920 --> 1:14:41.760\n And so.\n\n1:14:41.760 --> 1:14:43.720\n Yeah, but that would be illegal, I mean.\n\n1:14:43.720 --> 1:14:47.600\n Like if you do that, you have to do that in the black market.\n\n1:14:47.600 --> 1:14:49.520\n If you wanna get an official replicant.\n\n1:14:49.520 --> 1:14:51.000\n Okay, it's not so easy.\n\n1:14:51.000 --> 1:14:53.680\n It's supposed to create multiple replicants.\n\n1:14:55.360 --> 1:14:57.940\n The original rights,\n\n1:14:59.840 --> 1:15:03.180\n maybe for one person and not for a whole group of people.\n\n1:15:04.640 --> 1:15:05.480\n Sure.\n\n1:15:08.520 --> 1:15:10.600\n So there has to be at least one.\n\n1:15:10.600 --> 1:15:13.160\n And then all the other ones kinda share the rights.\n\n1:15:14.260 --> 1:15:16.480\n Yeah, I just don't think that,\n\n1:15:16.480 --> 1:15:18.680\n that's very difficult to conceive for us humans,\n\n1:15:18.680 --> 1:15:20.760\n the idea that this country.\n\n1:15:20.760 --> 1:15:23.640\n You create a replicant that has certain,\n\n1:15:24.600 --> 1:15:26.800\n I mean, I've talked to people about this,\n\n1:15:26.800 --> 1:15:30.540\n including my wife, who would like to get back her father.\n\n1:15:32.640 --> 1:15:36.560\n And she doesn't worry about who has rights to what.\n\n1:15:38.280 --> 1:15:40.440\n She would have somebody that she could visit with\n\n1:15:40.440 --> 1:15:42.440\n and might give her some satisfaction.\n\n1:15:44.300 --> 1:15:49.240\n And she wouldn't care about any of these other rights.\n\n1:15:49.240 --> 1:15:52.200\n What does your wife think about multiple rake or as wells?\n\n1:15:53.560 --> 1:15:54.400\n Have you had that discussion?\n\n1:15:54.400 --> 1:15:56.000\n I haven't addressed that with her.\n\n1:15:58.200 --> 1:16:00.640\n I think ultimately that's an important question,\n\n1:16:00.640 --> 1:16:03.560\n loved ones, how they feel about.\n\n1:16:03.560 --> 1:16:05.040\n There's something about love.\n\n1:16:05.040 --> 1:16:06.400\n Well, that's the key thing, right?\n\n1:16:06.400 --> 1:16:07.960\n If the loved one's rejected,\n\n1:16:07.960 --> 1:16:10.560\n it's not gonna work very well, so.\n\n1:16:12.320 --> 1:16:15.960\n So the loved ones really are the key determinant,\n\n1:16:15.960 --> 1:16:18.200\n whether or not this works or not.\n\n1:16:19.760 --> 1:16:21.940\n But there's also ethical rules.\n\n1:16:22.840 --> 1:16:24.200\n We have to contend with the idea,\n\n1:16:24.200 --> 1:16:27.920\n and we have to contend with that idea with AI.\n\n1:16:27.920 --> 1:16:30.320\n But what's gonna motivate it is,\n\n1:16:30.320 --> 1:16:34.680\n I mean, I talk to people who really miss people who are gone\n\n1:16:34.680 --> 1:16:37.760\n and they would love to get something back,\n\n1:16:37.760 --> 1:16:39.500\n even if it isn't perfect.\n\n1:16:40.840 --> 1:16:42.800\n And that's what's gonna motivate this.\n\n1:16:47.120 --> 1:16:51.200\n And that person lives on in some form.\n\n1:16:51.200 --> 1:16:52.880\n And the more data we have,\n\n1:16:52.880 --> 1:16:56.080\n the more we're able to reconstruct that person\n\n1:16:56.080 --> 1:16:57.540\n and allow them to live on.\n\n1:16:59.360 --> 1:17:01.440\n And eventually as we go forward,\n\n1:17:01.440 --> 1:17:03.160\n we're gonna have more and more of this data\n\n1:17:03.160 --> 1:17:06.360\n because we're gonna have none of us\n\n1:17:06.360 --> 1:17:08.360\n that are inside our neocortex\n\n1:17:08.360 --> 1:17:10.200\n and we're gonna collect a lot of data.\n\n1:17:11.120 --> 1:17:14.840\n In fact, anything that's data is always collected.\n\n1:17:15.800 --> 1:17:18.680\n There is something a little bit sad,\n\n1:17:18.680 --> 1:17:23.200\n which is becoming, or maybe it's hopeful,\n\n1:17:23.200 --> 1:17:26.800\n which is more and more common these days,\n\n1:17:26.800 --> 1:17:28.360\n which when a person passes away,\n\n1:17:28.360 --> 1:17:29.960\n you have their Twitter account,\n\n1:17:31.080 --> 1:17:34.080\n and you have the last tweet they tweeted,\n\n1:17:34.080 --> 1:17:35.040\n like something they needed.\n\n1:17:35.040 --> 1:17:36.520\n And you can recreate them now\n\n1:17:36.520 --> 1:17:38.360\n with large language models and so on.\n\n1:17:38.360 --> 1:17:40.880\n I mean, you can create somebody that's just like them\n\n1:17:40.880 --> 1:17:45.040\n and can actually continue to communicate.\n\n1:17:45.040 --> 1:17:46.440\n I think that's really exciting\n\n1:17:46.440 --> 1:17:49.360\n because I think in some sense,\n\n1:17:49.360 --> 1:17:51.760\n like if I were to die today,\n\n1:17:51.760 --> 1:17:55.240\n in some sense I would continue on if I continued tweeting.\n\n1:17:56.120 --> 1:17:57.640\n I tweet, therefore I am.\n\n1:17:58.880 --> 1:18:02.040\n Yeah, well, I mean, that's one of the advantages\n\n1:18:02.040 --> 1:18:06.600\n of a replicant, they can recreate the communications\n\n1:18:06.600 --> 1:18:08.360\n of that person.\n\n1:18:10.320 --> 1:18:14.400\n Do you hope, do you think, do you hope\n\n1:18:14.400 --> 1:18:17.440\n humans will become a multi planetary species?\n\n1:18:17.440 --> 1:18:20.040\n You've talked about the phases, the six epochs,\n\n1:18:20.040 --> 1:18:23.600\n and one of them is reaching out into the stars in part.\n\n1:18:23.600 --> 1:18:28.240\n Yes, but the kind of attempts we're making now\n\n1:18:28.240 --> 1:18:33.040\n to go to other planetary objects\n\n1:18:34.400 --> 1:18:36.560\n doesn't excite me that much\n\n1:18:36.560 --> 1:18:38.840\n because it's not really advancing anything.\n\n1:18:38.840 --> 1:18:41.160\n It's not efficient enough?\n\n1:18:41.160 --> 1:18:45.160\n Yeah, and we're also putting out other human beings,\n\n1:18:48.120 --> 1:18:50.440\n which is a very inefficient way\n\n1:18:50.440 --> 1:18:52.600\n to explore these other objects.\n\n1:18:52.600 --> 1:18:57.600\n What I'm really talking about in the sixth epoch,\n\n1:18:57.800 --> 1:18:59.240\n the universe wakes up.\n\n1:19:00.240 --> 1:19:03.080\n It's where we can spread our super intelligence\n\n1:19:03.080 --> 1:19:05.400\n throughout the universe.\n\n1:19:05.400 --> 1:19:08.120\n And that doesn't mean sending a very soft,\n\n1:19:08.120 --> 1:19:10.200\n squishy creatures like humans.\n\n1:19:10.200 --> 1:19:13.840\n Yeah, the universe wakes up.\n\n1:19:13.840 --> 1:19:18.840\n I mean, we would send intelligence masses of nanobots\n\n1:19:18.840 --> 1:19:23.840\n which can then go out and colonize\n\n1:19:24.880 --> 1:19:27.840\n these other parts of the universe.\n\n1:19:29.000 --> 1:19:31.600\n Do you think there's intelligent alien civilizations\n\n1:19:31.600 --> 1:19:34.120\n out there that our bots might meet?\n\n1:19:35.240 --> 1:19:37.440\n My hunch is no.\n\n1:19:38.760 --> 1:19:40.720\n Most people say yes, absolutely.\n\n1:19:40.720 --> 1:19:43.480\n I mean, and the universe is too big.\n\n1:19:43.480 --> 1:19:46.160\n And they'll cite the Drake equation.\n\n1:19:46.160 --> 1:19:50.720\n And I think in Singularity is Near,\n\n1:19:52.560 --> 1:19:56.200\n I have two analyses of the Drake equation,\n\n1:19:56.200 --> 1:19:58.920\n both with very reasonable assumptions.\n\n1:20:00.000 --> 1:20:04.960\n And one gives you thousands of advanced civilizations\n\n1:20:04.960 --> 1:20:06.240\n in each galaxy.\n\n1:20:07.440 --> 1:20:11.840\n And another one gives you one civilization.\n\n1:20:11.840 --> 1:20:13.680\n And we know of one.\n\n1:20:13.680 --> 1:20:16.600\n A lot of the analyses are forgetting\n\n1:20:16.600 --> 1:20:20.160\n the exponential growth of computation.\n\n1:20:21.200 --> 1:20:24.840\n Because we've gone from where the fastest way\n\n1:20:24.840 --> 1:20:28.480\n I could send a message to somebody was with a pony,\n\n1:20:30.160 --> 1:20:33.400\n which was what, like a century and a half ago?\n\n1:20:34.920 --> 1:20:37.880\n To the advanced civilization we have today.\n\n1:20:37.880 --> 1:20:40.880\n And if you accept what I've said,\n\n1:20:40.880 --> 1:20:42.720\n go forward a few decades,\n\n1:20:42.720 --> 1:20:46.800\n you can have absolutely fantastic amount of civilization\n\n1:20:46.800 --> 1:20:50.400\n compared to a pony, and that's in a couple hundred years.\n\n1:20:50.400 --> 1:20:53.320\n Yeah, the speed and the scale of information transfer\n\n1:20:53.320 --> 1:20:57.560\n is growing exponentially in a blink of an eye.\n\n1:20:58.720 --> 1:21:01.680\n Now think about these other civilizations.\n\n1:21:01.680 --> 1:21:05.160\n They're gonna be spread out at cosmic times.\n\n1:21:06.320 --> 1:21:10.160\n So if something is like ahead of us or behind us,\n\n1:21:10.160 --> 1:21:14.280\n it could be ahead of us or behind us by maybe millions\n\n1:21:14.280 --> 1:21:16.440\n of years, which isn't that much.\n\n1:21:16.440 --> 1:21:21.440\n I mean, the world is billions of years old,\n\n1:21:21.520 --> 1:21:23.960\n 14 billion or something.\n\n1:21:23.960 --> 1:21:28.960\n So even a thousand years, if two or 300 years is enough\n\n1:21:29.760 --> 1:21:33.920\n to go from a pony to fantastic amount of civilization,\n\n1:21:33.920 --> 1:21:35.920\n we would see that.\n\n1:21:35.920 --> 1:21:39.720\n So of other civilizations that have occurred,\n\n1:21:39.720 --> 1:21:43.960\n okay, some might be behind us, but some might be ahead of us.\n\n1:21:43.960 --> 1:21:45.800\n If they're ahead of us, they're ahead of us\n\n1:21:45.800 --> 1:21:49.560\n by thousands, millions of years,\n\n1:21:49.560 --> 1:21:51.760\n and they would be so far beyond us,\n\n1:21:51.760 --> 1:21:55.200\n they would be doing galaxy wide engineering.\n\n1:21:56.200 --> 1:22:00.080\n But we don't see anything doing galaxy wide engineering.\n\n1:22:00.080 --> 1:22:05.120\n So either they don't exist, or this very universe\n\n1:22:05.120 --> 1:22:08.340\n is a construction of an alien species.\n\n1:22:08.340 --> 1:22:10.900\n We're living inside a video game.\n\n1:22:11.760 --> 1:22:14.840\n Well, that's another explanation that yes,\n\n1:22:14.840 --> 1:22:19.140\n you've got some teenage kids in another civilization.\n\n1:22:19.140 --> 1:22:22.280\n Do you find compelling the simulation hypothesis\n\n1:22:22.280 --> 1:22:25.640\n as a thought experiment that we're living in a simulation?\n\n1:22:25.640 --> 1:22:29.400\n The universe is computational.\n\n1:22:29.400 --> 1:22:34.400\n So we are an example in a computational world.\n\n1:22:34.400 --> 1:22:39.120\n Therefore, it is a simulation.\n\n1:22:39.120 --> 1:22:41.040\n It doesn't necessarily mean an experiment\n\n1:22:41.040 --> 1:22:44.820\n by some high school kid in another world,\n\n1:22:44.820 --> 1:22:47.800\n but it nonetheless is taking place\n\n1:22:47.800 --> 1:22:50.120\n in a computational world.\n\n1:22:50.120 --> 1:22:51.600\n And everything that's going on\n\n1:22:51.600 --> 1:22:56.080\n is basically a form of computation.\n\n1:22:58.080 --> 1:23:00.640\n So you really have to define what you mean\n\n1:23:00.640 --> 1:23:05.640\n by this whole world being a simulation.\n\n1:23:06.360 --> 1:23:11.360\n Well, then it's the teenager that makes the video game.\n\n1:23:12.400 --> 1:23:16.720\n Us humans with our current limited cognitive capability\n\n1:23:16.720 --> 1:23:20.560\n have strived to understand ourselves\n\n1:23:20.560 --> 1:23:23.840\n and we have created religions.\n\n1:23:23.840 --> 1:23:25.160\n We think of God.\n\n1:23:25.160 --> 1:23:30.160\n Whatever that is, do you think God exists?\n\n1:23:32.240 --> 1:23:34.500\n And if so, who is God?\n\n1:23:35.440 --> 1:23:37.720\n I alluded to this before.\n\n1:23:37.720 --> 1:23:41.960\n We started out with lots of particles going around\n\n1:23:42.840 --> 1:23:47.840\n and there's nothing that represents love and creativity.\n\n1:23:53.160 --> 1:23:55.000\n And somehow we've gotten into a world\n\n1:23:55.000 --> 1:23:56.760\n where love actually exists\n\n1:23:57.920 --> 1:23:59.960\n and that has to do actually with consciousness\n\n1:23:59.960 --> 1:24:03.120\n because you can't have love without consciousness.\n\n1:24:03.120 --> 1:24:06.720\n So to me, that's God, the fact that we have something\n\n1:24:06.720 --> 1:24:11.200\n where love, where you can be devoted to someone else\n\n1:24:11.200 --> 1:24:15.200\n and really feel the love, that's God.\n\n1:24:19.080 --> 1:24:21.040\n And if you look at the Old Testament,\n\n1:24:21.040 --> 1:24:26.040\n it was actually created by several different\n\n1:24:26.680 --> 1:24:29.200\n ravenants in there.\n\n1:24:29.200 --> 1:24:32.500\n And I think they've identified three of them.\n\n1:24:34.080 --> 1:24:39.080\n One of them dealt with God as a person\n\n1:24:39.400 --> 1:24:42.440\n that you can make deals with and he gets angry\n\n1:24:42.440 --> 1:24:47.440\n and he wrecks vengeance on various people.\n\n1:24:48.320 --> 1:24:50.440\n But two of them actually talk about God\n\n1:24:50.440 --> 1:24:55.440\n as a symbol of love and peace and harmony and so forth.\n\n1:24:58.280 --> 1:24:59.980\n That's how they describe God.\n\n1:25:01.360 --> 1:25:06.120\n So that's my view of God, not as a person in the sky\n\n1:25:06.120 --> 1:25:08.140\n that you can make deals with.\n\n1:25:09.120 --> 1:25:13.200\n It's whatever the magic that goes from basic elements\n\n1:25:13.200 --> 1:25:15.960\n to things like consciousness and love.\n\n1:25:15.960 --> 1:25:19.200\n Do you think one of the things I find\n\n1:25:19.200 --> 1:25:22.240\n extremely beautiful and powerful is cellular automata,\n\n1:25:22.240 --> 1:25:23.600\n which you also touch on?\n\n1:25:24.640 --> 1:25:27.720\n Do you think whatever the heck happens in cellular automata\n\n1:25:27.720 --> 1:25:30.460\n where interesting, complicated objects emerge,\n\n1:25:31.480 --> 1:25:33.480\n God is in there too?\n\n1:25:33.480 --> 1:25:38.480\n The emergence of love in this seemingly primitive universe?\n\n1:25:38.480 --> 1:25:41.560\n Well, that's the goal of creating a replicant\n\n1:25:42.600 --> 1:25:47.600\n is that they would love you and you would love them.\n\n1:25:47.600 --> 1:25:50.840\n There wouldn't be much point of doing it\n\n1:25:50.840 --> 1:25:52.720\n if that didn't happen.\n\n1:25:52.720 --> 1:25:54.880\n But all of it, I guess what I'm saying\n\n1:25:54.880 --> 1:25:59.280\n about cellular automata is it's primitive building blocks\n\n1:25:59.280 --> 1:26:03.440\n and they somehow create beautiful things.\n\n1:26:03.440 --> 1:26:06.280\n Is there some deep truth to that\n\n1:26:06.280 --> 1:26:07.960\n about how our universe works?\n\n1:26:07.960 --> 1:26:11.160\n Is the emergence from simple rules,\n\n1:26:11.160 --> 1:26:14.080\n beautiful, complex objects can emerge?\n\n1:26:14.080 --> 1:26:16.680\n Is that the thing that made us?\n\n1:26:16.680 --> 1:26:18.120\n Yeah, well. As we went through\n\n1:26:18.120 --> 1:26:21.560\n all the six phases of reality.\n\n1:26:21.560 --> 1:26:23.660\n That's a good way to look at it.\n\n1:26:23.660 --> 1:26:27.320\n It does make some point to the whole value\n\n1:26:27.320 --> 1:26:29.820\n of having a universe.\n\n1:26:31.400 --> 1:26:34.040\n Do you think about your own mortality?\n\n1:26:34.040 --> 1:26:35.080\n Are you afraid of it?\n\n1:26:36.240 --> 1:26:41.240\n Yes, but I keep going back to my idea\n\n1:26:41.240 --> 1:26:46.240\n of being able to expand human life quickly enough\n\n1:26:48.080 --> 1:26:53.080\n in advance of our getting there, longevity escape velocity,\n\n1:26:55.880 --> 1:26:57.660\n which we're not quite at yet,\n\n1:26:58.600 --> 1:27:01.520\n but I think we're actually pretty close,\n\n1:27:01.520 --> 1:27:05.320\n particularly with, for example, doing simulated biology.\n\n1:27:06.600 --> 1:27:08.920\n I think we can probably get there within,\n\n1:27:08.920 --> 1:27:12.800\n say, by the end of this decade, and that's my goal.\n\n1:27:12.800 --> 1:27:16.400\n Do you hope to achieve the longevity escape velocity?\n\n1:27:16.400 --> 1:27:18.520\n Do you hope to achieve immortality?\n\n1:27:20.900 --> 1:27:22.960\n Well, immortality is hard to say.\n\n1:27:22.960 --> 1:27:26.080\n I can't really come on your program saying I've done it.\n\n1:27:26.080 --> 1:27:30.280\n I've achieved immortality because it's never forever.\n\n1:27:32.480 --> 1:27:35.280\n A long time, a long time of living well.\n\n1:27:35.280 --> 1:27:37.180\n But we'd like to actually advance\n\n1:27:37.180 --> 1:27:41.000\n human life expectancy, advance my life expectancy\n\n1:27:41.000 --> 1:27:44.000\n more than a year every year,\n\n1:27:44.000 --> 1:27:45.820\n and I think we can get there within,\n\n1:27:45.820 --> 1:27:47.800\n by the end of this decade.\n\n1:27:47.800 --> 1:27:49.440\n How do you think we'd do it?\n\n1:27:49.440 --> 1:27:53.640\n So there's practical things in Transcend,\n\n1:27:53.640 --> 1:27:56.200\n the nine steps to living well forever, your book.\n\n1:27:56.200 --> 1:27:58.360\n You describe just that.\n\n1:27:58.360 --> 1:28:00.520\n There's practical things like health,\n\n1:28:00.520 --> 1:28:02.280\n exercise, all those things.\n\n1:28:02.280 --> 1:28:03.920\n Yeah, I mean, we live in a body\n\n1:28:03.920 --> 1:28:08.280\n that doesn't last forever.\n\n1:28:08.280 --> 1:28:10.280\n There's no reason why it can't, though,\n\n1:28:11.160 --> 1:28:14.000\n and we're discovering things, I think, that will extend it.\n\n1:28:17.640 --> 1:28:19.400\n But you do have to deal with,\n\n1:28:19.400 --> 1:28:22.080\n I mean, I've got various issues.\n\n1:28:23.240 --> 1:28:28.240\n Went to Mexico 40 years ago, developed salmonella.\n\n1:28:28.240 --> 1:28:33.060\n I created pancreatitis, which gave me\n\n1:28:33.060 --> 1:28:35.360\n a strange form of diabetes.\n\n1:28:37.380 --> 1:28:42.380\n It's not type one diabetes, because it's an autoimmune\n\n1:28:42.900 --> 1:28:44.860\n disorder that destroys your pancreas.\n\n1:28:44.860 --> 1:28:45.880\n I don't have that.\n\n1:28:46.780 --> 1:28:48.700\n But it's also not type two diabetes,\n\n1:28:48.700 --> 1:28:51.740\n because type two diabetes is your pancreas works fine,\n\n1:28:51.740 --> 1:28:55.740\n but your cells don't absorb the insulin well.\n\n1:28:55.740 --> 1:28:58.460\n I don't have that either.\n\n1:28:58.460 --> 1:29:03.460\n The pancreatitis I had partially damaged my pancreas,\n\n1:29:04.560 --> 1:29:06.100\n but it was a one time thing.\n\n1:29:06.100 --> 1:29:11.100\n It didn't continue, and I've learned now how to control it.\n\n1:29:11.620 --> 1:29:13.860\n But so that's just something that I had to do\n\n1:29:15.300 --> 1:29:18.540\n in order to continue to exist.\n\n1:29:18.540 --> 1:29:20.460\n Since your particular biological system,\n\n1:29:20.460 --> 1:29:22.540\n you had to figure out a few hacks,\n\n1:29:22.540 --> 1:29:24.900\n and the idea is that science would be able\n\n1:29:24.900 --> 1:29:26.420\n to do that much better, actually.\n\n1:29:26.420 --> 1:29:29.340\n Yeah, so I mean, I do spend a lot of time\n\n1:29:29.340 --> 1:29:32.220\n just tinkering with my own body to keep it going.\n\n1:29:34.240 --> 1:29:37.740\n So I do think I'll last till the end of this decade,\n\n1:29:37.740 --> 1:29:41.660\n and I think we'll achieve longevity, escape velocity.\n\n1:29:41.660 --> 1:29:43.540\n I think that we'll start with people\n\n1:29:43.540 --> 1:29:46.180\n who are very diligent about this.\n\n1:29:46.180 --> 1:29:48.860\n Eventually, it'll become sort of routine\n\n1:29:48.860 --> 1:29:51.400\n that people will be able to do it.\n\n1:29:51.400 --> 1:29:54.300\n So if you're talking about kids today,\n\n1:29:54.300 --> 1:29:56.700\n or even people in their 20s or 30s,\n\n1:29:56.700 --> 1:30:00.480\n that's really not a very serious problem.\n\n1:30:01.300 --> 1:30:05.100\n I have had some discussions with relatives\n\n1:30:05.100 --> 1:30:10.100\n who are like almost 100, and saying,\n\n1:30:10.340 --> 1:30:13.380\n well, we're working on it as quickly as possible.\n\n1:30:13.380 --> 1:30:14.980\n I don't know if that's gonna work.\n\n1:30:16.460 --> 1:30:18.400\n Is there a case, this is a difficult question,\n\n1:30:18.400 --> 1:30:23.400\n but is there a case to be made against living forever\n\n1:30:23.400 --> 1:30:28.400\n that a finite life, that mortality is a feature, not a bug,\n\n1:30:29.620 --> 1:30:34.620\n that living a shorter, so dying makes ice cream\n\n1:30:36.020 --> 1:30:40.260\n taste delicious, makes life intensely beautiful\n\n1:30:40.260 --> 1:30:42.220\n more than it otherwise might be?\n\n1:30:42.220 --> 1:30:46.820\n Most people believe that way, except if you present\n\n1:30:46.820 --> 1:30:51.500\n a death of anybody they care about or love,\n\n1:30:51.500 --> 1:30:55.300\n they find that extremely depressing.\n\n1:30:55.300 --> 1:30:58.420\n And I know people who feel that way\n\n1:30:58.420 --> 1:31:03.420\n 20, 30, 40 years later, they still want them back.\n\n1:31:06.200 --> 1:31:10.660\n So I mean, death is not something to celebrate,\n\n1:31:11.820 --> 1:31:16.260\n but we've lived in a world where people just accept this.\n\n1:31:16.260 --> 1:31:18.340\n Life is short, you see it all the time on TV,\n\n1:31:18.340 --> 1:31:21.340\n oh, life's short, you have to take advantage of it\n\n1:31:21.340 --> 1:31:23.860\n and nobody accepts the fact that you could actually\n\n1:31:23.860 --> 1:31:27.940\n go beyond normal lifetimes.\n\n1:31:27.940 --> 1:31:31.580\n But anytime we talk about death or a death of a person,\n\n1:31:31.580 --> 1:31:35.420\n even one death is a terrible tragedy.\n\n1:31:35.420 --> 1:31:39.000\n If you have somebody that lives to 100 years old,\n\n1:31:39.000 --> 1:31:42.860\n we still love them in return.\n\n1:31:43.820 --> 1:31:47.660\n And there's no limitation to that.\n\n1:31:47.660 --> 1:31:52.000\n In fact, these kinds of trends are gonna provide\n\n1:31:52.000 --> 1:31:54.700\n greater and greater opportunity for everybody,\n\n1:31:54.700 --> 1:31:56.200\n even if we have more people.\n\n1:31:57.100 --> 1:32:00.320\n So let me ask about an alien species\n\n1:32:00.320 --> 1:32:03.060\n or a super intelligent AI 500 years from now\n\n1:32:03.060 --> 1:32:08.060\n that will look back and remember Ray Kurzweil version zero.\n\n1:32:11.140 --> 1:32:13.140\n Before the replicants spread,\n\n1:32:13.140 --> 1:32:17.020\n how do you hope they remember you\n\n1:32:17.020 --> 1:32:21.420\n in Hitchhiker's Guide to the Galaxy summary of Ray Kurzweil?\n\n1:32:21.420 --> 1:32:23.160\n What do you hope your legacy is?\n\n1:32:24.120 --> 1:32:26.740\n Well, I mean, I do hope to be around, so that's.\n\n1:32:26.740 --> 1:32:27.900\n Some version of you, yes.\n\n1:32:27.900 --> 1:32:28.740\n So.\n\n1:32:29.740 --> 1:32:32.140\n Do you think you'll be the same person around?\n\n1:32:32.140 --> 1:32:37.020\n I mean, am I the same person I was when I was 20 or 10?\n\n1:32:37.020 --> 1:32:39.780\n You would be the same person in that same way,\n\n1:32:39.780 --> 1:32:43.380\n but yes, we're different, we're different.\n\n1:32:44.420 --> 1:32:46.900\n All we have of that, all you have of that person\n\n1:32:46.900 --> 1:32:51.900\n is your memories, which are probably distorted in some way.\n\n1:32:53.700 --> 1:32:55.860\n Maybe you just remember the good parts,\n\n1:32:55.860 --> 1:32:57.860\n depending on your psyche.\n\n1:32:57.860 --> 1:32:59.740\n You might focus on the bad parts,\n\n1:32:59.740 --> 1:33:01.260\n might focus on the good parts.\n\n1:33:02.820 --> 1:33:06.500\n Right, but I mean, I still have a relationship\n\n1:33:06.500 --> 1:33:10.820\n to the way I was when I was earlier, when I was younger.\n\n1:33:11.860 --> 1:33:14.220\n How will you and the other super intelligent AIs\n\n1:33:14.220 --> 1:33:17.720\n remember you of today from 500 years ago?\n\n1:33:18.940 --> 1:33:22.860\n What do you hope to be remembered by this version of you\n\n1:33:22.860 --> 1:33:24.680\n before the singularity?\n\n1:33:25.640 --> 1:33:28.220\n Well, I think it's expressed well in my books,\n\n1:33:28.220 --> 1:33:32.620\n trying to create some new realities that people will accept.\n\n1:33:32.620 --> 1:33:36.800\n I mean, that's something that gives me great pleasure,\n\n1:33:40.320 --> 1:33:45.320\n and greater insight into what makes humans valuable.\n\n1:33:49.720 --> 1:33:54.720\n I'm not the only person who's tempted to comment on that.\n\n1:33:57.220 --> 1:34:00.700\n And optimism that permeates your work.\n\n1:34:00.700 --> 1:34:04.700\n Optimism about the future is ultimately that optimism\n\n1:34:04.700 --> 1:34:06.780\n paves the way for building a better future.\n\n1:34:06.780 --> 1:34:09.080\n Yeah, I agree with that.\n\n1:34:10.100 --> 1:34:15.100\n So you asked your dad about the meaning of life,\n\n1:34:15.360 --> 1:34:19.260\n and he said, love, let me ask you the same question.\n\n1:34:19.260 --> 1:34:21.200\n What's the meaning of life?\n\n1:34:21.200 --> 1:34:22.900\n Why are we here?\n\n1:34:22.900 --> 1:34:26.900\n This beautiful journey that we're on in phase four,\n\n1:34:26.900 --> 1:34:31.900\n reaching for phase five of this evolution\n\n1:34:32.540 --> 1:34:34.660\n and information processing, why?\n\n1:34:35.500 --> 1:34:38.320\n Well, I think I'd give the same answers as my father.\n\n1:34:42.020 --> 1:34:43.860\n Because if there were no love,\n\n1:34:43.860 --> 1:34:45.640\n and we didn't care about anybody,\n\n1:34:46.580 --> 1:34:48.300\n there'd be no point existing.\n\n1:34:49.540 --> 1:34:51.460\n Love is the meaning of life.\n\n1:34:51.460 --> 1:34:54.260\n The AI version of your dad had a good point.\n\n1:34:54.260 --> 1:34:57.820\n Well, I think that's a beautiful way to end it.\n\n1:34:57.820 --> 1:34:59.260\n Ray, thank you for your work.\n\n1:34:59.260 --> 1:35:01.100\n Thank you for being who you are.\n\n1:35:01.100 --> 1:35:03.420\n Thank you for dreaming about a beautiful future\n\n1:35:03.420 --> 1:35:06.460\n and creating it along the way.\n\n1:35:06.460 --> 1:35:09.340\n And thank you so much for spending\n\n1:35:09.340 --> 1:35:10.900\n your really valuable time with me today.\n\n1:35:10.900 --> 1:35:12.260\n This was awesome.\n\n1:35:12.260 --> 1:35:16.300\n It was my pleasure, and you have some great insights,\n\n1:35:16.300 --> 1:35:20.540\n both into me and into humanity as well, so I appreciate that.\n\n1:35:21.440 --> 1:35:22.980\n Thanks for listening to this conversation\n\n1:35:22.980 --> 1:35:24.340\n with Ray Kurzweil.\n\n1:35:24.340 --> 1:35:25.580\n To support this podcast,\n\n1:35:25.580 --> 1:35:28.380\n please check out our sponsors in the description.\n\n1:35:28.380 --> 1:35:30.420\n And now, let me leave you with some words\n\n1:35:30.420 --> 1:35:31.900\n from Isaac Asimov.\n\n1:35:32.820 --> 1:35:37.660\n It is change, continuous change, inevitable change\n\n1:35:37.660 --> 1:35:41.060\n that is the dominant factor in society today.\n\n1:35:41.060 --> 1:35:43.860\n No sensible decision can be made any longer\n\n1:35:43.860 --> 1:35:47.320\n without taking into account not only the world as it is,\n\n1:35:47.320 --> 1:35:49.560\n but the world as it will be.\n\n1:35:49.560 --> 1:35:52.540\n This, in turn, means that our statesmen,\n\n1:35:52.540 --> 1:35:55.340\n our businessmen, our everyman,\n\n1:35:55.340 --> 1:35:58.460\n must take on a science fictional way of thinking.\n\n1:35:58.460 --> 1:36:21.460\n Thank you for listening, and hope to see you next time.\n\n"
}