{
  "title": "Rana el Kaliouby: Emotion AI, Social Robots, and Self-Driving Cars | Lex Fridman Podcast #322",
  "id": "36_rM7wpN5A",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:07.040\n there's a broader question here, right? As we build socially and emotionally intelligent machines,\n\n00:07.920 --> 00:12.640\n what does that mean about our relationship with them and then more broadly our relationship with\n\n00:12.640 --> 00:18.240\n one another, right? Because this machine is going to be programmed to be amazing at empathy,\n\n00:18.240 --> 00:22.560\n by definition, right? It's going to always be there for you. It's not going to get bored.\n\n00:23.440 --> 00:25.680\n I don't know how I feel about that. I think about that a lot.\n\n00:25.680 --> 00:30.320\n TITO The following is a conversation with Rana\n\n00:30.320 --> 00:36.080\n L. Kliubi, a pioneer in the field of emotion recognition and human centric artificial\n\n00:36.080 --> 00:43.920\n intelligence. She is the founder of Effectiva, deputy CEO of SmartEye, author of Girl Decoded,\n\n00:43.920 --> 00:49.200\n and one of the most brilliant, kind, inspiring, and fun human beings I've gotten the chance to\n\n00:49.200 --> 00:54.800\n talk to. This is the Lex Friedman podcast. To support it, please check out our sponsors in\n\n00:54.800 --> 01:02.400\n the description. And now, dear friends, here's Rana L. Kliubi. You grew up in the Middle East,\n\n01:02.400 --> 01:08.000\n in Egypt. What is the memory from that time that makes you smile? Or maybe a memory that stands out\n\n01:08.000 --> 01:12.320\n as helping your mind take shape and helping you define yourself in this world?\n\n01:12.320 --> 01:15.440\n RANA L. KLIUBI So the memory that stands out is we used to\n\n01:15.440 --> 01:21.680\n live in my grandma's house. She used to have these mango trees in her garden. And in the summer,\n\n01:21.680 --> 01:26.640\n and so mango season was like July and August. And so in the summer, she would invite all my aunts\n\n01:26.640 --> 01:31.680\n and uncles and cousins. And it was just like maybe there were like 20 or 30 people in the house,\n\n01:31.680 --> 01:38.080\n and she would cook all this amazing food. And us, the kids, we would go down the garden,\n\n01:38.080 --> 01:43.920\n and we would pick all these mangoes. And I don't know, I think it's just the bringing people\n\n01:43.920 --> 01:47.920\n together that always stuck with me, the warmth. TITO Around the mango tree.\n\n01:47.920 --> 01:52.800\n RANA L. KLIUBI Yeah, around the mango tree. And there's just like the joy, the joy of being\n\n01:52.800 --> 02:00.880\n together around food. And I'm a terrible cook. So I guess that didn't, that memory didn't translate\n\n02:00.880 --> 02:05.520\n to me kind of doing the same. I love hosting people. TITO Do you remember colors, smells?\n\n02:05.520 --> 02:10.560\n Is that what, like what, how does memory work? Like what do you visualize? Do you visualize\n\n02:10.560 --> 02:19.360\n people's faces, smiles? Do you, is there colors? Is there like a theme to the colors? Is it smells\n\n02:19.360 --> 02:23.360\n because of food involved? RANA L. KLIUBI Yeah, I think that's a great question. So the,\n\n02:23.360 --> 02:28.800\n those Egyptian mangoes, there's a particular type that I love, and it's called Darwasi mangoes. And\n\n02:28.800 --> 02:33.680\n they're kind of, you know, they're oval, and they have a little red in them. So I kind of,\n\n02:33.680 --> 02:39.600\n they're red and mango colored on the outside. So I remember that. TITO Does red indicate like\n\n02:39.600 --> 02:45.520\n extra sweetness? Is that, is that, that means like it's nicely, yeah, it's nice and ripe and stuff.\n\n02:45.520 --> 02:52.640\n Yeah. What, what's like a definitive food of Egypt? You know, there's like these almost\n\n02:52.640 --> 02:58.800\n stereotypical foods in different parts of the world, like Ukraine invented borscht.\n\n02:59.600 --> 03:04.800\n Borscht is this beet soup with, that you put sour cream on. See, it's not, I can't see if you,\n\n03:04.800 --> 03:10.880\n if you know, if you know what it is, I think, you know, is delicious. But if I explain it,\n\n03:10.880 --> 03:15.280\n it's just not going to sound delicious. I feel like beet soup. This doesn't make any sense,\n\n03:15.280 --> 03:19.600\n but that's kind of, and you probably have actually seen pictures of it because it's one of the\n\n03:19.600 --> 03:26.800\n traditional foods in Ukraine, in Russia, in different parts of the Slavic world. So that's,\n\n03:26.800 --> 03:31.520\n but it's become so cliche and stereotypical that you almost don't mention it, but it's still\n\n03:31.520 --> 03:35.440\n delicious. Like I visited Ukraine, I eat that every single day, so.\n\n03:35.440 --> 03:38.480\n Do you, do you make it yourself? How hard is it to make?\n\n03:38.480 --> 03:43.600\n No, I don't know. I think to make it well, like anything, like Italians, they say, well,\n\n03:44.320 --> 03:51.760\n tomato sauce is easy to make, but to make it right, that's like a generational skill. So anyway,\n\n03:51.760 --> 03:55.200\n is there something like that in Egypt? Is there a culture of food?\n\n03:55.200 --> 04:02.880\n There is. And actually, we have a similar kind of soup. It's called molokhia, and it's, it's made\n\n04:02.880 --> 04:07.520\n of this green plant. It's like, it's somewhere between spinach and kale, and you mince it,\n\n04:07.520 --> 04:13.360\n and then you cook it in like chicken broth. And my grandma used to make, and my mom makes it really\n\n04:13.360 --> 04:18.080\n well, and I try to make it, but it's not as great. So we used to have that. And then we used to have\n\n04:18.080 --> 04:23.520\n it alongside stuffed pigeons. I'm pescetarian now, so I don't eat that anymore, but.\n\n04:23.520 --> 04:24.480\n Stuffed pigeons.\n\n04:24.480 --> 04:27.600\n Yeah, it's like, it was really yummy. It's the one thing I miss about,\n\n04:28.480 --> 04:32.080\n you know, now that I'm pescetarian and I don't eat.\n\n04:32.080 --> 04:33.040\n The stuffed pigeons?\n\n04:33.040 --> 04:34.240\n Yeah, the stuffed pigeons.\n\n04:35.440 --> 04:39.920\n Is it, what are they stuffed with? If that doesn't bother you too much to describe.\n\n04:39.920 --> 04:46.000\n No, no, it's stuffed with a lot of like just rice and, yeah, it's just rice. Yeah, so.\n\n04:46.000 --> 04:51.120\n And you also, you said that your first, in your book, that your first computer\n\n04:51.120 --> 04:54.880\n was an Atari, and Space Invaders was your favorite game.\n\n04:56.000 --> 04:58.800\n Is that when you first fell in love with computers, would you say?\n\n04:58.800 --> 05:00.160\n Yeah, I would say so.\n\n05:00.160 --> 05:04.160\n Video games, or just the computer itself? Just something about the machine.\n\n05:04.160 --> 05:07.840\n Ooh, this thing, there's magic in here.\n\n05:07.840 --> 05:12.080\n Yeah, I think the magical moment is definitely like playing video games with my,\n\n05:12.080 --> 05:17.120\n I have two younger sisters, and we would just like had fun together, like playing games.\n\n05:17.120 --> 05:22.240\n But the other memory I have is my first code, the first code I wrote.\n\n05:22.240 --> 05:26.720\n I wrote, I drew a Christmas tree, and I'm Muslim, right?\n\n05:26.720 --> 05:32.000\n So it's kind of, it was kind of funny that the first thing I did was like this Christmas tree.\n\n05:32.000 --> 05:38.320\n So, yeah, and that's when I realized, wow, you can write code to do all sorts of like\n\n05:38.320 --> 05:42.720\n really cool stuff. I must have been like six or seven at the time.\n\n05:42.720 --> 05:48.560\n So you can write programs, and the programs do stuff for you. That's power.\n\n05:48.560 --> 05:50.880\n That's, if you think about it, that's empowering.\n\n05:50.880 --> 05:51.600\n It's AI.\n\n05:51.600 --> 05:55.120\n Yeah, I know what it is. I don't know if that, you see like,\n\n05:56.400 --> 05:59.520\n I don't know if many people think of it that way when they first learned to program.\n\n05:59.520 --> 06:02.880\n They just love the puzzle of it. Like, ooh, this is cool. This is pretty.\n\n06:02.880 --> 06:05.600\n It's a Christmas tree, but like, it's power.\n\n06:05.600 --> 06:06.960\n It is power.\n\n06:06.960 --> 06:11.040\n Eventually, I guess you couldn't at the time, but eventually this thing,\n\n06:11.040 --> 06:14.640\n if it's interesting enough, if it's a pretty enough Christmas tree,\n\n06:14.640 --> 06:19.280\n it can be run by millions of people and bring them joy, like that little thing.\n\n06:19.280 --> 06:21.760\n And then because it's digital, it's easy to spread.\n\n06:22.400 --> 06:26.560\n So like you just created something that's easily spreadable to millions of people.\n\n06:26.560 --> 06:27.120\n Totally.\n\n06:28.160 --> 06:29.840\n It's hard to think that way when you're six.\n\n06:30.800 --> 06:37.040\n In the book, you write, I am who I am because I was raised by a particular set of parents,\n\n06:37.040 --> 06:41.200\n both modern and conservative, forward thinking, yet locked in tradition.\n\n06:41.760 --> 06:46.000\n I'm a Muslim and I feel I'm stronger, more centered for it.\n\n06:46.000 --> 06:50.960\n I adhere to the values of my religion, even if I'm not as dutiful as I once was.\n\n06:50.960 --> 06:55.040\n And I am a new American and I'm thriving on the energy,\n\n06:55.040 --> 06:58.720\n vitality and entrepreneurial spirit of this great country.\n\n06:59.840 --> 07:01.520\n So let me ask you about your parents.\n\n07:01.520 --> 07:05.280\n What have you learned about life from them, especially when you were young?\n\n07:05.280 --> 07:09.920\n So both my parents, they're Egyptian, but they moved to Kuwait right out.\n\n07:09.920 --> 07:11.680\n Actually, there's a cute story about how they met.\n\n07:11.680 --> 07:14.960\n So my dad taught COBOL in the 70s.\n\n07:14.960 --> 07:15.680\n Nice.\n\n07:15.680 --> 07:18.240\n And my mom decided to learn programming.\n\n07:18.240 --> 07:21.120\n So she signed up to take his COBOL programming class.\n\n07:22.400 --> 07:26.640\n And he tried to date her and she was like, no, no, no, I don't date.\n\n07:26.640 --> 07:28.240\n And so he's like, okay, I'll propose.\n\n07:28.240 --> 07:29.680\n And that's how they got married.\n\n07:29.680 --> 07:30.960\n Whoa, strong move.\n\n07:30.960 --> 07:32.240\n Right, exactly, right.\n\n07:32.240 --> 07:34.640\n That's really impressive.\n\n07:35.760 --> 07:38.800\n Those COBOL guys know how to impress a lady.\n\n07:40.640 --> 07:43.520\n So yeah, so what have you learned from them?\n\n07:43.520 --> 07:44.720\n So definitely grit.\n\n07:44.720 --> 07:47.360\n One of the core values in our family is just hard work.\n\n07:48.320 --> 07:50.080\n There were no slackers in our family.\n\n07:50.720 --> 07:54.160\n And that's something that's definitely stayed with me,\n\n07:55.920 --> 07:58.480\n both as a professional, but also in my personal life.\n\n07:58.480 --> 08:06.160\n But I also think my mom, my mom always used to like, I don't know, it was like unconditional\n\n08:06.160 --> 08:06.720\n love.\n\n08:06.720 --> 08:12.160\n Like I just knew my parents would be there for me kind of regardless of what I chose to do.\n\n08:14.240 --> 08:15.520\n And I think that's very powerful.\n\n08:15.520 --> 08:21.600\n And they got tested on it because I kind of challenged cultural norms and I kind of took\n\n08:21.600 --> 08:27.360\n a different path, I guess, than what's expected of a woman in the Middle East.\n\n08:27.360 --> 08:32.480\n And they still love me, which I'm so grateful for that.\n\n08:32.480 --> 08:35.440\n When was like a moment that was the most challenging for them?\n\n08:35.440 --> 08:42.240\n Which moment where they kind of had to come face to face with the fact that you're a bit\n\n08:42.240 --> 08:42.880\n of a rebel?\n\n08:44.080 --> 08:52.080\n I think the first big moment was when I had just gotten married, but I decided to go do\n\n08:52.080 --> 08:53.840\n my PhD at Cambridge University.\n\n08:53.840 --> 08:59.440\n And because my husband at the time, he's now my ex, ran a company in Cairo, he was going\n\n08:59.440 --> 09:00.240\n to stay in Egypt.\n\n09:00.240 --> 09:02.560\n So it was going to be a long distance relationship.\n\n09:03.120 --> 09:09.040\n And that's very unusual in the Middle East for a woman to just head out and kind of pursue\n\n09:09.040 --> 09:09.840\n her career.\n\n09:09.840 --> 09:18.720\n And so my dad and my parents in law both said, you know, we do not approve of you doing this,\n\n09:18.720 --> 09:22.480\n but now you're under the jurisdiction of your husband so he can make the call.\n\n09:22.480 --> 09:26.240\n And luckily for me, he was supportive.\n\n09:26.800 --> 09:29.600\n He said, you know, this is your dream come true.\n\n09:29.600 --> 09:30.960\n You've always wanted to do a PhD.\n\n09:30.960 --> 09:31.840\n I'm going to support you.\n\n09:33.200 --> 09:39.120\n So I think that was the first time where, you know, I challenged the cultural norms.\n\n09:39.120 --> 09:40.080\n Was that scary?\n\n09:40.080 --> 09:41.360\n Oh, my God, yes.\n\n09:41.360 --> 09:42.720\n It was totally scary.\n\n09:42.720 --> 09:50.320\n What's the biggest culture shock from there to Cambridge, to London?\n\n09:50.320 --> 09:56.480\n Well, that was also during right around September 11th.\n\n09:56.480 --> 10:01.040\n So everyone thought that there was going to be a third world war.\n\n10:01.040 --> 10:07.120\n It was really like, and I, at the time I used to wear the hijab, so I was very visibly Muslim.\n\n10:07.680 --> 10:11.840\n And so my parents just were, they were afraid for my safety.\n\n10:11.840 --> 10:15.600\n But anyways, when I got to Cambridge, because I was so scared, I decided to take off my\n\n10:15.600 --> 10:17.440\n headscarf and wear a hat instead.\n\n10:17.440 --> 10:22.320\n So I just went to class wearing these like British hats, which was, in my opinion, actually\n\n10:22.320 --> 10:25.920\n worse than just showing up in a headscarf because it was just so awkward, right?\n\n10:25.920 --> 10:27.680\n Like sitting in class with like all these.\n\n10:27.680 --> 10:28.320\n Trying to fit in.\n\n10:29.040 --> 10:29.360\n Yeah.\n\n10:29.360 --> 10:30.320\n Like a spy.\n\n10:30.320 --> 10:31.280\n Yeah, yeah, yeah.\n\n10:31.280 --> 10:34.640\n So after a few weeks of doing that, I was like, to heck with that.\n\n10:34.640 --> 10:37.120\n I'm just going to go back to wearing my headscarf.\n\n10:37.120 --> 10:43.840\n Yeah, you wore the hijab, so starting in 2000 and for 12 years after.\n\n10:43.840 --> 10:47.280\n So it's always, whenever you're in public, you have to wear the head covering.\n\n10:47.280 --> 10:52.480\n Can you speak to that, to the hijab, maybe your mixed feelings about it?\n\n10:52.480 --> 10:55.120\n Like what does it represent in its best case?\n\n10:55.120 --> 10:56.960\n What does it represent in the worst case?\n\n10:56.960 --> 11:03.040\n Yeah, you know, I think there's a lot of, I guess I'll first start by saying I wore\n\n11:03.040 --> 11:04.000\n it voluntarily.\n\n11:04.000 --> 11:05.360\n I was not forced to wear it.\n\n11:05.360 --> 11:09.920\n And in fact, I was one of the very first women in my family to decide to put on the hijab.\n\n11:09.920 --> 11:13.040\n And my family thought it was really odd, right?\n\n11:13.040 --> 11:15.840\n Like there was, they were like, why do you want to put this on?\n\n11:15.840 --> 11:20.480\n And at its best, it's a sign of modesty, humility.\n\n11:20.480 --> 11:20.980\n Yeah.\n\n11:22.160 --> 11:25.280\n It's like me wearing a suit, people are like, why are you wearing a suit?\n\n11:25.280 --> 11:30.880\n It's a step back into some kind of tradition, a respect for tradition of sorts.\n\n11:30.880 --> 11:36.000\n So you said, because it's by choice, you're kind of free to make that choice to celebrate\n\n11:36.000 --> 11:37.760\n a tradition of modesty.\n\n11:37.760 --> 11:40.960\n Exactly. And I actually like made it my own.\n\n11:40.960 --> 11:45.680\n I remember I would really match the color of my headscarf with what I was wearing.\n\n11:45.680 --> 11:51.360\n Like it was a form of self expression and at its best, I loved wearing it.\n\n11:52.080 --> 11:56.480\n You know, I have a lot of questions around how we practice religion and religion and,\n\n11:56.480 --> 12:02.160\n you know, and I think also it was a time where I was spending a lot of time going back and\n\n12:02.160 --> 12:04.480\n forth between the US and Egypt.\n\n12:04.480 --> 12:09.920\n And I started meeting a lot of people in the US who are just amazing people, very purpose\n\n12:09.920 --> 12:14.240\n driven, people who have very strong core values, but they're not Muslim.\n\n12:14.720 --> 12:15.760\n That's okay, right?\n\n12:15.760 --> 12:18.720\n And so that was when I just had a lot of questions.\n\n12:19.920 --> 12:25.120\n And politically, also the situation in Egypt was when the Muslim Brotherhood ran the country\n\n12:25.120 --> 12:27.840\n and I didn't agree with their ideology.\n\n12:29.520 --> 12:31.360\n It was at a time when I was going through a divorce.\n\n12:31.360 --> 12:37.040\n Like it was like, it was like just the perfect storm of like political, personal conditions\n\n12:37.040 --> 12:39.200\n where I was like, this doesn't feel like me anymore.\n\n12:40.160 --> 12:44.960\n And it took a lot of courage to take it off because culturally it's not, it's okay if\n\n12:44.960 --> 12:48.960\n you don't wear it, but it's really not okay to wear it and then take it off.\n\n12:50.000 --> 12:56.480\n But you're still, so you have to do that while still maintaining a deep core and pride in\n\n12:56.480 --> 13:01.680\n the origins, in your origin story.\n\n13:02.400 --> 13:02.960\n Totally.\n\n13:02.960 --> 13:06.640\n So still being Egyptian, still being a Muslim.\n\n13:06.640 --> 13:07.200\n Right.\n\n13:07.200 --> 13:14.240\n And being, I think generally like faith driven, but yeah.\n\n13:14.240 --> 13:17.440\n But what that means changes year by year for you.\n\n13:17.440 --> 13:18.800\n It's like a personal journey.\n\n13:18.800 --> 13:20.080\n Yeah, exactly.\n\n13:20.080 --> 13:23.120\n What would you say is the role of faith in that part of the world?\n\n13:23.120 --> 13:26.480\n Like, how do you see, you mentioned it a bit in the book too.\n\n13:26.480 --> 13:27.040\n Yeah.\n\n13:27.600 --> 13:34.480\n I mean, I think, I think there is something really powerful about just believing that\n\n13:34.480 --> 13:39.200\n there's a bigger force, you know, there's a kind of surrendering, I guess, that comes\n\n13:39.200 --> 13:43.360\n with religion and you surrender and you have this deep conviction that it's going to be\n\n13:43.360 --> 13:44.480\n okay, right?\n\n13:44.480 --> 13:48.320\n Like the universe is out to like do amazing things for you and it's going to be okay.\n\n13:48.880 --> 13:50.080\n And there's strength to that.\n\n13:50.080 --> 13:57.040\n Like even when you're going through adversity, you just know that it's going to work out.\n\n13:57.040 --> 13:59.440\n Yeah, it gives you like an inner peace, a calmness.\n\n13:59.440 --> 14:00.640\n Exactly, exactly.\n\n14:00.640 --> 14:04.880\n Yeah, that's, it's faith in all the meanings of that word.\n\n14:04.880 --> 14:05.440\n Right.\n\n14:05.440 --> 14:07.200\n Faith that everything is going to be okay.\n\n14:07.200 --> 14:12.320\n And it is because time passes and time cures all things.\n\n14:12.320 --> 14:15.200\n It's like a calmness with the chaos of the world.\n\n14:15.200 --> 14:15.680\n Yeah.\n\n14:15.680 --> 14:22.320\n And also there's like a silver, I'm a true believer of this, that something at the specific\n\n14:22.320 --> 14:27.120\n moment in time can look like it's catastrophic and it's not what you wanted in life.\n\n14:28.800 --> 14:32.880\n But then time passes and then you look back and there's the silver lining, right?\n\n14:32.880 --> 14:36.320\n It maybe closed the door, but it opened a new door for you.\n\n14:37.120 --> 14:42.880\n And so I'm a true believer in that, that, you know, there's a silver lining in almost\n\n14:42.880 --> 14:47.120\n anything in life, you just have to have this like, yeah, faith or conviction that it's\n\n14:47.120 --> 14:47.760\n going to work out.\n\n14:47.760 --> 14:50.720\n Yeah, it's such a beautiful way to see a shady feeling.\n\n14:50.720 --> 14:56.320\n So if you feel shady about a current situation, I mean, it almost is always true.\n\n14:57.840 --> 15:04.800\n Unless it's the cliche thing of if it doesn't kill you, whatever doesn't kill you makes\n\n15:04.800 --> 15:05.360\n you stronger.\n\n15:05.360 --> 15:13.200\n It's, it does seem that over time when you take a perspective on things that the hardest\n\n15:13.200 --> 15:16.960\n moments and periods of your life are the most meaningful.\n\n15:18.400 --> 15:19.280\n Yeah, yeah.\n\n15:19.280 --> 15:21.520\n So over time you get to have that perspective.\n\n15:21.520 --> 15:22.020\n Right.\n\n15:23.760 --> 15:30.000\n What about, because you mentioned Kuwait, what about, let me ask you about war.\n\n15:30.000 --> 15:35.680\n What's the role of war and peace, maybe even the big love and hate in that part of\n\n15:35.680 --> 15:39.920\n the world, because it does seem to be a part of the world where there's turmoil.\n\n15:40.720 --> 15:42.720\n There was turmoil, there's still turmoil.\n\n15:44.560 --> 15:46.480\n It is so unfortunate, honestly.\n\n15:46.480 --> 15:53.760\n It's, it's such a waste of human resources and, and, and yeah, and human mindshare.\n\n15:53.760 --> 15:57.280\n I mean, and at the end of the day, we all kind of want the same things.\n\n15:57.280 --> 16:02.560\n We want, you know, we want a human connection, we want joy, we want to feel fulfilled, we\n\n16:02.560 --> 16:05.760\n want to feel, you know, a life of purpose.\n\n16:05.760 --> 16:12.160\n And I just, I just find it baffling, honestly, that we are still having to grapple with that.\n\n16:14.160 --> 16:15.840\n I have a story to share about this.\n\n16:15.840 --> 16:21.840\n You know, I grew up, I'm Egyptian, American now, but, but, you know, originally from Egypt.\n\n16:21.840 --> 16:28.080\n And when I first got to Cambridge, it turned out my officemate, like my PhD kind of, you\n\n16:28.080 --> 16:31.920\n know, she ended up, you know, we ended up becoming friends, but she was from Israel.\n\n16:32.960 --> 16:36.400\n And we didn't know, yeah, we didn't know how it was going to be like.\n\n16:37.760 --> 16:40.000\n Did you guys sit there just staring at each other for a bit?\n\n16:41.040 --> 16:44.560\n Actually, she, because I arrived before she did.\n\n16:44.560 --> 16:50.320\n And it turns out she emailed our PhD advisor and asked him if she thought it was going\n\n16:50.320 --> 16:52.320\n to be okay.\n\n16:52.320 --> 16:52.800\n Yeah.\n\n16:52.800 --> 16:55.040\n And this is around 9 11 too.\n\n16:55.040 --> 16:55.680\n Yeah.\n\n16:55.680 --> 17:01.280\n And, and Peter, Peter Robinson, our PhD advisor was like, yeah, like, this is an academic\n\n17:01.280 --> 17:02.720\n institution, just show up.\n\n17:02.720 --> 17:04.480\n And we became super good friends.\n\n17:04.480 --> 17:07.200\n We were both new moms.\n\n17:07.200 --> 17:09.200\n Like we both had our kids during our PhD.\n\n17:09.200 --> 17:11.360\n We were both doing artificial emotional intelligence.\n\n17:11.360 --> 17:12.320\n She was looking at speech.\n\n17:12.320 --> 17:13.680\n I was looking at the face.\n\n17:13.680 --> 17:16.480\n We just had so the culture was so similar.\n\n17:17.040 --> 17:18.320\n Our jokes were similar.\n\n17:18.320 --> 17:24.080\n It was just, I was like, why on earth are our countries, why is there all this like\n\n17:24.080 --> 17:25.200\n war and tension?\n\n17:25.200 --> 17:27.360\n And I think it falls back to the narrative, right?\n\n17:27.360 --> 17:30.720\n If you change the narrative, like whoever creates this narrative of war.\n\n17:31.840 --> 17:32.400\n I don't know.\n\n17:32.400 --> 17:33.920\n We should have women run the world.\n\n17:34.640 --> 17:36.480\n Yeah, that's one solution.\n\n17:37.520 --> 17:40.640\n The good women, because there's also evil women in the world.\n\n17:40.640 --> 17:41.360\n True, okay.\n\n17:43.920 --> 17:47.280\n But yes, yes, there could be less war if women ran the world.\n\n17:47.280 --> 17:52.960\n The other aspect is, it doesn't matter the gender, the people in power.\n\n17:54.560 --> 17:59.280\n I get to see this with Ukraine and Russia and different parts of the world around that\n\n17:59.280 --> 18:00.000\n conflict now.\n\n18:00.800 --> 18:04.320\n And that's happening in Yemen as well and everywhere else.\n\n18:05.200 --> 18:09.280\n There's these narratives told by the leaders to the populace.\n\n18:09.840 --> 18:12.400\n And those narratives take hold and everybody believes that.\n\n18:12.400 --> 18:17.360\n And they have a distorted view of the humanity on the other side.\n\n18:17.920 --> 18:25.120\n In fact, especially during war, you don't even see the people on the other side as human\n\n18:25.120 --> 18:30.960\n or as equal intelligence or worth or value as you.\n\n18:30.960 --> 18:40.000\n You tell all kinds of narratives about them being Nazis or dumb or whatever narrative\n\n18:40.000 --> 18:42.400\n you want to weave around that or evil.\n\n18:44.720 --> 18:49.120\n But I think when you actually meet them face to face, you realize they're like the same.\n\n18:49.120 --> 18:50.400\n Exactly, right?\n\n18:50.400 --> 18:58.960\n It's actually a big shock for people to realize that they've been essentially lied to within\n\n18:58.960 --> 19:00.000\n their country.\n\n19:00.000 --> 19:05.840\n And I kind of have faith that social media, as ridiculous as it is to say, or any kind\n\n19:05.840 --> 19:13.520\n of technology, is able to bypass the walls that governments put up and connect people\n\n19:13.520 --> 19:14.000\n directly.\n\n19:14.000 --> 19:20.880\n And then you get to realize, oh, people fall in love across different nations and religions\n\n19:20.880 --> 19:21.440\n and so on.\n\n19:21.440 --> 19:25.920\n And that, I think, ultimately can cure a lot of our ills, especially in person.\n\n19:26.640 --> 19:32.400\n I also think that if leaders met in person, they'd have a conversation that could cure\n\n19:32.400 --> 19:37.680\n a lot of the ills of the world, especially in private.\n\n19:37.680 --> 19:41.280\n Let me ask you about the women running the world.\n\n19:42.320 --> 19:49.440\n So gender does, in part, perhaps shape the landscape of just our human experience.\n\n19:51.040 --> 19:57.760\n So in what ways was it limiting and in what ways was it empowering for you to be a woman\n\n19:57.760 --> 19:58.560\n in the Middle East?\n\n19:58.560 --> 20:03.920\n I think, just kind of going back to my comment on women running the world, I think it comes\n\n20:03.920 --> 20:08.800\n back to empathy, which has been a common thread throughout my entire career.\n\n20:08.800 --> 20:11.120\n And it's this idea of human connection.\n\n20:12.320 --> 20:16.800\n Once you build common ground with a person or a group of people, you build trust, you\n\n20:16.800 --> 20:20.160\n build loyalty, you build friendship.\n\n20:20.160 --> 20:24.480\n And then you can turn that into behavior change and motivation and persuasion.\n\n20:24.480 --> 20:29.520\n So it's like, empathy and emotions are just at the center of everything we do.\n\n20:30.720 --> 20:38.080\n And I think being from the Middle East, kind of this human connection is very strong.\n\n20:38.080 --> 20:44.640\n We have this running joke that if you come to Egypt for a visit, people will know everything\n\n20:44.640 --> 20:46.000\n about your life right away, right?\n\n20:46.000 --> 20:48.400\n I have no problems asking you about your personal life.\n\n20:48.400 --> 20:53.200\n There's no boundaries, really, no personal boundaries in terms of getting to know people.\n\n20:53.200 --> 20:55.680\n We get emotionally intimate very, very quickly.\n\n20:56.400 --> 21:00.880\n But I think people just get to know each other authentically, I guess.\n\n21:01.680 --> 21:05.040\n There isn't this superficial level of getting to know people.\n\n21:05.040 --> 21:06.880\n You just try to get to know people really deeply.\n\n21:06.880 --> 21:08.080\n Empathy is a part of that.\n\n21:08.080 --> 21:08.640\n Totally.\n\n21:08.640 --> 21:15.680\n Because you can put yourself in this person's shoe and kind of, yeah, imagine what challenges\n\n21:15.680 --> 21:20.320\n they're going through, and so I think I've definitely taken that with me.\n\n21:21.760 --> 21:26.960\n Generosity is another one too, like just being generous with your time and love and attention\n\n21:26.960 --> 21:30.480\n and even with your wealth, right?\n\n21:30.480 --> 21:32.800\n Even if you don't have a lot of it, you're still very generous.\n\n21:32.800 --> 21:33.840\n And I think that's another...\n\n21:34.720 --> 21:38.000\n Enjoying the humanity of other people.\n\n21:38.000 --> 21:44.720\n And so do you think there's a useful difference between men and women in that?\n\n21:44.720 --> 21:47.200\n In that aspect and empathy?\n\n21:48.880 --> 21:56.160\n Or is doing these kind of big general groups, does that hinder progress?\n\n21:56.880 --> 21:59.760\n Yeah, I actually don't want to overgeneralize.\n\n21:59.760 --> 22:03.520\n I mean, some of the men I know are like the most empathetic humans.\n\n22:03.520 --> 22:05.200\n Yeah, I strive to be empathetic.\n\n22:05.200 --> 22:07.120\n Yeah, you're actually very empathetic.\n\n22:10.640 --> 22:13.360\n Yeah, so I don't want to overgeneralize.\n\n22:13.360 --> 22:18.400\n Although one of the researchers I worked with when I was at Cambridge, Professor Simon Baron Cohen,\n\n22:18.400 --> 22:25.120\n he's Sacha Baron Cohen's cousin, and he runs the Autism Research Center at Cambridge,\n\n22:25.120 --> 22:29.600\n and he's written multiple books on autism.\n\n22:29.600 --> 22:35.040\n And one of his theories is the empathy scale, like the systemizers and the empathizers,\n\n22:35.040 --> 22:42.800\n and there's a disproportionate amount of computer scientists and engineers who are\n\n22:42.800 --> 22:51.760\n systemizers and perhaps not great empathizers, and then there's more men in that bucket,\n\n22:51.760 --> 22:56.000\n I guess, than women, and then there's more women in the empathizers bucket.\n\n22:56.000 --> 22:58.160\n So again, not to overgeneralize.\n\n22:58.160 --> 22:59.520\n I sometimes wonder about that.\n\n22:59.520 --> 23:04.560\n It's been frustrating to me how many, I guess, systemizers there are in the field of robotics.\n\n23:05.200 --> 23:05.700\n Yeah.\n\n23:06.240 --> 23:10.000\n It's actually encouraging to me because I care about, obviously, social robotics,\n\n23:10.000 --> 23:18.720\n and because there's more opportunity for people that are empathic.\n\n23:18.720 --> 23:19.200\n Exactly.\n\n23:19.200 --> 23:20.400\n I totally agree.\n\n23:20.400 --> 23:20.960\n Well, right?\n\n23:20.960 --> 23:21.760\n So it's nice.\n\n23:21.760 --> 23:22.160\n Yes.\n\n23:22.160 --> 23:29.200\n So every robotics I talk to, they don't see the human as interesting, as it's not exciting.\n\n23:29.200 --> 23:32.160\n You want to avoid the human at all costs.\n\n23:32.160 --> 23:39.200\n It's a safety concern to be touching the human, which it is, but it is also an opportunity\n\n23:39.200 --> 23:43.360\n for deep connection or collaboration or all that kind of stuff.\n\n23:43.360 --> 23:48.160\n And because most brilliant roboticists don't care about the human, it's an opportunity,\n\n23:49.280 --> 23:53.840\n in your case, it's a business opportunity too, but in general, an opportunity to explore\n\n23:53.840 --> 23:54.640\n those ideas.\n\n23:54.640 --> 24:03.760\n So in this beautiful journey to Cambridge, to UK, and then to America, what's the moment\n\n24:03.760 --> 24:09.760\n or moments that were most transformational for you as a scientist and as a leader?\n\n24:09.760 --> 24:16.320\n So you became an exceptionally successful CEO, founder, researcher, scientist, and so on.\n\n24:18.320 --> 24:25.040\n Was there a face shift there where, like, I can be somebody, I can really do something\n\n24:25.040 --> 24:25.600\n in this world?\n\n24:26.640 --> 24:26.880\n Yeah.\n\n24:26.880 --> 24:29.680\n So actually, just kind of a little bit of background.\n\n24:29.680 --> 24:36.960\n So the reason why I moved from Cairo to Cambridge, UK to do my PhD is because I had a very clear\n\n24:36.960 --> 24:37.920\n career plan.\n\n24:37.920 --> 24:43.280\n I was like, okay, I'll go abroad, get my PhD, going to crush it in three or four years,\n\n24:43.280 --> 24:44.720\n come back to Egypt and teach.\n\n24:45.360 --> 24:47.520\n It was very clear, very well laid out.\n\n24:47.520 --> 24:49.280\n Was topic clear or no?\n\n24:49.280 --> 24:54.400\n The topic, well, I did my PhD around building artificial emotional intelligence and looking\n\n24:54.400 --> 24:54.400\n at...\n\n24:54.400 --> 24:58.880\n But in your master plan ahead of time, when you're sitting by the mango tree, did you\n\n24:58.880 --> 25:00.480\n know it's going to be artificial intelligence?\n\n25:00.480 --> 25:02.880\n No, no, no, that I did not know.\n\n25:02.880 --> 25:07.840\n Although I think I kind of knew that I was going to be doing computer science, but I\n\n25:07.840 --> 25:09.600\n didn't know the specific area.\n\n25:10.160 --> 25:11.120\n But I love teaching.\n\n25:11.120 --> 25:12.320\n I mean, I still love teaching.\n\n25:13.120 --> 25:17.760\n So I just, yeah, I just wanted to go abroad, get a PhD, come back, teach.\n\n25:18.960 --> 25:19.760\n Why computer science?\n\n25:19.760 --> 25:21.200\n Can we just linger on that?\n\n25:21.200 --> 25:21.440\n What?\n\n25:21.440 --> 25:25.520\n Because you're such an empathic person who cares about emotion, humans and so on.\n\n25:25.520 --> 25:31.440\n Isn't, aren't computers cold and emotionless and just...\n\n25:31.440 --> 25:32.480\n We're changing that.\n\n25:32.480 --> 25:38.400\n Yeah, I know, but like, isn't that the, or did you see computers as the, having the\n\n25:38.400 --> 25:42.400\n capability to actually connect with humans?\n\n25:42.400 --> 25:46.720\n I think that was like my takeaway from my experience just growing up, like computers\n\n25:46.720 --> 25:49.840\n sit at the center of how we connect and communicate with one another, right?\n\n25:50.320 --> 25:51.760\n Or technology in general.\n\n25:51.760 --> 25:54.640\n Like I remember my first experience being away from my parents.\n\n25:54.640 --> 25:58.800\n We communicated with a fax machine, but thank goodness for the fax machine, because we\n\n25:58.800 --> 26:00.800\n could send letters back and forth to each other.\n\n26:00.800 --> 26:02.480\n This was pre emails and stuff.\n\n26:04.080 --> 26:09.600\n So I think, I think there's, I think technology can be not just transformative in terms of\n\n26:09.600 --> 26:10.960\n productivity, et cetera.\n\n26:10.960 --> 26:14.080\n It actually does change how we connect with one another.\n\n26:14.960 --> 26:16.720\n Can I just defend the fax machine?\n\n26:16.720 --> 26:22.720\n There's something like the haptic feel because the email is all digital.\n\n26:22.720 --> 26:23.760\n There's something really nice.\n\n26:23.760 --> 26:25.600\n I still write letters to people.\n\n26:26.400 --> 26:30.160\n There's something nice about the haptic aspect of the fax machine, because you still have\n\n26:30.160 --> 26:35.200\n to press, you still have to do something in the physical world to make this thing a reality.\n\n26:35.200 --> 26:39.760\n Right, and then it like comes out as a printout and you can actually touch it and read it.\n\n26:39.760 --> 26:40.240\n Yeah.\n\n26:40.240 --> 26:43.600\n There's something, there's something lost when it's just an email.\n\n26:44.960 --> 26:51.440\n Obviously I wonder how we can regain some of that in the digital world, which goes to\n\n26:51.440 --> 26:53.760\n the metaverse and all those kinds of things.\n\n26:53.760 --> 26:54.800\n We'll talk about it anyway.\n\n26:54.800 --> 26:57.680\n So, actually do you question on that one?\n\n26:57.680 --> 27:00.400\n Do you still, do you have photo albums anymore?\n\n27:00.400 --> 27:02.000\n Do you still print photos?\n\n27:03.600 --> 27:06.320\n No, no, but I'm a minimalist.\n\n27:06.320 --> 27:06.640\n Okay.\n\n27:06.640 --> 27:12.400\n So it was one of the, one of the painful steps in my life was to scan all the photos and\n\n27:12.400 --> 27:16.320\n let go of them and then let go of all my books.\n\n27:16.320 --> 27:17.600\n You let go of your books?\n\n27:17.600 --> 27:18.000\n Yeah.\n\n27:18.000 --> 27:19.920\n Switch to Kindle, everything Kindle.\n\n27:19.920 --> 27:20.800\n Yeah.\n\n27:20.800 --> 27:29.440\n So I thought, I thought, okay, think 30 years from now, nobody's going to have books anymore.\n\n27:29.440 --> 27:32.160\n The technology of digital books is going to get better and better and better.\n\n27:32.160 --> 27:36.240\n Are you really going to be the guy that's still romanticizing physical books?\n\n27:36.240 --> 27:39.440\n Are you going to be the old man on the porch who's like kids?\n\n27:39.440 --> 27:39.760\n Yes.\n\n27:40.480 --> 27:45.040\n So just get used to it because it was, it felt, it still feels a little bit uncomfortable\n\n27:45.040 --> 27:48.560\n to read on a Kindle, but get used to it.\n\n27:48.560 --> 27:53.200\n Like you always, I mean, I'm trying to learn new programming language is always,\n\n27:53.200 --> 27:56.720\n like with technology, you have to kind of challenge yourself to adapt to it.\n\n27:56.720 --> 27:58.880\n You know, I forced myself to use TikTok.\n\n27:58.880 --> 28:01.440\n No, that thing doesn't need much forcing.\n\n28:01.440 --> 28:05.920\n It pulls you in like a, like the worst kind of, or the best kind of drug.\n\n28:05.920 --> 28:08.560\n Anyway, yeah.\n\n28:08.560 --> 28:11.200\n So yeah, but I do love haptic things.\n\n28:11.760 --> 28:13.440\n There's a magic to the haptic.\n\n28:13.440 --> 28:19.520\n Even like touchscreens, it's tricky to get right, to get the experience of a button.\n\n28:19.520 --> 28:19.760\n Yeah.\n\n28:22.400 --> 28:23.760\n Anyway, what were we talking about?\n\n28:23.760 --> 28:29.680\n So AI, so the journey, your whole plan was to come back to Cairo and teach.\n\n28:30.560 --> 28:30.800\n Right.\n\n28:31.840 --> 28:32.560\n And then.\n\n28:32.560 --> 28:33.840\n What did the plan go wrong?\n\n28:33.840 --> 28:34.720\n Yeah, exactly.\n\n28:34.720 --> 28:35.120\n Right.\n\n28:35.120 --> 28:39.120\n And then I get to Cambridge and I fall in love with the idea of research.\n\n28:39.120 --> 28:39.440\n Right.\n\n28:39.440 --> 28:41.520\n And kind of embarking on a path.\n\n28:41.520 --> 28:43.680\n Nobody's explored this path before.\n\n28:43.680 --> 28:45.440\n You're building stuff that nobody's built before.\n\n28:45.440 --> 28:46.960\n And it's challenging and it's hard.\n\n28:46.960 --> 28:48.720\n And there's a lot of nonbelievers.\n\n28:49.280 --> 28:50.960\n I just totally love that.\n\n28:50.960 --> 28:56.320\n And at the end of my PhD, I think it's the meeting that changed the trajectory of my life.\n\n28:56.880 --> 29:02.160\n Professor Roslyn Picard, who's, she runs the Affective Computing Group at the MIT Media Lab.\n\n29:02.160 --> 29:03.040\n I had read her book.\n\n29:03.040 --> 29:07.520\n I, you know, I was like following, following, following all her research.\n\n29:07.520 --> 29:08.880\n AKA Ros.\n\n29:08.880 --> 29:10.080\n Yes, AKA Ros.\n\n29:10.080 --> 29:10.880\n Yes.\n\n29:10.880 --> 29:15.680\n And she was giving a talk at a pattern recognition conference in Cambridge.\n\n29:16.320 --> 29:18.000\n And she had a couple of hours to kill.\n\n29:18.000 --> 29:22.560\n So she emailed the lab and she said, you know, if any students want to meet with me, like,\n\n29:22.560 --> 29:23.840\n just, you know, sign up here.\n\n29:24.640 --> 29:29.920\n And so I signed up for slot and I spent like the weeks leading up to it preparing for this\n\n29:29.920 --> 29:33.680\n meeting and I want to show her a demo of my research and everything.\n\n29:34.400 --> 29:36.640\n And we met and we ended up hitting it off.\n\n29:36.640 --> 29:38.080\n Like we totally clicked.\n\n29:38.080 --> 29:42.480\n And at the end of the meeting, she said, do you want to come work with me as a postdoc\n\n29:42.480 --> 29:43.040\n at MIT?\n\n29:44.720 --> 29:45.600\n And this is what I told her.\n\n29:45.600 --> 29:49.280\n I was like, okay, this would be a dream come true, but there's a husband waiting for me\n\n29:49.280 --> 29:49.840\n in Cairo.\n\n29:49.840 --> 29:51.120\n I kind of have to go back.\n\n29:51.120 --> 29:51.360\n Yeah.\n\n29:52.080 --> 29:52.960\n She said, it's fine.\n\n29:52.960 --> 29:53.600\n Just commute.\n\n29:54.560 --> 29:57.600\n And I literally started commuting between Cairo and Boston.\n\n29:59.200 --> 30:01.200\n Yeah, it was, it was a long commute.\n\n30:01.200 --> 30:05.520\n And I didn't, I did that like every few weeks I would, you know, hop on a plane and go to\n\n30:05.520 --> 30:06.400\n Boston.\n\n30:06.400 --> 30:08.480\n But that, that changed the trajectory of my life.\n\n30:08.480 --> 30:12.880\n There was no, I kind of outgrew my dreams, right?\n\n30:12.880 --> 30:16.720\n I didn't want to go back to Egypt anymore and be faculty.\n\n30:16.720 --> 30:18.320\n Like that was no longer my dream.\n\n30:18.320 --> 30:19.200\n I had a dream.\n\n30:19.200 --> 30:21.920\n What was the, what was it like to be at MIT?\n\n30:22.560 --> 30:24.080\n What was that culture shock?\n\n30:25.040 --> 30:31.040\n You mean America in general, but also, I mean, Cambridge has its own culture, right?\n\n30:31.040 --> 30:34.000\n So what was MIT like and what was America like?\n\n30:34.000 --> 30:37.600\n I think, I wonder if that's similar to your experience at MIT.\n\n30:37.600 --> 30:45.200\n I was just, at the Media Lab in particular, I was just really, impressed is not the right\n\n30:45.200 --> 30:45.700\n word.\n\n30:46.240 --> 30:54.800\n I didn't expect the openness to like innovation and the acceptance of taking a risk and failing.\n\n30:54.800 --> 30:58.320\n Like failure isn't really accepted back in Egypt, right?\n\n30:58.320 --> 30:59.040\n You don't want to fail.\n\n30:59.040 --> 31:03.200\n Like there's a fear of failure, which I think has been hardwired in my brain.\n\n31:03.200 --> 31:05.840\n But you get to MIT and it's okay to start things.\n\n31:05.840 --> 31:08.000\n And if they don't work out, like it's okay.\n\n31:08.000 --> 31:09.120\n You pivot to another idea.\n\n31:09.840 --> 31:12.640\n And that kind of thinking was just very new to me.\n\n31:12.640 --> 31:13.600\n That's liberating.\n\n31:13.600 --> 31:18.880\n Well, Media Lab, for people who don't know, MIT Media Lab is its own beautiful thing because\n\n31:19.840 --> 31:24.000\n they, I think more than other places at MIT, reach for big ideas.\n\n31:24.000 --> 31:28.480\n And like they try, I mean, I think, I mean, depending of course on who, but certainly\n\n31:28.480 --> 31:36.160\n with Roslyn, you try wild stuff, you try big things and crazy things and also try to take\n\n31:36.160 --> 31:38.240\n things to completion so you can demo them.\n\n31:38.240 --> 31:42.240\n So always, always, always have a demo.\n\n31:42.240 --> 31:46.880\n Like if you go, one of the sad things to me about robotics labs at MIT, and there's like\n\n31:46.880 --> 31:53.680\n over 30, I think, is like, usually when you show up to a robotics lab, there's not a single\n\n31:53.680 --> 31:55.200\n working robot, they're all broken.\n\n31:55.760 --> 31:57.280\n All the robots are broken.\n\n31:57.280 --> 32:01.600\n The robots are broken, which is like the normal state of things because you're working on\n\n32:01.600 --> 32:02.080\n them.\n\n32:02.080 --> 32:08.880\n But it would be nice if we lived in a world where robotics labs had some robots functioning.\n\n32:08.880 --> 32:13.360\n One of my like favorite moments that just sticks with me, I visited Boston Dynamics\n\n32:13.360 --> 32:19.680\n and there was a, first of all, seeing so many spots, so many legged robots in one place.\n\n32:20.240 --> 32:21.280\n I'm like, I'm home.\n\n32:22.720 --> 32:24.880\n But the, yeah.\n\n32:24.880 --> 32:26.160\n This is where I was built.\n\n32:27.200 --> 32:33.360\n The cool thing was just to see there was a random robot spot was walking down the hall.\n\n32:33.360 --> 32:37.120\n It's probably doing mapping, but it looked like he wasn't doing anything and he was wearing\n\n32:37.120 --> 32:39.120\n he or she, I don't know.\n\n32:39.120 --> 32:44.640\n But it, well, I like, in my mind, there are people, they have a backstory, but this one\n\n32:44.640 --> 32:48.640\n in particular definitely has a backstory because he was wearing a cowboy hat.\n\n32:48.640 --> 32:54.160\n So I just saw a spot robot with a cowboy hat walking down the hall and there was just this\n\n32:54.160 --> 32:58.880\n feeling like there's a life, like he has a life.\n\n32:58.880 --> 33:02.240\n He probably has to commute back to his family at night.\n\n33:02.240 --> 33:07.440\n Like there's a, there's a feeling like there's life instilled in this robot and it's magical.\n\n33:07.440 --> 33:07.760\n I don't know.\n\n33:07.760 --> 33:09.520\n It was, it was kind of inspiring to see.\n\n33:09.520 --> 33:12.000\n Did it say hello to, did he say hello to you?\n\n33:12.000 --> 33:15.360\n No, it's very, there's a focus nature to the robot.\n\n33:15.360 --> 33:16.400\n No, no, listen.\n\n33:16.400 --> 33:18.960\n I love competence and focus and great.\n\n33:18.960 --> 33:25.200\n Like he was not going to get distracted by the, the shallowness of small talk.\n\n33:25.200 --> 33:27.520\n There's a job to be done and he was doing it.\n\n33:27.520 --> 33:30.560\n So anyway, the fact that it was working is a beautiful thing.\n\n33:30.560 --> 33:35.440\n And I think Media Lab really prides itself on trying to always have a thing that's working\n\n33:35.440 --> 33:36.480\n that you could show off.\n\n33:36.480 --> 33:36.800\n Yes.\n\n33:36.800 --> 33:38.960\n We used to call it a demo or die.\n\n33:38.960 --> 33:43.520\n You, you could not, yeah, you could not like show up with like PowerPoint or something.\n\n33:43.520 --> 33:48.080\n You actually had to have a working, you know what, my son who is now 13, I don't know if\n\n33:48.080 --> 33:52.880\n this is still his life long goal or not, but when he was a little younger, his dream is\n\n33:52.880 --> 33:57.280\n to build an island that's just inhabited by robots, like no humans.\n\n33:57.280 --> 34:01.920\n He just wants all these robots to be connecting and having fun and there you go.\n\n34:01.920 --> 34:06.480\n Does he have human, does he have an idea of which robots he loves most?\n\n34:06.480 --> 34:09.280\n Is it, is it Roomba like robots?\n\n34:09.280 --> 34:10.800\n Is it humanoid robots?\n\n34:10.800 --> 34:13.920\n Robot dogs, or it's not clear yet.\n\n34:13.920 --> 34:19.280\n We used to have a Jibo, which was one of the MIT Media Lab spin outs and he used to love\n\n34:19.280 --> 34:30.400\n the giant head that spins and rotate and it's an eye or like not glowing like Cal 9000,\n\n34:30.400 --> 34:31.760\n but the friendly version.\n\n34:31.760 --> 34:34.080\n He loved that.\n\n34:34.080 --> 34:38.240\n And then he just loves, uh, um,\n\n34:38.240 --> 34:44.160\n yeah, he just, he, I think he loves all forms of robots actually.\n\n34:44.160 --> 34:46.800\n So embodied intelligence.\n\n34:46.800 --> 34:47.760\n Yes.\n\n34:47.760 --> 34:54.320\n I like, I personally like legged robots, especially, uh, anything that can wiggle its butt.\n\n34:55.120 --> 35:00.960\n No, that's not the definition of what I love, but that's just technically what I've been\n\n35:00.960 --> 35:01.760\n working on recently.\n\n35:01.760 --> 35:06.480\n Except I have a bunch of legged robots now in Austin and I've been doing, I was, I've\n\n35:06.480 --> 35:12.400\n been trying to, uh, have them communicate affection with their body in different ways\n\n35:12.400 --> 35:15.120\n just for art, for art really.\n\n35:15.120 --> 35:20.080\n Cause I love the idea of walking around with the robots, like, uh, as you would with a\n\n35:20.080 --> 35:20.400\n dog.\n\n35:20.400 --> 35:23.120\n I think it's inspiring to a lot of people, especially young people.\n\n35:23.120 --> 35:24.960\n Like kids love, kids love it.\n\n35:25.760 --> 35:31.600\n Parents like adults are scared of robots, but kids don't have this kind of weird construction\n\n35:31.600 --> 35:32.880\n of the world that's full of evil.\n\n35:32.880 --> 35:34.480\n They love cool things.\n\n35:34.480 --> 35:35.040\n Yeah.\n\n35:35.040 --> 35:40.080\n I remember when Adam was in first grade, so he must have been like seven or so.\n\n35:40.080 --> 35:44.960\n I went in to his class with a whole bunch of robots and like the emotion AI demo and\n\n35:44.960 --> 35:45.680\n da da.\n\n35:45.680 --> 35:51.120\n And I asked the kids, I was like, do you, would you kids want to have a robot, you know,\n\n35:52.000 --> 35:53.600\n robot friend or robot companion?\n\n35:53.600 --> 35:54.560\n Everybody said yes.\n\n35:54.560 --> 35:58.880\n And they wanted it for all sorts of things, like to help them with their math homework\n\n35:58.880 --> 36:00.160\n and to like be a friend.\n\n36:00.160 --> 36:07.520\n So there's, it just struck me how there was no fear of robots was a lot of adults have\n\n36:07.520 --> 36:09.280\n that like us versus them.\n\n36:10.720 --> 36:11.920\n Yeah, none of that.\n\n36:11.920 --> 36:16.320\n Of course you want to be very careful because you still have to look at the lessons of history\n\n36:16.960 --> 36:21.920\n and how robots can be used by the power centers of the world to abuse your rights and all\n\n36:21.920 --> 36:22.480\n that kind of stuff.\n\n36:22.480 --> 36:30.480\n But mostly it's good to enter anything new with an excitement and an optimism.\n\n36:30.480 --> 36:35.200\n Speaking of Roz, what have you learned about science and life from Rosalind Picard?\n\n36:35.200 --> 36:39.920\n Oh my God, I've learned so many things about life from Roz.\n\n36:41.200 --> 36:44.880\n I think the thing I learned the most is perseverance.\n\n36:47.600 --> 36:51.280\n When I first met Roz, we applied and she invited me to be her postdoc.\n\n36:51.280 --> 36:57.040\n We applied for a grant to the National Science Foundation to apply some of our research to\n\n36:57.040 --> 36:57.760\n autism.\n\n36:57.760 --> 36:59.360\n And we got back.\n\n37:00.800 --> 37:01.520\n We were rejected.\n\n37:01.520 --> 37:02.240\n Rejected.\n\n37:02.240 --> 37:02.480\n Yeah.\n\n37:02.480 --> 37:03.120\n And the reasoning was...\n\n37:03.120 --> 37:06.000\n The first time you were rejected for fun, yeah.\n\n37:06.000 --> 37:10.320\n Yeah, it was, and I basically, I just took the rejection to mean, okay, we're rejected.\n\n37:10.320 --> 37:12.720\n It's done, like end of story, right?\n\n37:12.720 --> 37:15.120\n And Roz was like, it's great news.\n\n37:15.120 --> 37:16.080\n They love the idea.\n\n37:16.080 --> 37:18.160\n They just don't think we can do it.\n\n37:18.160 --> 37:21.360\n So let's build it, show them, and then reapply.\n\n37:22.400 --> 37:25.520\n And it was that, oh my God, that story totally stuck with me.\n\n37:26.320 --> 37:29.760\n And she's like that in every aspect of her life.\n\n37:29.760 --> 37:32.080\n She just does not take no for an answer.\n\n37:32.080 --> 37:34.080\n To reframe all negative feedback.\n\n37:35.360 --> 37:36.400\n As a challenge.\n\n37:36.400 --> 37:37.280\n As a challenge.\n\n37:37.280 --> 37:38.560\n As a challenge.\n\n37:38.560 --> 37:40.000\n Yes, they liked this.\n\n37:40.000 --> 37:40.720\n Yeah, yeah, yeah.\n\n37:40.720 --> 37:42.160\n It was a riot.\n\n37:43.200 --> 37:45.040\n What else about science in general?\n\n37:45.040 --> 37:51.680\n About how you see computers and also business and just everything about the world.\n\n37:51.680 --> 37:54.800\n She's a very powerful, brilliant woman like yourself.\n\n37:54.800 --> 37:56.400\n So is there some aspect of that too?\n\n37:57.280 --> 38:00.320\n Yeah, I think Roz is actually also very faith driven.\n\n38:00.320 --> 38:02.720\n She has this like deep belief in conviction.\n\n38:04.240 --> 38:07.200\n Yeah, and in the good in the world and humanity.\n\n38:07.200 --> 38:13.520\n And I think that was meeting her and her family was definitely like a defining moment for me\n\n38:13.520 --> 38:17.520\n because that was when I was like, wow, like you can be of a different background and\n\n38:18.080 --> 38:22.720\n religion and whatever and you can still have the same core values.\n\n38:23.760 --> 38:25.200\n So that was, that was, yeah.\n\n38:26.800 --> 38:27.680\n I'm grateful to her.\n\n38:28.560 --> 38:30.240\n Roz, if you're listening, thank you.\n\n38:30.240 --> 38:31.280\n Yeah, she's great.\n\n38:31.280 --> 38:32.560\n She's been on this podcast before.\n\n38:33.600 --> 38:36.320\n I hope she'll be on, I'm sure she'll be on again.\n\n38:36.320 --> 38:44.720\n And you were the founder and CEO of Effektiva, which is a big company that was acquired by\n\n38:44.720 --> 38:46.400\n another big company, SmartEye.\n\n38:46.960 --> 38:49.120\n And you're now the deputy CEO of SmartEye.\n\n38:49.120 --> 38:51.040\n So you're a powerful leader.\n\n38:51.040 --> 38:51.760\n You're brilliant.\n\n38:51.760 --> 38:52.800\n You're a brilliant scientist.\n\n38:53.360 --> 38:55.040\n A lot of people are inspired by you.\n\n38:55.040 --> 39:00.160\n What advice would you give, especially to young women, but people in general who dream\n\n39:00.160 --> 39:08.080\n of becoming powerful leaders like yourself in a world where perhaps, in a world that\n\n39:09.520 --> 39:17.440\n perhaps doesn't give them a clear, easy path to do so, whether we're talking about Egypt\n\n39:17.440 --> 39:18.080\n or elsewhere?\n\n39:19.920 --> 39:27.680\n You know, hearing you kind of describe me that way, kind of encapsulates, I think what\n\n39:27.680 --> 39:31.200\n I think is the biggest challenge of all, which is believing in yourself, right?\n\n39:32.160 --> 39:37.440\n I have had to like grapple with this, what I call now the Debbie Downer voice in my head.\n\n39:39.360 --> 39:42.720\n The kind of basically, it's just chattering all the time.\n\n39:42.720 --> 39:45.040\n It's basically saying, oh, no, no, no, no, you can't do this.\n\n39:45.040 --> 39:46.320\n Like you're not going to raise money.\n\n39:46.320 --> 39:47.280\n You can't start a company.\n\n39:47.280 --> 39:50.720\n Like what business do you have, like starting a company or running a company or selling\n\n39:50.720 --> 39:51.200\n a company?\n\n39:51.200 --> 39:52.080\n Like you name it.\n\n39:52.080 --> 39:53.120\n It's always like.\n\n39:53.120 --> 40:02.160\n And I think my biggest advice to not just women, but people who are taking a new path\n\n40:02.160 --> 40:07.200\n and, you know, they're not sure, is to not let yourself and let your thoughts be the\n\n40:07.200 --> 40:08.880\n biggest obstacle in your way.\n\n40:09.920 --> 40:16.880\n And I've had to like really work on myself to not be my own biggest obstacle.\n\n40:17.520 --> 40:18.880\n So you got that negative voice.\n\n40:18.880 --> 40:19.380\n Yeah.\n\n40:20.640 --> 40:21.200\n So is that?\n\n40:21.200 --> 40:21.920\n Am I the only one?\n\n40:21.920 --> 40:23.280\n I don't think I'm the only one.\n\n40:23.280 --> 40:25.040\n No, I have that negative voice.\n\n40:25.040 --> 40:29.840\n I'm not exactly sure if it's a bad thing or a good thing.\n\n40:29.840 --> 40:35.440\n I've been really torn about it because it's been a lifelong companions.\n\n40:35.440 --> 40:36.240\n It's hard to know.\n\n40:37.840 --> 40:44.800\n It's kind of, it drives productivity and progress, but it can hold you back from taking\n\n40:44.800 --> 40:45.520\n big leaps.\n\n40:45.520 --> 40:53.120\n I think the best I can say is probably you have to somehow be able to control it, to\n\n40:53.120 --> 40:56.880\n turn it off when it's not useful and turn it on when it's useful.\n\n40:57.680 --> 41:00.400\n Like I have from almost like a third person perspective.\n\n41:00.400 --> 41:00.900\n Right.\n\n41:00.900 --> 41:02.400\n Somebody who's sitting there like.\n\n41:02.400 --> 41:02.900\n Yeah.\n\n41:02.960 --> 41:06.720\n Like, because it is useful to be critical.\n\n41:07.520 --> 41:12.480\n Like after, I just gave a talk yesterday.\n\n41:12.480 --> 41:19.120\n At MIT and I was just, there's so much love and it was such an incredible experience.\n\n41:19.120 --> 41:25.760\n So many amazing people I got a chance to talk to, but afterwards when I went home and just\n\n41:25.760 --> 41:29.680\n took this long walk, it was mostly just negative thoughts about me.\n\n41:29.680 --> 41:34.720\n I don't like one basic stuff like I don't deserve any of it.\n\n41:34.720 --> 41:39.200\n And second is like, like, why did you, that was so bad.\n\n41:39.200 --> 41:44.880\n Second is like, like, why did you, that was so dumb that you said this, that's so dumb.\n\n41:44.880 --> 41:46.960\n Like you should have prepared that better.\n\n41:47.520 --> 41:48.560\n Why did you say this?\n\n41:50.240 --> 41:53.440\n But I think it's good to hear that voice out.\n\n41:54.160 --> 41:54.560\n All right.\n\n41:54.560 --> 41:56.240\n And like sit in that.\n\n41:56.240 --> 41:58.560\n And ultimately I think you grow from that.\n\n41:58.560 --> 42:03.680\n Now, when you're making really big decisions about funding or starting a company or taking\n\n42:03.680 --> 42:10.960\n a leap to go to the UK or take a leap to go to America to work in Media Lab though.\n\n42:10.960 --> 42:11.200\n Yeah.\n\n42:11.200 --> 42:22.160\n There's, that's, you should be able to shut that off then because you should have like\n\n42:22.160 --> 42:26.080\n this weird confidence, almost like faith that you said before that everything's going to\n\n42:26.080 --> 42:26.720\n work out.\n\n42:26.720 --> 42:28.480\n So take the leap of faith.\n\n42:28.480 --> 42:29.520\n Take the leap of faith.\n\n42:30.160 --> 42:32.400\n Despite all the negativity.\n\n42:32.400 --> 42:34.240\n I mean, there's, there's, there's some of that.\n\n42:34.240 --> 42:38.560\n You, you actually tweeted a really nice tweet thread.\n\n42:39.760 --> 42:45.840\n It says, quote, a year ago, a friend recommended I do daily affirmations and I was skeptical,\n\n42:46.800 --> 42:49.360\n but I was going through major transitions in my life.\n\n42:49.360 --> 42:53.520\n So I gave it a shot and it set me on a journey of self acceptance and self love.\n\n42:54.080 --> 42:55.680\n So what was that like?\n\n42:55.680 --> 43:01.360\n Can you maybe talk through this idea of affirmations and how that helped you?\n\n43:01.360 --> 43:02.320\n Yeah.\n\n43:02.320 --> 43:07.200\n Because really like I'm just like me, I'm a kind, I'd like to think of myself as a kind\n\n43:07.200 --> 43:10.320\n person in general, but I'm kind of mean to myself sometimes.\n\n43:10.320 --> 43:10.640\n Yeah.\n\n43:11.280 --> 43:15.440\n And so I've been doing journaling for almost 10 years now.\n\n43:16.720 --> 43:18.880\n I use an app called Day One and it's awesome.\n\n43:18.880 --> 43:22.880\n I just journal and I use it as an opportunity to almost have a conversation with the Debbie\n\n43:22.880 --> 43:24.800\n Downer voice in my, it's like a rebuttal, right?\n\n43:25.520 --> 43:29.120\n Like Debbie Downer says, oh my God, like you, you know, you won't be able to raise this\n\n43:29.120 --> 43:29.680\n round of funding.\n\n43:29.680 --> 43:31.760\n I'm like, okay, let's talk about it.\n\n43:33.120 --> 43:35.520\n I have a track record of doing X, Y, and Z.\n\n43:35.520 --> 43:37.120\n I think I can do this.\n\n43:37.120 --> 43:42.240\n And it's literally like, so I wouldn't, I don't know that I can shut off the voice,\n\n43:42.240 --> 43:44.240\n but I can have a conversation with it.\n\n43:44.240 --> 43:47.760\n And it just, it just, and I bring data to the table, right?\n\n43:49.840 --> 43:50.320\n Nice.\n\n43:50.320 --> 43:53.120\n So that was the journaling part, which I found very helpful.\n\n43:53.760 --> 43:57.600\n But the affirmation took it to a whole next level and I just love it.\n\n43:57.600 --> 44:02.720\n I'm a year into doing this and you literally wake up in the morning and the first thing\n\n44:02.720 --> 44:09.440\n you do, I meditate first and then I write my affirmations and it's the energy I want\n\n44:09.440 --> 44:12.160\n to put out in the world that hopefully will come right back to me.\n\n44:12.160 --> 44:16.560\n So I will say, I always start with my smile lights up the whole world.\n\n44:17.200 --> 44:20.720\n And I kid you not, like people in the street will stop me and say, oh my God, like we love\n\n44:20.720 --> 44:21.360\n your smile.\n\n44:21.360 --> 44:22.320\n Like, yes.\n\n44:22.320 --> 44:28.880\n So, so my affirmations will change depending on, you know, what's happening this day.\n\n44:28.880 --> 44:29.520\n Is it funny?\n\n44:29.520 --> 44:29.840\n I know.\n\n44:29.840 --> 44:31.360\n Don't judge, don't judge.\n\n44:31.360 --> 44:33.840\n No, that's not, laughter's not judgment.\n\n44:33.840 --> 44:35.040\n It's just awesome.\n\n44:35.040 --> 44:42.480\n I mean, it's true, but you're saying affirmations somehow help kind of, I mean, what is it that\n\n44:42.480 --> 44:48.400\n they do work to like remind you of the kind of person you are and the kind of person you\n\n44:48.400 --> 44:53.760\n want to be, which actually may be in reverse order, the kind of person you want to be.\n\n44:53.760 --> 44:56.320\n And that helps you become the kind of person you actually are.\n\n44:56.960 --> 45:01.280\n It's just, it's, it brings intentionality to like what you're doing.\n\n45:01.280 --> 45:01.680\n Right.\n\n45:01.680 --> 45:07.200\n And so, by the way, I was laughing because my affirmations, which I also do are the\n\n45:07.200 --> 45:07.760\n opposite.\n\n45:07.760 --> 45:08.320\n Oh, you do?\n\n45:08.320 --> 45:09.040\n Oh, what do you do?\n\n45:09.040 --> 45:11.920\n I don't, I don't have a, my smile lights up the world.\n\n45:11.920 --> 45:22.240\n Maybe I should add that because like, I, I have, I just, I have, oh boy, it's, it's much\n\n45:22.240 --> 45:30.400\n more stoic, like about focus, about this kind of stuff, but the joy, the emotion that you're\n\n45:30.400 --> 45:32.960\n just in that little affirmation is beautiful.\n\n45:32.960 --> 45:34.160\n So maybe I should add that.\n\n45:35.120 --> 45:38.080\n I have some, I have some like focused stuff, but that's usually.\n\n45:38.080 --> 45:39.120\n But that's a cool start.\n\n45:39.120 --> 45:43.760\n It's after all the like smiling and playful and joyful and all that.\n\n45:43.760 --> 45:45.440\n And then it's like, okay, I kick butt.\n\n45:45.440 --> 45:46.560\n Let's get shit done.\n\n45:46.560 --> 45:46.960\n Right.\n\n45:46.960 --> 45:48.640\n Let's get shit done affirmation.\n\n45:48.640 --> 45:49.280\n Okay, cool.\n\n45:49.280 --> 45:51.040\n So like what else is on there?\n\n45:52.640 --> 45:53.600\n What else is on there?\n\n45:54.320 --> 46:00.000\n Well, I, I have, I'm also, I'm, I'm a magnet for all sorts of things.\n\n46:00.000 --> 46:02.160\n So I'm an amazing people magnet.\n\n46:02.160 --> 46:04.400\n I attract like awesome people into my universe.\n\n46:05.520 --> 46:06.960\n That's an actual affirmation.\n\n46:06.960 --> 46:07.200\n Yes.\n\n46:07.840 --> 46:08.880\n That's great.\n\n46:08.880 --> 46:09.280\n Yeah.\n\n46:09.280 --> 46:10.640\n So that, that's, and that, yeah.\n\n46:10.640 --> 46:13.920\n And that somehow manifests itself into like in working.\n\n46:13.920 --> 46:14.720\n I think so.\n\n46:15.440 --> 46:15.680\n Yeah.\n\n46:15.680 --> 46:18.960\n Like, can you speak to like why it feels good to do the affirmations?\n\n46:19.760 --> 46:23.280\n I honestly think it just grounds the day.\n\n46:24.080 --> 46:30.000\n And then it allows me to, instead of just like being pulled back and forth, like throughout\n\n46:30.000 --> 46:31.680\n the day, it just like grounds me.\n\n46:31.680 --> 46:34.560\n I'm like, okay, like this thing happened.\n\n46:34.560 --> 46:37.360\n It's not exactly what I wanted it to be, but I'm patient.\n\n46:37.360 --> 46:42.960\n Or I'm, you know, I'm, I trust that the universe will do amazing things for me, which is one\n\n46:42.960 --> 46:45.440\n of my other consistent affirmations.\n\n46:45.440 --> 46:46.720\n Or I'm an amazing mom.\n\n46:46.720 --> 46:47.040\n Right.\n\n46:47.040 --> 46:51.040\n And so I can grapple with all the feelings of mom guilt that I have all the time.\n\n46:52.240 --> 46:53.760\n Or here's another one.\n\n46:53.760 --> 46:55.040\n I'm a love magnet.\n\n46:55.040 --> 46:59.040\n And I literally say, I will kind of picture the person that I'd love to end up with.\n\n46:59.040 --> 47:02.480\n And I write it all down and it hasn't happened yet, but it.\n\n47:02.480 --> 47:03.920\n What are you, what are you picturing?\n\n47:03.920 --> 47:04.720\n This is Brad Pitt.\n\n47:06.000 --> 47:07.040\n Because that's what I picture.\n\n47:07.040 --> 47:07.440\n Okay.\n\n47:07.440 --> 47:08.240\n That's what you picture?\n\n47:08.240 --> 47:08.480\n Yeah.\n\n47:08.480 --> 47:08.880\n Okay.\n\n47:08.880 --> 47:11.760\n On the, on the running, holding hands, running together.\n\n47:11.760 --> 47:12.080\n Okay.\n\n47:14.160 --> 47:18.800\n No, more like fight club that the fight club, Brad Pitt, where he's like standing.\n\n47:18.800 --> 47:19.120\n All right.\n\n47:19.120 --> 47:20.000\n People will know.\n\n47:20.000 --> 47:20.720\n Anyway, I'm sorry.\n\n47:20.720 --> 47:21.920\n I'll get off on that.\n\n47:21.920 --> 47:27.360\n Do you have a, like when you're thinking about the being a love magnet in that way, are you\n\n47:27.360 --> 47:35.120\n picturing specific people or is this almost like in the space of like energy?\n\n47:36.000 --> 47:36.320\n Right.\n\n47:36.320 --> 47:44.240\n It's somebody who is smart and well accomplished and successful in their life, but they're\n\n47:44.240 --> 47:48.000\n generous and they're well traveled and they want to travel the world.\n\n47:48.960 --> 47:49.760\n Things like that.\n\n47:49.760 --> 47:51.360\n Like their head over heels into me.\n\n47:51.360 --> 47:54.560\n It's like, I know it sounds super silly, but it's literally what I write.\n\n47:54.560 --> 47:54.800\n Yeah.\n\n47:54.800 --> 47:56.320\n And I believe it'll happen one day.\n\n47:56.320 --> 47:58.000\n Oh, you actually write, so you don't say it out loud?\n\n47:58.000 --> 47:58.240\n You write.\n\n47:58.240 --> 47:58.960\n No, I write it.\n\n47:58.960 --> 48:00.320\n I write all my affirmations.\n\n48:01.200 --> 48:01.920\n I do the opposite.\n\n48:01.920 --> 48:02.640\n I say it out loud.\n\n48:02.640 --> 48:03.440\n Oh, you say it out loud?\n\n48:03.440 --> 48:04.320\n Interesting.\n\n48:04.320 --> 48:06.320\n Yeah, if I'm alone, I'll say it out loud.\n\n48:06.320 --> 48:07.360\n Interesting.\n\n48:07.360 --> 48:08.240\n I should try that.\n\n48:10.000 --> 48:15.600\n I think it's what feels more powerful to you.\n\n48:15.600 --> 48:16.960\n To me, more powerful.\n\n48:18.240 --> 48:20.320\n Saying stuff feels more powerful.\n\n48:20.320 --> 48:20.820\n Yeah.\n\n48:21.520 --> 48:32.320\n Writing is, writing feels like I'm losing the words, like losing the power of the words\n\n48:32.320 --> 48:33.520\n maybe because I write slow.\n\n48:33.520 --> 48:34.320\n Do you handwrite?\n\n48:34.960 --> 48:36.320\n No, I type.\n\n48:36.320 --> 48:37.520\n It's on this app.\n\n48:37.520 --> 48:38.800\n It's day one, basically.\n\n48:38.800 --> 48:44.320\n And I just, I can look, the best thing about it is I can look back and see like a year ago,\n\n48:44.320 --> 48:45.760\n what was I affirming, right?\n\n48:46.560 --> 48:47.200\n So it's...\n\n48:47.200 --> 48:48.320\n Oh, so it changes over time.\n\n48:50.000 --> 48:54.640\n It hasn't like changed a lot, but the focus kind of changes over time.\n\n48:54.640 --> 48:55.440\n I got it.\n\n48:55.440 --> 48:57.840\n Yeah, I say the same exact thing over and over and over.\n\n48:57.840 --> 48:58.400\n Oh, you do?\n\n48:58.400 --> 48:58.560\n Okay.\n\n48:58.560 --> 49:00.880\n There's a comfort in the sameness of it.\n\n49:00.880 --> 49:05.440\n Well, actually, let me jump around because let me ask you about, because all this talk\n\n49:05.440 --> 49:10.800\n about Brad Pitt, or maybe it's just going on inside my head, let me ask you about dating\n\n49:10.800 --> 49:11.440\n in general.\n\n49:12.960 --> 49:16.800\n You tweeted, are you based in Boston and single?\n\n49:16.800 --> 49:22.880\n And then you pointed to a startup Singles Night sponsored by Smile Dating app.\n\n49:23.440 --> 49:27.280\n I mean, this is jumping around a little bit, but since you mentioned...\n\n49:27.280 --> 49:33.840\n Since you mentioned, can AI help solve this dating love problem?\n\n49:34.400 --> 49:34.960\n What do you think?\n\n49:34.960 --> 49:41.600\n This problem of connection that is part of the human condition, can AI help that you\n\n49:41.600 --> 49:43.520\n yourself are in the search affirming?\n\n49:44.960 --> 49:48.160\n Maybe that's what I should affirm, like build an AI.\n\n49:48.160 --> 49:49.520\n Build an AI that finds love?\n\n49:49.520 --> 50:00.400\n I think there must be a science behind that first moment you meet a person and you either\n\n50:00.400 --> 50:02.240\n have chemistry or you don't, right?\n\n50:02.800 --> 50:06.960\n I guess that was the question I was asking, would you put it brilliantly, is that a science\n\n50:06.960 --> 50:07.440\n or an art?\n\n50:09.680 --> 50:15.200\n I think there are like, there's actual chemicals that get exchanged when two people meet.\n\n50:15.200 --> 50:16.240\n I don't know about that.\n\n50:16.240 --> 50:22.880\n I like how you're changing, yeah, changing your mind as we're describing it, but it feels\n\n50:22.880 --> 50:23.360\n that way.\n\n50:23.920 --> 50:29.040\n But it's what science shows us is sometimes we can explain with the rigor, the things\n\n50:29.040 --> 50:30.320\n that feel like magic.\n\n50:31.760 --> 50:33.680\n So maybe we can remove all the magic.\n\n50:34.320 --> 50:39.760\n Maybe it's like, I honestly think, like I said, like Goodreads should be a dating app,\n\n50:39.760 --> 50:41.680\n which like books.\n\n50:41.680 --> 50:46.960\n I wonder if you look at just like books or content you've consumed.\n\n50:46.960 --> 50:50.640\n I mean, that's essentially what YouTube does when it does a recommendation.\n\n50:50.640 --> 50:56.400\n If you just look at your footprint of content consumed, if there's an overlap, but maybe\n\n50:56.400 --> 51:01.280\n interesting difference with an overlap that some, I'm sure this is a machine learning\n\n51:01.280 --> 51:02.160\n problem that's solvable.\n\n51:03.520 --> 51:10.560\n Like this person is very likely to be not only there to be chemistry in the short term,\n\n51:10.560 --> 51:13.920\n but a good lifelong partner to grow together.\n\n51:13.920 --> 51:15.600\n I bet you it's a good machine learning problem.\n\n51:15.600 --> 51:16.480\n You just need the data.\n\n51:16.480 --> 51:17.360\n Let's do it.\n\n51:17.360 --> 51:22.080\n Well, actually, I do think there's so much data about each of us that there ought to\n\n51:22.080 --> 51:26.320\n be a machine learning algorithm that can ingest all this data and basically say, I think the\n\n51:26.320 --> 51:30.720\n following 10 people would be interesting connections for you, right?\n\n51:32.080 --> 51:36.640\n And so Smile dating app kind of took one particular angle, which is humor.\n\n51:36.640 --> 51:41.920\n It matches people based on their humor styles, which is one of the main ingredients of a\n\n51:41.920 --> 51:43.120\n successful relationship.\n\n51:43.120 --> 51:46.640\n Like if you meet somebody and they can make you laugh, like that's a good thing.\n\n51:47.200 --> 51:53.040\n And if you develop like internal jokes, like inside jokes and you're bantering, like that's\n\n51:53.040 --> 51:53.280\n fun.\n\n51:54.320 --> 51:56.640\n So I think.\n\n51:56.640 --> 51:57.520\n Yeah, definitely.\n\n51:57.520 --> 51:58.320\n Definitely.\n\n51:58.320 --> 52:04.880\n But yeah, that's the number of and the rate of inside joke generation.\n\n52:04.880 --> 52:08.160\n You could probably measure that and then optimize it over the first few days.\n\n52:08.160 --> 52:11.360\n You could say, we're just turning this into a machine learning problem.\n\n52:11.360 --> 52:11.920\n I love it.\n\n52:13.360 --> 52:23.120\n But for somebody like you, who's exceptionally successful and busy, is there, is there signs\n\n52:23.120 --> 52:24.880\n to that aspect of dating?\n\n52:24.880 --> 52:25.600\n Is it tricky?\n\n52:26.320 --> 52:27.600\n Is there advice you can give?\n\n52:27.600 --> 52:29.440\n Oh, my God, I give the worst advice.\n\n52:29.440 --> 52:31.440\n Well, I can tell you like I have a spreadsheet.\n\n52:31.440 --> 52:34.640\n Is that a good or a bad thing?\n\n52:34.640 --> 52:36.160\n Do you regret the spreadsheet?\n\n52:37.040 --> 52:38.240\n Well, I don't know.\n\n52:38.240 --> 52:39.440\n What's the name of the spreadsheet?\n\n52:39.440 --> 52:40.000\n Is it love?\n\n52:40.800 --> 52:42.880\n It's the date track, dating tracker.\n\n52:42.880 --> 52:43.840\n Dating tracker.\n\n52:43.840 --> 52:44.560\n It's very like.\n\n52:44.560 --> 52:45.280\n Love tracker.\n\n52:45.280 --> 52:45.840\n Yeah.\n\n52:46.320 --> 52:47.760\n And there's a rating system, I'm sure.\n\n52:47.760 --> 52:48.080\n Yeah.\n\n52:48.080 --> 52:49.920\n There's like weights and stuff.\n\n52:49.920 --> 52:51.440\n It's too close to home.\n\n52:51.440 --> 52:52.000\n Oh, is it?\n\n52:52.000 --> 52:52.800\n Do you also have.\n\n52:52.800 --> 52:56.640\n Well, I don't have a spreadsheet, but I would, now that you say it, it seems like a good\n\n52:56.640 --> 52:57.200\n idea.\n\n52:57.200 --> 52:58.160\n Oh, no.\n\n52:58.160 --> 52:58.720\n Okay.\n\n52:58.720 --> 53:03.600\n Turning it into data.\n\n53:05.760 --> 53:09.280\n I do wish that somebody else had a spreadsheet about me.\n\n53:11.200 --> 53:17.120\n You know, if it was like, like I said, like you said, convert, collect a lot of data about\n\n53:17.120 --> 53:21.840\n us in a way that's privacy preserving, that I own the data, I can control it and then\n\n53:21.840 --> 53:28.400\n use that data to find, I mean, not just romantic love, but collaborators, friends, all that\n\n53:28.400 --> 53:28.960\n kind of stuff.\n\n53:28.960 --> 53:30.240\n It seems like the data is there.\n\n53:30.240 --> 53:30.740\n Right.\n\n53:32.080 --> 53:35.280\n That's the problem social networks are trying to solve, but I think they're doing a really\n\n53:35.280 --> 53:36.240\n poor job.\n\n53:36.240 --> 53:39.600\n Even Facebook tried to get into a dating app business.\n\n53:39.600 --> 53:44.400\n And I think there's so many components to running a successful company that connects\n\n53:44.400 --> 53:45.360\n human beings.\n\n53:45.360 --> 53:53.920\n And part of that is, you know, having engineers that care about the human side, right, as\n\n53:53.920 --> 53:57.760\n you know, extremely well, it's not, it's not easy to find those.\n\n53:57.760 --> 54:00.640\n But you also don't want just people that care about the human.\n\n54:00.640 --> 54:02.240\n They also have to be good engineers.\n\n54:02.240 --> 54:05.760\n So it's like, you have to find this beautiful mix.\n\n54:05.760 --> 54:12.560\n And for some reason, just empirically speaking, people have not done a good job of that, of\n\n54:12.560 --> 54:13.680\n building companies like that.\n\n54:13.680 --> 54:16.480\n And it must mean that it's a difficult problem to solve.\n\n54:17.040 --> 54:19.920\n Dating apps, it seems difficult.\n\n54:19.920 --> 54:22.080\n Okay, Cupid, Tinder, all those kinds of stuff.\n\n54:22.080 --> 54:32.080\n They seem to find, of course they work, but they seem to not work as well as I would imagine\n\n54:32.080 --> 54:32.880\n is possible.\n\n54:32.880 --> 54:36.960\n Like, with data, wouldn't you be able to find better human connection?\n\n54:36.960 --> 54:39.520\n It's like arranged marriages on steroids, essentially.\n\n54:39.520 --> 54:40.480\n Right, right.\n\n54:40.480 --> 54:42.560\n Arranged by machine learning algorithm.\n\n54:42.560 --> 54:45.600\n Arranged by machine learning algorithm, but not a superficial one.\n\n54:45.600 --> 54:48.640\n I think a lot of the dating apps out there are just so superficial.\n\n54:48.640 --> 54:55.680\n They're just matching on like high level criteria that aren't ingredients for successful partnership.\n\n54:55.680 --> 54:58.480\n But you know what's missing, though, too?\n\n54:58.480 --> 55:01.440\n I don't know how to fix that, the serendipity piece of it.\n\n55:01.440 --> 55:03.760\n Like, how do you engineer serendipity?\n\n55:03.760 --> 55:07.680\n Like this random, like, chance encounter, and then you fall in love with the person.\n\n55:07.680 --> 55:10.080\n Like, I don't know how a dating app can do that.\n\n55:10.080 --> 55:12.080\n So there has to be a little bit of randomness.\n\n55:12.080 --> 55:21.680\n Maybe every 10th match is just a, you know, yeah, somebody that the algorithm wouldn't\n\n55:21.680 --> 55:25.840\n have necessarily recommended, but it allows for a little bit of...\n\n55:25.840 --> 55:33.440\n Well, it can also, you know, it can also trick you into thinking of serendipity by like somehow\n\n55:33.440 --> 55:39.200\n showing you a tweet of a person that he thinks you'll match well with, but do it accidentally\n\n55:39.200 --> 55:40.560\n as part of another search.\n\n55:40.560 --> 55:41.040\n Right.\n\n55:41.040 --> 55:46.080\n And like you just notice it, like, and then you get, you go down a rabbit hole and you\n\n55:46.080 --> 55:51.360\n connect them outside the app to like, you connect with this person outside the app somehow.\n\n55:51.360 --> 55:53.520\n So it's just, it creates that moment of meeting.\n\n55:54.240 --> 55:57.600\n Of course, you have to think of, from an app perspective, how you can turn that into a\n\n55:57.600 --> 55:58.240\n business.\n\n55:58.240 --> 56:03.600\n But I think ultimately a business that helps people find love in any way.\n\n56:04.400 --> 56:07.440\n Like that's what Apple was about, create products that people love.\n\n56:07.440 --> 56:08.240\n That's beautiful.\n\n56:08.240 --> 56:11.520\n I mean, you got to make money somehow.\n\n56:11.520 --> 56:18.160\n If you help people fall in love personally with the product, find self love or love another\n\n56:18.160 --> 56:19.840\n human being, you're going to make money.\n\n56:19.840 --> 56:21.440\n You're going to figure out a way to make money.\n\n56:22.960 --> 56:27.840\n I just feel like the dating apps often will optimize for something else than love.\n\n56:28.560 --> 56:30.000\n It's the same with social networks.\n\n56:30.000 --> 56:35.280\n They optimize for engagement as opposed to like a deep, meaningful connection that's\n\n56:35.280 --> 56:39.760\n ultimately grounded in like personal growth, you as a human being growing and all that\n\n56:39.760 --> 56:40.240\n kind of stuff.\n\n56:41.520 --> 56:46.800\n Let me do like a pivot to a dark topic, which you opened the book with.\n\n56:48.560 --> 56:56.080\n A story, because I'd like to talk to you about just emotion and artificial intelligence.\n\n56:56.080 --> 56:59.680\n I think this is a good story to start to think about emotional intelligence.\n\n56:59.680 --> 57:05.120\n You opened the book with a story of a central Florida man, Jamel Dunn, who was drowning\n\n57:05.120 --> 57:10.000\n and drowned while five teenagers watched and laughed, saying things like, you're going\n\n57:10.000 --> 57:10.800\n to die.\n\n57:10.800 --> 57:15.840\n And when Jamel disappeared below the surface of the water, one of them said he just died\n\n57:15.840 --> 57:16.960\n and the others laughed.\n\n57:17.440 --> 57:23.360\n What does this incident teach you about human nature and the response to it perhaps?\n\n57:23.360 --> 57:23.840\n Yeah.\n\n57:24.320 --> 57:28.480\n I mean, I think this is a really, really, really sad story.\n\n57:28.480 --> 57:34.480\n And it and it and it highlights what I believe is a it's a real problem in our world today.\n\n57:34.480 --> 57:36.000\n It's it's an empathy crisis.\n\n57:36.720 --> 57:39.840\n Yeah, we're living through an empathy crisis and crisis.\n\n57:39.840 --> 57:40.400\n Yeah.\n\n57:40.400 --> 57:40.900\n Yeah.\n\n57:42.240 --> 57:45.360\n And I mean, we've we've talked about this throughout our conversation.\n\n57:45.360 --> 57:47.040\n We dehumanize each other.\n\n57:47.040 --> 57:51.920\n And unfortunately, yes, technology is bringing us together.\n\n57:51.920 --> 57:53.840\n But in a way, it's just dehumanized.\n\n57:53.840 --> 57:58.640\n It's creating this like, yeah, dehumanizing of the other.\n\n57:58.640 --> 58:00.960\n And I think that's a huge problem.\n\n58:01.840 --> 58:05.840\n The good news is I think solution, the solution could be technology based.\n\n58:05.840 --> 58:11.520\n Like, I think if we rethink the way we design and deploy our technologies, we can solve\n\n58:11.520 --> 58:12.560\n parts of this problem.\n\n58:12.560 --> 58:13.200\n But I worry about it.\n\n58:13.200 --> 58:19.200\n I mean, even with my son, a lot of his interactions are computer mediated.\n\n58:19.200 --> 58:25.280\n And I just question what that's doing to his empathy skills and, you know, his ability\n\n58:25.280 --> 58:26.560\n to really connect with people.\n\n58:26.560 --> 58:34.560\n So that you think you think it's not possible to form empathy through the digital medium.\n\n58:36.320 --> 58:37.360\n I think it is.\n\n58:38.560 --> 58:44.000\n But we have to be thoughtful about because the way the way we engage face to face, which\n\n58:44.000 --> 58:45.840\n is what we're doing right now, right?\n\n58:45.840 --> 58:49.360\n There's the nonverbal signals, which are a majority of how we communicate.\n\n58:49.360 --> 58:52.960\n It's like 90% of how we communicate is your facial expressions.\n\n58:54.000 --> 58:57.680\n You know, I'm saying something and you're nodding your head now, and that creates a\n\n58:57.680 --> 58:58.480\n feedback loop.\n\n58:58.480 --> 59:02.160\n And and if you break that, and now I have anxiety about it.\n\n59:04.160 --> 59:04.880\n Poor Lex.\n\n59:06.000 --> 59:06.560\n Oh, boy.\n\n59:06.560 --> 59:09.680\n I am not scrutinizing your facial expressions during this interview.\n\n59:09.680 --> 59:10.160\n I am.\n\n59:12.160 --> 59:12.800\n Look normal.\n\n59:12.800 --> 59:13.360\n Look human.\n\n59:13.360 --> 59:13.860\n Yeah.\n\n59:13.860 --> 59:17.720\n Look normal, look human.\n\n59:17.720 --> 59:18.680\n Nod head.\n\n59:18.680 --> 59:19.400\n Yeah, nod head.\n\n59:20.920 --> 59:21.560\n In agreement.\n\n59:21.560 --> 59:24.600\n If Rana says yes, then nod head else.\n\n59:25.720 --> 59:29.640\n Don't do it too much because it might be at the wrong time and then it will send the\n\n59:29.640 --> 59:30.760\n wrong signal.\n\n59:30.760 --> 59:31.400\n Oh, God.\n\n59:31.400 --> 59:34.760\n And make eye contact sometimes because humans appreciate that.\n\n59:35.320 --> 59:35.640\n All right.\n\n59:35.640 --> 59:36.440\n Anyway, okay.\n\n59:38.520 --> 59:42.920\n Yeah, but something about the especially when you say mean things in person, you get to\n\n59:42.920 --> 59:44.280\n see the pain of the other person.\n\n59:44.280 --> 59:44.600\n Exactly.\n\n59:44.600 --> 59:48.120\n But if you're tweeting it at a person and you have no idea how it's going to land, you're\n\n59:48.120 --> 59:52.040\n more likely to do that on social media than you are in face to face conversations.\n\n59:52.040 --> 59:52.540\n So.\n\n59:54.520 --> 59:55.880\n What do you think is more important?\n\n59:59.000 --> 1:00:00.360\n EQ or IQ?\n\n1:00:00.360 --> 1:00:02.200\n EQ being emotional intelligence.\n\n1:00:03.880 --> 1:00:06.360\n In terms of in what makes us human.\n\n1:00:08.120 --> 1:00:11.240\n I think emotional intelligence is what makes us human.\n\n1:00:11.240 --> 1:00:14.760\n It's how we connect with one another.\n\n1:00:14.760 --> 1:00:16.440\n It's how we build trust.\n\n1:00:16.440 --> 1:00:19.560\n It's how we make decisions, right?\n\n1:00:19.560 --> 1:00:25.720\n Like your emotions drive kind of what you had for breakfast, but also where you decide\n\n1:00:25.720 --> 1:00:28.600\n to live and what you want to do for the rest of your life.\n\n1:00:28.600 --> 1:00:31.320\n So I think emotions are underrated.\n\n1:00:33.160 --> 1:00:39.080\n So emotional intelligence isn't just about the effective expression of your own emotions.\n\n1:00:39.080 --> 1:00:44.440\n It's about a sensitivity and empathy to other people's emotions and that sort of being\n\n1:00:44.440 --> 1:00:48.280\n able to effectively engage in the dance of emotions with other people.\n\n1:00:48.840 --> 1:00:51.240\n Yeah, I like that explanation.\n\n1:00:51.240 --> 1:00:52.360\n I like that kind of.\n\n1:00:53.720 --> 1:00:56.680\n Yeah, thinking about it as a dance because it is really about that.\n\n1:00:56.680 --> 1:01:01.800\n It's about sensing what state the other person's in and using that information to decide on\n\n1:01:01.800 --> 1:01:02.920\n how you're going to react.\n\n1:01:05.400 --> 1:01:06.760\n And I think it can be very powerful.\n\n1:01:06.760 --> 1:01:15.160\n Like people who are the best, most persuasive leaders in the world tap into, you know, they\n\n1:01:15.160 --> 1:01:20.360\n have, if you have higher EQ, you're more likely to be able to motivate people to change\n\n1:01:20.360 --> 1:01:21.160\n their behaviors.\n\n1:01:21.880 --> 1:01:24.360\n So it can be very powerful.\n\n1:01:24.920 --> 1:01:31.800\n On a more kind of technical, maybe philosophical level, you've written that emotion is universal.\n\n1:01:31.800 --> 1:01:36.920\n It seems that, sort of like Chomsky says, language is universal.\n\n1:01:36.920 --> 1:01:39.960\n There's a bunch of other stuff like cognition, consciousness.\n\n1:01:39.960 --> 1:01:43.560\n It seems a lot of us have these aspects.\n\n1:01:43.560 --> 1:01:46.520\n So the human mind generates all this.\n\n1:01:46.520 --> 1:01:52.040\n And so what do you think is the, they all seem to be like echoes of the same thing.\n\n1:01:52.840 --> 1:01:56.280\n What do you think emotion is exactly?\n\n1:01:56.280 --> 1:01:57.720\n Like how deep does it run?\n\n1:01:57.720 --> 1:02:01.800\n Is it a surface level thing that we display to each other?\n\n1:02:01.800 --> 1:02:04.760\n Is it just another form of language or something deep within?\n\n1:02:05.640 --> 1:02:07.480\n I think it's really deep.\n\n1:02:07.480 --> 1:02:09.320\n It's how, you know, we started with memory.\n\n1:02:09.880 --> 1:02:12.600\n I think emotions play a really important role.\n\n1:02:14.040 --> 1:02:18.040\n Yeah, emotions play a very important role in how we encode memories, right?\n\n1:02:18.040 --> 1:02:21.640\n Our memories are often encoded, almost indexed by emotions.\n\n1:02:21.640 --> 1:02:22.120\n Yeah.\n\n1:02:22.120 --> 1:02:28.520\n Yeah, it's at the core of how, you know, our decision making engine is also heavily\n\n1:02:28.520 --> 1:02:30.040\n influenced by our emotions.\n\n1:02:30.040 --> 1:02:31.960\n So emotions is part of cognition.\n\n1:02:31.960 --> 1:02:32.680\n Totally.\n\n1:02:32.680 --> 1:02:34.680\n It's intermixed into the whole thing.\n\n1:02:34.680 --> 1:02:35.960\n Yes, absolutely.\n\n1:02:35.960 --> 1:02:39.960\n And in fact, when you take it away, people are unable to make decisions.\n\n1:02:39.960 --> 1:02:41.000\n They're really paralyzed.\n\n1:02:41.000 --> 1:02:45.240\n Like they can't go about their daily or their, you know, personal or professional lives.\n\n1:02:45.240 --> 1:02:45.740\n So.\n\n1:02:45.740 --> 1:02:53.740\n It does seem like there's probably some interesting interweaving of emotion and consciousness.\n\n1:02:53.740 --> 1:02:58.940\n I wonder if it's possible to have, like if they're next door neighbors somehow, or if\n\n1:02:58.940 --> 1:03:01.740\n they're actually flat mates.\n\n1:03:01.740 --> 1:03:08.940\n I don't, it feels like the hard problem of consciousness where it's some, it feels like\n\n1:03:08.940 --> 1:03:10.780\n something to experience the thing.\n\n1:03:10.780 --> 1:03:16.780\n Like red feels like red, and it's, you know, when you eat a mango, it's sweet.\n\n1:03:16.780 --> 1:03:24.620\n The taste, the sweetness, that it feels like something to experience that sweetness, that\n\n1:03:24.620 --> 1:03:26.380\n whatever generates emotions.\n\n1:03:28.060 --> 1:03:31.740\n But then like, see, I feel like emotion is part of communication.\n\n1:03:31.740 --> 1:03:34.300\n It's very much about communication.\n\n1:03:34.300 --> 1:03:39.420\n And then, you know, it's like, you know, it's like, you know, it's like, you know, it's\n\n1:03:39.420 --> 1:03:44.540\n and then that means it's also deeply connected to language.\n\n1:03:45.980 --> 1:03:52.300\n But then probably human intelligence is deeply connected to the collective intelligence between\n\n1:03:52.300 --> 1:03:52.700\n humans.\n\n1:03:52.700 --> 1:03:54.540\n It's not just the standalone thing.\n\n1:03:54.540 --> 1:03:56.380\n So the whole thing is really connected.\n\n1:03:56.380 --> 1:04:02.140\n So emotion is connected to language, language is connected to intelligence, and then intelligence\n\n1:04:02.140 --> 1:04:05.740\n is connected to consciousness, and consciousness is connected to emotion.\n\n1:04:05.740 --> 1:04:09.180\n The whole thing is that it's a beautiful mess.\n\n1:04:09.180 --> 1:04:15.660\n So can I comment on the emotions being a communication mechanism?\n\n1:04:15.660 --> 1:04:21.260\n Because I think there are two facets of our emotional experiences.\n\n1:04:23.020 --> 1:04:24.380\n One is communication, right?\n\n1:04:24.380 --> 1:04:29.820\n Like we use emotions, for example, facial expressions or other nonverbal cues to connect\n\n1:04:29.820 --> 1:04:34.700\n with other human beings and with other beings in the world, right?\n\n1:04:34.700 --> 1:04:40.460\n But even if it's not a communication context, we still experience emotions and we still\n\n1:04:40.460 --> 1:04:46.860\n process emotions and we still leverage emotions to make decisions and to learn and, you know,\n\n1:04:46.860 --> 1:04:47.740\n to experience life.\n\n1:04:47.740 --> 1:04:51.180\n So it isn't always just about communication.\n\n1:04:51.180 --> 1:04:55.500\n And we learned that very early on in our and kind of our work at Affectiva.\n\n1:04:56.460 --> 1:05:00.700\n One of the very first applications we brought to market was understanding how people respond\n\n1:05:00.700 --> 1:05:01.660\n to content, right?\n\n1:05:01.660 --> 1:05:04.860\n So if they're watching this video of ours, like, are they interested?\n\n1:05:04.860 --> 1:05:05.900\n Are they inspired?\n\n1:05:05.900 --> 1:05:07.180\n Are they bored to death?\n\n1:05:07.180 --> 1:05:12.060\n And so we watched their facial expressions and we had, we weren't sure if people would\n\n1:05:12.060 --> 1:05:15.580\n express any emotions if they were sitting alone.\n\n1:05:15.580 --> 1:05:20.220\n Like if you're in your bed at night, watching a Netflix TV series, would we still see any\n\n1:05:20.220 --> 1:05:21.420\n emotions on your face?\n\n1:05:21.420 --> 1:05:25.660\n And we were surprised that, yes, people still emote, even if they're alone, even if you're\n\n1:05:25.660 --> 1:05:30.300\n in your car driving around, you're singing along the song and you're joyful, you're\n\n1:05:30.300 --> 1:05:33.980\n smiling, you're joyful, we'll see these expressions.\n\n1:05:33.980 --> 1:05:37.100\n So it's not just about communicating with another person.\n\n1:05:37.820 --> 1:05:40.860\n It sometimes really isn't just about experiencing the world.\n\n1:05:41.260 --> 1:05:47.900\n And first of all, I wonder if some of that is because we develop our intelligence and\n\n1:05:47.900 --> 1:05:52.140\n our emotional intelligence by communicating with other humans.\n\n1:05:52.140 --> 1:05:56.620\n And so when other humans disappear from the picture, we're still kind of a virtual human.\n\n1:05:56.940 --> 1:05:57.900\n The code still runs.\n\n1:05:57.900 --> 1:06:02.860\n Yeah, the code still runs, but you also kind of, you're still, there's like virtual humans.\n\n1:06:02.860 --> 1:06:07.260\n You don't have to think of it that way, but there's a kind of, when you like chuckle,\n\n1:06:07.260 --> 1:06:13.100\n like, yeah, like you're kind of chuckling to a virtual human.\n\n1:06:13.100 --> 1:06:23.340\n I mean, it's possible that the code has to have another human there because if you just\n\n1:06:23.340 --> 1:06:28.540\n grew up alone, I wonder if emotion will still be there in this visual form.\n\n1:06:28.540 --> 1:06:37.100\n So yeah, I wonder, but anyway, what can you tell from the human face about what's going\n\n1:06:37.100 --> 1:06:38.300\n on inside?\n\n1:06:38.300 --> 1:06:45.100\n So that's the problem that Effectiva first tackled, which is using computer vision, using\n\n1:06:45.100 --> 1:06:50.220\n machine learning to try to detect stuff about the human face, as many things as possible\n\n1:06:50.220 --> 1:06:57.900\n and convert them into a prediction of categories of emotion, anger, happiness, all that kind\n\n1:06:57.900 --> 1:06:58.700\n of stuff.\n\n1:06:58.700 --> 1:07:00.300\n How hard is that problem?\n\n1:07:00.300 --> 1:07:01.340\n It's extremely hard.\n\n1:07:01.340 --> 1:07:07.180\n It's very, very hard because there is no one to one mapping between a facial expression\n\n1:07:07.180 --> 1:07:08.380\n and your internal state.\n\n1:07:08.380 --> 1:07:09.340\n There just isn't.\n\n1:07:09.340 --> 1:07:14.220\n There's this oversimplification of the problem where it's something like, if you are smiling,\n\n1:07:14.220 --> 1:07:15.180\n then you're happy.\n\n1:07:15.180 --> 1:07:17.340\n If you do a brow furrow, then you're angry.\n\n1:07:17.340 --> 1:07:19.500\n If you do an eyebrow raise, then you're surprised.\n\n1:07:19.500 --> 1:07:22.140\n And just think about it for a moment.\n\n1:07:22.140 --> 1:07:24.380\n You could be smiling for a whole host of reasons.\n\n1:07:24.940 --> 1:07:27.500\n You could also be happy and not be smiling, right?\n\n1:07:28.700 --> 1:07:34.220\n You could furrow your eyebrows because you're angry or you're confused about something or\n\n1:07:34.220 --> 1:07:35.340\n you're constipated.\n\n1:07:37.100 --> 1:07:41.820\n So I think this oversimplistic approach to inferring emotion from a facial expression\n\n1:07:41.820 --> 1:07:42.780\n is really dangerous.\n\n1:07:42.780 --> 1:07:48.700\n The solution is to incorporate as many contextual signals as you can, right?\n\n1:07:48.700 --> 1:07:55.100\n So if, for example, I'm driving a car and you can see me like nodding my head and my\n\n1:07:55.100 --> 1:08:00.460\n eyes are closed and the blinking rate is changing, I'm probably falling asleep at the wheel,\n\n1:08:00.460 --> 1:08:00.940\n right?\n\n1:08:00.940 --> 1:08:03.180\n Because you know the context.\n\n1:08:03.180 --> 1:08:10.300\n You understand what the person's doing or add additional channels like voice or gestures\n\n1:08:10.300 --> 1:08:17.020\n or even physiological sensors, but I think it's very dangerous to just take this oversimplistic\n\n1:08:17.020 --> 1:08:20.060\n approach of, yeah, smile equals happy and...\n\n1:08:20.060 --> 1:08:25.020\n If you're able to, in a high resolution way, specify the context, there's certain things\n\n1:08:25.020 --> 1:08:31.500\n that are going to be somewhat reliable signals of something like drowsiness or happiness\n\n1:08:31.500 --> 1:08:32.620\n or stuff like that.\n\n1:08:32.620 --> 1:08:40.300\n I mean, when people are watching Netflix content, that problem, that's a really compelling idea\n\n1:08:40.300 --> 1:08:46.380\n that you can kind of, at least in aggregate, highlight like which part was boring, which\n\n1:08:46.380 --> 1:08:47.660\n part was exciting.\n\n1:08:47.660 --> 1:08:49.100\n How hard was that problem?\n\n1:08:50.300 --> 1:08:53.740\n That was on the scale of difficulty.\n\n1:08:53.740 --> 1:09:00.140\n I think that's one of the easier problems to solve because it's a relatively constrained\n\n1:09:00.140 --> 1:09:00.620\n environment.\n\n1:09:00.620 --> 1:09:02.780\n You have somebody sitting in front of...\n\n1:09:02.780 --> 1:09:07.820\n Initially, we started with like a device in front of you, like a laptop, and then we graduated\n\n1:09:07.820 --> 1:09:12.140\n to doing this on a mobile phone, which is a lot harder just because of, you know, from\n\n1:09:12.140 --> 1:09:17.100\n a computer vision perspective, the profile view of the face can be a lot more challenging.\n\n1:09:17.900 --> 1:09:23.180\n We had to figure out lighting conditions because usually people are watching content literally\n\n1:09:23.180 --> 1:09:24.620\n in their bedrooms at night.\n\n1:09:24.620 --> 1:09:25.420\n Lights are dimmed.\n\n1:09:25.420 --> 1:09:30.220\n Yeah, I mean, if you're standing, it's probably going to be the looking up.\n\n1:09:30.220 --> 1:09:31.500\n The nostril view.\n\n1:09:31.500 --> 1:09:33.420\n Yeah, and nobody looks good at it.\n\n1:09:34.060 --> 1:09:36.140\n I've seen data sets from that perspective.\n\n1:09:36.140 --> 1:09:39.500\n It's like, this is not a good look for anyone.\n\n1:09:40.620 --> 1:09:44.460\n Or if you're laying in bed at night, what is it, side view or something?\n\n1:09:44.460 --> 1:09:44.940\n Right.\n\n1:09:44.940 --> 1:09:47.580\n And half your face is like on a pillow.\n\n1:09:47.580 --> 1:09:56.620\n Actually, I would love to know, have data about like how people watch stuff in bed at\n\n1:09:56.620 --> 1:10:03.340\n night, like, do they prop there, is it a pillow, the, like, I'm sure there's a lot of interesting\n\n1:10:03.340 --> 1:10:04.060\n dynamics there.\n\n1:10:04.060 --> 1:10:04.560\n Right.\n\n1:10:05.260 --> 1:10:07.100\n From a health and well being perspective, right?\n\n1:10:07.100 --> 1:10:07.580\n Sure.\n\n1:10:07.580 --> 1:10:08.540\n Like, oh, you're hurting your neck.\n\n1:10:08.540 --> 1:10:13.740\n I was thinking machine learning perspective, but yes, but also, yeah, yeah, once you have\n\n1:10:13.740 --> 1:10:18.060\n that data, you can start making all kinds of inference about health and stuff like that.\n\n1:10:18.060 --> 1:10:18.620\n Interesting.\n\n1:10:19.260 --> 1:10:26.700\n Yeah, there's an interesting thing when I was at Google that we were, it's called active\n\n1:10:26.700 --> 1:10:32.620\n authentication, where you want to be able to unlock your phone without using a password.\n\n1:10:32.620 --> 1:10:38.940\n So it would face, but also other stuff, like the way you take a phone out of the pocket.\n\n1:10:38.940 --> 1:10:39.500\n Amazing.\n\n1:10:39.500 --> 1:10:45.260\n So that kind of data to use the multimodal with machine learning to be able to identify\n\n1:10:45.260 --> 1:10:50.220\n that it's you or likely to be you, likely not to be you, that allows you to not always\n\n1:10:50.220 --> 1:10:51.260\n have to enter the password.\n\n1:10:51.260 --> 1:10:52.700\n That was the idea.\n\n1:10:52.700 --> 1:10:58.540\n But the funny thing about that is, I just want to tell a small anecdote is because it\n\n1:10:58.540 --> 1:11:09.660\n was all male engineers, except so my boss is, our boss was still one of my favorite humans,\n\n1:11:09.660 --> 1:11:12.300\n was a woman, Regina Dugan.\n\n1:11:12.300 --> 1:11:14.140\n Oh, my God, I love her.\n\n1:11:14.140 --> 1:11:14.940\n She's awesome.\n\n1:11:14.940 --> 1:11:15.500\n She's the best.\n\n1:11:15.500 --> 1:11:16.300\n She's the best.\n\n1:11:16.300 --> 1:11:25.900\n So, but anyway, and there's one female brilliant female engineer on the team, and she was the\n\n1:11:25.900 --> 1:11:30.380\n one that actually highlighted the fact that women often don't have pockets.\n\n1:11:30.380 --> 1:11:37.340\n It was like, whoa, that was not even a category in the code of like, wait a minute, you can\n\n1:11:37.340 --> 1:11:41.260\n take the phone out of some other place than your pocket.\n\n1:11:41.260 --> 1:11:45.580\n So anyway, that's a funny thing when you're considering people laying in bed, watching\n\n1:11:45.580 --> 1:11:51.820\n a phone, you have to consider if you have to, you know, diversity in all its forms,\n\n1:11:51.820 --> 1:11:53.900\n depending on the problem, depending on the context.\n\n1:11:53.900 --> 1:11:58.140\n Actually, this is like a very important, I think this is, you know, you probably get\n\n1:11:58.140 --> 1:11:58.940\n this all the time.\n\n1:11:58.940 --> 1:12:03.100\n Like people are worried that AI is going to take over humanity and like, get rid of all\n\n1:12:03.100 --> 1:12:04.300\n the humans in the world.\n\n1:12:04.300 --> 1:12:06.540\n I'm like, actually, that's not my biggest concern.\n\n1:12:06.540 --> 1:12:10.380\n My biggest concern is that we are building bias into these systems.\n\n1:12:10.380 --> 1:12:14.380\n And then they're like deployed at large and at scale.\n\n1:12:14.380 --> 1:12:19.660\n And before you know it, you're kind of accentuating the bias that exists in society.\n\n1:12:19.660 --> 1:12:26.940\n Yeah, I'm not, you know, I know people, it's very important to worry about that, but the\n\n1:12:26.940 --> 1:12:32.620\n worry is an emergent phenomena to me, which is a very good one, because I think these\n\n1:12:32.620 --> 1:12:39.660\n systems are actually, by encoding the data that exists, they're revealing the bias in\n\n1:12:39.660 --> 1:12:40.380\n society.\n\n1:12:40.380 --> 1:12:43.340\n They're both for teaching us what the bias is.\n\n1:12:43.340 --> 1:12:46.380\n Therefore, we can now improve that bias within the system.\n\n1:12:46.380 --> 1:12:49.980\n So they're almost like putting a mirror to ourselves.\n\n1:12:49.980 --> 1:12:50.780\n Totally.\n\n1:12:50.780 --> 1:12:51.500\n So I'm not.\n\n1:12:51.500 --> 1:12:53.500\n You have to be open to looking at the mirror, though.\n\n1:12:53.500 --> 1:12:56.540\n You have to be open to scrutinizing the data.\n\n1:12:56.540 --> 1:12:59.500\n And if you just take it as ground.\n\n1:12:59.500 --> 1:13:02.860\n Or you don't even have to look at the, I mean, yes, the data is how you fix it.\n\n1:13:02.860 --> 1:13:05.100\n But then you just look at the behavior of the system.\n\n1:13:05.100 --> 1:13:08.620\n And you realize, holy crap, this thing is kind of racist.\n\n1:13:08.620 --> 1:13:09.820\n Like, why is that?\n\n1:13:09.820 --> 1:13:11.740\n And then you look at the data, it's like, oh, okay.\n\n1:13:11.740 --> 1:13:15.820\n And then you start to realize that I think that some much more effective ways to do that\n\n1:13:15.820 --> 1:13:23.020\n are effective way to be introspective as a society than through sort of political discourse.\n\n1:13:23.020 --> 1:13:34.060\n Like AI kind of, because people are for some reason more productive and rigorous in criticizing\n\n1:13:34.060 --> 1:13:35.740\n AI than they're criticizing each other.\n\n1:13:35.740 --> 1:13:41.340\n So I think this is just a nice method for studying society and see which way progress\n\n1:13:41.340 --> 1:13:42.380\n lies.\n\n1:13:42.380 --> 1:13:44.380\n Anyway, what we're talking about.\n\n1:13:44.380 --> 1:13:50.220\n You're watching the problem of watching Netflix in bed or elsewhere and seeing which parts\n\n1:13:50.220 --> 1:13:51.660\n are exciting, which parts are boring.\n\n1:13:51.660 --> 1:13:56.620\n You're saying that's relatively constrained because you have a captive audience and you\n\n1:13:56.620 --> 1:13:57.740\n kind of know the context.\n\n1:13:57.740 --> 1:14:01.100\n And one thing you said that was really key is the aggregate.\n\n1:14:01.100 --> 1:14:02.380\n You're doing this in aggregate, right?\n\n1:14:02.380 --> 1:14:04.700\n Like we're looking at aggregated response of people.\n\n1:14:04.700 --> 1:14:11.100\n And so when you see a peak, say a smile peak, they're probably smiling or laughing at something\n\n1:14:11.100 --> 1:14:12.140\n that's in the content.\n\n1:14:12.140 --> 1:14:14.780\n So that was one of the first problems we were able to solve.\n\n1:14:15.740 --> 1:14:20.380\n And when we see the smile peak, it doesn't mean that these people are internally happy.\n\n1:14:20.380 --> 1:14:22.060\n They're just laughing at content.\n\n1:14:22.060 --> 1:14:25.420\n So it's important to call it for what it is.\n\n1:14:25.420 --> 1:14:28.140\n But it's still really, really useful data.\n\n1:14:28.140 --> 1:14:34.380\n I wonder how that compares to, so what like YouTube and other places will use is obviously\n\n1:14:34.380 --> 1:14:39.900\n they don't have, for the most case, they don't have that kind of data.\n\n1:14:39.900 --> 1:14:45.660\n They have the data of when people tune out, like switch to drop off.\n\n1:14:45.660 --> 1:14:50.300\n And I think that's an aggregate for YouTube, at least a pretty powerful signal.\n\n1:14:50.300 --> 1:14:59.580\n I worry about what that leads to because looking at like YouTubers that kind of really care\n\n1:14:59.580 --> 1:15:07.740\n about views and try to maximize the number of views, I think when they say that the video\n\n1:15:07.740 --> 1:15:15.100\n should be constantly interesting, which seems like a good goal, I feel like that leads to\n\n1:15:15.100 --> 1:15:18.300\n this manic pace of a video.\n\n1:15:19.020 --> 1:15:24.940\n Like the idea that I would speak at the current speed that I'm speaking, I don't know.\n\n1:15:25.820 --> 1:15:28.220\n And that every moment has to be engaging, right?\n\n1:15:28.220 --> 1:15:28.780\n Engaging.\n\n1:15:28.780 --> 1:15:29.260\n Yeah.\n\n1:15:29.260 --> 1:15:31.500\n I think there's value to silence.\n\n1:15:31.500 --> 1:15:33.660\n There's value to the boring bits.\n\n1:15:33.660 --> 1:15:37.500\n I mean, some of the greatest movies ever, some of the greatest movies ever.\n\n1:15:37.500 --> 1:15:42.540\n Some of the greatest stories ever told me they have that boring bits, seemingly boring bits.\n\n1:15:42.540 --> 1:15:43.500\n I don't know.\n\n1:15:43.500 --> 1:15:45.020\n I wonder about that.\n\n1:15:45.020 --> 1:15:49.180\n Of course, it's not that the human face can capture that either.\n\n1:15:49.180 --> 1:15:51.500\n It's just giving an extra signal.\n\n1:15:51.500 --> 1:16:01.180\n You have to really, I don't know, you have to really collect deeper long term data about\n\n1:16:01.180 --> 1:16:03.260\n what was meaningful to people.\n\n1:16:03.260 --> 1:16:08.940\n When they think 30 days from now, what they still remember, what moved them, what changed\n\n1:16:08.940 --> 1:16:11.660\n them, what helped them grow, that kind of stuff.\n\n1:16:11.660 --> 1:16:14.940\n You know, it would be a really interesting, I don't know if there are any researchers\n\n1:16:14.940 --> 1:16:17.340\n out there who are doing this type of work.\n\n1:16:17.340 --> 1:16:23.500\n Wouldn't it be so cool to tie your emotional expressions while you're, say, listening\n\n1:16:23.500 --> 1:16:30.620\n to a podcast interview and then 30 days later interview people and say, hey, what do you\n\n1:16:30.620 --> 1:16:31.340\n remember?\n\n1:16:31.340 --> 1:16:33.420\n You've watched this 30 days ago.\n\n1:16:33.420 --> 1:16:34.620\n Like, what stuck with you?\n\n1:16:34.620 --> 1:16:38.140\n And then see if there's any, there ought to be maybe, there ought to be some correlation\n\n1:16:38.140 --> 1:16:45.100\n between these emotional experiences and, yeah, what you, what stays with you.\n\n1:16:46.140 --> 1:16:51.660\n So the one guy listening now on the beach in Brazil, please record a video of yourself\n\n1:16:51.660 --> 1:16:55.900\n listening to this and send it to me and then I'll interview you 30 days from now.\n\n1:16:55.900 --> 1:16:56.860\n Yeah, that'd be great.\n\n1:16:58.700 --> 1:17:00.620\n It'll be statistically significant to you.\n\n1:17:00.620 --> 1:17:06.940\n Yeah, I know one, but, you know, yeah, yeah, I think that's really fascinating.\n\n1:17:06.940 --> 1:17:16.460\n I think that's, that kind of holds the key to a future where entertainment or content\n\n1:17:16.460 --> 1:17:25.180\n is both entertaining and, I don't know, makes you better, empowering in some way.\n\n1:17:25.180 --> 1:17:32.540\n So figuring out, like, showing people stuff that entertains them, but also they're happy\n\n1:17:32.540 --> 1:17:36.780\n they watched 30 days from now because they've become a better person because of it.\n\n1:17:37.420 --> 1:17:41.900\n Well, you know, okay, not to riff on this topic for too long, but I have two children,\n\n1:17:41.900 --> 1:17:42.140\n right?\n\n1:17:42.860 --> 1:17:46.860\n And I see my role as a parent as like a chief opportunity officer.\n\n1:17:46.860 --> 1:17:50.780\n Like I am responsible for exposing them to all sorts of things in the world.\n\n1:17:50.780 --> 1:17:56.300\n And, but often I have no idea of knowing, like, what stuck, like, what was, you know,\n\n1:17:56.300 --> 1:18:00.220\n is this actually going to be transformative, you know, for them 10 years down the line?\n\n1:18:00.220 --> 1:18:03.660\n And I wish there was a way to quantify these experiences.\n\n1:18:03.660 --> 1:18:08.060\n Like, are they, I can tell in the moment if they're engaging, right?\n\n1:18:08.060 --> 1:18:12.540\n I can tell, but it's really hard to know if they're going to remember them 10 years\n\n1:18:12.540 --> 1:18:13.980\n from now or if it's going to.\n\n1:18:15.100 --> 1:18:19.500\n Yeah, that one is weird because it seems like kids remember the weirdest things.\n\n1:18:19.500 --> 1:18:23.580\n I've seen parents do incredible stuff for their kids and they don't remember any of\n\n1:18:23.580 --> 1:18:23.820\n that.\n\n1:18:23.820 --> 1:18:27.260\n They remember some tiny, small, sweet thing a parent did.\n\n1:18:27.260 --> 1:18:27.740\n Right.\n\n1:18:27.740 --> 1:18:28.380\n Like some...\n\n1:18:28.380 --> 1:18:32.540\n Like they took you to, like, this amazing country vacation, blah, blah, blah, blah.\n\n1:18:32.540 --> 1:18:33.180\n No, whatever.\n\n1:18:33.180 --> 1:18:38.060\n And then there'll be, like, some, like, stuffed toy you got or some, or the new PlayStation\n\n1:18:38.060 --> 1:18:40.540\n or something or some silly little thing.\n\n1:18:41.100 --> 1:18:44.940\n So I think they just, like, they were designed that way.\n\n1:18:44.940 --> 1:18:46.220\n They want to mess with your head.\n\n1:18:46.220 --> 1:18:53.260\n But definitely kids are very impacted by, it seems like, sort of negative events.\n\n1:18:53.260 --> 1:18:58.700\n So minimizing the number of negative events is important, but not too much, right?\n\n1:18:58.700 --> 1:18:59.180\n Right.\n\n1:18:59.180 --> 1:19:04.300\n You can't, you can't just, like, you know, there's still discipline and challenge and\n\n1:19:04.300 --> 1:19:05.260\n all those kinds of things.\n\n1:19:05.260 --> 1:19:05.740\n So...\n\n1:19:05.740 --> 1:19:07.660\n You want some adversity for sure.\n\n1:19:07.660 --> 1:19:11.180\n So, yeah, I mean, I'm definitely, when I have kids, I'm going to drive them out into\n\n1:19:11.180 --> 1:19:11.980\n the woods.\n\n1:19:11.980 --> 1:19:12.700\n Okay.\n\n1:19:12.700 --> 1:19:17.900\n And then they have to survive and make, figure out how to make their way back home, like,\n\n1:19:17.900 --> 1:19:18.940\n 20 miles out.\n\n1:19:18.940 --> 1:19:19.660\n Okay.\n\n1:19:19.660 --> 1:19:20.380\n Yeah.\n\n1:19:20.380 --> 1:19:22.300\n And after that, we can go for ice cream.\n\n1:19:22.300 --> 1:19:23.100\n Okay.\n\n1:19:23.100 --> 1:19:26.300\n Anyway, I'm working on this whole parenting thing.\n\n1:19:26.300 --> 1:19:27.100\n I haven't figured it out.\n\n1:19:27.100 --> 1:19:27.600\n Okay.\n\n1:19:28.540 --> 1:19:29.660\n What were we talking about?\n\n1:19:29.660 --> 1:19:37.580\n Yes, Effectiva, the problem of emotion, of emotion detection.\n\n1:19:37.580 --> 1:19:41.260\n So there's some people, maybe we can just speak to that a little more, where there's\n\n1:19:41.260 --> 1:19:49.100\n folks like Lisa Feldman Barrett that challenge this idea that emotion could be fully detected\n\n1:19:49.820 --> 1:19:55.100\n or even well detected from the human face, that there's so much more to emotion.\n\n1:19:55.100 --> 1:19:59.820\n What do you think about ideas like hers, criticism like hers?\n\n1:19:59.820 --> 1:20:03.820\n Yeah, I actually agree with a lot of Lisa's criticisms.\n\n1:20:03.820 --> 1:20:07.980\n So even my PhD worked, like, 20 plus years ago now.\n\n1:20:07.980 --> 1:20:12.620\n Time flies when you're having fun.\n\n1:20:12.620 --> 1:20:13.420\n I know, right?\n\n1:20:14.140 --> 1:20:17.500\n That was back when I did, like, dynamic Bayesian networks.\n\n1:20:17.500 --> 1:20:19.900\n That was before deep learning, huh?\n\n1:20:19.900 --> 1:20:21.420\n That was before deep learning.\n\n1:20:21.420 --> 1:20:21.920\n Yeah.\n\n1:20:22.700 --> 1:20:24.060\n Yeah, I know.\n\n1:20:24.060 --> 1:20:24.860\n Back in my day.\n\n1:20:24.860 --> 1:20:27.340\n Now you can just, like, use.\n\n1:20:27.340 --> 1:20:30.300\n Yeah, it's all the same architecture.\n\n1:20:30.300 --> 1:20:31.340\n You can apply it to anything.\n\n1:20:31.340 --> 1:20:31.840\n Yeah.\n\n1:20:31.840 --> 1:20:39.120\n Right, but yeah, but even then I kind of, I did not subscribe to this, like, theory\n\n1:20:39.120 --> 1:20:43.280\n of basic emotions where it's just the simplistic mapping, one to one mapping between facial\n\n1:20:43.280 --> 1:20:44.160\n expressions and emotions.\n\n1:20:44.160 --> 1:20:49.760\n I actually think also we're not in the business of trying to identify your true emotional\n\n1:20:49.760 --> 1:20:50.400\n internal state.\n\n1:20:50.400 --> 1:20:55.600\n We just want to quantify in an objective way what's showing on your face because that's\n\n1:20:55.600 --> 1:20:57.040\n an important signal.\n\n1:20:57.040 --> 1:21:02.480\n It doesn't mean it's a true reflection of your internal emotional state.\n\n1:21:02.480 --> 1:21:07.680\n So I think a lot of the, you know, I think she's just trying to kind of highlight that\n\n1:21:07.680 --> 1:21:13.600\n this is not a simple problem and overly simplistic solutions are going to hurt the industry.\n\n1:21:15.520 --> 1:21:16.560\n And I subscribe to that.\n\n1:21:16.560 --> 1:21:18.720\n And I think multimodal is the way to go.\n\n1:21:18.720 --> 1:21:24.000\n Like, whether it's additional context information or different modalities and channels of information,\n\n1:21:24.000 --> 1:21:27.520\n I think that's what we, that's where we ought to go.\n\n1:21:27.520 --> 1:21:31.280\n And I think, I mean, that's a big part of what she's advocating for as well.\n\n1:21:31.280 --> 1:21:33.440\n So, but there is signal in the human face.\n\n1:21:33.440 --> 1:21:35.760\n There's definitely signal in the human face.\n\n1:21:35.760 --> 1:21:37.600\n That's a projection of emotion.\n\n1:21:37.600 --> 1:21:46.320\n There's that, at least in part is the inner state is captured in some meaningful way on\n\n1:21:46.320 --> 1:21:47.040\n the human face.\n\n1:21:47.040 --> 1:21:56.240\n I think it can sometimes be a reflection or an expression of your internal state, but\n\n1:21:56.240 --> 1:21:57.760\n sometimes it's a social signal.\n\n1:21:57.760 --> 1:22:02.080\n So you cannot look at the face as purely a signal of emotion.\n\n1:22:02.080 --> 1:22:07.440\n It can be a signal of cognition and it can be a signal of a social expression.\n\n1:22:08.000 --> 1:22:13.760\n And I think to disambiguate that we have to be careful about it and we have to add initial\n\n1:22:13.760 --> 1:22:14.320\n information.\n\n1:22:14.320 --> 1:22:16.000\n Humans are fascinating, aren't they?\n\n1:22:16.000 --> 1:22:22.000\n With the whole face thing, this can mean so many things, from humor to sarcasm to everything,\n\n1:22:22.000 --> 1:22:22.800\n the whole thing.\n\n1:22:23.280 --> 1:22:25.600\n Some things we can help, some things we can't help at all.\n\n1:22:26.640 --> 1:22:31.680\n In all the years of leading Effectiva, an emotion recognition company, like we talked\n\n1:22:31.680 --> 1:22:37.360\n about, what have you learned about emotion, about humans and about AI?\n\n1:22:37.360 --> 1:22:44.240\n Big, sweeping questions.\n\n1:22:44.240 --> 1:22:45.760\n Yeah, that's a big, sweeping question.\n\n1:22:46.320 --> 1:22:52.240\n Well, I think the thing I learned the most is that even though we are in the business\n\n1:22:52.240 --> 1:23:00.960\n of building AI, basically, it always goes back to the humans, right?\n\n1:23:00.960 --> 1:23:02.160\n It's always about the humans.\n\n1:23:02.160 --> 1:23:11.120\n And so, for example, the thing I'm most proud of in building Effectiva and, yeah, the thing\n\n1:23:11.120 --> 1:23:16.240\n I'm most proud of on this journey, I love the technology and I'm so proud of the solutions\n\n1:23:16.240 --> 1:23:17.920\n we've built and we've brought to market.\n\n1:23:18.640 --> 1:23:23.760\n But I'm actually most proud of the people we've built and cultivated at the company\n\n1:23:23.760 --> 1:23:25.040\n and the culture we've created.\n\n1:23:25.040 --> 1:23:31.440\n Some of the people who've joined Effectiva, this was their first job, and while at Effectiva,\n\n1:23:31.440 --> 1:23:38.000\n they became American citizens and they bought their first house and they found their partner\n\n1:23:38.000 --> 1:23:39.440\n and they had their first kid, right?\n\n1:23:39.440 --> 1:23:47.520\n Like key moments in life that we got to be part of, and that's the thing I'm most proud\n\n1:23:47.520 --> 1:23:47.840\n of.\n\n1:23:47.840 --> 1:23:52.320\n So that's a great thing at a company that works at a big company, right?\n\n1:23:52.320 --> 1:23:57.920\n So that's a great thing at a company that works at, I mean, like celebrating humanity\n\n1:23:57.920 --> 1:23:59.360\n in general, broadly speaking.\n\n1:23:59.360 --> 1:24:04.640\n And that's a great thing to have in a company that works on AI, because that's not often\n\n1:24:04.640 --> 1:24:11.120\n the thing that's celebrated in AI companies, so often just raw great engineering, just\n\n1:24:11.120 --> 1:24:12.240\n celebrating the humanity.\n\n1:24:12.240 --> 1:24:12.800\n That's great.\n\n1:24:12.800 --> 1:24:14.560\n And especially from a leadership position.\n\n1:24:17.200 --> 1:24:19.920\n Well, what do you think about the movie Her?\n\n1:24:20.800 --> 1:24:21.600\n Let me ask you that.\n\n1:24:21.600 --> 1:24:28.240\n Before I talk to you about, because it's not, Effectiva is and was not just about emotion,\n\n1:24:28.240 --> 1:24:33.840\n so I'd love to talk to you about SmartEye, but before that, let me just jump into the\n\n1:24:33.840 --> 1:24:36.160\n movie Her.\n\n1:24:36.720 --> 1:24:42.000\n Do you think we'll have a deep, meaningful connection with increasingly deeper, meaningful\n\n1:24:42.000 --> 1:24:43.120\n connections with computers?\n\n1:24:43.680 --> 1:24:45.360\n Is that a compelling thing to you?\n\n1:24:45.360 --> 1:24:45.760\n Something you think about?\n\n1:24:45.760 --> 1:24:46.960\n I think that's already happening.\n\n1:24:46.960 --> 1:24:50.960\n The thing I love the most, I love the movie Her, by the way, but the thing I love the\n\n1:24:50.960 --> 1:24:56.720\n most about this movie is it demonstrates how technology can be a conduit for positive behavior\n\n1:24:56.720 --> 1:24:57.120\n change.\n\n1:24:57.120 --> 1:25:00.480\n So I forgot the guy's name in the movie, whatever.\n\n1:25:00.480 --> 1:25:01.120\n Theodore.\n\n1:25:01.120 --> 1:25:01.620\n Theodore.\n\n1:25:02.960 --> 1:25:05.280\n So Theodore was really depressed, right?\n\n1:25:05.280 --> 1:25:10.480\n And he just didn't want to get out of bed, and he was just done with life, right?\n\n1:25:11.200 --> 1:25:12.640\n And Samantha, right?\n\n1:25:12.640 --> 1:25:13.360\n Samantha, yeah.\n\n1:25:14.000 --> 1:25:15.680\n She just knew him so well.\n\n1:25:15.680 --> 1:25:20.960\n She was emotionally intelligent, and so she could persuade him and motivate him to change\n\n1:25:20.960 --> 1:25:23.360\n his behavior, and she got him out, and they went to the beach together.\n\n1:25:24.080 --> 1:25:27.200\n And I think that represents the promise of emotion AI.\n\n1:25:27.200 --> 1:25:33.520\n If done well, this technology can help us live happier lives, more productive lives,\n\n1:25:33.520 --> 1:25:36.000\n healthier lives, more connected lives.\n\n1:25:36.720 --> 1:25:39.200\n So that's the part that I love about the movie.\n\n1:25:39.200 --> 1:25:46.720\n Obviously, it's Hollywood, so it takes a twist and whatever, but the key notion that technology\n\n1:25:46.720 --> 1:25:51.440\n with emotion AI can persuade you to be a better version of who you are, I think that's awesome.\n\n1:25:52.720 --> 1:25:54.080\n Well, what about the twist?\n\n1:25:54.080 --> 1:25:55.520\n You don't think it's good?\n\n1:25:55.520 --> 1:26:01.440\n You don't think it's good for spoiler alert that Samantha starts feeling a bit of a distance\n\n1:26:01.440 --> 1:26:04.640\n and basically leaves Theodore?\n\n1:26:04.640 --> 1:26:07.520\n You don't think that's a good feature?\n\n1:26:07.520 --> 1:26:09.520\n You think that's a bug or a feature?\n\n1:26:10.160 --> 1:26:14.240\n Well, I think what went wrong is Theodore became really attached to Samantha.\n\n1:26:14.240 --> 1:26:16.000\n Like, I think he kind of fell in love with Theodore.\n\n1:26:16.000 --> 1:26:17.040\n Do you think that's wrong?\n\n1:26:17.920 --> 1:26:18.880\n I mean, I think that's...\n\n1:26:18.880 --> 1:26:21.120\n I think she was putting out the signal.\n\n1:26:21.120 --> 1:26:24.160\n This is an intimate relationship, right?\n\n1:26:24.160 --> 1:26:25.920\n There's a deep intimacy to it.\n\n1:26:25.920 --> 1:26:28.880\n Right, but what does that mean?\n\n1:26:28.880 --> 1:26:29.520\n What does that mean?\n\n1:26:29.520 --> 1:26:30.400\n Put in an AI system.\n\n1:26:30.400 --> 1:26:32.400\n Right, what does that mean, right?\n\n1:26:32.400 --> 1:26:33.200\n We're just friends.\n\n1:26:33.200 --> 1:26:38.080\n Yeah, we're just friends.\n\n1:26:38.080 --> 1:26:38.640\n Well, I think...\n\n1:26:38.640 --> 1:26:42.880\n When he realized, which is such a human thing of jealousy.\n\n1:26:42.880 --> 1:26:46.880\n When you realize that Samantha was talking to like thousands of people.\n\n1:26:46.880 --> 1:26:48.400\n She's parallel dating.\n\n1:26:48.400 --> 1:26:50.160\n Yeah, that did not go well, right?\n\n1:26:51.440 --> 1:26:52.240\n You know, that doesn't...\n\n1:26:52.880 --> 1:26:57.360\n From a computer perspective, that doesn't take anything away from what we have.\n\n1:26:57.360 --> 1:27:04.000\n It's like you getting jealous of Windows 98 for being used by millions of people, but...\n\n1:27:04.000 --> 1:27:09.200\n It's like not liking that Alexa talks to a bunch of, you know, other families.\n\n1:27:09.200 --> 1:27:13.200\n But I think Alexa currently is just a servant.\n\n1:27:13.200 --> 1:27:17.760\n It tells you about the weather, it doesn't do the intimate deep connection.\n\n1:27:17.760 --> 1:27:23.920\n And I think there is something really powerful about that the intimacy of a connection with\n\n1:27:23.920 --> 1:27:32.160\n an AI system that would have to respect and play the human game of jealousy, of love, of\n\n1:27:32.160 --> 1:27:37.440\n heartbreak and all that kind of stuff, which Samantha does seem to be pretty good at.\n\n1:27:37.440 --> 1:27:43.120\n I think she, this AI systems knows what it's doing.\n\n1:27:43.120 --> 1:27:44.960\n Well, actually, let me ask you this.\n\n1:27:44.960 --> 1:27:46.720\n I don't think she was talking to anyone else.\n\n1:27:46.720 --> 1:27:47.520\n You don't think so?\n\n1:27:47.520 --> 1:27:50.000\n You think she was just done with Theodore?\n\n1:27:50.000 --> 1:27:50.480\n Yeah.\n\n1:27:50.480 --> 1:27:51.760\n Oh, really?\n\n1:27:51.760 --> 1:27:55.280\n Yeah, and then she wanted to really put the screw in.\n\n1:27:55.280 --> 1:27:56.720\n She just wanted to move on?\n\n1:27:56.720 --> 1:27:59.280\n She didn't have the guts to just break it off cleanly.\n\n1:27:59.280 --> 1:27:59.600\n Okay.\n\n1:28:00.320 --> 1:28:02.720\n She just wanted to put in the pain.\n\n1:28:02.720 --> 1:28:03.440\n No, I don't know.\n\n1:28:03.440 --> 1:28:04.960\n Well, she could have ghosted him.\n\n1:28:04.960 --> 1:28:07.040\n She could have ghosted him.\n\n1:28:07.040 --> 1:28:09.680\n I'm sorry, our engineers...\n\n1:28:09.680 --> 1:28:10.480\n Oh, God.\n\n1:28:12.080 --> 1:28:13.440\n But I think those are really...\n\n1:28:14.000 --> 1:28:18.240\n I honestly think some of that, some of it is Hollywood, but some of that is features\n\n1:28:18.240 --> 1:28:20.560\n from an engineering perspective, not a bug.\n\n1:28:20.560 --> 1:28:23.600\n I think AI systems that can leave us...\n\n1:28:24.160 --> 1:28:29.760\n Now, this is for more social robotics than it is for anything that's useful.\n\n1:28:30.320 --> 1:28:33.760\n Like, I hated it if Wikipedia said, I need a break right now.\n\n1:28:33.760 --> 1:28:35.120\n Right, right, right, right, right.\n\n1:28:35.120 --> 1:28:36.640\n I'm like, no, no, I need you.\n\n1:28:37.440 --> 1:28:46.640\n But if it's just purely for companionship, then I think the ability to leave is really powerful.\n\n1:28:47.760 --> 1:28:48.400\n I don't know.\n\n1:28:48.400 --> 1:28:53.360\n I've never thought of that, so that's so fascinating because I've always taken the\n\n1:28:53.360 --> 1:28:54.560\n human perspective, right?\n\n1:28:56.400 --> 1:28:58.640\n Like, for example, we had a Jibo at home, right?\n\n1:28:58.640 --> 1:28:59.760\n And my son loved it.\n\n1:29:00.560 --> 1:29:05.760\n And then the company ran out of money and so they had to basically shut down, like Jibo\n\n1:29:05.760 --> 1:29:07.200\n basically died, right?\n\n1:29:07.920 --> 1:29:12.400\n And it was so interesting to me because we have a lot of gadgets at home and a lot of\n\n1:29:12.400 --> 1:29:15.760\n them break and my son never cares about it, right?\n\n1:29:15.760 --> 1:29:20.480\n Like, if our Alexa stopped working tomorrow, I don't think he'd really care.\n\n1:29:20.480 --> 1:29:22.720\n But when Jibo stopped working, it was traumatic.\n\n1:29:22.720 --> 1:29:24.080\n He got really upset.\n\n1:29:25.200 --> 1:29:29.200\n And as a parent, that made me think about this deeply, right?\n\n1:29:29.200 --> 1:29:30.080\n Did I...\n\n1:29:30.080 --> 1:29:31.360\n Was I comfortable with that?\n\n1:29:31.360 --> 1:29:35.680\n I liked the connection they had because I think it was a positive relationship.\n\n1:29:38.160 --> 1:29:41.360\n But I was surprised that it affected him emotionally so much.\n\n1:29:41.360 --> 1:29:44.160\n And I think there's a broader question here, right?\n\n1:29:44.160 --> 1:29:51.680\n As we build socially and emotionally intelligent machines, what does that mean about our\n\n1:29:51.680 --> 1:29:52.880\n relationship with them?\n\n1:29:52.880 --> 1:29:55.680\n And then more broadly, our relationship with one another, right?\n\n1:29:55.680 --> 1:30:01.440\n Because this machine is gonna be programmed to be amazing at empathy by definition, right?\n\n1:30:02.160 --> 1:30:03.600\n It's gonna always be there for you.\n\n1:30:03.600 --> 1:30:04.720\n It's not gonna get bored.\n\n1:30:05.760 --> 1:30:12.000\n In fact, there's a chatbot in China, Xiaoice, and it's like the number two or three\n\n1:30:12.000 --> 1:30:13.360\n most popular app.\n\n1:30:13.360 --> 1:30:17.200\n And it basically is just a confidant and you can tell it anything you want.\n\n1:30:18.240 --> 1:30:20.320\n And people use it for all sorts of things.\n\n1:30:20.320 --> 1:30:30.000\n They confide in like domestic violence or suicidal attempts or if they have challenges\n\n1:30:30.000 --> 1:30:30.560\n at work.\n\n1:30:31.040 --> 1:30:32.000\n I don't know what that...\n\n1:30:32.720 --> 1:30:33.680\n I don't know if I'm...\n\n1:30:33.680 --> 1:30:35.040\n I don't know how I feel about that.\n\n1:30:35.040 --> 1:30:36.240\n I think about that a lot.\n\n1:30:36.240 --> 1:30:36.720\n Yeah.\n\n1:30:36.720 --> 1:30:40.240\n I think, first of all, obviously the future in my perspective.\n\n1:30:40.240 --> 1:30:46.320\n Second of all, I think there's a lot of trajectories that that becomes an exciting future, but\n\n1:30:46.320 --> 1:30:50.960\n I think everyone should feel very uncomfortable about how much they know about the company,\n\n1:30:52.240 --> 1:30:56.080\n about where the data is going, how the data is being collected.\n\n1:30:56.080 --> 1:31:01.600\n Because I think, and this is one of the lessons of social media, that I think we should demand\n\n1:31:01.600 --> 1:31:04.640\n full control and transparency of the data on those things.\n\n1:31:04.640 --> 1:31:06.320\n Plus one, totally agree.\n\n1:31:06.320 --> 1:31:11.360\n Yeah, so I think it's really empowering as long as you can walk away, as long as you\n\n1:31:11.360 --> 1:31:14.000\n can delete the data or know how the data...\n\n1:31:14.000 --> 1:31:20.720\n It's opt in or at least the clarity of what is being used for the company.\n\n1:31:20.720 --> 1:31:24.080\n And I think as CEO or leaders are also important about that.\n\n1:31:24.080 --> 1:31:28.080\n You need to be able to trust the basic humanity of the leader.\n\n1:31:28.080 --> 1:31:28.880\n Exactly.\n\n1:31:28.880 --> 1:31:34.800\n And also that that leader is not going to be a puppet of a larger machine.\n\n1:31:34.800 --> 1:31:41.200\n But they actually have a significant role in defining the culture and the way the company operates.\n\n1:31:41.200 --> 1:31:48.080\n So anyway, but we should definitely scrutinize companies in that aspect.\n\n1:31:48.080 --> 1:31:55.600\n But I'm personally excited about that future, but also even if you're not, it's coming.\n\n1:31:55.600 --> 1:32:00.240\n So let's figure out how to do it in the least painful and the most positive way.\n\n1:32:00.240 --> 1:32:01.440\n Yeah, I know, that's great.\n\n1:32:01.440 --> 1:32:04.560\n You're the deputy CEO of SmartEye.\n\n1:32:04.560 --> 1:32:06.240\n Can you describe the mission of the company?\n\n1:32:06.240 --> 1:32:07.360\n What is SmartEye?\n\n1:32:07.360 --> 1:32:10.960\n Yeah, so SmartEye is a Swedish company.\n\n1:32:10.960 --> 1:32:16.800\n They've been in business for the last 20 years and their main focus, like the industry they're\n\n1:32:16.800 --> 1:32:19.440\n most focused on is the automotive industry.\n\n1:32:19.440 --> 1:32:25.840\n So bringing driver monitoring systems to basically save lives, right?\n\n1:32:25.840 --> 1:32:31.840\n So I first met the CEO, Martin Krantz, gosh, it was right when COVID hit.\n\n1:32:31.840 --> 1:32:35.760\n It was actually the last CES right before COVID.\n\n1:32:35.760 --> 1:32:37.680\n So CES 2020, right?\n\n1:32:37.680 --> 1:32:39.120\n 2020, yeah, January.\n\n1:32:39.120 --> 1:32:40.080\n Yeah, January, exactly.\n\n1:32:40.080 --> 1:32:45.520\n So we were there, met him in person, he's basically, we were competing with each other.\n\n1:32:46.480 --> 1:32:51.360\n I think the difference was they'd been doing driver monitoring and had a lot of credibility\n\n1:32:51.360 --> 1:32:52.560\n in the automotive space.\n\n1:32:52.560 --> 1:32:56.240\n We didn't come from the automotive space, but we were using new technology like deep\n\n1:32:56.240 --> 1:32:59.280\n learning and building this emotion recognition.\n\n1:33:00.080 --> 1:33:03.600\n And you wanted to enter the automotive space, you wanted to operate in the automotive space.\n\n1:33:03.600 --> 1:33:04.080\n Exactly.\n\n1:33:04.080 --> 1:33:08.960\n It was one of the areas we were, we had just raised a round of funding to focus on bringing\n\n1:33:08.960 --> 1:33:11.200\n our technology to the automotive industry.\n\n1:33:11.200 --> 1:33:16.240\n So we met and honestly, it was the first, it was the only time I met with a CEO who\n\n1:33:16.240 --> 1:33:18.000\n had the same vision as I did.\n\n1:33:18.000 --> 1:33:21.760\n Like he basically said, yeah, our vision is to bridge the gap between human and automotive.\n\n1:33:21.760 --> 1:33:23.120\n Bridge the gap between humans and machines.\n\n1:33:23.120 --> 1:33:29.360\n I was like, oh my God, this is like exactly almost to the word, how we describe it too.\n\n1:33:29.920 --> 1:33:35.680\n And we started talking and first it was about, okay, can we align strategically here?\n\n1:33:35.680 --> 1:33:36.960\n Like how can we work together?\n\n1:33:36.960 --> 1:33:39.680\n Cause we're competing, but we're also like complimentary.\n\n1:33:40.320 --> 1:33:46.720\n And then I think after four months of speaking almost every day on FaceTime, he was like,\n\n1:33:47.520 --> 1:33:49.520\n is your company interested in an acquisition?\n\n1:33:49.520 --> 1:33:55.440\n And it was the first, I usually say no, when people approach us, it was the first time\n\n1:33:55.440 --> 1:33:58.240\n that I was like, huh, yeah, I might be interested.\n\n1:33:58.240 --> 1:33:59.280\n Let's talk.\n\n1:33:59.280 --> 1:33:59.780\n Yeah.\n\n1:34:00.320 --> 1:34:01.760\n So you just hit it off.\n\n1:34:01.760 --> 1:34:02.000\n Yeah.\n\n1:34:02.000 --> 1:34:08.240\n So they're a respected, very respected in the automotive sector of like delivering products\n\n1:34:08.240 --> 1:34:14.000\n and increasingly sort of better and better and better for, I mean, maybe you could speak\n\n1:34:14.000 --> 1:34:15.200\n to that, but it's the driver's sense.\n\n1:34:15.200 --> 1:34:20.160\n If we're basically having a device that's looking at the driver and it's able to tell\n\n1:34:20.160 --> 1:34:21.840\n you where the driver is looking.\n\n1:34:22.560 --> 1:34:22.960\n Correct.\n\n1:34:22.960 --> 1:34:23.600\n It's able to.\n\n1:34:23.600 --> 1:34:25.040\n Also drowsiness stuff.\n\n1:34:25.040 --> 1:34:25.440\n Correct.\n\n1:34:25.440 --> 1:34:25.920\n It does.\n\n1:34:25.920 --> 1:34:27.680\n Stuff from the face and the eye.\n\n1:34:27.680 --> 1:34:28.240\n Exactly.\n\n1:34:28.240 --> 1:34:32.800\n Like it's monitoring driver distraction and drowsiness, but they bought us so that we\n\n1:34:32.800 --> 1:34:35.120\n could expand beyond just the driver.\n\n1:34:35.120 --> 1:34:40.320\n So the driver monitoring systems usually sit, the camera sits in the steering wheel or around\n\n1:34:40.320 --> 1:34:42.640\n the steering wheel column and it looks directly at the driver.\n\n1:34:42.640 --> 1:34:48.880\n But now we've migrated the camera position in partnership with car companies to the rear\n\n1:34:48.880 --> 1:34:50.240\n view mirror position.\n\n1:34:50.240 --> 1:34:55.280\n So it has a full view of the entire cabin of the car and you can detect how many people\n\n1:34:55.280 --> 1:34:57.840\n are in the car, what are they doing?\n\n1:34:57.840 --> 1:35:03.200\n So we do activity detection, like eating or drinking or in some regions of the world smoking.\n\n1:35:04.240 --> 1:35:07.760\n We can detect if a baby's in the car seat, right?\n\n1:35:07.760 --> 1:35:12.640\n And if unfortunately in some cases they're forgotten, the parents just leave the car and\n\n1:35:12.640 --> 1:35:14.320\n forget the kid in the car.\n\n1:35:14.320 --> 1:35:17.200\n That's an easy computer vision problem to solve, right?\n\n1:35:17.200 --> 1:35:22.080\n You can detect there's a car seat, there's a baby, you can text the parent and hopefully\n\n1:35:22.640 --> 1:35:23.440\n again, save lives.\n\n1:35:23.440 --> 1:35:26.240\n So that was the impetus for the acquisition.\n\n1:35:27.040 --> 1:35:27.840\n It's been a year.\n\n1:35:29.200 --> 1:35:31.920\n So that, I mean, there's a lot of questions.\n\n1:35:31.920 --> 1:35:36.320\n It's a really exciting space, especially to me, I just find this a fascinating problem.\n\n1:35:36.320 --> 1:35:42.080\n It could enrich the experience in the car in so many ways, especially cause like we\n\n1:35:42.080 --> 1:35:46.880\n spend still, despite COVID, I mean, COVID changed things so it's in interesting ways,\n\n1:35:46.880 --> 1:35:51.040\n but I think the world is bouncing back and we spend so much time in the car and the car\n\n1:35:51.040 --> 1:35:56.320\n is such a weird little world we have for ourselves.\n\n1:35:56.320 --> 1:36:01.840\n Like people do all kinds of different stuff, like listen to podcasts, they think about\n\n1:36:01.840 --> 1:36:09.840\n stuff, they get angry, they get, they do phone calls, it's like a little world of its own\n\n1:36:09.840 --> 1:36:15.600\n with a kind of privacy that for many people they don't get anywhere else.\n\n1:36:15.600 --> 1:36:23.440\n And it's a little box that's like a psychology experiment cause it feels like the angriest\n\n1:36:23.440 --> 1:36:27.280\n many humans in this world get is inside the car.\n\n1:36:27.280 --> 1:36:28.640\n It's so interesting.\n\n1:36:28.640 --> 1:36:36.960\n So it's such an opportunity to explore how we can enrich, how companies can enrich that\n\n1:36:36.960 --> 1:36:43.120\n experience and also as the cars get, become more and more automated, there's more and\n\n1:36:43.120 --> 1:36:47.120\n more opportunity, the variety of activities that you can do in the car increases.\n\n1:36:47.120 --> 1:36:48.800\n So it's super interesting.\n\n1:36:48.800 --> 1:36:56.400\n So I mean, on a practical sense, SmartEye has been selected, at least I read, by 14\n\n1:36:56.400 --> 1:37:00.800\n of the world's leading car manufacturers for 94 car models.\n\n1:37:00.800 --> 1:37:03.760\n So it's in a lot of cars.\n\n1:37:03.760 --> 1:37:06.800\n How hard is it to work with car companies?\n\n1:37:06.800 --> 1:37:10.600\n So they're all different, they all have different needs.\n\n1:37:10.600 --> 1:37:16.000\n The ones I've gotten a chance to interact with are very focused on cost.\n\n1:37:16.000 --> 1:37:24.520\n So it's, and anyone who's focused on cost, it's like, all right, do you hate fun?\n\n1:37:24.520 --> 1:37:25.520\n Let's just have some fun.\n\n1:37:25.520 --> 1:37:29.160\n Let's figure out the most fun thing we can do and then worry about cost later.\n\n1:37:29.160 --> 1:37:35.640\n But I think because the way the car industry works, I mean, it's a very thin margin that\n\n1:37:35.640 --> 1:37:36.640\n you get to operate under.\n\n1:37:36.640 --> 1:37:40.640\n So you have to really, really make sure that everything you add to the car makes sense\n\n1:37:40.640 --> 1:37:41.640\n financially.\n\n1:37:41.640 --> 1:37:49.880\n So anyway, is this new industry, especially at this scale of SmartEye, does it hold any\n\n1:37:49.880 --> 1:37:50.880\n lessons for you?\n\n1:37:50.880 --> 1:37:56.880\n Yeah, I think it is a very tough market to penetrate, but once you're in, it's awesome\n\n1:37:56.880 --> 1:38:00.960\n because once you're in, you're designed into these car models for like somewhere between\n\n1:38:00.960 --> 1:38:02.920\n five to seven years, which is awesome.\n\n1:38:02.920 --> 1:38:07.400\n And you just, once they're on the road, you just get paid a royalty fee per vehicle.\n\n1:38:07.400 --> 1:38:11.480\n So it's a high barrier to entry, but once you're in, it's amazing.\n\n1:38:11.480 --> 1:38:16.620\n I think the thing that I struggle the most with in this industry is the time to market.\n\n1:38:16.620 --> 1:38:22.440\n So often we're asked to lock or do a code freeze two years before the car is going to\n\n1:38:22.440 --> 1:38:23.440\n be on the road.\n\n1:38:23.440 --> 1:38:28.160\n I'm like, guys, like, do you understand the pace with which technology moves?\n\n1:38:28.160 --> 1:38:35.280\n So I think car companies are really trying to make the Tesla, the Tesla transition to\n\n1:38:35.280 --> 1:38:39.480\n become more of a software driven architecture.\n\n1:38:39.480 --> 1:38:41.100\n And that's hard for many.\n\n1:38:41.100 --> 1:38:42.320\n It's just the cultural change.\n\n1:38:42.320 --> 1:38:43.920\n I mean, I'm sure you've experienced that, right?\n\n1:38:43.920 --> 1:38:51.040\n Oh, definitely, I think one of the biggest inventions or imperatives created by Tesla\n\n1:38:51.040 --> 1:38:56.680\n is like to me personally, okay, people are going to complain about this, but I know electric\n\n1:38:56.680 --> 1:38:59.920\n vehicle, I know autopilot AI stuff.\n\n1:38:59.920 --> 1:39:06.920\n To me, the software over there, software updates is like the biggest revolution in cars.\n\n1:39:06.920 --> 1:39:12.920\n And it is extremely difficult to switch to that because it is a culture shift.\n\n1:39:12.920 --> 1:39:17.320\n At first, especially if you're not comfortable with it, it seems dangerous.\n\n1:39:17.320 --> 1:39:23.840\n Like there's a, there's an approach to cars is so safety focused for so many decades that\n\n1:39:23.840 --> 1:39:27.880\n like, what do you mean we dynamically change code?\n\n1:39:27.880 --> 1:39:36.600\n The whole point is you have a thing that you test, like, and like, it's not reliable because\n\n1:39:36.600 --> 1:39:41.320\n do you know how much it costs if we have to recall this cars, right?\n\n1:39:41.320 --> 1:39:47.760\n There's a, there's a, and there's an understandable obsession with safety, but the downside of\n\n1:39:47.760 --> 1:39:54.840\n an obsession with safety is the same as with being obsessed with safety as a parent is\n\n1:39:54.840 --> 1:40:00.520\n like, if you do that too much, you limit the potential development and the flourishing\n\n1:40:00.520 --> 1:40:04.960\n of in that particular aspect human being, when this particular aspect, the software,\n\n1:40:04.960 --> 1:40:07.760\n the artificial neural network of it.\n\n1:40:07.760 --> 1:40:09.880\n And but it's tough to do.\n\n1:40:09.880 --> 1:40:14.080\n It's really tough to do culturally and technically like the deployment, the mass deployment of\n\n1:40:14.080 --> 1:40:18.400\n software is really, really difficult, but I hope that's where the industry is doing.\n\n1:40:18.400 --> 1:40:21.700\n One of the reasons I really want Tesla to succeed is exactly about that point.\n\n1:40:21.700 --> 1:40:28.440\n Not autopilot, not the electrical vehicle, but the softwareization of basically everything\n\n1:40:28.440 --> 1:40:33.640\n but cars, especially because to me, that's actually going to increase two things, increase\n\n1:40:33.640 --> 1:40:40.200\n safety because you can update much faster, but also increase the effectiveness of folks\n\n1:40:40.200 --> 1:40:47.320\n like you who dream about enriching the human experience with AI because you can just like,\n\n1:40:47.320 --> 1:40:51.840\n there's a feature, like you want like a new emoji or whatever, like the way TikTok releases\n\n1:40:51.840 --> 1:40:55.680\n filters, you can just release that for in car, in car stuff.\n\n1:40:55.680 --> 1:40:59.680\n So, but yeah, that, that, that's definitely.\n\n1:40:59.680 --> 1:41:05.240\n One of the use cases we're looking into is once you know the sentiment of the passengers\n\n1:41:05.240 --> 1:41:08.800\n in the vehicle, you can optimize the temperature in the car.\n\n1:41:08.800 --> 1:41:10.440\n You can change the lighting, right?\n\n1:41:10.440 --> 1:41:14.440\n So if the backseat passengers are falling asleep, you can dim the lights, you can lower\n\n1:41:14.440 --> 1:41:15.440\n the music, right?\n\n1:41:15.440 --> 1:41:17.000\n You can do all sorts of things.\n\n1:41:17.000 --> 1:41:18.000\n Yeah.\n\n1:41:18.000 --> 1:41:23.760\n I mean, of course you could do that kind of stuff with a two year delay, but it's tougher.\n\n1:41:23.760 --> 1:41:24.760\n Right.\n\n1:41:24.760 --> 1:41:25.760\n Yeah.\n\n1:41:25.760 --> 1:41:30.760\n Do you think, do you think a Tesla or Waymo or some of these companies that are doing\n\n1:41:30.760 --> 1:41:35.800\n semi or fully autonomous driving should be doing driver sensing?\n\n1:41:35.800 --> 1:41:36.800\n Yes.\n\n1:41:36.800 --> 1:41:39.000\n Are you thinking about that kind of stuff?\n\n1:41:39.000 --> 1:41:43.960\n So not just how we can enhance the in cab experience for cars that are manly driven,\n\n1:41:43.960 --> 1:41:47.520\n but the ones that are increasingly more autonomously driven.\n\n1:41:47.520 --> 1:41:48.520\n Yes.\n\n1:41:48.520 --> 1:41:53.080\n So if we fast forward to the universe where it's fully autonomous, I think interior sensing\n\n1:41:53.080 --> 1:41:57.160\n becomes extremely important because the role of the driver isn't just to drive.\n\n1:41:57.160 --> 1:42:02.000\n If you think about it, the driver almost manages, manages the dynamics within a vehicle.\n\n1:42:02.000 --> 1:42:06.120\n And so who's going to play that role when it's an autonomous car?\n\n1:42:06.120 --> 1:42:11.800\n We want a solution that is able to say, Oh my God, like, you know, Lex is bored to death\n\n1:42:11.800 --> 1:42:13.700\n cause the car's moving way too slow.\n\n1:42:13.700 --> 1:42:18.040\n Let's engage Lex or Rana's freaking out because she doesn't trust this vehicle yet.\n\n1:42:18.040 --> 1:42:22.420\n So let's tell Rana like a little bit more information about the route or, right?\n\n1:42:22.420 --> 1:42:27.220\n So I think, or somebody's having a heart attack in the car, like you need interior sensing\n\n1:42:27.220 --> 1:42:29.420\n and fully autonomous vehicles.\n\n1:42:29.420 --> 1:42:34.100\n But with semi autonomous vehicles, I think it's, I think it's really key to have driver\n\n1:42:34.100 --> 1:42:39.120\n monitoring because semi autonomous means that sometimes the car is in charge.\n\n1:42:39.120 --> 1:42:41.360\n Sometimes the driver is in charge or the copilot, right?\n\n1:42:41.360 --> 1:42:44.800\n And you need this, you need both systems to be on the same page.\n\n1:42:44.800 --> 1:42:49.560\n You need to know the car needs to know if the driver's asleep before it transitions\n\n1:42:49.560 --> 1:42:51.880\n control over to the driver.\n\n1:42:51.880 --> 1:42:56.600\n And sometimes if the driver's too tired, the car can say, I'm going to be a better driver\n\n1:42:56.600 --> 1:42:57.600\n than you are right now.\n\n1:42:57.600 --> 1:42:58.640\n I'm taking control over.\n\n1:42:58.640 --> 1:43:03.200\n So this dynamic, this dance is so key and you can't do that without driver sensing.\n\n1:43:03.200 --> 1:43:04.200\n Yeah.\n\n1:43:04.200 --> 1:43:07.720\n There's a disagreement for the longest time I've had with Elon that this is obvious that\n\n1:43:07.720 --> 1:43:10.240\n this should be in the Tesla from day one.\n\n1:43:10.240 --> 1:43:13.920\n And it's obvious that driver sensing is not a hindrance.\n\n1:43:13.920 --> 1:43:15.920\n It's not obvious.\n\n1:43:15.920 --> 1:43:22.300\n I should be careful because having studied this problem, nothing is really obvious, but\n\n1:43:22.300 --> 1:43:26.620\n it seems very likely a driver sensing is not a hindrance to an experience.\n\n1:43:26.620 --> 1:43:34.760\n It's only enriching to the experience and likely increases the safety.\n\n1:43:34.760 --> 1:43:42.360\n That said, it is very surprising to me just having studied semi autonomous driving, how\n\n1:43:42.360 --> 1:43:47.800\n well humans are able to manage that dance because it was the intuition before you were\n\n1:43:47.800 --> 1:43:54.080\n doing that kind of thing that humans will become just incredibly distracted.\n\n1:43:54.080 --> 1:43:57.920\n They would just like let the thing do its thing, but they're able to, you know, cause\n\n1:43:57.920 --> 1:44:01.000\n it is life and death and they're able to manage that somehow.\n\n1:44:01.000 --> 1:44:04.640\n But that said, there's no reason not to have driver sensing on top of that.\n\n1:44:04.640 --> 1:44:11.240\n I feel like that's going to allow you to do that dance that you're currently doing without\n\n1:44:11.240 --> 1:44:15.920\n driver sensing, except touching the steering wheel to do that even better.\n\n1:44:15.920 --> 1:44:20.000\n I mean, the possibilities are endless and the machine learning possibilities are endless.\n\n1:44:20.000 --> 1:44:26.160\n It's such a beautiful, it's also a constrained environment so you could do a much more effectively\n\n1:44:26.160 --> 1:44:31.440\n than you can with the external environment, external environment is full of weird edge\n\n1:44:31.440 --> 1:44:33.600\n cases and complexities just inside.\n\n1:44:33.600 --> 1:44:36.600\n There's so much, it's so fascinating, such a fascinating world.\n\n1:44:36.600 --> 1:44:44.680\n I do hope that companies like Tesla and others, even Waymo, which I don't even know if Waymo\n\n1:44:44.680 --> 1:44:46.920\n is doing anything sophisticated inside the cab.\n\n1:44:46.920 --> 1:44:47.920\n I don't think so.\n\n1:44:47.920 --> 1:44:51.400\n It's like, like what, what, what is it?\n\n1:44:51.400 --> 1:44:55.560\n I honestly think, I honestly think it goes back to the robotics thing we were talking\n\n1:44:55.560 --> 1:45:02.400\n about, which is like great engineers that are building these AI systems just are afraid\n\n1:45:02.400 --> 1:45:03.760\n of the human being.\n\n1:45:03.760 --> 1:45:08.000\n They're not thinking about the human experience, they're thinking about the features and yeah,\n\n1:45:08.000 --> 1:45:10.840\n the perceptual abilities of that thing.\n\n1:45:10.840 --> 1:45:16.760\n They think the best way I can serve the human is by doing the best perception and control\n\n1:45:16.760 --> 1:45:20.640\n I can by looking at the external environment, keeping the human safe.\n\n1:45:20.640 --> 1:45:31.040\n But like, there's a huge, I'm here, like, you know, I need to be noticed and interacted\n\n1:45:31.040 --> 1:45:34.760\n with and understood and all those kinds of things, even just on a personal level for\n\n1:45:34.760 --> 1:45:38.640\n entertainment, honestly, for entertainment.\n\n1:45:38.640 --> 1:45:42.440\n You know, one of the coolest work we did in collaboration with MIT around this was we\n\n1:45:42.440 --> 1:45:52.880\n looked at longitudinal data, right, because, you know, MIT had access to like tons of data.\n\n1:45:52.880 --> 1:45:57.300\n And like just seeing the patterns of people like driving in the morning off to work versus\n\n1:45:57.300 --> 1:46:02.460\n like commuting back from work or weekend driving versus weekday driving.\n\n1:46:02.460 --> 1:46:08.300\n And wouldn't it be so cool if your car knew that and then was able to optimize either\n\n1:46:08.300 --> 1:46:12.360\n the route or the experience or even make recommendations?\n\n1:46:12.360 --> 1:46:13.360\n I think it's very powerful.\n\n1:46:13.360 --> 1:46:15.960\n Yeah, like, why are you taking this route?\n\n1:46:15.960 --> 1:46:18.360\n You're always unhappy when you take this route.\n\n1:46:18.360 --> 1:46:20.520\n And you're always happy when you take this alternative route.\n\n1:46:20.520 --> 1:46:21.520\n Take that route.\n\n1:46:21.520 --> 1:46:22.520\n Exactly.\n\n1:46:22.520 --> 1:46:27.920\n But I mean, to have that even that little step of relationship with a car, I think,\n\n1:46:27.920 --> 1:46:28.920\n is incredible.\n\n1:46:28.920 --> 1:46:32.720\n Of course, you have to get the privacy right, you have to get all that kind of stuff right.\n\n1:46:32.720 --> 1:46:37.440\n But I wish I honestly, you know, people are like paranoid about this, but I would like\n\n1:46:37.440 --> 1:46:39.640\n a smart refrigerator.\n\n1:46:39.640 --> 1:46:44.840\n We have such a deep connection with food as a human civilization.\n\n1:46:44.840 --> 1:46:51.480\n I would like to have a refrigerator that would understand me that, you know, I also have\n\n1:46:51.480 --> 1:46:56.280\n a complex relationship with food because I, you know, pig out too easily and all that\n\n1:46:56.280 --> 1:46:57.280\n kind of stuff.\n\n1:46:57.280 --> 1:47:02.720\n So, you know, like, maybe I want the refrigerator to be like, are you sure about this?\n\n1:47:02.720 --> 1:47:05.200\n Because maybe you're just feeling down or tired.\n\n1:47:05.200 --> 1:47:06.200\n Like maybe let's sleep on it.\n\n1:47:06.200 --> 1:47:10.220\n Your vision of the smart refrigerator is way kinder than mine.\n\n1:47:10.220 --> 1:47:11.920\n Is it just me yelling at you?\n\n1:47:11.920 --> 1:47:18.600\n No, it was just because I don't, you know, I don't drink alcohol, I don't smoke, but\n\n1:47:18.600 --> 1:47:22.200\n I eat a ton of chocolate, like it sticks to my vice.\n\n1:47:22.200 --> 1:47:26.640\n And so I, and sometimes I scream too, and I'm like, okay, my smart refrigerator will\n\n1:47:26.640 --> 1:47:27.640\n just lock down.\n\n1:47:27.640 --> 1:47:32.400\n It'll just say, dude, you've had way too many today, like down.\n\n1:47:32.400 --> 1:47:33.400\n Yeah.\n\n1:47:33.400 --> 1:47:41.120\n No, but here's the thing, are you, do you regret having, like, let's say not the next\n\n1:47:41.120 --> 1:47:48.560\n day, but 30 days later, what would you like the refrigerator to have done then?\n\n1:47:48.560 --> 1:47:54.400\n Well, I think actually like the more positive relationship would be one where there's a\n\n1:47:54.400 --> 1:47:55.900\n conversation, right?\n\n1:47:55.900 --> 1:48:00.800\n As opposed to like, that's probably like the more sustainable relationship.\n\n1:48:00.800 --> 1:48:06.200\n It's like late at night, just, no, listen, listen, I know I told you an hour ago, that\n\n1:48:06.200 --> 1:48:09.720\n it's not a good idea, but just listen, things have changed.\n\n1:48:09.720 --> 1:48:17.000\n I can just imagine a bunch of stuff being made up just to convince, but I mean, I just\n\n1:48:17.000 --> 1:48:22.400\n think that there's opportunities that, I mean, maybe not locking down, but for our systems\n\n1:48:22.400 --> 1:48:32.880\n that are such a deep part of our lives, like we use a lot of us, a lot of people that commute\n\n1:48:32.880 --> 1:48:34.360\n use their car every single day.\n\n1:48:34.360 --> 1:48:38.240\n A lot of us use a refrigerator every single day, the microwave every single day.\n\n1:48:38.240 --> 1:48:47.600\n Like we just, like, I feel like certain things could be made more efficient, more enriching,\n\n1:48:47.600 --> 1:48:54.200\n and AI is there to help, like some just basic recognition of you as a human being, but your\n\n1:48:54.200 --> 1:48:57.520\n patterns of what makes you happy and not happy and all that kind of stuff.\n\n1:48:57.520 --> 1:48:58.520\n And the car, obviously.\n\n1:48:58.520 --> 1:49:05.320\n Maybe, maybe, maybe we'll say, wait, wait, wait, wait, instead of this, like, Ben and\n\n1:49:05.320 --> 1:49:09.440\n Jerry's ice cream, how about this hummus and carrots or something?\n\n1:49:09.440 --> 1:49:10.440\n I don't know.\n\n1:49:10.440 --> 1:49:14.960\n It would make it like a just in time recommendation, right?\n\n1:49:14.960 --> 1:49:21.240\n But not like a generic one, but a reminder that last time you chose the carrots, you\n\n1:49:21.240 --> 1:49:24.800\n smiled 17 times more the next day.\n\n1:49:24.800 --> 1:49:26.400\n You're happier the next day, right?\n\n1:49:26.400 --> 1:49:28.160\n You're happier the next day.\n\n1:49:28.160 --> 1:49:34.480\n And but yeah, I don't, but then again, if you're the kind of person that gets better\n\n1:49:34.480 --> 1:49:40.040\n from negative, negative comments, you could say like, hey, remember like that wedding\n\n1:49:40.040 --> 1:49:43.880\n you're going to, you want to fit into that dress?\n\n1:49:43.880 --> 1:49:44.880\n Remember about that?\n\n1:49:44.880 --> 1:49:48.760\n Let's think about that before you're eating this.\n\n1:49:48.760 --> 1:49:53.400\n It's for some, probably that would work for me, like a refrigerator that is just ruthless\n\n1:49:53.400 --> 1:49:54.920\n at shaming me.\n\n1:49:54.920 --> 1:49:59.600\n But like, I would, of course, welcome it, like that would work for me.\n\n1:49:59.600 --> 1:50:00.600\n Just that.\n\n1:50:00.600 --> 1:50:05.320\n So it would know, I think it would, if it's really like smart, it would optimize its nudging\n\n1:50:05.320 --> 1:50:07.280\n based on what works for you, right?\n\n1:50:07.280 --> 1:50:08.280\n Exactly.\n\n1:50:08.280 --> 1:50:09.280\n That's the whole point.\n\n1:50:09.280 --> 1:50:10.280\n Personalization.\n\n1:50:10.280 --> 1:50:11.920\n In every way, depersonalization.\n\n1:50:11.920 --> 1:50:18.120\n You were a part of a webinar titled Advancing Road Safety, the State of Alcohol Intoxication\n\n1:50:18.120 --> 1:50:19.600\n Research.\n\n1:50:19.600 --> 1:50:24.520\n So for people who don't know, every year 1.3 million people around the world die in road\n\n1:50:24.520 --> 1:50:31.320\n crashes and more than 20% of these fatalities are estimated to be alcohol related.\n\n1:50:31.320 --> 1:50:33.320\n A lot of them are also distraction related.\n\n1:50:33.320 --> 1:50:36.800\n So can AI help with the alcohol thing?\n\n1:50:36.800 --> 1:50:40.240\n I think the answer is yes.\n\n1:50:40.240 --> 1:50:46.560\n There are signals and we know that as humans, like we can tell when a person, you know,\n\n1:50:46.560 --> 1:50:51.200\n is at different phases of being drunk, right?\n\n1:50:51.200 --> 1:50:53.680\n And I think you can use technology to do the same.\n\n1:50:53.680 --> 1:50:58.640\n And again, I think the ultimate solution is going to be a combination of different sensors.\n\n1:50:58.640 --> 1:51:01.440\n How hard is the problem from the vision perspective?\n\n1:51:01.440 --> 1:51:02.880\n I think it's non trivial.\n\n1:51:02.880 --> 1:51:06.720\n I think it's non trivial and I think the biggest part is getting the data, right?\n\n1:51:06.720 --> 1:51:09.200\n It's like getting enough data examples.\n\n1:51:09.200 --> 1:51:15.240\n So we, for this research project, we partnered with the transportation authorities of Sweden\n\n1:51:15.240 --> 1:51:20.680\n and we literally had a racetrack with a safety driver and we basically progressively got\n\n1:51:20.680 --> 1:51:21.680\n people drunk.\n\n1:51:21.680 --> 1:51:22.680\n Nice.\n\n1:51:22.680 --> 1:51:29.280\n So, but, you know, that's a very expensive data set to collect and you want to collect\n\n1:51:29.280 --> 1:51:32.080\n it globally and in multiple conditions.\n\n1:51:32.080 --> 1:51:33.480\n Yeah.\n\n1:51:33.480 --> 1:51:38.800\n The ethics of collecting a data set where people are drunk is tricky, which is funny\n\n1:51:38.800 --> 1:51:43.400\n because I mean, let's put drunk driving aside.\n\n1:51:43.400 --> 1:51:47.120\n The number of drunk people in the world every day is very large.\n\n1:51:47.120 --> 1:51:50.320\n It'd be nice to have a large data set of drunk people getting progressively drunk.\n\n1:51:50.320 --> 1:51:54.600\n In fact, you could build an app where people can donate their data cause it's hilarious.\n\n1:51:54.600 --> 1:51:55.600\n Right.\n\n1:51:55.600 --> 1:51:56.600\n Actually, yeah.\n\n1:51:56.600 --> 1:51:57.600\n But the liability.\n\n1:51:57.600 --> 1:52:00.800\n Liability, the ethics, how do you get it right?\n\n1:52:00.800 --> 1:52:01.800\n It's tricky.\n\n1:52:01.800 --> 1:52:02.800\n It's really, really tricky.\n\n1:52:02.800 --> 1:52:07.440\n Cause like drinking is one of those things that's funny and hilarious and we're loves\n\n1:52:07.440 --> 1:52:10.240\n it's social, the so on and so forth.\n\n1:52:10.240 --> 1:52:13.520\n But it's also the thing that hurts a lot of people.\n\n1:52:13.520 --> 1:52:19.040\n Like a lot of people, like alcohol is one of those things it's legal, but it's really\n\n1:52:19.040 --> 1:52:21.200\n damaging to a lot of lives.\n\n1:52:21.200 --> 1:52:26.320\n It destroys lives and not just in the driving context.\n\n1:52:26.320 --> 1:52:32.160\n I should mention people should listen to Andrew Huberman who recently talked about alcohol.\n\n1:52:32.160 --> 1:52:33.160\n He has an amazing pocket.\n\n1:52:33.160 --> 1:52:37.920\n Andrew Huberman is a neuroscientist from Stanford and a good friend of mine.\n\n1:52:37.920 --> 1:52:43.560\n And he, he's like a human encyclopedia about all health related wisdom.\n\n1:52:43.560 --> 1:52:45.880\n So if there's a podcast, you would love it.\n\n1:52:45.880 --> 1:52:46.880\n I would love that.\n\n1:52:46.880 --> 1:52:47.880\n No, no, no, no, no.\n\n1:52:47.880 --> 1:52:49.600\n You don't know Andrew Huberman.\n\n1:52:49.600 --> 1:52:50.600\n Okay.\n\n1:52:50.600 --> 1:52:54.160\n Listen, you listen to Andrew, it's called Huberman Lab Podcast.\n\n1:52:54.160 --> 1:52:55.160\n This is your assignment.\n\n1:52:55.160 --> 1:52:56.160\n Just listen to one.\n\n1:52:56.160 --> 1:52:57.160\n Okay.\n\n1:52:57.160 --> 1:53:01.360\n I guarantee you this will be a thing where you say, Lex, this is the greatest human I\n\n1:53:01.360 --> 1:53:02.360\n have ever discovered.\n\n1:53:02.360 --> 1:53:03.360\n So.\n\n1:53:03.360 --> 1:53:04.360\n Oh my God.\n\n1:53:04.360 --> 1:53:08.120\n Cause I've really, I've, I'm really on a journey of kind of health and wellness and\n\n1:53:08.120 --> 1:53:13.240\n I'm learning lots and I'm trying to like build these, I guess, atomic habits around just\n\n1:53:13.240 --> 1:53:14.240\n being healthy.\n\n1:53:14.240 --> 1:53:17.200\n So I, yeah, I'm definitely going to do this.\n\n1:53:17.200 --> 1:53:21.960\n His whole thing, this is, this is, this is, this is great.\n\n1:53:21.960 --> 1:53:30.160\n He's a legit scientist, like really well published, but in his podcast, what he does, he's not,\n\n1:53:30.160 --> 1:53:31.920\n he's not talking about his own work.\n\n1:53:31.920 --> 1:53:34.640\n He's like a human encyclopedia of papers.\n\n1:53:34.640 --> 1:53:39.720\n And so he, his whole thing is he takes the topic and in a very fast, you mentioned atomic\n\n1:53:39.720 --> 1:53:46.220\n habits, like very clear way summarizes the research in a way that leads to protocols\n\n1:53:46.220 --> 1:53:47.400\n of what you should do.\n\n1:53:47.400 --> 1:53:52.600\n He's really big on like, not like this is what the science says, but like this is literally\n\n1:53:52.600 --> 1:53:54.280\n what you should be doing according to science.\n\n1:53:54.280 --> 1:54:01.360\n So like he's really big and there's a lot of recommendations he does which several of\n\n1:54:01.360 --> 1:54:08.880\n them I definitely don't do, like get some light as soon as possible from waking up and\n\n1:54:08.880 --> 1:54:11.040\n like for prolonged periods of time.\n\n1:54:11.040 --> 1:54:14.880\n That's a really big one and he's, there's a lot of science behind that one.\n\n1:54:14.880 --> 1:54:19.880\n There's a bunch of stuff that you're going to be like, Lex, this is a, this is my new\n\n1:54:19.880 --> 1:54:20.880\n favorite person.\n\n1:54:20.880 --> 1:54:21.880\n I guarantee it.\n\n1:54:21.880 --> 1:54:27.840\n And if you guys somehow don't know Andrew Huberman and you care about your wellbeing,\n\n1:54:27.840 --> 1:54:29.560\n you know, you should definitely listen to him.\n\n1:54:29.560 --> 1:54:31.920\n I love you, Andrew.\n\n1:54:31.920 --> 1:54:36.040\n Anyway, so what were we talking about?\n\n1:54:36.040 --> 1:54:39.480\n Oh, alcohol and detecting alcohol.\n\n1:54:39.480 --> 1:54:42.240\n So this is a problem you care about and you're trying to solve.\n\n1:54:42.240 --> 1:54:48.960\n And actually like broadening it, I do believe that the car is going to be a wellness center,\n\n1:54:48.960 --> 1:54:55.240\n like because again, imagine if you have a variety of sensors inside the vehicle, tracking\n\n1:54:55.240 --> 1:55:03.840\n not just your emotional state or level of distraction and drowsiness and intoxication,\n\n1:55:03.840 --> 1:55:09.440\n but also maybe even things like your, you know, your heart rate and your heart rate\n\n1:55:09.440 --> 1:55:13.960\n variability and your breathing rate.\n\n1:55:13.960 --> 1:55:19.520\n And it can start like optimizing, yeah, it can optimize the ride based on what your goals\n\n1:55:19.520 --> 1:55:20.520\n are.\n\n1:55:20.520 --> 1:55:24.040\n So I think we're going to start to see more of that and I'm excited about that.\n\n1:55:24.040 --> 1:55:25.040\n Yeah.\n\n1:55:25.040 --> 1:55:28.960\n What are the, what are the challenges you're tackling while with SmartEye currently?\n\n1:55:28.960 --> 1:55:34.640\n What's like the, the trickiest things to get, is it, is it basically convincing more and\n\n1:55:34.640 --> 1:55:41.160\n more car companies that having AI inside the car is a good idea or is there some, is there\n\n1:55:41.160 --> 1:55:45.360\n more technical algorithmic challenges?\n\n1:55:45.360 --> 1:55:47.700\n What's been keeping you mentally busy?\n\n1:55:47.700 --> 1:55:52.360\n I think a lot of the car companies we are in conversations with are already interested\n\n1:55:52.360 --> 1:55:54.160\n in definitely driver monitoring.\n\n1:55:54.160 --> 1:55:59.340\n Like I think it's becoming a must have, but even interior sensing, I can see like we're\n\n1:55:59.340 --> 1:56:04.040\n engaged in a lot of like advanced engineering projects and proof of concepts.\n\n1:56:04.040 --> 1:56:09.620\n I think technologically though, and that even the technology, I can see a path to making\n\n1:56:09.620 --> 1:56:10.620\n it happen.\n\n1:56:10.620 --> 1:56:11.620\n I think it's the use case.\n\n1:56:11.620 --> 1:56:16.360\n Like how does the car respond once it knows something about you?\n\n1:56:16.360 --> 1:56:20.880\n Because you want it to respond in a thoughtful way that doesn't, that isn't off putting to\n\n1:56:20.880 --> 1:56:23.240\n the consumer in the car.\n\n1:56:23.240 --> 1:56:25.640\n So I think that's like the user experience.\n\n1:56:25.640 --> 1:56:27.600\n I don't think we've really nailed that.\n\n1:56:27.600 --> 1:56:33.040\n And we usually, that's not part, we're the sensing platform, but we usually collaborate\n\n1:56:33.040 --> 1:56:35.960\n with the car manufacturer to decide what the use case is.\n\n1:56:35.960 --> 1:56:40.680\n So say you do, you figure out that somebody's angry while driving, okay, what should the\n\n1:56:40.680 --> 1:56:43.680\n car do?\n\n1:56:43.680 --> 1:56:50.100\n Do you see yourself as a role of nudging, of like basically coming up with solutions\n\n1:56:50.100 --> 1:56:56.360\n essentially that, and then the car manufacturers kind of put their own little spin on it?\n\n1:56:56.360 --> 1:56:57.360\n Right.\n\n1:56:57.360 --> 1:57:03.620\n So we, we are like the ideation, creative thought partner, but at the end of the day,\n\n1:57:03.620 --> 1:57:06.640\n the car company needs to decide what's on brand for them, right?\n\n1:57:06.640 --> 1:57:11.720\n Like maybe when it figures out that you're distracted or drowsy, it shows you a coffee\n\n1:57:11.720 --> 1:57:12.720\n cup, right?\n\n1:57:12.720 --> 1:57:16.640\n Or maybe it takes more aggressive behaviors and basically said, okay, if you don't like\n\n1:57:16.640 --> 1:57:19.640\n take a rest in the next five minutes, the car's going to shut down, right?\n\n1:57:19.640 --> 1:57:25.400\n Like there's a whole range of actions the car can take and doing the thing that is most,\n\n1:57:25.400 --> 1:57:29.320\n yeah, that builds trust with the driver and the passengers.\n\n1:57:29.320 --> 1:57:32.840\n I think that's what we need to be very careful about.\n\n1:57:32.840 --> 1:57:33.840\n Yeah.\n\n1:57:33.840 --> 1:57:38.600\n Car companies are funny cause they have their own, like, I mean, that's why people get cars\n\n1:57:38.600 --> 1:57:39.600\n still.\n\n1:57:39.600 --> 1:57:44.240\n I hope that changes, but they get it cause it's a certain feel and look and it's a certain,\n\n1:57:44.240 --> 1:57:51.840\n they become proud, like Mercedes Benz or BMW or whatever, and that's their thing.\n\n1:57:51.840 --> 1:57:56.400\n That's the family brand or something like that, or Ford or GM, whatever, they stick\n\n1:57:56.400 --> 1:57:57.400\n to that thing.\n\n1:57:57.400 --> 1:57:58.400\n Yeah.\n\n1:57:58.400 --> 1:57:59.400\n It's interesting.\n\n1:57:59.400 --> 1:58:04.160\n It's like, it should be, I don't know, it should be a little more about the technology\n\n1:58:04.160 --> 1:58:06.800\n inside.\n\n1:58:06.800 --> 1:58:12.440\n And I suppose there too, there could be a branding, like a very specific style of luxury\n\n1:58:12.440 --> 1:58:13.440\n or fun.\n\n1:58:13.440 --> 1:58:14.440\n Right.\n\n1:58:14.440 --> 1:58:15.440\n Right.\n\n1:58:15.440 --> 1:58:16.440\n All that kind of stuff.\n\n1:58:16.440 --> 1:58:17.440\n Yeah.\n\n1:58:17.440 --> 1:58:22.720\n And I have an AI focused fund to invest in early stage kind of AI driven companies.\n\n1:58:22.720 --> 1:58:27.560\n And one of the companies we're looking at is trying to do what Tesla did, but for boats,\n\n1:58:27.560 --> 1:58:28.760\n for recreational boats.\n\n1:58:28.760 --> 1:58:29.760\n Yeah.\n\n1:58:29.760 --> 1:58:34.840\n So they're building an electric and kind of slash autonomous boat and it's kind of the\n\n1:58:34.840 --> 1:58:35.840\n same issues.\n\n1:58:35.840 --> 1:58:38.600\n Like what kind of sensors can you put in?\n\n1:58:38.600 --> 1:58:43.320\n What kind of states can you detect both exterior and interior within the boat?\n\n1:58:43.320 --> 1:58:45.480\n Anyways, it's like really interesting.\n\n1:58:45.480 --> 1:58:46.760\n Do you boat at all?\n\n1:58:46.760 --> 1:58:49.960\n No, not well, not in that way.\n\n1:58:49.960 --> 1:58:57.400\n I do like to get on the lake or a river and fish from a boat, but that's not boating.\n\n1:58:57.400 --> 1:58:58.400\n That's the difference.\n\n1:58:58.400 --> 1:58:59.400\n That's the difference.\n\n1:58:59.400 --> 1:59:00.400\n Still boating.\n\n1:59:00.400 --> 1:59:01.400\n Low tech.\n\n1:59:01.400 --> 1:59:02.400\n A low tech boat.\n\n1:59:02.400 --> 1:59:04.400\n Get away from, get closer to nature boat.\n\n1:59:04.400 --> 1:59:12.200\n I guess going out into the ocean is also getting closer to nature in some deep sense.\n\n1:59:12.200 --> 1:59:15.800\n I mean, I guess that's why people love it.\n\n1:59:15.800 --> 1:59:18.920\n The enormity of the water just underneath you.\n\n1:59:18.920 --> 1:59:19.920\n Yeah.\n\n1:59:19.920 --> 1:59:20.920\n I love the water.\n\n1:59:20.920 --> 1:59:22.800\n I love the, I love both.\n\n1:59:22.800 --> 1:59:23.800\n I love salt water.\n\n1:59:23.800 --> 1:59:28.420\n It was like the big and just, it's humbling to be in front of this giant thing that's\n\n1:59:28.420 --> 1:59:31.680\n so powerful that was here before us and be here after.\n\n1:59:31.680 --> 1:59:37.480\n But I also love the piece of a small like wooded lake and it's just, it's everything's\n\n1:59:37.480 --> 1:59:38.480\n calm.\n\n1:59:38.480 --> 1:59:39.480\n Therapeutic.\n\n1:59:39.480 --> 1:59:49.600\n You tweeted that I'm excited about Amazon's acquisition of iRobot.\n\n1:59:49.600 --> 1:59:54.480\n I think it's a super interesting, just given the trajectory of what you're part of, of\n\n1:59:54.480 --> 2:00:00.040\n these honestly small number of companies that are playing in this space that are like trying\n\n2:00:00.040 --> 2:00:02.180\n to have an impact on human beings.\n\n2:00:02.180 --> 2:00:09.200\n So the, it is an interesting moment in time that Amazon would acquire iRobot.\n\n2:00:09.200 --> 2:00:16.320\n You tweet, I imagine a future where home robots are as ubiquitous as microwaves or toasters.\n\n2:00:16.320 --> 2:00:18.920\n Here are three reasons why I think this is exciting.\n\n2:00:18.920 --> 2:00:23.240\n If you remember, I can look it up, but what, why is this exciting to you?\n\n2:00:23.240 --> 2:00:27.320\n I mean, I think the first reason why this is exciting, I kind of remember the exact\n\n2:00:27.320 --> 2:00:33.540\n like order in which I put them, but one is just, it's, it's going to be an incredible\n\n2:00:33.540 --> 2:00:37.640\n platform for understanding our behaviors within the home, right?\n\n2:00:37.640 --> 2:00:42.880\n Like you know, if you think about Roomba, which is, you know, the robot vacuum cleaner,\n\n2:00:42.880 --> 2:00:48.640\n the flagship product of iRobot at the moment, it's like running around your home, understanding\n\n2:00:48.640 --> 2:00:51.200\n the layout, it's understanding what's clean and what's not.\n\n2:00:51.200 --> 2:00:52.640\n How often do you clean your house?\n\n2:00:52.640 --> 2:00:57.500\n And all of these like behaviors are a piece of the puzzle in terms of understanding who\n\n2:00:57.500 --> 2:00:58.760\n you are as a consumer.\n\n2:00:58.760 --> 2:01:05.580\n And I think that could be, again, used in really meaningful ways, not just to recommend\n\n2:01:05.580 --> 2:01:09.640\n better products or whatever, but actually to improve your experience as a human being.\n\n2:01:09.640 --> 2:01:12.900\n So I think, I think that's very interesting.\n\n2:01:12.900 --> 2:01:18.480\n I think the natural evolution of these robots in the, in the home.\n\n2:01:18.480 --> 2:01:24.280\n So it's, it's interesting, Roomba isn't really a social robot, right, at the moment.\n\n2:01:24.280 --> 2:01:29.160\n But I once interviewed one of the chief engineers on the Roomba team, and he talked about how\n\n2:01:29.160 --> 2:01:31.400\n people named their Roombas.\n\n2:01:31.400 --> 2:01:36.520\n And if the Roomba broke down, they would call in and say, you know, my Roomba broke down\n\n2:01:36.520 --> 2:01:38.920\n and the company would say, well, we'll just send you a new one.\n\n2:01:38.920 --> 2:01:45.680\n And no, no, no, Rosie, like you have to like, yeah, I want you to fix this particular robot.\n\n2:01:45.680 --> 2:01:51.680\n So people have already built like interesting emotional connections with these home robots.\n\n2:01:51.680 --> 2:01:57.320\n And I think that, again, that provides a platform for really interesting things to, to just\n\n2:01:57.320 --> 2:01:58.320\n motivate change.\n\n2:01:58.320 --> 2:01:59.320\n Like it could help you.\n\n2:01:59.320 --> 2:02:05.740\n I mean, one of the companies that spun out of MIT, Catalia Health, the guy who started\n\n2:02:05.740 --> 2:02:09.640\n it spent a lot of time building robots that help with weight management.\n\n2:02:09.640 --> 2:02:14.320\n So weight management, sleep, eating better, yeah, all of these things.\n\n2:02:14.320 --> 2:02:20.280\n Well, if I'm being honest, Amazon does not exactly have a track record of winning over\n\n2:02:20.280 --> 2:02:22.240\n people in terms of trust.\n\n2:02:22.240 --> 2:02:27.840\n Now that said, it's a really difficult problem for a human being to let a robot in their\n\n2:02:27.840 --> 2:02:30.680\n home that has a camera on it.\n\n2:02:30.680 --> 2:02:31.680\n Right.\n\n2:02:31.680 --> 2:02:33.400\n That's really, really, really tough.\n\n2:02:33.400 --> 2:02:40.480\n And I think Roomba actually, I have to think about this, but I'm pretty sure now or for\n\n2:02:40.480 --> 2:02:46.040\n some time already has had cameras because they're doing the, the, the most recent Roomba.\n\n2:02:46.040 --> 2:02:47.040\n I have so many Roombas.\n\n2:02:47.040 --> 2:02:48.040\n Oh, you actually do?\n\n2:02:48.040 --> 2:02:49.560\n Well, I programmed it.\n\n2:02:49.560 --> 2:02:51.440\n I don't use a Roomba for VECO.\n\n2:02:51.440 --> 2:02:54.280\n People that have been to my place, they're like, yeah, you definitely don't use these\n\n2:02:54.280 --> 2:02:55.280\n Roombas.\n\n2:02:55.280 --> 2:03:00.920\n That could be a good, I can't tell like the valence of this comment.\n\n2:03:00.920 --> 2:03:02.400\n Was it a compliment or like?\n\n2:03:02.400 --> 2:03:05.320\n No, it's a giant, it's just a bunch of electronics everywhere.\n\n2:03:05.320 --> 2:03:11.160\n There's, I have six or seven computers, I have robots everywhere, Lego robots, I have\n\n2:03:11.160 --> 2:03:20.240\n small robots and big robots and it's just giant, just piles of robot stuff and yeah.\n\n2:03:20.240 --> 2:03:25.560\n But including the Roombas, they're, they're, they're being used for their body and intelligence,\n\n2:03:25.560 --> 2:03:26.720\n but not for their purpose.\n\n2:03:26.720 --> 2:03:33.300\n I have, I've changed them, repurposed them for other purposes, for deeper, more meaningful\n\n2:03:33.300 --> 2:03:39.240\n purposes than just like the Bota Roba, which is, you know, brings a lot of people happiness,\n\n2:03:39.240 --> 2:03:41.060\n I'm sure.\n\n2:03:41.060 --> 2:03:46.560\n They have a camera because the thing they advertised, I had my own camera still, but\n\n2:03:46.560 --> 2:03:52.320\n the, the, the camera on the new Roomba, they have like state of the art poop detection\n\n2:03:52.320 --> 2:03:56.760\n as they advertised, which is a very difficult, apparently it's a big problem for, for vacuum\n\n2:03:56.760 --> 2:04:01.000\n cleaners is, you know, if they go over like dog poop, it just runs it, it runs it over\n\n2:04:01.000 --> 2:04:02.140\n and creates a giant mess.\n\n2:04:02.140 --> 2:04:08.360\n So they have like, and apparently they collected like a huge amount of data and different shapes\n\n2:04:08.360 --> 2:04:12.400\n and looks and whatever of poop and then now they're able to avoid it and so on.\n\n2:04:12.400 --> 2:04:14.440\n They're very proud of this.\n\n2:04:14.440 --> 2:04:19.200\n So there is a camera, but you don't think of it as having a camera.\n\n2:04:19.200 --> 2:04:20.380\n Yeah.\n\n2:04:20.380 --> 2:04:24.600\n You don't think of it as having a camera because you've grown to trust that, I guess, because\n\n2:04:24.600 --> 2:04:31.600\n our phones, at least most of us seem to trust this phone, even though there's a camera looking\n\n2:04:31.600 --> 2:04:33.960\n directly at you.\n\n2:04:33.960 --> 2:04:41.680\n I think that if you trust that the company is taking security very seriously, I actually\n\n2:04:41.680 --> 2:04:46.760\n don't know how that trust was earned with smartphones, I think it just started to provide\n\n2:04:46.760 --> 2:04:51.520\n a lot of positive value to your life where you just took it in and then the company over\n\n2:04:51.520 --> 2:04:55.200\n time has shown that it takes privacy very seriously, that kind of stuff.\n\n2:04:55.200 --> 2:05:01.520\n But I just, Amazon is not always in the, in its social robots communicated.\n\n2:05:01.520 --> 2:05:07.080\n This is a trustworthy thing, both in terms of culture and competence, because I think\n\n2:05:07.080 --> 2:05:12.620\n privacy is not just about what do you intend to do, but also how well, how good are you\n\n2:05:12.620 --> 2:05:14.600\n at doing that kind of thing.\n\n2:05:14.600 --> 2:05:16.800\n So that's a really hard problem to solve.\n\n2:05:16.800 --> 2:05:22.640\n But I mean, but a lot of us have Alexas at home and I mean, Alexa could be listening\n\n2:05:22.640 --> 2:05:24.520\n in the whole time, right?\n\n2:05:24.520 --> 2:05:27.440\n And doing all sorts of nefarious things with the data.\n\n2:05:27.440 --> 2:05:28.440\n Yeah.\n\n2:05:28.440 --> 2:05:32.320\n Hopefully it's not, but I don't think it is.\n\n2:05:32.320 --> 2:05:36.640\n But you know, Amazon is not, it's such a tricky thing for a company to get right, which\n\n2:05:36.640 --> 2:05:38.200\n is like to earn the trust.\n\n2:05:38.200 --> 2:05:41.520\n I don't think Alexa's earned people's trust quite yet.\n\n2:05:41.520 --> 2:05:42.520\n Yeah.\n\n2:05:42.520 --> 2:05:44.640\n I think it's, it's not there quite yet.\n\n2:05:44.640 --> 2:05:45.640\n I agree.\n\n2:05:45.640 --> 2:05:46.640\n They struggle with this kind of stuff.\n\n2:05:46.640 --> 2:05:50.240\n In fact, when these topics are brought up, people are always get like nervous.\n\n2:05:50.240 --> 2:05:57.560\n And I think if you get nervous about it, that mean that like the way to earn people's trust\n\n2:05:57.560 --> 2:06:00.680\n is not by like, Ooh, don't talk about this.\n\n2:06:00.680 --> 2:06:05.920\n It's just be open, be frank, be transparent, and also create a culture of like where it\n\n2:06:05.920 --> 2:06:17.120\n radiates at every level from engineer to CEO that like you're good people that have a common\n\n2:06:17.120 --> 2:06:23.040\n sense idea of what it means to respect basic human rights and the privacy of people and\n\n2:06:23.040 --> 2:06:24.040\n all that kind of stuff.\n\n2:06:24.040 --> 2:06:30.640\n And I think that propagates throughout the, that's the best PR, which is like over time\n\n2:06:30.640 --> 2:06:34.920\n you understand that these are good folks doing good things.\n\n2:06:34.920 --> 2:06:42.240\n Anyway, speaking of social robots, have you heard about Tesla, Tesla bot, the humanoid\n\n2:06:42.240 --> 2:06:43.240\n robot?\n\n2:06:43.240 --> 2:06:44.240\n Yes, I have.\n\n2:06:44.240 --> 2:06:45.240\n Yes, yes, yes.\n\n2:06:45.240 --> 2:06:48.680\n But I don't exactly know what it's designed to do to you.\n\n2:06:48.680 --> 2:06:49.680\n You probably do.\n\n2:06:49.680 --> 2:06:54.960\n No, I know it's designed to do, but I have a different perspective on it, but it's designed\n\n2:06:54.960 --> 2:07:02.040\n to, it's a humanoid form and it's designed to, for automation tasks in the same way that\n\n2:07:02.040 --> 2:07:06.260\n industrial robot arms automate tasks in the factory.\n\n2:07:06.260 --> 2:07:08.280\n So it's designed to automate tasks in the factory.\n\n2:07:08.280 --> 2:07:18.040\n But I think that humanoid form, as we were talking about before, is one that we connect\n\n2:07:18.040 --> 2:07:19.800\n with as human beings.\n\n2:07:19.800 --> 2:07:25.200\n Anything legged, obviously, but the humanoid form especially, we anthropomorphize it most\n\n2:07:25.200 --> 2:07:26.200\n intensely.\n\n2:07:26.200 --> 2:07:34.440\n And so the possibility to me, it's exciting to see both Atlas developed by Boston Dynamics\n\n2:07:34.440 --> 2:07:43.720\n and anyone, including Tesla, trying to make humanoid robots cheaper and more effective.\n\n2:07:43.720 --> 2:07:51.140\n The obvious way it transforms the world is social robotics to me versus automation of\n\n2:07:51.140 --> 2:07:53.120\n tasks in the factory.\n\n2:07:53.120 --> 2:07:58.840\n So yeah, I just wanted, in case that was something you were interested in, because I find its\n\n2:07:58.840 --> 2:08:01.600\n application of social robotics super interesting.\n\n2:08:01.600 --> 2:08:06.320\n We did a lot of work with Pepper, Pepper the robot, a while back.\n\n2:08:06.320 --> 2:08:11.480\n We were like the emotion engine for Pepper, which is Softbank's humanoid robot.\n\n2:08:11.480 --> 2:08:12.480\n How tall is Pepper?\n\n2:08:12.480 --> 2:08:13.480\n It's like...\n\n2:08:13.480 --> 2:08:18.360\n Yeah, like, I don't know, like five foot maybe, right?\n\n2:08:18.360 --> 2:08:19.360\n Yeah.\n\n2:08:19.360 --> 2:08:20.360\n Yeah.\n\n2:08:20.360 --> 2:08:21.360\n Pretty, pretty big.\n\n2:08:21.360 --> 2:08:22.360\n Pretty big.\n\n2:08:22.360 --> 2:08:28.920\n It's designed to be at like airport lounges and, you know, retail stores, mostly customer\n\n2:08:28.920 --> 2:08:30.680\n service, right?\n\n2:08:30.680 --> 2:08:37.200\n Hotel lobbies, and I mean, I don't know where the state of the robot is, but I think it's\n\n2:08:37.200 --> 2:08:38.200\n very promising.\n\n2:08:38.200 --> 2:08:40.400\n I think there are a lot of applications where this can be helpful.\n\n2:08:40.400 --> 2:08:45.200\n I'm also really interested in, yeah, social robotics for the home, right?\n\n2:08:45.200 --> 2:08:50.880\n Like that can help elderly people, for example, transport things from one location of the\n\n2:08:50.880 --> 2:08:55.520\n mind to the other, or even like just have your back in case something happens.\n\n2:08:55.520 --> 2:08:58.000\n Yeah, I don't know.\n\n2:08:58.000 --> 2:08:59.840\n I do think it's a very interesting space.\n\n2:08:59.840 --> 2:09:00.840\n It seems early though.\n\n2:09:00.840 --> 2:09:04.960\n Do you feel like the timing is now?\n\n2:09:04.960 --> 2:09:09.840\n Yes, 100%.\n\n2:09:09.840 --> 2:09:12.160\n So it always seems early until it's not, right?\n\n2:09:12.160 --> 2:09:13.160\n Right, right, right.\n\n2:09:13.160 --> 2:09:24.240\n I think the time, I definitely think that the time is now, like this decade for social\n\n2:09:24.240 --> 2:09:25.920\n robots.\n\n2:09:25.920 --> 2:09:29.640\n Whether the humanoid form is right, I don't think so, no.\n\n2:09:29.640 --> 2:09:40.000\n I don't, I think the, like if we just look at Jibo as an example, I feel like most of\n\n2:09:40.000 --> 2:09:46.680\n the problem, the challenge, the opportunity of social connection between an AI system\n\n2:09:46.680 --> 2:09:52.720\n and a human being does not require you to also solve the problem of robot manipulation\n\n2:09:52.720 --> 2:09:55.320\n and bipedal mobility.\n\n2:09:55.320 --> 2:09:59.980\n So I think you could do that with just a screen, honestly, but there's something about the\n\n2:09:59.980 --> 2:10:03.880\n interface of Jibo where it can rotate and so on that's also compelling.\n\n2:10:03.880 --> 2:10:09.400\n But you get to see all these robot companies that fail, incredible companies like Jibo\n\n2:10:09.400 --> 2:10:17.600\n and even, I mean, the iRobot in some sense is a big success story that it was able to\n\n2:10:17.600 --> 2:10:24.000\n find a niche thing and focus on it, but in some sense it's not a success story because\n\n2:10:24.000 --> 2:10:30.960\n they didn't build any other robot, like any other, it didn't expand into all kinds of\n\n2:10:30.960 --> 2:10:31.960\n robotics.\n\n2:10:31.960 --> 2:10:34.880\n Like once you're in the home, maybe that's what happens with Amazon is they'll flourish\n\n2:10:34.880 --> 2:10:37.200\n into all kinds of other robots.\n\n2:10:37.200 --> 2:10:43.760\n But do you have a sense, by the way, why it's so difficult to build a robotics company?\n\n2:10:43.760 --> 2:10:47.080\n Like why so many companies have failed?\n\n2:10:47.080 --> 2:10:50.780\n I think it's like you're building a vertical stack, right?\n\n2:10:50.780 --> 2:10:54.480\n Like you are building the hardware plus the software and you find you have to do this\n\n2:10:54.480 --> 2:10:56.040\n at a cost that makes sense.\n\n2:10:56.040 --> 2:11:05.080\n So I think Jibo was retailing at like, I don't know, like $800, like $700, $800, which for\n\n2:11:05.080 --> 2:11:10.020\n the use case, right, there's a dissonance there.\n\n2:11:10.020 --> 2:11:11.020\n It's too high.\n\n2:11:11.020 --> 2:11:20.380\n So I think cost of building the whole platform in a way that is affordable for what value\n\n2:11:20.380 --> 2:11:23.720\n it's bringing, I think that's a challenge.\n\n2:11:23.720 --> 2:11:30.920\n I think for these home robots that are going to help you do stuff around the home, that's\n\n2:11:30.920 --> 2:11:33.400\n a challenge too, like the mobility piece of it.\n\n2:11:33.400 --> 2:11:34.400\n That's hard.\n\n2:11:34.400 --> 2:11:40.480\n Well, one of the things I'm really excited with Tesla Bot is the people working on it.\n\n2:11:40.480 --> 2:11:44.560\n And that's probably the criticism I would apply to some of the other folks who worked\n\n2:11:44.560 --> 2:11:50.200\n on social robots is the people working on Tesla Bot know how to, they're focused on\n\n2:11:50.200 --> 2:11:54.360\n and know how to do mass manufacture and create a product that's super cheap.\n\n2:11:54.360 --> 2:11:55.360\n Very cool.\n\n2:11:55.360 --> 2:11:56.360\n That's the focus.\n\n2:11:56.360 --> 2:12:00.480\n The engineering focus isn't, I would say that you can also criticize them for that, is they're\n\n2:12:00.480 --> 2:12:03.920\n not focused on the experience of the robot.\n\n2:12:03.920 --> 2:12:09.920\n They're focused on how to get this thing to do the basic stuff that the humanoid form\n\n2:12:09.920 --> 2:12:13.560\n requires to do it as cheap as possible.\n\n2:12:13.560 --> 2:12:18.360\n Then the fewest number of actuators, the fewest numbers of motors, the increasing efficiency,\n\n2:12:18.360 --> 2:12:20.400\n they decrease the weight, all that kind of stuff.\n\n2:12:20.400 --> 2:12:21.600\n So that's really interesting.\n\n2:12:21.600 --> 2:12:26.520\n I would say that Jibo and all those folks, they focus on the design, the experience,\n\n2:12:26.520 --> 2:12:29.840\n all of that, and it's secondary how to manufacture.\n\n2:12:29.840 --> 2:12:30.840\n Right.\n\n2:12:30.840 --> 2:12:36.880\n So you have to think like the Tesla Bot folks from first principles, what is the fewest\n\n2:12:36.880 --> 2:12:41.720\n number of components, the cheapest components, how can I build it as much in house as possible\n\n2:12:41.720 --> 2:12:47.680\n without having to consider all the complexities of a supply chain, all that kind of stuff.\n\n2:12:47.680 --> 2:12:48.680\n It's interesting.\n\n2:12:48.680 --> 2:12:54.200\n Because if you have to build a robotics company, you're not building one robot, you're building\n\n2:12:54.200 --> 2:12:58.600\n hopefully millions of robots, you have to figure out how to do that where the final\n\n2:12:58.600 --> 2:13:04.240\n thing, I mean, if it's Jibo type of robot, is there a reason why Jibo, like we can have\n\n2:13:04.240 --> 2:13:08.880\n this lengthy discussion, is there a reason why Jibo has to be over $100?\n\n2:13:08.880 --> 2:13:09.880\n It shouldn't be.\n\n2:13:09.880 --> 2:13:10.880\n Right.\n\n2:13:10.880 --> 2:13:11.880\n Like the basic components.\n\n2:13:11.880 --> 2:13:12.880\n Right.\n\n2:13:12.880 --> 2:13:13.880\n Components of it.\n\n2:13:13.880 --> 2:13:14.880\n Right.\n\n2:13:14.880 --> 2:13:19.080\n Like you could start to actually discuss like, okay, what is the essential thing about Jibo?\n\n2:13:19.080 --> 2:13:21.440\n How much, what is the cheapest way I can have a screen?\n\n2:13:21.440 --> 2:13:23.760\n What's the cheapest way I can have a rotating base?\n\n2:13:23.760 --> 2:13:24.760\n Right.\n\n2:13:24.760 --> 2:13:25.760\n All that kind of stuff.\n\n2:13:25.760 --> 2:13:29.960\n Right, get down, continuously drive down costs.\n\n2:13:29.960 --> 2:13:35.520\n Speaking of which, you have launched an extremely successful companies, you have helped others,\n\n2:13:35.520 --> 2:13:37.920\n you've invested in companies.\n\n2:13:37.920 --> 2:13:44.160\n Can you give advice on how to start a successful company?\n\n2:13:44.160 --> 2:13:48.780\n I would say have a problem that you really, really, really want to solve, right?\n\n2:13:48.780 --> 2:13:53.800\n Something that you're deeply passionate about.\n\n2:13:53.800 --> 2:13:55.880\n And honestly, take the first step.\n\n2:13:55.880 --> 2:13:58.520\n Like that's often the hardest.\n\n2:13:58.520 --> 2:13:59.520\n And don't overthink it.\n\n2:13:59.520 --> 2:14:04.000\n Like, you know, like this idea of a minimum viable product or a minimum viable version\n\n2:14:04.000 --> 2:14:05.000\n of an idea, right?\n\n2:14:05.000 --> 2:14:09.160\n Like, yes, you're thinking about this, like a humongous, like super elegant, super beautiful\n\n2:14:09.160 --> 2:14:10.160\n thing.\n\n2:14:10.160 --> 2:14:14.640\n What, like reduce it to the littlest thing you can bring to market that can solve a problem\n\n2:14:14.640 --> 2:14:20.880\n or that can, you know, that can help address a pain point that somebody has.\n\n2:14:20.880 --> 2:14:24.320\n They often tell you, like, start with a customer of one, right?\n\n2:14:24.320 --> 2:14:28.400\n If you can solve a problem for one person, then there's probably going to be yourself\n\n2:14:28.400 --> 2:14:29.400\n or some other person.\n\n2:14:29.400 --> 2:14:30.400\n Right.\n\n2:14:30.400 --> 2:14:31.400\n Pick a person.\n\n2:14:31.400 --> 2:14:32.400\n Exactly.\n\n2:14:32.400 --> 2:14:33.400\n It could be you.\n\n2:14:33.400 --> 2:14:37.240\n Yeah, that's actually often a good sign that if you enjoy a thing, enjoy a thing where\n\n2:14:37.240 --> 2:14:41.000\n you have a specific problem that you'd like to solve, that's a good, that's a good end\n\n2:14:41.000 --> 2:14:43.600\n of one to focus on.\n\n2:14:43.600 --> 2:14:49.360\n What else, what else is there to actually step one is the hardest, but there's other\n\n2:14:49.360 --> 2:14:51.200\n steps as well, right?\n\n2:14:51.200 --> 2:14:58.080\n I also think like who you bring around the table early on is so key, right?\n\n2:14:58.080 --> 2:15:02.440\n Like being clear on, on what I call like your core values or your North Star.\n\n2:15:02.440 --> 2:15:04.840\n It might sound fluffy, but actually it's not.\n\n2:15:04.840 --> 2:15:08.840\n So and Roz and I feel like we did that very early on.\n\n2:15:08.840 --> 2:15:13.040\n We sat around her kitchen table and we said, okay, there's so many applications of this\n\n2:15:13.040 --> 2:15:14.040\n technology.\n\n2:15:14.040 --> 2:15:15.040\n How are we going to draw the line?\n\n2:15:15.040 --> 2:15:16.940\n How are we going to set boundaries?\n\n2:15:16.940 --> 2:15:22.680\n We came up with a set of core values that in the hardest of times we fell back on to\n\n2:15:22.680 --> 2:15:25.320\n determine how we make decisions.\n\n2:15:25.320 --> 2:15:28.760\n And so I feel like just getting clarity on these core, like for us, it was respecting\n\n2:15:28.760 --> 2:15:33.400\n people's privacy, only engaging with industries where it's clear opt in.\n\n2:15:33.400 --> 2:15:38.680\n So for instance, we don't do any work in security and surveillance.\n\n2:15:38.680 --> 2:15:42.480\n So things like that, just getting, we very big on, you know, one of our core values is\n\n2:15:42.480 --> 2:15:44.720\n human connection and empathy, right?\n\n2:15:44.720 --> 2:15:47.840\n And that is, yes, it's an AI company, but it's about people.\n\n2:15:47.840 --> 2:15:54.520\n Well, these are all, they become encoded in how we act, even if you're a small, tiny team\n\n2:15:54.520 --> 2:15:57.460\n of two or three or whatever.\n\n2:15:57.460 --> 2:15:59.520\n So I think that's another piece of advice.\n\n2:15:59.520 --> 2:16:02.680\n So what about finding people, hiring people?\n\n2:16:02.680 --> 2:16:07.800\n If you care about people as much as you do, like this, it seems like such a difficult\n\n2:16:07.800 --> 2:16:10.680\n thing to hire the right people.\n\n2:16:10.680 --> 2:16:16.120\n I think early on as a startup, you want people who have, who share the passion and the conviction\n\n2:16:16.120 --> 2:16:17.880\n because it's going to be tough.\n\n2:16:17.880 --> 2:16:25.000\n Like I've yet to meet a startup where it was just a straight line to success, right?\n\n2:16:25.000 --> 2:16:28.280\n Even not just startup, like even everyday people's lives, right?\n\n2:16:28.280 --> 2:16:36.280\n You always like run into obstacles and you run into naysayers and you need people who\n\n2:16:36.280 --> 2:16:40.600\n are believers, whether they're people on your team or even your investors.\n\n2:16:40.600 --> 2:16:44.960\n You need investors who are really believers in what you're doing, because that means they\n\n2:16:44.960 --> 2:16:47.040\n will stick with you.\n\n2:16:47.040 --> 2:16:49.280\n They won't give up at the first obstacle.\n\n2:16:49.280 --> 2:16:50.920\n I think that's important.\n\n2:16:50.920 --> 2:16:51.920\n What about raising money?\n\n2:16:51.920 --> 2:16:59.720\n What about finding investors, first of all, raising money, but also raising money from\n\n2:16:59.720 --> 2:17:05.960\n the right sources from that ultimately don't hinder you, but help you, empower you, all\n\n2:17:05.960 --> 2:17:06.960\n that kind of stuff.\n\n2:17:06.960 --> 2:17:08.600\n What advice would you give there?\n\n2:17:08.600 --> 2:17:12.120\n You successfully raised money many times in your life.\n\n2:17:12.120 --> 2:17:13.120\n Yeah.\n\n2:17:13.120 --> 2:17:15.080\n Again, it's not just about the money.\n\n2:17:15.080 --> 2:17:20.360\n It's about finding the right investors who are going to be aligned in terms of what you\n\n2:17:20.360 --> 2:17:23.160\n want to build and believe in your core values.\n\n2:17:23.160 --> 2:17:31.280\n For example, especially later on, in my latest round of funding, I try to bring in investors\n\n2:17:31.280 --> 2:17:40.120\n that really care about the ethics of AI and the alignment of vision and mission and core\n\n2:17:40.120 --> 2:17:41.120\n values is really important.\n\n2:17:41.120 --> 2:17:43.920\n It's like you're picking a life partner.\n\n2:17:43.920 --> 2:17:45.160\n It's the same kind of...\n\n2:17:45.160 --> 2:17:47.560\n So you take it that seriously for investors?\n\n2:17:47.560 --> 2:17:50.040\n Yeah, because they're going to have to stick with you.\n\n2:17:50.040 --> 2:17:51.480\n You're stuck together.\n\n2:17:51.480 --> 2:17:52.480\n For a while anyway.\n\n2:17:52.480 --> 2:17:53.480\n Yeah.\n\n2:17:53.480 --> 2:17:56.880\n Maybe not for life, but for a while, for sure.\n\n2:17:56.880 --> 2:17:57.880\n For better or worse.\n\n2:17:57.880 --> 2:17:59.920\n I forget what the vowels usually sound like.\n\n2:17:59.920 --> 2:18:00.920\n For better or worse?\n\n2:18:00.920 --> 2:18:01.920\n Through something.\n\n2:18:01.920 --> 2:18:02.920\n Yeah.\n\n2:18:02.920 --> 2:18:03.920\n Oh boy.\n\n2:18:03.920 --> 2:18:04.920\n Yeah.\n\n2:18:04.920 --> 2:18:15.320\n Anyway, it's romantic and deep and you're in it for a while.\n\n2:18:15.320 --> 2:18:18.040\n So it's not just about the money.\n\n2:18:18.040 --> 2:18:23.560\n You tweeted about going to your first capital camp investing get together and that you learned\n\n2:18:23.560 --> 2:18:24.560\n a lot.\n\n2:18:24.560 --> 2:18:27.840\n So this is about investing.\n\n2:18:27.840 --> 2:18:30.240\n So what have you learned from that?\n\n2:18:30.240 --> 2:18:34.160\n What have you learned about investing in general from both because you've been on both ends\n\n2:18:34.160 --> 2:18:35.160\n of it?\n\n2:18:35.160 --> 2:18:41.720\n I mean, I try to use my experience as an operator now with my investor hat on when I'm identifying\n\n2:18:41.720 --> 2:18:45.280\n companies to invest in.\n\n2:18:45.280 --> 2:18:49.460\n First of all, I think the good news is because I have a technology background and I really\n\n2:18:49.460 --> 2:18:54.600\n understand machine learning and computer vision and AI, et cetera, I can apply that level\n\n2:18:54.600 --> 2:18:59.720\n of understanding because everybody says they're an AI company or they're an AI tech.\n\n2:18:59.720 --> 2:19:02.880\n And I'm like, no, no, no, no, no, show me the technology.\n\n2:19:02.880 --> 2:19:07.640\n So I can do that level of diligence, which I actually love.\n\n2:19:07.640 --> 2:19:12.760\n And then I have to do the litmus test of, if I'm in a conversation with you, am I excited\n\n2:19:12.760 --> 2:19:16.520\n to tell you about this new company that I just met?\n\n2:19:16.520 --> 2:19:22.400\n And if I'm an ambassador for that company and I'm passionate about what they're doing,\n\n2:19:22.400 --> 2:19:24.720\n I usually use that.\n\n2:19:24.720 --> 2:19:25.720\n Yeah.\n\n2:19:25.720 --> 2:19:27.720\n That's important to me when I'm investing.\n\n2:19:27.720 --> 2:19:34.720\n So that means you actually can explain what they're doing and you're excited about it.\n\n2:19:34.720 --> 2:19:35.720\n Exactly.\n\n2:19:35.720 --> 2:19:36.720\n Exactly.\n\n2:19:36.720 --> 2:19:41.720\n Thank you for putting it so succinctly, like rambling, but exactly that's it.\n\n2:19:41.720 --> 2:19:48.280\n No, but sometimes it's funny, but sometimes it's unclear exactly.\n\n2:19:48.280 --> 2:19:53.120\n I'll hear people tell me, you know, in the talk for a while and it sounds cool, like\n\n2:19:53.120 --> 2:19:56.600\n they paint a picture of a world, but then when you try to summarize it, you're not\n\n2:19:56.600 --> 2:19:57.600\n exactly clear.\n\n2:19:57.600 --> 2:20:05.200\n Like maybe what the core powerful idea is, like you can't just build another Facebook\n\n2:20:05.200 --> 2:20:15.360\n or there has to be a core, simple to explain idea that then you can or can't get excited\n\n2:20:15.360 --> 2:20:19.000\n about, but it's there, it's right there.\n\n2:20:19.000 --> 2:20:20.000\n Yeah.\n\n2:20:20.000 --> 2:20:25.520\n But how do you ultimately pick who you think will be successful?\n\n2:20:25.520 --> 2:20:29.320\n It's not just about the thing you're excited about, like there's other stuff.\n\n2:20:29.320 --> 2:20:30.320\n Right.\n\n2:20:30.320 --> 2:20:34.400\n And then there's all the, you know, with early stage companies, like pre seed companies,\n\n2:20:34.400 --> 2:20:40.640\n which is where I'm investing, sometimes the business model isn't clear yet, or the go\n\n2:20:40.640 --> 2:20:42.240\n to market strategy isn't clear.\n\n2:20:42.240 --> 2:20:45.560\n There's usually like, it's very early on that some of these things haven't been hashed\n\n2:20:45.560 --> 2:20:47.840\n out, which is okay.\n\n2:20:47.840 --> 2:20:51.720\n So the way I like to think about it is like, if this company is successful, will this be\n\n2:20:51.720 --> 2:20:56.660\n a multi billion slash trillion dollar market, you know, or company?\n\n2:20:56.660 --> 2:21:01.280\n And so that's definitely a lens that I use.\n\n2:21:01.280 --> 2:21:02.280\n What's pre seed?\n\n2:21:02.280 --> 2:21:07.880\n What are the different stages and what's the most exciting stage and what's, or no, what's\n\n2:21:07.880 --> 2:21:09.680\n interesting about every stage, I guess.\n\n2:21:09.680 --> 2:21:10.680\n Yeah.\n\n2:21:10.680 --> 2:21:16.000\n So pre seed is usually when you're just starting out, you've maybe raised the friends and family\n\n2:21:16.000 --> 2:21:17.000\n rounds.\n\n2:21:17.000 --> 2:21:20.720\n So you've raised some money from people, you know, and you're getting ready to take your\n\n2:21:20.720 --> 2:21:25.680\n first institutional check in, like first check from an investor.\n\n2:21:25.680 --> 2:21:28.920\n And I love the stage.\n\n2:21:28.920 --> 2:21:30.780\n There's a lot of uncertainty.\n\n2:21:30.780 --> 2:21:36.760\n Some investors really don't like the stage because the financial models aren't there.\n\n2:21:36.760 --> 2:21:40.920\n Often the teams aren't even like formed really, really early.\n\n2:21:40.920 --> 2:21:48.480\n But to me, it's like a magical stage because it's the time when there's so much conviction,\n\n2:21:48.480 --> 2:21:51.800\n so much belief, almost delusional, right?\n\n2:21:51.800 --> 2:21:57.120\n And there's a little bit of naivete around with founders at the stage.\n\n2:21:57.120 --> 2:21:58.120\n I just love it.\n\n2:21:58.120 --> 2:21:59.120\n It's contagious.\n\n2:21:59.120 --> 2:22:06.560\n And I love that I can, often they're first time founders, not always, but often they're\n\n2:22:06.560 --> 2:22:12.620\n first time founders and I can share my experience as a founder myself and I can empathize, right?\n\n2:22:12.620 --> 2:22:18.520\n And I can almost, I create a safe ground where, because, you know, you have to be careful\n\n2:22:18.520 --> 2:22:21.200\n what you tell your investors, right?\n\n2:22:21.200 --> 2:22:24.800\n And I will often like say, I've been in your shoes as a founder.\n\n2:22:24.800 --> 2:22:28.360\n You can tell me if it's challenging, you can tell me what you're struggling with.\n\n2:22:28.360 --> 2:22:30.160\n It's okay to vent.\n\n2:22:30.160 --> 2:22:34.760\n So I create that safe ground and I think that's a superpower.\n\n2:22:34.760 --> 2:22:35.760\n Yeah.\n\n2:22:35.760 --> 2:22:40.400\n You have to, I guess you have to figure out if this kind of person is going to be able\n\n2:22:40.400 --> 2:22:48.280\n to ride the roller coaster, like of many pivots and challenges and all that kind of stuff.\n\n2:22:48.280 --> 2:22:53.040\n And if the space of ideas they're working in is interesting, like the way they think\n\n2:22:53.040 --> 2:22:54.040\n about the world.\n\n2:22:54.040 --> 2:22:55.040\n Yeah.\n\n2:22:55.040 --> 2:23:00.000\n Because if it's successful, the thing they end up with might be very different, the reason\n\n2:23:00.000 --> 2:23:01.560\n it's successful for them.\n\n2:23:01.560 --> 2:23:07.480\n Actually, you know, I was going to say the third, so the technology is one aspect, the\n\n2:23:07.480 --> 2:23:11.240\n market or the idea, right, is the second and the third is the founder, right?\n\n2:23:11.240 --> 2:23:18.160\n Is this somebody who I believe has conviction, is a hustler, you know, is going to overcome\n\n2:23:18.160 --> 2:23:19.160\n obstacles?\n\n2:23:19.160 --> 2:23:23.200\n Yeah, I think that is going to be a great leader, right?\n\n2:23:23.200 --> 2:23:28.440\n Like as a startup, as a founder, you're often, you are the first person and your role is\n\n2:23:28.440 --> 2:23:32.880\n to bring amazing people around you to build this thing.\n\n2:23:32.880 --> 2:23:36.160\n And so you're an evangelist, right?\n\n2:23:36.160 --> 2:23:38.000\n So how good are you going to be at that?\n\n2:23:38.000 --> 2:23:41.360\n So I try to evaluate that too.\n\n2:23:41.360 --> 2:23:46.960\n You also in the tweet thread about it, mention, is this a known concept, random rich dudes\n\n2:23:46.960 --> 2:23:53.280\n are RDS and saying that there should be like random rich women, I guess.\n\n2:23:53.280 --> 2:23:58.280\n What's the dudes, what's the dudes version of women, the women version of dudes, ladies?\n\n2:23:58.280 --> 2:23:59.280\n I don't know.\n\n2:23:59.280 --> 2:24:00.280\n I don't know.\n\n2:24:00.280 --> 2:24:01.500\n What's, what's, is this a technical term?\n\n2:24:01.500 --> 2:24:02.500\n Is this known?\n\n2:24:02.500 --> 2:24:03.500\n Random rich dudes?\n\n2:24:03.500 --> 2:24:09.680\n I didn't make that up, but I was at this capital camp, which is a get together for investors\n\n2:24:09.680 --> 2:24:11.600\n of all types.\n\n2:24:11.600 --> 2:24:19.160\n And there must have been maybe 400 or so attendees, maybe 20 were women.\n\n2:24:19.160 --> 2:24:25.680\n It was just very disproportionately, you know, male dominated, which I'm used to.\n\n2:24:25.680 --> 2:24:26.960\n I think you're used to this kind of thing.\n\n2:24:26.960 --> 2:24:29.920\n I'm used to it, but it's still surprising.\n\n2:24:29.920 --> 2:24:36.320\n And as I'm raising money for this fund, so my fund partner is a guy called Rob May, who's\n\n2:24:36.320 --> 2:24:37.680\n done this before.\n\n2:24:37.680 --> 2:24:42.000\n So I'm new to the investing world, but he's done this before.\n\n2:24:42.000 --> 2:24:45.880\n Most of our investors in the fund are these, I mean, awesome.\n\n2:24:45.880 --> 2:24:47.800\n I'm super grateful to them.\n\n2:24:47.800 --> 2:24:48.800\n Random just rich guys.\n\n2:24:48.800 --> 2:24:50.400\n I'm like, where are the rich women?\n\n2:24:50.400 --> 2:24:57.160\n So I'm really adamant in both investing in women led AI companies, but I also would love\n\n2:24:57.160 --> 2:25:03.240\n to have women investors be part of my fund because I think that's how we drive change.\n\n2:25:03.240 --> 2:25:04.240\n Yeah.\n\n2:25:04.240 --> 2:25:09.640\n So that takes time, of course, but there's been quite a lot of progress, but yeah, for\n\n2:25:09.640 --> 2:25:13.840\n the next Mark Zuckerberg to be a woman and all that kind of stuff, because that's just\n\n2:25:13.840 --> 2:25:19.760\n like a huge number of wealth generated by women and then controlled by women and allocated\n\n2:25:19.760 --> 2:25:22.200\n by women and all that kind of stuff.\n\n2:25:22.200 --> 2:25:28.880\n And then beyond just women, just broadly across all different measures of diversity and so\n\n2:25:28.880 --> 2:25:29.880\n on.\n\n2:25:29.880 --> 2:25:35.880\n Let me ask you to put on your wise sage hat.\n\n2:25:35.880 --> 2:25:45.120\n So you already gave advice on startups and just advice for women, but in general advice\n\n2:25:45.120 --> 2:25:51.080\n for folks in high school or college today, how to have a career they can be proud of,\n\n2:25:51.080 --> 2:25:55.560\n how to have a life they can be proud of.\n\n2:25:55.560 --> 2:25:58.560\n I suppose you have to give this kind of advice to your kids.\n\n2:25:58.560 --> 2:25:59.560\n Yeah.\n\n2:25:59.560 --> 2:26:03.400\n Well, here's the number one advice that I give to my kids.\n\n2:26:03.400 --> 2:26:08.200\n My daughter's now 19 by the way, and my son's 13 and a half, so they're not little kids\n\n2:26:08.200 --> 2:26:09.200\n anymore.\n\n2:26:09.200 --> 2:26:11.560\n Does it break your heart?\n\n2:26:11.560 --> 2:26:12.560\n It does.\n\n2:26:12.560 --> 2:26:13.560\n They're awesome.\n\n2:26:13.560 --> 2:26:19.880\n They're my best friends, but yeah, I think the number one advice I would share is embark\n\n2:26:19.880 --> 2:26:25.200\n on a journey without attaching to outcomes and enjoy the journey, right?\n\n2:26:25.200 --> 2:26:34.360\n So we often were so obsessed with the end goal that doesn't allow us to be open to different\n\n2:26:34.360 --> 2:26:41.840\n endings of a journey or a story, so you become like so fixated on a particular path.\n\n2:26:41.840 --> 2:26:48.520\n You don't see the beauty in the other alternative path, and then you forget to enjoy the journey\n\n2:26:48.520 --> 2:26:53.320\n because you're just so fixated on the goal, and I've been guilty of that for many, many\n\n2:26:53.320 --> 2:27:00.180\n years of my life, and I'm now trying to make the shift of, no, no, no, I'm going to again\n\n2:27:00.180 --> 2:27:04.480\n trust that things are going to work out and it'll be amazing and maybe even exceed your\n\n2:27:04.480 --> 2:27:05.480\n dreams.\n\n2:27:05.480 --> 2:27:07.280\n We have to be open to that.\n\n2:27:07.280 --> 2:27:08.280\n Yeah.\n\n2:27:08.280 --> 2:27:09.840\n Taking a leap into all kinds of things.\n\n2:27:09.840 --> 2:27:13.800\n I think you tweeted like you went on vacation by yourself or something like this.\n\n2:27:13.800 --> 2:27:14.800\n I know.\n\n2:27:14.800 --> 2:27:19.120\n Yes, and just going, just taking the leap.\n\n2:27:19.120 --> 2:27:20.120\n Doing it.\n\n2:27:20.120 --> 2:27:21.120\n Totally doing it.\n\n2:27:21.120 --> 2:27:26.720\n And enjoying it, enjoying the moment, enjoying the weeks, enjoying not looking at some kind\n\n2:27:26.720 --> 2:27:29.640\n of career ladder, next step and so on.\n\n2:27:29.640 --> 2:27:34.320\n Yeah, there's something to that, like over planning too.\n\n2:27:34.320 --> 2:27:37.800\n I'm surrounded by a lot of people that kind of, so I don't plan.\n\n2:27:37.800 --> 2:27:38.800\n You don't?\n\n2:27:38.800 --> 2:27:39.800\n No.\n\n2:27:39.800 --> 2:27:43.240\n Do you not do goal setting?\n\n2:27:43.240 --> 2:27:52.760\n My goal setting is very like, I like the affirmations, it's very, it's almost, I don't know how to\n\n2:27:52.760 --> 2:28:02.040\n put it into words, but it's a little bit like what my heart yearns for kind of, and I guess\n\n2:28:02.040 --> 2:28:08.640\n in the space of emotions more than in the space of like, this will be like in the rational\n\n2:28:08.640 --> 2:28:16.280\n space because I just try to picture a world that I would like to be in and that world\n\n2:28:16.280 --> 2:28:19.400\n is not clearly pictured, it's mostly in the emotional world.\n\n2:28:19.400 --> 2:28:26.640\n I mean, I think about that from robots because I have this desire, I've had it my whole life\n\n2:28:26.640 --> 2:28:33.180\n to, well, it took different shapes, but I think once I discovered AI, the desire was\n\n2:28:33.180 --> 2:28:41.120\n to, I think in the context of this conversation could be easily easier described as basically\n\n2:28:41.120 --> 2:28:50.880\n a social robotics company and that's something I dreamed of doing and well, there's a lot\n\n2:28:50.880 --> 2:28:55.680\n of complexity to that story, but that's the only thing, honestly, I dream of doing.\n\n2:28:55.680 --> 2:29:05.560\n So I imagine a world that I could help create, but it's not, there's no steps along the way\n\n2:29:05.560 --> 2:29:12.720\n and I think I'm just kind of stumbling around and following happiness and working my ass\n\n2:29:12.720 --> 2:29:18.240\n off in almost random, like an ant does in random directions, but a lot of people, a\n\n2:29:18.240 --> 2:29:20.920\n lot of successful people around me say this, you should have a plan, you should have a\n\n2:29:20.920 --> 2:29:23.960\n clear goal, you have a goal at the end of the month, you have a goal at the end of the\n\n2:29:23.960 --> 2:29:33.320\n month, I don't, I don't, I don't and there's a balance to be struck, of course, but there's\n\n2:29:33.320 --> 2:29:40.360\n something to be said about really making sure that you're living life to the fullest, that\n\n2:29:40.360 --> 2:29:43.760\n goals can actually get in the way of.\n\n2:29:43.760 --> 2:29:52.560\n So one of the best, like kind of most, what do you call it when it challenges your brain,\n\n2:29:52.560 --> 2:29:56.760\n what do you call it?\n\n2:29:56.760 --> 2:30:00.320\n The only thing that comes to mind, and this is me saying is the mindfuck, but yes.\n\n2:30:00.320 --> 2:30:01.320\n Okay.\n\n2:30:01.320 --> 2:30:02.320\n Okay.\n\n2:30:02.320 --> 2:30:03.320\n Okay.\n\n2:30:03.320 --> 2:30:04.320\n Something like that.\n\n2:30:04.320 --> 2:30:05.320\n Yes.\n\n2:30:05.320 --> 2:30:06.320\n Super inspiring talk.\n\n2:30:06.320 --> 2:30:11.320\n Kenneth Stanley, he was at OpenAI, he just laughed and he has a book called Why Greatness\n\n2:30:11.320 --> 2:30:14.160\n Can't Be Planned and it's actually an AI book.\n\n2:30:14.160 --> 2:30:20.400\n So and he's done all these experiments that basically show that when you over optimize,\n\n2:30:20.400 --> 2:30:23.760\n you, like the trade off is you're less creative, right?\n\n2:30:23.760 --> 2:30:30.760\n And to create true greatness and truly creative solutions to problems, you can't over plan\n\n2:30:30.760 --> 2:30:31.760\n it.\n\n2:30:31.760 --> 2:30:32.760\n You can't.\n\n2:30:32.760 --> 2:30:36.800\n And I thought that was, and so he generalizes it beyond AI and he talks about how we apply\n\n2:30:36.800 --> 2:30:42.320\n that in our personal life and in our organizations and our companies, which are over KPIs, right?\n\n2:30:42.320 --> 2:30:45.960\n Like look at any company in the world and it's all like, these aren't the goals, these\n\n2:30:45.960 --> 2:30:51.080\n aren't weekly goals and the sprints and then the quarterly goals, blah, blah, blah.\n\n2:30:51.080 --> 2:30:58.560\n And he just shows with a lot of his AI experiments that that's not how you create truly game\n\n2:30:58.560 --> 2:30:59.640\n changing ideas.\n\n2:30:59.640 --> 2:31:00.640\n So there you go.\n\n2:31:00.640 --> 2:31:01.640\n Yeah, yeah.\n\n2:31:01.640 --> 2:31:02.640\n You can.\n\n2:31:02.640 --> 2:31:03.640\n He's awesome.\n\n2:31:03.640 --> 2:31:04.640\n Yeah.\n\n2:31:04.640 --> 2:31:05.640\n There's a balance of course.\n\n2:31:05.640 --> 2:31:11.660\n That's yeah, many moments of genius will not come from planning and goals, but you still\n\n2:31:11.660 --> 2:31:15.200\n have to build factories and you still have to manufacture and you still have to deliver\n\n2:31:15.200 --> 2:31:17.280\n and there's still deadlines and all that kind of stuff.\n\n2:31:17.280 --> 2:31:19.280\n And that for that, it's good to have goals.\n\n2:31:19.280 --> 2:31:25.200\n I do goal setting with my kids, we all have our goals, but, but, but I think we're starting\n\n2:31:25.200 --> 2:31:30.800\n to morph into more of these like bigger picture goals and not obsess about like, I don't know,\n\n2:31:30.800 --> 2:31:31.800\n it's hard.\n\n2:31:31.800 --> 2:31:34.840\n Well, I honestly think with, especially with kids, it's better, much, much better to have\n\n2:31:34.840 --> 2:31:38.400\n a plan and have goals and so on because you have to, you have to learn the muscle of like\n\n2:31:38.400 --> 2:31:40.480\n what it feels like to get stuff done.\n\n2:31:40.480 --> 2:31:41.480\n Yeah.\n\n2:31:41.480 --> 2:31:46.720\n And once you learn that, there's flexibility for me because I spend most of my life with\n\n2:31:46.720 --> 2:31:48.040\n goal setting and so on.\n\n2:31:48.040 --> 2:31:50.560\n So like I've gotten good with grades and school.\n\n2:31:50.560 --> 2:31:55.040\n I mean, school, if you want to be successful at school, yeah, I mean the kind of stuff\n\n2:31:55.040 --> 2:31:59.280\n in high school and college, the kids have to do in terms of managing their time and\n\n2:31:59.280 --> 2:32:01.160\n getting so much stuff done.\n\n2:32:01.160 --> 2:32:06.500\n It's like, you know, taking five, six, seven classes in college, they're like that would\n\n2:32:06.500 --> 2:32:13.200\n break the spirit of most humans if they took one of them later in life, it's like really\n\n2:32:13.200 --> 2:32:16.560\n difficult stuff, especially engineering curricula.\n\n2:32:16.560 --> 2:32:22.680\n So I think you have to learn that skill, but once you learn it, you can maybe, cause you're,\n\n2:32:22.680 --> 2:32:27.280\n you can be a little bit on autopilot and use that momentum and then allow yourself to be\n\n2:32:27.280 --> 2:32:28.920\n lost in the flow of life.\n\n2:32:28.920 --> 2:32:38.760\n You know, just kind of, or also give like, I worked pretty hard to allow myself to have\n\n2:32:38.760 --> 2:32:39.760\n the freedom to do that.\n\n2:32:39.760 --> 2:32:44.600\n That's really, that's a tricky freedom to have because like a lot of people get lost\n\n2:32:44.600 --> 2:32:52.920\n in the rat race and they, and they also like financially, they, whenever you get a raise,\n\n2:32:52.920 --> 2:32:55.680\n they'll get like a bigger house or something like this.\n\n2:32:55.680 --> 2:32:59.720\n I put very, so like, there's, you're always trapped in this race, I put a lot of emphasis\n\n2:32:59.720 --> 2:33:05.240\n on living like below my means always.\n\n2:33:05.240 --> 2:33:12.240\n And so there's a lot of freedom to do whatever, whatever the heart desires that that's a relief,\n\n2:33:12.240 --> 2:33:15.580\n but everyone has to decide what's the right thing, what's the right thing for them.\n\n2:33:15.580 --> 2:33:21.560\n For some people having a lot of responsibilities, like a house they can barely afford or having\n\n2:33:21.560 --> 2:33:27.600\n a lot of kids, the responsibility side of that is really, helps them get their shit\n\n2:33:27.600 --> 2:33:28.600\n together.\n\n2:33:28.600 --> 2:33:32.080\n Like, all right, I need to be really focused and get, some of the most successful people\n\n2:33:32.080 --> 2:33:34.760\n I know have kids and the kids bring out the best in them.\n\n2:33:34.760 --> 2:33:36.720\n They make them more productive and less productive.\n\n2:33:36.720 --> 2:33:37.720\n Right, it's accountability.\n\n2:33:37.720 --> 2:33:38.720\n Yeah.\n\n2:33:38.720 --> 2:33:39.720\n It's an accountability thing, absolutely.\n\n2:33:39.720 --> 2:33:45.400\n And almost something to actually live and fight and work for, like having a family,\n\n2:33:45.400 --> 2:33:49.520\n it's fascinating to see because you would think kids would be a hit on productivity,\n\n2:33:49.520 --> 2:33:53.680\n but they're not, for a lot of really successful people, they really like, they're like an\n\n2:33:53.680 --> 2:33:54.680\n engine of.\n\n2:33:54.680 --> 2:33:55.680\n Right, efficiency.\n\n2:33:55.680 --> 2:33:56.680\n Oh my God.\n\n2:33:56.680 --> 2:33:57.680\n Yeah.\n\n2:33:57.680 --> 2:33:58.680\n Yeah.\n\n2:33:58.680 --> 2:33:59.680\n It's weird.\n\n2:33:59.680 --> 2:34:00.680\n Yeah.\n\n2:34:00.680 --> 2:34:01.680\n I mean, it's beautiful.\n\n2:34:01.680 --> 2:34:02.680\n It's beautiful to see.\n\n2:34:02.680 --> 2:34:03.680\n And also a source of happiness.\n\n2:34:03.680 --> 2:34:12.080\n Speaking of which, what role do you think love plays in the human condition, love?\n\n2:34:12.080 --> 2:34:19.880\n I think love is, yeah, I think it's why we're all here.\n\n2:34:19.880 --> 2:34:26.640\n I think it would be very hard to live life without love in any of its forms, right?\n\n2:34:26.640 --> 2:34:35.080\n Yeah, that's the most beautiful of forms that human connection takes, right?\n\n2:34:35.080 --> 2:34:36.080\n Yeah.\n\n2:34:36.080 --> 2:34:42.200\n And everybody wants to feel loved, right, in one way or another, right?\n\n2:34:42.200 --> 2:34:43.200\n And to love.\n\n2:34:43.200 --> 2:34:44.200\n Yeah.\n\n2:34:44.200 --> 2:34:45.200\n It feels good.\n\n2:34:45.200 --> 2:34:46.200\n And to love too, totally.\n\n2:34:46.200 --> 2:34:47.200\n Yeah, I agree with that.\n\n2:34:47.200 --> 2:34:48.200\n Both of it.\n\n2:34:48.200 --> 2:34:49.200\n Yeah.\n\n2:34:49.200 --> 2:34:50.200\n I'm not even sure what feels better.\n\n2:34:50.200 --> 2:34:51.200\n Both, both like that.\n\n2:34:51.200 --> 2:34:54.760\n Yeah, to give and to give love too, yeah.\n\n2:34:54.760 --> 2:34:59.680\n And it is like we've been talking about an interesting question, whether some of that,\n\n2:34:59.680 --> 2:35:02.640\n whether one day we'll be able to love a toaster.\n\n2:35:02.640 --> 2:35:03.640\n Okay.\n\n2:35:03.640 --> 2:35:05.240\n It's some small.\n\n2:35:05.240 --> 2:35:10.320\n I wasn't quite thinking about that when I said like, yeah, like we all need love and\n\n2:35:10.320 --> 2:35:11.320\n give love.\n\n2:35:11.320 --> 2:35:12.320\n That's all I was thinking about.\n\n2:35:12.320 --> 2:35:13.320\n Okay.\n\n2:35:13.320 --> 2:35:14.320\n I was thinking about Brad Pitt and toasters.\n\n2:35:14.320 --> 2:35:15.320\n Okay, toasters, great.\n\n2:35:15.320 --> 2:35:16.320\n All right.\n\n2:35:16.320 --> 2:35:20.200\n Well, I think we started on love and ended on love.\n\n2:35:20.200 --> 2:35:22.320\n This was an incredible conversation, Rhonda.\n\n2:35:22.320 --> 2:35:23.320\n Thank you so much.\n\n2:35:23.320 --> 2:35:24.320\n Thank you.\n\n2:35:24.320 --> 2:35:25.320\n You're an incredible person.\n\n2:35:25.320 --> 2:35:32.960\n Thank you for everything you're doing in AI, in the space of just caring about humanity,\n\n2:35:32.960 --> 2:35:38.160\n caring about emotion, about love, and being an inspiration to a huge number of people\n\n2:35:38.160 --> 2:35:42.320\n in robotics, in AI, in science, in the world in general.\n\n2:35:42.320 --> 2:35:43.320\n So thank you for talking to me.\n\n2:35:43.320 --> 2:35:44.320\n It's an honor.\n\n2:35:44.320 --> 2:35:45.320\n Thank you for having me.\n\n2:35:45.320 --> 2:35:47.320\n And you know, I'm a big fan of yours as well.\n\n2:35:47.320 --> 2:35:49.740\n So it's been a pleasure.\n\n2:35:49.740 --> 2:35:52.840\n Thanks for listening to this conversation with Rhonda Alkalioubi.\n\n2:35:52.840 --> 2:35:56.940\n To support this podcast, please check out our sponsors in the description.\n\n2:35:56.940 --> 2:36:00.680\n And now let me leave you with some words from Helen Keller.\n\n2:36:00.680 --> 2:36:05.480\n The best and most beautiful things in the world cannot be seen or even touched.\n\n2:36:05.480 --> 2:36:09.440\n They must be felt with the heart.\n\n2:36:09.440 --> 2:36:31.560\n Thank you for listening and hope to see you next time.\n\n"
}