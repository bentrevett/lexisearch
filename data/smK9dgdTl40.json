{
  "title": "Elon Musk: Neuralink, AI, Autopilot, and the Pale Blue Dot | Lex Fridman Podcast #49",
  "id": "smK9dgdTl40",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:07.280\n The following is a conversation with Elon Musk, Part 2, the second time we spoke on the podcast,\n\n00:07.280 --> 00:13.120\n with parallels, if not in quality, than an outfit, to the objectively speaking greatest\n\n00:13.120 --> 00:20.720\n sequel of all time, Godfather Part 2. As many people know, Elon Musk is a leader of Tesla,\n\n00:20.720 --> 00:26.560\n SpaceX, Neuralink, and the Boring Company. What may be less known is that he's a world\n\n00:26.560 --> 00:32.480\n class engineer and designer, constantly emphasizing first principles thinking and taking on big\n\n00:32.480 --> 00:39.600\n engineering problems that many before him will consider impossible. As scientists and engineers,\n\n00:39.600 --> 00:44.160\n most of us don't question the way things are done, we simply follow the momentum of the crowd.\n\n00:44.880 --> 00:51.520\n But revolutionary ideas that change the world on the small and large scales happen when you\n\n00:51.520 --> 00:57.840\n return to the fundamentals and ask, is there a better way? This conversation focuses on the\n\n00:57.840 --> 01:02.960\n incredible engineering and innovation done in brain computer interfaces at Neuralink.\n\n01:04.160 --> 01:09.440\n This work promises to help treat neurobiological diseases to help us further understand the\n\n01:09.440 --> 01:14.400\n connection between the individual neuron to the high level function of the human brain.\n\n01:14.400 --> 01:20.240\n And finally, to one day expand the capacity of the brain through two way communication\n\n01:20.240 --> 01:24.640\n with computational devices, the internet, and artificial intelligence systems.\n\n01:25.440 --> 01:31.040\n This is the Artificial Intelligence Podcast. If you enjoy it, subscribe by YouTube,\n\n01:31.040 --> 01:36.320\n Apple Podcasts, Spotify, support on Patreon, or simply connect with me on Twitter\n\n01:36.320 --> 01:43.520\n at Lex Friedman, spelled F R I D M A N. And now, as an anonymous YouTube commenter referred to\n\n01:43.520 --> 01:49.440\n our previous conversation as the quote, historical first video of two robots conversing without\n\n01:49.440 --> 01:56.560\n supervision, here's the second time, the second conversation with Elon Musk.\n\n01:57.840 --> 02:03.120\n Let's start with an easy question about consciousness. In your view, is consciousness\n\n02:03.120 --> 02:07.600\n something that's unique to humans or is it something that permeates all matter, almost like\n\n02:07.600 --> 02:13.680\n a fundamental force of physics? I don't think consciousness permeates all matter. Panpsychists\n\n02:13.680 --> 02:21.120\n believe that. Yeah. There's a philosophical. How would you tell? That's true. That's a good point.\n\n02:21.120 --> 02:24.240\n I believe in scientific methods. I don't know about your mind or anything, but the scientific\n\n02:24.240 --> 02:28.800\n method is like, if you cannot test the hypothesis, then you cannot reach meaningful conclusion that\n\n02:28.800 --> 02:34.000\n it is true. Do you think consciousness, understanding consciousness is within the\n\n02:34.000 --> 02:40.160\n reach of science of the scientific method? We can dramatically improve our understanding of\n\n02:40.160 --> 02:47.120\n consciousness. You know, hot press to say that we understand anything with complete accuracy,\n\n02:47.120 --> 02:51.360\n but can we dramatically improve our understanding of consciousness? I believe the answer is yes.\n\n02:53.360 --> 02:58.480\n Does an AI system in your view have to have consciousness in order to achieve human level\n\n02:58.480 --> 03:03.360\n or superhuman level intelligence? Does it need to have some of these human qualities that\n\n03:03.360 --> 03:11.120\n consciousness, maybe a body, maybe a fear of mortality, capacity to love those kinds of\n\n03:11.120 --> 03:19.440\n silly human things? There's a different, you know, there's this, there's the scientific method,\n\n03:19.440 --> 03:25.200\n which I very much believe in where something is true to the degree that it is testably. So\n\n03:25.200 --> 03:34.560\n and otherwise, you're really just talking about, you know, preferences or untestable beliefs or\n\n03:34.560 --> 03:41.520\n that, you know, that kind of thing. So it ends up being somewhat of a semantic question, where\n\n03:42.320 --> 03:46.880\n we were conflating a lot of things with the word intelligence. If we parse them out and say,\n\n03:46.880 --> 03:56.240\n you know, are we headed towards the future where an AI will be able to outthink us in every way?\n\n03:57.520 --> 03:59.440\n Then the answer is unequivocally yes.\n\n04:01.440 --> 04:06.400\n In order for an AI system that needs to outthink us in every way, it also needs to have\n\n04:07.360 --> 04:12.320\n a capacity to have consciousness, self awareness, and understanding.\n\n04:12.320 --> 04:18.400\n It will be self aware. Yes, that's different from consciousness. I mean, to me, in terms of what\n\n04:18.400 --> 04:21.840\n what consciousness feels like, it feels like consciousness is in a different dimension.\n\n04:22.640 --> 04:30.480\n But this is this could be just an illusion. You know, if you damage your brain in some way,\n\n04:30.480 --> 04:35.920\n physically, you get you, you damage your consciousness, which implies that consciousness\n\n04:35.920 --> 04:42.720\n is a physical phenomenon. And in my view, the thing is that that I think are really quite,\n\n04:42.720 --> 04:48.880\n quite likely is that digital intelligence will be able to outthink us in every way, and it will\n\n04:48.880 --> 04:54.080\n simply be able to simulate what we consider consciousness. So to the degree that you would\n\n04:54.080 --> 04:58.160\n not be able to tell the difference. And from the from the aspect of the scientific method,\n\n04:58.160 --> 05:01.440\n it's might as well be consciousness, if we can simulate it perfectly.\n\n05:01.440 --> 05:06.800\n If you can't tell the difference, when this is sort of the Turing test, but think of a more\n\n05:06.800 --> 05:13.600\n sort of advanced version of the Turing test. If you're if you're talking to a digital super\n\n05:13.600 --> 05:19.440\n intelligence and can't tell if that is a computer or a human, like let's say you're just having\n\n05:19.440 --> 05:26.480\n conversation over a phone or a video conference or something where you're you think you're talking\n\n05:26.480 --> 05:33.360\n looks like a person makes all of the right inflections and movements and all the small\n\n05:33.360 --> 05:41.360\n subtleties that constitute a human and talks like human makes mistakes like a human like\n\n05:42.400 --> 05:49.120\n and you literally just can't tell is this Are you video conferencing with a person or or an AI\n\n05:49.120 --> 05:54.960\n might as well might as well be human. So on a darker topic, you've expressed serious concern\n\n05:54.960 --> 06:02.400\n about existential threats of AI. It's perhaps one of the greatest challenges our civilization faces,\n\n06:02.400 --> 06:08.480\n but since I would say we're kind of an optimistic descendants of apes, perhaps we can find several\n\n06:08.480 --> 06:16.480\n paths of escaping the harm of AI. So if I can give you an example of an example of an example\n\n06:16.480 --> 06:21.920\n of escaping the harm of AI. So if I can give you three options, maybe you can comment which do you\n\n06:21.920 --> 06:29.040\n think is the most promising. So one is scaling up efforts on AI safety and beneficial AI research\n\n06:29.040 --> 06:35.920\n in hope of finding an algorithmic or maybe a policy solution. Two is becoming a multi planetary\n\n06:35.920 --> 06:44.000\n species as quickly as possible. And three is merging with AI and riding the wave of that\n\n06:44.000 --> 06:49.280\n increasing intelligence as it continuously improves. What do you think is most promising,\n\n06:49.280 --> 06:52.720\n most interesting, as a civilization that we should invest in?\n\n06:54.640 --> 06:59.200\n I think there's a lot of tremendous amount of investment going on in AI, where there's a lack\n\n06:59.200 --> 07:06.640\n of investment is in AI safety. And there should be in my view, a government agency that oversees\n\n07:07.760 --> 07:12.960\n anything related to AI to confirm that it is does not represent a public safety risk,\n\n07:12.960 --> 07:19.120\n just as there is a regulatory authority for the Food and Drug Administration is that's for\n\n07:20.320 --> 07:25.920\n automotive safety, there's the FAA for aircraft safety, which I really come to the conclusion that\n\n07:25.920 --> 07:31.120\n it is important to have a government referee or referee that is serving the public interest\n\n07:31.120 --> 07:37.120\n in ensuring that things are safe when when there's a potential danger to the public.\n\n07:37.120 --> 07:43.920\n I would argue that AI is unequivocally something that has potential to be dangerous to the public,\n\n07:43.920 --> 07:48.480\n and therefore should have a regulatory agency just as other things that are dangerous to the public\n\n07:48.480 --> 07:54.240\n have a regulatory agency. But let me tell you, the problem with this is that the government\n\n07:54.240 --> 08:01.920\n moves very slowly. And the rate of the rate, the usually way a regulatory agency comes into being\n\n08:01.920 --> 08:09.120\n is that something terrible happens. There's a huge public outcry. And years after that,\n\n08:09.680 --> 08:15.120\n there's a regulatory agency or a rule put in place, take something like, like seatbelts,\n\n08:15.120 --> 08:25.840\n it was known for a decade or more that seatbelts would have a massive impact on safety and save so\n\n08:25.840 --> 08:32.000\n many lives in serious injuries. And the car industry fought the requirement to put seatbelts in\n\n08:32.000 --> 08:39.600\n tooth and nail. That's crazy. Yeah. And hundreds of 1000s of people probably died because of that.\n\n08:41.040 --> 08:44.080\n And they said people wouldn't buy cars if they had seatbelts, which is obviously absurd.\n\n08:45.680 --> 08:51.920\n Yeah, or look at the tobacco industry and how long they fought any thing about smoking. That's part\n\n08:51.920 --> 08:58.320\n of why I helped make that movie. Thank you for smoking. You can sort of see just how pernicious\n\n08:58.320 --> 09:09.680\n it can be when you have these companies effectively achieve regulatory capture of government. The bad\n\n09:11.280 --> 09:17.040\n people in the community refer to the advent of digital super intelligence as a singularity.\n\n09:17.040 --> 09:23.680\n That is not to say that it is good or bad, but that it is very difficult to predict what will\n\n09:23.680 --> 09:28.480\n happen after that point. And then there's some probability it will be bad, some probably it'll\n\n09:28.480 --> 09:34.720\n be it will be good. We obviously want to affect that probability and have it be more good than bad.\n\n09:35.920 --> 09:40.960\n Well, let me on the merger with AI question and the incredible work that's being done at Neuralink.\n\n09:40.960 --> 09:47.280\n There's a lot of fascinating innovation here across different disciplines going on. So the flexible\n\n09:47.280 --> 09:52.960\n wires, the robotic sewing machine, that responsive brain movement, everything around ensuring safety\n\n09:52.960 --> 10:02.560\n and so on. So we currently understand very little about the human brain. Do you also hope that the\n\n10:02.560 --> 10:07.840\n work at Neuralink will help us understand more about our about our human brain?\n\n10:07.840 --> 10:13.840\n Yeah, I think the work in Neuralink will definitely shed a lot of insight into how the brain, the mind\n\n10:13.840 --> 10:20.640\n works. Right now, just the data we have regarding how the brain works is very limited. You know,\n\n10:20.640 --> 10:28.480\n we've got fMRI, which is that that's kind of like putting us, you know, stethoscope on the outside\n\n10:28.480 --> 10:33.200\n of a factory wall and then putting it like all over the factory wall and you can sort of hear\n\n10:33.200 --> 10:38.720\n the sounds, but you don't know what the machines are doing, really. It's hard. You can infer a few\n\n10:38.720 --> 10:43.200\n things, but it's very broad brushstroke. In order to really know what's going on in the brain,\n\n10:43.200 --> 10:47.600\n you really need you have to have high precision sensors. And then you want to have stimulus and\n\n10:47.600 --> 10:53.280\n response. Like if you trigger a neuron, what, how do you feel? What do you see? How does it change\n\n10:53.280 --> 10:57.200\n your perception of the world? You're speaking to physically just getting close to the brain,\n\n10:57.200 --> 11:00.400\n being able to measure signals, how do you know what's going on in the brain?\n\n11:00.400 --> 11:04.160\n Physically, just getting close to the brain, being able to measure signals from the brain\n\n11:04.160 --> 11:07.680\n will give us sort of open the door inside the factory.\n\n11:08.480 --> 11:15.280\n Yes, exactly. Being able to have high precision sensors that tell you what individual neurons\n\n11:15.280 --> 11:20.960\n are doing. And then being able to trigger a neuron and see what the response is in the brain.\n\n11:22.000 --> 11:28.720\n So you can see the consequences of if you fire this neuron, what happens? How do you feel? What\n\n11:28.720 --> 11:34.160\n does it change? It'll be really profound to have this in people because people can articulate\n\n11:35.520 --> 11:43.040\n their change. Like if there's a change in mood, or if they can tell you if they can see better,\n\n11:43.040 --> 11:51.040\n or hear better, or be able to form sentences better or worse, or their memories are jogged,\n\n11:51.040 --> 11:56.880\n or that kind of thing. So on the human side, there's this incredible general malleability,\n\n11:56.880 --> 12:01.040\n plasticity of the human brain, the human brain adapts, adjusts, and so on.\n\n12:01.040 --> 12:03.200\n So that's not that plastic, to be totally frank.\n\n12:03.200 --> 12:09.040\n So there's a firm structure, but nevertheless, there's some plasticity. And the open question is,\n\n12:09.040 --> 12:15.120\n sort of, if I could ask a broad question is how much that plasticity can be utilized. Sort of,\n\n12:15.120 --> 12:20.560\n on the human side, there's some plasticity in the human brain. And on the machine side,\n\n12:20.560 --> 12:26.640\n we have neural networks, machine learning, artificial intelligence, it's able to adjust\n\n12:26.640 --> 12:31.760\n and figure out signals. So there's a mysterious language that we don't perfectly understand\n\n12:31.760 --> 12:37.120\n that's within the human brain. And then we're trying to understand that language to communicate\n\n12:37.120 --> 12:42.160\n both directions. So the brain is adjusting a little bit, we don't know how much, and the\n\n12:42.160 --> 12:48.080\n machine is adjusting. Where do you see, as they try to sort of reach together, almost like with\n\n12:48.080 --> 12:53.600\n an alien species, try to find a protocol, communication protocol that works? Where do\n\n12:53.600 --> 12:59.360\n you see the biggest, the biggest benefit arriving from on the machine side or the human side? Do you\n\n12:59.360 --> 13:03.680\n see both of them working together? I think the machine side is far more malleable than the\n\n13:03.680 --> 13:12.480\n biological side, by a huge amount. So it'll be the machine that adapts to the brain. That's the only\n\n13:12.480 --> 13:19.120\n thing that's possible. The brain can't adapt that well to the machine. You can't have neurons start\n\n13:19.120 --> 13:24.960\n to regard an electrode as another neuron, because neurons just, there's like the pulse. And so\n\n13:24.960 --> 13:31.520\n something else is pulsing. So there is that elasticity in the interface, which we believe is\n\n13:32.320 --> 13:37.520\n something that can happen. But the vast majority of the malleability will have to be on the machine\n\n13:37.520 --> 13:42.800\n side. But it's interesting, when you look at that synaptic plasticity at the interface side,\n\n13:43.680 --> 13:48.560\n there might be like an emergent plasticity. Because it's a whole nother, it's not like in the\n\n13:48.560 --> 13:53.840\n brain, it's a whole nother extension of the brain. You know, we might have to redefine what it means\n\n13:53.840 --> 13:59.440\n to be malleable for the brain. So maybe the brain is able to adjust to external interfaces. There\n\n13:59.440 --> 14:03.680\n will be some adjustments to the brain, because there's going to be something reading and simulating\n\n14:03.680 --> 14:12.400\n the brain. And so it will adjust to that thing. But most, the vast majority of the adjustment\n\n14:12.400 --> 14:18.720\n will be on the machine side. This is just, this is just, it has to be that otherwise it will not\n\n14:18.720 --> 14:23.440\n work. Ultimately, like, we currently operate on two layers, we have sort of a limbic, like prime\n\n14:23.440 --> 14:29.680\n primitive brain layer, which is where all of our kind of impulses are coming from. It's sort of\n\n14:29.680 --> 14:34.720\n like we've got, we've got like a monkey brain with a computer stuck on it. That's that's the\n\n14:34.720 --> 14:38.480\n human brain. And a lot of our impulses and everything are driven by the monkey brain.\n\n14:39.360 --> 14:44.720\n And the computer, the cortex is constantly trying to make the monkey brain happy.\n\n14:44.720 --> 14:49.040\n It's not the cortex that's steering the monkey brains, the monkey brain steering the cortex.\n\n14:51.040 --> 14:56.000\n You know, the cortex is the part that tells the story of the whole thing. So we convince ourselves\n\n14:56.000 --> 15:01.360\n it's, it's more interesting than just the monkey brain. The cortex is like what we call like human\n\n15:01.360 --> 15:05.280\n intelligence. You know, it's just like, that's like the advanced computer relative to other\n\n15:05.280 --> 15:11.840\n creatures. The other creatures do not have either. Really, they don't, they don't have the\n\n15:11.840 --> 15:19.840\n computer, or they have a very weak computer relative to humans. But it's, it's like, it sort\n\n15:19.840 --> 15:24.880\n of seems like surely the really smart thing should control the dumb thing. But actually,\n\n15:24.880 --> 15:30.160\n the dumb thing controls the smart thing. So do you think some of the same kind of machine learning\n\n15:30.160 --> 15:35.920\n methods, whether that's natural language processing applications are going to be applied for the\n\n15:35.920 --> 15:43.040\n communication between the machine and the brain to learn how to do certain things like movement\n\n15:43.040 --> 15:50.320\n of the body, how to process visual stimuli, and so on. Do you see the value of using machine\n\n15:50.320 --> 15:55.440\n learning to understand the language of the two way communication with the brain? Sure. Yeah,\n\n15:55.440 --> 16:02.240\n absolutely. I mean, we're neural net. And that, you know, AI is basically neural net.\n\n16:02.800 --> 16:06.000\n So it's like digital neural net will interface with biological neural net.\n\n16:08.160 --> 16:14.320\n And hopefully bring us along for the ride. Yeah. But the vast majority of our intelligence will be\n\n16:14.320 --> 16:23.120\n digital. Like, so like, think of like, the difference in intelligence between your cortex\n\n16:23.120 --> 16:29.840\n and your limbic system is gigantic, your limbic system really has no comprehension of what the\n\n16:29.840 --> 16:40.240\n hell the cortex is doing. It's just literally hungry, you know, or tired or angry or sexy or\n\n16:40.240 --> 16:46.480\n something, you know, that's just and then that communicates that that impulse to the cortex and\n\n16:47.600 --> 16:54.480\n tells the cortex to go satisfy that then love a great deal of like, a massive amount of thinking,\n\n16:54.480 --> 17:00.960\n like truly stupendous amount of thinking has gone into sex without purpose, without procreation,\n\n17:00.960 --> 17:11.440\n without procreation. Which is actually quite a silly action in the absence of procreation. It's\n\n17:11.440 --> 17:16.960\n a bit silly. Why are you doing it? Because it makes the limbic system happy. That's why. That's why.\n\n17:17.840 --> 17:24.880\n But it's pretty absurd, really. Well, the whole of existence is pretty absurd in some kind of sense.\n\n17:24.880 --> 17:32.160\n Yeah. But I mean, this is a lot of computation has gone into how can I do more of that with\n\n17:32.160 --> 17:37.440\n procreation not even being a factor? This is, I think, a very important area of research by NSFW.\n\n17:40.160 --> 17:44.160\n An agency that should receive a lot of funding, especially after this conversation.\n\n17:44.160 --> 17:48.480\n I propose the formation of a new agency. Oh, boy.\n\n17:48.480 --> 17:53.520\n What is the most exciting or some of the most exciting things that you see in the future impact\n\n17:53.520 --> 17:58.080\n of Neuralink, both in the science, the engineering and societal broad impact?\n\n17:59.120 --> 18:05.600\n Neuralink, I think, at first will solve a lot of brain related diseases. So it could be anything\n\n18:05.600 --> 18:11.600\n from like autism, schizophrenia, memory loss, like everyone experiences memory loss at certain points\n\n18:11.600 --> 18:16.480\n in age. Parents can't remember their kids names and that kind of thing. So it could be anything\n\n18:16.480 --> 18:19.280\n from like autism, schizophrenia, memory loss, like everyone experiences memory loss at certain points\n\n18:19.280 --> 18:24.400\n in age. Parents can't remember their kids names and that kind of thing. So there's a tremendous\n\n18:24.400 --> 18:34.480\n amount of good that Neuralink can do in solving critical damage to the brain or the spinal cord.\n\n18:34.480 --> 18:40.720\n There's a lot that can be done to improve quality of life of individuals. And those will be steps\n\n18:40.720 --> 18:48.240\n to address the existential risk associated with digital superintelligence. Like we will not be\n\n18:48.240 --> 18:56.880\n able to be smarter than a digital supercomputer. So therefore, if you cannot beat them, join them.\n\n18:58.240 --> 18:59.680\n And at least we won't have that option.\n\n19:01.520 --> 19:09.200\n So you have hope that Neuralink will be able to be a kind of connection to allow us to merge,\n\n19:09.200 --> 19:14.640\n the wave of the improving AI systems. I think the chance is above zero percent.\n\n19:15.600 --> 19:20.720\n So it's non zero. There's a chance. And that's what I've seen. Dumb and Dumber.\n\n19:21.920 --> 19:26.400\n Yes. So I'm saying there's a chance. He's saying one in a billion or one in a million,\n\n19:26.400 --> 19:30.560\n whatever it was, a dumb and dumber. You know, it went from maybe one in a million to improving.\n\n19:31.120 --> 19:35.040\n Maybe it'll be one in a thousand and then one in a hundred, then one in ten. Depends on the rate\n\n19:35.040 --> 19:40.400\n of improvement of Neuralink and how fast we're able to do make progress.\n\n19:41.040 --> 19:45.440\n Well, I've talked to a few folks here that are quite brilliant engineers, so I'm excited.\n\n19:45.440 --> 19:47.200\n Yeah, I think it's like fundamentally good, you know,\n\n19:48.400 --> 19:52.400\n giving somebody back full motor control after they've had a spinal cord injury.\n\n19:53.840 --> 19:56.240\n You know, restoring brain functionality after a stroke,\n\n19:57.920 --> 20:02.160\n solving debilitating genetically oriented brain diseases. These are all incredibly\n\n20:02.160 --> 20:07.440\n great, I think. And in order to do these, you have to be able to interface with neurons at\n\n20:07.440 --> 20:12.160\n a detailed level and you need to be able to fire the right neurons, read the right neurons, and\n\n20:13.200 --> 20:18.720\n and then effectively you can create a circuit, replace what's broken with\n\n20:19.760 --> 20:26.000\n with silicon and essentially fill in the missing functionality. And then over time,\n\n20:26.000 --> 20:31.120\n we can develop a tertiary layer. So if like the limbic system is the primary layer, then the\n\n20:31.120 --> 20:36.320\n cortex is like the second layer. And as I said, obviously the cortex is vastly more intelligent\n\n20:36.320 --> 20:40.080\n than the limbic system, but people generally like the fact that they have a limbic system\n\n20:40.080 --> 20:44.480\n and a cortex. I haven't met anyone who wants to delete either one of them. They're like,\n\n20:44.480 --> 20:47.440\n okay, I'll keep them both. That's cool. The limbic system is kind of fun.\n\n20:47.440 --> 20:53.360\n That's where the fun is, absolutely. And then people generally don't want to lose their\n\n20:53.360 --> 20:59.360\n cortex either. They're like having the cortex and the limbic system. And then there's a tertiary\n\n20:59.360 --> 21:05.520\n layer, which will be digital superintelligence. And I think there's room for optimism given that\n\n21:05.520 --> 21:11.760\n the cortex, the cortex is very intelligent and limbic system is not, and yet they work together\n\n21:11.760 --> 21:18.560\n well. Perhaps there can be a tertiary layer where digital superintelligence lies, and that will be\n\n21:18.560 --> 21:24.880\n vastly more intelligent than the cortex, but still coexist peacefully and in a benign manner with the\n\n21:24.880 --> 21:30.320\n cortex and limbic system. That's a super exciting future, both in low level engineering that I saw\n\n21:30.320 --> 21:36.080\n as being done here and the actual possibility in the next few decades. It's important that\n\n21:36.080 --> 21:40.880\n Neuralink solve this problem sooner rather than later, because the point at which we have digital\n\n21:40.880 --> 21:45.440\n superintelligence, that's when we pass the singularity and things become just very uncertain.\n\n21:45.440 --> 21:48.640\n It doesn't mean that they're necessarily bad or good. For the point at which we pass singularity,\n\n21:48.640 --> 21:55.440\n things become extremely unstable. So we want to have a human brain interface before the singularity,\n\n21:55.440 --> 22:01.360\n or at least not long after it, to minimize existential risk for humanity and consciousness\n\n22:01.360 --> 22:07.200\n as we know it. So there's a lot of fascinating actual engineering, low level problems here at\n\n22:07.200 --> 22:15.600\n Neuralink that are quite exciting. The problems that we face in Neuralink are material science,\n\n22:15.600 --> 22:21.520\n electrical engineering, software, mechanical engineering, microfabrication. It's a bunch of\n\n22:22.560 --> 22:26.080\n engineering disciplines, essentially. That's what it comes down to, is you have to have a\n\n22:26.080 --> 22:35.520\n tiny electrode, so small it doesn't hurt neurons, but it's got to last for as long as a person. So\n\n22:35.520 --> 22:40.880\n it's going to last for decades. And then you've got to take that signal, you've got to process\n\n22:40.880 --> 22:48.800\n that signal locally at low power. So we need a lot of chip design engineers, because we're going to\n\n22:48.800 --> 22:56.320\n do signal processing, and do so in a very power efficient way, so that we don't heat your brain\n\n22:56.320 --> 23:01.040\n up, because the brain is very heat sensitive. And then we've got to take those signals and\n\n23:01.040 --> 23:10.080\n we're going to do something with them. And then we've got to stimulate the back to bidirectional\n\n23:10.080 --> 23:15.360\n communication. So somebody's good at material science, software, and we've got to do a lot of\n\n23:15.360 --> 23:20.880\n that. So somebody's good at material science, software, mechanical engineering, electrical\n\n23:20.880 --> 23:26.080\n engineering, chip design, microfabrication. Those are the things we need to work on.\n\n23:27.520 --> 23:32.080\n We need to be good at material science, so that we can have tiny electrodes that last a long time.\n\n23:32.080 --> 23:35.760\n And it's a tough thing with the material science problem, it's a tough one, because\n\n23:35.760 --> 23:43.680\n you're trying to read and simulate electrically in an electrically active area. Your brain is\n\n23:43.680 --> 23:49.520\n very electrically active and electrochemically active. So how do you have a coating on the\n\n23:49.520 --> 23:57.200\n electrode that doesn't dissolve over time and is safe in the brain? This is a very hard problem.\n\n23:59.040 --> 24:06.880\n And then how do you collect those signals in a way that is most efficient? Because you really\n\n24:06.880 --> 24:12.720\n just have very tiny amounts of power to process those signals. And then we need to automate the\n\n24:12.720 --> 24:20.960\n whole thing so it's like LASIK. If this is done by neurosurgeons, there's no way it can scale to\n\n24:20.960 --> 24:24.800\n a large number of people. And it needs to scale to a large number of people, because I think\n\n24:24.800 --> 24:32.720\n ultimately we want the future to be determined by a large number of humans. Do you think that\n\n24:32.720 --> 24:39.040\n this has a chance to revolutionize surgery period? So neurosurgery and surgery all across?\n\n24:39.040 --> 24:45.120\n Yeah, for sure. It's got to be like LASIK. If LASIK had to be done by hand by a person,\n\n24:45.680 --> 24:54.320\n that wouldn't be great. It's done by a robot. And the ophthalmologist kind of just needs to make\n\n24:54.320 --> 24:58.480\n sure your head's in the right position, and then they just press a button and go.\n\n25:00.000 --> 25:05.920\n SmartSummon and soon Autopark takes on the full beautiful mess of parking lots and their human\n\n25:05.920 --> 25:13.680\n to human nonverbal communication. I think it has actually the potential to have a profound impact\n\n25:13.680 --> 25:19.440\n in changing how our civilization looks at AI and robotics, because this is the first time human\n\n25:19.440 --> 25:24.080\n beings, people that don't own a Tesla may have never seen a Tesla or heard about a Tesla,\n\n25:24.080 --> 25:30.880\n get to watch hundreds of thousands of cars without a driver. Do you see it this way, almost like an\n\n25:30.880 --> 25:36.080\n education tool for the world about AI? Do you feel the burden of that, the excitement of that,\n\n25:36.080 --> 25:42.160\n or do you just think it's a smart parking feature? I do think you are getting at something\n\n25:42.160 --> 25:47.680\n important, which is most people have never really seen a robot. And what is the car that is\n\n25:47.680 --> 25:53.200\n autonomous? It's a four wheeled robot. Yeah, it communicates a certain sort of message with\n\n25:53.200 --> 25:59.520\n everything from safety to the possibility of what AI could bring to its current limitations,\n\n25:59.520 --> 26:04.000\n its current challenges, it's what's possible. Do you feel the burden of that almost like a\n\n26:04.000 --> 26:09.600\n communicator educator to the world about AI? We were just really trying to make people's\n\n26:09.600 --> 26:15.040\n lives easier with autonomy. But now that you mentioned it, I think it will be an eye opener\n\n26:15.040 --> 26:19.920\n to people about robotics, because they've really never seen most people never seen a robot. And\n\n26:20.960 --> 26:25.440\n there are hundreds of thousands of Tesla's won't be long before there's a million of them that\n\n26:25.440 --> 26:31.760\n have autonomous capability, and the drive without a person in it. And you can see the kind of\n\n26:31.760 --> 26:40.080\n evolution of the car's personality and, and thinking with each iteration of autopilot,\n\n26:40.080 --> 26:47.600\n you can see it's, it's uncertain about this, or it gets it, but now it's more certain. Now it's\n\n26:47.600 --> 26:53.200\n moving in a slightly different way. Like, I can tell immediately if a car is on Tesla autopilot,\n\n26:53.200 --> 26:56.880\n because it's got just little nuances of movement, it just moves in a slightly different way.\n\n26:58.720 --> 27:02.960\n Cars on Tesla autopilot, for example, on the highway are far more precise about being in the\n\n27:02.960 --> 27:08.960\n center of the lane than a person. If you drive down the highway and look at how at where cars\n\n27:08.960 --> 27:13.840\n are, the human driven cars are within their lane, they're like bumper cars. They're like moving all\n\n27:13.840 --> 27:20.720\n over the place. The car in autopilot, dead center. Yeah, so the incredible work that's going into\n\n27:20.720 --> 27:27.040\n that neural network, it's learning fast. Autonomy is still very, very hard. We don't actually know\n\n27:27.040 --> 27:33.840\n how hard it is fully, of course. You look at the most problems you tackle, this one included,\n\n27:34.880 --> 27:39.520\n with an exponential lens, but even with an exponential improvement, things can take longer\n\n27:39.520 --> 27:47.840\n than expected sometimes. So where does Tesla currently stand on its quest for full autonomy?\n\n27:47.840 --> 27:54.720\n What's your sense? When can we see successful deployment of full autonomy?\n\n27:55.840 --> 28:00.160\n Well, on the highway already, the the probability of intervention is extremely low.\n\n28:00.160 --> 28:08.480\n Yes. So for highway autonomy, with the latest release, especially the probability of needing\n\n28:08.480 --> 28:13.200\n to intervene is really quite low. In fact, I'd say for stop and go traffic,\n\n28:13.200 --> 28:18.880\n it's far safer than a person right now. The probability of an injury or impact is much,\n\n28:18.880 --> 28:25.360\n much lower for autopilot than a person. And then with navigating autopilot, you can change lanes,\n\n28:25.360 --> 28:30.320\n take highway interchanges, and then we're coming at it from the other direction, which is low speed,\n\n28:30.320 --> 28:35.920\n full autonomy. And in a way, this is like, how does a person learn to drive? You learn to drive\n\n28:35.920 --> 28:40.720\n in the parking lot. You know, the first time you learn to drive probably wasn't jumping on\n\n28:40.720 --> 28:45.200\n August Street in San Francisco. That'd be crazy. You learn to drive in the parking lot, get things\n\n28:45.200 --> 28:52.400\n get things right at low speed. And then the missing piece that we're working on is traffic\n\n28:52.400 --> 28:59.200\n lights and stop streets. Stop streets, I would say actually also relatively easy, because, you know,\n\n28:59.200 --> 29:04.320\n you kind of know where the stop street is, worst case in geocoders, and then use visualization to\n\n29:04.320 --> 29:10.720\n see where the line is and stop at the line to eliminate the GPS error. So actually, I'd say it's\n\n29:10.720 --> 29:19.680\n probably complex traffic lights and very windy roads are the two things that need to get solved.\n\n29:19.680 --> 29:24.000\n What's harder, perception or control for these problems? So being able to perfectly perceive\n\n29:24.000 --> 29:29.600\n everything, or figuring out a plan once you perceive everything, how to interact with all the\n\n29:29.600 --> 29:35.440\n agents in the environment in your sense, from a learning perspective, is perception or action\n\n29:35.440 --> 29:42.240\n harder? And that giant, beautiful multitask learning neural network, the hottest thing is\n\n29:42.240 --> 29:48.960\n having accurate representation of the physical objects in vector space. So transfer taking the\n\n29:48.960 --> 29:56.880\n visual input, primarily visual input, some sonar and radar, and and then creating the an accurate\n\n29:56.880 --> 30:02.400\n vector space representation of the objects around you. Once you have an accurate vector space\n\n30:02.400 --> 30:08.160\n representation, the planning and control is relatively easier. That is relatively easy.\n\n30:08.160 --> 30:14.560\n Basically, once you have accurate vector space representation, then you're kind of like a video\n\n30:14.560 --> 30:19.600\n game, like cars and like Grand Theft Auto or something like they work pretty well. They drive\n\n30:19.600 --> 30:24.160\n down the road, they don't crash, you know, pretty much unless you crash into them. That's because\n\n30:24.160 --> 30:27.360\n they've they've got an accurate vector space representation of where the cars are, and they're\n\n30:27.360 --> 30:33.520\n just and then they're rendering that as the as the output. Do you have a sense, high level, that\n\n30:33.520 --> 30:42.000\n Tesla's on track on being able to achieve full autonomy? So on the highway? Yeah, absolutely.\n\n30:42.000 --> 30:48.320\n And still no driver state, driver sensing? And we have driver sensing with torque on the wheel.\n\n30:48.320 --> 30:55.120\n That's right. Yeah. By the way, just a quick comment on karaoke. Most people think it's fun,\n\n30:55.120 --> 30:59.040\n but I also think it is a driving feature. I've been saying for a long time, singing in the car\n\n30:59.040 --> 31:02.720\n is really good for attention management and vigilance management. That's right.\n\n31:02.720 --> 31:08.480\n Tesla karaoke is great. It's one of the most fun features of the car. Do you think of a connection\n\n31:08.480 --> 31:12.640\n between fun and safety sometimes? Yeah, you can do both at the same time. That's great.\n\n31:12.640 --> 31:19.760\n I just met with Andrew and wife of Carl Sagan, directed Cosmos. I'm generally a big fan of Carl\n\n31:19.760 --> 31:25.280\n Sagan. He's super cool. And had a great way of putting things. All of our consciousness,\n\n31:25.280 --> 31:29.360\n all civilization, everything we've ever known and done is on this tiny blue dot.\n\n31:29.920 --> 31:34.720\n People also get they get too trapped in there. This is like squabbles amongst humans.\n\n31:34.720 --> 31:39.680\n Let's not think of the big picture. They take civilization and our continued existence for\n\n31:39.680 --> 31:47.120\n granted. I shouldn't do that. Look at the history of civilizations. They rise and they fall. And now\n\n31:47.760 --> 31:56.480\n civilization is all it's globalized. And so civilization, I think now rises and falls together.\n\n31:56.480 --> 32:05.120\n There's no there's not geographic isolation. This is a big risk. Things don't always go up. That\n\n32:05.120 --> 32:12.720\n should be that's an important lesson of history. In 1990, at the request of Carl Sagan, the Voyager\n\n32:12.720 --> 32:18.560\n One spacecraft, which is a spacecraft that's reaching out farther than anything human made\n\n32:18.560 --> 32:24.720\n into space, turned around to take a picture of Earth from 3.6 billion years ago. And that's\n\n32:24.720 --> 32:31.520\n a picture of Earth from 3.7 billion miles away. And as you're talking about the pale blue dot,\n\n32:31.520 --> 32:37.600\n that picture there takes up less than a single pixel in that image. Yes. Appearing as a tiny\n\n32:37.600 --> 32:46.640\n blue dot, as a pale blue dot, as Carl Sagan called it. So he spoke about this dot of ours in 1994.\n\n32:46.640 --> 32:54.160\n And if you could humor me, I was wondering if in the last two minutes you could read the words\n\n32:54.160 --> 33:01.520\n that he wrote describing this pale blue dot. Sure. Yes, it's funny. The universe appears to be 13.8\n\n33:01.520 --> 33:07.920\n billion years old. Earth is like four and a half billion years old.\n\n33:07.920 --> 33:14.320\n In another half billion years or so, the sun will expand and probably evaporate the oceans and make\n\n33:14.320 --> 33:19.200\n life impossible on Earth, which means that if it had taken consciousness 10% longer to evolve,\n\n33:19.200 --> 33:29.680\n it would never have evolved at all. It's 10% longer. And I wonder how many dead one planet\n\n33:29.680 --> 33:31.520\n civilizations there are out there in the cosmos.\n\n33:31.520 --> 33:35.200\n That never made it to the other planet and ultimately extinguished themselves or were destroyed\n\n33:35.200 --> 33:46.080\n by external factors. Probably a few. It's only just possible to travel to Mars. Just barely.\n\n33:46.640 --> 33:50.080\n If G was 10% more, it wouldn't work really.\n\n33:50.080 --> 34:00.240\n If G was 10% lower, it would be easy. Like you can go single stage from the surface of Mars all the\n\n34:00.240 --> 34:08.240\n way to the surface of the Earth. Because Mars is 37% Earth's gravity. We need a giant booster\n\n34:08.240 --> 34:24.240\n to get off the Earth. Channeling Carl Sagan. Look again at that dot. That's here. That's home. That's us.\n\n34:25.360 --> 34:30.960\n On it, everyone you love, everyone you know, everyone you've ever heard of, every human being\n\n34:30.960 --> 34:37.600\n who ever was, lived out their lives. The aggregate of our joy and suffering, thousands of confident\n\n34:37.600 --> 34:42.960\n religions, ideologies and economic doctrines. Every hunter and farger, every hero and coward,\n\n34:42.960 --> 34:49.120\n every creator and destroyer of civilization, every king and peasant, every young couple in love,\n\n34:49.840 --> 34:57.760\n every mother and father, hopeful child, inventor and explorer, every teacher of morals, every\n\n34:57.760 --> 35:06.400\n corrupt politician, every superstar, every supreme leader, every saint and sinner in the history of\n\n35:06.400 --> 35:13.840\n our species lived there on a mode of dust suspended in a sunbeam. Our planet is a lonely speck in the\n\n35:13.840 --> 35:20.640\n great enveloping cosmic dark. In our obscurity, in all this vastness, there is no hint that help\n\n35:20.640 --> 35:25.680\n will come from elsewhere to save us from ourselves. The Earth is the only world known so far to harbor\n\n35:25.680 --> 35:32.080\n life. There is nowhere else, at least in the near future, to which our species could migrate. This\n\n35:32.080 --> 35:39.840\n is not true. This is false. Mars. And I think Carl Sagan would agree with that. He couldn't even\n\n35:39.840 --> 35:45.760\n imagine it at that time. So thank you for making the world dream. And thank you for talking today.\n\n35:45.760 --> 35:59.520\n I really appreciate it. Thank you.\n\n"
}