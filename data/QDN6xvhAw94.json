{
  "title": "Kevin Scott: Microsoft CTO | Lex Fridman Podcast #30",
  "id": "QDN6xvhAw94",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:03.440\n The following is a conversation with Kevin Scott,\n\n00:03.440 --> 00:06.080\n the CTO of Microsoft.\n\n00:06.080 --> 00:08.540\n Before that, he was the senior vice president\n\n00:08.540 --> 00:11.080\n of engineering and operations at LinkedIn.\n\n00:11.080 --> 00:14.160\n And before that, he oversaw mobile ads engineering\n\n00:14.160 --> 00:15.000\n at Google.\n\n00:15.960 --> 00:18.960\n He also has a podcast called Behind the Tech\n\n00:18.960 --> 00:21.860\n with Kevin Scott, which I'm a fan of.\n\n00:21.860 --> 00:24.240\n This was a fun and wide ranging conversation\n\n00:24.240 --> 00:26.680\n that covered many aspects of computing.\n\n00:26.680 --> 00:28.800\n It happened over a month ago,\n\n00:28.800 --> 00:30.960\n before the announcement of Microsoft's investment\n\n00:30.960 --> 00:34.400\n in OpenAI that a few people have asked me about.\n\n00:34.400 --> 00:38.080\n I'm sure there'll be one or two people in the future\n\n00:38.080 --> 00:41.360\n that'll talk with me about the impact of that investment.\n\n00:42.240 --> 00:45.400\n This is the Artificial Intelligence Podcast.\n\n00:45.400 --> 00:47.640\n If you enjoy it, subscribe on YouTube,\n\n00:47.640 --> 00:49.400\n give it five stars on iTunes,\n\n00:49.400 --> 00:50.920\n support it on Patreon,\n\n00:50.920 --> 00:54.240\n or simply connect with me on Twitter at Lex Friedman,\n\n00:54.240 --> 00:57.640\n spelled F R I D M A N.\n\n00:57.640 --> 00:59.200\n And I'd like to give a special thank you\n\n00:59.200 --> 01:01.920\n to Tom and Nelante Bighousen\n\n01:01.920 --> 01:04.560\n for their support of the podcast on Patreon.\n\n01:04.560 --> 01:06.040\n Thanks Tom and Nelante.\n\n01:06.040 --> 01:08.340\n Hope I didn't mess up your last name too bad.\n\n01:08.340 --> 01:10.480\n Your support means a lot\n\n01:10.480 --> 01:13.440\n and inspires me to keep this series going.\n\n01:13.440 --> 01:18.100\n And now, here's my conversation with Kevin Scott.\n\n01:18.100 --> 01:20.680\n You've described yourself as a kid in a candy store\n\n01:20.680 --> 01:22.920\n at Microsoft because of all the interesting projects\n\n01:22.920 --> 01:24.160\n that are going on.\n\n01:24.160 --> 01:27.920\n Can you try to do the impossible task\n\n01:27.920 --> 01:31.720\n and give a brief whirlwind view\n\n01:31.720 --> 01:34.500\n of all the spaces that Microsoft is working in?\n\n01:34.500 --> 01:37.400\n Both research and product?\n\n01:37.400 --> 01:38.360\n If you include research,\n\n01:38.360 --> 01:42.240\n it becomes even more difficult.\n\n01:46.840 --> 01:48.800\n I think broadly speaking,\n\n01:48.800 --> 01:53.680\n Microsoft's product portfolio includes everything\n\n01:53.680 --> 01:56.880\n from big cloud business,\n\n01:56.880 --> 01:59.320\n like a big set of SaaS services.\n\n01:59.320 --> 02:01.920\n We have sort of the original,\n\n02:01.920 --> 02:05.520\n or like some of what are among the original\n\n02:05.520 --> 02:09.560\n productivity software products that everybody uses.\n\n02:09.560 --> 02:11.160\n We have an operating system business.\n\n02:11.160 --> 02:14.540\n We have a hardware business where we make everything\n\n02:14.540 --> 02:18.400\n from computer mice and headphones\n\n02:18.400 --> 02:23.400\n to high end personal computers and laptops.\n\n02:23.400 --> 02:27.640\n We have a fairly broad ranging research group\n\n02:27.640 --> 02:29.640\n where we have people doing everything\n\n02:29.640 --> 02:31.840\n from economics research.\n\n02:31.840 --> 02:35.880\n So there's this really, really smart young economist,\n\n02:35.880 --> 02:39.720\n Glenn Weil, who my group works with a lot,\n\n02:39.720 --> 02:42.840\n who's doing this research on these things\n\n02:42.840 --> 02:45.120\n called radical markets.\n\n02:45.120 --> 02:48.080\n He's written an entire technical book\n\n02:48.080 --> 02:51.080\n about this whole notion of radical markets.\n\n02:51.080 --> 02:53.480\n So like the research group sort of spans from that\n\n02:53.480 --> 02:56.800\n to human computer interaction to artificial intelligence.\n\n02:56.800 --> 03:01.000\n And we have GitHub, we have LinkedIn,\n\n03:01.000 --> 03:05.760\n we have a search advertising and news business\n\n03:05.760 --> 03:07.320\n and like probably a bunch of stuff\n\n03:07.320 --> 03:11.240\n that I'm embarrassingly not recounting in this list.\n\n03:11.240 --> 03:12.840\n Gaming to Xbox and so on, right?\n\n03:12.840 --> 03:14.080\n Yeah, gaming for sure.\n\n03:14.080 --> 03:17.880\n Like I was having a super fun conversation this morning\n\n03:17.880 --> 03:19.480\n with Phil Spencer.\n\n03:19.480 --> 03:21.260\n So when I was in college,\n\n03:21.260 --> 03:25.560\n there was this game that LucasArts made\n\n03:25.560 --> 03:27.600\n called Day of the Tentacle\n\n03:27.600 --> 03:30.160\n that my friends and I played forever.\n\n03:30.160 --> 03:33.920\n And like we're doing some interesting collaboration now\n\n03:33.920 --> 03:37.920\n with the folks who made Day of the Tentacle.\n\n03:37.920 --> 03:40.840\n And I was like completely nerding out with Tim Schafer,\n\n03:40.840 --> 03:43.880\n like the guy who wrote a Day of the Tentacle this morning,\n\n03:43.880 --> 03:45.800\n just a complete fan boy,\n\n03:45.800 --> 03:49.880\n which sort of it like happens a lot.\n\n03:49.880 --> 03:53.320\n Like Microsoft has been doing so much stuff\n\n03:53.320 --> 03:56.000\n at such breadth for such a long period of time\n\n03:56.000 --> 04:00.880\n that like being CTO like most of the time,\n\n04:00.880 --> 04:02.200\n my job is very, very serious.\n\n04:02.200 --> 04:05.620\n And sometimes like I get caught up\n\n04:05.620 --> 04:10.620\n in like how amazing it is to be able to have\n\n04:10.620 --> 04:12.800\n the conversations that I have with the people\n\n04:12.800 --> 04:14.640\n I get to have them with.\n\n04:14.640 --> 04:17.080\n Yeah, to reach back into the sentimental.\n\n04:17.080 --> 04:21.640\n And what's the radical markets and the economics?\n\n04:21.640 --> 04:24.760\n So the idea with radical markets is like,\n\n04:24.760 --> 04:29.760\n can you come up with new market based mechanisms to,\n\n04:32.320 --> 04:33.840\n you know, I think we have this,\n\n04:33.840 --> 04:35.240\n we're having this debate right now,\n\n04:35.240 --> 04:40.040\n like does capitalism work like free markets work?\n\n04:40.040 --> 04:43.000\n Can the incentive structures\n\n04:43.000 --> 04:46.320\n that are built into these systems produce outcomes\n\n04:46.320 --> 04:51.320\n that are creating sort of equitably distributed benefits\n\n04:51.520 --> 04:53.520\n for every member of society?\n\n04:55.360 --> 04:56.960\n You know, and I think it's a reasonable,\n\n04:56.960 --> 04:59.520\n reasonable set of questions to be asking.\n\n04:59.520 --> 05:02.120\n And so what Glenn, and so like, you know,\n\n05:02.120 --> 05:03.120\n one mode of thought there,\n\n05:03.120 --> 05:05.920\n like if you have doubts that the markets\n\n05:05.920 --> 05:08.360\n are actually working, you can sort of like tip towards\n\n05:08.360 --> 05:10.760\n like, okay, let's become more socialist\n\n05:10.760 --> 05:13.640\n and, you know, like have central planning and, you know,\n\n05:13.640 --> 05:15.760\n governments or some other central organization\n\n05:15.760 --> 05:18.240\n is like making a bunch of decisions\n\n05:18.240 --> 05:22.000\n about how, you know, sort of work gets done\n\n05:22.000 --> 05:24.520\n and, you know, like where the, you know,\n\n05:24.520 --> 05:26.360\n where the investments and where the outputs\n\n05:26.360 --> 05:28.840\n of those investments get distributed.\n\n05:28.840 --> 05:32.120\n Glenn's notion is like, lean more\n\n05:32.120 --> 05:35.760\n into like the market based mechanism.\n\n05:35.760 --> 05:37.840\n So like, for instance, you know,\n\n05:37.840 --> 05:39.540\n this is one of the more radical ideas,\n\n05:39.540 --> 05:44.540\n like suppose that you had a radical pricing mechanism\n\n05:45.140 --> 05:50.140\n for assets like real estate where you were,\n\n05:50.560 --> 05:53.560\n you could be bid out of your position\n\n05:53.560 --> 05:58.560\n in your home, you know, for instance.\n\n05:58.680 --> 06:01.080\n So like if somebody came along and said,\n\n06:01.080 --> 06:04.380\n you know, like I can find higher economic utility\n\n06:04.380 --> 06:05.720\n for this piece of real estate\n\n06:05.720 --> 06:08.680\n that you're running your business in,\n\n06:08.680 --> 06:13.040\n like then like you either have to, you know,\n\n06:13.040 --> 06:16.480\n sort of bid to sort of stay\n\n06:16.480 --> 06:19.960\n or like the thing that's got the higher economic utility,\n\n06:19.960 --> 06:21.480\n you know, sort of takes over the asset\n\n06:21.480 --> 06:23.700\n which would make it very difficult\n\n06:23.700 --> 06:27.580\n to have the same sort of rent seeking behaviors\n\n06:27.580 --> 06:29.000\n that you've got right now\n\n06:29.000 --> 06:34.000\n because like if you did speculative bidding,\n\n06:34.000 --> 06:39.000\n like you would very quickly like lose a whole lot of money.\n\n06:40.440 --> 06:42.380\n And so like the prices of the assets\n\n06:42.380 --> 06:45.640\n would be sort of like very closely indexed\n\n06:45.640 --> 06:49.720\n to like the value that they could produce.\n\n06:49.720 --> 06:52.120\n And like, because like you'd have this sort\n\n06:52.120 --> 06:53.940\n of real time mechanism that would force you\n\n06:53.940 --> 06:56.800\n to sort of mark the value of the asset to the market,\n\n06:56.800 --> 06:58.560\n then it could be taxed appropriately.\n\n06:58.560 --> 07:00.400\n Like you couldn't sort of sit on this thing and say,\n\n07:00.400 --> 07:03.040\n oh, like this house is only worth 10,000 bucks\n\n07:03.040 --> 07:06.620\n when like everything around it is worth 10 million.\n\n07:06.620 --> 07:08.720\n That's really, so it's an incentive structure\n\n07:08.720 --> 07:13.160\n that where the prices match the value much better.\n\n07:13.160 --> 07:16.360\n Yeah, and Glenn does a much better job than I do\n\n07:16.360 --> 07:18.960\n at selling and I probably picked the world's worst example,\n\n07:18.960 --> 07:23.960\n you know, and it's intentionally provocative,\n\n07:24.560 --> 07:25.800\n so like this whole notion,\n\n07:25.800 --> 07:28.920\n like I'm not sure whether I like this notion\n\n07:28.920 --> 07:31.120\n that like we can have a set of market mechanisms\n\n07:31.120 --> 07:35.360\n where I could get bid out of my property, you know,\n\n07:35.360 --> 07:37.680\n but you know, like if you're thinking about something\n\n07:37.680 --> 07:42.480\n like Elizabeth Warren's wealth tax, for instance,\n\n07:42.480 --> 07:45.600\n like you would have, I mean, it'd be really interesting\n\n07:45.600 --> 07:50.100\n in like how you would actually set the price on the assets\n\n07:50.100 --> 07:52.040\n and like you might have to have a mechanism like that\n\n07:52.040 --> 07:54.160\n if you put a tax like that in place.\n\n07:54.160 --> 07:56.440\n It's really interesting that that kind of research,\n\n07:56.440 --> 08:00.280\n at least tangentially is touching Microsoft research.\n\n08:00.280 --> 08:02.560\n That you're really thinking broadly.\n\n08:02.560 --> 08:07.560\n Maybe you can speak to, this connects to AI,\n\n08:08.360 --> 08:10.640\n so we have a candidate, Andrew Yang,\n\n08:10.640 --> 08:13.440\n who kind of talks about artificial intelligence\n\n08:13.440 --> 08:16.620\n and the concern that people have about, you know,\n\n08:16.620 --> 08:19.920\n automation's impact on society and arguably,\n\n08:19.920 --> 08:23.340\n Microsoft is at the cutting edge of innovation\n\n08:23.340 --> 08:27.040\n in all these kinds of ways and so it's pushing AI forward.\n\n08:27.040 --> 08:30.000\n How do you think about combining all our conversations\n\n08:30.000 --> 08:32.840\n together here with radical markets and socialism\n\n08:32.840 --> 08:37.500\n and innovation in AI that Microsoft is doing\n\n08:37.500 --> 08:42.500\n and then Andrew Yang's worry that that will result\n\n08:44.560 --> 08:46.840\n in job loss for the lower and so on.\n\n08:46.840 --> 08:47.680\n How do you think about that?\n\n08:47.680 --> 08:51.140\n I think it's sort of one of the most important questions\n\n08:51.140 --> 08:54.920\n in technology like maybe even in society right now\n\n08:54.920 --> 08:59.640\n about how is AI going to develop\n\n08:59.640 --> 09:01.960\n over the course of the next several decades\n\n09:01.960 --> 09:03.600\n and what's it going to be used for\n\n09:03.600 --> 09:06.520\n and what benefits will it produce\n\n09:06.520 --> 09:08.480\n and what negative impacts will it produce\n\n09:08.480 --> 09:12.940\n and who gets to steer this whole thing.\n\n09:13.960 --> 09:16.240\n I'll say at the highest level,\n\n09:17.240 --> 09:22.240\n one of the real joys of getting to do what I do at Microsoft\n\n09:22.920 --> 09:27.560\n is Microsoft has this heritage as a platform company\n\n09:27.560 --> 09:32.560\n and so Bill has this thing that he said a bunch of years ago\n\n09:32.880 --> 09:36.440\n where the measure of a successful platform\n\n09:36.440 --> 09:39.800\n is that it produces far more economic value\n\n09:39.800 --> 09:41.820\n for the people who build on top of the platform\n\n09:41.820 --> 09:46.820\n than is created for the platform owner or builder\n\n09:47.320 --> 09:51.160\n and I think we have to think about AI that way.\n\n09:51.160 --> 09:52.240\n As a platform.\n\n09:52.240 --> 09:56.260\n Yeah, it has to be a platform that other people can use\n\n09:56.260 --> 10:01.260\n to build businesses, to fulfill their creative objectives,\n\n10:01.280 --> 10:04.640\n to be entrepreneurs, to solve problems that they have\n\n10:04.640 --> 10:07.680\n in their work and in their lives.\n\n10:07.680 --> 10:11.960\n It can't be a thing where there are a handful of companies\n\n10:11.960 --> 10:16.440\n sitting in a very small handful of cities geographically\n\n10:16.440 --> 10:21.440\n who are making all the decisions about what goes into the AI\n\n10:21.440 --> 10:26.440\n and then on top of all this infrastructure,\n\n10:26.880 --> 10:30.960\n then build all of the commercially valuable uses for it.\n\n10:30.960 --> 10:35.960\n So I think that's bad from a sort of economics\n\n10:36.480 --> 10:40.120\n and sort of equitable distribution of value perspective,\n\n10:40.120 --> 10:44.520\n sort of back to this whole notion of did the markets work?\n\n10:44.520 --> 10:47.560\n But I think it's also bad from an innovation perspective\n\n10:47.560 --> 10:51.360\n because I have infinite amounts of faith\n\n10:51.360 --> 10:55.720\n in human beings that if you give folks powerful tools,\n\n10:55.720 --> 10:58.240\n they will go do interesting things\n\n10:58.240 --> 11:02.280\n and it's more than just a few tens of thousands of people\n\n11:02.280 --> 11:03.340\n with the interesting tools,\n\n11:03.340 --> 11:05.380\n it should be millions of people with the tools.\n\n11:05.380 --> 11:10.160\n So it's sort of like you think about the steam engine\n\n11:10.160 --> 11:14.480\n in the late 18th century, like it was maybe the first\n\n11:14.480 --> 11:16.760\n large scale substitute for human labor\n\n11:16.760 --> 11:19.080\n that we've built like a machine\n\n11:19.080 --> 11:23.480\n and in the beginning when these things are getting deployed,\n\n11:23.480 --> 11:28.280\n the folks who got most of the value from the steam engines\n\n11:28.280 --> 11:30.140\n were the folks who had capital\n\n11:30.140 --> 11:31.560\n so they could afford to build them\n\n11:31.560 --> 11:34.680\n and like they built factories around them and businesses\n\n11:34.680 --> 11:38.640\n and the experts who knew how to build and maintain them.\n\n11:38.640 --> 11:42.840\n But access to that technology democratized over time.\n\n11:42.840 --> 11:47.840\n Like now, like an engine, it's not like a differentiated\n\n11:47.840 --> 11:50.260\n thing, like there isn't one engine company\n\n11:50.260 --> 11:51.500\n that builds all the engines\n\n11:51.500 --> 11:53.100\n and all of the things that use engines\n\n11:53.100 --> 11:54.220\n are made by this company\n\n11:54.220 --> 11:57.420\n and like they get all the economics from all of that.\n\n11:57.420 --> 12:00.540\n Like fully demarcated, like they're probably,\n\n12:00.540 --> 12:02.300\n we're sitting here in this room\n\n12:02.300 --> 12:05.220\n and like even though they're probably things\n\n12:05.220 --> 12:09.100\n like the MEMS gyroscope that are in both of our phones,\n\n12:09.100 --> 12:13.220\n like there's like little engines sort of everywhere.\n\n12:13.220 --> 12:16.260\n They're just a component in how we build the modern world.\n\n12:16.260 --> 12:17.700\n Like AI needs to get there.\n\n12:17.700 --> 12:20.220\n Yeah, so that's a really powerful way to think.\n\n12:20.220 --> 12:22.700\n If we think of AI as a platform\n\n12:22.700 --> 12:26.860\n versus a tool that Microsoft owns,\n\n12:26.860 --> 12:30.140\n as a platform that enables creation on top of it,\n\n12:30.140 --> 12:31.500\n that's the way to democratize it.\n\n12:31.500 --> 12:34.220\n That's really interesting actually.\n\n12:34.220 --> 12:36.060\n And Microsoft throughout its history\n\n12:36.060 --> 12:38.260\n has been positioned well to do that.\n\n12:38.260 --> 12:41.660\n And the tie back to this radical markets thing,\n\n12:41.660 --> 12:49.100\n like so my team has been working with Glenn on this,\n\n12:49.100 --> 12:50.940\n and Jaren Lanier actually.\n\n12:50.940 --> 12:56.180\n So Jaren is the sort of father of virtual reality.\n\n12:56.180 --> 13:00.100\n Like he's one of the most interesting human beings on the planet,\n\n13:00.100 --> 13:02.220\n like a sweet, sweet guy.\n\n13:02.220 --> 13:07.660\n And so Jaren and Glenn and folks in my team have been working\n\n13:07.660 --> 13:10.300\n on this notion of data as labor\n\n13:10.300 --> 13:13.100\n or like they call it data dignity as well.\n\n13:13.100 --> 13:16.700\n And so the idea is that if you,\n\n13:16.700 --> 13:20.580\n again going back to this sort of industrial analogy,\n\n13:20.580 --> 13:24.700\n if you think about data as the raw material that is\n\n13:24.700 --> 13:30.060\n consumed by the machine of AI in order to do useful things,\n\n13:30.060 --> 13:34.940\n then like we're not doing a really great job right now in having\n\n13:34.940 --> 13:39.580\n transparent marketplaces for valuing those data contributions.\n\n13:39.580 --> 13:43.540\n So and we all make them explicitly like you go to LinkedIn,\n\n13:43.540 --> 13:46.140\n you sort of set up your profile on LinkedIn,\n\n13:46.140 --> 13:47.780\n like that's an explicit contribution.\n\n13:47.780 --> 13:49.460\n Like you know exactly the information\n\n13:49.460 --> 13:50.700\n that you're putting into the system.\n\n13:50.700 --> 13:52.780\n And like you put it there because you have\n\n13:52.780 --> 13:56.620\n some nominal notion of what value you're going to get in return.\n\n13:56.620 --> 13:57.700\n But it's like only nominal,\n\n13:57.700 --> 14:00.460\n like you don't know exactly what value you're getting in return.\n\n14:00.460 --> 14:01.860\n Like service is free,\n\n14:01.860 --> 14:04.620\n like it's low amount of perceived debt.\n\n14:04.620 --> 14:06.900\n And then you've got all this indirect contribution that you're\n\n14:06.900 --> 14:09.540\n making just by virtue of interacting with all of\n\n14:09.540 --> 14:13.180\n the technology that's in your daily life.\n\n14:13.180 --> 14:15.580\n And so like what Glenn and\n\n14:15.580 --> 14:19.340\n Jaren and this data dignity team are trying to do is like,\n\n14:19.340 --> 14:23.820\n can we figure out a set of mechanisms that let us value\n\n14:23.820 --> 14:27.260\n those data contributions so that you could create\n\n14:27.260 --> 14:31.700\n an economy and like a set of controls and incentives that\n\n14:31.700 --> 14:36.860\n would allow people to like maybe even in the limit,\n\n14:36.860 --> 14:38.860\n like earn part of their living\n\n14:38.860 --> 14:41.020\n through the data that they're creating.\n\n14:41.020 --> 14:42.660\n And like you can sort of see it in explicit ways.\n\n14:42.660 --> 14:46.020\n There are these companies like Scale AI,\n\n14:46.020 --> 14:49.420\n and like there are a whole bunch of them in China\n\n14:49.420 --> 14:52.380\n right now that are basically data labeling companies.\n\n14:52.380 --> 14:54.540\n So like you're doing supervised machine learning,\n\n14:54.540 --> 14:57.900\n you need lots and lots of label training data.\n\n14:57.900 --> 15:01.540\n And like those people who work for\n\n15:01.540 --> 15:03.460\n those companies are getting compensated\n\n15:03.460 --> 15:06.180\n for their data contributions into the system.\n\n15:06.180 --> 15:07.380\n And so.\n\n15:07.380 --> 15:09.500\n That's easier to put a number on\n\n15:09.500 --> 15:11.980\n their contribution because they're explicitly labeling data.\n\n15:11.980 --> 15:12.380\n Correct.\n\n15:12.380 --> 15:13.620\n But you're saying that we're all\n\n15:13.620 --> 15:15.540\n contributing data in different kinds of ways.\n\n15:15.540 --> 15:18.260\n And it's fascinating to start to\n\n15:18.260 --> 15:20.860\n explicitly try to put a number on it.\n\n15:20.860 --> 15:22.580\n Do you think that's possible?\n\n15:22.580 --> 15:24.980\n I don't know. It's hard. It really is.\n\n15:24.980 --> 15:29.420\n Because we don't have\n\n15:29.420 --> 15:33.740\n as much transparency as I think\n\n15:33.740 --> 15:37.220\n we need in like how the data is getting used.\n\n15:37.220 --> 15:38.660\n And it's super complicated.\n\n15:38.660 --> 15:41.300\n Like we, I think as\n\n15:41.300 --> 15:42.860\n technologists sort of appreciate\n\n15:42.860 --> 15:44.140\n like some of the subtlety there.\n\n15:44.140 --> 15:48.820\n It's like the data gets created and then it gets,\n\n15:48.820 --> 15:50.940\n it's not valuable.\n\n15:50.940 --> 15:55.740\n Like the data exhaust that you give off,\n\n15:55.740 --> 16:00.580\n or the explicit data that I am putting into\n\n16:00.580 --> 16:05.100\n the system isn't super valuable atomically.\n\n16:05.100 --> 16:07.260\n Like it's only valuable when you sort of\n\n16:07.260 --> 16:10.420\n aggregate it together into sort of large numbers.\n\n16:10.420 --> 16:12.100\n This is true even for these like folks who are\n\n16:12.100 --> 16:14.860\n getting compensated for like labeling things.\n\n16:14.860 --> 16:16.460\n Like for supervised machine learning now,\n\n16:16.460 --> 16:18.860\n like you need lots of labels to\n\n16:18.860 --> 16:21.900\n train a model that performs well.\n\n16:21.900 --> 16:24.420\n And so I think that's one of the challenges.\n\n16:24.420 --> 16:27.220\n It's like how do you sort of figure\n\n16:27.220 --> 16:29.900\n out like because this data is getting combined in\n\n16:29.900 --> 16:32.620\n so many ways like through\n\n16:32.620 --> 16:35.700\n these combinations like how the value is flowing.\n\n16:35.700 --> 16:37.620\n Yeah, that's fascinating.\n\n16:37.620 --> 16:41.860\n Yeah. And it's fascinating that you're thinking about this.\n\n16:41.860 --> 16:44.980\n And I wasn't even going into this conversation expecting\n\n16:44.980 --> 16:48.180\n the breadth of research really\n\n16:48.180 --> 16:50.580\n that Microsoft broadly is thinking about,\n\n16:50.580 --> 16:52.100\n you're thinking about at Microsoft.\n\n16:52.100 --> 16:57.580\n So if we go back to 89 when Microsoft released Office,\n\n16:57.580 --> 17:01.900\n or 1990 when they released Windows 3.0.\n\n17:02.740 --> 17:07.980\n In your view, I know you weren't there through its history,\n\n17:07.980 --> 17:09.940\n but how has the company changed in\n\n17:09.940 --> 17:12.580\n the 30 years since as you look at it now?\n\n17:12.580 --> 17:16.900\n The good thing is it's started off as a platform company.\n\n17:16.900 --> 17:20.020\n Like it's still a platform company,\n\n17:20.020 --> 17:22.940\n like the parts of the business that are thriving and\n\n17:22.940 --> 17:26.660\n most successful are those that are building platforms.\n\n17:26.660 --> 17:28.980\n Like the mission of the company now is,\n\n17:28.980 --> 17:30.220\n the mission's changed.\n\n17:30.220 --> 17:32.380\n It's like changed in a very interesting way.\n\n17:32.380 --> 17:35.860\n So back in 89,\n\n17:35.860 --> 17:39.100\n 90 like they were still on the original mission,\n\n17:39.100 --> 17:43.940\n which was like put a PC on every desk and in every home.\n\n17:43.940 --> 17:47.700\n And it was basically about democratizing access to\n\n17:47.700 --> 17:50.140\n this new personal computing technology,\n\n17:50.140 --> 17:52.540\n which when Bill started the company,\n\n17:52.540 --> 17:57.740\n integrated circuit microprocessors were a brand new thing.\n\n17:57.740 --> 18:03.900\n And people were building homebrew computers from kits,\n\n18:03.900 --> 18:08.140\n like the way people build ham radios right now.\n\n18:08.140 --> 18:10.700\n I think this is the interesting thing\n\n18:10.700 --> 18:12.860\n for folks who build platforms in general.\n\n18:12.860 --> 18:17.060\n Bill saw the opportunity there and\n\n18:17.060 --> 18:18.780\n what personal computers could do.\n\n18:18.780 --> 18:20.500\n And it was like, it was sort of a reach.\n\n18:20.500 --> 18:22.460\n Like you just sort of imagine like where things\n\n18:22.460 --> 18:24.900\n were when they started the company\n\n18:24.900 --> 18:26.100\n versus where things are now.\n\n18:26.100 --> 18:27.860\n Like in success,\n\n18:27.860 --> 18:29.400\n when you've democratized a platform,\n\n18:29.400 --> 18:31.020\n it just sort of vanishes into the platform.\n\n18:31.020 --> 18:32.500\n You don't pay attention to it anymore.\n\n18:32.500 --> 18:35.420\n Like operating systems aren't a thing anymore.\n\n18:35.420 --> 18:36.780\n Like they're super important,\n\n18:36.780 --> 18:38.020\n like completely critical.\n\n18:38.020 --> 18:41.460\n And like when you see one fail,\n\n18:41.460 --> 18:43.500\n like you just sort of understand.\n\n18:43.500 --> 18:46.060\n But like it's not a thing where you're not like\n\n18:46.060 --> 18:50.220\n waiting for the next operating system thing\n\n18:50.220 --> 18:52.860\n in the same way that you were in 1995, right?\n\n18:52.860 --> 18:54.500\n Like in 1995, like we had\n\n18:54.500 --> 18:57.580\n Rolling Stones on the stage with the Windows 95 rollout.\n\n18:57.580 --> 18:59.300\n Like it was like the biggest thing in the world.\n\n18:59.300 --> 19:01.340\n Everybody lined up for it the way\n\n19:01.340 --> 19:03.380\n that people used to line up for iPhone.\n\n19:03.380 --> 19:04.820\n But like, you know, eventually,\n\n19:04.820 --> 19:07.160\n and like this isn't necessarily a bad thing.\n\n19:07.160 --> 19:08.820\n Like it just sort of, you know,\n\n19:08.820 --> 19:12.860\n the success is that it's sort of, it becomes ubiquitous.\n\n19:12.860 --> 19:14.780\n It's like everywhere, like human beings,\n\n19:14.780 --> 19:16.580\n when their technology becomes ubiquitous,\n\n19:16.580 --> 19:18.180\n they just sort of start taking it for granted.\n\n19:18.180 --> 19:22.100\n So the mission now that Satya\n\n19:22.100 --> 19:25.220\n rearticulated five plus years ago now,\n\n19:25.220 --> 19:28.100\n when he took over as CEO of the company.\n\n19:28.260 --> 19:33.620\n Our mission is to empower every individual and\n\n19:33.620 --> 19:38.340\n every organization in the world to be more successful.\n\n19:38.340 --> 19:40.860\n And so, you know, again,\n\n19:40.860 --> 19:43.100\n like that's a platform mission.\n\n19:43.100 --> 19:46.300\n And like the way that we do it now is, is different.\n\n19:46.300 --> 19:48.780\n It's like we have a hyperscale cloud that\n\n19:48.780 --> 19:51.620\n people are building their applications on top of.\n\n19:51.620 --> 19:53.740\n Like we have a bunch of AI infrastructure that\n\n19:53.740 --> 19:56.220\n people are building their AI applications on top of.\n\n19:56.220 --> 19:58.100\n We have, you know,\n\n19:58.100 --> 20:02.060\n we have a productivity suite of software,\n\n20:02.060 --> 20:05.740\n like Microsoft Dynamics, which, you know,\n\n20:05.740 --> 20:07.820\n some people might not think is the sexiest thing in the world,\n\n20:07.820 --> 20:10.820\n but it's like helping people figure out how to automate\n\n20:10.820 --> 20:13.580\n all of their business processes and workflows\n\n20:13.580 --> 20:19.060\n and to help those businesses using it to grow and be more.\n\n20:19.060 --> 20:23.180\n So it's a much broader vision\n\n20:23.180 --> 20:25.460\n in a way now than it was back then.\n\n20:25.460 --> 20:27.380\n Like it was sort of very particular thing.\n\n20:27.380 --> 20:29.380\n And like now, like we live in this world where\n\n20:29.380 --> 20:32.380\n technology is so powerful and it's like\n\n20:32.380 --> 20:39.700\n such a basic fact of life that it both exists\n\n20:39.700 --> 20:42.700\n and is going to get better and better over time\n\n20:42.700 --> 20:45.980\n or at least more and more powerful over time.\n\n20:45.980 --> 20:48.140\n So like, you know, what you have to do as a platform player\n\n20:48.140 --> 20:49.900\n is just much bigger.\n\n20:49.900 --> 20:52.980\n Right. There's so many directions in which you can transform.\n\n20:52.980 --> 20:55.140\n You didn't mention mixed reality, too.\n\n20:55.140 --> 20:59.140\n You know, that's probably early days\n\n20:59.140 --> 21:00.620\n or it depends how you think of it.\n\n21:00.620 --> 21:02.140\n But if we think on a scale of centuries,\n\n21:02.140 --> 21:04.020\n it's the early days of mixed reality.\n\n21:04.020 --> 21:04.900\n Oh, for sure.\n\n21:04.900 --> 21:08.420\n And so with HoloLens,\n\n21:08.420 --> 21:10.580\n Microsoft is doing some really interesting work there.\n\n21:10.580 --> 21:13.540\n Do you touch that part of the effort?\n\n21:13.540 --> 21:14.820\n What's the thinking?\n\n21:14.820 --> 21:17.620\n Do you think of mixed reality as a platform, too?\n\n21:17.620 --> 21:18.460\n Oh, sure.\n\n21:18.460 --> 21:21.300\n When we look at what the platforms of the future could be,\n\n21:21.300 --> 21:23.900\n it's like fairly obvious that like AI is one.\n\n21:23.900 --> 21:26.580\n Like you don't have to, I mean, like that's,\n\n21:26.580 --> 21:29.140\n you know, you sort of say it to like someone\n\n21:29.140 --> 21:31.940\n and you know, like they get it.\n\n21:31.940 --> 21:36.300\n But like we also think of the like mixed reality\n\n21:36.300 --> 21:39.580\n and quantum as like these two interesting,\n\n21:39.580 --> 21:40.900\n you know, potentially.\n\n21:40.900 --> 21:41.820\n Quantum computing?\n\n21:41.820 --> 21:42.660\n Yeah.\n\n21:42.660 --> 21:44.500\n Okay. So let's get crazy then.\n\n21:44.500 --> 21:48.900\n So you're talking about some futuristic things here.\n\n21:48.900 --> 21:50.860\n Well, the mixed reality, Microsoft is really,\n\n21:50.860 --> 21:52.620\n it's not even futuristic, it's here.\n\n21:52.620 --> 21:53.460\n It is.\n\n21:53.460 --> 21:54.300\n It's incredible stuff.\n\n21:54.300 --> 21:56.660\n And look, and it's having an impact right now.\n\n21:56.660 --> 21:58.740\n Like one of the more interesting things\n\n21:58.740 --> 21:59.980\n that's happened with mixed reality\n\n21:59.980 --> 22:04.140\n over the past couple of years that I didn't clearly see\n\n22:04.140 --> 22:08.420\n is that it's become the computing device\n\n22:08.420 --> 22:13.180\n for folks who, for doing their work,\n\n22:13.180 --> 22:16.060\n who haven't used any computing device at all\n\n22:16.060 --> 22:16.980\n to do their work before.\n\n22:16.980 --> 22:21.500\n So technicians and service folks and people\n\n22:21.500 --> 22:25.340\n who are doing like machine maintenance on factory floors.\n\n22:25.340 --> 22:28.780\n So like they, you know, because they're mobile\n\n22:28.780 --> 22:30.300\n and like they're out in the world\n\n22:30.300 --> 22:32.340\n and they're working with their hands\n\n22:32.340 --> 22:34.260\n and, you know, sort of servicing these like\n\n22:34.260 --> 22:37.460\n very complicated things, they're,\n\n22:37.460 --> 22:39.420\n they don't use their mobile phone\n\n22:39.420 --> 22:41.420\n and like they don't carry a laptop with them\n\n22:41.420 --> 22:43.500\n and, you know, they're not tethered to a desk.\n\n22:43.500 --> 22:47.340\n And so mixed reality, like where it's getting traction\n\n22:47.340 --> 22:50.740\n right now, where HoloLens is selling a lot of units\n\n22:50.740 --> 22:54.580\n is for these sorts of applications for these workers.\n\n22:54.580 --> 22:58.060\n And it's become like, I mean, like the people love it.\n\n22:58.060 --> 23:01.140\n They're like, oh my God, like this is like for them,\n\n23:01.140 --> 23:03.460\n like the same sort of productivity boosts that,\n\n23:03.460 --> 23:05.500\n you know, like an office worker had\n\n23:05.500 --> 23:08.220\n when they got their first personal computer.\n\n23:08.220 --> 23:12.100\n Yeah, but you did mention it's certainly obvious AI\n\n23:12.100 --> 23:15.580\n as a platform, but can we dig into it a little bit?\n\n23:15.580 --> 23:18.300\n How does AI begin to infuse some of the products\n\n23:18.300 --> 23:19.500\n in Microsoft?\n\n23:19.500 --> 23:24.500\n So currently providing training of,\n\n23:24.500 --> 23:26.700\n for example, neural networks in the cloud\n\n23:26.700 --> 23:31.700\n or providing pre trained models or just even providing\n\n23:34.300 --> 23:37.540\n computing resources and whatever different inference\n\n23:37.540 --> 23:39.940\n that you wanna do using neural networks.\n\n23:39.940 --> 23:44.500\n How do you think of AI infusing as a platform\n\n23:44.500 --> 23:45.900\n that Microsoft can provide?\n\n23:45.900 --> 23:48.340\n Yeah, I mean, I think it's super interesting.\n\n23:48.340 --> 23:49.580\n It's like everywhere.\n\n23:49.580 --> 23:54.580\n And like we run these review meetings now\n\n23:54.580 --> 23:59.580\n where it's me and Satya and like members\n\n24:00.700 --> 24:04.340\n of Satya's leadership team and like a cross functional\n\n24:04.340 --> 24:06.180\n group of folks across the entire company\n\n24:06.180 --> 24:11.180\n who are working on like either AI infrastructure\n\n24:11.820 --> 24:16.820\n or like have some substantial part of their product work\n\n24:18.900 --> 24:22.580\n using AI in some significant way.\n\n24:22.580 --> 24:23.940\n Now, the important thing to understand\n\n24:23.940 --> 24:26.620\n is like when you think about like how the AI\n\n24:26.620 --> 24:29.980\n is gonna manifest in like an experience\n\n24:29.980 --> 24:31.820\n for something that's gonna make it better,\n\n24:31.820 --> 24:36.820\n like I think you don't want the AIness\n\n24:36.900 --> 24:38.780\n to be the first order thing.\n\n24:38.780 --> 24:40.900\n It's like whatever the product is\n\n24:40.900 --> 24:43.700\n and like the thing that is trying to help you do,\n\n24:43.700 --> 24:45.620\n like the AI just sort of makes it better.\n\n24:45.620 --> 24:47.900\n And this is a gross exaggeration,\n\n24:47.900 --> 24:51.580\n but like people get super excited about like\n\n24:51.580 --> 24:54.220\n where the AI is showing up in products and I'm like,\n\n24:54.220 --> 24:55.780\n do you get that excited about like\n\n24:55.780 --> 24:59.660\n where you're using a hash table like in your code?\n\n24:59.660 --> 25:01.100\n Like it's just another.\n\n25:01.100 --> 25:01.940\n It's just a tool.\n\n25:01.940 --> 25:03.780\n It's a very interesting programming tool,\n\n25:03.780 --> 25:06.180\n but it's sort of like it's an engineering tool.\n\n25:07.340 --> 25:09.340\n And so like it shows up everywhere.\n\n25:09.340 --> 25:12.300\n So like we've got dozens and dozens of features\n\n25:12.300 --> 25:15.660\n now in Office that are powered by\n\n25:15.660 --> 25:18.060\n like fairly sophisticated machine learning,\n\n25:18.060 --> 25:21.980\n our search engine wouldn't work at all\n\n25:21.980 --> 25:24.620\n if you took the machine learning out of it.\n\n25:24.620 --> 25:29.620\n The like increasingly things like content moderation\n\n25:30.860 --> 25:35.860\n on our Xbox and xCloud platform.\n\n25:36.820 --> 25:37.900\n When you mean moderation,\n\n25:37.900 --> 25:39.500\n you mean like the recommender is like showing\n\n25:39.500 --> 25:41.540\n what you wanna look at next.\n\n25:41.540 --> 25:43.780\n No, no, no, it's like anti bullying stuff.\n\n25:43.780 --> 25:45.780\n So the usual social network stuff\n\n25:45.780 --> 25:46.820\n that you have to deal with.\n\n25:46.820 --> 25:47.660\n Yeah, correct.\n\n25:47.660 --> 25:49.860\n But it's like really it's targeted,\n\n25:49.860 --> 25:52.060\n it's targeted towards a gaming audience.\n\n25:52.060 --> 25:54.580\n So it's like a very particular type of thing\n\n25:54.580 --> 25:59.260\n where the line between playful banter\n\n25:59.260 --> 26:02.100\n and like legitimate bullying is like a subtle one.\n\n26:02.100 --> 26:05.860\n And like you have to like, it's sort of tough.\n\n26:05.860 --> 26:07.340\n Like I have.\n\n26:07.340 --> 26:08.860\n I'd love to if we could dig into it\n\n26:08.860 --> 26:10.060\n because you're also,\n\n26:10.060 --> 26:12.980\n you led the engineering efforts of LinkedIn.\n\n26:12.980 --> 26:17.460\n And if we look at LinkedIn as a social network,\n\n26:17.460 --> 26:21.700\n and if we look at the Xbox gaming as the social components,\n\n26:21.700 --> 26:24.780\n the very different kinds of I imagine communication\n\n26:24.780 --> 26:26.740\n going on on the two platforms, right?\n\n26:26.740 --> 26:29.420\n And the line in terms of bullying and so on\n\n26:29.420 --> 26:31.420\n is different on the platforms.\n\n26:31.420 --> 26:33.140\n So how do you,\n\n26:33.140 --> 26:36.180\n I mean, it's such a fascinating philosophical discussion\n\n26:36.180 --> 26:37.140\n of where that line is.\n\n26:37.140 --> 26:39.780\n I don't think anyone knows the right answer.\n\n26:39.780 --> 26:43.260\n Twitter folks are under fire now, Jack at Twitter\n\n26:43.260 --> 26:45.060\n for trying to find that line.\n\n26:45.060 --> 26:46.860\n Nobody knows what that line is.\n\n26:46.860 --> 26:50.940\n But how do you try to find the line\n\n26:50.940 --> 26:55.940\n for trying to prevent abusive behavior\n\n26:57.940 --> 27:00.140\n and at the same time, let people be playful\n\n27:00.140 --> 27:02.780\n and joke around and that kind of thing?\n\n27:02.780 --> 27:03.980\n I think in a certain way,\n\n27:03.980 --> 27:08.980\n like if you have what I would call vertical social networks,\n\n27:10.300 --> 27:12.140\n it gets to be a little bit easier.\n\n27:12.140 --> 27:14.380\n So like if you have a clear notion\n\n27:14.380 --> 27:17.940\n of like what your social network should be used for,\n\n27:17.940 --> 27:22.220\n or like what you are designing a community around,\n\n27:22.220 --> 27:25.740\n then you don't have as many dimensions\n\n27:25.740 --> 27:28.900\n to your sort of content safety problem\n\n27:28.900 --> 27:33.700\n as you do in a general purpose platform.\n\n27:33.700 --> 27:37.460\n I mean, so like on LinkedIn,\n\n27:37.460 --> 27:38.820\n like the whole social network\n\n27:38.820 --> 27:41.540\n is about connecting people with opportunity,\n\n27:41.540 --> 27:43.140\n whether it's helping them find a job\n\n27:43.140 --> 27:46.380\n or to sort of find mentors\n\n27:46.380 --> 27:51.380\n or to sort of help them like find their next sales lead\n\n27:52.180 --> 27:56.220\n or to just sort of allow them to broadcast\n\n27:56.220 --> 27:59.500\n their sort of professional identity\n\n27:59.500 --> 28:04.500\n to their network of peers and collaborators\n\n28:06.740 --> 28:08.300\n and sort of professional community.\n\n28:08.300 --> 28:09.940\n Like that is, I mean, like in some ways,\n\n28:09.940 --> 28:11.580\n like that's very, very broad,\n\n28:11.580 --> 28:15.180\n but in other ways it's sort of, it's narrow.\n\n28:15.180 --> 28:20.180\n And so like you can build AI's like machine learning systems\n\n28:20.980 --> 28:25.620\n that are capable with those boundaries\n\n28:25.620 --> 28:28.100\n of making better automated decisions\n\n28:28.100 --> 28:30.740\n about like what is sort of inappropriate\n\n28:30.740 --> 28:32.940\n and offensive comment or dangerous comment\n\n28:32.940 --> 28:37.940\n or illegal content when you have some constraints.\n\n28:37.940 --> 28:42.940\n You know, same thing with like the gaming social network.\n\n28:43.900 --> 28:45.740\n So for instance, like it's about playing games,\n\n28:45.740 --> 28:47.260\n not having fun.\n\n28:47.260 --> 28:49.460\n And like the thing that you don't want to have happen\n\n28:49.460 --> 28:52.220\n on the platform is why bullying is such an important thing.\n\n28:52.220 --> 28:53.740\n Like bullying is not fun.\n\n28:53.740 --> 28:56.460\n So you want to do everything in your power\n\n28:56.460 --> 28:59.380\n to encourage that not to happen.\n\n28:59.380 --> 29:03.420\n And yeah, but I think it's sort of a tough problem\n\n29:03.420 --> 29:05.260\n in general and it's one where I think, you know,\n\n29:05.260 --> 29:09.980\n eventually we're going to have to have some sort\n\n29:09.980 --> 29:14.980\n of clarification from our policymakers about what it is\n\n29:15.940 --> 29:18.940\n that we should be doing, like where the lines are,\n\n29:18.940 --> 29:20.860\n because it's tough.\n\n29:20.860 --> 29:23.740\n Like you don't, like in democracy, right?\n\n29:23.740 --> 29:25.540\n Like you don't want,\n\n29:25.540 --> 29:28.900\n you want some sort of democratic involvement.\n\n29:28.900 --> 29:30.460\n Like people should have a say\n\n29:30.460 --> 29:34.660\n in like where the lines are drawn.\n\n29:34.660 --> 29:37.500\n Like you don't want a bunch of people making\n\n29:37.500 --> 29:39.460\n like unilateral decisions.\n\n29:39.460 --> 29:43.140\n And like we are in a state right now\n\n29:43.140 --> 29:44.220\n for some of these platforms\n\n29:44.220 --> 29:46.260\n where you actually do have to make unilateral decisions\n\n29:46.260 --> 29:48.620\n where the policymaking isn't going to happen fast enough\n\n29:48.620 --> 29:52.540\n in order to like prevent very bad things from happening.\n\n29:52.540 --> 29:56.020\n But like we need the policymaking side of that to catch up,\n\n29:56.020 --> 29:58.460\n I think, as quickly as possible\n\n29:58.460 --> 30:01.980\n because you want that whole process to be a democratic thing,\n\n30:01.980 --> 30:05.740\n not a, you know, not some sort of weird thing\n\n30:05.740 --> 30:08.020\n where you've got a non representative group\n\n30:08.020 --> 30:10.420\n of people making decisions that have, you know,\n\n30:10.420 --> 30:12.500\n like national and global impact.\n\n30:12.500 --> 30:15.580\n And it's fascinating because the digital space is different\n\n30:15.580 --> 30:18.340\n than the physical space in which nations\n\n30:18.340 --> 30:19.860\n and governments were established.\n\n30:19.860 --> 30:23.980\n And so what policy looks like globally,\n\n30:23.980 --> 30:25.740\n what bullying looks like globally,\n\n30:25.740 --> 30:28.420\n what's healthy communication looks like globally\n\n30:28.420 --> 30:31.900\n is an open question and we're all figuring it out together,\n\n30:31.900 --> 30:33.260\n which is fascinating.\n\n30:33.260 --> 30:37.220\n Yeah, I mean with, you know, sort of fake news, for instance.\n\n30:37.220 --> 30:38.740\n And...\n\n30:38.740 --> 30:42.380\n Deep fakes and fake news generated by humans?\n\n30:42.380 --> 30:44.660\n Yeah, so we can talk about deep fakes,\n\n30:44.660 --> 30:46.180\n like I think that is another like, you know,\n\n30:46.180 --> 30:48.340\n sort of very interesting level of complexity.\n\n30:48.340 --> 30:51.540\n But like if you think about just the written word, right?\n\n30:51.540 --> 30:54.460\n Like we have, you know, we invented papyrus,\n\n30:54.460 --> 30:56.820\n what, 3,000 years ago where we, you know,\n\n30:56.820 --> 31:01.220\n you could sort of put word on paper.\n\n31:01.220 --> 31:06.220\n And then 500 years ago, like we get the printing press,\n\n31:07.300 --> 31:11.540\n like where the word gets a little bit more ubiquitous.\n\n31:11.540 --> 31:14.660\n And then like you really, really didn't get ubiquitous\n\n31:14.660 --> 31:18.500\n printed word until the end of the 19th century\n\n31:18.500 --> 31:20.780\n when the offset press was invented.\n\n31:20.780 --> 31:22.460\n And then, you know, just sort of explodes\n\n31:22.460 --> 31:25.420\n and like, you know, the cross product of that\n\n31:25.420 --> 31:28.980\n and the Industrial Revolution's need\n\n31:28.980 --> 31:32.900\n for educated citizens resulted in like\n\n31:32.900 --> 31:34.780\n this rapid expansion of literacy\n\n31:34.780 --> 31:36.060\n and the rapid expansion of the word.\n\n31:36.060 --> 31:39.740\n But like we had 3,000 years up to that point\n\n31:39.740 --> 31:43.300\n to figure out like how to, you know,\n\n31:43.300 --> 31:46.940\n like what's journalism, what's editorial integrity,\n\n31:46.940 --> 31:50.140\n like what's, you know, what's scientific peer review.\n\n31:50.140 --> 31:52.860\n And so like you built all of this mechanism\n\n31:52.860 --> 31:57.060\n to like try to filter through all of the noise\n\n31:57.060 --> 31:59.820\n that the technology made possible\n\n31:59.820 --> 32:01.900\n to like, you know, sort of getting to something\n\n32:01.900 --> 32:03.980\n that society could cope with.\n\n32:03.980 --> 32:06.580\n And like, if you think about just the piece,\n\n32:06.580 --> 32:09.780\n the PC didn't exist 50 years ago.\n\n32:09.780 --> 32:11.780\n And so in like this span of, you know,\n\n32:11.780 --> 32:16.140\n like half a century, like we've gone from no digital,\n\n32:16.140 --> 32:18.300\n you know, no ubiquitous digital technology\n\n32:18.300 --> 32:21.060\n to like having a device that sits in your pocket\n\n32:21.060 --> 32:23.740\n where you can sort of say whatever is on your mind\n\n32:23.740 --> 32:27.100\n to like what did Mary have in her,\n\n32:27.100 --> 32:32.100\n Mary Meeker just released her new like slide deck last week.\n\n32:32.420 --> 32:37.340\n You know, we've got 50% penetration of the internet\n\n32:37.340 --> 32:38.500\n to the global population.\n\n32:38.500 --> 32:40.260\n Like there are like three and a half billion people\n\n32:40.260 --> 32:41.740\n who are connected now.\n\n32:41.740 --> 32:44.980\n So it's like, it's crazy, crazy, like inconceivable,\n\n32:44.980 --> 32:46.460\n like how fast all of this happened.\n\n32:46.460 --> 32:48.700\n So, you know, it's not surprising\n\n32:48.700 --> 32:50.980\n that we haven't figured out what to do yet,\n\n32:50.980 --> 32:55.660\n but like we gotta really like lean into this set of problems\n\n32:55.660 --> 33:00.220\n because like we basically have three millennia worth of work\n\n33:00.220 --> 33:02.500\n to do about how to deal with all of this\n\n33:02.500 --> 33:04.580\n and like probably what, you know,\n\n33:04.580 --> 33:07.020\n amounts to the next decade worth of time.\n\n33:07.020 --> 33:09.980\n So since we're on the topic of tough, you know,\n\n33:09.980 --> 33:11.620\n tough challenging problems,\n\n33:11.620 --> 33:15.220\n let's look at more on the tooling side in AI\n\n33:15.220 --> 33:18.420\n that Microsoft is looking at is face recognition software.\n\n33:18.420 --> 33:21.860\n So there's a lot of powerful positive use cases\n\n33:21.860 --> 33:24.220\n for face recognition, but there's some negative ones\n\n33:24.220 --> 33:27.180\n and we're seeing those in different governments\n\n33:27.180 --> 33:28.140\n in the world.\n\n33:28.140 --> 33:30.900\n So how do you, how does Microsoft think about the use\n\n33:30.900 --> 33:35.740\n of face recognition software as a platform\n\n33:35.740 --> 33:38.780\n in governments and companies?\n\n33:39.820 --> 33:42.300\n How do we strike an ethical balance here?\n\n33:42.300 --> 33:47.300\n Yeah, I think we've articulated a clear point of view.\n\n33:47.300 --> 33:51.900\n So Brad Smith wrote a blog post last fall,\n\n33:51.900 --> 33:54.180\n I believe that sort of like outlined\n\n33:54.180 --> 33:57.100\n like very specifically what, you know,\n\n33:57.100 --> 33:59.340\n what our point of view is there.\n\n33:59.340 --> 34:01.060\n And, you know, I think we believe\n\n34:01.060 --> 34:02.340\n that there are certain uses\n\n34:02.340 --> 34:04.740\n to which face recognition should not be put.\n\n34:04.740 --> 34:06.060\n And we believe again,\n\n34:06.060 --> 34:09.220\n that there's a need for regulation there.\n\n34:09.220 --> 34:11.940\n Like the government should like really come in\n\n34:11.940 --> 34:15.780\n and say that, you know, this is where the lines are.\n\n34:15.780 --> 34:18.380\n And like, we very much wanted to like figuring out\n\n34:18.380 --> 34:20.380\n where the lines are, should be a democratic process.\n\n34:20.380 --> 34:22.780\n But in the short term, like we've drawn some lines\n\n34:22.780 --> 34:26.180\n where, you know, we push back against uses\n\n34:26.180 --> 34:29.940\n of face recognition technology, you know,\n\n34:29.940 --> 34:32.300\n like the city of San Francisco, for instance,\n\n34:32.300 --> 34:36.580\n I think has completely outlawed any government agency\n\n34:36.580 --> 34:39.580\n from using face recognition tech.\n\n34:39.580 --> 34:44.580\n And like that may prove to be a little bit overly broad.\n\n34:44.580 --> 34:48.820\n But for like certain law enforcement things,\n\n34:48.820 --> 34:53.820\n like you really, I would personally rather be overly\n\n34:54.060 --> 34:57.380\n sort of cautious in terms of restricting use of it\n\n34:57.380 --> 34:58.900\n until like we have, you know,\n\n34:58.900 --> 35:02.140\n sort of defined a reasonable, you know,\n\n35:02.140 --> 35:04.860\n democratically determined regulatory framework\n\n35:04.860 --> 35:08.820\n for like where we could and should use it.\n\n35:08.820 --> 35:12.140\n And, you know, the other thing there is like,\n\n35:12.140 --> 35:13.980\n we've got a bunch of research that we're doing\n\n35:13.980 --> 35:18.380\n and a bunch of progress that we've made on bias there.\n\n35:18.380 --> 35:20.820\n And like, there are all sorts of like weird biases\n\n35:20.820 --> 35:22.980\n that these models can have,\n\n35:22.980 --> 35:25.580\n like all the way from like the most noteworthy one\n\n35:25.580 --> 35:30.580\n where, you know, you may have underrepresented minorities\n\n35:31.660 --> 35:34.660\n who are like underrepresented in the training data\n\n35:34.660 --> 35:39.180\n and then you start learning like strange things.\n\n35:39.180 --> 35:42.100\n But like there are even, you know, other weird things.\n\n35:42.100 --> 35:46.460\n Like we've, I think we've seen in the public research,\n\n35:46.460 --> 35:49.460\n like models can learn strange things,\n\n35:49.460 --> 35:54.460\n like all doctors are men, for instance, just, yeah.\n\n35:54.700 --> 35:58.900\n I mean, and so like, it really is a thing\n\n35:58.900 --> 36:03.580\n where it's very important for everybody\n\n36:03.580 --> 36:08.420\n who is working on these things before they push publish,\n\n36:08.420 --> 36:12.780\n they launch the experiment, they, you know, push the code\n\n36:12.780 --> 36:17.100\n to, you know, online, or they even publish the paper\n\n36:17.100 --> 36:20.420\n that they are at least starting to think about\n\n36:20.420 --> 36:25.260\n what some of the potential negative consequences are,\n\n36:25.260 --> 36:26.100\n some of this stuff.\n\n36:26.100 --> 36:29.020\n I mean, this is where, you know, like the deep fake stuff\n\n36:29.020 --> 36:32.340\n I find very worrisome just because\n\n36:32.340 --> 36:37.340\n there are going to be some very good beneficial uses\n\n36:39.780 --> 36:44.700\n of like GAN generated imagery.\n\n36:46.100 --> 36:48.460\n And funny enough, like one of the places\n\n36:48.460 --> 36:52.940\n where it's actually useful is we're using the technology\n\n36:52.940 --> 36:55.940\n right now to generate synthetic visual data\n\n36:58.620 --> 37:01.140\n for training some of the face recognition models\n\n37:01.140 --> 37:03.420\n to get rid of the bias.\n\n37:03.420 --> 37:05.740\n So like, that's one like super good use of the tech,\n\n37:05.740 --> 37:09.620\n but like, you know, it's getting good enough now\n\n37:09.620 --> 37:12.300\n where, you know, it's going to sort of challenge\n\n37:12.300 --> 37:14.300\n a normal human being's ability to,\n\n37:14.300 --> 37:15.740\n like now you're just sort of say,\n\n37:15.740 --> 37:19.300\n like it's very expensive for someone\n\n37:19.300 --> 37:23.220\n to fabricate a photorealistic fake video.\n\n37:24.140 --> 37:26.900\n And like GANs are going to make it fantastically cheap\n\n37:26.900 --> 37:30.420\n to fabricate a photorealistic fake video.\n\n37:30.420 --> 37:34.460\n And so like what you assume you can sort of trust is true\n\n37:34.460 --> 37:38.380\n versus like be skeptical about is about to change.\n\n37:38.380 --> 37:40.540\n And like, we're not ready for it, I don't think.\n\n37:40.540 --> 37:41.980\n The nature of truth, right.\n\n37:41.980 --> 37:46.580\n That's, it's also exciting because I think both you and I\n\n37:46.580 --> 37:49.580\n probably would agree that the way to solve,\n\n37:49.580 --> 37:52.820\n to take on that challenge is with technology, right?\n\n37:52.820 --> 37:56.820\n There's probably going to be ideas of ways to verify\n\n37:56.820 --> 38:00.820\n which kind of video is legitimate, which kind is not.\n\n38:00.820 --> 38:03.860\n So to me, that's an exciting possibility,\n\n38:03.860 --> 38:07.180\n most likely for just the comedic genius\n\n38:07.180 --> 38:10.980\n that the internet usually creates with these kinds of videos\n\n38:10.980 --> 38:13.940\n and hopefully will not result in any serious harm.\n\n38:13.940 --> 38:17.100\n Yeah, and it could be, you know,\n\n38:17.100 --> 38:19.940\n like I think we will have technology to,\n\n38:21.180 --> 38:23.580\n that may be able to detect whether or not\n\n38:23.580 --> 38:24.460\n something's fake or real.\n\n38:24.460 --> 38:29.460\n Although the fakes are pretty convincing,\n\n38:30.180 --> 38:34.340\n even like when you subject them to machine scrutiny.\n\n38:34.340 --> 38:37.820\n But, you know, we also have these increasingly\n\n38:37.820 --> 38:40.540\n interesting social networks, you know,\n\n38:40.540 --> 38:42.660\n that are under fire right now\n\n38:43.580 --> 38:46.220\n for some of the bad things that they do.\n\n38:46.220 --> 38:47.700\n Like one of the things you could choose to do\n\n38:47.700 --> 38:51.780\n with a social network is like you could,\n\n38:51.780 --> 38:55.580\n you could use crypto and the networks\n\n38:55.580 --> 38:57.740\n to like have content signed\n\n38:57.740 --> 39:01.420\n where you could have a like full chain of custody\n\n39:01.420 --> 39:03.900\n that accompanied every piece of content.\n\n39:03.900 --> 39:06.780\n So like when you're viewing something\n\n39:06.780 --> 39:08.540\n and like you want to ask yourself,\n\n39:08.540 --> 39:11.020\n like how much can I trust this?\n\n39:11.020 --> 39:12.380\n Like you can click something\n\n39:12.380 --> 39:14.980\n and like have a verified chain of custody\n\n39:14.980 --> 39:19.060\n that shows like, oh, this is coming from this source.\n\n39:19.060 --> 39:21.660\n And it's like signed by like someone\n\n39:21.660 --> 39:24.100\n whose identity I trust.\n\n39:24.100 --> 39:25.420\n Yeah, I think having that, you know,\n\n39:25.420 --> 39:26.620\n having that chain of custody,\n\n39:26.620 --> 39:29.340\n like being able to like say, oh, here's this video.\n\n39:29.340 --> 39:31.940\n Like it may or may not have been produced\n\n39:31.940 --> 39:33.740\n using some of this deepfake technology,\n\n39:33.740 --> 39:35.660\n but if you've got a verified chain of custody\n\n39:35.660 --> 39:37.780\n where you can sort of trace it all the way back\n\n39:37.780 --> 39:39.940\n to an identity and you can decide whether or not\n\n39:39.940 --> 39:41.540\n like I trust this identity.\n\n39:41.540 --> 39:43.340\n Like, oh no, this is really from the White House\n\n39:43.340 --> 39:45.500\n or like this is really from the, you know,\n\n39:45.500 --> 39:48.820\n the office of this particular presidential candidate\n\n39:48.820 --> 39:53.540\n or it's really from, you know, Jeff Wiener, CEO of LinkedIn\n\n39:53.540 --> 39:55.540\n or Satya Nadella, CEO of Microsoft.\n\n39:55.540 --> 39:58.420\n Like that might be like one way\n\n39:58.420 --> 39:59.940\n that you can solve some of the problems.\n\n39:59.940 --> 40:01.780\n So like that's not the super high tech.\n\n40:01.780 --> 40:04.500\n Like we've had all of this technology forever.\n\n40:04.500 --> 40:06.700\n And, but I think you're right.\n\n40:06.700 --> 40:11.100\n Like it has to be some sort of technological thing\n\n40:11.100 --> 40:15.820\n because the underlying tech that is used to create this\n\n40:15.820 --> 40:18.780\n is not going to do anything but get better over time\n\n40:18.780 --> 40:21.140\n and the genie is sort of out of the bottle.\n\n40:21.140 --> 40:22.780\n There's no stuffing it back in.\n\n40:22.780 --> 40:24.500\n And there's a social component,\n\n40:24.500 --> 40:26.620\n which I think is really healthy for a democracy\n\n40:26.620 --> 40:28.500\n where people will be skeptical\n\n40:28.500 --> 40:32.140\n about the thing they watch in general.\n\n40:32.140 --> 40:34.180\n So, you know, which is good.\n\n40:34.180 --> 40:37.300\n Skepticism in general is good for content.\n\n40:37.300 --> 40:41.780\n So deepfakes in that sense are creating a global skepticism\n\n40:41.780 --> 40:44.780\n about can they trust what they read.\n\n40:44.780 --> 40:46.900\n It encourages further research.\n\n40:46.900 --> 40:48.860\n I come from the Soviet Union\n\n40:49.860 --> 40:53.380\n where basically nobody trusted the media\n\n40:53.380 --> 40:55.180\n because you knew it was propaganda.\n\n40:55.180 --> 40:59.220\n And that kind of skepticism encouraged further research\n\n40:59.220 --> 41:02.420\n about ideas as opposed to just trusting any one source.\n\n41:02.420 --> 41:04.340\n Well, look, I think it's one of the reasons why\n\n41:04.340 --> 41:09.340\n the scientific method and our apparatus\n\n41:09.500 --> 41:11.540\n of modern science is so good.\n\n41:11.540 --> 41:15.420\n Like, because you don't have to trust anything.\n\n41:15.420 --> 41:20.180\n Like, the whole notion of modern science\n\n41:20.180 --> 41:22.460\n beyond the fact that this is a hypothesis\n\n41:22.460 --> 41:24.900\n and this is an experiment to test the hypothesis\n\n41:24.900 --> 41:27.380\n and this is a peer review process\n\n41:27.380 --> 41:30.140\n for scrutinizing published results.\n\n41:30.140 --> 41:33.300\n But stuff's also supposed to be reproducible.\n\n41:33.300 --> 41:35.260\n So you know it's been vetted by this process,\n\n41:35.260 --> 41:38.060\n but you also are expected to publish enough detail\n\n41:38.060 --> 41:42.100\n where if you are sufficiently skeptical of the thing,\n\n41:42.100 --> 41:44.740\n you can go try to reproduce it yourself.\n\n41:44.740 --> 41:47.580\n And like, I don't know what it is.\n\n41:47.580 --> 41:49.980\n Like, I think a lot of engineers are like this\n\n41:49.980 --> 41:51.940\n where like, you know, sort of this,\n\n41:51.940 --> 41:55.580\n like your brain is sort of wired for skepticism.\n\n41:55.580 --> 41:58.060\n Like, you don't just first order trust everything\n\n41:58.060 --> 42:00.100\n that you see and encounter.\n\n42:00.100 --> 42:02.620\n And like, you're sort of curious to understand,\n\n42:02.620 --> 42:04.540\n you know, the next thing.\n\n42:04.540 --> 42:09.140\n But like, I think it's an entirely healthy thing.\n\n42:09.140 --> 42:12.340\n And like, we need a little bit more of that right now.\n\n42:12.340 --> 42:16.300\n So I'm not a large business owner.\n\n42:16.300 --> 42:21.300\n So I'm just a huge fan of many of Microsoft products.\n\n42:23.300 --> 42:25.460\n I mean, I still, actually in terms of,\n\n42:25.460 --> 42:27.060\n I generate a lot of graphics and images\n\n42:27.060 --> 42:28.740\n and I still use PowerPoint to do that.\n\n42:28.740 --> 42:30.500\n It beats Illustrator for me.\n\n42:30.500 --> 42:34.540\n Even professional sort of, it's fascinating.\n\n42:34.540 --> 42:38.460\n So I wonder, what is the future of,\n\n42:38.460 --> 42:42.020\n let's say Windows and Office look like?\n\n42:42.020 --> 42:43.940\n Is, do you see it?\n\n42:43.940 --> 42:45.940\n I mean, I remember looking forward to XP.\n\n42:45.940 --> 42:48.260\n Was it exciting when XP was released?\n\n42:48.260 --> 42:51.180\n Just like you said, I don't remember when 95 was released.\n\n42:51.180 --> 42:53.900\n But XP for me was a big celebration.\n\n42:53.900 --> 42:56.420\n And when 10 came out, I was like, oh, okay.\n\n42:56.420 --> 42:57.260\n Well, it's nice.\n\n42:57.260 --> 42:59.100\n It's a nice improvement.\n\n42:59.100 --> 43:02.120\n So what do you see the future of these products?\n\n43:03.380 --> 43:04.700\n I think there's a bunch of excite.\n\n43:04.700 --> 43:07.260\n I mean, on the Office front,\n\n43:07.260 --> 43:12.260\n there's gonna be this like increasing productivity wins\n\n43:13.900 --> 43:17.200\n that are coming out of some of these AI powered features\n\n43:17.200 --> 43:18.040\n that are coming.\n\n43:18.040 --> 43:20.000\n Like the products will sort of get smarter and smarter\n\n43:20.000 --> 43:21.240\n in like a very subtle way.\n\n43:21.240 --> 43:24.260\n Like there's not gonna be this big bang moment\n\n43:24.260 --> 43:28.020\n where like Clippy is gonna reemerge and it's gonna be.\n\n43:28.020 --> 43:28.860\n Wait a minute.\n\n43:28.860 --> 43:30.660\n Okay, we'll have to wait, wait, wait.\n\n43:30.660 --> 43:32.580\n Is Clippy coming back?\n\n43:32.580 --> 43:37.140\n But quite seriously, so injection of AI.\n\n43:37.140 --> 43:39.220\n There's not much, or at least I'm not familiar,\n\n43:39.220 --> 43:41.340\n sort of assistive type of stuff going on\n\n43:41.340 --> 43:43.700\n inside the Office products.\n\n43:43.700 --> 43:47.740\n Like a Clippy style assistant, personal assistant.\n\n43:47.740 --> 43:50.740\n Do you think that there's a possibility\n\n43:50.740 --> 43:52.100\n of that in the future?\n\n43:52.100 --> 43:54.820\n So I think there are a bunch of like very small ways\n\n43:54.820 --> 43:58.540\n in which like machine learning powered assistive things\n\n43:58.540 --> 44:00.260\n are in the product right now.\n\n44:00.260 --> 44:04.940\n So there are a bunch of interesting things.\n\n44:04.940 --> 44:09.500\n Like the auto response stuff's getting better and better.\n\n44:09.500 --> 44:11.180\n And it's like getting to the point\n\n44:11.180 --> 44:14.820\n where it can auto respond with like,\n\n44:14.820 --> 44:19.260\n okay, this person's clearly trying to schedule a meeting.\n\n44:19.260 --> 44:21.700\n So it looks at your calendar and it automatically\n\n44:21.700 --> 44:24.260\n like tries to find like a time and a space\n\n44:24.260 --> 44:26.360\n that's mutually interesting.\n\n44:27.420 --> 44:32.420\n Like we have this notion of Microsoft search\n\n44:32.420 --> 44:34.940\n at a Microsoft search where it's like not just web search,\n\n44:34.940 --> 44:38.180\n but it's like search across like all of your information\n\n44:38.180 --> 44:43.180\n that's sitting inside of like your Office 365 tenant\n\n44:43.300 --> 44:46.900\n and like potentially in other products.\n\n44:46.900 --> 44:49.680\n And like we have this thing called the Microsoft Graph\n\n44:49.680 --> 44:53.980\n that is basically an API federator that sort of like\n\n44:53.980 --> 44:57.980\n gets you hooked up across the entire breadth\n\n44:57.980 --> 45:01.640\n of like all of the, like what were information silos\n\n45:01.640 --> 45:04.740\n before they got woven together with the graph.\n\n45:05.660 --> 45:07.860\n Like that is like getting increasing,\n\n45:07.860 --> 45:09.140\n with increasing effectiveness,\n\n45:09.140 --> 45:13.120\n sort of plumbed into some of these auto response things\n\n45:13.120 --> 45:15.860\n where you're gonna be able to see the system\n\n45:15.860 --> 45:18.220\n like automatically retrieve information for you.\n\n45:18.220 --> 45:21.140\n Like if, you know, like I frequently send out,\n\n45:21.140 --> 45:24.060\n you know, emails to folks where like I can't find a paper\n\n45:24.060 --> 45:25.380\n or a document or whatnot.\n\n45:25.380 --> 45:26.340\n There's no reason why the system\n\n45:26.340 --> 45:27.540\n won't be able to do that for you.\n\n45:27.540 --> 45:31.980\n And like, I think the, it's building towards\n\n45:31.980 --> 45:34.480\n like having things that look more like,\n\n45:34.480 --> 45:37.900\n like a fully integrated, you know, assistant,\n\n45:37.900 --> 45:40.740\n but like you'll have a bunch of steps\n\n45:40.740 --> 45:42.820\n that you will see before you,\n\n45:42.820 --> 45:45.140\n like it will not be this like big bang thing\n\n45:45.140 --> 45:47.420\n where like Clippy comes back and you've got this like,\n\n45:47.420 --> 45:49.400\n you know, manifestation of, you know,\n\n45:49.400 --> 45:52.060\n like a fully, fully powered assistant.\n\n45:53.380 --> 45:56.940\n So I think that's, that's definitely coming in,\n\n45:56.940 --> 45:58.700\n like all of the, you know, collaboration,\n\n45:58.700 --> 46:00.740\n coauthoring stuff's getting better.\n\n46:00.740 --> 46:02.220\n You know, it's like really interesting.\n\n46:02.220 --> 46:05.500\n Like if you look at how we use\n\n46:06.500 --> 46:09.020\n the Office product portfolio at Microsoft,\n\n46:09.020 --> 46:12.140\n like more and more of it is happening inside of\n\n46:12.140 --> 46:14.500\n like Teams as a canvas.\n\n46:14.500 --> 46:17.180\n And like, it's this thing where, you know,\n\n46:17.180 --> 46:20.620\n you've got collaboration is like at the center\n\n46:20.620 --> 46:25.620\n of the product and like we built some like really cool stuff\n\n46:25.620 --> 46:28.420\n that's some of, which is about to be open source\n\n46:28.420 --> 46:30.980\n that are sort of framework level things\n\n46:30.980 --> 46:34.540\n for doing, for doing coauthoring.\n\n46:34.540 --> 46:35.380\n That's awesome.\n\n46:35.380 --> 46:37.860\n So in, is there a cloud component to that?\n\n46:37.860 --> 46:40.300\n So on the web, or is it,\n\n46:40.300 --> 46:42.660\n and forgive me if I don't already know this,\n\n46:42.660 --> 46:45.580\n but with Office 365, we still,\n\n46:45.580 --> 46:47.540\n the collaboration we do if we're doing Word,\n\n46:47.540 --> 46:49.660\n we still send the file around.\n\n46:49.660 --> 46:50.500\n No, no.\n\n46:50.500 --> 46:51.340\n So this is.\n\n46:51.340 --> 46:54.300\n We're already a little bit better than that.\n\n46:54.300 --> 46:55.900\n A little bit better than that and like, you know,\n\n46:55.900 --> 46:57.700\n so like the fact that you're unaware of it means\n\n46:57.700 --> 46:59.180\n we've got a better job to do,\n\n46:59.180 --> 47:01.980\n like helping you discover, discover this stuff.\n\n47:02.900 --> 47:06.380\n But yeah, I mean, it's already like got a huge,\n\n47:06.380 --> 47:07.220\n huge cloud component.\n\n47:07.220 --> 47:09.700\n And like part of, you know, part of this framework stuff,\n\n47:09.700 --> 47:12.660\n I think we're calling it, like I,\n\n47:12.660 --> 47:14.540\n like we've been working on it for a couple of years.\n\n47:14.540 --> 47:17.220\n So like, I know the internal code name for it,\n\n47:17.220 --> 47:18.660\n but I think when we launched it to build,\n\n47:18.660 --> 47:20.760\n it's called the Fluid Framework.\n\n47:20.760 --> 47:25.060\n And, but like what Fluid lets you do is like,\n\n47:25.060 --> 47:27.900\n you can go into a conversation that you're having in Teams\n\n47:27.900 --> 47:30.240\n and like reference like part of a spreadsheet\n\n47:30.240 --> 47:33.900\n that you're working on where somebody's like sitting\n\n47:33.900 --> 47:35.580\n in the Excel canvas,\n\n47:35.580 --> 47:37.740\n like working on the spreadsheet with a, you know,\n\n47:37.740 --> 47:38.860\n chart or whatnot,\n\n47:38.860 --> 47:41.940\n and like you can sort of embed like part of the spreadsheet\n\n47:41.940 --> 47:45.400\n in the Teams conversation where like you can dynamically\n\n47:45.400 --> 47:48.740\n update it and like all of the changes that you're making\n\n47:48.740 --> 47:51.220\n to the, to this object are like, you know,\n\n47:51.220 --> 47:54.620\n coordinate and everything is sort of updating in real time.\n\n47:54.620 --> 47:57.940\n So like you can be in whatever canvas is most convenient\n\n47:57.940 --> 48:00.380\n for you to get your work done.\n\n48:00.380 --> 48:03.380\n So I, out of my own sort of curiosity as an engineer,\n\n48:03.380 --> 48:06.220\n I know what it's like to sort of lead a team\n\n48:06.220 --> 48:08.220\n of 10, 15 engineers.\n\n48:08.220 --> 48:11.660\n Microsoft has, I don't know what the numbers are,\n\n48:11.660 --> 48:14.940\n maybe 50, maybe 60,000 engineers, maybe 40.\n\n48:14.940 --> 48:17.220\n I don't know exactly what the number is, it's a lot.\n\n48:17.220 --> 48:18.900\n It's tens of thousands.\n\n48:18.900 --> 48:20.700\n Right, so it's more than 10 or 15.\n\n48:20.700 --> 48:25.700\n What, I mean, you've led different sizes,\n\n48:28.700 --> 48:30.540\n mostly large size of engineers.\n\n48:30.540 --> 48:33.820\n What does it take to lead such a large group\n\n48:33.820 --> 48:37.500\n into a continue innovation,\n\n48:37.500 --> 48:40.260\n continue being highly productive\n\n48:40.260 --> 48:44.220\n and yet develop all kinds of new ideas and yet maintain,\n\n48:44.220 --> 48:47.100\n like what does it take to lead such a large group\n\n48:47.100 --> 48:48.980\n of brilliant people?\n\n48:48.980 --> 48:52.060\n I think the thing that you learn\n\n48:52.060 --> 48:55.140\n as you manage larger and larger scale\n\n48:55.140 --> 48:57.940\n is that there are three things\n\n48:57.940 --> 49:00.500\n that are like very, very important\n\n49:00.500 --> 49:02.340\n for big engineering teams.\n\n49:02.340 --> 49:06.300\n Like one is like having some sort of forethought\n\n49:06.300 --> 49:09.860\n about what it is that you're gonna be building\n\n49:09.860 --> 49:11.060\n over large periods of time.\n\n49:11.060 --> 49:13.100\n Like not exactly, like you don't need to know\n\n49:13.100 --> 49:15.300\n that like, you know, I'm putting all my chips\n\n49:15.300 --> 49:17.820\n on this one product and like this is gonna be the thing,\n\n49:17.820 --> 49:21.460\n but like it's useful to know like what sort of capabilities\n\n49:21.460 --> 49:23.140\n you think you're going to need to have\n\n49:23.140 --> 49:24.740\n to build the products of the future.\n\n49:24.740 --> 49:28.060\n And then like invest in that infrastructure,\n\n49:28.060 --> 49:30.180\n like whether, and like I'm not just talking\n\n49:30.180 --> 49:32.740\n about storage systems or cloud APIs,\n\n49:32.740 --> 49:35.380\n it's also like what does your development process look like?\n\n49:35.380 --> 49:36.780\n What tools do you want?\n\n49:36.780 --> 49:40.020\n Like what culture do you want to build around?\n\n49:40.020 --> 49:42.780\n Like how you're, you know, sort of collaborating together\n\n49:42.780 --> 49:45.780\n to like make complicated technical things.\n\n49:45.780 --> 49:48.100\n And so like having an opinion and investing in that\n\n49:48.100 --> 49:50.500\n is like, it just gets more and more important.\n\n49:50.500 --> 49:54.540\n And like the sooner you can get a concrete set of opinions,\n\n49:54.540 --> 49:57.700\n like the better you're going to be.\n\n49:57.700 --> 50:01.620\n Like you can wing it for a while at small scales,\n\n50:01.620 --> 50:03.180\n like, you know, when you start a company,\n\n50:03.180 --> 50:06.340\n like you don't have to be like super specific about it,\n\n50:06.340 --> 50:09.980\n but like the biggest miseries that I've ever seen\n\n50:09.980 --> 50:12.660\n as an engineering leader are in places\n\n50:12.660 --> 50:14.500\n where you didn't have a clear enough opinion\n\n50:14.500 --> 50:16.780\n about those things soon enough.\n\n50:16.780 --> 50:18.740\n And then you just sort of go create a bunch\n\n50:18.740 --> 50:21.940\n of technical debt and like culture debt\n\n50:21.940 --> 50:25.820\n that is excruciatingly painful to clean up.\n\n50:25.820 --> 50:28.700\n So like, that's one bundle of things.\n\n50:28.700 --> 50:33.260\n Like the other, you know, another bundle of things\n\n50:33.260 --> 50:36.620\n is like, it's just really, really important\n\n50:36.620 --> 50:41.620\n to like have a clear mission\n\n50:41.620 --> 50:46.260\n that's not just some cute crap you say\n\n50:46.260 --> 50:48.940\n because like you think you should have a mission,\n\n50:48.940 --> 50:52.940\n but like something that clarifies for people\n\n50:52.940 --> 50:55.740\n like where it is that you're headed together.\n\n50:57.220 --> 50:59.180\n Like, I know it's like probably like a little bit\n\n50:59.180 --> 51:00.380\n too popular right now,\n\n51:00.380 --> 51:05.380\n but Yuval Harari's book, Sapiens,\n\n51:05.380 --> 51:10.380\n one of the central ideas in his book is that\n\n51:10.380 --> 51:15.380\n like storytelling is like the quintessential thing\n\n51:15.380 --> 51:18.780\n for coordinating the activities of large groups of people.\n\n51:18.780 --> 51:21.380\n Like once you get past Dunbar's number,\n\n51:21.380 --> 51:23.980\n and like I've really, really seen that\n\n51:23.980 --> 51:25.580\n just managing engineering teams.\n\n51:25.580 --> 51:30.580\n Like you can just brute force things\n\n51:30.580 --> 51:33.500\n when you're less than 120, 150 folks\n\n51:33.500 --> 51:35.980\n where you can sort of know and trust\n\n51:35.980 --> 51:38.380\n and understand what the dynamics are\n\n51:38.380 --> 51:40.380\n between all the people, but like past that,\n\n51:40.380 --> 51:43.780\n like things just sort of start to catastrophically fail\n\n51:43.780 --> 51:47.180\n if you don't have some sort of set of shared goals\n\n51:47.180 --> 51:48.980\n that you're marching towards.\n\n51:48.980 --> 51:51.380\n And so like, even though it sounds touchy feely\n\n51:51.380 --> 51:54.180\n and you know, like a bunch of technical people\n\n51:54.180 --> 51:56.180\n will sort of balk at the idea that like,\n\n51:56.180 --> 52:00.180\n you need to like have a clear, like the missions,\n\n52:00.180 --> 52:02.180\n like very, very, very important.\n\n52:02.180 --> 52:04.180\n You're always right, right?\n\n52:04.180 --> 52:06.180\n Stories, that's how our society,\n\n52:06.180 --> 52:08.180\n that's the fabric that connects us,\n\n52:08.180 --> 52:10.180\n all of us is these powerful stories.\n\n52:10.180 --> 52:12.180\n And that works for companies too, right?\n\n52:12.180 --> 52:14.180\n It works for everything.\n\n52:14.180 --> 52:16.180\n Like, I mean, even down to like, you know,\n\n52:16.180 --> 52:18.180\n you sort of really think about it,\n\n52:18.180 --> 52:20.180\n like our currency, for instance, is a story.\n\n52:20.180 --> 52:22.180\n Our constitution is a story.\n\n52:22.180 --> 52:24.180\n Our laws are stories.\n\n52:24.180 --> 52:27.180\n I mean, like we believe very, very, very strongly in them.\n\n52:27.180 --> 52:29.180\n And thank God we do.\n\n52:29.180 --> 52:31.180\n But like they are,\n\n52:31.180 --> 52:33.180\n they're just abstract things.\n\n52:33.180 --> 52:34.180\n Like they're just words.\n\n52:34.180 --> 52:36.180\n Like if we don't believe in them, they're nothing.\n\n52:36.180 --> 52:39.180\n And in some sense, those stories are platforms\n\n52:39.180 --> 52:43.180\n and the kinds, some of which Microsoft is creating, right?\n\n52:43.180 --> 52:46.180\n They have platforms on which we define the future.\n\n52:46.180 --> 52:48.180\n So last question, what do you,\n\n52:48.180 --> 52:50.180\n let's get philosophical maybe,\n\n52:50.180 --> 52:51.180\n bigger than even Microsoft,\n\n52:51.180 --> 52:56.180\n what do you think the next 20, 30 plus years\n\n52:56.180 --> 53:00.180\n looks like for computing, for technology, for devices?\n\n53:00.180 --> 53:04.180\n Do you have crazy ideas about the future of the world?\n\n53:04.180 --> 53:06.180\n Yeah, look, I think we, you know,\n\n53:06.180 --> 53:10.180\n we're entering this time where we've got,\n\n53:10.180 --> 53:13.180\n we have technology that is progressing\n\n53:13.180 --> 53:15.180\n at the fastest rate that it ever has.\n\n53:15.180 --> 53:18.180\n And you've got,\n\n53:18.180 --> 53:21.180\n you've got some really big social problems,\n\n53:21.180 --> 53:26.180\n like society scale problems that we have to tackle.\n\n53:26.180 --> 53:28.180\n And so, you know, I think we're going to rise to the challenge\n\n53:28.180 --> 53:30.180\n and like figure out how to intersect\n\n53:30.180 --> 53:32.180\n like all of the power of this technology\n\n53:32.180 --> 53:35.180\n with all of the big challenges that are facing us,\n\n53:35.180 --> 53:37.180\n whether it's, you know, global warming,\n\n53:37.180 --> 53:41.180\n whether it's like the biggest remainder of the population boom\n\n53:41.180 --> 53:46.180\n is in Africa for the next 50 years or so.\n\n53:46.180 --> 53:49.180\n And like global warming is going to make it increasingly difficult\n\n53:49.180 --> 53:52.180\n to feed the global population in particular,\n\n53:52.180 --> 53:54.180\n like in this place where you're going to have\n\n53:54.180 --> 53:57.180\n like the biggest population boom.\n\n53:57.180 --> 54:01.180\n I think we, you know, like AI is going to,\n\n54:01.180 --> 54:03.180\n like if we push it in the right direction,\n\n54:03.180 --> 54:07.180\n like it can do like incredible things to empower all of us\n\n54:07.180 --> 54:12.180\n to achieve our full potential and to, you know,\n\n54:12.180 --> 54:15.180\n like live better lives.\n\n54:15.180 --> 54:20.180\n But like that also means focus on like\n\n54:20.180 --> 54:21.180\n some super important things.\n\n54:21.180 --> 54:26.180\n Like how can you apply it to healthcare to make sure that,\n\n54:26.180 --> 54:29.180\n you know, like our quality and cost of\n\n54:29.180 --> 54:33.180\n and sort of ubiquity of health coverage is better\n\n54:33.180 --> 54:35.180\n and better over time.\n\n54:35.180 --> 54:38.180\n Like that's more and more important every day is like\n\n54:38.180 --> 54:43.180\n in the United States and like the rest of the industrialized world,\n\n54:43.180 --> 54:45.180\n so Western Europe, China, Japan, Korea,\n\n54:45.180 --> 54:50.180\n like you've got this population bubble of like aging,\n\n54:50.180 --> 54:54.180\n working, you know, working age folks who are,\n\n54:54.180 --> 54:56.180\n you know, at some point over the next 20, 30 years,\n\n54:56.180 --> 54:58.180\n they're going to be largely retired.\n\n54:58.180 --> 55:00.180\n And like you're going to have more retired people\n\n55:00.180 --> 55:01.180\n than working age people.\n\n55:01.180 --> 55:02.180\n And then like you've got, you know,\n\n55:02.180 --> 55:05.180\n sort of natural questions about who's going to take care of\n\n55:05.180 --> 55:07.180\n all the old folks and who's going to do all the work.\n\n55:07.180 --> 55:11.180\n And the answers to like all of these sorts of questions,\n\n55:11.180 --> 55:13.180\n like where you're sort of running into, you know,\n\n55:13.180 --> 55:16.180\n like constraints of the, you know,\n\n55:16.180 --> 55:20.180\n the world and of society has always been like\n\n55:20.180 --> 55:23.180\n what tech is going to like help us get around this?\n\n55:23.180 --> 55:26.180\n Like when I was a kid in the 70s and 80s,\n\n55:26.180 --> 55:29.180\n like we talked all the time about like population boom,\n\n55:29.180 --> 55:31.180\n population boom, like we're going to,\n\n55:31.180 --> 55:34.180\n like we're not going to be able to like feed the planet.\n\n55:34.180 --> 55:38.180\n And like we were like right in the middle of the Green Revolution\n\n55:38.180 --> 55:44.180\n where like this massive technology driven increase\n\n55:44.180 --> 55:47.180\n in crop productivity like worldwide.\n\n55:47.180 --> 55:49.180\n And like some of that was like taking some of the things\n\n55:49.180 --> 55:52.180\n that we knew in the West and like getting them distributed\n\n55:52.180 --> 55:55.180\n to the, you know, to the developing world.\n\n55:55.180 --> 55:59.180\n And like part of it were things like, you know,\n\n55:59.180 --> 56:03.180\n just smarter biology like helping us increase.\n\n56:03.180 --> 56:08.180\n And like we don't talk about like overpopulation anymore\n\n56:08.180 --> 56:10.180\n because like we can more or less,\n\n56:10.180 --> 56:12.180\n we sort of figured out how to feed the world.\n\n56:12.180 --> 56:14.180\n Like that's a technology story.\n\n56:14.180 --> 56:19.180\n And so like I'm super, super hopeful about the future\n\n56:19.180 --> 56:24.180\n and in the ways where we will be able to apply technology\n\n56:24.180 --> 56:28.180\n to solve some of these super challenging problems.\n\n56:28.180 --> 56:33.180\n Like I've, like one of the things that I'm trying to spend\n\n56:33.180 --> 56:36.180\n my time doing right now is trying to get everybody else\n\n56:36.180 --> 56:39.180\n to be hopeful as well because, you know, back to Harare,\n\n56:39.180 --> 56:41.180\n like we are the stories that we tell.\n\n56:41.180 --> 56:44.180\n Like if we, you know, if we get overly pessimistic right now\n\n56:44.180 --> 56:48.180\n about like the potential future of technology,\n\n56:48.180 --> 56:53.180\n like we, you know, like we may fail to get all of the things\n\n56:53.180 --> 56:56.180\n in place that we need to like have our best possible future.\n\n56:56.180 --> 57:00.180\n And that kind of hopeful optimism, I'm glad that you have it\n\n57:00.180 --> 57:03.180\n because you're leading large groups of engineers\n\n57:03.180 --> 57:06.180\n that are actually defining, that are writing that story,\n\n57:06.180 --> 57:09.180\n that are helping build that future, which is super exciting.\n\n57:09.180 --> 57:13.180\n And I agree with everything you said except I do hope\n\n57:13.180 --> 57:15.180\n Clippy comes back.\n\n57:15.180 --> 57:19.180\n We miss him. I speak for the people.\n\n57:19.180 --> 57:21.180\n So, Galen, thank you so much for talking to me.\n\n57:21.180 --> 57:46.180\n Thank you so much for having me. It was a pleasure.\n\n"
}