{
  "title": "Stuart Russell: Long-Term Future of Artificial Intelligence | Lex Fridman Podcast #9",
  "id": "KsZI5oXBC0k",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:04.720\n The following is a conversation with Stuart Russell. He's a professor of computer science at\n\n00:04.720 --> 00:10.240\n UC Berkeley and a coauthor of a book that introduced me and millions of other people\n\n00:10.240 --> 00:16.720\n to the amazing world of AI called Artificial Intelligence, A Modern Approach. So it was an\n\n00:16.720 --> 00:23.120\n honor for me to have this conversation as part of MIT course in artificial general intelligence\n\n00:23.120 --> 00:28.560\n and the artificial intelligence podcast. If you enjoy it, please subscribe on YouTube,\n\n00:28.560 --> 00:34.320\n iTunes or your podcast provider of choice, or simply connect with me on Twitter at Lex Friedman\n\n00:34.320 --> 00:40.080\n spelled F R I D. And now here's my conversation with Stuart Russell.\n\n00:41.440 --> 00:47.600\n So you've mentioned in 1975 in high school, you've created one of your first AI programs\n\n00:47.600 --> 00:57.360\n that play chess. Were you ever able to build a program that beat you at chess or another board\n\n00:57.360 --> 01:06.880\n game? So my program never beat me at chess. I actually wrote the program at Imperial College.\n\n01:06.880 --> 01:14.400\n So I used to take the bus every Wednesday with a box of cards this big and shove them into the\n\n01:14.400 --> 01:21.440\n card reader. And they gave us eight seconds of CPU time. It took about five seconds to read the cards\n\n01:21.440 --> 01:28.080\n in and compile the code. So we had three seconds of CPU time, which was enough to make one move,\n\n01:28.080 --> 01:32.080\n you know, with a not very deep search. And then we would print that move out and then\n\n01:32.080 --> 01:35.840\n we'd have to go to the back of the queue and wait to feed the cards in again.\n\n01:35.840 --> 01:39.760\n How deep was the search? Are we talking about one move, two moves, three moves?\n\n01:39.760 --> 01:48.160\n No, I think we got an eight move, a depth eight with alpha beta. And we had some tricks of our\n\n01:48.160 --> 01:55.120\n own about move ordering and some pruning of the tree. But you were still able to beat that program?\n\n01:55.120 --> 02:01.840\n Yeah, yeah. I was a reasonable chess player in my youth. I did an Othello program and a\n\n02:01.840 --> 02:08.640\n backgammon program. So when I got to Berkeley, I worked a lot on what we call meta reasoning,\n\n02:08.640 --> 02:14.240\n which really means reasoning about reasoning. And in the case of a game playing program,\n\n02:14.240 --> 02:19.040\n you need to reason about what parts of the search tree you're actually going to explore because the\n\n02:19.040 --> 02:27.840\n search tree is enormous, bigger than the number of atoms in the universe. And the way programs\n\n02:27.840 --> 02:33.280\n succeed and the way humans succeed is by only looking at a small fraction of the search tree.\n\n02:33.280 --> 02:37.760\n And if you look at the right fraction, you play really well. If you look at the wrong fraction,\n\n02:37.760 --> 02:41.600\n if you waste your time thinking about things that are never going to happen,\n\n02:41.600 --> 02:46.480\n moves that no one's ever going to make, then you're going to lose because you won't be able\n\n02:46.480 --> 02:53.920\n to figure out the right decision. So that question of how machines can manage their own computation,\n\n02:53.920 --> 03:00.000\n how they decide what to think about, is the meta reasoning question. And we developed some methods\n\n03:00.720 --> 03:07.040\n for doing that. And very simply, the machine should think about whatever thoughts are going\n\n03:07.040 --> 03:13.840\n to improve its decision quality. We were able to show that both for Othello, which is a standard\n\n03:13.840 --> 03:19.680\n two player game, and for Backgammon, which includes dice rolls, so it's a two player game\n\n03:19.680 --> 03:25.600\n with uncertainty. For both of those cases, we could come up with algorithms that were actually\n\n03:25.600 --> 03:31.760\n much more efficient than the standard alpha beta search, which chess programs at the time were\n\n03:31.760 --> 03:42.000\n using. And that those programs could beat me. And I think you can see the same basic ideas in Alpha\n\n03:42.000 --> 03:51.600\n Go and Alpha Zero today. The way they explore the tree is using a form of meta reasoning to select\n\n03:51.600 --> 03:57.360\n what to think about based on how useful it is to think about it. Is there any insights you can\n\n03:57.360 --> 04:04.720\n describe with our Greek symbols of how do we select which paths to go down? There's really\n\n04:04.720 --> 04:11.280\n two kinds of learning going on. So as you say, Alpha Go learns to evaluate board positions. So\n\n04:11.280 --> 04:19.760\n it can look at a go board. And it actually has probably a superhuman ability to instantly tell\n\n04:19.760 --> 04:28.240\n how promising that situation is. To me, the amazing thing about Alpha Go is not that it can\n\n04:28.240 --> 04:36.960\n be the world champion with its hands tied behind his back, but the fact that if you stop it from\n\n04:36.960 --> 04:42.160\n searching altogether, so you say, okay, you're not allowed to do any thinking ahead. You can just\n\n04:42.160 --> 04:48.240\n consider each of your legal moves and then look at the resulting situation and evaluate it. So\n\n04:48.240 --> 04:53.760\n what we call a depth one search. So just the immediate outcome of your moves and decide if\n\n04:53.760 --> 05:01.040\n that's good or bad. That version of Alpha Go can still play at a professional level.\n\n05:02.000 --> 05:06.960\n And human professionals are sitting there for five, 10 minutes deciding what to do and Alpha Go\n\n05:06.960 --> 05:14.800\n in less than a second can instantly intuit what is the right move to make based on its ability to\n\n05:14.800 --> 05:23.280\n evaluate positions. And that is remarkable because we don't have that level of intuition about Go.\n\n05:23.280 --> 05:31.680\n We actually have to think about the situation. So anyway, that capability that Alpha Go has is one\n\n05:31.680 --> 05:41.520\n big part of why it beats humans. The other big part is that it's able to look ahead 40, 50, 60 moves\n\n05:41.520 --> 05:49.840\n into the future. And if it was considering all possibilities, 40 or 50 or 60 moves into the\n\n05:49.840 --> 06:01.360\n future, that would be 10 to the 200 possibilities. So way more than atoms in the universe and so on.\n\n06:01.360 --> 06:08.800\n So it's very, very selective about what it looks at. So let me try to give you an intuition about\n\n06:08.800 --> 06:14.800\n how you decide what to think about. It's a combination of two things. One is how promising\n\n06:14.800 --> 06:22.560\n it is. So if you're already convinced that a move is terrible, there's no point spending a lot more\n\n06:22.560 --> 06:28.800\n time convincing yourself that it's terrible because it's probably not going to change your mind. So\n\n06:28.800 --> 06:34.400\n the real reason you think is because there's some possibility of changing your mind about what to do.\n\n06:34.400 --> 06:40.960\n And it's that changing your mind that would result then in a better final action in the real world.\n\n06:40.960 --> 06:47.920\n So that's the purpose of thinking is to improve the final action in the real world. So if you\n\n06:47.920 --> 06:53.440\n think about a move that is guaranteed to be terrible, you can convince yourself it's terrible,\n\n06:53.440 --> 06:59.280\n you're still not going to change your mind. But on the other hand, suppose you had a choice between\n\n06:59.280 --> 07:05.040\n two moves. One of them you've already figured out is guaranteed to be a draw, let's say. And then\n\n07:05.040 --> 07:10.000\n the other one looks a little bit worse. It looks fairly likely that if you make that move, you're\n\n07:10.000 --> 07:16.640\n going to lose. But there's still some uncertainty about the value of that move. There's still some\n\n07:16.640 --> 07:22.080\n possibility that it will turn out to be a win. Then it's worth thinking about that. So even though\n\n07:22.080 --> 07:27.840\n it's less promising on average than the other move, which is a good move, it's worth thinking\n\n07:27.840 --> 07:32.160\n about on average than the other move, which is guaranteed to be a draw. There's still some\n\n07:32.160 --> 07:36.800\n purpose in thinking about it because there's a chance that you will change your mind and discover\n\n07:36.800 --> 07:42.720\n that in fact it's a better move. So it's a combination of how good the move appears to be\n\n07:42.720 --> 07:48.640\n and how much uncertainty there is about its value. The more uncertainty, the more it's worth thinking\n\n07:48.640 --> 07:52.240\n about because there's a higher upside if you want to think of it that way.\n\n07:52.240 --> 07:59.760\n And of course in the beginning, especially in the AlphaGo Zero formulation, everything is shrouded\n\n07:59.760 --> 08:06.240\n in uncertainty. So you're really swimming in a sea of uncertainty. So it benefits you to,\n\n08:07.600 --> 08:11.120\n I mean, actually following the same process as you described, but because you're so uncertain\n\n08:11.120 --> 08:15.360\n about everything, you basically have to try a lot of different directions.\n\n08:15.360 --> 08:22.480\n Yeah. So the early parts of the search tree are fairly bushy that it will look at a lot\n\n08:22.480 --> 08:27.840\n of different possibilities, but fairly quickly, the degree of certainty about some of the moves,\n\n08:27.840 --> 08:32.000\n I mean, if a move is really terrible, you'll pretty quickly find out, right? You lose half\n\n08:32.000 --> 08:37.280\n your pieces or half your territory and then you'll say, okay, this is not worth thinking\n\n08:37.280 --> 08:45.360\n about anymore. And then so further down the tree becomes very long and narrow and you're following\n\n08:45.360 --> 08:55.280\n various lines of play, 10, 20, 30, 40, 50 moves into the future. And that again is something that\n\n08:55.280 --> 09:02.000\n human beings have a very hard time doing mainly because they just lack the short term memory.\n\n09:02.000 --> 09:08.960\n You just can't remember a sequence of moves that's 50 moves long. And you can't imagine\n\n09:08.960 --> 09:12.400\n the board correctly for that many moves into the future.\n\n09:13.040 --> 09:18.880\n Of course, the top players, I'm much more familiar with chess, but the top players probably have,\n\n09:19.680 --> 09:26.000\n they have echoes of the same kind of intuition instinct that in a moment's time AlphaGo applies\n\n09:26.720 --> 09:31.600\n when they see a board. I mean, they've seen those patterns, human beings have seen those patterns\n\n09:31.600 --> 09:41.360\n before at the top, at the grandmaster level. It seems that there is some similarities or maybe\n\n09:41.360 --> 09:47.360\n it's our imagination creates a vision of those similarities, but it feels like this kind of\n\n09:47.360 --> 09:53.920\n pattern recognition that the AlphaGo approaches are using is similar to what human beings at the\n\n09:53.920 --> 09:55.360\n top level are using.\n\n09:55.360 --> 10:03.040\n I think there's, there's some truth to that, but not entirely. Yeah. I mean, I think the,\n\n10:03.040 --> 10:10.720\n the extent to which a human grandmaster can reliably instantly recognize the right move\n\n10:10.720 --> 10:15.840\n and instantly recognize the value of the position. I think that's a little bit overrated.\n\n10:15.840 --> 10:20.480\n But if you sacrifice a queen, for example, I mean, there's these, there's these beautiful games of\n\n10:20.480 --> 10:28.400\n chess with Bobby Fischer, somebody where it's seeming to make a bad move. And I'm not sure\n\n10:28.400 --> 10:34.720\n there's a perfect degree of calculation involved where they've calculated all the possible things\n\n10:34.720 --> 10:39.040\n that happen, but there's an instinct there, right? That somehow adds up to\n\n10:40.640 --> 10:46.160\n Yeah. So I think what happens is you, you, you get a sense that there's some possibility in the\n\n10:46.160 --> 10:54.080\n position, even if you make a weird looking move, that it opens up some, some lines of,\n\n10:56.080 --> 11:05.040\n of calculation that otherwise would be definitely bad. And, and it's that intuition that there's\n\n11:05.040 --> 11:10.880\n something here in this position that might, might yield a win.\n\n11:10.880 --> 11:16.080\n And then you follow that, right? And, and in some sense, when a, when a chess player is\n\n11:16.080 --> 11:23.440\n following a line and in his or her mind, they're, they're mentally simulating what the other person\n\n11:23.440 --> 11:29.200\n is going to do, what the opponent is going to do. And they can do that as long as the moves are kind\n\n11:29.200 --> 11:34.640\n of forced, right? As long as there's, you know, there's a, a fort we call a forcing variation\n\n11:34.640 --> 11:39.120\n where the opponent doesn't really have much choice how to respond. And then you follow that,\n\n11:39.120 --> 11:43.520\n how to respond. And then you see if you can force them into a situation where you win.\n\n11:43.520 --> 11:51.920\n You know, we see plenty of mistakes even, even in grandmaster games where they just miss some\n\n11:51.920 --> 11:58.560\n simple three, four, five move combination that, you know, wasn't particularly apparent in,\n\n11:58.560 --> 12:02.560\n in the position, but was still there. That's the thing that makes us human.\n\n12:02.560 --> 12:09.680\n Yeah. So when you mentioned that in Othello, those games were after some matter reasoning\n\n12:09.680 --> 12:14.240\n improvements and research was able to beat you. How did that make you feel?\n\n12:14.960 --> 12:23.680\n Part of the meta reasoning capability that it had was based on learning and, and you could\n\n12:23.680 --> 12:30.240\n sit down the next day and you could just feel that it had got a lot smarter, you know, and all of a\n\n12:30.240 --> 12:37.280\n sudden you really felt like you're sort of pressed against the wall because it was, it was much more\n\n12:37.280 --> 12:43.440\n aggressive and, and was totally unforgiving of any minor mistake that you might make. And, and\n\n12:43.440 --> 12:51.200\n actually it seemed understood the game better than I did. And Gary Kasparov has this quote where\n\n12:52.000 --> 12:56.880\n during his match against Deep Blue, he said, he suddenly felt that there was a new kind of\n\n12:56.880 --> 13:02.480\n intelligence across the board. Do you think that's a scary or an exciting\n\n13:03.120 --> 13:10.320\n possibility for, for Kasparov and for yourself in, in the context of chess, purely sort of\n\n13:10.320 --> 13:16.720\n in this, like that feeling, whatever that is? I think it's definitely an exciting feeling.\n\n13:17.680 --> 13:23.680\n You know, this is what made me work on AI in the first place was as soon as I really understood\n\n13:23.680 --> 13:29.920\n what a computer was, I wanted to make it smart. You know, I started out with the first program\n\n13:29.920 --> 13:35.680\n I wrote was for the Sinclair programmable calculator. And I think you could write a\n\n13:35.680 --> 13:42.800\n 21 step algorithm. That was the biggest program you could write, something like that. And do\n\n13:42.800 --> 13:48.080\n little arithmetic calculations. So I think I implemented Newton's method for a square\n\n13:48.080 --> 13:54.240\n roots and a few other things like that. But then, you know, I thought, okay, if I just had more\n\n13:54.240 --> 14:02.080\n space, I could make this thing intelligent. And so I started thinking about AI and,\n\n14:04.880 --> 14:11.280\n and I think the, the, the thing that's scary is not, is not the chess program\n\n14:11.280 --> 14:17.520\n because, you know, chess programs, they're not in the taking over the world business.\n\n14:19.520 --> 14:29.040\n But if you extrapolate, you know, there are things about chess that don't resemble\n\n14:29.040 --> 14:32.480\n the real world, right? We know, we know the rules of chess.\n\n14:35.120 --> 14:40.720\n The chess board is completely visible to the program where of course the real world is not\n\n14:40.720 --> 14:45.840\n most, most of the real world is, is not visible from wherever you're sitting, so to speak.\n\n14:47.520 --> 14:56.720\n And to overcome those kinds of problems, you need qualitatively different algorithms. Another thing\n\n14:56.720 --> 15:05.520\n about the real world is that, you know, we, we regularly plan ahead on the timescales involving\n\n15:05.520 --> 15:12.400\n billions or trillions of steps. Now we don't plan those in detail, but you know, when you\n\n15:12.400 --> 15:19.680\n choose to do a PhD at Berkeley, that's a five year commitment and that amounts to about a trillion\n\n15:19.680 --> 15:26.240\n motor control steps that you will eventually be committed to. Including going up the stairs,\n\n15:26.240 --> 15:32.240\n opening doors, drinking water. Yeah. I mean, every, every finger movement while you're typing,\n\n15:32.240 --> 15:36.240\n every character of every paper and the thesis and everything. So you're not committing in\n\n15:36.240 --> 15:41.600\n advance to the specific motor control steps, but you're still reasoning on a timescale that\n\n15:41.600 --> 15:50.080\n will eventually reduce to trillions of motor control actions. And so for all of these reasons,\n\n15:52.160 --> 15:58.080\n you know, AlphaGo and Deep Blue and so on don't represent any kind of threat to humanity,\n\n15:58.080 --> 16:07.040\n but they are a step towards it, right? And progress in AI occurs by essentially removing\n\n16:07.040 --> 16:14.640\n one by one these assumptions that make problems easy. Like the assumption of complete observability\n\n16:14.640 --> 16:19.280\n of the situation, right? We remove that assumption, you need a much more complicated\n\n16:20.160 --> 16:25.120\n kind of computing design. It needs, it needs something that actually keeps track of all the\n\n16:25.120 --> 16:30.320\n things you can't see and tries to estimate what's going on. And there's inevitable uncertainty\n\n16:31.040 --> 16:36.880\n in that. So it becomes a much more complicated problem. But, you know, we are removing those\n\n16:36.880 --> 16:42.320\n assumptions. We are starting to have algorithms that can cope with much longer timescales,\n\n16:42.320 --> 16:45.680\n that can cope with uncertainty, that can cope with partial observability.\n\n16:47.520 --> 16:54.240\n And so each of those steps sort of magnifies by a thousand the range of things that we can\n\n16:54.240 --> 16:58.880\n do with AI systems. So the way I started in AI, I wanted to be a psychiatrist for a long time. I\n\n16:58.880 --> 17:04.240\n wanted to understand the mind in high school and of course program and so on. And I showed up\n\n17:04.960 --> 17:10.160\n University of Illinois to an AI lab and they said, okay, I don't have time for you,\n\n17:10.160 --> 17:15.200\n but here's a book, AI and Modern Approach. I think it was the first edition at the time.\n\n17:16.640 --> 17:22.080\n Here, go, go, go learn this. And I remember the lay of the land was, well, it's incredible that\n\n17:22.080 --> 17:26.400\n we solved chess, but we'll never solve go. I mean, it was pretty certain that go\n\n17:27.360 --> 17:33.440\n in the way we thought about systems that reason wasn't possible to solve. And now we've solved\n\n17:33.440 --> 17:39.120\n this. So it's a very... Well, I think I would have said that it's unlikely we could take\n\n17:39.840 --> 17:46.480\n the kind of algorithm that was used for chess and just get it to scale up and work well for go.\n\n17:46.480 --> 17:56.800\n And at the time what we thought was that in order to solve go, we would have to do something similar\n\n17:56.800 --> 18:02.800\n to the way humans manage the complexity of go, which is to break it down into kind of sub games.\n\n18:02.800 --> 18:08.320\n So when a human thinks about a go board, they think about different parts of the board as sort\n\n18:08.320 --> 18:13.280\n of weakly connected to each other. And they think about, okay, within this part of the board, here's\n\n18:13.280 --> 18:18.000\n how things could go in that part of board, here's how things could go. And then you try to sort of\n\n18:18.000 --> 18:24.000\n couple those two analyses together and deal with the interactions and maybe revise your views of\n\n18:24.000 --> 18:28.640\n how things are going to go in each part. And then you've got maybe five, six, seven, ten parts of\n\n18:28.640 --> 18:38.160\n the board. And that actually resembles the real world much more than chess does because in the\n\n18:38.160 --> 18:46.880\n real world, we have work, we have home life, we have sport, different kinds of activities,\n\n18:46.880 --> 18:54.560\n shopping, these all are connected to each other, but they're weakly connected. So when I'm typing\n\n18:54.560 --> 19:01.280\n a paper, I don't simultaneously have to decide which order I'm going to get the milk and the\n\n19:01.280 --> 19:08.240\n butter, that doesn't affect the typing. But I do need to realize, okay, I better finish this\n\n19:08.240 --> 19:12.320\n before the shops close because I don't have anything, I don't have any food at home. So\n\n19:12.320 --> 19:19.040\n there's some weak connection, but not in the way that chess works where everything is tied into a\n\n19:19.040 --> 19:26.080\n single stream of thought. So the thought was that to solve go, we'd have to make progress on stuff\n\n19:26.080 --> 19:29.520\n that would be useful for the real world. And in a way, AlphaGo is a little bit disappointing,\n\n19:29.520 --> 19:39.680\n right? Because the program designed for AlphaGo is actually not that different from Deep Blue\n\n19:39.680 --> 19:48.160\n or even from Arthur Samuel's checker playing program from the 1950s. And in fact, the two\n\n19:48.160 --> 19:53.360\n things that make AlphaGo work is one is this amazing ability to evaluate the positions,\n\n19:53.360 --> 19:57.520\n and the other is the meta reasoning capability, which allows it to\n\n19:57.520 --> 20:04.400\n explore some paths in the tree very deeply and to abandon other paths very quickly.\n\n20:04.400 --> 20:14.160\n So this word meta reasoning, while technically correct, inspires perhaps the wrong degree of\n\n20:14.160 --> 20:19.280\n power that AlphaGo has, for example, the word reasoning is a powerful word. So let me ask you,\n\n20:19.280 --> 20:27.760\n sort of, you were part of the symbolic AI world for a while, like AI was, there's a lot of\n\n20:27.760 --> 20:37.520\n excellent, interesting ideas there that unfortunately met a winter. And so do you think it reemerges?\n\n20:38.320 --> 20:44.320\n So I would say, yeah, it's not quite as simple as that. So the AI winter\n\n20:44.320 --> 20:49.440\n for the first winter that was actually named as such was the one in the late 80s.\n\n20:51.440 --> 21:00.960\n And that came about because in the mid 80s, there was a really a concerted attempt to push AI\n\n21:01.520 --> 21:09.120\n out into the real world using what was called expert system technology. And for the most part,\n\n21:09.120 --> 21:17.040\n that technology was just not ready for primetime. They were trying, in many cases, to do a form of\n\n21:17.040 --> 21:23.200\n uncertain reasoning, judgment, combinations of evidence, diagnosis, those kinds of things,\n\n21:24.480 --> 21:31.600\n which was simply invalid. And when you try to apply invalid reasoning methods to real problems,\n\n21:31.600 --> 21:36.720\n you can fudge it for small versions of the problem. But when it starts to get larger,\n\n21:36.720 --> 21:44.240\n the thing just falls apart. So many companies found that the stuff just didn't work, and they\n\n21:44.240 --> 21:50.400\n were spending tons of money on consultants to try to make it work. And there were other\n\n21:50.400 --> 21:56.160\n practical reasons, like they were asking the companies to buy incredibly expensive\n\n21:56.160 --> 22:06.080\n Lisp machine workstations, which were literally between $50,000 and $100,000 in 1980s money,\n\n22:06.080 --> 22:13.920\n which would be like between $150,000 and $300,000 per workstation in current prices.\n\n22:13.920 --> 22:17.280\n And then the bottom line, they weren't seeing a profit from it.\n\n22:17.280 --> 22:22.880\n Yeah, in many cases. I think there were some successes, there's no doubt about that. But\n\n22:23.920 --> 22:30.800\n people, I would say, overinvested. Every major company was starting an AI department, just like\n\n22:30.800 --> 22:40.000\n now. And I worry a bit that we might see similar disappointments, not because the current technology\n\n22:40.000 --> 22:51.280\n is invalid, but it's limited in its scope. And it's almost the duel of the scope problems that\n\n22:51.280 --> 22:56.720\n expert systems had. So what have you learned from that hype cycle? And what can we do to\n\n22:56.720 --> 23:02.800\n prevent another winter, for example? Yeah, so when I'm giving talks these days,\n\n23:02.800 --> 23:11.120\n that's one of the warnings that I give. So this is a two part warning slide. One is that rather\n\n23:11.120 --> 23:18.880\n than data being the new oil, data is the new snake oil. That's a good line. And then the other\n\n23:18.880 --> 23:30.800\n is that we might see a kind of very visible failure in some of the major application areas. And I think\n\n23:30.800 --> 23:40.000\n self driving cars would be the flagship. And I think when you look at the history,\n\n23:40.000 --> 23:49.760\n so the first self driving car was on the freeway, driving itself, changing lanes, overtaking in 1987.\n\n23:52.000 --> 23:59.040\n And so it's more than 30 years. And that kind of looks like where we are today, right? You know,\n\n23:59.040 --> 24:05.920\n prototypes on the freeway, changing lanes and overtaking. Now, I think that's one of the things\n\n24:05.920 --> 24:12.400\n that's been made, particularly on the perception side. So we worked a lot on autonomous vehicles\n\n24:12.400 --> 24:21.200\n in the early mid 90s at Berkeley. And we had our own big demonstrations. We put congressmen into\n\n24:21.680 --> 24:30.640\n self driving cars and had them zooming along the freeway. And the problem was clearly perception.\n\n24:30.640 --> 24:36.160\n At the time, the problem was perception. Yeah. So in simulation, with perfect perception,\n\n24:36.160 --> 24:40.640\n you could actually show that you can drive safely for a long time, even if the other cars are\n\n24:40.640 --> 24:48.800\n misbehaving and so on. But simultaneously, we worked on machine vision for detecting cars and\n\n24:48.800 --> 24:56.880\n tracking pedestrians and so on. And we couldn't get the cars to do that. And so we had to do\n\n24:56.880 --> 25:02.560\n that for pedestrians and so on. And we couldn't get the reliability of detection and tracking\n\n25:03.120 --> 25:10.800\n up to a high enough level, particularly in bad weather conditions, nighttime,\n\n25:10.800 --> 25:15.920\n rainfall. Good enough for demos, but perhaps not good enough to cover the general operation.\n\n25:15.920 --> 25:19.680\n Yeah. So the thing about driving is, you know, suppose you're a taxi driver, you know,\n\n25:19.680 --> 25:25.200\n and you drive every day, eight hours a day for 10 years, right? That's 100 million seconds of\n\n25:25.200 --> 25:30.560\n driving, you know, and any one of those seconds, you can make a fatal mistake. So you're talking\n\n25:30.560 --> 25:40.080\n about eight nines of reliability, right? Now, if your vision system only detects 98.3% of the\n\n25:40.080 --> 25:47.200\n vehicles, right, then that's sort of, you know, one in a bit nines of reliability. So you have\n\n25:47.200 --> 25:54.560\n another seven orders of magnitude to go. And this is what people don't understand. They think,\n\n25:54.560 --> 26:01.440\n oh, because I had a successful demo, I'm pretty much done. But you're not even within seven orders\n\n26:01.440 --> 26:09.760\n of magnitude of being done. And that's the difficulty. And it's not the, can I follow a\n\n26:09.760 --> 26:15.280\n white line? That's not the problem, right? We follow a white line all the way across the country.\n\n26:16.640 --> 26:22.160\n But it's the weird stuff that happens. It's all the edge cases, yeah.\n\n26:22.160 --> 26:28.480\n The edge case, other drivers doing weird things. You know, so if you talk to Google, right, so\n\n26:29.200 --> 26:35.600\n they had actually a very classical architecture where, you know, you had machine vision which\n\n26:35.600 --> 26:41.440\n would detect all the other cars and pedestrians and the white lines and the road signs. And then\n\n26:41.440 --> 26:48.880\n basically that was fed into a logical database. And then you had a classical 1970s rule based\n\n26:48.880 --> 26:55.360\n expert system telling you, okay, if you're in the middle lane and there's a bicyclist in the right\n\n26:55.360 --> 27:02.640\n lane who is signaling this, then you do that, right? And what they found was that every day\n\n27:02.640 --> 27:06.560\n they'd go out and there'd be another situation that the rules didn't cover. You know, so they'd\n\n27:06.560 --> 27:10.880\n come to a traffic circle and there's a little girl riding her bicycle the wrong way around\n\n27:10.880 --> 27:14.720\n the traffic circle. Okay, what do you do? We don't have a rule. Oh my God. Okay, stop.\n\n27:14.720 --> 27:20.560\n And then, you know, they come back and add more rules and they just found that this was not really\n\n27:20.560 --> 27:28.240\n converging. And if you think about it, right, how do you deal with an unexpected situation,\n\n27:28.240 --> 27:35.040\n meaning one that you've never previously encountered and the sort of reasoning required\n\n27:35.600 --> 27:41.200\n to figure out the solution for that situation has never been done. It doesn't match any previous\n\n27:41.200 --> 27:46.560\n situation in terms of the kind of reasoning you have to do. Well, you know, in chess programs,\n\n27:46.560 --> 27:51.280\n this happens all the time, right? You're constantly coming up with situations you haven't\n\n27:51.280 --> 27:56.480\n seen before and you have to reason about them and you have to think about, okay, here are the\n\n27:56.480 --> 28:01.680\n possible things I could do. Here are the outcomes. Here's how desirable the outcomes are and then\n\n28:01.680 --> 28:05.440\n pick the right one. You know, in the 90s, we were saying, okay, this is how you're going to have to\n\n28:05.440 --> 28:10.880\n do automated vehicles. They're going to have to have a look ahead capability, but the look ahead\n\n28:10.880 --> 28:18.000\n for driving is more difficult than it is for chess because there's humans and they're less\n\n28:18.000 --> 28:23.840\n predictable than chess pieces. Well, then you have an opponent in chess who's also somewhat\n\n28:23.840 --> 28:29.920\n unpredictable. But for example, in chess, you always know the opponent's intention. They're\n\n28:29.920 --> 28:36.000\n trying to beat you, right? Whereas in driving, you don't know is this guy trying to turn left\n\n28:36.000 --> 28:42.000\n or has he just forgotten to turn off his turn signal or is he drunk or is he changing the\n\n28:42.000 --> 28:47.520\n channel on his radio or whatever it might be. You've got to try and figure out the mental state,\n\n28:47.520 --> 28:53.520\n the intent of the other drivers to forecast the possible evolutions of their trajectories.\n\n28:54.880 --> 28:58.960\n And then you've got to figure out, okay, which is the trajectory for me that's going to be safest.\n\n29:00.400 --> 29:04.720\n And those all interact with each other because the other drivers are going to react to your\n\n29:04.720 --> 29:10.640\n trajectory and so on. So, you know, they've got the classic merging onto the freeway problem where\n\n29:10.640 --> 29:15.520\n you're kind of racing a vehicle that's already on the freeway and you're going to pull ahead of\n\n29:15.520 --> 29:19.920\n them or you're going to let them go first and pull in behind and you get this sort of uncertainty\n\n29:19.920 --> 29:29.440\n about who's going first. So all those kinds of things mean that you need a decision making\n\n29:29.440 --> 29:37.200\n architecture that's very different from either a rule based system or it seems to me kind of an\n\n29:37.200 --> 29:43.840\n end to end neural network system. So just as AlphaGo is pretty good when it doesn't do any\n\n29:43.840 --> 29:49.920\n look ahead, but it's way, way, way, way better when it does, I think the same is going to be\n\n29:49.920 --> 29:55.120\n true for driving. You can have a driving system that's pretty good when it doesn't do any look\n\n29:55.120 --> 30:03.440\n ahead, but that's not good enough. And we've already seen multiple deaths caused by poorly\n\n30:03.440 --> 30:08.480\n designed machine learning algorithms that don't really understand what they're doing.\n\n30:09.360 --> 30:16.480\n Yeah. On several levels, I think on the perception side, there's mistakes being made by those\n\n30:16.480 --> 30:21.200\n algorithms where the perception is very shallow. On the planning side, the look ahead, like you\n\n30:21.200 --> 30:31.200\n said, and the thing that we come up against that's really interesting when you try to deploy systems\n\n30:31.200 --> 30:36.160\n in the real world is you can't think of an artificial intelligence system as a thing that\n\n30:36.160 --> 30:41.440\n responds to the world always. You have to realize that it's an agent that others will respond to as\n\n30:41.440 --> 30:47.680\n well. So in order to drive successfully, you can't just try to do obstacle avoidance.\n\n30:47.680 --> 30:51.920\n Right. You can't pretend that you're invisible, right? You're the invisible car.\n\n30:51.920 --> 30:53.440\n Right. It doesn't work that way.\n\n30:53.440 --> 30:58.320\n I mean, but you have to assert yet others have to be scared of you. Just we're all,\n\n30:58.320 --> 31:04.080\n there's this tension, there's this game. So if we study a lot of work with pedestrians,\n\n31:04.080 --> 31:10.000\n if you approach pedestrians as purely an obstacle avoidance, so you're doing look ahead as in\n\n31:10.000 --> 31:15.200\n modeling the intent that they're not going to, they're going to take advantage of you. They're\n\n31:15.200 --> 31:21.040\n not going to respect you at all. There has to be a tension, a fear, some amount of uncertainty.\n\n31:21.040 --> 31:24.160\n That's how we have created.\n\n31:24.160 --> 31:29.760\n Or at least just a kind of a resoluteness. You have to display a certain amount of\n\n31:29.760 --> 31:39.120\n resoluteness. You can't be too tentative. And yeah, so the solutions then become\n\n31:39.120 --> 31:46.000\n pretty complicated, right? You get into game theoretic analyses. And so at Berkeley now,\n\n31:46.000 --> 31:51.440\n we're working a lot on this kind of interaction between machines and humans.\n\n31:51.440 --> 31:53.200\n And that's exciting.\n\n31:53.200 --> 32:04.400\n And so my colleague, Ankur Dragan, actually, if you formulate the problem game theoretically,\n\n32:04.400 --> 32:10.080\n you just let the system figure out the solution. It does interesting unexpected things. Like\n\n32:10.080 --> 32:17.920\n sometimes at a stop sign, if no one is going first, the car will actually back up a little,\n\n32:18.640 --> 32:23.680\n right? And just to indicate to the other cars that they should go. And that's something it\n\n32:23.680 --> 32:29.920\n invented entirely by itself. We didn't say this is the language of communication at stop signs.\n\n32:29.920 --> 32:30.720\n It figured it out.\n\n32:30.720 --> 32:38.960\n That's really interesting. So let me one just step back for a second. Just this beautiful\n\n32:38.960 --> 32:47.040\n philosophical notion. So Pamela McCordick in 1979 wrote, AI began with the ancient wish to\n\n32:47.040 --> 32:53.840\n forge the gods. So when you think about the history of our civilization, do you think\n\n32:53.840 --> 33:01.520\n that there is an inherent desire to create, let's not say gods, but to create superintelligence?\n\n33:01.520 --> 33:10.320\n Is it inherent to us? Is it in our genes? That the natural arc of human civilization is to create\n\n33:11.280 --> 33:19.200\n things that are of greater and greater power and perhaps echoes of ourselves. So to create the gods\n\n33:19.200 --> 33:32.080\n as Pamela said. Maybe. I mean, we're all individuals, but certainly we see over and over\n\n33:32.080 --> 33:40.240\n again in history, individuals who thought about this possibility. Hopefully when I'm not being too\n\n33:40.240 --> 33:47.440\n philosophical here, but if you look at the arc of this, where this is going and we'll talk about AI\n\n33:47.440 --> 33:54.320\n safety, we'll talk about greater and greater intelligence. Do you see that there in, when you\n\n33:54.320 --> 33:59.680\n created the Othello program and you felt this excitement, what was that excitement? Was it\n\n33:59.680 --> 34:07.680\n excitement of a tinkerer who created something cool like a clock? Or was there a magic or was\n\n34:07.680 --> 34:14.320\n it more like a child being born? Yeah. So I mean, I certainly understand that viewpoint. And if you\n\n34:14.320 --> 34:23.520\n look at the Lighthill report, which was, so in the 70s, there was a lot of controversy in the UK\n\n34:23.520 --> 34:30.000\n about AI and whether it was for real and how much money the government should invest. And\n\n34:32.320 --> 34:39.040\n there was a long story, but the government commissioned a report by Lighthill, who was a\n\n34:39.040 --> 34:48.800\n physicist, and he wrote a very damning report about AI, which I think was the point. And he\n\n34:48.800 --> 34:59.200\n said that these are frustrated men who are unable to have children would like to create and create\n\n34:59.200 --> 35:16.800\n a life as a kind of replacement, which I think is really pretty unfair. But there is a kind of magic,\n\n35:17.360 --> 35:28.000\n I would say, when you build something and what you're building in is really just, you're building\n\n35:28.000 --> 35:35.200\n in some understanding of the principles of learning and decision making. And to see those\n\n35:35.200 --> 35:45.600\n principles actually then turn into intelligent behavior in specific situations, it's an\n\n35:45.600 --> 35:58.400\n incredible thing. And that is naturally going to make you think, okay, where does this end?\n\n36:00.000 --> 36:08.240\n And so there's magical optimistic views of where it ends, whatever your view of optimism is,\n\n36:08.240 --> 36:13.280\n whatever your view of utopia is, it's probably different for everybody. But you've often talked\n\n36:13.280 --> 36:26.000\n about concerns you have of how things may go wrong. So I've talked to Max Tegmark. There's a\n\n36:26.000 --> 36:33.280\n lot of interesting ways to think about AI safety. You're one of the seminal people thinking about\n\n36:33.280 --> 36:39.440\n this problem amongst sort of being in the weeds of actually solving specific AI problems. You're\n\n36:39.440 --> 36:44.800\n also thinking about the big picture of where are we going? So can you talk about several elements\n\n36:44.800 --> 36:52.800\n of it? Let's just talk about maybe the control problem. So this idea of losing ability to control\n\n36:52.800 --> 37:00.000\n the behavior in our AI system. So how do you see that? How do you see that coming about?\n\n37:00.000 --> 37:04.480\n What do you think we can do to manage it?\n\n37:04.480 --> 37:09.280\n Well, so it doesn't take a genius to realize that if you make something that's smarter than you,\n\n37:09.280 --> 37:20.640\n you might have a problem. Alan Turing wrote about this and gave lectures about this in 1951.\n\n37:22.240 --> 37:31.200\n He did a lecture on the radio and he basically says, once the machine thinking method starts,\n\n37:31.200 --> 37:42.640\n very quickly they'll outstrip humanity. And if we're lucky, we might be able to turn off the power\n\n37:42.640 --> 37:49.360\n at strategic moments, but even so, our species would be humbled. Actually, he was wrong about\n\n37:49.360 --> 37:55.120\n that. If it's sufficiently intelligent machine, it's not going to let you switch it off. It's\n\n37:55.120 --> 37:59.440\n actually in competition with you. So what do you think is most likely going to happen?\n\n37:59.440 --> 38:06.560\n What do you think is meant just for a quick tangent, if we shut off this super intelligent\n\n38:06.560 --> 38:16.400\n machine that our species will be humbled? I think he means that we would realize that\n\n38:16.400 --> 38:22.240\n we are inferior, right? That we only survive by the skin of our teeth because we happen to get\n\n38:22.240 --> 38:30.240\n to the off switch just in time. And if we hadn't, then we would have lost control over the earth.\n\n38:32.160 --> 38:36.800\n Are you more worried when you think about this stuff about super intelligent AI,\n\n38:36.800 --> 38:43.200\n or are you more worried about super powerful AI that's not aligned with our values? So the\n\n38:43.200 --> 38:54.560\n paperclip scenarios kind of... So the main problem I'm working on is the control problem, the problem\n\n38:54.560 --> 39:01.520\n of machines pursuing objectives that are, as you say, not aligned with human objectives. And\n\n39:02.320 --> 39:07.520\n this has been the way we've thought about AI since the beginning.\n\n39:07.520 --> 39:14.320\n You build a machine for optimizing, and then you put in some objective, and it optimizes, right?\n\n39:14.320 --> 39:23.920\n And we can think of this as the King Midas problem, right? Because if the King Midas put\n\n39:23.920 --> 39:30.080\n in this objective, everything I touch should turn to gold. And the gods, that's like the machine,\n\n39:30.080 --> 39:35.520\n they said, okay, done. You now have this power. And of course, his father,\n\n39:35.520 --> 39:43.840\n his drink, and his family all turned to gold. And then he dies of misery and starvation. And\n\n39:47.200 --> 39:54.240\n it's a warning, it's a failure mode that pretty much every culture in history has had some story\n\n39:54.240 --> 39:59.520\n along the same lines. There's the genie that gives you three wishes, and the third wish is always,\n\n39:59.520 --> 40:09.040\n you know, please undo the first two wishes because I messed up. And when Arthur Samuel wrote his\n\n40:09.920 --> 40:13.680\n checker playing program, which learned to play checkers considerably better than\n\n40:13.680 --> 40:17.200\n Arthur Samuel could play, and actually reached a pretty decent standard.\n\n40:20.080 --> 40:24.640\n Norbert Wiener, who was one of the major mathematicians of the 20th century,\n\n40:24.640 --> 40:31.680\n he's sort of the father of modern automation control systems. He saw this and he basically\n\n40:31.680 --> 40:38.160\n extrapolated, as Turing did, and said, okay, this is how we could lose control.\n\n40:39.840 --> 40:49.680\n And specifically, that we have to be certain that the purpose we put into the machine is the\n\n40:49.680 --> 40:55.520\n purpose which we really desire. And the problem is, we can't do that.\n\n40:57.840 --> 41:00.720\n You mean we're not, it's a very difficult to encode,\n\n41:00.720 --> 41:05.360\n to put our values on paper is really difficult, or you're just saying it's impossible?\n\n41:10.720 --> 41:17.840\n So theoretically, it's possible, but in practice, it's extremely unlikely that we could\n\n41:17.840 --> 41:23.680\n specify correctly in advance, the full range of concerns of humanity.\n\n41:24.160 --> 41:27.120\n You talked about cultural transmission of values,\n\n41:27.120 --> 41:30.640\n I think is how humans to human transmission of values happens, right?\n\n41:31.680 --> 41:37.760\n Well, we learn, yeah, I mean, as we grow up, we learn about the values that matter,\n\n41:37.760 --> 41:43.040\n how things should go, what is reasonable to pursue and what isn't reasonable to pursue.\n\n41:43.600 --> 41:46.000\n You think machines can learn in the same kind of way?\n\n41:46.000 --> 41:52.000\n Yeah, so I think that what we need to do is to get away from this idea that\n\n41:52.560 --> 41:55.680\n you build an optimising machine, and then you put the objective into it.\n\n41:56.800 --> 42:03.840\n Because if it's possible that you might put in a wrong objective, and we already know this is\n\n42:03.840 --> 42:09.600\n possible because it's happened lots of times, right? That means that the machine should never\n\n42:09.600 --> 42:18.000\n take an objective that's given as gospel truth. Because once it takes the objective as gospel\n\n42:18.000 --> 42:26.480\n truth, then it believes that whatever actions it's taking in pursuit of that objective are\n\n42:26.480 --> 42:30.480\n the correct things to do. So you could be jumping up and down and saying, no, no, no,\n\n42:30.480 --> 42:35.280\n no, you're going to destroy the world, but the machine knows what the true objective is and is\n\n42:35.280 --> 42:41.840\n pursuing it, and tough luck to you. And this is not restricted to AI, right? This is, I think,\n\n42:42.480 --> 42:48.080\n many of the 20th century technologies, right? So in statistics, you minimise a loss function,\n\n42:48.080 --> 42:53.440\n the loss function is exogenously specified. In control theory, you minimise a cost function.\n\n42:53.440 --> 42:59.040\n In operations research, you maximise a reward function, and so on. So in all these disciplines,\n\n42:59.040 --> 43:07.040\n this is how we conceive of the problem. And it's the wrong problem because we cannot specify\n\n43:07.040 --> 43:13.840\n with certainty the correct objective, right? We need uncertainty, we need the machine to be\n\n43:13.840 --> 43:18.080\n uncertain about what it is that it's supposed to be maximising.\n\n43:18.080 --> 43:23.920\n Favourite idea of yours, I've heard you say somewhere, well, I shouldn't pick favourites,\n\n43:23.920 --> 43:31.440\n but it just sounds beautiful, we need to teach machines humility. It's a beautiful way to put it,\n\n43:31.440 --> 43:32.640\n I love it.\n\n43:32.640 --> 43:39.520\n That they're humble, they know that they don't know what it is they're supposed to be doing,\n\n43:39.520 --> 43:47.200\n and that those objectives, I mean, they exist, they're within us, but we may not be able to\n\n43:47.200 --> 43:56.160\n we may not be able to explicate them, we may not even know how we want our future to go.\n\n43:56.160 --> 43:58.240\n Exactly.\n\n43:58.240 --> 44:06.800\n And the machine, a machine that's uncertain is going to be deferential to us. So if we say,\n\n44:06.800 --> 44:11.840\n don't do that, well, now the machines learn something a bit more about our true objectives,\n\n44:11.840 --> 44:16.480\n because something that it thought was reasonable in pursuit of our objective,\n\n44:16.480 --> 44:20.640\n turns out not to be, so now it's learned something. So it's going to defer because\n\n44:20.640 --> 44:30.240\n it wants to be doing what we really want. And that point, I think, is absolutely central\n\n44:30.240 --> 44:37.920\n to solving the control problem. And it's a different kind of AI when you take away this\n\n44:37.920 --> 44:44.560\n idea that the objective is known, then in fact, a lot of the theoretical frameworks that we're so\n\n44:44.560 --> 44:53.440\n familiar with, you know, Markov decision processes, goal based planning, you know,\n\n44:53.440 --> 44:59.280\n standard games research, all of these techniques actually become inapplicable.\n\n44:59.280 --> 45:11.360\n And you get a more complicated problem because now the interaction with the human becomes part\n\n45:11.360 --> 45:20.400\n of the problem. Because the human by making choices is giving you more information about\n\n45:21.200 --> 45:25.360\n the true objective and that information helps you achieve the objective better.\n\n45:26.640 --> 45:31.840\n And so that really means that you're mostly dealing with game theoretic problems where\n\n45:31.840 --> 45:34.320\n you've got the machine and the human and they're coupled together,\n\n45:35.840 --> 45:39.040\n rather than a machine going off by itself with a fixed objective.\n\n45:39.040 --> 45:46.800\n LW. Which is fascinating on the machine and the human level that we, when you don't have an\n\n45:46.800 --> 45:53.120\n objective, means you're together coming up with an objective. I mean, there's a lot of philosophy\n\n45:53.120 --> 45:58.880\n that, you know, you could argue that life doesn't really have meaning. We together agree on what\n\n45:58.880 --> 46:05.920\n gives it meaning and we kind of culturally create things that give why the heck we are on this earth\n\n46:05.920 --> 46:11.280\n anyway. We together as a society create that meaning and you have to learn that objective.\n\n46:11.280 --> 46:14.960\n And one of the biggest, I thought that's where you were going to go for a second,\n\n46:15.760 --> 46:21.200\n one of the biggest troubles we run into outside of statistics and machine learning and AI\n\n46:21.200 --> 46:28.080\n and just human civilization is when you look at, I came from, I was born in the Soviet Union\n\n46:28.080 --> 46:36.320\n and the history of the 20th century, we ran into the most trouble, us humans, when there was a\n\n46:36.320 --> 46:41.200\n certainty about the objective and you do whatever it takes to achieve that objective, whether you're\n\n46:41.200 --> 46:47.040\n talking about Germany or communist Russia. You get into trouble with humans.\n\n46:47.040 --> 46:52.400\n I would say with, you know, corporations, in fact, some people argue that, you know,\n\n46:52.400 --> 46:57.200\n we don't have to look forward to a time when AI systems take over the world. They already have\n\n46:57.200 --> 47:03.760\n and they call corporations, right? That corporations happen to be using people as\n\n47:03.760 --> 47:10.160\n components right now, but they are effectively algorithmic machines and they're optimizing\n\n47:10.160 --> 47:17.520\n an objective, which is quarterly profit that isn't aligned with overall wellbeing of the human race.\n\n47:17.520 --> 47:23.440\n And they are destroying the world. They are primarily responsible for our inability to tackle\n\n47:23.440 --> 47:30.240\n climate change. So I think that's one way of thinking about what's going on with corporations,\n\n47:30.240 --> 47:39.680\n but I think the point you're making is valid that there are many systems in the real world where\n\n47:39.680 --> 47:48.480\n we've sort of prematurely fixed on the objective and then decoupled the machine from those that's\n\n47:48.480 --> 47:54.800\n supposed to be serving. And I think you see this with government, right? Government is supposed to\n\n47:54.800 --> 48:02.720\n be a machine that serves people, but instead it tends to be taken over by people who have their\n\n48:02.720 --> 48:08.160\n own objective and use government to optimize that objective regardless of what people want.\n\n48:09.120 --> 48:16.080\n Do you find appealing the idea of almost arguing machines where you have multiple AI systems with\n\n48:16.080 --> 48:22.480\n a clear fixed objective. We have in government, the red team and the blue team, they're very fixed on\n\n48:22.480 --> 48:28.640\n their objectives and they argue and they kind of may disagree, but it kind of seems to make it\n\n48:29.760 --> 48:39.680\n work somewhat that the duality of it. Okay. Let's go a hundred years back when there was still was\n\n48:39.680 --> 48:46.480\n going on or at the founding of this country, there was disagreements and that disagreement is where,\n\n48:46.480 --> 48:52.160\n so it was a balance between certainty and forced humility because the power was distributed.\n\n48:53.840 --> 49:04.000\n Yeah. I think that the nature of debate and disagreement argument takes as a premise,\n\n49:04.000 --> 49:12.320\n the idea that you could be wrong, which means that you're not necessarily absolutely convinced\n\n49:12.320 --> 49:19.440\n that your objective is the correct one. If you were absolutely convinced, there'd be no point\n\n49:19.440 --> 49:24.080\n in having any discussion or argument because you would never change your mind and there wouldn't\n\n49:24.080 --> 49:32.000\n be any sort of synthesis or anything like that. I think you can think of argumentation as an\n\n49:32.000 --> 49:44.640\n implementation of a form of uncertain reasoning. I've been reading recently about utilitarianism\n\n49:44.640 --> 49:50.640\n and the history of efforts to define in a sort of clear mathematical way,\n\n49:53.600 --> 50:00.400\n if you like a formula for moral or political decision making. It's really interesting that\n\n50:00.400 --> 50:07.920\n the parallels between the philosophical discussions going back 200 years and what you see now in\n\n50:07.920 --> 50:14.640\n discussions about existential risk because it's almost exactly the same. Someone would say,\n\n50:14.640 --> 50:20.720\n okay, well here's a formula for how we should make decisions. Utilitarianism is roughly each\n\n50:20.720 --> 50:27.120\n person has a utility function and then we make decisions to maximize the sum of everybody's\n\n50:27.120 --> 50:36.480\n utility. Then people point out, well, in that case, the best policy is one that leads to\n\n50:36.480 --> 50:42.560\n the enormously vast population, all of whom are living a life that's barely worth living.\n\n50:44.000 --> 50:50.640\n This is called the repugnant conclusion. Another version is that we should maximize\n\n50:50.640 --> 50:57.840\n pleasure and that's what we mean by utility. Then you'll get people effectively saying, well,\n\n50:57.840 --> 51:03.040\n in that case, we might as well just have everyone hooked up to a heroin drip. They didn't use those\n\n51:03.040 --> 51:11.520\n words, but that debate was happening in the 19th century as it is now about AI that if we get the\n\n51:11.520 --> 51:20.160\n formula wrong, we're going to have AI systems working towards an outcome that in retrospect\n\n51:20.160 --> 51:26.400\n would be exactly wrong. Do you think there's, as beautifully put, so the echoes are there,\n\n51:26.400 --> 51:32.880\n but do you think, I mean, if you look at Sam Harris, our imagination worries about the AI\n\n51:32.880 --> 51:44.080\n version of that because of the speed at which the things going wrong in the utilitarian context\n\n51:44.080 --> 51:53.520\n could happen. Is that a worry for you? Yeah. I think that in most cases, not in all, but if we\n\n51:53.520 --> 52:00.560\n have a wrong political idea, we see it starting to go wrong and we're not completely stupid and so\n\n52:00.560 --> 52:09.600\n we say, okay, maybe that was a mistake. Let's try something different. Also, we're very slow and\n\n52:09.600 --> 52:14.800\n inefficient about implementing these things and so on. So you have to worry when you have\n\n52:14.800 --> 52:20.800\n corporations or political systems that are extremely efficient. But when we look at AI systems\n\n52:22.240 --> 52:29.200\n or even just computers in general, they have this different characteristic from ordinary\n\n52:29.760 --> 52:36.000\n human activity in the past. So let's say you were a surgeon, you had some idea about how to do some\n\n52:36.000 --> 52:42.400\n operation. Well, and let's say you were wrong, that way of doing the operation would mostly\n\n52:42.400 --> 52:49.280\n kill the patient. Well, you'd find out pretty quickly, like after three, maybe three or four\n\n52:49.280 --> 53:00.160\n tries. But that isn't true for pharmaceutical companies because they don't do three or four\n\n53:00.160 --> 53:05.840\n operations. They manufacture three or four billion pills and they sell them and then they find out\n\n53:05.840 --> 53:11.520\n maybe six months or a year later that, oh, people are dying of heart attacks or getting cancer from\n\n53:11.520 --> 53:18.720\n this drug. And so that's why we have the FDA, right? Because of the scalability of pharmaceutical\n\n53:18.720 --> 53:29.840\n production. And there have been some unbelievably bad episodes in the history of pharmaceuticals\n\n53:29.840 --> 53:36.640\n and adulteration of products and so on that have killed tens of thousands or paralyzed hundreds\n\n53:36.640 --> 53:43.760\n of thousands of people. Now with computers, we have that same scalability problem that you can\n\n53:43.760 --> 53:49.760\n sit there and type for I equals one to five billion do, right? And all of a sudden you're\n\n53:49.760 --> 53:56.160\n having an impact on a global scale. And yet we have no FDA, right? There's absolutely no controls\n\n53:56.160 --> 54:02.480\n at all over what a bunch of undergraduates with too much caffeine can do to the world.\n\n54:03.440 --> 54:10.160\n And we look at what happened with Facebook, well, social media in general and click through\n\n54:10.160 --> 54:18.720\n optimization. So you have a simple feedback algorithm that's trying to just optimize click\n\n54:18.720 --> 54:24.400\n through, right? That sounds reasonable, right? Because you don't want to be feeding people ads\n\n54:24.400 --> 54:33.200\n that they don't care about or not interested in. And you might even think of that process as\n\n54:33.840 --> 54:40.560\n simply adjusting the feeding of ads or news articles or whatever it might be\n\n54:41.280 --> 54:45.440\n to match people's preferences, right? Which sounds like a good idea.\n\n54:47.360 --> 54:54.080\n But in fact, that isn't how the algorithm works, right? You make more money,\n\n54:54.080 --> 55:01.200\n the algorithm makes more money if it can better predict what people are going to click on,\n\n55:01.200 --> 55:07.680\n because then it can feed them exactly that, right? So the way to maximize click through\n\n55:07.680 --> 55:14.640\n is actually to modify the people to make them more predictable. And one way to do that is to\n\n55:16.320 --> 55:23.600\n feed them information, which will change their behavior and preferences towards extremes that\n\n55:23.600 --> 55:28.400\n make them predictable. Whatever is the nearest extreme or the nearest predictable point,\n\n55:29.200 --> 55:33.920\n that's where you're going to end up. And the machines will force you there.\n\n55:35.520 --> 55:40.240\n And I think there's a reasonable argument to say that this, among other things,\n\n55:40.240 --> 55:43.920\n is contributing to the destruction of democracy in the world.\n\n55:47.280 --> 55:52.720\n And where was the oversight of this process? Where were the people saying, okay,\n\n55:52.720 --> 55:57.680\n you would like to apply this algorithm to 5 billion people on the face of the earth.\n\n55:58.560 --> 56:03.760\n Can you show me that it's safe? Can you show me that it won't have various kinds of negative\n\n56:03.760 --> 56:09.200\n effects? No, there was no one asking that question. There was no one placed between\n\n56:11.120 --> 56:16.160\n the undergrads with too much caffeine and the human race. They just did it.\n\n56:16.160 --> 56:22.800\n But some way outside the scope of my knowledge, so economists would argue that the, what is it,\n\n56:22.800 --> 56:29.280\n the invisible hand, so the capitalist system, it was the oversight. So if you're going to corrupt\n\n56:29.280 --> 56:33.600\n society with whatever decision you make as a company, then that's going to be reflected in\n\n56:33.600 --> 56:38.160\n people not using your product. That's one model of oversight.\n\n56:38.160 --> 56:48.000\n We shall see, but in the meantime, but you might even have broken the political system\n\n56:48.000 --> 56:51.440\n that enables capitalism to function. Well, you've changed it.\n\n56:53.040 --> 56:54.960\n We shall see.\n\n56:54.960 --> 57:01.360\n Change is often painful. So my question is absolutely, it's fascinating. You're absolutely\n\n57:01.360 --> 57:09.040\n right that there was zero oversight on algorithms that can have a profound civilization changing\n\n57:09.040 --> 57:15.840\n effect. So do you think it's possible? I mean, I haven't, have you seen government? So do you\n\n57:15.840 --> 57:24.400\n think it's possible to create regulatory bodies oversight over AI algorithms, which are inherently\n\n57:24.400 --> 57:28.400\n such cutting edge set of ideas and technologies?\n\n57:28.400 --> 57:35.040\n Yeah, but I think it takes time to figure out what kind of oversight, what kinds of controls.\n\n57:35.040 --> 57:40.160\n I mean, it took time to design the FDA regime, you know, and some people still don't like it and\n\n57:40.160 --> 57:45.520\n they want to fix it. And I think there are clear ways that it could be improved.\n\n57:46.960 --> 57:51.680\n But the whole notion that you have stage one, stage two, stage three, and here are the criteria\n\n57:51.680 --> 57:58.320\n for what you have to do to pass a stage one trial, right? We haven't even thought about what those\n\n57:58.320 --> 58:07.040\n would be for algorithms. So, I mean, I think there are things we could do right now with regard to\n\n58:07.040 --> 58:15.280\n bias, for example, we have a pretty good technical handle on how to detect algorithms that are\n\n58:15.280 --> 58:22.960\n propagating bias that exists in data sets, how to de bias those algorithms, and even what it's going\n\n58:22.960 --> 58:30.320\n to cost you to do that. So I think we could start having some standards on that. I think there are\n\n58:30.320 --> 58:37.280\n things to do with impersonation and falsification that we could work on.\n\n58:37.280 --> 58:38.400\n Fakes, yeah.\n\n58:38.400 --> 58:44.960\n A very simple point. So impersonation is a machine acting as if it was a person.\n\n58:46.000 --> 58:53.200\n I can't see a real justification for why we shouldn't insist that machines self identify\n\n58:53.200 --> 59:02.800\n as machines. Where is the social benefit in fooling people into thinking that this is really\n\n59:02.800 --> 59:09.360\n a person when it isn't? I don't mind if it uses a human like voice, that's easy to understand,\n\n59:09.360 --> 59:13.360\n that's fine, but it should just say, I'm a machine in some form.\n\n59:14.960 --> 59:20.000\n And how many people are speaking to that? I would think relatively obvious facts.\n\n59:20.000 --> 59:27.280\n Yeah, I mean, there is actually a law in California that bans impersonation, but only in certain\n\n59:27.280 --> 59:36.000\n restricted circumstances. So for the purpose of engaging in a fraudulent transaction and for the\n\n59:36.000 --> 59:44.160\n purpose of modifying someone's voting behavior. So those are the circumstances where machines have\n\n59:44.160 --> 59:51.280\n to self identify. But I think arguably, it should be in all circumstances. And\n\n59:51.280 --> 59:58.480\n then when you talk about deep fakes, we're just at the beginning, but already it's possible to\n\n59:58.480 --> 1:00:04.480\n make a movie of anybody saying anything in ways that are pretty hard to detect.\n\n1:00:05.440 --> 1:00:09.040\n Including yourself because you're on camera now and your voice is coming through with high\n\n1:00:09.040 --> 1:00:09.520\n resolution.\n\n1:00:09.520 --> 1:00:13.600\n Yeah, so you could take what I'm saying and replace it with pretty much anything else you\n\n1:00:13.600 --> 1:00:17.040\n wanted me to be saying. And it's a very simple thing.\n\n1:00:17.040 --> 1:00:21.440\n Take what I'm saying and replace it with pretty much anything else you wanted me to be saying. And\n\n1:00:21.440 --> 1:00:29.040\n even it would change my lips and facial expressions to fit. And there's actually not much\n\n1:00:30.640 --> 1:00:38.160\n in the way of real legal protection against that. I think in the commercial area, you could say,\n\n1:00:38.160 --> 1:00:45.600\n yeah, you're using my brand and so on. There are rules about that. But in the political sphere,\n\n1:00:45.600 --> 1:00:52.480\n I think at the moment, anything goes. That could be really, really damaging.\n\n1:00:53.840 --> 1:01:03.280\n And let me just try to make not an argument, but try to look back at history and say something dark\n\n1:01:04.160 --> 1:01:10.240\n in essence is while regulation seems to be, oversight seems to be exactly the right thing to\n\n1:01:10.240 --> 1:01:15.440\n do here. It seems that human beings, what they naturally do is they wait for something to go\n\n1:01:15.440 --> 1:01:21.840\n wrong. If you're talking about nuclear weapons, you can't talk about nuclear weapons being dangerous\n\n1:01:21.840 --> 1:01:28.720\n until somebody actually like the United States drops the bomb or Chernobyl melting. Do you think\n\n1:01:28.720 --> 1:01:36.880\n we will have to wait for things going wrong in a way that's obviously damaging to society,\n\n1:01:36.880 --> 1:01:43.440\n not an existential risk, but obviously damaging? Or do you have faith that...\n\n1:01:43.440 --> 1:01:48.000\n I hope not, but I think we do have to look at history.\n\n1:01:49.840 --> 1:01:57.280\n And so the two examples you gave, nuclear weapons and nuclear power are very, very interesting\n\n1:01:57.280 --> 1:02:07.520\n because nuclear weapons, we knew in the early years of the 20th century that atoms contained\n\n1:02:07.520 --> 1:02:12.880\n a huge amount of energy. We had E equals MC squared. We knew the mass differences between\n\n1:02:12.880 --> 1:02:15.440\n the different atoms and their components. And we knew that\n\n1:02:17.920 --> 1:02:23.760\n you might be able to make an incredibly powerful explosive. So HG Wells wrote science fiction book,\n\n1:02:23.760 --> 1:02:31.920\n I think in 1912. Frederick Soddy, who was the guy who discovered isotopes, the Nobel prize winner,\n\n1:02:31.920 --> 1:02:40.400\n he gave a speech in 1915 saying that one pound of this new explosive would be the equivalent\n\n1:02:40.400 --> 1:02:48.320\n of 150 tons of dynamite, which turns out to be about right. And this was in World War I,\n\n1:02:48.320 --> 1:02:56.160\n so he was imagining how much worse the world war would be if we were using that kind of explosive.\n\n1:02:56.160 --> 1:03:04.000\n But the physics establishment simply refused to believe that these things could be made.\n\n1:03:04.000 --> 1:03:05.760\n Including the people who are making it.\n\n1:03:05.760 --> 1:03:11.200\n Well, so they were doing the nuclear physics. I mean, eventually were the ones who made it.\n\n1:03:11.200 --> 1:03:13.440\n You talk about Fermi or whoever.\n\n1:03:13.440 --> 1:03:22.240\n Well, so up to the development was mostly theoretical. So it was people using sort of\n\n1:03:22.240 --> 1:03:29.440\n primitive kinds of particle acceleration and doing experiments at the level of single particles\n\n1:03:29.440 --> 1:03:37.280\n or collections of particles. They weren't yet thinking about how to actually make a bomb or\n\n1:03:37.280 --> 1:03:40.640\n anything like that. But they knew the energy was there and they figured if they understood it\n\n1:03:40.640 --> 1:03:47.040\n better, it might be possible. But the physics establishment, their view, and I think because\n\n1:03:47.040 --> 1:03:54.320\n they did not want it to be true, their view was that it could not be true. That this could not\n\n1:03:54.320 --> 1:04:03.520\n not provide a way to make a super weapon. And there was this famous speech given by Rutherford,\n\n1:04:03.520 --> 1:04:11.840\n who was the sort of leader of nuclear physics. And it was on September 11th, 1933. And he said,\n\n1:04:11.840 --> 1:04:17.760\n anyone who talks about the possibility of obtaining energy from transformation of atoms\n\n1:04:17.760 --> 1:04:26.080\n is talking complete moonshine. And the next morning, Leo Szilard read about that speech\n\n1:04:26.080 --> 1:04:32.880\n and then invented the nuclear chain reaction. And so as soon as he invented, as soon as he had that\n\n1:04:32.880 --> 1:04:38.560\n idea that you could make a chain reaction with neutrons, because neutrons were not repelled by\n\n1:04:38.560 --> 1:04:44.240\n the nucleus, so they could enter the nucleus and then continue the reaction. As soon as he has that\n\n1:04:44.240 --> 1:04:54.400\n idea, he instantly realized that the world was in deep doo doo. Because this is 1933, right? Hitler\n\n1:04:54.400 --> 1:05:02.800\n had recently come to power in Germany. Szilard was in London and eventually became a refugee\n\n1:05:04.000 --> 1:05:11.920\n and came to the US. And in the process of having the idea about the chain reaction,\n\n1:05:11.920 --> 1:05:18.960\n he figured out basically how to make a bomb and also how to make a reactor. And he patented the\n\n1:05:18.960 --> 1:05:27.920\n reactor in 1934. But because of the situation, the great power conflict situation that he could see\n\n1:05:27.920 --> 1:05:39.920\n happening, he kept that a secret. And so between then and the beginning of World War II, people\n\n1:05:39.920 --> 1:05:49.200\n were working, including the Germans, on how to actually create neutron sources, what specific\n\n1:05:50.320 --> 1:05:55.120\n fission reactions would produce neutrons of the right energy to continue the reaction.\n\n1:05:57.440 --> 1:06:01.440\n And that was demonstrated in Germany, I think in 1938, if I remember correctly.\n\n1:06:01.440 --> 1:06:16.480\n The first nuclear weapon patent was 1939 by the French. So this was actually going on well before\n\n1:06:16.480 --> 1:06:22.640\n World War II really got going. And then the British probably had the most advanced capability\n\n1:06:22.640 --> 1:06:30.160\n in this area. But for safety reasons, among others, and just resources, they moved the program\n\n1:06:30.160 --> 1:06:37.840\n from Britain to the US and then that became Manhattan Project. So the reason why we couldn't\n\n1:06:40.560 --> 1:06:44.880\n have any kind of oversight of nuclear weapons and nuclear technology\n\n1:06:46.560 --> 1:06:50.800\n was because we were basically already in an arms race and a war.\n\n1:06:50.800 --> 1:07:00.960\n LR But you mentioned then in the 20s and 30s. So what are the echoes? The way you've described\n\n1:07:00.960 --> 1:07:05.040\n this story, I mean, there's clearly echoes. Why do you think most AI researchers,\n\n1:07:06.800 --> 1:07:11.760\n folks who are really close to the metal, they really are not concerned about AI. They don't\n\n1:07:11.760 --> 1:07:18.240\n think about it, whether it's they don't want to think about it. But why do you think that is,\n\n1:07:18.240 --> 1:07:27.120\n is what are the echoes of the nuclear situation to the current AI situation? And what can we do\n\n1:07:27.120 --> 1:07:35.520\n about it? BF I think there is a kind of motivated cognition, which is a term in psychology means\n\n1:07:35.520 --> 1:07:46.000\n that you believe what you would like to be true, rather than what is true. And it's unsettling\n\n1:07:46.000 --> 1:07:52.640\n to think that what you're working on might be the end of the human race, obviously. So you would\n\n1:07:52.640 --> 1:08:00.560\n rather instantly deny it and come up with some reason why it couldn't be true. And I have,\n\n1:08:00.560 --> 1:08:08.160\n I collected a long list of reasons that extremely intelligent, competent AI scientists have come up\n\n1:08:08.160 --> 1:08:16.800\n with for why we shouldn't worry about this. For example, calculators are superhuman at arithmetic\n\n1:08:16.800 --> 1:08:22.000\n and they haven't taken over the world. So there's nothing to worry about. Well, okay, my five year\n\n1:08:22.000 --> 1:08:29.040\n old, you know, could have figured out why that was an unreasonable and really quite weak argument.\n\n1:08:29.040 --> 1:08:40.320\n Another one was, while it's theoretically possible that you could have superhuman AI destroy the\n\n1:08:40.320 --> 1:08:45.680\n world, it's also theoretically possible that a black hole could materialize right next to the\n\n1:08:45.680 --> 1:08:50.960\n earth and destroy humanity. I mean, yes, it's theoretically possible, quantum theoretically,\n\n1:08:50.960 --> 1:08:58.080\n extremely unlikely that it would just materialize right there. But that's a completely bogus analogy,\n\n1:08:58.080 --> 1:09:04.240\n because, you know, if the whole physics community on earth was working to materialize a black hole\n\n1:09:04.240 --> 1:09:10.160\n in near earth orbit, right? Wouldn't you ask them, is that a good idea? Is that going to be safe?\n\n1:09:10.160 --> 1:09:16.720\n You know, what if you succeed? Right. And that's the thing, right? The AI community is sort of\n\n1:09:16.720 --> 1:09:24.240\n refused to ask itself, what if you succeed? And initially I think that was because it was too hard,\n\n1:09:24.240 --> 1:09:32.720\n but, you know, Alan Turing asked himself that, and he said, we'd be toast, right? If we were lucky,\n\n1:09:32.720 --> 1:09:37.600\n we might be able to switch off the power, but probably we'd be toast. But there's also an aspect\n\n1:09:37.600 --> 1:09:45.200\n that because we're not exactly sure what the future holds, it's not clear exactly,\n\n1:09:45.200 --> 1:09:52.640\n so technically what to worry about, sort of how things go wrong. And so there is something,\n\n1:09:53.360 --> 1:09:58.800\n it feels like, maybe you can correct me if I'm wrong, but there's something paralyzing about\n\n1:09:58.800 --> 1:10:05.200\n worrying about something that logically is inevitable, but you have to think about it,\n\n1:10:05.200 --> 1:10:10.000\n logically is inevitable, but you don't really know what that will look like.\n\n1:10:10.720 --> 1:10:18.480\n Yeah, I think that's, it's a reasonable point and, you know, it's certainly in terms of\n\n1:10:18.480 --> 1:10:24.000\n existential risks, it's different from, you know, asteroid collides with the earth, right? Which,\n\n1:10:24.000 --> 1:10:29.520\n again, is quite possible, you know, it's happened in the past, it'll probably happen again,\n\n1:10:29.520 --> 1:10:34.960\n we don't know right now, but if we did detect an asteroid that was going to hit the earth\n\n1:10:34.960 --> 1:10:39.760\n in 75 years time, we'd certainly be doing something about it.\n\n1:10:39.760 --> 1:10:42.080\n Well, it's clear there's got big rock and there's,\n\n1:10:42.080 --> 1:10:45.600\n we'll probably have a meeting and see what do we do about the big rock with AI.\n\n1:10:46.160 --> 1:10:50.160\n Right, with AI, I mean, there are very few people who think it's not going to happen within the\n\n1:10:50.160 --> 1:10:56.160\n next 75 years. I know Rod Brooks doesn't think it's going to happen, maybe Andrew Ng doesn't\n\n1:10:56.160 --> 1:11:02.800\n think it's happened, but, you know, a lot of the people who work day to day, you know, as you say,\n\n1:11:02.800 --> 1:11:10.640\n at the rock face, they think it's going to happen. I think the median estimate from AI researchers is\n\n1:11:10.640 --> 1:11:16.000\n somewhere in 40 to 50 years from now, or maybe, you know, I think in Asia, they think it's going\n\n1:11:16.000 --> 1:11:23.440\n to be even faster than that. I'm a little bit more conservative, I think it'd probably take\n\n1:11:24.080 --> 1:11:30.720\n longer than that, but I think, you know, as happened with nuclear weapons, it can happen\n\n1:11:30.720 --> 1:11:34.240\n overnight that you have these breakthroughs and we need more than one breakthrough, but,\n\n1:11:34.960 --> 1:11:40.640\n you know, it's on the order of half a dozen, I mean, this is a very rough scale, but sort of\n\n1:11:40.640 --> 1:11:49.920\n half a dozen breakthroughs of that nature would have to happen for us to reach the superhuman AI.\n\n1:11:49.920 --> 1:11:57.280\n But the, you know, the AI research community is vast now, the massive investments from governments,\n\n1:11:57.280 --> 1:12:03.360\n from corporations, tons of really, really smart people, you know, you just have to look at the\n\n1:12:03.360 --> 1:12:09.200\n rate of progress in different areas of AI to see that things are moving pretty fast. So to say,\n\n1:12:09.200 --> 1:12:14.560\n oh, it's just going to be thousands of years, I don't see any basis for that. You know, I see,\n\n1:12:15.920 --> 1:12:26.400\n you know, for example, the Stanford 100 year AI project, right, which is supposed to be sort of,\n\n1:12:26.400 --> 1:12:32.400\n you know, the serious establishment view, their most recent report actually said it's probably\n\n1:12:32.400 --> 1:12:34.720\n not even possible. Oh, wow.\n\n1:12:35.280 --> 1:12:42.880\n Right. Which if you want a perfect example of people in denial, that's it. Because, you know,\n\n1:12:42.880 --> 1:12:49.520\n for the whole history of AI, we've been saying to philosophers who said it wasn't possible,\n\n1:12:49.520 --> 1:12:53.920\n well, you have no idea what you're talking about. Of course it's possible, right? Give me an argument\n\n1:12:53.920 --> 1:13:00.400\n for why it couldn't happen. And there isn't one, right? And now, because people are worried that\n\n1:13:00.400 --> 1:13:06.080\n maybe AI might get a bad name, or I just don't want to think about this, they're saying, okay,\n\n1:13:06.080 --> 1:13:12.240\n well, of course, it's not really possible. You know, imagine if, you know, the leaders of the\n\n1:13:12.240 --> 1:13:17.360\n cancer biology community got up and said, well, you know, of course, curing cancer,\n\n1:13:17.360 --> 1:13:28.320\n it's not really possible. There'd be complete outrage and dismay. And, you know, I find this\n\n1:13:28.320 --> 1:13:35.680\n really a strange phenomenon. So, okay, so if you accept that it's possible,\n\n1:13:35.680 --> 1:13:42.400\n and if you accept that it's probably going to happen, the point that you're making that,\n\n1:13:42.400 --> 1:13:50.160\n you know, how does it go wrong? A valid question. Without that, without an answer to that question,\n\n1:13:50.160 --> 1:13:54.320\n then you're stuck with what I call the gorilla problem, which is, you know, the problem that\n\n1:13:54.320 --> 1:14:00.480\n the gorillas face, right? They made something more intelligent than them, namely us, a few million\n\n1:14:00.480 --> 1:14:07.680\n years ago, and now they're in deep doo doo. So there's really nothing they can do. They've lost\n\n1:14:07.680 --> 1:14:13.760\n the control. They failed to solve the control problem of controlling humans, and so they've\n\n1:14:13.760 --> 1:14:20.240\n lost. So we don't want to be in that situation. And if the gorilla problem is the only formulation\n\n1:14:20.240 --> 1:14:25.360\n you have, there's not a lot you can do, right? Other than to say, okay, we should try to stop,\n\n1:14:26.640 --> 1:14:31.760\n you know, we should just not make the humans, or in this case, not make the AI. And I think\n\n1:14:31.760 --> 1:14:40.320\n that's really hard to do. I'm not actually proposing that that's a feasible course of\n\n1:14:40.320 --> 1:14:46.080\n action. I also think that, you know, if properly controlled AI could be incredibly beneficial.\n\n1:14:48.800 --> 1:14:56.720\n But it seems to me that there's a consensus that one of the major failure modes is this\n\n1:14:56.720 --> 1:15:05.040\n loss of control, that we create AI systems that are pursuing incorrect objectives. And because\n\n1:15:05.040 --> 1:15:12.240\n the AI system believes it knows what the objective is, it has no incentive to listen to us anymore,\n\n1:15:12.240 --> 1:15:21.680\n so to speak, right? It's just carrying out the strategy that it has computed as being the optimal\n\n1:15:21.680 --> 1:15:30.480\n solution. And, you know, it may be that in the process, it needs to acquire more resources to\n\n1:15:30.480 --> 1:15:36.800\n increase the possibility of success or prevent various failure modes by defending itself against\n\n1:15:36.800 --> 1:15:45.920\n interference. And so that collection of problems, I think, is something we can address. The other\n\n1:15:45.920 --> 1:15:55.680\n problems are, roughly speaking, you know, misuse, right? So even if we solve the control problem,\n\n1:15:55.680 --> 1:16:01.600\n we make perfectly safe controllable AI systems. Well, why? You know, why does Dr. Evil going to\n\n1:16:01.600 --> 1:16:06.480\n use those, right? He wants to just take over the world and he'll make unsafe AI systems that then\n\n1:16:06.480 --> 1:16:12.960\n get out of control. So that's one problem, which is sort of a, you know, partly a policing problem,\n\n1:16:12.960 --> 1:16:21.280\n partly a sort of a cultural problem for the profession of how we teach people what kinds\n\n1:16:21.280 --> 1:16:26.000\n of AI systems are safe. You talk about autonomous weapon system and how pretty much everybody\n\n1:16:26.000 --> 1:16:32.000\n agrees that there's too many ways that that can go horribly wrong. This great slaughterbots movie\n\n1:16:32.000 --> 1:16:36.000\n that kind of illustrates that beautifully. I want to talk about that. That's another,\n\n1:16:36.960 --> 1:16:41.200\n there's another topic I'm having to talk about. I just want to mention that what I see is the\n\n1:16:41.200 --> 1:16:49.760\n third major failure mode, which is overuse, not so much misuse, but overuse of AI that we become\n\n1:16:49.760 --> 1:16:54.960\n overly dependent. So I call this the WALL E problem. So if you've seen WALL E, the movie,\n\n1:16:54.960 --> 1:17:00.240\n all right, all the humans are on the spaceship and the machines look after everything for them,\n\n1:17:00.240 --> 1:17:07.440\n and they just watch TV and drink big gulps. And they're all sort of obese and stupid and they\n\n1:17:07.440 --> 1:17:17.680\n sort of totally lost any notion of human autonomy. And, you know, so in effect, right. This would\n\n1:17:17.680 --> 1:17:24.240\n happen like the slow boiling frog, right? We would gradually turn over more and more of the\n\n1:17:24.240 --> 1:17:29.520\n management of our civilization to machines as we are already doing. And this, you know, if this\n\n1:17:29.520 --> 1:17:37.920\n if this process continues, you know, we sort of gradually switch from sort of being the masters\n\n1:17:37.920 --> 1:17:44.160\n of technology to just being the guests. Right. So we become guests on a cruise ship, you know,\n\n1:17:44.160 --> 1:17:51.360\n which is fine for a week, but not not for the rest of eternity. You know, and it's almost\n\n1:17:51.360 --> 1:17:58.640\n irreversible. Right. Once you once you lose the incentive to, for example, you know, learn to be\n\n1:17:58.640 --> 1:18:08.000\n an engineer or a doctor or a sanitation operative or any other of the infinitely many ways that we\n\n1:18:08.000 --> 1:18:14.240\n maintain and propagate our civilization. You know, if you if you don't have the incentive to do any\n\n1:18:14.240 --> 1:18:20.320\n of that, you won't. And then it's really hard to recover. And of course, as just one of the\n\n1:18:20.320 --> 1:18:24.400\n technologies that could that third failure mode result in that there's probably other\n\n1:18:24.400 --> 1:18:31.120\n technology in general detaches us from it does a bit. But the difference is that in terms of\n\n1:18:31.120 --> 1:18:38.240\n the knowledge to to run our civilization, you know, up to now, we've had no alternative but\n\n1:18:38.240 --> 1:18:43.920\n to put it into people's heads. Right. And if you software with Google, I mean, so software in\n\n1:18:43.920 --> 1:18:51.200\n general, so computers in general, but but the, you know, the knowledge of how, you know, how a\n\n1:18:51.200 --> 1:18:56.000\n sanitation system works, you know, that's an AI has to understand that it's no good putting it\n\n1:18:56.000 --> 1:19:02.560\n into Google. So, I mean, we we've always put knowledge in on paper, but paper doesn't run our\n\n1:19:02.560 --> 1:19:07.120\n civilization and only runs when it goes from the paper into people's heads again. Right. So we've\n\n1:19:07.120 --> 1:19:13.680\n always propagated civilization through human minds. And we've spent about a trillion person\n\n1:19:13.680 --> 1:19:19.440\n years doing that. I literally write you, you can work it out. It's about right. There's about just\n\n1:19:19.440 --> 1:19:25.280\n over 100 billion people who've ever lived. And each of them has spent about 10 years learning\n\n1:19:25.280 --> 1:19:30.640\n stuff to keep their civilization going. And so that's a trillion person years we put into this\n\n1:19:30.640 --> 1:19:36.160\n effort. Beautiful way to describe all civilization. And now we're, you know, we're in danger of\n\n1:19:36.160 --> 1:19:40.880\n throwing that away. So this is a problem that AI can't solve. It's not a technical problem. It's\n\n1:19:40.880 --> 1:19:48.560\n you know, if we do our job right, the AI systems will say, you know, the human race doesn't in the\n\n1:19:48.560 --> 1:19:54.560\n long run want to be passengers in a cruise ship. The human race wants autonomy. This is part of\n\n1:19:54.560 --> 1:20:01.200\n human preferences. So we, the AI systems are not going to do this stuff for you. You've got to do\n\n1:20:01.200 --> 1:20:06.320\n it for yourself. Right. I'm not going to carry you to the top of Everest in an autonomous\n\n1:20:06.320 --> 1:20:14.960\n helicopter. You have to climb it if you want to get the benefit and so on. So, but I'm afraid that\n\n1:20:14.960 --> 1:20:22.400\n because we are short sighted and lazy, we're going to override the AI systems. And, and there's an\n\n1:20:22.400 --> 1:20:28.720\n amazing short story that I recommend to everyone that I talked to about this called The Machine\n\n1:20:28.720 --> 1:20:37.520\n Stops, written in 1909 by E.M. Forster, who, you know, wrote novels about the British Empire and\n\n1:20:37.520 --> 1:20:42.240\n sort of things that became costume dramas on the BBC. But he wrote this one science fiction story,\n\n1:20:42.240 --> 1:20:51.680\n which is an amazing vision of the future. It has basically iPads, it has video conferencing,\n\n1:20:51.680 --> 1:21:00.320\n it has MOOCs, it has computer induced obesity. I mean, literally it's what people spend their\n\n1:21:00.320 --> 1:21:05.920\n time doing is giving online courses or listening to online courses and talking about ideas,\n\n1:21:05.920 --> 1:21:11.200\n but they never get out there in the real world. They don't really have a lot of face to face\n\n1:21:11.200 --> 1:21:16.480\n contact. Everything is done online, you know, so all the things we're worrying about now\n\n1:21:17.520 --> 1:21:22.000\n were described in the story. And, and then the human race becomes more and more dependent on\n\n1:21:22.000 --> 1:21:30.000\n the machine, loses knowledge of how things really run and then becomes vulnerable to collapse. And\n\n1:21:31.360 --> 1:21:38.640\n so it's a, it's a pretty unbelievably amazing story for someone writing in 1909 to imagine all\n\n1:21:38.640 --> 1:21:45.760\n this. So there's very few people that represent artificial intelligence more than you Stuart\n\n1:21:45.760 --> 1:21:57.200\n Russell. If you say it's okay, that's very kind. So it's all my fault. Right. You're often brought\n\n1:21:57.200 --> 1:22:03.680\n up as the person, well, Stuart Russell, like the AI person is worried about this. That's why you\n\n1:22:03.680 --> 1:22:10.240\n should be worried about it. Do you feel the burden of that? I don't know if you feel that at all,\n\n1:22:10.240 --> 1:22:15.840\n but when I talk to people like from, you talk about people outside of computer science,\n\n1:22:15.840 --> 1:22:21.280\n when they think about this, Stuart Russell is worried about AI safety. You should be worried\n\n1:22:21.280 --> 1:22:29.840\n too. Do you feel the burden of that? I mean, in a practical sense, yeah, because I get, you know,\n\n1:22:29.840 --> 1:22:38.640\n a dozen, sometimes 25 invitations a day to talk about it, to give interviews, to write press\n\n1:22:38.640 --> 1:22:46.160\n articles and so on. So in that very practical sense, I'm seeing that people are concerned and\n\n1:22:46.160 --> 1:22:52.320\n really interested about this. Are you worried that you could be wrong as all good scientists are?\n\n1:22:52.320 --> 1:22:57.920\n Of course. I worry about that all the time. I mean, that's, that's always been the way that I,\n\n1:22:57.920 --> 1:23:03.440\n I've worked, you know, is like I have an argument in my head with myself, right? So I have,\n\n1:23:03.440 --> 1:23:10.560\n I have some idea and then I think, okay, how could that be wrong? Or did someone else already have\n\n1:23:10.560 --> 1:23:16.720\n that idea? So I'll go and, you know, search in as much literature as I can to see whether someone\n\n1:23:16.720 --> 1:23:23.680\n else already thought of that or, or even refuted it. So, you know, I, right now I'm, I'm reading a\n\n1:23:23.680 --> 1:23:32.800\n lot of philosophy because, you know, in, in the form of the debates over, over utilitarianism and,\n\n1:23:32.800 --> 1:23:41.600\n and other kinds of moral, moral formulas, shall we say, people have already thought through\n\n1:23:42.800 --> 1:23:47.680\n some of these issues. But, you know, what, one of the things I'm, I'm not seeing in a lot of\n\n1:23:47.680 --> 1:23:54.880\n these debates is this specific idea about the importance of uncertainty in the objective\n\n1:23:56.400 --> 1:24:01.920\n that this is the way we should think about machines that are beneficial to humans. So this\n\n1:24:01.920 --> 1:24:08.960\n idea of provably beneficial machines based on explicit uncertainty in the objective,\n\n1:24:10.000 --> 1:24:17.600\n you know, it seems to be, you know, my gut feeling is this is the core of it. It's going to have to\n\n1:24:17.600 --> 1:24:23.600\n be elaborated in a lot of different directions and there are a lot of beneficial. Yeah. But there,\n\n1:24:23.600 --> 1:24:30.640\n there are, I mean, it has to be right. We can't afford, you know, hand wavy beneficial because\n\n1:24:30.640 --> 1:24:34.800\n there are, you know, whenever we do hand wavy stuff, there are loopholes. And the thing about\n\n1:24:34.800 --> 1:24:40.560\n super intelligent machines is they find the loopholes, you know, just like, you know, tax\n\n1:24:40.560 --> 1:24:46.400\n evaders. If you don't write your tax law properly, people will find the loopholes and end up paying\n\n1:24:46.400 --> 1:24:54.400\n no tax. And, and so you should think of it this way and, and getting those definitions right,\n\n1:24:56.480 --> 1:25:04.400\n you know, it is really a long process, you know, so you can, you can define mathematical frameworks\n\n1:25:04.400 --> 1:25:08.560\n and within that framework, you can prove mathematical theorems that yes, this will,\n\n1:25:08.560 --> 1:25:13.680\n you know, this, this theoretical entity will be provably beneficial to that theoretical entity,\n\n1:25:13.680 --> 1:25:20.160\n but that framework may not match the real world in some crucial way. So it's a long process,\n\n1:25:20.160 --> 1:25:27.120\n thinking through it, iterating and so on. Last question. Yep. You have 10 seconds to answer it.\n\n1:25:27.120 --> 1:25:34.480\n What is your favorite sci fi movie about AI? I would say interstellar has my favorite robots.\n\n1:25:34.480 --> 1:25:42.160\n Oh, beats space. Yeah. Yeah. Yeah. So, so Tars, the robots, one of the robots in interstellar is\n\n1:25:42.160 --> 1:25:50.960\n the way robot should behave. And, uh, I would say ex machina is in some ways, the one,\n\n1:25:51.520 --> 1:25:58.080\n the one that makes you think, uh, in a nervous kind of way about, about where we're going.\n\n1:25:58.080 --> 1:26:12.880\n Well Stuart, thank you so much for talking today. Pleasure.\n\n"
}