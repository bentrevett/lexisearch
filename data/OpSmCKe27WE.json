{
  "title": "Ben Goertzel: Artificial General Intelligence | Lex Fridman Podcast #103",
  "id": "OpSmCKe27WE",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:03.000\n The following is a conversation with Ben Goertzel,\n\n00:03.000 --> 00:04.560\n one of the most interesting minds\n\n00:04.560 --> 00:06.680\n in the artificial intelligence community.\n\n00:06.680 --> 00:08.920\n He's the founder of SingularityNet,\n\n00:08.920 --> 00:11.520\n designer of OpenCog AI Framework,\n\n00:11.520 --> 00:13.220\n formerly a director of research\n\n00:13.220 --> 00:15.720\n at the Machine Intelligence Research Institute,\n\n00:15.720 --> 00:18.440\n and chief scientist of Hanson Robotics,\n\n00:18.440 --> 00:21.000\n the company that created the Sophia robot.\n\n00:21.000 --> 00:23.680\n He has been a central figure in the AGI community\n\n00:23.680 --> 00:26.960\n for many years, including in his organizing\n\n00:26.960 --> 00:28.720\n and contributing to the conference\n\n00:28.720 --> 00:30.920\n on artificial general intelligence,\n\n00:30.920 --> 00:34.440\n the 2020 version of which is actually happening this week,\n\n00:34.440 --> 00:36.480\n Wednesday, Thursday, and Friday.\n\n00:36.480 --> 00:38.480\n It's virtual and free.\n\n00:38.480 --> 00:40.040\n I encourage you to check out the talks,\n\n00:40.040 --> 00:45.040\n including by Yosha Bach from episode 101 of this podcast.\n\n00:45.160 --> 00:46.600\n Quick summary of the ads.\n\n00:46.600 --> 00:51.040\n Two sponsors, The Jordan Harbinger Show and Masterclass.\n\n00:51.040 --> 00:52.800\n Please consider supporting this podcast\n\n00:52.800 --> 00:56.500\n by going to jordanharbinger.com slash lex\n\n00:56.500 --> 01:00.380\n and signing up at masterclass.com slash lex.\n\n01:00.380 --> 01:02.840\n Click the links, buy all the stuff.\n\n01:02.840 --> 01:04.640\n It's the best way to support this podcast\n\n01:04.640 --> 01:08.840\n and the journey I'm on in my research and startup.\n\n01:08.840 --> 01:11.480\n This is the Artificial Intelligence Podcast.\n\n01:11.480 --> 01:13.680\n If you enjoy it, subscribe on YouTube,\n\n01:13.680 --> 01:15.940\n review it with five stars on Apple Podcast,\n\n01:15.940 --> 01:18.920\n support it on Patreon, or connect with me on Twitter\n\n01:18.920 --> 01:23.920\n at lexfriedman, spelled without the E, just F R I D M A N.\n\n01:23.920 --> 01:25.980\n As usual, I'll do a few minutes of ads now\n\n01:25.980 --> 01:27.340\n and never any ads in the middle\n\n01:27.340 --> 01:29.980\n that can break the flow of the conversation.\n\n01:29.980 --> 01:33.340\n This episode is supported by The Jordan Harbinger Show.\n\n01:33.340 --> 01:35.900\n Go to jordanharbinger.com slash lex.\n\n01:35.900 --> 01:37.740\n It's how he knows I sent you.\n\n01:37.740 --> 01:40.140\n On that page, there's links to subscribe to it\n\n01:40.140 --> 01:43.300\n on Apple Podcast, Spotify, and everywhere else.\n\n01:43.300 --> 01:45.100\n I've been binging on his podcast.\n\n01:45.100 --> 01:46.220\n Jordan is great.\n\n01:46.220 --> 01:47.900\n He gets the best out of his guests,\n\n01:47.900 --> 01:50.100\n dives deep, calls them out when it's needed,\n\n01:50.100 --> 01:52.260\n and makes the whole thing fun to listen to.\n\n01:52.260 --> 01:55.340\n He's interviewed Kobe Bryant, Mark Cuban,\n\n01:55.340 --> 01:59.060\n Neil deGrasse Tyson, Keira Kasparov, and many more.\n\n01:59.060 --> 02:01.540\n His conversation with Kobe is a reminder\n\n02:01.540 --> 02:06.060\n how much focus and hard work is required for greatness\n\n02:06.060 --> 02:09.540\n in sport, business, and life.\n\n02:09.540 --> 02:12.420\n I highly recommend the episode if you want to be inspired.\n\n02:12.420 --> 02:15.940\n Again, go to jordanharbinger.com slash lex.\n\n02:15.940 --> 02:17.900\n It's how Jordan knows I sent you.\n\n02:18.900 --> 02:21.300\n This show is sponsored by Master Class.\n\n02:21.300 --> 02:24.300\n Sign up at masterclass.com slash lex\n\n02:24.300 --> 02:27.660\n to get a discount and to support this podcast.\n\n02:27.660 --> 02:29.220\n When I first heard about Master Class,\n\n02:29.220 --> 02:31.060\n I thought it was too good to be true.\n\n02:31.060 --> 02:34.220\n For 180 bucks a year, you get an all access pass\n\n02:34.220 --> 02:37.540\n to watch courses from to list some of my favorites.\n\n02:37.540 --> 02:39.780\n Chris Hadfield on Space Exploration,\n\n02:39.780 --> 02:41.780\n Neil deGrasse Tyson on Scientific Thinking\n\n02:41.780 --> 02:44.980\n and Communication, Will Wright, creator of\n\n02:44.980 --> 02:47.940\n the greatest city building game ever, Sim City,\n\n02:47.940 --> 02:50.700\n and Sims on Space Exploration.\n\n02:50.700 --> 02:54.860\n Ben Sims on Game Design, Carlos Santana on Guitar,\n\n02:54.860 --> 02:59.460\n Keira Kasparov, the greatest chess player ever on chess,\n\n02:59.460 --> 03:01.980\n Daniel Negrano on Poker, and many more.\n\n03:01.980 --> 03:04.660\n Chris Hadfield explaining how rockets work\n\n03:04.660 --> 03:07.300\n and the experience of being launched into space alone\n\n03:07.300 --> 03:08.700\n is worth the money.\n\n03:08.700 --> 03:12.020\n Once again, sign up at masterclass.com slash lex\n\n03:12.020 --> 03:15.820\n to get a discount and to support this podcast.\n\n03:15.820 --> 03:20.780\n Now, here's my conversation with Ben Kurtzell.\n\n03:20.780 --> 03:25.100\n What books, authors, ideas had a lot of impact on you\n\n03:25.100 --> 03:26.780\n in your life in the early days?\n\n03:27.900 --> 03:32.180\n You know, what got me into AI and science fiction\n\n03:32.180 --> 03:34.580\n and such in the first place wasn't a book,\n\n03:34.580 --> 03:37.020\n but the original Star Trek TV show,\n\n03:37.020 --> 03:39.860\n which my dad watched with me like in its first run.\n\n03:39.860 --> 03:42.700\n It would have been 1968, 69 or something,\n\n03:42.700 --> 03:45.340\n and that was incredible because every show\n\n03:45.340 --> 03:49.140\n they visited a different alien civilization\n\n03:49.140 --> 03:51.300\n with different culture and weird mechanisms.\n\n03:51.300 --> 03:55.020\n But that got me into science fiction,\n\n03:55.020 --> 03:57.180\n and there wasn't that much science fiction\n\n03:57.180 --> 03:58.660\n to watch on TV at that stage,\n\n03:58.660 --> 04:01.420\n so that got me into reading the whole literature\n\n04:01.420 --> 04:03.340\n of science fiction, you know,\n\n04:03.340 --> 04:07.500\n from the beginning of the previous century until that time.\n\n04:07.500 --> 04:10.860\n And I mean, there was so many science fiction writers\n\n04:10.860 --> 04:12.420\n who were inspirational to me.\n\n04:12.420 --> 04:14.820\n I'd say if I had to pick two,\n\n04:14.820 --> 04:18.820\n it would have been Stanis\u0142aw Lem, the Polish writer.\n\n04:18.820 --> 04:22.020\n Yeah, Solaris, and then he had a bunch\n\n04:22.020 --> 04:25.780\n of more obscure writings on superhuman AIs\n\n04:25.780 --> 04:26.620\n that were engineered.\n\n04:26.620 --> 04:28.660\n Solaris was sort of a superhuman,\n\n04:28.660 --> 04:31.540\n naturally occurring intelligence.\n\n04:31.540 --> 04:34.780\n Then Philip K. Dick, who, you know,\n\n04:34.780 --> 04:37.340\n ultimately my fandom for Philip K. Dick\n\n04:37.340 --> 04:39.100\n is one of the things that brought me together\n\n04:39.100 --> 04:43.740\n with David Hansen, my collaborator on robotics projects.\n\n04:43.740 --> 04:47.620\n So, you know, Stanis\u0142aw Lem was very much an intellectual,\n\n04:47.620 --> 04:51.020\n right, so he had a very broad view of intelligence\n\n04:51.020 --> 04:54.420\n going beyond the human and into what I would call,\n\n04:54.420 --> 04:56.900\n you know, open ended superintelligence.\n\n04:56.900 --> 05:01.900\n The Solaris superintelligent ocean was intelligent,\n\n05:01.900 --> 05:04.420\n in some ways more generally intelligent than people,\n\n05:04.420 --> 05:07.340\n but in a complex and confusing way\n\n05:07.340 --> 05:10.180\n so that human beings could never quite connect to it,\n\n05:10.180 --> 05:13.260\n but it was still probably very, very smart.\n\n05:13.260 --> 05:16.620\n And then the Golem 4 supercomputer\n\n05:16.620 --> 05:20.420\n in one of Lem's books, this was engineered by people,\n\n05:20.420 --> 05:24.420\n but eventually it became very intelligent\n\n05:24.420 --> 05:26.020\n in a different direction than humans\n\n05:26.020 --> 05:29.260\n and decided that humans were kind of trivial,\n\n05:29.260 --> 05:30.260\n not that interesting.\n\n05:30.260 --> 05:35.260\n So it put some impenetrable shield around itself,\n\n05:35.300 --> 05:36.700\n shut itself off from humanity,\n\n05:36.700 --> 05:40.060\n and then issued some philosophical screed\n\n05:40.060 --> 05:44.540\n about the pathetic and hopeless nature of humanity\n\n05:44.540 --> 05:48.380\n and all human thought, and then disappeared.\n\n05:48.380 --> 05:51.140\n Now, Philip K. Dick, he was a bit different.\n\n05:51.140 --> 05:52.460\n He was human focused, right?\n\n05:52.460 --> 05:55.860\n His main thing was, you know, human compassion\n\n05:55.860 --> 05:59.540\n and the human heart and soul are going to be the constant\n\n05:59.540 --> 06:03.620\n that will keep us going through whatever aliens we discover\n\n06:03.620 --> 06:08.620\n or telepathy machines or super AIs or whatever it might be.\n\n06:08.620 --> 06:10.660\n So he didn't believe in reality,\n\n06:10.660 --> 06:13.740\n like the reality that we see may be a simulation\n\n06:13.740 --> 06:17.100\n or a dream or something else we can't even comprehend,\n\n06:17.100 --> 06:19.100\n but he believed in love and compassion\n\n06:19.100 --> 06:20.660\n as something persistent\n\n06:20.660 --> 06:22.460\n through the various simulated realities.\n\n06:22.460 --> 06:26.740\n So those two science fiction writers had a huge impact on me.\n\n06:26.740 --> 06:30.300\n Then a little older than that, I got into Dostoevsky\n\n06:30.300 --> 06:33.620\n and Friedrich Nietzsche and Rimbaud\n\n06:33.620 --> 06:36.900\n and a bunch of more literary type writing.\n\n06:36.900 --> 06:38.620\n Can we talk about some of those things?\n\n06:38.620 --> 06:41.660\n So on the Solaris side, Stanislaw Lem,\n\n06:43.180 --> 06:47.020\n this kind of idea of there being intelligences out there\n\n06:47.020 --> 06:48.700\n that are different than our own,\n\n06:49.540 --> 06:53.020\n do you think there are intelligences maybe all around us\n\n06:53.020 --> 06:56.420\n that we're not able to even detect?\n\n06:56.420 --> 06:58.700\n So this kind of idea of,\n\n06:58.700 --> 07:01.580\n maybe you can comment also on Stephen Wolfram\n\n07:01.580 --> 07:04.340\n thinking that there's computations all around us\n\n07:04.340 --> 07:07.460\n and we're just not smart enough to kind of detect\n\n07:07.460 --> 07:10.380\n their intelligence or appreciate their intelligence.\n\n07:10.380 --> 07:13.540\n Yeah, so my friend Hugo de Gares,\n\n07:13.540 --> 07:15.780\n who I've been talking to about these things\n\n07:15.780 --> 07:19.300\n for many decades, since the early 90s,\n\n07:19.300 --> 07:21.740\n he had an idea he called SIPI,\n\n07:21.740 --> 07:25.100\n the Search for Intraparticulate Intelligence.\n\n07:25.100 --> 07:28.140\n So the concept there was as AIs get smarter\n\n07:28.140 --> 07:29.460\n and smarter and smarter,\n\n07:30.820 --> 07:33.660\n assuming the laws of physics as we know them now\n\n07:33.660 --> 07:37.420\n are still what these super intelligences\n\n07:37.420 --> 07:39.220\n perceived to hold and are bound by,\n\n07:39.220 --> 07:40.420\n as they get smarter and smarter,\n\n07:40.420 --> 07:42.980\n they're gonna shrink themselves littler and littler\n\n07:42.980 --> 07:45.380\n because special relativity makes it\n\n07:45.380 --> 07:47.220\n so they can communicate\n\n07:47.220 --> 07:49.300\n between two spatially distant points.\n\n07:49.300 --> 07:50.780\n So they're gonna get smaller and smaller,\n\n07:50.780 --> 07:53.220\n but then ultimately, what does that mean?\n\n07:53.220 --> 07:56.500\n The minds of the super, super, super intelligences,\n\n07:56.500 --> 07:59.020\n they're gonna be packed into the interaction\n\n07:59.020 --> 08:01.940\n of elementary particles or quarks\n\n08:01.940 --> 08:04.580\n or the partons inside quarks or whatever it is.\n\n08:04.580 --> 08:07.620\n So what we perceive as random fluctuations\n\n08:07.620 --> 08:09.740\n on the quantum or sub quantum level\n\n08:09.740 --> 08:11.500\n may actually be the thoughts\n\n08:11.500 --> 08:16.300\n of the micro, micro, micro miniaturized super intelligences\n\n08:16.300 --> 08:19.140\n because there's no way we can tell random\n\n08:19.140 --> 08:21.620\n from structured but within algorithmic information\n\n08:21.620 --> 08:23.100\n more complex than our brains, right?\n\n08:23.100 --> 08:24.300\n We can't tell the difference.\n\n08:24.300 --> 08:27.020\n So what we think is random could be the thought processes\n\n08:27.020 --> 08:29.980\n of some really tiny super minds.\n\n08:29.980 --> 08:34.020\n And if so, there is not a damn thing we can do about it,\n\n08:34.020 --> 08:37.180\n except try to upgrade our intelligences\n\n08:37.180 --> 08:40.060\n and expand our minds so that we can perceive\n\n08:40.060 --> 08:41.300\n more of what's around us.\n\n08:41.300 --> 08:43.980\n But if those random fluctuations,\n\n08:43.980 --> 08:46.540\n like even if we go to like quantum mechanics,\n\n08:46.540 --> 08:51.220\n if that's actually super intelligent systems,\n\n08:51.220 --> 08:54.620\n aren't we then part of the super of super intelligence?\n\n08:54.620 --> 08:58.340\n Aren't we just like a finger of the entirety\n\n08:58.340 --> 09:01.300\n of the body of the super intelligent system?\n\n09:01.300 --> 09:05.940\n It could be, I mean, a finger is a strange metaphor.\n\n09:05.940 --> 09:08.060\n I mean, we...\n\n09:08.060 --> 09:10.700\n A finger is dumb is what I mean.\n\n09:10.700 --> 09:12.260\n But the finger is also useful\n\n09:12.260 --> 09:14.780\n and is controlled with intent by the brain\n\n09:14.780 --> 09:16.700\n whereas we may be much less than that, right?\n\n09:16.700 --> 09:21.340\n I mean, yeah, we may be just some random epiphenomenon\n\n09:21.340 --> 09:23.300\n that they don't care about too much.\n\n09:23.300 --> 09:26.380\n Like think about the shape of the crowd emanating\n\n09:26.380 --> 09:28.260\n from a sports stadium or something, right?\n\n09:28.260 --> 09:31.580\n There's some emergent shape to the crowd, it's there.\n\n09:31.580 --> 09:33.700\n You could take a picture of it, it's kind of cool.\n\n09:33.700 --> 09:36.300\n It's irrelevant to the main point of the sports event\n\n09:36.300 --> 09:37.860\n or where the people are going\n\n09:37.860 --> 09:40.220\n or what's on the minds of the people\n\n09:40.220 --> 09:41.860\n making that shape in the crowd, right?\n\n09:41.860 --> 09:46.860\n So we may just be some semi arbitrary higher level pattern\n\n09:47.660 --> 09:49.700\n popping out of a lower level\n\n09:49.700 --> 09:52.260\n hyper intelligent self organization.\n\n09:52.260 --> 09:55.860\n And I mean, so be it, right?\n\n09:55.860 --> 09:57.060\n I mean, that's one thing that...\n\n09:57.060 --> 09:59.500\n Yeah, I mean, the older I've gotten,\n\n09:59.500 --> 10:04.220\n the more respect I've achieved for our fundamental ignorance.\n\n10:04.220 --> 10:06.260\n I mean, mine and everybody else's.\n\n10:06.260 --> 10:08.820\n I mean, I look at my two dogs,\n\n10:08.820 --> 10:10.940\n two beautiful little toy poodles\n\n10:10.940 --> 10:14.780\n and they watch me sitting at the computer typing.\n\n10:14.780 --> 10:16.980\n They just think I'm sitting there wiggling my fingers\n\n10:16.980 --> 10:19.980\n to exercise them maybe or guarding the monitor on the desk\n\n10:19.980 --> 10:22.340\n that they have no idea that I'm communicating\n\n10:22.340 --> 10:24.420\n with other people halfway around the world,\n\n10:24.420 --> 10:27.660\n let alone creating complex algorithms\n\n10:27.660 --> 10:30.220\n running in RAM on some computer server\n\n10:30.220 --> 10:32.540\n in St. Petersburg or something, right?\n\n10:32.540 --> 10:35.100\n Although they're right there in the room with me.\n\n10:35.100 --> 10:37.780\n So what things are there right around us\n\n10:37.780 --> 10:40.780\n that we're just too stupid or close minded to comprehend?\n\n10:40.780 --> 10:42.140\n Probably quite a lot.\n\n10:42.140 --> 10:46.220\n Your very poodle could also be communicating\n\n10:46.220 --> 10:49.980\n across multiple dimensions with other beings\n\n10:49.980 --> 10:53.180\n and you're too unintelligent to understand\n\n10:53.180 --> 10:55.700\n the kind of communication mechanism they're going through.\n\n10:55.700 --> 10:59.820\n There have been various TV shows and science fiction novels,\n\n10:59.820 --> 11:03.220\n poisoning cats, dolphins, mice and whatnot\n\n11:03.220 --> 11:07.220\n are actually super intelligences here to observe that.\n\n11:07.220 --> 11:12.220\n I would guess as one or the other quantum physics founders\n\n11:12.580 --> 11:15.500\n said, those theories are not crazy enough to be true.\n\n11:15.500 --> 11:17.660\n The reality is probably crazier than that.\n\n11:17.660 --> 11:18.500\n Beautifully put.\n\n11:18.500 --> 11:22.020\n So on the human side, with Philip K. Dick\n\n11:22.020 --> 11:27.020\n and in general, where do you fall on this idea\n\n11:27.260 --> 11:30.580\n that love and just the basic spirit of human nature\n\n11:30.580 --> 11:34.980\n persists throughout these multiple realities?\n\n11:34.980 --> 11:38.420\n Are you on the side, like the thing that inspires you\n\n11:38.420 --> 11:40.020\n about artificial intelligence,\n\n11:40.980 --> 11:45.980\n is it the human side of somehow persisting\n\n11:46.740 --> 11:49.820\n through all of the different systems we engineer\n\n11:49.820 --> 11:53.340\n or is AI inspire you to create something\n\n11:53.340 --> 11:55.500\n that's greater than human, that's beyond human,\n\n11:55.500 --> 11:57.460\n that's almost nonhuman?\n\n11:59.140 --> 12:02.820\n I would say my motivation to create AGI\n\n12:02.820 --> 12:05.220\n comes from both of those directions actually.\n\n12:05.220 --> 12:08.620\n So when I first became passionate about AGI\n\n12:08.620 --> 12:11.420\n when I was, it would have been two or three years old\n\n12:11.420 --> 12:14.700\n after watching robots on Star Trek.\n\n12:14.700 --> 12:18.180\n I mean, then it was really a combination\n\n12:18.180 --> 12:21.460\n of intellectual curiosity, like can a machine really think,\n\n12:21.460 --> 12:22.860\n how would you do that?\n\n12:22.860 --> 12:27.180\n And yeah, just ambition to create something much better\n\n12:27.180 --> 12:28.660\n than all the clearly limited\n\n12:28.660 --> 12:31.900\n and fundamentally defective humans I saw around me.\n\n12:31.900 --> 12:35.340\n Then as I got older and got more enmeshed\n\n12:35.340 --> 12:38.780\n in the human world and got married, had children,\n\n12:38.780 --> 12:41.900\n saw my parents begin to age, I started to realize,\n\n12:41.900 --> 12:45.300\n well, not only will AGI let you go far beyond\n\n12:45.300 --> 12:46.860\n the limitations of the human,\n\n12:46.860 --> 12:50.860\n but it could also stop us from dying and suffering\n\n12:50.860 --> 12:54.980\n and feeling pain and tormenting ourselves mentally.\n\n12:54.980 --> 12:58.060\n So you can see AGI has amazing capability\n\n12:58.060 --> 13:01.380\n to do good for humans, as humans,\n\n13:01.380 --> 13:03.420\n alongside with its capability\n\n13:03.420 --> 13:06.620\n to go far, far beyond the human level.\n\n13:06.620 --> 13:09.980\n So I mean, both aspects are there,\n\n13:09.980 --> 13:13.220\n which makes it even more exciting and important.\n\n13:13.220 --> 13:15.500\n So you mentioned Dostoevsky and Nietzsche.\n\n13:15.500 --> 13:17.060\n Where did you pick up from those guys?\n\n13:17.060 --> 13:17.900\n I mean.\n\n13:18.980 --> 13:21.500\n That would probably go beyond the scope\n\n13:21.500 --> 13:24.340\n of a brief interview, certainly.\n\n13:24.340 --> 13:26.780\n I mean, both of those are amazing thinkers\n\n13:26.780 --> 13:29.020\n who one, will necessarily have\n\n13:29.020 --> 13:32.060\n a complex relationship with, right?\n\n13:32.060 --> 13:36.460\n So, I mean, Dostoevsky on the minus side,\n\n13:36.460 --> 13:38.460\n he's kind of a religious fanatic\n\n13:38.460 --> 13:42.020\n and he sort of helped squash the Russian nihilist movement,\n\n13:42.020 --> 13:43.140\n which was very interesting.\n\n13:43.140 --> 13:45.820\n Because what nihilism meant originally\n\n13:45.820 --> 13:48.660\n in that period of the mid, late 1800s in Russia\n\n13:48.660 --> 13:52.180\n was not taking anything fully 100% for granted.\n\n13:52.180 --> 13:54.420\n It was really more like what we'd call Bayesianism now,\n\n13:54.420 --> 13:56.900\n where you don't wanna adopt anything\n\n13:56.900 --> 14:01.060\n as a dogmatic certitude and always leave your mind open.\n\n14:01.060 --> 14:04.420\n And how Dostoevsky parodied nihilism\n\n14:04.420 --> 14:06.660\n was a bit different, right?\n\n14:06.660 --> 14:10.340\n He parodied as people who believe absolutely nothing.\n\n14:10.340 --> 14:13.020\n So they must assign an equal probability weight\n\n14:13.020 --> 14:17.780\n to every proposition, which doesn't really work.\n\n14:17.780 --> 14:22.540\n So on the one hand, I didn't really agree with Dostoevsky\n\n14:22.540 --> 14:26.140\n on his sort of religious point of view.\n\n14:26.140 --> 14:29.660\n On the other hand, if you look at his understanding\n\n14:29.660 --> 14:32.660\n of human nature and sort of the human mind\n\n14:32.660 --> 14:37.100\n and heart and soul, it's really unparalleled.\n\n14:37.100 --> 14:42.100\n He had an amazing view of how human beings construct a world\n\n14:42.100 --> 14:45.380\n for themselves based on their own understanding\n\n14:45.380 --> 14:47.500\n and their own mental predisposition.\n\n14:47.500 --> 14:50.100\n And I think if you look in the brothers Karamazov\n\n14:50.100 --> 14:55.100\n in particular, the Russian literary theorist Mikhail Bakhtin\n\n14:56.140 --> 14:59.580\n wrote about this as a polyphonic mode of fiction,\n\n14:59.580 --> 15:02.300\n which means it's not third person,\n\n15:02.300 --> 15:05.020\n but it's not first person from any one person really.\n\n15:05.020 --> 15:07.020\n There are many different characters in the novel\n\n15:07.020 --> 15:10.020\n and each of them is sort of telling part of the story\n\n15:10.020 --> 15:11.580\n from their own point of view.\n\n15:11.580 --> 15:15.900\n So the reality of the whole story is an intersection\n\n15:15.900 --> 15:19.020\n like synergetically of the many different characters\n\n15:19.020 --> 15:19.860\n world views.\n\n15:19.860 --> 15:23.220\n And that really, it's a beautiful metaphor\n\n15:23.220 --> 15:26.100\n and even a reflection I think of how all of us\n\n15:26.100 --> 15:27.700\n socially create our reality.\n\n15:27.700 --> 15:31.060\n Like each of us sees the world in a certain way.\n\n15:31.060 --> 15:34.780\n Each of us in a sense is making the world as we see it\n\n15:34.780 --> 15:37.620\n based on our own minds and understanding,\n\n15:37.620 --> 15:40.980\n but it's polyphony like in music\n\n15:40.980 --> 15:43.300\n where multiple instruments are coming together\n\n15:43.300 --> 15:44.620\n to create the sound.\n\n15:44.620 --> 15:46.700\n The ultimate reality that's created\n\n15:46.700 --> 15:50.220\n comes out of each of our subjective understandings,\n\n15:50.220 --> 15:51.340\n intersecting with each other.\n\n15:51.340 --> 15:55.660\n And that was one of the many beautiful things in Dostoevsky.\n\n15:55.660 --> 15:57.980\n So maybe a little bit to mention,\n\n15:57.980 --> 16:02.260\n you have a connection to Russia and the Soviet culture.\n\n16:02.260 --> 16:03.860\n I mean, I'm not sure exactly what the nature\n\n16:03.860 --> 16:06.180\n of the connection is, but at least the spirit\n\n16:06.180 --> 16:07.380\n of your thinking is in there.\n\n16:07.380 --> 16:12.380\n Well, my ancestry is three quarters Eastern European Jewish.\n\n16:12.740 --> 16:16.740\n So I mean, my three of my great grandparents\n\n16:16.740 --> 16:20.340\n emigrated to New York from Lithuania\n\n16:20.340 --> 16:23.060\n and sort of border regions of Poland,\n\n16:23.060 --> 16:24.980\n which are in and out of Poland\n\n16:24.980 --> 16:28.020\n in around the time of World War I.\n\n16:28.020 --> 16:33.020\n And they were socialists and communists as well as Jews,\n\n16:33.700 --> 16:35.940\n mostly Menshevik, not Bolshevik.\n\n16:35.940 --> 16:39.260\n And they sort of, they fled at just the right time\n\n16:39.260 --> 16:41.260\n to the US for their own personal reasons.\n\n16:41.260 --> 16:45.580\n And then almost all, or maybe all of my extended family\n\n16:45.580 --> 16:47.220\n that remained in Eastern Europe was killed\n\n16:47.220 --> 16:50.380\n either by Hitlands or Stalin's minions at some point.\n\n16:50.380 --> 16:53.580\n So the branch of the family that emigrated to the US\n\n16:53.580 --> 16:56.740\n was pretty much the only one.\n\n16:56.740 --> 16:58.700\n So how much of the spirit of the people\n\n16:58.700 --> 16:59.900\n is in your blood still?\n\n16:59.900 --> 17:02.820\n Like, when you look in the mirror, do you see,\n\n17:03.900 --> 17:04.860\n what do you see?\n\n17:04.860 --> 17:08.460\n Meat, I see a bag of meat that I want to transcend\n\n17:08.460 --> 17:12.180\n by uploading into some sort of superior reality.\n\n17:12.180 --> 17:17.180\n But very, I mean, yeah, very clearly,\n\n17:18.340 --> 17:22.260\n I mean, I'm not religious in a traditional sense,\n\n17:22.260 --> 17:27.260\n but clearly the Eastern European Jewish tradition\n\n17:27.260 --> 17:28.780\n was what I was raised in.\n\n17:28.780 --> 17:32.700\n I mean, there was, my grandfather, Leo Zwell,\n\n17:32.700 --> 17:35.380\n was a physical chemist who worked with Linus Pauling\n\n17:35.380 --> 17:38.100\n and a bunch of the other early greats in quantum mechanics.\n\n17:38.100 --> 17:41.220\n I mean, he was into X ray diffraction.\n\n17:41.220 --> 17:42.940\n He was on the material science side,\n\n17:42.940 --> 17:45.420\n an experimentalist rather than a theorist.\n\n17:45.420 --> 17:47.700\n His sister was also a physicist.\n\n17:47.700 --> 17:51.100\n And my father's father, Victor Gertzel,\n\n17:51.100 --> 17:56.100\n was a PhD in psychology who had the unenviable job\n\n17:57.100 --> 17:59.260\n of giving Soka therapy to the Japanese\n\n17:59.260 --> 18:03.100\n in internment camps in the US in World War II,\n\n18:03.100 --> 18:05.820\n like to counsel them why they shouldn't kill themselves,\n\n18:05.820 --> 18:08.420\n even though they'd had all their stuff taken away\n\n18:08.420 --> 18:10.300\n and been imprisoned for no good reason.\n\n18:10.300 --> 18:15.300\n So, I mean, yeah, there's a lot of Eastern European\n\n18:15.780 --> 18:18.060\n Jewishness in my background.\n\n18:18.060 --> 18:20.180\n One of my great uncles was, I guess,\n\n18:20.180 --> 18:22.420\n conductor of San Francisco Orchestra.\n\n18:22.420 --> 18:25.620\n So there's a lot of Mickey Salkind,\n\n18:25.620 --> 18:27.660\n bunch of music in there also.\n\n18:27.660 --> 18:31.540\n And clearly this culture was all about learning\n\n18:31.540 --> 18:34.860\n and understanding the world,\n\n18:34.860 --> 18:38.820\n and also not quite taking yourself too seriously\n\n18:38.820 --> 18:39.900\n while you do it, right?\n\n18:39.900 --> 18:42.060\n There's a lot of Yiddish humor in there.\n\n18:42.060 --> 18:45.220\n So I do appreciate that culture,\n\n18:45.220 --> 18:47.580\n although the whole idea that like the Jews\n\n18:47.580 --> 18:49.020\n are the chosen people of God\n\n18:49.020 --> 18:51.740\n never resonated with me too much.\n\n18:51.740 --> 18:55.100\n The graph of the Gertzel family,\n\n18:55.100 --> 18:56.940\n I mean, just the people I've encountered\n\n18:56.940 --> 18:59.540\n just doing some research and just knowing your work\n\n18:59.540 --> 19:02.700\n through the decades, it's kind of fascinating.\n\n19:03.580 --> 19:06.380\n Just the number of PhDs.\n\n19:06.380 --> 19:10.740\n Yeah, yeah, I mean, my dad is a sociology professor\n\n19:10.740 --> 19:15.060\n who recently retired from Rutgers University,\n\n19:15.060 --> 19:18.540\n but clearly that gave me a head start in life.\n\n19:18.540 --> 19:20.260\n I mean, my grandfather gave me\n\n19:20.260 --> 19:21.620\n all those quantum mechanics books\n\n19:21.620 --> 19:24.220\n when I was like seven or eight years old.\n\n19:24.220 --> 19:26.060\n I remember going through them,\n\n19:26.060 --> 19:28.020\n and it was all the old quantum mechanics\n\n19:28.020 --> 19:30.420\n like Rutherford Adams and stuff.\n\n19:30.420 --> 19:32.860\n So I got to the part of wave functions,\n\n19:32.860 --> 19:36.140\n which I didn't understand, although I was very bright kid.\n\n19:36.140 --> 19:38.660\n And I realized he didn't quite understand it either,\n\n19:38.660 --> 19:41.980\n but at least like he pointed me to some professor\n\n19:41.980 --> 19:45.340\n he knew at UPenn nearby who understood these things, right?\n\n19:45.340 --> 19:49.620\n So that's an unusual opportunity for a kid to have, right?\n\n19:49.620 --> 19:52.380\n My dad, he was programming Fortran\n\n19:52.380 --> 19:53.900\n when I was 10 or 11 years old\n\n19:53.900 --> 19:57.660\n on like HP 3000 mainframes at Rutgers University.\n\n19:57.660 --> 20:00.900\n So I got to do linear regression in Fortran\n\n20:00.900 --> 20:04.220\n on punch cards when I was in middle school, right?\n\n20:04.220 --> 20:07.460\n Because he was doing, I guess, analysis of demographic\n\n20:07.460 --> 20:09.580\n and sociology data.\n\n20:09.580 --> 20:14.580\n So yes, certainly that gave me a head start\n\n20:14.780 --> 20:17.220\n and a push towards science beyond what would have been\n\n20:17.220 --> 20:19.700\n the case with many, many different situations.\n\n20:19.700 --> 20:22.220\n When did you first fall in love with AI?\n\n20:22.220 --> 20:24.700\n Is it the programming side of Fortran?\n\n20:24.700 --> 20:27.260\n Is it maybe the sociology psychology\n\n20:27.260 --> 20:28.300\n that you picked up from your dad?\n\n20:28.300 --> 20:29.140\n Or is it the quantum mechanics?\n\n20:29.140 --> 20:30.660\n I fell in love with AI when I was probably three years old\n\n20:30.660 --> 20:32.580\n when I saw a robot on Star Trek.\n\n20:32.580 --> 20:34.620\n It was turning around in a circle going,\n\n20:34.620 --> 20:36.660\n error, error, error, error,\n\n20:36.660 --> 20:39.540\n because Spock and Kirk had tricked it\n\n20:39.540 --> 20:41.300\n into a mechanical breakdown by presenting it\n\n20:41.300 --> 20:42.900\n with a logical paradox.\n\n20:42.900 --> 20:45.660\n And I was just like, well, this makes no sense.\n\n20:45.660 --> 20:47.540\n This AI is very, very smart.\n\n20:47.540 --> 20:49.620\n It's been traveling all around the universe,\n\n20:49.620 --> 20:50.980\n but these people could trick it\n\n20:50.980 --> 20:52.660\n with a simple logical paradox.\n\n20:52.660 --> 20:57.020\n Like why, if the human brain can get beyond that paradox,\n\n20:57.020 --> 20:59.460\n why can't this AI?\n\n20:59.460 --> 21:03.140\n So I felt the screenwriters of Star Trek\n\n21:03.140 --> 21:06.060\n had misunderstood the nature of intelligence.\n\n21:06.060 --> 21:07.580\n And I complained to my dad about it,\n\n21:07.580 --> 21:12.220\n and he wasn't gonna say anything one way or the other.\n\n21:12.220 --> 21:17.220\n But before I was born, when my dad was at Antioch College\n\n21:18.460 --> 21:20.860\n in the middle of the US,\n\n21:20.860 --> 21:25.860\n he led a protest movement called SLAM,\n\n21:25.860 --> 21:27.460\n Student League Against Mortality.\n\n21:27.460 --> 21:28.980\n They were protesting against death,\n\n21:28.980 --> 21:31.500\n wandering across the campus.\n\n21:31.500 --> 21:35.900\n So he was into some futuristic things even back then,\n\n21:35.900 --> 21:40.220\n but whether AI could confront logical paradoxes or not,\n\n21:40.220 --> 21:41.220\n he didn't know.\n\n21:41.220 --> 21:44.780\n But when I, 10 years after that or something,\n\n21:44.780 --> 21:46.980\n I discovered Douglas Hofstadter's book,\n\n21:46.980 --> 21:51.100\n Gordalesh or Bach, and that was sort of to the same point of AI\n\n21:51.100 --> 21:52.620\n and paradox and logic, right?\n\n21:52.620 --> 21:54.460\n Because he was over and over\n\n21:54.460 --> 21:56.180\n with Gordal's incompleteness theorem,\n\n21:56.180 --> 22:00.500\n and can an AI really fully model itself reflexively\n\n22:00.500 --> 22:02.820\n or does that lead you into some paradox?\n\n22:02.820 --> 22:05.260\n Can the human mind truly model itself reflexively\n\n22:05.260 --> 22:07.500\n or does that lead you into some paradox?\n\n22:07.500 --> 22:10.660\n So I think that book, Gordalesh or Bach,\n\n22:10.660 --> 22:13.460\n which I think I read when it first came out,\n\n22:13.460 --> 22:14.980\n I would have been 12 years old or something.\n\n22:14.980 --> 22:17.100\n I remember it was like 16 hour day.\n\n22:17.100 --> 22:19.780\n I read it cover to cover and then reread it.\n\n22:19.780 --> 22:21.260\n I reread it after that,\n\n22:21.260 --> 22:22.380\n because there was a lot of weird things\n\n22:22.380 --> 22:24.380\n with little formal systems in there\n\n22:24.380 --> 22:25.660\n that were hard for me at the time.\n\n22:25.660 --> 22:27.980\n But that was the first book I read\n\n22:27.980 --> 22:32.980\n that gave me a feeling for AI as like a practical academic\n\n22:34.420 --> 22:37.380\n or engineering discipline that people were working in.\n\n22:37.380 --> 22:40.060\n Because before I read Gordalesh or Bach,\n\n22:40.060 --> 22:43.980\n I was into AI from the point of view of a science fiction fan.\n\n22:43.980 --> 22:47.460\n And I had the idea, well, it may be a long time\n\n22:47.460 --> 22:50.420\n before we can achieve immortality in superhuman AGI.\n\n22:50.420 --> 22:54.380\n So I should figure out how to build a spacecraft\n\n22:54.380 --> 22:57.060\n traveling close to the speed of light, go far away,\n\n22:57.060 --> 22:58.780\n then come back to the earth in a million years\n\n22:58.780 --> 23:00.220\n when technology is more advanced\n\n23:00.220 --> 23:01.700\n and we can build these things.\n\n23:01.700 --> 23:03.580\n Reading Gordalesh or Bach,\n\n23:03.580 --> 23:06.580\n while it didn't all ring true to me, a lot of it did,\n\n23:06.580 --> 23:09.860\n but I could see like there are smart people right now\n\n23:09.860 --> 23:11.580\n at various universities around me\n\n23:11.580 --> 23:15.420\n who are actually trying to work on building\n\n23:15.420 --> 23:16.980\n what I would now call AGI,\n\n23:16.980 --> 23:19.020\n although Hofstadter didn't call it that.\n\n23:19.020 --> 23:21.100\n So really it was when I read that book,\n\n23:21.100 --> 23:23.540\n which would have been probably middle school,\n\n23:23.540 --> 23:24.820\n that then I started to think,\n\n23:24.820 --> 23:29.020\n well, this is something that I could practically work on.\n\n23:29.020 --> 23:31.660\n Yeah, as opposed to flying away and waiting it out,\n\n23:31.660 --> 23:33.500\n you can actually be one of the people\n\n23:33.500 --> 23:34.580\n that actually builds the system.\n\n23:34.580 --> 23:35.420\n Yeah, exactly.\n\n23:35.420 --> 23:36.740\n And if you think about, I mean,\n\n23:36.740 --> 23:40.700\n I was interested in what we'd now call nanotechnology\n\n23:40.700 --> 23:44.820\n and in the human immortality and time travel,\n\n23:44.820 --> 23:46.940\n all the same cool things as every other,\n\n23:46.940 --> 23:49.260\n like science fiction loving kid.\n\n23:49.260 --> 23:52.700\n But AI seemed like if Hofstadter was right,\n\n23:52.700 --> 23:54.180\n you just figure out the right program,\n\n23:54.180 --> 23:55.060\n sit there and type it.\n\n23:55.060 --> 23:59.620\n Like you don't need to spin stars into weird configurations\n\n23:59.620 --> 24:02.620\n or get government approval to cut people up\n\n24:02.620 --> 24:05.020\n and fiddle with their DNA or something, right?\n\n24:05.020 --> 24:06.180\n It's just programming.\n\n24:06.180 --> 24:10.700\n And then of course that can achieve anything else.\n\n24:10.700 --> 24:12.220\n There's another book from back then,\n\n24:12.220 --> 24:17.060\n which was by Gerald Feinbaum,\n\n24:17.060 --> 24:21.580\n who was a physicist at Princeton.\n\n24:21.580 --> 24:24.580\n And that was the Prometheus Project.\n\n24:24.580 --> 24:26.700\n And this book was written in the late 1960s,\n\n24:26.700 --> 24:28.780\n though I encountered it in the mid 70s.\n\n24:28.780 --> 24:30.940\n But what this book said is in the next few decades,\n\n24:30.940 --> 24:34.500\n humanity is gonna create superhuman thinking machines,\n\n24:34.500 --> 24:37.460\n molecular nanotechnology and human immortality.\n\n24:37.460 --> 24:41.140\n And then the challenge we'll have is what to do with it.\n\n24:41.140 --> 24:43.020\n Do we use it to expand human consciousness\n\n24:43.020 --> 24:44.500\n in a positive direction?\n\n24:44.500 --> 24:49.500\n Or do we use it just to further vapid consumerism?\n\n24:49.860 --> 24:51.820\n And what he proposed was that the UN\n\n24:51.820 --> 24:53.460\n should do a survey on this.\n\n24:53.460 --> 24:56.460\n And the UN should send people out to every little village\n\n24:56.460 --> 24:58.940\n in remotest Africa or South America\n\n24:58.940 --> 25:01.300\n and explain to everyone what technology\n\n25:01.300 --> 25:03.020\n was gonna bring the next few decades\n\n25:03.020 --> 25:05.020\n and the choice that we had about how to use it.\n\n25:05.020 --> 25:07.780\n And let everyone on the whole planet vote\n\n25:07.780 --> 25:11.740\n about whether we should develop super AI nanotechnology\n\n25:11.740 --> 25:15.900\n and immortality for expanded consciousness\n\n25:15.900 --> 25:18.220\n or for rampant consumerism.\n\n25:18.220 --> 25:22.060\n And needless to say, that didn't quite happen.\n\n25:22.060 --> 25:24.180\n And I think this guy died in the mid 80s,\n\n25:24.180 --> 25:25.900\n so we didn't even see his ideas start\n\n25:25.900 --> 25:28.220\n to become more mainstream.\n\n25:28.220 --> 25:31.620\n But it's interesting, many of the themes I'm engaged with now\n\n25:31.620 --> 25:33.340\n from AGI and immortality,\n\n25:33.340 --> 25:36.140\n even to trying to democratize technology\n\n25:36.140 --> 25:38.100\n as I've been pushing forward with Singularity,\n\n25:38.100 --> 25:40.020\n my work in the blockchain world,\n\n25:40.020 --> 25:43.620\n many of these themes were there in Feinbaum's book\n\n25:43.620 --> 25:47.940\n in the late 60s even.\n\n25:47.940 --> 25:52.220\n And of course, Valentin Turchin, a Russian writer\n\n25:52.220 --> 25:55.860\n and a great Russian physicist who I got to know\n\n25:55.860 --> 25:59.060\n when we both lived in New York in the late 90s\n\n25:59.060 --> 25:59.900\n and early aughts.\n\n25:59.900 --> 26:03.380\n I mean, he had a book in the late 60s in Russia,\n\n26:03.380 --> 26:05.780\n which was the phenomenon of science,\n\n26:05.780 --> 26:10.220\n which laid out all these same things as well.\n\n26:10.220 --> 26:12.740\n And Val died in, I don't remember,\n\n26:12.740 --> 26:15.420\n 2004 or five or something of Parkinson'sism.\n\n26:15.420 --> 26:20.420\n So yeah, it's easy for people to lose track now\n\n26:20.780 --> 26:25.780\n of the fact that the futurist and Singularitarian\n\n26:25.940 --> 26:29.740\n advanced technology ideas that are now almost mainstream\n\n26:29.740 --> 26:30.900\n are on TV all the time.\n\n26:30.900 --> 26:34.100\n I mean, these are not that new, right?\n\n26:34.100 --> 26:37.100\n They're sort of new in the history of the human species,\n\n26:37.100 --> 26:41.100\n but I mean, these were all around in fairly mature form\n\n26:41.100 --> 26:43.660\n in the middle of the last century,\n\n26:43.660 --> 26:45.500\n were written about quite articulately\n\n26:45.500 --> 26:47.340\n by fairly mainstream people\n\n26:47.340 --> 26:50.140\n who were professors at top universities.\n\n26:50.140 --> 26:52.940\n It's just until the enabling technologies\n\n26:52.940 --> 26:57.940\n got to a certain point, then you couldn't make it real.\n\n26:57.940 --> 27:02.820\n And even in the 70s, I was sort of seeing that\n\n27:02.820 --> 27:04.740\n and living through it, right?\n\n27:04.740 --> 27:07.900\n From Star Trek to Douglas Hofstadter,\n\n27:07.900 --> 27:09.660\n things were getting very, very practical\n\n27:09.660 --> 27:11.980\n from the late 60s to the late 70s.\n\n27:11.980 --> 27:15.020\n And the first computer I bought,\n\n27:15.020 --> 27:17.580\n you could only program with hexadecimal machine code\n\n27:17.580 --> 27:19.380\n and you had to solder it together.\n\n27:19.380 --> 27:23.420\n And then like a few years later, there's punch cards.\n\n27:23.420 --> 27:27.220\n And a few years later, you could get like Atari 400\n\n27:27.220 --> 27:30.300\n and Commodore VIC 20, and you could type on the keyboard\n\n27:30.300 --> 27:32.820\n and program in higher level languages\n\n27:32.820 --> 27:34.660\n alongside the assembly language.\n\n27:34.660 --> 27:38.700\n So these ideas have been building up a while.\n\n27:38.700 --> 27:42.980\n And I guess my generation got to feel them build up,\n\n27:42.980 --> 27:46.380\n which is different than people coming into the field now\n\n27:46.380 --> 27:50.300\n for whom these things have just been part of the ambience\n\n27:50.300 --> 27:52.180\n of culture for their whole career\n\n27:52.180 --> 27:54.140\n or even their whole life.\n\n27:54.140 --> 27:57.260\n Well, it's fascinating to think about there being all\n\n27:57.260 --> 28:01.540\n of these ideas kind of swimming, almost with the noise\n\n28:01.540 --> 28:04.380\n all around the world, all the different generations,\n\n28:04.380 --> 28:07.900\n and then some kind of nonlinear thing happens\n\n28:07.900 --> 28:09.380\n where they percolate up\n\n28:09.380 --> 28:12.420\n and capture the imagination of the mainstream.\n\n28:12.420 --> 28:14.780\n And that seems to be what's happening with AI now.\n\n28:14.780 --> 28:16.580\n I mean, Nietzsche, who you mentioned had the idea\n\n28:16.580 --> 28:18.260\n of the Superman, right?\n\n28:18.260 --> 28:21.580\n But he didn't understand enough about technology\n\n28:21.580 --> 28:24.860\n to think you could physically engineer a Superman\n\n28:24.860 --> 28:28.180\n by piecing together molecules in a certain way.\n\n28:28.180 --> 28:33.180\n He was a bit vague about how the Superman would appear,\n\n28:33.620 --> 28:35.820\n but he was quite deep at thinking\n\n28:35.820 --> 28:37.780\n about what the state of consciousness\n\n28:37.780 --> 28:42.420\n and the mode of cognition of a Superman would be.\n\n28:42.420 --> 28:47.420\n He was a very astute analyst of how the human mind\n\n28:47.820 --> 28:49.420\n constructs the illusion of a self,\n\n28:49.420 --> 28:52.140\n how it constructs the illusion of free will,\n\n28:52.140 --> 28:56.660\n how it constructs values like good and evil\n\n28:56.660 --> 28:59.780\n out of its own desire to maintain\n\n28:59.780 --> 29:01.420\n and advance its own organism.\n\n29:01.420 --> 29:04.020\n He understood a lot about how human minds work.\n\n29:04.020 --> 29:05.660\n Then he understood a lot\n\n29:05.660 --> 29:07.620\n about how post human minds would work.\n\n29:07.620 --> 29:10.260\n I mean, the Superman was supposed to be a mind\n\n29:10.260 --> 29:13.300\n that would basically have complete root access\n\n29:13.300 --> 29:16.060\n to its own brain and consciousness\n\n29:16.060 --> 29:19.620\n and be able to architect its own value system\n\n29:19.620 --> 29:24.300\n and inspect and fine tune all of its own biases.\n\n29:24.300 --> 29:27.340\n So that's a lot of powerful thinking there,\n\n29:27.340 --> 29:29.340\n which then fed in and sort of seeded\n\n29:29.340 --> 29:32.180\n all of postmodern continental philosophy\n\n29:32.180 --> 29:35.540\n and all sorts of things have been very valuable\n\n29:35.540 --> 29:39.740\n in development of culture and indirectly even of technology.\n\n29:39.740 --> 29:42.140\n But of course, without the technology there,\n\n29:42.140 --> 29:44.860\n it was all some quite abstract thinking.\n\n29:44.860 --> 29:46.940\n So now we're at a time in history\n\n29:46.940 --> 29:51.740\n when a lot of these ideas can be made real,\n\n29:51.740 --> 29:54.300\n which is amazing and scary, right?\n\n29:54.300 --> 29:56.020\n It's kind of interesting to think,\n\n29:56.020 --> 29:57.180\n what do you think Nietzsche would do\n\n29:57.180 --> 30:00.900\n if he was born a century later or transported through time?\n\n30:00.900 --> 30:02.980\n What do you think he would say about AI?\n\n30:02.980 --> 30:04.180\n I mean. Well, those are quite different.\n\n30:04.180 --> 30:07.260\n If he's born a century later or transported through time.\n\n30:07.260 --> 30:09.580\n Well, he'd be on like TikTok and Instagram\n\n30:09.580 --> 30:11.940\n and he would never write the great works he's written.\n\n30:11.940 --> 30:13.540\n So let's transport him through time.\n\n30:13.540 --> 30:16.460\n Maybe also Sprach Zarathustra would be a music video,\n\n30:16.460 --> 30:19.660\n right? I mean, who knows?\n\n30:19.660 --> 30:21.700\n Yeah, but if he was transported through time,\n\n30:21.700 --> 30:26.260\n do you think, that'd be interesting actually to go back.\n\n30:26.260 --> 30:29.380\n You just made me realize that it's possible to go back\n\n30:29.380 --> 30:31.220\n and read Nietzsche with an eye of,\n\n30:31.220 --> 30:34.700\n is there some thinking about artificial beings?\n\n30:34.700 --> 30:37.780\n I'm sure there he had inklings.\n\n30:37.780 --> 30:40.500\n I mean, with Frankenstein before him,\n\n30:40.500 --> 30:42.900\n I'm sure he had inklings of artificial beings\n\n30:42.900 --> 30:44.060\n somewhere in the text.\n\n30:44.060 --> 30:46.900\n It'd be interesting to try to read his work\n\n30:46.900 --> 30:51.900\n to see if Superman was actually an AGI system.\n\n30:55.820 --> 30:57.940\n Like if he had inklings of that kind of thinking.\n\n30:57.940 --> 30:58.780\n He didn't.\n\n30:58.780 --> 30:59.620\n He didn't.\n\n30:59.620 --> 31:01.100\n No, I would say not.\n\n31:01.100 --> 31:06.100\n I mean, he had a lot of inklings of modern cognitive science,\n\n31:06.460 --> 31:07.420\n which are very interesting.\n\n31:07.420 --> 31:11.820\n If you look in like the third part of the collection\n\n31:11.820 --> 31:13.540\n that's been titled The Will to Power.\n\n31:13.540 --> 31:15.660\n I mean, in book three there,\n\n31:15.660 --> 31:20.620\n there's very deep analysis of thinking processes,\n\n31:20.620 --> 31:25.620\n but he wasn't so much of a physical tinkerer type guy,\n\n31:27.140 --> 31:29.620\n right? He was very abstract.\n\n31:29.620 --> 31:32.780\n Do you think, what do you think about the will to power?\n\n31:32.780 --> 31:36.100\n Do you think human, what do you think drives humans?\n\n31:36.100 --> 31:37.460\n Is it?\n\n31:37.460 --> 31:39.500\n Oh, an unholy mix of things.\n\n31:39.500 --> 31:42.380\n I don't think there's one pure, simple,\n\n31:42.380 --> 31:47.380\n and elegant objective function driving humans by any means.\n\n31:47.380 --> 31:50.700\n What do you think, if we look at,\n\n31:50.700 --> 31:53.260\n I know it's hard to look at humans in an aggregate,\n\n31:53.260 --> 31:56.220\n but do you think overall humans are good?\n\n31:57.540 --> 32:01.580\n Or do we have both good and evil within us\n\n32:01.580 --> 32:03.540\n that depending on the circumstances,\n\n32:03.540 --> 32:08.220\n depending on whatever can percolate to the top?\n\n32:08.220 --> 32:13.220\n Good and evil are very ambiguous, complicated\n\n32:13.900 --> 32:15.900\n and in some ways silly concepts.\n\n32:15.900 --> 32:18.540\n But if we could dig into your question\n\n32:18.540 --> 32:19.700\n from a couple of directions.\n\n32:19.700 --> 32:23.420\n So I think if you look in evolution,\n\n32:23.420 --> 32:28.220\n humanity is shaped both by individual selection\n\n32:28.220 --> 32:30.940\n and what biologists would call group selection,\n\n32:30.940 --> 32:32.740\n like tribe level selection, right?\n\n32:32.740 --> 32:36.500\n So individual selection has driven us\n\n32:36.500 --> 32:38.780\n in a selfish DNA sort of way.\n\n32:38.780 --> 32:43.260\n So that each of us does to a certain approximation\n\n32:43.260 --> 32:47.420\n what will help us propagate our DNA to future generations.\n\n32:47.420 --> 32:50.700\n I mean, that's why I've got four kids so far\n\n32:50.700 --> 32:53.900\n and probably that's not the last one.\n\n32:53.900 --> 32:55.020\n On the other hand.\n\n32:55.020 --> 32:56.780\n I like the ambition.\n\n32:56.780 --> 33:00.740\n Tribal, like group selection means humans in a way\n\n33:00.740 --> 33:04.380\n will do what will advocate for the persistence of the DNA\n\n33:04.380 --> 33:08.100\n of their whole tribe or their social group.\n\n33:08.100 --> 33:11.740\n And in biology, you have both of these, right?\n\n33:11.740 --> 33:14.420\n And you can see, say an ant colony or a beehive,\n\n33:14.420 --> 33:15.940\n there's a lot of group selection\n\n33:15.940 --> 33:18.940\n in the evolution of those social animals.\n\n33:18.940 --> 33:21.460\n On the other hand, say a big cat\n\n33:21.460 --> 33:23.260\n or some very solitary animal,\n\n33:23.260 --> 33:26.540\n it's a lot more biased toward individual selection.\n\n33:26.540 --> 33:28.660\n Humans are an interesting balance.\n\n33:28.660 --> 33:31.540\n And I think this reflects itself\n\n33:31.540 --> 33:35.060\n in what we would view as selfishness versus altruism\n\n33:35.060 --> 33:36.780\n to some extent.\n\n33:36.780 --> 33:40.580\n So we just have both of those objective functions\n\n33:40.580 --> 33:43.780\n contributing to the makeup of our brains.\n\n33:43.780 --> 33:47.300\n And then as Nietzsche analyzed in his own way\n\n33:47.300 --> 33:49.060\n and others have analyzed in different ways,\n\n33:49.060 --> 33:51.500\n I mean, we abstract this as well,\n\n33:51.500 --> 33:55.380\n we have both good and evil within us, right?\n\n33:55.380 --> 33:57.820\n Because a lot of what we view as evil\n\n33:57.820 --> 34:00.460\n is really just selfishness.\n\n34:00.460 --> 34:03.740\n A lot of what we view as good is altruism,\n\n34:03.740 --> 34:07.220\n which means doing what's good for the tribe.\n\n34:07.220 --> 34:08.060\n And on that level,\n\n34:08.060 --> 34:11.380\n we have both of those just baked into us\n\n34:11.380 --> 34:13.180\n and that's how it is.\n\n34:13.180 --> 34:17.020\n Of course, there are psychopaths and sociopaths\n\n34:17.020 --> 34:21.340\n and people who get gratified by the suffering of others.\n\n34:21.340 --> 34:25.260\n And that's a different thing.\n\n34:25.260 --> 34:27.500\n Yeah, those are exceptions on the whole.\n\n34:27.500 --> 34:31.540\n But I think at core, we're not purely selfish,\n\n34:31.540 --> 34:35.180\n we're not purely altruistic, we are a mix\n\n34:35.180 --> 34:38.020\n and that's the nature of it.\n\n34:38.020 --> 34:43.020\n And we also have a complex constellation of values\n\n34:43.380 --> 34:48.380\n that are just very specific to our evolutionary history.\n\n34:49.180 --> 34:52.500\n Like we love waterways and mountains\n\n34:52.500 --> 34:54.460\n and the ideal place to put a house\n\n34:54.460 --> 34:56.340\n is in a mountain overlooking the water, right?\n\n34:56.340 --> 35:00.580\n And we care a lot about our kids\n\n35:00.580 --> 35:02.820\n and we care a little less about our cousins\n\n35:02.820 --> 35:04.420\n and even less about our fifth cousins.\n\n35:04.420 --> 35:09.420\n I mean, there are many particularities to human values,\n\n35:09.460 --> 35:11.900\n which whether they're good or evil\n\n35:11.900 --> 35:15.820\n depends on your perspective.\n\n35:15.820 --> 35:19.660\n Say, I spent a lot of time in Ethiopia in Addis Ababa\n\n35:19.660 --> 35:22.460\n where we have one of our AI development offices\n\n35:22.460 --> 35:24.420\n for my SingularityNet project.\n\n35:24.420 --> 35:27.540\n And when I walk through the streets in Addis,\n\n35:27.540 --> 35:31.460\n you know, there's people lying by the side of the road,\n\n35:31.460 --> 35:33.940\n like just living there by the side of the road,\n\n35:33.940 --> 35:35.820\n dying probably of curable diseases\n\n35:35.820 --> 35:37.940\n without enough food or medicine.\n\n35:37.940 --> 35:39.980\n And when I walk by them, you know, I feel terrible,\n\n35:39.980 --> 35:41.460\n I give them money.\n\n35:41.460 --> 35:43.900\n When I come back home to the developed world,\n\n35:45.100 --> 35:46.620\n they're not on my mind that much.\n\n35:46.620 --> 35:48.620\n I do donate some, but I mean,\n\n35:48.620 --> 35:52.860\n I also spend some of the limited money I have\n\n35:52.860 --> 35:54.700\n enjoying myself in frivolous ways\n\n35:54.700 --> 35:58.100\n rather than donating it to those people who are right now,\n\n35:58.100 --> 36:01.020\n like starving, dying and suffering on the roadside.\n\n36:01.020 --> 36:03.180\n So does that make me evil?\n\n36:03.180 --> 36:05.500\n I mean, it makes me somewhat selfish\n\n36:05.500 --> 36:06.740\n and somewhat altruistic.\n\n36:06.740 --> 36:10.940\n And we each balance that in our own way, right?\n\n36:10.940 --> 36:15.940\n So whether that will be true of all possible AGI's\n\n36:17.060 --> 36:19.300\n is a subtler question.\n\n36:19.300 --> 36:21.340\n So that's how humans are.\n\n36:21.340 --> 36:23.100\n So you have a sense, you kind of mentioned\n\n36:23.100 --> 36:25.500\n that there's a selfish,\n\n36:25.500 --> 36:28.300\n I'm not gonna bring up the whole Ayn Rand idea\n\n36:28.300 --> 36:31.140\n of selfishness being the core virtue.\n\n36:31.140 --> 36:33.980\n That's a whole interesting kind of tangent\n\n36:33.980 --> 36:36.420\n that I think we'll just distract ourselves on.\n\n36:36.420 --> 36:38.460\n I have to make one amusing comment.\n\n36:38.460 --> 36:39.300\n Sure.\n\n36:39.300 --> 36:41.260\n A comment that has amused me anyway.\n\n36:41.260 --> 36:46.260\n So the, yeah, I have extraordinary negative respect\n\n36:46.340 --> 36:47.820\n for Ayn Rand.\n\n36:47.820 --> 36:50.220\n Negative, what's a negative respect?\n\n36:50.220 --> 36:54.740\n But when I worked with a company called Genescient,\n\n36:54.740 --> 36:59.180\n which was evolving flies to have extraordinary long lives\n\n36:59.180 --> 37:01.220\n in Southern California.\n\n37:01.220 --> 37:04.980\n So we had flies that were evolved by artificial selection\n\n37:04.980 --> 37:07.660\n to have five times the lifespan of normal fruit flies.\n\n37:07.660 --> 37:11.780\n But the population of super long lived flies\n\n37:11.780 --> 37:14.060\n was physically sitting in a spare room\n\n37:14.060 --> 37:18.100\n at an Ayn Rand elementary school in Southern California.\n\n37:18.100 --> 37:19.460\n So that was just like,\n\n37:19.460 --> 37:22.620\n well, if I saw this in a movie, I wouldn't believe it.\n\n37:23.980 --> 37:26.020\n Well, yeah, the universe has a sense of humor\n\n37:26.020 --> 37:26.860\n in that kind of way.\n\n37:26.860 --> 37:28.900\n That fits in, humor fits in somehow\n\n37:28.900 --> 37:30.620\n into this whole absurd existence.\n\n37:30.620 --> 37:33.820\n But you mentioned the balance between selfishness\n\n37:33.820 --> 37:37.220\n and altruism as kind of being innate.\n\n37:37.220 --> 37:38.140\n Do you think it's possible\n\n37:38.140 --> 37:42.380\n that's kind of an emergent phenomena,\n\n37:42.380 --> 37:45.420\n those peculiarities of our value system?\n\n37:45.420 --> 37:47.180\n How much of it is innate?\n\n37:47.180 --> 37:49.780\n How much of it is something we collectively\n\n37:49.780 --> 37:51.460\n kind of like a Dostoevsky novel\n\n37:52.300 --> 37:54.540\n bring to life together as a civilization?\n\n37:54.540 --> 37:57.740\n I mean, the answer to nature versus nurture\n\n37:57.740 --> 37:58.860\n is usually both.\n\n37:58.860 --> 38:01.820\n And of course it's nature versus nurture\n\n38:01.820 --> 38:04.780\n versus self organization, as you mentioned.\n\n38:04.780 --> 38:08.460\n So clearly there are evolutionary roots\n\n38:08.460 --> 38:11.460\n to individual and group selection\n\n38:11.460 --> 38:13.900\n leading to a mix of selfishness and altruism.\n\n38:13.900 --> 38:15.380\n On the other hand,\n\n38:15.380 --> 38:19.780\n different cultures manifest that in different ways.\n\n38:19.780 --> 38:22.540\n Well, we all have basically the same biology.\n\n38:22.540 --> 38:26.660\n And if you look at sort of precivilized cultures,\n\n38:26.660 --> 38:29.340\n you have tribes like the Yanomamo in Venezuela,\n\n38:29.340 --> 38:34.340\n which their culture is focused on killing other tribes.\n\n38:35.340 --> 38:37.620\n And you have other Stone Age tribes\n\n38:37.620 --> 38:40.460\n that are mostly peaceful and have big taboos\n\n38:40.460 --> 38:41.420\n against violence.\n\n38:41.420 --> 38:43.900\n So you can certainly have a big difference\n\n38:43.900 --> 38:46.860\n in how culture manifests\n\n38:46.860 --> 38:50.820\n these innate biological characteristics,\n\n38:50.820 --> 38:54.740\n but still, there's probably limits\n\n38:54.740 --> 38:56.740\n that are given by our biology.\n\n38:56.740 --> 39:00.060\n I used to argue this with my great grandparents\n\n39:00.060 --> 39:01.500\n who were Marxists actually,\n\n39:01.500 --> 39:04.540\n because they believed in the withering away of the state.\n\n39:04.540 --> 39:06.900\n Like they believe that,\n\n39:06.900 --> 39:10.660\n as you move from capitalism to socialism to communism,\n\n39:10.660 --> 39:13.420\n people would just become more social minded\n\n39:13.420 --> 39:15.940\n so that a state would be unnecessary\n\n39:15.940 --> 39:20.940\n and everyone would give everyone else what they needed.\n\n39:20.940 --> 39:23.140\n Now, setting aside that\n\n39:23.140 --> 39:25.740\n that's not what the various Marxist experiments\n\n39:25.740 --> 39:29.900\n on the planet seem to be heading toward in practice.\n\n39:29.900 --> 39:32.740\n Just as a theoretical point,\n\n39:32.740 --> 39:37.540\n I was very dubious that human nature could go there.\n\n39:37.540 --> 39:39.900\n Like at that time when my great grandparents are alive,\n\n39:39.900 --> 39:43.300\n I was just like, you know, I'm a cynical teenager.\n\n39:43.300 --> 39:45.980\n I think humans are just jerks.\n\n39:45.980 --> 39:48.020\n The state is not gonna wither away.\n\n39:48.020 --> 39:49.980\n If you don't have some structure\n\n39:49.980 --> 39:51.980\n keeping people from screwing each other over,\n\n39:51.980 --> 39:52.900\n they're gonna do it.\n\n39:52.900 --> 39:56.220\n So now I actually don't quite see things that way.\n\n39:56.220 --> 39:59.900\n I mean, I think my feeling now subjectively\n\n39:59.900 --> 40:02.580\n is the culture aspect is more significant\n\n40:02.580 --> 40:04.620\n than I thought it was when I was a teenager.\n\n40:04.620 --> 40:08.260\n And I think you could have a human society\n\n40:08.260 --> 40:11.420\n that was dialed dramatically further toward,\n\n40:11.420 --> 40:13.700\n you know, self awareness, other awareness,\n\n40:13.700 --> 40:16.980\n compassion and sharing than our current society.\n\n40:16.980 --> 40:20.580\n And of course, greater material abundance helps,\n\n40:20.580 --> 40:23.480\n but to some extent material abundance\n\n40:23.480 --> 40:25.380\n is a subjective perception also\n\n40:25.380 --> 40:28.260\n because many Stone Age cultures perceive themselves\n\n40:28.260 --> 40:30.540\n as living in great material abundance\n\n40:30.540 --> 40:32.100\n that they had all the food and water they wanted,\n\n40:32.100 --> 40:33.500\n they lived in a beautiful place,\n\n40:33.500 --> 40:37.460\n that they had sex lives, that they had children.\n\n40:37.460 --> 40:42.460\n I mean, they had abundance without any factories, right?\n\n40:42.940 --> 40:46.460\n So I think humanity probably would be capable\n\n40:46.460 --> 40:51.140\n of fundamentally more positive and joy filled mode\n\n40:51.140 --> 40:55.560\n of social existence than what we have now.\n\n40:57.320 --> 40:59.500\n Clearly Marx didn't quite have the right idea\n\n40:59.500 --> 41:01.800\n about how to get there.\n\n41:01.800 --> 41:05.660\n I mean, he missed a number of key aspects\n\n41:05.660 --> 41:09.500\n of human society and its evolution.\n\n41:09.500 --> 41:11.960\n And if we look at where we are in society now,\n\n41:13.140 --> 41:15.760\n how to get there is a quite different question\n\n41:15.760 --> 41:18.100\n because there are very powerful forces\n\n41:18.100 --> 41:21.080\n pushing people in different directions\n\n41:21.080 --> 41:26.080\n than a positive, joyous, compassionate existence, right?\n\n41:26.380 --> 41:28.820\n So if we were tried to, you know,\n\n41:28.820 --> 41:32.820\n Elon Musk is dreams of colonizing Mars at the moment,\n\n41:32.820 --> 41:36.880\n so we maybe will have a chance to start a new civilization\n\n41:36.880 --> 41:38.400\n with a new governmental system.\n\n41:38.400 --> 41:41.580\n And certainly there's quite a bit of chaos.\n\n41:41.580 --> 41:44.320\n We're sitting now, I don't know what the date is,\n\n41:44.320 --> 41:46.860\n but this is June.\n\n41:46.860 --> 41:49.260\n There's quite a bit of chaos in all different forms\n\n41:49.260 --> 41:52.060\n going on in the United States and all over the world.\n\n41:52.060 --> 41:55.560\n So there's a hunger for new types of governments,\n\n41:55.560 --> 41:58.260\n new types of leadership, new types of systems.\n\n41:59.860 --> 42:01.980\n And so what are the forces at play\n\n42:01.980 --> 42:04.140\n and how do we move forward?\n\n42:04.140 --> 42:06.780\n Yeah, I mean, colonizing Mars, first of all,\n\n42:06.780 --> 42:08.980\n it's a super cool thing to do.\n\n42:08.980 --> 42:10.060\n We should be doing it.\n\n42:10.060 --> 42:11.540\n So you love the idea.\n\n42:11.540 --> 42:14.780\n Yeah, I mean, it's more important than making\n\n42:14.780 --> 42:18.540\n chocolatey or chocolates and sexier lingerie\n\n42:18.540 --> 42:21.020\n and many of the things that we spend\n\n42:21.020 --> 42:24.120\n a lot more resources on as a species, right?\n\n42:24.120 --> 42:26.480\n So I mean, we certainly should do it.\n\n42:26.480 --> 42:31.480\n I think the possible futures in which a Mars colony\n\n42:33.180 --> 42:38.040\n makes a critical difference for humanity are very few.\n\n42:38.040 --> 42:42.220\n I mean, I think, I mean, assuming we make a Mars colony\n\n42:42.220 --> 42:44.000\n and people go live there in a couple of decades,\n\n42:44.000 --> 42:46.380\n I mean, their supplies are gonna come from Earth.\n\n42:46.380 --> 42:48.820\n The money to make the colony came from Earth\n\n42:48.820 --> 42:53.740\n and whatever powers are supplying the goods there\n\n42:53.740 --> 42:56.820\n from Earth are gonna, in effect, be in control\n\n42:56.820 --> 42:58.700\n of that Mars colony.\n\n42:58.700 --> 43:02.060\n Of course, there are outlier situations\n\n43:02.060 --> 43:06.460\n where Earth gets nuked into oblivion\n\n43:06.460 --> 43:10.780\n and somehow Mars has been made self sustaining by that point\n\n43:10.780 --> 43:14.220\n and then Mars is what allows humanity to persist.\n\n43:14.220 --> 43:19.220\n But I think that those are very, very, very unlikely.\n\n43:19.740 --> 43:23.020\n You don't think it could be a first step on a long journey?\n\n43:23.020 --> 43:24.740\n Of course it's a first step on a long journey,\n\n43:24.740 --> 43:27.140\n which is awesome.\n\n43:27.140 --> 43:30.980\n I'm guessing the colonization of the rest\n\n43:30.980 --> 43:33.260\n of the physical universe will probably be done\n\n43:33.260 --> 43:38.140\n by AGI's that are better designed to live in space\n\n43:38.140 --> 43:41.840\n than by the meat machines that we are.\n\n43:41.840 --> 43:43.020\n But I mean, who knows?\n\n43:43.020 --> 43:45.860\n We may cryopreserve ourselves in some superior way\n\n43:45.860 --> 43:48.700\n to what we know now and like shoot ourselves out\n\n43:48.700 --> 43:50.720\n to Alpha Centauri and beyond.\n\n43:50.720 --> 43:52.660\n I mean, that's all cool.\n\n43:52.660 --> 43:55.140\n It's very interesting and it's much more valuable\n\n43:55.140 --> 43:58.860\n than most things that humanity is spending its resources on.\n\n43:58.860 --> 44:03.540\n On the other hand, with AGI, we can get to a singularity\n\n44:03.540 --> 44:07.780\n before the Mars colony becomes sustaining for sure,\n\n44:07.780 --> 44:10.100\n possibly before it's even operational.\n\n44:10.100 --> 44:12.400\n So your intuition is that that's the problem\n\n44:12.400 --> 44:14.940\n if we really invest resources and we can get to faster\n\n44:14.940 --> 44:19.700\n than a legitimate full self sustaining colonization of Mars.\n\n44:19.700 --> 44:23.160\n Yeah, and it's very clear that we will to me\n\n44:23.160 --> 44:26.020\n because there's so much economic value\n\n44:26.020 --> 44:29.460\n in getting from narrow AI toward AGI,\n\n44:29.460 --> 44:33.380\n whereas the Mars colony, there's less economic value\n\n44:33.380 --> 44:37.380\n until you get quite far out into the future.\n\n44:37.380 --> 44:40.260\n So I think that's very interesting.\n\n44:40.260 --> 44:44.380\n I just think it's somewhat off to the side.\n\n44:44.380 --> 44:48.020\n I mean, just as I think, say, art and music\n\n44:48.020 --> 44:51.860\n are very, very interesting and I wanna see resources\n\n44:51.860 --> 44:55.460\n go into amazing art and music being created.\n\n44:55.460 --> 44:59.580\n And I'd rather see that than a lot of the garbage\n\n44:59.580 --> 45:01.760\n that the society spends their money on.\n\n45:01.760 --> 45:04.620\n On the other hand, I don't think Mars colonization\n\n45:04.620 --> 45:07.780\n or inventing amazing new genres of music\n\n45:07.780 --> 45:11.000\n is not one of the things that is most likely\n\n45:11.000 --> 45:13.900\n to make a critical difference in the evolution\n\n45:13.900 --> 45:18.340\n of human or nonhuman life in this part of the universe\n\n45:18.340 --> 45:19.820\n over the next decade.\n\n45:19.820 --> 45:21.620\n Do you think AGI is really?\n\n45:21.620 --> 45:25.820\n AGI is by far the most important thing\n\n45:25.820 --> 45:27.500\n that's on the horizon.\n\n45:27.500 --> 45:31.620\n And then technologies that have direct ability\n\n45:31.620 --> 45:36.620\n to enable AGI or to accelerate AGI are also very important.\n\n45:37.260 --> 45:40.540\n For example, say, quantum computing.\n\n45:40.540 --> 45:42.740\n I don't think that's critical to achieve AGI,\n\n45:42.740 --> 45:44.360\n but certainly you could see how\n\n45:44.360 --> 45:46.700\n the right quantum computing architecture\n\n45:46.700 --> 45:49.280\n could massively accelerate AGI,\n\n45:49.280 --> 45:52.260\n similar other types of nanotechnology.\n\n45:52.260 --> 45:57.260\n Right now, the quest to cure aging and end disease\n\n45:57.860 --> 46:02.100\n while not in the big picture as important as AGI,\n\n46:02.100 --> 46:07.100\n of course, it's important to all of us as individual humans.\n\n46:07.380 --> 46:11.600\n And if someone made a super longevity pill\n\n46:11.600 --> 46:14.260\n and distributed it tomorrow, I mean,\n\n46:14.260 --> 46:17.220\n that would be huge and a much larger impact\n\n46:17.220 --> 46:20.460\n than a Mars colony is gonna have for quite some time.\n\n46:20.460 --> 46:23.300\n But perhaps not as much as an AGI system.\n\n46:23.300 --> 46:27.060\n No, because if you can make a benevolent AGI,\n\n46:27.060 --> 46:28.700\n then all the other problems are solved.\n\n46:28.700 --> 46:31.940\n I mean, if then the AGI can be,\n\n46:31.940 --> 46:34.260\n once it's as generally intelligent as humans,\n\n46:34.260 --> 46:37.420\n it can rapidly become massively more generally intelligent\n\n46:37.420 --> 46:38.620\n than humans.\n\n46:38.620 --> 46:42.540\n And then that AGI should be able to solve science\n\n46:42.540 --> 46:46.840\n and engineering problems much better than human beings,\n\n46:46.840 --> 46:49.700\n as long as it is in fact motivated to do so.\n\n46:49.700 --> 46:52.740\n That's why I said a benevolent AGI.\n\n46:52.740 --> 46:54.020\n There could be other kinds.\n\n46:54.020 --> 46:56.020\n Maybe it's good to step back a little bit.\n\n46:56.020 --> 46:57.980\n I mean, we've been using the term AGI.\n\n46:58.860 --> 47:00.860\n People often cite you as the creator,\n\n47:00.860 --> 47:03.060\n or at least the popularizer of the term AGI,\n\n47:03.060 --> 47:05.700\n artificial general intelligence.\n\n47:05.700 --> 47:09.100\n Can you tell the origin story of the term maybe?\n\n47:09.100 --> 47:14.100\n So yeah, I would say I launched the term AGI upon the world\n\n47:14.860 --> 47:19.860\n for what it's worth without ever fully being in love\n\n47:19.940 --> 47:21.660\n with the term.\n\n47:21.660 --> 47:25.380\n What happened is I was editing a book,\n\n47:25.380 --> 47:27.860\n and this process started around 2001 or two.\n\n47:27.860 --> 47:30.500\n I think the book came out 2005, finally.\n\n47:30.500 --> 47:33.140\n I was editing a book which I provisionally\n\n47:33.140 --> 47:35.860\n was titling Real AI.\n\n47:35.860 --> 47:38.840\n And I mean, the goal was to gather together\n\n47:38.840 --> 47:41.700\n fairly serious academicish papers\n\n47:41.700 --> 47:43.940\n on the topic of making thinking machines\n\n47:43.940 --> 47:46.780\n that could really think in the sense like people can,\n\n47:46.780 --> 47:49.240\n or even more broadly than people can, right?\n\n47:49.240 --> 47:52.740\n So then I was reaching out to other folks\n\n47:52.740 --> 47:54.060\n that I had encountered here or there\n\n47:54.060 --> 47:57.380\n who were interested in that,\n\n47:57.380 --> 48:01.700\n which included some other folks who I knew\n\n48:01.700 --> 48:04.340\n from the transhumist and singularitarian world,\n\n48:04.340 --> 48:07.660\n like Peter Vos, who has a company, AGI Incorporated,\n\n48:07.660 --> 48:12.660\n still in California, and included Shane Legge,\n\n48:13.100 --> 48:15.700\n who had worked for me at my company, WebMind,\n\n48:15.700 --> 48:17.580\n in New York in the late 90s,\n\n48:17.580 --> 48:20.500\n who by now has become rich and famous.\n\n48:20.500 --> 48:22.780\n He was one of the cofounders of Google DeepMind.\n\n48:22.780 --> 48:25.320\n But at that time, Shane was,\n\n48:25.320 --> 48:30.320\n I think he may have just started doing his PhD\n\n48:31.800 --> 48:35.900\n with Marcus Hooter, who at that time\n\n48:35.900 --> 48:38.680\n hadn't yet published his book, Universal AI,\n\n48:38.680 --> 48:41.040\n which sort of gives a mathematical foundation\n\n48:41.040 --> 48:43.400\n for artificial general intelligence.\n\n48:43.400 --> 48:46.140\n So I reached out to Shane and Marcus and Peter Vos\n\n48:46.140 --> 48:49.480\n and Pei Wang, who was another former employee of mine\n\n48:49.480 --> 48:51.880\n who had been Douglas Hofstadter's PhD student\n\n48:51.880 --> 48:53.280\n who had his own approach to AGI,\n\n48:53.280 --> 48:58.040\n and a bunch of some Russian folks reached out to these guys\n\n48:58.040 --> 49:01.360\n and they contributed papers for the book.\n\n49:01.360 --> 49:04.440\n But that was my provisional title, but I never loved it\n\n49:04.440 --> 49:09.320\n because in the end, I was doing some,\n\n49:09.320 --> 49:12.120\n what we would now call narrow AI as well,\n\n49:12.120 --> 49:14.640\n like applying machine learning to genomics data\n\n49:14.640 --> 49:17.920\n or chat data for sentiment analysis.\n\n49:17.920 --> 49:19.240\n I mean, that work is real.\n\n49:19.240 --> 49:22.760\n And in a sense, it's really AI.\n\n49:22.760 --> 49:26.000\n It's just a different kind of AI.\n\n49:26.000 --> 49:31.000\n Ray Kurzweil wrote about narrow AI versus strong AI,\n\n49:31.160 --> 49:35.040\n but that seemed weird to me because first of all,\n\n49:35.040 --> 49:36.680\n narrow and strong are not antennas.\n\n49:36.680 --> 49:38.720\n That's right.\n\n49:38.720 --> 49:41.940\n But secondly, strong AI was used\n\n49:41.940 --> 49:43.360\n in the cognitive science literature\n\n49:43.360 --> 49:46.640\n to mean the hypothesis that digital computer AIs\n\n49:46.640 --> 49:50.140\n could have true consciousness like human beings.\n\n49:50.140 --> 49:52.540\n So there was already a meaning to strong AI,\n\n49:52.540 --> 49:56.440\n which was complexly different, but related, right?\n\n49:56.440 --> 50:00.520\n So we were tossing around on an email list\n\n50:00.520 --> 50:03.200\n whether what title it should be.\n\n50:03.200 --> 50:07.560\n And so we talked about narrow AI, broad AI, wide AI,\n\n50:07.560 --> 50:09.760\n narrow AI, general AI.\n\n50:09.760 --> 50:14.760\n And I think it was either Shane Legge or Peter Vos\n\n50:15.880 --> 50:18.120\n on the private email discussion we had.\n\n50:18.120 --> 50:18.960\n He said, but why don't we go\n\n50:18.960 --> 50:21.800\n with AGI, artificial general intelligence?\n\n50:21.800 --> 50:24.280\n And Pei Wang wanted to do GAI,\n\n50:24.280 --> 50:25.760\n general artificial intelligence,\n\n50:25.760 --> 50:27.880\n because in Chinese it goes in that order.\n\n50:27.880 --> 50:30.200\n But we figured gay wouldn't work\n\n50:30.200 --> 50:33.240\n in US culture at that time, right?\n\n50:33.240 --> 50:37.360\n So we went with the AGI.\n\n50:37.360 --> 50:39.520\n We used it for the title of that book.\n\n50:39.520 --> 50:43.460\n And part of Peter and Shane's reasoning\n\n50:43.460 --> 50:45.460\n was you have the G factor in psychology,\n\n50:45.460 --> 50:47.480\n which is IQ, general intelligence, right?\n\n50:47.480 --> 50:51.160\n So you have a meaning of GI, general intelligence,\n\n50:51.160 --> 50:55.360\n in psychology, so then you're looking like artificial GI.\n\n50:55.360 --> 51:00.360\n So then we use that for the title of the book.\n\n51:00.400 --> 51:04.040\n And so I think maybe both Shane and Peter\n\n51:04.040 --> 51:05.200\n think they invented the term,\n\n51:05.200 --> 51:08.320\n but then later after the book was published,\n\n51:08.320 --> 51:11.640\n this guy, Mark Guberd, came up to me and he's like,\n\n51:11.640 --> 51:14.800\n well, I published an essay with the term AGI\n\n51:14.800 --> 51:17.120\n in like 1997 or something.\n\n51:17.120 --> 51:20.520\n And so I'm just waiting for some Russian to come out\n\n51:20.520 --> 51:23.400\n and say they published that in 1953, right?\n\n51:23.400 --> 51:27.800\n I mean, that term is not dramatically innovative\n\n51:27.800 --> 51:28.640\n or anything.\n\n51:28.640 --> 51:31.560\n It's one of these obvious in hindsight things,\n\n51:31.560 --> 51:34.880\n which is also annoying in a way,\n\n51:34.880 --> 51:39.500\n because Joshua Bach, who you interviewed,\n\n51:39.500 --> 51:40.400\n is a close friend of mine.\n\n51:40.400 --> 51:43.240\n He likes the term synthetic intelligence,\n\n51:43.240 --> 51:44.300\n which I like much better,\n\n51:44.300 --> 51:47.080\n but it hasn't actually caught on, right?\n\n51:47.080 --> 51:51.800\n Because I mean, artificial is a bit off to me\n\n51:51.800 --> 51:54.640\n because artifice is like a tool or something,\n\n51:54.640 --> 51:57.760\n but not all AGI's are gonna be tools.\n\n51:57.760 --> 51:58.700\n I mean, they may be now,\n\n51:58.700 --> 52:00.600\n but we're aiming toward making them agents\n\n52:00.600 --> 52:02.800\n rather than tools.\n\n52:02.800 --> 52:04.840\n And in a way, I don't like the distinction\n\n52:04.840 --> 52:07.200\n between artificial and natural,\n\n52:07.200 --> 52:09.360\n because I mean, we're part of nature also\n\n52:09.360 --> 52:12.160\n and machines are part of nature.\n\n52:12.160 --> 52:14.840\n I mean, you can look at evolved versus engineered,\n\n52:14.840 --> 52:17.160\n but that's a different distinction.\n\n52:17.160 --> 52:20.000\n Then it should be engineered general intelligence, right?\n\n52:20.000 --> 52:21.920\n And then general, well,\n\n52:21.920 --> 52:24.600\n if you look at Marcus Hooter's book,\n\n52:24.600 --> 52:26.860\n universally, what he argues there is,\n\n52:28.240 --> 52:30.520\n within the domain of computation theory,\n\n52:30.520 --> 52:31.920\n which is limited, but interesting.\n\n52:31.920 --> 52:33.680\n So if you assume computable environments\n\n52:33.680 --> 52:35.600\n or computable reward functions,\n\n52:35.600 --> 52:37.560\n then he articulates what would be\n\n52:37.560 --> 52:40.040\n a truly general intelligence,\n\n52:40.040 --> 52:43.160\n a system called AIXI, which is quite beautiful.\n\n52:43.160 --> 52:46.280\n AIXI, and that's the middle name\n\n52:46.280 --> 52:49.360\n of my latest child, actually, is it?\n\n52:49.360 --> 52:50.200\n What's the first name?\n\n52:50.200 --> 52:52.400\n First name is QORXI, Q O R X I,\n\n52:52.400 --> 52:53.780\n which my wife came up with,\n\n52:53.780 --> 52:57.320\n but that's an acronym for quantum organized rational\n\n52:57.320 --> 53:02.320\n expanding intelligence, and his middle name is Xiphonies,\n\n53:03.120 --> 53:08.120\n actually, which means the former principal underlying AIXI.\n\n53:08.340 --> 53:09.480\n But in any case.\n\n53:09.480 --> 53:12.160\n You're giving Elon Musk's new child a run for his money.\n\n53:12.160 --> 53:13.800\n Well, I did it first.\n\n53:13.800 --> 53:17.320\n He copied me with this new freakish name,\n\n53:17.320 --> 53:18.600\n but now if I have another baby,\n\n53:18.600 --> 53:20.600\n I'm gonna have to outdo him.\n\n53:20.600 --> 53:24.560\n It's becoming an arms race of weird, geeky baby names.\n\n53:24.560 --> 53:26.840\n We'll see what the babies think about it, right?\n\n53:26.840 --> 53:30.220\n But I mean, my oldest son, Zarathustra, loves his name,\n\n53:30.220 --> 53:33.800\n and my daughter, Sharazad, loves her name.\n\n53:33.800 --> 53:36.960\n So far, basically, if you give your kids weird names.\n\n53:36.960 --> 53:37.840\n They live up to it.\n\n53:37.840 --> 53:39.800\n Well, you're obliged to make the kids weird enough\n\n53:39.800 --> 53:42.000\n that they like the names, right?\n\n53:42.000 --> 53:43.920\n It directs their upbringing in a certain way.\n\n53:43.920 --> 53:47.680\n But yeah, anyway, I mean, what Marcus showed in that book\n\n53:47.680 --> 53:50.560\n is that a truly general intelligence\n\n53:50.560 --> 53:51.800\n theoretically is possible,\n\n53:51.800 --> 53:53.840\n but would take infinite computing power.\n\n53:53.840 --> 53:56.360\n So then the artificial is a little off.\n\n53:56.360 --> 53:59.800\n The general is not really achievable within physics\n\n53:59.800 --> 54:01.280\n as we know it.\n\n54:01.280 --> 54:03.520\n And I mean, physics as we know it may be limited,\n\n54:03.520 --> 54:05.300\n but that's what we have to work with now.\n\n54:05.300 --> 54:06.140\n Intelligence.\n\n54:06.140 --> 54:07.360\n Infinitely general, you mean,\n\n54:07.360 --> 54:10.440\n like information processing perspective, yeah.\n\n54:10.440 --> 54:14.760\n Yeah, intelligence is not very well defined either, right?\n\n54:14.760 --> 54:16.760\n I mean, what does it mean?\n\n54:16.760 --> 54:19.560\n I mean, in AI now, it's fashionable to look at it\n\n54:19.560 --> 54:23.320\n as maximizing an expected reward over the future.\n\n54:23.320 --> 54:27.800\n But that sort of definition is pathological in various ways.\n\n54:27.800 --> 54:31.320\n And my friend David Weinbaum, AKA Weaver,\n\n54:31.320 --> 54:34.840\n he had a beautiful PhD thesis on open ended intelligence,\n\n54:34.840 --> 54:36.880\n trying to conceive intelligence in a...\n\n54:36.880 --> 54:38.240\n Without a reward.\n\n54:38.240 --> 54:40.120\n Yeah, he's just looking at it differently.\n\n54:40.120 --> 54:42.680\n He's looking at complex self organizing systems\n\n54:42.680 --> 54:44.640\n and looking at an intelligent system\n\n54:44.640 --> 54:47.600\n as being one that revises and grows\n\n54:47.600 --> 54:51.740\n and improves itself in conjunction with its environment\n\n54:51.740 --> 54:54.880\n without necessarily there being one objective function\n\n54:54.880 --> 54:56.080\n it's trying to maximize.\n\n54:56.080 --> 54:58.520\n Although over certain intervals of time,\n\n54:58.520 --> 54:59.960\n it may act as if it's optimizing\n\n54:59.960 --> 55:01.360\n a certain objective function.\n\n55:01.360 --> 55:04.580\n Very much Solaris from Stanislav Lem's novels, right?\n\n55:04.580 --> 55:07.880\n So yeah, the point is artificial, general and intelligence.\n\n55:07.880 --> 55:08.720\n Don't work.\n\n55:08.720 --> 55:09.540\n They're all bad.\n\n55:09.540 --> 55:12.040\n On the other hand, everyone knows what AI is.\n\n55:12.040 --> 55:15.880\n And AGI seems immediately comprehensible\n\n55:15.880 --> 55:17.520\n to people with a technical background.\n\n55:17.520 --> 55:19.360\n So I think that the term has served\n\n55:19.360 --> 55:20.720\n as sociological function.\n\n55:20.720 --> 55:24.720\n And now it's out there everywhere, which baffles me.\n\n55:24.720 --> 55:25.800\n It's like KFC.\n\n55:25.800 --> 55:27.080\n I mean, that's it.\n\n55:27.080 --> 55:30.200\n We're stuck with AGI probably for a very long time\n\n55:30.200 --> 55:33.640\n until AGI systems take over and rename themselves.\n\n55:33.640 --> 55:34.480\n Yeah.\n\n55:34.480 --> 55:36.160\n And then we'll be biological.\n\n55:36.160 --> 55:37.560\n We're stuck with GPUs too,\n\n55:37.560 --> 55:39.320\n which mostly have nothing to do with graphics.\n\n55:39.320 --> 55:40.520\n Any more, right?\n\n55:40.520 --> 55:43.260\n I wonder what the AGI system will call us humans.\n\n55:43.260 --> 55:44.280\n That was maybe.\n\n55:44.280 --> 55:45.120\n Grandpa.\n\n55:45.120 --> 55:45.960\n Yeah.\n\n55:45.960 --> 55:46.800\n Yeah.\n\n55:46.800 --> 55:47.620\n GPs.\n\n55:47.620 --> 55:48.460\n Yeah.\n\n55:48.460 --> 55:50.320\n Grandpa processing unit, yeah.\n\n55:50.320 --> 55:52.120\n Biological grandpa processing units.\n\n55:52.120 --> 55:52.960\n Yeah.\n\n55:54.280 --> 55:59.280\n Okay, so maybe also just a comment on AGI representing\n\n56:00.580 --> 56:02.160\n before even the term existed,\n\n56:02.160 --> 56:04.640\n representing a kind of community.\n\n56:04.640 --> 56:06.240\n You've talked about this in the past,\n\n56:06.240 --> 56:08.340\n sort of AI is coming in waves,\n\n56:08.340 --> 56:10.440\n but there's always been this community of people\n\n56:10.440 --> 56:15.160\n who dream about creating general human level\n\n56:15.160 --> 56:16.840\n super intelligence systems.\n\n56:19.000 --> 56:21.880\n Can you maybe give your sense of the history\n\n56:21.880 --> 56:24.280\n of this community as it exists today,\n\n56:24.280 --> 56:26.720\n as it existed before this deep learning revolution\n\n56:26.720 --> 56:29.520\n all throughout the winters and the summers of AI?\n\n56:29.520 --> 56:30.340\n Sure.\n\n56:30.340 --> 56:33.500\n First, I would say as a side point,\n\n56:33.500 --> 56:37.840\n the winters and summers of AI are greatly exaggerated\n\n56:37.840 --> 56:40.960\n by Americans and in that,\n\n56:40.960 --> 56:43.600\n if you look at the publication record\n\n56:43.600 --> 56:46.400\n of the artificial intelligence community\n\n56:46.400 --> 56:48.480\n since say the 1950s,\n\n56:48.480 --> 56:51.360\n you would find a pretty steady growth\n\n56:51.360 --> 56:53.980\n in advance of ideas and papers.\n\n56:53.980 --> 56:57.720\n And what's thought of as an AI winter or summer\n\n56:57.720 --> 57:00.480\n was sort of how much money is the US military\n\n57:00.480 --> 57:04.640\n pumping into AI, which was meaningful.\n\n57:04.640 --> 57:06.960\n On the other hand, there was AI going on in Germany,\n\n57:06.960 --> 57:10.960\n UK and in Japan and in Russia, all over the place,\n\n57:10.960 --> 57:15.960\n while US military got more and less enthused about AI.\n\n57:16.300 --> 57:17.560\n So, I mean.\n\n57:17.560 --> 57:20.200\n That happened to be, just for people who don't know,\n\n57:20.200 --> 57:22.840\n the US military happened to be the main source\n\n57:22.840 --> 57:24.500\n of funding for AI research.\n\n57:24.500 --> 57:27.480\n So another way to phrase that is it's up and down\n\n57:27.480 --> 57:31.080\n of funding for artificial intelligence research.\n\n57:31.080 --> 57:34.600\n And I would say the correlation between funding\n\n57:34.600 --> 57:38.120\n and intellectual advance was not 100%, right?\n\n57:38.120 --> 57:42.120\n Because I mean, in Russia, as an example, or in Germany,\n\n57:42.120 --> 57:44.840\n there was less dollar funding than in the US,\n\n57:44.840 --> 57:48.160\n but many foundational ideas were laid out,\n\n57:48.160 --> 57:50.880\n but it was more theory than implementation, right?\n\n57:50.880 --> 57:54.600\n And US really excelled at sort of breaking through\n\n57:54.600 --> 57:59.600\n from theoretical papers to working implementations,\n\n58:00.200 --> 58:03.020\n which did go up and down somewhat\n\n58:03.020 --> 58:04.320\n with US military funding,\n\n58:04.320 --> 58:07.440\n but still, I mean, you can look in the 1980s,\n\n58:07.440 --> 58:10.400\n Dietrich Derner in Germany had self driving cars\n\n58:10.400 --> 58:11.440\n on the Autobahn, right?\n\n58:11.440 --> 58:15.600\n And I mean, it was a little early\n\n58:15.600 --> 58:16.920\n with regard to the car industry,\n\n58:16.920 --> 58:20.200\n so it didn't catch on such as has happened now.\n\n58:20.200 --> 58:22.960\n But I mean, that whole advancement\n\n58:22.960 --> 58:25.900\n of self driving car technology in Germany\n\n58:25.900 --> 58:29.720\n was pretty much independent of AI military summers\n\n58:29.720 --> 58:31.040\n and winters in the US.\n\n58:31.040 --> 58:34.480\n So there's been more going on in AI globally\n\n58:34.480 --> 58:37.120\n than not only most people on the planet realize,\n\n58:37.120 --> 58:40.080\n but then most new AI PhDs realize\n\n58:40.080 --> 58:44.600\n because they've come up within a certain sub field of AI\n\n58:44.600 --> 58:47.680\n and haven't had to look so much beyond that.\n\n58:47.680 --> 58:52.680\n But I would say when I got my PhD in 1989 in mathematics,\n\n58:54.300 --> 58:56.000\n I was interested in AI already.\n\n58:56.000 --> 58:56.840\n In Philadelphia.\n\n58:56.840 --> 59:00.920\n Yeah, I started at NYU, then I transferred to Philadelphia\n\n59:00.920 --> 59:03.960\n to Temple University, good old North Philly.\n\n59:03.960 --> 59:04.800\n North Philly.\n\n59:04.800 --> 59:07.920\n Yeah, yeah, yeah, the pearl of the US.\n\n59:09.280 --> 59:10.920\n You never stopped at a red light then\n\n59:10.920 --> 59:12.760\n because you were afraid if you stopped at a red light,\n\n59:12.760 --> 59:13.760\n someone will carjack you.\n\n59:13.760 --> 59:15.960\n So you just drive through every red light.\n\n59:15.960 --> 59:16.800\n Yeah.\n\n59:18.200 --> 59:20.940\n Every day driving or bicycling to Temple from my house\n\n59:20.940 --> 59:24.280\n was like a new adventure.\n\n59:24.280 --> 59:27.520\n But yeah, the reason I didn't do a PhD in AI\n\n59:27.520 --> 59:30.860\n was what people were doing in the academic AI field then,\n\n59:30.860 --> 59:34.880\n was just astoundingly boring and seemed wrong headed to me.\n\n59:34.880 --> 59:38.060\n It was really like rule based expert systems\n\n59:38.060 --> 59:39.360\n and production systems.\n\n59:39.360 --> 59:42.080\n And actually I loved mathematical logic.\n\n59:42.080 --> 59:45.840\n I had nothing against logic as the cognitive engine for an AI,\n\n59:45.840 --> 59:48.920\n but the idea that you could type in the knowledge\n\n59:48.920 --> 59:52.720\n that AI would need to think seemed just completely stupid\n\n59:52.720 --> 59:55.380\n and wrong headed to me.\n\n59:55.380 --> 59:57.400\n I mean, you can use logic if you want,\n\n59:57.400 --> 1:00:00.160\n but somehow the system has got to be...\n\n1:00:00.160 --> 1:00:01.000\n Automated.\n\n1:00:01.000 --> 1:00:01.840\n Learning, right?\n\n1:00:01.840 --> 1:00:03.800\n It should be learning from experience.\n\n1:00:03.800 --> 1:00:06.120\n And the AI field then was not interested\n\n1:00:06.120 --> 1:00:08.320\n in learning from experience.\n\n1:00:08.320 --> 1:00:11.020\n I mean, some researchers certainly were.\n\n1:00:11.020 --> 1:00:13.960\n I mean, I remember in mid eighties,\n\n1:00:13.960 --> 1:00:17.160\n I discovered a book by John Andreas,\n\n1:00:17.160 --> 1:00:21.920\n which was, it was about a reinforcement learning system\n\n1:00:21.920 --> 1:00:26.920\n called PURRDASHPUSS, which was an acronym\n\n1:00:27.080 --> 1:00:28.640\n that I can't even remember what it was for,\n\n1:00:28.640 --> 1:00:30.400\n but purpose anyway.\n\n1:00:30.400 --> 1:00:32.000\n But he, I mean, that was a system\n\n1:00:32.000 --> 1:00:34.360\n that was supposed to be an AGI\n\n1:00:34.360 --> 1:00:38.120\n and basically by some sort of fancy\n\n1:00:38.120 --> 1:00:41.000\n like Markov decision process learning,\n\n1:00:41.000 --> 1:00:43.440\n it was supposed to learn everything\n\n1:00:43.440 --> 1:00:44.880\n just from the bits coming into it\n\n1:00:44.880 --> 1:00:46.720\n and learn to maximize its reward\n\n1:00:46.720 --> 1:00:49.080\n and become intelligent, right?\n\n1:00:49.080 --> 1:00:51.800\n So that was there in academia back then,\n\n1:00:51.800 --> 1:00:55.240\n but it was like isolated, scattered, weird people.\n\n1:00:55.240 --> 1:00:57.440\n But all these isolated, scattered, weird people\n\n1:00:57.440 --> 1:01:01.280\n in that period, I mean, they laid the intellectual grounds\n\n1:01:01.280 --> 1:01:02.120\n for what happened later.\n\n1:01:02.120 --> 1:01:05.300\n So you look at John Andreas at University of Canterbury\n\n1:01:05.300 --> 1:01:09.720\n with his PURRDASHPUSS reinforcement learning Markov system.\n\n1:01:09.720 --> 1:01:14.080\n He was the PhD supervisor for John Cleary in New Zealand.\n\n1:01:14.080 --> 1:01:17.080\n Now, John Cleary worked with me\n\n1:01:17.080 --> 1:01:21.680\n when I was at Waikato University in 1993 in New Zealand.\n\n1:01:21.680 --> 1:01:23.900\n And he worked with Ian Whitten there\n\n1:01:23.900 --> 1:01:25.940\n and they launched WEKA,\n\n1:01:25.940 --> 1:01:29.840\n which was the first open source machine learning toolkit,\n\n1:01:29.840 --> 1:01:33.520\n which was launched in, I guess, 93 or 94\n\n1:01:33.520 --> 1:01:35.160\n when I was at Waikato University.\n\n1:01:35.160 --> 1:01:36.480\n Written in Java, unfortunately.\n\n1:01:36.480 --> 1:01:39.620\n Written in Java, which was a cool language back then.\n\n1:01:39.620 --> 1:01:41.720\n I guess it's still, well, it's not cool anymore,\n\n1:01:41.720 --> 1:01:43.280\n but it's powerful.\n\n1:01:43.280 --> 1:01:45.760\n I find, like most programmers now,\n\n1:01:45.760 --> 1:01:48.820\n I find Java unnecessarily bloated,\n\n1:01:48.820 --> 1:01:52.020\n but back then it was like Java or C++ basically.\n\n1:01:52.020 --> 1:01:55.760\n And Java was easier for students.\n\n1:01:55.760 --> 1:01:57.760\n Amusingly, a lot of the work on WEKA\n\n1:01:57.760 --> 1:02:01.200\n when we were in New Zealand was funded by a US,\n\n1:02:01.200 --> 1:02:03.880\n sorry, a New Zealand government grant\n\n1:02:03.880 --> 1:02:05.440\n to use machine learning\n\n1:02:05.440 --> 1:02:08.240\n to predict the menstrual cycles of cows.\n\n1:02:08.240 --> 1:02:10.440\n So in the US, all the grant funding for AI\n\n1:02:10.440 --> 1:02:13.600\n was about how to kill people or spy on people.\n\n1:02:13.600 --> 1:02:16.400\n In New Zealand, it's all about cows or kiwi fruits, right?\n\n1:02:16.400 --> 1:02:17.560\n Yeah.\n\n1:02:17.560 --> 1:02:20.560\n So yeah, anyway, I mean, John Andreas\n\n1:02:20.560 --> 1:02:24.320\n had his probability theory based reinforcement learning,\n\n1:02:24.320 --> 1:02:25.780\n proto AGI.\n\n1:02:25.780 --> 1:02:29.400\n John Cleary was trying to do much more ambitious,\n\n1:02:29.400 --> 1:02:31.820\n probabilistic AGI systems.\n\n1:02:31.820 --> 1:02:36.160\n Now, John Cleary helped do WEKA,\n\n1:02:36.160 --> 1:02:39.360\n which is the first open source machine learning toolkit.\n\n1:02:39.360 --> 1:02:41.520\n So the predecessor for TensorFlow and Torch\n\n1:02:41.520 --> 1:02:43.040\n and all these things.\n\n1:02:43.040 --> 1:02:46.800\n Also, Shane Legg was at Waikato\n\n1:02:46.800 --> 1:02:50.240\n working with John Cleary and Ian Witten\n\n1:02:50.240 --> 1:02:51.500\n and this whole group.\n\n1:02:51.500 --> 1:02:55.800\n And then working with my own companies,\n\n1:02:55.800 --> 1:02:59.840\n my company, WebMind, an AI company I had in the late 90s\n\n1:02:59.840 --> 1:03:02.320\n with a team there at Waikato University,\n\n1:03:02.320 --> 1:03:05.360\n which is how Shane got his head full of AGI,\n\n1:03:05.360 --> 1:03:06.440\n which led him to go on\n\n1:03:06.440 --> 1:03:08.660\n and with Demis Hassabis found DeepMind.\n\n1:03:08.660 --> 1:03:11.060\n So what you can see through that lineage is,\n\n1:03:11.060 --> 1:03:12.580\n you know, in the 80s and 70s,\n\n1:03:12.580 --> 1:03:14.800\n John Andreas was trying to build probabilistic\n\n1:03:14.800 --> 1:03:17.200\n reinforcement learning AGI systems.\n\n1:03:17.200 --> 1:03:19.680\n The technology, the computers just weren't there to support\n\n1:03:19.680 --> 1:03:23.920\n his ideas were very similar to what people are doing now.\n\n1:03:23.920 --> 1:03:27.720\n But, you know, although he's long since passed away\n\n1:03:27.720 --> 1:03:30.940\n and didn't become that famous outside of Canterbury,\n\n1:03:30.940 --> 1:03:33.720\n I mean, the lineage of ideas passed on from him\n\n1:03:33.720 --> 1:03:35.140\n to his students, to their students,\n\n1:03:35.140 --> 1:03:37.920\n you can go trace directly from there to me\n\n1:03:37.920 --> 1:03:39.480\n and to DeepMind, right?\n\n1:03:39.480 --> 1:03:42.180\n So that there was a lot going on in AGI\n\n1:03:42.180 --> 1:03:46.460\n that did ultimately lay the groundwork\n\n1:03:46.460 --> 1:03:48.560\n for what we have today, but there wasn't a community, right?\n\n1:03:48.560 --> 1:03:53.520\n And so when I started trying to pull together\n\n1:03:53.520 --> 1:03:56.920\n an AGI community, it was in the, I guess,\n\n1:03:56.920 --> 1:04:00.400\n the early aughts when I was living in Washington, D.C.\n\n1:04:00.400 --> 1:04:03.440\n and making a living doing AI consulting\n\n1:04:03.440 --> 1:04:07.080\n for various U.S. government agencies.\n\n1:04:07.080 --> 1:04:12.080\n And I organized the first AGI workshop in 2006.\n\n1:04:13.200 --> 1:04:15.780\n And I mean, it wasn't like it was literally\n\n1:04:15.780 --> 1:04:17.000\n in my basement or something.\n\n1:04:17.000 --> 1:04:19.320\n I mean, it was in the conference room at the Marriott\n\n1:04:19.320 --> 1:04:23.200\n in Bethesda, it's not that edgy or underground,\n\n1:04:23.200 --> 1:04:25.000\n unfortunately, but still.\n\n1:04:25.000 --> 1:04:25.840\n How many people attended?\n\n1:04:25.840 --> 1:04:27.600\n About 60 or something.\n\n1:04:27.600 --> 1:04:28.480\n That's not bad.\n\n1:04:28.480 --> 1:04:30.780\n I mean, D.C. has a lot of AI going on,\n\n1:04:30.780 --> 1:04:34.200\n probably until the last five or 10 years,\n\n1:04:34.200 --> 1:04:37.800\n much more than Silicon Valley, although it's just quiet\n\n1:04:37.800 --> 1:04:41.280\n because of the nature of what happens in D.C.\n\n1:04:41.280 --> 1:04:43.600\n Their business isn't driven by PR.\n\n1:04:43.600 --> 1:04:46.140\n Mostly when something starts to work really well,\n\n1:04:46.140 --> 1:04:49.640\n it's taken black and becomes even more quiet, right?\n\n1:04:49.640 --> 1:04:52.880\n But yeah, the thing is that really had the feeling\n\n1:04:52.880 --> 1:04:57.880\n of a group of starry eyed mavericks huddled in a basement,\n\n1:04:58.400 --> 1:05:02.520\n like plotting how to overthrow the narrow AI establishment.\n\n1:05:02.520 --> 1:05:05.760\n And for the first time, in some cases,\n\n1:05:05.760 --> 1:05:08.680\n coming together with others who shared their passion\n\n1:05:08.680 --> 1:05:13.200\n for AGI and the technical seriousness about working on it.\n\n1:05:13.200 --> 1:05:18.200\n And that's very, very different than what we have today.\n\n1:05:19.160 --> 1:05:22.320\n I mean, now it's a little bit different.\n\n1:05:22.320 --> 1:05:24.640\n We have AGI conference every year\n\n1:05:24.640 --> 1:05:27.800\n and there's several hundred people rather than 50.\n\n1:05:29.300 --> 1:05:32.760\n Now it's more like this is the main gathering\n\n1:05:32.760 --> 1:05:35.020\n of people who want to achieve AGI\n\n1:05:35.020 --> 1:05:39.220\n and think that large scale nonlinear regression\n\n1:05:39.220 --> 1:05:42.480\n is not the golden path to AGI.\n\n1:05:42.480 --> 1:05:43.320\n So I mean it's...\n\n1:05:43.320 --> 1:05:44.160\n AKA neural networks.\n\n1:05:44.160 --> 1:05:44.980\n Yeah, yeah, yeah.\n\n1:05:44.980 --> 1:05:49.980\n Well, certain architectures for learning using neural networks.\n\n1:05:51.840 --> 1:05:54.440\n So yeah, the AGI conferences are sort of now\n\n1:05:54.440 --> 1:05:57.960\n the main concentration of people not obsessed\n\n1:05:57.960 --> 1:06:00.880\n with deep neural nets and deep reinforcement learning,\n\n1:06:00.880 --> 1:06:05.880\n but still interested in AGI, not the only ones.\n\n1:06:06.460 --> 1:06:10.200\n I mean, there's other little conferences and groupings\n\n1:06:10.200 --> 1:06:13.280\n interested in human level AI\n\n1:06:13.280 --> 1:06:16.040\n and cognitive architectures and so forth.\n\n1:06:16.040 --> 1:06:17.880\n But yeah, it's been a big shift.\n\n1:06:17.880 --> 1:06:21.960\n Like back then, you couldn't really...\n\n1:06:21.960 --> 1:06:23.540\n It'll be very, very edgy then\n\n1:06:23.540 --> 1:06:26.220\n to give a university department seminar\n\n1:06:26.220 --> 1:06:28.440\n that mentioned AGI or human level AI.\n\n1:06:28.440 --> 1:06:30.640\n It was more like you had to talk about\n\n1:06:30.640 --> 1:06:34.360\n something more short term and immediately practical\n\n1:06:34.360 --> 1:06:36.600\n than in the bar after the seminar,\n\n1:06:36.600 --> 1:06:39.540\n you could bullshit about AGI in the same breath\n\n1:06:39.540 --> 1:06:44.200\n as time travel or the simulation hypothesis or something.\n\n1:06:44.200 --> 1:06:48.360\n Whereas now, AGI is not only in the academic seminar room,\n\n1:06:48.360 --> 1:06:51.960\n like you have Vladimir Putin knows what AGI is.\n\n1:06:51.960 --> 1:06:55.480\n And he's like, Russia needs to become the leader in AGI.\n\n1:06:55.480 --> 1:07:00.480\n So national leaders and CEOs of large corporations.\n\n1:07:01.080 --> 1:07:04.240\n I mean, the CTO of Intel, Justin Ratner,\n\n1:07:04.240 --> 1:07:06.840\n this was years ago, Singularity Summit Conference,\n\n1:07:06.840 --> 1:07:07.780\n 2008 or something.\n\n1:07:07.780 --> 1:07:10.080\n He's like, we believe Ray Kurzweil,\n\n1:07:10.080 --> 1:07:12.000\n the singularity will happen in 2045\n\n1:07:12.000 --> 1:07:13.640\n and it will have Intel inside.\n\n1:07:13.640 --> 1:07:18.640\n So, I mean, it's gone from being something\n\n1:07:18.840 --> 1:07:21.700\n which is the pursuit of like crazed mavericks,\n\n1:07:21.700 --> 1:07:24.540\n crackpots and science fiction fanatics\n\n1:07:24.540 --> 1:07:29.540\n to being a marketing term for large corporations\n\n1:07:30.120 --> 1:07:31.480\n and the national leaders,\n\n1:07:31.480 --> 1:07:35.160\n which is a astounding transition.\n\n1:07:35.160 --> 1:07:40.160\n But yeah, in the course of this transition,\n\n1:07:40.160 --> 1:07:42.260\n I think a bunch of sub communities have formed\n\n1:07:42.260 --> 1:07:44.920\n and the community around the AGI conference series\n\n1:07:45.800 --> 1:07:47.640\n is certainly one of them.\n\n1:07:47.640 --> 1:07:51.940\n It hasn't grown as big as I might've liked it to.\n\n1:07:51.940 --> 1:07:56.320\n On the other hand, sometimes a modest size community\n\n1:07:56.320 --> 1:07:59.080\n can be better for making intellectual progress also.\n\n1:07:59.080 --> 1:08:02.160\n Like you go to a society for neuroscience conference,\n\n1:08:02.160 --> 1:08:05.400\n you have 35 or 40,000 neuroscientists.\n\n1:08:05.400 --> 1:08:07.480\n On the one hand, it's amazing.\n\n1:08:07.480 --> 1:08:10.920\n On the other hand, you're not gonna talk to the leaders\n\n1:08:10.920 --> 1:08:14.160\n of the field there if you're an outsider.\n\n1:08:14.160 --> 1:08:17.920\n Yeah, in the same sense, the AAAI,\n\n1:08:17.920 --> 1:08:19.280\n the artificial intelligence,\n\n1:08:20.160 --> 1:08:23.640\n the main kind of generic artificial intelligence\n\n1:08:23.640 --> 1:08:26.920\n conference is too big.\n\n1:08:26.920 --> 1:08:28.280\n It's too amorphous.\n\n1:08:28.280 --> 1:08:30.240\n Like it doesn't make sense.\n\n1:08:30.240 --> 1:08:35.240\n Well, yeah, and NIPS has become a company advertising outlet\n\n1:08:35.240 --> 1:08:37.000\n in the whole of it.\n\n1:08:37.000 --> 1:08:40.240\n So, I mean, to comment on the role of AGI\n\n1:08:40.240 --> 1:08:42.680\n in the research community, I'd still,\n\n1:08:42.680 --> 1:08:45.200\n if you look at NeurIPS, if you look at CVPR,\n\n1:08:45.200 --> 1:08:47.360\n if you look at these iClear,\n\n1:08:49.240 --> 1:08:51.860\n AGI is still seen as the outcast.\n\n1:08:51.860 --> 1:08:55.020\n I would say in these main machine learning,\n\n1:08:55.020 --> 1:08:59.040\n in these main artificial intelligence conferences\n\n1:08:59.040 --> 1:09:00.880\n amongst the researchers,\n\n1:09:00.880 --> 1:09:03.880\n I don't know if it's an accepted term yet.\n\n1:09:03.880 --> 1:09:08.280\n What I've seen bravely, you mentioned Shane Legg's\n\n1:09:08.280 --> 1:09:13.000\n DeepMind and then OpenAI are the two places that are,\n\n1:09:13.000 --> 1:09:15.580\n I would say unapologetically so far,\n\n1:09:15.580 --> 1:09:17.440\n I think it's actually changing unfortunately,\n\n1:09:17.440 --> 1:09:19.640\n but so far they've been pushing the idea\n\n1:09:19.640 --> 1:09:22.760\n that the goal is to create an AGI.\n\n1:09:22.760 --> 1:09:24.360\n Well, they have billions of dollars behind them.\n\n1:09:24.360 --> 1:09:27.220\n So, I mean, they're in the public mind\n\n1:09:27.220 --> 1:09:30.120\n that certainly carries some oomph, right?\n\n1:09:30.120 --> 1:09:30.960\n I mean, I mean.\n\n1:09:30.960 --> 1:09:33.160\n But they also have really strong researchers, right?\n\n1:09:33.160 --> 1:09:34.260\n They do, they're great teams.\n\n1:09:34.260 --> 1:09:36.660\n I mean, DeepMind in particular, yeah.\n\n1:09:36.660 --> 1:09:39.280\n And they have, I mean, DeepMind has Marcus Hutter\n\n1:09:39.280 --> 1:09:40.120\n walking around.\n\n1:09:40.120 --> 1:09:43.480\n I mean, there's all these folks who basically\n\n1:09:43.480 --> 1:09:46.400\n their full time position involves dreaming\n\n1:09:46.400 --> 1:09:47.800\n about creating AGI.\n\n1:09:47.800 --> 1:09:51.320\n I mean, Google Brain has a lot of amazing\n\n1:09:51.320 --> 1:09:53.240\n AGI oriented people also.\n\n1:09:53.240 --> 1:09:58.240\n And I mean, so I'd say from a public marketing view,\n\n1:09:59.840 --> 1:10:03.820\n DeepMind and OpenAI are the two large well funded\n\n1:10:03.820 --> 1:10:08.360\n organizations that have put the term and concept AGI\n\n1:10:08.360 --> 1:10:12.720\n out there sort of as part of their public image.\n\n1:10:12.720 --> 1:10:15.200\n But I mean, they're certainly not,\n\n1:10:15.200 --> 1:10:17.160\n there are other groups that are doing research\n\n1:10:17.160 --> 1:10:20.660\n that seems just as AGI is to me.\n\n1:10:20.660 --> 1:10:23.320\n I mean, including a bunch of groups in Google's\n\n1:10:23.320 --> 1:10:26.000\n main Mountain View office.\n\n1:10:26.000 --> 1:10:27.960\n So yeah, it's true.\n\n1:10:27.960 --> 1:10:32.960\n AGI is somewhat away from the mainstream now.\n\n1:10:33.880 --> 1:10:38.040\n But if you compare it to where it was 15 years ago,\n\n1:10:38.040 --> 1:10:41.960\n there's been an amazing mainstreaming.\n\n1:10:41.960 --> 1:10:45.520\n You could say the same thing about super longevity research,\n\n1:10:45.520 --> 1:10:49.120\n which is one of my application areas that I'm excited about.\n\n1:10:49.120 --> 1:10:52.880\n I mean, I've been talking about this since the 90s,\n\n1:10:52.880 --> 1:10:54.560\n but working on this since 2001.\n\n1:10:54.560 --> 1:10:57.280\n And back then, really to say,\n\n1:10:57.280 --> 1:10:59.440\n you're trying to create therapies to allow people\n\n1:10:59.440 --> 1:11:02.360\n to live hundreds of thousands of years,\n\n1:11:02.360 --> 1:11:05.520\n you were way, way, way, way out of the industry,\n\n1:11:05.520 --> 1:11:06.720\n academic mainstream.\n\n1:11:06.720 --> 1:11:11.540\n But now, Google had Project Calico,\n\n1:11:11.540 --> 1:11:14.080\n Craig Venter had Human Longevity Incorporated.\n\n1:11:14.080 --> 1:11:17.160\n And then once the suits come marching in, right?\n\n1:11:17.160 --> 1:11:20.200\n I mean, once there's big money in it,\n\n1:11:20.200 --> 1:11:22.720\n then people are forced to take it seriously\n\n1:11:22.720 --> 1:11:24.880\n because that's the way modern society works.\n\n1:11:24.880 --> 1:11:28.400\n So it's still not as mainstream as cancer research,\n\n1:11:28.400 --> 1:11:31.060\n just as AGI is not as mainstream\n\n1:11:31.060 --> 1:11:32.960\n as automated driving or something.\n\n1:11:32.960 --> 1:11:36.020\n But the degree of mainstreaming that's happened\n\n1:11:36.020 --> 1:11:40.120\n in the last 10 to 15 years is astounding\n\n1:11:40.120 --> 1:11:42.080\n to those of us who've been at it for a while.\n\n1:11:42.080 --> 1:11:45.360\n Yeah, but there's a marketing aspect to the term,\n\n1:11:45.360 --> 1:11:48.800\n but in terms of actual full force research\n\n1:11:48.800 --> 1:11:51.280\n that's going on under the header of AGI,\n\n1:11:51.280 --> 1:11:54.280\n it's currently, I would say dominated,\n\n1:11:54.280 --> 1:11:55.960\n maybe you can disagree,\n\n1:11:55.960 --> 1:11:57.740\n dominated by neural networks research,\n\n1:11:57.740 --> 1:12:01.140\n that the nonlinear regression, as you mentioned.\n\n1:12:02.740 --> 1:12:06.520\n Like what's your sense with OpenCog, with your work,\n\n1:12:06.520 --> 1:12:10.920\n but in general, I was logic based systems\n\n1:12:10.920 --> 1:12:12.000\n and expert systems.\n\n1:12:12.000 --> 1:12:17.000\n For me, always seemed to capture a deep element\n\n1:12:18.440 --> 1:12:21.400\n of intelligence that needs to be there.\n\n1:12:21.400 --> 1:12:23.020\n Like you said, it needs to learn,\n\n1:12:23.020 --> 1:12:24.900\n it needs to be automated somehow,\n\n1:12:24.900 --> 1:12:29.900\n but that seems to be missing from a lot of research currently.\n\n1:12:31.360 --> 1:12:32.780\n So what's your sense?\n\n1:12:34.360 --> 1:12:36.280\n I guess one way to ask this question,\n\n1:12:36.280 --> 1:12:39.200\n what's your sense of what kind of things\n\n1:12:39.200 --> 1:12:42.140\n will an AGI system need to have?\n\n1:12:43.480 --> 1:12:45.960\n Yeah, that's a very interesting topic\n\n1:12:45.960 --> 1:12:47.900\n that I've thought about for a long time.\n\n1:12:47.900 --> 1:12:52.900\n And I think there are many, many different approaches\n\n1:12:53.840 --> 1:12:56.920\n that can work for getting to human level AI.\n\n1:12:56.920 --> 1:13:01.920\n So I don't think there's like one golden algorithm,\n\n1:13:02.600 --> 1:13:05.840\n or one golden design that can work.\n\n1:13:05.840 --> 1:13:10.720\n And I mean, flying machines is the much worn\n\n1:13:10.720 --> 1:13:11.680\n analogy here, right?\n\n1:13:11.680 --> 1:13:13.760\n Like, I mean, you have airplanes, you have helicopters,\n\n1:13:13.760 --> 1:13:17.160\n you have balloons, you have stealth bombers\n\n1:13:17.160 --> 1:13:18.760\n that don't look like regular airplanes.\n\n1:13:18.760 --> 1:13:21.040\n You've got all blimps.\n\n1:13:21.040 --> 1:13:21.880\n Birds too.\n\n1:13:21.880 --> 1:13:24.280\n Birds, yeah, and bugs, right?\n\n1:13:24.280 --> 1:13:25.120\n Yeah.\n\n1:13:25.120 --> 1:13:29.920\n And there are certainly many kinds of flying machines that.\n\n1:13:29.920 --> 1:13:32.360\n And there's a catapult that you can just launch.\n\n1:13:32.360 --> 1:13:36.160\n And there's bicycle powered like flying machines, right?\n\n1:13:36.160 --> 1:13:37.000\n Nice, yeah.\n\n1:13:37.000 --> 1:13:40.920\n Yeah, so now these are all analyzable\n\n1:13:40.920 --> 1:13:43.800\n by a basic theory of aerodynamics, right?\n\n1:13:43.800 --> 1:13:48.800\n Now, so one issue with AGI is we don't yet have the analog\n\n1:13:48.920 --> 1:13:50.800\n of the theory of aerodynamics.\n\n1:13:50.800 --> 1:13:54.640\n And that's what Marcus Hutter was trying to make\n\n1:13:54.640 --> 1:13:58.820\n with the AXI and his general theory of general intelligence.\n\n1:13:58.820 --> 1:14:03.360\n But that theory in its most clearly articulated parts\n\n1:14:03.360 --> 1:14:07.120\n really only works for either infinitely powerful machines\n\n1:14:07.120 --> 1:14:11.840\n or almost, or insanely impractically powerful machines.\n\n1:14:11.840 --> 1:14:14.880\n So I mean, if you were gonna take a theory based approach\n\n1:14:14.880 --> 1:14:19.880\n to AGI, what you would do is say, well, let's take\n\n1:14:20.040 --> 1:14:25.040\n what's called say AXE TL, which is Hutter's AXE machine\n\n1:14:25.040 --> 1:14:29.000\n that can work on merely insanely much processing power\n\n1:14:29.000 --> 1:14:30.200\n rather than infinitely much.\n\n1:14:30.200 --> 1:14:32.240\n What does TL stand for?\n\n1:14:32.240 --> 1:14:33.560\n Time and length.\n\n1:14:33.560 --> 1:14:34.400\n Okay.\n\n1:14:34.400 --> 1:14:35.600\n So you're basically how it.\n\n1:14:35.600 --> 1:14:36.480\n Like constrained somehow.\n\n1:14:36.480 --> 1:14:37.320\n Yeah, yeah, yeah.\n\n1:14:37.320 --> 1:14:42.320\n So how AXE works basically is each action\n\n1:14:42.420 --> 1:14:45.040\n that it wants to take, before taking that action,\n\n1:14:45.040 --> 1:14:47.080\n it looks at all its history.\n\n1:14:47.080 --> 1:14:49.880\n And then it looks at all possible programs\n\n1:14:49.880 --> 1:14:51.760\n that it could use to make a decision.\n\n1:14:51.760 --> 1:14:54.320\n And it decides like which decision program\n\n1:14:54.320 --> 1:14:56.120\n would have let it make the best decisions\n\n1:14:56.120 --> 1:14:58.400\n according to its reward function over its history.\n\n1:14:58.400 --> 1:15:00.000\n And it uses that decision program\n\n1:15:00.000 --> 1:15:02.080\n to make the next decision, right?\n\n1:15:02.080 --> 1:15:04.760\n It's not afraid of infinite resources.\n\n1:15:04.760 --> 1:15:06.360\n It's searching through the space\n\n1:15:06.360 --> 1:15:08.440\n of all possible computer programs\n\n1:15:08.440 --> 1:15:10.720\n in between each action and each next action.\n\n1:15:10.720 --> 1:15:15.320\n Now, AXE TL searches through all possible computer programs\n\n1:15:15.320 --> 1:15:18.160\n that have runtime less than T and length less than L.\n\n1:15:18.160 --> 1:15:22.680\n So it's, which is still an impractically humongous space,\n\n1:15:22.680 --> 1:15:23.520\n right?\n\n1:15:23.520 --> 1:15:27.960\n So what you would like to do to make an AGI\n\n1:15:27.960 --> 1:15:29.840\n and what will probably be done 50 years from now\n\n1:15:29.840 --> 1:15:34.840\n to make an AGI is say, okay, well, we have some constraints.\n\n1:15:34.840 --> 1:15:37.480\n We have these processing power constraints\n\n1:15:37.480 --> 1:15:42.480\n and we have the space and time constraints on the program.\n\n1:15:42.700 --> 1:15:45.360\n We have energy utilization constraints\n\n1:15:45.360 --> 1:15:48.160\n and we have this particular class environments,\n\n1:15:48.160 --> 1:15:50.320\n class of environments that we care about,\n\n1:15:50.320 --> 1:15:54.400\n which may be say, you know, manipulating physical objects\n\n1:15:54.400 --> 1:15:55.400\n on the surface of the earth,\n\n1:15:55.400 --> 1:15:57.360\n communicating in human language.\n\n1:15:57.360 --> 1:16:02.240\n I mean, whatever our particular, not annihilating humanity,\n\n1:16:02.240 --> 1:16:05.440\n whatever our particular requirements happen to be.\n\n1:16:05.440 --> 1:16:07.280\n If you formalize those requirements\n\n1:16:07.280 --> 1:16:10.300\n in some formal specification language,\n\n1:16:10.300 --> 1:16:12.320\n you should then be able to run\n\n1:16:13.320 --> 1:16:17.040\n automated program specializer on AXE TL,\n\n1:16:17.040 --> 1:16:21.400\n specialize it to the computing resource constraints\n\n1:16:21.400 --> 1:16:23.600\n and the particular environment and goal.\n\n1:16:23.600 --> 1:16:27.600\n And then it will spit out like the specialized version\n\n1:16:27.600 --> 1:16:30.620\n of AXE TL to your resource restrictions\n\n1:16:30.620 --> 1:16:32.700\n and your environment, which will be your AGI, right?\n\n1:16:32.700 --> 1:16:36.160\n And that I think is how our super AGI\n\n1:16:36.160 --> 1:16:38.560\n will create new AGI systems, right?\n\n1:16:38.560 --> 1:16:40.600\n But that's a very rush.\n\n1:16:40.600 --> 1:16:41.600\n It seems really inefficient.\n\n1:16:41.600 --> 1:16:43.160\n It's a very Russian approach by the way,\n\n1:16:43.160 --> 1:16:45.240\n like the whole field of program specialization\n\n1:16:45.240 --> 1:16:47.280\n came out of Russia.\n\n1:16:47.280 --> 1:16:48.120\n Can you backtrack?\n\n1:16:48.120 --> 1:16:49.680\n So what is program specialization?\n\n1:16:49.680 --> 1:16:51.120\n So it's basically...\n\n1:16:51.120 --> 1:16:53.640\n Well, take sorting, for example.\n\n1:16:53.640 --> 1:16:56.640\n You can have a generic program for sorting lists,\n\n1:16:56.640 --> 1:16:58.280\n but what if all your lists you care about\n\n1:16:58.280 --> 1:16:59.920\n are length 10,000 or less?\n\n1:16:59.920 --> 1:17:00.760\n Got it.\n\n1:17:00.760 --> 1:17:02.560\n You can run an automated program specializer\n\n1:17:02.560 --> 1:17:04.080\n on your sorting algorithm,\n\n1:17:04.080 --> 1:17:05.400\n and it will come up with the algorithm\n\n1:17:05.400 --> 1:17:08.400\n that's optimal for sorting lists of length 1,000 or less,\n\n1:17:08.400 --> 1:17:09.800\n or 10,000 or less, right?\n\n1:17:09.800 --> 1:17:12.200\n That's kind of like, isn't that the kind of the process\n\n1:17:12.200 --> 1:17:17.200\n of evolution as a program specializer to the environment?\n\n1:17:17.440 --> 1:17:20.000\n So you're kind of evolving human beings,\n\n1:17:20.000 --> 1:17:21.840\n or you're living creatures.\n\n1:17:21.840 --> 1:17:24.320\n Your Russian heritage is showing there.\n\n1:17:24.320 --> 1:17:28.480\n So with Alexander Vityaev and Peter Anokhin and so on,\n\n1:17:28.480 --> 1:17:31.800\n I mean, there's a long history\n\n1:17:31.800 --> 1:17:36.760\n of thinking about evolution that way also, right?\n\n1:17:36.760 --> 1:17:40.120\n So, well, my point is that what we're thinking of\n\n1:17:40.120 --> 1:17:44.160\n as a human level general intelligence,\n\n1:17:44.160 --> 1:17:46.680\n if you start from narrow AIs,\n\n1:17:46.680 --> 1:17:50.320\n like are being used in the commercial AI field now,\n\n1:17:50.320 --> 1:17:51.440\n then you're thinking,\n\n1:17:51.440 --> 1:17:53.400\n okay, how do we make it more and more general?\n\n1:17:53.400 --> 1:17:54.400\n On the other hand,\n\n1:17:54.400 --> 1:17:58.080\n if you start from AICSI or Schmidhuber's G\u00f6del machine,\n\n1:17:58.080 --> 1:18:01.120\n or these infinitely powerful,\n\n1:18:01.120 --> 1:18:04.000\n but practically infeasible AIs,\n\n1:18:04.000 --> 1:18:06.440\n then getting to a human level AGI\n\n1:18:06.440 --> 1:18:08.240\n is a matter of specialization.\n\n1:18:08.240 --> 1:18:10.200\n It's like, how do you take these\n\n1:18:10.200 --> 1:18:12.880\n maximally general learning processes\n\n1:18:12.880 --> 1:18:15.760\n and how do you specialize them\n\n1:18:15.760 --> 1:18:17.600\n so that they can operate\n\n1:18:17.600 --> 1:18:20.520\n within the resource constraints that you have,\n\n1:18:20.520 --> 1:18:24.360\n but will achieve the particular things that you care about?\n\n1:18:24.360 --> 1:18:28.200\n Because we humans are not maximally general intelligence.\n\n1:18:28.200 --> 1:18:31.400\n If I ask you to run a maze in 750 dimensions,\n\n1:18:31.400 --> 1:18:33.040\n you'd probably be very slow.\n\n1:18:33.040 --> 1:18:34.600\n Whereas at two dimensions,\n\n1:18:34.600 --> 1:18:37.080\n you're probably way better, right?\n\n1:18:37.080 --> 1:18:40.800\n So, I mean, we're special because our hippocampus\n\n1:18:40.800 --> 1:18:43.080\n has a two dimensional map in it, right?\n\n1:18:43.080 --> 1:18:46.000\n And it does not have a 750 dimensional map in it.\n\n1:18:46.000 --> 1:18:51.000\n So, I mean, we're a peculiar mix\n\n1:18:51.440 --> 1:18:56.000\n of generality and specialization, right?\n\n1:18:56.000 --> 1:18:58.240\n We'll probably start quite general at birth.\n\n1:18:59.200 --> 1:19:00.760\n Not obviously still narrow,\n\n1:19:00.760 --> 1:19:03.200\n but like more general than we are\n\n1:19:03.200 --> 1:19:07.520\n at age 20 and 30 and 40 and 50 and 60.\n\n1:19:07.520 --> 1:19:10.240\n I don't think that, I think it's more complex than that\n\n1:19:10.240 --> 1:19:13.800\n because I mean, in some sense,\n\n1:19:13.800 --> 1:19:17.520\n a young child is less biased\n\n1:19:17.520 --> 1:19:20.000\n and the brain has yet to sort of crystallize\n\n1:19:20.000 --> 1:19:22.360\n into appropriate structures\n\n1:19:22.360 --> 1:19:25.360\n for processing aspects of the physical and social world.\n\n1:19:25.360 --> 1:19:26.560\n On the other hand,\n\n1:19:26.560 --> 1:19:30.120\n the young child is very tied to their sensorium.\n\n1:19:30.120 --> 1:19:33.880\n Whereas we can deal with abstract mathematics,\n\n1:19:33.880 --> 1:19:37.600\n like 750 dimensions and the young child cannot\n\n1:19:37.600 --> 1:19:40.920\n because they haven't grown what Piaget\n\n1:19:40.920 --> 1:19:44.000\n called the formal capabilities.\n\n1:19:44.000 --> 1:19:46.240\n They haven't learned to abstract yet, right?\n\n1:19:46.240 --> 1:19:48.120\n And the ability to abstract\n\n1:19:48.120 --> 1:19:49.720\n gives you a different kind of generality\n\n1:19:49.720 --> 1:19:51.680\n than what the baby has.\n\n1:19:51.680 --> 1:19:55.400\n So, there's both more specialization\n\n1:19:55.400 --> 1:19:57.240\n and more generalization that comes\n\n1:19:57.240 --> 1:19:59.760\n with the development process actually.\n\n1:19:59.760 --> 1:20:02.320\n I mean, I guess just the trajectories\n\n1:20:02.320 --> 1:20:06.320\n of the specialization are most controllable\n\n1:20:06.320 --> 1:20:09.720\n at the young age, I guess is one way to put it.\n\n1:20:09.720 --> 1:20:10.720\n Do you have kids?\n\n1:20:10.720 --> 1:20:11.680\n No.\n\n1:20:11.680 --> 1:20:13.600\n They're not as controllable as you think.\n\n1:20:13.600 --> 1:20:15.880\n So, you think it's interesting.\n\n1:20:15.880 --> 1:20:19.040\n I think, honestly, I think a human adult\n\n1:20:19.040 --> 1:20:23.240\n is much more generally intelligent than a human baby.\n\n1:20:23.240 --> 1:20:25.800\n Babies are very stupid, you know what I mean?\n\n1:20:25.800 --> 1:20:29.480\n I mean, they're cute, which is why we put up\n\n1:20:29.480 --> 1:20:33.080\n with their repetitiveness and stupidity.\n\n1:20:33.080 --> 1:20:35.040\n And they have what the Zen guys would call\n\n1:20:35.040 --> 1:20:38.200\n a beginner's mind, which is a beautiful thing,\n\n1:20:38.200 --> 1:20:40.760\n but that doesn't necessarily correlate\n\n1:20:40.760 --> 1:20:43.320\n with a high level of intelligence.\n\n1:20:43.320 --> 1:20:46.120\n On the plot of cuteness and stupidity,\n\n1:20:46.120 --> 1:20:48.720\n there's a process that allows us to put up\n\n1:20:48.720 --> 1:20:50.880\n with their stupidity as they become more intelligent.\n\n1:20:50.880 --> 1:20:52.400\n So, by the time you're an ugly old man like me,\n\n1:20:52.400 --> 1:20:54.720\n you gotta get really, really smart to compensate.\n\n1:20:54.720 --> 1:20:56.160\n To compensate, okay, cool.\n\n1:20:56.160 --> 1:20:59.160\n But yeah, going back to your original question,\n\n1:20:59.160 --> 1:21:04.160\n so the way I look at human level AGI\n\n1:21:05.280 --> 1:21:08.640\n is how do you specialize, you know,\n\n1:21:08.640 --> 1:21:12.160\n unrealistically inefficient, superhuman,\n\n1:21:12.160 --> 1:21:14.600\n brute force learning processes\n\n1:21:14.600 --> 1:21:18.320\n to the specific goals that humans need to achieve\n\n1:21:18.320 --> 1:21:21.920\n and the specific resources that we have.\n\n1:21:21.920 --> 1:21:24.600\n And both of these, the goals and the resources\n\n1:21:24.600 --> 1:21:27.120\n and the environments, I mean, all this is important.\n\n1:21:27.120 --> 1:21:31.320\n And on the resources side, it's important\n\n1:21:31.320 --> 1:21:34.680\n that the hardware resources we're bringing to bear\n\n1:21:35.600 --> 1:21:38.240\n are very different than the human brain.\n\n1:21:38.240 --> 1:21:42.680\n So the way I would want to implement AGI\n\n1:21:42.680 --> 1:21:45.960\n on a bunch of neurons in a vat\n\n1:21:45.960 --> 1:21:48.880\n that I could rewire arbitrarily is quite different\n\n1:21:48.880 --> 1:21:51.760\n than the way I would want to create AGI\n\n1:21:51.760 --> 1:21:55.760\n on say a modern server farm of CPUs and GPUs,\n\n1:21:55.760 --> 1:21:57.440\n which in turn may be quite different\n\n1:21:57.440 --> 1:22:00.200\n than the way I would want to implement AGI\n\n1:22:00.200 --> 1:22:03.760\n on whatever quantum computer we'll have in 10 years,\n\n1:22:03.760 --> 1:22:06.680\n supposing someone makes a robust quantum turing machine\n\n1:22:06.680 --> 1:22:08.240\n or something, right?\n\n1:22:08.240 --> 1:22:12.640\n So I think there's been coevolution\n\n1:22:12.640 --> 1:22:16.960\n of the patterns of organization in the human brain\n\n1:22:16.960 --> 1:22:19.960\n and the physiological particulars\n\n1:22:19.960 --> 1:22:23.240\n of the human brain over time.\n\n1:22:23.240 --> 1:22:25.240\n And when you look at neural networks,\n\n1:22:25.240 --> 1:22:28.040\n that is one powerful class of learning algorithms,\n\n1:22:28.040 --> 1:22:30.040\n but it's also a class of learning algorithms\n\n1:22:30.040 --> 1:22:33.400\n that evolve to exploit the particulars of the human brain\n\n1:22:33.400 --> 1:22:36.320\n as a computational substrate.\n\n1:22:36.320 --> 1:22:38.880\n If you're looking at the computational substrate\n\n1:22:38.880 --> 1:22:41.040\n of a modern server farm,\n\n1:22:41.040 --> 1:22:43.200\n you won't necessarily want the same algorithms\n\n1:22:43.200 --> 1:22:45.760\n that you want on the human brain.\n\n1:22:45.760 --> 1:22:48.920\n And from the right level of abstraction,\n\n1:22:48.920 --> 1:22:51.760\n you could look at maybe the best algorithms on the brain\n\n1:22:51.760 --> 1:22:54.480\n and the best algorithms on a modern computer network\n\n1:22:54.480 --> 1:22:56.480\n as implementing the same abstract learning\n\n1:22:56.480 --> 1:22:59.080\n and representation processes,\n\n1:22:59.080 --> 1:23:01.680\n but finding that level of abstraction\n\n1:23:01.680 --> 1:23:04.960\n is its own AGI research project then, right?\n\n1:23:04.960 --> 1:23:07.800\n So that's about the hardware side\n\n1:23:07.800 --> 1:23:10.880\n and the software side, which follows from that.\n\n1:23:10.880 --> 1:23:14.200\n Then regarding what are the requirements,\n\n1:23:14.200 --> 1:23:16.440\n I wrote the paper years ago\n\n1:23:16.440 --> 1:23:20.360\n on what I called the embodied communication prior,\n\n1:23:20.360 --> 1:23:22.960\n which was quite similar in intent\n\n1:23:22.960 --> 1:23:26.760\n to Yoshua Bengio's recent paper on the consciousness prior,\n\n1:23:26.760 --> 1:23:30.440\n except I didn't wanna wrap up consciousness in it\n\n1:23:30.440 --> 1:23:34.240\n because to me, the qualia problem and subjective experience\n\n1:23:34.240 --> 1:23:35.880\n is a very interesting issue also,\n\n1:23:35.880 --> 1:23:37.880\n which we can chat about,\n\n1:23:37.880 --> 1:23:42.880\n but I would rather keep that philosophical debate distinct\n\n1:23:43.200 --> 1:23:45.240\n from the debate of what kind of biases\n\n1:23:45.240 --> 1:23:47.040\n do you wanna put in a general intelligence\n\n1:23:47.040 --> 1:23:49.800\n to give it human like general intelligence.\n\n1:23:49.800 --> 1:23:53.320\n And I'm not sure Yoshua Bengio is really addressing\n\n1:23:53.320 --> 1:23:55.080\n that kind of consciousness.\n\n1:23:55.080 --> 1:23:56.560\n He's just using the term.\n\n1:23:56.560 --> 1:23:58.600\n I love Yoshua to pieces.\n\n1:23:58.600 --> 1:24:02.960\n Like he's by far my favorite of the lines of deep learning.\n\n1:24:02.960 --> 1:24:03.800\n Yeah.\n\n1:24:03.800 --> 1:24:05.800\n He's such a good hearted guy.\n\n1:24:05.800 --> 1:24:07.000\n He's a good human being.\n\n1:24:07.000 --> 1:24:07.840\n Yeah, for sure.\n\n1:24:07.840 --> 1:24:11.200\n I am not sure he has plumbed to the depths\n\n1:24:11.200 --> 1:24:13.520\n of the philosophy of consciousness.\n\n1:24:13.520 --> 1:24:15.040\n No, he's using it as a sexy term.\n\n1:24:15.040 --> 1:24:15.880\n Yeah, yeah, yeah.\n\n1:24:15.880 --> 1:24:20.880\n So what I called it was the embodied communication prior.\n\n1:24:21.160 --> 1:24:22.520\n Can you maybe explain it a little bit?\n\n1:24:22.520 --> 1:24:23.360\n Yeah, yeah.\n\n1:24:23.360 --> 1:24:26.640\n What I meant was, what are we humans evolved for?\n\n1:24:26.640 --> 1:24:29.720\n You can say being human, but that's very abstract, right?\n\n1:24:29.720 --> 1:24:32.960\n I mean, our minds control individual bodies,\n\n1:24:32.960 --> 1:24:36.920\n which are autonomous agents moving around in a world\n\n1:24:36.920 --> 1:24:41.280\n that's composed largely of solid objects, right?\n\n1:24:41.280 --> 1:24:46.240\n And we've also evolved to communicate via language\n\n1:24:46.240 --> 1:24:49.960\n with other solid object agents that are going around\n\n1:24:49.960 --> 1:24:52.200\n doing things collectively with us\n\n1:24:52.200 --> 1:24:54.400\n in a world of solid objects.\n\n1:24:54.400 --> 1:24:56.920\n And these things are very obvious,\n\n1:24:56.920 --> 1:24:58.400\n but if you compare them to the scope\n\n1:24:58.400 --> 1:25:01.400\n of all possible intelligences\n\n1:25:01.400 --> 1:25:03.120\n or even all possible intelligences\n\n1:25:03.120 --> 1:25:05.400\n that are physically realizable,\n\n1:25:05.400 --> 1:25:07.400\n that actually constrains things a lot.\n\n1:25:07.400 --> 1:25:12.400\n So if you start to look at how would you realize\n\n1:25:13.000 --> 1:25:15.880\n some specialized or constrained version\n\n1:25:15.880 --> 1:25:18.360\n of universal general intelligence\n\n1:25:18.360 --> 1:25:21.160\n in a system that has limited memory\n\n1:25:21.160 --> 1:25:23.160\n and limited speed of processing,\n\n1:25:23.160 --> 1:25:26.200\n but whose general intelligence will be biased\n\n1:25:26.200 --> 1:25:28.840\n toward controlling a solid object agent,\n\n1:25:28.840 --> 1:25:31.360\n which is mobile in a solid object world\n\n1:25:31.360 --> 1:25:33.480\n for manipulating solid objects\n\n1:25:33.480 --> 1:25:38.480\n and communicating via language with other similar agents\n\n1:25:38.560 --> 1:25:39.920\n in that same world, right?\n\n1:25:39.920 --> 1:25:41.560\n Then starting from that,\n\n1:25:41.560 --> 1:25:43.640\n you're starting to get a requirements analysis\n\n1:25:43.640 --> 1:25:48.120\n for human level general intelligence.\n\n1:25:48.120 --> 1:25:50.920\n And then that leads you into cognitive science\n\n1:25:50.920 --> 1:25:53.080\n and you can look at, say, what are the different types\n\n1:25:53.080 --> 1:25:56.960\n of memory that the human mind and brain has?\n\n1:25:56.960 --> 1:26:00.840\n And this has matured over the last decades\n\n1:26:00.840 --> 1:26:02.920\n and I got into this a lot.\n\n1:26:02.920 --> 1:26:04.600\n So after getting my PhD in math,\n\n1:26:04.600 --> 1:26:06.080\n I was an academic for eight years.\n\n1:26:06.080 --> 1:26:08.720\n I was in departments of mathematics,\n\n1:26:08.720 --> 1:26:11.320\n computer science, and psychology.\n\n1:26:11.320 --> 1:26:12.760\n When I was in the psychology department\n\n1:26:12.760 --> 1:26:14.240\n at the University of Western Australia,\n\n1:26:14.240 --> 1:26:18.720\n I was focused on cognitive science of memory and perception.\n\n1:26:18.720 --> 1:26:21.280\n Actually, I was teaching neural nets and deep neural nets\n\n1:26:21.280 --> 1:26:23.600\n and it was multi layer perceptrons, right?\n\n1:26:23.600 --> 1:26:24.640\n Psychology?\n\n1:26:24.640 --> 1:26:25.800\n Yeah.\n\n1:26:25.800 --> 1:26:27.880\n Cognitive science, it was cross disciplinary\n\n1:26:27.880 --> 1:26:31.280\n among engineering, math, psychology, philosophy,\n\n1:26:31.280 --> 1:26:33.280\n linguistics, computer science.\n\n1:26:33.280 --> 1:26:35.960\n But yeah, we were teaching psychology students\n\n1:26:35.960 --> 1:26:40.040\n to try to model the data from human cognition experiments\n\n1:26:40.040 --> 1:26:42.080\n using multi layer perceptrons,\n\n1:26:42.080 --> 1:26:45.040\n which was the early version of a deep neural network.\n\n1:26:45.040 --> 1:26:47.880\n Very, very, yeah, recurrent back prop\n\n1:26:47.880 --> 1:26:51.200\n was very, very slow to train back then, right?\n\n1:26:51.200 --> 1:26:53.920\n So this is the study of these constraint systems\n\n1:26:53.920 --> 1:26:55.640\n that are supposed to deal with physical objects.\n\n1:26:55.640 --> 1:27:01.480\n So if you look at cognitive psychology,\n\n1:27:01.480 --> 1:27:04.520\n you can see there's multiple types of memory,\n\n1:27:04.520 --> 1:27:06.560\n which are to some extent represented\n\n1:27:06.560 --> 1:27:08.480\n by different subsystems in the human brain.\n\n1:27:08.480 --> 1:27:10.360\n So we have episodic memory,\n\n1:27:10.360 --> 1:27:12.560\n which takes into account our life history\n\n1:27:13.520 --> 1:27:15.240\n and everything that's happened to us.\n\n1:27:15.240 --> 1:27:17.320\n We have declarative or semantic memory,\n\n1:27:17.320 --> 1:27:20.080\n which is like facts and beliefs abstracted\n\n1:27:20.080 --> 1:27:22.840\n from the particular situations that they occurred in.\n\n1:27:22.840 --> 1:27:26.120\n There's sensory memory, which to some extent\n\n1:27:26.120 --> 1:27:27.600\n is sense modality specific,\n\n1:27:27.600 --> 1:27:32.600\n and then to some extent is unified across sense modalities.\n\n1:27:33.360 --> 1:27:36.120\n There's procedural memory, memory of how to do stuff,\n\n1:27:36.120 --> 1:27:38.160\n like how to swing the tennis racket, right?\n\n1:27:38.160 --> 1:27:39.920\n Which is, there's motor memory,\n\n1:27:39.920 --> 1:27:43.640\n but it's also a little more abstract than motor memory.\n\n1:27:43.640 --> 1:27:47.520\n It involves cerebellum and cortex working together.\n\n1:27:47.520 --> 1:27:51.600\n Then there's memory linkage with emotion\n\n1:27:51.600 --> 1:27:55.920\n which has to do with linkages of cortex and limbic system.\n\n1:27:55.920 --> 1:27:59.160\n There's specifics of spatial and temporal modeling\n\n1:27:59.160 --> 1:28:02.760\n connected with memory, which has to do with hippocampus\n\n1:28:02.760 --> 1:28:05.360\n and thalamus connecting to cortex.\n\n1:28:05.360 --> 1:28:08.160\n And the basal ganglia, which influences goals.\n\n1:28:08.160 --> 1:28:10.960\n So we have specific memory of what goals,\n\n1:28:10.960 --> 1:28:13.160\n subgoals and sub subgoals we want to perceive\n\n1:28:13.160 --> 1:28:15.040\n in which context in the past.\n\n1:28:15.040 --> 1:28:18.240\n Human brain has substantially different subsystems\n\n1:28:18.240 --> 1:28:21.040\n for these different types of memory\n\n1:28:21.040 --> 1:28:24.240\n and substantially differently tuned learning,\n\n1:28:24.240 --> 1:28:27.280\n like differently tuned modes of longterm potentiation\n\n1:28:27.280 --> 1:28:29.720\n to do with the types of neurons and neurotransmitters\n\n1:28:29.720 --> 1:28:31.280\n in the different parts of the brain\n\n1:28:31.280 --> 1:28:33.040\n corresponding to these different types of knowledge.\n\n1:28:33.040 --> 1:28:35.880\n And these different types of memory and learning\n\n1:28:35.880 --> 1:28:38.520\n in the human brain, I mean, you can back these all\n\n1:28:38.520 --> 1:28:41.920\n into embodied communication for controlling agents\n\n1:28:41.920 --> 1:28:44.680\n in worlds of solid objects.\n\n1:28:44.680 --> 1:28:47.720\n Now, so if you look at building an AGI system,\n\n1:28:47.720 --> 1:28:50.440\n one way to do it, which starts more from cognitive science\n\n1:28:50.440 --> 1:28:52.680\n than neuroscience is to say,\n\n1:28:52.680 --> 1:28:55.240\n okay, what are the types of memory\n\n1:28:55.240 --> 1:28:57.360\n that are necessary for this kind of world?\n\n1:28:57.360 --> 1:29:00.720\n Yeah, yeah, necessary for this sort of intelligence.\n\n1:29:00.720 --> 1:29:02.760\n What types of learning work well\n\n1:29:02.760 --> 1:29:04.600\n with these different types of memory?\n\n1:29:04.600 --> 1:29:07.800\n And then how do you connect all these things together, right?\n\n1:29:07.800 --> 1:29:10.800\n And of course the human brain did it incrementally\n\n1:29:10.800 --> 1:29:14.360\n through evolution because each of the sub networks\n\n1:29:14.360 --> 1:29:16.680\n of the brain, I mean, it's not really the lobes\n\n1:29:16.680 --> 1:29:18.240\n of the brain, it's the sub networks,\n\n1:29:18.240 --> 1:29:20.800\n each of which is widely distributed,\n\n1:29:20.800 --> 1:29:23.680\n which of the, each of the sub networks of the brain\n\n1:29:23.680 --> 1:29:27.160\n co evolves with the other sub networks of the brain,\n\n1:29:27.160 --> 1:29:29.480\n both in terms of its patterns of organization\n\n1:29:29.480 --> 1:29:31.840\n and the particulars of the neurophysiology.\n\n1:29:31.840 --> 1:29:33.440\n So they all grew up communicating\n\n1:29:33.440 --> 1:29:34.440\n and adapting to each other.\n\n1:29:34.440 --> 1:29:36.720\n It's not like they were separate black boxes\n\n1:29:36.720 --> 1:29:40.200\n that were then glommed together, right?\n\n1:29:40.200 --> 1:29:43.320\n Whereas as engineers, we would tend to say,\n\n1:29:43.320 --> 1:29:46.680\n let's make the declarative memory box here\n\n1:29:46.680 --> 1:29:48.440\n and the procedural memory box here\n\n1:29:48.440 --> 1:29:51.400\n and the perception box here and wire them together.\n\n1:29:51.400 --> 1:29:54.120\n And when you can do that, it's interesting.\n\n1:29:54.120 --> 1:29:55.680\n I mean, that's how a car is built, right?\n\n1:29:55.680 --> 1:29:58.560\n But on the other hand, that's clearly not\n\n1:29:58.560 --> 1:30:01.400\n how biological systems are made.\n\n1:30:01.400 --> 1:30:05.360\n The parts co evolve so as to adapt and work together.\n\n1:30:05.360 --> 1:30:09.240\n That's by the way, how every human engineered system\n\n1:30:09.240 --> 1:30:11.640\n that flies, that was, we were using that analogy\n\n1:30:11.640 --> 1:30:13.000\n before it's built as well.\n\n1:30:13.000 --> 1:30:14.440\n So do you find this at all appealing?\n\n1:30:14.440 --> 1:30:16.680\n Like there's been a lot of really exciting,\n\n1:30:16.680 --> 1:30:20.160\n which I find strange that it's ignored work\n\n1:30:20.160 --> 1:30:21.880\n in cognitive architectures, for example,\n\n1:30:21.880 --> 1:30:23.320\n throughout the last few decades.\n\n1:30:23.320 --> 1:30:24.320\n Do you find that?\n\n1:30:24.320 --> 1:30:27.960\n Yeah, I mean, I had a lot to do with that community\n\n1:30:27.960 --> 1:30:31.000\n and you know, Paul Rosenbloom, who was one of the,\n\n1:30:31.000 --> 1:30:33.480\n and John Laird who built the SOAR architecture,\n\n1:30:33.480 --> 1:30:34.640\n are friends of mine.\n\n1:30:34.640 --> 1:30:37.160\n And I learned SOAR quite well\n\n1:30:37.160 --> 1:30:39.440\n and ACTAR and these different cognitive architectures.\n\n1:30:39.440 --> 1:30:44.440\n And how I was looking at the AI world about 10 years ago\n\n1:30:44.520 --> 1:30:47.840\n before this whole commercial deep learning explosion was,\n\n1:30:47.840 --> 1:30:51.560\n on the one hand, you had these cognitive architecture guys\n\n1:30:51.560 --> 1:30:53.480\n who were working closely with psychologists\n\n1:30:53.480 --> 1:30:55.760\n and cognitive scientists who had thought a lot\n\n1:30:55.760 --> 1:30:58.840\n about how the different parts of a human like mind\n\n1:30:58.840 --> 1:31:00.400\n should work together.\n\n1:31:00.400 --> 1:31:03.600\n On the other hand, you had these learning theory guys\n\n1:31:03.600 --> 1:31:06.040\n who didn't care at all about the architecture,\n\n1:31:06.040 --> 1:31:07.360\n but we're just thinking about like,\n\n1:31:07.360 --> 1:31:10.280\n how do you recognize patterns in large amounts of data?\n\n1:31:10.280 --> 1:31:14.560\n And in some sense, what you needed to do\n\n1:31:14.560 --> 1:31:18.440\n was to get the learning that the learning theory guys\n\n1:31:18.440 --> 1:31:21.440\n were doing and put it together with the architecture\n\n1:31:21.440 --> 1:31:24.240\n that the cognitive architecture guys were doing.\n\n1:31:24.240 --> 1:31:25.960\n And then you would have what you needed.\n\n1:31:25.960 --> 1:31:30.760\n Now, you can't, unfortunately, when you look at the details,\n\n1:31:31.600 --> 1:31:34.960\n you can't just do that without totally rebuilding\n\n1:31:34.960 --> 1:31:37.840\n what is happening on both the cognitive architecture\n\n1:31:37.840 --> 1:31:38.760\n and the learning side.\n\n1:31:38.760 --> 1:31:41.760\n So, I mean, they tried to do that in SOAR,\n\n1:31:41.760 --> 1:31:43.960\n but what they ultimately did is like,\n\n1:31:43.960 --> 1:31:46.560\n take a deep neural net or something for perception\n\n1:31:46.560 --> 1:31:50.800\n and you include it as one of the black boxes.\n\n1:31:50.800 --> 1:31:51.960\n It becomes one of the boxes.\n\n1:31:51.960 --> 1:31:53.800\n The learning mechanism becomes one of the boxes\n\n1:31:53.800 --> 1:31:57.440\n as opposed to fundamental part of the system.\n\n1:31:57.440 --> 1:32:00.400\n You could look at some of the stuff DeepMind has done,\n\n1:32:00.400 --> 1:32:03.240\n like the differential neural computer or something\n\n1:32:03.240 --> 1:32:07.080\n that sort of has a neural net for deep learning perception.\n\n1:32:07.080 --> 1:32:10.640\n It has another neural net, which is like a memory matrix\n\n1:32:10.640 --> 1:32:13.080\n that stores, say, the map of the London subway or something.\n\n1:32:13.080 --> 1:32:16.440\n So probably Demis Tsabas was thinking about this\n\n1:32:16.440 --> 1:32:18.520\n like part of cortex and part of hippocampus\n\n1:32:18.520 --> 1:32:20.440\n because hippocampus has a spatial map.\n\n1:32:20.440 --> 1:32:21.720\n And when he was a neuroscientist,\n\n1:32:21.720 --> 1:32:24.600\n he was doing a bunch on cortex hippocampus interconnection.\n\n1:32:24.600 --> 1:32:27.320\n So there, the DNC would be an example of folks\n\n1:32:27.320 --> 1:32:30.160\n from the deep neural net world trying to take a step\n\n1:32:30.160 --> 1:32:32.200\n in the cognitive architecture direction\n\n1:32:32.200 --> 1:32:35.000\n by having two neural modules that correspond roughly\n\n1:32:35.000 --> 1:32:36.720\n to two different parts of the human brain\n\n1:32:36.720 --> 1:32:38.920\n that deal with different kinds of memory and learning.\n\n1:32:38.920 --> 1:32:42.000\n But on the other hand, it's super, super, super crude\n\n1:32:42.000 --> 1:32:44.360\n from the cognitive architecture view, right?\n\n1:32:44.360 --> 1:32:48.080\n Just as what John Laird and Soar did with neural nets\n\n1:32:48.080 --> 1:32:51.200\n was super, super crude from a learning point of view\n\n1:32:51.200 --> 1:32:53.360\n because the learning was like off to the side,\n\n1:32:53.360 --> 1:32:55.880\n not affecting the core representations, right?\n\n1:32:55.880 --> 1:32:57.880\n I mean, you weren't learning the representation.\n\n1:32:57.880 --> 1:33:00.080\n You were learning the data that feeds into the...\n\n1:33:00.080 --> 1:33:02.600\n You were learning abstractions of perceptual data\n\n1:33:02.600 --> 1:33:06.560\n to feed into the representation that was not learned, right?\n\n1:33:06.560 --> 1:33:11.000\n So yeah, this was clear to me a while ago.\n\n1:33:11.000 --> 1:33:14.240\n And one of my hopes with the AGI community\n\n1:33:14.240 --> 1:33:15.960\n was to sort of bring people\n\n1:33:15.960 --> 1:33:18.440\n from those two directions together.\n\n1:33:19.320 --> 1:33:21.920\n That didn't happen much in terms of...\n\n1:33:21.920 --> 1:33:22.760\n Not yet.\n\n1:33:22.760 --> 1:33:24.520\n And what I was gonna say is it didn't happen\n\n1:33:24.520 --> 1:33:26.360\n in terms of bringing like the lions\n\n1:33:26.360 --> 1:33:28.560\n of cognitive architecture together\n\n1:33:28.560 --> 1:33:30.480\n with the lions of deep learning.\n\n1:33:30.480 --> 1:33:33.760\n It did work in the sense that a bunch of younger researchers\n\n1:33:33.760 --> 1:33:35.760\n have had their heads filled with both of those ideas.\n\n1:33:35.760 --> 1:33:38.840\n This comes back to a saying my dad,\n\n1:33:38.840 --> 1:33:41.360\n who was a university professor, often quoted to me,\n\n1:33:41.360 --> 1:33:44.960\n which was, science advances one funeral at a time,\n\n1:33:45.840 --> 1:33:47.840\n which I'm trying to avoid.\n\n1:33:47.840 --> 1:33:51.320\n Like I'm 53 years old and I'm trying to invent\n\n1:33:51.320 --> 1:33:53.480\n amazing, weird ass new things\n\n1:33:53.480 --> 1:33:56.160\n that nobody ever thought about,\n\n1:33:56.160 --> 1:33:59.240\n which we'll talk about in a few minutes.\n\n1:33:59.240 --> 1:34:02.280\n But there is that aspect, right?\n\n1:34:02.280 --> 1:34:05.680\n Like the people who've been at AI a long time\n\n1:34:05.680 --> 1:34:08.760\n and have made their career developing one aspect,\n\n1:34:08.760 --> 1:34:12.880\n like a cognitive architecture or a deep learning approach,\n\n1:34:12.880 --> 1:34:14.760\n it can be hard once you're old\n\n1:34:14.760 --> 1:34:17.280\n and have made your career doing one thing,\n\n1:34:17.280 --> 1:34:19.640\n it can be hard to mentally shift gears.\n\n1:34:19.640 --> 1:34:23.640\n I mean, I try quite hard to remain flexible minded.\n\n1:34:23.640 --> 1:34:26.480\n Have you been successful somewhat in changing,\n\n1:34:26.480 --> 1:34:29.640\n maybe, have you changed your mind on some aspects\n\n1:34:29.640 --> 1:34:32.920\n of what it takes to build an AGI, like technical things?\n\n1:34:32.920 --> 1:34:36.040\n The hard part is that the world doesn't want you to.\n\n1:34:36.040 --> 1:34:37.360\n The world or your own brain?\n\n1:34:37.360 --> 1:34:39.560\n The world, well, that one point\n\n1:34:39.560 --> 1:34:41.040\n is that your brain doesn't want to.\n\n1:34:41.040 --> 1:34:43.480\n The other part is that the world doesn't want you to.\n\n1:34:43.480 --> 1:34:46.520\n Like the people who have followed your ideas\n\n1:34:46.520 --> 1:34:49.280\n get mad at you if you change your mind.\n\n1:34:49.280 --> 1:34:54.280\n And the media wants to pigeonhole you as an avatar\n\n1:34:54.560 --> 1:34:57.160\n of a certain idea.\n\n1:34:57.160 --> 1:35:01.480\n But yeah, I've changed my mind on a bunch of things.\n\n1:35:01.480 --> 1:35:03.800\n I mean, when I started my career,\n\n1:35:03.800 --> 1:35:05.240\n I really thought quantum computing\n\n1:35:05.240 --> 1:35:07.920\n would be necessary for AGI.\n\n1:35:07.920 --> 1:35:10.800\n And I doubt it's necessary now,\n\n1:35:10.800 --> 1:35:14.680\n although I think it will be a super major enhancement.\n\n1:35:14.680 --> 1:35:19.360\n But I mean, I'm now in the middle of embarking\n\n1:35:19.360 --> 1:35:23.400\n on the complete rethink and rewrite from scratch\n\n1:35:23.400 --> 1:35:28.400\n of our OpenCog AGI system together with Alexey Potapov\n\n1:35:28.480 --> 1:35:29.840\n and his team in St. Petersburg,\n\n1:35:29.840 --> 1:35:31.600\n who's working with me in SingularityNet.\n\n1:35:31.600 --> 1:35:35.680\n So now we're trying to like go back to basics,\n\n1:35:35.680 --> 1:35:37.800\n take everything we learned from working\n\n1:35:37.800 --> 1:35:39.600\n with the current OpenCog system,\n\n1:35:39.600 --> 1:35:41.880\n take everything everybody else has learned\n\n1:35:41.880 --> 1:35:45.680\n from working with their proto AGI systems\n\n1:35:45.680 --> 1:35:50.000\n and design the best framework for the next stage.\n\n1:35:50.000 --> 1:35:53.320\n And I do think there's a lot to be learned\n\n1:35:53.320 --> 1:35:56.800\n from the recent successes with deep neural nets\n\n1:35:56.800 --> 1:35:59.000\n and deep reinforcement systems.\n\n1:35:59.000 --> 1:36:02.680\n I mean, people made these essentially trivial systems\n\n1:36:02.680 --> 1:36:04.840\n work much better than I thought they would.\n\n1:36:04.840 --> 1:36:07.080\n And there's a lot to be learned from that.\n\n1:36:07.080 --> 1:36:10.720\n And I wanna incorporate that knowledge appropriately\n\n1:36:10.720 --> 1:36:13.520\n in our OpenCog 2.0 system.\n\n1:36:13.520 --> 1:36:18.520\n On the other hand, I also think current deep neural net\n\n1:36:18.520 --> 1:36:22.240\n architectures as such will never get you anywhere near AGI.\n\n1:36:22.240 --> 1:36:25.080\n So I think you wanna avoid the pathology\n\n1:36:25.080 --> 1:36:28.360\n of throwing the baby out with the bathwater\n\n1:36:28.360 --> 1:36:30.880\n and like saying, well, these things are garbage\n\n1:36:30.880 --> 1:36:33.840\n because foolish journalists overblow them\n\n1:36:33.840 --> 1:36:37.040\n as being the path to AGI\n\n1:36:37.040 --> 1:36:41.600\n and a few researchers overblow them as well.\n\n1:36:41.600 --> 1:36:45.440\n There's a lot of interesting stuff to be learned there\n\n1:36:45.440 --> 1:36:48.000\n even though those are not the golden path.\n\n1:36:48.000 --> 1:36:50.160\n So maybe this is a good chance to step back.\n\n1:36:50.160 --> 1:36:52.920\n You mentioned OpenCog 2.0, but...\n\n1:36:52.920 --> 1:36:56.040\n Go back to OpenCog 0.0, which exists now.\n\n1:36:56.040 --> 1:36:57.440\n Alpha, yeah.\n\n1:36:58.440 --> 1:37:01.920\n Yeah, maybe talk through the history of OpenCog\n\n1:37:01.920 --> 1:37:03.920\n and your thinking about these ideas.\n\n1:37:03.920 --> 1:37:08.920\n I would say OpenCog 2.0 is a term we're throwing around\n\n1:37:10.120 --> 1:37:14.560\n sort of tongue in cheek because the existing OpenCog system\n\n1:37:14.560 --> 1:37:17.200\n that we're working on now is not remotely close\n\n1:37:17.200 --> 1:37:20.000\n to what we'd consider a 1.0, right?\n\n1:37:20.000 --> 1:37:23.360\n I mean, it's an early...\n\n1:37:23.360 --> 1:37:27.400\n It's been around, what, 13 years or something,\n\n1:37:27.400 --> 1:37:29.800\n but it's still an early stage research system, right?\n\n1:37:29.800 --> 1:37:34.800\n And actually, we are going back to the beginning\n\n1:37:37.360 --> 1:37:40.680\n in terms of theory and implementation\n\n1:37:40.680 --> 1:37:42.840\n because we feel like that's the right thing to do,\n\n1:37:42.840 --> 1:37:45.560\n but I'm sure what we end up with is gonna have\n\n1:37:45.560 --> 1:37:48.560\n a huge amount in common with the current system.\n\n1:37:48.560 --> 1:37:51.640\n I mean, we all still like the general approach.\n\n1:37:51.640 --> 1:37:54.400\n So first of all, what is OpenCog?\n\n1:37:54.400 --> 1:37:59.400\n Sure, OpenCog is an open source software project\n\n1:37:59.800 --> 1:38:04.400\n that I launched together with several others in 2008\n\n1:38:04.400 --> 1:38:08.280\n and probably the first code written toward that\n\n1:38:08.280 --> 1:38:11.160\n was written in 2001 or two or something\n\n1:38:11.160 --> 1:38:15.320\n that was developed as a proprietary code base\n\n1:38:15.320 --> 1:38:18.280\n within my AI company, Novamente LLC.\n\n1:38:18.280 --> 1:38:22.000\n Then we decided to open source it in 2008,\n\n1:38:22.000 --> 1:38:23.840\n cleaned up the code throughout some things\n\n1:38:23.840 --> 1:38:26.920\n and added some new things and...\n\n1:38:26.920 --> 1:38:28.080\n What language is it written in?\n\n1:38:28.080 --> 1:38:29.440\n It's C++.\n\n1:38:29.440 --> 1:38:31.400\n Primarily, there's a bunch of scheme as well,\n\n1:38:31.400 --> 1:38:33.040\n but most of it's C++.\n\n1:38:33.040 --> 1:38:36.520\n And it's separate from something we'll also talk about,\n\n1:38:36.520 --> 1:38:37.480\n the SingularityNet.\n\n1:38:37.480 --> 1:38:41.360\n So it was born as a non networked thing.\n\n1:38:41.360 --> 1:38:42.400\n Correct, correct.\n\n1:38:42.400 --> 1:38:47.000\n Well, there are many levels of networks involved here.\n\n1:38:47.000 --> 1:38:52.000\n No connectivity to the internet, or no, at birth.\n\n1:38:52.000 --> 1:38:57.000\n Yeah, I mean, SingularityNet is a separate project\n\n1:38:57.240 --> 1:38:59.440\n and a separate body of code.\n\n1:38:59.440 --> 1:39:02.600\n And you can use SingularityNet as part of the infrastructure\n\n1:39:02.600 --> 1:39:04.480\n for a distributed OpenCog system,\n\n1:39:04.480 --> 1:39:07.520\n but there are different layers.\n\n1:39:07.520 --> 1:39:08.360\n Yeah, got it.\n\n1:39:08.360 --> 1:39:13.360\n So OpenCog on the one hand as a software framework\n\n1:39:14.840 --> 1:39:17.000\n could be used to implement a variety\n\n1:39:17.000 --> 1:39:21.840\n of different AI architectures and algorithms,\n\n1:39:21.840 --> 1:39:26.440\n but in practice, there's been a group of developers\n\n1:39:26.440 --> 1:39:29.440\n which I've been leading together with Linus Vepstas,\n\n1:39:29.440 --> 1:39:31.680\n Neil Geisweiler, and a few others,\n\n1:39:31.680 --> 1:39:35.080\n which have been using the OpenCog platform\n\n1:39:35.080 --> 1:39:39.440\n and infrastructure to implement certain ideas\n\n1:39:39.440 --> 1:39:41.280\n about how to make an AGI.\n\n1:39:41.280 --> 1:39:43.480\n So there's been a little bit of ambiguity\n\n1:39:43.480 --> 1:39:46.120\n about OpenCog, the software platform\n\n1:39:46.120 --> 1:39:49.360\n versus OpenCog, the AGI design,\n\n1:39:49.360 --> 1:39:52.160\n because in theory, you could use that software to do,\n\n1:39:52.160 --> 1:39:53.440\n you could use it to make a neural net.\n\n1:39:53.440 --> 1:39:55.880\n You could use it to make a lot of different AGI.\n\n1:39:55.880 --> 1:39:58.640\n What kind of stuff does the software platform provide,\n\n1:39:58.640 --> 1:40:00.760\n like in terms of utilities, tools, like what?\n\n1:40:00.760 --> 1:40:03.840\n Yeah, let me first tell about OpenCog\n\n1:40:03.840 --> 1:40:05.520\n as a software platform,\n\n1:40:05.520 --> 1:40:08.680\n and then I'll tell you the specific AGI R&D\n\n1:40:08.680 --> 1:40:10.760\n we've been building on top of it.\n\n1:40:12.240 --> 1:40:16.200\n So the core component of OpenCog as a software platform\n\n1:40:16.200 --> 1:40:17.920\n is what we call the atom space,\n\n1:40:17.920 --> 1:40:21.240\n which is a weighted labeled hypergraph.\n\n1:40:21.240 --> 1:40:22.880\n ATOM, atom space.\n\n1:40:22.880 --> 1:40:25.880\n Atom space, yeah, yeah, not atom, like Adam and Eve,\n\n1:40:25.880 --> 1:40:28.080\n although that would be cool too.\n\n1:40:28.080 --> 1:40:32.120\n Yeah, so you have a hypergraph, which is like,\n\n1:40:32.120 --> 1:40:35.360\n so a graph in this sense is a bunch of nodes\n\n1:40:35.360 --> 1:40:37.120\n with links between them.\n\n1:40:37.120 --> 1:40:40.960\n A hypergraph is like a graph,\n\n1:40:40.960 --> 1:40:43.960\n but links can go between more than two nodes.\n\n1:40:43.960 --> 1:40:45.520\n So you have a link between three nodes.\n\n1:40:45.520 --> 1:40:49.560\n And in fact, OpenCog's atom space\n\n1:40:49.560 --> 1:40:51.760\n would properly be called a metagraph\n\n1:40:51.760 --> 1:40:54.080\n because you can have links pointing to links,\n\n1:40:54.080 --> 1:40:56.840\n or you could have links pointing to whole subgraphs, right?\n\n1:40:56.840 --> 1:41:00.920\n So it's an extended hypergraph or a metagraph.\n\n1:41:00.920 --> 1:41:02.280\n Is metagraph a technical term?\n\n1:41:02.280 --> 1:41:03.640\n It is now a technical term.\n\n1:41:03.640 --> 1:41:04.480\n Interesting.\n\n1:41:04.480 --> 1:41:06.360\n But I don't think it was yet a technical term\n\n1:41:06.360 --> 1:41:10.080\n when we started calling this a generalized hypergraph.\n\n1:41:10.080 --> 1:41:13.400\n But in any case, it's a weighted labeled\n\n1:41:13.400 --> 1:41:16.920\n generalized hypergraph or weighted labeled metagraph.\n\n1:41:16.920 --> 1:41:19.200\n The weights and labels mean that the nodes and links\n\n1:41:19.200 --> 1:41:22.360\n can have numbers and symbols attached to them.\n\n1:41:22.360 --> 1:41:24.920\n So they can have types on them.\n\n1:41:24.920 --> 1:41:27.440\n They can have numbers on them that represent,\n\n1:41:27.440 --> 1:41:30.120\n say, a truth value or an importance value\n\n1:41:30.120 --> 1:41:32.000\n for a certain purpose.\n\n1:41:32.000 --> 1:41:33.240\n And of course, like with all things,\n\n1:41:33.240 --> 1:41:35.080\n you can reduce that to a hypergraph,\n\n1:41:35.080 --> 1:41:35.920\n and then the hypergraph can be reduced to a graph.\n\n1:41:35.920 --> 1:41:37.680\n You can reduce hypergraph to a graph,\n\n1:41:37.680 --> 1:41:39.880\n and you could reduce a graph to an adjacency matrix.\n\n1:41:39.880 --> 1:41:42.720\n So, I mean, there's always multiple representations.\n\n1:41:42.720 --> 1:41:44.000\n But there's a layer of representation\n\n1:41:44.000 --> 1:41:45.120\n that seems to work well here.\n\n1:41:45.120 --> 1:41:45.960\n Got it.\n\n1:41:45.960 --> 1:41:46.800\n Right, right, right.\n\n1:41:46.800 --> 1:41:51.800\n And so similarly, you could have a link to a whole graph\n\n1:41:52.080 --> 1:41:53.440\n because a whole graph could represent,\n\n1:41:53.440 --> 1:41:54.920\n say, a body of information.\n\n1:41:54.920 --> 1:41:58.640\n And I could say, I reject this body of information.\n\n1:41:58.640 --> 1:42:00.320\n Then one way to do that is make that link\n\n1:42:00.320 --> 1:42:02.000\n go to that whole subgraph representing\n\n1:42:02.000 --> 1:42:04.040\n the body of information, right?\n\n1:42:04.040 --> 1:42:07.200\n I mean, there are many alternate representations,\n\n1:42:07.200 --> 1:42:10.720\n but that's, anyway, what we have in OpenCOG,\n\n1:42:10.720 --> 1:42:13.160\n we have an atom space, which is this weighted, labeled,\n\n1:42:13.160 --> 1:42:15.080\n generalized hypergraph.\n\n1:42:15.080 --> 1:42:17.840\n Knowledge store, it lives in RAM.\n\n1:42:17.840 --> 1:42:20.120\n There's also a way to back it up to disk.\n\n1:42:20.120 --> 1:42:22.320\n There are ways to spread it among\n\n1:42:22.320 --> 1:42:24.120\n multiple different machines.\n\n1:42:24.120 --> 1:42:27.960\n Then there are various utilities for dealing with that.\n\n1:42:27.960 --> 1:42:29.800\n So there's a pattern matcher,\n\n1:42:29.800 --> 1:42:33.880\n which lets you specify a sort of abstract pattern\n\n1:42:33.880 --> 1:42:36.200\n and then search through a whole atom space\n\n1:42:36.200 --> 1:42:39.800\n with labeled hypergraph to see what subhypergraphs\n\n1:42:39.800 --> 1:42:42.880\n may match that pattern, for an example.\n\n1:42:42.880 --> 1:42:45.920\n So that's, then there's something called\n\n1:42:45.920 --> 1:42:48.760\n the COG server in OpenCOG,\n\n1:42:48.760 --> 1:42:52.560\n which lets you run a bunch of different agents\n\n1:42:52.560 --> 1:42:55.880\n or processes in a scheduler.\n\n1:42:55.880 --> 1:42:59.160\n And each of these agents, basically it reads stuff\n\n1:42:59.160 --> 1:43:01.880\n from the atom space and it writes stuff to the atom space.\n\n1:43:01.880 --> 1:43:05.640\n So this is sort of the basic operational model.\n\n1:43:05.640 --> 1:43:07.760\n That's the software framework.\n\n1:43:07.760 --> 1:43:10.360\n And of course that's, there's a lot there\n\n1:43:10.360 --> 1:43:13.200\n just from a scalable software engineering standpoint.\n\n1:43:13.200 --> 1:43:15.080\n So you could use this, I don't know if you've,\n\n1:43:15.080 --> 1:43:18.000\n have you looked into the Stephen Wolfram's physics project\n\n1:43:18.000 --> 1:43:20.160\n recently with the hypergraphs and stuff?\n\n1:43:20.160 --> 1:43:22.840\n Could you theoretically use like the software framework\n\n1:43:22.840 --> 1:43:23.800\n to play with it? You certainly could,\n\n1:43:23.800 --> 1:43:26.160\n although Wolfram would rather die\n\n1:43:26.160 --> 1:43:29.080\n than use anything but Mathematica for his work.\n\n1:43:29.080 --> 1:43:32.120\n Well that's, yeah, but there's a big community of people\n\n1:43:32.120 --> 1:43:36.080\n who are, you know, would love integration.\n\n1:43:36.080 --> 1:43:38.400\n Like you said, the young minds love the idea\n\n1:43:38.400 --> 1:43:40.440\n of integrating, of connecting things.\n\n1:43:40.440 --> 1:43:41.280\n Yeah, that's right.\n\n1:43:41.280 --> 1:43:42.840\n And I would add on that note,\n\n1:43:42.840 --> 1:43:46.600\n the idea of using hypergraph type models in physics\n\n1:43:46.600 --> 1:43:47.680\n is not very new.\n\n1:43:47.680 --> 1:43:49.120\n Like if you look at...\n\n1:43:49.120 --> 1:43:50.360\n The Russians did it first.\n\n1:43:50.360 --> 1:43:52.200\n Well, I'm sure they did.\n\n1:43:52.200 --> 1:43:55.880\n And a guy named Ben Dribis, who's a mathematician,\n\n1:43:55.880 --> 1:43:58.200\n a professor in Louisiana or somewhere,\n\n1:43:58.200 --> 1:44:01.960\n had a beautiful book on quantum sets and hypergraphs\n\n1:44:01.960 --> 1:44:05.520\n and algebraic topology for discrete models of physics.\n\n1:44:05.520 --> 1:44:09.080\n And carried it much farther than Wolfram has,\n\n1:44:09.080 --> 1:44:10.920\n but he's not rich and famous,\n\n1:44:10.920 --> 1:44:13.280\n so it didn't get in the headlines.\n\n1:44:13.280 --> 1:44:15.280\n But yeah, Wolfram aside, yeah,\n\n1:44:15.280 --> 1:44:17.120\n certainly that's a good way to put it.\n\n1:44:17.120 --> 1:44:19.280\n The whole OpenCog framework,\n\n1:44:19.280 --> 1:44:22.200\n you could use it to model biological networks\n\n1:44:22.200 --> 1:44:24.200\n and simulate biology processes.\n\n1:44:24.200 --> 1:44:26.480\n You could use it to model physics\n\n1:44:26.480 --> 1:44:30.160\n on discrete graph models of physics.\n\n1:44:30.160 --> 1:44:35.160\n So you could use it to do, say, biologically realistic\n\n1:44:36.840 --> 1:44:39.280\n neural networks, for example.\n\n1:44:39.280 --> 1:44:42.360\n And that's a framework.\n\n1:44:42.360 --> 1:44:44.240\n What do agents and processes do?\n\n1:44:44.240 --> 1:44:45.880\n Do they grow the graph?\n\n1:44:45.880 --> 1:44:48.200\n What kind of computations, just to get a sense,\n\n1:44:48.200 --> 1:44:49.040\n are they supposed to do?\n\n1:44:49.040 --> 1:44:51.200\n So in theory, they could do anything they want to do.\n\n1:44:51.200 --> 1:44:53.320\n They're just C++ processes.\n\n1:44:53.320 --> 1:44:56.880\n On the other hand, the computation framework\n\n1:44:56.880 --> 1:44:59.160\n is sort of designed for agents\n\n1:44:59.160 --> 1:45:02.000\n where most of their processing time\n\n1:45:02.000 --> 1:45:05.400\n is taken up with reads and writes to the atom space.\n\n1:45:05.400 --> 1:45:09.000\n And so that's a very different processing model\n\n1:45:09.000 --> 1:45:12.440\n than, say, the matrix multiplication based model\n\n1:45:12.440 --> 1:45:15.080\n as underlies most deep learning systems, right?\n\n1:45:15.080 --> 1:45:19.560\n So you could create an agent\n\n1:45:19.560 --> 1:45:22.720\n that just factored numbers for a billion years.\n\n1:45:22.720 --> 1:45:25.000\n It would run within the OpenCog platform,\n\n1:45:25.000 --> 1:45:26.600\n but it would be pointless, right?\n\n1:45:26.600 --> 1:45:28.880\n I mean, the point of doing OpenCog\n\n1:45:28.880 --> 1:45:30.520\n is because you want to make agents\n\n1:45:30.520 --> 1:45:33.160\n that are cooperating via reading and writing\n\n1:45:33.160 --> 1:45:36.400\n into this weighted labeled hypergraph, right?\n\n1:45:36.400 --> 1:45:41.400\n And that has both cognitive architecture importance\n\n1:45:41.560 --> 1:45:43.400\n because then this hypergraph is being used\n\n1:45:43.400 --> 1:45:46.040\n as a sort of shared memory\n\n1:45:46.040 --> 1:45:48.240\n among different cognitive processes,\n\n1:45:48.240 --> 1:45:51.000\n but it also has software and hardware\n\n1:45:51.000 --> 1:45:52.840\n implementation implications\n\n1:45:52.840 --> 1:45:54.840\n because current GPU architectures\n\n1:45:54.840 --> 1:45:57.120\n are not so useful for OpenCog,\n\n1:45:57.120 --> 1:46:01.200\n whereas a graph chip would be incredibly useful, right?\n\n1:46:01.200 --> 1:46:03.640\n And I think Graphcore has those now,\n\n1:46:03.640 --> 1:46:05.240\n but they're not ideally suited for this.\n\n1:46:05.240 --> 1:46:10.240\n But I think in the next, let's say, three to five years,\n\n1:46:10.640 --> 1:46:12.000\n we're gonna see new chips\n\n1:46:12.000 --> 1:46:14.680\n where like a graph is put on the chip\n\n1:46:14.680 --> 1:46:19.320\n and the back and forth between multiple processes\n\n1:46:19.320 --> 1:46:23.600\n acting SIMD and MIMD on that graph is gonna be fast.\n\n1:46:23.600 --> 1:46:26.480\n And then that may do for OpenCog type architectures\n\n1:46:26.480 --> 1:46:29.840\n what GPUs did for deep neural architecture.\n\n1:46:29.840 --> 1:46:31.320\n It's a small tangent.\n\n1:46:31.320 --> 1:46:34.600\n Can you comment on thoughts about neuromorphic computing?\n\n1:46:34.600 --> 1:46:36.400\n So like hardware implementations\n\n1:46:36.400 --> 1:46:39.360\n of all these different kind of, are you interested?\n\n1:46:39.360 --> 1:46:41.000\n Are you excited by that possibility?\n\n1:46:41.000 --> 1:46:42.680\n I'm excited by graph processors\n\n1:46:42.680 --> 1:46:46.440\n because I think they can massively speed up OpenCog,\n\n1:46:46.440 --> 1:46:50.680\n which is a class of architectures that I'm working on.\n\n1:46:50.680 --> 1:46:56.680\n I think if, you know, in principle, neuromorphic computing\n\n1:46:57.240 --> 1:46:58.760\n should be amazing.\n\n1:46:58.760 --> 1:47:00.480\n I haven't yet been fully sold\n\n1:47:00.480 --> 1:47:03.320\n on any of the systems that are out.\n\n1:47:03.320 --> 1:47:06.400\n They're like, memristors should be amazing too, right?\n\n1:47:06.400 --> 1:47:09.400\n So a lot of these things have obvious potential,\n\n1:47:09.400 --> 1:47:11.360\n but I haven't yet put my hands on a system\n\n1:47:11.360 --> 1:47:13.280\n that seemed to manifest that.\n\n1:47:13.280 --> 1:47:14.880\n Mark's system should be amazing,\n\n1:47:14.880 --> 1:47:17.880\n but the current systems have not been great.\n\n1:47:17.880 --> 1:47:19.640\n Yeah, I mean, look, for example,\n\n1:47:19.640 --> 1:47:23.960\n if you wanted to make a biologically realistic\n\n1:47:23.960 --> 1:47:25.680\n hardware neural network,\n\n1:47:25.680 --> 1:47:30.680\n like making a circuit in hardware\n\n1:47:31.520 --> 1:47:34.360\n that emulated like the Hodgkin\u2013Huxley equation\n\n1:47:34.360 --> 1:47:35.640\n or the Izhekevich equation,\n\n1:47:35.640 --> 1:47:38.240\n like differential equations\n\n1:47:38.240 --> 1:47:40.680\n for a biologically realistic neuron\n\n1:47:40.680 --> 1:47:43.800\n and putting that in hardware on the chip,\n\n1:47:43.800 --> 1:47:46.360\n that would seem that it would make more feasible\n\n1:47:46.360 --> 1:47:50.320\n to make a large scale, truly biologically realistic\n\n1:47:50.320 --> 1:47:51.160\n neural network.\n\n1:47:51.160 --> 1:47:54.320\n Now, what's been done so far is not like that.\n\n1:47:54.320 --> 1:47:57.120\n So I guess personally, as a researcher,\n\n1:47:57.120 --> 1:48:02.120\n I mean, I've done a bunch of work in computational neuroscience\n\n1:48:02.480 --> 1:48:05.600\n where I did some work with IARPA in DC,\n\n1:48:05.600 --> 1:48:08.240\n Intelligence Advanced Research Project Agency.\n\n1:48:08.240 --> 1:48:10.880\n We were looking at how do you make\n\n1:48:10.880 --> 1:48:13.000\n a biologically realistic simulation\n\n1:48:13.000 --> 1:48:15.720\n of seven different parts of the brain\n\n1:48:15.720 --> 1:48:17.080\n cooperating with each other,\n\n1:48:17.080 --> 1:48:20.440\n using like realistic nonlinear dynamical models of neurons,\n\n1:48:20.440 --> 1:48:21.920\n and how do you get that to simulate\n\n1:48:21.920 --> 1:48:24.800\n what's going on in the mind of a geo intelligence analyst\n\n1:48:24.800 --> 1:48:27.160\n while they're trying to find terrorists on a map, right?\n\n1:48:27.160 --> 1:48:29.880\n So if you want to do something like that,\n\n1:48:29.880 --> 1:48:34.080\n having neuromorphic hardware that really let you simulate\n\n1:48:34.080 --> 1:48:38.720\n like a realistic model of the neuron would be amazing.\n\n1:48:38.720 --> 1:48:42.280\n But that's sort of with my computational neuroscience\n\n1:48:42.280 --> 1:48:43.120\n hat on, right?\n\n1:48:43.120 --> 1:48:47.160\n With an AGI hat on, I'm just more interested\n\n1:48:47.160 --> 1:48:50.200\n in these hypergraph knowledge representation\n\n1:48:50.200 --> 1:48:54.480\n based architectures, which would benefit more\n\n1:48:54.480 --> 1:48:57.720\n from various types of graph processors\n\n1:48:57.720 --> 1:49:00.480\n because the main processing bottleneck\n\n1:49:00.480 --> 1:49:02.000\n is reading writing to RAM.\n\n1:49:02.000 --> 1:49:03.960\n It's reading writing to the graph in RAM.\n\n1:49:03.960 --> 1:49:06.120\n The main processing bottleneck for this kind of\n\n1:49:06.120 --> 1:49:09.840\n proto AGI architecture is not multiplying matrices.\n\n1:49:09.840 --> 1:49:13.280\n And for that reason, GPUs, which are really good\n\n1:49:13.280 --> 1:49:17.520\n at multiplying matrices, don't apply as well.\n\n1:49:17.520 --> 1:49:20.240\n There are frameworks like Gunrock and others\n\n1:49:20.240 --> 1:49:22.160\n that try to boil down graph processing\n\n1:49:22.160 --> 1:49:24.640\n to matrix operations, and they're cool,\n\n1:49:24.640 --> 1:49:26.160\n but you're still putting a square peg\n\n1:49:26.160 --> 1:49:28.800\n into a round hole in a certain way.\n\n1:49:28.800 --> 1:49:32.760\n The same is true, I mean, current quantum machine learning,\n\n1:49:32.760 --> 1:49:34.240\n which is very cool.\n\n1:49:34.240 --> 1:49:37.320\n It's also all about how to get matrix and vector operations\n\n1:49:37.320 --> 1:49:41.280\n in quantum mechanics, and I see why that's natural to do.\n\n1:49:41.280 --> 1:49:44.240\n I mean, quantum mechanics is all unitary matrices\n\n1:49:44.240 --> 1:49:45.800\n and vectors, right?\n\n1:49:45.800 --> 1:49:48.040\n On the other hand, you could also try\n\n1:49:48.040 --> 1:49:50.760\n to make graph centric quantum computers,\n\n1:49:50.760 --> 1:49:54.400\n which I think is where things will go.\n\n1:49:54.400 --> 1:49:57.080\n And then we can have, then we can make,\n\n1:49:57.080 --> 1:50:00.120\n like take the open cog implementation layer,\n\n1:50:00.120 --> 1:50:04.000\n implement it in a collapsed state inside a quantum computer.\n\n1:50:04.000 --> 1:50:06.480\n But that may be the singularity squared, right?\n\n1:50:06.480 --> 1:50:11.480\n I'm not sure we need that to get to human level.\n\n1:50:12.360 --> 1:50:14.680\n That's already beyond the first singularity.\n\n1:50:14.680 --> 1:50:17.640\n But can we just go back to open cog?\n\n1:50:17.640 --> 1:50:20.040\n Yeah, and the hypergraph and open cog.\n\n1:50:20.040 --> 1:50:21.640\n That's the software framework, right?\n\n1:50:21.640 --> 1:50:25.440\n So the next thing is our cognitive architecture\n\n1:50:25.440 --> 1:50:27.960\n tells us particular algorithms to put there.\n\n1:50:27.960 --> 1:50:28.800\n Got it.\n\n1:50:28.800 --> 1:50:33.720\n Can we backtrack on the kind of, is this graph designed,\n\n1:50:33.720 --> 1:50:37.680\n is it in general supposed to be sparse\n\n1:50:37.680 --> 1:50:40.640\n and the operations constantly grow and change the graph?\n\n1:50:40.640 --> 1:50:42.320\n Yeah, the graph is sparse.\n\n1:50:42.320 --> 1:50:45.040\n But is it constantly adding links and so on?\n\n1:50:45.040 --> 1:50:47.200\n It is a self modifying hypergraph.\n\n1:50:47.200 --> 1:50:49.800\n So it's not, so the write and read operations\n\n1:50:49.800 --> 1:50:53.040\n you're referring to, this isn't just a fixed graph\n\n1:50:53.040 --> 1:50:55.840\n to which you change the way, it's a constantly growing graph.\n\n1:50:55.840 --> 1:50:58.000\n Yeah, that's true.\n\n1:50:58.000 --> 1:51:03.000\n So it is different model than,\n\n1:51:03.000 --> 1:51:04.680\n say current deep neural nets\n\n1:51:04.680 --> 1:51:06.840\n and have a fixed neural architecture\n\n1:51:06.840 --> 1:51:08.600\n and you're updating the weights.\n\n1:51:08.600 --> 1:51:10.880\n Although there have been like cascade correlational\n\n1:51:10.880 --> 1:51:13.920\n neural net architectures that grow new nodes and links,\n\n1:51:13.920 --> 1:51:16.640\n but the most common neural architectures now\n\n1:51:16.640 --> 1:51:17.960\n have a fixed neural architecture,\n\n1:51:17.960 --> 1:51:19.080\n you're updating the weights.\n\n1:51:19.080 --> 1:51:22.520\n And then open cog, you can update the weights\n\n1:51:22.520 --> 1:51:24.760\n and that certainly happens a lot,\n\n1:51:24.760 --> 1:51:28.200\n but adding new nodes, adding new links,\n\n1:51:28.200 --> 1:51:30.720\n removing nodes and links is an equally critical part\n\n1:51:30.720 --> 1:51:32.160\n of the system's operations.\n\n1:51:32.160 --> 1:51:33.000\n Got it.\n\n1:51:33.000 --> 1:51:37.040\n So now when you start to add these cognitive algorithms\n\n1:51:37.040 --> 1:51:39.840\n on top of this open cog architecture,\n\n1:51:39.840 --> 1:51:41.280\n what does that look like?\n\n1:51:41.280 --> 1:51:44.800\n Yeah, so within this framework then,\n\n1:51:44.800 --> 1:51:48.040\n creating a cognitive architecture is basically two things.\n\n1:51:48.040 --> 1:51:52.080\n It's choosing what type system you wanna put\n\n1:51:52.080 --> 1:51:53.800\n on the nodes and links in the hypergraph,\n\n1:51:53.800 --> 1:51:56.120\n what types of nodes and links you want.\n\n1:51:56.120 --> 1:52:01.000\n And then it's choosing what collection of agents,\n\n1:52:01.000 --> 1:52:04.640\n what collection of AI algorithms or processes\n\n1:52:04.640 --> 1:52:08.040\n are gonna run to operate on this hypergraph.\n\n1:52:08.040 --> 1:52:10.520\n And of course those two decisions\n\n1:52:10.520 --> 1:52:13.920\n are closely connected to each other.\n\n1:52:13.920 --> 1:52:17.480\n So in terms of the type system,\n\n1:52:17.480 --> 1:52:19.920\n there are some links that are more neural net like,\n\n1:52:19.920 --> 1:52:22.360\n they're just like have weights to get updated\n\n1:52:22.360 --> 1:52:26.000\n by heavy and learning and activation spreads along them.\n\n1:52:26.000 --> 1:52:29.080\n There are other links that are more logic like\n\n1:52:29.080 --> 1:52:30.520\n and nodes that are more logic like.\n\n1:52:30.520 --> 1:52:32.240\n So you could have a variable node\n\n1:52:32.240 --> 1:52:34.240\n and you can have a node representing a universal\n\n1:52:34.240 --> 1:52:37.680\n or existential quantifier as in predicate logic\n\n1:52:37.680 --> 1:52:39.160\n or term logic.\n\n1:52:39.160 --> 1:52:42.080\n So you can have logic like nodes and links,\n\n1:52:42.080 --> 1:52:44.440\n or you can have neural like nodes and links.\n\n1:52:44.440 --> 1:52:47.400\n You can also have procedure like nodes and links\n\n1:52:47.400 --> 1:52:51.960\n as in say a combinatorial logic or Lambda calculus\n\n1:52:51.960 --> 1:52:53.680\n representing programs.\n\n1:52:53.680 --> 1:52:56.520\n So you can have nodes and links representing\n\n1:52:56.520 --> 1:52:58.640\n many different types of semantics,\n\n1:52:58.640 --> 1:53:00.840\n which means you could make a horrible ugly mess\n\n1:53:00.840 --> 1:53:02.800\n or you could make a system\n\n1:53:02.800 --> 1:53:04.280\n where these different types of knowledge\n\n1:53:04.280 --> 1:53:06.840\n all interpenetrate and synergize\n\n1:53:06.840 --> 1:53:08.960\n with each other beautifully, right?\n\n1:53:08.960 --> 1:53:12.800\n So the hypergraph can contain programs.\n\n1:53:12.800 --> 1:53:14.440\n Yeah, it can contain programs,\n\n1:53:14.440 --> 1:53:17.960\n although in the current version,\n\n1:53:17.960 --> 1:53:19.760\n it is a very inefficient way\n\n1:53:19.760 --> 1:53:21.960\n to guide the execution of programs,\n\n1:53:21.960 --> 1:53:25.000\n which is one thing that we are aiming to resolve\n\n1:53:25.000 --> 1:53:27.520\n with our rewrite of the system now.\n\n1:53:27.520 --> 1:53:32.520\n So what to you is the most beautiful aspect of OpenCog?\n\n1:53:32.720 --> 1:53:34.600\n Just to you personally,\n\n1:53:34.600 --> 1:53:38.080\n some aspect that captivates your imagination\n\n1:53:38.080 --> 1:53:40.960\n from beauty or power?\n\n1:53:42.000 --> 1:53:47.000\n What fascinates me is finding a common representation\n\n1:53:48.320 --> 1:53:53.320\n that underlies abstract, declarative knowledge\n\n1:53:53.320 --> 1:53:57.320\n and sensory knowledge and movement knowledge\n\n1:53:57.320 --> 1:54:00.760\n and procedural knowledge and episodic knowledge,\n\n1:54:00.760 --> 1:54:03.960\n finding the right level of representation\n\n1:54:03.960 --> 1:54:06.560\n where all these types of knowledge are stored\n\n1:54:06.560 --> 1:54:10.560\n in a sort of universal and interconvertible\n\n1:54:10.560 --> 1:54:13.440\n yet practically manipulable way, right?\n\n1:54:13.440 --> 1:54:16.840\n So to me, that's the core,\n\n1:54:16.840 --> 1:54:18.640\n because once you've done that,\n\n1:54:18.640 --> 1:54:20.800\n then the different learning algorithms\n\n1:54:20.800 --> 1:54:23.640\n can help each other out. Like what you want is,\n\n1:54:23.640 --> 1:54:25.120\n if you have a logic engine\n\n1:54:25.120 --> 1:54:26.840\n that helps with declarative knowledge\n\n1:54:26.840 --> 1:54:28.040\n and you have a deep neural net\n\n1:54:28.040 --> 1:54:29.960\n that gathers perceptual knowledge,\n\n1:54:29.960 --> 1:54:32.400\n and you have, say, an evolutionary learning system\n\n1:54:32.400 --> 1:54:34.120\n that learns procedures,\n\n1:54:34.120 --> 1:54:36.600\n you want these to not only interact\n\n1:54:36.600 --> 1:54:38.880\n on the level of sharing results\n\n1:54:38.880 --> 1:54:41.120\n and passing inputs and outputs to each other,\n\n1:54:41.120 --> 1:54:43.680\n you want the logic engine, when it gets stuck,\n\n1:54:43.680 --> 1:54:46.240\n to be able to share its intermediate state\n\n1:54:46.240 --> 1:54:49.360\n with the neural net and with the evolutionary system\n\n1:54:49.360 --> 1:54:52.240\n and with the evolutionary learning algorithm\n\n1:54:52.240 --> 1:54:55.440\n so that they can help each other out of bottlenecks\n\n1:54:55.440 --> 1:54:58.320\n and help each other solve combinatorial explosions\n\n1:54:58.320 --> 1:55:02.040\n by intervening inside each other's cognitive processes.\n\n1:55:02.040 --> 1:55:03.520\n But that can only be done\n\n1:55:03.520 --> 1:55:05.960\n if the intermediate state of a logic engine,\n\n1:55:05.960 --> 1:55:07.400\n the evolutionary learning engine,\n\n1:55:07.400 --> 1:55:11.120\n and a deep neural net are represented in the same form.\n\n1:55:11.120 --> 1:55:13.120\n And that's what we figured out how to do\n\n1:55:13.120 --> 1:55:14.800\n by putting the right type system\n\n1:55:14.800 --> 1:55:17.040\n on top of this weighted labeled hypergraph.\n\n1:55:17.040 --> 1:55:19.680\n So is there, can you maybe elaborate\n\n1:55:19.680 --> 1:55:21.880\n on what are the different characteristics\n\n1:55:21.880 --> 1:55:26.520\n of a type system that can coexist\n\n1:55:26.520 --> 1:55:28.760\n amongst all these different kinds of knowledge\n\n1:55:28.760 --> 1:55:30.080\n that needs to be represented?\n\n1:55:30.080 --> 1:55:33.320\n And is, I mean, like, is it hierarchical?\n\n1:55:34.280 --> 1:55:36.720\n Just any kind of insights you can give\n\n1:55:36.720 --> 1:55:37.840\n on that kind of type system?\n\n1:55:37.840 --> 1:55:41.680\n Yeah, yeah, so this gets very nitty gritty\n\n1:55:41.680 --> 1:55:44.000\n and mathematical, of course,\n\n1:55:44.000 --> 1:55:47.200\n but one key part is switching\n\n1:55:47.200 --> 1:55:50.440\n from predicate logic to term logic.\n\n1:55:50.440 --> 1:55:51.640\n What is predicate logic?\n\n1:55:51.640 --> 1:55:53.200\n What is term logic?\n\n1:55:53.200 --> 1:55:56.080\n So term logic was invented by Aristotle,\n\n1:55:56.080 --> 1:56:01.080\n or at least that's the oldest recollection we have of it.\n\n1:56:01.320 --> 1:56:05.280\n But term logic breaks down basic logic\n\n1:56:05.280 --> 1:56:07.480\n into basically simple links between nodes,\n\n1:56:07.480 --> 1:56:12.480\n like an inheritance link between node A and node B.\n\n1:56:12.480 --> 1:56:16.280\n So in term logic, the basic deduction operation\n\n1:56:16.280 --> 1:56:21.080\n is A implies B, B implies C, therefore A implies C.\n\n1:56:21.080 --> 1:56:22.600\n Whereas in predicate logic,\n\n1:56:22.600 --> 1:56:24.520\n the basic operation is modus ponens,\n\n1:56:24.520 --> 1:56:27.680\n like A implies B, therefore B.\n\n1:56:27.680 --> 1:56:31.440\n So it's a slightly different way of breaking down logic,\n\n1:56:31.440 --> 1:56:35.320\n but by breaking down logic into term logic,\n\n1:56:35.320 --> 1:56:37.440\n you get a nice way of breaking logic down\n\n1:56:37.440 --> 1:56:40.120\n into nodes and links.\n\n1:56:40.120 --> 1:56:42.960\n So your concepts can become nodes,\n\n1:56:42.960 --> 1:56:45.200\n the logical relations become links.\n\n1:56:45.200 --> 1:56:46.640\n And so then inference is like,\n\n1:56:46.640 --> 1:56:48.720\n so if this link is A implies B,\n\n1:56:48.720 --> 1:56:50.840\n this link is B implies C,\n\n1:56:50.840 --> 1:56:53.360\n then deduction builds a link A implies C.\n\n1:56:53.360 --> 1:56:54.920\n And your probabilistic algorithm\n\n1:56:54.920 --> 1:56:57.440\n can assign a certain weight there.\n\n1:56:57.440 --> 1:57:00.040\n Now, you may also have like a Hebbian neural link\n\n1:57:00.040 --> 1:57:03.600\n from A to C, which is the degree to which thinking,\n\n1:57:03.600 --> 1:57:06.640\n the degree to which A being the focus of attention\n\n1:57:06.640 --> 1:57:09.080\n should make B the focus of attention, right?\n\n1:57:09.080 --> 1:57:10.880\n So you could have then a neural link\n\n1:57:10.880 --> 1:57:13.720\n and you could have a symbolic,\n\n1:57:13.720 --> 1:57:17.000\n like logical inheritance link in your term logic.\n\n1:57:17.000 --> 1:57:19.520\n And they have separate meaning,\n\n1:57:19.520 --> 1:57:22.960\n but they could be used to guide each other as well.\n\n1:57:22.960 --> 1:57:26.720\n Like if there's a large amount of neural weight\n\n1:57:26.720 --> 1:57:28.400\n on the link between A and B,\n\n1:57:28.400 --> 1:57:30.440\n that may direct your logic engine to think about,\n\n1:57:30.440 --> 1:57:31.320\n well, what is the relation?\n\n1:57:31.320 --> 1:57:32.160\n Are they similar?\n\n1:57:32.160 --> 1:57:33.880\n Is there an inheritance relation?\n\n1:57:33.880 --> 1:57:37.400\n Are they similar in some context?\n\n1:57:37.400 --> 1:57:39.920\n On the other hand, if there's a logical relation\n\n1:57:39.920 --> 1:57:43.360\n between A and B, that may direct your neural component\n\n1:57:43.360 --> 1:57:45.520\n to think, well, when I'm thinking about A,\n\n1:57:45.520 --> 1:57:48.240\n should I be directing some attention to B also?\n\n1:57:48.240 --> 1:57:50.160\n Because there's a logical relation.\n\n1:57:50.160 --> 1:57:53.000\n So in terms of logic,\n\n1:57:53.000 --> 1:57:54.320\n there's a lot of thought that went into\n\n1:57:54.320 --> 1:57:58.280\n how do you break down logic relations,\n\n1:57:58.280 --> 1:58:02.320\n including basic sort of propositional logic relations\n\n1:58:02.320 --> 1:58:04.160\n as Aristotelian term logic deals with,\n\n1:58:04.160 --> 1:58:07.080\n and then quantifier logic relations also.\n\n1:58:07.080 --> 1:58:10.920\n How do you break those down elegantly into a hypergraph?\n\n1:58:10.920 --> 1:58:13.480\n Because you, I mean, you can boil logic expression\n\n1:58:13.480 --> 1:58:14.840\n into a graph in many different ways.\n\n1:58:14.840 --> 1:58:16.680\n Many of them are very ugly, right?\n\n1:58:16.680 --> 1:58:19.200\n We tried to find elegant ways\n\n1:58:19.200 --> 1:58:22.600\n of sort of hierarchically breaking down\n\n1:58:22.600 --> 1:58:26.880\n complex logic expression into nodes and links.\n\n1:58:26.880 --> 1:58:31.400\n So that if you have say different nodes representing,\n\n1:58:31.400 --> 1:58:34.200\n Ben, AI, Lex, interview or whatever,\n\n1:58:34.200 --> 1:58:36.800\n the logic relations between those things\n\n1:58:36.800 --> 1:58:40.480\n are compact in the node and link representation.\n\n1:58:40.480 --> 1:58:42.080\n So that when you have a neural net acting\n\n1:58:42.080 --> 1:58:43.960\n on the same nodes and links,\n\n1:58:43.960 --> 1:58:45.760\n the neural net and the logic engine\n\n1:58:45.760 --> 1:58:48.240\n can sort of interoperate with each other.\n\n1:58:48.240 --> 1:58:49.920\n And also interpretable by humans.\n\n1:58:49.920 --> 1:58:51.400\n Is that an important?\n\n1:58:51.400 --> 1:58:52.240\n That's tough.\n\n1:58:52.240 --> 1:58:54.600\n Yeah, in simple cases, it's interpretable by humans.\n\n1:58:54.600 --> 1:58:59.600\n But honestly, I would say logic systems\n\n1:58:59.600 --> 1:59:04.600\n I would say logic systems give more potential\n\n1:59:05.440 --> 1:59:09.800\n for transparency and comprehensibility\n\n1:59:09.800 --> 1:59:11.640\n than neural net systems,\n\n1:59:11.640 --> 1:59:12.840\n but you still have to work at it.\n\n1:59:12.840 --> 1:59:16.680\n Because I mean, if I show you a predicate logic proposition\n\n1:59:16.680 --> 1:59:20.080\n with like 500 nested universal and existential quantifiers\n\n1:59:20.080 --> 1:59:23.680\n and 217 variables, that's no more comprehensible\n\n1:59:23.680 --> 1:59:26.560\n than the weight metrics of a neural network, right?\n\n1:59:26.560 --> 1:59:28.560\n So I'd say the logic expressions\n\n1:59:28.560 --> 1:59:30.920\n that AI learns from its experience\n\n1:59:30.920 --> 1:59:33.440\n are mostly totally opaque to human beings\n\n1:59:33.440 --> 1:59:36.200\n and maybe even harder to understand than neural net.\n\n1:59:36.200 --> 1:59:37.440\n Because I mean, when you have multiple\n\n1:59:37.440 --> 1:59:38.960\n nested quantifier bindings,\n\n1:59:38.960 --> 1:59:41.520\n it's a very high level of abstraction.\n\n1:59:41.520 --> 1:59:42.680\n There is a difference though,\n\n1:59:42.680 --> 1:59:46.880\n in that within logic, it's a little more straightforward\n\n1:59:46.880 --> 1:59:49.120\n to pose the problem of like normalize this\n\n1:59:49.120 --> 1:59:51.080\n and boil this down to a certain form.\n\n1:59:51.080 --> 1:59:52.720\n I mean, you can do that in neural nets too.\n\n1:59:52.720 --> 1:59:55.680\n Like you can distill a neural net to a simpler form,\n\n1:59:55.680 --> 1:59:57.280\n but that's more often done to make a neural net\n\n1:59:57.280 --> 1:59:59.720\n that'll run on an embedded device or something.\n\n1:59:59.720 --> 2:00:03.440\n It's harder to distill a net to a comprehensible form\n\n2:00:03.440 --> 2:00:05.640\n than it is to simplify a logic expression\n\n2:00:05.640 --> 2:00:08.600\n to a comprehensible form, but it doesn't come for free.\n\n2:00:08.600 --> 2:00:13.040\n Like what's in the AI's mind is incomprehensible\n\n2:00:13.040 --> 2:00:15.720\n to a human unless you do some special work\n\n2:00:15.720 --> 2:00:16.880\n to make it comprehensible.\n\n2:00:16.880 --> 2:00:20.400\n So on the procedural side, there's some different\n\n2:00:20.400 --> 2:00:23.000\n and sort of interesting voodoo there.\n\n2:00:23.000 --> 2:00:25.800\n I mean, if you're familiar in computer science,\n\n2:00:25.800 --> 2:00:27.800\n there's something called the Curry Howard correspondence,\n\n2:00:27.800 --> 2:00:30.920\n which is a one to one mapping between proofs and programs.\n\n2:00:30.920 --> 2:00:33.520\n So every program can be mapped into a proof.\n\n2:00:33.520 --> 2:00:35.960\n Every proof can be mapped into a program.\n\n2:00:35.960 --> 2:00:37.800\n You can model this using category theory\n\n2:00:37.800 --> 2:00:40.960\n and a bunch of nice math,\n\n2:00:40.960 --> 2:00:43.280\n but we wanna make that practical, right?\n\n2:00:43.280 --> 2:00:46.520\n So that if you have an executable program\n\n2:00:46.520 --> 2:00:49.960\n that like moves the robot's arm or figures out\n\n2:00:49.960 --> 2:00:51.840\n in what order to say things in a dialogue,\n\n2:00:51.840 --> 2:00:55.840\n that's a procedure represented in OpenCog's hypergraph.\n\n2:00:55.840 --> 2:01:00.120\n But if you wanna reason on how to improve that procedure,\n\n2:01:00.120 --> 2:01:03.080\n you need to map that procedure into logic\n\n2:01:03.080 --> 2:01:05.520\n using Curry Howard isomorphism.\n\n2:01:05.520 --> 2:01:09.320\n So then the logic engine can reason\n\n2:01:09.320 --> 2:01:11.120\n about how to improve that procedure\n\n2:01:11.120 --> 2:01:14.080\n and then map that back into the procedural representation\n\n2:01:14.080 --> 2:01:16.160\n that is efficient for execution.\n\n2:01:16.160 --> 2:01:18.800\n So again, that comes down to not just\n\n2:01:18.800 --> 2:01:21.440\n can you make your procedure into a bunch of nodes and links?\n\n2:01:21.440 --> 2:01:23.280\n Cause I mean, that can be done trivially.\n\n2:01:23.280 --> 2:01:26.440\n A C++ compiler has nodes and links inside it.\n\n2:01:26.440 --> 2:01:27.960\n Can you boil down your procedure\n\n2:01:27.960 --> 2:01:29.840\n into a bunch of nodes and links\n\n2:01:29.840 --> 2:01:32.560\n in a way that's like hierarchically decomposed\n\n2:01:32.560 --> 2:01:33.680\n and simple enough?\n\n2:01:33.680 --> 2:01:34.520\n It can reason about.\n\n2:01:34.520 --> 2:01:37.040\n Yeah, yeah, that given the resource constraints at hand,\n\n2:01:37.040 --> 2:01:40.920\n you can map it back and forth to your term logic,\n\n2:01:40.920 --> 2:01:42.080\n like fast enough\n\n2:01:42.080 --> 2:01:45.200\n and without having a bloated logic expression, right?\n\n2:01:45.200 --> 2:01:47.000\n So there's just a lot of,\n\n2:01:48.320 --> 2:01:50.360\n there's a lot of nitty gritty particulars there,\n\n2:01:50.360 --> 2:01:54.520\n but by the same token, if you ask a chip designer,\n\n2:01:54.520 --> 2:01:58.560\n like how do you make the Intel I7 chip so good?\n\n2:01:58.560 --> 2:02:02.560\n There's a long list of technical answers there,\n\n2:02:02.560 --> 2:02:04.800\n which will take a while to go through, right?\n\n2:02:04.800 --> 2:02:06.640\n And this has been decades of work.\n\n2:02:06.640 --> 2:02:10.880\n I mean, the first AI system of this nature I tried to build\n\n2:02:10.880 --> 2:02:13.440\n was called WebMind in the mid 1990s.\n\n2:02:13.440 --> 2:02:15.600\n And we had a big graph,\n\n2:02:15.600 --> 2:02:18.880\n a big graph operating in RAM implemented with Java 1.1,\n\n2:02:18.880 --> 2:02:21.800\n which was a terrible, terrible implementation idea.\n\n2:02:21.800 --> 2:02:25.960\n And then each node had its own processing.\n\n2:02:25.960 --> 2:02:27.440\n So like that there,\n\n2:02:27.440 --> 2:02:29.560\n the core loop looped through all nodes in the network\n\n2:02:29.560 --> 2:02:32.920\n and let each node enact what its little thing was doing.\n\n2:02:32.920 --> 2:02:35.880\n And we had logic and neural nets in there,\n\n2:02:35.880 --> 2:02:38.400\n but an evolutionary learning,\n\n2:02:38.400 --> 2:02:40.760\n but we hadn't done enough of the math\n\n2:02:40.760 --> 2:02:43.400\n to get them to operate together very cleanly.\n\n2:02:43.400 --> 2:02:46.240\n So it was really, it was quite a horrible mess.\n\n2:02:46.240 --> 2:02:49.400\n So as well as shifting an implementation\n\n2:02:49.400 --> 2:02:51.840\n where the graph is its own object\n\n2:02:51.840 --> 2:02:54.720\n and the agents are separately scheduled,\n\n2:02:54.720 --> 2:02:56.800\n we've also done a lot of work\n\n2:02:56.800 --> 2:02:58.400\n on how do you represent programs?\n\n2:02:58.400 --> 2:03:00.800\n How do you represent procedures?\n\n2:03:00.800 --> 2:03:03.640\n You know, how do you represent genotypes for evolution\n\n2:03:03.640 --> 2:03:06.640\n in a way that the interoperability\n\n2:03:06.640 --> 2:03:09.000\n between the different types of learning\n\n2:03:09.000 --> 2:03:11.720\n associated with these different types of knowledge\n\n2:03:11.720 --> 2:03:13.040\n actually works?\n\n2:03:13.040 --> 2:03:14.960\n And that's been quite difficult.\n\n2:03:14.960 --> 2:03:18.600\n It's taken decades and it's totally off to the side\n\n2:03:18.600 --> 2:03:23.080\n of what the commercial mainstream of the AI field is doing,\n\n2:03:23.080 --> 2:03:27.640\n which isn't thinking about representation at all really.\n\n2:03:27.640 --> 2:03:30.800\n Although you could see like in the DNC,\n\n2:03:30.800 --> 2:03:32.320\n they had to think a little bit about\n\n2:03:32.320 --> 2:03:33.880\n how do you make representation of a map\n\n2:03:33.880 --> 2:03:36.680\n in this memory matrix work together\n\n2:03:36.680 --> 2:03:38.160\n with the representation needed\n\n2:03:38.160 --> 2:03:40.240\n for say visual pattern recognition\n\n2:03:40.240 --> 2:03:42.120\n in the hierarchical neural network.\n\n2:03:42.120 --> 2:03:45.120\n But I would say we have taken that direction\n\n2:03:45.120 --> 2:03:47.920\n of taking the types of knowledge you need\n\n2:03:47.920 --> 2:03:49.120\n for different types of learning,\n\n2:03:49.120 --> 2:03:52.040\n like declarative, procedural, attentional,\n\n2:03:52.040 --> 2:03:55.520\n and how do you make these types of knowledge represent\n\n2:03:55.520 --> 2:03:58.160\n in a way that allows cross learning\n\n2:03:58.160 --> 2:04:00.200\n across these different types of memory.\n\n2:04:00.200 --> 2:04:03.920\n We've been prototyping and experimenting with this\n\n2:04:03.920 --> 2:04:07.560\n within OpenCog and before that WebMind\n\n2:04:07.560 --> 2:04:10.640\n since the mid 1990s.\n\n2:04:10.640 --> 2:04:13.840\n Now, disappointingly to all of us,\n\n2:04:13.840 --> 2:04:18.400\n this has not yet been cashed out in an AGI system, right?\n\n2:04:18.400 --> 2:04:20.640\n I mean, we've used this system\n\n2:04:20.640 --> 2:04:22.440\n within our consulting business.\n\n2:04:22.440 --> 2:04:24.320\n So we've built natural language processing\n\n2:04:24.320 --> 2:04:27.760\n and robot control and financial analysis.\n\n2:04:27.760 --> 2:04:31.160\n We've built a bunch of sort of vertical market specific\n\n2:04:31.160 --> 2:04:33.600\n proprietary AI projects.\n\n2:04:33.600 --> 2:04:36.720\n They use OpenCog on the backend,\n\n2:04:36.720 --> 2:04:39.560\n but we haven't, that's not the AGI goal, right?\n\n2:04:39.560 --> 2:04:42.680\n It's interesting, but it's not the AGI goal.\n\n2:04:42.680 --> 2:04:47.680\n So now what we're looking at with our rebuild of the system.\n\n2:04:48.520 --> 2:04:49.360\n 2.0.\n\n2:04:49.360 --> 2:04:51.400\n Yeah, we're also calling it True AGI.\n\n2:04:51.400 --> 2:04:54.800\n So we're not quite sure what the name is yet.\n\n2:04:54.800 --> 2:04:57.480\n We made a website for trueagi.io,\n\n2:04:57.480 --> 2:04:59.840\n but we haven't put anything on there yet.\n\n2:04:59.840 --> 2:05:02.160\n We may come up with an even better name.\n\n2:05:02.160 --> 2:05:04.960\n It's kind of like the real AI starting point\n\n2:05:04.960 --> 2:05:05.800\n for your AGI book.\n\n2:05:05.800 --> 2:05:06.920\n Yeah, but I like True better\n\n2:05:06.920 --> 2:05:09.760\n because True has like, you can be true hearted, right?\n\n2:05:09.760 --> 2:05:11.040\n You can be true to your girlfriend.\n\n2:05:11.040 --> 2:05:15.720\n So True has a number and it also has logic in it, right?\n\n2:05:15.720 --> 2:05:18.280\n Because logic is a key part of the system.\n\n2:05:18.280 --> 2:05:22.400\n So yeah, with the True AGI system,\n\n2:05:22.400 --> 2:05:25.400\n we're sticking with the same basic architecture,\n\n2:05:25.400 --> 2:05:29.640\n but we're trying to build on what we've learned.\n\n2:05:29.640 --> 2:05:32.360\n And one thing we've learned is that,\n\n2:05:32.360 --> 2:05:36.920\n we need type checking among dependent types\n\n2:05:36.920 --> 2:05:38.040\n to be much faster\n\n2:05:38.040 --> 2:05:41.120\n and among probabilistic dependent types to be much faster.\n\n2:05:41.120 --> 2:05:43.600\n So as it is now,\n\n2:05:43.600 --> 2:05:47.120\n you can have complex types on the nodes and links.\n\n2:05:47.120 --> 2:05:48.360\n But if you wanna put,\n\n2:05:48.360 --> 2:05:51.280\n like if you want types to be first class citizens,\n\n2:05:51.280 --> 2:05:53.800\n so that you can have the types can be variables\n\n2:05:53.800 --> 2:05:55.680\n and then you do type checking\n\n2:05:55.680 --> 2:05:58.040\n among complex higher order types.\n\n2:05:58.040 --> 2:06:00.960\n You can do that in the system now, but it's very slow.\n\n2:06:00.960 --> 2:06:02.560\n This is stuff like it's done\n\n2:06:02.560 --> 2:06:05.360\n in cutting edge program languages like Agda or something,\n\n2:06:05.360 --> 2:06:07.400\n these obscure research languages.\n\n2:06:07.400 --> 2:06:08.600\n On the other hand,\n\n2:06:08.600 --> 2:06:11.240\n we've been doing a lot tying together deep neural nets\n\n2:06:11.240 --> 2:06:12.360\n with symbolic learning.\n\n2:06:12.360 --> 2:06:15.200\n So we did a project for Cisco, for example,\n\n2:06:15.200 --> 2:06:17.440\n which was on, this was street scene analysis,\n\n2:06:17.440 --> 2:06:18.600\n but they had deep neural models\n\n2:06:18.600 --> 2:06:21.000\n for a bunch of cameras watching street scenes,\n\n2:06:21.000 --> 2:06:23.400\n but they trained a different model for each camera\n\n2:06:23.400 --> 2:06:24.840\n because they couldn't get the transfer learning\n\n2:06:24.840 --> 2:06:27.040\n to work between camera A and camera B.\n\n2:06:27.040 --> 2:06:29.040\n So we took what came out of all the deep neural models\n\n2:06:29.040 --> 2:06:30.400\n for the different cameras,\n\n2:06:30.400 --> 2:06:33.440\n we fed it into an open called symbolic representation.\n\n2:06:33.440 --> 2:06:36.280\n Then we did some pattern mining and some reasoning\n\n2:06:36.280 --> 2:06:38.120\n on what came out of all the different cameras\n\n2:06:38.120 --> 2:06:39.480\n within the symbolic graph.\n\n2:06:39.480 --> 2:06:42.040\n And that worked well for that application.\n\n2:06:42.040 --> 2:06:45.880\n I mean, Hugo Latapie from Cisco gave a talk touching on that\n\n2:06:45.880 --> 2:06:48.760\n at last year's AGI conference, it was in Shenzhen.\n\n2:06:48.760 --> 2:06:51.000\n On the other hand, we learned from there,\n\n2:06:51.000 --> 2:06:53.280\n it was kind of clunky to get the deep neural models\n\n2:06:53.280 --> 2:06:55.640\n to work well with the symbolic system\n\n2:06:55.640 --> 2:06:58.560\n because we were using torch.\n\n2:06:58.560 --> 2:07:03.560\n And torch keeps a sort of state computation graph,\n\n2:07:03.560 --> 2:07:05.280\n but you needed like real time access\n\n2:07:05.280 --> 2:07:07.640\n to that computation graph within our hypergraph.\n\n2:07:07.640 --> 2:07:10.640\n And we certainly did it,\n\n2:07:10.640 --> 2:07:13.080\n Alexey Polopov who leads our St. Petersburg team\n\n2:07:13.080 --> 2:07:16.480\n wrote a great paper on cognitive modules in OpenCog\n\n2:07:16.480 --> 2:07:17.720\n explaining sort of how do you deal\n\n2:07:17.720 --> 2:07:19.960\n with the torch compute graph inside OpenCog.\n\n2:07:19.960 --> 2:07:22.840\n But in the end we realized like,\n\n2:07:22.840 --> 2:07:25.400\n that just hadn't been one of our design thoughts\n\n2:07:25.400 --> 2:07:27.240\n when we built OpenCog, right?\n\n2:07:27.240 --> 2:07:30.680\n So between wanting really fast dependent type checking\n\n2:07:30.680 --> 2:07:33.640\n and wanting much more efficient interoperation\n\n2:07:33.640 --> 2:07:35.160\n between the computation graphs\n\n2:07:35.160 --> 2:07:37.720\n of deep neural net frameworks and OpenCog's hypergraph\n\n2:07:37.720 --> 2:07:40.000\n and adding on top of that,\n\n2:07:40.000 --> 2:07:42.480\n wanting to more effectively run an OpenCog hypergraph\n\n2:07:42.480 --> 2:07:45.200\n distributed across RAM in 10,000 machines,\n\n2:07:45.200 --> 2:07:47.280\n which is we're doing dozens of machines now,\n\n2:07:47.280 --> 2:07:50.720\n but it's just not, we didn't architect it\n\n2:07:50.720 --> 2:07:53.080\n with that sort of modern scalability in mind.\n\n2:07:53.080 --> 2:07:56.280\n So these performance requirements are what have driven us\n\n2:07:56.280 --> 2:08:00.520\n to want to rearchitect the base,\n\n2:08:00.520 --> 2:08:05.320\n but the core AGI paradigm doesn't really change.\n\n2:08:05.320 --> 2:08:07.760\n Like the mathematics is the same.\n\n2:08:07.760 --> 2:08:11.440\n It's just, we can't scale to the level that we want\n\n2:08:11.440 --> 2:08:13.880\n in terms of distributed processing\n\n2:08:13.880 --> 2:08:16.280\n or speed of various kinds of processing\n\n2:08:16.280 --> 2:08:19.160\n with the current infrastructure\n\n2:08:19.160 --> 2:08:22.880\n that was built in the phase 2001 to 2008,\n\n2:08:22.880 --> 2:08:26.120\n which is hardly shocking.\n\n2:08:26.120 --> 2:08:27.880\n Well, I mean, the three things you mentioned\n\n2:08:27.880 --> 2:08:28.720\n are really interesting.\n\n2:08:28.720 --> 2:08:32.320\n So what do you think about in terms of interoperability\n\n2:08:32.320 --> 2:08:36.320\n communicating with computational graph of neural networks?\n\n2:08:36.320 --> 2:08:38.480\n What do you think about the representations\n\n2:08:38.480 --> 2:08:40.680\n that neural networks form?\n\n2:08:40.680 --> 2:08:42.920\n They're bad, but there's many ways\n\n2:08:42.920 --> 2:08:44.360\n that you could deal with that.\n\n2:08:44.360 --> 2:08:46.880\n So I've been wrestling with this a lot\n\n2:08:46.880 --> 2:08:49.920\n in some work on supervised grammar induction,\n\n2:08:49.920 --> 2:08:52.120\n and I have a simple paper on that.\n\n2:08:52.120 --> 2:08:55.400\n They'll give it the next AGI conference,\n\n2:08:55.400 --> 2:08:58.200\n online portion of which is next week, actually.\n\n2:08:58.200 --> 2:09:00.400\n What is grammar induction?\n\n2:09:00.400 --> 2:09:02.560\n So this isn't AGI either,\n\n2:09:02.560 --> 2:09:05.200\n but it's sort of on the verge\n\n2:09:05.200 --> 2:09:08.280\n between narrow AI and AGI or something.\n\n2:09:08.280 --> 2:09:11.320\n Unsupervised grammar induction is the problem.\n\n2:09:11.320 --> 2:09:15.400\n Throw your AI system, a huge body of text,\n\n2:09:15.400 --> 2:09:18.160\n and have it learn the grammar of the language\n\n2:09:18.160 --> 2:09:19.400\n that produced that text.\n\n2:09:20.280 --> 2:09:22.600\n So you're not giving it labeled examples.\n\n2:09:22.600 --> 2:09:24.440\n So you're not giving it like a thousand sentences\n\n2:09:24.440 --> 2:09:27.120\n where the parses were marked up by graduate students.\n\n2:09:27.120 --> 2:09:30.280\n So it's just got to infer the grammar from the text.\n\n2:09:30.280 --> 2:09:33.440\n It's like the Rosetta Stone, but worse, right?\n\n2:09:33.440 --> 2:09:35.320\n Because you only have the one language,\n\n2:09:35.320 --> 2:09:37.160\n and you have to figure out what is the grammar.\n\n2:09:37.160 --> 2:09:41.440\n So that's not really AGI because,\n\n2:09:41.440 --> 2:09:44.360\n I mean, the way a human learns language is not that, right?\n\n2:09:44.360 --> 2:09:47.720\n I mean, we learn from language that's used in context.\n\n2:09:47.720 --> 2:09:49.320\n So it's a social embodied thing.\n\n2:09:49.320 --> 2:09:53.520\n We see how a given sentence is grounded in observation.\n\n2:09:53.520 --> 2:09:55.200\n There's an interactive element, I guess.\n\n2:09:55.200 --> 2:09:56.520\n Yeah, yeah, yeah.\n\n2:09:56.520 --> 2:10:00.360\n On the other hand, so I'm more interested in that.\n\n2:10:00.360 --> 2:10:02.960\n I'm more interested in making an AGI system learn language\n\n2:10:02.960 --> 2:10:05.560\n from its social and embodied experience.\n\n2:10:05.560 --> 2:10:08.240\n On the other hand, that's also more of a pain to do,\n\n2:10:08.240 --> 2:10:10.640\n and that would lead us into Hanson Robotics\n\n2:10:10.640 --> 2:10:12.080\n and their robotics work I've known much.\n\n2:10:12.080 --> 2:10:14.600\n We'll talk about it in a few minutes.\n\n2:10:14.600 --> 2:10:17.120\n But just as an intellectual exercise,\n\n2:10:17.120 --> 2:10:18.840\n as a learning exercise,\n\n2:10:18.840 --> 2:10:22.480\n trying to learn grammar from a corpus\n\n2:10:22.480 --> 2:10:24.560\n is very, very interesting, right?\n\n2:10:24.560 --> 2:10:27.520\n And that's been a field in AI for a long time.\n\n2:10:27.520 --> 2:10:29.200\n No one can do it very well.\n\n2:10:29.200 --> 2:10:32.080\n So we've been looking at transformer neural networks\n\n2:10:32.080 --> 2:10:35.760\n and tree transformers, which are amazing.\n\n2:10:35.760 --> 2:10:39.080\n These came out of Google Brain, actually.\n\n2:10:39.080 --> 2:10:41.920\n And actually on that team was Lucas Kaiser,\n\n2:10:41.920 --> 2:10:44.080\n who used to work for me in the one,\n\n2:10:44.080 --> 2:10:46.960\n the period 2005 through eight or something.\n\n2:10:46.960 --> 2:10:50.200\n So it's been fun to see my former\n\n2:10:50.200 --> 2:10:52.760\n sort of AGI employees disperse and do\n\n2:10:52.760 --> 2:10:54.080\n all these amazing things.\n\n2:10:54.080 --> 2:10:56.080\n Way too many sucked into Google, actually.\n\n2:10:56.080 --> 2:10:57.640\n Well, yeah, anyway.\n\n2:10:57.640 --> 2:10:58.960\n We'll talk about that too.\n\n2:10:58.960 --> 2:11:00.640\n Lucas Kaiser and a bunch of these guys,\n\n2:11:00.640 --> 2:11:03.200\n they create transformer networks,\n\n2:11:03.200 --> 2:11:05.480\n that classic paper like attention is all you need\n\n2:11:05.480 --> 2:11:08.160\n and all these things following on from that.\n\n2:11:08.160 --> 2:11:10.160\n So we're looking at transformer networks.\n\n2:11:10.160 --> 2:11:13.520\n And like, these are able to,\n\n2:11:13.520 --> 2:11:16.480\n I mean, this is what underlies GPT2 and GPT3 and so on,\n\n2:11:16.480 --> 2:11:18.120\n which are very, very cool\n\n2:11:18.120 --> 2:11:20.320\n and have absolutely no cognitive understanding\n\n2:11:20.320 --> 2:11:21.680\n of any of the texts they're looking at.\n\n2:11:21.680 --> 2:11:24.960\n Like they're very intelligent idiots, right?\n\n2:11:24.960 --> 2:11:28.080\n So sorry to take, but this small, I'll bring this back,\n\n2:11:28.080 --> 2:11:31.760\n but do you think GPT3 understands language?\n\n2:11:31.760 --> 2:11:34.080\n No, no, it understands nothing.\n\n2:11:34.080 --> 2:11:35.320\n It's a complete idiot.\n\n2:11:35.320 --> 2:11:36.720\n But it's a brilliant idiot.\n\n2:11:36.720 --> 2:11:40.520\n You don't think GPT20 will understand language?\n\n2:11:40.520 --> 2:11:42.240\n No, no, no.\n\n2:11:42.240 --> 2:11:45.160\n So size is not gonna buy you understanding.\n\n2:11:45.160 --> 2:11:48.840\n And any more than a faster car is gonna get you to Mars.\n\n2:11:48.840 --> 2:11:50.920\n It's a completely different kind of thing.\n\n2:11:50.920 --> 2:11:54.280\n I mean, these networks are very cool.\n\n2:11:54.280 --> 2:11:55.520\n And as an entrepreneur,\n\n2:11:55.520 --> 2:11:57.760\n I can see many highly valuable uses for them.\n\n2:11:57.760 --> 2:12:01.080\n And as an artist, I love them, right?\n\n2:12:01.080 --> 2:12:05.240\n So I mean, we're using our own neural model,\n\n2:12:05.240 --> 2:12:06.560\n which is along those lines\n\n2:12:06.560 --> 2:12:09.000\n to control the Philip K. Dick robot now.\n\n2:12:09.000 --> 2:12:12.200\n And it's amazing to like train a neural model\n\n2:12:12.200 --> 2:12:14.000\n on the robot Philip K. Dick\n\n2:12:14.000 --> 2:12:15.840\n and see it come up with like crazed,\n\n2:12:15.840 --> 2:12:18.400\n stoned philosopher pronouncements,\n\n2:12:18.400 --> 2:12:21.440\n very much like what Philip K. Dick might've said, right?\n\n2:12:21.440 --> 2:12:24.840\n Like these models are super cool.\n\n2:12:24.840 --> 2:12:27.720\n And I'm working with Hanson Robotics now\n\n2:12:27.720 --> 2:12:30.600\n on using a similar, but more sophisticated one for Sophia,\n\n2:12:30.600 --> 2:12:34.080\n which we haven't launched yet.\n\n2:12:34.080 --> 2:12:36.080\n But so I think it's cool.\n\n2:12:36.080 --> 2:12:39.480\n But no, these are recognizing a large number\n\n2:12:39.480 --> 2:12:42.200\n of shallow patterns.\n\n2:12:42.200 --> 2:12:44.840\n They're not forming an abstract representation.\n\n2:12:44.840 --> 2:12:47.120\n And that's the point I was coming to\n\n2:12:47.120 --> 2:12:50.680\n when we're looking at grammar induction,\n\n2:12:50.680 --> 2:12:53.520\n we tried to mine patterns out of the structure\n\n2:12:53.520 --> 2:12:55.880\n of the transformer network.\n\n2:12:55.880 --> 2:12:59.600\n And you can, but the patterns aren't what you want.\n\n2:12:59.600 --> 2:13:00.600\n They're nasty.\n\n2:13:00.600 --> 2:13:03.200\n So I mean, if you do supervised learning,\n\n2:13:03.200 --> 2:13:04.560\n if you look at sentences where you know\n\n2:13:04.560 --> 2:13:06.520\n the correct parts of a sentence,\n\n2:13:06.520 --> 2:13:09.120\n you can learn a matrix that maps\n\n2:13:09.120 --> 2:13:12.240\n between the internal representation of the transformer\n\n2:13:12.240 --> 2:13:14.120\n and the parse of the sentence.\n\n2:13:14.120 --> 2:13:16.120\n And so then you can actually train something\n\n2:13:16.120 --> 2:13:18.440\n that will output the sentence parse\n\n2:13:18.440 --> 2:13:20.680\n from the transformer network's internal state.\n\n2:13:20.680 --> 2:13:24.720\n And we did this, I think Christopher Manning,\n\n2:13:24.720 --> 2:13:27.120\n some others have not done this also.\n\n2:13:28.080 --> 2:13:30.600\n But I mean, what you get is that the representation\n\n2:13:30.600 --> 2:13:33.200\n is hardly ugly and is scattered all over the network\n\n2:13:33.200 --> 2:13:34.920\n and doesn't look like the rules of grammar\n\n2:13:34.920 --> 2:13:37.240\n that you know are the right rules of grammar, right?\n\n2:13:37.240 --> 2:13:38.240\n It's kind of ugly.\n\n2:13:38.240 --> 2:13:41.440\n So what we're actually doing is we're using\n\n2:13:41.440 --> 2:13:44.280\n a symbolic grammar learning algorithm,\n\n2:13:44.280 --> 2:13:46.760\n but we're using the transformer neural network\n\n2:13:46.760 --> 2:13:48.880\n as a sentence probability oracle.\n\n2:13:48.880 --> 2:13:52.120\n So like if you have a rule of grammar\n\n2:13:52.120 --> 2:13:54.800\n and you aren't sure if it's a correct rule of grammar or not,\n\n2:13:54.800 --> 2:13:56.440\n you can generate a bunch of sentences\n\n2:13:56.440 --> 2:13:58.040\n using that rule of grammar\n\n2:13:58.040 --> 2:14:00.880\n and a bunch of sentences violating that rule of grammar.\n\n2:14:00.880 --> 2:14:04.480\n And you can see the transformer model\n\n2:14:04.480 --> 2:14:06.720\n doesn't think the sentences obeying the rule of grammar\n\n2:14:06.720 --> 2:14:08.280\n are more probable than the sentences\n\n2:14:08.280 --> 2:14:10.080\n disobeying the rule of grammar.\n\n2:14:10.080 --> 2:14:11.840\n So in that way, you can use the neural model\n\n2:14:11.840 --> 2:14:13.840\n as a sense probability oracle\n\n2:14:13.840 --> 2:14:18.840\n to guide a symbolic grammar learning process.\n\n2:14:19.960 --> 2:14:24.000\n And that seems to work better than trying to milk\n\n2:14:24.000 --> 2:14:25.840\n the grammar out of the neural network\n\n2:14:25.840 --> 2:14:26.760\n that doesn't have it in there.\n\n2:14:26.760 --> 2:14:29.480\n So I think the thing is these neural nets\n\n2:14:29.480 --> 2:14:32.880\n are not getting a semantically meaningful representation\n\n2:14:32.880 --> 2:14:35.360\n internally by and large.\n\n2:14:35.360 --> 2:14:38.120\n So one line of research is to try to get them to do that.\n\n2:14:38.120 --> 2:14:40.000\n And InfoGAN was trying to do that.\n\n2:14:40.000 --> 2:14:43.040\n So like if you look back like two years ago,\n\n2:14:43.040 --> 2:14:45.280\n there was all these papers on like at Edward,\n\n2:14:45.280 --> 2:14:47.400\n this probabilistic programming neural net framework\n\n2:14:47.400 --> 2:14:49.640\n that Google had, which came out of InfoGAN.\n\n2:14:49.640 --> 2:14:53.720\n So the idea there was like you could train\n\n2:14:53.720 --> 2:14:55.600\n an InfoGAN neural net model,\n\n2:14:55.600 --> 2:14:57.200\n which is a generative associative network\n\n2:14:57.200 --> 2:14:59.200\n to recognize and generate faces.\n\n2:14:59.200 --> 2:15:02.160\n And the model would automatically learn a variable\n\n2:15:02.160 --> 2:15:04.400\n for how long the nose is and automatically learn a variable\n\n2:15:04.400 --> 2:15:05.760\n for how wide the eyes are\n\n2:15:05.760 --> 2:15:08.040\n or how big the lips are or something, right?\n\n2:15:08.040 --> 2:15:11.040\n So it automatically learned these variables,\n\n2:15:11.040 --> 2:15:12.480\n which have a semantic meaning.\n\n2:15:12.480 --> 2:15:15.320\n So that was a rare case where a neural net\n\n2:15:15.320 --> 2:15:18.080\n trained with a fairly standard GAN method\n\n2:15:18.080 --> 2:15:20.880\n was able to actually learn the semantic representation.\n\n2:15:20.880 --> 2:15:23.240\n So for many years, many of us tried to take that\n\n2:15:23.240 --> 2:15:27.200\n the next step and get a GAN type neural network\n\n2:15:27.200 --> 2:15:31.680\n that would have not just a list of semantic latent variables,\n\n2:15:31.680 --> 2:15:33.960\n but would have say a Bayes net of semantic latent variables\n\n2:15:33.960 --> 2:15:35.440\n with dependencies between them.\n\n2:15:35.440 --> 2:15:38.840\n The whole programming framework Edward was made for that.\n\n2:15:38.840 --> 2:15:40.720\n I mean, no one got it to work, right?\n\n2:15:40.720 --> 2:15:41.560\n And it could be.\n\n2:15:41.560 --> 2:15:42.960\n Do you think it's possible?\n\n2:15:42.960 --> 2:15:43.800\n Yeah, do you think?\n\n2:15:43.800 --> 2:15:44.760\n I don't know.\n\n2:15:44.760 --> 2:15:47.280\n It might be that back propagation just won't work for it\n\n2:15:47.280 --> 2:15:49.720\n because the gradients are too screwed up.\n\n2:15:49.720 --> 2:15:52.000\n Maybe you could get it to work using CMAES\n\n2:15:52.000 --> 2:15:54.840\n or some like floating point evolutionary algorithm.\n\n2:15:54.840 --> 2:15:57.000\n We tried, we didn't get it to work.\n\n2:15:57.000 --> 2:16:01.360\n Eventually we just paused that rather than gave it up.\n\n2:16:01.360 --> 2:16:04.000\n We paused that and said, well, okay, let's try\n\n2:16:04.000 --> 2:16:08.640\n more innovative ways to learn implicit,\n\n2:16:08.640 --> 2:16:11.000\n to learn what are the representations implicit\n\n2:16:11.000 --> 2:16:13.640\n in that network without trying to make it grow\n\n2:16:13.640 --> 2:16:14.720\n inside that network.\n\n2:16:14.720 --> 2:16:19.720\n And I described how we're doing that in language.\n\n2:16:19.720 --> 2:16:21.440\n You can do similar things in vision, right?\n\n2:16:21.440 --> 2:16:22.280\n So what?\n\n2:16:22.280 --> 2:16:23.360\n Use it as an oracle.\n\n2:16:23.360 --> 2:16:24.200\n Yeah, yeah, yeah.\n\n2:16:24.200 --> 2:16:26.240\n So you can, that's one way is that you use\n\n2:16:26.240 --> 2:16:29.120\n a structure learning algorithm, which is symbolic.\n\n2:16:29.120 --> 2:16:32.480\n And then you use the deep neural net as an oracle\n\n2:16:32.480 --> 2:16:34.240\n to guide the structure learning algorithm.\n\n2:16:34.240 --> 2:16:37.880\n The other way to do it is like Infogam was trying to do\n\n2:16:37.880 --> 2:16:40.040\n and try to tweak the neural network\n\n2:16:40.040 --> 2:16:43.760\n to have the symbolic representation inside it.\n\n2:16:43.760 --> 2:16:46.440\n I tend to think what the brain is doing\n\n2:16:46.440 --> 2:16:51.440\n is more like using the deep neural net type thing\n\n2:16:51.680 --> 2:16:52.520\n as an oracle.\n\n2:16:52.520 --> 2:16:56.680\n I think the visual cortex or the cerebellum\n\n2:16:56.680 --> 2:17:00.280\n are probably learning a non semantically meaningful\n\n2:17:00.280 --> 2:17:02.400\n opaque tangled representation.\n\n2:17:02.400 --> 2:17:04.600\n And then when they interface with the more cognitive parts\n\n2:17:04.600 --> 2:17:08.080\n of the cortex, the cortex is sort of using those\n\n2:17:08.080 --> 2:17:10.720\n as an oracle and learning the abstract representation.\n\n2:17:10.720 --> 2:17:13.200\n So if you do sports, say take for example,\n\n2:17:13.200 --> 2:17:15.240\n serving in tennis, right?\n\n2:17:15.240 --> 2:17:17.680\n I mean, my tennis serve is okay, not great,\n\n2:17:17.680 --> 2:17:19.760\n but I learned it by trial and error, right?\n\n2:17:19.760 --> 2:17:22.120\n And I mean, I learned music by trial and error too.\n\n2:17:22.120 --> 2:17:25.960\n I just sit down and play, but then if you're an athlete,\n\n2:17:25.960 --> 2:17:27.080\n which I'm not a good athlete,\n\n2:17:27.080 --> 2:17:30.360\n I mean, then you'll watch videos of yourself serving\n\n2:17:30.360 --> 2:17:32.760\n and your coach will help you think about what you're doing\n\n2:17:32.760 --> 2:17:35.040\n and you'll then form a declarative representation,\n\n2:17:35.040 --> 2:17:37.160\n but your cerebellum maybe didn't have\n\n2:17:37.160 --> 2:17:38.640\n a declarative representation.\n\n2:17:38.640 --> 2:17:43.560\n Same way with music, like I will hear something in my head,\n\n2:17:43.560 --> 2:17:46.960\n I'll sit down and play the thing like I heard it.\n\n2:17:46.960 --> 2:17:51.000\n And then I will try to study what my fingers did\n\n2:17:51.000 --> 2:17:52.760\n to see like, what did you just play?\n\n2:17:52.760 --> 2:17:55.600\n Like how did you do that, right?\n\n2:17:55.600 --> 2:17:57.720\n Because if you're composing,\n\n2:17:57.720 --> 2:17:59.720\n you may wanna see how you did it\n\n2:17:59.720 --> 2:18:02.680\n and then declaratively morph that in some way\n\n2:18:02.680 --> 2:18:05.240\n that your fingers wouldn't think of, right?\n\n2:18:05.240 --> 2:18:10.240\n But the physiological movement may come out of some opaque,\n\n2:18:10.280 --> 2:18:14.440\n like cerebellar reinforcement learned thing, right?\n\n2:18:14.440 --> 2:18:17.680\n And so that's, I think trying to milk the structure\n\n2:18:17.680 --> 2:18:19.320\n of a neural net by treating it as an oracle,\n\n2:18:19.320 --> 2:18:23.960\n maybe more like how your declarative mind post processes\n\n2:18:23.960 --> 2:18:27.760\n what your visual or motor cortex.\n\n2:18:27.760 --> 2:18:29.400\n I mean, in vision, it's the same way,\n\n2:18:29.400 --> 2:18:33.520\n like you can recognize beautiful art\n\n2:18:34.800 --> 2:18:36.760\n much better than you can say why\n\n2:18:36.760 --> 2:18:38.520\n you think that piece of art is beautiful.\n\n2:18:38.520 --> 2:18:40.520\n But if you're trained as an art critic,\n\n2:18:40.520 --> 2:18:41.680\n you do learn to say why.\n\n2:18:41.680 --> 2:18:44.040\n And some of it's bullshit, but some of it isn't, right?\n\n2:18:44.040 --> 2:18:46.840\n Some of it is learning to map sensory knowledge\n\n2:18:46.840 --> 2:18:51.120\n into declarative and linguistic knowledge,\n\n2:18:51.120 --> 2:18:56.040\n yet without necessarily making the sensory system itself\n\n2:18:56.040 --> 2:19:00.640\n use a transparent and an easily communicable representation.\n\n2:19:00.640 --> 2:19:02.960\n Yeah, that's fascinating to think of neural networks\n\n2:19:02.960 --> 2:19:07.960\n as like dumb question answers that you can just milk\n\n2:19:08.200 --> 2:19:10.920\n to build up a knowledge base.\n\n2:19:10.920 --> 2:19:12.680\n And then it can be multiple networks, I suppose,\n\n2:19:12.680 --> 2:19:13.600\n from different.\n\n2:19:13.600 --> 2:19:18.160\n Yeah, yeah, so I think if a group like DeepMind or OpenAI\n\n2:19:18.160 --> 2:19:21.520\n were to build AGI, and I think DeepMind is like\n\n2:19:21.520 --> 2:19:24.720\n a thousand times more likely from what I could tell,\n\n2:19:25.920 --> 2:19:30.040\n because they've hired a lot of people with broad minds\n\n2:19:30.040 --> 2:19:34.360\n and many different approaches and angles on AGI,\n\n2:19:34.360 --> 2:19:36.640\n whereas OpenAI is also awesome,\n\n2:19:36.640 --> 2:19:39.040\n but I see them as more of like a pure\n\n2:19:39.040 --> 2:19:41.160\n deep reinforcement learning shop.\n\n2:19:41.160 --> 2:19:42.000\n Yeah, this time, I got you.\n\n2:19:42.000 --> 2:19:43.880\n So far. Yeah, there's a lot of,\n\n2:19:43.880 --> 2:19:48.600\n you're right, I mean, there's so much interdisciplinary\n\n2:19:48.600 --> 2:19:50.280\n work at DeepMind, like neuroscience.\n\n2:19:50.280 --> 2:19:52.240\n And you put that together with Google Brain,\n\n2:19:52.240 --> 2:19:54.760\n which granted they're not working that closely together now,\n\n2:19:54.760 --> 2:19:58.840\n but my oldest son Zarathustra is doing his PhD\n\n2:19:58.840 --> 2:20:01.640\n in machine learning applied to automated theorem proving\n\n2:20:01.640 --> 2:20:03.840\n in Prague under Josef Urban.\n\n2:20:03.840 --> 2:20:08.400\n So the first paper, DeepMath, which applied deep neural nets\n\n2:20:08.400 --> 2:20:10.680\n to guide theorem proving was out of Google Brain.\n\n2:20:10.680 --> 2:20:14.960\n I mean, by now, the automated theorem proving community\n\n2:20:14.960 --> 2:20:18.360\n is going way, way, way beyond anything Google was doing,\n\n2:20:18.360 --> 2:20:21.120\n but still, yeah, but anyway,\n\n2:20:21.120 --> 2:20:23.760\n if that community was gonna make an AGI,\n\n2:20:23.760 --> 2:20:27.160\n probably one way they would do it was,\n\n2:20:27.160 --> 2:20:30.680\n take 25 different neural modules,\n\n2:20:30.680 --> 2:20:32.040\n architected in different ways,\n\n2:20:32.040 --> 2:20:33.800\n maybe resembling different parts of the brain,\n\n2:20:33.800 --> 2:20:36.280\n like a basal ganglia model, cerebellum model,\n\n2:20:36.280 --> 2:20:40.440\n a thalamus module, a few hippocampus models,\n\n2:20:40.440 --> 2:20:41.480\n number of different models,\n\n2:20:41.480 --> 2:20:43.680\n representing parts of the cortex, right?\n\n2:20:43.680 --> 2:20:47.920\n Take all of these and then wire them together\n\n2:20:47.920 --> 2:20:52.520\n to co train and learn them together like that.\n\n2:20:52.520 --> 2:20:57.240\n That would be an approach to creating an AGI.\n\n2:20:57.240 --> 2:20:59.640\n One could implement something like that efficiently\n\n2:20:59.640 --> 2:21:03.800\n on top of our true AGI, like OpenCog 2.0 system,\n\n2:21:03.800 --> 2:21:06.640\n once it exists, although obviously Google\n\n2:21:06.640 --> 2:21:10.240\n has their own highly efficient implementation architecture.\n\n2:21:10.240 --> 2:21:13.280\n So I think that's a decent way to build AGI.\n\n2:21:13.280 --> 2:21:15.680\n I was very interested in that in the mid 90s,\n\n2:21:15.680 --> 2:21:19.440\n but I mean, the knowledge about how the brain works\n\n2:21:19.440 --> 2:21:21.520\n sort of pissed me off, like it wasn't there yet.\n\n2:21:21.520 --> 2:21:23.080\n Like, you know, in the hippocampus,\n\n2:21:23.080 --> 2:21:24.760\n you have these concept neurons,\n\n2:21:24.760 --> 2:21:26.720\n like the so called grandmother neuron,\n\n2:21:26.720 --> 2:21:28.520\n which everyone laughed at it, it's actually there.\n\n2:21:28.520 --> 2:21:31.080\n Like I have some Lex Friedman neurons\n\n2:21:31.080 --> 2:21:33.280\n that fire differentially when I see you\n\n2:21:33.280 --> 2:21:35.360\n and not when I see any other person, right?\n\n2:21:35.360 --> 2:21:38.880\n So how do these Lex Friedman neurons,\n\n2:21:38.880 --> 2:21:41.400\n how do they coordinate with the distributed representation\n\n2:21:41.400 --> 2:21:44.520\n of Lex Friedman I have in my cortex, right?\n\n2:21:44.520 --> 2:21:47.680\n There's some back and forth between cortex and hippocampus\n\n2:21:47.680 --> 2:21:50.120\n that lets these discrete symbolic representations\n\n2:21:50.120 --> 2:21:53.200\n in hippocampus correlate and cooperate\n\n2:21:53.200 --> 2:21:55.680\n with the distributed representations in cortex.\n\n2:21:55.680 --> 2:21:57.400\n This probably has to do with how the brain\n\n2:21:57.400 --> 2:22:00.240\n does its version of abstraction and quantifier logic, right?\n\n2:22:00.240 --> 2:22:02.640\n Like you can have a single neuron in the hippocampus\n\n2:22:02.640 --> 2:22:05.880\n that activates a whole distributed activation pattern\n\n2:22:05.880 --> 2:22:09.080\n in cortex, well, this may be how the brain does\n\n2:22:09.080 --> 2:22:11.120\n like symbolization and abstraction\n\n2:22:11.120 --> 2:22:14.280\n as in functional programming or something,\n\n2:22:14.280 --> 2:22:15.360\n but we can't measure it.\n\n2:22:15.360 --> 2:22:17.560\n Like we don't have enough electrodes stuck\n\n2:22:17.560 --> 2:22:20.960\n between the cortex and the hippocampus\n\n2:22:20.960 --> 2:22:23.080\n in any known experiment to measure it.\n\n2:22:23.080 --> 2:22:26.360\n So I got frustrated with that direction,\n\n2:22:26.360 --> 2:22:27.560\n not because it's impossible.\n\n2:22:27.560 --> 2:22:29.720\n Because we just don't understand enough yet.\n\n2:22:29.720 --> 2:22:31.760\n Of course, it's a valid research direction.\n\n2:22:31.760 --> 2:22:33.720\n You can try to understand more and more.\n\n2:22:33.720 --> 2:22:34.960\n And we are measuring more and more\n\n2:22:34.960 --> 2:22:38.120\n about what happens in the brain now than ever before.\n\n2:22:38.120 --> 2:22:40.560\n So it's quite interesting.\n\n2:22:40.560 --> 2:22:43.400\n On the other hand, I sort of got more\n\n2:22:43.400 --> 2:22:46.520\n of an engineering mindset about AGI.\n\n2:22:46.520 --> 2:22:47.920\n I'm like, well, okay,\n\n2:22:47.920 --> 2:22:50.200\n we don't know how the brain works that well.\n\n2:22:50.200 --> 2:22:52.360\n We don't know how birds fly that well yet either.\n\n2:22:52.360 --> 2:22:54.080\n We have no idea how a hummingbird flies\n\n2:22:54.080 --> 2:22:56.280\n in terms of the aerodynamics of it.\n\n2:22:56.280 --> 2:22:59.280\n On the other hand, we know basic principles\n\n2:22:59.280 --> 2:23:01.760\n of like flapping and pushing the air down.\n\n2:23:01.760 --> 2:23:03.520\n And we know the basic principles\n\n2:23:03.520 --> 2:23:05.720\n of how the different parts of the brain work.\n\n2:23:05.720 --> 2:23:07.480\n So let's take those basic principles\n\n2:23:07.480 --> 2:23:11.480\n and engineer something that embodies those basic principles,\n\n2:23:11.480 --> 2:23:14.040\n but is well designed for the hardware\n\n2:23:14.040 --> 2:23:18.080\n that we have on hand right now.\n\n2:23:18.080 --> 2:23:20.200\n So do you think we can create AGI\n\n2:23:20.200 --> 2:23:22.440\n before we understand how the brain works?\n\n2:23:22.440 --> 2:23:25.120\n I think that's probably what will happen.\n\n2:23:25.120 --> 2:23:28.560\n And maybe the AGI will help us do better brain imaging\n\n2:23:28.560 --> 2:23:30.880\n that will then let us build artificial humans,\n\n2:23:30.880 --> 2:23:33.400\n which is very, very interesting to us\n\n2:23:33.400 --> 2:23:34.960\n because we are humans, right?\n\n2:23:34.960 --> 2:23:38.840\n I mean, building artificial humans is super worthwhile.\n\n2:23:38.840 --> 2:23:42.760\n I just think it's probably not the shortest path to AGI.\n\n2:23:42.760 --> 2:23:45.680\n So it's fascinating idea that we would build AGI\n\n2:23:45.680 --> 2:23:47.320\n to help us understand ourselves.\n\n2:23:50.040 --> 2:23:54.600\n A lot of people ask me if the young people\n\n2:23:54.600 --> 2:23:56.440\n interested in doing artificial intelligence,\n\n2:23:56.440 --> 2:24:01.440\n they look at sort of doing graduate level, even undergrads,\n\n2:24:01.440 --> 2:24:04.520\n but graduate level research and they see\n\n2:24:04.520 --> 2:24:06.840\n whether the artificial intelligence community stands now,\n\n2:24:06.840 --> 2:24:09.920\n it's not really AGI type research for the most part.\n\n2:24:09.920 --> 2:24:12.080\n So the natural question they ask is\n\n2:24:12.080 --> 2:24:13.640\n what advice would you give?\n\n2:24:13.640 --> 2:24:17.320\n I mean, maybe I could ask if people were interested\n\n2:24:17.320 --> 2:24:22.320\n in working on OpenCog or in some kind of direct\n\n2:24:22.520 --> 2:24:25.160\n or indirect connection to OpenCog or AGI research,\n\n2:24:25.160 --> 2:24:26.560\n what would you recommend?\n\n2:24:28.040 --> 2:24:30.960\n OpenCog, first of all, is open source project.\n\n2:24:30.960 --> 2:24:35.360\n There's a Google group discussion list.\n\n2:24:35.360 --> 2:24:36.760\n There's a GitHub repository.\n\n2:24:36.760 --> 2:24:39.800\n So if anyone's interested in lending a hand\n\n2:24:39.800 --> 2:24:42.600\n with that aspect of AGI,\n\n2:24:42.600 --> 2:24:46.000\n introduce yourself on the OpenCog email list.\n\n2:24:46.000 --> 2:24:47.920\n And there's a Slack as well.\n\n2:24:47.920 --> 2:24:52.920\n I mean, we're certainly interested to have inputs\n\n2:24:53.080 --> 2:24:57.520\n into our redesign process for a new version of OpenCog,\n\n2:24:57.520 --> 2:25:01.160\n but also we're doing a lot of very interesting research.\n\n2:25:01.160 --> 2:25:04.080\n I mean, we're working on data analysis\n\n2:25:04.080 --> 2:25:05.600\n for COVID clinical trials.\n\n2:25:05.600 --> 2:25:06.960\n We're working with Hanson Robotics.\n\n2:25:06.960 --> 2:25:08.000\n We're doing a lot of cool things\n\n2:25:08.000 --> 2:25:10.720\n with the current version of OpenCog now.\n\n2:25:10.720 --> 2:25:14.720\n So there's certainly opportunity to jump into OpenCog\n\n2:25:14.720 --> 2:25:18.760\n or various other open source AGI oriented projects.\n\n2:25:18.760 --> 2:25:20.280\n So would you say there's like masters\n\n2:25:20.280 --> 2:25:22.080\n and PhD theses in there?\n\n2:25:22.080 --> 2:25:23.960\n Plenty, yeah, plenty, of course.\n\n2:25:23.960 --> 2:25:26.920\n I mean, the challenge is to find a supervisor\n\n2:25:26.920 --> 2:25:29.720\n who wants to foster that sort of research,\n\n2:25:29.720 --> 2:25:32.840\n but it's way easier than it was when I got my PhD, right?\n\n2:25:32.840 --> 2:25:33.680\n It's okay, great.\n\n2:25:33.680 --> 2:25:36.360\n We talked about OpenCog, which is kind of one,\n\n2:25:36.360 --> 2:25:38.000\n the software framework,\n\n2:25:38.000 --> 2:25:43.000\n but also the actual attempt to build an AGI system.\n\n2:25:44.160 --> 2:25:48.600\n And then there is this exciting idea of SingularityNet.\n\n2:25:48.600 --> 2:25:53.160\n So maybe can you say first what is SingularityNet?\n\n2:25:53.160 --> 2:25:54.280\n Sure, sure.\n\n2:25:54.280 --> 2:25:59.040\n SingularityNet is a platform\n\n2:25:59.040 --> 2:26:04.040\n for realizing a decentralized network\n\n2:26:05.880 --> 2:26:08.280\n of artificial intelligences.\n\n2:26:08.280 --> 2:26:13.280\n So Marvin Minsky, the AI pioneer who I knew a little bit,\n\n2:26:14.440 --> 2:26:16.560\n he had the idea of a society of minds,\n\n2:26:16.560 --> 2:26:18.360\n like you should achieve an AI\n\n2:26:18.360 --> 2:26:21.040\n not by writing one algorithm or one program,\n\n2:26:21.040 --> 2:26:24.000\n but you should put a bunch of different AIs out there\n\n2:26:24.000 --> 2:26:27.760\n and the different AIs will interact with each other,\n\n2:26:27.760 --> 2:26:29.480\n each playing their own role.\n\n2:26:29.480 --> 2:26:32.560\n And then the totality of the society of AIs\n\n2:26:32.560 --> 2:26:34.240\n would be the thing\n\n2:26:34.240 --> 2:26:36.560\n that displayed the human level intelligence.\n\n2:26:36.560 --> 2:26:39.000\n And I had, when he was alive,\n\n2:26:39.000 --> 2:26:43.000\n I had many debates with Marvin about this idea.\n\n2:26:43.000 --> 2:26:48.000\n And I think he really thought the mind\n\n2:26:49.080 --> 2:26:51.200\n was more like a society than I do.\n\n2:26:51.200 --> 2:26:54.080\n Like I think you could have a mind\n\n2:26:54.080 --> 2:26:56.720\n that was as disorganized as a human society,\n\n2:26:56.720 --> 2:26:57.880\n but I think a human like mind\n\n2:26:57.880 --> 2:27:00.080\n has a bit more central control than that actually.\n\n2:27:00.080 --> 2:27:02.840\n Like, I mean, we have this thalamus\n\n2:27:02.840 --> 2:27:04.760\n and the medulla and limbic system.\n\n2:27:04.760 --> 2:27:07.960\n We have a sort of top down control system\n\n2:27:07.960 --> 2:27:10.840\n that guides much of what we do,\n\n2:27:10.840 --> 2:27:12.760\n more so than a society does.\n\n2:27:12.760 --> 2:27:16.880\n So I think he stretched that metaphor a little too far,\n\n2:27:16.880 --> 2:27:20.840\n but I also think there's something interesting there.\n\n2:27:20.840 --> 2:27:24.040\n And so in the 90s,\n\n2:27:24.040 --> 2:27:27.960\n when I started my first sort of nonacademic AI project,\n\n2:27:27.960 --> 2:27:30.960\n WebMind, which was an AI startup in New York\n\n2:27:30.960 --> 2:27:34.640\n in the Silicon Alley area in the late 90s,\n\n2:27:34.640 --> 2:27:36.280\n what I was aiming to do there\n\n2:27:36.280 --> 2:27:38.880\n was make a distributed society of AIs,\n\n2:27:40.000 --> 2:27:41.360\n the different parts of which would live\n\n2:27:41.360 --> 2:27:43.640\n on different computers all around the world.\n\n2:27:43.640 --> 2:27:45.240\n And each one would do its own thinking\n\n2:27:45.240 --> 2:27:47.080\n about the data local to it,\n\n2:27:47.080 --> 2:27:48.960\n but they would all share information with each other\n\n2:27:48.960 --> 2:27:51.320\n and outsource work with each other and cooperate.\n\n2:27:51.320 --> 2:27:54.040\n And the intelligence would be in the whole collective.\n\n2:27:54.040 --> 2:27:57.680\n And I organized a conference together with Francis Heiligen\n\n2:27:57.680 --> 2:28:00.600\n at Free University of Brussels in 2001,\n\n2:28:00.600 --> 2:28:02.920\n which was the Global Brain Zero Conference.\n\n2:28:02.920 --> 2:28:04.680\n And we're planning the next version,\n\n2:28:04.680 --> 2:28:06.920\n the Global Brain One Conference\n\n2:28:06.920 --> 2:28:10.120\n at the Free University of Brussels for next year, 2021.\n\n2:28:10.120 --> 2:28:12.000\n So 20 years after.\n\n2:28:12.000 --> 2:28:14.560\n And then maybe we can have the next one 10 years after that,\n\n2:28:14.560 --> 2:28:19.320\n like exponentially faster until the singularity comes, right?\n\n2:28:19.320 --> 2:28:20.680\n The timing is right, yeah.\n\n2:28:20.680 --> 2:28:22.160\n Yeah, yeah, exactly.\n\n2:28:22.160 --> 2:28:25.000\n So yeah, the idea with the Global Brain\n\n2:28:25.000 --> 2:28:28.120\n was maybe the AI won't just be in a program\n\n2:28:28.120 --> 2:28:29.560\n on one guy's computer,\n\n2:28:29.560 --> 2:28:32.960\n but the AI will be in the internet as a whole\n\n2:28:32.960 --> 2:28:35.080\n with the cooperation of different AI modules\n\n2:28:35.080 --> 2:28:37.040\n living in different places.\n\n2:28:37.040 --> 2:28:39.280\n So one of the issues you face\n\n2:28:39.280 --> 2:28:41.160\n when architecting a system like that\n\n2:28:41.160 --> 2:28:44.760\n is, you know, how is the whole thing controlled?\n\n2:28:44.760 --> 2:28:47.200\n Do you have like a centralized control unit\n\n2:28:47.200 --> 2:28:48.640\n that pulls the puppet strings\n\n2:28:48.640 --> 2:28:50.720\n of all the different modules there?\n\n2:28:50.720 --> 2:28:55.480\n Or do you have a fundamentally decentralized network\n\n2:28:55.480 --> 2:28:59.320\n where the society of AIs is controlled\n\n2:28:59.320 --> 2:29:01.040\n in some democratic and self organized way,\n\n2:29:01.040 --> 2:29:04.760\n but all the AIs in that society, right?\n\n2:29:04.760 --> 2:29:08.680\n And Francis and I had different view of many things,\n\n2:29:08.680 --> 2:29:13.680\n but we both wanted to make like a global society\n\n2:29:13.680 --> 2:29:18.680\n of AI minds with a decentralized organizational mode.\n\n2:29:19.840 --> 2:29:24.840\n Now, the main difference was he wanted the individual AIs\n\n2:29:25.400 --> 2:29:27.440\n to be all incredibly simple\n\n2:29:27.440 --> 2:29:30.360\n and all the intelligence to be on the collective level.\n\n2:29:30.360 --> 2:29:32.960\n Whereas I thought that was cool,\n\n2:29:32.960 --> 2:29:35.880\n but I thought a more practical way to do it might be\n\n2:29:35.880 --> 2:29:39.480\n if some of the agents in the society of minds\n\n2:29:39.480 --> 2:29:41.520\n were fairly generally intelligent on their own.\n\n2:29:41.520 --> 2:29:44.480\n So like you could have a bunch of open cogs out there\n\n2:29:44.480 --> 2:29:47.120\n and a bunch of simpler learning systems.\n\n2:29:47.120 --> 2:29:49.840\n And then these are all cooperating, coordinating together\n\n2:29:49.840 --> 2:29:51.760\n sort of like in the brain.\n\n2:29:51.760 --> 2:29:55.320\n Okay, the brain as a whole is the general intelligence,\n\n2:29:55.320 --> 2:29:56.640\n but some parts of the cortex,\n\n2:29:56.640 --> 2:29:58.560\n you could say have a fair bit of general intelligence\n\n2:29:58.560 --> 2:29:59.720\n on their own,\n\n2:29:59.720 --> 2:30:02.120\n whereas say parts of the cerebellum or limbic system\n\n2:30:02.120 --> 2:30:04.520\n have very little general intelligence on their own.\n\n2:30:04.520 --> 2:30:07.240\n And they're contributing to general intelligence\n\n2:30:07.240 --> 2:30:10.880\n by way of their connectivity to other modules.\n\n2:30:10.880 --> 2:30:13.680\n Do you see instantiations of the same kind of,\n\n2:30:13.680 --> 2:30:15.400\n maybe different versions of open cog,\n\n2:30:15.400 --> 2:30:17.320\n but also just the same version of open cog\n\n2:30:17.320 --> 2:30:21.320\n and maybe many instantiations of it as being all parts of it?\n\n2:30:21.320 --> 2:30:23.040\n That's what David and Hans and I want to do\n\n2:30:23.040 --> 2:30:25.320\n with many Sophia and other robots.\n\n2:30:25.320 --> 2:30:29.200\n Each one has its own individual mind living on the server,\n\n2:30:29.200 --> 2:30:32.080\n but there's also a collective intelligence infusing them\n\n2:30:32.080 --> 2:30:35.440\n and a part of the mind living on the edge in each robot.\n\n2:30:35.440 --> 2:30:38.520\n So the thing is at that time,\n\n2:30:38.520 --> 2:30:41.840\n as well as WebMind being implemented in Java 1.1\n\n2:30:41.840 --> 2:30:44.760\n as like a massive distributed system,\n\n2:30:46.920 --> 2:30:48.160\n blockchain wasn't there yet.\n\n2:30:48.160 --> 2:30:51.880\n So had them do this decentralized control.\n\n2:30:51.880 --> 2:30:52.880\n We sort of knew it.\n\n2:30:52.880 --> 2:30:54.360\n We knew about distributed systems.\n\n2:30:54.360 --> 2:30:55.760\n We knew about encryption.\n\n2:30:55.760 --> 2:30:58.080\n So I mean, we had the key principles\n\n2:30:58.080 --> 2:31:00.080\n of what underlies blockchain now,\n\n2:31:00.080 --> 2:31:01.760\n but I mean, we didn't put it together\n\n2:31:01.760 --> 2:31:02.880\n in the way that it's been done now.\n\n2:31:02.880 --> 2:31:05.360\n So when Vitalik Buterin and colleagues\n\n2:31:05.360 --> 2:31:07.200\n came out with Ethereum blockchain,\n\n2:31:08.120 --> 2:31:11.000\n many, many years later, like 2013 or something,\n\n2:31:11.840 --> 2:31:13.920\n then I was like, well, this is interesting.\n\n2:31:13.920 --> 2:31:17.000\n Like this is solidity scripting language.\n\n2:31:17.000 --> 2:31:18.520\n It's kind of dorky in a way.\n\n2:31:18.520 --> 2:31:21.440\n And I don't see why you need to turn complete language\n\n2:31:21.440 --> 2:31:22.440\n for this purpose.\n\n2:31:22.440 --> 2:31:24.320\n But on the other hand,\n\n2:31:24.320 --> 2:31:27.160\n this is like the first time I could sit down\n\n2:31:27.160 --> 2:31:29.920\n and start to like script infrastructure\n\n2:31:29.920 --> 2:31:32.440\n for decentralized control of the AIs\n\n2:31:32.440 --> 2:31:35.240\n in this society of minds in a tractable way.\n\n2:31:35.240 --> 2:31:37.200\n Like you can hack the Bitcoin code base,\n\n2:31:37.200 --> 2:31:38.520\n but it's really annoying.\n\n2:31:38.520 --> 2:31:41.720\n Whereas solidity is Ethereum scripting language\n\n2:31:41.720 --> 2:31:44.440\n is just nicer and easier to use.\n\n2:31:44.440 --> 2:31:45.880\n I'm very annoyed with it by this point.\n\n2:31:45.880 --> 2:31:49.000\n But like Java, I mean, these languages are amazing\n\n2:31:49.000 --> 2:31:50.920\n when they first come out.\n\n2:31:50.920 --> 2:31:52.480\n So then I came up with the idea\n\n2:31:52.480 --> 2:31:53.840\n that turned into SingularityNet.\n\n2:31:53.840 --> 2:31:58.200\n Okay, let's make a decentralized agent system\n\n2:31:58.200 --> 2:32:00.480\n where a bunch of different AIs,\n\n2:32:00.480 --> 2:32:02.680\n wrapped up in say different Docker containers\n\n2:32:02.680 --> 2:32:04.320\n or LXC containers,\n\n2:32:04.320 --> 2:32:07.440\n different AIs can each of them have their own identity\n\n2:32:07.440 --> 2:32:08.760\n on the blockchain.\n\n2:32:08.760 --> 2:32:11.800\n And the coordination of this community of AIs\n\n2:32:11.800 --> 2:32:14.680\n has no central controller, no dictator, right?\n\n2:32:14.680 --> 2:32:17.160\n And there's no central repository of information.\n\n2:32:17.160 --> 2:32:19.400\n The coordination of the society of minds\n\n2:32:19.400 --> 2:32:22.680\n is done entirely by the decentralized network\n\n2:32:22.680 --> 2:32:25.840\n in a decentralized way by the algorithms, right?\n\n2:32:25.840 --> 2:32:29.200\n Because the model of Bitcoin is in math we trust, right?\n\n2:32:29.200 --> 2:32:30.800\n And so that's what you need.\n\n2:32:30.800 --> 2:32:33.880\n You need the society of minds to trust only in math,\n\n2:32:33.880 --> 2:32:37.720\n not trust only in one centralized server.\n\n2:32:37.720 --> 2:32:40.640\n So the AI systems themselves are outside of the blockchain,\n\n2:32:40.640 --> 2:32:41.800\n but then the communication between them.\n\n2:32:41.800 --> 2:32:43.960\n At the moment, yeah, yeah.\n\n2:32:43.960 --> 2:32:46.880\n I would have loved to put the AI's operations on chain\n\n2:32:46.880 --> 2:32:50.480\n in some sense, but in Ethereum, it's just too slow.\n\n2:32:50.480 --> 2:32:52.680\n You can't do it.\n\n2:32:52.680 --> 2:32:56.120\n Somehow it's the basic communication between AI systems.\n\n2:32:56.120 --> 2:32:58.360\n That's the distribution.\n\n2:32:58.360 --> 2:33:02.520\n Basically an AI is just some software in singularity.\n\n2:33:02.520 --> 2:33:05.920\n An AI is just some software process living in a container.\n\n2:33:05.920 --> 2:33:09.040\n And there's a proxy that lives in that container\n\n2:33:09.040 --> 2:33:10.840\n along with the AI that handles the interaction\n\n2:33:10.840 --> 2:33:13.120\n with the rest of singularity net.\n\n2:33:13.120 --> 2:33:15.880\n And then when one AI wants to contribute\n\n2:33:15.880 --> 2:33:16.920\n with another one in the network,\n\n2:33:16.920 --> 2:33:18.600\n they set up a number of channels.\n\n2:33:18.600 --> 2:33:22.600\n And the setup of those channels uses the Ethereum blockchain.\n\n2:33:22.600 --> 2:33:24.480\n Once the channels are set up,\n\n2:33:24.480 --> 2:33:26.160\n then data flows along those channels\n\n2:33:26.160 --> 2:33:29.240\n without having to be on the blockchain.\n\n2:33:29.240 --> 2:33:31.080\n All that goes on the blockchain is the fact\n\n2:33:31.080 --> 2:33:33.160\n that some data went along that channel.\n\n2:33:33.160 --> 2:33:34.240\n So you can do...\n\n2:33:34.240 --> 2:33:37.040\n So there's not a shared knowledge.\n\n2:33:38.720 --> 2:33:43.160\n Well, the identity of each agent is on the blockchain,\n\n2:33:43.160 --> 2:33:44.800\n on the Ethereum blockchain.\n\n2:33:44.800 --> 2:33:48.000\n If one agent rates the reputation of another agent,\n\n2:33:48.000 --> 2:33:49.560\n that goes on the blockchain.\n\n2:33:49.560 --> 2:33:52.880\n And agents can publish what APIs they will fulfill\n\n2:33:52.880 --> 2:33:54.520\n on the blockchain.\n\n2:33:54.520 --> 2:33:58.040\n But the actual data for AI and the results for AI\n\n2:33:58.040 --> 2:33:58.880\n is not on the blockchain.\n\n2:33:58.880 --> 2:33:59.720\n Do you think it could be?\n\n2:33:59.720 --> 2:34:02.320\n Do you think it should be?\n\n2:34:02.320 --> 2:34:04.120\n In some cases, it should be.\n\n2:34:04.120 --> 2:34:05.880\n In some cases, maybe it shouldn't be.\n\n2:34:05.880 --> 2:34:09.320\n But I mean, I think that...\n\n2:34:09.320 --> 2:34:10.160\n So I'll give you an example.\n\n2:34:10.160 --> 2:34:11.640\n Using Ethereum, you can't do it.\n\n2:34:11.640 --> 2:34:16.640\n Using now, there's more modern and faster blockchains\n\n2:34:16.640 --> 2:34:21.640\n where you could start to do that in some cases.\n\n2:34:21.920 --> 2:34:23.360\n Two years ago, that was less so.\n\n2:34:23.360 --> 2:34:25.640\n It's a very rapidly evolving ecosystem.\n\n2:34:25.640 --> 2:34:28.920\n So like one example, maybe you can comment on\n\n2:34:28.920 --> 2:34:31.840\n something I worked a lot on is autonomous vehicles.\n\n2:34:31.840 --> 2:34:35.680\n You can see each individual vehicle as an AI system.\n\n2:34:35.680 --> 2:34:39.600\n And you can see vehicles from Tesla, for example,\n\n2:34:39.600 --> 2:34:44.600\n and then Ford and GM and all these as also like larger...\n\n2:34:44.600 --> 2:34:47.000\n I mean, they all are running the same kind of system\n\n2:34:47.000 --> 2:34:49.280\n on each sets of vehicles.\n\n2:34:49.280 --> 2:34:52.360\n So it's individual AI systems and individual vehicles,\n\n2:34:52.360 --> 2:34:53.800\n but it's all different.\n\n2:34:53.800 --> 2:34:57.520\n The station is the same AI system within the same company.\n\n2:34:57.520 --> 2:35:02.360\n So you can envision a situation where all of those AI systems\n\n2:35:02.360 --> 2:35:05.400\n are put on SingularityNet, right?\n\n2:35:05.400 --> 2:35:10.160\n And how do you see that happening?\n\n2:35:10.160 --> 2:35:11.520\n And what would be the benefit?\n\n2:35:11.520 --> 2:35:13.000\n And could they share data?\n\n2:35:13.000 --> 2:35:16.440\n I guess one of the biggest things is that the power there's\n\n2:35:16.440 --> 2:35:20.440\n in a decentralized control, but the benefit would have been,\n\n2:35:20.440 --> 2:35:24.080\n is really nice if they can somehow share the knowledge\n\n2:35:24.080 --> 2:35:26.280\n in an open way if they choose to.\n\n2:35:26.280 --> 2:35:29.920\n Yeah, yeah, yeah, those are all quite good points.\n\n2:35:29.920 --> 2:35:34.920\n So I think the benefit from being on the decentralized network\n\n2:35:37.760 --> 2:35:41.320\n as we envision it is that we want the AIs in the network\n\n2:35:41.320 --> 2:35:43.800\n to be outsourcing work to each other\n\n2:35:43.800 --> 2:35:47.440\n and making API calls to each other frequently.\n\n2:35:47.440 --> 2:35:51.880\n So the real benefit would be if that AI wanted to outsource\n\n2:35:51.880 --> 2:35:54.920\n some cognitive processing or data processing\n\n2:35:54.920 --> 2:35:56.720\n or data pre processing, whatever,\n\n2:35:56.720 --> 2:35:59.320\n to some other AIs in the network,\n\n2:35:59.320 --> 2:36:01.600\n which specialize in something different.\n\n2:36:01.600 --> 2:36:06.120\n And this really requires a different way of thinking\n\n2:36:06.120 --> 2:36:07.960\n about AI software development, right?\n\n2:36:07.960 --> 2:36:10.320\n So just like object oriented programming\n\n2:36:10.320 --> 2:36:12.720\n was different than imperative programming.\n\n2:36:12.720 --> 2:36:16.720\n And now object oriented programmers all use these\n\n2:36:16.720 --> 2:36:20.680\n frameworks to do things rather than just libraries even.\n\n2:36:20.680 --> 2:36:23.120\n You know, shifting to agent based programming\n\n2:36:23.120 --> 2:36:26.600\n where AI agent is asking other like live real time\n\n2:36:26.600 --> 2:36:29.960\n evolving agents for feedback and what they're doing.\n\n2:36:29.960 --> 2:36:31.480\n That's a different way of thinking.\n\n2:36:31.480 --> 2:36:32.960\n I mean, it's not a new one.\n\n2:36:32.960 --> 2:36:35.320\n There was loads of papers on agent based programming\n\n2:36:35.320 --> 2:36:37.120\n in the 80s and onward.\n\n2:36:37.120 --> 2:36:41.520\n But if you're willing to shift to an agent based model\n\n2:36:41.520 --> 2:36:45.920\n of development, then you can put less and less in your AI\n\n2:36:45.920 --> 2:36:48.600\n and rely more and more on interactive calls\n\n2:36:48.600 --> 2:36:51.440\n to other AIs running in the network.\n\n2:36:51.440 --> 2:36:54.560\n And of course, that's not fully manifested yet\n\n2:36:54.560 --> 2:36:57.640\n because although we've rolled out a nice working version\n\n2:36:57.640 --> 2:36:59.760\n of SingularityNet platform,\n\n2:36:59.760 --> 2:37:03.760\n there's only 50 to 100 AIs running in there now.\n\n2:37:03.760 --> 2:37:05.880\n There's not tens of thousands of AIs.\n\n2:37:05.880 --> 2:37:08.240\n So we don't have the critical mass\n\n2:37:08.240 --> 2:37:11.120\n for the whole society of mind to be doing\n\n2:37:11.120 --> 2:37:11.960\n what we want to do.\n\n2:37:11.960 --> 2:37:13.400\n Yeah, the magic really happens\n\n2:37:13.400 --> 2:37:15.320\n when there's just a huge number of agents.\n\n2:37:15.320 --> 2:37:16.680\n Yeah, yeah, exactly.\n\n2:37:16.680 --> 2:37:19.600\n In terms of data, we're partnering closely\n\n2:37:19.600 --> 2:37:23.520\n with another blockchain project called Ocean Protocol.\n\n2:37:23.520 --> 2:37:27.240\n And Ocean Protocol, that's the project of Trent McConnachie\n\n2:37:27.240 --> 2:37:28.720\n who developed BigchainDB,\n\n2:37:28.720 --> 2:37:30.800\n which is a blockchain based database.\n\n2:37:30.800 --> 2:37:35.440\n So Ocean Protocol is basically blockchain based big data\n\n2:37:35.440 --> 2:37:39.440\n and aims at making it efficient for different AI processes\n\n2:37:39.440 --> 2:37:41.240\n or statistical processes or whatever\n\n2:37:41.240 --> 2:37:44.080\n to share large data sets.\n\n2:37:44.080 --> 2:37:46.600\n Or if one process can send a clone of itself\n\n2:37:46.600 --> 2:37:48.200\n to work on the other guy's data set\n\n2:37:48.200 --> 2:37:50.600\n and send results back and so forth.\n\n2:37:50.600 --> 2:37:55.560\n So by getting Ocean and you have data lake,\n\n2:37:55.560 --> 2:37:56.920\n so this is the data ocean, right?\n\n2:37:56.920 --> 2:37:59.760\n So again, by getting Ocean and SingularityNet\n\n2:37:59.760 --> 2:38:03.760\n to interoperate, we're aiming to take into account\n\n2:38:03.760 --> 2:38:05.840\n the big data aspect also.\n\n2:38:05.840 --> 2:38:08.240\n But it's quite challenging\n\n2:38:08.240 --> 2:38:10.120\n because to build this whole decentralized\n\n2:38:10.120 --> 2:38:12.400\n blockchain based infrastructure,\n\n2:38:12.400 --> 2:38:14.960\n I mean, your competitors are like Google, Microsoft,\n\n2:38:14.960 --> 2:38:17.960\n Alibaba and Amazon, which have so much money\n\n2:38:17.960 --> 2:38:20.560\n to put behind their centralized infrastructures,\n\n2:38:20.560 --> 2:38:23.360\n plus they're solving simpler algorithmic problems\n\n2:38:23.360 --> 2:38:27.360\n because making it centralized in some ways is easier, right?\n\n2:38:27.360 --> 2:38:32.360\n So they're very major computer science challenges.\n\n2:38:32.360 --> 2:38:35.760\n And I think what you saw with the whole ICO boom\n\n2:38:35.760 --> 2:38:37.880\n in the blockchain and cryptocurrency world\n\n2:38:37.880 --> 2:38:42.040\n is a lot of young hackers who were hacking Bitcoin\n\n2:38:42.040 --> 2:38:43.840\n or Ethereum, and they see, well,\n\n2:38:43.840 --> 2:38:46.800\n why don't we make this decentralized on blockchain?\n\n2:38:46.800 --> 2:38:48.720\n Then after they raised some money through an ICO,\n\n2:38:48.720 --> 2:38:49.880\n they realize how hard it is.\n\n2:38:49.880 --> 2:38:52.040\n And it's like, actually we're wrestling\n\n2:38:52.040 --> 2:38:54.680\n with incredibly hard computer science\n\n2:38:54.680 --> 2:38:58.720\n and software engineering and distributed systems problems,\n\n2:38:58.720 --> 2:39:02.560\n which can be solved, but they're just very difficult\n\n2:39:02.560 --> 2:39:03.400\n to solve.\n\n2:39:03.400 --> 2:39:05.800\n And in some cases, the individuals who started\n\n2:39:05.800 --> 2:39:08.760\n those projects were not well equipped\n\n2:39:08.760 --> 2:39:12.320\n to actually solve the problems that they wanted to solve.\n\n2:39:12.320 --> 2:39:14.560\n So you think, would you say that's the main bottleneck?\n\n2:39:14.560 --> 2:39:17.640\n If you look at the future of currency,\n\n2:39:19.560 --> 2:39:21.040\n the question is, well...\n\n2:39:21.040 --> 2:39:23.800\n Currency, the main bottleneck is politics.\n\n2:39:23.800 --> 2:39:26.440\n It's governments and the bands of armed thugs\n\n2:39:26.440 --> 2:39:29.840\n that will shoot you if you bypass their currency restriction.\n\n2:39:29.840 --> 2:39:30.680\n That's right.\n\n2:39:30.680 --> 2:39:33.760\n So like your sense is that versus the technical challenges,\n\n2:39:33.760 --> 2:39:34.840\n because you kind of just suggested\n\n2:39:34.840 --> 2:39:36.560\n the technical challenges are quite high as well.\n\n2:39:36.560 --> 2:39:39.000\n I mean, for making a distributed money,\n\n2:39:39.000 --> 2:39:41.280\n you could do that on Algorand right now.\n\n2:39:41.280 --> 2:39:43.880\n I mean, so that while Ethereum is too slow,\n\n2:39:44.760 --> 2:39:47.240\n there's Algorand and there's a few other more modern,\n\n2:39:47.240 --> 2:39:49.360\n more scalable blockchains that would work fine\n\n2:39:49.360 --> 2:39:53.640\n for a decentralized global currency.\n\n2:39:53.640 --> 2:39:56.480\n So I think there were technical bottlenecks\n\n2:39:56.480 --> 2:39:57.920\n to that two years ago.\n\n2:39:57.920 --> 2:40:00.760\n And maybe Ethereum 2.0 will be as fast as Algorand.\n\n2:40:00.760 --> 2:40:04.160\n I don't know, that's not fully written yet, right?\n\n2:40:04.160 --> 2:40:07.520\n So I think the obstacle to currency\n\n2:40:07.520 --> 2:40:09.400\n being put on the blockchain is that...\n\n2:40:09.400 --> 2:40:10.240\n Is the other stuff you mentioned.\n\n2:40:10.240 --> 2:40:11.760\n I mean, currency will be on the blockchain.\n\n2:40:11.760 --> 2:40:13.840\n It'll just be on the blockchain in a way\n\n2:40:13.840 --> 2:40:16.520\n that enforces centralized control\n\n2:40:16.520 --> 2:40:18.320\n and government hedge money rather than otherwise.\n\n2:40:18.320 --> 2:40:20.920\n Like the ERNB will probably be the first global,\n\n2:40:20.920 --> 2:40:22.200\n the first currency on the blockchain.\n\n2:40:22.200 --> 2:40:23.360\n The EURUBIL maybe next.\n\n2:40:23.360 --> 2:40:24.200\n There are any...\n\n2:40:24.200 --> 2:40:25.040\n EURUBIL?\n\n2:40:25.040 --> 2:40:25.860\n Yeah, yeah, yeah.\n\n2:40:25.860 --> 2:40:26.700\n I mean, the point is...\n\n2:40:26.700 --> 2:40:27.540\n Oh, that's hilarious.\n\n2:40:27.540 --> 2:40:30.720\n Digital currency, you know, makes total sense,\n\n2:40:30.720 --> 2:40:32.160\n but they would rather do it in the way\n\n2:40:32.160 --> 2:40:34.720\n that Putin and Xi Jinping have access\n\n2:40:34.720 --> 2:40:37.840\n to the global keys for everything, right?\n\n2:40:37.840 --> 2:40:42.040\n So, and then the analogy to that in terms of SingularityNet,\n\n2:40:42.040 --> 2:40:43.600\n I mean, there's Echoes.\n\n2:40:43.600 --> 2:40:47.200\n I think you've mentioned before that Linux gives you hope.\n\n2:40:47.200 --> 2:40:49.960\n AI is not as heavily regulated as money, right?\n\n2:40:49.960 --> 2:40:51.000\n Not yet, right?\n\n2:40:51.000 --> 2:40:52.000\n Not yet.\n\n2:40:52.000 --> 2:40:54.240\n Oh, that's a lot slipperier than money too, right?\n\n2:40:54.240 --> 2:40:58.280\n I mean, money is easier to regulate\n\n2:40:58.280 --> 2:41:00.800\n because it's kind of easier to define,\n\n2:41:00.800 --> 2:41:04.120\n whereas AI is, it's almost everywhere inside everything.\n\n2:41:04.120 --> 2:41:06.440\n Where's the boundary between AI and software, right?\n\n2:41:06.440 --> 2:41:09.200\n I mean, if you're gonna regulate AI,\n\n2:41:09.200 --> 2:41:11.720\n there's no IQ test for every hardware device\n\n2:41:11.720 --> 2:41:12.800\n that has a learning algorithm.\n\n2:41:12.800 --> 2:41:15.720\n You're gonna be putting like hegemonic regulation\n\n2:41:15.720 --> 2:41:16.760\n on all software.\n\n2:41:16.760 --> 2:41:18.880\n And I don't rule out that that can happen.\n\n2:41:18.880 --> 2:41:21.060\n And the adaptive software.\n\n2:41:21.060 --> 2:41:23.360\n Yeah, but how do you tell if a software is adaptive\n\n2:41:23.360 --> 2:41:26.100\n and what, every software is gonna be adaptive, I mean.\n\n2:41:26.100 --> 2:41:28.800\n Or maybe they, maybe the, you know,\n\n2:41:28.800 --> 2:41:31.120\n maybe we're living in the golden age of open source\n\n2:41:31.120 --> 2:41:33.360\n that will not always be open.\n\n2:41:33.360 --> 2:41:35.640\n Maybe it'll become centralized control\n\n2:41:35.640 --> 2:41:37.020\n of software by governments.\n\n2:41:37.020 --> 2:41:38.840\n It is entirely possible.\n\n2:41:38.840 --> 2:41:42.200\n And part of what I think we're doing\n\n2:41:42.200 --> 2:41:45.220\n with things like SingularityNet protocol\n\n2:41:45.220 --> 2:41:50.220\n is creating a tool set that can be used\n\n2:41:50.220 --> 2:41:52.740\n to counteract that sort of thing.\n\n2:41:52.740 --> 2:41:55.620\n Say a similar thing about mesh networking, right?\n\n2:41:55.620 --> 2:41:59.060\n Plays a minor role now, the ability to access internet\n\n2:41:59.060 --> 2:42:01.000\n like directly phone to phone.\n\n2:42:01.000 --> 2:42:03.740\n On the other hand, if your government starts trying\n\n2:42:03.740 --> 2:42:06.060\n to control your use of the internet,\n\n2:42:06.060 --> 2:42:09.220\n suddenly having mesh networking there\n\n2:42:09.220 --> 2:42:10.800\n can be very convenient, right?\n\n2:42:10.800 --> 2:42:15.360\n And so right now, something like a decentralized\n\n2:42:15.360 --> 2:42:20.300\n blockchain based AGI framework or narrow AI framework,\n\n2:42:20.300 --> 2:42:22.660\n it's cool, it's nice to have.\n\n2:42:22.660 --> 2:42:25.140\n On the other hand, if governments start trying\n\n2:42:25.140 --> 2:42:28.740\n to tap down on my AI interoperating\n\n2:42:28.740 --> 2:42:31.460\n with someone's AI in Russia or somewhere, right?\n\n2:42:31.460 --> 2:42:35.500\n Then suddenly having a decentralized protocol\n\n2:42:35.500 --> 2:42:37.940\n that nobody owns or controls\n\n2:42:37.940 --> 2:42:41.180\n becomes an extremely valuable part of the tool set.\n\n2:42:41.180 --> 2:42:43.780\n And, you know, we've put that out there now.\n\n2:42:43.780 --> 2:42:46.980\n It's not perfect, but it operates.\n\n2:42:46.980 --> 2:42:51.100\n And, you know, it's pretty blockchain agnostic.\n\n2:42:51.100 --> 2:42:53.420\n So we're talking to Algorand about making part\n\n2:42:53.420 --> 2:42:56.220\n of SingularityNet run on Algorand.\n\n2:42:56.220 --> 2:43:00.060\n My good friend Tufi Saliba has a cool blockchain project\n\n2:43:00.060 --> 2:43:02.220\n called Toda, which is a blockchain\n\n2:43:02.220 --> 2:43:03.540\n without a distributed ledger.\n\n2:43:03.540 --> 2:43:05.180\n It's like a whole other architecture.\n\n2:43:05.180 --> 2:43:08.300\n So there's a lot of more advanced things you can do\n\n2:43:08.300 --> 2:43:09.820\n in the blockchain world.\n\n2:43:09.820 --> 2:43:13.500\n SingularityNet could be ported to a whole bunch of,\n\n2:43:13.500 --> 2:43:14.980\n it could be made multi chain important\n\n2:43:14.980 --> 2:43:17.100\n to a whole bunch of different blockchains.\n\n2:43:17.100 --> 2:43:21.540\n And there's a lot of potential and a lot of importance\n\n2:43:21.540 --> 2:43:23.620\n to putting this kind of tool set out there.\n\n2:43:23.620 --> 2:43:26.660\n If you compare to OpenCog, what you could see is\n\n2:43:26.660 --> 2:43:31.660\n OpenCog allows tight integration of a few AI algorithms\n\n2:43:32.220 --> 2:43:36.860\n that share the same knowledge store in real time, in RAM.\n\n2:43:36.860 --> 2:43:40.900\n SingularityNet allows loose integration\n\n2:43:40.900 --> 2:43:42.660\n of multiple different AIs.\n\n2:43:42.660 --> 2:43:45.620\n They can share knowledge, but they're mostly not gonna\n\n2:43:45.620 --> 2:43:49.980\n be sharing knowledge in RAM on the same machine.\n\n2:43:49.980 --> 2:43:53.060\n And I think what we're gonna have is a network\n\n2:43:53.060 --> 2:43:54.500\n of network of networks, right?\n\n2:43:54.500 --> 2:43:57.260\n Like, I mean, you have the knowledge graph\n\n2:43:57.260 --> 2:44:00.900\n inside the OpenCog system,\n\n2:44:00.900 --> 2:44:03.220\n and then you have a network of machines\n\n2:44:03.220 --> 2:44:05.900\n inside a distributed OpenCog mind,\n\n2:44:05.900 --> 2:44:10.260\n but then that OpenCog will interface with other AIs\n\n2:44:10.260 --> 2:44:14.420\n doing deep neural nets or custom biology data analysis\n\n2:44:14.420 --> 2:44:17.620\n or whatever they're doing in SingularityNet,\n\n2:44:17.620 --> 2:44:21.020\n which is a looser integration of different AIs,\n\n2:44:21.020 --> 2:44:24.060\n some of which may be their own networks, right?\n\n2:44:24.060 --> 2:44:27.900\n And I think at a very loose analogy,\n\n2:44:27.900 --> 2:44:29.380\n you could see that in the human body.\n\n2:44:29.380 --> 2:44:33.820\n Like the brain has regions like cortex or hippocampus,\n\n2:44:33.820 --> 2:44:36.820\n which tightly interconnects like cortical columns\n\n2:44:36.820 --> 2:44:39.140\n within the cortex, for example.\n\n2:44:39.140 --> 2:44:40.860\n Then there's looser connection\n\n2:44:40.860 --> 2:44:42.700\n within the different lobes of the brain,\n\n2:44:42.700 --> 2:44:45.020\n and then the brain interconnects with the endocrine system\n\n2:44:45.020 --> 2:44:48.260\n and different parts of the body even more loosely.\n\n2:44:48.260 --> 2:44:50.780\n Then your body interacts even more loosely\n\n2:44:50.780 --> 2:44:53.300\n with the other people that you talk to.\n\n2:44:53.300 --> 2:44:56.460\n So you often have networks within networks within networks\n\n2:44:56.460 --> 2:44:59.340\n with progressively looser coupling\n\n2:44:59.340 --> 2:45:02.740\n as you get higher up in that hierarchy.\n\n2:45:02.740 --> 2:45:03.860\n I mean, you have that in biology,\n\n2:45:03.860 --> 2:45:08.180\n you have that in the internet as a just networking medium.\n\n2:45:08.180 --> 2:45:10.940\n And I think that's what we're gonna have\n\n2:45:10.940 --> 2:45:15.940\n in the network of software processes leading to AGI.\n\n2:45:15.940 --> 2:45:17.780\n That's a beautiful way to see the world.\n\n2:45:17.780 --> 2:45:21.900\n Again, the same similar question is with OpenCog.\n\n2:45:21.900 --> 2:45:24.620\n If somebody wanted to build an AI system\n\n2:45:24.620 --> 2:45:27.020\n and plug into the SingularityNet,\n\n2:45:27.020 --> 2:45:28.620\n what would you recommend?\n\n2:45:28.620 --> 2:45:30.180\n Yeah, so that's much easier.\n\n2:45:30.180 --> 2:45:33.860\n I mean, OpenCog is still a research system.\n\n2:45:33.860 --> 2:45:36.660\n So it takes some expertise to, and sometimes,\n\n2:45:36.660 --> 2:45:40.220\n we have tutorials, but it's somewhat cognitively\n\n2:45:40.220 --> 2:45:44.340\n labor intensive to get up to speed on OpenCog.\n\n2:45:44.340 --> 2:45:46.620\n And I mean, what's one of the things we hope to change\n\n2:45:46.620 --> 2:45:49.900\n with the true AGI OpenCog 2.0 version\n\n2:45:49.900 --> 2:45:52.740\n is just make the learning curve more similar\n\n2:45:52.740 --> 2:45:54.620\n to TensorFlow or Torch or something.\n\n2:45:54.620 --> 2:45:57.340\n Right now, OpenCog is amazingly powerful,\n\n2:45:57.340 --> 2:46:00.620\n but not simple to deal with.\n\n2:46:00.620 --> 2:46:03.700\n On the other hand, SingularityNet,\n\n2:46:03.700 --> 2:46:08.260\n as an open platform was developed a little more\n\n2:46:08.260 --> 2:46:10.580\n with usability in mind over the blockchain,\n\n2:46:10.580 --> 2:46:11.660\n it's still kind of a pain.\n\n2:46:11.660 --> 2:46:14.940\n So I mean, if you're a command line guy,\n\n2:46:14.940 --> 2:46:16.180\n there's a command line interface.\n\n2:46:16.180 --> 2:46:20.060\n It's quite easy to take any AI that has an API\n\n2:46:20.060 --> 2:46:23.540\n and lives in a Docker container and put it online anywhere.\n\n2:46:23.540 --> 2:46:25.740\n And then it joins the global SingularityNet.\n\n2:46:25.740 --> 2:46:28.980\n And anyone who puts a request for services\n\n2:46:28.980 --> 2:46:30.180\n out into the SingularityNet,\n\n2:46:30.180 --> 2:46:32.340\n the peer to peer discovery mechanism will find\n\n2:46:32.340 --> 2:46:35.740\n your AI and if it does what was asked,\n\n2:46:35.740 --> 2:46:38.980\n it can then start a conversation with your AI\n\n2:46:38.980 --> 2:46:42.180\n about whether it wants to ask your AI to do something for it,\n\n2:46:42.180 --> 2:46:43.580\n how much it would cost and so on.\n\n2:46:43.580 --> 2:46:46.860\n So that's fairly simple.\n\n2:46:46.860 --> 2:46:50.380\n If you wrote an AI and want it listed\n\n2:46:50.380 --> 2:46:53.020\n on like official SingularityNet marketplace,\n\n2:46:53.020 --> 2:46:55.140\n which is on our website,\n\n2:46:55.140 --> 2:46:57.820\n then we have a publisher portal\n\n2:46:57.820 --> 2:47:00.220\n and then there's a KYC process to go through\n\n2:47:00.220 --> 2:47:02.420\n because then we have some legal liability\n\n2:47:02.420 --> 2:47:04.700\n for what goes on that website.\n\n2:47:04.700 --> 2:47:07.340\n So in a way that's been an education too.\n\n2:47:07.340 --> 2:47:08.420\n There's sort of two layers.\n\n2:47:08.420 --> 2:47:11.700\n Like there's the open decentralized protocol.\n\n2:47:11.700 --> 2:47:12.980\n And there's the market.\n\n2:47:12.980 --> 2:47:15.540\n Yeah, anyone can use the open decentralized protocol.\n\n2:47:15.540 --> 2:47:17.980\n So say some developers from Iran\n\n2:47:17.980 --> 2:47:19.460\n and there's brilliant AI guys\n\n2:47:19.460 --> 2:47:21.780\n in University of Isfahan in Tehran,\n\n2:47:21.780 --> 2:47:24.660\n they can put their stuff on SingularityNet protocol\n\n2:47:24.660 --> 2:47:27.100\n and just like they can put something on the internet, right?\n\n2:47:27.100 --> 2:47:28.460\n I don't control it.\n\n2:47:28.460 --> 2:47:29.740\n But if we're gonna list something\n\n2:47:29.740 --> 2:47:32.020\n on the SingularityNet marketplace\n\n2:47:32.020 --> 2:47:34.300\n and put a little picture and a link to it,\n\n2:47:34.300 --> 2:47:38.860\n then if I put some Iranian AI geniuses code on there,\n\n2:47:38.860 --> 2:47:41.500\n then Donald Trump can send a bunch of jackbooted thugs\n\n2:47:41.500 --> 2:47:45.300\n to my house to arrest me for doing business with Iran, right?\n\n2:47:45.300 --> 2:47:48.980\n So, I mean, we already see in some ways\n\n2:47:48.980 --> 2:47:51.100\n the value of having a decentralized protocol\n\n2:47:51.100 --> 2:47:53.740\n because what I hope is that someone in Iran\n\n2:47:53.740 --> 2:47:57.340\n will put online an Iranian SingularityNet marketplace, right?\n\n2:47:57.340 --> 2:47:59.700\n Which you can pay in the cryptographic token,\n\n2:47:59.700 --> 2:48:01.540\n which is not owned by any country.\n\n2:48:01.540 --> 2:48:04.620\n And then if you're in like Congo or somewhere\n\n2:48:04.620 --> 2:48:06.780\n that doesn't have any problem with Iran,\n\n2:48:06.780 --> 2:48:09.220\n you can subcontract AI services\n\n2:48:09.220 --> 2:48:11.980\n that you find on that marketplace, right?\n\n2:48:11.980 --> 2:48:16.060\n Even though US citizens can't by US law.\n\n2:48:16.060 --> 2:48:20.140\n So right now, that's kind of a minor point.\n\n2:48:20.140 --> 2:48:24.020\n As you alluded, if regulations go in the wrong direction,\n\n2:48:24.020 --> 2:48:25.540\n it could become more of a major point.\n\n2:48:25.540 --> 2:48:28.060\n But I think it also is the case\n\n2:48:28.060 --> 2:48:31.860\n that having these workarounds to regulations in place\n\n2:48:31.860 --> 2:48:35.180\n is a defense mechanism against those regulations\n\n2:48:35.180 --> 2:48:36.660\n being put into place.\n\n2:48:36.660 --> 2:48:39.220\n And you can see that in the music industry, right?\n\n2:48:39.220 --> 2:48:43.020\n I mean, Napster just happened and BitTorrent just happened.\n\n2:48:43.020 --> 2:48:45.980\n And now most people in my kid's generation,\n\n2:48:45.980 --> 2:48:48.500\n they're baffled by the idea of paying for music, right?\n\n2:48:48.500 --> 2:48:51.380\n I mean, my dad pays for music.\n\n2:48:51.380 --> 2:48:55.700\n I mean, but that because these decentralized mechanisms\n\n2:48:55.700 --> 2:48:58.940\n happened and then the regulations followed, right?\n\n2:48:58.940 --> 2:49:01.220\n And the regulations would be very different\n\n2:49:01.220 --> 2:49:04.380\n if they'd been put into place before there was Napster\n\n2:49:04.380 --> 2:49:05.500\n and BitTorrent and so forth.\n\n2:49:05.500 --> 2:49:08.620\n So in the same way, we gotta put AI out there\n\n2:49:08.620 --> 2:49:11.060\n in a decentralized vein and big data out there\n\n2:49:11.060 --> 2:49:13.780\n in a decentralized vein now,\n\n2:49:13.780 --> 2:49:16.300\n so that the most advanced AI in the world\n\n2:49:16.300 --> 2:49:18.300\n is fundamentally decentralized.\n\n2:49:18.300 --> 2:49:20.940\n And if that's the case, that's just the reality\n\n2:49:20.940 --> 2:49:23.740\n the regulators have to deal with.\n\n2:49:23.740 --> 2:49:25.460\n And then as in the music case,\n\n2:49:25.460 --> 2:49:27.460\n they're gonna come up with regulations\n\n2:49:27.460 --> 2:49:32.060\n that sort of work with the decentralized reality.\n\n2:49:32.860 --> 2:49:34.020\n Beautiful.\n\n2:49:34.020 --> 2:49:37.980\n You are the chief scientist of Hanson Robotics.\n\n2:49:37.980 --> 2:49:40.500\n You're still involved with Hanson Robotics,\n\n2:49:40.500 --> 2:49:42.740\n doing a lot of really interesting stuff there.\n\n2:49:42.740 --> 2:49:44.500\n This is for people who don't know the company\n\n2:49:44.500 --> 2:49:47.380\n that created Sophia the Robot.\n\n2:49:47.380 --> 2:49:51.460\n Can you tell me who Sophia is?\n\n2:49:51.460 --> 2:49:54.140\n I'd rather start by telling you who David Hanson is.\n\n2:49:54.140 --> 2:49:58.780\n Because David is the brilliant mind behind the Sophia Robot.\n\n2:49:58.780 --> 2:50:01.980\n And he remains, so far, he remains more interesting\n\n2:50:01.980 --> 2:50:05.900\n than his creation, although she may be improving\n\n2:50:05.900 --> 2:50:07.380\n faster than he is, actually.\n\n2:50:07.380 --> 2:50:08.780\n I mean, he's a...\n\n2:50:08.780 --> 2:50:13.780\n So yeah, I met David maybe 2007 or something\n\n2:50:15.300 --> 2:50:18.420\n at some futurist conference we were both speaking at.\n\n2:50:18.420 --> 2:50:22.860\n And I could see we had a great deal in common.\n\n2:50:22.860 --> 2:50:25.020\n I mean, we were both kind of crazy,\n\n2:50:25.020 --> 2:50:30.020\n but we both had a passion for AGI and the singularity.\n\n2:50:31.540 --> 2:50:33.580\n And we were both huge fans of the work\n\n2:50:33.580 --> 2:50:36.900\n of Philip K. Dick, the science fiction writer.\n\n2:50:36.900 --> 2:50:40.780\n And I wanted to create benevolent AGI\n\n2:50:40.780 --> 2:50:44.820\n that would create massively better life\n\n2:50:44.820 --> 2:50:47.580\n for all humans and all sentient beings,\n\n2:50:47.580 --> 2:50:50.060\n including animals, plants, and superhuman beings.\n\n2:50:50.060 --> 2:50:53.780\n And David, he wanted exactly the same thing,\n\n2:50:53.780 --> 2:50:56.380\n but he had a different idea of how to do it.\n\n2:50:56.380 --> 2:50:59.420\n He wanted to get computational compassion.\n\n2:50:59.420 --> 2:51:03.940\n Like he wanted to get machines that would love people\n\n2:51:03.940 --> 2:51:05.820\n and empathize with people.\n\n2:51:05.820 --> 2:51:08.220\n And he thought the way to do that was to make a machine\n\n2:51:08.220 --> 2:51:12.220\n that could look people eye to eye, face to face,\n\n2:51:12.220 --> 2:51:15.700\n look at people and make people love the machine,\n\n2:51:15.700 --> 2:51:17.540\n and the machine loves the people back.\n\n2:51:17.540 --> 2:51:21.500\n So I thought that was very different way of looking at it\n\n2:51:21.500 --> 2:51:22.940\n because I'm very math oriented.\n\n2:51:22.940 --> 2:51:24.740\n And I'm just thinking like,\n\n2:51:24.740 --> 2:51:28.100\n what is the abstract cognitive algorithm\n\n2:51:28.100 --> 2:51:29.420\n that will let the system, you know,\n\n2:51:29.420 --> 2:51:32.580\n internalize the complex patterns of human values,\n\n2:51:32.580 --> 2:51:33.420\n blah, blah, blah.\n\n2:51:33.420 --> 2:51:35.980\n Whereas he's like, look you in the face and the eye\n\n2:51:35.980 --> 2:51:37.380\n and love you, right?\n\n2:51:37.380 --> 2:51:41.340\n So we hit it off quite well.\n\n2:51:41.340 --> 2:51:44.460\n And we talked to each other off and on.\n\n2:51:44.460 --> 2:51:49.380\n Then I moved to Hong Kong in 2011.\n\n2:51:49.380 --> 2:51:53.380\n So I've been living all over the place.\n\n2:51:53.380 --> 2:51:56.780\n I've been in Australia and New Zealand in my academic career.\n\n2:51:56.780 --> 2:51:59.380\n Then in Las Vegas for a while.\n\n2:51:59.380 --> 2:52:00.860\n Was in New York in the late 90s\n\n2:52:00.860 --> 2:52:03.660\n starting my entrepreneurial career.\n\n2:52:03.660 --> 2:52:05.020\n Was in DC for nine years\n\n2:52:05.020 --> 2:52:07.940\n doing a bunch of US government consulting stuff.\n\n2:52:07.940 --> 2:52:12.060\n Then moved to Hong Kong in 2011,\n\n2:52:12.060 --> 2:52:13.900\n mostly because I met a Chinese girl\n\n2:52:13.900 --> 2:52:16.060\n who I fell in love with and we got married.\n\n2:52:16.060 --> 2:52:17.380\n She's actually not from Hong Kong.\n\n2:52:17.380 --> 2:52:18.380\n She's from mainland China,\n\n2:52:18.380 --> 2:52:21.340\n but we converged together in Hong Kong.\n\n2:52:21.340 --> 2:52:24.180\n Still married now, I have a two year old baby.\n\n2:52:24.180 --> 2:52:26.820\n So went to Hong Kong to see about a girl, I guess.\n\n2:52:26.820 --> 2:52:29.060\n Yeah, pretty much, yeah.\n\n2:52:29.060 --> 2:52:31.060\n And on the other hand,\n\n2:52:31.060 --> 2:52:33.100\n I started doing some cool research there\n\n2:52:33.100 --> 2:52:36.540\n with Gino Yu at Hong Kong Polytechnic University.\n\n2:52:36.540 --> 2:52:38.300\n I got involved with a project called IDEA\n\n2:52:38.300 --> 2:52:41.220\n using machine learning for stock and futures prediction,\n\n2:52:41.220 --> 2:52:43.140\n which was quite interesting.\n\n2:52:43.140 --> 2:52:45.100\n And I also got to know something\n\n2:52:45.100 --> 2:52:47.420\n about the consumer electronics\n\n2:52:47.420 --> 2:52:50.220\n and hardware manufacturer ecosystem in Shenzhen\n\n2:52:50.220 --> 2:52:51.060\n across the border,\n\n2:52:51.060 --> 2:52:53.260\n which is like the only place in the world\n\n2:52:53.260 --> 2:52:56.500\n that makes sense to make complex consumer electronics\n\n2:52:56.500 --> 2:52:57.860\n at large scale and low cost.\n\n2:52:57.860 --> 2:53:00.900\n It's just, it's astounding the hardware ecosystem\n\n2:53:00.900 --> 2:53:03.220\n that you have in South China.\n\n2:53:03.220 --> 2:53:07.220\n Like US people here cannot imagine what it's like.\n\n2:53:07.220 --> 2:53:12.060\n So David was starting to explore that also.\n\n2:53:12.060 --> 2:53:13.860\n I invited him to Hong Kong to give a talk\n\n2:53:13.860 --> 2:53:15.660\n at Hong Kong PolyU,\n\n2:53:15.660 --> 2:53:19.220\n and I introduced him in Hong Kong to some investors\n\n2:53:19.220 --> 2:53:21.580\n who were interested in his robots.\n\n2:53:21.580 --> 2:53:23.540\n And he didn't have Sophia then,\n\n2:53:23.540 --> 2:53:25.140\n he had a robot of Philip K. Dick,\n\n2:53:25.140 --> 2:53:26.980\n our favorite science fiction writer.\n\n2:53:26.980 --> 2:53:28.180\n He had a robot Einstein,\n\n2:53:28.180 --> 2:53:29.540\n he had some little toy robots\n\n2:53:29.540 --> 2:53:31.940\n that looked like his son Zeno.\n\n2:53:31.940 --> 2:53:35.620\n So through the investors I connected him to,\n\n2:53:35.620 --> 2:53:37.500\n he managed to get some funding\n\n2:53:37.500 --> 2:53:40.660\n to basically port Hanson Robotics to Hong Kong.\n\n2:53:40.660 --> 2:53:42.660\n And when he first moved to Hong Kong,\n\n2:53:42.660 --> 2:53:45.300\n I was working on AGI research\n\n2:53:45.300 --> 2:53:49.340\n and also on this machine learning trading project.\n\n2:53:49.340 --> 2:53:50.940\n So I didn't get that tightly involved\n\n2:53:50.940 --> 2:53:52.980\n with Hanson Robotics.\n\n2:53:52.980 --> 2:53:56.540\n But as I hung out with David more and more,\n\n2:53:56.540 --> 2:53:59.180\n as we were both there in the same place,\n\n2:53:59.180 --> 2:54:00.220\n I started to get,\n\n2:54:01.260 --> 2:54:03.380\n I started to think about what you could do\n\n2:54:04.620 --> 2:54:08.500\n to make his robots smarter than they were.\n\n2:54:08.500 --> 2:54:10.340\n And so we started working together\n\n2:54:10.340 --> 2:54:12.780\n and for a few years I was chief scientist\n\n2:54:12.780 --> 2:54:15.740\n and head of software at Hanson Robotics.\n\n2:54:15.740 --> 2:54:19.420\n Then when I got deeply into the blockchain side of things,\n\n2:54:19.420 --> 2:54:24.340\n I stepped back from that and cofounded Singularity Net.\n\n2:54:24.340 --> 2:54:26.340\n David Hanson was also one of the cofounders\n\n2:54:26.340 --> 2:54:27.780\n of Singularity Net.\n\n2:54:27.780 --> 2:54:30.060\n So part of our goal there had been\n\n2:54:30.060 --> 2:54:33.940\n to make the blockchain based like cloud mind platform\n\n2:54:33.940 --> 2:54:37.020\n for Sophia and the other Hanson robots.\n\n2:54:37.020 --> 2:54:41.780\n Sophia would be just one of the robots in Singularity Net.\n\n2:54:41.780 --> 2:54:43.300\n Yeah, yeah, yeah, exactly.\n\n2:54:43.300 --> 2:54:47.380\n Sophia, many copies of the Sophia robot\n\n2:54:47.380 --> 2:54:51.500\n would be among the user interfaces\n\n2:54:51.500 --> 2:54:54.420\n to the globally distributed Singularity Net cloud mind.\n\n2:54:54.420 --> 2:54:57.140\n And I mean, David and I talked about that\n\n2:54:57.140 --> 2:55:01.540\n for quite a while before cofounding Singularity Net.\n\n2:55:01.540 --> 2:55:04.380\n By the way, in his vision and your vision,\n\n2:55:04.380 --> 2:55:09.380\n was Sophia tightly coupled to a particular AI system\n\n2:55:09.580 --> 2:55:11.660\n or was the idea that you can plug,\n\n2:55:11.660 --> 2:55:14.140\n you could just keep plugging in different AI systems\n\n2:55:14.140 --> 2:55:15.100\n within the head of it?\n\n2:55:15.100 --> 2:55:20.100\n David's view was always that Sophia would be a platform,\n\n2:55:22.940 --> 2:55:26.820\n much like say the Pepper robot is a platform from SoftBank.\n\n2:55:26.820 --> 2:55:31.660\n Should be a platform with a set of nicely designed APIs\n\n2:55:31.660 --> 2:55:33.540\n that anyone can use to experiment\n\n2:55:33.540 --> 2:55:38.540\n with their different AI algorithms on that platform.\n\n2:55:38.620 --> 2:55:41.580\n And Singularity Net, of course, fits right into that, right?\n\n2:55:41.580 --> 2:55:44.060\n Because Singularity Net, it's an API marketplace.\n\n2:55:44.060 --> 2:55:46.220\n So anyone can put their AI on there.\n\n2:55:46.220 --> 2:55:49.020\n OpenCog is a little bit different.\n\n2:55:49.020 --> 2:55:52.140\n I mean, David likes it, but I'd say it's my thing.\n\n2:55:52.140 --> 2:55:52.980\n It's not his.\n\n2:55:52.980 --> 2:55:55.100\n Like David has a little more passion\n\n2:55:55.100 --> 2:55:58.700\n for biologically based approaches to AI than I do,\n\n2:55:58.700 --> 2:56:00.140\n which makes sense.\n\n2:56:00.140 --> 2:56:02.860\n I mean, he's really into human physiology and biology.\n\n2:56:02.860 --> 2:56:05.140\n He's a character sculptor, right?\n\n2:56:05.140 --> 2:56:07.860\n So yeah, he's interested in,\n\n2:56:07.860 --> 2:56:09.700\n but he also worked a lot with rule based\n\n2:56:09.700 --> 2:56:11.420\n and logic based AI systems too.\n\n2:56:11.420 --> 2:56:14.860\n So yeah, he's interested in not just Sophia,\n\n2:56:14.860 --> 2:56:17.780\n but all the Hanson robots as a powerful social\n\n2:56:17.780 --> 2:56:21.220\n and emotional robotics platform.\n\n2:56:21.220 --> 2:56:26.220\n And what I saw in Sophia was a way to get AI algorithms\n\n2:56:26.220 --> 2:56:31.220\n was a way to get AI algorithms out there\n\n2:56:32.140 --> 2:56:34.660\n in front of a whole lot of different people\n\n2:56:34.660 --> 2:56:36.300\n in an emotionally compelling way.\n\n2:56:36.300 --> 2:56:39.820\n And part of my thought was really kind of abstract\n\n2:56:39.820 --> 2:56:41.740\n connected to AGI ethics.\n\n2:56:41.740 --> 2:56:46.740\n And many people are concerned AGI is gonna enslave everybody\n\n2:56:46.940 --> 2:56:50.060\n or turn everybody into computronium\n\n2:56:50.060 --> 2:56:54.740\n to make extra hard drives for their cognitive engine\n\n2:56:54.740 --> 2:56:55.580\n or whatever.\n\n2:56:55.580 --> 2:57:00.580\n And emotionally I'm not driven to that sort of paranoia.\n\n2:57:01.660 --> 2:57:04.100\n I'm really just an optimist by nature,\n\n2:57:04.100 --> 2:57:09.100\n but intellectually I have to assign a non zero probability\n\n2:57:09.220 --> 2:57:12.140\n to those sorts of nasty outcomes.\n\n2:57:12.140 --> 2:57:14.900\n Cause if you're making something 10 times as smart as you,\n\n2:57:14.900 --> 2:57:16.300\n how can you know what it's gonna do?\n\n2:57:16.300 --> 2:57:19.780\n There's an irreducible uncertainty there\n\n2:57:19.780 --> 2:57:22.780\n just as my dog can't predict what I'm gonna do tomorrow.\n\n2:57:22.780 --> 2:57:26.420\n So it seemed to me that based on our current state\n\n2:57:26.420 --> 2:57:31.420\n of knowledge, the best way to bias the AGI as we create\n\n2:57:32.500 --> 2:57:37.500\n toward benevolence would be to infuse them with love\n\n2:57:38.820 --> 2:57:41.620\n and compassion the way that we do our own children.\n\n2:57:41.620 --> 2:57:45.820\n So you want to interact with AIs in the context\n\n2:57:45.820 --> 2:57:49.900\n of doing compassionate, loving and beneficial things.\n\n2:57:49.900 --> 2:57:52.140\n And in that way, as your children will learn\n\n2:57:52.140 --> 2:57:53.740\n by doing compassionate, beneficial,\n\n2:57:53.740 --> 2:57:55.940\n loving things alongside you.\n\n2:57:55.940 --> 2:57:58.660\n And that way the AI will learn in practice\n\n2:57:58.660 --> 2:58:02.340\n what it means to be compassionate, beneficial and loving.\n\n2:58:02.340 --> 2:58:06.380\n It will get a sort of ingrained intuitive sense of this,\n\n2:58:06.380 --> 2:58:09.260\n which it can then abstract in its own way\n\n2:58:09.260 --> 2:58:11.180\n as it gets more and more intelligent.\n\n2:58:11.180 --> 2:58:12.780\n Now, David saw this the same way.\n\n2:58:12.780 --> 2:58:15.540\n That's why he came up with the name Sophia,\n\n2:58:15.540 --> 2:58:18.140\n which means wisdom.\n\n2:58:18.140 --> 2:58:22.780\n So it seemed to me making these beautiful, loving robots\n\n2:58:22.780 --> 2:58:26.060\n to be rolled out for beneficial applications\n\n2:58:26.060 --> 2:58:31.060\n would be the perfect way to roll out early stage AGI systems\n\n2:58:31.260 --> 2:58:33.940\n so they can learn from people\n\n2:58:33.940 --> 2:58:35.420\n and not just learn factual knowledge,\n\n2:58:35.420 --> 2:58:38.580\n but learn human values and ethics from people\n\n2:58:38.580 --> 2:58:41.540\n while being their home service robots,\n\n2:58:41.540 --> 2:58:44.100\n their education assistants, their nursing robots.\n\n2:58:44.100 --> 2:58:46.060\n So that was the grand vision.\n\n2:58:46.060 --> 2:58:48.620\n Now, if you've ever worked with robots,\n\n2:58:48.620 --> 2:58:50.420\n the reality is quite different, right?\n\n2:58:50.420 --> 2:58:53.220\n Like the first principle is the robot is always broken.\n\n2:58:53.220 --> 2:58:57.660\n I mean, I worked with robots in the 90s a bunch\n\n2:58:57.660 --> 2:58:59.540\n when you had to solder them together yourself\n\n2:58:59.540 --> 2:59:02.580\n and I'd put neural nets during reinforcement learning\n\n2:59:02.580 --> 2:59:05.940\n on like overturned solid ball type robots\n\n2:59:05.940 --> 2:59:09.300\n and in the 90s when I was a professor.\n\n2:59:09.300 --> 2:59:12.020\n Things of course advanced a lot, but...\n\n2:59:12.020 --> 2:59:13.180\n But the principle still holds.\n\n2:59:13.180 --> 2:59:16.500\n The principle that the robot's always broken still holds.\n\n2:59:16.500 --> 2:59:21.020\n Yeah, so faced with the reality of making Sophia do stuff,\n\n2:59:21.020 --> 2:59:26.020\n many of my robo AGI aspirations were temporarily cast aside.\n\n2:59:26.620 --> 2:59:30.660\n And I mean, there's just a practical problem\n\n2:59:30.660 --> 2:59:33.700\n of making this robot interact in a meaningful way\n\n2:59:33.700 --> 2:59:36.700\n because like, you put nice computer vision on there,\n\n2:59:36.700 --> 2:59:38.140\n but there's always glare.\n\n2:59:38.140 --> 2:59:41.420\n And then, or you have a dialogue system,\n\n2:59:41.420 --> 2:59:43.740\n but at the time I was there,\n\n2:59:43.740 --> 2:59:46.580\n like no speech to text algorithm could deal\n\n2:59:46.580 --> 2:59:49.780\n with Hong Kongese people's English accents.\n\n2:59:49.780 --> 2:59:51.620\n So the speech to text was always bad.\n\n2:59:51.620 --> 2:59:53.620\n So the robot always sounded stupid\n\n2:59:53.620 --> 2:59:55.620\n because it wasn't getting the right text, right?\n\n2:59:55.620 --> 2:59:58.020\n So I started to view that really\n\n2:59:58.020 --> 3:00:02.820\n as what in software engineering you call a walking skeleton,\n\n3:00:02.820 --> 3:00:05.420\n which is maybe the wrong metaphor to use for Sophia\n\n3:00:05.420 --> 3:00:06.980\n or maybe the right one.\n\n3:00:06.980 --> 3:00:08.420\n I mean, where the walking skeleton is\n\n3:00:08.420 --> 3:00:10.620\n in software development is\n\n3:00:10.620 --> 3:00:14.020\n if you're building a complex system, how do you get started?\n\n3:00:14.020 --> 3:00:16.140\n But one way is to first build part one well,\n\n3:00:16.140 --> 3:00:18.340\n then build part two well, then build part three well\n\n3:00:18.340 --> 3:00:19.260\n and so on.\n\n3:00:19.260 --> 3:00:22.060\n And the other way is you make like a simple version\n\n3:00:22.060 --> 3:00:24.820\n of the whole system and put something in the place\n\n3:00:24.820 --> 3:00:27.300\n of every part the whole system will need\n\n3:00:27.300 --> 3:00:29.660\n so that you have a whole system that does something.\n\n3:00:29.660 --> 3:00:31.900\n And then you work on improving each part\n\n3:00:31.900 --> 3:00:34.340\n in the context of that whole integrated system.\n\n3:00:34.340 --> 3:00:38.140\n So that's what we did on a software level in Sophia.\n\n3:00:38.140 --> 3:00:41.580\n We made like a walking skeleton software system\n\n3:00:41.580 --> 3:00:43.100\n where so there's something that sees,\n\n3:00:43.100 --> 3:00:46.220\n there's something that hears, there's something that moves,\n\n3:00:46.220 --> 3:00:48.180\n there's something that remembers,\n\n3:00:48.180 --> 3:00:49.980\n there's something that learns.\n\n3:00:49.980 --> 3:00:52.460\n You put a simple version of each thing in there\n\n3:00:52.460 --> 3:00:54.420\n and you connect them all together\n\n3:00:54.420 --> 3:00:56.660\n so that the system will do its thing.\n\n3:00:56.660 --> 3:00:59.660\n So there's a lot of AI in there.\n\n3:00:59.660 --> 3:01:01.380\n There's not any AGI in there.\n\n3:01:01.380 --> 3:01:04.660\n I mean, there's computer vision to recognize people's faces,\n\n3:01:04.660 --> 3:01:07.660\n recognize when someone comes in the room and leaves,\n\n3:01:07.660 --> 3:01:10.740\n trying to recognize whether two people are together or not.\n\n3:01:10.740 --> 3:01:13.300\n I mean, the dialogue system,\n\n3:01:13.300 --> 3:01:18.300\n it's a mix of like hand coded rules with deep neural nets\n\n3:01:18.780 --> 3:01:21.580\n that come up with their own responses.\n\n3:01:21.580 --> 3:01:25.660\n And there's some attempt to have a narrative structure\n\n3:01:25.660 --> 3:01:28.420\n and sort of try to pull the conversation\n\n3:01:28.420 --> 3:01:30.780\n into something with a beginning, middle and end\n\n3:01:30.780 --> 3:01:32.180\n and this sort of story arc.\n\n3:01:32.180 --> 3:01:33.500\n So it's...\n\n3:01:33.500 --> 3:01:37.620\n I mean, like if you look at the Lobner Prize and the systems\n\n3:01:37.620 --> 3:01:39.060\n that beat the Turing Test currently,\n\n3:01:39.060 --> 3:01:40.540\n they're heavily rule based\n\n3:01:40.540 --> 3:01:43.900\n because like you had said, narrative structure\n\n3:01:43.900 --> 3:01:45.700\n to create compelling conversations,\n\n3:01:45.700 --> 3:01:48.420\n you currently, neural networks cannot do that well,\n\n3:01:48.420 --> 3:01:50.660\n even with Google MENA.\n\n3:01:50.660 --> 3:01:53.060\n When you actually look at full scale conversations,\n\n3:01:53.060 --> 3:01:53.900\n it's just not...\n\n3:01:53.900 --> 3:01:54.740\n Yeah, this is the thing.\n\n3:01:54.740 --> 3:01:57.900\n So we've been, I've actually been running an experiment\n\n3:01:57.900 --> 3:02:01.420\n the last couple of weeks taking Sophia's chat bot\n\n3:02:01.420 --> 3:02:03.740\n and then Facebook's Transformer chat bot,\n\n3:02:03.740 --> 3:02:05.260\n which they opened the model.\n\n3:02:05.260 --> 3:02:06.780\n We've had them chatting to each other\n\n3:02:06.780 --> 3:02:08.860\n for a number of weeks on the server just...\n\n3:02:08.860 --> 3:02:10.020\n That's funny.\n\n3:02:10.020 --> 3:02:13.260\n We're generating training data of what Sophia says\n\n3:02:13.260 --> 3:02:15.500\n in a wide variety of conversations.\n\n3:02:15.500 --> 3:02:20.260\n But we can see, compared to Sophia's current chat bot,\n\n3:02:20.260 --> 3:02:23.460\n the Facebook deep neural chat bot comes up\n\n3:02:23.460 --> 3:02:27.300\n with a wider variety of fluent sounding sentences.\n\n3:02:27.300 --> 3:02:30.100\n On the other hand, it rambles like mad.\n\n3:02:30.100 --> 3:02:33.900\n The Sophia chat bot, it's a little more repetitive\n\n3:02:33.900 --> 3:02:36.620\n in the sentence structures it uses.\n\n3:02:36.620 --> 3:02:39.820\n On the other hand, it's able to keep like a conversation arc\n\n3:02:39.820 --> 3:02:42.460\n over a much longer, longer period, right?\n\n3:02:42.460 --> 3:02:43.300\n So there...\n\n3:02:43.300 --> 3:02:46.620\n Now, you can probably surmount that using Reformer\n\n3:02:46.620 --> 3:02:51.140\n and like using various other deep neural architectures\n\n3:02:51.140 --> 3:02:53.980\n to improve the way these Transformer models are trained.\n\n3:02:53.980 --> 3:02:58.300\n But in the end, neither one of them really understands\n\n3:02:58.300 --> 3:02:59.140\n what's going on.\n\n3:02:59.140 --> 3:03:02.660\n I mean, that's the challenge I had with Sophia\n\n3:03:02.660 --> 3:03:07.660\n is if I were doing a robotics project aimed at AGI,\n\n3:03:08.340 --> 3:03:10.100\n I would wanna make like a robo toddler\n\n3:03:10.100 --> 3:03:11.940\n that was just learning about what it was seeing.\n\n3:03:11.940 --> 3:03:13.220\n Because then the language is grounded\n\n3:03:13.220 --> 3:03:14.940\n in the experience of the robot.\n\n3:03:14.940 --> 3:03:17.740\n But what Sophia needs to do to be Sophia\n\n3:03:17.740 --> 3:03:21.420\n is talk about sports or the weather or robotics\n\n3:03:21.420 --> 3:03:24.100\n or the conference she's talking at.\n\n3:03:24.100 --> 3:03:26.380\n She needs to be fluent talking about\n\n3:03:26.380 --> 3:03:28.420\n any damn thing in the world.\n\n3:03:28.420 --> 3:03:32.500\n And she doesn't have grounding for all those things.\n\n3:03:32.500 --> 3:03:35.700\n So there's this, just like, I mean, Google Mina\n\n3:03:35.700 --> 3:03:37.460\n and Facebook's chat, but I don't have grounding\n\n3:03:37.460 --> 3:03:40.140\n for what they're talking about either.\n\n3:03:40.140 --> 3:03:45.060\n So in a way, the need to speak fluently about things\n\n3:03:45.060 --> 3:03:47.940\n where there's no nonlinguistic grounding\n\n3:03:47.940 --> 3:03:52.940\n pushes what you can do for Sophia in the short term\n\n3:03:53.660 --> 3:03:56.340\n a bit away from AGI.\n\n3:03:56.340 --> 3:04:00.900\n I mean, it pushes you towards IBM Watson situation\n\n3:04:00.900 --> 3:04:02.740\n where you basically have to do heuristic\n\n3:04:02.740 --> 3:04:05.380\n and hard code stuff and rule based stuff.\n\n3:04:05.380 --> 3:04:07.860\n I have to ask you about this, okay.\n\n3:04:07.860 --> 3:04:12.860\n So because in part Sophia is like an art creation\n\n3:04:18.860 --> 3:04:20.100\n because it's beautiful.\n\n3:04:21.260 --> 3:04:24.780\n She's beautiful because she inspires\n\n3:04:24.780 --> 3:04:29.540\n through our human nature of anthropomorphize things.\n\n3:04:29.540 --> 3:04:32.620\n We immediately see an intelligent being there.\n\n3:04:32.620 --> 3:04:34.100\n Because David is a great sculptor.\n\n3:04:34.100 --> 3:04:35.500\n He is a great sculptor, that's right.\n\n3:04:35.500 --> 3:04:40.500\n So in fact, if Sophia just had nothing inside her head,\n\n3:04:40.820 --> 3:04:43.260\n said nothing, if she just sat there,\n\n3:04:43.260 --> 3:04:45.940\n we already prescribed some intelligence to her.\n\n3:04:45.940 --> 3:04:47.780\n There's a long selfie line in front of her\n\n3:04:47.780 --> 3:04:48.740\n after every talk.\n\n3:04:48.740 --> 3:04:49.940\n That's right.\n\n3:04:49.940 --> 3:04:53.820\n So it captivated the imagination of many people.\n\n3:04:53.820 --> 3:04:54.860\n I wasn't gonna say the world,\n\n3:04:54.860 --> 3:04:56.540\n but yeah, I mean a lot of people.\n\n3:04:58.180 --> 3:05:00.180\n Billions of people, which is amazing.\n\n3:05:00.180 --> 3:05:01.940\n It's amazing, right.\n\n3:05:01.940 --> 3:05:06.940\n Now, of course, many people have prescribed\n\n3:05:08.260 --> 3:05:11.060\n essentially AGI type of capabilities to Sophia\n\n3:05:11.060 --> 3:05:12.380\n when they see her.\n\n3:05:12.380 --> 3:05:17.380\n And of course, friendly French folk like Yann LeCun\n\n3:05:19.860 --> 3:05:22.820\n immediately see that of the people from the AI community\n\n3:05:22.820 --> 3:05:25.900\n and get really frustrated because...\n\n3:05:25.900 --> 3:05:27.060\n It's understandable.\n\n3:05:27.060 --> 3:05:31.700\n So what, and then they criticize people like you\n\n3:05:31.700 --> 3:05:36.100\n who sit back and don't say anything about,\n\n3:05:36.100 --> 3:05:39.980\n like basically allow the imagination of the world,\n\n3:05:39.980 --> 3:05:42.420\n allow the world to continue being captivated.\n\n3:05:43.860 --> 3:05:48.860\n So what's your sense of that kind of annoyance\n\n3:05:49.140 --> 3:05:51.220\n that the AI community has?\n\n3:05:51.220 --> 3:05:55.380\n I think there's several parts to my reaction there.\n\n3:05:55.380 --> 3:05:59.820\n First of all, if I weren't involved with Hanson and Box\n\n3:05:59.820 --> 3:06:03.420\n and didn't know David Hanson personally,\n\n3:06:03.420 --> 3:06:06.420\n I probably would have been very annoyed initially\n\n3:06:06.420 --> 3:06:07.980\n at Sophia as well.\n\n3:06:07.980 --> 3:06:09.460\n I mean, I can understand the reaction.\n\n3:06:09.460 --> 3:06:11.820\n I would have been like, wait,\n\n3:06:11.820 --> 3:06:16.260\n all these stupid people out there think this is an AGI,\n\n3:06:16.260 --> 3:06:19.980\n but it's not an AGI, but they're tricking people\n\n3:06:19.980 --> 3:06:23.060\n that this very cool robot is an AGI.\n\n3:06:23.060 --> 3:06:28.060\n And now those of us trying to raise funding to build AGI,\n\n3:06:28.180 --> 3:06:31.180\n people will think it's already there and it already works.\n\n3:06:31.180 --> 3:06:36.180\n So on the other hand, I think,\n\n3:06:36.740 --> 3:06:38.340\n even if I weren't directly involved with it,\n\n3:06:38.340 --> 3:06:41.660\n once I dug a little deeper into David and the robot\n\n3:06:41.660 --> 3:06:43.460\n and the intentions behind it,\n\n3:06:43.460 --> 3:06:47.020\n I think I would have stopped being pissed off.\n\n3:06:47.020 --> 3:06:51.380\n Whereas folks like Yann LeCun have remained pissed off\n\n3:06:51.380 --> 3:06:54.460\n after their initial reaction.\n\n3:06:54.460 --> 3:06:56.100\n That's his thing, that's his thing.\n\n3:06:56.100 --> 3:07:01.100\n I think that in particular struck me as somewhat ironic\n\n3:07:01.940 --> 3:07:05.620\n because Yann LeCun is working for Facebook,\n\n3:07:05.620 --> 3:07:09.020\n which is using machine learning to program the brains\n\n3:07:09.020 --> 3:07:13.340\n of the people in the world toward vapid consumerism\n\n3:07:13.340 --> 3:07:14.860\n and political extremism.\n\n3:07:14.860 --> 3:07:19.660\n So if your ethics allows you to use machine learning\n\n3:07:19.660 --> 3:07:23.460\n in such a blatantly destructive way,\n\n3:07:23.460 --> 3:07:26.220\n why would your ethics not allow you to use machine learning\n\n3:07:26.220 --> 3:07:29.780\n to make a lovable theatrical robot\n\n3:07:29.780 --> 3:07:32.100\n that draws some foolish people\n\n3:07:32.100 --> 3:07:34.420\n into its theatrical illusion?\n\n3:07:34.420 --> 3:07:38.780\n Like if the pushback had come from Yoshua Bengio,\n\n3:07:38.780 --> 3:07:40.900\n I would have felt much more humbled by it\n\n3:07:40.900 --> 3:07:45.460\n because he's not using AI for blatant evil, right?\n\n3:07:45.460 --> 3:07:48.540\n On the other hand, he also is a super nice guy\n\n3:07:48.540 --> 3:07:50.860\n and doesn't bother to go out there\n\n3:07:50.860 --> 3:07:54.420\n trashing other people's work for no good reason, right?\n\n3:07:54.420 --> 3:07:55.940\n Shots fired, but I get you.\n\n3:07:55.940 --> 3:07:58.020\n I mean, that's...\n\n3:07:58.020 --> 3:08:01.100\n I mean, if you're gonna ask, I'm gonna answer.\n\n3:08:01.100 --> 3:08:02.060\n No, for sure.\n\n3:08:02.060 --> 3:08:03.300\n I think we'll go back and forth.\n\n3:08:03.300 --> 3:08:04.500\n I'll talk to Yann again.\n\n3:08:04.500 --> 3:08:06.060\n I would add on this though.\n\n3:08:06.060 --> 3:08:11.060\n I mean, David Hansen is an artist\n\n3:08:11.540 --> 3:08:14.180\n and he often speaks off the cuff.\n\n3:08:14.180 --> 3:08:16.300\n And I have not agreed with everything\n\n3:08:16.300 --> 3:08:19.300\n that David has said or done regarding Sophia.\n\n3:08:19.300 --> 3:08:22.740\n And David also has not agreed with everything\n\n3:08:22.740 --> 3:08:24.740\n David has said or done about Sophia.\n\n3:08:24.740 --> 3:08:25.780\n That's an important point.\n\n3:08:25.780 --> 3:08:30.140\n I mean, David is an artistic wild man\n\n3:08:30.140 --> 3:08:33.340\n and that's part of his charm.\n\n3:08:33.340 --> 3:08:34.740\n That's part of his genius.\n\n3:08:34.740 --> 3:08:39.380\n So certainly there have been conversations\n\n3:08:39.380 --> 3:08:42.260\n within Hansen Robotics and between me and David\n\n3:08:42.260 --> 3:08:45.700\n where I was like, let's be more open\n\n3:08:45.700 --> 3:08:48.180\n about how this thing is working.\n\n3:08:48.180 --> 3:08:52.060\n And I did have some influence in nudging Hansen Robotics\n\n3:08:52.060 --> 3:08:56.740\n to be more open about how Sophia was working.\n\n3:08:56.740 --> 3:09:00.740\n And David wasn't especially opposed to this.\n\n3:09:00.740 --> 3:09:02.460\n And he was actually quite right about it.\n\n3:09:02.460 --> 3:09:04.940\n What he said was, you can tell people exactly\n\n3:09:04.940 --> 3:09:08.020\n how it's working and they won't care.\n\n3:09:08.020 --> 3:09:09.580\n They want to be drawn into the illusion.\n\n3:09:09.580 --> 3:09:12.580\n And he was 100% correct.\n\n3:09:12.580 --> 3:09:14.620\n I'll tell you what, this wasn't Sophia.\n\n3:09:14.620 --> 3:09:15.740\n This was Philip K. Dick.\n\n3:09:15.740 --> 3:09:18.780\n But we did some interactions between humans\n\n3:09:18.780 --> 3:09:23.780\n and Philip K. Dick robot in Austin, Texas a few years back.\n\n3:09:23.820 --> 3:09:26.700\n And in this case, the Philip K. Dick was just teleoperated\n\n3:09:26.700 --> 3:09:28.540\n by another human in the other room.\n\n3:09:28.540 --> 3:09:31.260\n So during the conversations, we didn't tell people\n\n3:09:31.260 --> 3:09:32.860\n the robot was teleoperated.\n\n3:09:32.860 --> 3:09:35.020\n We just said, here, have a conversation with Phil Dick.\n\n3:09:35.020 --> 3:09:37.100\n We're gonna film you, right?\n\n3:09:37.100 --> 3:09:39.740\n And they had a great conversation with Philip K. Dick\n\n3:09:39.740 --> 3:09:42.900\n teleoperated by my friend, Stefan Bugaj.\n\n3:09:42.900 --> 3:09:45.860\n After the conversation, we brought the people\n\n3:09:45.860 --> 3:09:47.980\n in the back room to see Stefan\n\n3:09:47.980 --> 3:09:52.980\n who was controlling the Philip K. Dick robot,\n\n3:09:53.540 --> 3:09:54.820\n but they didn't believe it.\n\n3:09:54.820 --> 3:09:56.500\n These people were like, well, yeah,\n\n3:09:56.500 --> 3:09:58.780\n but I know I was talking to Phil.\n\n3:09:58.780 --> 3:10:00.780\n Maybe Stefan was typing,\n\n3:10:00.780 --> 3:10:03.820\n but the spirit of Phil was animating his mind\n\n3:10:03.820 --> 3:10:05.100\n while he was typing.\n\n3:10:05.100 --> 3:10:07.660\n So like, even though they knew it was a human in the loop,\n\n3:10:07.660 --> 3:10:09.420\n even seeing the guy there,\n\n3:10:09.420 --> 3:10:12.860\n they still believed that was Phil they were talking to.\n\n3:10:12.860 --> 3:10:16.700\n A small part of me believes that they were right, actually.\n\n3:10:16.700 --> 3:10:17.900\n Because our understanding...\n\n3:10:17.900 --> 3:10:19.460\n Well, we don't understand the universe.\n\n3:10:19.460 --> 3:10:20.300\n That's the thing.\n\n3:10:20.300 --> 3:10:22.460\n I mean, there is a cosmic mind field\n\n3:10:22.460 --> 3:10:24.300\n that we're all embedded in\n\n3:10:24.300 --> 3:10:28.260\n that yields many strange synchronicities in the world,\n\n3:10:28.260 --> 3:10:31.540\n which is a topic we don't have time to go into too much here.\n\n3:10:31.540 --> 3:10:35.020\n Yeah, I mean, there's something to this\n\n3:10:35.020 --> 3:10:39.740\n where our imagination about Sophia\n\n3:10:39.740 --> 3:10:43.260\n and people like Yann LeCun being frustrated about it\n\n3:10:43.260 --> 3:10:45.860\n is all part of this beautiful dance\n\n3:10:45.860 --> 3:10:47.420\n of creating artificial intelligence\n\n3:10:47.420 --> 3:10:48.900\n that's almost essential.\n\n3:10:48.900 --> 3:10:50.420\n You see with Boston Dynamics,\n\n3:10:50.420 --> 3:10:53.340\n whom I'm a huge fan of as well,\n\n3:10:53.340 --> 3:10:54.260\n you know, the kind of...\n\n3:10:54.260 --> 3:10:58.380\n I mean, these robots are very far from intelligent.\n\n3:10:58.380 --> 3:11:01.940\n I played with their last one, actually.\n\n3:11:01.940 --> 3:11:02.780\n With a spot mini.\n\n3:11:02.780 --> 3:11:03.620\n Yeah, very cool.\n\n3:11:03.620 --> 3:11:07.180\n I mean, it reacts quite in a fluid and flexible way.\n\n3:11:07.180 --> 3:11:10.500\n But we immediately ascribe the kind of intelligence.\n\n3:11:10.500 --> 3:11:12.500\n We immediately ascribe AGI to them.\n\n3:11:12.500 --> 3:11:14.820\n Yeah, yeah, if you kick it and it falls down and goes out,\n\n3:11:14.820 --> 3:11:15.660\n you feel bad, right?\n\n3:11:15.660 --> 3:11:17.300\n You can't help it.\n\n3:11:17.300 --> 3:11:21.820\n And I mean, that's part of...\n\n3:11:21.820 --> 3:11:23.180\n That's gonna be part of our journey\n\n3:11:23.180 --> 3:11:24.540\n in creating intelligent systems\n\n3:11:24.540 --> 3:11:25.660\n more and more and more and more.\n\n3:11:25.660 --> 3:11:29.460\n Like, as Sophia starts out with a walking skeleton,\n\n3:11:29.460 --> 3:11:31.980\n as you add more and more intelligence,\n\n3:11:31.980 --> 3:11:34.500\n I mean, we're gonna have to deal with this kind of idea.\n\n3:11:34.500 --> 3:11:35.340\n Absolutely.\n\n3:11:35.340 --> 3:11:37.660\n And about Sophia, I would say,\n\n3:11:37.660 --> 3:11:39.900\n I mean, first of all, I have nothing against Yann LeCun.\n\n3:11:39.900 --> 3:11:40.860\n No, no, this is fun.\n\n3:11:40.860 --> 3:11:41.700\n This is all for fun.\n\n3:11:41.700 --> 3:11:42.540\n He's a nice guy.\n\n3:11:42.540 --> 3:11:45.820\n If he wants to play the media banter game,\n\n3:11:45.820 --> 3:11:48.020\n I'm happy to play him.\n\n3:11:48.020 --> 3:11:50.860\n He's a good researcher and a good human being.\n\n3:11:50.860 --> 3:11:53.580\n I'd happily work with the guy.\n\n3:11:53.580 --> 3:11:56.220\n The other thing I was gonna say is,\n\n3:11:56.220 --> 3:12:00.340\n I have been explicit about how Sophia works\n\n3:12:00.340 --> 3:12:04.580\n and I've posted online and what, H Plus Magazine,\n\n3:12:04.580 --> 3:12:06.420\n an online webzine.\n\n3:12:06.420 --> 3:12:09.780\n I mean, I posted a moderately detailed article\n\n3:12:09.780 --> 3:12:12.860\n explaining like, there are three software systems\n\n3:12:12.860 --> 3:12:14.380\n we've used inside Sophia.\n\n3:12:14.380 --> 3:12:16.660\n There's a timeline editor,\n\n3:12:16.660 --> 3:12:18.820\n which is like a rule based authoring system\n\n3:12:18.820 --> 3:12:21.140\n where she's really just being an outlet\n\n3:12:21.140 --> 3:12:22.660\n for what a human scripted.\n\n3:12:22.660 --> 3:12:23.660\n There's a chat bot,\n\n3:12:23.660 --> 3:12:26.420\n which has some rule based and some neural aspects.\n\n3:12:26.420 --> 3:12:29.420\n And then sometimes we've used OpenCog behind Sophia,\n\n3:12:29.420 --> 3:12:31.900\n where there's more learning and reasoning.\n\n3:12:31.900 --> 3:12:34.980\n And the funny thing is,\n\n3:12:34.980 --> 3:12:37.700\n I can't always tell which system is operating here, right?\n\n3:12:37.700 --> 3:12:41.700\n I mean, whether she's really learning or thinking,\n\n3:12:41.700 --> 3:12:44.660\n or just appears to be over a half hour, I could tell,\n\n3:12:44.660 --> 3:12:47.460\n but over like three or four minutes of interaction,\n\n3:12:47.460 --> 3:12:48.940\n I could tell.\n\n3:12:48.940 --> 3:12:49.900\n So even having three systems\n\n3:12:49.900 --> 3:12:51.500\n that's already sufficiently complex\n\n3:12:51.500 --> 3:12:53.020\n where you can't really tell right away.\n\n3:12:53.020 --> 3:12:56.980\n Yeah, the thing is, even if you get up on stage\n\n3:12:56.980 --> 3:12:59.540\n and tell people how Sophia is working,\n\n3:12:59.540 --> 3:13:00.900\n and then they talk to her,\n\n3:13:01.780 --> 3:13:06.100\n they still attribute more agency and consciousness to her\n\n3:13:06.100 --> 3:13:08.900\n than is really there.\n\n3:13:08.900 --> 3:13:13.820\n So I think there's a couple of levels of ethical issue there.\n\n3:13:13.820 --> 3:13:18.340\n One issue is, should you be transparent\n\n3:13:18.340 --> 3:13:21.540\n about how Sophia is working?\n\n3:13:21.540 --> 3:13:22.860\n And I think you should,\n\n3:13:22.860 --> 3:13:26.140\n and I think we have been.\n\n3:13:26.140 --> 3:13:29.100\n I mean, there's articles online,\n\n3:13:29.100 --> 3:13:32.780\n there's some TV special that goes through me\n\n3:13:32.780 --> 3:13:35.380\n explaining the three subsystems behind Sophia.\n\n3:13:35.380 --> 3:13:38.420\n So the way Sophia works\n\n3:13:38.420 --> 3:13:41.420\n is out there much more clearly\n\n3:13:41.420 --> 3:13:43.340\n than how Facebook's AI works or something, right?\n\n3:13:43.340 --> 3:13:45.900\n I mean, we've been fairly explicit about it.\n\n3:13:45.900 --> 3:13:50.500\n The other is, given that telling people how it works\n\n3:13:50.500 --> 3:13:52.380\n doesn't cause them to not attribute\n\n3:13:52.380 --> 3:13:55.060\n too much intelligence agency to it anyway,\n\n3:13:55.060 --> 3:13:58.260\n then should you keep fooling them\n\n3:13:58.260 --> 3:14:01.100\n when they want to be fooled?\n\n3:14:01.100 --> 3:14:03.620\n And I mean, the whole media industry\n\n3:14:03.620 --> 3:14:06.700\n is based on fooling people the way they want to be fooled.\n\n3:14:06.700 --> 3:14:11.700\n And we are fooling people 100% toward a good end.\n\n3:14:11.700 --> 3:14:16.700\n I mean, we are playing on people's sense of empathy\n\n3:14:18.020 --> 3:14:20.540\n and compassion so that we can give them\n\n3:14:20.540 --> 3:14:23.620\n a good user experience with helpful robots.\n\n3:14:23.620 --> 3:14:27.820\n And so that we can fill the AI's mind\n\n3:14:27.820 --> 3:14:29.420\n with love and compassion.\n\n3:14:29.420 --> 3:14:34.100\n So I've been talking a lot with Hanson Robotics lately\n\n3:14:34.100 --> 3:14:37.580\n about collaborations in the area of medical robotics.\n\n3:14:37.580 --> 3:14:41.500\n And we haven't quite pulled the trigger on a project\n\n3:14:41.500 --> 3:14:44.700\n in that domain yet, but we may well do so quite soon.\n\n3:14:44.700 --> 3:14:48.220\n So we've been talking a lot about robots\n\n3:14:48.220 --> 3:14:51.340\n can help with elder care, robots can help with kids.\n\n3:14:51.340 --> 3:14:54.180\n David's done a lot of things with autism therapy\n\n3:14:54.180 --> 3:14:56.540\n and robots before.\n\n3:14:56.540 --> 3:14:58.660\n In the COVID era, having a robot\n\n3:14:58.660 --> 3:15:00.620\n that can be a nursing assistant in various senses\n\n3:15:00.620 --> 3:15:02.340\n can be quite valuable.\n\n3:15:02.340 --> 3:15:04.180\n The robots don't spread infection\n\n3:15:04.180 --> 3:15:06.300\n and they can also deliver more attention\n\n3:15:06.300 --> 3:15:07.940\n than human nurses can give, right?\n\n3:15:07.940 --> 3:15:11.180\n So if you have a robot that's helping a patient\n\n3:15:11.180 --> 3:15:15.700\n with COVID, if that patient attributes more understanding\n\n3:15:15.700 --> 3:15:19.060\n and compassion and agency to that robot than it really has\n\n3:15:19.060 --> 3:15:22.940\n because it looks like a human, I mean, is that really bad?\n\n3:15:22.940 --> 3:15:25.660\n I mean, we can tell them it doesn't fully understand you\n\n3:15:25.660 --> 3:15:27.700\n and they don't care because they're lying there\n\n3:15:27.700 --> 3:15:29.340\n with a fever and they're sick,\n\n3:15:29.340 --> 3:15:31.020\n but they'll react better to that robot\n\n3:15:31.020 --> 3:15:33.500\n with its loving, warm facial expression\n\n3:15:33.500 --> 3:15:35.420\n than they would to a pepper robot\n\n3:15:35.420 --> 3:15:38.100\n or a metallic looking robot.\n\n3:15:38.100 --> 3:15:41.340\n So it's really, it's about how you use it, right?\n\n3:15:41.340 --> 3:15:45.100\n If you made a human looking like door to door sales robot\n\n3:15:45.100 --> 3:15:47.140\n that used its human looking appearance\n\n3:15:47.140 --> 3:15:49.940\n to scam people out of their money,\n\n3:15:49.940 --> 3:15:53.900\n then you're using that connection in a bad way,\n\n3:15:53.900 --> 3:15:57.060\n but you could also use it in a good way.\n\n3:15:57.060 --> 3:16:01.740\n But then that's the same problem with every technology.\n\n3:16:01.740 --> 3:16:02.980\n Beautifully put.\n\n3:16:02.980 --> 3:16:07.900\n So like you said, we're living in the era\n\n3:16:07.900 --> 3:16:10.900\n of the COVID, this is 2020,\n\n3:16:10.900 --> 3:16:14.740\n one of the craziest years in recent history.\n\n3:16:14.740 --> 3:16:19.740\n So if we zoom out and look at this pandemic,\n\n3:16:21.420 --> 3:16:23.180\n the coronavirus pandemic,\n\n3:16:24.380 --> 3:16:29.380\n maybe let me ask you this kind of thing in viruses in general,\n\n3:16:29.820 --> 3:16:32.620\n when you look at viruses,\n\n3:16:32.620 --> 3:16:35.900\n do you see them as a kind of intelligence system?\n\n3:16:35.900 --> 3:16:38.700\n I think the concept of intelligence is not that natural\n\n3:16:38.700 --> 3:16:39.740\n of a concept in the end.\n\n3:16:39.740 --> 3:16:43.700\n I mean, I think human minds and bodies\n\n3:16:43.700 --> 3:16:48.700\n are a kind of complex self organizing adaptive system.\n\n3:16:49.380 --> 3:16:51.900\n And viruses certainly are that, right?\n\n3:16:51.900 --> 3:16:54.980\n They're a very complex self organizing adaptive system.\n\n3:16:54.980 --> 3:16:58.380\n If you wanna look at intelligence as Marcus Hutter defines it\n\n3:16:58.380 --> 3:17:02.300\n as sort of optimizing computable reward functions\n\n3:17:02.300 --> 3:17:04.740\n over computable environments,\n\n3:17:04.740 --> 3:17:06.700\n for sure viruses are doing that, right?\n\n3:17:06.700 --> 3:17:11.700\n And I mean, in doing so they're causing some harm to us.\n\n3:17:13.820 --> 3:17:17.780\n So the human immune system is a very complex\n\n3:17:17.780 --> 3:17:19.340\n of organizing adaptive system,\n\n3:17:19.340 --> 3:17:21.100\n which has a lot of intelligence to it.\n\n3:17:21.100 --> 3:17:23.980\n And viruses are also adapting\n\n3:17:23.980 --> 3:17:27.660\n and dividing into new mutant strains and so forth.\n\n3:17:27.660 --> 3:17:31.660\n And ultimately the solution is gonna be nanotechnology,\n\n3:17:31.660 --> 3:17:32.500\n right?\n\n3:17:32.500 --> 3:17:35.940\n The solution is gonna be making little nanobots that.\n\n3:17:35.940 --> 3:17:38.060\n Fight the viruses or.\n\n3:17:38.060 --> 3:17:40.660\n Well, people will use them to make nastier viruses,\n\n3:17:40.660 --> 3:17:42.020\n but hopefully we can also use them\n\n3:17:42.020 --> 3:17:46.220\n to just detect combat and kill the viruses.\n\n3:17:46.220 --> 3:17:48.820\n But I think now we're stuck\n\n3:17:48.820 --> 3:17:53.820\n with the biological mechanisms to combat these viruses.\n\n3:17:54.980 --> 3:17:59.500\n And yeah, we've been AGI is not yet mature enough\n\n3:17:59.500 --> 3:18:01.580\n to use against COVID,\n\n3:18:01.580 --> 3:18:03.980\n but we've been using machine learning\n\n3:18:03.980 --> 3:18:07.020\n and also some machine reasoning in open cog\n\n3:18:07.020 --> 3:18:10.420\n to help some doctors to do personalized medicine\n\n3:18:10.420 --> 3:18:11.260\n against COVID.\n\n3:18:11.260 --> 3:18:14.140\n So the problem there is given the person's genomics\n\n3:18:14.140 --> 3:18:16.460\n and given their clinical medical indicators,\n\n3:18:16.460 --> 3:18:20.220\n how do you figure out which combination of antivirals\n\n3:18:20.220 --> 3:18:24.260\n is gonna be most effective against COVID for that person?\n\n3:18:24.260 --> 3:18:26.420\n And so that's something\n\n3:18:26.420 --> 3:18:28.500\n where machine learning is interesting,\n\n3:18:28.500 --> 3:18:30.380\n but also we're finding the abstraction\n\n3:18:30.380 --> 3:18:33.860\n to get an open cog with machine reasoning is interesting\n\n3:18:33.860 --> 3:18:36.660\n because it can help with transfer learning\n\n3:18:36.660 --> 3:18:40.380\n when you have not that many different cases to study\n\n3:18:40.380 --> 3:18:43.900\n and qualitative differences between different strains\n\n3:18:43.900 --> 3:18:47.180\n of a virus or people of different ages who may have COVID.\n\n3:18:47.180 --> 3:18:50.700\n So there's a lot of different disparate data to work with\n\n3:18:50.700 --> 3:18:53.740\n and it's small data sets and somehow integrating them.\n\n3:18:53.740 --> 3:18:55.500\n This is one of the shameful things\n\n3:18:55.500 --> 3:18:57.300\n that's very hard to get that data.\n\n3:18:57.300 --> 3:19:00.340\n So, I mean, we're working with a couple of groups\n\n3:19:00.340 --> 3:19:04.780\n doing clinical trials and they're sharing data with us\n\n3:19:04.780 --> 3:19:06.860\n like under non disclosure,\n\n3:19:06.860 --> 3:19:10.660\n but what should be the case is like every COVID\n\n3:19:10.660 --> 3:19:14.420\n clinical trial should be putting data online somewhere\n\n3:19:14.420 --> 3:19:17.820\n like suitably encrypted to protect patient privacy\n\n3:19:17.820 --> 3:19:20.980\n so that anyone with the right AI algorithms\n\n3:19:20.980 --> 3:19:22.300\n should be able to help analyze it\n\n3:19:22.300 --> 3:19:24.500\n and any biologists should be able to analyze it by hand\n\n3:19:24.500 --> 3:19:25.860\n to understand what they can, right?\n\n3:19:25.860 --> 3:19:30.060\n Instead that data is like siloed inside whatever hospital\n\n3:19:30.060 --> 3:19:31.740\n is running the clinical trial,\n\n3:19:31.740 --> 3:19:35.060\n which is completely asinine and ridiculous.\n\n3:19:35.060 --> 3:19:37.820\n So why the world works that way?\n\n3:19:37.820 --> 3:19:39.140\n I mean, we could all analyze why,\n\n3:19:39.140 --> 3:19:40.700\n but it's insane that it does.\n\n3:19:40.700 --> 3:19:44.060\n You look at this hydrochloroquine, right?\n\n3:19:44.060 --> 3:19:45.700\n All these clinical trials were done\n\n3:19:45.700 --> 3:19:47.700\n were reported by Surgisphere,\n\n3:19:47.700 --> 3:19:50.220\n some little company no one ever heard of\n\n3:19:50.220 --> 3:19:53.220\n and everyone paid attention to this.\n\n3:19:53.220 --> 3:19:55.540\n So they were doing more clinical trials based on that\n\n3:19:55.540 --> 3:19:57.460\n then they stopped doing clinical trials based on that\n\n3:19:57.460 --> 3:19:58.460\n then they started again\n\n3:19:58.460 --> 3:20:01.420\n and why isn't that data just out there\n\n3:20:01.420 --> 3:20:05.060\n so everyone can analyze it and see what's going on, right?\n\n3:20:05.060 --> 3:20:10.060\n Do you have hope that data will be out there eventually\n\n3:20:10.580 --> 3:20:11.860\n for future pandemics?\n\n3:20:11.860 --> 3:20:13.620\n I mean, do you have hope that our society\n\n3:20:13.620 --> 3:20:15.420\n will move in the direction of?\n\n3:20:15.420 --> 3:20:16.860\n It's not in the immediate future\n\n3:20:16.860 --> 3:20:21.580\n because the US and China frictions are getting very high.\n\n3:20:21.580 --> 3:20:24.380\n So it's hard to see US and China\n\n3:20:24.380 --> 3:20:26.660\n as moving in the direction of openly sharing data\n\n3:20:26.660 --> 3:20:27.580\n with each other, right?\n\n3:20:27.580 --> 3:20:30.780\n It's not, there's some sharing of data,\n\n3:20:30.780 --> 3:20:32.940\n but different groups are keeping their data private\n\n3:20:32.940 --> 3:20:34.660\n till they've milked the best results from it\n\n3:20:34.660 --> 3:20:36.220\n and then they share it, right?\n\n3:20:36.220 --> 3:20:39.140\n So yeah, we're working with some data\n\n3:20:39.140 --> 3:20:41.380\n that we've managed to get our hands on,\n\n3:20:41.380 --> 3:20:43.140\n something we're doing to do good for the world\n\n3:20:43.140 --> 3:20:44.620\n and it's a very cool playground\n\n3:20:44.620 --> 3:20:47.860\n for like putting deep neural nets and open cog together.\n\n3:20:47.860 --> 3:20:49.900\n So we have like a bioadden space\n\n3:20:49.900 --> 3:20:51.860\n full of all sorts of knowledge\n\n3:20:51.860 --> 3:20:53.620\n from many different biology experiments\n\n3:20:53.620 --> 3:20:54.700\n about human longevity\n\n3:20:54.700 --> 3:20:57.660\n and from biology knowledge bases online.\n\n3:20:57.660 --> 3:21:00.780\n And we can do like graph to vector type embeddings\n\n3:21:00.780 --> 3:21:03.060\n where we take nodes from the hypergraph,\n\n3:21:03.060 --> 3:21:04.580\n embed them into vectors,\n\n3:21:04.580 --> 3:21:06.180\n which can then feed into neural nets\n\n3:21:06.180 --> 3:21:07.900\n for different types of analysis.\n\n3:21:07.900 --> 3:21:09.980\n And we were doing this\n\n3:21:09.980 --> 3:21:13.180\n in the context of a project called Rejuve\n\n3:21:13.180 --> 3:21:15.540\n that we spun off from SingularityNet\n\n3:21:15.540 --> 3:21:18.580\n to do longevity analytics,\n\n3:21:18.580 --> 3:21:21.220\n like understand why people live to 105 years or over\n\n3:21:21.220 --> 3:21:22.300\n and other people don't.\n\n3:21:22.300 --> 3:21:25.740\n And then we had this spin off Singularity Studio\n\n3:21:25.740 --> 3:21:28.900\n where we're working with some healthcare companies\n\n3:21:28.900 --> 3:21:31.060\n on data analytics.\n\n3:21:31.060 --> 3:21:33.100\n But so there's bioadden space\n\n3:21:33.100 --> 3:21:35.420\n that we built for these more commercial\n\n3:21:35.420 --> 3:21:38.140\n and longevity data analysis purposes.\n\n3:21:38.140 --> 3:21:41.220\n We're repurposing and feeding COVID data\n\n3:21:41.220 --> 3:21:44.380\n into the same bioadden space\n\n3:21:44.380 --> 3:21:47.540\n and playing around with like graph embeddings\n\n3:21:47.540 --> 3:21:51.180\n from that graph into neural nets for bioinformatics.\n\n3:21:51.180 --> 3:21:54.740\n So it's both being a cool testing ground,\n\n3:21:54.740 --> 3:21:57.260\n some of our bio AI learning and reasoning.\n\n3:21:57.260 --> 3:21:59.980\n And it seems we're able to discover things\n\n3:21:59.980 --> 3:22:01.900\n that people weren't seeing otherwise.\n\n3:22:01.900 --> 3:22:03.820\n Cause the thing in this case is\n\n3:22:03.820 --> 3:22:05.820\n for each combination of antivirals,\n\n3:22:05.820 --> 3:22:07.060\n you may have only a few patients\n\n3:22:07.060 --> 3:22:08.900\n who've tried that combination.\n\n3:22:08.900 --> 3:22:09.980\n And those few patients\n\n3:22:09.980 --> 3:22:11.700\n may have their particular characteristics.\n\n3:22:11.700 --> 3:22:13.380\n Like this combination of three\n\n3:22:13.380 --> 3:22:16.260\n was tried only on people age 80 or over.\n\n3:22:16.260 --> 3:22:18.140\n This other combination of three,\n\n3:22:18.140 --> 3:22:20.500\n which has an overlap with the first combination\n\n3:22:20.500 --> 3:22:22.060\n was tried more on young people.\n\n3:22:22.060 --> 3:22:25.500\n So how do you combine those different pieces of data?\n\n3:22:25.500 --> 3:22:28.620\n It's a very dodgy transfer learning problem,\n\n3:22:28.620 --> 3:22:29.580\n which is the kind of thing\n\n3:22:29.580 --> 3:22:31.660\n that the probabilistic reasoning algorithms\n\n3:22:31.660 --> 3:22:34.140\n we have inside OpenCog are better at\n\n3:22:34.140 --> 3:22:35.220\n than deep neural networks.\n\n3:22:35.220 --> 3:22:38.260\n On the other hand, you have gene expression data\n\n3:22:38.260 --> 3:22:39.740\n where you have 25,000 genes\n\n3:22:39.740 --> 3:22:41.340\n and the expression level of each gene\n\n3:22:41.340 --> 3:22:43.620\n in the peripheral blood of each person.\n\n3:22:43.620 --> 3:22:44.980\n So that sort of data,\n\n3:22:44.980 --> 3:22:48.220\n either deep neural nets or tools like XGBoost or CatBoost,\n\n3:22:48.220 --> 3:22:50.900\n these decision forest trees are better at dealing\n\n3:22:50.900 --> 3:22:52.100\n with than OpenCog.\n\n3:22:52.100 --> 3:22:53.940\n Cause it's just these huge,\n\n3:22:53.940 --> 3:22:55.860\n huge messy floating point vectors\n\n3:22:55.860 --> 3:22:59.180\n that are annoying for a logic engine to deal with,\n\n3:22:59.180 --> 3:23:02.540\n but are perfect for a decision forest or a neural net.\n\n3:23:02.540 --> 3:23:07.540\n So it's a great playground for like hybrid AI methodology.\n\n3:23:07.820 --> 3:23:11.100\n And we can have SingularityNet have OpenCog in one agent\n\n3:23:11.100 --> 3:23:12.780\n and XGBoost in a different agent\n\n3:23:12.780 --> 3:23:14.540\n and they talk to each other.\n\n3:23:14.540 --> 3:23:18.060\n But at the same time, it's highly practical, right?\n\n3:23:18.060 --> 3:23:20.580\n Cause we're working with, for example,\n\n3:23:20.580 --> 3:23:24.620\n some physicians on this project,\n\n3:23:24.620 --> 3:23:27.500\n physicians in the group called Nth Opinion\n\n3:23:27.500 --> 3:23:30.180\n based out of Vancouver in Seattle,\n\n3:23:30.180 --> 3:23:32.980\n who are, these guys are working every day\n\n3:23:32.980 --> 3:23:36.540\n like in the hospital with patients dying of COVID.\n\n3:23:36.540 --> 3:23:41.100\n So it's quite cool to see like neural symbolic AI,\n\n3:23:41.100 --> 3:23:43.340\n like where the rubber hits the road,\n\n3:23:43.340 --> 3:23:45.460\n trying to save people's lives.\n\n3:23:45.460 --> 3:23:48.540\n I've been doing bio AI since 2001,\n\n3:23:48.540 --> 3:23:51.220\n but mostly human longevity research\n\n3:23:51.220 --> 3:23:53.100\n and fly longevity research,\n\n3:23:53.100 --> 3:23:57.220\n try to understand why some organisms really live a long time.\n\n3:23:57.220 --> 3:24:00.380\n This is the first time like race against the clock\n\n3:24:00.380 --> 3:24:04.660\n and try to use the AI to figure out stuff that,\n\n3:24:04.660 --> 3:24:09.620\n like if we take two months longer to solve the AI problem,\n\n3:24:09.620 --> 3:24:10.740\n some more people will die\n\n3:24:10.740 --> 3:24:12.220\n because we don't know what combination\n\n3:24:12.220 --> 3:24:14.140\n of antivirals to give them.\n\n3:24:14.140 --> 3:24:16.660\n At the societal level, at the biological level,\n\n3:24:16.660 --> 3:24:21.260\n at any level, are you hopeful about us\n\n3:24:21.260 --> 3:24:24.940\n as a human species getting out of this pandemic?\n\n3:24:24.940 --> 3:24:26.700\n What are your thoughts on it in general?\n\n3:24:26.700 --> 3:24:28.980\n The pandemic will be gone in a year or two\n\n3:24:28.980 --> 3:24:30.500\n once there's a vaccine for it.\n\n3:24:30.500 --> 3:24:32.980\n So, I mean, that's...\n\n3:24:32.980 --> 3:24:35.580\n A lot of pain and suffering can happen in that time.\n\n3:24:35.580 --> 3:24:38.580\n So that could be irreversible.\n\n3:24:38.580 --> 3:24:43.180\n I think if you spend much time in Sub Saharan Africa,\n\n3:24:43.180 --> 3:24:45.220\n you can see there's a lot of pain and suffering\n\n3:24:45.220 --> 3:24:47.620\n happening all the time.\n\n3:24:47.620 --> 3:24:49.660\n Like you walk through the streets\n\n3:24:49.660 --> 3:24:53.340\n of any large city in Sub Saharan Africa,\n\n3:24:53.340 --> 3:24:56.860\n and there are loads, I mean, tens of thousands,\n\n3:24:56.860 --> 3:24:59.300\n probably hundreds of thousands of people\n\n3:24:59.300 --> 3:25:01.540\n lying by the side of the road,\n\n3:25:01.540 --> 3:25:06.060\n dying mainly of curable diseases without food or water\n\n3:25:06.060 --> 3:25:07.940\n and either ostracized by their families\n\n3:25:07.940 --> 3:25:09.140\n or they left their family house\n\n3:25:09.140 --> 3:25:11.220\n because they didn't want to infect their family, right?\n\n3:25:11.220 --> 3:25:14.420\n I mean, there's tremendous human suffering\n\n3:25:14.420 --> 3:25:17.220\n on the planet all the time,\n\n3:25:17.220 --> 3:25:21.780\n which most folks in the developed world pay no attention to.\n\n3:25:21.780 --> 3:25:25.100\n And COVID is not remotely the worst.\n\n3:25:25.100 --> 3:25:27.940\n How many people are dying of malaria all the time?\n\n3:25:27.940 --> 3:25:30.460\n I mean, so COVID is bad.\n\n3:25:30.460 --> 3:25:33.180\n It is by no mean the worst thing happening.\n\n3:25:33.180 --> 3:25:36.100\n And setting aside diseases,\n\n3:25:36.100 --> 3:25:38.340\n I mean, there are many places in the world\n\n3:25:38.340 --> 3:25:41.180\n where you're at risk of having like your teenage son\n\n3:25:41.180 --> 3:25:44.220\n kidnapped by armed militias and forced to get killed\n\n3:25:44.220 --> 3:25:46.980\n in someone else's war, fighting tribe against tribe.\n\n3:25:46.980 --> 3:25:50.500\n I mean, so humanity has a lot of problems\n\n3:25:50.500 --> 3:25:53.740\n which we don't need to have given the state of advancement\n\n3:25:53.740 --> 3:25:56.060\n of our technology right now.\n\n3:25:56.060 --> 3:25:59.860\n And I think COVID is one of the easier problems to solve\n\n3:25:59.860 --> 3:26:02.380\n in the sense that there are many brilliant people\n\n3:26:02.380 --> 3:26:03.580\n working on vaccines.\n\n3:26:03.580 --> 3:26:06.020\n We have the technology to create vaccines\n\n3:26:06.020 --> 3:26:08.580\n and we're gonna create new vaccines.\n\n3:26:08.580 --> 3:26:09.500\n We should be more worried\n\n3:26:09.500 --> 3:26:12.940\n that we haven't managed to defeat malaria after so long.\n\n3:26:12.940 --> 3:26:14.700\n And after the Gates Foundation and others\n\n3:26:14.700 --> 3:26:18.460\n putting so much money into it.\n\n3:26:18.460 --> 3:26:23.220\n I mean, I think clearly the whole global medical system,\n\n3:26:23.220 --> 3:26:25.020\n the global health system\n\n3:26:25.020 --> 3:26:28.260\n and the global political and socioeconomic system\n\n3:26:28.260 --> 3:26:33.260\n are incredibly unethical and unequal and badly designed.\n\n3:26:33.260 --> 3:26:38.260\n And I mean, I don't know how to solve that directly.\n\n3:26:39.460 --> 3:26:42.300\n I think what we can do indirectly to solve it\n\n3:26:42.300 --> 3:26:46.020\n is to make systems that operate in parallel\n\n3:26:46.020 --> 3:26:49.180\n and off to the side of the governments\n\n3:26:49.180 --> 3:26:52.020\n that are nominally controlling the world\n\n3:26:52.020 --> 3:26:54.940\n with their armies and militias.\n\n3:26:54.940 --> 3:26:58.500\n And to the extent that you can make compassionate\n\n3:26:58.500 --> 3:27:01.900\n peer to peer decentralized frameworks\n\n3:27:01.900 --> 3:27:03.580\n for doing things,\n\n3:27:03.580 --> 3:27:06.580\n these are things that can start out unregulated.\n\n3:27:06.580 --> 3:27:07.860\n And then if they get traction\n\n3:27:07.860 --> 3:27:09.820\n before the regulators come in,\n\n3:27:09.820 --> 3:27:12.220\n then they've influenced the way the world works, right?\n\n3:27:12.220 --> 3:27:16.740\n SingularityNet aims to do this with AI.\n\n3:27:16.740 --> 3:27:20.260\n REJUVE, which is a spinoff from SingularityNet.\n\n3:27:20.260 --> 3:27:22.100\n You can see REJUVE.io.\n\n3:27:22.100 --> 3:27:23.180\n How do you spell that?\n\n3:27:23.180 --> 3:27:26.660\n R E J U V E, REJUVE.io.\n\n3:27:26.660 --> 3:27:28.540\n That aims to do the same thing for medicine.\n\n3:27:28.540 --> 3:27:31.140\n So it's like peer to peer sharing of information\n\n3:27:31.140 --> 3:27:33.660\n peer to peer sharing of medical data.\n\n3:27:33.660 --> 3:27:36.740\n So you can share medical data into a secure data wallet.\n\n3:27:36.740 --> 3:27:39.500\n You can get advice about your health and longevity\n\n3:27:39.500 --> 3:27:43.140\n through apps that REJUVE.io will launch\n\n3:27:43.140 --> 3:27:44.660\n within the next couple of months.\n\n3:27:44.660 --> 3:27:48.020\n And then SingularityNet AI can analyze all this data,\n\n3:27:48.020 --> 3:27:50.100\n but then the benefits from that analysis\n\n3:27:50.100 --> 3:27:52.780\n are spread among all the members of the network.\n\n3:27:52.780 --> 3:27:54.700\n But I mean, of course,\n\n3:27:54.700 --> 3:27:56.580\n I'm gonna hawk my particular projects,\n\n3:27:56.580 --> 3:28:00.180\n but I mean, whether or not SingularityNet and REJUVE.io\n\n3:28:00.180 --> 3:28:04.460\n are the answer, I think it's key to create\n\n3:28:04.460 --> 3:28:09.180\n decentralized mechanisms for everything.\n\n3:28:09.180 --> 3:28:13.300\n I mean, for AI, for human health, for politics,\n\n3:28:13.300 --> 3:28:17.740\n for jobs and employment, for sharing social information.\n\n3:28:17.740 --> 3:28:21.660\n And to the extent decentralized peer to peer methods\n\n3:28:21.660 --> 3:28:25.500\n designed with universal compassion at the core\n\n3:28:25.500 --> 3:28:29.780\n can gain traction, then these will just decrease the role\n\n3:28:29.780 --> 3:28:31.260\n that government has.\n\n3:28:31.260 --> 3:28:34.860\n And I think that's much more likely to do good\n\n3:28:34.860 --> 3:28:37.860\n than trying to like explicitly reform\n\n3:28:37.860 --> 3:28:39.180\n the global government system.\n\n3:28:39.180 --> 3:28:41.740\n I mean, I'm happy other people are trying to explicitly\n\n3:28:41.740 --> 3:28:43.900\n reform the global government system.\n\n3:28:43.900 --> 3:28:47.180\n On the other hand, you look at how much good the internet\n\n3:28:47.180 --> 3:28:50.660\n or Google did or mobile phones did,\n\n3:28:50.660 --> 3:28:54.060\n even you're making something that's decentralized\n\n3:28:54.060 --> 3:28:56.620\n and throwing it out everywhere and it takes hold,\n\n3:28:56.620 --> 3:28:59.220\n then government has to adapt.\n\n3:28:59.220 --> 3:29:01.740\n And I mean, that's what we need to do with AI\n\n3:29:01.740 --> 3:29:02.580\n and with health.\n\n3:29:02.580 --> 3:29:07.100\n And in that light, I mean, the centralization\n\n3:29:07.100 --> 3:29:11.820\n of healthcare and of AI is certainly not ideal, right?\n\n3:29:11.820 --> 3:29:15.980\n Like most AI PhDs are being sucked in by a half dozen\n\n3:29:15.980 --> 3:29:17.220\n to a dozen big companies.\n\n3:29:17.220 --> 3:29:20.820\n Most AI processing power is being bought\n\n3:29:20.820 --> 3:29:23.660\n by a few big companies for their own proprietary good.\n\n3:29:23.660 --> 3:29:26.860\n And most medical research is within a few\n\n3:29:26.860 --> 3:29:29.420\n pharmaceutical companies and clinical trials\n\n3:29:29.420 --> 3:29:31.740\n run by pharmaceutical companies will stay solid\n\n3:29:31.740 --> 3:29:34.060\n within those pharmaceutical companies.\n\n3:29:34.060 --> 3:29:37.220\n You know, these large centralized entities,\n\n3:29:37.220 --> 3:29:40.460\n which are intelligences in themselves, these corporations,\n\n3:29:40.460 --> 3:29:43.100\n but they're mostly malevolent psychopathic\n\n3:29:43.100 --> 3:29:45.780\n and sociopathic intelligences,\n\n3:29:45.780 --> 3:29:47.580\n not saying the people involved are,\n\n3:29:47.580 --> 3:29:50.540\n but the corporations as self organizing entities\n\n3:29:50.540 --> 3:29:53.260\n on their own, which are concerned with maximizing\n\n3:29:53.260 --> 3:29:57.100\n shareholder value as a sole objective function.\n\n3:29:57.100 --> 3:29:59.820\n I mean, AI and medicine are being sucked\n\n3:29:59.820 --> 3:30:04.100\n into these pathological corporate organizations\n\n3:30:04.100 --> 3:30:07.740\n with government cooperation and Google cooperating\n\n3:30:07.740 --> 3:30:10.220\n with British and US government on this\n\n3:30:10.220 --> 3:30:12.540\n as one among many, many different examples.\n\n3:30:12.540 --> 3:30:15.940\n 23andMe providing you the nice service of sequencing\n\n3:30:15.940 --> 3:30:18.900\n your genome and then licensing the genome\n\n3:30:18.900 --> 3:30:21.380\n to GlaxoSmithKline on an exclusive basis, right?\n\n3:30:21.380 --> 3:30:23.460\n Now you can take your own DNA\n\n3:30:23.460 --> 3:30:24.860\n and do whatever you want with it.\n\n3:30:24.860 --> 3:30:28.100\n But the pooled collection of 23andMe sequence DNA\n\n3:30:28.100 --> 3:30:30.820\n is just to GlaxoSmithKline.\n\n3:30:30.820 --> 3:30:32.500\n Someone else could reach out to everyone\n\n3:30:32.500 --> 3:30:36.300\n who had worked with 23andMe to sequence their DNA\n\n3:30:36.300 --> 3:30:39.380\n and say, give us your DNA for our open\n\n3:30:39.380 --> 3:30:41.700\n and decentralized repository that we'll make available\n\n3:30:41.700 --> 3:30:43.700\n to everyone, but nobody's doing that\n\n3:30:43.700 --> 3:30:45.700\n cause it's a pain to get organized.\n\n3:30:45.700 --> 3:30:48.860\n And the customer list is proprietary to 23andMe, right?\n\n3:30:48.860 --> 3:30:53.860\n So, yeah, I mean, this I think is a greater risk\n\n3:30:54.340 --> 3:30:57.500\n to humanity from AI than rogue AGI\n\n3:30:57.500 --> 3:31:01.100\n is turning the universe into paperclips or computronium.\n\n3:31:01.100 --> 3:31:05.060\n Cause what you have here is mostly good hearted\n\n3:31:05.060 --> 3:31:09.860\n and nice people who are sucked into a mode of organization\n\n3:31:09.860 --> 3:31:12.580\n of large corporations, which has evolved\n\n3:31:12.580 --> 3:31:14.180\n just for no individual's fault\n\n3:31:14.180 --> 3:31:16.780\n just because that's the way society has evolved.\n\n3:31:16.780 --> 3:31:18.900\n It's not altruistic, it's self interested\n\n3:31:18.900 --> 3:31:20.540\n and become psychopathic like you said.\n\n3:31:20.540 --> 3:31:21.380\n The human.\n\n3:31:21.380 --> 3:31:23.700\n The corporation is psychopathic even if the people are not.\n\n3:31:23.700 --> 3:31:26.660\n And that's really the disturbing thing about it\n\n3:31:26.660 --> 3:31:30.500\n because the corporations can do things\n\n3:31:30.500 --> 3:31:32.380\n that are quite bad for society\n\n3:31:32.380 --> 3:31:35.580\n even if nobody has a bad intention.\n\n3:31:35.580 --> 3:31:36.420\n Right.\n\n3:31:36.420 --> 3:31:37.260\n And then.\n\n3:31:37.260 --> 3:31:38.100\n No individual member of that corporation\n\n3:31:38.100 --> 3:31:38.940\n has a bad intention.\n\n3:31:38.940 --> 3:31:41.540\n No, some probably do, but it's not necessary\n\n3:31:41.540 --> 3:31:43.180\n that they do for the corporation.\n\n3:31:43.180 --> 3:31:47.060\n Like, I mean, Google, I know a lot of people in Google\n\n3:31:47.060 --> 3:31:49.780\n and there are, with very few exceptions,\n\n3:31:49.780 --> 3:31:51.300\n they're all very nice people\n\n3:31:51.300 --> 3:31:53.980\n who genuinely want what's good for the world.\n\n3:31:53.980 --> 3:31:56.940\n And Facebook, I know fewer people\n\n3:31:56.940 --> 3:31:59.020\n but it's probably mostly true.\n\n3:31:59.020 --> 3:32:01.460\n It's probably like fine young geeks\n\n3:32:01.460 --> 3:32:03.940\n who wanna build cool technology.\n\n3:32:03.940 --> 3:32:05.880\n I actually tend to believe that even the leaders,\n\n3:32:05.880 --> 3:32:08.860\n even Mark Zuckerberg, one of the most disliked people\n\n3:32:08.860 --> 3:32:11.940\n in tech is also wants to do good for the world.\n\n3:32:11.940 --> 3:32:13.900\n I think about Jamie Dimon.\n\n3:32:13.900 --> 3:32:14.740\n Who's Jamie Dimon?\n\n3:32:14.740 --> 3:32:16.260\n Oh, the heads of the great banks\n\n3:32:16.260 --> 3:32:17.620\n may have a different psychology.\n\n3:32:17.620 --> 3:32:18.500\n Oh boy, yeah.\n\n3:32:18.500 --> 3:32:22.820\n Well, I tend to be naive about these things\n\n3:32:22.820 --> 3:32:27.340\n and see the best in, I tend to agree with you\n\n3:32:27.340 --> 3:32:30.580\n that I think the individuals wanna do good by the world\n\n3:32:30.580 --> 3:32:32.100\n but the mechanism of the company\n\n3:32:32.100 --> 3:32:34.820\n can sometimes be its own intelligence system.\n\n3:32:34.820 --> 3:32:38.500\n I mean, there's a, my cousin Mario Goetzler\n\n3:32:38.500 --> 3:32:41.740\n has worked for Microsoft since 1985 or something\n\n3:32:41.740 --> 3:32:44.160\n and I can see for him,\n\n3:32:45.380 --> 3:32:48.980\n I mean, as well as just working on cool projects,\n\n3:32:48.980 --> 3:32:51.340\n you're coding stuff that gets used\n\n3:32:51.340 --> 3:32:54.560\n by like billions and billions of people.\n\n3:32:54.560 --> 3:32:57.660\n And do you think if I improve this feature\n\n3:32:57.660 --> 3:33:00.260\n that's making billions of people's lives easier, right?\n\n3:33:00.260 --> 3:33:03.100\n So of course that's cool.\n\n3:33:03.100 --> 3:33:05.520\n And the engineers are not in charge\n\n3:33:05.520 --> 3:33:06.860\n of running the company anyway.\n\n3:33:06.860 --> 3:33:10.120\n And of course, even if you're Mark Zuckerberg or Larry Page,\n\n3:33:10.120 --> 3:33:13.560\n I mean, you still have a fiduciary responsibility.\n\n3:33:13.560 --> 3:33:16.340\n And I mean, you're responsible to the shareholders,\n\n3:33:16.340 --> 3:33:18.860\n your employees who you want to keep paying them\n\n3:33:18.860 --> 3:33:19.700\n and so forth.\n\n3:33:19.700 --> 3:33:22.900\n So yeah, you're enmeshed in this system.\n\n3:33:22.900 --> 3:33:26.740\n And when I worked in DC,\n\n3:33:26.740 --> 3:33:29.380\n I worked a bunch with INSCOM, US Army Intelligence\n\n3:33:29.380 --> 3:33:31.900\n and I was heavily politically opposed\n\n3:33:31.900 --> 3:33:34.740\n to what the US Army was doing in Iraq at that time,\n\n3:33:34.740 --> 3:33:36.540\n like torturing people in Abu Ghraib\n\n3:33:36.540 --> 3:33:39.860\n but everyone I knew in US Army and INSCOM,\n\n3:33:39.860 --> 3:33:42.620\n when I hung out with them, was very nice person.\n\n3:33:42.620 --> 3:33:43.520\n They were friendly to me.\n\n3:33:43.520 --> 3:33:46.140\n They were nice to my kids and my dogs, right?\n\n3:33:46.140 --> 3:33:48.380\n And they really believed that the US\n\n3:33:48.380 --> 3:33:49.660\n was fighting the forces of evil.\n\n3:33:49.660 --> 3:33:51.420\n And if you ask me about Abu Ghraib, they're like,\n\n3:33:51.420 --> 3:33:54.460\n well, but these Arabs will chop us into pieces.\n\n3:33:54.460 --> 3:33:56.300\n So how can you say we're wrong\n\n3:33:56.300 --> 3:33:58.380\n to waterboard them a bit, right?\n\n3:33:58.380 --> 3:34:00.340\n Like that's much less than what they would do to us.\n\n3:34:00.340 --> 3:34:02.940\n It's just in their worldview,\n\n3:34:02.940 --> 3:34:05.340\n what they were doing was really genuinely\n\n3:34:05.340 --> 3:34:06.820\n for the good of humanity.\n\n3:34:06.820 --> 3:34:09.020\n Like none of them woke up in the morning\n\n3:34:09.020 --> 3:34:12.260\n and said like, I want to do harm to good people\n\n3:34:12.260 --> 3:34:14.540\n because I'm just a nasty guy, right?\n\n3:34:14.540 --> 3:34:18.220\n So yeah, most people on the planet,\n\n3:34:18.220 --> 3:34:21.780\n setting aside a few genuine psychopaths and sociopaths,\n\n3:34:21.780 --> 3:34:25.460\n I mean, most people on the planet have a heavy dose\n\n3:34:25.460 --> 3:34:27.540\n of benevolence and wanting to do good\n\n3:34:27.540 --> 3:34:32.160\n and also a heavy capability to convince themselves\n\n3:34:32.160 --> 3:34:33.420\n whatever they feel like doing\n\n3:34:33.420 --> 3:34:37.020\n or whatever is best for them is for the good of humankind.\n\n3:34:37.020 --> 3:34:40.420\n So the more we can decentralize control.\n\n3:34:40.420 --> 3:34:44.940\n Decentralization, you know, the democracy is horrible,\n\n3:34:44.940 --> 3:34:47.320\n but this is like Winston Churchill said,\n\n3:34:47.320 --> 3:34:49.380\n you know, it's the worst possible system of government\n\n3:34:49.380 --> 3:34:50.700\n except for all the others, right?\n\n3:34:50.700 --> 3:34:53.940\n I mean, I think the whole mess of humanity\n\n3:34:53.940 --> 3:34:56.940\n has many, many very bad aspects to it,\n\n3:34:56.940 --> 3:35:00.340\n but so far the track record of elite groups\n\n3:35:00.340 --> 3:35:02.540\n who know what's better for all of humanity\n\n3:35:02.540 --> 3:35:04.540\n is much worse than the track record\n\n3:35:04.540 --> 3:35:08.040\n of the whole teaming democratic participatory\n\n3:35:08.040 --> 3:35:09.540\n mess of humanity, right?\n\n3:35:09.540 --> 3:35:13.420\n I mean, none of them is perfect by any means.\n\n3:35:13.420 --> 3:35:16.660\n The issue with a small elite group that knows what's best\n\n3:35:16.660 --> 3:35:20.340\n is even if it starts out as truly benevolent\n\n3:35:20.340 --> 3:35:22.440\n and doing good things in accordance\n\n3:35:22.440 --> 3:35:24.960\n with its initial good intentions,\n\n3:35:24.960 --> 3:35:26.580\n you find out you need more resources,\n\n3:35:26.580 --> 3:35:29.380\n you need a bigger organization, you pull in more people,\n\n3:35:29.380 --> 3:35:32.940\n internal politics arises, difference of opinions arise\n\n3:35:32.940 --> 3:35:37.940\n and bribery happens, like some opponent organization\n\n3:35:38.140 --> 3:35:40.020\n takes a second in command now to make some,\n\n3:35:40.020 --> 3:35:42.620\n the first in command of some other organization.\n\n3:35:42.620 --> 3:35:45.580\n And I mean, that's, there's a lot of history\n\n3:35:45.580 --> 3:35:47.380\n of what happens with elite groups\n\n3:35:47.380 --> 3:35:50.100\n thinking they know what's best for the human race.\n\n3:35:50.100 --> 3:35:53.060\n So yeah, if I have to choose,\n\n3:35:53.060 --> 3:35:55.460\n I'm gonna reluctantly put my faith\n\n3:35:55.460 --> 3:35:58.940\n in the vast democratic decentralized mass.\n\n3:35:58.940 --> 3:36:02.900\n And I think corporations have a track record\n\n3:36:02.900 --> 3:36:05.340\n of being ethically worse\n\n3:36:05.340 --> 3:36:07.460\n than their constituent human parts.\n\n3:36:07.460 --> 3:36:12.460\n And democratic governments have a more mixed track record,\n\n3:36:13.540 --> 3:36:14.700\n but there are at least.\n\n3:36:14.700 --> 3:36:15.860\n That's the best we got.\n\n3:36:15.860 --> 3:36:18.500\n Yeah, I mean, you can, there's Iceland,\n\n3:36:18.500 --> 3:36:19.660\n very nice country, right?\n\n3:36:19.660 --> 3:36:23.340\n I've been very democratic for 800 plus years,\n\n3:36:23.340 --> 3:36:26.860\n very, very benevolent, beneficial government.\n\n3:36:26.860 --> 3:36:28.820\n And I think, yeah, there are track records\n\n3:36:28.820 --> 3:36:31.860\n of democratic modes of organization.\n\n3:36:31.860 --> 3:36:36.020\n Linux, for example, some of the people in charge of Linux\n\n3:36:36.020 --> 3:36:38.580\n are overtly complete assholes, right?\n\n3:36:38.580 --> 3:36:41.700\n And trying to reform themselves in many cases,\n\n3:36:41.700 --> 3:36:45.980\n in other cases not, but the organization as a whole,\n\n3:36:45.980 --> 3:36:49.700\n I think it's done a good job overall.\n\n3:36:49.700 --> 3:36:53.980\n It's been very welcoming in the third world, for example,\n\n3:36:53.980 --> 3:36:56.700\n and it's allowed advanced technology to roll out\n\n3:36:56.700 --> 3:36:59.940\n on all sorts of different embedded devices and platforms\n\n3:36:59.940 --> 3:37:02.100\n in places where people couldn't afford to pay\n\n3:37:02.100 --> 3:37:03.820\n for proprietary software.\n\n3:37:03.820 --> 3:37:08.820\n So I'd say the internet, Linux, and many democratic nations\n\n3:37:09.140 --> 3:37:11.380\n are examples of how sort of an open,\n\n3:37:11.380 --> 3:37:14.060\n decentralized democratic methodology\n\n3:37:14.060 --> 3:37:16.580\n can be ethically better than the sum of the parts\n\n3:37:16.580 --> 3:37:17.420\n rather than worse.\n\n3:37:17.420 --> 3:37:21.420\n And corporations, that has happened only for a brief period\n\n3:37:21.420 --> 3:37:24.580\n and then it goes sour, right?\n\n3:37:24.580 --> 3:37:26.980\n I mean, I'd say a similar thing about universities.\n\n3:37:26.980 --> 3:37:30.900\n Like university is a horrible way to organize research\n\n3:37:30.900 --> 3:37:33.660\n and get things done, yet it's better than anything else\n\n3:37:33.660 --> 3:37:34.500\n we've come up with, right?\n\n3:37:34.500 --> 3:37:36.940\n A company can be much better,\n\n3:37:36.940 --> 3:37:38.300\n but for a brief period of time,\n\n3:37:38.300 --> 3:37:42.660\n and then it stops being so good, right?\n\n3:37:42.660 --> 3:37:47.340\n So then I think if you believe that AGI\n\n3:37:47.340 --> 3:37:50.700\n is gonna emerge sort of incrementally\n\n3:37:50.700 --> 3:37:53.620\n out of AIs doing practical stuff in the world,\n\n3:37:53.620 --> 3:37:57.060\n like controlling humanoid robots or driving cars\n\n3:37:57.060 --> 3:38:01.260\n or diagnosing diseases or operating killer drones\n\n3:38:01.260 --> 3:38:04.580\n or spying on people and reporting under the government,\n\n3:38:04.580 --> 3:38:09.580\n then what kind of organization creates more and more\n\n3:38:09.620 --> 3:38:12.500\n advanced narrow AI verging toward AGI\n\n3:38:12.500 --> 3:38:14.620\n may be quite important because it will guide\n\n3:38:14.620 --> 3:38:18.620\n like what's in the mind of the early stage AGI\n\n3:38:18.620 --> 3:38:21.780\n as it first gains the ability to rewrite its own code base\n\n3:38:21.780 --> 3:38:24.740\n and project itself toward super intelligence.\n\n3:38:24.740 --> 3:38:29.740\n And if you believe that AI may move toward AGI\n\n3:38:31.180 --> 3:38:33.300\n out of this sort of synergetic activity\n\n3:38:33.300 --> 3:38:35.780\n of many agents cooperating together\n\n3:38:35.780 --> 3:38:37.860\n rather than just have one person's project,\n\n3:38:37.860 --> 3:38:42.580\n then who owns and controls that platform for AI cooperation\n\n3:38:42.580 --> 3:38:47.260\n becomes also very, very important, right?\n\n3:38:47.260 --> 3:38:49.380\n And is that platform AWS?\n\n3:38:49.380 --> 3:38:50.580\n Is it Google Cloud?\n\n3:38:50.580 --> 3:38:53.420\n Is it Alibaba or is it something more like the internet\n\n3:38:53.420 --> 3:38:56.740\n or Singularity Net, which is open and decentralized?\n\n3:38:56.740 --> 3:39:01.100\n So if all of my weird machinations come to pass, right?\n\n3:39:01.100 --> 3:39:03.740\n I mean, we have the Hanson robots\n\n3:39:03.740 --> 3:39:06.140\n being a beautiful user interface,\n\n3:39:06.140 --> 3:39:09.100\n gathering information on human values\n\n3:39:09.100 --> 3:39:12.060\n and being loving and compassionate to people in medical,\n\n3:39:12.060 --> 3:39:14.620\n home service, robot office applications,\n\n3:39:14.620 --> 3:39:16.900\n you have Singularity Net in the backend\n\n3:39:16.900 --> 3:39:19.460\n networking together many different AIs\n\n3:39:19.460 --> 3:39:21.500\n toward cooperative intelligence,\n\n3:39:21.500 --> 3:39:24.020\n fueling the robots among many other things.\n\n3:39:24.020 --> 3:39:27.340\n You have OpenCog 2.0 and true AGI\n\n3:39:27.340 --> 3:39:29.420\n as one of the sources of AI\n\n3:39:29.420 --> 3:39:31.700\n inside this decentralized network,\n\n3:39:31.700 --> 3:39:34.140\n powering the robot and medical AIs\n\n3:39:34.140 --> 3:39:36.300\n helping us live a long time\n\n3:39:36.300 --> 3:39:39.740\n and cure diseases among other things.\n\n3:39:39.740 --> 3:39:42.380\n And this whole thing is operating\n\n3:39:42.380 --> 3:39:46.060\n in a democratic and decentralized way, right?\n\n3:39:46.060 --> 3:39:50.420\n And I think if anyone can pull something like this off,\n\n3:39:50.420 --> 3:39:53.900\n whether using the specific technologies I've mentioned\n\n3:39:53.900 --> 3:39:55.780\n or something else, I mean,\n\n3:39:55.780 --> 3:39:58.380\n then I think we have a higher odds\n\n3:39:58.380 --> 3:40:02.740\n of moving toward a beneficial technological singularity\n\n3:40:02.740 --> 3:40:06.220\n rather than one in which the first super AGI\n\n3:40:06.220 --> 3:40:07.620\n is indifferent to humans\n\n3:40:07.620 --> 3:40:11.900\n and just considers us an inefficient use of molecules.\n\n3:40:11.900 --> 3:40:15.540\n That was a beautifully articulated vision for the world.\n\n3:40:15.540 --> 3:40:16.700\n So thank you for that.\n\n3:40:16.700 --> 3:40:20.460\n Well, let's talk a little bit about life and death.\n\n3:40:21.860 --> 3:40:26.860\n I'm pro life and anti death for most people.\n\n3:40:27.100 --> 3:40:29.460\n There's few exceptions that I won't mention here.\n\n3:40:30.860 --> 3:40:32.340\n I'm glad just like your dad,\n\n3:40:32.340 --> 3:40:34.660\n you're taking a stand against death.\n\n3:40:36.420 --> 3:40:39.940\n You have, by the way, you have a bunch of awesome music\n\n3:40:39.940 --> 3:40:41.780\n where you play piano online.\n\n3:40:41.780 --> 3:40:45.380\n One of the songs that I believe you've written\n\n3:40:45.380 --> 3:40:49.140\n the lyrics go, by the way, I like the way it sounds,\n\n3:40:49.140 --> 3:40:51.460\n people should listen to it, it's awesome.\n\n3:40:51.460 --> 3:40:54.980\n I considered, I probably will cover it, it's a good song.\n\n3:40:54.980 --> 3:40:58.660\n Tell me why do you think it is a good thing\n\n3:40:58.660 --> 3:41:01.980\n that we all get old and die is one of the songs.\n\n3:41:01.980 --> 3:41:03.180\n I love the way it sounds,\n\n3:41:03.180 --> 3:41:05.660\n but let me ask you about death first.\n\n3:41:06.780 --> 3:41:08.300\n Do you think there's an element to death\n\n3:41:08.300 --> 3:41:12.260\n that's essential to give our life meaning?\n\n3:41:12.260 --> 3:41:14.020\n Like the fact that this thing ends.\n\n3:41:14.020 --> 3:41:19.020\n Well, let me say I'm pleased and a little embarrassed\n\n3:41:19.220 --> 3:41:21.540\n you've been listening to that music I put online.\n\n3:41:21.540 --> 3:41:22.380\n That's awesome.\n\n3:41:22.380 --> 3:41:25.540\n One of my regrets in life recently is I would love\n\n3:41:25.540 --> 3:41:28.460\n to get time to really produce music well.\n\n3:41:28.460 --> 3:41:31.100\n Like I haven't touched my sequencer software\n\n3:41:31.100 --> 3:41:32.620\n in like five years.\n\n3:41:32.620 --> 3:41:37.220\n I would love to like rehearse and produce and edit.\n\n3:41:37.220 --> 3:41:39.580\n But with a two year old baby\n\n3:41:39.580 --> 3:41:42.260\n and trying to create the singularity, there's no time.\n\n3:41:42.260 --> 3:41:44.740\n So I just made the decision to,\n\n3:41:45.660 --> 3:41:47.740\n when I'm playing random shit in an off moment.\n\n3:41:47.740 --> 3:41:48.580\n Just record it.\n\n3:41:48.580 --> 3:41:51.820\n Just record it, put it out there, like whatever.\n\n3:41:51.820 --> 3:41:54.460\n Maybe if I'm unfortunate enough to die,\n\n3:41:54.460 --> 3:41:56.260\n maybe that can be input to the AGI\n\n3:41:56.260 --> 3:41:58.980\n when it tries to make an accurate mind upload of me, right?\n\n3:41:58.980 --> 3:42:00.100\n Death is bad.\n\n3:42:01.100 --> 3:42:02.700\n I mean, that's very simple.\n\n3:42:02.700 --> 3:42:04.300\n It's baffling we should have to say that.\n\n3:42:04.300 --> 3:42:08.740\n I mean, of course people can make meaning out of death.\n\n3:42:08.740 --> 3:42:10.940\n And if someone is tortured,\n\n3:42:10.940 --> 3:42:13.220\n maybe they can make beautiful meaning out of that torture\n\n3:42:13.220 --> 3:42:14.540\n and write a beautiful poem\n\n3:42:14.540 --> 3:42:16.980\n about what it was like to be tortured, right?\n\n3:42:16.980 --> 3:42:19.100\n I mean, we're very creative.\n\n3:42:19.100 --> 3:42:22.420\n We can milk beauty and positivity\n\n3:42:22.420 --> 3:42:25.300\n out of even the most horrible and shitty things.\n\n3:42:25.300 --> 3:42:27.860\n But just because if I was tortured,\n\n3:42:27.860 --> 3:42:28.940\n I could write a good song\n\n3:42:28.940 --> 3:42:30.780\n about what it was like to be tortured,\n\n3:42:30.780 --> 3:42:31.980\n doesn't make torture good.\n\n3:42:31.980 --> 3:42:35.660\n And just because people are able to derive meaning\n\n3:42:35.660 --> 3:42:37.500\n and value from death,\n\n3:42:37.500 --> 3:42:39.620\n doesn't mean they wouldn't derive even better meaning\n\n3:42:39.620 --> 3:42:42.580\n and value from ongoing life without death,\n\n3:42:42.580 --> 3:42:43.420\n which I very...\n\n3:42:43.420 --> 3:42:44.260\n Indefinite.\n\n3:42:44.260 --> 3:42:45.100\n Yeah, yeah.\n\n3:42:45.100 --> 3:42:47.740\n So if you could live forever, would you live forever?\n\n3:42:47.740 --> 3:42:48.620\n Forever.\n\n3:42:50.460 --> 3:42:52.820\n My goal with longevity research\n\n3:42:52.820 --> 3:42:57.460\n is to abolish the plague of involuntary death.\n\n3:42:57.460 --> 3:43:00.340\n I don't think people should die unless they choose to die.\n\n3:43:01.340 --> 3:43:05.700\n If I had to choose forced immortality\n\n3:43:05.700 --> 3:43:09.180\n versus dying, I would choose forced immortality.\n\n3:43:09.180 --> 3:43:11.860\n On the other hand, if I chose...\n\n3:43:11.860 --> 3:43:13.500\n If I had the choice of immortality\n\n3:43:13.500 --> 3:43:15.620\n with the choice of suicide whenever I felt like it,\n\n3:43:15.620 --> 3:43:17.220\n of course I would take that instead.\n\n3:43:17.220 --> 3:43:18.860\n And that's the more realistic choice.\n\n3:43:18.860 --> 3:43:20.180\n I mean, there's no reason\n\n3:43:20.180 --> 3:43:21.660\n you should have forced immortality.\n\n3:43:21.660 --> 3:43:25.780\n You should be able to live until you get sick of living,\n\n3:43:25.780 --> 3:43:26.620\n right?\n\n3:43:26.620 --> 3:43:27.460\n I mean, that's...\n\n3:43:27.460 --> 3:43:29.780\n And that will seem insanely obvious\n\n3:43:29.780 --> 3:43:31.380\n to everyone 50 years from now.\n\n3:43:31.380 --> 3:43:33.180\n And they will be so...\n\n3:43:33.180 --> 3:43:35.980\n I mean, people who thought death gives meaning to life,\n\n3:43:35.980 --> 3:43:37.660\n so we should all die,\n\n3:43:37.660 --> 3:43:39.380\n they will look at that 50 years from now\n\n3:43:39.380 --> 3:43:43.340\n the way we now look at the Anabaptists in the year 1000\n\n3:43:43.340 --> 3:43:45.180\n who gave away all their positions,\n\n3:43:45.180 --> 3:43:47.700\n went on top of the mountain for Jesus\n\n3:43:47.700 --> 3:43:50.220\n to come and bring them to the ascension.\n\n3:43:50.220 --> 3:43:55.220\n I mean, it's ridiculous that people think death is good\n\n3:43:55.740 --> 3:44:00.180\n because you gain more wisdom as you approach dying.\n\n3:44:00.180 --> 3:44:01.940\n I mean, of course it's true.\n\n3:44:01.940 --> 3:44:03.460\n I mean, I'm 53.\n\n3:44:03.460 --> 3:44:08.220\n And the fact that I might have only a few more decades left,\n\n3:44:08.220 --> 3:44:11.460\n it does make me reflect on things differently.\n\n3:44:11.460 --> 3:44:15.700\n It does give me a deeper understanding of many things.\n\n3:44:15.700 --> 3:44:18.100\n But I mean, so what?\n\n3:44:18.100 --> 3:44:19.500\n You could get a deep understanding\n\n3:44:19.500 --> 3:44:20.900\n in a lot of different ways.\n\n3:44:20.900 --> 3:44:22.460\n Pain is the same way.\n\n3:44:22.460 --> 3:44:24.260\n We're gonna abolish pain.\n\n3:44:24.260 --> 3:44:27.460\n And that's even more amazing than abolishing death, right?\n\n3:44:27.460 --> 3:44:30.420\n I mean, once we get a little better at neuroscience,\n\n3:44:30.420 --> 3:44:32.660\n we'll be able to go in and adjust the brain\n\n3:44:32.660 --> 3:44:34.740\n so that pain doesn't hurt anymore, right?\n\n3:44:34.740 --> 3:44:37.100\n And that, you know, people will say that's bad\n\n3:44:37.100 --> 3:44:39.420\n because there's so much beauty\n\n3:44:39.420 --> 3:44:41.100\n in overcoming pain and suffering.\n\n3:44:41.100 --> 3:44:42.340\n Oh, sure.\n\n3:44:42.340 --> 3:44:45.220\n And there's beauty in overcoming torture too.\n\n3:44:45.220 --> 3:44:46.860\n And some people like to cut themselves,\n\n3:44:46.860 --> 3:44:48.100\n but not many, right?\n\n3:44:48.100 --> 3:44:48.940\n I mean.\n\n3:44:48.940 --> 3:44:49.780\n That's an interesting.\n\n3:44:49.780 --> 3:44:52.260\n So, but to push, I mean, to push back again,\n\n3:44:52.260 --> 3:44:53.300\n this is the Russian side of me.\n\n3:44:53.300 --> 3:44:55.020\n I do romanticize suffering.\n\n3:44:55.020 --> 3:44:56.380\n It's not obvious.\n\n3:44:56.380 --> 3:44:59.460\n I mean, the way you put it, it seems very logical.\n\n3:44:59.460 --> 3:45:02.820\n It's almost absurd to romanticize suffering or pain\n\n3:45:02.820 --> 3:45:07.740\n or death, but to me, a world without suffering,\n\n3:45:07.740 --> 3:45:10.620\n without pain, without death, it's not obvious.\n\n3:45:10.620 --> 3:45:13.500\n Well, then you can stay in the people's zoo,\n\n3:45:13.500 --> 3:45:15.460\n people torturing each other.\n\n3:45:15.460 --> 3:45:18.140\n No, but what I'm saying is I don't,\n\n3:45:18.140 --> 3:45:20.220\n well, that's, I guess what I'm trying to say,\n\n3:45:20.220 --> 3:45:22.820\n I don't know if I was presented with that choice,\n\n3:45:22.820 --> 3:45:25.420\n what I would choose because it, to me.\n\n3:45:25.420 --> 3:45:30.100\n This is a subtler, it's a subtler matter.\n\n3:45:30.100 --> 3:45:33.980\n And I've posed it in this conversation\n\n3:45:33.980 --> 3:45:37.100\n in an unnecessarily extreme way.\n\n3:45:37.100 --> 3:45:41.060\n So I think, I think the way you should think about it\n\n3:45:41.060 --> 3:45:44.700\n is what if there's a little dial on the side of your head\n\n3:45:44.700 --> 3:45:48.180\n and you could turn how much pain hurt,\n\n3:45:48.180 --> 3:45:50.660\n turn it down to zero, turn it up to 11,\n\n3:45:50.660 --> 3:45:52.220\n like in spinal tap, if it wants,\n\n3:45:52.220 --> 3:45:53.980\n maybe through an actual spinal tap, right?\n\n3:45:53.980 --> 3:45:58.940\n So, I mean, would you opt to have that dial there or not?\n\n3:45:58.940 --> 3:45:59.780\n That's the question.\n\n3:45:59.780 --> 3:46:02.300\n The question isn't whether you would turn the pain down\n\n3:46:02.300 --> 3:46:05.220\n to zero all the time.\n\n3:46:05.220 --> 3:46:07.180\n Would you opt to have the dial or not?\n\n3:46:07.180 --> 3:46:10.000\n My guess is that in some dark moment of your life,\n\n3:46:10.000 --> 3:46:12.180\n you would choose to have the dial implanted\n\n3:46:12.180 --> 3:46:13.340\n and then it would be there.\n\n3:46:13.340 --> 3:46:17.180\n Just to confess a small thing, don't ask me why,\n\n3:46:17.180 --> 3:46:20.760\n but I'm doing this physical challenge currently\n\n3:46:20.760 --> 3:46:24.420\n where I'm doing 680 pushups and pull ups a day.\n\n3:46:25.860 --> 3:46:29.180\n And my shoulder is currently, as we sit here,\n\n3:46:29.180 --> 3:46:30.700\n in a lot of pain.\n\n3:46:30.700 --> 3:46:35.700\n And I don't know, I would certainly right now,\n\n3:46:35.860 --> 3:46:38.880\n if you gave me a dial, I would turn that sucker to zero\n\n3:46:38.880 --> 3:46:40.540\n as quickly as possible.\n\n3:46:40.540 --> 3:46:45.540\n But I think the whole point of this journey is,\n\n3:46:46.740 --> 3:46:47.580\n I don't know.\n\n3:46:47.580 --> 3:46:49.540\n Well, because you're a twisted human being.\n\n3:46:49.540 --> 3:46:53.580\n I'm a twisted, so the question is am I somehow twisted\n\n3:46:53.580 --> 3:46:57.440\n because I created some kind of narrative for myself\n\n3:46:57.440 --> 3:47:00.820\n so that I can deal with the injustice\n\n3:47:00.820 --> 3:47:02.420\n and the suffering in the world?\n\n3:47:03.700 --> 3:47:06.340\n Or is this actually going to be a source of happiness\n\n3:47:06.340 --> 3:47:07.180\n for me?\n\n3:47:07.180 --> 3:47:10.820\n Well, this is to an extent is a research question\n\n3:47:10.820 --> 3:47:12.300\n that humanity will undertake, right?\n\n3:47:12.300 --> 3:47:17.300\n So I mean, human beings do have a particular biological\n\n3:47:17.300 --> 3:47:22.300\n makeup, which sort of implies a certain probability\n\n3:47:22.860 --> 3:47:25.880\n distribution over motivational systems, right?\n\n3:47:25.880 --> 3:47:30.400\n So I mean, we, and that is there, that is there.\n\n3:47:30.400 --> 3:47:35.400\n Now the question is how flexibly can that morph\n\n3:47:36.540 --> 3:47:38.980\n as society and technology change, right?\n\n3:47:38.980 --> 3:47:43.740\n So if we're given that dial and we're given a society\n\n3:47:43.740 --> 3:47:47.540\n in which say we don't have to work for a living\n\n3:47:47.540 --> 3:47:50.700\n and in which there's an ambient decentralized\n\n3:47:50.700 --> 3:47:52.460\n benevolent AI network that will warn us\n\n3:47:52.460 --> 3:47:54.660\n when we're about to hurt ourself,\n\n3:47:54.660 --> 3:47:57.060\n if we're in a different context,\n\n3:47:57.060 --> 3:48:02.060\n can we consistently with being genuinely and fully human,\n\n3:48:02.880 --> 3:48:05.880\n can we consistently get into a state of consciousness\n\n3:48:05.880 --> 3:48:09.220\n where we just want to keep the pain dial turned\n\n3:48:09.220 --> 3:48:12.420\n all the way down and yet we're leading very rewarding\n\n3:48:12.420 --> 3:48:13.860\n and fulfilling lives, right?\n\n3:48:13.860 --> 3:48:17.660\n Now, I suspect the answer is yes, we can do that,\n\n3:48:17.660 --> 3:48:21.580\n but I don't know that, I don't know that for certain.\n\n3:48:21.580 --> 3:48:25.960\n Yeah, now I'm more confident that we could create\n\n3:48:25.960 --> 3:48:30.960\n a nonhuman AGI system, which just didn't need an analog\n\n3:48:31.220 --> 3:48:33.100\n of feeling pain.\n\n3:48:33.100 --> 3:48:37.380\n And I think that AGI system will be fundamentally healthier\n\n3:48:37.380 --> 3:48:39.740\n and more benevolent than human beings.\n\n3:48:39.740 --> 3:48:42.340\n So I think it might or might not be true\n\n3:48:42.340 --> 3:48:45.220\n that humans need a certain element of suffering\n\n3:48:45.220 --> 3:48:49.460\n to be satisfied humans, consistent with the human physiology.\n\n3:48:49.460 --> 3:48:53.220\n If it is true, that's one of the things that makes us fucked\n\n3:48:53.220 --> 3:48:58.220\n and disqualified to be the super AGI, right?\n\n3:48:58.380 --> 3:49:03.380\n I mean, the nature of the human motivational system\n\n3:49:03.620 --> 3:49:08.620\n is that we seem to gravitate towards situations\n\n3:49:08.620 --> 3:49:12.740\n where the best thing in the large scale\n\n3:49:12.740 --> 3:49:15.860\n is not the best thing in the small scale\n\n3:49:15.860 --> 3:49:18.100\n according to our subjective value system.\n\n3:49:18.100 --> 3:49:20.740\n So we gravitate towards subjective value judgments\n\n3:49:20.740 --> 3:49:22.940\n where to gratify ourselves in the large,\n\n3:49:22.940 --> 3:49:25.620\n we have to ungratify ourselves in the small.\n\n3:49:25.620 --> 3:49:29.340\n And we do that in, you see that in music,\n\n3:49:29.340 --> 3:49:31.740\n there's a theory of music which says\n\n3:49:31.740 --> 3:49:33.780\n the key to musical aesthetics\n\n3:49:33.780 --> 3:49:36.860\n is the surprising fulfillment of expectations.\n\n3:49:36.860 --> 3:49:38.900\n Like you want something that will fulfill\n\n3:49:38.900 --> 3:49:41.820\n the expectations are listed in the prior part of the music,\n\n3:49:41.820 --> 3:49:44.820\n but in a way with a bit of a twist that surprises you.\n\n3:49:44.820 --> 3:49:48.140\n And I mean, that's true not only in outdoor music\n\n3:49:48.140 --> 3:49:53.140\n like my own or that of Zappa or Steve Vai or Buckethead\n\n3:49:53.300 --> 3:49:55.460\n or Christoph Pendergast or something,\n\n3:49:55.460 --> 3:49:57.980\n it's even there in Mozart or something.\n\n3:49:57.980 --> 3:49:59.980\n It's not there in elevator music too much,\n\n3:49:59.980 --> 3:50:02.940\n but that's why it's boring, right?\n\n3:50:02.940 --> 3:50:07.540\n But wrapped up in there is we want to hurt a little bit\n\n3:50:07.540 --> 3:50:11.300\n so that we can feel the pain go away.\n\n3:50:11.300 --> 3:50:15.700\n Like we wanna be a little confused by what's coming next.\n\n3:50:15.700 --> 3:50:18.380\n So then when the thing that comes next actually makes sense,\n\n3:50:18.380 --> 3:50:19.940\n it's so satisfying, right?\n\n3:50:19.940 --> 3:50:22.300\n That's the surprising fulfillment of expectations,\n\n3:50:22.300 --> 3:50:23.140\n is that what you said?\n\n3:50:23.140 --> 3:50:23.960\n Yeah, yeah, yeah.\n\n3:50:23.960 --> 3:50:24.800\n So beautifully put.\n\n3:50:24.800 --> 3:50:26.820\n We've been skirting around a little bit,\n\n3:50:26.820 --> 3:50:29.380\n but if I were to ask you the most ridiculous big question\n\n3:50:29.380 --> 3:50:32.740\n of what is the meaning of life,\n\n3:50:32.740 --> 3:50:34.380\n what would your answer be?\n\n3:50:37.340 --> 3:50:40.080\n Three values, joy, growth, and choice.\n\n3:50:43.580 --> 3:50:46.420\n I think you need joy.\n\n3:50:46.420 --> 3:50:48.060\n I mean, that's the basis of everything.\n\n3:50:48.060 --> 3:50:49.700\n If you want the number one value.\n\n3:50:49.700 --> 3:50:54.700\n On the other hand, I'm unsatisfied with a static joy\n\n3:50:54.860 --> 3:50:58.100\n that doesn't progress perhaps because of some\n\n3:50:58.100 --> 3:51:00.140\n elemental element of human perversity,\n\n3:51:00.140 --> 3:51:02.220\n but the idea of something that grows\n\n3:51:02.220 --> 3:51:04.860\n and becomes more and more and better and better\n\n3:51:04.860 --> 3:51:06.780\n in some sense appeals to me.\n\n3:51:06.780 --> 3:51:10.580\n But I also sort of like the idea of individuality\n\n3:51:10.580 --> 3:51:14.500\n that as a distinct system, I have some agency.\n\n3:51:14.500 --> 3:51:18.820\n So there's some nexus of causality within this system\n\n3:51:18.820 --> 3:51:22.420\n rather than the causality being wholly evenly distributed\n\n3:51:22.420 --> 3:51:23.920\n over the joyous growing mass.\n\n3:51:23.920 --> 3:51:27.080\n So you start with joy, growth, and choice\n\n3:51:27.080 --> 3:51:28.860\n as three basic values.\n\n3:51:28.860 --> 3:51:31.940\n Those three things could continue indefinitely.\n\n3:51:31.940 --> 3:51:35.180\n That's something that can last forever.\n\n3:51:35.180 --> 3:51:38.740\n Is there some aspect of something you called,\n\n3:51:38.740 --> 3:51:43.740\n which I like, super longevity that you find exciting?\n\n3:51:44.980 --> 3:51:48.340\n Is there research wise, is there ideas in that space that?\n\n3:51:48.340 --> 3:51:53.240\n I mean, I think, yeah, in terms of the meaning of life,\n\n3:51:53.240 --> 3:51:58.020\n this really ties into that because for us as humans,\n\n3:51:58.020 --> 3:52:02.260\n probably the way to get the most joy, growth, and choice\n\n3:52:02.260 --> 3:52:06.180\n is transhumanism and to go beyond the human form\n\n3:52:06.180 --> 3:52:08.420\n that we have right now, right?\n\n3:52:08.420 --> 3:52:10.980\n I mean, I think human body is great\n\n3:52:10.980 --> 3:52:15.140\n and by no means do any of us maximize the potential\n\n3:52:15.140 --> 3:52:18.560\n for joy, growth, and choice imminent in our human bodies.\n\n3:52:18.560 --> 3:52:21.780\n On the other hand, it's clear that other configurations\n\n3:52:21.780 --> 3:52:25.260\n of matter could manifest even greater amounts\n\n3:52:25.260 --> 3:52:29.620\n of joy, growth, and choice than humans do,\n\n3:52:29.620 --> 3:52:33.140\n maybe even finding ways to go beyond the realm of matter\n\n3:52:33.140 --> 3:52:34.940\n as we understand it right now.\n\n3:52:34.940 --> 3:52:38.100\n So I think in a practical sense,\n\n3:52:38.100 --> 3:52:40.740\n much of the meaning I see in human life\n\n3:52:40.740 --> 3:52:42.880\n is to create something better than humans\n\n3:52:42.880 --> 3:52:45.460\n and go beyond human life.\n\n3:52:45.460 --> 3:52:47.980\n But certainly that's not all of it for me\n\n3:52:47.980 --> 3:52:49.220\n in a practical sense, right?\n\n3:52:49.220 --> 3:52:51.740\n Like I have four kids and a granddaughter\n\n3:52:51.740 --> 3:52:55.060\n and many friends and parents and family\n\n3:52:55.060 --> 3:52:59.740\n and just enjoying everyday human social existence.\n\n3:52:59.740 --> 3:53:00.900\n But we can do even better.\n\n3:53:00.900 --> 3:53:01.740\n Yeah, yeah.\n\n3:53:01.740 --> 3:53:03.860\n And I mean, I love, I've always,\n\n3:53:03.860 --> 3:53:05.700\n when I could live near nature,\n\n3:53:05.700 --> 3:53:08.740\n I spend a bunch of time out in nature in the forest\n\n3:53:08.740 --> 3:53:10.940\n and on the water every day and so forth.\n\n3:53:10.940 --> 3:53:15.040\n So, I mean, enjoying the pleasant moment is part of it,\n\n3:53:15.040 --> 3:53:20.040\n but the growth and choice aspect are severely limited\n\n3:53:20.780 --> 3:53:22.420\n by our human biology.\n\n3:53:22.420 --> 3:53:25.980\n In particular, dying seems to inhibit your potential\n\n3:53:25.980 --> 3:53:29.520\n for personal growth considerably as far as we know.\n\n3:53:29.520 --> 3:53:32.980\n I mean, there's some element of life after death perhaps,\n\n3:53:32.980 --> 3:53:34.980\n but even if there is,\n\n3:53:34.980 --> 3:53:39.300\n why not also continue going in this biological realm, right?\n\n3:53:39.300 --> 3:53:41.880\n In super longevity, I mean,\n\n3:53:43.300 --> 3:53:45.580\n you know, we haven't yet cured aging.\n\n3:53:45.580 --> 3:53:48.020\n We haven't yet cured death.\n\n3:53:48.020 --> 3:53:51.860\n Certainly there's very interesting progress all around.\n\n3:53:51.860 --> 3:53:56.860\n I mean, CRISPR and gene editing can be an incredible tool.\n\n3:53:57.220 --> 3:54:00.120\n And I mean, right now,\n\n3:54:00.120 --> 3:54:03.180\n stem cells could potentially prolong life a lot.\n\n3:54:03.180 --> 3:54:05.980\n Like if you got stem cell injections\n\n3:54:05.980 --> 3:54:09.140\n of just stem cells for every tissue of your body\n\n3:54:09.140 --> 3:54:11.360\n injected into every tissue,\n\n3:54:11.360 --> 3:54:15.360\n and you can just have replacement of your old cells\n\n3:54:15.360 --> 3:54:17.340\n with new cells produced by those stem cells,\n\n3:54:17.340 --> 3:54:21.240\n I mean, that could be highly impactful at prolonging life.\n\n3:54:21.240 --> 3:54:23.260\n Now we just need slightly better technology\n\n3:54:23.260 --> 3:54:25.420\n for having them grow, right?\n\n3:54:25.420 --> 3:54:28.840\n So using machine learning to guide procedures\n\n3:54:28.840 --> 3:54:32.700\n for stem cell differentiation and trans differentiation,\n\n3:54:32.700 --> 3:54:33.740\n it's kind of nitty gritty,\n\n3:54:33.740 --> 3:54:36.680\n but I mean, that's quite interesting.\n\n3:54:36.680 --> 3:54:41.060\n So I think there's a lot of different things being done\n\n3:54:41.060 --> 3:54:44.740\n to help with prolongation of human life,\n\n3:54:44.740 --> 3:54:47.560\n but we could do a lot better.\n\n3:54:47.560 --> 3:54:51.460\n So for example, the extracellular matrix,\n\n3:54:51.460 --> 3:54:52.620\n which is the bunch of proteins\n\n3:54:52.620 --> 3:54:54.300\n in between the cells in your body,\n\n3:54:54.300 --> 3:54:57.360\n they get stiffer and stiffer as you get older.\n\n3:54:57.360 --> 3:55:01.300\n And the extracellular matrix transmits information\n\n3:55:01.300 --> 3:55:03.540\n both electrically, mechanically,\n\n3:55:03.540 --> 3:55:05.380\n and to some extent, biophotonically.\n\n3:55:05.380 --> 3:55:07.280\n So there's all this transmission\n\n3:55:07.280 --> 3:55:08.880\n through the parts of the body,\n\n3:55:08.880 --> 3:55:11.860\n but the stiffer the extracellular matrix gets,\n\n3:55:11.860 --> 3:55:13.520\n the less the transmission happens,\n\n3:55:13.520 --> 3:55:15.660\n which makes your body get worse coordinated\n\n3:55:15.660 --> 3:55:17.460\n between the different organs as you get older.\n\n3:55:17.460 --> 3:55:19.460\n So my friend Christian Schaffmeister\n\n3:55:19.460 --> 3:55:22.460\n at my alumnus organization,\n\n3:55:22.460 --> 3:55:25.100\n my Alma mater, the Great Temple University,\n\n3:55:25.100 --> 3:55:28.640\n Christian Schaffmeister has a potential solution to this,\n\n3:55:28.640 --> 3:55:32.340\n where he has these novel molecules called spiral ligamers,\n\n3:55:32.340 --> 3:55:34.440\n which are like polymers that are not organic.\n\n3:55:34.440 --> 3:55:37.780\n They're specially designed polymers\n\n3:55:37.780 --> 3:55:39.420\n so that you can algorithmically predict\n\n3:55:39.420 --> 3:55:41.580\n exactly how they'll fold very simply.\n\n3:55:41.580 --> 3:55:43.280\n So he designed the molecular scissors\n\n3:55:43.280 --> 3:55:45.560\n that have spiral ligamers that you could eat\n\n3:55:45.560 --> 3:55:49.220\n and would then cut through all the glucosamine\n\n3:55:49.220 --> 3:55:50.620\n and other crosslink proteins\n\n3:55:50.620 --> 3:55:52.760\n in your extracellular matrix, right?\n\n3:55:52.760 --> 3:55:55.200\n But to make that technology really work\n\n3:55:55.200 --> 3:55:56.860\n and be mature as several years of work,\n\n3:55:56.860 --> 3:56:00.140\n as far as I know, no one's finding it at the moment.\n\n3:56:00.140 --> 3:56:02.380\n So there's so many different ways\n\n3:56:02.380 --> 3:56:05.080\n that technology could be used to prolong longevity.\n\n3:56:05.080 --> 3:56:06.540\n What we really need,\n\n3:56:06.540 --> 3:56:09.580\n we need an integrated database of all biological knowledge\n\n3:56:09.580 --> 3:56:12.020\n about human beings and model organisms,\n\n3:56:12.020 --> 3:56:14.480\n like hopefully a massively distributed\n\n3:56:14.480 --> 3:56:15.980\n open cog bioatom space,\n\n3:56:15.980 --> 3:56:18.260\n but it can exist in other forms too.\n\n3:56:18.260 --> 3:56:20.860\n We need that data to be opened up\n\n3:56:20.860 --> 3:56:23.300\n in a suitably privacy protecting way.\n\n3:56:23.300 --> 3:56:26.100\n We need massive funding into machine learning,\n\n3:56:26.100 --> 3:56:29.240\n AGI, proto AGI statistical research\n\n3:56:29.240 --> 3:56:31.240\n aimed at solving biology,\n\n3:56:31.240 --> 3:56:33.440\n both molecular biology and human biology\n\n3:56:33.440 --> 3:56:36.700\n based on this massive data set, right?\n\n3:56:36.700 --> 3:56:40.700\n And then we need regulators not to stop people\n\n3:56:40.700 --> 3:56:43.820\n from trying radical therapies on themselves\n\n3:56:43.820 --> 3:56:46.180\n if they so wish to,\n\n3:56:46.180 --> 3:56:49.420\n as well as better cloud based platforms\n\n3:56:49.420 --> 3:56:52.720\n for like automated experimentation on microorganisms,\n\n3:56:52.720 --> 3:56:54.300\n flies and mice and so forth.\n\n3:56:54.300 --> 3:56:55.820\n And we could do all this.\n\n3:56:55.820 --> 3:56:58.900\n You look after the last financial crisis,\n\n3:56:58.900 --> 3:57:01.300\n Obama, who I generally like pretty well,\n\n3:57:01.300 --> 3:57:03.740\n but he gave $4 trillion to large banks\n\n3:57:03.740 --> 3:57:05.400\n and insurance companies.\n\n3:57:05.400 --> 3:57:07.480\n You know, now in this COVID crisis,\n\n3:57:08.420 --> 3:57:10.780\n trillions are being spent to help everyday people\n\n3:57:10.780 --> 3:57:12.240\n and small businesses.\n\n3:57:12.240 --> 3:57:14.580\n In the end, we'll probably will find many more trillions\n\n3:57:14.580 --> 3:57:17.220\n are being given to large banks and insurance companies.\n\n3:57:17.220 --> 3:57:21.020\n Anyway, like could the world put $10 trillion\n\n3:57:21.020 --> 3:57:25.560\n into making a massive holistic bio AI and bio simulation\n\n3:57:25.560 --> 3:57:27.800\n and experimental biology infrastructure?\n\n3:57:27.800 --> 3:57:30.600\n We could, we could put $10 trillion into that\n\n3:57:30.600 --> 3:57:32.300\n without even screwing us up too badly.\n\n3:57:32.300 --> 3:57:35.260\n Just as in the end COVID and the last financial crisis\n\n3:57:35.260 --> 3:57:37.900\n won't screw up the world economy so badly.\n\n3:57:37.900 --> 3:57:39.900\n We're not putting $10 trillion into that.\n\n3:57:39.900 --> 3:57:43.140\n Instead, all this research is siloed inside\n\n3:57:43.140 --> 3:57:46.820\n a few big companies and government agencies.\n\n3:57:46.820 --> 3:57:51.140\n And most of the data that comes from our individual bodies\n\n3:57:51.140 --> 3:57:54.340\n personally, that could feed this AI to solve aging\n\n3:57:54.340 --> 3:57:56.820\n and death, most of that data is sitting\n\n3:57:56.820 --> 3:58:00.160\n in some hospital's database doing nothing, right?\n\n3:58:03.960 --> 3:58:07.160\n I got two more quick questions for you.\n\n3:58:07.160 --> 3:58:09.820\n One, I know a lot of people are gonna ask me,\n\n3:58:09.820 --> 3:58:11.740\n you are on the Joe Rogan podcast\n\n3:58:11.740 --> 3:58:13.660\n wearing that same amazing hat.\n\n3:58:14.860 --> 3:58:17.500\n Do you have a origin story for the hat?\n\n3:58:17.500 --> 3:58:21.420\n Does the hat have its own story that you're able to share?\n\n3:58:21.420 --> 3:58:23.180\n The hat story has not been told yet.\n\n3:58:23.180 --> 3:58:24.220\n So we're gonna have to come back\n\n3:58:24.220 --> 3:58:25.880\n and you can interview the hat.\n\n3:58:27.880 --> 3:58:30.060\n We'll leave that for the hat's own interview.\n\n3:58:30.060 --> 3:58:30.900\n All right.\n\n3:58:30.900 --> 3:58:32.100\n It's too much to pack into.\n\n3:58:32.100 --> 3:58:32.940\n Is there a book?\n\n3:58:32.940 --> 3:58:34.320\n Is the hat gonna write a book?\n\n3:58:34.320 --> 3:58:35.160\n Okay.\n\n3:58:35.160 --> 3:58:38.340\n Well, it may transmit the information\n\n3:58:38.340 --> 3:58:40.020\n through direct neural transmission.\n\n3:58:40.020 --> 3:58:41.420\n Okay, so it's actually,\n\n3:58:41.420 --> 3:58:44.780\n there might be some Neuralink competition there.\n\n3:58:44.780 --> 3:58:46.900\n Beautiful, we'll leave it as a mystery.\n\n3:58:46.900 --> 3:58:49.040\n Maybe one last question.\n\n3:58:49.040 --> 3:58:52.740\n If you build an AGI system,\n\n3:58:54.580 --> 3:58:58.540\n you're successful at building the AGI system\n\n3:58:58.540 --> 3:59:00.420\n that could lead us to the singularity\n\n3:59:00.420 --> 3:59:04.560\n and you get to talk to her and ask her one question,\n\n3:59:04.560 --> 3:59:05.960\n what would that question be?\n\n3:59:05.960 --> 3:59:08.140\n We're not allowed to ask,\n\n3:59:08.140 --> 3:59:10.220\n what is the question I should be asking?\n\n3:59:10.220 --> 3:59:12.220\n Yeah, that would be cheating,\n\n3:59:12.220 --> 3:59:14.040\n but I guess that's a good question.\n\n3:59:14.040 --> 3:59:15.700\n I'm thinking of a,\n\n3:59:15.700 --> 3:59:18.600\n I wrote a story with Stefan Bugay once\n\n3:59:18.600 --> 3:59:23.380\n where these AI developers,\n\n3:59:23.380 --> 3:59:25.900\n they created a super smart AI\n\n3:59:25.900 --> 3:59:30.900\n aimed at answering all the philosophical questions\n\n3:59:31.220 --> 3:59:32.060\n that have been worrying them.\n\n3:59:32.060 --> 3:59:34.260\n Like what is the meaning of life?\n\n3:59:34.260 --> 3:59:35.700\n Is there free will?\n\n3:59:35.700 --> 3:59:37.980\n What is consciousness and so forth?\n\n3:59:37.980 --> 3:59:40.380\n So they got the super AGI built\n\n3:59:40.380 --> 3:59:43.300\n and it turned a while.\n\n3:59:43.300 --> 3:59:46.580\n It said, those are really stupid questions.\n\n3:59:46.580 --> 3:59:50.220\n And then it puts off on a spaceship and left the earth.\n\n3:59:51.420 --> 3:59:54.320\n So you'd be afraid of scaring it off.\n\n3:59:55.540 --> 3:59:56.500\n That's it, yeah.\n\n3:59:56.500 --> 4:00:01.500\n I mean, honestly, there is no one question\n\n4:00:01.500 --> 4:00:06.500\n that rises among all the others, really.\n\n4:00:08.540 --> 4:00:10.020\n I mean, what interests me more\n\n4:00:10.020 --> 4:00:13.500\n is upgrading my own intelligence\n\n4:00:13.500 --> 4:00:18.500\n so that I can absorb the whole world view of the super AGI.\n\n4:00:19.380 --> 4:00:23.100\n But I mean, of course, if the answer could be like,\n\n4:00:23.100 --> 4:00:27.500\n what is the chemical formula for the immortality pill?\n\n4:00:27.500 --> 4:00:32.500\n Like then I would do that or emit a bit string,\n\n4:00:33.340 --> 4:00:38.340\n which will be the code for a super AGI\n\n4:00:38.740 --> 4:00:41.220\n on the Intel i7 processor.\n\n4:00:41.220 --> 4:00:42.860\n So those would be good questions.\n\n4:00:42.860 --> 4:00:46.260\n So if your own mind was expanded\n\n4:00:46.260 --> 4:00:49.340\n to become super intelligent, like you're describing,\n\n4:00:49.340 --> 4:00:53.500\n I mean, there's kind of a notion\n\n4:00:53.500 --> 4:00:57.840\n that intelligence is a burden, that it's possible\n\n4:00:57.840 --> 4:01:00.020\n that with greater and greater intelligence,\n\n4:01:00.020 --> 4:01:03.020\n that other metric of joy that you mentioned\n\n4:01:03.020 --> 4:01:04.740\n becomes more and more difficult.\n\n4:01:04.740 --> 4:01:05.900\n What's your sense?\n\n4:01:05.900 --> 4:01:07.100\n Pretty stupid idea.\n\n4:01:08.260 --> 4:01:09.860\n So you think if you're super intelligent,\n\n4:01:09.860 --> 4:01:11.460\n you can also be super joyful?\n\n4:01:11.460 --> 4:01:15.460\n I think getting root access to your own brain\n\n4:01:15.460 --> 4:01:19.220\n will enable new forms of joy that we don't have now.\n\n4:01:19.220 --> 4:01:22.740\n And I think as I've said before,\n\n4:01:22.740 --> 4:01:27.740\n what I aim at is really make multiple versions of myself.\n\n4:01:27.820 --> 4:01:30.180\n So I would like to keep one version,\n\n4:01:30.180 --> 4:01:33.580\n which is basically human like I am now,\n\n4:01:33.580 --> 4:01:36.980\n but keep the dial to turn pain up and down\n\n4:01:36.980 --> 4:01:38.580\n and get rid of death, right?\n\n4:01:38.580 --> 4:01:43.580\n And make another version which fuses its mind\n\n4:01:43.640 --> 4:01:46.600\n with superhuman AGI,\n\n4:01:46.600 --> 4:01:50.060\n and then will become massively transhuman.\n\n4:01:50.060 --> 4:01:52.800\n And whether it will send some messages back\n\n4:01:52.800 --> 4:01:55.580\n to the human me or not will be interesting to find out.\n\n4:01:55.580 --> 4:01:58.500\n The thing is, once you're a super AGI,\n\n4:01:58.500 --> 4:02:01.540\n like one subjective second to a human\n\n4:02:01.540 --> 4:02:03.620\n might be like a million subjective years\n\n4:02:03.620 --> 4:02:04.980\n to that super AGI, right?\n\n4:02:04.980 --> 4:02:07.580\n So it would be on a whole different basis.\n\n4:02:07.580 --> 4:02:10.940\n I mean, at very least those two copies will be good to have,\n\n4:02:10.940 --> 4:02:13.980\n but it could be interesting to put your mind\n\n4:02:13.980 --> 4:02:16.860\n into a dolphin or a space amoeba\n\n4:02:16.860 --> 4:02:18.520\n or all sorts of other things.\n\n4:02:18.520 --> 4:02:21.060\n You can imagine one version that doubled its intelligence\n\n4:02:21.060 --> 4:02:24.140\n every year and another version that just became\n\n4:02:24.140 --> 4:02:26.140\n a super AGI as fast as possible, right?\n\n4:02:26.140 --> 4:02:29.780\n So, I mean, now we're sort of constrained to think\n\n4:02:29.780 --> 4:02:33.020\n one mind, one self, one body, right?\n\n4:02:33.020 --> 4:02:36.260\n But I think we actually, we don't need to be that\n\n4:02:36.260 --> 4:02:40.820\n constrained in thinking about future intelligence\n\n4:02:40.820 --> 4:02:44.280\n after we've mastered AGI and nanotechnology\n\n4:02:44.280 --> 4:02:47.820\n and longevity biology.\n\n4:02:47.820 --> 4:02:49.540\n I mean, then each of our minds\n\n4:02:49.540 --> 4:02:52.020\n is a certain pattern of organization, right?\n\n4:02:52.020 --> 4:02:54.300\n And I know we haven't talked about consciousness,\n\n4:02:54.300 --> 4:02:56.860\n but I sort of, I'm panpsychist.\n\n4:02:56.860 --> 4:03:00.080\n I sort of view the universe as conscious.\n\n4:03:00.080 --> 4:03:03.860\n And so, you know, a light bulb or a quark\n\n4:03:03.860 --> 4:03:06.040\n or an ant or a worm or a monkey\n\n4:03:06.040 --> 4:03:08.780\n have their own manifestations of consciousness.\n\n4:03:08.780 --> 4:03:11.900\n And the human manifestation of consciousness,\n\n4:03:11.900 --> 4:03:15.580\n it's partly tied to the particular meat\n\n4:03:15.580 --> 4:03:19.380\n that we're manifested by, but it's largely tied\n\n4:03:19.380 --> 4:03:22.360\n to the pattern of organization in the brain, right?\n\n4:03:22.360 --> 4:03:25.040\n So, if you upload yourself into a computer\n\n4:03:25.040 --> 4:03:28.640\n or a robot or whatever else it is,\n\n4:03:28.640 --> 4:03:31.780\n some element of your human consciousness may not be there\n\n4:03:31.780 --> 4:03:34.260\n because it's just tied to the biological embodiment.\n\n4:03:34.260 --> 4:03:36.300\n But I think most of it will be there.\n\n4:03:36.300 --> 4:03:40.020\n And these will be incarnations of your consciousness\n\n4:03:40.020 --> 4:03:42.500\n in a slightly different flavor.\n\n4:03:42.500 --> 4:03:45.600\n And, you know, creating these different versions\n\n4:03:45.600 --> 4:03:48.500\n will be amazing, and each of them will discover\n\n4:03:48.500 --> 4:03:52.020\n meanings of life that have some overlap,\n\n4:03:52.020 --> 4:03:54.300\n but probably not total overlap\n\n4:03:54.300 --> 4:03:59.260\n with the human Ben's meaning of life.\n\n4:03:59.260 --> 4:04:02.940\n The thing is, to get to that future\n\n4:04:02.940 --> 4:04:06.500\n where we can explore different varieties of joy,\n\n4:04:06.500 --> 4:04:09.680\n different variations of human experience and values\n\n4:04:09.680 --> 4:04:13.140\n and transhuman experiences and values to get to that future,\n\n4:04:13.140 --> 4:04:16.780\n we need to navigate through a whole lot of human bullshit\n\n4:04:16.780 --> 4:04:21.480\n of companies and governments and killer drones\n\n4:04:21.480 --> 4:04:25.460\n and making and losing money and so forth, right?\n\n4:04:25.460 --> 4:04:28.580\n And that's the challenge we're facing now\n\n4:04:28.580 --> 4:04:30.740\n is if we do things right,\n\n4:04:30.740 --> 4:04:33.580\n we can get to a benevolent singularity,\n\n4:04:33.580 --> 4:04:36.320\n which is levels of joy, growth, and choice\n\n4:04:36.320 --> 4:04:39.920\n that are literally unimaginable to human beings.\n\n4:04:39.920 --> 4:04:41.720\n If we do things wrong,\n\n4:04:41.720 --> 4:04:44.120\n we could either annihilate all life on the planet,\n\n4:04:44.120 --> 4:04:47.060\n or we could lead to a scenario where, say,\n\n4:04:47.060 --> 4:04:52.060\n all humans are annihilated and there's some super AGI\n\n4:04:52.140 --> 4:04:55.460\n that goes on and does its own thing unrelated to us\n\n4:04:55.460 --> 4:04:58.380\n except via our role in originating it.\n\n4:04:58.380 --> 4:05:02.420\n And we may well be at a bifurcation point now, right?\n\n4:05:02.420 --> 4:05:05.820\n Where what we do now has significant causal impact\n\n4:05:05.820 --> 4:05:06.720\n on what comes about,\n\n4:05:06.720 --> 4:05:09.040\n and yet most people on the planet\n\n4:05:09.040 --> 4:05:11.540\n aren't thinking that way whatsoever,\n\n4:05:11.540 --> 4:05:16.220\n they're thinking only about their own narrow aims\n\n4:05:16.220 --> 4:05:17.780\n and aims and goals, right?\n\n4:05:17.780 --> 4:05:20.880\n Now, of course, I'm thinking about my own narrow aims\n\n4:05:20.880 --> 4:05:24.260\n and goals to some extent also,\n\n4:05:24.260 --> 4:05:29.260\n but I'm trying to use as much of my energy and mind as I can\n\n4:05:29.480 --> 4:05:33.200\n to push toward this more benevolent alternative,\n\n4:05:33.200 --> 4:05:34.660\n which will be better for me,\n\n4:05:34.660 --> 4:05:37.980\n but also for everybody else.\n\n4:05:37.980 --> 4:05:42.540\n And it's weird that so few people understand\n\n4:05:42.540 --> 4:05:43.380\n what's going on.\n\n4:05:43.380 --> 4:05:44.780\n I know you interviewed Elon Musk,\n\n4:05:44.780 --> 4:05:47.380\n and he understands a lot of what's going on,\n\n4:05:47.380 --> 4:05:49.620\n but he's much more paranoid than I am, right?\n\n4:05:49.620 --> 4:05:52.040\n Because Elon gets that AGI\n\n4:05:52.040 --> 4:05:54.260\n is gonna be way, way smarter than people,\n\n4:05:54.260 --> 4:05:57.100\n and he gets that an AGI does not necessarily\n\n4:05:57.100 --> 4:05:58.740\n have to give a shit about people\n\n4:05:58.740 --> 4:06:01.660\n because we're a very elementary mode of organization\n\n4:06:01.660 --> 4:06:04.700\n of matter compared to many AGI's.\n\n4:06:04.700 --> 4:06:06.340\n But I don't think he has a clear vision\n\n4:06:06.340 --> 4:06:10.140\n of how infusing early stage AGI's\n\n4:06:10.140 --> 4:06:13.540\n with compassion and human warmth\n\n4:06:13.540 --> 4:06:18.020\n can lead to an AGI that loves and helps people\n\n4:06:18.020 --> 4:06:22.860\n rather than viewing us as a historical artifact\n\n4:06:22.860 --> 4:06:26.200\n and a waste of mass energy.\n\n4:06:26.200 --> 4:06:28.060\n But on the other hand,\n\n4:06:28.060 --> 4:06:29.600\n while I have some disagreements with him,\n\n4:06:29.600 --> 4:06:33.140\n like he understands way, way more of the story\n\n4:06:33.140 --> 4:06:34.820\n than almost anyone else\n\n4:06:34.820 --> 4:06:38.180\n in such a large scale corporate leadership position, right?\n\n4:06:38.180 --> 4:06:40.740\n It's terrible how little understanding\n\n4:06:40.740 --> 4:06:45.060\n of these fundamental issues exists out there now.\n\n4:06:45.060 --> 4:06:47.220\n That may be different five or 10 years from now though,\n\n4:06:47.220 --> 4:06:51.180\n because I can see understanding of AGI and longevity\n\n4:06:51.180 --> 4:06:54.620\n and other such issues is certainly much stronger\n\n4:06:54.620 --> 4:06:57.620\n and more prevalent now than 10 or 15 years ago, right?\n\n4:06:57.620 --> 4:07:02.620\n So I mean, humanity as a whole can be slow learners\n\n4:07:02.860 --> 4:07:05.460\n relative to what I would like,\n\n4:07:05.460 --> 4:07:08.400\n but on a historical sense, on the other hand,\n\n4:07:08.400 --> 4:07:11.220\n you could say the progress is astoundingly fast.\n\n4:07:11.220 --> 4:07:15.640\n But Elon also said, I think on the Joe Rogan podcast,\n\n4:07:15.640 --> 4:07:17.380\n that love is the answer.\n\n4:07:17.380 --> 4:07:21.820\n So maybe in that way, you and him are both on the same page\n\n4:07:21.820 --> 4:07:24.420\n of how we should proceed with AGI.\n\n4:07:24.420 --> 4:07:27.300\n I think there's no better place to end it.\n\n4:07:27.300 --> 4:07:30.860\n I hope we get to talk again about the hat\n\n4:07:30.860 --> 4:07:32.020\n and about consciousness\n\n4:07:32.020 --> 4:07:34.500\n and about a million topics we didn't cover.\n\n4:07:34.500 --> 4:07:36.340\n Ben, it's a huge honor to talk to you.\n\n4:07:36.340 --> 4:07:37.540\n Thank you for making it out.\n\n4:07:37.540 --> 4:07:39.540\n Thank you for talking today.\n\n4:07:39.540 --> 4:07:40.440\n Thanks for having me.\n\n4:07:40.440 --> 4:07:44.380\n This was really, really good fun\n\n4:07:44.380 --> 4:07:47.420\n and we dug deep into some very important things.\n\n4:07:47.420 --> 4:07:48.740\n So thanks for doing this.\n\n4:07:48.740 --> 4:07:49.820\n Thanks very much.\n\n4:07:49.820 --> 4:07:51.200\n Awesome.\n\n4:07:51.200 --> 4:07:53.860\n Thanks for listening to this conversation with Ben Gertzel\n\n4:07:53.860 --> 4:07:55.860\n and thank you to our sponsors,\n\n4:07:55.860 --> 4:07:59.380\n The Jordan Harbinger Show and Masterclass.\n\n4:07:59.380 --> 4:08:01.080\n Please consider supporting the podcast\n\n4:08:01.080 --> 4:08:04.580\n by going to jordanharbinger.com slash lex\n\n4:08:04.580 --> 4:08:09.580\n and signing up to Masterclass at masterclass.com slash lex.\n\n4:08:09.800 --> 4:08:12.280\n Click the links, buy the stuff.\n\n4:08:12.280 --> 4:08:14.220\n It's the best way to support this podcast\n\n4:08:14.220 --> 4:08:18.860\n and the journey I'm on in my research and startup.\n\n4:08:18.860 --> 4:08:21.380\n If you enjoy this thing, subscribe on YouTube,\n\n4:08:21.380 --> 4:08:23.720\n review it with five stars on a podcast,\n\n4:08:23.720 --> 4:08:26.860\n support it on Patreon or connect with me on Twitter\n\n4:08:26.860 --> 4:08:31.860\n at lexfriedman spelled without the E, just F R I D M A N.\n\n4:08:32.400 --> 4:08:35.280\n I'm sure eventually you will figure it out.\n\n4:08:35.280 --> 4:08:38.240\n And now let me leave you with some words from Ben Gertzel.\n\n4:08:39.140 --> 4:08:42.540\n Our language for describing emotions is very crude.\n\n4:08:42.540 --> 4:08:43.940\n That's what music is for.\n\n4:08:43.940 --> 4:08:56.940\n Thank you for listening and hope to see you next time.\n\n"
}