{
  "title": "Gary Marcus: Toward a Hybrid of Deep Learning and Symbolic AI | Lex Fridman Podcast #43",
  "id": "vNOTDn3D_RI",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:02.740\n The following is a conversation with Gary Marcus.\n\n00:02.740 --> 00:04.980\n He's a professor emeritus at NYU,\n\n00:04.980 --> 00:08.200\n founder of Robust AI and Geometric Intelligence.\n\n00:08.200 --> 00:10.300\n The latter is a machine learning company\n\n00:10.300 --> 00:13.500\n that was acquired by Uber in 2016.\n\n00:13.500 --> 00:15.740\n He's the author of several books,\n\n00:15.740 --> 00:18.180\n Unnatural and Artificial Intelligence,\n\n00:18.180 --> 00:20.840\n including his new book, Rebooting AI,\n\n00:20.840 --> 00:23.340\n Building Machines We Can Trust.\n\n00:23.340 --> 00:25.480\n Gary has been a critical voice,\n\n00:25.480 --> 00:28.780\n highlighting the limits of deep learning and AI in general\n\n00:28.780 --> 00:33.700\n and discussing the challenges before our AI community\n\n00:33.700 --> 00:35.740\n that must be solved in order to achieve\n\n00:35.740 --> 00:38.300\n artificial general intelligence.\n\n00:38.300 --> 00:40.100\n As I'm having these conversations,\n\n00:40.100 --> 00:43.600\n I try to find paths toward insight, towards new ideas.\n\n00:43.600 --> 00:45.940\n I try to have no ego in the process.\n\n00:45.940 --> 00:47.640\n It gets in the way.\n\n00:47.640 --> 00:52.300\n I'll often continuously try on several hats, several roles.\n\n00:52.300 --> 00:54.740\n One, for example, is the role of a three year old\n\n00:54.740 --> 00:57.140\n who understands very little about anything\n\n00:57.140 --> 01:00.340\n and asks big what and why questions.\n\n01:00.340 --> 01:02.940\n The other might be a role of a devil's advocate\n\n01:02.940 --> 01:05.600\n who presents counter ideas with the goal of arriving\n\n01:05.600 --> 01:08.240\n at greater understanding through debate.\n\n01:08.240 --> 01:11.240\n Hopefully, both are useful, interesting,\n\n01:11.240 --> 01:13.400\n and even entertaining at times.\n\n01:13.400 --> 01:15.400\n I ask for your patience as I learn\n\n01:15.400 --> 01:17.760\n to have better conversations.\n\n01:17.760 --> 01:20.800\n This is the Artificial Intelligence Podcast.\n\n01:20.800 --> 01:23.140\n If you enjoy it, subscribe on YouTube,\n\n01:23.140 --> 01:26.340\n give it five stars on iTunes, support it on Patreon,\n\n01:26.340 --> 01:28.560\n or simply connect with me on Twitter\n\n01:28.560 --> 01:32.540\n at Lex Friedman, spelled F R I D M A N.\n\n01:32.540 --> 01:36.340\n And now, here's my conversation with Gary Marcus.\n\n01:37.220 --> 01:40.400\n Do you think human civilization will one day have\n\n01:40.400 --> 01:42.960\n to face an AI driven technological singularity\n\n01:42.960 --> 01:45.620\n that will, in a societal way,\n\n01:45.620 --> 01:47.260\n modify our place in the food chain\n\n01:47.260 --> 01:50.140\n of intelligent living beings on this planet?\n\n01:50.140 --> 01:54.860\n I think our place in the food chain has already changed.\n\n01:54.860 --> 01:57.340\n So there are lots of things people used to do by hand\n\n01:57.340 --> 01:59.180\n that they do with machine.\n\n01:59.180 --> 02:01.800\n If you think of a singularity as like one single moment,\n\n02:01.800 --> 02:03.220\n which is, I guess, what it suggests,\n\n02:03.220 --> 02:04.580\n I don't know if it'll be like that,\n\n02:04.580 --> 02:07.340\n but I think that there's a lot of gradual change\n\n02:07.340 --> 02:09.220\n and AI is getting better and better.\n\n02:09.220 --> 02:11.420\n I mean, I'm here to tell you why I think it's not nearly\n\n02:11.420 --> 02:14.380\n as good as people think, but the overall trend is clear.\n\n02:14.380 --> 02:17.380\n Maybe Rick Hertzweil thinks it's an exponential\n\n02:17.380 --> 02:18.440\n and I think it's linear.\n\n02:18.440 --> 02:20.800\n In some cases, it's close to zero right now,\n\n02:20.800 --> 02:21.820\n but it's all gonna happen.\n\n02:21.820 --> 02:24.780\n I mean, we are gonna get to human level intelligence\n\n02:24.780 --> 02:28.660\n or whatever you want, artificial general intelligence\n\n02:28.660 --> 02:31.380\n at some point, and that's certainly gonna change\n\n02:31.380 --> 02:32.500\n our place in the food chain,\n\n02:32.500 --> 02:35.200\n because a lot of the tedious things that we do now,\n\n02:35.200 --> 02:36.040\n we're gonna have machines do,\n\n02:36.040 --> 02:38.540\n and a lot of the dangerous things that we do now,\n\n02:38.540 --> 02:39.900\n we're gonna have machines do.\n\n02:39.900 --> 02:41.660\n I think our whole lives are gonna change\n\n02:41.660 --> 02:45.020\n from people finding their meaning through their work\n\n02:45.020 --> 02:46.700\n through people finding their meaning\n\n02:46.700 --> 02:48.660\n through creative expression.\n\n02:48.660 --> 02:53.660\n So the singularity will be a very gradual,\n\n02:53.660 --> 02:56.620\n in fact, removing the meaning of the word singularity.\n\n02:56.620 --> 03:00.540\n It'll be a very gradual transformation in your view.\n\n03:00.540 --> 03:03.460\n I think that it'll be somewhere in between,\n\n03:03.460 --> 03:05.700\n and I guess it depends what you mean by gradual and sudden.\n\n03:05.700 --> 03:07.340\n I don't think it's gonna be one day.\n\n03:07.340 --> 03:08.860\n I think it's important to realize\n\n03:08.860 --> 03:11.820\n that intelligence is a multidimensional variable.\n\n03:11.820 --> 03:14.420\n So people sort of write this stuff\n\n03:14.420 --> 03:19.420\n as if IQ was one number, and the day that you hit 262\n\n03:20.620 --> 03:22.700\n or whatever, you displace the human beings.\n\n03:22.700 --> 03:25.300\n And really, there's lots of facets to intelligence.\n\n03:25.300 --> 03:26.740\n So there's verbal intelligence,\n\n03:26.740 --> 03:28.580\n and there's motor intelligence,\n\n03:28.580 --> 03:32.060\n and there's mathematical intelligence and so forth.\n\n03:32.060 --> 03:34.620\n Machines, in their mathematical intelligence,\n\n03:34.620 --> 03:36.900\n far exceed most people already.\n\n03:36.900 --> 03:38.140\n In their ability to play games,\n\n03:38.140 --> 03:40.080\n they far exceed most people already.\n\n03:40.080 --> 03:41.760\n In their ability to understand language,\n\n03:41.760 --> 03:43.140\n they lag behind my five year old,\n\n03:43.140 --> 03:44.740\n far behind my five year old.\n\n03:44.740 --> 03:46.860\n So there are some facets of intelligence\n\n03:46.860 --> 03:49.460\n that machines have grasped, and some that they haven't,\n\n03:49.460 --> 03:51.780\n and we have a lot of work left to do\n\n03:51.780 --> 03:54.300\n to get them to, say, understand natural language,\n\n03:54.300 --> 03:57.780\n or to understand how to flexibly approach\n\n03:57.780 --> 04:01.340\n some kind of novel MacGyver problem solving\n\n04:01.340 --> 04:03.020\n kind of situation.\n\n04:03.020 --> 04:05.620\n And I don't know that all of these things will come at once.\n\n04:05.620 --> 04:07.940\n I think there are certain vital prerequisites\n\n04:07.940 --> 04:09.320\n that we're missing now.\n\n04:09.320 --> 04:12.500\n So for example, machines don't really have common sense now.\n\n04:12.500 --> 04:15.540\n So they don't understand that bottles contain water,\n\n04:15.540 --> 04:18.160\n and that people drink water to quench their thirst,\n\n04:18.160 --> 04:19.500\n and that they don't wanna dehydrate.\n\n04:19.500 --> 04:22.100\n They don't know these basic facts about human beings,\n\n04:22.100 --> 04:24.440\n and I think that that's a rate limiting step\n\n04:24.440 --> 04:25.300\n for many things.\n\n04:25.300 --> 04:27.680\n It's a great limiting step for reading, for example,\n\n04:27.680 --> 04:29.740\n because stories depend on things like,\n\n04:29.740 --> 04:31.540\n oh my God, that person's running out of water.\n\n04:31.540 --> 04:33.040\n That's why they did this thing.\n\n04:33.040 --> 04:37.100\n Or if they only had water, they could put out the fire.\n\n04:37.100 --> 04:39.380\n So you watch a movie, and your knowledge\n\n04:39.380 --> 04:41.220\n about how things work matter.\n\n04:41.220 --> 04:44.320\n And so a computer can't understand that movie\n\n04:44.320 --> 04:45.780\n if it doesn't have that background knowledge.\n\n04:45.780 --> 04:47.900\n Same thing if you read a book.\n\n04:47.900 --> 04:49.660\n And so there are lots of places where,\n\n04:49.660 --> 04:53.740\n if we had a good machine interpretable set of common sense,\n\n04:53.740 --> 04:56.580\n many things would accelerate relatively quickly,\n\n04:56.580 --> 04:59.940\n but I don't think even that is a single point.\n\n04:59.940 --> 05:02.540\n There's many different aspects of knowledge.\n\n05:02.540 --> 05:05.260\n And we might, for example, find that we make a lot\n\n05:05.260 --> 05:06.660\n of progress on physical reasoning,\n\n05:06.660 --> 05:09.140\n getting machines to understand, for example,\n\n05:09.140 --> 05:11.980\n how keys fit into locks, or that kind of stuff,\n\n05:11.980 --> 05:16.980\n or how this gadget here works, and so forth and so on.\n\n05:16.980 --> 05:19.500\n And so machines might do that long before they do\n\n05:19.500 --> 05:21.780\n really good psychological reasoning,\n\n05:21.780 --> 05:24.380\n because it's easier to get kind of labeled data\n\n05:24.380 --> 05:28.680\n or to do direct experimentation on a microphone stand\n\n05:28.680 --> 05:31.780\n than it is to do direct experimentation on human beings\n\n05:31.780 --> 05:34.860\n to understand the levers that guide them.\n\n05:34.860 --> 05:36.860\n That's a really interesting point, actually,\n\n05:36.860 --> 05:39.740\n whether it's easier to gain common sense knowledge\n\n05:39.740 --> 05:41.740\n or psychological knowledge.\n\n05:41.740 --> 05:43.300\n I would say the common sense knowledge\n\n05:43.300 --> 05:46.860\n includes both physical knowledge and psychological knowledge.\n\n05:46.860 --> 05:47.700\n And the argument I was making.\n\n05:47.700 --> 05:49.660\n Well, you said physical versus psychological.\n\n05:49.660 --> 05:51.100\n Yeah, physical versus psychological.\n\n05:51.100 --> 05:53.260\n And the argument I was making is physical knowledge\n\n05:53.260 --> 05:55.300\n might be more accessible, because you could have a robot,\n\n05:55.300 --> 05:58.420\n for example, lift a bottle, try putting a bottle cap on it,\n\n05:58.420 --> 06:00.420\n see that it falls off if it does this,\n\n06:00.420 --> 06:02.020\n and see that it could turn it upside down,\n\n06:02.020 --> 06:04.700\n and so the robot could do some experimentation.\n\n06:04.700 --> 06:07.220\n We do some of our psychological reasoning\n\n06:07.220 --> 06:09.240\n by looking at our own minds.\n\n06:09.240 --> 06:11.940\n So I can sort of guess how you might react to something\n\n06:11.940 --> 06:13.660\n based on how I think I would react to it.\n\n06:13.660 --> 06:15.980\n And robots don't have that intuition,\n\n06:15.980 --> 06:18.460\n and they also can't do experiments on people\n\n06:18.460 --> 06:20.500\n in the same way or we'll probably shut them down.\n\n06:20.500 --> 06:24.260\n So if we wanted to have robots figure out\n\n06:24.260 --> 06:27.800\n how I respond to pain by pinching me in different ways,\n\n06:27.800 --> 06:29.660\n like that's probably, it's not gonna make it\n\n06:29.660 --> 06:31.020\n past the human subjects board\n\n06:31.020 --> 06:32.900\n and companies are gonna get sued or whatever.\n\n06:32.900 --> 06:35.860\n So there's certain kinds of practical experience\n\n06:35.860 --> 06:39.660\n that are limited or off limits to robots.\n\n06:39.660 --> 06:41.060\n That's a really interesting point.\n\n06:41.060 --> 06:46.060\n What is more difficult to gain a grounding in?\n\n06:47.540 --> 06:49.940\n Because to play devil's advocate,\n\n06:49.940 --> 06:54.940\n I would say that human behavior is easier expressed\n\n06:54.980 --> 06:56.940\n in data and digital form.\n\n06:56.940 --> 06:59.100\n And so when you look at Facebook algorithms,\n\n06:59.100 --> 07:01.100\n they get to observe human behavior.\n\n07:01.100 --> 07:04.620\n So you get to study and manipulate even a human behavior\n\n07:04.620 --> 07:07.740\n in a way that you perhaps cannot study\n\n07:07.740 --> 07:09.540\n or manipulate the physical world.\n\n07:09.540 --> 07:14.400\n So it's true why you said pain is like physical pain,\n\n07:14.400 --> 07:16.020\n but that's again, the physical world.\n\n07:16.020 --> 07:20.080\n Emotional pain might be much easier to experiment with,\n\n07:20.080 --> 07:22.740\n perhaps unethical, but nevertheless,\n\n07:22.740 --> 07:25.380\n some would argue it's already going on.\n\n07:25.380 --> 07:27.340\n I think that you're right, for example,\n\n07:27.340 --> 07:30.980\n that Facebook does a lot of experimentation\n\n07:30.980 --> 07:32.900\n in psychological reasoning.\n\n07:32.900 --> 07:36.040\n In fact, Zuckerberg talked about AI\n\n07:36.040 --> 07:38.400\n at a talk that he gave in NIPS.\n\n07:38.400 --> 07:40.300\n I wasn't there, but the conference\n\n07:40.300 --> 07:41.300\n has been renamed NeurIPS,\n\n07:41.300 --> 07:43.740\n but he used to be called NIPS when he gave the talk.\n\n07:43.740 --> 07:45.300\n And he talked about Facebook basically\n\n07:45.300 --> 07:47.100\n having a gigantic theory of mind.\n\n07:47.100 --> 07:49.540\n So I think it is certainly possible.\n\n07:49.540 --> 07:51.220\n I mean, Facebook does some of that.\n\n07:51.220 --> 07:52.620\n I think they have a really good idea\n\n07:52.620 --> 07:53.900\n of how to addict people to things.\n\n07:53.900 --> 07:56.420\n They understand what draws people back to things.\n\n07:56.420 --> 07:57.580\n I think they exploit it in ways\n\n07:57.580 --> 07:59.220\n that I'm not very comfortable with.\n\n07:59.220 --> 08:03.300\n But even so, I think that there are only some slices\n\n08:03.300 --> 08:05.620\n of human experience that they can access\n\n08:05.620 --> 08:07.220\n through the kind of interface they have.\n\n08:07.220 --> 08:08.980\n And of course, they're doing all kinds of VR stuff,\n\n08:08.980 --> 08:11.940\n and maybe that'll change and they'll expand their data.\n\n08:11.940 --> 08:14.940\n And I'm sure that that's part of their goal.\n\n08:14.940 --> 08:16.860\n So it is an interesting question.\n\n08:16.860 --> 08:21.700\n I think love, fear, insecurity,\n\n08:21.700 --> 08:24.300\n all of the things that,\n\n08:24.300 --> 08:26.620\n I would say some of the deepest things\n\n08:26.620 --> 08:28.620\n about human nature and the human mind\n\n08:28.620 --> 08:30.500\n could be explored through digital form.\n\n08:30.500 --> 08:32.220\n It's that you're actually the first person\n\n08:32.220 --> 08:33.680\n just now that brought up,\n\n08:33.680 --> 08:35.860\n I wonder what is more difficult.\n\n08:35.860 --> 08:40.220\n Because I think folks who are the slow,\n\n08:40.220 --> 08:41.820\n and we'll talk a lot about deep learning,\n\n08:41.820 --> 08:44.860\n but the people who are thinking beyond deep learning\n\n08:44.860 --> 08:46.420\n are thinking about the physical world.\n\n08:46.420 --> 08:48.060\n You're starting to think about robotics\n\n08:48.060 --> 08:49.180\n in the home robotics.\n\n08:49.180 --> 08:52.300\n How do we make robots manipulate objects,\n\n08:52.300 --> 08:55.020\n which requires an understanding of the physical world\n\n08:55.020 --> 08:57.300\n and then requires common sense reasoning.\n\n08:57.300 --> 08:59.440\n And that has felt to be like the next step\n\n08:59.440 --> 09:00.420\n for common sense reasoning,\n\n09:00.420 --> 09:02.100\n but you've now brought up the idea\n\n09:02.100 --> 09:03.620\n that there's also the emotional part.\n\n09:03.620 --> 09:06.840\n And it's interesting whether that's hard or easy.\n\n09:06.840 --> 09:08.540\n I think some parts of it are and some aren't.\n\n09:08.540 --> 09:12.660\n So my company that I recently founded with Rod Brooks,\n\n09:12.660 --> 09:15.000\n from MIT for many years and so forth,\n\n09:15.940 --> 09:17.240\n we're interested in both.\n\n09:17.240 --> 09:18.580\n We're interested in physical reasoning\n\n09:18.580 --> 09:21.500\n and psychological reasoning, among many other things.\n\n09:21.500 --> 09:26.140\n And there are pieces of each of these that are accessible.\n\n09:26.140 --> 09:28.020\n So if you want a robot to figure out\n\n09:28.020 --> 09:29.720\n whether it can fit under a table,\n\n09:29.720 --> 09:33.660\n that's a relatively accessible piece of physical reasoning.\n\n09:33.660 --> 09:34.760\n If you know the height of the table\n\n09:34.760 --> 09:36.980\n and you know the height of the robot, it's not that hard.\n\n09:36.980 --> 09:39.900\n If you wanted to do physical reasoning about Jenga,\n\n09:39.900 --> 09:41.500\n it gets a little bit more complicated\n\n09:41.500 --> 09:43.820\n and you have to have higher resolution data\n\n09:43.820 --> 09:45.260\n in order to do it.\n\n09:45.260 --> 09:46.900\n With psychological reasoning,\n\n09:46.900 --> 09:49.320\n it's not that hard to know, for example,\n\n09:49.320 --> 09:51.700\n that people have goals and they like to act on those goals,\n\n09:51.700 --> 09:54.900\n but it's really hard to know exactly what those goals are.\n\n09:54.900 --> 09:56.780\n But ideas of frustration.\n\n09:56.780 --> 09:58.780\n I mean, you could argue it's extremely difficult\n\n09:58.780 --> 10:01.460\n to understand the sources of human frustration\n\n10:01.460 --> 10:05.740\n as they're playing Jenga with you, or not.\n\n10:05.740 --> 10:08.020\n You could argue that it's very accessible.\n\n10:08.020 --> 10:09.740\n There's some things that are gonna be obvious\n\n10:09.740 --> 10:10.580\n and some not.\n\n10:10.580 --> 10:14.220\n So I don't think anybody really can do this well yet,\n\n10:14.220 --> 10:16.620\n but I think it's not inconceivable\n\n10:16.620 --> 10:20.120\n to imagine machines in the not so distant future\n\n10:20.120 --> 10:24.220\n being able to understand that if people lose in a game,\n\n10:24.220 --> 10:26.260\n that they don't like that.\n\n10:26.260 --> 10:27.940\n That's not such a hard thing to program\n\n10:27.940 --> 10:29.980\n and it's pretty consistent across people.\n\n10:29.980 --> 10:31.540\n Most people don't enjoy losing\n\n10:31.540 --> 10:34.620\n and so that makes it relatively easy to code.\n\n10:34.620 --> 10:36.860\n On the other hand, if you wanted to capture everything\n\n10:36.860 --> 10:39.180\n about frustration, well, people can get frustrated\n\n10:39.180 --> 10:40.320\n for a lot of different reasons.\n\n10:40.320 --> 10:42.340\n They might get sexually frustrated,\n\n10:42.340 --> 10:43.180\n they might get frustrated,\n\n10:43.180 --> 10:45.140\n they can get their promotion at work,\n\n10:45.140 --> 10:46.900\n all kinds of different things.\n\n10:46.900 --> 10:48.580\n And the more you expand the scope,\n\n10:48.580 --> 10:51.540\n the harder it is for anything like the existing techniques\n\n10:51.540 --> 10:53.000\n to really do that.\n\n10:53.000 --> 10:55.660\n So I'm talking to Garret Kasparov next week\n\n10:55.660 --> 10:57.220\n and he seemed pretty frustrated\n\n10:57.220 --> 10:58.940\n with his game against Deep Blue, so.\n\n10:58.940 --> 11:00.300\n Yeah, well, I'm frustrated with my game\n\n11:00.300 --> 11:01.340\n against him last year,\n\n11:01.340 --> 11:03.620\n because I played him, I had two excuses,\n\n11:03.620 --> 11:04.900\n I'll give you my excuses up front,\n\n11:04.900 --> 11:07.060\n but it won't mitigate the outcome.\n\n11:07.060 --> 11:11.100\n I was jet lagged and I hadn't played in 25 or 30 years,\n\n11:11.100 --> 11:13.020\n but the outcome is he completely destroyed me\n\n11:13.020 --> 11:14.420\n and it wasn't even close.\n\n11:14.420 --> 11:19.420\n Have you ever been beaten in any board game by a machine?\n\n11:19.740 --> 11:24.740\n I have, I actually played the predecessor to Deep Blue.\n\n11:24.740 --> 11:27.940\n Deep Thought, I believe it was called,\n\n11:27.940 --> 11:30.000\n and that too crushed me.\n\n11:30.000 --> 11:35.000\n And that was, and after that you realize it's over for us.\n\n11:35.340 --> 11:36.820\n Well, there's no point in my playing Deep Blue.\n\n11:36.820 --> 11:40.260\n I mean, it's a waste of Deep Blue's computation.\n\n11:40.260 --> 11:41.540\n I mean, I played Kasparov\n\n11:41.540 --> 11:44.820\n because we both gave lectures this same event\n\n11:44.820 --> 11:46.020\n and he was playing 30 people.\n\n11:46.020 --> 11:46.900\n I forgot to mention that.\n\n11:46.900 --> 11:47.980\n Not only did he crush me,\n\n11:47.980 --> 11:50.660\n but he crushed 29 other people at the same time.\n\n11:50.660 --> 11:55.460\n I mean, but the actual philosophical and emotional experience\n\n11:55.460 --> 11:59.100\n of being beaten by a machine, I imagine is a,\n\n11:59.100 --> 12:01.380\n I mean, to you who thinks about these things\n\n12:01.380 --> 12:03.580\n may be a profound experience.\n\n12:03.580 --> 12:07.780\n Or no, it was a simple mathematical experience.\n\n12:07.780 --> 12:10.300\n Yeah, I think a game like chess particularly\n\n12:10.300 --> 12:12.740\n where you have perfect information,\n\n12:12.740 --> 12:14.780\n it's two player closed end\n\n12:14.780 --> 12:16.940\n and there's more computation for the computer,\n\n12:16.940 --> 12:18.860\n it's no surprise the machine wins.\n\n12:18.860 --> 12:22.020\n I mean, I'm not sad when a computer,\n\n12:22.020 --> 12:23.940\n I'm not sad when a computer calculates\n\n12:23.940 --> 12:25.220\n a cube root faster than me.\n\n12:25.220 --> 12:27.860\n Like, I know I can't win that game.\n\n12:27.860 --> 12:28.900\n I'm not gonna try.\n\n12:28.900 --> 12:32.080\n Well, with a system like AlphaGo or AlphaZero,\n\n12:32.080 --> 12:35.060\n do you see a little bit more magic in a system like that\n\n12:35.060 --> 12:37.260\n even though it's simply playing a board game?\n\n12:37.260 --> 12:39.940\n But because there's a strong learning component?\n\n12:39.940 --> 12:41.300\n You know, I find you should mention that\n\n12:41.300 --> 12:42.580\n in the context of this conversation\n\n12:42.580 --> 12:45.300\n because Kasparov and I are working on an article\n\n12:45.300 --> 12:47.300\n that's gonna be called AI is not magic.\n\n12:47.300 --> 12:50.500\n And, you know, neither one of us thinks that it's magic.\n\n12:50.500 --> 12:51.980\n And part of the point of this article\n\n12:51.980 --> 12:55.140\n is that AI is actually a grab bag of different techniques\n\n12:55.140 --> 12:56.060\n and some of them have,\n\n12:56.060 --> 12:59.140\n or they each have their own unique strengths and weaknesses.\n\n13:00.060 --> 13:02.820\n So, you know, you read media accounts\n\n13:02.820 --> 13:05.200\n and it's like, ooh, AI, it must be magical\n\n13:05.200 --> 13:06.580\n or it can solve any problem.\n\n13:06.580 --> 13:09.500\n Well, no, some problems are really accessible\n\n13:09.500 --> 13:11.980\n like chess and go and other problems like reading\n\n13:11.980 --> 13:14.940\n are completely outside the current technology.\n\n13:14.940 --> 13:17.100\n And it's not like you can take the technology,\n\n13:17.100 --> 13:20.100\n that drives AlphaGo and apply it to reading\n\n13:20.100 --> 13:21.340\n and get anywhere.\n\n13:21.340 --> 13:23.180\n You know, DeepMind has tried that a bit.\n\n13:23.180 --> 13:24.500\n They have all kinds of resources.\n\n13:24.500 --> 13:26.180\n You know, they built AlphaGo and they have,\n\n13:26.180 --> 13:29.460\n you know, I wrote a piece recently that they lost\n\n13:29.460 --> 13:30.540\n and you can argue about the word lost,\n\n13:30.540 --> 13:34.900\n but they spent $530 million more than they made last year.\n\n13:34.900 --> 13:36.620\n So, you know, they're making huge investments.\n\n13:36.620 --> 13:37.860\n They have a large budget\n\n13:37.860 --> 13:40.900\n and they have applied the same kinds of techniques\n\n13:40.900 --> 13:43.220\n to reading or to language.\n\n13:43.220 --> 13:45.540\n It's just much less productive there\n\n13:45.540 --> 13:47.900\n because it's a fundamentally different kind of problem.\n\n13:47.900 --> 13:50.660\n Chess and go and so forth are closed end problems.\n\n13:50.660 --> 13:52.980\n The rules haven't changed in 2,500 years.\n\n13:52.980 --> 13:54.700\n There's only so many moves you can make.\n\n13:54.700 --> 13:56.460\n You can talk about the exponential\n\n13:56.460 --> 13:58.180\n as you look at the combinations of moves,\n\n13:58.180 --> 14:01.240\n but fundamentally, you know, the go board has 361 squares.\n\n14:01.240 --> 14:02.080\n That's it.\n\n14:02.080 --> 14:04.100\n That's the only, you know, those intersections\n\n14:04.100 --> 14:07.300\n are the only places that you can place your stone.\n\n14:07.300 --> 14:09.140\n Whereas when you're reading,\n\n14:09.140 --> 14:11.460\n the next sentence could be anything.\n\n14:11.460 --> 14:13.300\n You know, it's completely up to the writer\n\n14:13.300 --> 14:14.460\n what they're gonna do next.\n\n14:14.460 --> 14:16.260\n That's fascinating that you think this way.\n\n14:16.260 --> 14:17.980\n You're clearly a brilliant mind\n\n14:17.980 --> 14:19.700\n who points out the emperor has no clothes,\n\n14:19.700 --> 14:22.300\n but so I'll play the role of a person who says.\n\n14:22.300 --> 14:23.300\n You're gonna put clothes on the emperor?\n\n14:23.300 --> 14:24.140\n Good luck with it.\n\n14:24.140 --> 14:27.980\n It romanticizes the notion of the emperor, period,\n\n14:27.980 --> 14:30.140\n suggesting that clothes don't even matter.\n\n14:30.140 --> 14:33.580\n Okay, so that's really interesting\n\n14:33.580 --> 14:35.380\n that you're talking about language.\n\n14:36.260 --> 14:37.780\n So there's the physical world\n\n14:37.780 --> 14:39.680\n of being able to move about the world,\n\n14:39.680 --> 14:41.940\n making an omelet and coffee and so on.\n\n14:41.940 --> 14:46.020\n There's language where you first understand\n\n14:46.020 --> 14:48.860\n what's being written and then maybe even more complicated\n\n14:48.860 --> 14:51.260\n than that, having a natural dialogue.\n\n14:51.260 --> 14:53.620\n And then there's the game of go and chess.\n\n14:53.620 --> 14:57.540\n I would argue that language is much closer to go\n\n14:57.540 --> 14:59.700\n than it is to the physical world.\n\n14:59.700 --> 15:01.460\n Like it is still very constrained.\n\n15:01.460 --> 15:04.740\n When you say the possibility of the number of sentences\n\n15:04.740 --> 15:06.500\n that could come, it is huge,\n\n15:06.500 --> 15:09.260\n but it nevertheless is much more constrained.\n\n15:09.260 --> 15:12.740\n It feels maybe I'm wrong than the possibilities\n\n15:12.740 --> 15:14.540\n that the physical world brings us.\n\n15:14.540 --> 15:15.860\n There's something to what you say\n\n15:15.860 --> 15:17.700\n in some ways in which I disagree.\n\n15:17.700 --> 15:20.620\n So one interesting thing about language\n\n15:20.620 --> 15:23.340\n is that it abstracts away.\n\n15:23.340 --> 15:26.140\n This bottle, I don't know if it would be in the field of view\n\n15:26.140 --> 15:28.900\n is on this table and I use the word on here\n\n15:28.900 --> 15:32.980\n and I can use the word on here, maybe not here,\n\n15:32.980 --> 15:36.980\n but that one word encompasses in analog space\n\n15:36.980 --> 15:39.340\n sort of infinite number of possibilities.\n\n15:39.340 --> 15:43.060\n So there is a way in which language filters down\n\n15:43.060 --> 15:46.660\n the variation of the world and there's other ways.\n\n15:46.660 --> 15:49.900\n So we have a grammar and more or less\n\n15:49.900 --> 15:51.700\n you have to follow the rules of that grammar.\n\n15:51.700 --> 15:52.700\n You can break them a little bit,\n\n15:52.700 --> 15:55.420\n but by and large we follow the rules of grammar\n\n15:55.420 --> 15:57.020\n and so that's a constraint on language.\n\n15:57.020 --> 15:59.460\n So there are ways in which language is a constrained system.\n\n15:59.460 --> 16:02.300\n On the other hand, there are many arguments\n\n16:02.300 --> 16:04.740\n that say there's an infinite number of possible sentences\n\n16:04.740 --> 16:07.660\n and you can establish that by just stacking them up.\n\n16:07.660 --> 16:09.500\n So I think there's water on the table,\n\n16:09.500 --> 16:11.740\n you think that I think there's water on the table,\n\n16:11.740 --> 16:13.340\n your mother thinks that you think that I think\n\n16:13.340 --> 16:15.620\n that water's on the table, your brother thinks\n\n16:15.620 --> 16:17.300\n that maybe your mom is wrong to think\n\n16:17.300 --> 16:18.660\n that you think that I think, right?\n\n16:18.660 --> 16:21.980\n So we can make sentences of infinite length\n\n16:21.980 --> 16:23.580\n or we can stack up adjectives.\n\n16:23.580 --> 16:26.420\n This is a very silly example, a very, very silly example,\n\n16:26.420 --> 16:28.780\n a very, very, very, very, very, very silly example\n\n16:28.780 --> 16:29.620\n and so forth.\n\n16:29.620 --> 16:30.980\n So there are good arguments\n\n16:30.980 --> 16:32.420\n that there's an infinite range of sentences.\n\n16:32.420 --> 16:35.780\n In any case, it's vast by any reasonable measure\n\n16:35.780 --> 16:37.980\n and for example, almost anything in the physical world\n\n16:37.980 --> 16:40.460\n we can talk about in the language world\n\n16:40.460 --> 16:43.820\n and interestingly, many of the sentences that we understand,\n\n16:43.820 --> 16:46.820\n we can only understand if we have a very rich model\n\n16:46.820 --> 16:47.820\n of the physical world.\n\n16:47.820 --> 16:50.620\n So I don't ultimately want to adjudicate the debate\n\n16:50.620 --> 16:53.380\n that I think you just set up, but I find it interesting.\n\n16:54.420 --> 16:57.180\n Maybe the physical world is even more complicated\n\n16:57.180 --> 16:59.580\n than language, I think that's fair, but.\n\n16:59.580 --> 17:03.100\n Language is really, really complicated.\n\n17:03.100 --> 17:04.100\n It's really, really hard.\n\n17:04.100 --> 17:06.100\n Well, it's really, really hard for machines,\n\n17:06.100 --> 17:08.500\n for linguists, people trying to understand it.\n\n17:08.500 --> 17:09.660\n It's not that hard for children\n\n17:09.660 --> 17:12.100\n and that's part of what's driven my whole career.\n\n17:12.100 --> 17:14.340\n I was a student of Steven Pinker's\n\n17:14.340 --> 17:15.340\n and we were trying to figure out\n\n17:15.340 --> 17:18.700\n why kids could learn language when machines couldn't.\n\n17:18.700 --> 17:20.540\n I think we're gonna get into language,\n\n17:20.540 --> 17:22.460\n we're gonna get into communication intelligence\n\n17:22.460 --> 17:24.220\n and neural networks and so on,\n\n17:24.220 --> 17:28.860\n but let me return to the high level,\n\n17:28.860 --> 17:32.540\n the futuristic for a brief moment.\n\n17:32.540 --> 17:37.300\n So you've written in your book, in your new book,\n\n17:37.300 --> 17:39.940\n it would be arrogant to suppose that we could forecast\n\n17:39.940 --> 17:42.500\n where AI will be or the impact it will have\n\n17:42.500 --> 17:45.180\n in a thousand years or even 500 years.\n\n17:45.180 --> 17:47.080\n So let me ask you to be arrogant.\n\n17:48.340 --> 17:51.500\n What do AI systems with or without physical bodies\n\n17:51.500 --> 17:53.500\n look like 100 years from now?\n\n17:53.500 --> 17:56.820\n If you would just, you can't predict,\n\n17:56.820 --> 18:00.540\n but if you were to philosophize and imagine, do.\n\n18:00.540 --> 18:02.020\n Can I first justify the arrogance\n\n18:02.020 --> 18:04.100\n before you try to push me beyond it?\n\n18:04.100 --> 18:04.940\n Sure.\n\n18:05.940 --> 18:07.700\n I mean, there are examples like,\n\n18:07.700 --> 18:09.720\n people figured out how electricity worked,\n\n18:09.720 --> 18:13.060\n they had no idea that that was gonna lead to cell phones.\n\n18:13.060 --> 18:15.600\n I mean, things can move awfully fast\n\n18:15.600 --> 18:17.940\n once new technologies are perfected.\n\n18:17.940 --> 18:19.460\n Even when they made transistors,\n\n18:19.460 --> 18:21.100\n they weren't really thinking that cell phones\n\n18:21.100 --> 18:23.340\n would lead to social networking.\n\n18:23.340 --> 18:25.740\n There are nevertheless predictions of the future,\n\n18:25.740 --> 18:28.820\n which are statistically unlikely to come to be,\n\n18:28.820 --> 18:29.660\n but nevertheless is the best.\n\n18:29.660 --> 18:31.380\n You're asking me to be wrong.\n\n18:31.380 --> 18:32.220\n Asking you to be statistically.\n\n18:32.220 --> 18:34.020\n In which way would I like to be wrong?\n\n18:34.020 --> 18:37.500\n Pick the least unlikely to be wrong thing,\n\n18:37.500 --> 18:39.760\n even though it's most very likely to be wrong.\n\n18:39.760 --> 18:40.600\n I mean, here's some things\n\n18:40.600 --> 18:42.740\n that we can safely predict, I suppose.\n\n18:42.740 --> 18:46.260\n We can predict that AI will be faster than it is now.\n\n18:47.300 --> 18:49.520\n It will be cheaper than it is now.\n\n18:49.520 --> 18:52.880\n It will be better in the sense of being more general\n\n18:52.880 --> 18:55.760\n and applicable in more places.\n\n18:56.980 --> 18:58.340\n It will be pervasive.\n\n18:59.300 --> 19:01.620\n I mean, these are easy predictions.\n\n19:01.620 --> 19:03.320\n I'm sort of modeling them in my head\n\n19:03.320 --> 19:05.820\n on Jeff Bezos's famous predictions.\n\n19:05.820 --> 19:07.340\n He says, I can't predict the future,\n\n19:07.340 --> 19:09.820\n not in every way, I'm paraphrasing.\n\n19:09.820 --> 19:11.060\n But I can predict that people\n\n19:11.060 --> 19:13.220\n will never wanna pay more money for their stuff.\n\n19:13.220 --> 19:15.580\n They're never gonna want it to take longer to get there.\n\n19:15.580 --> 19:17.800\n So you can't predict everything,\n\n19:17.800 --> 19:18.880\n but you can predict something.\n\n19:18.880 --> 19:21.220\n Sure, of course it's gonna be faster and better.\n\n19:21.220 --> 19:24.500\n But what we can't really predict\n\n19:24.500 --> 19:28.700\n is the full scope of where AI will be in a certain period.\n\n19:28.700 --> 19:31.900\n I mean, I think it's safe to say that,\n\n19:31.900 --> 19:34.780\n although I'm very skeptical about current AI,\n\n19:35.660 --> 19:37.700\n that it's possible to do much better.\n\n19:37.700 --> 19:39.700\n You know, there's no in principled argument\n\n19:39.700 --> 19:42.100\n that says AI is an insolvable problem,\n\n19:42.100 --> 19:43.620\n that there's magic inside our brains\n\n19:43.620 --> 19:44.980\n that will never be captured.\n\n19:44.980 --> 19:46.780\n I mean, I've heard people make those kind of arguments.\n\n19:46.780 --> 19:48.980\n I don't think they're very good.\n\n19:48.980 --> 19:53.980\n So AI's gonna come, and probably 500 years\n\n19:54.100 --> 19:55.540\n is plenty to get there.\n\n19:55.540 --> 19:58.340\n And then once it's here, it really will change everything.\n\n19:59.260 --> 20:01.060\n So when you say AI's gonna come,\n\n20:01.060 --> 20:03.660\n are you talking about human level intelligence?\n\n20:03.660 --> 20:04.980\n So maybe I...\n\n20:04.980 --> 20:06.660\n I like the term general intelligence.\n\n20:06.660 --> 20:09.500\n So I don't think that the ultimate AI,\n\n20:09.500 --> 20:11.980\n if there is such a thing, is gonna look just like humans.\n\n20:11.980 --> 20:13.600\n I think it's gonna do some things\n\n20:13.600 --> 20:16.580\n that humans do better than current machines,\n\n20:16.580 --> 20:18.580\n like reason flexibly.\n\n20:18.580 --> 20:21.180\n And understand language and so forth.\n\n20:21.180 --> 20:23.460\n But it doesn't mean they have to be identical to humans.\n\n20:23.460 --> 20:25.980\n So for example, humans have terrible memory,\n\n20:25.980 --> 20:28.780\n and they suffer from what some people\n\n20:28.780 --> 20:29.920\n call motivated reasoning.\n\n20:29.920 --> 20:32.460\n So they like arguments that seem to support them,\n\n20:32.460 --> 20:35.460\n and they dismiss arguments that they don't like.\n\n20:35.460 --> 20:38.660\n There's no reason that a machine should ever do that.\n\n20:38.660 --> 20:42.280\n So you see that those limitations of memory\n\n20:42.280 --> 20:43.940\n as a bug, not a feature.\n\n20:43.940 --> 20:44.820\n Absolutely.\n\n20:44.820 --> 20:46.620\n I'll say two things about that.\n\n20:46.620 --> 20:48.660\n One is I was on a panel with Danny Kahneman,\n\n20:48.660 --> 20:50.300\n the Nobel Prize winner, last night,\n\n20:50.300 --> 20:51.760\n and we were talking about this stuff.\n\n20:51.760 --> 20:53.480\n And I think what we converged on\n\n20:53.480 --> 20:56.120\n is that humans are a low bar to exceed.\n\n20:56.120 --> 20:58.940\n They may be outside of our skill right now,\n\n20:58.940 --> 21:03.940\n but as AI programmers, but eventually AI will exceed it.\n\n21:04.300 --> 21:06.060\n So we're not talking about human level AI.\n\n21:06.060 --> 21:07.900\n We're talking about general intelligence\n\n21:07.900 --> 21:09.420\n that can do all kinds of different things\n\n21:09.420 --> 21:12.220\n and do it without some of the flaws that human beings have.\n\n21:12.220 --> 21:13.700\n The other thing I'll say is I wrote a whole book,\n\n21:13.700 --> 21:15.280\n actually, about the flaws of humans.\n\n21:15.280 --> 21:17.980\n It's actually a nice bookend to the,\n\n21:17.980 --> 21:19.180\n or counterpoint to the current book.\n\n21:19.180 --> 21:21.380\n So I wrote a book called Cluj,\n\n21:21.380 --> 21:24.020\n which was about the limits of the human mind.\n\n21:24.020 --> 21:26.380\n The current book is kind of about those few things\n\n21:26.380 --> 21:28.760\n that humans do a lot better than machines.\n\n21:28.760 --> 21:30.820\n Do you think it's possible that the flaws\n\n21:30.820 --> 21:33.260\n of the human mind, the limits of memory,\n\n21:33.260 --> 21:37.300\n our mortality, our bias,\n\n21:38.460 --> 21:40.300\n is a strength, not a weakness,\n\n21:40.300 --> 21:43.500\n that that is the thing that enables,\n\n21:43.500 --> 21:47.940\n from which motivation springs and meaning springs or not?\n\n21:47.940 --> 21:49.460\n I've heard a lot of arguments like this.\n\n21:49.460 --> 21:50.860\n I've never found them that convincing.\n\n21:50.860 --> 21:55.120\n I think that there's a lot of making lemonade out of lemons.\n\n21:55.120 --> 21:58.260\n So we, for example, do a lot of free association\n\n21:58.260 --> 22:00.780\n where one idea just leads to the next\n\n22:00.780 --> 22:02.540\n and they're not really that well connected.\n\n22:02.540 --> 22:04.500\n And we enjoy that and we make poetry out of it\n\n22:04.500 --> 22:07.100\n and we make kind of movies with free associations\n\n22:07.100 --> 22:08.140\n and it's fun and whatever.\n\n22:08.140 --> 22:12.300\n I don't think that's really a virtue of the system.\n\n22:12.300 --> 22:15.340\n I think that the limitations in human reasoning\n\n22:15.340 --> 22:16.580\n actually get us in a lot of trouble.\n\n22:16.580 --> 22:19.300\n Like, for example, politically we can't see eye to eye\n\n22:19.300 --> 22:21.780\n because we have the motivational reasoning I was talking\n\n22:21.780 --> 22:25.080\n about and something related called confirmation bias.\n\n22:25.080 --> 22:27.460\n So we have all of these problems that actually make\n\n22:27.460 --> 22:29.920\n for a rougher society because we can't get along\n\n22:29.920 --> 22:32.720\n because we can't interpret the data in shared ways.\n\n22:34.320 --> 22:36.460\n And then we do some nice stuff with that.\n\n22:36.460 --> 22:38.900\n So my free associations are different from yours\n\n22:38.900 --> 22:41.600\n and you're kind of amused by them and that's great.\n\n22:41.600 --> 22:42.620\n And hence poetry.\n\n22:42.620 --> 22:45.060\n So there are lots of ways in which we take\n\n22:45.060 --> 22:47.540\n a lousy situation and make it good.\n\n22:47.540 --> 22:50.580\n Another example would be our memories are terrible.\n\n22:50.580 --> 22:53.300\n So we play games like Concentration where you flip over\n\n22:53.300 --> 22:54.980\n two cards, try to find a pair.\n\n22:54.980 --> 22:56.480\n Can you imagine a computer playing that?\n\n22:56.480 --> 22:58.300\n Computer's like, this is the dullest game in the world.\n\n22:58.300 --> 22:59.940\n I know where all the cards are, I see it once,\n\n22:59.940 --> 23:02.580\n I know where it is, what are you even talking about?\n\n23:02.580 --> 23:07.040\n So we make a fun game out of having this terrible memory.\n\n23:07.040 --> 23:12.040\n So we are imperfect in discovering and optimizing\n\n23:12.220 --> 23:13.540\n some kind of utility function.\n\n23:13.540 --> 23:16.300\n But you think in general, there is a utility function.\n\n23:16.300 --> 23:18.860\n There's an objective function that's better than others.\n\n23:18.860 --> 23:20.340\n I didn't say that.\n\n23:20.340 --> 23:24.420\n But see, the presumption, when you say...\n\n23:24.420 --> 23:27.220\n I think you could design a better memory system.\n\n23:27.220 --> 23:29.900\n You could argue about utility functions\n\n23:29.900 --> 23:32.100\n and how you wanna think about that.\n\n23:32.100 --> 23:34.180\n But objectively, it would be really nice\n\n23:34.180 --> 23:36.500\n to do some of the following things.\n\n23:36.500 --> 23:39.980\n To get rid of memories that are no longer useful.\n\n23:41.140 --> 23:42.700\n Objectively, that would just be good.\n\n23:42.700 --> 23:43.580\n And we're not that good at it.\n\n23:43.580 --> 23:46.540\n So when you park in the same lot every day,\n\n23:46.540 --> 23:47.900\n you confuse where you parked today\n\n23:47.900 --> 23:48.860\n with where you parked yesterday\n\n23:48.860 --> 23:50.700\n with where you parked the day before and so forth.\n\n23:50.700 --> 23:52.620\n So you blur together a series of memories.\n\n23:52.620 --> 23:55.380\n There's just no way that that's optimal.\n\n23:55.380 --> 23:56.940\n I mean, I've heard all kinds of wacky arguments\n\n23:56.940 --> 23:58.140\n of people trying to defend that.\n\n23:58.140 --> 23:58.980\n But in the end of the day,\n\n23:58.980 --> 24:00.420\n I don't think any of them hold water.\n\n24:00.420 --> 24:01.260\n It's just above.\n\n24:01.260 --> 24:04.420\n Or memories of traumatic events would be possibly\n\n24:04.420 --> 24:06.780\n a very nice feature to have to get rid of those.\n\n24:06.780 --> 24:08.300\n It'd be great if you could just be like,\n\n24:08.300 --> 24:10.580\n I'm gonna wipe this sector.\n\n24:10.580 --> 24:12.020\n I'm done with that.\n\n24:12.020 --> 24:13.260\n I didn't have fun last night.\n\n24:13.260 --> 24:14.780\n I don't wanna think about it anymore.\n\n24:14.780 --> 24:15.820\n Whoop, bye bye.\n\n24:15.820 --> 24:16.660\n I'm gone.\n\n24:16.660 --> 24:17.740\n But we can't.\n\n24:17.740 --> 24:20.380\n Do you think it's possible to build a system...\n\n24:20.380 --> 24:23.780\n So you said human level intelligence is a weird concept, but...\n\n24:23.780 --> 24:25.420\n Well, I'm saying I prefer general intelligence.\n\n24:25.420 --> 24:26.260\n General intelligence.\n\n24:26.260 --> 24:28.140\n I mean, human level intelligence is a real thing.\n\n24:28.140 --> 24:29.820\n And you could try to make a machine\n\n24:29.820 --> 24:31.940\n that matches people or something like that.\n\n24:31.940 --> 24:34.220\n I'm saying that per se shouldn't be the objective,\n\n24:34.220 --> 24:37.220\n but rather that we should learn from humans\n\n24:37.220 --> 24:39.660\n the things they do well and incorporate that into our AI,\n\n24:39.660 --> 24:42.100\n just as we incorporate the things that machines do well\n\n24:42.100 --> 24:43.260\n that people do terribly.\n\n24:43.260 --> 24:45.780\n So, I mean, it's great that AI systems\n\n24:45.780 --> 24:48.340\n can do all this brute force computation that people can't.\n\n24:48.340 --> 24:50.820\n And one of the reasons I work on this stuff\n\n24:50.820 --> 24:53.300\n is because I would like to see machines solve problems\n\n24:53.300 --> 24:56.020\n that people can't, that combine the strength,\n\n24:56.020 --> 24:59.460\n or that in order to be solved would combine\n\n24:59.460 --> 25:02.220\n the strengths of machines to do all this computation\n\n25:02.220 --> 25:04.220\n with the ability, let's say, of people to read.\n\n25:04.220 --> 25:06.180\n So I'd like machines that can read\n\n25:06.180 --> 25:08.660\n the entire medical literature in a day.\n\n25:08.660 --> 25:10.780\n 7,000 new papers or whatever the numbers,\n\n25:10.780 --> 25:11.740\n comes out every day.\n\n25:11.740 --> 25:15.740\n There's no way for any doctor or whatever to read them all.\n\n25:15.740 --> 25:17.980\n A machine that could read would be a brilliant thing.\n\n25:17.980 --> 25:21.060\n And that would be strengths of brute force computation\n\n25:21.060 --> 25:24.300\n combined with kind of subtlety and understanding medicine\n\n25:24.300 --> 25:26.900\n that a good doctor or scientist has.\n\n25:26.900 --> 25:28.020\n So if we can linger a little bit\n\n25:28.020 --> 25:29.660\n on the idea of general intelligence.\n\n25:29.660 --> 25:32.860\n So Yann LeCun believes that human intelligence\n\n25:32.860 --> 25:35.580\n isn't general at all, it's very narrow.\n\n25:35.580 --> 25:36.700\n How do you think?\n\n25:36.700 --> 25:38.140\n I don't think that makes sense.\n\n25:38.140 --> 25:42.140\n We have lots of narrow intelligences for specific problems.\n\n25:42.140 --> 25:45.940\n But the fact is, like, anybody can walk into,\n\n25:45.940 --> 25:47.620\n let's say, a Hollywood movie,\n\n25:47.620 --> 25:49.140\n and reason about the content\n\n25:49.140 --> 25:51.700\n of almost anything that goes on there.\n\n25:51.700 --> 25:55.180\n So you can reason about what happens in a bank robbery,\n\n25:55.180 --> 25:58.620\n or what happens when someone is infertile\n\n25:58.620 --> 26:02.780\n and wants to go to IVF to try to have a child,\n\n26:02.780 --> 26:05.940\n or you can, the list is essentially endless.\n\n26:05.940 --> 26:09.580\n And not everybody understands every scene in the movie,\n\n26:09.580 --> 26:11.740\n but there's a huge range of things\n\n26:11.740 --> 26:15.060\n that pretty much any ordinary adult can understand.\n\n26:15.060 --> 26:18.220\n His argument is, is that actually,\n\n26:18.220 --> 26:20.700\n the set of things seems large for us humans\n\n26:20.700 --> 26:24.380\n because we're very limited in considering\n\n26:24.380 --> 26:27.340\n the kind of possibilities of experiences that are possible.\n\n26:27.340 --> 26:30.180\n But in fact, the amount of experience that are possible\n\n26:30.180 --> 26:32.500\n is infinitely larger.\n\n26:32.500 --> 26:35.140\n Well, I mean, if you wanna make an argument\n\n26:35.140 --> 26:38.780\n that humans are constrained in what they can understand,\n\n26:38.780 --> 26:40.940\n I have no issue with that.\n\n26:40.940 --> 26:41.780\n I think that's right.\n\n26:41.780 --> 26:44.460\n But it's still not the same thing at all\n\n26:44.460 --> 26:47.460\n as saying, here's a system that can play Go.\n\n26:47.460 --> 26:49.700\n It's been trained on five million games.\n\n26:49.700 --> 26:52.580\n And then I say, can it play on a rectangular board\n\n26:52.580 --> 26:53.700\n rather than a square board?\n\n26:53.700 --> 26:56.580\n And you say, well, if I retrain it from scratch\n\n26:56.580 --> 26:58.340\n on another five million games, it can.\n\n26:58.340 --> 27:01.140\n That's really, really narrow, and that's where we are.\n\n27:01.140 --> 27:05.140\n We don't have even a system that could play Go\n\n27:05.140 --> 27:07.100\n and then without further retraining,\n\n27:07.100 --> 27:08.700\n play on a rectangular board,\n\n27:08.700 --> 27:12.600\n which any human could do with very little problem.\n\n27:12.600 --> 27:14.860\n So that's what I mean by narrow.\n\n27:14.860 --> 27:16.900\n And so it's just wordplay to say.\n\n27:16.900 --> 27:18.060\n That is semantics, yeah.\n\n27:18.060 --> 27:19.300\n Then it's just words.\n\n27:19.300 --> 27:21.180\n Then yeah, you mean general in a sense\n\n27:21.180 --> 27:25.780\n that you can do all kinds of Go board shapes flexibly.\n\n27:25.780 --> 27:28.100\n Well, that would be like a first step\n\n27:28.100 --> 27:29.020\n in the right direction,\n\n27:29.020 --> 27:30.540\n but obviously that's not what it really meaning.\n\n27:30.540 --> 27:31.380\n You're kidding.\n\n27:32.380 --> 27:36.140\n What I mean by general is that you could transfer\n\n27:36.140 --> 27:38.940\n the knowledge you learn in one domain to another.\n\n27:38.940 --> 27:43.320\n So if you learn about bank robberies in movies\n\n27:43.320 --> 27:44.780\n and there's chase scenes,\n\n27:44.780 --> 27:47.740\n then you can understand that amazing scene in Breaking Bad\n\n27:47.740 --> 27:50.580\n when Walter White has a car chase scene\n\n27:50.580 --> 27:51.500\n with only one person.\n\n27:51.500 --> 27:52.620\n He's the only one in it.\n\n27:52.620 --> 27:55.540\n And you can reflect on how that car chase scene\n\n27:55.540 --> 27:58.060\n is like all the other car chase scenes you've ever seen\n\n27:58.060 --> 28:01.140\n and totally different and why that's cool.\n\n28:01.140 --> 28:03.100\n And the fact that the number of domains\n\n28:03.100 --> 28:04.540\n you can do that with is finite\n\n28:04.540 --> 28:05.760\n doesn't make it less general.\n\n28:05.760 --> 28:07.340\n So the idea of general is you could just do it\n\n28:07.340 --> 28:09.380\n on a lot of, don't transfer it across a lot of domains.\n\n28:09.380 --> 28:11.740\n Yeah, I mean, I'm not saying humans are infinitely general\n\n28:11.740 --> 28:12.960\n or that humans are perfect.\n\n28:12.960 --> 28:15.340\n I just said a minute ago, it's a low bar,\n\n28:15.340 --> 28:17.420\n but it's just, it's a low bar.\n\n28:17.420 --> 28:20.460\n But right now, like the bar is here and we're there\n\n28:20.460 --> 28:22.660\n and eventually we'll get way past it.\n\n28:22.660 --> 28:25.600\n So speaking of low bars,\n\n28:25.600 --> 28:27.420\n you've highlighted in your new book as well,\n\n28:27.420 --> 28:29.340\n but a couple of years ago wrote a paper\n\n28:29.340 --> 28:31.300\n titled Deep Learning, A Critical Appraisal\n\n28:31.300 --> 28:33.340\n that lists 10 challenges faced\n\n28:33.340 --> 28:36.020\n by current deep learning systems.\n\n28:36.020 --> 28:40.140\n So let me summarize them as data efficiency,\n\n28:40.140 --> 28:42.900\n transfer learning, hierarchical knowledge,\n\n28:42.900 --> 28:46.300\n open ended inference, explainability,\n\n28:46.300 --> 28:49.660\n integrating prior knowledge, cause of reasoning,\n\n28:49.660 --> 28:53.220\n modeling on a stable world, robustness, adversarial examples\n\n28:53.220 --> 28:54.140\n and so on.\n\n28:54.140 --> 28:56.900\n And then my favorite probably is reliability\n\n28:56.900 --> 28:59.140\n in the engineering of real world systems.\n\n28:59.140 --> 29:01.600\n So whatever people can read the paper,\n\n29:01.600 --> 29:02.940\n they should definitely read the paper,\n\n29:02.940 --> 29:04.320\n should definitely read your book.\n\n29:04.320 --> 29:08.140\n But which of these challenges is solved in your view\n\n29:08.140 --> 29:11.060\n has the biggest impact on the AI community?\n\n29:11.060 --> 29:12.620\n It's a very good question.\n\n29:13.940 --> 29:16.300\n And I'm gonna be evasive because I think that\n\n29:16.300 --> 29:17.980\n they go together a lot.\n\n29:17.980 --> 29:21.420\n So some of them might be solved independently of others,\n\n29:21.420 --> 29:23.700\n but I think a good solution to AI\n\n29:23.700 --> 29:25.460\n starts by having real,\n\n29:25.460 --> 29:28.420\n what I would call cognitive models of what's going on.\n\n29:28.420 --> 29:31.340\n So right now we have a approach that's dominant\n\n29:31.340 --> 29:33.920\n where you take statistical approximations of things,\n\n29:33.920 --> 29:35.740\n but you don't really understand them.\n\n29:35.740 --> 29:39.100\n So you know that bottles are correlated in your data\n\n29:39.100 --> 29:40.300\n with bottle caps,\n\n29:40.300 --> 29:42.220\n but you don't understand that there's a thread\n\n29:42.220 --> 29:45.300\n on the bottle cap that fits with the thread on the bottle\n\n29:45.300 --> 29:46.620\n and then that's what tightens it.\n\n29:46.620 --> 29:48.540\n If I tighten enough that there's a seal\n\n29:48.540 --> 29:49.660\n and the water won't come out.\n\n29:49.660 --> 29:51.980\n Like there's no machine that understands that.\n\n29:51.980 --> 29:53.820\n And having a good cognitive model\n\n29:53.820 --> 29:55.480\n of that kind of everyday phenomena\n\n29:55.480 --> 29:56.620\n is what we call common sense.\n\n29:56.620 --> 29:57.820\n And if you had that,\n\n29:57.820 --> 30:00.700\n then a lot of these other things start to fall\n\n30:00.700 --> 30:02.860\n into at least a little bit better place.\n\n30:02.860 --> 30:05.640\n Right now you're like learning correlations between pixels\n\n30:05.640 --> 30:07.660\n when you play a video game or something like that.\n\n30:07.660 --> 30:08.940\n And it doesn't work very well.\n\n30:08.940 --> 30:10.720\n It works when the video game is just the way\n\n30:10.720 --> 30:12.940\n that you studied it and then you alter the video game\n\n30:12.940 --> 30:13.760\n in small ways,\n\n30:13.760 --> 30:15.780\n like you move the paddle and break out a few pixels\n\n30:15.780 --> 30:17.460\n and the system falls apart.\n\n30:17.460 --> 30:19.020\n Because it doesn't understand,\n\n30:19.020 --> 30:20.900\n it doesn't have a representation of a paddle,\n\n30:20.900 --> 30:23.340\n a ball, a wall, a set of bricks and so forth.\n\n30:23.340 --> 30:26.440\n And so it's reasoning at the wrong level.\n\n30:26.440 --> 30:29.220\n So the idea of common sense,\n\n30:29.220 --> 30:30.220\n it's full of mystery,\n\n30:30.220 --> 30:31.060\n you've worked on it,\n\n30:31.060 --> 30:33.560\n but it's nevertheless full of mystery,\n\n30:33.560 --> 30:34.720\n full of promise.\n\n30:34.720 --> 30:36.540\n What does common sense mean?\n\n30:36.540 --> 30:38.020\n What does knowledge mean?\n\n30:38.020 --> 30:40.020\n So the way you've been discussing it now\n\n30:40.020 --> 30:40.940\n is very intuitive.\n\n30:40.940 --> 30:42.580\n It makes a lot of sense that that is something\n\n30:42.580 --> 30:43.700\n we should have and that's something\n\n30:43.700 --> 30:45.600\n deep learning systems don't have.\n\n30:45.600 --> 30:49.740\n But the argument could be that we're oversimplifying it\n\n30:49.740 --> 30:53.180\n because we're oversimplifying the notion of common sense\n\n30:53.180 --> 30:57.140\n because that's how it feels like we as humans\n\n30:57.140 --> 30:59.320\n at the cognitive level approach problems.\n\n30:59.320 --> 31:00.160\n So maybe.\n\n31:00.160 --> 31:03.320\n A lot of people aren't actually gonna read my book.\n\n31:03.320 --> 31:05.220\n But if they did read the book,\n\n31:05.220 --> 31:07.140\n one of the things that might come as a surprise to them\n\n31:07.140 --> 31:10.660\n is that we actually say common sense is really hard\n\n31:10.660 --> 31:11.640\n and really complicated.\n\n31:11.640 --> 31:13.020\n So they would probably,\n\n31:13.020 --> 31:15.140\n my critics know that I like common sense,\n\n31:15.140 --> 31:18.600\n but that chapter actually starts by us beating up\n\n31:18.600 --> 31:19.900\n not on deep learning,\n\n31:19.900 --> 31:21.960\n but kind of on our own home team as it will.\n\n31:21.960 --> 31:25.180\n So Ernie and I are first and foremost\n\n31:25.180 --> 31:26.780\n people that believe in at least some\n\n31:26.780 --> 31:28.700\n of what good old fashioned AI tried to do.\n\n31:28.700 --> 31:31.580\n So we believe in symbols and logic and programming.\n\n31:32.500 --> 31:33.740\n Things like that are important.\n\n31:33.740 --> 31:37.020\n And we go through why even those tools\n\n31:37.020 --> 31:39.560\n that we hold fairly dear aren't really enough.\n\n31:39.560 --> 31:42.660\n So we talk about why common sense is actually many things.\n\n31:42.660 --> 31:45.300\n And some of them fit really well with those\n\n31:45.300 --> 31:46.540\n classical sets of tools.\n\n31:46.540 --> 31:48.240\n So things like taxonomy.\n\n31:48.240 --> 31:51.460\n So I know that a bottle is an object\n\n31:51.460 --> 31:52.860\n or it's a vessel, let's say.\n\n31:52.860 --> 31:54.480\n And I know a vessel is an object\n\n31:54.480 --> 31:57.580\n and objects are material things in the physical world.\n\n31:57.580 --> 32:00.500\n So I can make some inferences.\n\n32:00.500 --> 32:05.500\n If I know that vessels need to not have holes in them,\n\n32:07.020 --> 32:09.540\n then I can infer that in order to carry their contents,\n\n32:09.540 --> 32:10.920\n then I can infer that a bottle\n\n32:10.920 --> 32:12.860\n shouldn't have a hole in it in order to carry its contents.\n\n32:12.860 --> 32:15.620\n So you can do hierarchical inference and so forth.\n\n32:15.620 --> 32:17.260\n And we say that's great,\n\n32:17.260 --> 32:21.100\n but it's only a tiny piece of what you need for common sense.\n\n32:21.100 --> 32:23.460\n We give lots of examples that don't fit into that.\n\n32:23.460 --> 32:26.500\n So another one that we talk about is a cheese grater.\n\n32:26.500 --> 32:28.040\n You've got holes in a cheese grater.\n\n32:28.040 --> 32:29.500\n You've got a handle on top.\n\n32:29.500 --> 32:33.380\n You can build a model in the game engine sense of a model\n\n32:33.380 --> 32:35.820\n so that you could have a little cartoon character\n\n32:35.820 --> 32:37.980\n flying around through the holes of the grater.\n\n32:37.980 --> 32:39.980\n But we don't have a system yet.\n\n32:39.980 --> 32:41.620\n Taxonomy doesn't help us that much\n\n32:41.620 --> 32:43.780\n that really understands why the handle is on top\n\n32:43.780 --> 32:45.240\n and what you do with the handle,\n\n32:45.240 --> 32:47.620\n or why all of those circles are sharp,\n\n32:47.620 --> 32:50.500\n or how you'd hold the cheese with respect to the grater\n\n32:50.500 --> 32:52.120\n in order to make it actually work.\n\n32:52.120 --> 32:55.020\n Do you think these ideas are just abstractions\n\n32:55.020 --> 32:57.140\n that could emerge on a system\n\n32:57.140 --> 32:59.920\n like a very large deep neural network?\n\n32:59.920 --> 33:03.140\n I'm a skeptic that that kind of emergence per se can work.\n\n33:03.140 --> 33:05.840\n So I think that deep learning might play a role\n\n33:05.840 --> 33:08.760\n in the systems that do what I want systems to do,\n\n33:08.760 --> 33:09.900\n but it won't do it by itself.\n\n33:09.900 --> 33:13.140\n I've never seen a deep learning system\n\n33:13.140 --> 33:15.900\n really extract an abstract concept.\n\n33:15.900 --> 33:18.820\n What they do, principled reasons for that\n\n33:18.820 --> 33:20.540\n stemming from how back propagation works,\n\n33:20.540 --> 33:22.920\n how the architectures are set up.\n\n33:22.920 --> 33:25.120\n One example is deep learning people\n\n33:25.120 --> 33:29.620\n actually all build in something called convolution,\n\n33:29.620 --> 33:33.180\n which Jan Lacune is famous for, which is an abstraction.\n\n33:33.180 --> 33:34.960\n They don't have their systems learn this.\n\n33:34.960 --> 33:37.740\n So the abstraction is an object that looks the same\n\n33:37.740 --> 33:39.220\n if it appears in different places.\n\n33:39.220 --> 33:41.940\n And what Lacune figured out and why,\n\n33:41.940 --> 33:44.300\n essentially why he was a co winner of the Turing Award\n\n33:44.300 --> 33:47.620\n was that if you programmed this in innately,\n\n33:47.620 --> 33:50.680\n then your system would be a whole lot more efficient.\n\n33:50.680 --> 33:53.220\n In principle, this should be learnable,\n\n33:53.220 --> 33:56.220\n but people don't have systems that kind of reify things\n\n33:56.220 --> 33:58.000\n and make them more abstract.\n\n33:58.000 --> 34:00.420\n And so what you'd really wind up with\n\n34:00.420 --> 34:02.700\n if you don't program that in advance is a system\n\n34:02.700 --> 34:05.460\n that kind of realizes that this is the same thing as this,\n\n34:05.460 --> 34:06.980\n but then I take your little clock there\n\n34:06.980 --> 34:08.380\n and I move it over and it doesn't realize\n\n34:08.380 --> 34:10.460\n that the same thing applies to the clock.\n\n34:10.460 --> 34:12.680\n So the really nice thing, you're right,\n\n34:12.680 --> 34:14.760\n that convolution is just one of the things\n\n34:14.760 --> 34:17.160\n that's like, it's an innate feature\n\n34:17.160 --> 34:19.460\n that's programmed by the human expert.\n\n34:19.460 --> 34:21.260\n We need more of those, not less.\n\n34:21.260 --> 34:24.420\n Yes, but the nice feature is it feels like\n\n34:24.420 --> 34:28.200\n that requires coming up with that brilliant idea,\n\n34:28.200 --> 34:29.780\n can get you a Turing Award,\n\n34:29.780 --> 34:34.780\n but it requires less effort than encoding\n\n34:34.780 --> 34:36.620\n and something we'll talk about, the expert system.\n\n34:36.620 --> 34:40.020\n So encoding a lot of knowledge by hand.\n\n34:40.020 --> 34:43.500\n So it feels like there's a huge amount of limitations\n\n34:43.500 --> 34:46.500\n which you clearly outline with deep learning,\n\n34:46.500 --> 34:47.820\n but the nice feature of deep learning,\n\n34:47.820 --> 34:49.600\n whatever it is able to accomplish,\n\n34:49.600 --> 34:53.500\n it does a lot of stuff automatically\n\n34:53.500 --> 34:54.900\n without human intervention.\n\n34:54.900 --> 34:57.100\n Well, and that's part of why people love it, right?\n\n34:57.100 --> 34:59.820\n But I always think of this quote from Bertrand Russell,\n\n34:59.820 --> 35:02.740\n which is it has all the advantages\n\n35:02.740 --> 35:04.420\n of theft over honest toil.\n\n35:04.420 --> 35:08.140\n It's really hard to program into a machine\n\n35:08.140 --> 35:11.300\n a notion of causality or even how a bottle works\n\n35:11.300 --> 35:12.640\n or what containers are.\n\n35:12.640 --> 35:14.260\n Ernie Davis and I wrote a, I don't know,\n\n35:14.260 --> 35:17.980\n 45 page academic paper trying just to understand\n\n35:17.980 --> 35:18.980\n what a container is,\n\n35:18.980 --> 35:21.100\n which I don't think anybody ever read the paper,\n\n35:21.100 --> 35:25.260\n but it's a very detailed analysis of all the things,\n\n35:25.260 --> 35:26.100\n well, not even all of it,\n\n35:26.100 --> 35:27.140\n some of the things you need to do\n\n35:27.140 --> 35:28.580\n in order to understand a container.\n\n35:28.580 --> 35:30.060\n It would be a whole lot nice,\n\n35:30.060 --> 35:32.200\n and I'm a coauthor on the paper,\n\n35:32.200 --> 35:33.180\n I made it a little bit better,\n\n35:33.180 --> 35:36.620\n but Ernie did the hard work for that particular paper.\n\n35:36.620 --> 35:38.060\n And it took him like three months\n\n35:38.060 --> 35:40.660\n to get the logical statements correct.\n\n35:40.660 --> 35:42.860\n And maybe that's not the right way to do it,\n\n35:42.860 --> 35:44.100\n it's a way to do it.\n\n35:44.100 --> 35:46.140\n But on that way of doing it,\n\n35:46.140 --> 35:48.440\n it's really hard work to do something\n\n35:48.440 --> 35:50.220\n as simple as understanding containers.\n\n35:50.220 --> 35:52.820\n And nobody wants to do that hard work,\n\n35:52.820 --> 35:55.600\n even Ernie didn't want to do that hard work.\n\n35:55.600 --> 35:58.380\n Everybody would rather just like feed their system in\n\n35:58.380 --> 36:00.340\n with a bunch of videos with a bunch of containers\n\n36:00.340 --> 36:03.820\n and have the systems infer how containers work.\n\n36:03.820 --> 36:05.420\n It would be like so much less effort,\n\n36:05.420 --> 36:06.820\n let the machine do the work.\n\n36:06.820 --> 36:08.220\n And so I understand the impulse,\n\n36:08.220 --> 36:10.220\n I understand why people want to do that.\n\n36:10.220 --> 36:11.860\n I just don't think that it works.\n\n36:11.860 --> 36:14.580\n I've never seen anybody build a system\n\n36:14.580 --> 36:18.700\n that in a robust way can actually watch videos\n\n36:18.700 --> 36:21.300\n and predict exactly which containers would leak\n\n36:21.300 --> 36:23.540\n and which ones wouldn't or something like,\n\n36:23.540 --> 36:25.060\n and I know someone's gonna go out and do that\n\n36:25.060 --> 36:28.100\n since I said it, and I look forward to seeing it.\n\n36:28.100 --> 36:30.540\n But getting these things to work robustly\n\n36:30.540 --> 36:32.900\n is really, really hard.\n\n36:32.900 --> 36:37.740\n So Yann LeCun, who was my colleague at NYU for many years,\n\n36:37.740 --> 36:40.760\n thinks that the hard work should go into defining\n\n36:40.760 --> 36:43.180\n an unsupervised learning algorithm\n\n36:43.180 --> 36:46.680\n that will watch videos, use the next frame basically\n\n36:46.680 --> 36:48.540\n in order to tell it what's going on.\n\n36:48.540 --> 36:49.940\n And he thinks that's the Royal road\n\n36:49.940 --> 36:51.260\n and he's willing to put in the work\n\n36:51.260 --> 36:53.300\n in devising that algorithm.\n\n36:53.300 --> 36:55.580\n Then he wants the machine to do the rest.\n\n36:55.580 --> 36:57.820\n And again, I understand the impulse.\n\n36:57.820 --> 37:01.700\n My intuition, based on years of watching this stuff\n\n37:01.700 --> 37:03.940\n and making predictions 20 years ago that still hold\n\n37:03.940 --> 37:06.500\n even though there's a lot more computation and so forth,\n\n37:06.500 --> 37:07.460\n is that we actually have to do\n\n37:07.460 --> 37:08.520\n a different kind of hard work,\n\n37:08.520 --> 37:11.320\n which is more like building a design specification\n\n37:11.320 --> 37:13.100\n for what we want the system to do,\n\n37:13.100 --> 37:15.060\n doing hard engineering work to figure out\n\n37:15.060 --> 37:18.420\n how we do things like what Yann did for convolution\n\n37:18.420 --> 37:21.660\n in order to figure out how to encode complex knowledge\n\n37:21.660 --> 37:22.620\n into the systems.\n\n37:22.620 --> 37:25.340\n The current systems don't have that much knowledge\n\n37:25.340 --> 37:27.580\n other than convolution, which is again,\n\n37:27.580 --> 37:30.500\n this objects being in different places\n\n37:30.500 --> 37:33.240\n and having the same perception, I guess I'll say.\n\n37:34.460 --> 37:35.300\n Same appearance.\n\n37:36.740 --> 37:38.260\n People don't want to do that work.\n\n37:38.260 --> 37:41.580\n They don't see how to naturally fit one with the other.\n\n37:41.580 --> 37:43.300\n I think that's, yes, absolutely.\n\n37:43.300 --> 37:45.540\n But also on the expert system side,\n\n37:45.540 --> 37:47.620\n there's a temptation to go too far the other way.\n\n37:47.620 --> 37:49.860\n So we're just having an expert sort of sit down\n\n37:49.860 --> 37:51.940\n and encode the description,\n\n37:51.940 --> 37:54.060\n the framework for what a container is,\n\n37:54.060 --> 37:56.540\n and then having the system reason the rest.\n\n37:56.540 --> 37:59.260\n From my view, one really exciting possibility\n\n37:59.260 --> 38:02.180\n is of active learning where it's continuous interaction\n\n38:02.180 --> 38:04.080\n between a human and machine.\n\n38:04.080 --> 38:07.060\n As the machine, there's kind of deep learning type\n\n38:07.060 --> 38:10.120\n extraction of information from data patterns and so on,\n\n38:10.120 --> 38:14.660\n but humans also guiding the learning procedures,\n\n38:14.660 --> 38:19.660\n guiding both the process and the framework\n\n38:19.940 --> 38:22.100\n of how the machine learns, whatever the task is.\n\n38:22.100 --> 38:24.100\n I was with you with almost everything you said\n\n38:24.100 --> 38:26.460\n except the phrase deep learning.\n\n38:26.460 --> 38:28.180\n What I think you really want there\n\n38:28.180 --> 38:30.500\n is a new form of machine learning.\n\n38:30.500 --> 38:32.980\n So let's remember, deep learning is a particular way\n\n38:32.980 --> 38:33.980\n of doing machine learning.\n\n38:33.980 --> 38:36.980\n Most often it's done with supervised data\n\n38:36.980 --> 38:38.820\n for perceptual categories.\n\n38:38.820 --> 38:41.780\n There are other things you can do with deep learning,\n\n38:41.780 --> 38:42.740\n some of them quite technical,\n\n38:42.740 --> 38:44.600\n but the standard use of deep learning\n\n38:44.600 --> 38:47.600\n is I have a lot of examples and I have labels for them.\n\n38:47.600 --> 38:48.820\n So here are pictures.\n\n38:48.820 --> 38:50.380\n This one's the Eiffel Tower.\n\n38:50.380 --> 38:51.660\n This one's the Sears Tower.\n\n38:51.660 --> 38:53.320\n This one's the Empire State Building.\n\n38:53.320 --> 38:54.160\n This one's a cat.\n\n38:54.160 --> 38:55.180\n This one's a pig and so forth.\n\n38:55.180 --> 38:58.900\n You just get millions of examples, millions of labels,\n\n38:58.900 --> 39:01.220\n and deep learning is extremely good at that.\n\n39:01.220 --> 39:04.460\n It's better than any other solution that anybody has devised,\n\n39:04.460 --> 39:07.380\n but it is not good at representing abstract knowledge.\n\n39:07.380 --> 39:09.380\n It's not good at representing things\n\n39:09.380 --> 39:13.980\n like bottles contain liquid and have tops to them\n\n39:13.980 --> 39:14.820\n and so forth.\n\n39:14.820 --> 39:15.860\n It's not very good at learning\n\n39:15.860 --> 39:17.860\n or representing that kind of knowledge.\n\n39:17.860 --> 39:21.300\n It is an example of having a machine learn something,\n\n39:21.300 --> 39:23.900\n but it's a machine that learns a particular kind of thing,\n\n39:23.900 --> 39:25.540\n which is object classification.\n\n39:25.540 --> 39:28.580\n It's not a particularly good algorithm for learning\n\n39:28.580 --> 39:30.780\n about the abstractions that govern our world.\n\n39:30.780 --> 39:33.080\n There may be such a thing.\n\n39:33.080 --> 39:34.300\n Part of what we counsel in the book\n\n39:34.300 --> 39:36.980\n is maybe people should be working on devising such things.\n\n39:36.980 --> 39:40.580\n So one possibility, just I wonder what you think about it,\n\n39:40.580 --> 39:45.180\n is that deep neural networks do form abstractions,\n\n39:45.180 --> 39:48.500\n but they're not accessible to us humans\n\n39:48.500 --> 39:49.340\n in terms of we can't.\n\n39:49.340 --> 39:50.780\n There's some truth in that.\n\n39:50.780 --> 39:54.180\n So is it possible that either current or future\n\n39:54.180 --> 39:56.520\n neural networks form very high level abstractions,\n\n39:56.520 --> 40:01.520\n which are as powerful as our human abstractions\n\n40:01.820 --> 40:02.660\n of common sense.\n\n40:02.660 --> 40:04.900\n We just can't get a hold of them.\n\n40:04.900 --> 40:06.620\n And so the problem is essentially\n\n40:06.620 --> 40:09.220\n we need to make them explainable.\n\n40:09.220 --> 40:10.640\n This is an astute question,\n\n40:10.640 --> 40:13.080\n but I think the answer is at least partly no.\n\n40:13.080 --> 40:16.060\n One of the kinds of classical neural network architecture\n\n40:16.060 --> 40:17.620\n is what we call an auto associator.\n\n40:17.620 --> 40:20.140\n It just tries to take an input,\n\n40:20.140 --> 40:21.500\n goes through a set of hidden layers,\n\n40:21.500 --> 40:23.040\n and comes out with an output.\n\n40:23.040 --> 40:24.420\n And it's supposed to learn essentially\n\n40:24.420 --> 40:25.460\n the identity function,\n\n40:25.460 --> 40:27.260\n that your input is the same as your output.\n\n40:27.260 --> 40:28.460\n So you think of it as binary numbers.\n\n40:28.460 --> 40:30.660\n You've got the one, the two, the four, the eight,\n\n40:30.660 --> 40:32.180\n the 16, and so forth.\n\n40:32.180 --> 40:33.940\n And so if you want to input 24,\n\n40:33.940 --> 40:35.860\n you turn on the 16, you turn on the eight.\n\n40:35.860 --> 40:38.940\n It's like binary one, one, and a bunch of zeros.\n\n40:38.940 --> 40:41.620\n So I did some experiments in 1998\n\n40:41.620 --> 40:46.620\n with the precursors of contemporary deep learning.\n\n40:46.620 --> 40:50.460\n And what I showed was you could train these networks\n\n40:50.460 --> 40:52.060\n on all the even numbers,\n\n40:52.060 --> 40:54.620\n and they would never generalize to the odd number.\n\n40:54.620 --> 40:56.700\n A lot of people thought that I was, I don't know,\n\n40:56.700 --> 40:58.460\n an idiot or faking the experiment,\n\n40:58.460 --> 41:00.100\n or it wasn't true or whatever.\n\n41:00.100 --> 41:03.260\n But it is true that with this class of networks\n\n41:03.260 --> 41:04.860\n that we had in that day,\n\n41:04.860 --> 41:07.140\n that they would never ever make this generalization.\n\n41:07.140 --> 41:09.660\n And it's not that the networks were stupid,\n\n41:09.660 --> 41:13.380\n it's that they see the world in a different way than we do.\n\n41:13.380 --> 41:14.720\n They were basically concerned,\n\n41:14.720 --> 41:18.580\n what is the probability that the rightmost output node\n\n41:18.580 --> 41:19.980\n is going to be one?\n\n41:19.980 --> 41:21.220\n And as far as they were concerned,\n\n41:21.220 --> 41:24.420\n in everything they'd ever been trained on, it was a zero.\n\n41:24.420 --> 41:27.020\n That node had never been turned on,\n\n41:27.020 --> 41:28.960\n and so they figured, why turn it on now?\n\n41:28.960 --> 41:30.940\n Whereas a person would look at the same problem and say,\n\n41:30.940 --> 41:31.780\n well, it's obvious,\n\n41:31.780 --> 41:33.780\n we're just doing the thing that corresponds.\n\n41:33.780 --> 41:35.500\n The Latin for it is mutatis mutandis,\n\n41:35.500 --> 41:38.220\n we'll change what needs to be changed.\n\n41:38.220 --> 41:40.500\n And we do this, this is what algebra is.\n\n41:40.500 --> 41:43.840\n So I can do f of x equals y plus two,\n\n41:43.840 --> 41:45.380\n and I can do it for a couple of values,\n\n41:45.380 --> 41:46.500\n I can tell you if y is three,\n\n41:46.500 --> 41:49.140\n then x is five, and if y is four, x is six.\n\n41:49.140 --> 41:50.980\n And now I can do it with some totally different number,\n\n41:50.980 --> 41:51.980\n like a million, then you can say,\n\n41:51.980 --> 41:53.140\n well, obviously it's a million and two,\n\n41:53.140 --> 41:55.620\n because you have an algebraic operation\n\n41:55.620 --> 41:57.460\n that you're applying to a variable.\n\n41:57.460 --> 42:00.620\n And deep learning systems kind of emulate that,\n\n42:00.620 --> 42:02.500\n but they don't actually do it.\n\n42:02.500 --> 42:04.140\n The particular example,\n\n42:04.140 --> 42:08.140\n you could fudge a solution to that particular problem.\n\n42:08.140 --> 42:10.500\n The general form of that problem remains,\n\n42:10.500 --> 42:12.400\n that what they learn is really correlations\n\n42:12.400 --> 42:14.180\n between different input and output nodes.\n\n42:14.180 --> 42:16.140\n And they're complex correlations\n\n42:16.140 --> 42:18.780\n with multiple nodes involved and so forth.\n\n42:18.780 --> 42:20.260\n Ultimately, they're correlative,\n\n42:20.260 --> 42:23.060\n they're not structured over these operations over variables.\n\n42:23.060 --> 42:25.960\n Now, someday, people may do a new form of deep learning\n\n42:25.960 --> 42:27.300\n that incorporates that stuff,\n\n42:27.300 --> 42:28.460\n and I think it will help a lot.\n\n42:28.460 --> 42:30.260\n And there's some tentative work on things\n\n42:30.260 --> 42:32.180\n like differentiable programming right now\n\n42:32.180 --> 42:34.240\n that fall into that category.\n\n42:34.240 --> 42:35.500\n But the sort of classic stuff\n\n42:35.500 --> 42:38.780\n like people use for ImageNet doesn't have it.\n\n42:38.780 --> 42:41.060\n And you have people like Hinton going around saying,\n\n42:41.060 --> 42:42.860\n symbol manipulation, like what Marcus,\n\n42:42.860 --> 42:45.680\n what I advocate is like the gasoline engine.\n\n42:45.680 --> 42:46.520\n It's obsolete.\n\n42:46.520 --> 42:48.820\n We should just use this cool electric power\n\n42:48.820 --> 42:50.320\n that we've got with the deep learning.\n\n42:50.320 --> 42:51.980\n And that's really destructive,\n\n42:51.980 --> 42:55.900\n because we really do need to have the gasoline engine stuff\n\n42:55.900 --> 42:59.580\n that represents, I mean, I don't think it's a good analogy,\n\n42:59.580 --> 43:02.180\n but we really do need to have the stuff\n\n43:02.180 --> 43:03.660\n that represents symbols.\n\n43:03.660 --> 43:06.200\n Yeah, and Hinton as well would say\n\n43:06.200 --> 43:08.960\n that we do need to throw out everything and start over.\n\n43:08.960 --> 43:12.820\n Hinton said that to Axios,\n\n43:12.820 --> 43:15.540\n and I had a friend who interviewed him\n\n43:15.540 --> 43:16.460\n and tried to pin him down\n\n43:16.460 --> 43:17.820\n on what exactly we need to throw out,\n\n43:17.820 --> 43:19.900\n and he was very evasive.\n\n43:19.900 --> 43:22.700\n Well, of course, because we can't, if he knew.\n\n43:22.700 --> 43:23.940\n Then he'd throw it out himself.\n\n43:23.940 --> 43:25.400\n But I mean, you can't have it both ways.\n\n43:25.400 --> 43:27.520\n You can't be like, I don't know what to throw out,\n\n43:27.520 --> 43:29.980\n but I am gonna throw out the symbols.\n\n43:29.980 --> 43:32.140\n I mean, and not just the symbols,\n\n43:32.140 --> 43:34.100\n but the variables and the operations over variables.\n\n43:34.100 --> 43:36.140\n Don't forget, the operations over variables,\n\n43:36.140 --> 43:37.740\n the stuff that I'm endorsing\n\n43:37.740 --> 43:41.500\n and which John McCarthy did when he founded AI,\n\n43:41.500 --> 43:42.660\n that stuff is the stuff\n\n43:42.660 --> 43:44.180\n that we build most computers out of.\n\n43:44.180 --> 43:45.460\n There are people now who say,\n\n43:45.460 --> 43:48.780\n we don't need computer programmers anymore.\n\n43:48.780 --> 43:50.240\n Not quite looking at the statistics\n\n43:50.240 --> 43:51.180\n of how much computer programmers\n\n43:51.180 --> 43:52.980\n actually get paid right now.\n\n43:52.980 --> 43:54.380\n We need lots of computer programmers,\n\n43:54.380 --> 43:57.780\n and most of them, they do a little bit of machine learning,\n\n43:57.780 --> 43:59.900\n but they still do a lot of code, right?\n\n43:59.900 --> 44:02.220\n Code where it's like, if the value of X\n\n44:02.220 --> 44:03.580\n is greater than the value of Y,\n\n44:03.580 --> 44:04.500\n then do this kind of thing,\n\n44:04.500 --> 44:08.100\n like conditionals and comparing operations over variables.\n\n44:08.100 --> 44:10.220\n Like, there's this fantasy you can machine learn anything.\n\n44:10.220 --> 44:12.580\n There's some things you would never wanna machine learn.\n\n44:12.580 --> 44:14.980\n I would not use a phone operating system\n\n44:14.980 --> 44:16.100\n that was machine learned.\n\n44:16.100 --> 44:17.820\n Like, you made a bunch of phone calls\n\n44:17.820 --> 44:19.740\n and you recorded which packets were transmitted\n\n44:19.740 --> 44:22.500\n and you just machine learned it, it'd be insane.\n\n44:22.500 --> 44:27.500\n Or to build a web browser by taking logs of keystrokes\n\n44:27.500 --> 44:29.420\n and images, screenshots,\n\n44:29.420 --> 44:31.500\n and then trying to learn the relation between them.\n\n44:31.500 --> 44:32.860\n Nobody would ever,\n\n44:32.860 --> 44:35.100\n no rational person would ever try to build a browser\n\n44:35.100 --> 44:37.460\n that made, they would use symbol manipulation,\n\n44:37.460 --> 44:40.140\n the stuff that I think AI needs to avail itself of\n\n44:40.140 --> 44:42.140\n in addition to deep learning.\n\n44:42.140 --> 44:46.540\n Can you describe your view of symbol manipulation\n\n44:46.540 --> 44:47.920\n in its early days?\n\n44:47.920 --> 44:49.540\n Can you describe expert systems\n\n44:49.540 --> 44:52.540\n and where do you think they hit a wall\n\n44:52.540 --> 44:53.940\n or a set of challenges?\n\n44:53.940 --> 44:56.580\n Sure, so I mean, first I just wanna clarify,\n\n44:56.580 --> 44:58.940\n I'm not endorsing expert systems per se.\n\n44:58.940 --> 45:00.760\n You've been kind of contrasting them.\n\n45:00.760 --> 45:01.600\n There is a contrast,\n\n45:01.600 --> 45:04.220\n but that's not the thing that I'm endorsing.\n\n45:04.220 --> 45:06.500\n So expert systems tried to capture things\n\n45:06.500 --> 45:09.460\n like medical knowledge with a large set of rules.\n\n45:09.460 --> 45:12.860\n So if the patient has this symptom and this other symptom,\n\n45:12.860 --> 45:15.700\n then it is likely that they have this disease.\n\n45:15.700 --> 45:16.860\n So there are logical rules\n\n45:16.860 --> 45:18.340\n and they were symbol manipulating rules\n\n45:18.340 --> 45:20.980\n of just the sort that I'm talking about.\n\n45:20.980 --> 45:21.820\n And the problem.\n\n45:21.820 --> 45:24.980\n They encode a set of knowledge that the experts then put in.\n\n45:24.980 --> 45:26.260\n And very explicitly so.\n\n45:26.260 --> 45:28.780\n So you'd have somebody interview an expert\n\n45:28.780 --> 45:31.940\n and then try to turn that stuff into rules.\n\n45:31.940 --> 45:33.980\n And at some level I'm arguing for rules.\n\n45:33.980 --> 45:37.700\n But the difference is those guys did in the 80s\n\n45:37.700 --> 45:40.040\n was almost entirely rules,\n\n45:40.040 --> 45:42.980\n almost entirely handwritten with no machine learning.\n\n45:42.980 --> 45:44.340\n What a lot of people are doing now\n\n45:44.340 --> 45:47.340\n is almost entirely one species of machine learning\n\n45:47.340 --> 45:48.260\n with no rules.\n\n45:48.260 --> 45:50.380\n And what I'm counseling is actually a hybrid.\n\n45:50.380 --> 45:52.900\n I'm saying that both of these things have their advantage.\n\n45:52.900 --> 45:55.300\n So if you're talking about perceptual classification,\n\n45:55.300 --> 45:57.140\n how do I recognize a bottle?\n\n45:57.140 --> 45:59.540\n Deep learning is the best tool we've got right now.\n\n45:59.540 --> 46:00.940\n If you're talking about making inferences\n\n46:00.940 --> 46:02.420\n about what a bottle does,\n\n46:02.420 --> 46:04.140\n something closer to the expert systems\n\n46:04.140 --> 46:07.340\n is probably still the best available alternative.\n\n46:07.340 --> 46:09.860\n And probably we want something that is better able\n\n46:09.860 --> 46:12.620\n to handle quantitative and statistical information\n\n46:12.620 --> 46:14.940\n than those classical systems typically were.\n\n46:14.940 --> 46:16.980\n So we need new technologies\n\n46:16.980 --> 46:18.620\n that are gonna draw some of the strengths\n\n46:18.620 --> 46:21.060\n of both the expert systems and the deep learning,\n\n46:21.060 --> 46:23.260\n but are gonna find new ways to synthesize them.\n\n46:23.260 --> 46:27.740\n How hard do you think it is to add knowledge at the low level?\n\n46:27.740 --> 46:32.140\n So mine human intellects to add extra information\n\n46:32.140 --> 46:36.540\n to symbol manipulating systems?\n\n46:36.540 --> 46:37.840\n In some domains it's not that hard,\n\n46:37.840 --> 46:40.100\n but it's often really hard.\n\n46:40.100 --> 46:44.120\n Partly because a lot of the things that are important,\n\n46:44.120 --> 46:46.060\n people wouldn't bother to tell you.\n\n46:46.060 --> 46:49.680\n So if you pay someone on Amazon Mechanical Turk\n\n46:49.680 --> 46:52.060\n to tell you stuff about bottles,\n\n46:52.060 --> 46:55.060\n they probably won't even bother to tell you\n\n46:55.060 --> 46:57.020\n some of the basic level stuff\n\n46:57.020 --> 46:59.180\n that's just so obvious to a human being\n\n46:59.180 --> 47:02.140\n and yet so hard to capture in machines.\n\n47:04.580 --> 47:06.540\n They're gonna tell you more exotic things,\n\n47:06.540 --> 47:08.940\n and they're all well and good,\n\n47:08.940 --> 47:12.460\n but they're not getting to the root of the problem.\n\n47:12.460 --> 47:16.540\n So untutored humans aren't very good at knowing,\n\n47:16.540 --> 47:18.340\n and why should they be,\n\n47:18.340 --> 47:22.260\n what kind of knowledge the computer system developers\n\n47:22.260 --> 47:23.460\n actually need?\n\n47:23.460 --> 47:26.620\n I don't think that that's an irremediable problem.\n\n47:26.620 --> 47:28.620\n I think it's historically been a problem.\n\n47:28.620 --> 47:31.080\n People have had crowdsourcing efforts,\n\n47:31.080 --> 47:32.060\n and they don't work that well.\n\n47:32.060 --> 47:35.300\n There's one at MIT, we're recording this at MIT,\n\n47:35.300 --> 47:37.500\n called Virtual Home, where,\n\n47:37.500 --> 47:39.540\n and we talk about this in the book,\n\n47:39.540 --> 47:40.740\n find the exact example there,\n\n47:40.740 --> 47:42.800\n but people were asked to do things\n\n47:42.800 --> 47:44.880\n like describe an exercise routine.\n\n47:44.880 --> 47:47.460\n And the things that the people describe\n\n47:47.460 --> 47:48.580\n are at a very low level\n\n47:48.580 --> 47:50.100\n and don't really capture what's going on.\n\n47:50.100 --> 47:52.340\n So they're like, go to the room\n\n47:52.340 --> 47:54.700\n with the television and the weights,\n\n47:54.700 --> 47:56.100\n turn on the television,\n\n47:56.100 --> 47:59.020\n press the remote to turn on the television,\n\n47:59.020 --> 48:01.440\n lift weight, put weight down, whatever.\n\n48:01.440 --> 48:03.620\n It's like very micro level,\n\n48:03.620 --> 48:04.900\n and it's not telling you\n\n48:04.900 --> 48:06.860\n what an exercise routine is really about,\n\n48:06.860 --> 48:09.860\n which is like, I wanna fit a certain number of exercises\n\n48:09.860 --> 48:10.940\n in a certain time period,\n\n48:10.940 --> 48:12.700\n I wanna emphasize these muscles.\n\n48:12.700 --> 48:15.060\n You want some kind of abstract description.\n\n48:15.060 --> 48:17.260\n The fact that you happen to press the remote control\n\n48:17.260 --> 48:20.020\n in this room when you watch this television\n\n48:20.020 --> 48:23.060\n isn't really the essence of the exercise routine.\n\n48:23.060 --> 48:24.780\n But if you just ask people like, what did they do?\n\n48:24.780 --> 48:26.980\n Then they give you this fine grain.\n\n48:26.980 --> 48:29.780\n And so it takes a level of expertise\n\n48:29.780 --> 48:31.900\n about how the AI works\n\n48:31.900 --> 48:34.540\n in order to craft the right kind of knowledge.\n\n48:34.540 --> 48:37.580\n So there's this ocean of knowledge that we all operate on.\n\n48:37.580 --> 48:39.340\n Some of them may not even be conscious,\n\n48:39.340 --> 48:43.300\n or at least we're not able to communicate it effectively.\n\n48:43.300 --> 48:45.700\n Yeah, most of it we would recognize if somebody said it,\n\n48:45.700 --> 48:47.420\n if it was true or not,\n\n48:47.420 --> 48:49.660\n but we wouldn't think to say that it's true or not.\n\n48:49.660 --> 48:53.060\n That's a really interesting mathematical property.\n\n48:53.060 --> 48:54.720\n This ocean has the property\n\n48:54.720 --> 48:56.720\n that every piece of knowledge in it,\n\n48:56.720 --> 48:59.940\n we will recognize it as true if we're told,\n\n48:59.940 --> 49:04.140\n but we're unlikely to retrieve it in the reverse.\n\n49:04.140 --> 49:07.180\n So that interesting property,\n\n49:07.180 --> 49:10.580\n I would say there's a huge ocean of that knowledge.\n\n49:10.580 --> 49:11.580\n What's your intuition?\n\n49:11.580 --> 49:14.700\n Is it accessible to AI systems somehow?\n\n49:14.700 --> 49:15.940\n Can we?\n\n49:15.940 --> 49:16.780\n So you said this.\n\n49:16.780 --> 49:18.780\n I mean, most of it is not,\n\n49:18.780 --> 49:20.540\n well, I'll give you an asterisk on this in a second,\n\n49:20.540 --> 49:23.260\n but most of it has not ever been encoded\n\n49:23.260 --> 49:25.660\n in machine interpretable form.\n\n49:25.660 --> 49:27.300\n And so, I mean, if you say accessible,\n\n49:27.300 --> 49:28.640\n there's two meanings of that.\n\n49:28.640 --> 49:31.540\n One is like, could you build it into a machine?\n\n49:31.540 --> 49:32.380\n Yes.\n\n49:32.380 --> 49:34.460\n The other is like, is there some database\n\n49:34.460 --> 49:38.380\n that we could go download and stick into our machine?\n\n49:38.380 --> 49:40.660\n But the first thing, could we?\n\n49:40.660 --> 49:42.020\n What's your intuition? I think we could.\n\n49:42.020 --> 49:45.020\n I think it hasn't been done right.\n\n49:45.020 --> 49:47.300\n You know, the closest, and this is the asterisk,\n\n49:47.300 --> 49:51.140\n is the CYC psych system tried to do this.\n\n49:51.140 --> 49:53.020\n A lot of logicians worked for Doug Lennon\n\n49:53.020 --> 49:55.460\n for 30 years on this project.\n\n49:55.460 --> 49:57.900\n I think they stuck too closely to logic,\n\n49:57.900 --> 50:00.220\n didn't represent enough about probabilities,\n\n50:00.220 --> 50:01.180\n tried to hand code it.\n\n50:01.180 --> 50:02.180\n There are various issues,\n\n50:02.180 --> 50:04.480\n and it hasn't been that successful.\n\n50:04.480 --> 50:08.500\n That is the closest existing system\n\n50:08.500 --> 50:10.620\n to trying to encode this.\n\n50:10.620 --> 50:13.460\n Why do you think there's not more excitement\n\n50:13.460 --> 50:16.420\n slash money behind this idea currently?\n\n50:16.420 --> 50:17.260\n There was.\n\n50:17.260 --> 50:19.180\n People view that project as a failure.\n\n50:19.180 --> 50:22.060\n I think that they confuse the failure\n\n50:22.060 --> 50:25.100\n of a specific instance that was conceived 30 years ago\n\n50:25.100 --> 50:26.180\n for the failure of an approach,\n\n50:26.180 --> 50:28.160\n which they don't do for deep learning.\n\n50:28.160 --> 50:31.940\n So in 2010, people had the same attitude\n\n50:31.940 --> 50:32.780\n towards deep learning.\n\n50:32.780 --> 50:35.500\n They're like, this stuff doesn't really work.\n\n50:35.500 --> 50:39.140\n And all these other algorithms work better and so forth.\n\n50:39.140 --> 50:41.900\n And then certain key technical advances were made,\n\n50:41.900 --> 50:43.780\n but mostly it was the advent\n\n50:43.780 --> 50:46.400\n of graphics processing units that changed that.\n\n50:46.400 --> 50:50.060\n It wasn't even anything foundational in the techniques.\n\n50:50.060 --> 50:51.220\n And there was some new tricks,\n\n50:51.220 --> 50:55.300\n but mostly it was just more compute and more data,\n\n50:55.300 --> 50:57.900\n things like ImageNet that didn't exist before\n\n50:57.900 --> 50:59.020\n that allowed deep learning.\n\n50:59.020 --> 51:00.860\n And it could be, to work,\n\n51:00.860 --> 51:03.780\n it could be that CYC just needs a few more things\n\n51:03.780 --> 51:05.500\n or something like CYC,\n\n51:05.500 --> 51:08.820\n but the widespread view is that that just doesn't work.\n\n51:08.820 --> 51:11.820\n And people are reasoning from a single example.\n\n51:11.820 --> 51:13.260\n They don't do that with deep learning.\n\n51:13.260 --> 51:16.580\n They don't say nothing that existed in 2010,\n\n51:16.580 --> 51:18.860\n and there were many, many efforts in deep learning\n\n51:18.860 --> 51:20.580\n was really worth anything.\n\n51:20.580 --> 51:23.820\n I mean, really, there's no model from 2010\n\n51:23.820 --> 51:26.620\n in deep learning or the predecessors of deep learning\n\n51:26.620 --> 51:29.660\n that has any commercial value whatsoever at this point.\n\n51:29.660 --> 51:31.540\n They're all failures.\n\n51:31.540 --> 51:33.500\n But that doesn't mean that there wasn't anything there.\n\n51:33.500 --> 51:35.940\n I have a friend, I was getting to know him,\n\n51:35.940 --> 51:38.820\n and he said, I had a company too,\n\n51:38.820 --> 51:40.580\n I was talking about I had a new company.\n\n51:40.580 --> 51:42.900\n He said, I had a company too, and it failed.\n\n51:42.900 --> 51:44.260\n And I said, well, what did you do?\n\n51:44.260 --> 51:45.660\n And he said, deep learning.\n\n51:45.660 --> 51:47.940\n And the problem was he did it in 1986\n\n51:47.940 --> 51:48.780\n or something like that.\n\n51:48.780 --> 51:51.060\n And we didn't have the tools then, or 1990,\n\n51:51.060 --> 51:53.980\n we didn't have the tools then, not the algorithms.\n\n51:53.980 --> 51:56.540\n His algorithms weren't that different from model algorithms,\n\n51:56.540 --> 51:58.420\n but he didn't have the GPUs to run it fast enough.\n\n51:58.420 --> 51:59.620\n He didn't have the data.\n\n51:59.620 --> 52:01.340\n And so it failed.\n\n52:01.340 --> 52:06.340\n It could be that symbol manipulation per se\n\n52:06.820 --> 52:09.580\n with modern amounts of data and compute\n\n52:09.580 --> 52:11.940\n and maybe some advance in compute\n\n52:11.940 --> 52:14.900\n for that kind of compute might be great.\n\n52:14.900 --> 52:19.340\n My perspective on it is not that we want to resuscitate\n\n52:19.340 --> 52:21.540\n that stuff per se, but we want to borrow lessons from it,\n\n52:21.540 --> 52:23.380\n bring together with other things that we've learned.\n\n52:23.380 --> 52:25.900\n And it might have an ImageNet moment\n\n52:25.900 --> 52:28.220\n where it would spark the world's imagination\n\n52:28.220 --> 52:31.460\n and there'll be an explosion of symbol manipulation efforts.\n\n52:31.460 --> 52:33.660\n Yeah, I think that people at AI2,\n\n52:33.660 --> 52:38.660\n Paul Allen's AI Institute, are trying to build data sets.\n\n52:39.060 --> 52:39.900\n Well, they're not doing it\n\n52:39.900 --> 52:41.100\n for quite the reason that you say,\n\n52:41.100 --> 52:43.220\n but they're trying to build data sets\n\n52:43.220 --> 52:45.380\n that at least spark interest in common sense reasoning.\n\n52:45.380 --> 52:46.780\n To create benchmarks.\n\n52:46.780 --> 52:48.220\n Benchmarks for common sense.\n\n52:48.220 --> 52:50.860\n That's a large part of what the AI2.org\n\n52:50.860 --> 52:51.980\n is working on right now.\n\n52:51.980 --> 52:53.260\n So speaking of compute,\n\n52:53.260 --> 52:56.380\n Rich Sutton wrote a blog post titled Bitter Lesson.\n\n52:56.380 --> 52:57.220\n I don't know if you've read it,\n\n52:57.220 --> 52:59.900\n but he said that the biggest lesson that can be read\n\n52:59.900 --> 53:01.580\n from so many years of AI research\n\n53:01.580 --> 53:04.180\n is that general methods that leverage computation\n\n53:04.180 --> 53:06.300\n are ultimately the most effective.\n\n53:06.300 --> 53:07.140\n Do you think that?\n\n53:07.140 --> 53:08.620\n The most effective at what?\n\n53:08.620 --> 53:11.820\n Right, so they have been most effective\n\n53:11.820 --> 53:14.500\n for perceptual classification problems\n\n53:14.500 --> 53:18.060\n and for some reinforcement learning problems.\n\n53:18.060 --> 53:19.380\n And he works on reinforcement learning.\n\n53:19.380 --> 53:20.700\n Well, no, let me push back on that.\n\n53:20.700 --> 53:22.820\n You're actually absolutely right.\n\n53:22.820 --> 53:27.820\n But I would also say they have been most effective generally\n\n53:28.060 --> 53:31.500\n because everything we've done up to...\n\n53:31.500 --> 53:32.900\n Would you argue against that?\n\n53:32.900 --> 53:36.220\n Is, to me, deep learning is the first thing\n\n53:36.220 --> 53:41.220\n that has been successful at anything in AI.\n\n53:41.860 --> 53:45.300\n And you're pointing out that this success\n\n53:45.300 --> 53:47.100\n is very limited, folks,\n\n53:47.100 --> 53:50.260\n but has there been something truly successful\n\n53:50.260 --> 53:51.660\n before deep learning?\n\n53:51.660 --> 53:54.860\n Sure, I mean, I want to make a larger point,\n\n53:54.860 --> 53:59.860\n but on the narrower point, classical AI is used,\n\n54:00.020 --> 54:03.660\n for example, in doing navigation instructions.\n\n54:03.660 --> 54:06.020\n It's very successful.\n\n54:06.020 --> 54:07.780\n Everybody on the planet uses it now,\n\n54:07.780 --> 54:09.420\n like multiple times a day.\n\n54:09.420 --> 54:11.260\n That's a measure of success, right?\n\n54:12.220 --> 54:16.060\n So I don't think classical AI was wildly successful,\n\n54:16.060 --> 54:17.580\n but there are cases like that.\n\n54:17.580 --> 54:19.140\n They're just used all the time.\n\n54:19.140 --> 54:21.820\n Nobody even notices them because they're so pervasive.\n\n54:23.740 --> 54:26.580\n So there are some successes for classical AI.\n\n54:26.580 --> 54:28.700\n I think deep learning has been more successful,\n\n54:28.700 --> 54:32.020\n but my usual line about this, and I didn't invent it,\n\n54:32.020 --> 54:33.060\n but I like it a lot,\n\n54:33.060 --> 54:34.780\n is just because you can build a better ladder\n\n54:34.780 --> 54:37.140\n doesn't mean you can build a ladder to the moon.\n\n54:37.140 --> 54:39.660\n So the bitter lesson is if you have\n\n54:39.660 --> 54:42.220\n a perceptual classification problem,\n\n54:42.220 --> 54:45.740\n throwing a lot of data at it is better than anything else.\n\n54:45.740 --> 54:49.980\n But that has not given us any material progress\n\n54:49.980 --> 54:51.860\n in natural language understanding,\n\n54:51.860 --> 54:53.060\n common sense reasoning,\n\n54:53.060 --> 54:56.220\n like a robot would need to navigate a home.\n\n54:56.220 --> 54:59.420\n Problems like that, there's no actual progress there.\n\n54:59.420 --> 55:02.220\n So flip side of that, if we remove data from the picture,\n\n55:02.220 --> 55:05.780\n another bitter lesson is that you just have\n\n55:05.780 --> 55:10.100\n a very simple algorithm,\n\n55:10.100 --> 55:12.500\n and you wait for compute to scale.\n\n55:12.500 --> 55:13.540\n It doesn't have to be learning.\n\n55:13.540 --> 55:14.580\n It doesn't have to be deep learning.\n\n55:14.580 --> 55:16.420\n It doesn't have to be data driven,\n\n55:16.420 --> 55:18.220\n but just wait for the compute.\n\n55:18.220 --> 55:19.060\n So my question for you,\n\n55:19.060 --> 55:21.660\n do you think compute can unlock some of the things\n\n55:21.660 --> 55:25.460\n with either deep learning or symbol manipulation that?\n\n55:25.460 --> 55:29.780\n Sure, but I'll put a proviso on that.\n\n55:29.780 --> 55:31.940\n I think more compute's always better.\n\n55:31.940 --> 55:33.660\n Nobody's gonna argue with more compute.\n\n55:33.660 --> 55:34.700\n It's like having more money.\n\n55:34.700 --> 55:36.020\n I mean, there's the data.\n\n55:36.020 --> 55:37.460\n There's diminishing returns on more money.\n\n55:37.460 --> 55:39.740\n Exactly, there's diminishing returns on more money,\n\n55:39.740 --> 55:40.980\n but nobody's gonna argue\n\n55:40.980 --> 55:42.620\n if you wanna give them more money, right?\n\n55:42.620 --> 55:44.620\n Except maybe the people who signed the giving pledge,\n\n55:44.620 --> 55:46.300\n and some of them have a problem.\n\n55:46.300 --> 55:47.980\n They've promised to give away more money\n\n55:47.980 --> 55:49.660\n than they're able to.\n\n55:49.660 --> 55:52.500\n But the rest of us, if you wanna give me more money, fine.\n\n55:52.500 --> 55:54.580\n I'm saying more money, more problems, but okay.\n\n55:54.580 --> 55:55.500\n That's true too.\n\n55:55.500 --> 56:00.100\n What I would say to you is your brain uses like 20 watts,\n\n56:00.100 --> 56:02.660\n and it does a lot of things that deep learning doesn't do,\n\n56:02.660 --> 56:05.140\n or that symbol manipulation doesn't do,\n\n56:05.140 --> 56:07.020\n that AI just hasn't figured out how to do.\n\n56:07.020 --> 56:09.100\n So it's an existence proof\n\n56:09.100 --> 56:12.140\n that you don't need server resources\n\n56:12.140 --> 56:16.460\n that are Google scale in order to have an intelligence.\n\n56:16.460 --> 56:18.900\n I built, with a lot of help from my wife,\n\n56:18.900 --> 56:21.660\n two intelligences that are 20 watts each,\n\n56:21.660 --> 56:24.060\n and far exceed anything that anybody else\n\n56:25.060 --> 56:26.780\n has built at a silicon.\n\n56:26.780 --> 56:30.020\n Speaking of those two robots,\n\n56:30.020 --> 56:32.380\n what have you learned about AI from having?\n\n56:33.260 --> 56:35.300\n Well, they're not robots, but.\n\n56:35.300 --> 56:36.740\n Sorry, intelligent agents.\n\n56:36.740 --> 56:38.140\n Those two intelligent agents.\n\n56:38.140 --> 56:42.780\n I've learned a lot by watching my two intelligent agents.\n\n56:42.780 --> 56:45.820\n I think that what's fundamentally interesting,\n\n56:45.820 --> 56:46.980\n well, one of the many things\n\n56:46.980 --> 56:48.660\n that's fundamentally interesting about them\n\n56:48.660 --> 56:51.940\n is the way that they set their own problems to solve.\n\n56:51.940 --> 56:54.540\n So my two kids are a year and a half apart.\n\n56:54.540 --> 56:56.420\n They're both five and six and a half.\n\n56:56.420 --> 56:58.180\n They play together all the time,\n\n56:58.180 --> 57:00.940\n and they're constantly creating new challenges.\n\n57:00.940 --> 57:03.780\n That's what they do, is they make up games,\n\n57:03.780 --> 57:05.940\n and they're like, well, what if this, or what if that,\n\n57:05.940 --> 57:07.860\n or what if I had this superpower,\n\n57:07.860 --> 57:10.340\n or what if you could walk through this wall?\n\n57:10.340 --> 57:14.020\n So they're doing these what if scenarios all the time,\n\n57:14.020 --> 57:17.540\n and that's how they learn something about the world\n\n57:17.540 --> 57:22.540\n and grow their minds, and machines don't really do that.\n\n57:22.580 --> 57:24.460\n So that's interesting, and you've talked about this,\n\n57:24.460 --> 57:26.100\n you've written about it, you've thought about it,\n\n57:26.100 --> 57:27.540\n nature versus nurture.\n\n57:29.260 --> 57:33.580\n So what innate knowledge do you think we're born with,\n\n57:33.580 --> 57:35.540\n and what do we learn along the way\n\n57:35.540 --> 57:38.260\n in those early months and years?\n\n57:38.260 --> 57:40.460\n Can I just say how much I like that question?\n\n57:41.540 --> 57:45.780\n You phrased it just right, and almost nobody ever does,\n\n57:45.780 --> 57:47.220\n which is what is the innate knowledge\n\n57:47.220 --> 57:49.180\n and what's learned along the way?\n\n57:49.180 --> 57:51.180\n So many people dichotomize it,\n\n57:51.180 --> 57:53.380\n and they think it's nature versus nurture,\n\n57:53.380 --> 57:56.740\n when it is obviously has to be nature and nurture.\n\n57:56.740 --> 57:58.540\n They have to work together.\n\n57:58.540 --> 58:00.500\n You can't learn this stuff along the way\n\n58:00.500 --> 58:02.340\n unless you have some innate stuff,\n\n58:02.340 --> 58:03.860\n but just because you have the innate stuff\n\n58:03.860 --> 58:05.820\n doesn't mean you don't learn anything.\n\n58:05.820 --> 58:09.340\n And so many people get that wrong, including in the field.\n\n58:09.340 --> 58:12.220\n People think if I work in machine learning,\n\n58:12.220 --> 58:15.260\n the learning side, I must not be allowed to work\n\n58:15.260 --> 58:17.300\n on the innate side, or that will be cheating.\n\n58:17.300 --> 58:19.620\n Exactly, people have said that to me,\n\n58:19.620 --> 58:22.380\n and it's just absurd, so thank you.\n\n58:23.380 --> 58:25.140\n But you could break that apart more.\n\n58:25.140 --> 58:26.540\n I've talked to folks who studied\n\n58:26.540 --> 58:28.260\n the development of the brain,\n\n58:28.260 --> 58:32.940\n and the growth of the brain in the first few days\n\n58:32.940 --> 58:35.660\n in the first few months in the womb,\n\n58:35.660 --> 58:39.500\n all of that, is that innate?\n\n58:39.500 --> 58:42.300\n So that process of development from a stem cell\n\n58:42.300 --> 58:45.140\n to the growth of the central nervous system and so on,\n\n58:46.020 --> 58:49.300\n to the information that's encoded\n\n58:49.300 --> 58:52.300\n through the long arc of evolution.\n\n58:52.300 --> 58:55.300\n So all of that comes into play, and it's unclear.\n\n58:55.300 --> 58:57.340\n It's not just whether it's a dichotomy or not.\n\n58:57.340 --> 59:02.060\n It's where most, or where the knowledge is encoded.\n\n59:02.060 --> 59:07.060\n So what's your intuition about the innate knowledge,\n\n59:07.780 --> 59:09.700\n the power of it, what's contained in it,\n\n59:09.700 --> 59:11.340\n what can we learn from it?\n\n59:11.340 --> 59:12.740\n One of my earlier books was actually trying\n\n59:12.740 --> 59:14.020\n to understand the biology of this.\n\n59:14.020 --> 59:15.860\n The book was called The Birth of the Mind.\n\n59:15.860 --> 59:18.900\n Like how is it the genes even build innate knowledge?\n\n59:18.900 --> 59:21.460\n And from the perspective of the conversation\n\n59:21.460 --> 59:23.580\n we're having today, there's actually two questions.\n\n59:23.580 --> 59:26.460\n One is what innate knowledge or mechanisms,\n\n59:26.460 --> 59:29.660\n or what have you, people or other animals\n\n59:29.660 --> 59:30.900\n might be endowed with.\n\n59:30.900 --> 59:32.260\n I always like showing this video\n\n59:32.260 --> 59:34.620\n of a baby ibex climbing down a mountain.\n\n59:34.620 --> 59:37.380\n That baby ibex, a few hours after its birth,\n\n59:37.380 --> 59:38.420\n knows how to climb down a mountain.\n\n59:38.420 --> 59:40.940\n That means that it knows, not consciously,\n\n59:40.940 --> 59:43.020\n something about its own body and physics\n\n59:43.020 --> 59:46.420\n and 3D geometry and all of this kind of stuff.\n\n59:47.500 --> 59:49.660\n So there's one question about what does biology\n\n59:49.660 --> 59:53.220\n give its creatures and what has evolved in our brains?\n\n59:53.220 --> 59:54.940\n How is that represented in our brains?\n\n59:54.940 --> 59:56.180\n The question I thought about in the book\n\n59:56.180 --> 59:57.340\n The Birth of the Mind.\n\n59:57.340 --> 59:59.300\n And then there's a question of what AI should have.\n\n59:59.300 --> 1:00:01.540\n And they don't have to be the same.\n\n1:00:01.540 --> 1:00:06.540\n But I would say that it's a pretty interesting\n\n1:00:06.940 --> 1:00:08.660\n set of things that we are equipped with\n\n1:00:08.660 --> 1:00:10.500\n that allows us to do a lot of interesting things.\n\n1:00:10.500 --> 1:00:13.740\n So I would argue or guess, based on my reading\n\n1:00:13.740 --> 1:00:15.220\n of the developmental psychology literature,\n\n1:00:15.220 --> 1:00:16.820\n which I've also participated in,\n\n1:00:17.980 --> 1:00:21.740\n that children are born with a notion of space,\n\n1:00:21.740 --> 1:00:24.420\n time, other agents, places,\n\n1:00:25.740 --> 1:00:27.620\n and also this kind of mental algebra\n\n1:00:27.620 --> 1:00:30.220\n that I was describing before.\n\n1:00:30.220 --> 1:00:33.060\n No certain causation if I didn't just say that.\n\n1:00:33.060 --> 1:00:35.220\n So at least those kinds of things.\n\n1:00:35.220 --> 1:00:38.940\n They're like frameworks for learning the other things.\n\n1:00:38.940 --> 1:00:40.340\n Are they disjoint in your view\n\n1:00:40.340 --> 1:00:42.860\n or is it just somehow all connected?\n\n1:00:42.860 --> 1:00:44.340\n You've talked a lot about language.\n\n1:00:44.340 --> 1:00:47.940\n Is it all kind of connected in some mesh\n\n1:00:47.940 --> 1:00:50.260\n that's language like?\n\n1:00:50.260 --> 1:00:52.740\n If understanding concepts all together or?\n\n1:00:52.740 --> 1:00:55.740\n I don't think we know for people how they're represented\n\n1:00:55.740 --> 1:00:58.180\n and machines just don't really do this yet.\n\n1:00:58.180 --> 1:01:00.540\n So I think it's an interesting open question\n\n1:01:00.540 --> 1:01:02.620\n both for science and for engineering.\n\n1:01:03.540 --> 1:01:06.340\n Some of it has to be at least interrelated\n\n1:01:06.340 --> 1:01:10.180\n in the way that the interfaces of a software package\n\n1:01:10.180 --> 1:01:12.140\n have to be able to talk to one another.\n\n1:01:12.140 --> 1:01:16.620\n So the systems that represent space and time\n\n1:01:16.620 --> 1:01:19.820\n can't be totally disjoint because a lot of the things\n\n1:01:19.820 --> 1:01:21.500\n that we reason about are the relations\n\n1:01:21.500 --> 1:01:22.980\n between space and time and cause.\n\n1:01:22.980 --> 1:01:26.460\n So I put this on and I have expectations\n\n1:01:26.460 --> 1:01:28.180\n about what's gonna happen with the bottle cap\n\n1:01:28.180 --> 1:01:32.540\n on top of the bottle and those span space and time.\n\n1:01:32.540 --> 1:01:35.740\n If the cap is over here, I get a different outcome.\n\n1:01:35.740 --> 1:01:38.540\n If the timing is different, if I put this here,\n\n1:01:38.540 --> 1:01:41.900\n after I move that, then I get a different outcome.\n\n1:01:41.900 --> 1:01:43.060\n That relates to causality.\n\n1:01:43.060 --> 1:01:47.840\n So obviously these mechanisms, whatever they are,\n\n1:01:47.840 --> 1:01:50.100\n can certainly communicate with each other.\n\n1:01:50.100 --> 1:01:53.180\n So I think evolution had a significant role\n\n1:01:53.180 --> 1:01:57.100\n to play in the development of this whole kluge, right?\n\n1:01:57.100 --> 1:01:59.220\n How efficient do you think is evolution?\n\n1:01:59.220 --> 1:02:01.620\n Oh, it's terribly inefficient except that.\n\n1:02:01.620 --> 1:02:03.060\n Okay, well, can we do better?\n\n1:02:03.980 --> 1:02:05.740\n Well, I'll come to that in a sec.\n\n1:02:05.740 --> 1:02:08.100\n It's inefficient except that.\n\n1:02:08.100 --> 1:02:10.900\n Once it gets a good idea, it runs with it.\n\n1:02:10.900 --> 1:02:15.660\n So it took, I guess, a billion years,\n\n1:02:15.660 --> 1:02:20.420\n if I went roughly a billion years, to evolve\n\n1:02:20.420 --> 1:02:24.040\n to a vertebrate brain plan.\n\n1:02:24.040 --> 1:02:26.920\n And once that vertebrate brain plan evolved,\n\n1:02:26.920 --> 1:02:28.480\n it spread everywhere.\n\n1:02:28.480 --> 1:02:31.700\n So fish have it and dogs have it and we have it.\n\n1:02:31.700 --> 1:02:34.140\n We have adaptations of it and specializations of it,\n\n1:02:34.140 --> 1:02:37.160\n but, and the same thing with a primate brain plan.\n\n1:02:37.160 --> 1:02:41.100\n So monkeys have it and apes have it and we have it.\n\n1:02:41.100 --> 1:02:43.780\n So there are additional innovations like color vision\n\n1:02:43.780 --> 1:02:45.860\n and those spread really rapidly.\n\n1:02:45.860 --> 1:02:48.820\n So it takes evolution a long time to get a good idea,\n\n1:02:48.820 --> 1:02:53.300\n but, and I'm being anthropomorphic and not literal here,\n\n1:02:53.300 --> 1:02:55.580\n but once it has that idea, so to speak,\n\n1:02:55.580 --> 1:02:58.540\n which cashes out into one set of genes or in the genome,\n\n1:02:58.540 --> 1:03:00.420\n those genes spread very rapidly\n\n1:03:00.420 --> 1:03:02.660\n and they're like subroutines or libraries,\n\n1:03:02.660 --> 1:03:04.540\n I guess the word people might use nowadays\n\n1:03:04.540 --> 1:03:05.620\n or be more familiar with.\n\n1:03:05.620 --> 1:03:08.780\n They're libraries that get used over and over again.\n\n1:03:08.780 --> 1:03:11.740\n So once you have the library for building something\n\n1:03:11.740 --> 1:03:13.840\n with multiple digits, you can use it for a hand,\n\n1:03:13.840 --> 1:03:15.540\n but you can also use it for a foot.\n\n1:03:15.540 --> 1:03:17.420\n You just kind of reuse the library\n\n1:03:17.420 --> 1:03:19.080\n with slightly different parameters.\n\n1:03:19.080 --> 1:03:20.660\n Evolution does a lot of that,\n\n1:03:20.660 --> 1:03:23.500\n which means that the speed over time picks up.\n\n1:03:23.500 --> 1:03:25.560\n So evolution can happen faster\n\n1:03:25.560 --> 1:03:28.380\n because you have bigger and bigger libraries.\n\n1:03:28.380 --> 1:03:32.220\n And what I think has happened in attempts\n\n1:03:32.220 --> 1:03:35.740\n at evolutionary computation is that people start\n\n1:03:35.740 --> 1:03:40.340\n with libraries that are very, very minimal,\n\n1:03:40.340 --> 1:03:44.260\n like almost nothing, and then progress is slow\n\n1:03:44.260 --> 1:03:46.620\n and it's hard for someone to get a good PhD thesis\n\n1:03:46.620 --> 1:03:48.260\n out of it and they give up.\n\n1:03:48.260 --> 1:03:50.260\n If we had richer libraries to begin with,\n\n1:03:50.260 --> 1:03:52.580\n if you were evolving from systems\n\n1:03:52.580 --> 1:03:55.320\n that had an rich innate structure to begin with,\n\n1:03:55.320 --> 1:03:56.780\n then things might speed up.\n\n1:03:56.780 --> 1:03:59.900\n Or more PhD students, if the evolutionary process\n\n1:03:59.900 --> 1:04:04.260\n is indeed in a meta way runs away with good ideas,\n\n1:04:04.260 --> 1:04:06.740\n you need to have a lot of ideas,\n\n1:04:06.740 --> 1:04:08.820\n pool of ideas in order for it to discover one\n\n1:04:08.820 --> 1:04:10.260\n that you can run away with.\n\n1:04:10.260 --> 1:04:13.220\n And PhD students representing individual ideas as well.\n\n1:04:13.220 --> 1:04:14.340\n Yeah, I mean, you could throw\n\n1:04:14.340 --> 1:04:16.220\n a billion PhD students at it.\n\n1:04:16.220 --> 1:04:18.980\n Yeah, the monkeys are typewriters with Shakespeare, yep.\n\n1:04:20.180 --> 1:04:22.060\n Well, I mean, those aren't cumulative, right?\n\n1:04:22.060 --> 1:04:23.420\n That's just random.\n\n1:04:23.420 --> 1:04:24.940\n And part of the point that I'm making\n\n1:04:24.940 --> 1:04:26.780\n is that evolution is cumulative.\n\n1:04:26.780 --> 1:04:31.140\n So if you have a billion monkeys independently,\n\n1:04:31.140 --> 1:04:32.420\n you don't really get anywhere.\n\n1:04:32.420 --> 1:04:33.820\n But if you have a billion monkeys,\n\n1:04:33.820 --> 1:04:35.700\n and I think Dawkins made this point originally,\n\n1:04:35.700 --> 1:04:37.580\n or probably other people, Dawkins made it very nice\n\n1:04:37.580 --> 1:04:40.420\n and either a selfish gene or blind watchmaker.\n\n1:04:40.420 --> 1:04:44.060\n If there is some sort of fitness function\n\n1:04:44.060 --> 1:04:45.860\n that can drive you towards something,\n\n1:04:45.860 --> 1:04:47.060\n I guess that's Dawkins point.\n\n1:04:47.060 --> 1:04:49.420\n And my point, which is a variation on that,\n\n1:04:49.420 --> 1:04:51.940\n is that if the evolution is cumulative,\n\n1:04:51.940 --> 1:04:53.820\n I mean, the related points,\n\n1:04:53.820 --> 1:04:55.600\n then you can start going faster.\n\n1:04:55.600 --> 1:04:57.760\n Do you think something like the process of evolution\n\n1:04:57.760 --> 1:05:00.180\n is required to build intelligent systems?\n\n1:05:00.180 --> 1:05:01.560\n So if we... Not logically.\n\n1:05:01.560 --> 1:05:04.040\n So all the stuff that evolution did,\n\n1:05:04.040 --> 1:05:07.040\n a good engineer might be able to do.\n\n1:05:07.040 --> 1:05:10.540\n So for example, evolution made quadrupeds,\n\n1:05:10.540 --> 1:05:14.180\n which distribute the load across a horizontal surface.\n\n1:05:14.180 --> 1:05:16.980\n A good engineer could come up with that idea.\n\n1:05:16.980 --> 1:05:18.740\n I mean, sometimes good engineers come up with ideas\n\n1:05:18.740 --> 1:05:19.760\n by looking at biology.\n\n1:05:19.760 --> 1:05:22.500\n There's lots of ways to get your ideas.\n\n1:05:22.500 --> 1:05:23.660\n Part of what I'm suggesting\n\n1:05:23.660 --> 1:05:25.980\n is we should look at biology a lot more.\n\n1:05:25.980 --> 1:05:30.180\n We should look at the biology of thought and understanding\n\n1:05:30.180 --> 1:05:33.480\n and the biology by which creatures intuitively reason\n\n1:05:33.480 --> 1:05:35.960\n about physics or other agents,\n\n1:05:35.960 --> 1:05:37.900\n or like how do dogs reason about people?\n\n1:05:37.900 --> 1:05:39.620\n Like they're actually pretty good at it.\n\n1:05:39.620 --> 1:05:44.000\n If we could understand, at my college we joked dognition,\n\n1:05:44.000 --> 1:05:46.280\n if we could understand dognition well,\n\n1:05:46.280 --> 1:05:49.780\n and how it was implemented, that might help us with our AI.\n\n1:05:49.780 --> 1:05:53.780\n So do you think it's possible\n\n1:05:53.780 --> 1:05:57.180\n that the kind of timescale that evolution took\n\n1:05:57.180 --> 1:05:58.940\n is the kind of timescale that will be needed\n\n1:05:58.940 --> 1:06:00.500\n to build intelligent systems?\n\n1:06:00.500 --> 1:06:02.980\n Or can we significantly accelerate that process\n\n1:06:02.980 --> 1:06:04.020\n inside a computer?\n\n1:06:04.020 --> 1:06:07.580\n I mean, I think the way that we accelerate that process\n\n1:06:07.580 --> 1:06:12.100\n is we borrow from biology, not slavishly,\n\n1:06:12.100 --> 1:06:15.260\n but I think we look at how biology has solved problems\n\n1:06:15.260 --> 1:06:16.780\n and we say, does that inspire\n\n1:06:16.780 --> 1:06:18.940\n any engineering solutions here?\n\n1:06:18.940 --> 1:06:20.700\n Try to mimic biological systems\n\n1:06:20.700 --> 1:06:22.380\n and then therefore have a shortcut.\n\n1:06:22.380 --> 1:06:25.020\n Yeah, I mean, there's a field called biomimicry\n\n1:06:25.020 --> 1:06:28.980\n and people do that for like material science all the time.\n\n1:06:28.980 --> 1:06:32.940\n We should be doing the analog of that for AI\n\n1:06:32.940 --> 1:06:34.460\n and the analog for that for AI\n\n1:06:34.460 --> 1:06:37.020\n is to look at cognitive science or the cognitive sciences,\n\n1:06:37.020 --> 1:06:40.380\n which is psychology, maybe neuroscience, linguistics,\n\n1:06:40.380 --> 1:06:43.460\n and so forth, look to those for insight.\n\n1:06:43.460 --> 1:06:45.340\n What do you think is a good test of intelligence\n\n1:06:45.340 --> 1:06:46.180\n in your view?\n\n1:06:46.180 --> 1:06:48.500\n So I don't think there's one good test.\n\n1:06:48.500 --> 1:06:51.780\n In fact, I tried to organize a movement\n\n1:06:51.780 --> 1:06:53.380\n towards something called a Turing Olympics\n\n1:06:53.380 --> 1:06:56.140\n and my hope is that Francois is actually gonna take,\n\n1:06:56.140 --> 1:06:58.260\n Francois Chollet is gonna take over this.\n\n1:06:58.260 --> 1:06:59.940\n I think he's interested and I don't,\n\n1:06:59.940 --> 1:07:03.500\n I just don't have place in my busy life at this moment,\n\n1:07:03.500 --> 1:07:06.460\n but the notion is that there'd be many tests\n\n1:07:06.460 --> 1:07:09.500\n and not just one because intelligence is multifaceted.\n\n1:07:09.500 --> 1:07:12.900\n There can't really be a single measure of it\n\n1:07:12.900 --> 1:07:14.660\n because it isn't a single thing.\n\n1:07:15.620 --> 1:07:17.340\n Like just the crudest level,\n\n1:07:17.340 --> 1:07:19.860\n the SAT has a verbal component and a math component\n\n1:07:19.860 --> 1:07:21.340\n because they're not identical.\n\n1:07:21.340 --> 1:07:23.660\n And Howard Gardner has talked about multiple intelligences\n\n1:07:23.660 --> 1:07:25.420\n like kinesthetic intelligence\n\n1:07:25.420 --> 1:07:27.740\n and verbal intelligence and so forth.\n\n1:07:27.740 --> 1:07:29.940\n There are a lot of things that go into intelligence\n\n1:07:29.940 --> 1:07:32.580\n and people can get good at one or the other.\n\n1:07:32.580 --> 1:07:35.260\n I mean, in some sense, like every expert has developed\n\n1:07:35.260 --> 1:07:37.260\n a very specific kind of intelligence\n\n1:07:37.260 --> 1:07:39.300\n and then there are people that are generalists\n\n1:07:39.300 --> 1:07:41.740\n and I think of myself as a generalist\n\n1:07:41.740 --> 1:07:43.380\n with respect to cognitive science,\n\n1:07:43.380 --> 1:07:45.620\n which doesn't mean I know anything about quantum mechanics,\n\n1:07:45.620 --> 1:07:49.260\n but I know a lot about the different facets of the mind.\n\n1:07:49.260 --> 1:07:51.380\n And there's a kind of intelligence\n\n1:07:51.380 --> 1:07:52.660\n to thinking about intelligence.\n\n1:07:52.660 --> 1:07:54.740\n I like to think that I have some of that,\n\n1:07:54.740 --> 1:07:57.500\n but social intelligence, I'm just okay.\n\n1:07:57.500 --> 1:08:00.140\n There are people that are much better at that than I am.\n\n1:08:00.140 --> 1:08:03.020\n Sure, but what would be really impressive to you?\n\n1:08:04.140 --> 1:08:07.060\n I think the idea of a touring Olympics is really interesting\n\n1:08:07.060 --> 1:08:09.660\n especially if somebody like Francois is running it,\n\n1:08:09.660 --> 1:08:14.380\n but to you in general, not as a benchmark,\n\n1:08:14.380 --> 1:08:17.300\n but if you saw an AI system being able to accomplish\n\n1:08:17.300 --> 1:08:21.740\n something that would impress the heck out of you,\n\n1:08:21.740 --> 1:08:22.740\n what would that thing be?\n\n1:08:22.740 --> 1:08:24.700\n Would it be natural language conversation?\n\n1:08:24.700 --> 1:08:28.580\n For me personally, I would like to see\n\n1:08:28.580 --> 1:08:30.660\n a kind of comprehension that relates to what you just said.\n\n1:08:30.660 --> 1:08:34.980\n So I wrote a piece in the New Yorker in I think 2015\n\n1:08:34.980 --> 1:08:39.940\n right after Eugene Guestman, which was a software package,\n\n1:08:39.940 --> 1:08:42.940\n won a version of the Turing test.\n\n1:08:42.940 --> 1:08:45.060\n And the way that it did this is it be,\n\n1:08:45.060 --> 1:08:46.900\n well, the way you win the Turing test,\n\n1:08:46.900 --> 1:08:50.700\n so called win it, is the Turing test is you fool a person\n\n1:08:50.700 --> 1:08:54.420\n into thinking that a machine is a person,\n\n1:08:54.420 --> 1:08:57.940\n is you're evasive, you pretend to have limitations\n\n1:08:57.940 --> 1:09:00.540\n so you don't have to answer certain questions and so forth.\n\n1:09:00.540 --> 1:09:04.300\n So this particular system pretended to be a 13 year old boy\n\n1:09:04.300 --> 1:09:06.980\n from Odessa who didn't understand English\n\n1:09:06.980 --> 1:09:08.060\n and was kind of sarcastic\n\n1:09:08.060 --> 1:09:09.660\n and wouldn't answer your questions and so forth.\n\n1:09:09.660 --> 1:09:12.460\n And so judges got fooled into thinking briefly\n\n1:09:12.460 --> 1:09:14.660\n with a very little exposure, it was a 13 year old boy,\n\n1:09:14.660 --> 1:09:16.340\n and it docked all the questions\n\n1:09:16.340 --> 1:09:17.540\n Turing was actually interested in,\n\n1:09:17.540 --> 1:09:18.780\n which is like how do you make the machine\n\n1:09:18.780 --> 1:09:20.420\n actually intelligent?\n\n1:09:20.420 --> 1:09:22.100\n So that test itself is not that good.\n\n1:09:22.100 --> 1:09:26.100\n And so in New Yorker, I proposed an alternative, I guess,\n\n1:09:26.100 --> 1:09:27.260\n and the one that I proposed there\n\n1:09:27.260 --> 1:09:29.020\n was a comprehension test.\n\n1:09:30.020 --> 1:09:31.060\n And I must like Breaking Bad\n\n1:09:31.060 --> 1:09:32.900\n because I've already given you one Breaking Bad example\n\n1:09:32.900 --> 1:09:35.660\n and in that article, I have one as well,\n\n1:09:35.660 --> 1:09:37.660\n which was something like if Walter,\n\n1:09:37.660 --> 1:09:40.340\n you should be able to watch an episode of Breaking Bad\n\n1:09:40.340 --> 1:09:41.700\n or maybe you have to watch the whole series\n\n1:09:41.700 --> 1:09:43.500\n to be able to answer the question and say,\n\n1:09:43.500 --> 1:09:45.580\n if Walter White took a hit out on Jesse,\n\n1:09:45.580 --> 1:09:47.180\n why did he do that?\n\n1:09:47.180 --> 1:09:49.380\n So if you could answer kind of arbitrary questions\n\n1:09:49.380 --> 1:09:52.700\n about characters motivations, I would be really impressed\n\n1:09:52.700 --> 1:09:55.380\n with that and he built software to do that.\n\n1:09:55.380 --> 1:09:58.500\n They could watch a film or there are different versions.\n\n1:09:58.500 --> 1:10:01.940\n And so ultimately, I wrote this up with Praveen Paritosh\n\n1:10:01.940 --> 1:10:04.060\n in a special issue of AI Magazine\n\n1:10:04.060 --> 1:10:05.780\n that basically was about the Turing Olympics.\n\n1:10:05.780 --> 1:10:07.700\n There were like 14 tests proposed.\n\n1:10:07.700 --> 1:10:10.100\n The one that I was pushing was a comprehension challenge\n\n1:10:10.100 --> 1:10:12.380\n and Praveen who's at Google was trying to figure out\n\n1:10:12.380 --> 1:10:13.460\n like how we would actually run it\n\n1:10:13.460 --> 1:10:15.340\n and so we wrote a paper together.\n\n1:10:15.340 --> 1:10:17.300\n And you could have a text version too\n\n1:10:17.300 --> 1:10:19.780\n or you could have an auditory podcast version,\n\n1:10:19.780 --> 1:10:20.620\n you could have a written version.\n\n1:10:20.620 --> 1:10:23.820\n But the point is that you win at this test\n\n1:10:23.820 --> 1:10:27.060\n if you can do, let's say human level or better than humans\n\n1:10:27.060 --> 1:10:29.780\n at answering kind of arbitrary questions.\n\n1:10:29.780 --> 1:10:31.660\n Why did this person pick up the stone?\n\n1:10:31.660 --> 1:10:34.180\n What were they thinking when they picked up the stone?\n\n1:10:34.180 --> 1:10:36.260\n Were they trying to knock down glass?\n\n1:10:36.260 --> 1:10:38.700\n And I mean, ideally these wouldn't be multiple choice either\n\n1:10:38.700 --> 1:10:41.140\n because multiple choice is pretty easily gamed.\n\n1:10:41.140 --> 1:10:44.180\n So if you could have relatively open ended questions\n\n1:10:44.180 --> 1:10:47.380\n and you can answer why people are doing this stuff,\n\n1:10:47.380 --> 1:10:48.220\n I would be very impressed.\n\n1:10:48.220 --> 1:10:50.060\n And of course, humans can do this, right?\n\n1:10:50.060 --> 1:10:52.820\n If you watch a well constructed movie\n\n1:10:52.820 --> 1:10:55.540\n and somebody picks up a rock,\n\n1:10:55.540 --> 1:10:56.940\n everybody watching the movie\n\n1:10:56.940 --> 1:10:59.420\n knows why they picked up the rock, right?\n\n1:10:59.420 --> 1:11:01.140\n They all know, oh my gosh,\n\n1:11:01.140 --> 1:11:03.620\n he's gonna hit this character or whatever.\n\n1:11:03.620 --> 1:11:06.220\n We have an example in the book about\n\n1:11:06.220 --> 1:11:08.700\n when a whole bunch of people say, I am Spartacus,\n\n1:11:08.700 --> 1:11:10.060\n you know, this famous scene.\n\n1:11:11.780 --> 1:11:13.540\n The viewers understand,\n\n1:11:13.540 --> 1:11:18.220\n first of all, that everybody or everybody minus one\n\n1:11:18.220 --> 1:11:19.060\n has to be lying.\n\n1:11:19.060 --> 1:11:20.340\n They can't all be Spartacus.\n\n1:11:20.340 --> 1:11:21.780\n We have enough common sense knowledge\n\n1:11:21.780 --> 1:11:24.100\n to know they couldn't all have the same name.\n\n1:11:24.100 --> 1:11:25.340\n We know that they're lying\n\n1:11:25.340 --> 1:11:27.100\n and we can infer why they're lying, right?\n\n1:11:27.100 --> 1:11:28.460\n They're lying to protect someone\n\n1:11:28.460 --> 1:11:30.340\n and to protect things they believe in.\n\n1:11:30.340 --> 1:11:32.340\n You get a machine that can do that.\n\n1:11:32.340 --> 1:11:35.100\n They can say, this is why these guys all got up\n\n1:11:35.100 --> 1:11:36.940\n and said, I am Spartacus.\n\n1:11:36.940 --> 1:11:40.540\n I will sit down and say, AI has really achieved a lot.\n\n1:11:40.540 --> 1:11:41.380\n Thank you.\n\n1:11:41.380 --> 1:11:43.860\n Without cheating any part of the system.\n\n1:11:43.860 --> 1:11:45.620\n Yeah, I mean, if you do it,\n\n1:11:45.620 --> 1:11:46.700\n there are lots of ways you could cheat.\n\n1:11:46.700 --> 1:11:48.820\n You could build a Spartacus machine\n\n1:11:48.820 --> 1:11:50.260\n that works on that film.\n\n1:11:50.260 --> 1:11:51.100\n That's not what I'm talking about.\n\n1:11:51.100 --> 1:11:52.860\n I'm talking about, you can do this\n\n1:11:52.860 --> 1:11:54.860\n with essentially arbitrary films\n\n1:11:54.860 --> 1:11:56.580\n or from a large set. Even beyond films\n\n1:11:56.580 --> 1:11:58.980\n because it's possible such a system would discover\n\n1:11:58.980 --> 1:12:02.580\n that the number of narrative arcs in film\n\n1:12:02.580 --> 1:12:04.740\n is limited to 1930. Well, there's a famous thing\n\n1:12:04.740 --> 1:12:07.060\n about the classic seven plots or whatever.\n\n1:12:07.060 --> 1:12:07.900\n I don't care.\n\n1:12:07.900 --> 1:12:09.140\n If you wanna build in the system,\n\n1:12:09.140 --> 1:12:11.660\n boy meets girl, boy loses girl, boy finds girl.\n\n1:12:11.660 --> 1:12:12.500\n That's fine.\n\n1:12:12.500 --> 1:12:13.980\n I don't mind having some head stories on it.\n\n1:12:13.980 --> 1:12:14.820\n And they acknowledge.\n\n1:12:14.820 --> 1:12:16.340\n Okay, good.\n\n1:12:16.340 --> 1:12:17.980\n I mean, you could build it in innately\n\n1:12:17.980 --> 1:12:20.460\n or you could have your system watch a lot of films again.\n\n1:12:20.460 --> 1:12:22.380\n If you can do this at all,\n\n1:12:22.380 --> 1:12:23.740\n but with a wide range of films,\n\n1:12:23.740 --> 1:12:26.220\n not just one film in one genre.\n\n1:12:27.340 --> 1:12:28.860\n But even if you could do it for all Westerns,\n\n1:12:28.860 --> 1:12:30.300\n I'd be reasonably impressed.\n\n1:12:30.300 --> 1:12:31.940\n Yeah.\n\n1:12:31.940 --> 1:12:34.100\n So in terms of being impressed,\n\n1:12:34.100 --> 1:12:35.820\n just for the fun of it,\n\n1:12:35.820 --> 1:12:38.420\n because you've put so many interesting ideas out there\n\n1:12:38.420 --> 1:12:40.420\n in your book,\n\n1:12:40.420 --> 1:12:43.700\n challenging the community for further steps.\n\n1:12:43.700 --> 1:12:46.740\n Is it possible on the deep learning front\n\n1:12:46.740 --> 1:12:50.260\n that you're wrong about its limitations?\n\n1:12:50.260 --> 1:12:52.260\n That deep learning will unlock,\n\n1:12:52.260 --> 1:12:54.500\n Yann LeCun next year will publish a paper\n\n1:12:54.500 --> 1:12:56.940\n that achieves this comprehension.\n\n1:12:56.940 --> 1:13:00.300\n So do you think that way often as a scientist?\n\n1:13:00.300 --> 1:13:03.060\n Do you consider that your intuition\n\n1:13:03.060 --> 1:13:06.740\n that deep learning could actually run away with it?\n\n1:13:06.740 --> 1:13:09.780\n I'm more worried about rebranding\n\n1:13:09.780 --> 1:13:11.380\n as a kind of political thing.\n\n1:13:11.380 --> 1:13:14.100\n So, I mean, what's gonna happen, I think,\n\n1:13:14.100 --> 1:13:15.660\n is the deep learning is gonna start\n\n1:13:15.660 --> 1:13:17.380\n to encompass symbol manipulation.\n\n1:13:17.380 --> 1:13:19.260\n So I think Hinton's just wrong.\n\n1:13:19.260 --> 1:13:20.860\n Hinton says we don't want hybrids.\n\n1:13:20.860 --> 1:13:22.380\n I think people will work towards hybrids\n\n1:13:22.380 --> 1:13:24.740\n and they will relabel their hybrids as deep learning.\n\n1:13:24.740 --> 1:13:25.860\n We've already seen some of that.\n\n1:13:25.860 --> 1:13:29.620\n So AlphaGo is often described as a deep learning system,\n\n1:13:29.620 --> 1:13:31.740\n but it's more correctly described as a system\n\n1:13:31.740 --> 1:13:33.940\n that has deep learning, but also Monte Carlo tree search,\n\n1:13:33.940 --> 1:13:35.580\n which is a classical AI technique.\n\n1:13:35.580 --> 1:13:37.540\n And people will start to blur the lines\n\n1:13:37.540 --> 1:13:39.820\n in the way that IBM blurred Watson.\n\n1:13:39.820 --> 1:13:41.580\n First, Watson meant this particular system,\n\n1:13:41.580 --> 1:13:43.140\n and then it was just anything that IBM built\n\n1:13:43.140 --> 1:13:44.140\n in their cognitive division.\n\n1:13:44.140 --> 1:13:45.740\n But purely, let me ask, for sure,\n\n1:13:45.740 --> 1:13:49.500\n that's a branding question and that's like a giant mess.\n\n1:13:49.500 --> 1:13:51.940\n I mean, purely, a single neural network\n\n1:13:51.940 --> 1:13:54.060\n being able to accomplish reasonable comprehension.\n\n1:13:54.060 --> 1:13:55.780\n I don't stay up at night worrying\n\n1:13:55.780 --> 1:13:57.780\n that that's gonna happen.\n\n1:13:57.780 --> 1:13:59.220\n And I'll just give you two examples.\n\n1:13:59.220 --> 1:14:03.540\n One is a guy at DeepMind thought he had finally outfoxed me.\n\n1:14:03.540 --> 1:14:06.980\n At Zergilord, I think is his Twitter handle.\n\n1:14:06.980 --> 1:14:10.580\n And he said, he specifically made an example.\n\n1:14:10.580 --> 1:14:12.620\n Marcus said that such and such.\n\n1:14:12.620 --> 1:14:16.420\n He fed it into GP2, which is the AI system\n\n1:14:16.420 --> 1:14:19.060\n that is so smart that OpenAI couldn't release it\n\n1:14:19.060 --> 1:14:21.180\n because it would destroy the world, right?\n\n1:14:21.180 --> 1:14:22.940\n You remember that a few months ago.\n\n1:14:22.940 --> 1:14:27.220\n So he feeds it into GPT2, and my example\n\n1:14:27.220 --> 1:14:28.740\n was something like a rose is a rose,\n\n1:14:28.740 --> 1:14:31.340\n a tulip is a tulip, a lily is a blank.\n\n1:14:31.340 --> 1:14:32.860\n And he got it to actually do that,\n\n1:14:32.860 --> 1:14:34.020\n which was a little bit impressive.\n\n1:14:34.020 --> 1:14:35.340\n And I wrote back and I said, that's impressive,\n\n1:14:35.340 --> 1:14:37.740\n but can I ask you a few questions?\n\n1:14:37.740 --> 1:14:40.060\n I said, was that just one example?\n\n1:14:40.060 --> 1:14:41.620\n Can it do it generally?\n\n1:14:41.620 --> 1:14:43.220\n And can it do it with novel words,\n\n1:14:43.220 --> 1:14:45.300\n which was part of what I was talking about in 1998\n\n1:14:45.300 --> 1:14:46.740\n when I first raised the example.\n\n1:14:46.740 --> 1:14:49.380\n So a dax is a dax, right?\n\n1:14:50.340 --> 1:14:53.020\n And he sheepishly wrote back about 20 minutes later.\n\n1:14:53.020 --> 1:14:55.340\n And the answer was, well, it had some problems with those.\n\n1:14:55.340 --> 1:15:00.340\n So I made some predictions 21 years ago that still hold.\n\n1:15:00.500 --> 1:15:02.660\n In the world of computer science, that's amazing, right?\n\n1:15:02.660 --> 1:15:06.500\n Because there's a thousand or a million times more memory\n\n1:15:06.500 --> 1:15:10.020\n and computations a million times,\n\n1:15:10.020 --> 1:15:13.140\n do million times more operations per second\n\n1:15:13.140 --> 1:15:15.340\n spread across a cluster.\n\n1:15:15.340 --> 1:15:19.260\n And there's been advances in replacing sigmoids\n\n1:15:20.780 --> 1:15:23.380\n with other functions and so forth.\n\n1:15:23.380 --> 1:15:25.380\n There's all kinds of advances,\n\n1:15:25.380 --> 1:15:27.100\n but the fundamental architecture hasn't changed\n\n1:15:27.100 --> 1:15:28.580\n and the fundamental limit hasn't changed.\n\n1:15:28.580 --> 1:15:30.860\n And what I said then is kind of still true.\n\n1:15:30.860 --> 1:15:32.220\n Then here's a second example.\n\n1:15:32.220 --> 1:15:34.020\n I recently had a piece in Wired\n\n1:15:34.020 --> 1:15:35.260\n that's adapted from the book.\n\n1:15:35.260 --> 1:15:40.140\n And the book went to press before GP2 came out,\n\n1:15:40.140 --> 1:15:42.300\n but we described this children's story\n\n1:15:42.300 --> 1:15:45.580\n and all the inferences that you make in this story\n\n1:15:45.580 --> 1:15:48.260\n about a boy finding a lost wallet.\n\n1:15:48.260 --> 1:15:52.860\n And for fun, in the Wired piece, we ran it through GP2.\n\n1:15:52.860 --> 1:15:55.460\n GPT2, something called talktotransformer.com,\n\n1:15:55.460 --> 1:15:58.180\n and your viewers can try this experiment themselves.\n\n1:15:58.180 --> 1:15:59.700\n Go to the Wired piece that has the link\n\n1:15:59.700 --> 1:16:01.100\n and it has the story.\n\n1:16:01.100 --> 1:16:04.300\n And the system made perfectly fluent text\n\n1:16:04.300 --> 1:16:06.420\n that was totally inconsistent\n\n1:16:06.420 --> 1:16:10.260\n with the conceptual underpinnings of the story, right?\n\n1:16:10.260 --> 1:16:13.220\n This is what, again, I predicted in 1998.\n\n1:16:13.220 --> 1:16:14.700\n And for that matter, Chomsky and Miller\n\n1:16:14.700 --> 1:16:16.660\n made the same prediction in 1963.\n\n1:16:16.660 --> 1:16:19.420\n I was just updating their claim for a slightly new text.\n\n1:16:19.420 --> 1:16:22.580\n So those particular architectures\n\n1:16:22.580 --> 1:16:24.820\n that don't have any built in knowledge,\n\n1:16:24.820 --> 1:16:27.020\n they're basically just a bunch of layers\n\n1:16:27.020 --> 1:16:28.940\n doing correlational stuff.\n\n1:16:28.940 --> 1:16:31.220\n They're not gonna solve these problems.\n\n1:16:31.220 --> 1:16:34.500\n So 20 years ago, you said the emperor has no clothes.\n\n1:16:34.500 --> 1:16:36.860\n Today, the emperor still has no clothes.\n\n1:16:36.860 --> 1:16:38.020\n The lighting's better though.\n\n1:16:38.020 --> 1:16:39.020\n The lighting is better.\n\n1:16:39.020 --> 1:16:42.260\n And I think you yourself are also, I mean.\n\n1:16:42.260 --> 1:16:44.340\n And we found out some things to do with naked emperors.\n\n1:16:44.340 --> 1:16:46.420\n I mean, it's not like stuff is worthless.\n\n1:16:46.420 --> 1:16:48.260\n I mean, they're not really naked.\n\n1:16:48.260 --> 1:16:49.580\n It's more like they're in their briefs\n\n1:16:49.580 --> 1:16:50.820\n than everybody thinks they are.\n\n1:16:50.820 --> 1:16:54.340\n And so like, I mean, they are great at speech recognition,\n\n1:16:54.340 --> 1:16:56.460\n but the problems that I said were hard.\n\n1:16:56.460 --> 1:16:58.220\n I didn't literally say the emperor has no clothes.\n\n1:16:58.220 --> 1:17:00.140\n I said, this is a set of problems\n\n1:17:00.140 --> 1:17:01.780\n that humans are really good at.\n\n1:17:01.780 --> 1:17:03.140\n And it wasn't couched as AI.\n\n1:17:03.140 --> 1:17:04.300\n It was couched as cognitive science.\n\n1:17:04.300 --> 1:17:07.700\n But I said, if you wanna build a neural model\n\n1:17:07.700 --> 1:17:10.340\n of how humans do certain class of things,\n\n1:17:10.340 --> 1:17:11.940\n you're gonna have to change the architecture.\n\n1:17:11.940 --> 1:17:13.620\n And I stand by those claims.\n\n1:17:13.620 --> 1:17:16.740\n So, and I think people should understand\n\n1:17:16.740 --> 1:17:19.020\n you're quite entertaining in your cynicism,\n\n1:17:19.020 --> 1:17:22.220\n but you're also very optimistic and a dreamer\n\n1:17:22.220 --> 1:17:23.900\n about the future of AI too.\n\n1:17:23.900 --> 1:17:25.340\n So you're both, it's just.\n\n1:17:25.340 --> 1:17:27.820\n There's a famous saying about being,\n\n1:17:27.820 --> 1:17:30.700\n people overselling technology in the short run\n\n1:17:30.700 --> 1:17:34.100\n and underselling it in the long run.\n\n1:17:34.100 --> 1:17:37.180\n And so I actually end the book,\n\n1:17:37.180 --> 1:17:40.500\n Ernie Davis and I end our book with an optimistic chapter,\n\n1:17:40.500 --> 1:17:41.700\n which kind of killed Ernie\n\n1:17:41.700 --> 1:17:44.380\n because he's even more pessimistic than I am.\n\n1:17:44.380 --> 1:17:47.580\n He describes me as a contrarian and him as a pessimist.\n\n1:17:47.580 --> 1:17:49.820\n But I persuaded him that we should end the book\n\n1:17:49.820 --> 1:17:52.620\n with a look at what would happen\n\n1:17:52.620 --> 1:17:55.340\n if AI really did incorporate, for example,\n\n1:17:55.340 --> 1:17:57.300\n the common sense reasoning and the nativism\n\n1:17:57.300 --> 1:17:59.660\n and so forth, the things that we counseled for.\n\n1:17:59.660 --> 1:18:02.140\n And we wrote it and it's an optimistic chapter\n\n1:18:02.140 --> 1:18:05.900\n that AI suitably reconstructed so that we could trust it,\n\n1:18:05.900 --> 1:18:09.500\n which we can't now, could really be world changing.\n\n1:18:09.500 --> 1:18:13.100\n So on that point, if you look at the future trajectories\n\n1:18:13.100 --> 1:18:17.140\n of AI, people have worries about negative effects of AI,\n\n1:18:17.140 --> 1:18:21.020\n whether it's at the large existential scale\n\n1:18:21.020 --> 1:18:25.220\n or smaller short term scale of negative impact on society.\n\n1:18:25.220 --> 1:18:27.140\n So you write about trustworthy AI,\n\n1:18:27.140 --> 1:18:31.500\n how can we build AI systems that align with our values,\n\n1:18:31.500 --> 1:18:32.780\n that make for a better world,\n\n1:18:32.780 --> 1:18:34.980\n that we can interact with, that we can trust?\n\n1:18:34.980 --> 1:18:35.820\n The first thing we have to do\n\n1:18:35.820 --> 1:18:38.260\n is to replace deep learning with deep understanding.\n\n1:18:38.260 --> 1:18:42.460\n So you can't have alignment with a system\n\n1:18:42.460 --> 1:18:44.620\n that traffics only in correlations\n\n1:18:44.620 --> 1:18:47.900\n and doesn't understand concepts like bottles or harm.\n\n1:18:47.900 --> 1:18:51.340\n So Asimov talked about these famous laws\n\n1:18:51.340 --> 1:18:54.060\n and the first one was first do no harm.\n\n1:18:54.060 --> 1:18:56.860\n And you can quibble about the details of Asimov's laws,\n\n1:18:56.860 --> 1:18:58.780\n but we have to, if we're gonna build real robots\n\n1:18:58.780 --> 1:19:00.540\n in the real world, have something like that.\n\n1:19:00.540 --> 1:19:02.500\n That means we have to program in a notion\n\n1:19:02.500 --> 1:19:04.240\n that's at least something like harm.\n\n1:19:04.240 --> 1:19:06.620\n That means we have to have these more abstract ideas\n\n1:19:06.620 --> 1:19:08.460\n that deep learning is not particularly good at.\n\n1:19:08.460 --> 1:19:10.620\n They have to be in the mix somewhere.\n\n1:19:10.620 --> 1:19:12.380\n And you could do statistical analysis\n\n1:19:12.380 --> 1:19:14.380\n about probabilities of given harms or whatever,\n\n1:19:14.380 --> 1:19:15.820\n but you have to know what a harm is\n\n1:19:15.820 --> 1:19:17.420\n in the same way that you have to understand\n\n1:19:17.420 --> 1:19:19.780\n that a bottle isn't just a collection of pixels.\n\n1:19:20.660 --> 1:19:24.020\n And also be able to, you're implying\n\n1:19:24.020 --> 1:19:25.940\n that you need to also be able to communicate\n\n1:19:25.940 --> 1:19:29.700\n that to humans so the AI systems would be able\n\n1:19:29.700 --> 1:19:33.780\n to prove to humans that they understand\n\n1:19:33.780 --> 1:19:35.460\n that they know what harm means.\n\n1:19:35.460 --> 1:19:37.380\n I might run it in the reverse direction,\n\n1:19:37.380 --> 1:19:38.620\n but roughly speaking, I agree with you.\n\n1:19:38.620 --> 1:19:42.500\n So we probably need to have committees\n\n1:19:42.500 --> 1:19:45.660\n of wise people, ethicists and so forth.\n\n1:19:45.660 --> 1:19:47.500\n Think about what these rules ought to be\n\n1:19:47.500 --> 1:19:49.780\n and we shouldn't just leave it to software engineers.\n\n1:19:49.780 --> 1:19:51.620\n It shouldn't just be software engineers\n\n1:19:51.620 --> 1:19:53.900\n and it shouldn't just be people\n\n1:19:53.900 --> 1:19:56.580\n who own large mega corporations\n\n1:19:56.580 --> 1:19:58.860\n that are good at technology, ethicists\n\n1:19:58.860 --> 1:20:00.260\n and so forth should be involved.\n\n1:20:00.260 --> 1:20:04.660\n But there should be some assembly of wise people\n\n1:20:04.660 --> 1:20:07.220\n as I was putting it that tries to figure out\n\n1:20:07.220 --> 1:20:08.700\n what the rules ought to be.\n\n1:20:08.700 --> 1:20:11.580\n And those have to get translated into code.\n\n1:20:12.460 --> 1:20:15.460\n You can argue or code or neural networks or something.\n\n1:20:15.460 --> 1:20:18.660\n They have to be translated into something\n\n1:20:18.660 --> 1:20:19.980\n that machines can work with.\n\n1:20:19.980 --> 1:20:21.940\n And that means there has to be a way\n\n1:20:21.940 --> 1:20:23.380\n of working the translation.\n\n1:20:23.380 --> 1:20:24.460\n And right now we don't.\n\n1:20:24.460 --> 1:20:25.340\n We don't have a way.\n\n1:20:25.340 --> 1:20:27.060\n So let's say you and I were the committee\n\n1:20:27.060 --> 1:20:29.820\n and we decide that Asimov's first law is actually right.\n\n1:20:29.820 --> 1:20:31.580\n And let's say it's not just two white guys,\n\n1:20:31.580 --> 1:20:34.020\n which would be kind of unfortunate that we have abroad.\n\n1:20:34.020 --> 1:20:36.260\n And so we've representative sample of the world\n\n1:20:36.260 --> 1:20:37.500\n or however we wanna do this.\n\n1:20:37.500 --> 1:20:40.460\n And the committee decides eventually,\n\n1:20:40.460 --> 1:20:42.820\n okay, Asimov's first law is actually pretty good.\n\n1:20:42.820 --> 1:20:44.060\n There are these exceptions to it.\n\n1:20:44.060 --> 1:20:46.060\n We wanna program in these exceptions.\n\n1:20:46.060 --> 1:20:47.460\n But let's start with just the first one\n\n1:20:47.460 --> 1:20:48.860\n and then we'll get to the exceptions.\n\n1:20:48.860 --> 1:20:50.620\n First one is first do no harm.\n\n1:20:50.620 --> 1:20:53.620\n Well, somebody has to now actually turn that into\n\n1:20:53.620 --> 1:20:56.220\n a computer program or a neural network or something.\n\n1:20:56.220 --> 1:20:58.740\n And one way of taking the whole book,\n\n1:20:58.740 --> 1:21:00.260\n the whole argument that I'm making\n\n1:21:00.260 --> 1:21:02.500\n is that we just don't have to do that yet.\n\n1:21:02.500 --> 1:21:03.540\n And we're fooling ourselves\n\n1:21:03.540 --> 1:21:05.860\n if we think that we can build trustworthy AI\n\n1:21:05.860 --> 1:21:09.500\n if we can't even specify in any kind of,\n\n1:21:09.500 --> 1:21:13.140\n we can't do it in Python and we can't do it in TensorFlow.\n\n1:21:13.140 --> 1:21:14.380\n We're fooling ourselves in thinking\n\n1:21:14.380 --> 1:21:15.820\n that we can make trustworthy AI\n\n1:21:15.820 --> 1:21:18.780\n if we can't translate harm into something\n\n1:21:18.780 --> 1:21:19.940\n that we can execute.\n\n1:21:19.940 --> 1:21:22.820\n And if we can't, then we should be thinking really hard\n\n1:21:22.820 --> 1:21:24.620\n how could we ever do such a thing?\n\n1:21:24.620 --> 1:21:26.500\n Because if we're gonna use AI\n\n1:21:26.500 --> 1:21:27.940\n in the ways that we wanna use it,\n\n1:21:27.940 --> 1:21:31.060\n to make job interviews or to do surveillance,\n\n1:21:31.060 --> 1:21:32.460\n not that I personally wanna do that or whatever.\n\n1:21:32.460 --> 1:21:33.780\n I mean, if we're gonna use AI\n\n1:21:33.780 --> 1:21:36.180\n in ways that have practical impact on people's lives\n\n1:21:36.180 --> 1:21:38.980\n or medicine, it's gotta be able\n\n1:21:38.980 --> 1:21:41.180\n to understand stuff like that.\n\n1:21:41.180 --> 1:21:42.820\n So one of the things your book highlights\n\n1:21:42.820 --> 1:21:47.380\n is that a lot of people in the deep learning community,\n\n1:21:47.380 --> 1:21:50.220\n but also the general public, politicians,\n\n1:21:50.220 --> 1:21:53.220\n just people in all general groups and walks of life\n\n1:21:53.220 --> 1:21:57.340\n have different levels of misunderstanding of AI.\n\n1:21:57.340 --> 1:21:59.460\n So when you talk about committees,\n\n1:22:00.940 --> 1:22:05.620\n what's your advice to our society?\n\n1:22:05.620 --> 1:22:08.140\n How do we grow, how do we learn about AI\n\n1:22:08.140 --> 1:22:10.820\n such that such committees could emerge\n\n1:22:10.820 --> 1:22:13.500\n where large groups of people could have\n\n1:22:13.500 --> 1:22:15.180\n a productive discourse about\n\n1:22:15.180 --> 1:22:17.820\n how to build successful AI systems?\n\n1:22:17.820 --> 1:22:19.660\n Part of the reason we wrote the book\n\n1:22:19.660 --> 1:22:22.060\n was to try to inform those committees.\n\n1:22:22.060 --> 1:22:23.540\n So part of the reason we wrote the book\n\n1:22:23.540 --> 1:22:25.660\n was to inspire a future generation of students\n\n1:22:25.660 --> 1:22:27.860\n to solve what we think are the important problems.\n\n1:22:27.860 --> 1:22:29.860\n So a lot of the book is trying to pinpoint\n\n1:22:29.860 --> 1:22:31.220\n what we think are the hard problems\n\n1:22:31.220 --> 1:22:34.020\n where we think effort would most be rewarded.\n\n1:22:34.020 --> 1:22:36.740\n And part of it is to try to train people\n\n1:22:37.780 --> 1:22:41.020\n who talk about AI, but aren't experts in the field\n\n1:22:41.020 --> 1:22:43.500\n to understand what's realistic and what's not.\n\n1:22:43.500 --> 1:22:44.660\n One of my favorite parts in the book\n\n1:22:44.660 --> 1:22:46.940\n is the six questions you should ask\n\n1:22:46.940 --> 1:22:48.380\n anytime you read a media account.\n\n1:22:48.380 --> 1:22:51.060\n So like number one is if somebody talks about something,\n\n1:22:51.060 --> 1:22:51.900\n look for the demo.\n\n1:22:51.900 --> 1:22:54.100\n If there's no demo, don't believe it.\n\n1:22:54.100 --> 1:22:55.300\n Like the demo that you can try.\n\n1:22:55.300 --> 1:22:56.460\n If you can't try it at home,\n\n1:22:56.460 --> 1:22:58.380\n maybe it doesn't really work that well yet.\n\n1:22:58.380 --> 1:23:00.620\n So if, we don't have this example in the book,\n\n1:23:00.620 --> 1:23:04.140\n but if Sundar Pinchai says we have this thing\n\n1:23:04.140 --> 1:23:08.380\n that allows it to sound like human beings in conversation,\n\n1:23:08.380 --> 1:23:10.380\n you should ask, can I try it?\n\n1:23:10.380 --> 1:23:11.860\n And you should ask how general it is.\n\n1:23:11.860 --> 1:23:13.060\n And it turns out at that time,\n\n1:23:13.060 --> 1:23:15.460\n I'm alluding to Google Duplex when it was announced,\n\n1:23:15.460 --> 1:23:18.220\n it only worked on calling hairdressers,\n\n1:23:18.220 --> 1:23:20.020\n restaurants and finding opening hours.\n\n1:23:20.020 --> 1:23:22.260\n That's not very general, that's narrow AI.\n\n1:23:22.260 --> 1:23:24.580\n And I'm not gonna ask your thoughts about Sophia,\n\n1:23:24.580 --> 1:23:27.740\n but yeah, I understand that's a really good question\n\n1:23:27.740 --> 1:23:30.220\n to ask of any kind of hype top idea.\n\n1:23:30.220 --> 1:23:32.580\n Sophia has very good material written for her,\n\n1:23:32.580 --> 1:23:35.380\n but she doesn't understand the things that she's saying.\n\n1:23:35.380 --> 1:23:38.220\n So a while ago you've written a book\n\n1:23:38.220 --> 1:23:40.540\n on the science of learning, which I think is fascinating,\n\n1:23:40.540 --> 1:23:43.500\n but the learning case studies of playing guitar.\n\n1:23:43.500 --> 1:23:45.100\n That's called Guitar Zero.\n\n1:23:45.100 --> 1:23:47.340\n I love guitar myself, I've been playing my whole life.\n\n1:23:47.340 --> 1:23:50.260\n So let me ask a very important question.\n\n1:23:50.260 --> 1:23:53.500\n What is your favorite song, rock song,\n\n1:23:53.500 --> 1:23:56.300\n to listen to or try to play?\n\n1:23:56.300 --> 1:23:57.140\n Well, those would be different,\n\n1:23:57.140 --> 1:23:59.660\n but I'll say that my favorite rock song to listen to\n\n1:23:59.660 --> 1:24:01.060\n is probably All Along the Watchtower,\n\n1:24:01.060 --> 1:24:01.980\n the Jimi Hendrix version.\n\n1:24:01.980 --> 1:24:02.980\n The Jimi Hendrix version.\n\n1:24:02.980 --> 1:24:04.860\n It feels magic to me.\n\n1:24:04.860 --> 1:24:07.040\n I've actually recently learned it, I love that song.\n\n1:24:07.040 --> 1:24:09.380\n I've been trying to put it on YouTube, myself singing.\n\n1:24:09.380 --> 1:24:11.300\n Singing is the scary part.\n\n1:24:11.300 --> 1:24:13.380\n If you could party with a rock star for a weekend,\n\n1:24:13.380 --> 1:24:15.180\n living or dead, who would you choose?\n\n1:24:17.780 --> 1:24:21.140\n And pick their mind, it's not necessarily about the partying.\n\n1:24:21.140 --> 1:24:22.700\n Thanks for the clarification.\n\n1:24:24.700 --> 1:24:26.980\n I guess John Lennon's such an intriguing person,\n\n1:24:26.980 --> 1:24:31.660\n and I think a troubled person, but an intriguing one.\n\n1:24:31.660 --> 1:24:32.500\n Beautiful.\n\n1:24:32.500 --> 1:24:35.460\n Well, Imagine is one of my favorite songs.\n\n1:24:35.460 --> 1:24:37.100\n Also one of my favorite songs.\n\n1:24:37.100 --> 1:24:38.300\n That's a beautiful way to end it.\n\n1:24:38.300 --> 1:24:39.780\n Gary, thank you so much for talking to me.\n\n1:24:39.780 --> 1:25:08.780\n Thanks so much for having me.\n\n"
}