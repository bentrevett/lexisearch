{
  "title": "Rajat Monga: TensorFlow | Lex Fridman Podcast #22",
  "id": "NERNE4UThHU",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:03.080\n The following is a conversation with Rajat Manga.\n\n00:03.080 --> 00:04.920\n He's an engineer and director of Google,\n\n00:04.920 --> 00:06.960\n leading the TensorFlow team.\n\n00:06.960 --> 00:09.160\n TensorFlow is an open source library\n\n00:09.160 --> 00:11.540\n at the center of much of the work going on in the world\n\n00:11.540 --> 00:14.040\n in deep learning, both the cutting edge research\n\n00:14.040 --> 00:17.720\n and the large scale application of learning based approaches.\n\n00:17.720 --> 00:20.940\n But it's quickly becoming much more than a software library.\n\n00:20.940 --> 00:24.120\n It's now an ecosystem of tools for the deployment of machine\n\n00:24.120 --> 00:26.800\n learning in the cloud, on the phone, in the browser,\n\n00:26.800 --> 00:29.840\n on both generic and specialized hardware.\n\n00:29.840 --> 00:31.960\n TPU, GPU, and so on.\n\n00:31.960 --> 00:35.220\n Plus, there's a big emphasis on growing a passionate community\n\n00:35.220 --> 00:36.640\n of developers.\n\n00:36.640 --> 00:39.820\n Rajat, Jeff Dean, and a large team of engineers at Google\n\n00:39.820 --> 00:42.200\n Brain are working to define the future of machine\n\n00:42.200 --> 00:46.240\n learning with TensorFlow 2.0, which is now in alpha.\n\n00:46.240 --> 00:49.160\n I think the decision to open source TensorFlow\n\n00:49.160 --> 00:51.760\n is a definitive moment in the tech industry.\n\n00:51.760 --> 00:54.400\n It showed that open innovation can be successful\n\n00:54.400 --> 00:56.920\n and inspire many companies to open source their code,\n\n00:56.920 --> 00:58.880\n to publish, and in general engage\n\n00:58.880 --> 01:01.240\n in the open exchange of ideas.\n\n01:01.240 --> 01:03.940\n This conversation is part of the Artificial Intelligence\n\n01:03.940 --> 01:05.080\n podcast.\n\n01:05.080 --> 01:07.860\n If you enjoy it, subscribe on YouTube, iTunes,\n\n01:07.860 --> 01:10.880\n or simply connect with me on Twitter at Lex Friedman,\n\n01:10.880 --> 01:12.720\n spelled F R I D.\n\n01:12.720 --> 01:17.960\n And now, here's my conversation with Rajat Manga.\n\n01:17.960 --> 01:22.520\n You were involved with Google Brain since its start in 2011\n\n01:22.520 --> 01:24.880\n with Jeff Dean.\n\n01:24.880 --> 01:29.220\n It started with this belief, the proprietary machine learning\n\n01:29.220 --> 01:32.800\n library, and turned into TensorFlow in 2014,\n\n01:32.800 --> 01:35.720\n the open source library.\n\n01:35.720 --> 01:39.120\n So what were the early days of Google Brain like?\n\n01:39.120 --> 01:41.840\n What were the goals, the missions?\n\n01:41.840 --> 01:45.120\n How do you even proceed forward once there's\n\n01:45.120 --> 01:47.760\n so much possibilities before you?\n\n01:47.760 --> 01:50.560\n It was interesting back then when I started,\n\n01:50.560 --> 01:55.400\n or when you were even just talking about it,\n\n01:55.400 --> 01:59.520\n the idea of deep learning was interesting and intriguing\n\n01:59.520 --> 02:00.480\n in some ways.\n\n02:00.480 --> 02:04.920\n It hadn't yet taken off, but it held some promise.\n\n02:04.920 --> 02:08.740\n It had shown some very promising and early results.\n\n02:08.740 --> 02:11.400\n I think the idea where Andrew and Jeff had started\n\n02:11.400 --> 02:15.440\n was, what if we can take this work people are doing\n\n02:15.440 --> 02:18.800\n in research and scale it to what Google has\n\n02:18.800 --> 02:23.000\n in terms of the compute power, and also\n\n02:23.000 --> 02:24.320\n put that kind of data together?\n\n02:24.320 --> 02:25.320\n What does it mean?\n\n02:25.320 --> 02:28.300\n And so far, the results had been, if you scale the compute,\n\n02:28.300 --> 02:30.200\n scale the data, it does better.\n\n02:30.200 --> 02:31.520\n And would that work?\n\n02:31.520 --> 02:35.140\n And so that was the first year or two, can we prove that out?\n\n02:35.140 --> 02:37.480\n And with this belief, when we started the first year,\n\n02:37.480 --> 02:40.800\n we got some early wins, which is always great.\n\n02:40.800 --> 02:41.960\n What were the wins like?\n\n02:41.960 --> 02:44.160\n What was the wins where you were,\n\n02:44.160 --> 02:46.640\n there's some problems to this, this is going to be good?\n\n02:46.640 --> 02:49.680\n I think there are two early wins where one was speech,\n\n02:49.680 --> 02:52.280\n that we collaborated very closely with the speech research\n\n02:52.280 --> 02:54.820\n team, who was also getting interested in this.\n\n02:54.820 --> 02:58.800\n And the other one was on images, where the cat paper,\n\n02:58.800 --> 03:03.160\n as we call it, that was covered by a lot of folks.\n\n03:03.160 --> 03:07.440\n And the birth of Google Brain was around neural networks.\n\n03:07.440 --> 03:09.320\n So it was deep learning from the very beginning.\n\n03:09.320 --> 03:10.800\n That was the whole mission.\n\n03:10.800 --> 03:15.040\n So what would, in terms of scale,\n\n03:15.040 --> 03:20.040\n what was the sort of dream of what this could become?\n\n03:21.080 --> 03:24.280\n Were there echoes of this open source TensorFlow community\n\n03:24.280 --> 03:26.240\n that might be brought in?\n\n03:26.240 --> 03:28.640\n Was there a sense of TPUs?\n\n03:28.640 --> 03:31.760\n Was there a sense of machine learning is now going to be\n\n03:31.760 --> 03:33.720\n at the core of the entire company,\n\n03:33.720 --> 03:36.040\n is going to grow into that direction?\n\n03:36.040 --> 03:38.320\n Yeah, I think, so that was interesting.\n\n03:38.320 --> 03:41.380\n And if I think back to 2012 or 2011,\n\n03:41.380 --> 03:45.240\n and first was can we scale it in the year or so,\n\n03:45.240 --> 03:47.520\n we had started scaling it to hundreds and thousands\n\n03:47.520 --> 03:48.360\n of machines.\n\n03:48.360 --> 03:51.080\n In fact, we had some runs even going to 10,000 machines.\n\n03:51.080 --> 03:52.880\n And all of those shows great promise.\n\n03:53.880 --> 03:56.800\n In terms of machine learning at Google,\n\n03:56.800 --> 03:58.780\n the good thing was Google's been doing machine learning\n\n03:58.780 --> 04:00.240\n for a long time.\n\n04:00.240 --> 04:03.760\n Deep learning was new, but as we scaled this up,\n\n04:03.760 --> 04:05.600\n we showed that, yes, that was possible.\n\n04:05.600 --> 04:07.840\n And it was going to impact lots of things.\n\n04:07.840 --> 04:11.200\n Like we started seeing real products wanting to use this.\n\n04:11.200 --> 04:13.800\n Again, speech was the first, there were image things\n\n04:13.800 --> 04:17.400\n that photos came out of and then many other products as well.\n\n04:17.400 --> 04:18.920\n So that was exciting.\n\n04:20.180 --> 04:23.160\n As we went into that a couple of years,\n\n04:23.160 --> 04:25.800\n externally also academia started to,\n\n04:25.800 --> 04:27.200\n there was lots of push on, okay,\n\n04:27.200 --> 04:28.320\n deep learning is interesting,\n\n04:28.320 --> 04:30.600\n we should be doing more and so on.\n\n04:30.600 --> 04:34.580\n And so by 2014, we were looking at, okay,\n\n04:34.580 --> 04:36.780\n this is a big thing, it's going to grow.\n\n04:36.780 --> 04:39.440\n And not just internally, externally as well.\n\n04:39.440 --> 04:42.280\n Yes, maybe Google's ahead of where everybody is,\n\n04:42.280 --> 04:43.640\n but there's a lot to do.\n\n04:43.640 --> 04:46.720\n So a lot of this started to make sense and come together.\n\n04:46.720 --> 04:49.560\n So the decision to open source,\n\n04:49.560 --> 04:52.200\n I was just chatting with Chris Glatner about this.\n\n04:52.200 --> 04:54.640\n The decision to go open source with TensorFlow,\n\n04:54.640 --> 04:57.080\n I would say sort of for me personally,\n\n04:57.080 --> 04:59.640\n seems to be one of the big seminal moments\n\n04:59.640 --> 05:01.720\n in all of software engineering ever.\n\n05:01.720 --> 05:04.620\n I think that's when a large company like Google\n\n05:04.620 --> 05:07.520\n decides to take a large project that many lawyers\n\n05:07.520 --> 05:10.800\n might argue has a lot of IP,\n\n05:10.800 --> 05:12.900\n just decide to go open source with it,\n\n05:12.900 --> 05:14.880\n and in so doing lead the entire world\n\n05:14.880 --> 05:16.520\n and saying, you know what, open innovation\n\n05:16.520 --> 05:20.780\n is a pretty powerful thing, and it's okay to do.\n\n05:22.360 --> 05:26.320\n That was, I mean, that's an incredible moment in time.\n\n05:26.320 --> 05:29.320\n So do you remember those discussions happening?\n\n05:29.320 --> 05:31.400\n Whether open source should be happening?\n\n05:31.400 --> 05:32.680\n What was that like?\n\n05:32.680 --> 05:36.880\n I would say, I think, so the initial idea came from Jeff,\n\n05:36.880 --> 05:39.440\n who was a big proponent of this.\n\n05:39.440 --> 05:42.480\n I think it came off of two big things.\n\n05:42.480 --> 05:46.320\n One was research wise, we were a research group.\n\n05:46.320 --> 05:49.640\n We were putting all our research out there.\n\n05:49.640 --> 05:51.720\n If you wanted to, we were building on others research\n\n05:51.720 --> 05:55.000\n and we wanted to push the state of the art forward.\n\n05:55.000 --> 05:56.840\n And part of that was to share the research.\n\n05:56.840 --> 05:58.960\n That's how I think deep learning and machine learning\n\n05:58.960 --> 06:00.460\n has really grown so fast.\n\n06:01.380 --> 06:03.360\n So the next step was, okay, now,\n\n06:03.360 --> 06:05.360\n would software help with that?\n\n06:05.360 --> 06:08.440\n And it seemed like they were existing\n\n06:08.440 --> 06:11.280\n a few libraries out there, Tiano being one,\n\n06:11.280 --> 06:14.000\n Torch being another, and a few others,\n\n06:14.000 --> 06:15.480\n but they were all done by academia\n\n06:15.480 --> 06:18.040\n and so the level was significantly different.\n\n06:18.960 --> 06:22.000\n The other one was from a software perspective,\n\n06:22.000 --> 06:23.880\n Google had done lots of software\n\n06:23.880 --> 06:27.080\n or that we used internally, you know,\n\n06:27.080 --> 06:29.080\n and we published papers.\n\n06:29.080 --> 06:31.680\n Often there was an open source project\n\n06:31.680 --> 06:33.600\n that came out of that that somebody else\n\n06:33.600 --> 06:35.400\n picked up that paper and implemented\n\n06:35.400 --> 06:37.020\n and they were very successful.\n\n06:38.240 --> 06:41.440\n Back then it was like, okay, there's Hadoop,\n\n06:41.440 --> 06:44.140\n which has come off of tech that we've built.\n\n06:44.140 --> 06:46.200\n We know the tech we've built is way better\n\n06:46.200 --> 06:47.880\n for a number of different reasons.\n\n06:47.880 --> 06:50.420\n We've invested a lot of effort in that.\n\n06:51.660 --> 06:54.320\n And turns out we have Google Cloud\n\n06:54.320 --> 06:57.520\n and we are now not really providing our tech,\n\n06:57.520 --> 07:00.360\n but we are saying, okay, we have Bigtable,\n\n07:00.360 --> 07:02.040\n which is the original thing.\n\n07:02.040 --> 07:03.880\n We are going to now provide H base APIs\n\n07:03.880 --> 07:06.040\n on top of that, which isn't as good,\n\n07:06.040 --> 07:07.480\n but that's what everybody's used to.\n\n07:07.480 --> 07:10.040\n So there's like, can we make something\n\n07:10.040 --> 07:12.320\n that is better and really just provide,\n\n07:12.320 --> 07:14.320\n helps the community in lots of ways,\n\n07:14.320 --> 07:18.320\n but also helps push a good standard forward.\n\n07:18.320 --> 07:19.940\n So how does Cloud fit into that?\n\n07:19.940 --> 07:22.680\n There's a TensorFlow open source library\n\n07:22.680 --> 07:24.760\n and how does the fact that you can\n\n07:25.800 --> 07:28.240\n use so many of the resources that Google provides\n\n07:28.240 --> 07:31.100\n and the Cloud fit into that strategy?\n\n07:31.100 --> 07:33.600\n So TensorFlow itself is open\n\n07:33.600 --> 07:34.920\n and you can use it anywhere, right?\n\n07:34.920 --> 07:38.360\n And we want to make sure that continues to be the case.\n\n07:38.360 --> 07:41.040\n On Google Cloud, we do make sure\n\n07:41.040 --> 07:43.840\n that there's lots of integrations with everything else\n\n07:43.840 --> 07:44.880\n and we want to make sure\n\n07:44.880 --> 07:47.320\n that it works really, really well there.\n\n07:47.320 --> 07:50.400\n You're leading the TensorFlow effort.\n\n07:50.400 --> 07:51.280\n Can you tell me the history\n\n07:51.280 --> 07:53.600\n and the timeline of TensorFlow project\n\n07:53.600 --> 07:55.880\n in terms of major design decisions,\n\n07:55.880 --> 07:58.160\n so like the open source decision,\n\n07:58.160 --> 08:01.600\n but really what to include and not?\n\n08:01.600 --> 08:03.200\n There's this incredible ecosystem\n\n08:03.200 --> 08:04.760\n that I'd like to talk about.\n\n08:04.760 --> 08:05.720\n There's all these parts,\n\n08:05.720 --> 08:10.720\n but what if just some sample moments\n\n08:11.240 --> 08:15.040\n that defined what TensorFlow eventually became\n\n08:15.040 --> 08:17.640\n through its, I don't know if you're allowed to say history\n\n08:17.640 --> 08:20.240\n when it's just, but in deep learning,\n\n08:20.240 --> 08:21.280\n everything moves so fast\n\n08:21.280 --> 08:23.460\n and just a few years is already history.\n\n08:23.460 --> 08:28.460\n Yes, yes, so looking back, we were building TensorFlow.\n\n08:29.780 --> 08:34.240\n I guess we open sourced it in 2015, November 2015.\n\n08:34.240 --> 08:38.600\n We started on it in summer of 2014, I guess.\n\n08:39.780 --> 08:42.960\n And somewhere like three to six, late 2014,\n\n08:42.960 --> 08:45.120\n by then we had decided that, okay,\n\n08:45.120 --> 08:47.080\n there's a high likelihood we'll open source it.\n\n08:47.080 --> 08:48.880\n So we started thinking about that\n\n08:48.880 --> 08:53.880\n and making sure we're heading down that path.\n\n08:53.960 --> 08:56.080\n At that point, by that point,\n\n08:56.080 --> 08:59.320\n we had seen a few, lots of different use cases at Google.\n\n08:59.320 --> 09:01.000\n So there were things like, okay,\n\n09:01.000 --> 09:04.200\n yes, you wanna run it at large scale in the data center.\n\n09:04.200 --> 09:07.560\n Yes, we need to support different kind of hardware.\n\n09:07.560 --> 09:09.440\n We had GPUs at that point.\n\n09:09.440 --> 09:11.880\n We had our first GPU at that point\n\n09:11.880 --> 09:14.700\n or was about to come out roughly around that time.\n\n09:15.700 --> 09:18.700\n So the design sort of included those.\n\n09:18.700 --> 09:21.800\n We had started to push on mobile.\n\n09:21.800 --> 09:24.920\n So we were running models on mobile.\n\n09:24.920 --> 09:28.160\n At that point, people were customizing code.\n\n09:28.160 --> 09:29.560\n So we wanted to make sure TensorFlow\n\n09:29.560 --> 09:30.700\n could support that as well.\n\n09:30.700 --> 09:35.260\n So that sort of became part of that overall design.\n\n09:35.260 --> 09:36.560\n When you say mobile,\n\n09:36.560 --> 09:38.680\n you mean like a pretty complicated algorithms\n\n09:38.680 --> 09:40.040\n running on the phone?\n\n09:40.040 --> 09:40.880\n That's correct.\n\n09:40.880 --> 09:44.320\n So when you have a model that you deploy on the phone\n\n09:44.320 --> 09:45.160\n and run it there, right?\n\n09:45.160 --> 09:46.420\n So already at that time,\n\n09:46.420 --> 09:48.800\n there was ideas of running machine learning on the phone.\n\n09:48.800 --> 09:49.640\n That's correct.\n\n09:49.640 --> 09:51.400\n We already had a couple of products\n\n09:51.400 --> 09:53.260\n that were doing that by then.\n\n09:53.260 --> 09:54.500\n And in those cases,\n\n09:54.500 --> 09:57.540\n we had basically customized handcrafted code\n\n09:57.540 --> 10:00.160\n or some internal libraries that we're using.\n\n10:00.160 --> 10:02.600\n So I was actually at Google during this time\n\n10:02.600 --> 10:04.560\n in a parallel, I guess, universe,\n\n10:04.560 --> 10:07.140\n but we were using Theano and Caffe.\n\n10:09.240 --> 10:11.600\n Was there some degree to which you were bouncing,\n\n10:11.600 --> 10:15.520\n like trying to see what Caffe was offering people,\n\n10:15.520 --> 10:17.960\n trying to see what Theano was offering\n\n10:17.960 --> 10:19.960\n that you want to make sure you're delivering\n\n10:19.960 --> 10:21.640\n on whatever that is?\n\n10:21.640 --> 10:23.720\n Perhaps the Python part of thing,\n\n10:23.720 --> 10:27.520\n maybe did that influence any design decisions?\n\n10:27.520 --> 10:28.360\n Totally.\n\n10:28.360 --> 10:29.600\n So when we built this belief\n\n10:29.600 --> 10:31.600\n and some of that was in parallel\n\n10:31.600 --> 10:33.400\n with some of these libraries coming up,\n\n10:33.400 --> 10:35.400\n I mean, Theano itself is older,\n\n10:36.680 --> 10:39.880\n but we were building this belief\n\n10:39.880 --> 10:41.160\n focused on our internal thing\n\n10:41.160 --> 10:42.960\n because our systems were very different.\n\n10:42.960 --> 10:44.080\n By the time we got to this,\n\n10:44.080 --> 10:47.120\n we looked at a number of libraries that were out there.\n\n10:47.120 --> 10:49.280\n Theano, there were folks in the group\n\n10:49.280 --> 10:52.140\n who had experience with Torch, with Lua.\n\n10:52.140 --> 10:54.800\n There were folks here who had seen Caffe.\n\n10:54.800 --> 10:57.540\n I mean, actually, Yang Jing was here as well.\n\n10:58.840 --> 11:02.980\n There's what other libraries?\n\n11:02.980 --> 11:04.920\n I think we looked at a number of things.\n\n11:04.920 --> 11:06.840\n Might even have looked at JNR back then.\n\n11:06.840 --> 11:09.400\n I'm trying to remember if it was there.\n\n11:09.400 --> 11:12.040\n In fact, yeah, we did discuss ideas around,\n\n11:12.040 --> 11:14.240\n okay, should we have a graph or not?\n\n11:17.840 --> 11:20.480\n So putting all these together was definitely,\n\n11:20.480 --> 11:22.800\n they were key decisions that we wanted.\n\n11:22.800 --> 11:27.220\n We had seen limitations in our prior disbelief things.\n\n11:28.800 --> 11:31.360\n A few of them were just in terms of research\n\n11:31.360 --> 11:33.740\n was moving so fast, we wanted the flexibility.\n\n11:35.040 --> 11:36.360\n The hardware was changing fast.\n\n11:36.360 --> 11:37.760\n We expected to change that\n\n11:37.760 --> 11:39.900\n so that those probably were two things.\n\n11:39.900 --> 11:43.140\n And yeah, I think the flexibility\n\n11:43.140 --> 11:44.380\n in terms of being able to express\n\n11:44.380 --> 11:46.980\n all kinds of crazy things was definitely a big one then.\n\n11:46.980 --> 11:49.020\n So what, the graph decisions though,\n\n11:49.020 --> 11:52.460\n with moving towards TensorFlow 2.0,\n\n11:52.460 --> 11:56.800\n there's more, by default, there'll be eager execution.\n\n11:56.800 --> 11:59.260\n So sort of hiding the graph a little bit\n\n11:59.260 --> 12:00.660\n because it's less intuitive\n\n12:00.660 --> 12:03.660\n in terms of the way people develop and so on.\n\n12:03.660 --> 12:06.800\n What was that discussion like in terms of using graphs?\n\n12:06.800 --> 12:09.420\n It seemed, it's kind of the Theano way.\n\n12:09.420 --> 12:11.660\n Did it seem the obvious choice?\n\n12:11.660 --> 12:15.780\n So I think where it came from was our disbelief\n\n12:15.780 --> 12:17.700\n had a graph like thing as well.\n\n12:17.700 --> 12:19.780\n A much more simple, it wasn't a general graph,\n\n12:19.780 --> 12:21.880\n it was more like a straight line thing.\n\n12:23.220 --> 12:25.060\n More like what you might think of cafe,\n\n12:25.060 --> 12:26.440\n I guess in that sense.\n\n12:26.440 --> 12:28.900\n But the graph was,\n\n12:28.900 --> 12:31.180\n and we always cared about the production stuff.\n\n12:31.180 --> 12:32.020\n Like even with disbelief,\n\n12:32.020 --> 12:34.500\n we were deploying a whole bunch of stuff in production.\n\n12:34.500 --> 12:37.460\n So graph did come from that when we thought of,\n\n12:37.460 --> 12:39.420\n okay, should we do that in Python?\n\n12:39.420 --> 12:40.900\n And we experimented with some ideas\n\n12:40.900 --> 12:43.880\n where it looked a lot simpler to use,\n\n12:44.740 --> 12:46.780\n but not having a graph meant,\n\n12:46.780 --> 12:47.980\n okay, how do you deploy now?\n\n12:47.980 --> 12:51.180\n So that was probably what tilted the balance for us\n\n12:51.180 --> 12:52.940\n and eventually we ended up with a graph.\n\n12:52.940 --> 12:55.400\n And I guess the question there is, did you,\n\n12:55.400 --> 12:57.420\n I mean, so production seems to be\n\n12:57.420 --> 12:59.900\n the really good thing to focus on,\n\n12:59.900 --> 13:02.500\n but did you even anticipate the other side of it\n\n13:02.500 --> 13:04.620\n where there could be, what is it?\n\n13:04.620 --> 13:05.460\n What are the numbers?\n\n13:05.460 --> 13:08.980\n It's been crazy, 41 million downloads.\n\n13:08.980 --> 13:09.820\n Yep.\n\n13:12.780 --> 13:16.300\n I mean, was that even like a possibility in your mind\n\n13:16.300 --> 13:19.220\n that it would be as popular as it became?\n\n13:19.220 --> 13:23.260\n So I think we did see a need for this\n\n13:24.480 --> 13:27.600\n a lot from the research perspective\n\n13:27.600 --> 13:30.940\n and like early days of deep learning in some ways.\n\n13:32.340 --> 13:35.140\n 41 million, no, I don't think I imagined this number.\n\n13:35.140 --> 13:40.140\n Then it seemed like there's a potential future\n\n13:41.700 --> 13:43.780\n where lots more people would be doing this\n\n13:43.780 --> 13:45.700\n and how do we enable that?\n\n13:45.700 --> 13:48.140\n I would say this kind of growth,\n\n13:49.100 --> 13:52.660\n I probably started seeing somewhat after the open sourcing\n\n13:52.660 --> 13:55.300\n where it was like, okay,\n\n13:55.300 --> 13:57.880\n deep learning is actually growing way faster\n\n13:57.880 --> 13:59.240\n for a lot of different reasons.\n\n13:59.240 --> 14:02.740\n And we are in just the right place to push on that\n\n14:02.740 --> 14:06.100\n and leverage that and deliver on lots of things\n\n14:06.100 --> 14:07.500\n that people want.\n\n14:07.500 --> 14:09.780\n So what changed once you open sourced?\n\n14:09.780 --> 14:13.380\n Like how this incredible amount of attention\n\n14:13.380 --> 14:16.540\n from a global population of developers,\n\n14:16.540 --> 14:18.260\n how did the project start changing?\n\n14:18.260 --> 14:22.220\n I don't even actually remember during those times.\n\n14:22.220 --> 14:24.620\n I know looking now, there's really good documentation,\n\n14:24.620 --> 14:26.620\n there's an ecosystem of tools,\n\n14:26.620 --> 14:27.980\n there's a community, there's a blog,\n\n14:27.980 --> 14:29.820\n there's a YouTube channel now, right?\n\n14:29.820 --> 14:31.180\n Yeah.\n\n14:31.180 --> 14:33.860\n It's very community driven.\n\n14:33.860 --> 14:37.660\n Back then, I guess 0.1 version,\n\n14:38.700 --> 14:39.860\n is that the version?\n\n14:39.860 --> 14:42.180\n I think we call it 0.6 or five,\n\n14:42.180 --> 14:43.740\n something like that, I forget.\n\n14:43.740 --> 14:46.160\n What changed leading into 1.0?\n\n14:47.180 --> 14:48.500\n It's interesting.\n\n14:48.500 --> 14:51.660\n I think we've gone through a few things there.\n\n14:51.660 --> 14:53.720\n When we started out, when we first came out,\n\n14:53.720 --> 14:56.100\n people loved the documentation we have\n\n14:56.100 --> 14:58.860\n because it was just a huge step up from everything else\n\n14:58.860 --> 15:00.440\n because all of those were academic projects,\n\n15:00.440 --> 15:03.380\n people doing, who don't think about documentation.\n\n15:04.580 --> 15:06.960\n I think what that changed was,\n\n15:06.960 --> 15:09.420\n instead of deep learning being a research thing,\n\n15:10.380 --> 15:12.580\n some people who were just developers\n\n15:12.580 --> 15:14.660\n could now suddenly take this out\n\n15:14.660 --> 15:16.940\n and do some interesting things with it, right?\n\n15:16.940 --> 15:20.300\n Who had no clue what machine learning was before then.\n\n15:20.300 --> 15:22.580\n And that I think really changed\n\n15:22.580 --> 15:24.760\n how things started to scale up in some ways\n\n15:24.760 --> 15:26.740\n and pushed on it.\n\n15:27.900 --> 15:30.420\n Over the next few months as we looked at\n\n15:30.420 --> 15:31.980\n how do we stabilize things,\n\n15:31.980 --> 15:33.900\n as we look at not just researchers,\n\n15:33.900 --> 15:36.520\n now we want stability, people want to deploy things.\n\n15:36.520 --> 15:38.980\n That's how we started planning for 1.0\n\n15:38.980 --> 15:42.180\n and there are certain needs for that perspective.\n\n15:42.180 --> 15:44.380\n And so again, documentation comes up,\n\n15:45.380 --> 15:48.200\n designs, more kinds of things to put that together.\n\n15:49.380 --> 15:52.240\n And so that was exciting to get that to a stage\n\n15:52.240 --> 15:55.420\n where more and more enterprises wanted to buy in\n\n15:55.420 --> 15:57.740\n and really get behind that.\n\n15:57.740 --> 16:01.800\n And I think post 1.0 and over the next few releases,\n\n16:01.800 --> 16:04.400\n that enterprise adoption also started to take off.\n\n16:04.400 --> 16:07.160\n I would say between the initial release and 1.0,\n\n16:07.160 --> 16:10.240\n it was, okay, researchers of course,\n\n16:10.240 --> 16:12.960\n then a lot of hobbies and early interest,\n\n16:12.960 --> 16:15.160\n people excited about this who started to get on board\n\n16:15.160 --> 16:18.200\n and then over the 1.x thing, lots of enterprises.\n\n16:18.200 --> 16:23.200\n I imagine anything that's below 1.0\n\n16:23.200 --> 16:25.160\n gives pressure to be,\n\n16:25.160 --> 16:28.040\n the enterprise probably wants something that's stable.\n\n16:28.040 --> 16:28.880\n Exactly.\n\n16:28.880 --> 16:33.320\n And do you have a sense now that TensorFlow is stable?\n\n16:33.320 --> 16:35.560\n Like it feels like deep learning in general\n\n16:35.560 --> 16:39.000\n is extremely dynamic field, so much is changing.\n\n16:40.440 --> 16:43.420\n And TensorFlow has been growing incredibly.\n\n16:43.420 --> 16:46.760\n Do you have a sense of stability at the helm of it?\n\n16:46.760 --> 16:48.400\n I mean, I know you're in the midst of it, but.\n\n16:48.400 --> 16:51.680\n Yeah, I think in the midst of it,\n\n16:51.680 --> 16:55.120\n it's often easy to forget what an enterprise wants\n\n16:55.120 --> 16:58.800\n and what some of the people on that side want.\n\n16:58.800 --> 17:00.420\n There are still people running models\n\n17:00.420 --> 17:02.680\n that are three years old, four years old.\n\n17:02.680 --> 17:06.040\n So Inception is still used by tons of people.\n\n17:06.040 --> 17:08.960\n Even ResNet 50 is what, couple of years old now or more,\n\n17:08.960 --> 17:12.240\n but there are tons of people who use that and they're fine.\n\n17:12.240 --> 17:15.320\n They don't need the last couple of bits of performance\n\n17:15.320 --> 17:17.720\n or quality, they want some stability\n\n17:17.720 --> 17:19.640\n in things that just work.\n\n17:19.640 --> 17:22.240\n And so there is value in providing that\n\n17:22.240 --> 17:25.200\n with that kind of stability and making it really simpler\n\n17:25.200 --> 17:27.800\n because that allows a lot more people to access it.\n\n17:27.800 --> 17:31.200\n And then there's the research crowd which wants,\n\n17:31.200 --> 17:33.080\n okay, they wanna do these crazy things\n\n17:33.080 --> 17:34.280\n exactly like you're saying, right?\n\n17:34.280 --> 17:37.080\n Not just deep learning in the straight up models\n\n17:37.080 --> 17:40.640\n that used to be there, they want RNNs\n\n17:40.640 --> 17:43.480\n and even RNNs are maybe old, they are transformers now.\n\n17:43.480 --> 17:48.440\n And now it needs to combine with RL and GANs and so on.\n\n17:48.440 --> 17:52.000\n So there's definitely that area that like the boundary\n\n17:52.000 --> 17:55.200\n that's shifting and pushing the state of the art.\n\n17:55.200 --> 17:57.200\n But I think there's more and more of the past\n\n17:57.200 --> 18:01.440\n that's much more stable and even stuff\n\n18:01.440 --> 18:03.880\n that was two, three years old is very, very usable\n\n18:03.880 --> 18:04.960\n by lots of people.\n\n18:04.960 --> 18:07.440\n So that part makes it a lot easier.\n\n18:07.440 --> 18:09.840\n So I imagine, maybe you can correct me if I'm wrong,\n\n18:09.840 --> 18:12.440\n one of the biggest use cases is essentially\n\n18:12.440 --> 18:14.440\n taking something like ResNet 50\n\n18:14.440 --> 18:17.280\n and doing some kind of transfer learning\n\n18:17.280 --> 18:19.600\n on a very particular problem that you have.\n\n18:19.600 --> 18:23.080\n It's basically probably what majority of the world does.\n\n18:24.520 --> 18:27.360\n And you wanna make that as easy as possible.\n\n18:27.360 --> 18:30.440\n So I would say for the hobbyist perspective,\n\n18:30.440 --> 18:32.800\n that's the most common case, right?\n\n18:32.800 --> 18:35.400\n In fact, the apps and phones and stuff that you'll see,\n\n18:35.400 --> 18:37.720\n the early ones, that's the most common case.\n\n18:37.720 --> 18:40.360\n I would say there are a couple of reasons for that.\n\n18:40.360 --> 18:43.520\n One is that everybody talks about that.\n\n18:44.440 --> 18:46.160\n It looks great on slides.\n\n18:46.160 --> 18:49.960\n That's a presentation, yeah, exactly.\n\n18:49.960 --> 18:53.080\n What enterprises want is that is part of it,\n\n18:53.080 --> 18:54.360\n but that's not the big thing.\n\n18:54.360 --> 18:56.080\n Enterprises really have data\n\n18:56.080 --> 18:58.000\n that they wanna make predictions on.\n\n18:58.000 --> 19:00.320\n This is often what they used to do\n\n19:00.320 --> 19:01.760\n with the people who were doing ML\n\n19:01.760 --> 19:03.560\n was just regression models,\n\n19:03.560 --> 19:06.360\n linear regression, logistic regression, linear models,\n\n19:06.360 --> 19:09.760\n or maybe gradient booster trees and so on.\n\n19:09.760 --> 19:11.680\n Some of them still benefit from deep learning,\n\n19:11.680 --> 19:14.400\n but they want that's the bread and butter,\n\n19:14.400 --> 19:16.360\n or like the structured data and so on.\n\n19:16.360 --> 19:18.280\n So depending on the audience you look at,\n\n19:18.280 --> 19:19.600\n they're a little bit different.\n\n19:19.600 --> 19:22.560\n And they just have, I mean, the best of enterprise\n\n19:23.440 --> 19:26.520\n probably just has a very large data set,\n\n19:26.520 --> 19:28.720\n or deep learning can probably shine.\n\n19:28.720 --> 19:30.320\n That's correct, that's right.\n\n19:30.320 --> 19:33.320\n And then I think the other pieces that they wanted,\n\n19:33.320 --> 19:36.480\n again, with 2.0, the developer summit we put together\n\n19:36.480 --> 19:39.080\n is the whole TensorFlow Extended piece,\n\n19:39.080 --> 19:40.680\n which is the entire pipeline.\n\n19:40.680 --> 19:43.640\n They care about stability across doing their entire thing.\n\n19:43.640 --> 19:46.320\n They want simplicity across the entire thing.\n\n19:46.320 --> 19:47.760\n I don't need to just train a model.\n\n19:47.760 --> 19:51.360\n I need to do that every day again, over and over again.\n\n19:51.360 --> 19:54.360\n I wonder to which degree you have a role in,\n\n19:54.360 --> 19:56.720\n I don't know, so I teach a course on deep learning.\n\n19:56.720 --> 20:01.400\n I have people like lawyers come up to me and say,\n\n20:01.400 --> 20:04.240\n when is machine learning gonna enter legal,\n\n20:04.240 --> 20:05.640\n the legal realm?\n\n20:05.640 --> 20:09.520\n The same thing in all kinds of disciplines,\n\n20:09.520 --> 20:14.520\n immigration, insurance, often when I see\n\n20:14.720 --> 20:17.440\n what it boils down to is these companies\n\n20:17.440 --> 20:19.480\n are often a little bit old school\n\n20:19.480 --> 20:20.880\n in the way they organize the data.\n\n20:20.880 --> 20:24.040\n So the data is just not ready yet, it's not digitized.\n\n20:24.040 --> 20:26.000\n Do you also find yourself being in the role\n\n20:26.000 --> 20:31.000\n of an evangelist for like, let's get,\n\n20:31.520 --> 20:33.760\n organize your data, folks, and then you'll get\n\n20:33.760 --> 20:35.480\n the big benefit of TensorFlow.\n\n20:35.480 --> 20:38.040\n Do you get those, have those conversations?\n\n20:38.040 --> 20:41.480\n Yeah, yeah, you know, I get all kinds of questions there\n\n20:41.480 --> 20:46.480\n from, okay, what do I need to make this work, right?\n\n20:49.080 --> 20:50.840\n Do we really need deep learning?\n\n20:50.840 --> 20:52.120\n I mean, there are all these things,\n\n20:52.120 --> 20:55.200\n I already use this linear model, why would this help?\n\n20:55.200 --> 20:57.200\n I don't have enough data, let's say,\n\n20:57.200 --> 21:00.000\n or I wanna use machine learning,\n\n21:00.000 --> 21:01.800\n but I have no clue where to start.\n\n21:01.800 --> 21:04.960\n So it varies, that to all the way to the experts\n\n21:04.960 --> 21:08.600\n to why support very specific things, it's interesting.\n\n21:08.600 --> 21:09.920\n Is there a good answer?\n\n21:09.920 --> 21:12.520\n It boils down to oftentimes digitizing data.\n\n21:12.520 --> 21:14.480\n So whatever you want automated,\n\n21:14.480 --> 21:17.560\n whatever data you want to make prediction based on,\n\n21:17.560 --> 21:20.200\n you have to make sure that it's in an organized form.\n\n21:21.280 --> 21:24.000\n Like within the TensorFlow ecosystem,\n\n21:24.000 --> 21:26.560\n there's now, you're providing more and more data sets\n\n21:26.560 --> 21:28.960\n and more and more pre trained models.\n\n21:28.960 --> 21:32.440\n Are you finding yourself also the organizer of data sets?\n\n21:32.440 --> 21:34.520\n Yes, I think the TensorFlow data sets\n\n21:34.520 --> 21:37.560\n that we just released, that's definitely come up\n\n21:37.560 --> 21:39.240\n where people want these data sets,\n\n21:39.240 --> 21:41.760\n can we organize them and can we make that easier?\n\n21:41.760 --> 21:45.320\n So that's definitely one important thing.\n\n21:45.320 --> 21:47.680\n The other related thing I would say is I often tell people,\n\n21:47.680 --> 21:51.000\n you know what, don't think of the most fanciest thing\n\n21:51.000 --> 21:53.320\n that the newest model that you see,\n\n21:53.320 --> 21:56.400\n make something very basic work and then you can improve it.\n\n21:56.400 --> 21:58.920\n There's just lots of things you can do with it.\n\n21:58.920 --> 22:00.640\n Yeah, start with the basics, true.\n\n22:00.640 --> 22:03.280\n One of the big things that makes TensorFlow\n\n22:03.280 --> 22:06.120\n even more accessible was the appearance\n\n22:06.120 --> 22:08.360\n whenever that happened of Keras,\n\n22:08.360 --> 22:12.400\n the Keras standard sort of outside of TensorFlow.\n\n22:12.400 --> 22:17.400\n I think it was Keras on top of Tiano at first only\n\n22:18.240 --> 22:22.520\n and then Keras became on top of TensorFlow.\n\n22:22.520 --> 22:27.520\n Do you know when Keras chose to also add TensorFlow\n\n22:28.760 --> 22:31.200\n as a backend, who was the,\n\n22:31.200 --> 22:34.000\n was it just the community that drove that initially?\n\n22:34.000 --> 22:37.040\n Do you know if there was discussions, conversations?\n\n22:37.040 --> 22:41.000\n Yeah, so Francois started the Keras project\n\n22:41.000 --> 22:44.600\n before he was at Google and the first thing was Tiano.\n\n22:44.600 --> 22:46.560\n I don't remember if that was\n\n22:46.560 --> 22:48.760\n after TensorFlow was created or way before.\n\n22:49.680 --> 22:51.440\n And then at some point,\n\n22:51.440 --> 22:53.040\n when TensorFlow started becoming popular,\n\n22:53.040 --> 22:54.200\n there were enough similarities\n\n22:54.200 --> 22:56.360\n that he decided to create this interface\n\n22:56.360 --> 22:58.200\n and put TensorFlow as a backend.\n\n22:58.200 --> 23:00.760\n I believe that might still have been\n\n23:00.760 --> 23:03.320\n before he joined Google.\n\n23:03.320 --> 23:06.720\n So we weren't really talking about that.\n\n23:06.720 --> 23:09.720\n He decided on his own and thought that was interesting\n\n23:09.720 --> 23:11.280\n and relevant to the community.\n\n23:12.800 --> 23:17.120\n In fact, I didn't find out about him being at Google\n\n23:17.120 --> 23:19.680\n until a few months after he was here.\n\n23:19.680 --> 23:21.880\n He was working on some research ideas\n\n23:21.880 --> 23:24.480\n and doing Keras on his nights and weekends project.\n\n23:24.480 --> 23:25.320\n Oh, interesting.\n\n23:25.320 --> 23:28.520\n He wasn't like part of the TensorFlow.\n\n23:28.520 --> 23:29.720\n He didn't join initially.\n\n23:29.720 --> 23:32.280\n He joined research and he was doing some amazing research.\n\n23:32.280 --> 23:34.360\n He has some papers on that and research,\n\n23:34.360 --> 23:36.920\n so he's a great researcher as well.\n\n23:38.400 --> 23:40.400\n And at some point we realized,\n\n23:40.400 --> 23:42.440\n oh, he's doing this good stuff.\n\n23:42.440 --> 23:45.400\n People seem to like the API and he's right here.\n\n23:45.400 --> 23:47.760\n So we talked to him and he said,\n\n23:47.760 --> 23:50.600\n okay, why don't I come over to your team\n\n23:50.600 --> 23:52.840\n and work with you for a quarter\n\n23:52.840 --> 23:55.520\n and let's make that integration happen.\n\n23:55.520 --> 23:56.840\n And we talked to his manager and he said,\n\n23:56.840 --> 23:58.600\n sure, quarter's fine.\n\n23:59.800 --> 24:02.400\n And that quarter's been something like two years now.\n\n24:02.400 --> 24:05.080\n And so he's fully on this.\n\n24:05.080 --> 24:10.080\n So Keras got integrated into TensorFlow in a deep way.\n\n24:12.000 --> 24:15.240\n And now with 2.0, TensorFlow 2.0,\n\n24:15.240 --> 24:18.800\n sort of Keras is kind of the recommended way\n\n24:18.800 --> 24:21.720\n for a beginner to interact with TensorFlow.\n\n24:21.720 --> 24:24.640\n Which makes that initial sort of transfer learning\n\n24:24.640 --> 24:28.040\n or the basic use cases, even for an enterprise,\n\n24:28.040 --> 24:29.320\n super simple, right?\n\n24:29.320 --> 24:30.440\n That's correct, that's right.\n\n24:30.440 --> 24:32.040\n So what was that decision like?\n\n24:32.040 --> 24:37.040\n That seems like it's kind of a bold decision as well.\n\n24:38.680 --> 24:41.240\n We did spend a lot of time thinking about that one.\n\n24:41.240 --> 24:46.000\n We had a bunch of APIs, some built by us.\n\n24:46.000 --> 24:48.760\n There was a parallel layers API that we were building.\n\n24:48.760 --> 24:51.560\n And when we decided to do Keras in parallel,\n\n24:51.560 --> 24:54.400\n so there were like, okay, two things that we are looking at.\n\n24:54.400 --> 24:55.960\n And the first thing we was trying to do\n\n24:55.960 --> 24:58.240\n is just have them look similar,\n\n24:58.240 --> 25:00.120\n like be as integrated as possible,\n\n25:00.120 --> 25:02.200\n share all of that stuff.\n\n25:02.200 --> 25:04.000\n There were also like three other APIs\n\n25:04.000 --> 25:05.840\n that others had built over time\n\n25:05.840 --> 25:07.760\n because we didn't have a standard one.\n\n25:09.040 --> 25:11.480\n But one of the messages that we kept hearing\n\n25:11.480 --> 25:13.240\n from the community, okay, which one do we use?\n\n25:13.240 --> 25:14.480\n And they kept seeing like, okay,\n\n25:14.480 --> 25:16.760\n here's a model in this one and here's a model in this one,\n\n25:16.760 --> 25:17.760\n which should I pick?\n\n25:18.880 --> 25:20.960\n So that's sort of like, okay,\n\n25:20.960 --> 25:24.080\n we had to address that straight on with 2.0.\n\n25:24.080 --> 25:26.360\n The whole idea was we need to simplify.\n\n25:26.360 --> 25:27.400\n We had to pick one.\n\n25:28.640 --> 25:30.520\n Based on where we were, we were like,\n\n25:30.520 --> 25:35.520\n okay, let's see what are the people like?\n\n25:35.680 --> 25:39.320\n And Keras was clearly one that lots of people loved.\n\n25:39.320 --> 25:41.640\n There were lots of great things about it.\n\n25:41.640 --> 25:43.920\n So we settled on that.\n\n25:43.920 --> 25:46.440\n Organically, that's kind of the best way to do it.\n\n25:46.440 --> 25:47.520\n It was great.\n\n25:47.520 --> 25:48.760\n It was surprising, nevertheless,\n\n25:48.760 --> 25:51.120\n to sort of bring in an outside.\n\n25:51.120 --> 25:52.560\n I mean, there was a feeling like Keras\n\n25:52.560 --> 25:55.440\n might be almost like a competitor\n\n25:55.440 --> 25:58.040\n in a certain kind of, to TensorFlow.\n\n25:58.040 --> 26:01.320\n And in a sense, it became an empowering element\n\n26:01.320 --> 26:02.240\n of TensorFlow.\n\n26:02.240 --> 26:03.280\n That's right.\n\n26:03.280 --> 26:06.440\n Yeah, it's interesting how you can put two things together,\n\n26:06.440 --> 26:08.800\n which can align.\n\n26:08.800 --> 26:11.800\n In this case, I think Francois, the team,\n\n26:11.800 --> 26:14.280\n and a bunch of us have chatted,\n\n26:14.280 --> 26:17.360\n and I think we all want to see the same kind of things.\n\n26:17.360 --> 26:18.800\n We all care about making it easier\n\n26:18.800 --> 26:21.440\n for the huge set of developers out there,\n\n26:21.440 --> 26:23.480\n and that makes a difference.\n\n26:23.480 --> 26:26.880\n So Python has Guido van Rossum,\n\n26:26.880 --> 26:28.920\n who until recently held the position\n\n26:28.920 --> 26:31.920\n of benevolent dictator for life.\n\n26:31.920 --> 26:36.480\n All right, so there's a huge successful open source project\n\n26:36.480 --> 26:40.680\n like TensorFlow need one person who makes a final decision.\n\n26:40.680 --> 26:45.480\n So you've did a pretty successful TensorFlow Dev Summit\n\n26:45.480 --> 26:47.520\n just now, last couple of days.\n\n26:47.520 --> 26:51.080\n There's clearly a lot of different new features\n\n26:51.080 --> 26:54.160\n being incorporated, an amazing ecosystem, so on.\n\n26:54.160 --> 26:57.320\n Who's, how are those design decisions made?\n\n26:57.320 --> 27:00.640\n Is there a BDFL in TensorFlow,\n\n27:02.800 --> 27:05.800\n or is it more distributed and organic?\n\n27:05.800 --> 27:08.760\n I think it's somewhat different, I would say.\n\n27:08.760 --> 27:14.560\n I've always been involved in the key design directions,\n\n27:14.560 --> 27:17.080\n but there are lots of things that are distributed\n\n27:17.080 --> 27:20.560\n where there are a number of people, Martin Wick being one,\n\n27:20.560 --> 27:23.880\n who has really driven a lot of our open source stuff,\n\n27:23.880 --> 27:26.080\n a lot of the APIs,\n\n27:26.080 --> 27:29.080\n and there are a number of other people who've been,\n\n27:29.080 --> 27:31.360\n you know, pushed and been responsible\n\n27:31.360 --> 27:34.080\n for different parts of it.\n\n27:34.080 --> 27:36.480\n We do have regular design reviews.\n\n27:36.480 --> 27:38.480\n Over the last year, we've had a lot of\n\n27:38.480 --> 27:41.480\n we've really spent a lot of time opening up to the community\n\n27:41.480 --> 27:44.160\n and adding transparency.\n\n27:44.160 --> 27:45.880\n We're setting more processes in place,\n\n27:45.880 --> 27:49.080\n so RFCs, special interest groups,\n\n27:49.080 --> 27:53.600\n to really grow that community and scale that.\n\n27:53.600 --> 27:57.720\n I think the kind of scale that ecosystem is in,\n\n27:57.720 --> 27:59.520\n I don't think we could scale with having me\n\n27:59.520 --> 28:02.280\n as the lone point of decision maker.\n\n28:02.280 --> 28:05.920\n I got it. So, yeah, the growth of that ecosystem,\n\n28:05.920 --> 28:08.040\n maybe you can talk about it a little bit.\n\n28:08.040 --> 28:10.720\n First of all, it started with Andrej Karpathy\n\n28:10.720 --> 28:13.120\n when he first did ComNetJS.\n\n28:13.120 --> 28:15.360\n The fact that you can train and you'll network\n\n28:15.360 --> 28:18.480\n in the browser was, in JavaScript, was incredible.\n\n28:18.480 --> 28:22.160\n So now TensorFlow.js is really making that\n\n28:22.160 --> 28:26.400\n a serious, like a legit thing,\n\n28:26.400 --> 28:28.520\n a way to operate, whether it's in the backend\n\n28:28.520 --> 28:29.520\n or the front end.\n\n28:29.520 --> 28:32.680\n Then there's the TensorFlow Extended, like you mentioned.\n\n28:32.680 --> 28:35.320\n There's TensorFlow Lite for mobile.\n\n28:35.320 --> 28:37.440\n And all of it, as far as I can tell,\n\n28:37.440 --> 28:40.440\n it's really converging towards being able to\n\n28:41.680 --> 28:43.440\n save models in the same kind of way.\n\n28:43.440 --> 28:46.680\n You can move around, you can train on the desktop\n\n28:46.680 --> 28:48.880\n and then move it to mobile and so on.\n\n28:48.880 --> 28:49.720\n That's right.\n\n28:49.720 --> 28:52.280\n So there's that cohesiveness.\n\n28:52.280 --> 28:56.120\n So can you maybe give me, whatever I missed,\n\n28:56.120 --> 28:58.840\n a bigger overview of the mission of the ecosystem\n\n28:58.840 --> 29:02.080\n that's trying to be built and where is it moving forward?\n\n29:02.080 --> 29:06.720\n Yeah. So in short, the way I like to think of this is\n\n29:06.720 --> 29:09.680\n our goals to enable machine learning.\n\n29:09.680 --> 29:13.120\n And in a couple of ways, you know, one is\n\n29:13.120 --> 29:16.520\n we have lots of exciting things going on in ML today.\n\n29:16.520 --> 29:17.520\n We started with deep learning,\n\n29:17.520 --> 29:20.240\n but we now support a bunch of other algorithms too.\n\n29:21.360 --> 29:23.760\n So one is to, on the research side,\n\n29:23.760 --> 29:25.280\n keep pushing on the state of the art.\n\n29:25.280 --> 29:27.200\n Can we, you know, how do we enable researchers\n\n29:27.200 --> 29:28.920\n to build the next amazing thing?\n\n29:28.920 --> 29:31.720\n So BERT came out recently, you know,\n\n29:31.720 --> 29:33.920\n it's great that people are able to do new kinds of research.\n\n29:33.920 --> 29:35.360\n And there are lots of amazing research\n\n29:35.360 --> 29:37.480\n that happens across the world.\n\n29:37.480 --> 29:38.800\n So that's one direction.\n\n29:38.800 --> 29:42.440\n The other is how do you take that across\n\n29:42.440 --> 29:45.200\n all the people outside who want to take that research\n\n29:45.200 --> 29:46.600\n and do some great things with it\n\n29:46.600 --> 29:48.600\n and integrate it to build real products,\n\n29:48.600 --> 29:50.840\n to have a real impact on people.\n\n29:51.720 --> 29:54.960\n And so if that's the other axes in some ways,\n\n29:56.320 --> 29:59.600\n you know, at a high level, one way I think about it is\n\n29:59.600 --> 30:02.440\n there are a crazy number of compute devices\n\n30:02.440 --> 30:04.160\n across the world.\n\n30:04.160 --> 30:07.840\n And we often used to think of ML and training\n\n30:07.840 --> 30:09.400\n and all of this as, okay, something you do\n\n30:09.400 --> 30:12.480\n either in the workstation or the data center or cloud.\n\n30:13.560 --> 30:15.640\n But we see things running on the phones.\n\n30:15.640 --> 30:17.600\n We see things running on really tiny chips.\n\n30:17.600 --> 30:20.680\n I mean, we had some demos at the developer summit.\n\n30:20.680 --> 30:25.680\n And so the way I think about this ecosystem is\n\n30:25.760 --> 30:29.880\n how do we help get machine learning on every device\n\n30:29.880 --> 30:32.480\n that has a compute capability?\n\n30:32.480 --> 30:36.440\n And that continues to grow and so in some ways\n\n30:36.440 --> 30:38.680\n this ecosystem is looked at, you know,\n\n30:38.680 --> 30:41.120\n various aspects of that and grown over time\n\n30:41.120 --> 30:42.440\n to cover more of those.\n\n30:42.440 --> 30:44.640\n And we continue to push the boundaries.\n\n30:44.640 --> 30:48.160\n In some areas we've built more tooling\n\n30:48.160 --> 30:50.000\n and things around that to help you.\n\n30:50.000 --> 30:52.760\n I mean, the first tool we started was TensorBoard.\n\n30:52.760 --> 30:54.960\n You wanted to learn just the training piece,\n\n30:56.240 --> 30:58.080\n the effects or TensorFlow extended\n\n30:58.080 --> 31:00.400\n to really do your entire ML pipelines.\n\n31:00.400 --> 31:03.880\n If you're, you know, care about all that production stuff,\n\n31:04.760 --> 31:06.600\n but then going to the edge,\n\n31:06.600 --> 31:09.480\n going to different kinds of things.\n\n31:09.480 --> 31:11.760\n And it's not just us now.\n\n31:11.760 --> 31:14.440\n We are a place where there are lots of libraries\n\n31:14.440 --> 31:15.800\n being built on top.\n\n31:15.800 --> 31:17.760\n So there are some for research,\n\n31:17.760 --> 31:20.040\n maybe things like TensorFlow agents\n\n31:20.040 --> 31:22.440\n or TensorFlow probability that started as research things\n\n31:22.440 --> 31:24.200\n or for researchers for focusing\n\n31:24.200 --> 31:26.120\n on certain kinds of algorithms,\n\n31:26.120 --> 31:27.280\n but they're also being deployed\n\n31:27.280 --> 31:30.240\n or used by, you know, production folks.\n\n31:30.240 --> 31:33.320\n And some have come from within Google,\n\n31:33.320 --> 31:34.720\n just teams across Google\n\n31:34.720 --> 31:37.000\n who wanted to build these things.\n\n31:37.000 --> 31:39.680\n Others have come from just the community\n\n31:39.680 --> 31:41.840\n because there are different pieces\n\n31:41.840 --> 31:44.600\n that different parts of the community care about.\n\n31:44.600 --> 31:49.480\n And I see our goal as enabling even that, right?\n\n31:49.480 --> 31:53.240\n It's not, we cannot and won't build every single thing.\n\n31:53.240 --> 31:54.840\n That just doesn't make sense.\n\n31:54.840 --> 31:57.360\n But if we can enable others to build the things\n\n31:57.360 --> 32:00.400\n that they care about, and there's a broader community\n\n32:00.400 --> 32:02.880\n that cares about that, and we can help encourage that,\n\n32:02.880 --> 32:05.280\n and that's great.\n\n32:05.280 --> 32:08.600\n That really helps the entire ecosystem, not just those.\n\n32:08.600 --> 32:11.840\n One of the big things about 2.0 that we're pushing on is,\n\n32:11.840 --> 32:14.640\n okay, we have these so many different pieces, right?\n\n32:14.640 --> 32:18.320\n How do we help make all of them work well together?\n\n32:18.320 --> 32:21.960\n So there are a few key pieces there that we're pushing on,\n\n32:21.960 --> 32:23.880\n one being the core format in there\n\n32:23.880 --> 32:26.600\n and how we share the models themselves\n\n32:26.600 --> 32:29.560\n through save model and TensorFlow hub and so on.\n\n32:30.480 --> 32:34.000\n And a few of the pieces that we really put this together.\n\n32:34.000 --> 32:35.600\n I was very skeptical that that's,\n\n32:35.600 --> 32:37.280\n you know, when TensorFlow.js came out,\n\n32:37.280 --> 32:40.160\n it didn't seem, or deep learning JS as it was earlier.\n\n32:40.160 --> 32:41.680\n Yeah, that was the first.\n\n32:41.680 --> 32:45.080\n It seems like technically very difficult project.\n\n32:45.080 --> 32:47.000\n As a standalone, it's not as difficult,\n\n32:47.000 --> 32:49.960\n but as a thing that integrates into the ecosystem,\n\n32:49.960 --> 32:51.240\n it seems very difficult.\n\n32:51.240 --> 32:53.240\n So, I mean, there's a lot of aspects of this\n\n32:53.240 --> 32:54.840\n you're making look easy, but,\n\n32:54.840 --> 32:57.160\n and the technical side,\n\n32:57.160 --> 32:59.480\n how many challenges have to be overcome here?\n\n33:00.520 --> 33:01.480\n A lot.\n\n33:01.480 --> 33:03.040\n And still have to be overcome.\n\n33:03.040 --> 33:04.680\n That's the question here too.\n\n33:04.680 --> 33:06.320\n There are lots of steps to it, right?\n\n33:06.320 --> 33:07.960\n And we've iterated over the last few years,\n\n33:07.960 --> 33:09.640\n so there's a lot we've learned.\n\n33:10.680 --> 33:14.200\n I, yeah, and often when things come together well,\n\n33:14.200 --> 33:16.360\n things look easy and that's exactly the point.\n\n33:16.360 --> 33:18.320\n It should be easy for the end user,\n\n33:18.320 --> 33:21.320\n but there are lots of things that go behind that.\n\n33:21.320 --> 33:25.320\n If I think about still challenges ahead,\n\n33:25.320 --> 33:26.680\n there are,\n\n33:29.400 --> 33:32.880\n you know, we have a lot more devices coming on board,\n\n33:32.880 --> 33:35.280\n for example, from the hardware perspective.\n\n33:35.280 --> 33:37.600\n How do we make it really easy for these vendors\n\n33:37.600 --> 33:42.040\n to integrate with something like TensorFlow, right?\n\n33:42.040 --> 33:43.600\n So there's a lot of compiler stuff\n\n33:43.600 --> 33:45.280\n that others are working on.\n\n33:45.280 --> 33:48.280\n There are things we can do in terms of our APIs\n\n33:48.280 --> 33:50.440\n and so on that we can do.\n\n33:50.440 --> 33:52.960\n As we, you know,\n\n33:52.960 --> 33:55.760\n TensorFlow started as a very monolithic system\n\n33:55.760 --> 33:57.600\n and to some extent it still is.\n\n33:57.600 --> 33:59.360\n There are less, lots of tools around it,\n\n33:59.360 --> 34:02.880\n but the core is still pretty large and monolithic.\n\n34:02.880 --> 34:05.680\n One of the key challenges for us to scale that out\n\n34:05.680 --> 34:10.320\n is how do we break that apart with clearer interfaces?\n\n34:10.320 --> 34:14.520\n It's, you know, in some ways it's software engineering 101,\n\n34:14.520 --> 34:18.480\n but for a system that's now four years old, I guess,\n\n34:18.480 --> 34:21.560\n or more, and that's still rapidly evolving\n\n34:21.560 --> 34:23.960\n and that we're not slowing down with,\n\n34:23.960 --> 34:28.200\n it's hard to change and modify and really break apart.\n\n34:28.200 --> 34:29.880\n It's sort of like, as people say, right,\n\n34:29.880 --> 34:32.560\n it's like changing the engine with a car running\n\n34:32.560 --> 34:33.600\n or trying to fix that.\n\n34:33.600 --> 34:35.040\n That's exactly what we're trying to do.\n\n34:35.040 --> 34:37.520\n So there's a challenge here\n\n34:37.520 --> 34:41.560\n because the downside of so many people\n\n34:41.560 --> 34:43.800\n being excited about TensorFlow\n\n34:43.800 --> 34:48.520\n and coming to rely on it in many of their applications\n\n34:48.520 --> 34:52.000\n is that you're kind of responsible,\n\n34:52.000 --> 34:53.480\n like it's the technical debt.\n\n34:53.480 --> 34:55.600\n You're responsible for previous versions\n\n34:55.600 --> 34:57.560\n to some degree still working.\n\n34:57.560 --> 34:59.840\n So when you're trying to innovate,\n\n34:59.840 --> 35:02.360\n I mean, it's probably easier\n\n35:02.360 --> 35:04.760\n to just start from scratch every few months.\n\n35:04.760 --> 35:07.160\n Absolutely.\n\n35:07.160 --> 35:09.240\n So do you feel the pain of that?\n\n35:09.240 --> 35:14.240\n 2.0 does break some back compatibility,\n\n35:14.320 --> 35:15.400\n but not too much.\n\n35:15.400 --> 35:18.160\n It seems like the conversion is pretty straightforward.\n\n35:18.160 --> 35:20.280\n Do you think that's still important\n\n35:20.280 --> 35:22.920\n given how quickly deep learning is changing?\n\n35:22.920 --> 35:26.440\n Can you just, the things that you've learned,\n\n35:26.440 --> 35:29.320\n can you just start over or is there pressure to not?\n\n35:29.320 --> 35:31.600\n It's a tricky balance.\n\n35:31.600 --> 35:36.360\n So if it was just a researcher writing a paper\n\n35:36.360 --> 35:39.400\n who a year later will not look at that code again,\n\n35:39.400 --> 35:40.720\n sure, it doesn't matter.\n\n35:41.600 --> 35:43.440\n There are a lot of production systems\n\n35:43.440 --> 35:44.680\n that rely on TensorFlow,\n\n35:44.680 --> 35:47.240\n both at Google and across the world.\n\n35:47.240 --> 35:49.760\n And people worry about this.\n\n35:49.760 --> 35:52.400\n I mean, these systems run for a long time.\n\n35:53.440 --> 35:57.280\n So it is important to keep that compatibility and so on.\n\n35:57.280 --> 35:59.720\n And yes, it does come with a huge cost.\n\n35:59.720 --> 36:02.960\n There's, we have to think about a lot of things\n\n36:02.960 --> 36:06.920\n as we do new things and make new changes.\n\n36:06.920 --> 36:09.080\n I think it's a trade off, right?\n\n36:09.080 --> 36:12.960\n You can, you might slow certain kinds of things down,\n\n36:12.960 --> 36:14.560\n but the overall value you're bringing\n\n36:14.560 --> 36:16.920\n because of that is much bigger\n\n36:16.920 --> 36:20.520\n because it's not just about breaking the person yesterday.\n\n36:20.520 --> 36:23.640\n It's also about telling the person tomorrow\n\n36:23.640 --> 36:26.240\n that, you know what, this is how we do things.\n\n36:26.240 --> 36:28.480\n We're not gonna break you when you come on board\n\n36:28.480 --> 36:29.800\n because there are lots of new people\n\n36:29.800 --> 36:31.400\n who are also gonna come on board.\n\n36:31.400 --> 36:34.680\n And, you know, one way I like to think about this,\n\n36:34.680 --> 36:37.960\n and I always push the team to think about it as well,\n\n36:37.960 --> 36:39.560\n when you wanna do new things,\n\n36:39.560 --> 36:42.040\n you wanna start with a clean slate.\n\n36:42.040 --> 36:44.880\n Design with a clean slate in mind,\n\n36:44.880 --> 36:46.160\n and then we'll figure out\n\n36:46.160 --> 36:48.640\n how to make sure all the other things work.\n\n36:48.640 --> 36:51.280\n And yes, we do make compromises occasionally,\n\n36:52.160 --> 36:55.200\n but unless you design with the clean slate\n\n36:55.200 --> 36:56.520\n and not worry about that,\n\n36:56.520 --> 36:58.360\n you'll never get to a good place.\n\n36:58.360 --> 37:02.560\n Oh, that's brilliant, so even if you are responsible\n\n37:02.560 --> 37:04.080\n when you're in the idea stage,\n\n37:04.080 --> 37:05.760\n when you're thinking of new,\n\n37:05.760 --> 37:07.720\n just put all that behind you.\n\n37:07.720 --> 37:09.600\n Okay, that's really, really well put.\n\n37:09.600 --> 37:11.080\n So I have to ask this\n\n37:11.080 --> 37:13.240\n because a lot of students, developers ask me\n\n37:13.240 --> 37:16.320\n how I feel about PyTorch versus TensorFlow.\n\n37:16.320 --> 37:18.280\n So I've recently completely switched\n\n37:18.280 --> 37:20.920\n my research group to TensorFlow.\n\n37:20.920 --> 37:23.280\n I wish everybody would just use the same thing,\n\n37:23.280 --> 37:26.960\n and TensorFlow is as close to that, I believe, as we have.\n\n37:26.960 --> 37:30.960\n But do you enjoy competition?\n\n37:32.040 --> 37:34.320\n So TensorFlow is leading in many ways,\n\n37:34.320 --> 37:36.760\n on many dimensions in terms of ecosystem,\n\n37:36.760 --> 37:39.040\n in terms of number of users,\n\n37:39.040 --> 37:41.200\n momentum, power, production levels, so on,\n\n37:41.200 --> 37:46.000\n but a lot of researchers are now also using PyTorch.\n\n37:46.000 --> 37:47.520\n Do you enjoy that kind of competition\n\n37:47.520 --> 37:48.840\n or do you just ignore it\n\n37:48.840 --> 37:52.320\n and focus on making TensorFlow the best that it can be?\n\n37:52.320 --> 37:55.480\n So just like research or anything people are doing,\n\n37:55.480 --> 37:58.120\n it's great to get different kinds of ideas.\n\n37:58.120 --> 38:01.480\n And when we started with TensorFlow,\n\n38:01.480 --> 38:03.280\n like I was saying earlier,\n\n38:03.280 --> 38:05.240\n one, it was very important\n\n38:05.240 --> 38:07.440\n for us to also have production in mind.\n\n38:07.440 --> 38:09.000\n We didn't want just research, right?\n\n38:09.000 --> 38:11.280\n And that's why we chose certain things.\n\n38:11.280 --> 38:12.720\n Now PyTorch came along and said,\n\n38:12.720 --> 38:14.880\n you know what, I only care about research.\n\n38:14.880 --> 38:16.280\n This is what I'm trying to do.\n\n38:16.280 --> 38:18.400\n What's the best thing I can do for this?\n\n38:18.400 --> 38:20.880\n And it started iterating and said,\n\n38:20.880 --> 38:22.560\n okay, I don't need to worry about graphs.\n\n38:22.560 --> 38:24.080\n Let me just run things.\n\n38:24.080 --> 38:27.440\n And I don't care if it's not as fast as it can be,\n\n38:27.440 --> 38:30.480\n but let me just make this part easy.\n\n38:30.480 --> 38:32.560\n And there are things you can learn from that, right?\n\n38:32.560 --> 38:36.760\n They, again, had the benefit of seeing what had come before,\n\n38:36.760 --> 38:40.520\n but also exploring certain different kinds of spaces.\n\n38:40.520 --> 38:43.560\n And they had some good things there,\n\n38:43.560 --> 38:46.680\n building on say things like JNR and so on before that.\n\n38:46.680 --> 38:49.320\n So competition is definitely interesting.\n\n38:49.320 --> 38:50.240\n It made us, you know,\n\n38:50.240 --> 38:51.880\n this is an area that we had thought about,\n\n38:51.880 --> 38:53.720\n like I said, way early on.\n\n38:53.720 --> 38:56.600\n Over time we had revisited this a couple of times,\n\n38:56.600 --> 38:59.000\n should we add this again?\n\n38:59.000 --> 39:01.040\n At some point we said, you know what,\n\n39:01.040 --> 39:02.880\n it seems like this can be done well,\n\n39:02.880 --> 39:04.320\n so let's try it again.\n\n39:04.320 --> 39:07.680\n And that's how we started pushing on eager execution.\n\n39:07.680 --> 39:09.880\n How do we combine those two together?\n\n39:09.880 --> 39:13.120\n Which has finally come very well together in 2.0,\n\n39:13.120 --> 39:15.760\n but it took us a while to get all the things together\n\n39:15.760 --> 39:16.600\n and so on.\n\n39:16.600 --> 39:19.320\n So let me ask, put another way,\n\n39:19.320 --> 39:21.800\n I think eager execution is a really powerful thing\n\n39:21.800 --> 39:22.640\n that was added.\n\n39:22.640 --> 39:24.440\n Do you think it wouldn't have been,\n\n39:25.800 --> 39:28.360\n you know, Muhammad Ali versus Frasier, right?\n\n39:28.360 --> 39:31.160\n Do you think it wouldn't have been added as quickly\n\n39:31.160 --> 39:33.740\n if PyTorch wasn't there?\n\n39:33.740 --> 39:35.400\n It might have taken longer.\n\n39:35.400 --> 39:36.240\n No longer?\n\n39:36.240 --> 39:37.080\n Yeah, it was, I mean,\n\n39:37.080 --> 39:38.900\n we had tried some variants of that before,\n\n39:38.900 --> 39:40.900\n so I'm sure it would have happened,\n\n39:40.900 --> 39:42.220\n but it might have taken longer.\n\n39:42.220 --> 39:44.080\n I'm grateful that TensorFlow is finally\n\n39:44.080 --> 39:44.920\n in the way they did.\n\n39:44.920 --> 39:47.740\n It's doing some incredible work last couple years.\n\n39:47.740 --> 39:49.600\n What other things that we didn't talk about\n\n39:49.600 --> 39:51.480\n are you looking forward in 2.0?\n\n39:51.480 --> 39:54.040\n That comes to mind.\n\n39:54.040 --> 39:56.520\n So we talked about some of the ecosystem stuff,\n\n39:56.520 --> 40:00.000\n making it easily accessible to Keras,\n\n40:00.000 --> 40:01.440\n eager execution.\n\n40:01.440 --> 40:03.000\n Is there other things that we missed?\n\n40:03.000 --> 40:07.500\n Yeah, so I would say one is just where 2.0 is,\n\n40:07.500 --> 40:10.740\n and you know, with all the things that we've talked about,\n\n40:10.740 --> 40:13.760\n I think as we think beyond that,\n\n40:13.760 --> 40:16.600\n there are lots of other things that it enables us to do\n\n40:16.600 --> 40:18.760\n and that we're excited about.\n\n40:18.760 --> 40:20.720\n So what it's setting us up for,\n\n40:20.720 --> 40:22.520\n okay, here are these really clean APIs.\n\n40:22.520 --> 40:25.640\n We've cleaned up the surface for what the users want.\n\n40:25.640 --> 40:28.320\n What it also allows us to do a whole bunch of stuff\n\n40:28.320 --> 40:31.600\n behind the scenes once we are ready with 2.0.\n\n40:31.600 --> 40:36.600\n So for example, in TensorFlow with graphs\n\n40:36.740 --> 40:37.720\n and all the things you could do,\n\n40:37.720 --> 40:40.600\n you could always get a lot of good performance\n\n40:40.600 --> 40:43.280\n if you spent the time to tune it, right?\n\n40:43.280 --> 40:47.720\n And we've clearly shown that, lots of people do that.\n\n40:47.720 --> 40:52.720\n With 2.0, with these APIs, where we are,\n\n40:53.040 --> 40:55.140\n we can give you a lot of performance\n\n40:55.140 --> 40:57.040\n just with whatever you do.\n\n40:57.040 --> 41:01.400\n You know, because we see these, it's much cleaner.\n\n41:01.400 --> 41:03.740\n We know most people are gonna do things this way.\n\n41:03.740 --> 41:05.520\n We can really optimize for that\n\n41:05.520 --> 41:09.040\n and get a lot of those things out of the box.\n\n41:09.040 --> 41:10.360\n And it really allows us, you know,\n\n41:10.360 --> 41:13.880\n both for single machine and distributed and so on,\n\n41:13.880 --> 41:17.200\n to really explore other spaces behind the scenes\n\n41:17.200 --> 41:19.720\n after 2.0 in the future versions as well.\n\n41:19.720 --> 41:23.040\n So right now the team's really excited about that,\n\n41:23.040 --> 41:25.840\n that over time I think we'll see that.\n\n41:25.840 --> 41:27.760\n The other piece that I was talking about\n\n41:27.760 --> 41:31.640\n in terms of just restructuring the monolithic thing\n\n41:31.640 --> 41:34.360\n into more pieces and making it more modular,\n\n41:34.360 --> 41:36.840\n I think that's gonna be really important\n\n41:36.840 --> 41:41.800\n for a lot of the other people in the ecosystem,\n\n41:41.800 --> 41:44.840\n other organizations and so on that wanted to build things.\n\n41:44.840 --> 41:46.400\n Can you elaborate a little bit what you mean\n\n41:46.400 --> 41:50.720\n by making TensorFlow ecosystem more modular?\n\n41:50.720 --> 41:55.040\n So the way it's organized today is there's one,\n\n41:55.040 --> 41:56.320\n there are lots of repositories\n\n41:56.320 --> 41:58.360\n in the TensorFlow organization at GitHub.\n\n41:58.360 --> 42:01.120\n The core one where we have TensorFlow,\n\n42:01.120 --> 42:04.120\n it has the execution engine,\n\n42:04.120 --> 42:08.320\n it has the key backends for CPUs and GPUs,\n\n42:08.320 --> 42:12.580\n it has the work to do distributed stuff.\n\n42:12.580 --> 42:14.420\n And all of these just work together\n\n42:14.420 --> 42:17.280\n in a single library or binary.\n\n42:17.280 --> 42:18.840\n There's no way to split them apart easily.\n\n42:18.840 --> 42:20.000\n I mean, there are some interfaces,\n\n42:20.000 --> 42:21.640\n but they're not very clean.\n\n42:21.640 --> 42:24.860\n In a perfect world, you would have clean interfaces where,\n\n42:24.860 --> 42:27.760\n okay, I wanna run it on my fancy cluster\n\n42:27.760 --> 42:29.400\n with some custom networking,\n\n42:29.400 --> 42:31.000\n just implement this and do that.\n\n42:31.000 --> 42:32.680\n I mean, we kind of support that,\n\n42:32.680 --> 42:34.620\n but it's hard for people today.\n\n42:35.520 --> 42:38.180\n I think as we are starting to see more interesting things\n\n42:38.180 --> 42:39.440\n in some of these spaces,\n\n42:39.440 --> 42:42.360\n having that clean separation will really start to help.\n\n42:42.360 --> 42:47.360\n And again, going to the large size of the ecosystem\n\n42:47.360 --> 42:50.140\n and the different groups involved there,\n\n42:50.140 --> 42:52.580\n enabling people to evolve\n\n42:52.580 --> 42:54.360\n and push on things more independently\n\n42:54.360 --> 42:56.040\n just allows it to scale better.\n\n42:56.040 --> 42:59.080\n And by people, you mean individual developers and?\n\n42:59.080 --> 42:59.960\n And organizations.\n\n42:59.960 --> 43:00.960\n And organizations.\n\n43:00.960 --> 43:01.800\n That's right.\n\n43:01.800 --> 43:04.240\n So the hope is that everybody sort of major,\n\n43:04.240 --> 43:06.900\n I don't know, Pepsi or something uses,\n\n43:06.900 --> 43:11.040\n like major corporations go to TensorFlow to this kind of.\n\n43:11.040 --> 43:13.640\n Yeah, if you look at enterprises like Pepsi or these,\n\n43:13.640 --> 43:15.800\n I mean, a lot of them are already using TensorFlow.\n\n43:15.800 --> 43:18.920\n They are not the ones that do the development\n\n43:18.920 --> 43:20.360\n or changes in the core.\n\n43:20.360 --> 43:21.960\n Some of them do, but a lot of them don't.\n\n43:21.960 --> 43:23.720\n I mean, they touch small pieces.\n\n43:23.720 --> 43:25.660\n There are lots of these,\n\n43:25.660 --> 43:27.660\n some of them being, let's say, hardware vendors\n\n43:27.660 --> 43:28.960\n who are building their custom hardware\n\n43:28.960 --> 43:30.840\n and they want their own pieces.\n\n43:30.840 --> 43:34.160\n Or some of them being bigger companies, say, IBM.\n\n43:34.160 --> 43:36.480\n I mean, they're involved in some of our\n\n43:36.480 --> 43:38.100\n special interest groups,\n\n43:38.100 --> 43:39.960\n and they see a lot of users\n\n43:39.960 --> 43:42.620\n who want certain things and they want to optimize for that.\n\n43:42.620 --> 43:44.440\n So folks like that often.\n\n43:44.440 --> 43:46.360\n Autonomous vehicle companies, perhaps.\n\n43:46.360 --> 43:48.160\n Exactly, yes.\n\n43:48.160 --> 43:50.000\n So, yeah, like I mentioned,\n\n43:50.000 --> 43:52.760\n TensorFlow has been downloaded 41 million times,\n\n43:52.760 --> 43:56.480\n 50,000 commits, almost 10,000 pull requests,\n\n43:56.480 --> 43:58.320\n and 1,800 contributors.\n\n43:58.320 --> 44:02.120\n So I'm not sure if you can explain it,\n\n44:02.120 --> 44:06.000\n but what does it take to build a community like that?\n\n44:06.000 --> 44:09.160\n In retrospect, what do you think,\n\n44:09.160 --> 44:11.180\n what is the critical thing that allowed\n\n44:11.180 --> 44:12.640\n for this growth to happen,\n\n44:12.640 --> 44:14.600\n and how does that growth continue?\n\n44:14.600 --> 44:17.920\n Yeah, yeah, that's an interesting question.\n\n44:17.920 --> 44:20.240\n I wish I had all the answers there, I guess,\n\n44:20.240 --> 44:21.600\n so you could replicate it.\n\n44:22.520 --> 44:25.560\n I think there are a number of things\n\n44:25.560 --> 44:27.880\n that need to come together, right?\n\n44:27.880 --> 44:32.480\n One, just like any new thing,\n\n44:32.480 --> 44:35.920\n it is about, there's a sweet spot of timing,\n\n44:35.920 --> 44:38.880\n what's needed, does it grow with,\n\n44:38.880 --> 44:41.640\n what's needed, so in this case, for example,\n\n44:41.640 --> 44:43.680\n TensorFlow's not just grown because it was a good tool,\n\n44:43.680 --> 44:46.720\n it's also grown with the growth of deep learning itself.\n\n44:46.720 --> 44:49.040\n So those factors come into play.\n\n44:49.040 --> 44:50.360\n Other than that, though,\n\n44:52.080 --> 44:55.240\n I think just hearing, listening to the community,\n\n44:55.240 --> 44:57.040\n what they do, what they need,\n\n44:57.040 --> 45:01.120\n being open to, like in terms of external contributions,\n\n45:01.120 --> 45:04.560\n we've spent a lot of time in making sure\n\n45:04.560 --> 45:06.880\n we can accept those contributions well,\n\n45:06.880 --> 45:09.480\n we can help the contributors in adding those,\n\n45:09.480 --> 45:11.320\n putting the right process in place,\n\n45:11.320 --> 45:13.360\n getting the right kind of community,\n\n45:13.360 --> 45:15.180\n welcoming them and so on.\n\n45:16.160 --> 45:19.320\n Like over the last year, we've really pushed on transparency,\n\n45:19.320 --> 45:22.280\n that's important for an open source project.\n\n45:22.280 --> 45:23.800\n People wanna know where things are going,\n\n45:23.800 --> 45:26.200\n and we're like, okay, here's a process\n\n45:26.200 --> 45:29.360\n where you can do that, here are our RFCs and so on.\n\n45:29.360 --> 45:32.920\n So thinking through, there are lots of community aspects\n\n45:32.920 --> 45:35.460\n that come into that you can really work on.\n\n45:35.460 --> 45:38.740\n As a small project, it's maybe easy to do\n\n45:38.740 --> 45:42.180\n because there's like two developers and you can do those.\n\n45:42.180 --> 45:46.980\n As you grow, putting more of these processes in place,\n\n45:46.980 --> 45:49.140\n thinking about the documentation,\n\n45:49.140 --> 45:51.940\n thinking about what two developers care about,\n\n45:51.940 --> 45:55.180\n what kind of tools would they want to use,\n\n45:55.180 --> 45:56.900\n all of these come into play, I think.\n\n45:56.900 --> 45:58.420\n So one of the big things I think\n\n45:58.420 --> 46:00.700\n that feeds the TensorFlow fire\n\n46:00.700 --> 46:03.980\n is people building something on TensorFlow,\n\n46:03.980 --> 46:07.700\n and implement a particular architecture\n\n46:07.700 --> 46:09.500\n that does something cool and useful,\n\n46:09.500 --> 46:11.100\n and they put that on GitHub.\n\n46:11.100 --> 46:15.580\n And so it just feeds this growth.\n\n46:15.580 --> 46:19.580\n Do you have a sense that with 2.0 and 1.0\n\n46:19.580 --> 46:21.580\n that there may be a little bit of a partitioning\n\n46:21.580 --> 46:24.100\n like there is with Python 2 and 3,\n\n46:24.100 --> 46:26.040\n that there'll be a code base\n\n46:26.040 --> 46:28.340\n and in the older versions of TensorFlow,\n\n46:28.340 --> 46:31.140\n they will not be as compatible easily?\n\n46:31.140 --> 46:35.620\n Or are you pretty confident that this kind of conversion\n\n46:35.620 --> 46:37.980\n is pretty natural and easy to do?\n\n46:37.980 --> 46:39.980\n So we're definitely working hard\n\n46:39.980 --> 46:41.500\n to make that very easy to do.\n\n46:41.500 --> 46:43.500\n There's lots of tooling that we talked about\n\n46:43.500 --> 46:45.820\n at the developer summit this week,\n\n46:45.820 --> 46:48.260\n and we'll continue to invest in that tooling.\n\n46:48.260 --> 46:50.500\n It's, you know, when you think\n\n46:50.500 --> 46:52.580\n of these significant version changes,\n\n46:52.580 --> 46:53.580\n that's always a risk,\n\n46:53.580 --> 46:55.740\n and we are really pushing hard\n\n46:55.740 --> 46:58.100\n to make that transition very, very smooth.\n\n46:58.100 --> 47:02.700\n So I think, so at some level,\n\n47:02.700 --> 47:05.620\n people wanna move and they see the value in the new thing.\n\n47:05.620 --> 47:07.740\n They don't wanna move just because it's a new thing,\n\n47:07.740 --> 47:08.580\n and some people do,\n\n47:08.580 --> 47:11.540\n but most people want a really good thing.\n\n47:11.540 --> 47:13.820\n And I think over the next few months,\n\n47:13.820 --> 47:15.460\n as people start to see the value,\n\n47:15.460 --> 47:17.700\n we'll definitely see that shift happening.\n\n47:17.700 --> 47:19.740\n So I'm pretty excited and confident\n\n47:19.740 --> 47:21.680\n that we will see people moving.\n\n47:22.540 --> 47:24.740\n As you said earlier, this field is also moving rapidly,\n\n47:24.740 --> 47:26.780\n so that'll help because we can do more things\n\n47:26.780 --> 47:29.500\n and all the new things will clearly happen in 2.x,\n\n47:29.500 --> 47:32.300\n so people will have lots of good reasons to move.\n\n47:32.300 --> 47:36.140\n So what do you think TensorFlow 3.0 looks like?\n\n47:36.140 --> 47:40.340\n Is there, are things happening so crazily\n\n47:40.340 --> 47:42.540\n that even at the end of this year\n\n47:42.540 --> 47:44.340\n seems impossible to plan for?\n\n47:45.300 --> 47:49.420\n Or is it possible to plan for the next five years?\n\n47:49.420 --> 47:50.820\n I think it's tricky.\n\n47:50.820 --> 47:54.540\n There are some things that we can expect\n\n47:54.540 --> 47:57.900\n in terms of, okay, change, yes, change is gonna happen.\n\n47:59.700 --> 48:01.660\n Are there some things gonna stick around\n\n48:01.660 --> 48:03.740\n and some things not gonna stick around?\n\n48:03.740 --> 48:08.140\n I would say the basics of deep learning,\n\n48:08.140 --> 48:10.420\n the, you know, say convolution models\n\n48:10.420 --> 48:12.700\n or the basic kind of things,\n\n48:12.700 --> 48:16.300\n they'll probably be around in some form still in five years.\n\n48:16.300 --> 48:18.620\n Will RL and GAN stay?\n\n48:18.620 --> 48:21.180\n Very likely, based on where they are.\n\n48:21.180 --> 48:22.860\n Will we have new things?\n\n48:22.860 --> 48:24.660\n Probably, but those are hard to predict.\n\n48:24.660 --> 48:29.660\n And some directionally, some things that we can see is,\n\n48:30.620 --> 48:32.740\n you know, in things that we're starting to do, right,\n\n48:32.740 --> 48:35.420\n with some of our projects right now\n\n48:35.420 --> 48:39.140\n is just 2.0 combining eager execution and graphs\n\n48:39.140 --> 48:41.460\n where we're starting to make it more like\n\n48:41.460 --> 48:43.140\n just your natural programming language.\n\n48:43.140 --> 48:45.660\n You're not trying to program something else.\n\n48:45.660 --> 48:47.220\n Similarly, with Swift for TensorFlow,\n\n48:47.220 --> 48:48.260\n we're taking that approach.\n\n48:48.260 --> 48:50.020\n Can you do something ground up, right?\n\n48:50.020 --> 48:52.100\n So some of those ideas seem like, okay,\n\n48:52.100 --> 48:54.100\n that's the right direction.\n\n48:54.100 --> 48:57.140\n In five years, we expect to see more in that area.\n\n48:58.340 --> 49:00.060\n Other things we don't know is,\n\n49:00.060 --> 49:03.180\n will hardware accelerators be the same?\n\n49:03.180 --> 49:06.620\n Will we be able to train with four bits\n\n49:06.620 --> 49:08.140\n instead of 32 bits?\n\n49:09.020 --> 49:11.500\n And I think the TPU side of things is exploring that.\n\n49:11.500 --> 49:13.940\n I mean, TPU is already on version three.\n\n49:13.940 --> 49:17.540\n It seems that the evolution of TPU and TensorFlow\n\n49:17.540 --> 49:22.540\n are sort of, they're coevolving almost in terms of\n\n49:23.260 --> 49:25.740\n both are learning from each other and from the community\n\n49:25.740 --> 49:27.980\n and from the applications\n\n49:27.980 --> 49:29.740\n where the biggest benefit is achieved.\n\n49:29.740 --> 49:30.580\n That's right.\n\n49:30.580 --> 49:33.340\n You've been trying to sort of, with Eager, with Keras,\n\n49:33.340 --> 49:34.940\n to make TensorFlow as accessible\n\n49:34.940 --> 49:36.500\n and easy to use as possible.\n\n49:36.500 --> 49:38.060\n What do you think, for beginners,\n\n49:38.060 --> 49:40.020\n is the biggest thing they struggle with?\n\n49:40.020 --> 49:42.100\n Have you encountered that?\n\n49:42.100 --> 49:46.260\n Or is basically what Keras is solving is that Eager,\n\n49:46.260 --> 49:47.420\n like we talked about?\n\n49:47.420 --> 49:50.620\n Yeah, for some of them, like you said, right,\n\n49:50.620 --> 49:53.620\n the beginners want to just be able to take\n\n49:53.620 --> 49:54.900\n some image model,\n\n49:54.900 --> 49:57.060\n they don't care if it's Inception or ResNet\n\n49:57.060 --> 49:58.100\n or something else,\n\n49:58.100 --> 50:00.820\n and do some training or transfer learning\n\n50:00.820 --> 50:02.500\n on their kind of model.\n\n50:02.500 --> 50:04.460\n Being able to make that easy is important.\n\n50:04.460 --> 50:07.060\n So in some ways,\n\n50:07.060 --> 50:09.380\n if you do that by providing them simple models\n\n50:09.380 --> 50:11.420\n with say, in hub or so on,\n\n50:11.420 --> 50:13.780\n they don't care about what's inside that box,\n\n50:13.780 --> 50:15.180\n but they want to be able to use it.\n\n50:15.180 --> 50:17.660\n So we're pushing on, I think, different levels.\n\n50:17.660 --> 50:20.020\n If you look at just a component that you get,\n\n50:20.020 --> 50:22.820\n which has the layers already smooshed in,\n\n50:22.820 --> 50:25.260\n the beginners probably just want that.\n\n50:25.260 --> 50:26.780\n Then the next step is, okay,\n\n50:26.780 --> 50:29.100\n look at building layers with Keras.\n\n50:29.100 --> 50:30.300\n If you go out to research,\n\n50:30.300 --> 50:33.180\n then they are probably writing custom layers themselves\n\n50:33.180 --> 50:34.460\n or doing their own loops.\n\n50:34.460 --> 50:36.380\n So there's a whole spectrum there.\n\n50:36.380 --> 50:38.660\n And then providing the pre trained models\n\n50:38.660 --> 50:43.660\n seems to really decrease the time from you trying to start.\n\n50:43.660 --> 50:46.860\n You could basically in a Colab notebook\n\n50:46.860 --> 50:48.220\n achieve what you need.\n\n50:49.140 --> 50:51.340\n So I'm basically answering my own question\n\n50:51.340 --> 50:54.300\n because I think what TensorFlow delivered on recently\n\n50:54.300 --> 50:56.980\n is trivial for beginners.\n\n50:56.980 --> 51:00.780\n So I was just wondering if there was other pain points\n\n51:00.780 --> 51:01.620\n you're trying to ease,\n\n51:01.620 --> 51:02.540\n but I'm not sure there would.\n\n51:02.540 --> 51:04.900\n No, those are probably the big ones.\n\n51:04.900 --> 51:07.420\n I see high schoolers doing a whole bunch of things now,\n\n51:07.420 --> 51:09.220\n which is pretty amazing.\n\n51:09.220 --> 51:11.420\n It's both amazing and terrifying.\n\n51:11.420 --> 51:12.700\n Yes.\n\n51:12.700 --> 51:14.980\n In a sense that when they grow up,\n\n51:15.940 --> 51:19.300\n it's some incredible ideas will be coming from them.\n\n51:19.300 --> 51:21.860\n So there's certainly a technical aspect to your work,\n\n51:21.860 --> 51:25.260\n but you also have a management aspect to your role\n\n51:25.260 --> 51:27.980\n with TensorFlow leading the project,\n\n51:27.980 --> 51:31.140\n a large number of developers and people.\n\n51:31.140 --> 51:34.700\n So what do you look for in a good team?\n\n51:34.700 --> 51:36.220\n What do you think?\n\n51:36.220 --> 51:38.420\n Google has been at the forefront of exploring\n\n51:38.420 --> 51:40.500\n what it takes to build a good team\n\n51:40.500 --> 51:45.500\n and TensorFlow is one of the most cutting edge technologies\n\n51:45.540 --> 51:46.380\n in the world.\n\n51:46.380 --> 51:49.340\n So in this context, what do you think makes for a good team?\n\n51:50.500 --> 51:53.180\n It's definitely something I think a favorite about.\n\n51:53.180 --> 51:58.180\n I think in terms of the team being able\n\n51:59.780 --> 52:01.180\n to deliver something well,\n\n52:01.180 --> 52:04.780\n one of the things that's important is a cohesion\n\n52:04.780 --> 52:05.820\n across the team.\n\n52:05.820 --> 52:10.420\n So being able to execute together in doing things\n\n52:10.420 --> 52:13.180\n that's not an end, like at this scale,\n\n52:13.180 --> 52:15.460\n an individual engineer can only do so much.\n\n52:15.460 --> 52:18.260\n There's a lot more that they can do together,\n\n52:18.260 --> 52:21.780\n even though we have some amazing superstars across Google\n\n52:21.780 --> 52:25.140\n and in the team, but there's, you know,\n\n52:25.140 --> 52:27.380\n often the way I see it as the product\n\n52:27.380 --> 52:29.140\n of what the team generates is way larger\n\n52:29.140 --> 52:34.140\n than the whole or the individual put together.\n\n52:34.460 --> 52:37.380\n And so how do we have all of them work together,\n\n52:37.380 --> 52:40.060\n the culture of the team itself,\n\n52:40.060 --> 52:42.020\n hiring good people is important.\n\n52:43.100 --> 52:45.340\n But part of that is it's not just that,\n\n52:45.340 --> 52:47.260\n okay, we hire a bunch of smart people\n\n52:47.260 --> 52:49.740\n and throw them together and let them do things.\n\n52:49.740 --> 52:52.980\n It's also people have to care about what they're building,\n\n52:52.980 --> 52:57.380\n people have to be motivated for the right kind of things.\n\n52:57.380 --> 52:59.840\n That's often an important factor.\n\n53:01.500 --> 53:04.660\n And, you know, finally, how do you put that together\n\n53:04.660 --> 53:08.860\n with a somewhat unified vision of where we wanna go?\n\n53:08.860 --> 53:11.220\n So are we all looking in the same direction\n\n53:11.220 --> 53:13.620\n or each of us going all over?\n\n53:13.620 --> 53:16.100\n And sometimes it's a mix.\n\n53:16.100 --> 53:20.580\n Google's a very bottom up organization in some sense,\n\n53:21.460 --> 53:26.420\n also research even more so, and that's how we started.\n\n53:26.420 --> 53:30.900\n But as we've become this larger product and ecosystem,\n\n53:30.900 --> 53:33.180\n I think it's also important to combine that well\n\n53:33.180 --> 53:38.020\n with a mix of, okay, here's the direction we wanna go in.\n\n53:38.020 --> 53:39.860\n There is exploration we'll do around that,\n\n53:39.860 --> 53:42.820\n but let's keep staying in that direction,\n\n53:42.820 --> 53:44.460\n not just all over the place.\n\n53:44.460 --> 53:46.860\n And is there a way you monitor the health of the team?\n\n53:46.860 --> 53:51.860\n Sort of like, is there a way you know you did a good job?\n\n53:51.980 --> 53:53.020\n The team is good?\n\n53:53.020 --> 53:56.220\n Like, I mean, you're sort of, you're saying nice things,\n\n53:56.220 --> 54:00.860\n but it's sometimes difficult to determine how aligned.\n\n54:00.860 --> 54:01.700\n Yes.\n\n54:01.700 --> 54:02.520\n Because it's not binary.\n\n54:02.520 --> 54:06.740\n It's not like there's tensions and complexities and so on.\n\n54:06.740 --> 54:09.460\n And the other element of the mission of superstars,\n\n54:09.460 --> 54:11.820\n there's so much, even at Google,\n\n54:11.820 --> 54:13.220\n such a large percentage of work\n\n54:13.220 --> 54:16.020\n is done by individual superstars too.\n\n54:16.020 --> 54:19.980\n So there's a, and sometimes those superstars\n\n54:19.980 --> 54:23.280\n can be against the dynamic of a team and those tensions.\n\n54:25.220 --> 54:26.580\n I mean, I'm sure in TensorFlow it might be\n\n54:26.580 --> 54:28.900\n a little bit easier because the mission of the project\n\n54:28.900 --> 54:31.740\n is so sort of beautiful.\n\n54:31.740 --> 54:34.860\n You're at the cutting edge, so it's exciting.\n\n54:34.860 --> 54:36.700\n But have you had struggle with that?\n\n54:36.700 --> 54:38.380\n Has there been challenges?\n\n54:38.380 --> 54:39.860\n There are always people challenges\n\n54:39.860 --> 54:41.260\n in different kinds of ways.\n\n54:41.260 --> 54:44.780\n That said, I think we've been what's good\n\n54:44.780 --> 54:48.980\n about getting people who care and are, you know,\n\n54:48.980 --> 54:50.420\n have the same kind of culture,\n\n54:50.420 --> 54:53.460\n and that's Google in general to a large extent.\n\n54:53.460 --> 54:56.140\n But also, like you said, given that the project\n\n54:56.140 --> 54:58.780\n has had so many exciting things to do,\n\n54:58.780 --> 55:00.760\n there's been room for lots of people\n\n55:00.760 --> 55:02.460\n to do different kinds of things and grow,\n\n55:02.460 --> 55:05.380\n which does make the problem a bit easier, I guess.\n\n55:05.380 --> 55:09.940\n And it allows people, depending on what they're doing,\n\n55:09.940 --> 55:13.140\n if there's room around them, then that's fine.\n\n55:13.140 --> 55:18.140\n But yes, we do care about whether a superstar or not,\n\n55:19.220 --> 55:22.580\n that they need to work well with the team across Google.\n\n55:22.580 --> 55:23.680\n That's interesting to hear.\n\n55:23.680 --> 55:26.500\n So it's like superstar or not,\n\n55:26.500 --> 55:30.540\n the productivity broadly is about the team.\n\n55:30.540 --> 55:31.540\n Yeah, yeah.\n\n55:31.540 --> 55:32.980\n I mean, they might add a lot of value,\n\n55:32.980 --> 55:35.740\n but if they're hurting the team, then that's a problem.\n\n55:35.740 --> 55:39.060\n So in hiring engineers, it's so interesting, right,\n\n55:39.060 --> 55:40.260\n the hiring process.\n\n55:40.260 --> 55:41.860\n What do you look for?\n\n55:41.860 --> 55:44.300\n How do you determine a good developer\n\n55:44.300 --> 55:46.240\n or a good member of a team\n\n55:46.240 --> 55:48.560\n from just a few minutes or hours together?\n\n55:50.420 --> 55:52.260\n Again, no magic answers, I'm sure.\n\n55:52.260 --> 55:55.340\n Yeah, I mean, Google has a hiring process\n\n55:55.340 --> 55:59.660\n that we've refined over the last 20 years, I guess,\n\n55:59.660 --> 56:02.220\n and that you've probably heard and seen a lot about.\n\n56:02.220 --> 56:04.980\n So we do work with the same hiring process\n\n56:04.980 --> 56:06.480\n and that's really helped.\n\n56:08.340 --> 56:10.900\n For me in particular, I would say,\n\n56:10.900 --> 56:14.220\n in addition to the core technical skills,\n\n56:14.220 --> 56:17.580\n what does matter is their motivation\n\n56:17.580 --> 56:19.600\n in what they wanna do.\n\n56:19.600 --> 56:21.380\n Because if that doesn't align well\n\n56:21.380 --> 56:22.980\n with where we wanna go,\n\n56:22.980 --> 56:25.360\n that's not gonna lead to long term success\n\n56:25.360 --> 56:26.860\n for either them or the team.\n\n56:27.700 --> 56:30.020\n And I think that becomes more important\n\n56:30.020 --> 56:31.480\n the more senior the person is,\n\n56:31.480 --> 56:33.580\n but it's important at every level.\n\n56:33.580 --> 56:34.940\n Like even the junior most engineer,\n\n56:34.940 --> 56:36.380\n if they're not motivated to do well\n\n56:36.380 --> 56:37.700\n at what they're trying to do,\n\n56:37.700 --> 56:38.820\n however smart they are,\n\n56:38.820 --> 56:40.380\n it's gonna be hard for them to succeed.\n\n56:40.380 --> 56:44.540\n Does the Google hiring process touch on that passion?\n\n56:44.540 --> 56:46.500\n So like trying to determine,\n\n56:46.500 --> 56:48.500\n because I think as far as I understand,\n\n56:48.500 --> 56:49.620\n maybe you can speak to it,\n\n56:49.620 --> 56:53.380\n that the Google hiring process sort of helps\n\n56:53.380 --> 56:56.380\n in the initial like determines the skill set there,\n\n56:56.380 --> 56:57.940\n is your puzzle solving ability,\n\n56:57.940 --> 56:59.920\n problem solving ability good?\n\n56:59.920 --> 57:02.540\n But like, I'm not sure,\n\n57:02.540 --> 57:05.040\n but it seems that the determining\n\n57:05.040 --> 57:07.580\n whether the person is like fire inside them,\n\n57:07.580 --> 57:09.060\n that burns to do anything really,\n\n57:09.060 --> 57:09.900\n it doesn't really matter.\n\n57:09.900 --> 57:11.540\n It's just some cool stuff,\n\n57:11.540 --> 57:12.500\n I'm gonna do it.\n\n57:15.340 --> 57:17.300\n Is that something that ultimately ends up\n\n57:17.300 --> 57:18.820\n when they have a conversation with you\n\n57:18.820 --> 57:22.640\n or once it gets closer to the team?\n\n57:22.640 --> 57:25.420\n So one of the things we do have as part of the process\n\n57:25.420 --> 57:27.180\n is just a culture fit,\n\n57:27.180 --> 57:29.200\n like part of the interview process itself,\n\n57:29.200 --> 57:31.020\n in addition to just the technical skills\n\n57:31.020 --> 57:34.260\n and each engineer or whoever the interviewer is,\n\n57:34.260 --> 57:38.340\n is supposed to rate the person on the culture\n\n57:38.340 --> 57:40.000\n and the culture fit with Google and so on.\n\n57:40.000 --> 57:42.180\n So that is definitely part of the process.\n\n57:42.180 --> 57:45.860\n Now, there are various kinds of projects\n\n57:45.860 --> 57:46.940\n and different kinds of things.\n\n57:46.940 --> 57:48.820\n So there might be variants\n\n57:48.820 --> 57:51.380\n and of the kind of culture you want there and so on.\n\n57:51.380 --> 57:52.740\n And yes, that does vary.\n\n57:52.740 --> 57:54.020\n So for example,\n\n57:54.020 --> 57:56.980\n TensorFlow has always been a fast moving project\n\n57:56.980 --> 57:59.380\n and we want people who are comfortable with that.\n\n58:00.980 --> 58:02.700\n But at the same time now, for example,\n\n58:02.700 --> 58:05.260\n we are at a place where we are also very full fledged product\n\n58:05.260 --> 58:07.820\n and we wanna make sure things that work\n\n58:07.820 --> 58:09.340\n really, really work, right?\n\n58:09.340 --> 58:11.700\n You can't cut corners all the time.\n\n58:11.700 --> 58:14.340\n So balancing that out and finding the people\n\n58:14.340 --> 58:17.580\n who are the right fit for those is important.\n\n58:17.580 --> 58:19.740\n And I think those kinds of things do vary a bit\n\n58:19.740 --> 58:23.220\n across projects and teams and product areas across Google.\n\n58:23.220 --> 58:25.260\n And so you'll see some differences there\n\n58:25.260 --> 58:27.700\n in the final checklist.\n\n58:27.700 --> 58:29.380\n But a lot of the core culture,\n\n58:29.380 --> 58:32.220\n it comes along with just the engineering excellence\n\n58:32.220 --> 58:33.040\n and so on.\n\n58:34.740 --> 58:37.600\n What is the hardest part of your job?\n\n58:39.780 --> 58:41.940\n I'll take your pick, I guess.\n\n58:41.940 --> 58:44.460\n It's fun, I would say, right?\n\n58:44.460 --> 58:45.540\n Hard, yes.\n\n58:45.540 --> 58:47.280\n I mean, lots of things at different times.\n\n58:47.280 --> 58:49.220\n I think that does vary.\n\n58:49.220 --> 58:52.680\n So let me clarify that difficult things are fun\n\n58:52.680 --> 58:53.980\n when you solve them, right?\n\n58:53.980 --> 58:57.500\n So it's fun in that sense.\n\n58:57.500 --> 59:02.500\n I think the key to a successful thing across the board\n\n59:02.640 --> 59:05.380\n and in this case, it's a large ecosystem now,\n\n59:05.380 --> 59:07.180\n but even a small product,\n\n59:07.180 --> 59:09.820\n is striking that fine balance\n\n59:09.820 --> 59:12.060\n across different aspects of it.\n\n59:12.060 --> 59:13.940\n Sometimes it's how fast do you go\n\n59:13.940 --> 59:17.060\n versus how perfect it is.\n\n59:17.060 --> 59:21.460\n Sometimes it's how do you involve this huge community?\n\n59:21.460 --> 59:23.640\n Who do you involve or do you decide,\n\n59:23.640 --> 59:25.480\n okay, now is not a good time to involve them\n\n59:25.480 --> 59:28.640\n because it's not the right fit.\n\n59:30.220 --> 59:33.660\n Sometimes it's saying no to certain kinds of things.\n\n59:33.660 --> 59:35.760\n Those are often the hard decisions.\n\n59:36.860 --> 59:39.600\n Some of them you make quickly\n\n59:39.600 --> 59:41.020\n because you don't have the time.\n\n59:41.020 --> 59:43.220\n Some of them you get time to think about them,\n\n59:43.220 --> 59:44.500\n but they're always hard.\n\n59:44.500 --> 59:49.220\n So both choices are pretty good, those decisions.\n\n59:49.220 --> 59:50.380\n What about deadlines?\n\n59:50.380 --> 59:53.580\n Is this, do you find TensorFlow,\n\n59:53.580 --> 59:58.220\n to be driven by deadlines\n\n59:58.220 --> 1:00:00.400\n to a degree that a product might?\n\n1:00:00.400 --> 1:00:04.940\n Or is there still a balance to where it's less deadline?\n\n1:00:04.940 --> 1:00:06.740\n You had the Dev Summit today\n\n1:00:06.740 --> 1:00:08.940\n that came together incredibly.\n\n1:00:08.940 --> 1:00:11.460\n Looked like there's a lot of moving pieces and so on.\n\n1:00:11.460 --> 1:00:15.140\n So did that deadline make people rise to the occasion\n\n1:00:15.140 --> 1:00:18.420\n releasing TensorFlow 2.0 alpha?\n\n1:00:18.420 --> 1:00:20.420\n I'm sure that was done last minute as well.\n\n1:00:20.420 --> 1:00:25.420\n I mean, up to the last point.\n\n1:00:25.620 --> 1:00:26.860\n Again, it's one of those things\n\n1:00:26.860 --> 1:00:29.940\n that you need to strike the good balance.\n\n1:00:29.940 --> 1:00:32.100\n There's some value that deadlines bring\n\n1:00:32.100 --> 1:00:33.980\n that does bring a sense of urgency\n\n1:00:33.980 --> 1:00:35.780\n to get the right things together.\n\n1:00:35.780 --> 1:00:38.340\n Instead of getting the perfect thing out,\n\n1:00:38.340 --> 1:00:41.320\n you need something that's good and works well.\n\n1:00:41.320 --> 1:00:43.260\n And the team definitely did a great job\n\n1:00:43.260 --> 1:00:44.100\n in putting that together.\n\n1:00:44.100 --> 1:00:45.920\n So I was very amazed and excited\n\n1:00:45.920 --> 1:00:47.820\n by everything how that came together.\n\n1:00:48.740 --> 1:00:49.860\n That said, across the year,\n\n1:00:49.860 --> 1:00:52.580\n we try not to put out official deadlines.\n\n1:00:52.580 --> 1:00:57.020\n We focus on key things that are important,\n\n1:00:57.020 --> 1:01:00.620\n figure out how much of it's important.\n\n1:01:00.620 --> 1:01:03.900\n And we are developing in the open,\n\n1:01:03.900 --> 1:01:05.820\n both internally and externally,\n\n1:01:05.820 --> 1:01:07.980\n everything's available to everybody.\n\n1:01:07.980 --> 1:01:11.220\n So you can pick and look at where things are.\n\n1:01:11.220 --> 1:01:13.260\n We do releases at a regular cadence.\n\n1:01:13.260 --> 1:01:16.180\n So fine, if something doesn't necessarily end up\n\n1:01:16.180 --> 1:01:17.820\n this month, it'll end up in the next release\n\n1:01:17.820 --> 1:01:18.780\n in a month or two.\n\n1:01:18.780 --> 1:01:22.860\n And that's okay, but we want to keep moving\n\n1:01:22.860 --> 1:01:25.060\n as fast as we can in these different areas.\n\n1:01:26.500 --> 1:01:29.660\n Because we can iterate and improve on things,\n\n1:01:29.660 --> 1:01:31.960\n sometimes it's okay to put things out\n\n1:01:31.960 --> 1:01:32.980\n that aren't fully ready.\n\n1:01:32.980 --> 1:01:34.580\n We'll make sure it's clear that okay,\n\n1:01:34.580 --> 1:01:36.540\n this is experimental, but it's out there\n\n1:01:36.540 --> 1:01:37.980\n if you want to try and give feedback.\n\n1:01:37.980 --> 1:01:39.420\n That's very, very useful.\n\n1:01:39.420 --> 1:01:42.540\n I think that quick cycle and quick iteration is important.\n\n1:01:43.580 --> 1:01:46.940\n That's what we often focus on rather than\n\n1:01:46.940 --> 1:01:49.220\n here's a deadline where you get everything else.\n\n1:01:49.220 --> 1:01:52.860\n Is 2.0, is there pressure to make that stable?\n\n1:01:52.860 --> 1:01:56.660\n Or like, for example, WordPress 5.0 just came out\n\n1:01:57.780 --> 1:02:00.300\n and there was no pressure to,\n\n1:02:00.300 --> 1:02:03.980\n it was a lot of build updates delivered way too late,\n\n1:02:03.980 --> 1:02:05.980\n but, and they said, okay, well,\n\n1:02:05.980 --> 1:02:07.440\n but we're gonna release a lot of updates\n\n1:02:07.440 --> 1:02:09.660\n really quickly to improve it.\n\n1:02:09.660 --> 1:02:12.220\n Do you see TensorFlow 2.0 in that same kind of way\n\n1:02:12.220 --> 1:02:15.260\n or is there this pressure to once it hits 2.0,\n\n1:02:15.260 --> 1:02:16.780\n once you get to the release candidate\n\n1:02:16.780 --> 1:02:18.980\n and then you get to the final,\n\n1:02:18.980 --> 1:02:22.460\n that's gonna be the stable thing?\n\n1:02:22.460 --> 1:02:25.740\n So it's gonna be stable in,\n\n1:02:25.740 --> 1:02:28.900\n just like when NodeX was where every API that's there\n\n1:02:28.900 --> 1:02:31.080\n is gonna remain in work.\n\n1:02:32.100 --> 1:02:34.820\n It doesn't mean we can't change things under the covers.\n\n1:02:34.820 --> 1:02:36.740\n It doesn't mean we can't add things.\n\n1:02:36.740 --> 1:02:39.200\n So there's still a lot more for us to do\n\n1:02:39.200 --> 1:02:41.100\n and we'll continue to have more releases.\n\n1:02:41.100 --> 1:02:42.640\n So in that sense, there's still,\n\n1:02:42.640 --> 1:02:44.740\n I don't think we'll be done in like two months\n\n1:02:44.740 --> 1:02:46.140\n when we release this.\n\n1:02:46.140 --> 1:02:49.900\n I don't know if you can say, but is there,\n\n1:02:49.900 --> 1:02:53.740\n there's not external deadlines for TensorFlow 2.0,\n\n1:02:53.740 --> 1:02:57.060\n but is there internal deadlines,\n\n1:02:57.060 --> 1:02:58.540\n the artificial or otherwise,\n\n1:02:58.540 --> 1:03:00.860\n that you're trying to set for yourself\n\n1:03:00.860 --> 1:03:03.100\n or is it whenever it's ready?\n\n1:03:03.100 --> 1:03:05.660\n So we want it to be a great product, right?\n\n1:03:05.660 --> 1:03:08.820\n And that's a big important piece for us.\n\n1:03:09.900 --> 1:03:11.140\n TensorFlow's already out there.\n\n1:03:11.140 --> 1:03:13.740\n We have 41 million downloads for 1.0 X.\n\n1:03:13.740 --> 1:03:16.420\n So it's not like we have to have this.\n\n1:03:16.420 --> 1:03:17.260\n Yeah, exactly.\n\n1:03:17.260 --> 1:03:19.340\n So it's not like, a lot of the features\n\n1:03:19.340 --> 1:03:21.180\n that we've really polishing\n\n1:03:21.180 --> 1:03:23.580\n and putting them together are there.\n\n1:03:23.580 --> 1:03:26.220\n We don't have to rush that just because.\n\n1:03:26.220 --> 1:03:28.020\n So in that sense, we wanna get it right\n\n1:03:28.020 --> 1:03:29.940\n and really focus on that.\n\n1:03:29.940 --> 1:03:31.860\n That said, we have said that we are looking\n\n1:03:31.860 --> 1:03:33.500\n to get this out in the next few months,\n\n1:03:33.500 --> 1:03:34.500\n in the next quarter.\n\n1:03:34.500 --> 1:03:37.100\n And as far as possible,\n\n1:03:37.100 --> 1:03:39.780\n we'll definitely try to make that happen.\n\n1:03:39.780 --> 1:03:44.340\n Yeah, my favorite line was, spring is a relative concept.\n\n1:03:44.340 --> 1:03:45.180\n I love it.\n\n1:03:45.180 --> 1:03:46.020\n Yes.\n\n1:03:46.020 --> 1:03:47.700\n Spoken like a true developer.\n\n1:03:47.700 --> 1:03:50.220\n So something I'm really interested in\n\n1:03:50.220 --> 1:03:52.980\n and your previous line of work is,\n\n1:03:52.980 --> 1:03:56.660\n before TensorFlow, you led a team at Google on search ads.\n\n1:03:57.740 --> 1:04:01.860\n I think this is a very interesting topic\n\n1:04:01.860 --> 1:04:04.020\n on every level, on a technical level,\n\n1:04:04.980 --> 1:04:07.220\n because at their best, ads connect people\n\n1:04:07.220 --> 1:04:09.420\n to the things they want and need.\n\n1:04:09.420 --> 1:04:12.300\n So, and at their worst, they're just these things\n\n1:04:12.300 --> 1:04:14.940\n that annoy the heck out of you\n\n1:04:14.940 --> 1:04:17.340\n to the point of ruining the entire user experience\n\n1:04:17.340 --> 1:04:19.140\n of whatever you're actually doing.\n\n1:04:20.260 --> 1:04:22.180\n So they have a bad rep, I guess.\n\n1:04:23.620 --> 1:04:28.100\n And on the other end, so that this connecting users\n\n1:04:28.100 --> 1:04:29.660\n to the thing they need and want\n\n1:04:29.660 --> 1:04:34.060\n is a beautiful opportunity for machine learning to shine.\n\n1:04:34.060 --> 1:04:36.340\n Like huge amounts of data that's personalized\n\n1:04:36.340 --> 1:04:37.860\n and you kind of map to the thing\n\n1:04:37.860 --> 1:04:40.380\n they actually want won't get annoyed.\n\n1:04:40.380 --> 1:04:43.220\n So what have you learned from this,\n\n1:04:43.220 --> 1:04:45.140\n Google that's leading the world in this aspect,\n\n1:04:45.140 --> 1:04:47.540\n what have you learned from that experience\n\n1:04:47.540 --> 1:04:51.540\n and what do you think is the future of ads?\n\n1:04:51.540 --> 1:04:52.540\n Take you back to that.\n\n1:04:52.540 --> 1:04:55.220\n Yeah, yes, it's been a while,\n\n1:04:55.220 --> 1:04:58.340\n but I totally agree with what you said.\n\n1:04:59.700 --> 1:05:03.180\n I think the search ads, the way it was always looked at\n\n1:05:03.180 --> 1:05:04.500\n and I believe it still is,\n\n1:05:04.500 --> 1:05:08.100\n is it's an extension of what search is trying to do.\n\n1:05:08.100 --> 1:05:10.580\n And the goal is to make the information\n\n1:05:10.580 --> 1:05:13.900\n and make the world's information accessible.\n\n1:05:14.740 --> 1:05:17.140\n That's it's not just information,\n\n1:05:17.140 --> 1:05:20.780\n but maybe products or other things that people care about.\n\n1:05:20.780 --> 1:05:23.860\n And so it's really important for them to align\n\n1:05:23.860 --> 1:05:26.500\n with what the users need.\n\n1:05:26.500 --> 1:05:30.940\n And in search ads, there's a minimum quality level\n\n1:05:30.940 --> 1:05:32.300\n before that ad would be shown.\n\n1:05:32.300 --> 1:05:34.060\n If you don't have an ad that hits that quality,\n\n1:05:34.060 --> 1:05:35.980\n but it will not be shown even if we have it\n\n1:05:35.980 --> 1:05:39.620\n and okay, maybe we lose some money there, that's fine.\n\n1:05:39.620 --> 1:05:41.300\n That is really, really important.\n\n1:05:41.300 --> 1:05:43.420\n And I think that that is something I really liked\n\n1:05:43.420 --> 1:05:45.060\n about being there.\n\n1:05:45.060 --> 1:05:48.180\n Advertising is a key part.\n\n1:05:48.180 --> 1:05:51.740\n I mean, as a model, it's been around for ages, right?\n\n1:05:51.740 --> 1:05:54.900\n It's not a new model, it's been adapted to the web\n\n1:05:54.900 --> 1:05:57.500\n and became a core part of search\n\n1:05:57.500 --> 1:06:00.780\n and many other search engines across the world.\n\n1:06:00.780 --> 1:06:04.420\n And I do hope, like you said,\n\n1:06:04.420 --> 1:06:06.700\n there are aspects of ads that are annoying\n\n1:06:06.700 --> 1:06:10.260\n and I go to a website and if it just keeps popping\n\n1:06:10.260 --> 1:06:12.540\n an ad in my face not to let me read,\n\n1:06:12.540 --> 1:06:13.860\n that's gonna be annoying clearly.\n\n1:06:13.860 --> 1:06:18.780\n So I hope we can strike that balance\n\n1:06:18.780 --> 1:06:23.780\n between showing a good ad where it's valuable to the user\n\n1:06:23.780 --> 1:06:29.740\n and provides the monetization to the service.\n\n1:06:29.740 --> 1:06:32.460\n And this might be search, this might be a website,\n\n1:06:32.460 --> 1:06:35.660\n all of these, they do need the monetization\n\n1:06:35.660 --> 1:06:37.660\n for them to provide that service.\n\n1:06:38.540 --> 1:06:41.940\n But if it's done in a good balance between\n\n1:06:43.660 --> 1:06:46.820\n showing just some random stuff that's distracting\n\n1:06:46.820 --> 1:06:49.660\n versus showing something that's actually valuable.\n\n1:06:49.660 --> 1:06:54.660\n So do you see it moving forward as to continue\n\n1:06:54.660 --> 1:06:59.660\n being a model that funds businesses like Google,\n\n1:07:00.340 --> 1:07:04.380\n that's a significant revenue stream?\n\n1:07:04.380 --> 1:07:07.420\n Because that's one of the most exciting things\n\n1:07:07.420 --> 1:07:09.020\n but also limiting things in the internet\n\n1:07:09.020 --> 1:07:11.500\n is nobody wants to pay for anything.\n\n1:07:11.500 --> 1:07:14.660\n And advertisements, again, coupled at their best,\n\n1:07:14.660 --> 1:07:16.660\n are actually really useful and not annoying.\n\n1:07:16.660 --> 1:07:21.660\n Do you see that continuing and growing and improving\n\n1:07:21.660 --> 1:07:26.140\n or is there, do you see sort of more Netflix type models\n\n1:07:26.140 --> 1:07:28.420\n where you have to start to pay for content?\n\n1:07:28.420 --> 1:07:29.780\n I think it's a mix.\n\n1:07:29.780 --> 1:07:32.260\n I think it's gonna take a long while for everything\n\n1:07:32.260 --> 1:07:35.580\n to be paid on the internet, if at all, probably not.\n\n1:07:35.580 --> 1:07:37.220\n I mean, I think there's always gonna be things\n\n1:07:37.220 --> 1:07:40.180\n that are sort of monetized with things like ads.\n\n1:07:40.180 --> 1:07:42.220\n But over the last few years, I would say\n\n1:07:42.220 --> 1:07:45.340\n we've definitely seen that transition towards\n\n1:07:45.340 --> 1:07:48.660\n more paid services across the web\n\n1:07:48.660 --> 1:07:50.420\n and people are willing to pay for them\n\n1:07:50.420 --> 1:07:51.740\n because they do see the value.\n\n1:07:51.740 --> 1:07:53.660\n I mean, Netflix is a great example.\n\n1:07:53.660 --> 1:07:56.580\n I mean, we have YouTube doing things.\n\n1:07:56.580 --> 1:07:58.780\n People pay for the apps they buy.\n\n1:07:58.780 --> 1:08:03.140\n More people I find are willing to pay for newspaper content\n\n1:08:03.140 --> 1:08:07.260\n for the good news websites across the web.\n\n1:08:07.260 --> 1:08:08.900\n That wasn't the case a few years,\n\n1:08:08.900 --> 1:08:11.060\n even a few years ago, I would say.\n\n1:08:11.060 --> 1:08:13.340\n And I just see that change in myself as well\n\n1:08:13.340 --> 1:08:14.860\n and just lots of people around me.\n\n1:08:14.860 --> 1:08:17.220\n So definitely hopeful that we'll transition\n\n1:08:17.220 --> 1:08:20.900\n to that mix model where maybe you get\n\n1:08:20.900 --> 1:08:24.180\n to try something out for free, maybe with ads,\n\n1:08:24.180 --> 1:08:27.420\n but then there's a more clear revenue model\n\n1:08:27.420 --> 1:08:29.420\n that sort of helps go beyond that.\n\n1:08:30.660 --> 1:08:35.660\n So speaking of revenue, how is it that a person\n\n1:08:35.940 --> 1:08:39.460\n can use the TPU in a Google call app for free?\n\n1:08:39.460 --> 1:08:43.980\n So what's the, I guess the question is,\n\n1:08:43.980 --> 1:08:48.940\n what's the future of TensorFlow in terms of empowering,\n\n1:08:48.940 --> 1:08:51.940\n say, a class of 300 students?\n\n1:08:51.940 --> 1:08:56.940\n And I'm asked by MIT, what is going to be the future\n\n1:08:56.940 --> 1:09:00.020\n of them being able to do their homework in TensorFlow?\n\n1:09:00.020 --> 1:09:02.860\n Like, where are they going to train these networks, right?\n\n1:09:02.860 --> 1:09:06.460\n What's that future look like with TPUs,\n\n1:09:06.460 --> 1:09:08.980\n with cloud services, and so on?\n\n1:09:08.980 --> 1:09:10.300\n I think a number of things there.\n\n1:09:10.300 --> 1:09:12.660\n I mean, any TensorFlow open source,\n\n1:09:12.660 --> 1:09:15.020\n you can run it wherever, you can run it on your desktop\n\n1:09:15.020 --> 1:09:17.500\n and your desktops always keep getting more powerful,\n\n1:09:17.500 --> 1:09:19.540\n so maybe you can do more.\n\n1:09:19.540 --> 1:09:21.420\n My phone is like, I don't know how many times\n\n1:09:21.420 --> 1:09:23.740\n more powerful than my first desktop.\n\n1:09:23.740 --> 1:09:25.220\n You'll probably train it on your phone though,\n\n1:09:25.220 --> 1:09:26.260\n yeah, that's true.\n\n1:09:26.260 --> 1:09:28.460\n Right, so in that sense, the power you have\n\n1:09:28.460 --> 1:09:30.620\n in your hands is a lot more.\n\n1:09:31.500 --> 1:09:34.420\n Clouds are actually very interesting from, say,\n\n1:09:34.420 --> 1:09:36.940\n students or courses perspective,\n\n1:09:36.940 --> 1:09:40.060\n because they make it very easy to get started.\n\n1:09:40.060 --> 1:09:42.740\n I mean, Colab, the great thing about it is,\n\n1:09:42.740 --> 1:09:45.180\n go to a website and it just works.\n\n1:09:45.180 --> 1:09:47.580\n No installation needed, nothing to,\n\n1:09:47.580 --> 1:09:50.020\n you're just there and things are working.\n\n1:09:50.020 --> 1:09:52.300\n That's really the power of cloud as well.\n\n1:09:52.300 --> 1:09:55.340\n And so I do expect that to grow.\n\n1:09:55.340 --> 1:09:57.940\n Again, Colab is a free service.\n\n1:09:57.940 --> 1:10:00.900\n It's great to get started, to play with things,\n\n1:10:00.900 --> 1:10:02.200\n to explore things.\n\n1:10:03.140 --> 1:10:06.140\n That said, with free, you can only get so much.\n\n1:10:06.140 --> 1:10:08.220\n You'd be, yeah.\n\n1:10:08.220 --> 1:10:10.140\n So just like we were talking about,\n\n1:10:10.140 --> 1:10:12.940\n free versus paid, yeah, there are services\n\n1:10:12.940 --> 1:10:15.340\n you can pay for and get a lot more.\n\n1:10:15.340 --> 1:10:17.740\n Great, so if I'm a complete beginner\n\n1:10:17.740 --> 1:10:19.980\n interested in machine learning and TensorFlow,\n\n1:10:19.980 --> 1:10:21.620\n what should I do?\n\n1:10:21.620 --> 1:10:23.540\n Probably start with going to our website\n\n1:10:23.540 --> 1:10:24.380\n and playing there.\n\n1:10:24.380 --> 1:10:26.620\n So just go to TensorFlow.org and start clicking on things.\n\n1:10:26.620 --> 1:10:28.500\n Yep, check out tutorials and guides.\n\n1:10:28.500 --> 1:10:29.860\n There's stuff you can just click there\n\n1:10:29.860 --> 1:10:31.340\n and go to a Colab and do things.\n\n1:10:31.340 --> 1:10:34.100\n No installation needed, you can get started right there.\n\n1:10:34.100 --> 1:10:36.740\n Okay, awesome, Rajit, thank you so much for talking today.\n\n1:10:36.740 --> 1:10:38.340\n Thank you, Lex, it was great.\n\n"
}