{
  "title": "Tomaso Poggio: Brains, Minds, and Machines | Lex Fridman Podcast #13",
  "id": "aSyZvBrPAyk",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:02.920\n The following is a conversation with Tommaso Poggio.\n\n00:02.920 --> 00:06.200\n He's a professor at MIT and is a director of the Center\n\n00:06.200 --> 00:08.360\n for Brains, Minds, and Machines.\n\n00:08.360 --> 00:11.640\n Cited over 100,000 times, his work\n\n00:11.640 --> 00:14.560\n has had a profound impact on our understanding\n\n00:14.560 --> 00:18.600\n of the nature of intelligence in both biological and artificial\n\n00:18.600 --> 00:19.920\n neural networks.\n\n00:19.920 --> 00:23.840\n He has been an advisor to many highly impactful researchers\n\n00:23.840 --> 00:26.120\n and entrepreneurs in AI, including\n\n00:26.120 --> 00:29.880\n Demis Hassabis of DeepMind, Amnon Shashua of Mobileye,\n\n00:29.880 --> 00:34.120\n and Christoph Koch of the Allen Institute for Brain Science.\n\n00:34.120 --> 00:36.400\n This conversation is part of the MIT course\n\n00:36.400 --> 00:38.120\n on artificial general intelligence\n\n00:38.120 --> 00:40.240\n and the artificial intelligence podcast.\n\n00:40.240 --> 00:42.760\n If you enjoy it, subscribe on YouTube, iTunes,\n\n00:42.760 --> 00:44.640\n or simply connect with me on Twitter\n\n00:44.640 --> 00:48.000\n at Lex Friedman, spelled F R I D.\n\n00:48.000 --> 00:52.480\n And now, here's my conversation with Tommaso Poggio.\n\n00:52.480 --> 00:54.520\n You've mentioned that in your childhood,\n\n00:54.520 --> 00:57.560\n you've developed a fascination with physics, especially\n\n00:57.560 --> 00:59.720\n the theory of relativity.\n\n00:59.720 --> 01:04.520\n And that Einstein was also a childhood hero to you.\n\n01:04.520 --> 01:09.000\n What aspect of Einstein's genius, the nature of his genius,\n\n01:09.000 --> 01:11.280\n do you think was essential for discovering\n\n01:11.280 --> 01:12.960\n the theory of relativity?\n\n01:12.960 --> 01:15.920\n You know, Einstein was a hero to me,\n\n01:15.920 --> 01:18.680\n and I'm sure to many people, because he\n\n01:18.680 --> 01:23.440\n was able to make, of course, a major, major contribution\n\n01:23.440 --> 01:31.920\n to physics with simplifying a bit just a gedanken experiment,\n\n01:31.920 --> 01:37.960\n a thought experiment, you know, imagining communication\n\n01:37.960 --> 01:41.560\n with lights between a stationary observer\n\n01:41.560 --> 01:43.240\n and somebody on a train.\n\n01:43.240 --> 01:47.960\n And I thought, you know, the fact\n\n01:47.960 --> 01:51.320\n that just with the force of his thought, of his thinking,\n\n01:51.320 --> 01:55.640\n of his mind, he could get to something so deep\n\n01:55.640 --> 01:58.760\n in terms of physical reality, how time\n\n01:58.760 --> 02:02.680\n depend on space and speed, it was something\n\n02:02.680 --> 02:04.120\n absolutely fascinating.\n\n02:04.120 --> 02:06.720\n It was the power of intelligence,\n\n02:06.720 --> 02:08.440\n the power of the mind.\n\n02:08.440 --> 02:11.120\n Do you think the ability to imagine,\n\n02:11.120 --> 02:15.200\n to visualize as he did, as a lot of great physicists do,\n\n02:15.200 --> 02:18.640\n do you think that's in all of us human beings?\n\n02:18.640 --> 02:21.840\n Or is there something special to that one particular human\n\n02:21.840 --> 02:22.880\n being?\n\n02:22.880 --> 02:30.480\n I think, you know, all of us can learn and have, in principle,\n\n02:30.480 --> 02:33.200\n similar breakthroughs.\n\n02:33.200 --> 02:37.200\n There are lessons to be learned from Einstein.\n\n02:37.200 --> 02:42.720\n He was one of five PhD students at ETA,\n\n02:42.720 --> 02:47.560\n the Eidgen\u00f6ssische Technische Hochschule in Zurich,\n\n02:47.560 --> 02:48.520\n in physics.\n\n02:48.520 --> 02:50.800\n And he was the worst of the five,\n\n02:50.800 --> 02:55.360\n the only one who did not get an academic position when\n\n02:55.360 --> 02:57.960\n he graduated, when he finished his PhD.\n\n02:57.960 --> 03:01.080\n And he went to work, as everybody knows,\n\n03:01.080 --> 03:02.480\n for the patent office.\n\n03:02.480 --> 03:05.840\n And so it's not so much that he worked for the patent office,\n\n03:05.840 --> 03:08.720\n but the fact that obviously he was smart,\n\n03:08.720 --> 03:11.760\n but he was not a top student, obviously\n\n03:11.760 --> 03:13.560\n was the anti conformist.\n\n03:13.560 --> 03:17.480\n He was not thinking in the traditional way that probably\n\n03:17.480 --> 03:20.040\n his teachers and the other students were doing.\n\n03:20.040 --> 03:23.880\n So there is a lot to be said about trying\n\n03:23.880 --> 03:29.800\n to do the opposite or something quite different from what\n\n03:29.800 --> 03:31.080\n other people are doing.\n\n03:31.080 --> 03:32.960\n That's certainly true for the stock market.\n\n03:32.960 --> 03:36.800\n Never buy if everybody's buying.\n\n03:36.800 --> 03:38.600\n And also true for science.\n\n03:38.600 --> 03:39.680\n Yes.\n\n03:39.680 --> 03:42.520\n So you've also mentioned, staying\n\n03:42.520 --> 03:47.600\n on the theme of physics, that you were excited at a young age\n\n03:47.600 --> 03:51.800\n by the mysteries of the universe that physics could uncover.\n\n03:51.800 --> 03:56.760\n Such, as I saw mentioned, the possibility of time travel.\n\n03:56.760 --> 03:58.960\n So the most out of the box question,\n\n03:58.960 --> 04:00.600\n I think I'll get to ask today, do you\n\n04:00.600 --> 04:03.440\n think time travel is possible?\n\n04:03.440 --> 04:07.800\n Well, it would be nice if it were possible right now.\n\n04:07.800 --> 04:12.800\n In science, you never say no.\n\n04:12.800 --> 04:15.040\n But your understanding of the nature of time.\n\n04:15.040 --> 04:15.920\n Yeah.\n\n04:15.920 --> 04:22.360\n It's very likely that it's not possible to travel in time.\n\n04:22.360 --> 04:26.000\n We may be able to travel forward in time\n\n04:26.000 --> 04:31.880\n if we can, for instance, freeze ourselves or go\n\n04:31.880 --> 04:37.640\n on some spacecraft traveling close to the speed of light.\n\n04:37.640 --> 04:40.440\n But in terms of actively traveling, for instance,\n\n04:40.440 --> 04:45.320\n back in time, I find probably very unlikely.\n\n04:45.320 --> 04:49.160\n So do you still hold the underlying dream\n\n04:49.160 --> 04:52.320\n of the engineering intelligence that\n\n04:52.320 --> 04:56.800\n will build systems that are able to do such huge leaps,\n\n04:56.800 --> 05:01.120\n like discovering the kind of mechanism that would be\n\n05:01.120 --> 05:02.600\n required to travel through time?\n\n05:02.600 --> 05:05.600\n Do you still hold that dream or echoes of it\n\n05:05.600 --> 05:07.080\n from your childhood?\n\n05:07.080 --> 05:08.640\n Yeah.\n\n05:08.640 --> 05:12.400\n I don't think whether there are certain problems that probably\n\n05:12.400 --> 05:16.840\n cannot be solved, depending what you believe\n\n05:16.840 --> 05:21.440\n about the physical reality, like maybe totally impossible\n\n05:21.440 --> 05:27.720\n to create energy from nothing or to travel back in time,\n\n05:27.720 --> 05:35.920\n but about making machines that can think as well as we do\n\n05:35.920 --> 05:38.680\n or better, or more likely, especially\n\n05:38.680 --> 05:42.400\n in the short and midterm, help us think better,\n\n05:42.400 --> 05:44.280\n which is, in a sense, is happening already\n\n05:44.280 --> 05:46.480\n with the computers we have.\n\n05:46.480 --> 05:48.440\n And it will happen more and more.\n\n05:48.440 --> 05:50.000\n But that I certainly believe.\n\n05:50.000 --> 05:55.480\n And I don't see, in principle, why computers at some point\n\n05:55.480 --> 05:59.440\n could not become more intelligent than we are,\n\n05:59.440 --> 06:03.640\n although the word intelligence is a tricky one\n\n06:03.640 --> 06:05.840\n and one we should discuss.\n\n06:05.840 --> 06:08.040\n What I mean with that.\n\n06:08.040 --> 06:13.200\n Intelligence, consciousness, words like love,\n\n06:13.200 --> 06:16.800\n all these need to be disentangled.\n\n06:16.800 --> 06:18.760\n So you've mentioned also that you believe\n\n06:18.760 --> 06:22.240\n the problem of intelligence is the greatest problem\n\n06:22.240 --> 06:24.480\n in science, greater than the origin of life\n\n06:24.480 --> 06:27.200\n and the origin of the universe.\n\n06:27.200 --> 06:30.760\n You've also, in the talk I've listened to,\n\n06:30.760 --> 06:34.880\n said that you're open to arguments against you.\n\n06:34.880 --> 06:40.400\n So what do you think is the most captivating aspect\n\n06:40.400 --> 06:43.320\n of this problem of understanding the nature of intelligence?\n\n06:43.320 --> 06:47.440\n Why does it captivate you as it does?\n\n06:47.440 --> 06:51.600\n Well, originally, I think one of the motivation\n\n06:51.600 --> 06:56.440\n that I had as, I guess, a teenager when I was infatuated\n\n06:56.440 --> 06:59.280\n with theory of relativity was really\n\n06:59.280 --> 07:05.960\n that I found that there was the problem of time and space\n\n07:05.960 --> 07:07.960\n and general relativity.\n\n07:07.960 --> 07:10.000\n But there were so many other problems\n\n07:10.000 --> 07:13.720\n of the same level of difficulty and importance\n\n07:13.720 --> 07:16.640\n that I could, even if I were Einstein,\n\n07:16.640 --> 07:19.600\n it was difficult to hope to solve all of them.\n\n07:19.600 --> 07:24.800\n So what about solving a problem whose solution allowed\n\n07:24.800 --> 07:26.640\n me to solve all the problems?\n\n07:26.640 --> 07:33.280\n And this was, what if we could find the key to an intelligence\n\n07:33.280 --> 07:37.040\n 10 times better or faster than Einstein?\n\n07:37.040 --> 07:40.160\n So that's sort of seeing artificial intelligence\n\n07:40.160 --> 07:43.200\n as a tool to expand our capabilities.\n\n07:43.200 --> 07:47.920\n But is there just an inherent curiosity in you\n\n07:47.920 --> 07:52.120\n in just understanding what it is in here\n\n07:52.120 --> 07:54.360\n that makes it all work?\n\n07:54.360 --> 07:55.760\n Yes, absolutely, you're right.\n\n07:55.760 --> 07:59.320\n So I started saying this was the motivation when\n\n07:59.320 --> 08:00.560\n I was a teenager.\n\n08:00.560 --> 08:07.160\n But soon after, I think the problem of human intelligence\n\n08:07.160 --> 08:15.160\n became a real focus of my science and my research\n\n08:15.160 --> 08:22.520\n because I think for me, the most interesting problem\n\n08:22.520 --> 08:28.120\n is really asking who we are.\n\n08:28.120 --> 08:31.720\n It's asking not only a question about science,\n\n08:31.720 --> 08:36.000\n but even about the very tool we are using to do science, which\n\n08:36.000 --> 08:37.920\n is our brain.\n\n08:37.920 --> 08:39.880\n How does our brain work?\n\n08:39.880 --> 08:42.120\n From where does it come from?\n\n08:42.120 --> 08:43.720\n What are its limitations?\n\n08:43.720 --> 08:46.160\n Can we make it better?\n\n08:46.160 --> 08:50.000\n And that, in many ways, is the ultimate question\n\n08:50.000 --> 08:54.400\n that underlies this whole effort of science.\n\n08:54.400 --> 08:56.360\n So you've made significant contributions\n\n08:56.360 --> 08:58.240\n in both the science of intelligence\n\n08:58.240 --> 09:02.160\n and the engineering of intelligence.\n\n09:02.160 --> 09:05.000\n In a hypothetical way, let me ask,\n\n09:05.000 --> 09:07.960\n how far do you think we can get in creating intelligence\n\n09:07.960 --> 09:12.080\n systems without understanding the biological,\n\n09:12.080 --> 09:15.800\n the understanding how the human brain creates intelligence?\n\n09:15.800 --> 09:17.200\n Put another way, do you think we can\n\n09:17.200 --> 09:22.080\n build a strong AI system without really getting at the core\n\n09:22.080 --> 09:25.240\n understanding the functional nature of the brain?\n\n09:25.240 --> 09:29.920\n Well, this is a real difficult question.\n\n09:29.920 --> 09:35.280\n We did solve problems like flying\n\n09:35.280 --> 09:40.680\n without really using too much our knowledge\n\n09:40.680 --> 09:44.720\n about how birds fly.\n\n09:44.720 --> 09:48.680\n It was important, I guess, to know that you could have\n\n09:48.680 --> 09:56.760\n things heavier than air being able to fly, like birds.\n\n09:56.760 --> 10:02.560\n But beyond that, probably we did not learn very much, some.\n\n10:02.560 --> 10:06.800\n The Brothers Wright did learn a lot of observation\n\n10:06.800 --> 10:12.120\n about birds and designing their aircraft.\n\n10:12.120 --> 10:16.000\n But you can argue we did not use much of biology\n\n10:16.000 --> 10:17.920\n in that particular case.\n\n10:17.920 --> 10:20.720\n Now, in the case of intelligence,\n\n10:20.720 --> 10:28.920\n I think that it's a bit of a bet right now.\n\n10:28.920 --> 10:36.280\n If you ask, OK, we all agree we'll get at some point,\n\n10:36.280 --> 10:39.480\n maybe soon, maybe later, to a machine that\n\n10:39.480 --> 10:42.400\n is indistinguishable from my secretary,\n\n10:42.400 --> 10:47.600\n say, in terms of what I can ask the machine to do.\n\n10:47.600 --> 10:49.000\n I think we'll get there.\n\n10:49.000 --> 10:51.960\n And now the question is, you can ask people,\n\n10:51.960 --> 10:54.240\n do you think we'll get there without any knowledge\n\n10:54.240 --> 10:57.000\n about the human brain?\n\n10:57.000 --> 10:59.760\n Or that the best way to get there\n\n10:59.760 --> 11:02.560\n is to understand better the human brain?\n\n11:02.560 --> 11:05.960\n OK, this is, I think, an educated bet\n\n11:05.960 --> 11:09.080\n that different people with different backgrounds\n\n11:09.080 --> 11:11.720\n will decide in different ways.\n\n11:11.720 --> 11:14.440\n The recent history of the progress\n\n11:14.440 --> 11:18.920\n in AI in the last, I would say, five years or 10 years\n\n11:18.920 --> 11:23.760\n has been that the main breakthroughs,\n\n11:23.760 --> 11:32.160\n the main recent breakthroughs, really start from neuroscience.\n\n11:32.160 --> 11:35.800\n I can mention reinforcement learning as one.\n\n11:35.800 --> 11:41.040\n It's one of the algorithms at the core of AlphaGo,\n\n11:41.040 --> 11:45.200\n which is the system that beat the kind of an official world\n\n11:45.200 --> 11:52.760\n champion of Go, Lee Sedol, two, three years ago in Seoul.\n\n11:52.760 --> 11:53.760\n That's one.\n\n11:53.760 --> 12:00.920\n And that started really with the work of Pavlov in 1900,\n\n12:00.920 --> 12:05.800\n Marvin Minsky in the 60s, and many other neuroscientists\n\n12:05.800 --> 12:07.720\n later on.\n\n12:07.720 --> 12:12.560\n And deep learning started, which is at the core, again,\n\n12:12.560 --> 12:17.720\n of AlphaGo and systems like autonomous driving\n\n12:17.720 --> 12:22.520\n systems for cars, like the systems that Mobileye,\n\n12:22.520 --> 12:25.600\n which is a company started by one of my ex postdocs,\n\n12:25.600 --> 12:28.480\n Amnon Shashua, did.\n\n12:28.480 --> 12:30.720\n So that is at the core of those things.\n\n12:30.720 --> 12:34.520\n And deep learning, really, the initial ideas\n\n12:34.520 --> 12:37.120\n in terms of the architecture of these layered\n\n12:37.120 --> 12:43.160\n hierarchical networks started with work of Torsten Wiesel\n\n12:43.160 --> 12:47.800\n and David Hubel at Harvard up the river in the 60s.\n\n12:47.800 --> 12:53.240\n So recent history suggests that neuroscience played a big role\n\n12:53.240 --> 12:54.320\n in these breakthroughs.\n\n12:54.320 --> 12:58.720\n My personal bet is that there is a good chance they continue\n\n12:58.720 --> 12:59.880\n to play a big role.\n\n12:59.880 --> 13:01.840\n Maybe not in all the future breakthroughs,\n\n13:01.840 --> 13:03.280\n but in some of them.\n\n13:03.280 --> 13:05.000\n At least in inspiration.\n\n13:05.000 --> 13:07.320\n At least in inspiration, absolutely, yes.\n\n13:07.320 --> 13:12.160\n So you studied both artificial and biological neural networks.\n\n13:12.160 --> 13:17.080\n You said these mechanisms that underlie deep learning\n\n13:17.080 --> 13:19.760\n and reinforcement learning.\n\n13:19.760 --> 13:23.880\n But there is nevertheless significant differences\n\n13:23.880 --> 13:26.080\n between biological and artificial neural networks\n\n13:26.080 --> 13:27.280\n as they stand now.\n\n13:27.280 --> 13:30.800\n So between the two, what do you find\n\n13:30.800 --> 13:33.080\n is the most interesting, mysterious, maybe even\n\n13:33.080 --> 13:35.560\n beautiful difference as it currently\n\n13:35.560 --> 13:37.800\n stands in our understanding?\n\n13:37.800 --> 13:41.560\n I must confess that until recently, I\n\n13:41.560 --> 13:46.040\n found that the artificial networks, too simplistic\n\n13:46.040 --> 13:49.720\n relative to real neural networks.\n\n13:49.720 --> 13:53.360\n But recently, I've been starting to think that, yes,\n\n13:53.360 --> 13:57.440\n there is a very big simplification of what\n\n13:57.440 --> 13:59.040\n you find in the brain.\n\n13:59.040 --> 14:03.000\n But on the other hand, they are much closer\n\n14:03.000 --> 14:07.080\n in terms of the architecture to the brain\n\n14:07.080 --> 14:11.280\n than other models that we had, that computer science used\n\n14:11.280 --> 14:16.560\n as model of thinking, which were mathematical logics, LISP,\n\n14:16.560 --> 14:19.480\n Prologue, and those kind of things.\n\n14:19.480 --> 14:21.520\n So in comparison to those, they're\n\n14:21.520 --> 14:23.320\n much closer to the brain.\n\n14:23.320 --> 14:26.360\n You have networks of neurons, which\n\n14:26.360 --> 14:27.840\n is what the brain is about.\n\n14:27.840 --> 14:32.200\n And the artificial neurons in the models, as I said,\n\n14:32.200 --> 14:35.480\n caricature of the biological neurons.\n\n14:35.480 --> 14:38.520\n But they're still neurons, single units communicating\n\n14:38.520 --> 14:41.400\n with other units, something that is absent\n\n14:41.400 --> 14:48.520\n in the traditional computer type models of mathematics,\n\n14:48.520 --> 14:50.840\n reasoning, and so on.\n\n14:50.840 --> 14:53.120\n So what aspect would you like to see\n\n14:53.120 --> 14:57.280\n in artificial neural networks added over time\n\n14:57.280 --> 14:59.920\n as we try to figure out ways to improve them?\n\n14:59.920 --> 15:07.320\n So one of the main differences and problems\n\n15:07.320 --> 15:11.840\n in terms of deep learning today, and it's not only\n\n15:11.840 --> 15:16.760\n deep learning, and the brain, is the need for deep learning\n\n15:16.760 --> 15:23.160\n techniques to have a lot of labeled examples.\n\n15:23.160 --> 15:24.920\n For instance, for ImageNet, you have\n\n15:24.920 --> 15:29.160\n like a training set, which is 1 million images, each one\n\n15:29.160 --> 15:34.600\n labeled by some human in terms of which object is there.\n\n15:34.600 --> 15:42.720\n And it's clear that in biology, a baby\n\n15:42.720 --> 15:44.840\n may be able to see millions of images\n\n15:44.840 --> 15:47.560\n in the first years of life, but will not\n\n15:47.560 --> 15:52.720\n have millions of labels given to him or her by parents\n\n15:52.720 --> 15:56.360\n or caretakers.\n\n15:56.360 --> 15:59.560\n So how do you solve that?\n\n15:59.560 --> 16:03.880\n I think there is this interesting challenge\n\n16:03.880 --> 16:08.080\n that today, deep learning and related techniques\n\n16:08.080 --> 16:11.200\n are all about big data, big data meaning\n\n16:11.200 --> 16:18.760\n a lot of examples labeled by humans,\n\n16:18.760 --> 16:24.400\n whereas in nature, you have this big data\n\n16:24.400 --> 16:26.280\n is n going to infinity.\n\n16:26.280 --> 16:30.200\n That's the best, n meaning labeled data.\n\n16:30.200 --> 16:34.920\n But I think the biological world is more n going to 1.\n\n16:34.920 --> 16:38.920\n A child can learn from a very small number\n\n16:38.920 --> 16:42.720\n of labeled examples.\n\n16:42.720 --> 16:44.920\n Like you tell a child, this is a car.\n\n16:44.920 --> 16:48.880\n You don't need to say, like in ImageNet, this is a car,\n\n16:48.880 --> 16:51.120\n this is a car, this is not a car, this is not a car,\n\n16:51.120 --> 16:54.360\n 1 million times.\n\n16:54.360 --> 16:57.720\n And of course, with AlphaGo, or at least the AlphaZero\n\n16:57.720 --> 17:01.360\n variants, because the world of Go\n\n17:01.360 --> 17:05.040\n is so simplistic that you can actually\n\n17:05.040 --> 17:06.720\n learn by yourself through self play,\n\n17:06.720 --> 17:08.520\n you can play against each other.\n\n17:08.520 --> 17:10.600\n In the real world, the visual system\n\n17:10.600 --> 17:14.200\n that you've studied extensively is a lot more complicated\n\n17:14.200 --> 17:16.720\n than the game of Go.\n\n17:16.720 --> 17:18.400\n On the comment about children, which\n\n17:18.400 --> 17:23.000\n are fascinatingly good at learning new stuff,\n\n17:23.000 --> 17:24.680\n how much of it do you think is hardware,\n\n17:24.680 --> 17:26.640\n and how much of it is software?\n\n17:26.640 --> 17:29.800\n Yeah, that's a good, deep question.\n\n17:29.800 --> 17:32.960\n In a sense, it's the old question of nurture and nature,\n\n17:32.960 --> 17:36.440\n how much is in the gene, and how much\n\n17:36.440 --> 17:41.280\n is in the experience of an individual.\n\n17:41.280 --> 17:44.720\n Obviously, it's both that play a role.\n\n17:44.720 --> 17:53.040\n And I believe that the way evolution gives,\n\n17:53.040 --> 17:55.760\n puts prior information, so to speak, hardwired,\n\n17:55.760 --> 17:58.400\n is not really hardwired.\n\n17:58.400 --> 18:02.720\n But that's essentially an hypothesis.\n\n18:02.720 --> 18:10.000\n I think what's going on is that evolution has almost\n\n18:10.000 --> 18:14.960\n necessarily, if you believe in Darwin, is very opportunistic.\n\n18:14.960 --> 18:24.320\n And think about our DNA and the DNA of Drosophila.\n\n18:24.320 --> 18:28.800\n Our DNA does not have many more genes than Drosophila.\n\n18:28.800 --> 18:29.720\n The fly.\n\n18:29.720 --> 18:32.560\n The fly, the fruit fly.\n\n18:32.560 --> 18:35.520\n Now, we know that the fruit fly does not\n\n18:35.520 --> 18:39.680\n learn very much during its individual existence.\n\n18:39.680 --> 18:42.360\n It looks like one of these machinery\n\n18:42.360 --> 18:47.240\n that it's really mostly, not 100%, but 95%,\n\n18:47.240 --> 18:51.720\n hardcoded by the genes.\n\n18:51.720 --> 18:55.040\n But since we don't have many more genes than Drosophila,\n\n18:55.040 --> 19:02.640\n evolution could encode in as a general learning machinery,\n\n19:02.640 --> 19:09.840\n and then had to give very weak priors.\n\n19:09.840 --> 19:15.000\n Like, for instance, let me give a specific example,\n\n19:15.000 --> 19:18.160\n which is recent work by a member of our Center for Brains,\n\n19:18.160 --> 19:20.680\n Minds, and Machines.\n\n19:20.680 --> 19:24.440\n We know because of work of other people in our group\n\n19:24.440 --> 19:26.720\n and other groups, that there are cells\n\n19:26.720 --> 19:31.160\n in a part of our brain, neurons, that are tuned to faces.\n\n19:31.160 --> 19:33.840\n They seem to be involved in face recognition.\n\n19:33.840 --> 19:41.600\n Now, this face area seems to be present in young children\n\n19:41.600 --> 19:44.600\n and adults.\n\n19:44.600 --> 19:48.400\n And one question is, is there from the beginning?\n\n19:48.400 --> 19:51.760\n Is hardwired by evolution?\n\n19:51.760 --> 19:55.000\n Or somehow it's learned very quickly.\n\n19:55.000 --> 19:58.960\n So what's your, by the way, a lot of the questions I'm asking,\n\n19:58.960 --> 20:00.920\n the answer is we don't really know.\n\n20:00.920 --> 20:04.520\n But as a person who has contributed\n\n20:04.520 --> 20:06.440\n some profound ideas in these fields,\n\n20:06.440 --> 20:08.360\n you're a good person to guess at some of these.\n\n20:08.360 --> 20:11.200\n So of course, there's a caveat before a lot of the stuff\n\n20:11.200 --> 20:11.760\n we talk about.\n\n20:11.760 --> 20:14.680\n But what is your hunch?\n\n20:14.680 --> 20:16.400\n Is the face, the part of the brain\n\n20:16.400 --> 20:20.120\n that seems to be concentrated on face recognition,\n\n20:20.120 --> 20:21.240\n are you born with that?\n\n20:21.240 --> 20:25.160\n Or you just is designed to learn that quickly,\n\n20:25.160 --> 20:26.920\n like the face of the mother and so on?\n\n20:26.920 --> 20:32.280\n My hunch, my bias was the second one, learned very quickly.\n\n20:32.280 --> 20:37.240\n And it turns out that Marge Livingstone at Harvard\n\n20:37.240 --> 20:41.480\n has done some amazing experiments in which she raised\n\n20:41.480 --> 20:45.200\n baby monkeys, depriving them of faces\n\n20:45.200 --> 20:48.560\n during the first weeks of life.\n\n20:48.560 --> 20:53.000\n So they see technicians, but the technician have a mask.\n\n20:53.000 --> 20:55.080\n Yes.\n\n20:55.080 --> 21:02.000\n And so when they looked at the area\n\n21:02.000 --> 21:05.720\n in the brain of these monkeys that were usually\n\n21:05.720 --> 21:10.840\n defined faces, they found no face preference.\n\n21:10.840 --> 21:16.800\n So my guess is that what evolution does in this case\n\n21:16.800 --> 21:19.760\n is there is a plastic area, which\n\n21:19.760 --> 21:22.760\n is plastic, which is kind of predetermined\n\n21:22.760 --> 21:26.520\n to be imprinted very easily.\n\n21:26.520 --> 21:30.160\n But the command from the gene is not a detailed circuitry\n\n21:30.160 --> 21:32.280\n for a face template.\n\n21:32.280 --> 21:36.280\n Could be, but this will require probably a lot of bits.\n\n21:36.280 --> 21:39.720\n You had to specify a lot of connection of a lot of neurons.\n\n21:39.720 --> 21:42.320\n Instead, the command from the gene\n\n21:42.320 --> 21:47.400\n is something like imprint, memorize what you see most\n\n21:47.400 --> 21:49.480\n often in the first two weeks of life,\n\n21:49.480 --> 21:53.440\n especially in connection with food and maybe nipples.\n\n21:53.440 --> 21:54.640\n I don't know.\n\n21:54.640 --> 21:55.960\n Well, source of food.\n\n21:55.960 --> 22:00.320\n And so that area is very plastic at first and then solidifies.\n\n22:00.320 --> 22:03.600\n It'd be interesting if a variant of that experiment\n\n22:03.600 --> 22:06.800\n would show a different kind of pattern associated\n\n22:06.800 --> 22:10.200\n with food than a face pattern, whether that could stick.\n\n22:10.200 --> 22:14.960\n There are indications that during that experiment,\n\n22:14.960 --> 22:19.560\n what the monkeys saw quite often were\n\n22:19.560 --> 22:23.200\n the blue gloves of the technicians that were giving\n\n22:23.200 --> 22:25.560\n to the baby monkeys the milk.\n\n22:25.560 --> 22:29.400\n And some of the cells, instead of being face sensitive\n\n22:29.400 --> 22:33.680\n in that area, are hand sensitive.\n\n22:33.680 --> 22:35.960\n That's fascinating.\n\n22:35.960 --> 22:40.600\n Can you talk about what are the different parts of the brain\n\n22:40.600 --> 22:43.920\n and, in your view, sort of loosely,\n\n22:43.920 --> 22:45.760\n and how do they contribute to intelligence?\n\n22:45.760 --> 22:49.520\n Do you see the brain as a bunch of different modules,\n\n22:49.520 --> 22:52.440\n and they together come in the human brain\n\n22:52.440 --> 22:53.800\n to create intelligence?\n\n22:53.800 --> 22:59.320\n Or is it all one mush of the same kind\n\n22:59.320 --> 23:04.600\n of fundamental architecture?\n\n23:04.600 --> 23:08.840\n Yeah, that's an important question.\n\n23:08.840 --> 23:15.200\n And there was a phase in neuroscience back in the 1950\n\n23:15.200 --> 23:19.360\n or so in which it was believed for a while\n\n23:19.360 --> 23:21.920\n that the brain was equipotential.\n\n23:21.920 --> 23:22.960\n This was the term.\n\n23:22.960 --> 23:28.000\n You could cut out a piece, and nothing special\n\n23:28.000 --> 23:32.360\n happened apart a little bit less performance.\n\n23:32.360 --> 23:37.120\n There was a surgeon, Lashley, who\n\n23:37.120 --> 23:41.800\n did a lot of experiments of this type with mice and rats\n\n23:41.800 --> 23:45.640\n and concluded that every part of the brain\n\n23:45.640 --> 23:48.400\n was essentially equivalent to any other one.\n\n23:51.360 --> 23:56.080\n It turns out that that's really not true.\n\n23:56.080 --> 24:00.480\n There are very specific modules in the brain, as you said.\n\n24:00.480 --> 24:05.280\n And people may lose the ability to speak\n\n24:05.280 --> 24:07.520\n if you have a stroke in a certain region,\n\n24:07.520 --> 24:12.840\n or may lose control of their legs in another region.\n\n24:12.840 --> 24:14.520\n So they're very specific.\n\n24:14.520 --> 24:17.920\n The brain is also quite flexible and redundant,\n\n24:17.920 --> 24:27.360\n so often it can correct things and take over functions\n\n24:27.360 --> 24:29.840\n from one part of the brain to the other.\n\n24:29.840 --> 24:33.760\n But really, there are specific modules.\n\n24:33.760 --> 24:40.000\n So the answer that we know from this old work, which\n\n24:40.000 --> 24:44.840\n was basically based on lesions, either on animals,\n\n24:44.840 --> 24:52.960\n or very often there was a mine of very interesting data\n\n24:52.960 --> 25:00.600\n coming from the war, from different types of injuries\n\n25:00.600 --> 25:03.800\n that soldiers had in the brain.\n\n25:03.800 --> 25:09.120\n And more recently, functional MRI,\n\n25:09.120 --> 25:13.840\n which allow you to check which part of the brain\n\n25:13.840 --> 25:21.640\n are active when you are doing different tasks,\n\n25:21.640 --> 25:23.720\n can replace some of this.\n\n25:23.720 --> 25:27.560\n You can see that certain parts of the brain are involved,\n\n25:27.560 --> 25:29.480\n are active in certain tasks.\n\n25:29.480 --> 25:32.320\n Vision, language, yeah, that's right.\n\n25:32.320 --> 25:36.520\n But sort of taking a step back to that part of the brain\n\n25:36.520 --> 25:39.320\n that discovers that specializes in the face\n\n25:39.320 --> 25:45.320\n and how that might be learned, what's your intuition behind?\n\n25:45.320 --> 25:48.880\n Is it possible that from a physicist perspective,\n\n25:48.880 --> 25:51.920\n when you get lower and lower, that it's all the same stuff\n\n25:51.920 --> 25:54.800\n and it just, when you're born, it's plastic\n\n25:54.800 --> 25:58.040\n and quickly figures out this part is going to be about vision,\n\n25:58.040 --> 25:59.440\n this is going to be about language,\n\n25:59.440 --> 26:02.000\n this is about common sense reasoning.\n\n26:02.000 --> 26:05.120\n Do you have an intuition that that kind of learning\n\n26:05.120 --> 26:07.080\n is going on really quickly, or is it really\n\n26:07.080 --> 26:09.760\n kind of solidified in hardware?\n\n26:09.760 --> 26:11.440\n That's a great question.\n\n26:11.440 --> 26:16.920\n So there are parts of the brain like the cerebellum\n\n26:16.920 --> 26:21.560\n or the hippocampus that are quite different from each other.\n\n26:21.560 --> 26:23.840\n They clearly have different anatomy,\n\n26:23.840 --> 26:26.880\n different connectivity.\n\n26:26.880 --> 26:33.400\n Then there is the cortex, which is the most developed part\n\n26:33.400 --> 26:36.080\n of the brain in humans.\n\n26:36.080 --> 26:39.560\n And in the cortex, you have different regions\n\n26:39.560 --> 26:43.360\n of the cortex that are responsible for vision,\n\n26:43.360 --> 26:47.880\n for audition, for motor control, for language.\n\n26:47.880 --> 26:50.760\n Now, one of the big puzzles of this\n\n26:50.760 --> 26:55.240\n is that in the cortex is the cortex is the cortex.\n\n26:55.240 --> 27:00.920\n Looks like it is the same in terms of hardware,\n\n27:00.920 --> 27:05.040\n in terms of type of neurons and connectivity\n\n27:05.040 --> 27:08.360\n across these different modalities.\n\n27:08.360 --> 27:13.680\n So for the cortex, I think aside these other parts\n\n27:13.680 --> 27:15.800\n of the brain like spinal cord, hippocampus,\n\n27:15.800 --> 27:18.840\n cerebellum, and so on, for the cortex,\n\n27:18.840 --> 27:21.920\n I think your question about hardware and software\n\n27:21.920 --> 27:28.400\n and learning and so on, I think is rather open.\n\n27:28.400 --> 27:33.720\n And I find it very interesting for Risa\n\n27:33.720 --> 27:36.960\n to think about an architecture, computer architecture, that\n\n27:36.960 --> 27:41.360\n is good for vision and at the same time is good for language.\n\n27:41.360 --> 27:49.320\n Seems to be so different problem areas that you have to solve.\n\n27:49.320 --> 27:51.280\n But the underlying mechanism might be the same.\n\n27:51.280 --> 27:55.200\n And that's really instructive for artificial neural networks.\n\n27:55.200 --> 27:58.000\n So we've done a lot of great work in vision,\n\n27:58.000 --> 28:01.640\n in human vision, computer vision.\n\n28:01.640 --> 28:03.800\n And you mentioned the problem of human vision\n\n28:03.800 --> 28:07.440\n is really as difficult as the problem of general intelligence.\n\n28:07.440 --> 28:11.480\n And maybe that connects to the cortex discussion.\n\n28:11.480 --> 28:15.320\n Can you describe the human visual cortex\n\n28:15.320 --> 28:20.320\n and how the humans begin to understand the world\n\n28:20.320 --> 28:22.480\n through the raw sensory information?\n\n28:22.480 --> 28:27.760\n What's, for folks who are not familiar,\n\n28:27.760 --> 28:30.120\n especially on the computer vision side,\n\n28:30.120 --> 28:33.400\n we don't often actually take a step back except saying\n\n28:33.400 --> 28:36.560\n with a sentence or two that one is inspired by the other.\n\n28:36.560 --> 28:40.000\n What is it that we know about the human visual cortex?\n\n28:40.000 --> 28:40.760\n That's interesting.\n\n28:40.760 --> 28:41.880\n We know quite a bit.\n\n28:41.880 --> 28:43.440\n At the same time, we don't know a lot.\n\n28:43.440 --> 28:50.080\n But the bit we know, in a sense, we know a lot of the details.\n\n28:50.080 --> 28:53.440\n And many we don't know.\n\n28:53.440 --> 28:58.520\n And we know a lot of the top level,\n\n28:58.520 --> 29:00.080\n the answer to the top level question.\n\n29:00.080 --> 29:02.200\n But we don't know some basic ones,\n\n29:02.200 --> 29:06.200\n even in terms of general neuroscience, forgetting vision.\n\n29:06.200 --> 29:08.960\n Why do we sleep?\n\n29:08.960 --> 29:11.960\n It's such a basic question.\n\n29:11.960 --> 29:15.360\n And we really don't have an answer to that.\n\n29:15.360 --> 29:17.160\n So taking a step back on that.\n\n29:17.160 --> 29:18.760\n So sleep, for example, is fascinating.\n\n29:18.760 --> 29:22.040\n Do you think that's a neuroscience question?\n\n29:22.040 --> 29:25.360\n Or if we talk about abstractions, what do you\n\n29:25.360 --> 29:28.160\n think is an interesting way to study intelligence\n\n29:28.160 --> 29:30.680\n or most effective on the levels of abstraction?\n\n29:30.680 --> 29:33.120\n Is it chemical, is it biological,\n\n29:33.120 --> 29:35.560\n is it electrophysical, mathematical,\n\n29:35.560 --> 29:37.880\n as you've done a lot of excellent work on that side?\n\n29:37.880 --> 29:43.280\n Which psychology, at which level of abstraction do you think?\n\n29:43.280 --> 29:46.880\n Well, in terms of levels of abstraction,\n\n29:46.880 --> 29:50.160\n I think we need all of them.\n\n29:50.160 --> 29:54.360\n It's like if you ask me, what does it\n\n29:54.360 --> 29:57.560\n mean to understand a computer?\n\n29:57.560 --> 29:58.640\n That's much simpler.\n\n29:58.640 --> 30:01.080\n But in a computer, I could say, well,\n\n30:01.080 --> 30:04.800\n I understand how to use PowerPoint.\n\n30:04.800 --> 30:08.080\n That's my level of understanding a computer.\n\n30:08.080 --> 30:09.400\n It is reasonable.\n\n30:09.400 --> 30:11.760\n It gives me some power to produce slides\n\n30:11.760 --> 30:14.480\n and beautiful slides.\n\n30:14.480 --> 30:17.320\n Now, you can ask somebody else.\n\n30:17.320 --> 30:19.840\n He says, well, I know how the transistors work\n\n30:19.840 --> 30:21.360\n that are inside the computer.\n\n30:21.360 --> 30:25.920\n I can write the equation for transistor and diodes\n\n30:25.920 --> 30:29.360\n and circuits, logical circuits.\n\n30:29.360 --> 30:32.440\n And I can ask this guy, do you know how to operate PowerPoint?\n\n30:32.440 --> 30:34.040\n No idea.\n\n30:34.040 --> 30:39.800\n So do you think if we discovered computers walking amongst us\n\n30:39.800 --> 30:43.400\n full of these transistors that are also operating\n\n30:43.400 --> 30:45.560\n under windows and have PowerPoint,\n\n30:45.560 --> 30:49.960\n do you think it's digging in a little bit more?\n\n30:49.960 --> 30:53.280\n How useful is it to understand the transistor in order\n\n30:53.280 --> 30:58.040\n to be able to understand PowerPoint\n\n30:58.040 --> 31:00.320\n and these higher level intelligent processes?\n\n31:00.320 --> 31:03.720\n So I think in the case of computers,\n\n31:03.720 --> 31:06.960\n because they were made by engineers, by us,\n\n31:06.960 --> 31:09.280\n this different level of understanding\n\n31:09.280 --> 31:13.280\n are rather separate on purpose.\n\n31:13.280 --> 31:17.240\n They are separate modules so that the engineer that\n\n31:17.240 --> 31:19.640\n designed the circuit for the chips does not\n\n31:19.640 --> 31:23.600\n need to know what is inside PowerPoint.\n\n31:23.600 --> 31:27.440\n And somebody can write the software translating\n\n31:27.440 --> 31:30.360\n from one to the other.\n\n31:30.360 --> 31:36.960\n So in that case, I don't think understanding the transistor\n\n31:36.960 --> 31:41.120\n helps you understand PowerPoint, or very little.\n\n31:41.120 --> 31:43.960\n If you want to understand the computer, this question,\n\n31:43.960 --> 31:45.960\n I would say you have to understand it\n\n31:45.960 --> 31:46.800\n at different levels.\n\n31:46.800 --> 31:51.520\n If you really want to build one, right?\n\n31:51.520 --> 31:57.320\n But for the brain, I think these levels of understanding,\n\n31:57.320 --> 32:00.840\n so the algorithms, which kind of computation,\n\n32:00.840 --> 32:04.640\n the equivalent of PowerPoint, and the circuits,\n\n32:04.640 --> 32:07.560\n the transistors, I think they are much more\n\n32:07.560 --> 32:09.560\n intertwined with each other.\n\n32:09.560 --> 32:14.480\n There is not a neatly level of the software separate\n\n32:14.480 --> 32:15.840\n from the hardware.\n\n32:15.840 --> 32:20.080\n And so that's why I think in the case of the brain,\n\n32:20.080 --> 32:23.640\n the problem is more difficult and more than for computers\n\n32:23.640 --> 32:26.560\n requires the interaction, the collaboration\n\n32:26.560 --> 32:30.080\n between different types of expertise.\n\n32:30.080 --> 32:32.320\n The brain is a big hierarchical mess.\n\n32:32.320 --> 32:35.120\n You can't just disentangle levels.\n\n32:35.120 --> 32:37.880\n I think you can, but it's much more difficult.\n\n32:37.880 --> 32:40.840\n And it's not completely obvious.\n\n32:40.840 --> 32:44.720\n And as I said, I think it's one of the, personally,\n\n32:44.720 --> 32:47.240\n I think is the greatest problem in science.\n\n32:47.240 --> 32:51.880\n So I think it's fair that it's difficult.\n\n32:51.880 --> 32:53.320\n That's a difficult one.\n\n32:53.320 --> 32:56.920\n That said, you do talk about compositionality\n\n32:56.920 --> 32:58.280\n and why it might be useful.\n\n32:58.280 --> 33:01.720\n And when you discuss why these neural networks,\n\n33:01.720 --> 33:05.200\n in artificial or biological sense, learn anything,\n\n33:05.200 --> 33:07.560\n you talk about compositionality.\n\n33:07.560 --> 33:13.480\n See, there's a sense that nature can be disentangled.\n\n33:13.480 --> 33:19.840\n Or, well, all aspects of our cognition\n\n33:19.840 --> 33:22.640\n could be disentangled to some degree.\n\n33:22.640 --> 33:25.920\n So why do you think, first of all,\n\n33:25.920 --> 33:27.720\n how do you see compositionality?\n\n33:27.720 --> 33:31.640\n And why do you think it exists at all in nature?\n\n33:31.640 --> 33:39.800\n I spoke about, I use the term compositionality\n\n33:39.800 --> 33:45.320\n when we looked at deep neural networks, multilayers,\n\n33:45.320 --> 33:50.560\n and trying to understand when and why they are more powerful\n\n33:50.560 --> 33:54.800\n than more classical one layer networks,\n\n33:54.800 --> 34:01.600\n like linear classifier, kernel machines, so called.\n\n34:01.600 --> 34:05.360\n And what we found is that in terms\n\n34:05.360 --> 34:08.360\n of approximating or learning or representing\n\n34:08.360 --> 34:12.200\n a function, a mapping from an input to an output,\n\n34:12.200 --> 34:16.760\n like from an image to the label in the image,\n\n34:16.760 --> 34:20.840\n if this function has a particular structure,\n\n34:20.840 --> 34:26.120\n then deep networks are much more powerful than shallow networks\n\n34:26.120 --> 34:28.880\n to approximate the underlying function.\n\n34:28.880 --> 34:33.920\n And the particular structure is a structure of compositionality.\n\n34:33.920 --> 34:38.960\n If the function is made up of functions of function,\n\n34:38.960 --> 34:45.800\n so that you need to look on when you are interpreting an image,\n\n34:45.800 --> 34:47.720\n classifying an image, you don't need\n\n34:47.720 --> 34:51.040\n to look at all pixels at once.\n\n34:51.040 --> 34:57.120\n But you can compute something from small groups of pixels.\n\n34:57.120 --> 34:59.920\n And then you can compute something\n\n34:59.920 --> 35:04.760\n on the output of this local computation and so on,\n\n35:04.760 --> 35:07.320\n which is similar to what you do when you read a sentence.\n\n35:07.320 --> 35:11.360\n You don't need to read the first and the last letter.\n\n35:11.360 --> 35:16.000\n But you can read syllables, combine them in words,\n\n35:16.000 --> 35:18.120\n combine the words in sentences.\n\n35:18.120 --> 35:21.040\n So this is this kind of structure.\n\n35:21.040 --> 35:22.600\n So that's as part of a discussion\n\n35:22.600 --> 35:26.120\n of why deep neural networks may be more\n\n35:26.120 --> 35:27.880\n effective than the shallow methods.\n\n35:27.880 --> 35:31.320\n And is your sense, for most things\n\n35:31.320 --> 35:37.400\n we can use neural networks for, those problems\n\n35:37.400 --> 35:42.440\n are going to be compositional in nature, like language,\n\n35:42.440 --> 35:44.240\n like vision?\n\n35:44.240 --> 35:47.840\n How far can we get in this kind of way?\n\n35:47.840 --> 35:51.560\n So here is almost philosophy.\n\n35:51.560 --> 35:53.120\n Well, let's go there.\n\n35:53.120 --> 35:54.240\n Yeah, let's go there.\n\n35:54.240 --> 36:00.200\n So a friend of mine, Max Tegmark, who is a physicist at MIT.\n\n36:00.200 --> 36:01.560\n I've talked to him on this thing.\n\n36:01.560 --> 36:03.800\n Yeah, and he disagrees with you, right?\n\n36:03.800 --> 36:04.440\n A little bit.\n\n36:04.440 --> 36:07.040\n Yeah, we agree on most.\n\n36:07.040 --> 36:10.160\n But the conclusion is a bit different.\n\n36:10.160 --> 36:14.640\n His conclusion is that for images, for instance,\n\n36:14.640 --> 36:19.440\n the compositional structure of this function\n\n36:19.440 --> 36:23.360\n that we have to learn or to solve these problems\n\n36:23.360 --> 36:27.760\n comes from physics, comes from the fact\n\n36:27.760 --> 36:31.920\n that you have local interactions in physics\n\n36:31.920 --> 36:37.440\n between atoms and other atoms, between particle\n\n36:37.440 --> 36:41.120\n of matter and other particles, between planets\n\n36:41.120 --> 36:44.400\n and other planets, between stars and other.\n\n36:44.400 --> 36:45.200\n It's all local.\n\n36:48.320 --> 36:51.160\n And that's true.\n\n36:51.160 --> 36:56.280\n But you could push this argument a bit further.\n\n36:56.280 --> 36:57.600\n Not this argument, actually.\n\n36:57.600 --> 37:02.800\n You could argue that maybe that's part of the truth.\n\n37:02.800 --> 37:06.800\n But maybe what happens is kind of the opposite,\n\n37:06.800 --> 37:11.840\n is that our brain is wired up as a deep network.\n\n37:11.840 --> 37:18.240\n So it can learn, understand, solve\n\n37:18.240 --> 37:22.800\n problems that have this compositional structure\n\n37:22.800 --> 37:27.520\n and it cannot solve problems that don't have\n\n37:27.520 --> 37:29.400\n this compositional structure.\n\n37:29.400 --> 37:34.920\n So the problems we are accustomed to, we think about,\n\n37:34.920 --> 37:40.160\n we test our algorithms on, are this compositional structure\n\n37:40.160 --> 37:42.600\n because our brain is made up.\n\n37:42.600 --> 37:45.400\n And that's, in a sense, an evolutionary perspective\n\n37:45.400 --> 37:46.400\n that we've.\n\n37:46.400 --> 37:50.120\n So the ones that didn't have, that weren't\n\n37:50.120 --> 37:55.200\n dealing with the compositional nature of reality died off?\n\n37:55.200 --> 38:00.320\n Yes, but also could be maybe the reason\n\n38:00.320 --> 38:05.480\n why we have this local connectivity in the brain,\n\n38:05.480 --> 38:08.840\n like simple cells in cortex looking\n\n38:08.840 --> 38:11.920\n only at the small part of the image, each one of them,\n\n38:11.920 --> 38:14.680\n and then other cells looking at the small number\n\n38:14.680 --> 38:16.360\n of these simple cells and so on.\n\n38:16.360 --> 38:19.960\n The reason for this may be purely\n\n38:19.960 --> 38:25.080\n that it was difficult to grow long range connectivity.\n\n38:25.080 --> 38:28.640\n So suppose it's for biology.\n\n38:28.640 --> 38:34.280\n It's possible to grow short range connectivity but not\n\n38:34.280 --> 38:38.560\n long range also because there is a limited number of long range\n\n38:38.560 --> 38:39.720\n that you can.\n\n38:39.720 --> 38:45.000\n And so you have this limitation from the biology.\n\n38:45.000 --> 38:50.160\n And this means you build a deep convolutional network.\n\n38:50.160 --> 38:53.600\n This would be something like a deep convolutional network.\n\n38:53.600 --> 38:57.800\n And this is great for solving certain class of problems.\n\n38:57.800 --> 39:02.880\n These are the ones we find easy and important for our life.\n\n39:02.880 --> 39:07.320\n And yes, they were enough for us to survive.\n\n39:07.320 --> 39:10.800\n And you can start a successful business\n\n39:10.800 --> 39:14.600\n on solving those problems with Mobileye.\n\n39:14.600 --> 39:17.360\n Driving is a compositional problem.\n\n39:17.360 --> 39:21.080\n So on the learning task, we don't\n\n39:21.080 --> 39:24.000\n know much about how the brain learns\n\n39:24.000 --> 39:26.320\n in terms of optimization.\n\n39:26.320 --> 39:29.040\n So the thing that's stochastic gradient descent\n\n39:29.040 --> 39:33.760\n is what artificial neural networks use for the most part\n\n39:33.760 --> 39:37.520\n to adjust the parameters in such a way that it's\n\n39:37.520 --> 39:40.640\n able to deal based on the label data,\n\n39:40.640 --> 39:42.520\n it's able to solve the problem.\n\n39:42.520 --> 39:50.040\n So what's your intuition about why it works at all?\n\n39:50.040 --> 39:53.360\n How hard of a problem it is to optimize\n\n39:53.360 --> 39:56.320\n a neural network, artificial neural network?\n\n39:56.320 --> 39:58.720\n Is there other alternatives?\n\n39:58.720 --> 40:01.640\n Just in general, your intuition is\n\n40:01.640 --> 40:03.800\n behind this very simplistic algorithm\n\n40:03.800 --> 40:06.640\n that seems to do pretty good, surprisingly so.\n\n40:06.640 --> 40:07.840\n Yes.\n\n40:07.840 --> 40:13.840\n So I find neuroscience, the architecture of cortex,\n\n40:13.840 --> 40:17.440\n is really similar to the architecture of deep networks.\n\n40:17.440 --> 40:20.360\n So there is a nice correspondence there\n\n40:20.360 --> 40:23.160\n between the biology and this kind\n\n40:23.160 --> 40:28.200\n of local connectivity, hierarchical architecture.\n\n40:28.200 --> 40:30.960\n The stochastic gradient descent, as you said,\n\n40:30.960 --> 40:35.760\n is a very simple technique.\n\n40:35.760 --> 40:41.320\n It seems pretty unlikely that biology could do that\n\n40:41.320 --> 40:47.360\n from what we know right now about cortex and neurons\n\n40:47.360 --> 40:50.200\n and synapses.\n\n40:50.200 --> 40:53.080\n So it's a big question open whether there\n\n40:53.080 --> 40:59.040\n are other optimization learning algorithms that\n\n40:59.040 --> 41:02.000\n can replace stochastic gradient descent.\n\n41:02.000 --> 41:11.760\n And my guess is yes, but nobody has found yet a real answer.\n\n41:11.760 --> 41:13.840\n I mean, people are trying, still trying,\n\n41:13.840 --> 41:18.280\n and there are some interesting ideas.\n\n41:18.280 --> 41:22.000\n The fact that stochastic gradient descent\n\n41:22.000 --> 41:26.160\n is so successful, this has become clearly not so\n\n41:26.160 --> 41:27.640\n mysterious.\n\n41:27.640 --> 41:33.840\n And the reason is that it's an interesting fact.\n\n41:33.840 --> 41:36.840\n It's a change, in a sense, in how\n\n41:36.840 --> 41:39.280\n people think about statistics.\n\n41:39.280 --> 41:45.160\n And this is the following, is that typically when\n\n41:45.160 --> 41:51.800\n you had data and you had, say, a model with parameters,\n\n41:51.800 --> 41:54.520\n you are trying to fit the model to the data,\n\n41:54.520 --> 41:55.960\n to fit the parameter.\n\n41:55.960 --> 42:04.520\n Typically, the kind of crowd wisdom type idea\n\n42:04.520 --> 42:09.720\n was you should have at least twice the number of data\n\n42:09.720 --> 42:12.880\n than the number of parameters.\n\n42:12.880 --> 42:15.480\n Maybe 10 times is better.\n\n42:15.480 --> 42:19.560\n Now, the way you train neural networks these days\n\n42:19.560 --> 42:23.480\n is that they have 10 or 100 times more parameters\n\n42:23.480 --> 42:26.760\n than data, exactly the opposite.\n\n42:26.760 --> 42:34.080\n And it has been one of the puzzles about neural networks.\n\n42:34.080 --> 42:37.120\n How can you get something that really works\n\n42:37.120 --> 42:40.640\n when you have so much freedom?\n\n42:40.640 --> 42:43.000\n From that little data, it can generalize somehow.\n\n42:43.000 --> 42:44.200\n Right, exactly.\n\n42:44.200 --> 42:46.400\n Do you think the stochastic nature of it\n\n42:46.400 --> 42:48.160\n is essential, the randomness?\n\n42:48.160 --> 42:50.640\n So I think we have some initial understanding\n\n42:50.640 --> 42:52.240\n why this happens.\n\n42:52.240 --> 42:56.480\n But one nice side effect of having\n\n42:56.480 --> 43:00.920\n this overparameterization, more parameters than data,\n\n43:00.920 --> 43:04.720\n is that when you look for the minima of a loss function,\n\n43:04.720 --> 43:08.240\n like stochastic gradient descent is doing,\n\n43:08.240 --> 43:12.120\n you find I made some calculations based\n\n43:12.120 --> 43:19.040\n on some old basic theorem of algebra called the Bezu\n\n43:19.040 --> 43:23.240\n theorem that gives you an estimate of the number\n\n43:23.240 --> 43:25.960\n of solution of a system of polynomial equation.\n\n43:25.960 --> 43:30.520\n Anyway, the bottom line is that there are probably\n\n43:30.520 --> 43:36.080\n more minima for a typical deep networks\n\n43:36.080 --> 43:39.480\n than atoms in the universe.\n\n43:39.480 --> 43:42.120\n Just to say, there are a lot because\n\n43:42.120 --> 43:44.760\n of the overparameterization.\n\n43:44.760 --> 43:50.280\n A more global minimum, zero minimum, good minimum.\n\n43:50.280 --> 43:51.560\n A more global minima.\n\n43:51.560 --> 43:53.200\n Yeah, a lot of them.\n\n43:53.200 --> 43:54.560\n So you have a lot of solutions.\n\n43:54.560 --> 43:57.920\n So it's not so surprising that you can find them\n\n43:57.920 --> 44:00.400\n relatively easily.\n\n44:00.400 --> 44:04.200\n And this is because of the overparameterization.\n\n44:04.200 --> 44:07.920\n The overparameterization sprinkles that entire space\n\n44:07.920 --> 44:09.720\n with solutions that are pretty good.\n\n44:09.720 --> 44:11.240\n It's not so surprising, right?\n\n44:11.240 --> 44:14.400\n It's like if you have a system of linear equation\n\n44:14.400 --> 44:18.520\n and you have more unknowns than equations, then you have,\n\n44:18.520 --> 44:22.040\n we know, you have an infinite number of solutions.\n\n44:22.040 --> 44:24.480\n And the question is to pick one.\n\n44:24.480 --> 44:25.440\n That's another story.\n\n44:25.440 --> 44:27.520\n But you have an infinite number of solutions.\n\n44:27.520 --> 44:31.040\n So there are a lot of value of your unknowns\n\n44:31.040 --> 44:33.160\n that satisfy the equations.\n\n44:33.160 --> 44:36.360\n But it's possible that there's a lot of those solutions that\n\n44:36.360 --> 44:37.560\n aren't very good.\n\n44:37.560 --> 44:39.160\n What's surprising is that they're pretty good.\n\n44:39.160 --> 44:40.160\n So that's a good question.\n\n44:40.160 --> 44:42.840\n Why can you pick one that generalizes well?\n\n44:42.840 --> 44:44.120\n Yeah.\n\n44:44.120 --> 44:47.120\n That's a separate question with separate answers.\n\n44:47.120 --> 44:51.160\n One theorem that people like to talk about that kind of\n\n44:51.160 --> 44:53.800\n inspires imagination of the power of neural networks\n\n44:53.800 --> 44:57.840\n is the universality, universal approximation theorem,\n\n44:57.840 --> 45:00.960\n that you can approximate any computable function\n\n45:00.960 --> 45:02.840\n with just a finite number of neurons\n\n45:02.840 --> 45:04.360\n in a single hidden layer.\n\n45:04.360 --> 45:07.680\n Do you find this theorem one surprising?\n\n45:07.680 --> 45:12.600\n Do you find it useful, interesting, inspiring?\n\n45:12.600 --> 45:16.440\n No, this one, I never found it very surprising.\n\n45:16.440 --> 45:22.640\n It was known since the 80s, since I entered the field,\n\n45:22.640 --> 45:27.560\n because it's basically the same as Weierstrass theorem, which\n\n45:27.560 --> 45:32.000\n says that I can approximate any continuous function\n\n45:32.000 --> 45:34.560\n with a polynomial of sufficiently,\n\n45:34.560 --> 45:38.120\n with a sufficient number of terms, monomials.\n\n45:38.120 --> 45:39.360\n So basically the same.\n\n45:39.360 --> 45:41.680\n And the proofs are very similar.\n\n45:41.680 --> 45:43.520\n So your intuition was there was never\n\n45:43.520 --> 45:45.680\n any doubt that neural networks in theory\n\n45:45.680 --> 45:48.000\n could be very strong approximators.\n\n45:48.000 --> 45:48.800\n Right.\n\n45:48.800 --> 45:50.760\n The question, the interesting question,\n\n45:50.760 --> 45:58.520\n is that if this theorem says you can approximate, fine.\n\n45:58.520 --> 46:03.200\n But when you ask how many neurons, for instance,\n\n46:03.200 --> 46:06.400\n or in the case of polynomial, how many monomials,\n\n46:06.400 --> 46:11.360\n I need to get a good approximation.\n\n46:11.360 --> 46:15.960\n Then it turns out that that depends\n\n46:15.960 --> 46:18.080\n on the dimensionality of your function,\n\n46:18.080 --> 46:20.520\n how many variables you have.\n\n46:20.520 --> 46:22.120\n But it depends on the dimensionality\n\n46:22.120 --> 46:25.080\n of your function in a bad way.\n\n46:25.080 --> 46:28.000\n It's, for instance, suppose you want\n\n46:28.000 --> 46:35.040\n an error which is no worse than 10% in your approximation.\n\n46:35.040 --> 46:38.120\n You come up with a network that approximate your function\n\n46:38.120 --> 46:40.440\n within 10%.\n\n46:40.440 --> 46:44.520\n Then it turns out that the number of units you need\n\n46:44.520 --> 46:48.360\n are in the order of 10 to the dimensionality, d,\n\n46:48.360 --> 46:50.080\n how many variables.\n\n46:50.080 --> 46:54.840\n So if you have two variables, these two words,\n\n46:54.840 --> 46:57.240\n you have 100 units and OK.\n\n46:57.240 --> 47:02.920\n But if you have, say, 200 by 200 pixel images,\n\n47:02.920 --> 47:06.840\n now this is 40,000, whatever.\n\n47:06.840 --> 47:09.800\n We again go to the size of the universe pretty quickly.\n\n47:09.800 --> 47:14.120\n Exactly, 10 to the 40,000 or something.\n\n47:14.120 --> 47:18.680\n And so this is called the curse of dimensionality,\n\n47:18.680 --> 47:22.280\n not quite appropriately.\n\n47:22.280 --> 47:24.200\n And the hope is with the extra layers,\n\n47:24.200 --> 47:28.040\n you can remove the curse.\n\n47:28.040 --> 47:32.280\n What we proved is that if you have deep layers,\n\n47:32.280 --> 47:36.200\n hierarchical architecture with the local connectivity\n\n47:36.200 --> 47:39.960\n of the type of convolutional deep learning,\n\n47:39.960 --> 47:42.000\n and if you're dealing with a function that\n\n47:42.000 --> 47:46.680\n has this kind of hierarchical architecture,\n\n47:46.680 --> 47:50.680\n then you avoid completely the curse.\n\n47:50.680 --> 47:54.520\n You've spoken a lot about supervised deep learning.\n\n47:54.520 --> 47:56.480\n What are your thoughts, hopes, views\n\n47:56.480 --> 47:59.640\n on the challenges of unsupervised learning\n\n47:59.640 --> 48:05.800\n with GANs, with Generative Adversarial Networks?\n\n48:05.800 --> 48:08.120\n Do you see those as distinct?\n\n48:08.120 --> 48:09.920\n The power of GANs, do you see those\n\n48:09.920 --> 48:13.120\n as distinct from supervised methods in neural networks,\n\n48:13.120 --> 48:16.640\n or are they really all in the same representation ballpark?\n\n48:16.640 --> 48:24.040\n GANs is one way to get estimation of probability\n\n48:24.040 --> 48:28.760\n densities, which is a somewhat new way that people have not\n\n48:28.760 --> 48:30.360\n done before.\n\n48:30.360 --> 48:36.080\n I don't know whether this will really play an important role\n\n48:36.080 --> 48:39.000\n in intelligence.\n\n48:39.000 --> 48:43.080\n Or it's interesting.\n\n48:43.080 --> 48:48.600\n I'm less enthusiastic about it than many people in the field.\n\n48:48.600 --> 48:50.880\n I have the feeling that many people in the field\n\n48:50.880 --> 48:54.320\n are really impressed by the ability\n\n48:54.320 --> 49:01.160\n of producing realistic looking images in this generative way.\n\n49:01.160 --> 49:03.080\n Which describes the popularity of the methods.\n\n49:03.080 --> 49:06.320\n But you're saying that while that's exciting and cool\n\n49:06.320 --> 49:11.200\n to look at, it may not be the tool that's useful for it.\n\n49:11.200 --> 49:13.560\n So you describe it kind of beautifully.\n\n49:13.560 --> 49:16.320\n Current supervised methods go n to infinity\n\n49:16.320 --> 49:18.200\n in terms of number of labeled points.\n\n49:18.200 --> 49:21.360\n And we really have to figure out how to go to n to 1.\n\n49:21.360 --> 49:23.200\n And you're thinking GANs might help,\n\n49:23.200 --> 49:25.080\n but they might not be the right.\n\n49:25.080 --> 49:28.480\n I don't think for that problem, which I really think\n\n49:28.480 --> 49:32.000\n is important, I think they may help.\n\n49:32.000 --> 49:33.680\n They certainly have applications,\n\n49:33.680 --> 49:35.760\n for instance, in computer graphics.\n\n49:35.760 --> 49:41.560\n And I did work long ago, which was\n\n49:41.560 --> 49:47.000\n a little bit similar in terms of saying, OK, I have a network.\n\n49:47.000 --> 49:49.760\n And I present images.\n\n49:49.760 --> 49:54.040\n And I can input its images.\n\n49:54.040 --> 49:57.520\n And output is, for instance, the pose of the image.\n\n49:57.520 --> 50:02.960\n A face, how much is smiling, is rotated 45 degrees or not.\n\n50:02.960 --> 50:07.440\n What about having a network that I train with the same data\n\n50:07.440 --> 50:10.600\n set, but now I invert input and output.\n\n50:10.600 --> 50:15.920\n Now the input is the pose or the expression, a number,\n\n50:15.920 --> 50:16.920\n set of numbers.\n\n50:16.920 --> 50:18.280\n And the output is the image.\n\n50:18.280 --> 50:20.240\n And I train it.\n\n50:20.240 --> 50:22.520\n And we did pretty good, interesting results\n\n50:22.520 --> 50:27.840\n in terms of producing very realistic looking images.\n\n50:27.840 --> 50:31.920\n It was a less sophisticated mechanism.\n\n50:31.920 --> 50:35.320\n But the output was pretty less than GANs.\n\n50:35.320 --> 50:38.960\n But the output was pretty much of the same quality.\n\n50:38.960 --> 50:43.400\n So I think for a computer graphics type application,\n\n50:43.400 --> 50:46.240\n yeah, definitely GANs can be quite useful.\n\n50:46.240 --> 50:52.880\n And not only for that, but for helping,\n\n50:52.880 --> 50:58.200\n for instance, on this problem of unsupervised example\n\n50:58.200 --> 51:02.400\n of reducing the number of labeled examples.\n\n51:02.400 --> 51:07.920\n I think people, it's like they think they can get out\n\n51:07.920 --> 51:11.080\n more than they put in.\n\n51:11.080 --> 51:14.000\n There's no free lunch, as you said.\n\n51:14.000 --> 51:17.320\n What do you think, what's your intuition?\n\n51:17.320 --> 51:22.720\n How can we slow the growth of N to infinity in supervised,\n\n51:22.720 --> 51:25.080\n N to infinity in supervised learning?\n\n51:25.080 --> 51:29.880\n So for example, Mobileye has very successfully,\n\n51:29.880 --> 51:33.000\n I mean, essentially annotated large amounts of data\n\n51:33.000 --> 51:34.680\n to be able to drive a car.\n\n51:34.680 --> 51:37.440\n Now one thought is, so we're trying\n\n51:37.440 --> 51:41.000\n to teach machines, school of AI.\n\n51:41.000 --> 51:45.560\n And we're trying to, so how can we become better teachers,\n\n51:45.560 --> 51:46.040\n maybe?\n\n51:46.040 --> 51:47.320\n That's one way.\n\n51:47.320 --> 51:51.240\n No, I like that.\n\n51:51.240 --> 51:57.680\n Because again, one caricature of the history of computer\n\n51:57.680 --> 52:05.360\n science, you could say, begins with programmers, expensive.\n\n52:05.360 --> 52:09.640\n Continuous labelers, cheap.\n\n52:09.640 --> 52:14.680\n And the future will be schools, like we have for kids.\n\n52:14.680 --> 52:16.360\n Yeah.\n\n52:16.360 --> 52:20.280\n Currently, the labeling methods were not\n\n52:20.280 --> 52:25.880\n selective about which examples we teach networks with.\n\n52:25.880 --> 52:31.320\n So I think the focus of making networks that learn much faster\n\n52:31.320 --> 52:33.680\n is often on the architecture side.\n\n52:33.680 --> 52:37.960\n But how can we pick better examples with which to learn?\n\n52:37.960 --> 52:39.440\n Do you have intuitions about that?\n\n52:39.440 --> 52:42.480\n Well, that's part of the problem.\n\n52:42.480 --> 52:50.360\n But the other one is, if we look at biology,\n\n52:50.360 --> 52:52.960\n a reasonable assumption, I think,\n\n52:52.960 --> 52:58.120\n is in the same spirit that I said,\n\n52:58.120 --> 53:03.400\n evolution is opportunistic and has weak priors.\n\n53:03.400 --> 53:08.280\n The way I think the intelligence of a child,\n\n53:08.280 --> 53:16.240\n the baby may develop is by bootstrapping weak priors\n\n53:16.240 --> 53:17.400\n from evolution.\n\n53:17.400 --> 53:24.720\n For instance, you can assume that you\n\n53:24.720 --> 53:28.960\n have in most organisms, including human babies,\n\n53:28.960 --> 53:35.400\n built in some basic machinery to detect motion\n\n53:35.400 --> 53:38.200\n and relative motion.\n\n53:38.200 --> 53:42.920\n And in fact, we know all insects from fruit flies\n\n53:42.920 --> 53:49.760\n to other animals, they have this,\n\n53:49.760 --> 53:53.120\n even in the retinas, in the very peripheral part.\n\n53:53.120 --> 53:56.160\n It's very conserved across species, something\n\n53:56.160 --> 53:59.040\n that evolution discovered early.\n\n53:59.040 --> 54:01.480\n It may be the reason why babies tend\n\n54:01.480 --> 54:06.160\n to look in the first few days to moving objects\n\n54:06.160 --> 54:08.320\n and not to not moving objects.\n\n54:08.320 --> 54:12.200\n Now, moving objects means, OK, they're attracted by motion.\n\n54:12.200 --> 54:15.480\n But motion also means that motion\n\n54:15.480 --> 54:20.560\n gives automatic segmentation from the background.\n\n54:20.560 --> 54:25.360\n So because of motion boundaries, either the object\n\n54:25.360 --> 54:30.600\n is moving or the eye of the baby is tracking the moving object\n\n54:30.600 --> 54:32.800\n and the background is moving, right?\n\n54:32.800 --> 54:36.040\n Yeah, so just purely on the visual characteristics\n\n54:36.040 --> 54:37.920\n of the scene, that seems to be the most useful.\n\n54:37.920 --> 54:43.960\n Right, so it's like looking at an object without background.\n\n54:43.960 --> 54:45.760\n It's ideal for learning the object.\n\n54:45.760 --> 54:48.760\n Otherwise, it's really difficult because you\n\n54:48.760 --> 54:50.440\n have so much stuff.\n\n54:50.440 --> 54:55.120\n So suppose you do this at the beginning, first weeks.\n\n54:55.120 --> 54:58.560\n Then after that, you can recognize object.\n\n54:58.560 --> 55:02.160\n Now they are imprinted, the number one,\n\n55:02.160 --> 55:05.800\n even in the background, even without motion.\n\n55:05.800 --> 55:08.160\n So that's, by the way, I just want\n\n55:08.160 --> 55:10.920\n to ask on the object recognition problem.\n\n55:10.920 --> 55:13.960\n So there is this being responsive to movement\n\n55:13.960 --> 55:16.760\n and doing edge detection, essentially.\n\n55:16.760 --> 55:21.600\n What's the gap between being effective at visually\n\n55:21.600 --> 55:24.560\n recognizing stuff, detecting where it is,\n\n55:24.560 --> 55:27.640\n and understanding the scene?\n\n55:27.640 --> 55:32.960\n Is this a huge gap in many layers, or is it close?\n\n55:32.960 --> 55:35.120\n No, I think that's a huge gap.\n\n55:35.120 --> 55:42.040\n I think present algorithm with all the success that we have\n\n55:42.040 --> 55:45.120\n and the fact that there are a lot of very useful,\n\n55:45.120 --> 55:48.440\n I think we are in a golden age for applications\n\n55:48.440 --> 55:53.720\n of low level vision and low level speech recognition\n\n55:53.720 --> 55:56.800\n and so on, Alexa and so on.\n\n55:56.800 --> 55:58.840\n There are many more things of similar level\n\n55:58.840 --> 56:02.040\n to be done, including medical diagnosis and so on.\n\n56:02.040 --> 56:05.600\n But we are far from what we call understanding\n\n56:05.600 --> 56:11.960\n of a scene, of language, of actions, of people.\n\n56:11.960 --> 56:18.480\n That is, despite the claims, that's, I think, very far.\n\n56:18.480 --> 56:19.560\n We're a little bit off.\n\n56:19.560 --> 56:23.160\n So in popular culture and among many researchers,\n\n56:23.160 --> 56:25.640\n some of which I've spoken with, the Stuart Russell\n\n56:25.640 --> 56:30.920\n and Elon Musk, in and out of the AI field,\n\n56:30.920 --> 56:34.520\n there's a concern about the existential threat of AI.\n\n56:34.520 --> 56:40.000\n And how do you think about this concern?\n\n56:40.000 --> 56:45.560\n And is it valuable to think about large scale, long term,\n\n56:45.560 --> 56:50.360\n unintended consequences of intelligent systems\n\n56:50.360 --> 56:51.440\n we try to build?\n\n56:51.440 --> 56:56.000\n I always think it's better to worry first, early,\n\n56:56.000 --> 56:58.640\n rather than late.\n\n56:58.640 --> 56:59.640\n So worry is good.\n\n56:59.640 --> 57:00.400\n Yeah.\n\n57:00.400 --> 57:03.000\n I'm not against worrying at all.\n\n57:03.000 --> 57:09.520\n Personally, I think that it will take a long time\n\n57:09.520 --> 57:15.920\n before there is real reason to be worried.\n\n57:15.920 --> 57:19.440\n But as I said, I think it's good to put in place\n\n57:19.440 --> 57:24.360\n and think about possible safety against.\n\n57:24.360 --> 57:27.360\n What I find a bit misleading are things\n\n57:27.360 --> 57:31.480\n like that have been said by people I know, like Elon Musk,\n\n57:31.480 --> 57:35.240\n and what is Bostrom in particular,\n\n57:35.240 --> 57:36.800\n and what is his first name?\n\n57:36.800 --> 57:37.400\n Nick Bostrom.\n\n57:37.400 --> 57:40.120\n Nick Bostrom, right.\n\n57:40.120 --> 57:44.080\n And a couple of other people that, for instance, AI\n\n57:44.080 --> 57:46.880\n is more dangerous than nuclear weapons.\n\n57:46.880 --> 57:50.400\n I think that's really wrong.\n\n57:50.400 --> 57:52.680\n That can be misleading.\n\n57:52.680 --> 57:56.440\n Because in terms of priority, we should still\n\n57:56.440 --> 57:59.480\n be more worried about nuclear weapons\n\n57:59.480 --> 58:05.600\n and what people are doing about it and so on than AI.\n\n58:05.600 --> 58:09.920\n And you've spoken about Demis Hassabis\n\n58:09.920 --> 58:12.840\n and yourself saying that you think\n\n58:12.840 --> 58:16.440\n you'll be about 100 years out before we\n\n58:16.440 --> 58:18.920\n have a general intelligence system that's\n\n58:18.920 --> 58:20.600\n on par with a human being.\n\n58:20.600 --> 58:22.520\n Do you have any updates for those predictions?\n\n58:22.520 --> 58:24.080\n Well, I think he said.\n\n58:24.080 --> 58:25.080\n He said 20, I think.\n\n58:25.080 --> 58:26.200\n He said 20, right.\n\n58:26.200 --> 58:27.680\n This was a couple of years ago.\n\n58:27.680 --> 58:29.160\n I have not asked him again.\n\n58:29.160 --> 58:31.480\n So should I have?\n\n58:31.480 --> 58:36.000\n Your own prediction, what's your prediction\n\n58:36.000 --> 58:38.880\n about when you'll be truly surprised?\n\n58:38.880 --> 58:43.000\n And what's the confidence interval on that?\n\n58:43.000 --> 58:45.760\n It's so difficult to predict the future and even\n\n58:45.760 --> 58:47.120\n the present sometimes.\n\n58:47.120 --> 58:48.480\n It's pretty hard to predict.\n\n58:48.480 --> 58:53.360\n But I would be, as I said, this is completely,\n\n58:53.360 --> 58:56.960\n I would be more like Rod Brooks.\n\n58:56.960 --> 58:58.960\n I think he's about 200 years.\n\n58:58.960 --> 59:01.560\n 200 years.\n\n59:01.560 --> 59:04.880\n When we have this kind of AGI system,\n\n59:04.880 --> 59:06.920\n artificial general intelligence system,\n\n59:06.920 --> 59:12.840\n you're sitting in a room with her, him, it.\n\n59:12.840 --> 59:17.120\n Do you think the underlying design of such a system\n\n59:17.120 --> 59:19.080\n is something we'll be able to understand?\n\n59:19.080 --> 59:20.480\n It will be simple?\n\n59:20.480 --> 59:25.800\n Do you think it'll be explainable,\n\n59:25.800 --> 59:27.560\n understandable by us?\n\n59:27.560 --> 59:30.760\n Your intuition, again, we're in the realm of philosophy\n\n59:30.760 --> 59:32.080\n a little bit.\n\n59:32.080 --> 59:36.120\n Well, probably no.\n\n59:36.120 --> 59:40.280\n But again, it depends what you really\n\n59:40.280 --> 59:42.000\n mean for understanding.\n\n59:42.000 --> 59:53.280\n So I think we don't understand how deep networks work.\n\n59:53.280 --> 59:56.520\n I think we are beginning to have a theory now.\n\n59:56.520 --> 59:59.240\n But in the case of deep networks,\n\n59:59.240 --> 1:00:04.120\n or even in the case of the simpler kernel machines\n\n1:00:04.120 --> 1:00:08.120\n or linear classifier, we really don't understand\n\n1:00:08.120 --> 1:00:11.520\n the individual units or so.\n\n1:00:11.520 --> 1:00:17.280\n But we understand what the computation and the limitations\n\n1:00:17.280 --> 1:00:20.440\n and the properties of it are.\n\n1:00:20.440 --> 1:00:24.040\n It's similar to many things.\n\n1:00:24.040 --> 1:00:29.600\n What does it mean to understand how a fusion bomb works?\n\n1:00:29.600 --> 1:00:36.360\n How many of us understand the basic principle?\n\n1:00:36.360 --> 1:00:40.600\n And some of us may understand deeper details.\n\n1:00:40.600 --> 1:00:43.440\n In that sense, understanding is, as a community,\n\n1:00:43.440 --> 1:00:47.360\n as a civilization, can we build another copy of it?\n\n1:00:47.360 --> 1:00:50.880\n And in that sense, do you think there\n\n1:00:50.880 --> 1:00:53.960\n will need to be some evolutionary component where\n\n1:00:53.960 --> 1:00:56.200\n it runs away from our understanding?\n\n1:00:56.200 --> 1:00:59.440\n Or do you think it could be engineered from the ground up,\n\n1:00:59.440 --> 1:01:02.640\n the same way you go from the transistor to PowerPoint?\n\n1:01:02.640 --> 1:01:09.160\n So many years ago, this was actually 40, 41 years ago,\n\n1:01:09.160 --> 1:01:13.560\n I wrote a paper with David Marr, who\n\n1:01:13.560 --> 1:01:18.000\n was one of the founding fathers of computer vision,\n\n1:01:18.000 --> 1:01:20.440\n computational vision.\n\n1:01:20.440 --> 1:01:23.840\n I wrote a paper about levels of understanding,\n\n1:01:23.840 --> 1:01:26.160\n which is related to the question we discussed earlier\n\n1:01:26.160 --> 1:01:30.200\n about understanding PowerPoint, understanding transistors,\n\n1:01:30.200 --> 1:01:31.840\n and so on.\n\n1:01:31.840 --> 1:01:36.560\n And in that kind of framework, we\n\n1:01:36.560 --> 1:01:39.760\n had the level of the hardware and the top level\n\n1:01:39.760 --> 1:01:42.240\n of the algorithms.\n\n1:01:42.240 --> 1:01:45.040\n We did not have learning.\n\n1:01:45.040 --> 1:01:48.280\n Recently, I updated adding levels.\n\n1:01:48.280 --> 1:01:55.160\n And one level I added to those three was learning.\n\n1:01:55.160 --> 1:01:59.520\n And you can imagine, you could have a good understanding\n\n1:01:59.520 --> 1:02:04.960\n of how you construct a learning machine, like we do.\n\n1:02:04.960 --> 1:02:09.720\n But being unable to describe in detail what the learning\n\n1:02:09.720 --> 1:02:13.680\n machines will discover, right?\n\n1:02:13.680 --> 1:02:17.120\n Now, that would be still a powerful understanding,\n\n1:02:17.120 --> 1:02:19.400\n if I can build a learning machine,\n\n1:02:19.400 --> 1:02:24.480\n even if I don't understand in detail every time it\n\n1:02:24.480 --> 1:02:26.160\n learns something.\n\n1:02:26.160 --> 1:02:28.440\n Just like our children, if they start\n\n1:02:28.440 --> 1:02:31.320\n listening to a certain type of music,\n\n1:02:31.320 --> 1:02:33.680\n I don't know, Miley Cyrus or something,\n\n1:02:33.680 --> 1:02:36.240\n you don't understand why they came\n\n1:02:36.240 --> 1:02:37.640\n to that particular preference.\n\n1:02:37.640 --> 1:02:39.400\n But you understand the learning process.\n\n1:02:39.400 --> 1:02:41.440\n That's very interesting.\n\n1:02:41.440 --> 1:02:50.360\n So on learning for systems to be part of our world,\n\n1:02:50.360 --> 1:02:53.480\n it has a certain, one of the challenging things\n\n1:02:53.480 --> 1:02:57.920\n that you've spoken about is learning ethics, learning\n\n1:02:57.920 --> 1:02:59.400\n morals.\n\n1:02:59.400 --> 1:03:04.560\n And how hard do you think is the problem of, first of all,\n\n1:03:04.560 --> 1:03:06.800\n humans understanding our ethics?\n\n1:03:06.800 --> 1:03:10.600\n What is the origin on the neural on the low level of ethics?\n\n1:03:10.600 --> 1:03:12.400\n What is it at the higher level?\n\n1:03:12.400 --> 1:03:15.160\n Is it something that's learnable from machines\n\n1:03:15.160 --> 1:03:17.840\n in your intuition?\n\n1:03:17.840 --> 1:03:23.960\n I think, yeah, ethics is learnable, very likely.\n\n1:03:23.960 --> 1:03:29.720\n I think it's one of these problems where\n\n1:03:29.720 --> 1:03:36.680\n I think understanding the neuroscience of ethics,\n\n1:03:36.680 --> 1:03:41.480\n people discuss there is an ethics of neuroscience.\n\n1:03:41.480 --> 1:03:42.800\n Yeah, yes.\n\n1:03:42.800 --> 1:03:46.560\n How a neuroscientist should or should not behave.\n\n1:03:46.560 --> 1:03:50.480\n Can you think of a neurosurgeon and the ethics\n\n1:03:50.480 --> 1:03:53.960\n rule he has to be or he, she has to be.\n\n1:03:53.960 --> 1:03:57.560\n But I'm more interested on the neuroscience of ethics.\n\n1:03:57.560 --> 1:03:58.840\n You're blowing my mind right now.\n\n1:03:58.840 --> 1:04:01.080\n The neuroscience of ethics is very meta.\n\n1:04:01.080 --> 1:04:05.080\n Yeah, and I think that would be important to understand also\n\n1:04:05.080 --> 1:04:10.880\n for being able to design machines that\n\n1:04:10.880 --> 1:04:15.160\n are ethical machines in our sense of ethics.\n\n1:04:15.160 --> 1:04:18.520\n And you think there is something in neuroscience,\n\n1:04:18.520 --> 1:04:21.520\n there's patterns, tools in neuroscience\n\n1:04:21.520 --> 1:04:25.320\n that could help us shed some light on ethics?\n\n1:04:25.320 --> 1:04:28.920\n Or is it mostly on the psychologists of sociology\n\n1:04:28.920 --> 1:04:29.840\n in which higher level?\n\n1:04:29.840 --> 1:04:30.960\n No, there is psychology.\n\n1:04:30.960 --> 1:04:35.160\n But there is also, in the meantime,\n\n1:04:35.160 --> 1:04:41.080\n there is evidence, fMRI, of specific areas of the brain\n\n1:04:41.080 --> 1:04:44.520\n that are involved in certain ethical judgment.\n\n1:04:44.520 --> 1:04:47.640\n And not only this, you can stimulate those area\n\n1:04:47.640 --> 1:04:53.920\n with magnetic fields and change the ethical decisions.\n\n1:04:53.920 --> 1:04:56.360\n Yeah, wow.\n\n1:04:56.360 --> 1:05:00.800\n So that's work by a colleague of mine, Rebecca Sachs.\n\n1:05:00.800 --> 1:05:05.320\n And there is other researchers doing similar work.\n\n1:05:05.320 --> 1:05:08.280\n And I think this is the beginning.\n\n1:05:08.280 --> 1:05:11.680\n But ideally, at some point, we'll\n\n1:05:11.680 --> 1:05:15.560\n have an understanding of how this works.\n\n1:05:15.560 --> 1:05:18.520\n And why it evolved, right?\n\n1:05:18.520 --> 1:05:19.720\n The big why question.\n\n1:05:19.720 --> 1:05:22.000\n Yeah, it must have some purpose.\n\n1:05:22.000 --> 1:05:30.120\n Yeah, obviously it has some social purposes, probably.\n\n1:05:30.120 --> 1:05:33.600\n If neuroscience holds the key to at least illuminate\n\n1:05:33.600 --> 1:05:35.240\n some aspect of ethics, that means\n\n1:05:35.240 --> 1:05:37.120\n it could be a learnable problem.\n\n1:05:37.120 --> 1:05:38.880\n Yeah, exactly.\n\n1:05:38.880 --> 1:05:42.040\n And as we're getting into harder and harder questions,\n\n1:05:42.040 --> 1:05:45.440\n let's go to the hard problem of consciousness.\n\n1:05:45.440 --> 1:05:48.080\n Is this an important problem for us\n\n1:05:48.080 --> 1:05:52.240\n to think about and solve on the engineering of intelligence\n\n1:05:52.240 --> 1:05:56.240\n side of your work, of our dream?\n\n1:05:56.240 --> 1:05:57.440\n It's unclear.\n\n1:05:57.440 --> 1:06:02.680\n So again, this is a deep problem,\n\n1:06:02.680 --> 1:06:05.720\n partly because it's very difficult to define\n\n1:06:05.720 --> 1:06:06.760\n consciousness.\n\n1:06:06.760 --> 1:06:17.800\n And there is a debate among neuroscientists\n\n1:06:17.800 --> 1:06:23.040\n about whether consciousness and philosophers, of course,\n\n1:06:23.040 --> 1:06:28.280\n whether consciousness is something that requires\n\n1:06:28.280 --> 1:06:31.360\n flesh and blood, so to speak.\n\n1:06:31.360 --> 1:06:38.680\n Or could be that we could have silicon devices that\n\n1:06:38.680 --> 1:06:42.840\n are conscious, or up to statement\n\n1:06:42.840 --> 1:06:45.800\n like everything has some degree of consciousness\n\n1:06:45.800 --> 1:06:48.480\n and some more than others.\n\n1:06:48.480 --> 1:06:53.960\n This is like Giulio Tonioni and phi.\n\n1:06:53.960 --> 1:06:56.280\n We just recently talked to Christoph Koch.\n\n1:06:56.280 --> 1:06:57.600\n OK.\n\n1:06:57.600 --> 1:07:00.680\n Christoph was my first graduate student.\n\n1:07:00.680 --> 1:07:04.480\n Do you think it's important to illuminate\n\n1:07:04.480 --> 1:07:07.480\n aspects of consciousness in order\n\n1:07:07.480 --> 1:07:10.320\n to engineer intelligence systems?\n\n1:07:10.320 --> 1:07:13.080\n Do you think an intelligent system would ultimately\n\n1:07:13.080 --> 1:07:14.480\n have consciousness?\n\n1:07:14.480 --> 1:07:16.080\n Are they interlinked?\n\n1:07:18.800 --> 1:07:22.800\n Most of the people working in artificial intelligence,\n\n1:07:22.800 --> 1:07:25.800\n I think, would answer, we don't strictly\n\n1:07:25.800 --> 1:07:30.040\n need consciousness to have an intelligent system.\n\n1:07:30.040 --> 1:07:31.800\n That's sort of the easier question,\n\n1:07:31.800 --> 1:07:36.000\n because it's a very engineering answer to the question.\n\n1:07:36.000 --> 1:07:38.120\n Pass the Turing test, we don't need consciousness.\n\n1:07:38.120 --> 1:07:41.360\n But if you were to go, do you think\n\n1:07:41.360 --> 1:07:46.200\n it's possible that we need to have\n\n1:07:46.200 --> 1:07:48.280\n that kind of self awareness?\n\n1:07:48.280 --> 1:07:49.920\n We may, yes.\n\n1:07:49.920 --> 1:07:53.800\n So for instance, I personally think\n\n1:07:53.800 --> 1:08:00.440\n that when test a machine or a person in a Turing test,\n\n1:08:00.440 --> 1:08:05.200\n in an extended Turing test, I think\n\n1:08:05.200 --> 1:08:11.520\n consciousness is part of what we require in that test,\n\n1:08:11.520 --> 1:08:15.000\n implicitly, to say that this is intelligent.\n\n1:08:15.000 --> 1:08:17.440\n Christoph disagrees.\n\n1:08:17.440 --> 1:08:20.240\n Yes, he does.\n\n1:08:20.240 --> 1:08:23.440\n Despite many other romantic notions he holds,\n\n1:08:23.440 --> 1:08:24.800\n he disagrees with that one.\n\n1:08:24.800 --> 1:08:26.520\n Yes, that's right.\n\n1:08:26.520 --> 1:08:29.880\n So we'll see.\n\n1:08:29.880 --> 1:08:34.640\n Do you think, as a quick question,\n\n1:08:34.640 --> 1:08:38.520\n Ernest Becker's fear of death, do you\n\n1:08:38.520 --> 1:08:41.920\n think mortality and those kinds of things\n\n1:08:41.920 --> 1:08:49.200\n are important for consciousness and for intelligence?\n\n1:08:49.200 --> 1:08:54.040\n The finiteness of life, finiteness of existence,\n\n1:08:54.040 --> 1:08:56.600\n or is that just a side effect of evolution,\n\n1:08:56.600 --> 1:09:01.120\n evolutionary side effect that's useful for natural selection?\n\n1:09:01.120 --> 1:09:03.840\n Do you think this kind of thing that this interview is\n\n1:09:03.840 --> 1:09:06.160\n going to run out of time soon, our life\n\n1:09:06.160 --> 1:09:08.200\n will run out of time soon, do you\n\n1:09:08.200 --> 1:09:11.720\n think that's needed to make this conversation good and life\n\n1:09:11.720 --> 1:09:12.240\n good?\n\n1:09:12.240 --> 1:09:13.480\n I never thought about it.\n\n1:09:13.480 --> 1:09:15.920\n It's a very interesting question.\n\n1:09:15.920 --> 1:09:21.200\n I think Steve Jobs, in his commencement speech\n\n1:09:21.200 --> 1:09:26.840\n at Stanford, argued that having a finite life\n\n1:09:26.840 --> 1:09:30.280\n was important for stimulating achievements.\n\n1:09:30.280 --> 1:09:31.640\n So it was different.\n\n1:09:31.640 --> 1:09:33.680\n Yeah, live every day like it's your last, right?\n\n1:09:33.680 --> 1:09:34.840\n Yeah.\n\n1:09:34.840 --> 1:09:41.840\n So rationally, I don't think strictly you need mortality\n\n1:09:41.840 --> 1:09:43.200\n for consciousness.\n\n1:09:43.200 --> 1:09:45.960\n But who knows?\n\n1:09:45.960 --> 1:09:48.760\n They seem to go together in our biological system, right?\n\n1:09:48.760 --> 1:09:51.320\n Yeah, yeah.\n\n1:09:51.320 --> 1:09:57.880\n You've mentioned before, and students are associated with,\n\n1:09:57.880 --> 1:10:01.280\n AlphaGo immobilized the big recent success stories in AI.\n\n1:10:01.280 --> 1:10:06.040\n And I think it's captivated the entire world of what AI can do.\n\n1:10:06.040 --> 1:10:10.360\n So what do you think will be the next breakthrough?\n\n1:10:10.360 --> 1:10:13.680\n And what's your intuition about the next breakthrough?\n\n1:10:13.680 --> 1:10:16.760\n Of course, I don't know where the next breakthrough is.\n\n1:10:16.760 --> 1:10:21.440\n I think that there is a good chance, as I said before,\n\n1:10:21.440 --> 1:10:23.200\n that the next breakthrough will also\n\n1:10:23.200 --> 1:10:27.920\n be inspired by neuroscience.\n\n1:10:27.920 --> 1:10:32.320\n But which one, I don't know.\n\n1:10:32.320 --> 1:10:35.880\n And there's, so MIT has this quest for intelligence.\n\n1:10:35.880 --> 1:10:39.240\n And there's a few moon shots, which in that spirit,\n\n1:10:39.240 --> 1:10:41.800\n which ones are you excited about?\n\n1:10:41.800 --> 1:10:44.080\n Which projects kind of?\n\n1:10:44.080 --> 1:10:47.400\n Well, of course, I'm excited about one\n\n1:10:47.400 --> 1:10:51.040\n of the moon shots, which is our Center for Brains, Minds,\n\n1:10:51.040 --> 1:10:58.560\n and Machines, which is the one which is fully funded by NSF.\n\n1:10:58.560 --> 1:11:02.760\n And it is about visual intelligence.\n\n1:11:02.760 --> 1:11:06.240\n And that one is particularly about understanding.\n\n1:11:06.240 --> 1:11:09.240\n Visual intelligence, so the visual cortex,\n\n1:11:09.240 --> 1:11:13.400\n and visual intelligence in the sense\n\n1:11:13.400 --> 1:11:20.000\n of how we look around ourselves and understand\n\n1:11:20.000 --> 1:11:25.440\n the world around ourselves, meaning what is going on,\n\n1:11:25.440 --> 1:11:29.880\n how we could go from here to there without hitting\n\n1:11:29.880 --> 1:11:34.360\n obstacles, whether there are other agents,\n\n1:11:34.360 --> 1:11:36.720\n people in the environment.\n\n1:11:36.720 --> 1:11:41.160\n These are all things that we perceive very quickly.\n\n1:11:41.160 --> 1:11:46.920\n And it's something actually quite close to being conscious,\n\n1:11:46.920 --> 1:11:47.640\n not quite.\n\n1:11:47.640 --> 1:11:50.360\n But there is this interesting experiment\n\n1:11:50.360 --> 1:11:54.800\n that was run at Google X, which is in a sense\n\n1:11:54.800 --> 1:11:58.840\n is just a virtual reality experiment,\n\n1:11:58.840 --> 1:12:02.760\n but in which they had a subject sitting, say,\n\n1:12:02.760 --> 1:12:11.800\n in a chair with goggles, like Oculus and so on, earphones.\n\n1:12:11.800 --> 1:12:15.040\n And they were seeing through the eyes of a robot\n\n1:12:15.040 --> 1:12:19.920\n nearby to cameras, microphones for receiving.\n\n1:12:19.920 --> 1:12:23.840\n So their sensory system was there.\n\n1:12:23.840 --> 1:12:28.120\n And the impression of all the subject, very strong,\n\n1:12:28.120 --> 1:12:31.520\n they could not shake it off, was that they\n\n1:12:31.520 --> 1:12:35.240\n were where the robot was.\n\n1:12:35.240 --> 1:12:38.640\n They could look at themselves from the robot\n\n1:12:38.640 --> 1:12:42.880\n and still feel they were where the robot is.\n\n1:12:42.880 --> 1:12:46.000\n They were looking at their body.\n\n1:12:46.000 --> 1:12:48.480\n Theirself had moved.\n\n1:12:48.480 --> 1:12:50.440\n So some aspect of scene understanding\n\n1:12:50.440 --> 1:12:54.880\n has to have ability to place yourself,\n\n1:12:54.880 --> 1:12:57.680\n have a self awareness about your position in the world\n\n1:12:57.680 --> 1:12:59.600\n and what the world is.\n\n1:12:59.600 --> 1:13:04.080\n So we may have to solve the hard problem of consciousness\n\n1:13:04.080 --> 1:13:04.840\n to solve it.\n\n1:13:04.840 --> 1:13:05.920\n On their way, yes.\n\n1:13:05.920 --> 1:13:07.760\n It's quite a moonshine.\n\n1:13:07.760 --> 1:13:12.440\n So you've been an advisor to some incredible minds,\n\n1:13:12.440 --> 1:13:15.680\n including Demis Hassabis, Krzysztof Koch, Amna Shashua,\n\n1:13:15.680 --> 1:13:17.360\n like you said.\n\n1:13:17.360 --> 1:13:20.120\n All went on to become seminal figures\n\n1:13:20.120 --> 1:13:22.000\n in their respective fields.\n\n1:13:22.000 --> 1:13:24.240\n From your own success as a researcher\n\n1:13:24.240 --> 1:13:29.320\n and from perspective as a mentor of these researchers,\n\n1:13:29.320 --> 1:13:34.160\n having guided them in the way of advice,\n\n1:13:34.160 --> 1:13:36.360\n what does it take to be successful in science\n\n1:13:36.360 --> 1:13:39.800\n and engineering careers?\n\n1:13:39.800 --> 1:13:43.280\n Whether you're talking to somebody in their teens,\n\n1:13:43.280 --> 1:13:48.160\n 20s, and 30s, what does that path look like?\n\n1:13:48.160 --> 1:13:53.200\n It's curiosity and having fun.\n\n1:13:53.200 --> 1:13:57.400\n And I think it's important also having\n\n1:13:57.400 --> 1:14:01.480\n fun with other curious minds.\n\n1:14:02.440 --> 1:14:04.520\n It's the people you surround with too,\n\n1:14:04.520 --> 1:14:06.640\n so fun and curiosity.\n\n1:14:06.640 --> 1:14:09.960\n Is there, you mentioned Steve Jobs,\n\n1:14:09.960 --> 1:14:13.160\n is there also an underlying ambition\n\n1:14:13.160 --> 1:14:14.720\n that's unique that you saw?\n\n1:14:14.720 --> 1:14:16.440\n Or does it really does boil down\n\n1:14:16.440 --> 1:14:18.800\n to insatiable curiosity and fun?\n\n1:14:18.800 --> 1:14:22.240\n Well of course, it's being curious\n\n1:14:22.240 --> 1:14:26.080\n in an active and ambitious way, yes.\n\n1:14:26.080 --> 1:14:29.640\n Definitely.\n\n1:14:29.640 --> 1:14:33.840\n But I think sometime in science,\n\n1:14:33.840 --> 1:14:37.000\n there are friends of mine who are like this.\n\n1:14:39.000 --> 1:14:40.680\n There are some of the scientists\n\n1:14:40.680 --> 1:14:42.880\n like to work by themselves\n\n1:14:44.080 --> 1:14:49.080\n and kind of communicate only when they complete their work\n\n1:14:50.920 --> 1:14:52.840\n or discover something.\n\n1:14:52.840 --> 1:14:57.840\n I think I always found the actual process\n\n1:14:58.720 --> 1:15:03.720\n of discovering something is more fun\n\n1:15:03.720 --> 1:15:07.280\n if it's together with other intelligent\n\n1:15:07.280 --> 1:15:09.240\n and curious and fun people.\n\n1:15:09.240 --> 1:15:11.320\n So if you see the fun in that process,\n\n1:15:11.320 --> 1:15:13.200\n the side effect of that process\n\n1:15:13.200 --> 1:15:14.360\n will be that you'll actually end up\n\n1:15:14.360 --> 1:15:16.320\n discovering some interesting things.\n\n1:15:16.320 --> 1:15:23.320\n So as you've led many incredible efforts here,\n\n1:15:23.320 --> 1:15:25.520\n what's the secret to being a good advisor,\n\n1:15:25.520 --> 1:15:28.360\n mentor, leader in a research setting?\n\n1:15:28.360 --> 1:15:30.240\n Is it a similar spirit?\n\n1:15:30.240 --> 1:15:32.600\n Or yeah, what advice could you give\n\n1:15:32.600 --> 1:15:35.960\n to people, young faculty and so on?\n\n1:15:35.960 --> 1:15:38.320\n It's partly repeating what I said\n\n1:15:38.320 --> 1:15:41.280\n about an environment that should be friendly\n\n1:15:41.280 --> 1:15:44.440\n and fun and ambitious.\n\n1:15:44.440 --> 1:15:49.280\n And I think I learned a lot\n\n1:15:49.280 --> 1:15:52.880\n from some of my advisors and friends\n\n1:15:52.880 --> 1:15:55.280\n and some who are physicists.\n\n1:15:55.280 --> 1:15:57.480\n And there was, for instance,\n\n1:15:57.480 --> 1:16:02.480\n this behavior that was encouraged\n\n1:16:02.800 --> 1:16:06.720\n of when somebody comes with a new idea in the group,\n\n1:16:06.720 --> 1:16:09.080\n you are, unless it's really stupid,\n\n1:16:09.080 --> 1:16:10.920\n but you are always enthusiastic.\n\n1:16:11.880 --> 1:16:14.280\n And then, and you're enthusiastic for a few minutes,\n\n1:16:14.280 --> 1:16:15.120\n for a few hours.\n\n1:16:15.120 --> 1:16:20.120\n Then you start asking critically a few questions,\n\n1:16:21.400 --> 1:16:23.040\n testing this.\n\n1:16:23.040 --> 1:16:26.280\n But this is a process that is,\n\n1:16:26.280 --> 1:16:28.240\n I think it's very good.\n\n1:16:29.360 --> 1:16:30.480\n You have to be enthusiastic.\n\n1:16:30.480 --> 1:16:33.680\n Sometimes people are very critical from the beginning.\n\n1:16:33.680 --> 1:16:36.280\n That's not...\n\n1:16:36.280 --> 1:16:37.600\n Yes, you have to give it a chance\n\n1:16:37.600 --> 1:16:39.400\n for that seed to grow.\n\n1:16:39.400 --> 1:16:41.600\n That said, with some of your ideas,\n\n1:16:41.600 --> 1:16:42.800\n which are quite revolutionary,\n\n1:16:42.800 --> 1:16:45.840\n so there's a witness, especially in the human vision side\n\n1:16:45.840 --> 1:16:47.320\n and neuroscience side,\n\n1:16:47.320 --> 1:16:50.000\n there could be some pretty heated arguments.\n\n1:16:50.000 --> 1:16:51.160\n Do you enjoy these?\n\n1:16:51.160 --> 1:16:54.520\n Is that a part of science and academic pursuits\n\n1:16:54.520 --> 1:16:55.360\n that you enjoy?\n\n1:16:55.360 --> 1:16:56.200\n Yeah.\n\n1:16:56.200 --> 1:17:01.040\n Is that something that happens in your group as well?\n\n1:17:01.040 --> 1:17:02.440\n Yeah, absolutely.\n\n1:17:02.440 --> 1:17:04.360\n I also spent some time in Germany.\n\n1:17:04.360 --> 1:17:05.880\n Again, there is this tradition\n\n1:17:05.880 --> 1:17:10.880\n in which people are more forthright,\n\n1:17:10.880 --> 1:17:14.160\n less kind than here.\n\n1:17:14.160 --> 1:17:19.160\n So in the U.S., when you write a bad letter,\n\n1:17:20.120 --> 1:17:23.080\n you still say, this guy's nice.\n\n1:17:23.080 --> 1:17:24.120\n Yes, yes.\n\n1:17:25.600 --> 1:17:26.440\n So...\n\n1:17:26.440 --> 1:17:28.840\n Yeah, here in America, it's degrees of nice.\n\n1:17:28.840 --> 1:17:29.680\n Yes.\n\n1:17:29.680 --> 1:17:31.040\n It's all just degrees of nice, yeah.\n\n1:17:31.040 --> 1:17:31.880\n Right, right.\n\n1:17:31.880 --> 1:17:34.960\n So as long as this does not become personal,\n\n1:17:36.400 --> 1:17:40.680\n and it's really like a football game\n\n1:17:40.680 --> 1:17:43.520\n with these rules, that's great.\n\n1:17:43.520 --> 1:17:45.200\n That's fun.\n\n1:17:46.600 --> 1:17:49.280\n So if you somehow found yourself in a position\n\n1:17:49.280 --> 1:17:51.840\n to ask one question of an oracle,\n\n1:17:51.840 --> 1:17:54.240\n like a genie, maybe a god,\n\n1:17:55.520 --> 1:17:57.720\n and you're guaranteed to get a clear answer,\n\n1:17:58.760 --> 1:18:01.320\n what kind of question would you ask?\n\n1:18:01.320 --> 1:18:03.640\n What would be the question you would ask?\n\n1:18:04.520 --> 1:18:06.040\n In the spirit of our discussion,\n\n1:18:06.040 --> 1:18:10.080\n it could be, how could I become 10 times more intelligent?\n\n1:18:10.080 --> 1:18:15.080\n And so, but see, you only get a clear short answer.\n\n1:18:16.240 --> 1:18:18.720\n So do you think there's a clear short answer to that?\n\n1:18:18.720 --> 1:18:19.560\n No.\n\n1:18:20.720 --> 1:18:22.760\n And that's the answer you'll get.\n\n1:18:22.760 --> 1:18:26.920\n Okay, so you've mentioned Flowers of Algernon.\n\n1:18:26.920 --> 1:18:27.960\n Oh, yeah.\n\n1:18:27.960 --> 1:18:31.800\n As a story that inspires you in your childhood,\n\n1:18:32.800 --> 1:18:37.200\n as this story of a mouse,\n\n1:18:37.200 --> 1:18:39.360\n human achieving genius level intelligence,\n\n1:18:39.360 --> 1:18:41.520\n and then understanding what was happening\n\n1:18:41.520 --> 1:18:44.200\n while slowly becoming not intelligent again,\n\n1:18:44.200 --> 1:18:46.600\n and this tragedy of gaining intelligence\n\n1:18:46.600 --> 1:18:48.600\n and losing intelligence,\n\n1:18:48.600 --> 1:18:51.440\n do you think in that spirit, in that story,\n\n1:18:51.440 --> 1:18:55.360\n do you think intelligence is a gift or a curse\n\n1:18:55.360 --> 1:19:00.160\n from the perspective of happiness and meaning of life?\n\n1:19:00.160 --> 1:19:02.200\n You try to create an intelligent system\n\n1:19:02.200 --> 1:19:03.880\n that understands the universe,\n\n1:19:03.880 --> 1:19:06.480\n but on an individual level, the meaning of life,\n\n1:19:06.480 --> 1:19:10.840\n do you think intelligence is a gift?\n\n1:19:10.840 --> 1:19:12.040\n It's a good question.\n\n1:19:17.120 --> 1:19:17.960\n I don't know.\n\n1:19:22.840 --> 1:19:26.520\n As one of the, as one people consider\n\n1:19:26.520 --> 1:19:29.280\n the smartest people in the world,\n\n1:19:29.280 --> 1:19:33.320\n in some dimension, at the very least, what do you think?\n\n1:19:33.320 --> 1:19:37.560\n I don't know, it may be invariant to intelligence,\n\n1:19:37.560 --> 1:19:39.640\n that degree of happiness.\n\n1:19:39.640 --> 1:19:41.200\n It would be nice if it were.\n\n1:19:43.680 --> 1:19:44.680\n That's the hope.\n\n1:19:44.680 --> 1:19:46.120\n Yeah.\n\n1:19:46.120 --> 1:19:50.160\n You could be smart and happy and clueless and happy.\n\n1:19:50.160 --> 1:19:51.800\n Yeah.\n\n1:19:51.800 --> 1:19:54.480\n As always, on the discussion of the meaning of life,\n\n1:19:54.480 --> 1:19:57.320\n it's probably a good place to end.\n\n1:19:57.320 --> 1:19:59.240\n Tommaso, thank you so much for talking today.\n\n1:19:59.240 --> 1:20:04.240\n Thank you, this was great.\n\n"
}