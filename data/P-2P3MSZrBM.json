{
  "title": "Joscha Bach: Artificial Consciousness and the Nature of Reality | Lex Fridman Podcast #101",
  "id": "P-2P3MSZrBM",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:05.520\n The following is a conversation with Yosha Bach, VP of Research at the AI Foundation,\n\n00:05.520 --> 00:12.160\n with a history of research positions at MIT and Harvard. Yosha is one of the most unique\n\n00:12.160 --> 00:16.720\n and brilliant people in the artificial intelligence community, exploring the workings\n\n00:16.720 --> 00:23.360\n of the human mind, intelligence, consciousness, life on Earth, and the possibly simulated\n\n00:23.360 --> 00:28.640\n fabric of our universe. I could see myself talking to Yosha many times in the future.\n\n00:28.640 --> 00:35.600\n Quick summary of the ads. Two sponsors, ExpressVPN and Cash App. Please consider supporting the\n\n00:35.600 --> 00:42.880\n podcast by signing up at expressvpn.com slash LexPod and downloading Cash App and using code\n\n00:42.880 --> 00:50.240\n LEXPodcast. This is the Artificial Intelligence Podcast. If you enjoy it, subscribe on YouTube,\n\n00:50.240 --> 00:54.720\n review it with five stars on Apple Podcast, support it on Patreon, or simply connect with\n\n00:54.720 --> 01:01.920\n me on Twitter at LexFriedman. Since this comes up more often than I ever would have imagined,\n\n01:02.480 --> 01:07.520\n I challenge you to try to figure out how to spell my last name without using the letter E.\n\n01:08.480 --> 01:14.240\n And it'll probably be the correct way. As usual, I'll do a few minutes of ads now and never hear\n\n01:14.240 --> 01:19.120\n any yas in the middle that can break the flow of the conversation. This show is sponsored by\n\n01:19.120 --> 01:27.120\n ExpressVPN. Get it at expressvpn.com slash LexPod to support this podcast and to get an extra three\n\n01:27.120 --> 01:33.600\n months free on a one year package. I've been using ExpressVPN for many years. I love it.\n\n01:34.160 --> 01:40.240\n I think ExpressVPN is the best VPN out there. They told me to say it, but I think it actually\n\n01:40.240 --> 01:46.880\n happens to be true. It doesn't log your data, it's crazy fast, and it's easy to use. Literally,\n\n01:46.880 --> 01:52.640\n just one big power on button. Again, for obvious reasons, it's really important that they don't\n\n01:52.640 --> 01:59.040\n log your data. It works on Linux and everywhere else too. Shout out to my favorite flavor of Linux,\n\n01:59.040 --> 02:08.640\n Bantu Mate 2004. Once again, get it at expressvpn.com slash LexPod to support this podcast and to get\n\n02:08.640 --> 02:17.040\n an extra three months free on a one year package. This show is presented by Cash App, the number one\n\n02:17.040 --> 02:23.600\n finance app in the App Store. When you get it, use code LexPodcast. Cash App lets you send money to\n\n02:23.600 --> 02:29.360\n friends, buy Bitcoin, and invest in the stock market with as little as one dollar. Since Cash\n\n02:29.360 --> 02:34.320\n App does fractional share trading, let me mention that the order execution algorithm that works\n\n02:34.320 --> 02:40.160\n behind the scenes to create the abstraction of the fractional orders is an algorithmic marvel.\n\n02:40.160 --> 02:45.120\n So big props to the Cash App engineers for taking a step up to the next layer of abstraction over\n\n02:45.120 --> 02:51.120\n the stock market, making trading more accessible for new investors and diversification much easier.\n\n02:51.680 --> 02:57.200\n So again, if you get Cash App from the App Store or Google Play and use the code LexPodcast,\n\n02:57.760 --> 03:03.840\n you get $10, and Cash App will also donate $10 to First, an organization that is helping\n\n03:03.840 --> 03:07.920\n advanced robotics and STEM education for young people around the world.\n\n03:09.200 --> 03:16.880\n And now here's my conversation with Joscha Bach. As you've said, you grew up in a forest in East\n\n03:16.880 --> 03:23.840\n Germany, just as we're talking about off mic, to parents who are artists. And now I think,\n\n03:23.840 --> 03:27.280\n at least to me, you've become one of the most unique thinkers in the AI world.\n\n03:27.920 --> 03:30.480\n So can we try to reverse engineer your mind a little bit?\n\n03:30.480 --> 03:38.400\n What were the key philosopher, scientist ideas, maybe even movies or just realizations that\n\n03:38.400 --> 03:43.680\n had an impact on you when you were growing up that kind of led to the trajectory,\n\n03:43.680 --> 03:47.920\n or were the key sort of crossroads in the trajectory of your intellectual development?\n\n03:49.600 --> 03:56.560\n My father came from a long tradition of architects, a distant branch of the Bach family.\n\n03:56.560 --> 04:03.680\n And so basically, he was technically a nerd. And nerds need to interface in society with\n\n04:03.680 --> 04:09.360\n nonstandard ways. Sometimes I define a nerd as somebody who thinks that the purpose of\n\n04:09.360 --> 04:15.920\n communication is to submit your ideas to peer review. And normal people understand that the\n\n04:15.920 --> 04:21.600\n primary purpose of communication is to negotiate alignment. And these purposes tend to conflict,\n\n04:21.600 --> 04:26.400\n which means that nerds have to learn how to interact with society at large.\n\n04:26.400 --> 04:31.360\n Who is the reviewer in the nerd's view of communication?\n\n04:31.360 --> 04:36.960\n Everybody who you consider to be a peer. So whatever hapless individual is around,\n\n04:36.960 --> 04:40.800\n well, you would try to make him or her the gift of information.\n\n04:42.400 --> 04:50.560\n Okay. So you're now, by the way, my research malinformed me. So you're architect or artist?\n\n04:50.560 --> 04:58.160\n So he did study architecture. But basically, my grandfather made the wrong decision. He married\n\n04:58.160 --> 05:07.120\n an aristocrat and was drawn into the war. And he came back after 15 years. So basically, my father\n\n05:07.680 --> 05:14.800\n was not parented by a nerd, but by somebody who tried to tell him what to do, and expected him\n\n05:14.800 --> 05:21.120\n to do what he was told. And he was unable to. He's unable to do things if he's not intrinsically\n\n05:21.120 --> 05:27.600\n motivated. So in some sense, my grandmother broke her son. And her son responded when he became an\n\n05:27.600 --> 05:33.200\n architect to become an artist. So he built 100 Wasser architecture. He built houses without\n\n05:33.200 --> 05:38.400\n right angles. He built lots of things that didn't work in the more brutalist traditions of eastern\n\n05:38.400 --> 05:44.640\n Germany. And so he bought an old watermill, moved out to the countryside, and did only what he wanted\n\n05:44.640 --> 05:50.640\n to do, which was art. Eastern Germany was perfect for Boheme, because you had complete material\n\n05:50.640 --> 05:55.520\n safety. Food was heavily subsidized, healthcare was free. You didn't have to worry about rent or\n\n05:55.520 --> 06:00.000\n pensions or anything. So it's a socialized communist side. Yes. And the other thing is,\n\n06:00.000 --> 06:04.080\n it was almost impossible not to be in political disagreement with your government, which is very\n\n06:04.080 --> 06:08.800\n productive for artists. So everything that you do is intrinsically meaningful, because it will\n\n06:08.800 --> 06:14.160\n always touch on the deeper currents of society of culture and be in conflict with it and tension\n\n06:14.160 --> 06:19.040\n with it. And you will always have to define yourself with respect to this. So what impacted\n\n06:19.040 --> 06:26.640\n your father, this outside of the box thinker against the government, against the world artists?\n\n06:26.640 --> 06:31.360\n He was actually not a thinker. He was somebody who only got self aware to the degree that he\n\n06:31.360 --> 06:39.200\n needed to make himself functional. So in some sense, he was also in the late 1960s. And he was\n\n06:39.200 --> 06:44.720\n in some sense a hippie. So he became a one person cult. He lived out there in his kingdom. He built\n\n06:44.720 --> 06:53.600\n big sculpture gardens and started many avenues of art and so on and convinced a woman to live with\n\n06:53.600 --> 06:58.480\n him. She was also an architect and she adored him and decided to share her life with him.\n\n06:58.480 --> 07:05.280\n And I basically grew up in a big cave full of books. I'm almost feral. And I was bored out\n\n07:05.280 --> 07:11.120\n there. It was very, very beautiful, very quiet, and quite lonely. So I started to read. And by\n\n07:11.120 --> 07:16.240\n the time I came to school, I've read everything until fourth grade and then some. And there was\n\n07:16.240 --> 07:21.760\n not a real way for me to relate to the outside world. And I couldn't quite put my finger on why.\n\n07:21.760 --> 07:26.800\n And today I know it was because I was a nerd, obviously, and it was the only nerd around. So\n\n07:26.800 --> 07:32.800\n there was no other kids like me. And there was nobody interested in physics or computing or\n\n07:32.800 --> 07:38.320\n mathematics and so on. And this village school that I went to was basically a nice school.\n\n07:38.960 --> 07:42.800\n Kids were nice to me. I was not beaten up, but I also didn't make many friends or\n\n07:42.800 --> 07:47.120\n build deep relationships. They only happened in starting from ninth grade when I went into a\n\n07:47.120 --> 07:51.200\n school for mathematics and physics. Do you remember any key books from this moment?\n\n07:51.200 --> 07:56.880\n I basically read everything. So I went to the library and I worked my way through the\n\n07:56.880 --> 08:01.200\n children's and young adult sections. And then I read a lot of science fiction,\n\n08:01.200 --> 08:06.800\n for instance, Stanislav Lem, basically the great author of Cybernetics, has influenced me. Back\n\n08:06.800 --> 08:10.640\n then, I didn't see him as a big influence because everything that he wrote seemed to be so natural\n\n08:10.640 --> 08:16.560\n to me. And it's only later that I contrasted it with what other people wrote. Another thing that\n\n08:16.560 --> 08:22.000\n was very influential on me were the classical philosophers and also the literature of romanticism.\n\n08:22.000 --> 08:29.680\n So German poetry and art, Troste Hilshoff and Heine and up to Hesse and so on.\n\n08:29.680 --> 08:35.840\n Hesse. I love Hesse. So at which point do the classical philosophers end? At this point,\n\n08:35.840 --> 08:40.560\n we're in the 21st century. What's the latest classical philosopher? Does this stretch through\n\n08:41.680 --> 08:45.840\n even as far as Nietzsche or is this, are we talking about Plato and Aristotle?\n\n08:45.840 --> 08:49.120\n I think that Nietzsche is the classical equivalent of a shit poster.\n\n08:52.560 --> 08:57.920\n He's very smart and easy to read, but he's not so much trolling others. He's trolling himself\n\n08:57.920 --> 09:02.480\n because he was at odds with the world. Largely his romantic relationships didn't work out.\n\n09:02.480 --> 09:04.640\n He got angry and he basically became a nihilist.\n\n09:06.240 --> 09:11.760\n Isn't that a beautiful way to be as an intellectual is to constantly be trolling yourself,\n\n09:11.760 --> 09:14.960\n to be in that conflict, in that tension?\n\n09:14.960 --> 09:18.720\n I think it's a lack of self awareness. At some point, you have to understand the\n\n09:18.720 --> 09:23.840\n comedy of your own situation. If you take yourself seriously and you are not functional,\n\n09:23.840 --> 09:25.840\n it ends in tragedy as it did for Nietzsche.\n\n09:25.840 --> 09:29.760\n I think you think he took himself too seriously in that tension.\n\n09:29.760 --> 09:34.160\n And as you find the same thing in Hesse and so on, this Steppenwolf syndrome is classic\n\n09:34.160 --> 09:38.880\n adolescence where you basically feel misunderstood by the world and you don't understand that all the\n\n09:38.880 --> 09:44.160\n misunderstandings are the result of your own lack of self awareness because you think that you are\n\n09:44.160 --> 09:48.960\n a prototypical human and the others around you should behave the same way as you expect them\n\n09:48.960 --> 09:53.600\n based on your innate instincts and it doesn't work out and you become a transcendentalist\n\n09:53.600 --> 09:58.480\n to deal with that. So it's very, very understandable and have great sympathies for this\n\n09:58.480 --> 10:02.160\n to the degree that I can have sympathy for my own intellectual history.\n\n10:02.160 --> 10:04.720\n But you have to grow out of it.\n\n10:04.720 --> 10:09.600\n So as an intellectual, a life well lived, a journey well traveled is one where you don't\n\n10:09.600 --> 10:11.120\n take yourself seriously from that perspective?\n\n10:11.120 --> 10:17.840\n No, I think that you are neither serious or not serious yourself because you need to become\n\n10:17.840 --> 10:24.720\n unimportant as a subject. That is, if you are a philosopher, belief is not a verb.\n\n10:24.720 --> 10:27.360\n You don't do this for the audience and you don't do it for yourself.\n\n10:27.360 --> 10:32.240\n You have to submit to the things that are possibly true and you have to follow wherever\n\n10:32.240 --> 10:36.400\n your inquiry leads. But it's not about you. It has nothing to do with you.\n\n10:36.400 --> 10:42.080\n So do you think then people like Ayn Rand believed sort of an idea of there's objective\n\n10:42.080 --> 10:48.000\n truth. So what's your sense in the philosophical, if you remove yourself as objective from the\n\n10:48.000 --> 10:52.560\n picture, you think it's possible to actually discover ideas that are true or are we just\n\n10:52.560 --> 10:57.840\n in a mesh of relative concepts that are either true nor false? It's just a giant mess.\n\n10:57.840 --> 11:02.720\n You cannot define objective truth without understanding the nature of truth in the first\n\n11:02.720 --> 11:08.000\n place. So what does the brain mean by saying that discover something as truth? So for instance,\n\n11:08.000 --> 11:14.560\n a model can be predictive or not predictive. Then there can be a sense in which a mathematical\n\n11:14.560 --> 11:19.520\n statement can be true because it's defined as true under certain conditions. So it's basically\n\n11:19.520 --> 11:26.320\n a particular state that a variable can have in a simple game. And then you can have a\n\n11:26.320 --> 11:31.120\n correspondence between systems and talk about truth, which is again, a type of model correspondence.\n\n11:31.120 --> 11:34.960\n And there also seems to be a particular kind of ground truth. So for instance,\n\n11:34.960 --> 11:41.200\n you're confronted with the enormity of something existing at all. It's stunning when you realize\n\n11:41.200 --> 11:47.360\n something exists rather than nothing. And this seems to be true. There's an absolute truth in\n\n11:47.360 --> 11:52.400\n the fact that something seems to be happening. Yeah, that to me is a showstopper. I could just\n\n11:52.400 --> 11:57.840\n think about that idea and be amazed by that idea for the rest of my life and not going any farther\n\n11:57.840 --> 12:01.040\n because I don't even know the answer to that. Why does anything exist at all?\n\n12:01.040 --> 12:04.960\n Well, the easiest answer is existence is the default, right? So this is the lowest number of\n\n12:04.960 --> 12:07.360\n bits that you would need to encode this. Whose answer?\n\n12:07.920 --> 12:11.280\n The simplest answer to this is that existence is the default.\n\n12:11.280 --> 12:13.200\n What about nonexistence? I mean, that seems...\n\n12:14.240 --> 12:18.080\n Nonexistence might not be a meaningful notion in this sense. So in some sense,\n\n12:18.080 --> 12:23.440\n if everything that can exist exists, for something to exist, it probably needs to be implementable.\n\n12:23.440 --> 12:28.240\n The only thing that can be implemented is finite automata. So maybe the whole of existence is the\n\n12:28.240 --> 12:33.120\n superposition of all finite automata and we are in some region of the fractal that has the properties\n\n12:33.120 --> 12:37.760\n that it can contain us. What does it mean to be a superposition of finite automata?\n\n12:40.000 --> 12:45.840\n Superposition of all possible rules? Imagine that every automaton is basically an operator\n\n12:45.840 --> 12:50.400\n that acts on some substrate and as a result, you get emergent patterns.\n\n12:50.400 --> 12:51.280\n What's the substrate?\n\n12:52.640 --> 12:55.040\n I have no idea to know. But some substrate.\n\n12:55.040 --> 12:58.400\n It's something that can store information.\n\n12:58.400 --> 13:00.320\n Something that can store information, there's a automaton.\n\n13:00.320 --> 13:01.680\n Something that can hold state.\n\n13:01.680 --> 13:05.360\n Still, it doesn't make sense to me the why that exists at all. I could just sit there\n\n13:06.480 --> 13:11.280\n with a beer or a vodka and just enjoy the fact, pondering the why.\n\n13:11.280 --> 13:16.960\n It may not have a why. This might be the wrong direction to ask into this. So there could be no\n\n13:16.960 --> 13:22.640\n relation in the why direction without asking for a purpose or for a cause. It doesn't mean\n\n13:22.640 --> 13:28.720\n that everything has to have a purpose or cause. So we mentioned some philosophers in that early,\n\n13:28.720 --> 13:34.800\n just taking a brief step back into that. So we asked ourselves when did classical philosophy end?\n\n13:34.800 --> 13:38.240\n I think for Germany, it largely ended with the first revolution.\n\n13:38.240 --> 13:45.680\n That's basically when we entered the monarchy and started a democracy. And at this point,\n\n13:45.680 --> 13:50.080\n we basically came up with a new form of government that didn't have a good sense of\n\n13:50.080 --> 13:56.080\n this new organism that society wanted to be. And in a way, it decapitated the universities.\n\n13:56.080 --> 13:59.760\n So the universities went on through modernism like a headless chicken.\n\n13:59.760 --> 14:03.600\n At the same time, democracy failed in Germany and we got fascism as a result.\n\n14:04.240 --> 14:08.640\n And it burned down things in a similar way as Stalinism burned down intellectual traditions\n\n14:08.640 --> 14:14.160\n in Russia. And Germany, both Germanys have not recovered from this. Eastern Germany had this\n\n14:14.160 --> 14:20.560\n vulgar dialectic materialism and Western Germany didn't get much more edgy than Habermas. So in\n\n14:20.560 --> 14:24.640\n some sense, both countries lost their intellectual traditions and killing off and driving out the\n\n14:24.640 --> 14:33.680\n Jews didn't help. Yeah. So that was the end of really rigorous what you would say is classical\n\n14:33.680 --> 14:39.440\n philosophy. There's also this thing that in some sense, the low hanging foods in philosophy\n\n14:39.440 --> 14:46.240\n were mostly wrapped. And the last big things that we discovered was the constructivist turn\n\n14:46.240 --> 14:51.120\n in mathematics. So to understand that the parts of mathematics that work are computation,\n\n14:51.680 --> 14:57.200\n there was a very significant discovery in the first half of the 20th century. And it hasn't\n\n14:57.200 --> 15:02.640\n fully permeated philosophy and even physics yet. Physicists checked out the code libraries\n\n15:02.640 --> 15:08.880\n for mathematics before constructivism became universal. What's constructivism? What are you\n\n15:08.880 --> 15:13.600\n referring to, G\u00f6del's incompleteness theorem, those kinds of ideas? So basically, G\u00f6del himself,\n\n15:13.600 --> 15:18.880\n I think, didn't get it yet. Hilbert could get it. Hilbert saw that, for instance, countries\n\n15:18.880 --> 15:25.120\n set theoretic experiments and mathematics led into contradictions. And he noticed that with the\n\n15:25.120 --> 15:29.440\n current semantics, we cannot build a computer in mathematics that runs mathematics without crashing.\n\n15:30.080 --> 15:35.040\n And G\u00f6del could prove this. And so what G\u00f6del could show is using classical mathematical\n\n15:35.040 --> 15:40.480\n semantics, you run into contradictions. And because G\u00f6del strongly believed in these semantics and\n\n15:40.480 --> 15:46.640\n more than what he could observe and so on, he was shocked. It basically shook his world to the core\n\n15:46.640 --> 15:50.240\n because in some sense, he felt that the world has to be implemented in classical mathematics.\n\n15:50.960 --> 15:56.400\n And for Turing, it wasn't quite so bad. I think that Turing could see that the solution is to\n\n15:56.400 --> 16:01.920\n understand that mathematics was computation all along, which means you, for instance, pi\n\n16:01.920 --> 16:08.080\n in classical mathematics is a value. It's also a function, but it's the same thing. And in\n\n16:08.080 --> 16:13.360\n computation, a function is only a value when you can compute it. And if you cannot compute the last\n\n16:13.360 --> 16:18.240\n digit of pi, you only have a function. You can plug this function into your local sun, let it run\n\n16:18.240 --> 16:22.880\n until the sun burns out. This is it. This is the last digit of pi you will know. But it also means\n\n16:22.880 --> 16:28.480\n there can be no process in the physical universe or in any physically realized computer that depends\n\n16:28.480 --> 16:34.160\n on having known the last digit of pi. Which means there are parts of physics that are defined in\n\n16:34.160 --> 16:38.160\n such a way that cannot strictly be true, because assuming that this could be true leads into\n\n16:38.160 --> 16:44.240\n contradictions. So I think putting computation at the center of the world view is actually the\n\n16:44.240 --> 16:49.840\n right way to think about it. Yes. And Wittgenstein could see it. And Wittgenstein basically preempted\n\n16:49.840 --> 16:55.360\n the logitist program of AI that Minsky started later, like 30 years later. Turing was actually\n\n16:55.360 --> 17:00.480\n a pupil of Wittgenstein. I didn't know there's any connection between Turing and Wittgenstein.\n\n17:00.480 --> 17:03.680\n Wittgenstein even cancelled some classes when Turing was not present because he thought it\n\n17:03.680 --> 17:09.680\n was not worth spending the time with the others. If you read the Tractatus, it's a very beautiful\n\n17:09.680 --> 17:15.040\n book, like basically one thought on 75 pages. It's very non typical for philosophy because it doesn't\n\n17:15.040 --> 17:21.600\n have arguments in it and it doesn't have references in it. It's just one thought that is not intending\n\n17:21.600 --> 17:26.320\n to convince anybody. He says, it's mostly for people that had the same insight as me,\n\n17:26.320 --> 17:32.080\n just spell it out. And this insight is there is a way in which mathematics and philosophy\n\n17:32.080 --> 17:37.040\n ought to meet. Mathematics tries to understand the domain of all languages by starting with those\n\n17:37.040 --> 17:42.000\n that are so formalizable that you can prove all the properties of the statements that you make.\n\n17:42.560 --> 17:46.960\n But the price that you pay is that your language is very, very simple. So it's very hard to say\n\n17:46.960 --> 17:52.320\n something meaningful in mathematics. And it looks complicated to people, but it's far less complicated\n\n17:52.320 --> 17:58.320\n than what our brain is casually doing all the time when it makes sense of reality. And philosophy is\n\n17:58.320 --> 18:03.600\n coming from the top. So it's mostly starting from natural languages with vaguely defined concepts.\n\n18:03.600 --> 18:08.720\n And the hope is that mathematics and philosophy can meet at some point. And Wittgenstein was trying\n\n18:08.720 --> 18:12.320\n to make them meet. And he already understood that, for instance, you could express everything with\n\n18:12.320 --> 18:17.440\n the NAND calculus, that you could reduce the entire logic to NAND gates as we do in our modern\n\n18:17.440 --> 18:22.320\n computers. So in some sense, he already understood Turing universality before Turing spelled it out.\n\n18:22.320 --> 18:26.640\n I think when he wrote the Tractatus, he didn't understand yet that the idea was so important\n\n18:26.640 --> 18:32.400\n and significant. And I suspect then when Turing wrote it out, nobody cared that much. Turing was\n\n18:32.400 --> 18:39.280\n not that famous when he lived. It was mostly his work in decrypting the German codes that made him\n\n18:39.280 --> 18:44.400\n famous or gave him some notoriety. But this saint status that he has to computer science right now\n\n18:44.400 --> 18:48.720\n and the AI is something that I think he could acquire later. That's kind of interesting. Do\n\n18:48.720 --> 18:53.040\n you think of computation and computer science? And you kind of represent that to me is maybe\n\n18:53.040 --> 19:00.080\n that's the modern day. You in a sense are the new philosopher by sort of the computer scientist\n\n19:00.720 --> 19:06.000\n who dares to ask the bigger questions that philosophy originally started is the new\n\n19:06.000 --> 19:12.240\n philosopher. Certainly not me. I think I'm mostly still this child that grows up in a\n\n19:12.240 --> 19:16.160\n very beautiful valley and looks at the world from the outside and tries to understand what's going\n\n19:16.160 --> 19:20.560\n on. And my teachers tell me things and they largely don't make sense. So I have to make my\n\n19:20.560 --> 19:25.040\n own models. I have to discover the foundations of what the others are saying. I have to try to fix\n\n19:25.040 --> 19:30.160\n them to be charitable. I try to understand what they must have thought originally or what their\n\n19:30.160 --> 19:34.240\n teachers or their teacher's teachers must have thought until everything got lost in translation\n\n19:34.240 --> 19:38.880\n and how to make sense of the reality that we are in. And whenever I have an original idea,\n\n19:38.880 --> 19:43.360\n I'm usually late to the party by say 400 years. And the only thing that's good is that the parties\n\n19:43.360 --> 19:49.120\n get smaller and smaller the older I get and the more I explore it. The parties get smaller and\n\n19:49.120 --> 19:55.680\n more exclusive and more exclusive. So it seems like one of the key qualities of your upbringing\n\n19:55.680 --> 20:01.040\n was that you were not tethered, whether it's because of your parents or in general,\n\n20:01.040 --> 20:07.840\n maybe something within your mind, some genetic material, you were not tethered to the ideas\n\n20:07.840 --> 20:14.720\n of the general populace, which is actually a unique property. We're kind of the education\n\n20:14.720 --> 20:20.160\n system and whatever, not education system, just existing in this world forces certain sets of\n\n20:20.160 --> 20:28.480\n ideas onto you. Can you disentangle that? Why are you not so tethered? Even in your work today,\n\n20:28.480 --> 20:38.400\n you seem to not care about perhaps a best paper in Europe, right? Being tethered to particular\n\n20:38.400 --> 20:44.960\n things that current today in this year, people seem to value as a thing you put on your CV and\n\n20:44.960 --> 20:50.160\n resume. You're a little bit more outside of that world, outside of the world of ideas that people\n\n20:50.160 --> 20:56.080\n are especially focused in the benchmarks of today, the things. Can you disentangle that?\n\n20:56.080 --> 20:59.440\n Because I think that's inspiring. And if there were more people like that,\n\n20:59.440 --> 21:05.760\n we might be able to solve some of the bigger problems that AI dreams to solve.\n\n21:05.760 --> 21:10.640\n And there's a big danger in this because in a way you are expected to marry into an\n\n21:10.640 --> 21:16.320\n intellectual tradition and visit this tradition into a particular school. If everybody comes up\n\n21:16.320 --> 21:22.000\n with their own paradigms, the whole thing is not cumulative as an enterprise. So in some sense,\n\n21:22.000 --> 21:26.320\n you need a healthy balance. You need paradigmatic thinkers and you need people that work within\n\n21:26.320 --> 21:32.080\n given paradigms. Basically, scientists today define themselves largely by methods. And it's almost a\n\n21:32.080 --> 21:38.080\n disease that we think as a scientist, as somebody who was convinced by their guidance counselor,\n\n21:38.080 --> 21:42.240\n that they should join a particular discipline and then they find a good mentor to learn the\n\n21:42.240 --> 21:47.120\n right methods. And then they are lucky enough and privileged enough to join the right team. And then\n\n21:47.120 --> 21:52.800\n their name will show up on influential papers. But we also see that there are diminishing returns\n\n21:52.800 --> 21:59.280\n with this approach. And when our field, computer science and AI started, most of the people that\n\n21:59.280 --> 22:05.440\n joined this field had interesting opinions. And today's thinkers in AI either don't have interesting\n\n22:05.440 --> 22:08.960\n opinions at all, or these opinions are inconsequential for what they're actually\n\n22:08.960 --> 22:13.520\n doing. Because what they're doing is they apply the state of the art methods with a small epsilon.\n\n22:13.520 --> 22:21.360\n And this is often a good idea if you think that this is the best way to make progress. And for me,\n\n22:21.360 --> 22:27.360\n it's first of all, very boring. If somebody else can do it, why should I do it? If the current\n\n22:27.360 --> 22:32.880\n methods of machine learning lead to strong AI, why should I be doing it? I will just wait until\n\n22:32.880 --> 22:38.240\n they're done and wait until they do this on the beach or read interesting books or write some\n\n22:38.240 --> 22:44.160\n and have fun. But if you don't think that we are currently doing the right thing, if we are missing\n\n22:45.040 --> 22:51.760\n some perspectives, then it's required to think outside of the box. It's also required to understand\n\n22:51.760 --> 22:59.200\n the boxes. But it's necessary to understand what worked and what didn't work and for what reasons.\n\n22:59.200 --> 23:03.920\n So you have to be willing to ask new questions and design new methods whenever you want to\n\n23:03.920 --> 23:09.120\n answer them. And you have to be willing to dismiss the existing methods if you think that they're\n\n23:09.120 --> 23:16.960\n not going to yield the right answers. It's very bad career advice to do that. So maybe to briefly\n\n23:16.960 --> 23:23.440\n stay for one more time in the early days, when would you say for you was the dream\n\n23:24.480 --> 23:30.880\n before we dive into the discussions that we just almost started, when was the dream to understand\n\n23:30.880 --> 23:33.920\n or maybe to create human level intelligence born for you?\n\n23:35.440 --> 23:44.000\n I think that you can see AI largely today as advanced information processing. If you would\n\n23:44.000 --> 23:48.640\n change the acronym of AI into that, most people in the field would be happy. It would not change\n\n23:48.640 --> 23:54.560\n anything what they're doing. We're automating statistics and many of the statistical models\n\n23:54.560 --> 23:59.360\n are more advanced than what statisticians had in the past. And it's pretty good work. It's very\n\n23:59.360 --> 24:04.480\n productive. And the other aspect of AI is philosophical project. And this philosophical\n\n24:04.480 --> 24:09.840\n project is very risky and very few people work on it and it's not clear if it succeeds.\n\n24:10.400 --> 24:16.000\n So first of all, you keep throwing sort of a lot of really interesting ideas and I have to\n\n24:16.000 --> 24:22.480\n pick which ones we go with. But first of all, you use the term information processing,\n\n24:22.480 --> 24:31.440\n just information processing as if it's the mere, it's the muck of existence as if it's the epitome\n\n24:31.440 --> 24:36.720\n of existence, that the entirety of the universe might be information processing, that consciousness\n\n24:36.720 --> 24:42.240\n and intelligence might be information processing. So that maybe you can comment on if the advanced\n\n24:42.240 --> 24:48.800\n information processing is a limiting kind of a round of ideas. And then the other one is,\n\n24:48.800 --> 24:54.320\n what do you mean by the philosophical project? So I suspect that general intelligence is the\n\n24:54.880 --> 25:00.800\n result of trying to solve general problems. So intelligence, I think, is the ability to model.\n\n25:00.800 --> 25:06.080\n It's not necessarily goal directed rationality or something. Many intelligent people are bad at this,\n\n25:06.640 --> 25:12.560\n but it's the ability to be presented with a number of patterns and see a structure in those patterns\n\n25:12.560 --> 25:17.360\n and be able to predict the next set of patterns, to make sense of things. And\n\n25:17.360 --> 25:22.240\n some problems are very general. Usually intelligence serves control, so you make these\n\n25:22.240 --> 25:26.720\n models for a particular purpose of interacting as an agent with the world and getting certain results.\n\n25:26.720 --> 25:31.760\n But the intelligence itself is in the sense instrumental to something, but by itself it's\n\n25:31.760 --> 25:36.400\n just the ability to make models. And some of the problems are so general that the system that makes\n\n25:36.400 --> 25:42.480\n them needs to understand what itself is and how it relates to the environment. So as a child,\n\n25:42.480 --> 25:47.360\n for instance, you notice you do certain things despite you perceiving yourself as wanting\n\n25:47.360 --> 25:53.040\n different things. So you become aware of your own psychology. You become aware of the fact that you\n\n25:53.040 --> 25:57.840\n have complex structure in yourself and you need to model yourself, to reverse engineer yourself,\n\n25:57.840 --> 26:02.640\n to be able to predict how you will react to certain situations and how you deal with yourself\n\n26:02.640 --> 26:08.000\n in relationship to your environment. And this process, this project, if you reverse engineer\n\n26:08.000 --> 26:12.720\n yourself and your relationship to reality and the nature of a universe that can continue, if you go\n\n26:12.720 --> 26:17.840\n all the way, this is basically the project of AI, or you could say the project of AI is a very\n\n26:17.840 --> 26:24.240\n important component in it. The Turing test, in a way, is you ask a system, what is intelligence?\n\n26:24.240 --> 26:32.000\n If that system is able to explain what it is, how it works, then you should assign it the property\n\n26:32.000 --> 26:36.320\n of being intelligent in this general sense. So the test that Turing was administering\n\n26:36.320 --> 26:41.120\n in a way, I don't think that he couldn't see it, but he didn't express it yet in the original 1950\n\n26:41.120 --> 26:47.600\n paper, is that he was trying to find out whether he was generally intelligent. Because in order to\n\n26:47.600 --> 26:51.920\n take this test, the rub is, of course, you need to be able to understand what that system is saying.\n\n26:51.920 --> 26:56.320\n And we don't yet know if we can build an AI. We don't yet know if we are generally intelligent.\n\n26:56.320 --> 27:03.440\n Basically, you win the Turing test by building an AI. Yes. So in a sense, hidden within the Turing\n\n27:03.440 --> 27:08.480\n test is a kind of recursive test. Yes, it's a test on us. The Turing test is basically\n\n27:08.480 --> 27:13.040\n a test of the conjecture, whether people are intelligent enough to understand themselves.\n\n27:14.240 --> 27:18.800\n Okay. But you also mentioned a little bit of a self awareness and then the project of AI.\n\n27:18.800 --> 27:25.200\n Do you think this kind of emergent self awareness is one of the fundamental aspects of intelligence?\n\n27:25.200 --> 27:31.200\n So as opposed to goal oriented, as you said, kind of puzzle solving, is\n\n27:31.200 --> 27:36.560\n coming to grips with the idea that you're an agent in the world.\n\n27:37.280 --> 27:42.400\n I find that many highly intelligent people are not very self aware, right? So self awareness\n\n27:42.400 --> 27:47.200\n and intelligence are not the same thing. And you can also be self aware if you have good priors,\n\n27:47.200 --> 27:52.480\n especially, without being especially intelligent. So you don't need to be very good at solving\n\n27:52.480 --> 27:56.080\n puzzles if the system that you are already implements the solution.\n\n27:56.080 --> 28:03.600\n But I do find intelligence, you kind of mentioned children, right? Is that the fundamental project\n\n28:03.600 --> 28:11.200\n of AI is to create the learning system that's able to exist in the world. So you kind of drew\n\n28:11.200 --> 28:18.320\n a difference between self awareness and intelligence. And yet you said that the self\n\n28:18.320 --> 28:23.600\n awareness seems to be important for children. So I call this ability to make sense of the\n\n28:23.600 --> 28:28.800\n world and your own place in it. So to make you able to understand what you're doing in this world,\n\n28:28.800 --> 28:33.440\n sentience. And I would distinguish sentience from intelligence because sentience is\n\n28:34.560 --> 28:39.200\n possessing certain classes of models. And intelligence is a way to get to these models\n\n28:39.200 --> 28:47.680\n if you don't already have them. I see. So can you maybe pause a bit and try to\n\n28:47.680 --> 28:53.120\n answer the question that we just said we may not be able to answer? And it might be a recursive\n\n28:53.120 --> 28:58.960\n meta question of what is intelligence? I think that intelligence is the ability to make models.\n\n28:59.840 --> 29:08.000\n So models. I think it's useful as examples. Very popular now. Neural networks form representations\n\n29:08.000 --> 29:17.200\n of a large scale data set. They form models of those data sets. When you say models and look\n\n29:17.200 --> 29:21.760\n at today's neural networks, what are the difference of how you're thinking about what is intelligent\n\n29:22.560 --> 29:29.120\n in saying that intelligence is the process of making models? Two aspects to this question. One\n\n29:29.120 --> 29:33.760\n is the representation. Is the representation adequate for the domain that we're talking about?\n\n29:33.760 --> 29:38.800\n One is the representation. Is the representation adequate for the domain that we want to represent?\n\n29:39.920 --> 29:45.760\n The other one is the type of the model that you arrive at adequate. So basically, are you\n\n29:45.760 --> 29:53.200\n modeling the correct domain? I think in both of these cases, modern AI is lacking still. I think\n\n29:53.200 --> 29:58.080\n that I'm not saying anything new here. I'm not criticizing the field. Most of the people that\n\n29:58.080 --> 30:05.040\n design our paradigms are aware of that. One aspect that we're missing is unified learning.\n\n30:05.040 --> 30:10.960\n When we learn, we at some point discover that everything that we sense is part of the same\n\n30:10.960 --> 30:14.800\n object, which means we learn it all into one model and we call this model the universe.\n\n30:14.800 --> 30:19.680\n So the experience of the world that we are embedded on is not a secret direct via to physical\n\n30:19.680 --> 30:24.720\n reality. Physical reality is a weird quantum graph that we can never experience or get access to.\n\n30:24.720 --> 30:29.600\n But it has these properties that it can create certain patterns that are systemic interface to\n\n30:29.600 --> 30:33.520\n the world. And we make sense of these patterns and the relationship between the patterns that\n\n30:33.520 --> 30:40.960\n we discover is what we call the physical universe. So at some point in our development as a nervous\n\n30:40.960 --> 30:47.120\n system, we discover that everything that we relate to in the world can be mapped to a region in the\n\n30:47.120 --> 30:52.720\n same three dimensional space, by and large. We now know in physics that this is not quite true.\n\n30:52.720 --> 30:56.960\n The world is not actually three dimensional, but the world that we are entangled with at the level\n\n30:56.960 --> 31:02.720\n which we are entangled with is largely a flat three dimensional space. And so this is the\n\n31:02.720 --> 31:07.840\n model that our brain is intuitively making. And this is, I think, what gave rise to this intuition\n\n31:07.840 --> 31:12.560\n of res extensa of this material world, this material domain. It's one of the mental domains,\n\n31:12.560 --> 31:17.440\n but it's just the class of all models that relate to this environment, this three dimensional\n\n31:17.440 --> 31:22.800\n physics engine in which we are embedded. Physics engine which we're embedded. I love that. Just\n\n31:22.800 --> 31:32.800\n slowly pause. So the quantum graph, I think you called it, which is the real world, which you\n\n31:32.800 --> 31:37.280\n can never get access to, there's a bunch of questions I want to sort of disentangle that.\n\n31:37.280 --> 31:43.840\n But maybe one useful one, one of your recent talks I looked at, can you just describe the basics?\n\n31:43.840 --> 31:49.920\n Can you talk about what is dualism? What is idealism? What is materialism? What is functionalism?\n\n31:49.920 --> 31:53.600\n And what connects with you most in terms of, because you just mentioned there's a reality\n\n31:53.600 --> 32:00.160\n we don't have access to. Okay. What does that even mean? And why don't we get access to it?\n\n32:00.160 --> 32:05.040\n Aren't we part of that reality? Why can't we access it? So the particular trajectory that\n\n32:05.040 --> 32:11.120\n mostly exists in the West is the result of our indoctrination by a cult for 2000 years.\n\n32:11.120 --> 32:15.600\n A cult? Which one? Oh, 2000 years. The Catholic cult mostly. And for better or worse,\n\n32:15.600 --> 32:20.800\n it has created or defined many of the modes of interaction that we have that has created\n\n32:20.800 --> 32:29.920\n the society. But it has also in some sense scarred our rationality. And the intuition that exists,\n\n32:29.920 --> 32:35.520\n if you would translate the mythology of the Catholic church into the modern world is that\n\n32:35.520 --> 32:41.680\n the world in which you and me interact is something like a multiplayer role playing adventure. And the\n\n32:41.680 --> 32:48.080\n money and the objects that we have in this world, this is all not real. Or as Eastern philosophers\n\n32:48.080 --> 32:54.320\n would say, it's Maya. It's just stuff that appears to be meaningful. And this embedding in this\n\n32:54.320 --> 33:00.160\n meaning, if you believe in it, is samsara. It's basically the identification with the needs of\n\n33:00.160 --> 33:06.880\n the mundane, secular, everyday existence. And the Catholics also introduced the notion of\n\n33:06.880 --> 33:12.480\n higher meaning, the sacred. And this existed before, but eventually the natural shape of God\n\n33:12.480 --> 33:17.040\n is the Platonic form of the civilization that you're part of. It's basically the superorganism\n\n33:17.040 --> 33:22.640\n that is formed by the individuals as an intentional agent. And basically, the Catholics\n\n33:22.640 --> 33:28.240\n used a relatively crude mythology to implement software on the minds of people and get the\n\n33:28.240 --> 33:34.240\n software synchronized to make them walk on lockstep, to basically get this God online\n\n33:34.240 --> 33:40.320\n and to make it efficient and effective. And I think God technically is just a self that\n\n33:40.320 --> 33:45.280\n spends multiple brains as opposed to your and my self, which mostly exists just on one brain.\n\n33:45.280 --> 33:50.240\n Right? And so in some sense, you can construct a self functionally as a function is implemented\n\n33:50.240 --> 33:54.720\n by brains that exists across brains. And this is a God with a small g.\n\n33:54.720 --> 33:58.720\n That's one of the, if you, Yuval Harari kind of talking about,\n\n33:59.920 --> 34:03.600\n this is one of the nice features of our brains. It seems to that we can\n\n34:03.600 --> 34:07.680\n all download the same piece of software like God in this case and kind of share it.\n\n34:07.680 --> 34:11.440\n Yeah. So basically you give everybody a spec and the mathematical constraints\n\n34:12.240 --> 34:16.080\n that are intrinsic to information processing,\n\n34:16.080 --> 34:20.160\n make sure that given the same spec, you come up with a compatible structure.\n\n34:20.160 --> 34:24.160\n Okay. So that's, there's the space of ideas that we all share. And we think that's kind\n\n34:24.160 --> 34:31.520\n of the mind, but that's separate from the idea is from Christianity for,\n\n34:32.160 --> 34:35.200\n from religion is that there's a separate thing between the mind.\n\n34:35.200 --> 34:39.600\n There is a real world. And this real world is the world in which God exists.\n\n34:39.600 --> 34:45.200\n God is the coder of the multiplayer adventure, so to speak. And we are all players in this game.\n\n34:45.840 --> 34:47.280\n And that's dualism.\n\n34:47.280 --> 34:53.680\n Yes. But the aspect is because the mental realm exists in a different implementation\n\n34:53.680 --> 34:59.280\n than the physical realm. And the mental realm is real. And a lot of people have this intuition\n\n34:59.280 --> 35:04.880\n that there is this real room in which you and me talk and speak right now, then comes a layer of\n\n35:04.880 --> 35:10.160\n physics and abstract rules and so on. And then comes another real room where our souls are\n\n35:10.160 --> 35:14.400\n and our true form isn't a thing that gives us phenomenal experience. And this is, of course,\n\n35:14.400 --> 35:20.880\n a very confused notion that you would get. And it's basically, it's the result of connecting\n\n35:20.880 --> 35:24.800\n materialism and idealism in the wrong way.\n\n35:24.800 --> 35:29.040\n So, okay. I apologize, but I think it's really helpful if we just try to define,\n\n35:30.240 --> 35:34.160\n try to define terms. Like what is dualism? What is idealism? What is materialism? For\n\n35:34.160 --> 35:34.960\n people that don't know.\n\n35:34.960 --> 35:40.000\n So the idea of dualism in our cultural tradition is that there are two substances, a mental\n\n35:40.000 --> 35:46.000\n substance and a physical substance. And they interact by different rules. And the physical\n\n35:46.000 --> 35:51.600\n world is basically causally closed and is built on a low level causal structure. So\n\n35:51.600 --> 35:55.520\n they're basically a bottom level that is causally closed. That's entirely mechanical\n\n35:56.160 --> 36:00.240\n and mechanical in the widest sense. So it's computational. There's basically a physical\n\n36:00.240 --> 36:05.200\n world in which information flows around and physics describes the laws of how information\n\n36:05.200 --> 36:06.560\n flows around in this world.\n\n36:06.560 --> 36:10.400\n Would you compare it to like a computer where you have hardware and software?\n\n36:10.400 --> 36:14.080\n The computer is a generalization of information flowing around. Basically,\n\n36:14.080 --> 36:17.760\n but if you want to discover that there is a universal principle, you can define this\n\n36:17.760 --> 36:23.680\n universal machine that is able to perform all the computations. So all these machines\n\n36:23.680 --> 36:27.840\n have the same power. This means that you can always define a translation between them,\n\n36:27.840 --> 36:33.680\n as long as they have unlimited memory to be able to perform each other's computations.\n\n36:33.680 --> 36:38.480\n So would you then say that materialism is this whole world is just the hardware and\n\n36:38.480 --> 36:40.800\n idealism is this whole world is just the software?\n\n36:40.800 --> 36:46.640\n Not quite. I think that most idealists don't have a notion of software yet because software\n\n36:46.640 --> 36:51.920\n also comes down to information processing. So what you notice is the only thing that\n\n36:51.920 --> 36:56.000\n is real to you and me is this experiential world in which things matter, in which things\n\n36:56.000 --> 36:59.440\n have taste, in which things have color, phenomenal content, and so on.\n\n37:00.080 --> 37:02.160\n You are bringing up consciousness. Okay.\n\n37:02.160 --> 37:07.760\n This is distinct from the physical world in which things have values only in an abstract\n\n37:07.760 --> 37:15.040\n sense. And you only look at cold patterns moving around. So how does anything feel like\n\n37:15.040 --> 37:19.200\n something? And this connection between the two things is very puzzling to a lot of people,\n\n37:19.200 --> 37:23.360\n of course, too many philosophers. So idealism starts out with the notion that mind is primary,\n\n37:23.360 --> 37:30.320\n materialism, things that matter is primary. And so for the idealist, the material patterns that\n\n37:30.320 --> 37:37.120\n we see playing out are part of the dream that the mind is dreaming. And we exist in a mind\n\n37:37.120 --> 37:43.360\n on a higher plane of existence, if you want. And for the materialist, there is only this\n\n37:43.360 --> 37:49.600\n material thing, and that generates some models, and we are the result of these models. And in\n\n37:49.600 --> 37:53.600\n some sense, I don't think that we should understand, if we understand it properly,\n\n37:53.600 --> 37:59.760\n materialism and idealism as a dichotomy, but as two different aspects of the same thing.\n\n37:59.760 --> 38:04.000\n So the weird thing is we don't exist in the physical world. We do exist inside of a story\n\n38:04.000 --> 38:14.240\n that the brain tells itself. Okay. Let my information processing take that in.\n\n38:15.040 --> 38:18.080\n We don't exist in the physical world. We exist in the narrative.\n\n38:18.080 --> 38:22.160\n Basically, a brain cannot feel anything. A neuron cannot feel anything. They're physical\n\n38:22.160 --> 38:26.720\n things. Physical systems are unable to experience anything. But it would be very useful for the\n\n38:26.720 --> 38:30.960\n brain or for the organism to know what it would be like to be a person and to feel something.\n\n38:30.960 --> 38:36.720\n Yeah. So the brain creates a simulacrum of such a person that it uses to model the interactions\n\n38:36.720 --> 38:41.680\n of the person. It's the best model of what that brain, this organism thinks it is in relationship\n\n38:41.680 --> 38:46.000\n to its environment. So it creates that model. It's a story, a multimedia novel that the brain\n\n38:46.000 --> 38:49.120\n is continuously writing and updating. But you also kind of said that\n\n38:50.000 --> 38:53.760\n you said that we kind of exist in that story. Yes.\n\n38:53.760 --> 39:04.800\n In that story. What is real in any of this? So again, these terms are... You kind of said\n\n39:04.800 --> 39:10.080\n there's a quantum graph. I mean, what is this whole thing running on then? Is the story...\n\n39:11.040 --> 39:16.640\n And is it completely fundamentally impossible to get access to it? Because isn't the story\n\n39:16.640 --> 39:24.560\n supposed to... Isn't the brain in something existing in some kind of context?\n\n39:24.560 --> 39:30.640\n So what we can identify as computer scientists, we can engineer systems and test our theories this\n\n39:30.640 --> 39:36.480\n way that might have the necessary insufficient properties to produce the phenomena that we are\n\n39:36.480 --> 39:44.240\n observing, which is the self in a virtual world that is generated in somebody's neocortex that is\n\n39:44.240 --> 39:48.480\n contained in the skull of this primate here. And when I point at this, this indexicality is of\n\n39:48.480 --> 39:55.440\n course wrong. But I do create something that is likely to give rise to patterns on your retina\n\n39:55.440 --> 40:00.480\n that allow you to interpret what I'm saying. But we both know that the world that you and me are\n\n40:00.480 --> 40:05.760\n seeing is not the real physical world. What we are seeing is a virtual reality generated in your\n\n40:05.760 --> 40:10.000\n brain to explain the patterns on your retina. How close is it to the real world? That's kind\n\n40:10.000 --> 40:18.400\n of the question. Is it when you have people like Donald Hoffman that say that you're really far\n\n40:18.400 --> 40:24.160\n away. The thing we're seeing, you and I now, that interface we have is very far away from anything.\n\n40:24.960 --> 40:29.840\n We don't even have anything close to the sense of what the real world is. Or is it a very surface\n\n40:29.840 --> 40:35.840\n piece of architecture? I imagine you look at the Mandelbrot fractal, this famous thing that\n\n40:35.840 --> 40:43.040\n Bernard Mandelbrot discovered. You see an overall shape in there. But if you truly understand it,\n\n40:43.040 --> 40:50.080\n you know it's two lines of code. It's basically in a series that is being tested for complex\n\n40:50.080 --> 40:55.520\n numbers in the complex number plane for every point. And for those where the series is diverging,\n\n40:56.240 --> 41:04.160\n you paint this black. And where it's converging, you don't. And you get the intermediate colors\n\n41:04.160 --> 41:13.040\n by taking how far it diverges. This gives you this shape of this fractal. But imagine you live\n\n41:13.040 --> 41:18.160\n inside of this fractal and you don't have access to where you are in the fractal. Or you have not\n\n41:18.160 --> 41:23.520\n discovered the generator function even. So what you see is, all I can see right now is a spiral.\n\n41:23.520 --> 41:28.000\n And this spiral moves a little bit to the right. Is this an accurate model of reality? Yes, it is.\n\n41:28.000 --> 41:33.680\n It is an adequate description. You know that there is actually no spiral in the Mandelbrot fractal.\n\n41:33.680 --> 41:39.200\n It only appears like this to an observer that is interpreting things as a two dimensional space and\n\n41:39.200 --> 41:44.000\n then defines certain regularities in there at a certain scale that it currently observes. Because\n\n41:44.000 --> 41:47.680\n if you zoom in, the spiral might disappear and turn out to be something different at a different\n\n41:47.680 --> 41:52.240\n resolution. So at this level, you have the spiral. And then you discover the spiral moves to the\n\n41:52.240 --> 41:56.960\n right and at some point it disappears. So you have a singularity. At this point, your model is no\n\n41:56.960 --> 42:01.520\n longer valid. You cannot predict what happens beyond the singularity. But you can observe again\n\n42:01.520 --> 42:06.320\n and you will see it hit another spiral and at this point it disappeared. So we now have a second\n\n42:06.320 --> 42:11.040\n order law. And if you make 30 layers of these laws, then you have a description of the world\n\n42:11.040 --> 42:14.720\n that is similar to the one that we come up with when we describe the reality around us.\n\n42:14.720 --> 42:19.440\n It's reasonably predictive. It does not cut to the core of it. It does not explain how it's\n\n42:19.440 --> 42:24.560\n being generated, how it actually works. But it's relatively good to explain the universe that we\n\n42:24.560 --> 42:28.560\n are entangled with. But you don't think the tools of computer science, the tools of physics could\n\n42:28.560 --> 42:35.040\n get, could step outside, see the whole drawing and get at the basic mechanism of how the pattern,\n\n42:35.040 --> 42:40.320\n the spirals are generated. Imagine you would find yourself embedded into a motherboard fractal and\n\n42:40.320 --> 42:46.000\n you try to figure out what works and you somehow have a Turing machine with enough memory to think.\n\n42:46.000 --> 42:51.920\n And as a result, you come to this idea, it must be some kind of automaton. And maybe you just\n\n42:51.920 --> 42:56.320\n enumerate all the possible automata until you get to the one that produces your reality.\n\n42:56.320 --> 42:59.920\n So you can identify necessary and sufficient condition. For instance,\n\n42:59.920 --> 43:05.440\n we discover that mathematics itself is the domain of all languages. And then we see that most of\n\n43:05.440 --> 43:10.320\n the domains of mathematics that we have discovered are in some sense describing the same fractals.\n\n43:10.320 --> 43:14.320\n This is what category theory is obsessed about, that you can map these different domains to each\n\n43:14.320 --> 43:19.360\n other. So they're not that many fractals. And some of these have interesting structure and\n\n43:19.360 --> 43:26.480\n symmetry breaks. And so you can discover what region of this global fractal you might be embedded\n\n43:26.480 --> 43:31.200\n in from first principles. But the only way you can get there is from first principles. So basically\n\n43:31.200 --> 43:35.760\n your understanding of the universe has to start with automata and then number theory and then\n\n43:35.760 --> 43:41.520\n spaces and so on. Yeah. I think like Stephen Wolfram still dreams that he'll be able to arrive\n\n43:41.520 --> 43:46.560\n at the fundamental rules of the cellular automata or the generalization of which\n\n43:46.560 --> 43:54.160\n is behind our universe. Yeah. You've said on this topic, you said in a recent conversation\n\n43:54.160 --> 44:00.560\n that quote, some people think that a simulation can't be conscious and only a physical system can,\n\n44:00.560 --> 44:05.920\n but they got it completely backward. A physical system cannot be conscious. Only a simulation can\n\n44:05.920 --> 44:11.920\n be conscious. Consciousness is a simulated property that simulates itself. Just like you said,\n\n44:11.920 --> 44:17.920\n the mind is kind of the, we'll call it story narrative. There's a simulation. So our mind\n\n44:17.920 --> 44:24.560\n is essentially a simulation. Usually I try to use the terminology so that the mind is basically\n\n44:24.560 --> 44:28.960\n a principles that produce the simulation. It's the software that is implemented by your brain.\n\n44:29.760 --> 44:36.560\n And the mind is creating both the universe that we are in and the self, the idea of a person that\n\n44:36.560 --> 44:41.600\n is on the other side of attention and is embedded in this world. Why is that important that\n\n44:41.600 --> 44:48.160\n idea of a self, why is that an important feature in the simulation? It's basically a result of\n\n44:48.160 --> 44:53.120\n the purpose that the mind has. It's a tool for modeling, right? We are not actually monkeys. We\n\n44:53.120 --> 44:59.760\n are side effects of the regulation needs of monkeys. And what the monkey has to regulate is\n\n45:00.560 --> 45:06.240\n the relationship of an organism to an outside world that is in large part also consisting of\n\n45:06.240 --> 45:12.400\n other organisms. And as a result, it basically has regulation targets that it tries to get to.\n\n45:12.400 --> 45:16.640\n These regulation targets start with priors. They're basically like unconditional reflexes\n\n45:16.640 --> 45:20.480\n that we are more or less born with. And then we can reverse engineer them to make them more\n\n45:20.480 --> 45:24.720\n consistent. And then we get more detailed models about how the world works and how to interact with\n\n45:24.720 --> 45:31.040\n it. And so these priors that you commit to are largely target values that our needs should\n\n45:31.040 --> 45:38.160\n approach set points. And this deviation to the set point creates some urge, some tension. And we find\n\n45:38.160 --> 45:42.640\n ourselves living inside of feedback loops, right? Consciousness emerges over dimensions of\n\n45:42.640 --> 45:48.000\n disagreements with the universe, things that you care, things are not the way they should be,\n\n45:48.000 --> 45:52.560\n but you need to regulate. And so in some sense, the sense self is the result of all the\n\n45:52.560 --> 45:56.560\n identifications that you're having. And that identification is a regulation target that\n\n45:56.560 --> 46:01.440\n you're committing to. It's a dimension that you care about, you think is important. And this is\n\n46:01.440 --> 46:07.040\n also what locks you in. If you let go of these commitments, of these identifications, you get\n\n46:07.040 --> 46:12.000\n free. There's nothing that you have to do anymore. And if you let go of all of them, you're completely\n\n46:12.000 --> 46:17.120\n free and you can enter nirvana because you're done. And actually, this is a good time to pause and say\n\n46:17.920 --> 46:23.440\n thank you to a friend of mine, Gustav Soderstr\u00f6m, who introduced me to your work. I wanted to give\n\n46:23.440 --> 46:29.360\n him a shout out. He's a brilliant guy. And I think the AI community is actually quite amazing. And\n\n46:29.360 --> 46:34.160\n Gustav is a good representative of that. You are as well. So I'm glad, first of all, I'm glad the\n\n46:34.160 --> 46:40.640\n internet exists, YouTube exists, where I can watch your talks and then get to your book and study\n\n46:40.640 --> 46:46.800\n your writing and think about, you know, that's amazing. Okay. But you've kind of described\n\n46:46.800 --> 46:52.720\n sort of this emergent phenomenon of consciousness from the simulation. So what about the hard\n\n46:52.720 --> 47:02.960\n problem of consciousness? Can you just linger on it? Why does it still feel like, I understand\n\n47:02.960 --> 47:08.800\n you're kind of, the self is an important part of the simulation, but why does the simulation\n\n47:08.800 --> 47:14.880\n feel like something? So if you look at a book by, say, George R. R. Martin, where the characters\n\n47:14.880 --> 47:19.840\n have plausible psychology and they stand on a hill because they want to conquer the city below\n\n47:19.840 --> 47:23.520\n the hill and they're done in it. And they look at the color of the sky and they are apprehensive\n\n47:24.080 --> 47:27.920\n and feel empowered and all these things. Why do they have these emotions? It's because it's\n\n47:27.920 --> 47:32.080\n written into the story, right? And it's written to the story because there's an adequate model of the\n\n47:32.080 --> 47:37.680\n person that predicts what they're going to do next. And the same thing is true for us. So it's\n\n47:37.680 --> 47:43.040\n basically a story that our brain is writing. It's not written in words. It's written in perceptual\n\n47:43.040 --> 47:50.560\n content, basically multimedia content. And it's a model of what the person would feel if it existed.\n\n47:50.560 --> 47:56.160\n So it's a virtual person. And you and me happen to be this virtual person. So this virtual person\n\n47:56.160 --> 48:01.120\n gets access to the language center and talks about the sky being blue. And this is us.\n\n48:01.760 --> 48:05.440\n But hold on a second. Do I exist in your simulation?\n\n48:05.440 --> 48:13.120\n You do exist in an almost similar way as me. So there are internal states that are less\n\n48:13.120 --> 48:20.640\n accessible for me that you have and so on. And my model might not be completely adequate.\n\n48:20.640 --> 48:25.120\n There are also things that I might perceive about you that you don't perceive. But in some sense,\n\n48:25.120 --> 48:31.360\n both you and me are some puppets, two puppets that enact a play in my mind. And I identify\n\n48:31.360 --> 48:36.080\n with one of them because I can control one of the puppets directly. And with the other one,\n\n48:36.080 --> 48:41.680\n I can create things in between. So for instance, we can go on an interaction that even leads to\n\n48:41.680 --> 48:46.400\n a coupling to a feedback loop. So we can think things together in a certain way or feel things\n\n48:46.400 --> 48:50.880\n together. But this coupling is itself not a physical phenomenon. It's entirely a software\n\n48:50.880 --> 48:54.960\n phenomenon. It's the result of two different implementations interacting with each other.\n\n48:54.960 --> 49:02.080\n So that's interesting. So are you suggesting, like the way you think about it, is the entirety\n\n49:02.080 --> 49:07.520\n of existence a simulation and where kind of each mind is a little subsimulation,\n\n49:08.640 --> 49:17.920\n that like, why don't you, why doesn't your mind have access to my mind's full state?\n\n49:18.640 --> 49:22.880\n Like, for the same reason that my mind doesn't have access to its own full state.\n\n49:22.880 --> 49:25.760\n So what, I mean,\n\n49:25.760 --> 49:29.280\n There is no trick involved. So basically, when I know something about myself,\n\n49:29.280 --> 49:33.760\n it's because I made a model. So one part of your brain is tasked with modeling what other parts of\n\n49:33.760 --> 49:35.200\n your brain are doing.\n\n49:35.200 --> 49:40.480\n Yes. But there seems to be an incredible consistency about this world in the physical\n\n49:40.480 --> 49:46.560\n sense that there's repeatable experiments and so on. How does that fit into our silly,\n\n49:46.560 --> 49:50.960\n the center of apes simulation of the world? So why is it so repeatable? Why is everything so\n\n49:50.960 --> 49:58.080\n repeatable? And not everything. There's a lot of fundamental physics experiments that are repeatable\n\n49:59.520 --> 50:05.040\n for a long time, all over the place and so on. Laws of physics. How does that fit in?\n\n50:05.040 --> 50:09.600\n It seems that the parts of the world that are not deterministic are not long lived.\n\n50:10.320 --> 50:17.040\n So if you build a system, any kind of automaton, so if you build simulations of something,\n\n50:17.040 --> 50:22.880\n you'll notice that the phenomena that endure are those that give rise to stable dynamics.\n\n50:23.520 --> 50:28.000\n So basically, if you see anything that is complex in the world, it's the result of usually of some\n\n50:28.000 --> 50:32.640\n control of some feedback that keeps it stable around certain attractors. And the things that\n\n50:32.640 --> 50:37.200\n are not stable that don't give rise to certain harmonic patterns and so on, they tend to get\n\n50:37.200 --> 50:44.400\n weeded out over time. So if we are in a region of the universe that sustains complexity, which is\n\n50:44.400 --> 50:50.160\n required to implement minds like ours, this is going to be a region of the universe that is very\n\n50:50.160 --> 50:55.680\n tightly controlled and controllable. So it's going to have lots of interesting symmetries and also\n\n50:55.680 --> 51:02.800\n symmetry breaks that allow the creation of structure. But they exist where? So there's\n\n51:02.800 --> 51:06.240\n such an interesting idea that our mind is simulation that's constructing the narrative.\n\n51:06.240 --> 51:14.880\n But my question is, just to try to understand how that fits with this, with the entirety of the\n\n51:14.880 --> 51:19.360\n universe, you're saying that there's a region of this universe that allows enough complexity to\n\n51:19.360 --> 51:27.280\n create creatures like us. But what's the connection between the brain, the mind, and the broader\n\n51:27.280 --> 51:32.720\n universe? Which comes first? Which is more fundamental? Is the mind the starting point,\n\n51:32.720 --> 51:36.880\n the universe is emergent? Is the universe the starting point, the minds are emergent?\n\n51:37.680 --> 51:42.480\n I think quite clearly the latter. That's at least a much easier explanation because it allows us to\n\n51:42.480 --> 51:47.440\n make causal models. And I don't see any way to construct an inverse causality.\n\n51:47.440 --> 51:50.320\n So what happens when you die to your mind simulation?\n\n51:51.760 --> 51:57.600\n My implementation ceases. So basically the thing that implements myself will no longer be present,\n\n51:57.600 --> 52:01.680\n which means if I am not implemented on the minds of other people, the thing that I identify with,\n\n52:01.680 --> 52:07.680\n the weird thing is I don't actually have an identity beyond the identity that I construct.\n\n52:07.680 --> 52:14.160\n If I was the Dalai Lama, he identifies as a form of government. So basically the Dalai Lama gets\n\n52:14.160 --> 52:21.760\n reborn, not because he's confused, but because he is not identifying as a human being. He runs on\n\n52:21.760 --> 52:27.040\n a human being. He's basically a governmental software that is instantiated in every new\n\n52:27.040 --> 52:31.680\n generation and you. So his advice is to pick someone who does this in the next generation.\n\n52:31.680 --> 52:37.280\n So if you identify with this, you are no longer a human and you don't die in the sense that what\n\n52:37.280 --> 52:42.320\n dies is only the body of the human that you run on. To kill the Dalai Lama, you would have to kill\n\n52:42.320 --> 52:48.160\n his tradition. And if we look at ourselves, we realize that we are to a small part like this,\n\n52:48.160 --> 52:53.360\n most of us. So for instance, if you have children, you realize something lives on in them. Or if you\n\n52:53.360 --> 52:58.400\n spark an idea in the world, something lives on, or if you identify with the society around you,\n\n52:58.400 --> 53:01.280\n because you are in part that you're not just this human being.\n\n53:01.280 --> 53:07.040\n Yeah. So in a sense, you are kind of like a Dalai Lama in the sense that you,\n\n53:07.040 --> 53:12.160\n Joshua Bach, is just a collection of ideas. So like you have this operating system on which\n\n53:12.160 --> 53:16.640\n a bunch of ideas live and interact. And then once you die, they kind of part, some of them\n\n53:17.760 --> 53:18.880\n jump off the ship.\n\n53:18.880 --> 53:23.360\n You put it put it the other way. Identity is a software state. It's a construction.\n\n53:23.360 --> 53:27.920\n It's not physically real. Identity is not a physical concept.\n\n53:27.920 --> 53:31.280\n It's basically a representation of different objects on the same world line.\n\n53:32.160 --> 53:41.120\n But identity lives and dies. Are you attached? What's the fundamental thing? Is it the ideas\n\n53:41.120 --> 53:46.400\n that come together to form identity? Or is each individual identity actually a fundamental thing?\n\n53:46.400 --> 53:49.920\n It's a representation that you can get agency over if you care. So basically,\n\n53:49.920 --> 53:53.120\n you can choose what you identify with if you want to.\n\n53:53.120 --> 54:04.400\n No, but it just seems if the mind is not real, that the birth and death is not a crucial part\n\n54:04.400 --> 54:16.000\n of it. Well, maybe I'm silly. Maybe I'm attached to this whole biological organism. But it seems\n\n54:16.000 --> 54:23.840\n that being a physical object in this world is an important aspect of birth and death.\n\n54:23.840 --> 54:29.120\n Like it feels like it has to be physical to die. It feels like simulations don't have to die.\n\n54:30.000 --> 54:34.640\n The physics that we experience is not the real physics. There is no color and sound in the real\n\n54:34.640 --> 54:40.560\n world. Color and sound are types of representations that you get if you want to model reality with\n\n54:40.560 --> 54:45.920\n oscillators. So colors and sound in some sense have octaves, and it's because they are represented\n\n54:45.920 --> 54:52.320\n probably with oscillators. So that's why colors form a circle of use. And colors have harmonics,\n\n54:52.320 --> 54:58.240\n sounds have harmonics as a result of synchronizing oscillators in the brain. So the world that we\n\n54:58.240 --> 55:03.680\n subjectively interact with is fundamentally the result of the representation mechanisms in our\n\n55:03.680 --> 55:08.000\n brain. They are mathematically to some degree universal. There are certain regularities that\n\n55:08.000 --> 55:12.720\n you can discover in the patterns and not others. But the patterns that we get, this is not the real\n\n55:12.720 --> 55:18.400\n world. The world that we interact with is always made of too many parts to count. So when you look\n\n55:18.400 --> 55:23.440\n at this table and so on, it's consisting of so many molecules and atoms that you cannot count\n\n55:23.440 --> 55:29.120\n them. So you only look at the aggregate dynamics, at limit dynamics. If you had almost infinitely\n\n55:29.120 --> 55:34.400\n many particles, what would be the dynamics of the table? And this is roughly what you get. So\n\n55:34.400 --> 55:38.880\n geometry that we are interacting with is the result of discovering those operators that\n\n55:39.680 --> 55:44.480\n work in the limit that you get by building an infinite series that converges. For those parts\n\n55:44.480 --> 55:48.400\n where it converges, it's geometry. For those parts where it doesn't converge, it's chaos.\n\n55:48.960 --> 55:55.200\n Right. And then so all of that is filtered through the consciousness that's emergent in our\n\n55:55.200 --> 56:00.000\n narrative. The consciousness gives it color, gives it feeling, gives it flavor.\n\n56:00.000 --> 56:06.720\n So I think the feeling, flavor and so on is given by the relationship that a feature has to all the\n\n56:06.720 --> 56:12.720\n other features. It's basically a giant relational graph that is our subjective universe. The color\n\n56:12.720 --> 56:18.720\n is given by those aspects of the representation or this experiential color where you care about,\n\n56:18.720 --> 56:22.880\n where you have identifications, where something means something, where you are the inside of a\n\n56:22.880 --> 56:28.320\n feedback loop. And the dimensions of caring are basically dimensions of this motivational system\n\n56:28.320 --> 56:34.880\n that we emerge over. The meaning of the relations, the graph. Can you elaborate on that a little bit?\n\n56:34.880 --> 56:41.120\n Like where does the, maybe we can even step back and ask the question of what is consciousness to\n\n56:41.120 --> 56:47.120\n be sort of more systematic. Like what do you, how do you think about consciousness?\n\n56:47.120 --> 56:52.240\n I think that consciousness is largely a model of the contents of your attention. It's a mechanism\n\n56:52.240 --> 56:58.000\n that has evolved for a certain type of learning. At the moment, our machine learning systems\n\n56:58.000 --> 57:05.120\n largely work by building chains of weighted sums of real numbers with some nonlinearity.\n\n57:05.120 --> 57:13.360\n And you learn by piping an error signal through these different chained layers and adjusting the\n\n57:13.360 --> 57:18.720\n weights in these weighted sums. And you can approximate most polynomials with this\n\n57:19.680 --> 57:24.640\n if you have enough training data. But the price is you need to change a lot of these weights.\n\n57:24.640 --> 57:30.000\n Basically, the error is piped backwards into the system until it accumulates at certain junctures\n\n57:30.000 --> 57:34.800\n in the network. And everything else evens out statistically. And only at these junctures,\n\n57:34.800 --> 57:38.320\n this is where you had the actual error in the network, you make the change there. This is a\n\n57:38.320 --> 57:42.720\n very slow process. And our brains don't have enough time for that because we don't get old\n\n57:42.720 --> 57:47.600\n enough to play Go the way that our machines learn to play Go. So instead, what we do is\n\n57:47.600 --> 57:52.960\n an attention based learning. We pinpoint the probable region in the network where we can\n\n57:52.960 --> 57:58.800\n make an improvement. And then we store this binding state together with the expected outcome\n\n57:58.800 --> 58:03.600\n in a protocol. And this ability to make index memories for the purpose of learning to revisit\n\n58:03.600 --> 58:10.160\n these commitments later, this requires a memory of the contents of our attention.\n\n58:10.160 --> 58:15.120\n Another aspect is when I construct my reality, I make mistakes. So I see things that turn out to\n\n58:15.120 --> 58:20.400\n be reflections or shadows and so on, which means I have to be able to point out which features of\n\n58:20.400 --> 58:27.360\n my perception gave rise to a present construction of reality. So the system needs to pay attention to\n\n58:28.000 --> 58:33.360\n the features that are currently in its focus. And it also needs to pay attention to whether\n\n58:33.360 --> 58:37.200\n it pays attention itself, in part because the attentional system gets trained with the same\n\n58:37.200 --> 58:41.840\n mechanism, so it's reflexive, but also in part because your attention lapses if you don't pay\n\n58:41.840 --> 58:46.960\n attention to the attention itself. So it's the thing that I'm currently seeing, just a dream\n\n58:46.960 --> 58:52.000\n that my brain has spun off into some kind of daydream, or am I still paying attention to my\n\n58:52.000 --> 58:56.800\n percept? So you have to periodically go back and see whether you're still paying attention. And\n\n58:56.800 --> 59:01.520\n if you have this loop and you make it tight enough between the system becoming aware of the contents\n\n59:01.520 --> 59:05.680\n of its attention and the fact that it's paying attention itself and makes attention the object\n\n59:05.680 --> 59:12.720\n of its attention, I think this is the loop over which we wake up. So there's this attentional\n\n59:12.720 --> 59:18.720\n mechanism that's somehow self referential that's fundamental to what consciousness is. So just\n\n59:19.600 --> 59:23.760\n ask you a question, I don't know how much you're familiar with the recent breakthroughs in natural\n\n59:23.760 --> 59:28.640\n language processing, they use attentional mechanism, they use something called transformers to\n\n59:31.040 --> 59:37.920\n learn patterns and sentences by allowing the network to focus its attention to particular\n\n59:37.920 --> 59:42.960\n parts of the sentence and each individual. So like parameterize and make it learnable\n\n59:42.960 --> 59:51.200\n the dynamics of a sentence by having like a little window into the sentence. Do you think\n\n59:51.200 --> 59:58.160\n that's like a little step towards that eventually will take us to the attentional mechanisms from\n\n59:58.160 --> 1:00:03.600\n which consciousness can emerge? Not quite. I think it models only one aspect of attention.\n\n1:00:03.600 --> 1:00:08.960\n In the early days of automated language translation, there was an example that I\n\n1:00:08.960 --> 1:00:13.840\n found particularly funny where somebody tried to translate a text from English into German\n\n1:00:13.840 --> 1:00:20.160\n and it was a bat broke the window. And the translation in German was\n\n1:00:25.760 --> 1:00:31.520\n to translate back into English a bat, this flying mammal broke the window with a baseball bat.\n\n1:00:31.520 --> 1:00:34.160\n Yes. And it seemed to be the\n\n1:00:34.160 --> 1:00:39.840\n most similar to this program because it somehow maximized the possibility of translating the\n\n1:00:39.840 --> 1:00:45.280\n concept bat into German in the same sentence. And this is a mistake that the transformer model is\n\n1:00:45.280 --> 1:00:49.840\n not doing because it's tracking identity. And the attentional mechanism in the transformer\n\n1:00:49.840 --> 1:00:56.160\n model is basically putting its finger on individual concepts and make sure that these concepts pop up\n\n1:00:56.160 --> 1:01:01.600\n later in the text and tracks basically the individuals through the text. And it's why\n\n1:01:02.560 --> 1:01:07.120\n the system can learn things that other systems couldn't before it, which makes it, for instance,\n\n1:01:07.120 --> 1:01:10.960\n possible to write a text where it talks about the scientist, then the scientist has a name\n\n1:01:10.960 --> 1:01:16.160\n and has a pronoun and it gets a consistent story about that thing. What it does not do,\n\n1:01:16.160 --> 1:01:20.800\n it doesn't fully integrate this. So this meaning falls apart at some point, it loses track of this\n\n1:01:20.800 --> 1:01:25.920\n context. It does not yet understand that everything that it says has to refer to the same universe.\n\n1:01:25.920 --> 1:01:31.520\n And this is where this thing falls apart. But the attention in the transformer model does not go\n\n1:01:31.520 --> 1:01:36.240\n beyond tracking identity. And tracking identity is an important part of attention, but it's a\n\n1:01:36.240 --> 1:01:41.360\n different, very specific attentional mechanism. And it's not the one that gives rise to the type\n\n1:01:41.360 --> 1:01:44.800\n of consciousness that we have. Just to linger on, what do you mean by\n\n1:01:44.800 --> 1:01:49.360\n identity in the context of language? So when you talk about language,\n\n1:01:49.360 --> 1:01:52.640\n you have different words that can refer to the same concept.\n\n1:01:52.640 --> 1:01:53.680\n Got it. And in the sense that...\n\n1:01:53.680 --> 1:01:59.040\n The space of concepts. So... Yes. And it can also be in a nominal sense\n\n1:01:59.040 --> 1:02:05.520\n or in an inexical sense that you say this word does not only refer to this class of objects,\n\n1:02:05.520 --> 1:02:11.520\n but it refers to a definite object, to some kind of agent that waves their way through the story.\n\n1:02:11.520 --> 1:02:16.720\n And it's only referred by different ways in the language. So the language is basically a\n\n1:02:16.720 --> 1:02:23.680\n projection from a conceptual representation from a scene that is evolving into a discrete string\n\n1:02:23.680 --> 1:02:29.680\n of symbols. And what the transformer is able to do, it learns aspects of this projection\n\n1:02:29.680 --> 1:02:34.720\n mechanism that other models couldn't learn. So have you ever seen an artificial intelligence\n\n1:02:34.720 --> 1:02:40.720\n or any kind of construction idea that allows for, unlike neural networks or perhaps within\n\n1:02:40.720 --> 1:02:47.440\n neural networks, that's able to form something where the space of concepts continues to be\n\n1:02:47.440 --> 1:02:54.080\n integrated? So what you're describing, building a knowledge base, building this consistent,\n\n1:02:54.080 --> 1:02:58.240\n larger and larger sets of ideas that would then allow for deeper understanding.\n\n1:02:59.200 --> 1:03:02.640\n Wittgenstein thought that we can build everything from language,\n\n1:03:02.640 --> 1:03:07.360\n from basically a logical grammatical construct. And I think to some degree,\n\n1:03:07.360 --> 1:03:12.560\n this was also what Minsky believed. So that's why he focused so much on common sense reasoning and\n\n1:03:12.560 --> 1:03:19.600\n so on. And a project that was inspired by him was Psych. That was basically going on.\n\n1:03:19.600 --> 1:03:23.600\n Yes. Of course, ideas don't die. Only people die.\n\n1:03:25.760 --> 1:03:26.400\n That's true.\n\n1:03:27.120 --> 1:03:30.960\n And Alt Psych is a productive project. It's just probably not one that is going to\n\n1:03:31.760 --> 1:03:35.440\n converge to general intelligence. The thing that Wittgenstein couldn't solve,\n\n1:03:35.440 --> 1:03:40.080\n and he looked at this in his book at the end of his life, Philosophical Investigations,\n\n1:03:40.080 --> 1:03:44.960\n was the notion of images. So images play an important role in Tractatus. The Tractatus is\n\n1:03:44.960 --> 1:03:49.360\n an attempt to basically turn philosophy into logical probing language, to design a logical\n\n1:03:49.360 --> 1:03:54.240\n language in which you can do actual philosophy that's rich enough for doing this. And the\n\n1:03:54.240 --> 1:04:00.480\n difficulty was to deal with perceptual content. And eventually, I think he decided that he was\n\n1:04:00.480 --> 1:04:06.400\n not able to solve it. And I think this preempted the failure of the logitist program in AI.\n\n1:04:06.400 --> 1:04:11.280\n And the solution, as we see it today, is we need more general function approximation. There are\n\n1:04:11.280 --> 1:04:16.960\n geometric functions that we learn to approximate that cannot be efficiently expressed and computed\n\n1:04:16.960 --> 1:04:22.240\n in a grammatical language. We can, of course, build automata that go via number theory and so on\n\n1:04:22.240 --> 1:04:26.080\n to learn in algebra and then compute an approximation of this geometry.\n\n1:04:26.080 --> 1:04:31.200\n But to equate language and geometry is not an efficient way to think about it.\n\n1:04:32.080 --> 1:04:37.360\n So function, well, you kind of just said that neural networks are sort of, the approach that\n\n1:04:37.360 --> 1:04:44.640\n neural networks takes is actually more general than what can be expressed through language.\n\n1:04:44.640 --> 1:04:49.840\n Yes. So what can be efficiently expressed through language at the data rates at which\n\n1:04:49.840 --> 1:04:51.360\n we process grammatical language?\n\n1:04:51.360 --> 1:04:56.880\n Okay. So you don't think languages, so you disagree with Wittgenstein,\n\n1:04:56.880 --> 1:04:58.720\n that language is not fundamental to...\n\n1:04:58.720 --> 1:05:02.960\n I agree with Wittgenstein. I just agree with the late Wittgenstein.\n\n1:05:03.680 --> 1:05:09.520\n And I also agree with the beauty of the early Wittgenstein. I think that the Tractatus itself\n\n1:05:09.520 --> 1:05:13.040\n is probably the most beautiful philosophical text that was written in the 20th century.\n\n1:05:13.920 --> 1:05:18.480\n But language is not fundamental to cognition and intelligence and consciousness.\n\n1:05:18.480 --> 1:05:23.520\n So I think that language is a particular way or the natural language that we're using is a\n\n1:05:23.520 --> 1:05:28.800\n particular level of abstraction that we use to communicate with each other. But the languages\n\n1:05:28.800 --> 1:05:35.040\n in which we express geometry are not grammatical languages in the same sense. So they work slightly\n\n1:05:35.040 --> 1:05:41.120\n differently, more general expressions of functions. And I think the general nature of a model is you\n\n1:05:41.120 --> 1:05:47.280\n have a bunch of parameters. These have a range, these are the variances of the world, and you have\n\n1:05:47.280 --> 1:05:52.400\n relationships between them, which are constraints, which say if certain parameters have these values,\n\n1:05:52.400 --> 1:05:58.880\n then other parameters have to have the following values. And this is a very early insight in\n\n1:05:58.880 --> 1:06:03.600\n computer science. And I think some of the earliest formulations is the Boltzmann machine.\n\n1:06:03.600 --> 1:06:07.680\n And the problem with the Boltzmann machine is that it has a measure of whether it's good. This\n\n1:06:07.680 --> 1:06:11.600\n is basically the energy on the system, the amount of tension that you have left in the constraints\n\n1:06:11.600 --> 1:06:16.960\n where the constraints don't quite match. It's very difficult to, despite having this global\n\n1:06:16.960 --> 1:06:22.720\n measure, to train it. Because as soon as you add more than trivially few elements,\n\n1:06:22.720 --> 1:06:26.880\n parameters into the system, it's very difficult to get it settled in the right architecture.\n\n1:06:27.760 --> 1:06:34.960\n And so the solution that Hinton and Zanofsky found was to use a restricted Boltzmann machine,\n\n1:06:34.960 --> 1:06:39.760\n which uses the hidden links, the internal links in the Boltzmann machine and only has\n\n1:06:39.760 --> 1:06:44.960\n basically input and output layer. But this limits the expressivity of the Boltzmann machine. So now\n\n1:06:44.960 --> 1:06:50.320\n he builds a network of these primitive Boltzmann machines. And in some sense, you can see almost\n\n1:06:50.320 --> 1:06:53.840\n continuous development from this to the deep learning models that we're using today,\n\n1:06:54.640 --> 1:06:58.400\n even though we don't use Boltzmann machines at this point. But the idea of the Boltzmann\n\n1:06:58.400 --> 1:07:02.640\n machine is you take this model, you clamp some of the values to perception, and this forces\n\n1:07:02.640 --> 1:07:06.240\n the entire machine to go into a state that is compatible with the states that you currently\n\n1:07:06.240 --> 1:07:12.960\n perceive. And this state is your model of the world. I think it's a very general way of thinking\n\n1:07:12.960 --> 1:07:19.920\n about models, but we have to use a different approach to make it work. We have to find\n\n1:07:19.920 --> 1:07:23.920\n different networks that train the Boltzmann machine. So the mechanism that trains the Boltzmann\n\n1:07:23.920 --> 1:07:27.680\n machine and the mechanism that makes the Boltzmann machine settle into its state\n\n1:07:28.320 --> 1:07:31.760\n are distinct from the constrained architecture of the Boltzmann machine itself.\n\n1:07:33.360 --> 1:07:36.480\n The kind of mechanisms that we want to develop, you're saying?\n\n1:07:36.480 --> 1:07:40.320\n Yes. So the direction in which I think our research is going to go\n\n1:07:40.320 --> 1:07:45.920\n is going to, for instance, what you notice in perception is our perceptual models of the world\n\n1:07:45.920 --> 1:07:50.880\n are not probabilistic, but possibilistic, which means you should be able to perceive things that\n\n1:07:50.880 --> 1:07:57.200\n are improbable, but possible. A perceptual state is valid, not if it's probable, but if it's\n\n1:07:57.200 --> 1:08:02.560\n possible, if it's coherent. So if you see a tiger coming after you, you should be able to see this\n\n1:08:02.560 --> 1:08:09.120\n even if it's unlikely. And the probability is necessary for convergence of the model. So\n\n1:08:09.120 --> 1:08:15.520\n given the state of possibilities that is very, very large and a set of perceptual features,\n\n1:08:15.520 --> 1:08:20.960\n how should you change the states of the model to get it to converge with your perception?\n\n1:08:21.840 --> 1:08:29.680\n But the space of ideas that are coherent with the context that you're sensing\n\n1:08:29.680 --> 1:08:34.160\n is perhaps not as large. I mean, that's perhaps pretty small.\n\n1:08:34.160 --> 1:08:39.280\n The degree of coherence that you need to achieve depends, of course, how deep your models go.\n\n1:08:39.920 --> 1:08:44.560\n That is, for instance, politics is very simple when you know very little about game theory and\n\n1:08:44.560 --> 1:08:49.040\n human nature. So the younger you are, the more obvious it is how politics would work, right?\n\n1:08:49.600 --> 1:08:55.520\n And because you get a coherent aesthetics from relatively few inputs. And the more layers you\n\n1:08:55.520 --> 1:09:00.880\n model, the more layers you model reality, the harder it gets to satisfy all the constraints.\n\n1:09:00.880 --> 1:09:07.040\n So, you know, the current neural networks are fundamentally supervised learning system with\n\n1:09:07.040 --> 1:09:12.240\n a feed forward neural network is back propagation to learn. What's your intuition about what kind\n\n1:09:12.240 --> 1:09:17.840\n of mechanisms might we move towards to improve the learning procedure?\n\n1:09:18.640 --> 1:09:22.960\n I think one big aspect is going to be meta learning and architecture search starts in\n\n1:09:22.960 --> 1:09:28.560\n this direction. In some sense, the first wave of classical AI work by identifying a problem\n\n1:09:28.560 --> 1:09:32.240\n and a possible solution and implementing the solution, right? A program that plays chess.\n\n1:09:32.880 --> 1:09:37.840\n And right now we are in the second wave of AI. So instead of writing the algorithm that\n\n1:09:37.840 --> 1:09:41.600\n implements the solution, we write an algorithm that automatically searches\n\n1:09:41.600 --> 1:09:46.400\n for an algorithm that implements the solution. So the learning system in some sense is an\n\n1:09:46.400 --> 1:09:51.600\n algorithm that itself discovers the algorithm that solves the problem, like Go. Go is too hard\n\n1:09:51.600 --> 1:09:56.240\n to implement the solution by hand, but we can implement an algorithm that finds the solution.\n\n1:09:56.240 --> 1:10:00.880\n Yeah. So now let's move to the third stage, right? The third stage would be meta learning.\n\n1:10:00.880 --> 1:10:05.040\n Find an algorithm that discovers a learning algorithm for the given domain.\n\n1:10:05.040 --> 1:10:08.000\n Our brain is probably not a learning system, but a meta learning system.\n\n1:10:08.640 --> 1:10:13.840\n This is one way of looking at what we are doing. There is another way. If you look at the way our\n\n1:10:13.840 --> 1:10:18.160\n brain is, for instance, implemented, there is no central control that tells all the neurons how\n\n1:10:18.160 --> 1:10:24.000\n to wire up. Instead, every neuron is an individual reinforcement learning agent. Every neuron is a\n\n1:10:24.000 --> 1:10:28.160\n single celled organism that is quite complicated and in some sense quite motivated to get fed.\n\n1:10:28.960 --> 1:10:36.160\n And it gets fed if it fires on average at the right time. And the right time depends on the\n\n1:10:36.160 --> 1:10:42.080\n context that the neuron exists in, which is the electrical and chemical environment that it has.\n\n1:10:42.080 --> 1:10:48.240\n So it basically has to learn a function over its environment that tells us when to fire to get fed.\n\n1:10:48.240 --> 1:10:52.720\n Or if you see it as a reinforcement learning agent, every neuron is in some sense making a\n\n1:10:52.720 --> 1:10:57.440\n hypothesis when it sends a signal and tries to pipe a signal through the universe and tries to\n\n1:10:57.440 --> 1:11:02.560\n get positive feedback for it. And the entire thing is set up in such a way that it's robustly\n\n1:11:02.560 --> 1:11:07.200\n self organizing into a brain, which means you start out with different neuron types\n\n1:11:07.200 --> 1:11:12.080\n that have different priors on which hypothesis to test and how to get its reward.\n\n1:11:12.080 --> 1:11:16.320\n And you put them into different concentrations in a certain spatial alignment,\n\n1:11:16.320 --> 1:11:22.080\n and then you entrain it in a particular order. And as a result, you get a well organized brain.\n\n1:11:22.080 --> 1:11:29.120\n Yeah, so okay, so the brain is a meta learning system with a bunch of reinforcement learning\n\n1:11:29.120 --> 1:11:40.000\n agents. And what I think you said, but just to clarify, there's no centralized government that\n\n1:11:40.000 --> 1:11:48.240\n tells you, here's a loss function, here's a loss function, here's a loss function. Who says what's\n\n1:11:48.240 --> 1:11:52.560\n the objective? There are also governments which impose loss functions on different parts of the\n\n1:11:52.560 --> 1:11:56.880\n brain. So we have differential attention. Some areas in your brain get specially rewarded when\n\n1:11:56.880 --> 1:12:00.960\n you look at faces. If you don't have that, you will get prosopagnosia, which basically\n\n1:12:01.600 --> 1:12:07.360\n the inability to tell people apart by their faces. And the reason that happens is because\n\n1:12:07.360 --> 1:12:12.240\n it had an evolutionary advantage. So like evolution comes into play here. But it's basically an\n\n1:12:12.240 --> 1:12:17.440\n extraordinary attention that we have for faces. I don't think that people with prosopagnosia have a\n\n1:12:17.440 --> 1:12:22.240\n perceived defective brain, the brain just has an average attention for faces. So people with\n\n1:12:22.240 --> 1:12:27.040\n prosopagnosia don't look at faces more than they look at cups. So the level at which they resolve\n\n1:12:27.040 --> 1:12:33.200\n the geometry of faces is not higher than for cups. And people that don't have prosopagnosia look\n\n1:12:34.400 --> 1:12:38.080\n obsessively at faces, right? For you and me, it's impossible to move through a crowd\n\n1:12:38.080 --> 1:12:43.440\n without scanning the faces. And as a result, we make insanely detailed models of faces that allow\n\n1:12:43.440 --> 1:12:49.840\n us to discern mental states of people. So obviously, we don't know 99% of the details\n\n1:12:49.840 --> 1:12:55.920\n of this meta learning system. That's our mind. Okay. But still, we took a leap from something\n\n1:12:55.920 --> 1:13:04.000\n much dumber to that from through the evolutionary process. Can you first of all, maybe say how hard\n\n1:13:04.000 --> 1:13:13.840\n the, how big of a leap is that from our brain, from our ancestors to multi cell organisms? And\n\n1:13:14.560 --> 1:13:21.680\n is there something we can think about? As we start to think about how to engineer intelligence,\n\n1:13:21.680 --> 1:13:28.880\n is there something we can learn from evolution? In some sense, life exists because of the market\n\n1:13:28.880 --> 1:13:34.960\n opportunity of controlled chemical reactions. We compete with dump chemical reactions and we win\n\n1:13:34.960 --> 1:13:40.000\n in some areas against this dump combustion because we can harness those entropy gradients where you\n\n1:13:40.000 --> 1:13:44.640\n need to add a little bit of energy in a specific way to harvest more energy. So we out competed\n\n1:13:44.640 --> 1:13:49.520\n combustion. Yes, in many regions we do and we try very hard because when we are in direct\n\n1:13:49.520 --> 1:13:55.120\n competition, we lose, right? So because the combustion is going to close the entropy\n\n1:13:55.120 --> 1:14:02.800\n gradients much faster than we can run. So basically we do this because every cell has\n\n1:14:02.800 --> 1:14:07.520\n a Turing machine built into it. It's like literally a read write head on the tape.\n\n1:14:09.680 --> 1:14:15.280\n So everything that's more complicated than a molecule that just is a vortex around attractors\n\n1:14:16.400 --> 1:14:21.920\n that needs a Turing machine for its regulation. And then you bind cells together and you get next\n\n1:14:21.920 --> 1:14:26.160\n level organizational organism where the cells together implement some kind of software.\n\n1:14:28.880 --> 1:14:33.120\n For me, a very interesting discovery in the last year was the word spirit because I realized that\n\n1:14:33.120 --> 1:14:38.640\n what spirit actually means is an operating system for an autonomous robot. And when the word was\n\n1:14:38.640 --> 1:14:43.520\n invented, people needed this word. But they didn't have robots that they built themselves yet. The\n\n1:14:43.520 --> 1:14:48.080\n only autonomous robots that were known were people, animals, plants, ecosystems, cities,\n\n1:14:48.080 --> 1:14:53.040\n and so on. And they all had spirits. And it makes sense to say that the plant has an operating\n\n1:14:53.040 --> 1:14:57.280\n system, right? If you pinch the plant in one area, then it's going to have repercussions\n\n1:14:57.280 --> 1:15:02.160\n throughout the plant. Everything in the plant is in some sense connected into some global aesthetics\n\n1:15:02.160 --> 1:15:06.560\n like in other organisms. An organism is not a collection of cells, it's a function that\n\n1:15:07.120 --> 1:15:13.600\n tells cells how to behave. And this function is not implemented as some kind of supernatural thing,\n\n1:15:13.600 --> 1:15:19.360\n like some morphogenetic field. It is an emergent result of the interactions of each cell with each\n\n1:15:19.360 --> 1:15:30.400\n other cell. Oh my God. So what you're saying is the organism is a function that tells what to do\n\n1:15:31.040 --> 1:15:39.840\n and the function emerges from the interaction of the cells. Yes. So it's basically a description\n\n1:15:39.840 --> 1:15:46.640\n of what the plant is doing in terms of microstates. And the microstates, the physical implementation\n\n1:15:46.640 --> 1:15:51.760\n are too many of them to describe them. So the software that we use to describe what the plant is\n\n1:15:51.760 --> 1:15:57.520\n doing, the spirit of the plant is the software, the operating system of the plant, right? This is\n\n1:15:57.520 --> 1:16:03.120\n a way in which we, the observers, make sense of the plant. And the same is true for people. So\n\n1:16:03.120 --> 1:16:07.120\n people have spirits, which is their operating system in a way, right? And there's aspects of\n\n1:16:07.120 --> 1:16:12.080\n that operating system that relate to how your body functions and others, how you socially interact,\n\n1:16:12.080 --> 1:16:18.480\n how you interact with yourself and so on. And we make models of that spirit. And we think it's a\n\n1:16:18.480 --> 1:16:24.320\n loaded term because it's from a pre scientific age. But it took the scientific age a long time\n\n1:16:24.320 --> 1:16:29.440\n to rediscover a term that is pretty much the same thing. And I suspect that the differences that we\n\n1:16:29.440 --> 1:16:33.840\n still see between the old word and the new word are translation errors that have happened over\n\n1:16:33.840 --> 1:16:39.760\n the centuries. Can you actually linger on that? Why do you say that spirit, just to clarify,\n\n1:16:39.760 --> 1:16:45.120\n because I'm a little bit confused. So the word spirit is a powerful thing. But why did you say\n\n1:16:45.120 --> 1:16:50.160\n in the last year or so that you discovered this? Do you mean the same old traditional idea of a\n\n1:16:50.160 --> 1:16:56.240\n spirit? I try to find out what people mean by spirit. When people say spirituality in the US,\n\n1:16:56.240 --> 1:17:00.240\n it usually refers to the phantom limb that they develop in the absence of culture.\n\n1:17:00.240 --> 1:17:06.880\n And a culture is in some sense, you could say the spirit of a society that is long game. This thing\n\n1:17:06.880 --> 1:17:12.720\n that is become self aware at a level above the individuals where you say, if you don't do the\n\n1:17:12.720 --> 1:17:16.960\n following things, then the grand, grand, grand grandchildren of our children will have nothing\n\n1:17:16.960 --> 1:17:22.800\n to eat. So if you take this long scope, where you try to maximize the length of the game that you\n\n1:17:22.800 --> 1:17:27.040\n are playing as a species, you realize that you're part of a larger thing that you cannot fully\n\n1:17:27.040 --> 1:17:32.560\n control. You probably need to submit to the ecosphere instead of trying to completely control\n\n1:17:32.560 --> 1:17:38.480\n it. There needs to be a certain level at which we can exist as a species if you want to endure.\n\n1:17:39.440 --> 1:17:44.320\n And our culture is not sustaining this anymore. We basically made this bet with the industrial\n\n1:17:44.320 --> 1:17:48.800\n revolution that we can control everything. And the modernist societies with basically unfettered\n\n1:17:48.800 --> 1:17:54.800\n growth led to a situation in which we depend on the ability to control the entire planet.\n\n1:17:54.800 --> 1:18:00.960\n And since we are not able to do that, as it seems, this culture will die. And we realize that it\n\n1:18:00.960 --> 1:18:06.320\n doesn't have a future, right? We called our children generation Z. That's a very optimistic\n\n1:18:06.320 --> 1:18:13.920\n thing to do. Yeah. So you can have this kind of intuition that our civilization, you said culture,\n\n1:18:13.920 --> 1:18:22.640\n but you really mean the spirit of the civilization, the entirety of the civilization may not exist\n\n1:18:22.640 --> 1:18:29.840\n for long. Yeah. Can you untangle that? What's your intuition behind that? So you kind of offline\n\n1:18:29.840 --> 1:18:35.440\n mentioned to me that the industrial revolution was kind of the moment we agreed to accept\n\n1:18:36.400 --> 1:18:42.640\n the offer sign on the paper on the dotted line with the industrial revolution, we doomed ourselves.\n\n1:18:42.640 --> 1:18:47.520\n Can you elaborate on that? This is a suspicion. I, of course, don't know how it plays out. But\n\n1:18:47.520 --> 1:18:55.840\n it seems to me that in a society in which you leverage yourself very far over an entropic abyss\n\n1:18:55.840 --> 1:19:00.560\n without land on the other side, it's relatively clear that your cantilever is at some point\n\n1:19:00.560 --> 1:19:06.240\n going to break down into this entropic abyss. And you have to pay the bill. Okay. Russia is\n\n1:19:06.240 --> 1:19:13.840\n my first language. And I'm also an idiot. Me too. This is just two apes.\n\n1:19:13.840 --> 1:19:21.840\n Instead of playing with a banana, trying to have fun by talking. Okay. Anthropic what? And what's\n\n1:19:21.840 --> 1:19:29.200\n entropic? Entropic. So entropic in the sense of entropy. Oh, entropic. Got it. And entropic,\n\n1:19:29.200 --> 1:19:35.360\n what was the other word you used? Abyss. What's that? It's a big gorge. Oh, abyss. Abyss, yes.\n\n1:19:35.360 --> 1:19:39.920\n Entropic abyss. So many of the things you say are poetic. It's hurting my ears. And this one\n\n1:19:39.920 --> 1:19:50.080\n is amazing, right? It's mispronounced, which makes you more poetic. Wittgenstein would be proud. So\n\n1:19:50.080 --> 1:19:58.320\n entropic abyss. Okay. Let's rewind then. The industrial revolution. So how does that get us\n\n1:19:58.320 --> 1:20:04.640\n into the entropic abyss? So in some sense, we burned a hundred million years worth of trees\n\n1:20:04.640 --> 1:20:10.480\n to get everybody plumbing. Yes. And the society that we had before that had a very limited number\n\n1:20:10.480 --> 1:20:18.560\n of people. So basically since zero BC, we hovered between 300 and 400 million people. Yes. And this\n\n1:20:18.560 --> 1:20:24.480\n only changed with the enlightenment and the subsequent industrial revolution. And in some\n\n1:20:24.480 --> 1:20:30.560\n sense, the enlightenment freed our rationality and also freed our norms from the preexisting order\n\n1:20:30.560 --> 1:20:35.600\n gradually. It was a process that basically happened in feedback loops. So it was not that\n\n1:20:35.600 --> 1:20:41.440\n just one caused the other. It was a dynamic that started. And the dynamic worked by basically\n\n1:20:41.440 --> 1:20:48.560\n increasing productivity to such a degree that we could feed all our children. And I think the\n\n1:20:48.560 --> 1:20:55.760\n definition of poverty is that you have as many children as you can feed before they die, which is\n\n1:20:55.760 --> 1:21:01.600\n in some sense, the state that all animals on earth are in. The definition of poverty is having\n\n1:21:01.600 --> 1:21:06.480\n enough. So you can have only so many children as you can feed and if you have more, they die. Yes.\n\n1:21:06.480 --> 1:21:12.240\n And in our societies, you can basically have as many children as you want, they don't die. Right.\n\n1:21:12.240 --> 1:21:17.680\n So the reason why we don't have as many children as we want is because we also have to pay a price\n\n1:21:17.680 --> 1:21:22.080\n in terms of we have to insert ourselves in a lower social stratum if we have too many children.\n\n1:21:22.080 --> 1:21:28.240\n So basically everybody in the under middle and lower upper class has only a limited number of\n\n1:21:28.240 --> 1:21:33.680\n children because having more of them would mean a big economic hit to the individual families.\n\n1:21:33.680 --> 1:21:39.040\n Yes. Because children, especially in the US, super expensive to have. And you only are taken out of\n\n1:21:39.040 --> 1:21:43.440\n this if you are basically super rich or if you are super poor. If you're super poor, it doesn't\n\n1:21:43.440 --> 1:21:48.080\n matter how many kids you have because your status is not going to change. And these children allow\n\n1:21:48.080 --> 1:21:54.320\n you not going to die of hunger. So how does this lead to self destruction? So there's a lot of\n\n1:21:54.320 --> 1:21:58.800\n unpleasant properties about this process. So basically what we try to do is we try to\n\n1:21:58.800 --> 1:22:06.240\n let our children survive, even if they have diseases. Like I would have died before my\n\n1:22:06.240 --> 1:22:12.080\n mid twenties without modern medicine. And most of my friends would have as well. And so many of us\n\n1:22:12.080 --> 1:22:18.880\n wouldn't live without the advantages of modern medicine and modern industrialized society. We\n\n1:22:18.880 --> 1:22:25.760\n get our protein largely by subduing the entirety of nature. Imagine there would be some very clever\n\n1:22:25.760 --> 1:22:32.960\n microbe that would live in our organisms and would completely harvest them and change them into a\n\n1:22:32.960 --> 1:22:38.640\n thing that is necessary to sustain itself. And it would discover that for instance,\n\n1:22:38.640 --> 1:22:43.440\n brain cells are kind of edible, but they're not quite nice. So you need to have more fat in them\n\n1:22:43.440 --> 1:22:47.600\n and you turn them into more fat cells. And basically this big organism would become a\n\n1:22:47.600 --> 1:22:52.000\n vegetable that is barely alive and it's going to be very brittle and not resilient when the\n\n1:22:52.000 --> 1:22:57.280\n environment changes. Yeah, but some part of that organism, the one that's actually doing all the\n\n1:22:57.280 --> 1:23:04.560\n using of the, there'll still be somebody thriving. So it relates back to this original question\n\n1:23:04.560 --> 1:23:10.400\n I suspect that we are not the smartest thing on this planet. I suspect that basically every complex\n\n1:23:10.400 --> 1:23:17.840\n system has to have some complex regulation if it depends on feedback loops. And so for instance,\n\n1:23:17.840 --> 1:23:24.320\n it's likely that we should describe a certain degree of intelligence to plants. The problem is\n\n1:23:24.320 --> 1:23:28.960\n that plants don't have a nervous system. So they don't have a way to telegraph messages over large\n\n1:23:28.960 --> 1:23:34.640\n distances almost instantly in the plant. And instead, they will rely on chemicals between\n\n1:23:34.640 --> 1:23:40.160\n adjacent cells, which means the signal processing speed depends on the signal processing with a\n\n1:23:40.160 --> 1:23:46.320\n rate of a few millimeters per second. And as a result, if the plant is intelligent,\n\n1:23:46.320 --> 1:23:49.200\n it's not going to be intelligent at similar timescales as this.\n\n1:23:49.200 --> 1:23:55.200\n Yeah, the time scale is different. So you suspect we might not be the most intelligent\n\n1:23:55.200 --> 1:24:00.000\n but we're the most intelligent in this spatial scale in our timescale.\n\n1:24:00.000 --> 1:24:05.440\n So basically, if you would zoom out very far, we might discover that there have been intelligent\n\n1:24:05.440 --> 1:24:12.080\n ecosystems on the planet that existed for thousands of years in an almost undisturbed state. And it\n\n1:24:12.080 --> 1:24:16.880\n could be that these ecosystems actively related their environment. So basically change the course\n\n1:24:16.880 --> 1:24:20.640\n of the evolution vision, this ecosystem to make it more efficient in the future.\n\n1:24:20.640 --> 1:24:25.440\n So it's possible something like plants is actually a set of living organisms,\n\n1:24:25.440 --> 1:24:30.080\n an ecosystem of living organisms that are just operating a different timescale and are far\n\n1:24:30.080 --> 1:24:34.560\n superior in intelligence than human beings. And then human beings will die out and plants will\n\n1:24:34.560 --> 1:24:36.880\n still be there and they'll be there.\n\n1:24:36.880 --> 1:24:41.920\n Yeah, there's an evolutionary adaptation playing a role at all of these levels. For instance,\n\n1:24:41.920 --> 1:24:45.840\n if mice don't get enough food and get stressed, the next step is to\n\n1:24:45.840 --> 1:24:51.280\n get more sparse and more scrawny. And the reason for this is because they in a natural\n\n1:24:51.280 --> 1:24:56.400\n environment, the mice have probably hidden a drought or something else. And if they're overgrazed,\n\n1:24:56.400 --> 1:25:01.200\n then all the things that sustain them might go extinct. And there will be no mice a few\n\n1:25:01.200 --> 1:25:05.600\n generations from now. So to make sure that there will be mice in five generations from now,\n\n1:25:05.600 --> 1:25:10.640\n basically the mice scale back. And a similar thing happens with the predators of mice.\n\n1:25:10.640 --> 1:25:15.840\n They should make sure that the mice don't completely go extinct. So in some sense, if the predators are\n\n1:25:15.840 --> 1:25:22.800\n smart enough, they will be tasked with shepherding their food supply. Maybe the reason why lions have\n\n1:25:22.800 --> 1:25:27.920\n much larger brains than antelopes is not so much because it's so hard to catch an antelope as\n\n1:25:27.920 --> 1:25:32.960\n opposed to run away from the lion. But the lions need to make complex models of their environment,\n\n1:25:33.680 --> 1:25:35.360\n more complex than the antelopes.\n\n1:25:35.360 --> 1:25:40.560\n So first of all, just describing that there's a bunch of complex systems and human beings may not\n\n1:25:40.560 --> 1:25:45.600\n even be the most special or intelligent of those complex systems, even on Earth, makes me feel a\n\n1:25:45.600 --> 1:25:48.800\n little better about the extinction of human species that we're talking about.\n\n1:25:48.800 --> 1:25:52.160\n Yes, maybe you're just Guy Astploit to put the carbon back into the atmosphere.\n\n1:25:52.160 --> 1:25:54.880\n Yeah, this is just a nice, we tried it out.\n\n1:25:54.880 --> 1:26:00.160\n The big stain on evolution is not us, it was trees. Earth evolved trees before there could be\n\n1:26:00.160 --> 1:26:05.680\n digested again. There were no insects that could break all of them apart. Cellulose is so robust\n\n1:26:05.680 --> 1:26:11.520\n that you cannot get all of it with microorganisms. So many of these trees fell into swamps and all\n\n1:26:11.520 --> 1:26:16.160\n this carbon became inert and could no longer be recycled into organisms. And we are the species\n\n1:26:16.160 --> 1:26:17.920\n that is destined to take care of that.\n\n1:26:17.920 --> 1:26:20.160\n So this is kind of...\n\n1:26:20.160 --> 1:26:24.320\n To get out of the ground, put it back into the atmosphere and the Earth is already greening.\n\n1:26:24.320 --> 1:26:26.400\n So we have to be careful about that.\n\n1:26:26.400 --> 1:26:30.240\n To get out of the ground, put it back into the atmosphere and the Earth is already greening.\n\n1:26:30.240 --> 1:26:35.680\n So within a million years or so when the ecosystems have recovered from the rapid changes,\n\n1:26:35.680 --> 1:26:39.600\n that they're not compatible with right now, the Earth is going to be awesome again.\n\n1:26:39.600 --> 1:26:42.400\n And there won't be even a memory of us, of us little apes.\n\n1:26:42.400 --> 1:26:46.480\n I think there will be memories of us. I suspect we are the first generally intelligent species\n\n1:26:46.480 --> 1:26:51.040\n in the sense. We are the first species within industrial society because we will leave more\n\n1:26:51.040 --> 1:26:53.360\n phones than bones in the stratosphere.\n\n1:26:53.360 --> 1:27:00.240\n Phones than bones. I like it. But then let me push back. You've kind of suggested that\n\n1:27:01.040 --> 1:27:08.800\n we have a very narrow definition of... I mean, why aren't trees a higher level of general\n\n1:27:08.800 --> 1:27:09.440\n intelligence?\n\n1:27:09.440 --> 1:27:13.760\n If trees were intelligent, then they would be at different timescales, which means within\n\n1:27:13.760 --> 1:27:17.520\n a hundred years, the tree is probably not going to make models that are as complex as\n\n1:27:17.520 --> 1:27:18.800\n the ones that we make in 10 years.\n\n1:27:18.800 --> 1:27:23.200\n But maybe the trees are the ones that made the phones, right?\n\n1:27:25.520 --> 1:27:31.120\n You could say the entirety of life did it. The first cell never died. The first cell\n\n1:27:31.120 --> 1:27:36.640\n only split, right? And every cell in our body is still an instance of the first cell that\n\n1:27:36.640 --> 1:27:40.240\n split off from that very first cell. There was only one cell on this planet as far as\n\n1:27:40.240 --> 1:27:46.480\n we know. And so the cell is not just a building block of life. It's a hyperorganism. And we\n\n1:27:46.480 --> 1:27:49.600\n are part of this hyperorganism.\n\n1:27:49.600 --> 1:27:56.720\n So nevertheless, this hyperorganism, no, this little particular branch of it, which is us\n\n1:27:56.720 --> 1:28:01.840\n humans, because of the industrial revolution and maybe the exponential growth of technology\n\n1:28:02.480 --> 1:28:07.840\n might somehow destroy ourselves. So what do you think is the most likely way we might\n\n1:28:07.840 --> 1:28:13.200\n destroy ourselves? So some people worry about genetic manipulation. Some people, as we've\n\n1:28:13.200 --> 1:28:18.400\n talked about, worry about either dumb artificial intelligence or super intelligent artificial\n\n1:28:18.400 --> 1:28:25.200\n intelligence destroying us. Some people worry about nuclear weapons and weapons of war in\n\n1:28:25.200 --> 1:28:29.920\n general. What do you think? If you were a betting man, what would you bet on in terms\n\n1:28:29.920 --> 1:28:34.800\n of self destruction? And then would it be higher than 50%?\n\n1:28:34.800 --> 1:28:40.960\n So it's very likely that nothing that we bet on matters after we win our bets. So I\n\n1:28:40.960 --> 1:28:44.000\n don't think that bets are literally the right way to go about this.\n\n1:28:44.000 --> 1:28:47.440\n I mean, once you're dead, you won't be there to collect the wings.\n\n1:28:47.440 --> 1:28:53.040\n So it's also not clear if we as a species go extinct. But I think that our present\n\n1:28:53.040 --> 1:28:57.120\n civilization is not sustainable. So the thing that will change is there will be probably\n\n1:28:57.120 --> 1:29:01.920\n fewer people on the planet than there are today. And even if not, then still most of\n\n1:29:01.920 --> 1:29:05.600\n people that are alive today will not have offspring in 100 years from now because of\n\n1:29:05.600 --> 1:29:10.880\n the geographic changes and so on and the changes in the food supply. It's quite likely\n\n1:29:10.880 --> 1:29:15.760\n that many areas of the planet will only be livable with a close cooling chain in 100\n\n1:29:15.760 --> 1:29:22.400\n years from now. So many of the areas around the equator and in subtropical climates that\n\n1:29:22.400 --> 1:29:27.520\n are now quite pleasant to live in, will stop to be inhabitable without air conditioning.\n\n1:29:27.520 --> 1:29:33.840\n So you honestly, wow, cooling chain, close knit cooling chain communities. So you think\n\n1:29:33.840 --> 1:29:38.000\n you have a strong worry about the effects of global warming?\n\n1:29:38.000 --> 1:29:42.480\n By itself, it's not a big issue. If you live in Arizona right now, you have basically three\n\n1:29:42.480 --> 1:29:47.440\n months in the summer in which you cannot be outside. And so you have a close cooling chain.\n\n1:29:47.440 --> 1:29:50.560\n You have air conditioning in your car and in your home and you're fine. And if the air\n\n1:29:50.560 --> 1:29:56.560\n conditioning would stop for a few days, then in many areas you would not be able to survive.\n\n1:29:56.560 --> 1:30:03.520\n Can we just pause for a second? You say so many brilliant, poetic things. Do people use\n\n1:30:03.520 --> 1:30:08.640\n that term closed cooling chain? I imagine that people use it when they describe how they get\n\n1:30:08.640 --> 1:30:13.840\n meat into a supermarket, right? If you break the cooling chain and this thing starts to thaw,\n\n1:30:13.840 --> 1:30:19.760\n you're in trouble and you have to throw it away. That's such a beautiful way to put it. It's like\n\n1:30:19.760 --> 1:30:25.120\n calling a city a closed social chain or something like that. I mean, that's right. I mean, the\n\n1:30:25.120 --> 1:30:28.720\n locality of it is really important. It basically means you wake up in a climatized room, you go\n\n1:30:28.720 --> 1:30:32.720\n to work in a climatized car, you work in a climatized office, you shop in a climatized\n\n1:30:32.720 --> 1:30:37.360\n supermarket and in between you have very short distance in which you run from your car to the\n\n1:30:37.360 --> 1:30:42.560\n supermarket, but you have to make sure that your temperature does not approach the temperature of\n\n1:30:42.560 --> 1:30:46.960\n the environment. The crucial thing is the wet barb temperature. The wet barb temperature. It's\n\n1:30:46.960 --> 1:30:54.560\n what you get when you take a wet cloth and you put it around your thermometer and then you move\n\n1:30:54.560 --> 1:31:01.920\n it very quickly through the air so you get the evaporation heat. And as soon as you can no longer\n\n1:31:01.920 --> 1:31:08.240\n cool your body temperature via evaporation to a temperature below something like I think 35\n\n1:31:08.240 --> 1:31:15.440\n degrees, you die. Which means if the outside world is dry, you can still cool yourself down\n\n1:31:15.440 --> 1:31:20.720\n by sweating. But if it has a certain degree of humidity or if it goes over a certain temperature,\n\n1:31:20.720 --> 1:31:26.320\n then sweating will not save you. And this means even if you're a healthy, fit individual within\n\n1:31:26.320 --> 1:31:31.040\n a few hours, even if you try to be in the shade and so on, you'll die unless you have\n\n1:31:31.040 --> 1:31:37.360\n some climatizing equipment. And this itself, as long as you maintain civilization and you have\n\n1:31:37.360 --> 1:31:41.840\n energy supply and you have foot trucks coming to your home that are climatized, everything is fine.\n\n1:31:41.840 --> 1:31:47.440\n But what if you lose large scale open agriculture at the same time? So basically you run into foot\n\n1:31:47.440 --> 1:31:52.480\n insecurity because climate becomes very irregular or weather becomes very irregular and you have a\n\n1:31:52.480 --> 1:31:59.760\n lot of extreme weather events. So you need to roll most of your foot maybe indoor or you need to\n\n1:31:59.760 --> 1:32:04.720\n import your foot from certain regions. And maybe you're not able to maintain the civilization\n\n1:32:04.720 --> 1:32:09.040\n throughout the planet to get the infrastructure to get the foot to your home.\n\n1:32:09.040 --> 1:32:13.920\n Right. But there could be significant impacts in the sense that people begin to suffer.\n\n1:32:13.920 --> 1:32:20.960\n There could be wars over resources and so on. But ultimately, do you not have a, not a faith, but\n\n1:32:20.960 --> 1:32:29.600\n what do you make of the capacity of technological innovation to help us prevent some of the worst\n\n1:32:30.560 --> 1:32:38.480\n damages that this condition can create? So as an example, as an almost out there example,\n\n1:32:38.480 --> 1:32:45.280\n is the work that SpaceX and Elon Musk is doing of trying to also consider our propagation\n\n1:32:45.280 --> 1:32:51.360\n throughout the universe in deep space to colonize other planets. That's one technological step.\n\n1:32:51.360 --> 1:32:56.160\n But of course, what Elon Musk is trying on Mars is not to save us from global warming,\n\n1:32:56.160 --> 1:33:01.760\n because Mars looks much worse than Earth will look like after the worst outcomes of global warming\n\n1:33:01.760 --> 1:33:06.000\n imaginable, right? Mars is essentially not habitable.\n\n1:33:06.000 --> 1:33:10.480\n It's exceptionally harsh environment, yes. But what he is doing, what a lot of people throughout\n\n1:33:10.480 --> 1:33:15.200\n history since the Industrial Revolution are doing, are just doing a lot of different technological\n\n1:33:15.200 --> 1:33:20.240\n innovation with some kind of target. And when it ends up happening, it's totally unexpected new\n\n1:33:20.240 --> 1:33:27.840\n things come up. So trying to terraform or trying to colonize Mars, extremely harsh environment,\n\n1:33:27.840 --> 1:33:36.240\n might give us totally new ideas of how to expand or increase the power of this closed cooling\n\n1:33:36.240 --> 1:33:44.880\n circuit that empowers the community. So it seems like there's a little bit of a race between our\n\n1:33:44.880 --> 1:33:55.280\n open ended technological innovation of this communal operating system that we have and our\n\n1:33:55.280 --> 1:34:02.800\n general tendency to want to overuse resources and thereby destroy ourselves. You don't think\n\n1:34:02.800 --> 1:34:08.960\n technology can win that race? I think the probability is relatively low, given that our\n\n1:34:08.960 --> 1:34:15.040\n technology is, for instance, the US is stagnating since the 1970s roughly, in terms of technology.\n\n1:34:15.040 --> 1:34:19.920\n Most of the things that we do are the result of incremental processes. What about Intel?\n\n1:34:19.920 --> 1:34:24.480\n What about Moore's Law? It's basically, it's very incremental. The things that we're doing is,\n\n1:34:24.480 --> 1:34:31.760\n so the invention of the microprocessor was a major thing, right? The miniaturization\n\n1:34:31.760 --> 1:34:38.880\n of transistors was really major. But the things that we did afterwards largely were not that\n\n1:34:38.880 --> 1:34:48.960\n innovative. We had gradual changes of scaling things from CPUs into GPUs and things like that.\n\n1:34:48.960 --> 1:34:54.080\n But I don't think that there are, basically there are not many things. If you take a person that\n\n1:34:54.080 --> 1:34:59.040\n died in the 70s and was at the top of their game, they would not need to read that many books\n\n1:34:59.040 --> 1:35:05.040\n to be current again. But it's all about books. Who cares about books? There might be things that are\n\n1:35:05.040 --> 1:35:11.040\n beyond books. Or say papers. No, papers. Forget papers. There might be things that are, so papers\n\n1:35:11.040 --> 1:35:16.400\n and books and knowledge, that's a concept of a time when you were sitting there by candlelight\n\n1:35:16.400 --> 1:35:21.280\n and individual consumers of knowledge. What about the impact that we're not in the middle of,\n\n1:35:21.280 --> 1:35:27.760\n might not be understanding of Twitter, of YouTube? The reason you and I are sitting here today\n\n1:35:27.760 --> 1:35:35.600\n is because of Twitter and YouTube. So the ripple effect, and there's two minds, sort of two dumb\n\n1:35:35.600 --> 1:35:42.160\n apes coming up with a new, perhaps a new clean insights, and there's 200 other apes listening\n\n1:35:42.160 --> 1:35:48.800\n right now, 200,000 other apes listening right now. And that effect, it's very difficult to understand\n\n1:35:48.800 --> 1:35:53.360\n what that effect will have. That might be bigger than any of the advancements of the microprocessor\n\n1:35:53.360 --> 1:36:00.560\n or any of the industrial revolution, the ability of spread knowledge. And that knowledge,\n\n1:36:02.880 --> 1:36:09.840\n like it allows good ideas to reach millions much faster. And the effect of that, that might be the\n\n1:36:09.840 --> 1:36:16.880\n new, that might be the 21st century, is the multiplying of ideas, of good ideas. Because if\n\n1:36:16.880 --> 1:36:24.160\n you say one good thing today, that will multiply across huge amounts of people, and then they will\n\n1:36:24.160 --> 1:36:27.840\n say something, and then they will have another podcast, and they'll say something, and then they'll\n\n1:36:27.840 --> 1:36:33.360\n write a paper. That could be a huge, you don't think that? Yeah, we should have billions for\n\n1:36:33.360 --> 1:36:38.960\n Neumann's right now in two rings, and we don't for some reason. I suspect the reason is that\n\n1:36:38.960 --> 1:36:43.200\n we destroy our attention span. Also the incentives, of course, different. Yeah, we have extreme\n\n1:36:43.200 --> 1:36:48.000\n Kardashians, yeah. So the reason why we're sitting here and doing this as a YouTube video is because\n\n1:36:48.000 --> 1:36:52.160\n you and me don't have the attention span to write a book together right now. And you guys probably\n\n1:36:52.160 --> 1:37:01.600\n don't have the attention span to read it. So let me tell you, it's very short. But we're an hour\n\n1:37:01.600 --> 1:37:06.240\n and 40 minutes in, and I guarantee you that 80% of the people are still listening. So there is an\n\n1:37:06.240 --> 1:37:13.520\n attention span. It's just the form. Who said that the book is the optimal way to transfer information?\n\n1:37:13.520 --> 1:37:17.440\n This is still an open question. That's what we're... It's something that social media could be doing\n\n1:37:17.440 --> 1:37:22.240\n that other forms could not be doing. I think the end game of social media is a global brain.\n\n1:37:22.240 --> 1:37:26.560\n And Twitter is in some sense a global brain that is completely hooked on dopamine, doesn't have any\n\n1:37:26.560 --> 1:37:32.320\n kind of inhibition, and as a result is caught in a permanent seizure. It's also in some sense a\n\n1:37:32.320 --> 1:37:38.000\n multiplayer role playing game. And people use it to play an avatar that is not like them,\n\n1:37:38.000 --> 1:37:41.600\n as they were in this sane world, and they look through the world through the lens of their phones\n\n1:37:41.600 --> 1:37:45.280\n and think it's the real world. But it's the Twitter world that is distorted by the popularity\n\n1:37:45.280 --> 1:37:52.640\n incentives of Twitter. Yeah, the incentives and just our natural biological, the dopamine rush\n\n1:37:52.640 --> 1:38:01.920\n of a like, no matter how... I try to be very kind of Zen like and minimalist and not be influenced\n\n1:38:01.920 --> 1:38:05.840\n by likes and so on, but it's probably very difficult to avoid that to some degree.\n\n1:38:07.280 --> 1:38:14.720\n Speaking at a small tangent of Twitter, how can Twitter be done better?\n\n1:38:15.760 --> 1:38:19.760\n I think it's an incredible mechanism that has a huge impact on society\n\n1:38:19.760 --> 1:38:25.040\n by doing exactly what you're doing. Sorry, doing exactly what you described, which is having this...\n\n1:38:25.040 --> 1:38:33.440\n We're like, is this some kind of game, and we're kind of our individual RL agents in this game,\n\n1:38:33.440 --> 1:38:37.440\n and it's uncontrollable because there's not really a centralized control. Neither Jack Dorsey nor\n\n1:38:37.440 --> 1:38:44.800\n the engineers at Twitter seem to be able to control this game. Or can they? That's sort\n\n1:38:44.800 --> 1:38:49.760\n of a question. Is there any advice you would give on how to control this game?\n\n1:38:49.760 --> 1:38:53.440\n I wouldn't give advice because I am certainly not an expert, but I can give my thoughts on this.\n\n1:38:53.440 --> 1:39:01.120\n And our brain has solved this problem to some degree. Our brain has lots of individual agents\n\n1:39:01.120 --> 1:39:06.080\n that manage to play together in a way. And we have also many contexts in which other organisms\n\n1:39:06.080 --> 1:39:11.840\n have found ways to solve the problems of cooperation that we don't solve on Twitter.\n\n1:39:12.480 --> 1:39:18.720\n And maybe the solution is to go for an evolutionary approach. So imagine that you\n\n1:39:18.720 --> 1:39:23.520\n have something like Reddit or something like Facebook and something like Twitter,\n\n1:39:23.520 --> 1:39:27.440\n and you think about what they have in common. What they have in common, they are companies\n\n1:39:27.440 --> 1:39:32.960\n that in some sense own a protocol. And this protocol is imposed on a community, and the\n\n1:39:32.960 --> 1:39:39.680\n protocol has different components for monetization, for user management, for user display, for rating,\n\n1:39:39.680 --> 1:39:44.320\n for anonymity, for import of other content, and so on. And now imagine that you take these\n\n1:39:44.320 --> 1:39:50.960\n components of the protocol apart, and you do it in some sense like communities within this\n\n1:39:50.960 --> 1:39:55.840\n social network. And these communities are allowed to mix and match their protocols and design new\n\n1:39:55.840 --> 1:40:02.000\n ones. So for instance, the UI and the UX can be defined by the community. The rules for sharing\n\n1:40:02.000 --> 1:40:07.440\n content across communities can be defined. The monetization can be redefined. The way you reward\n\n1:40:07.440 --> 1:40:13.040\n individual users for what can be redefined. The way users can represent themselves and to each\n\n1:40:13.040 --> 1:40:18.720\n other can redefined. Who could be the redefiner? So can individual human beings build enough\n\n1:40:18.720 --> 1:40:22.800\n intuition to redefine those things? This itself can become part of the protocol. So for instance,\n\n1:40:22.800 --> 1:40:27.600\n it could be in some communities, it will be a single person that comes up with these things.\n\n1:40:27.600 --> 1:40:32.480\n And others, it's a group of friends. Some might implement a voting scheme that has some interesting\n\n1:40:32.480 --> 1:40:36.640\n weighted voting. Who knows? Who knows what will be the best self organizing principle for this.\n\n1:40:36.640 --> 1:40:39.840\n But the process can't be automated. I mean, it seems like the brain.\n\n1:40:39.840 --> 1:40:45.360\n It can be automated so people can write software for this. And eventually the idea is,\n\n1:40:45.920 --> 1:40:50.240\n let's not make an assumption about this thing if you don't know what the right solution is. In\n\n1:40:50.240 --> 1:40:55.520\n those areas that we have no idea whether the right solution will be people designing this ad hoc,\n\n1:40:55.520 --> 1:41:01.200\n or machines doing this. Whether you want to enforce compliance by social norms like Wikipedia,\n\n1:41:01.200 --> 1:41:06.480\n or with software solutions, or with AI that goes through the posts of people, or with a\n\n1:41:06.480 --> 1:41:12.160\n legal principle, and so on. This is something maybe you need to find out. And so the idea would\n\n1:41:12.160 --> 1:41:17.840\n be if you let the communities evolve, and you just control it in such a way that you are\n\n1:41:17.840 --> 1:41:23.680\n incentivizing the most sentient communities. The ones that produce the most interesting\n\n1:41:24.400 --> 1:41:29.440\n behaviors that allow you to interact in the most helpful ways to the individuals.\n\n1:41:29.440 --> 1:41:32.640\n You have a network that gives you information that is relevant to you.\n\n1:41:32.640 --> 1:41:37.680\n It helps you to maintain relationships to others in healthy ways. It allows you to build teams. It\n\n1:41:37.680 --> 1:41:42.720\n allows you to basically bring the best of you into this thing and goes into a coupling into\n\n1:41:42.720 --> 1:41:47.040\n a relationship with others in which you produce things that you would be unable to produce alone.\n\n1:41:47.040 --> 1:41:53.040\n Yes, beautifully put. But the key process of that with incentives and evolution\n\n1:41:53.040 --> 1:42:01.840\n is things that don't adopt themselves to effectively get the incentives have to die.\n\n1:42:02.640 --> 1:42:07.600\n And the thing about social media is communities that are unhealthy or whatever you wanted that\n\n1:42:07.600 --> 1:42:13.120\n defines the incentives really don't like dying. One of the things that people really get aggressive,\n\n1:42:13.120 --> 1:42:19.280\n protest aggressively is when they're censored. Especially in America. I don't know much about\n\n1:42:19.280 --> 1:42:24.880\n the rest of the world, but the idea of freedom of speech, the idea of censorship is really painful\n\n1:42:24.880 --> 1:42:38.480\n in America. And so what do you think about that? Having grown up in East Germany, do you think\n\n1:42:38.480 --> 1:42:45.520\n censorship is an important tool in our brain and the intelligence and in social networks?\n\n1:42:45.520 --> 1:42:53.200\n So basically, if you're not a good member of the entirety of the system, they should be blocked\n\n1:42:53.200 --> 1:42:59.040\n away. Well, locked away, blocked. An important thing is who decides that you are a good member.\n\n1:42:59.040 --> 1:43:03.520\n Who? Is it distributed? And what is the outcome of the process that decides it,\n\n1:43:04.160 --> 1:43:09.840\n both for the individual and for society at large. For instance, if you have a high trust society,\n\n1:43:09.840 --> 1:43:14.720\n you don't need a lot of surveillance. And the surveillance is even in some sense undermining\n\n1:43:14.720 --> 1:43:21.280\n trust. Because it's basically punishing people that look suspicious when surveyed,\n\n1:43:21.280 --> 1:43:26.560\n but do the right thing anyway. And the opposite, if you have a low trust society,\n\n1:43:26.560 --> 1:43:30.960\n then surveillance can be a better trade off. And the US is currently making a transition from a\n\n1:43:30.960 --> 1:43:36.400\n relatively high trust or mixed trust society to a low trust society. So surveillance will increase.\n\n1:43:36.400 --> 1:43:40.880\n Another thing is that beliefs are not just inert representations. There are implementations that\n\n1:43:40.880 --> 1:43:45.600\n run code on your brain and change your reality and change the way you interact with each other\n\n1:43:45.600 --> 1:43:52.240\n at some level. And some of the beliefs are just public opinions that we use to display our\n\n1:43:52.240 --> 1:43:58.560\n alignment. So for instance, people might say, all cultures are the same and equally good,\n\n1:43:58.560 --> 1:44:03.760\n but still they prefer to live in some cultures over others, very, very strongly so. And it turns\n\n1:44:03.760 --> 1:44:08.240\n out that the cultures are defined by certain rules of interaction. And these rules of interaction\n\n1:44:08.240 --> 1:44:12.720\n lead to different results when you implement them. So if you adhere to certain rules,\n\n1:44:12.720 --> 1:44:18.720\n you get different outcomes in different societies. And this all leads to very tricky\n\n1:44:18.720 --> 1:44:21.600\n situations when people do not have a commitment to a shared purpose.\n\n1:44:22.640 --> 1:44:27.920\n And our societies probably need to rediscover what it means to have a shared purpose and how\n\n1:44:27.920 --> 1:44:34.320\n to make this compatible with a non totalitarian view. So in some sense, the US is caught in a\n\n1:44:34.320 --> 1:44:42.880\n conundrum between totalitarianism and diversity, and doesn't need to know how to resolve this.\n\n1:44:42.880 --> 1:44:47.280\n And the solutions that the US has found so far are very crude because it's a very young society\n\n1:44:47.280 --> 1:44:51.520\n that is also under a lot of tension. It seems to me that the US will have to reinvent itself.\n\n1:44:52.240 --> 1:45:01.120\n What do you think, just philosophizing, what kind of mechanisms of government do you think\n\n1:45:01.120 --> 1:45:05.440\n we as a species should be involved with, US or broadly? What do you think will work well\n\n1:45:07.200 --> 1:45:11.360\n as a system? Of course, we don't know. It all seems to work pretty crappily,\n\n1:45:11.360 --> 1:45:16.800\n some things worse than others. Some people argue that communism is the best. Others say,\n\n1:45:16.800 --> 1:45:22.720\n yeah, look at the Soviet Union. Some people argue that anarchy is the best and then completely\n\n1:45:22.720 --> 1:45:29.920\n discarding the positive effects of government. There's a lot of arguments. US seems to be doing\n\n1:45:29.920 --> 1:45:36.240\n pretty damn well in the span of history. There's a respect for human rights, which seems to be a\n\n1:45:36.240 --> 1:45:41.520\n nice feature, not a bug. And economically, a lot of growth, a lot of technological development.\n\n1:45:42.320 --> 1:45:46.000\n People seem to be relatively kind on the grand scheme of things.\n\n1:45:47.760 --> 1:45:52.400\n What lessons do you draw from that? What kind of government system do you think is good?\n\n1:45:52.400 --> 1:45:58.880\n Ideally, a government should not be perceivable. It should be frictionless. The more you notice the\n\n1:45:58.880 --> 1:46:04.240\n influence of the government, the more friction you experience, the less effective and efficient\n\n1:46:04.240 --> 1:46:09.840\n the government probably is. A government, game theoretically, is an agent that imposes\n\n1:46:10.560 --> 1:46:17.680\n an offset on your payout metrics to make your Nash equilibrium compatible with the common good.\n\n1:46:17.680 --> 1:46:23.120\n You have these situations where people act on local incentives and these local incentives,\n\n1:46:23.120 --> 1:46:27.040\n everybody does the thing that's locally the best for them, but the global outcome is not good.\n\n1:46:27.040 --> 1:46:31.760\n And this is even the case when people care about the global outcome, because a regulation mechanism\n\n1:46:31.760 --> 1:46:36.240\n exists that creates a causal relationship between what I want to have for the global good and what\n\n1:46:36.240 --> 1:46:41.440\n I do. For instance, if I think that we should fly less and I stay at home, there's not a single plane\n\n1:46:41.440 --> 1:46:49.120\n that is going to not start because of me, right? It's not going to have an influence, but I don't\n\n1:46:49.120 --> 1:46:55.840\n get from A to B. So the way to implement this would be to have a government that is sharing\n\n1:46:55.840 --> 1:46:59.600\n this idea that we should fly less and is then imposing a regulation that, for instance,\n\n1:46:59.600 --> 1:47:06.800\n makes flying more expensive and gives incentives for inventing other forms of transportation that\n\n1:47:06.800 --> 1:47:14.000\n are less putting that strain on the environment, for instance. So there's so much optimism and\n\n1:47:14.000 --> 1:47:18.160\n so many things you describe, and yet there's the pessimism of you think our civilization is going\n\n1:47:18.160 --> 1:47:22.880\n to come to an end. So that's not a hundred percent probability. Nothing in this world is.\n\n1:47:23.760 --> 1:47:30.960\n So what's the trajectory out of self destruction, do you think? I suspect that in some sense,\n\n1:47:30.960 --> 1:47:35.520\n we are both too smart and not smart enough, which means we are very good at solving near\n\n1:47:35.520 --> 1:47:43.040\n term problems. And at the same time, we are unwilling to submit to the imperatives that\n\n1:47:43.040 --> 1:47:48.720\n we would have to follow in if you want to stick around. So that makes it difficult. If you were\n\n1:47:48.720 --> 1:47:53.760\n unable to solve everything technologically, you can probably understand how high the child mortality\n\n1:47:53.760 --> 1:47:59.280\n needs to be to absorb the mutation rate and how high the mutation rate needs to be to adapt to a\n\n1:47:59.280 --> 1:48:04.480\n slowly changing ecosystemic environment. So you could in principle compute all these things game\n\n1:48:04.480 --> 1:48:10.720\n theoretically and adapt to it. But if you cannot do this, because you are like me and you have\n\n1:48:10.720 --> 1:48:14.880\n children, you don't want them to die, you will use any kind of medical information to keep\n\n1:48:16.320 --> 1:48:22.000\n mortality low. Even if it means that within a few generations, we have enormous genetic drift,\n\n1:48:22.000 --> 1:48:26.400\n and most of us have allergies as a result of not being adapted to the changes that we\n\n1:48:26.400 --> 1:48:31.760\n made to our food supply. That's for now, I say technologically speaking, we're just very young,\n\n1:48:31.760 --> 1:48:36.880\n 300 years industrial revolution, we're very new to this idea. So you're attached to your kids being\n\n1:48:36.880 --> 1:48:41.680\n alive and not being murdered for the good of society. But that might be a very temporary\n\n1:48:41.680 --> 1:48:48.800\n moment of time that we might evolve in our thinking. So like you said, we're both smart\n\n1:48:48.800 --> 1:48:54.720\n and not smart enough. We are probably not the first human civilization that has discovered\n\n1:48:54.720 --> 1:48:59.840\n technology that allows us to efficiently overgraze our resources. And this overgrazing,\n\n1:48:59.840 --> 1:49:04.320\n this thing, at some point, we think we can compensate this because if we have eaten all\n\n1:49:04.320 --> 1:49:10.160\n the grass, we will find a way to grow mushrooms. But it could also be that the ecosystems tip.\n\n1:49:10.160 --> 1:49:14.240\n And so what really concerns me is not so much the end of the civilization, because we will\n\n1:49:14.240 --> 1:49:21.280\n invent a new one. But what concerns me is the fact that, for instance, the oceans might tip.\n\n1:49:21.920 --> 1:49:27.520\n So for instance, maybe the plankton dies because of ocean acidification and cyanobacteria take over,\n\n1:49:27.520 --> 1:49:32.160\n and as a result, we can no longer breathe the atmosphere. This would be really concerning.\n\n1:49:32.160 --> 1:49:37.200\n So basically a major reboot of most complex organisms on Earth. And I think this is a\n\n1:49:37.200 --> 1:49:42.880\n possibility. I don't know what the percentage for this possibility is, but it doesn't seem to be\n\n1:49:42.880 --> 1:49:46.480\n outlandish to me if you look at the scale of the changes that we've already triggered on this\n\n1:49:46.480 --> 1:49:51.920\n planet. And so Danny Hiller suggests that, for instance, we may be able to put chalk into the\n\n1:49:51.920 --> 1:49:57.040\n stratosphere to limit solar radiation. Maybe it works. Maybe this is sufficient to counter\n\n1:49:57.040 --> 1:50:01.840\n the effects of what we've done. Maybe it won't be. Maybe we won't be able to implement it by\n\n1:50:01.840 --> 1:50:07.360\n the time it's prevalent. I have no idea how the future is going to play out in this regard. It's\n\n1:50:07.360 --> 1:50:12.560\n just, I think it's quite likely that we cannot continue like this. All our cousin species,\n\n1:50:12.560 --> 1:50:19.280\n the other hominids are gone. So the right step would be to what? To rewind\n\n1:50:19.280 --> 1:50:28.320\n and to rewind towards the industrial revolution and slow the, so try to contain the technological\n\n1:50:28.320 --> 1:50:33.440\n process that leads to the overconsumption of resources? Imagine you get to choose,\n\n1:50:33.440 --> 1:50:38.640\n you have one lifetime. You get born into a sustainable agricultural civilization,\n\n1:50:38.640 --> 1:50:45.840\n 300, maybe 400 million people on the planet tops. Or before this, some kind of nomadic\n\n1:50:45.840 --> 1:50:51.440\n species was like a million or 2 million. And so you don't meet new people unless you give birth\n\n1:50:51.440 --> 1:50:55.920\n to them. You cannot travel to other places in the world. There is no internet. There is no\n\n1:50:55.920 --> 1:51:00.160\n interesting intellectual tradition that reaches considerably deep. So you would not discover\n\n1:51:00.160 --> 1:51:06.480\n human completeness probably and so on. We wouldn't exist. And the alternative is you get born into an\n\n1:51:06.480 --> 1:51:11.440\n insane world. One that is doomed to die because it has just burned a hundred million years worth\n\n1:51:11.440 --> 1:51:16.720\n of trees in a single century. Which one do you like? I think I like this one. It's a very weird\n\n1:51:16.720 --> 1:51:21.280\n thing that when you find yourself on a Titanic and you see this iceberg and it looks like we\n\n1:51:21.280 --> 1:51:25.440\n are not going to miss it. And a lot of people are in denial. And most of the counter arguments\n\n1:51:25.440 --> 1:51:30.320\n sound like denial to me. They don't seem to be rational arguments. And the other thing is we\n\n1:51:30.320 --> 1:51:34.320\n are born on this Titanic. Without this Titanic, we wouldn't have been born. We wouldn't be here. We\n\n1:51:34.320 --> 1:51:38.320\n wouldn't be talking. We wouldn't be on the internet. We wouldn't do all the things that we enjoy.\n\n1:51:38.320 --> 1:51:45.680\n And we are not responsible for this happening. If we had the choice, we would probably try to\n\n1:51:45.680 --> 1:51:51.120\n prevent it. But when we were born, we were never asked when we want to be born, in which society\n\n1:51:51.120 --> 1:51:55.760\n we want to be born, what incentive structures we want to be exposed to. We have relatively\n\n1:51:55.760 --> 1:52:00.080\n little agency in the entire thing. Humanity has relatively little agency in the whole thing. It's\n\n1:52:00.080 --> 1:52:04.720\n basically a giant machine that's tumbling down a hill and everybody is frantically trying to push\n\n1:52:04.720 --> 1:52:09.840\n some buttons. Nobody knows what these buttons are meaning, what they connect to. And most of them\n\n1:52:09.840 --> 1:52:15.360\n are not stopping this tumbling down the hill. Is it possible that artificial intelligence will give\n\n1:52:15.360 --> 1:52:25.040\n us an escape latch somehow? So there's a lot of worry about existential threats of artificial\n\n1:52:25.040 --> 1:52:33.600\n intelligence. But what AI also allows, in general forms of automation, allows the potential of\n\n1:52:33.600 --> 1:52:39.680\n extreme productivity growth that will also perhaps in a positive way transform society,\n\n1:52:40.560 --> 1:52:52.480\n that may allow us to inadvertently to return to the more, to the same kind of ideals of closer to\n\n1:52:52.480 --> 1:52:59.600\n nature that's represented in hunter gatherer societies. That's not destroying the planet,\n\n1:52:59.600 --> 1:53:03.680\n that's not doing overconsumption and so on. I mean, generally speaking,\n\n1:53:03.680 --> 1:53:09.520\n do you have hope that AI can help somehow? I think it's not fun to be very close to nature\n\n1:53:09.520 --> 1:53:16.720\n until you completely subdue nature. So our idea of being close to nature means being close to\n\n1:53:16.720 --> 1:53:21.760\n agriculture, basically forests that don't have anything in them that eats us.\n\n1:53:21.760 --> 1:53:29.040\n TITO See, I mean, I want to disagree with that. I think the niceness of being close to nature\n\n1:53:30.000 --> 1:53:36.320\n is to being fully present and in like, when survival becomes your primary,\n\n1:53:37.040 --> 1:53:47.680\n not just your goal, but your whole existence. I'm not just romanticizing, I can just speak for\n\n1:53:47.680 --> 1:53:54.000\n myself. I am self aware enough that that is a fulfilling existence.\n\n1:53:54.880 --> 1:54:00.880\n I personally prefer to be in nature and not fight for my survival. I think fighting for your survival\n\n1:54:00.880 --> 1:54:06.480\n while being in the cold and in the rain and being hunted by animals and having open wounds\n\n1:54:06.480 --> 1:54:07.760\n is very unpleasant.\n\n1:54:07.760 --> 1:54:14.000\n There's a contradiction in there. Yes, I and you, just as you said, would not choose it.\n\n1:54:14.000 --> 1:54:17.680\n But if I was forced into it, it would be a fulfilling existence.\n\n1:54:17.680 --> 1:54:23.280\n Yes, if you are adapted to it, basically, if your brain is wired up in such a way that you\n\n1:54:23.280 --> 1:54:28.720\n get rewards optimally in such an environment. And there's some evidence for this that for\n\n1:54:29.280 --> 1:54:33.520\n a certain degree of complexity, basically, people are more happy in such an environment because\n\n1:54:33.520 --> 1:54:38.400\n it's what you largely have evolved for. In between, we had a few thousand years in which\n\n1:54:38.400 --> 1:54:41.680\n I think we have evolved for a slightly more comfortable environment. So\n\n1:54:41.680 --> 1:54:47.360\n there is probably something like an intermediate stage in which people would be more happy than\n\n1:54:47.360 --> 1:54:51.680\n they would be if they would have to fend for themselves in small groups in the forest and\n\n1:54:51.680 --> 1:54:57.680\n often die. Versus something like this, where we now have basically a big machine, a big\n\n1:54:57.680 --> 1:55:05.920\n Mordor in which we run through concrete boxes and press buttons and machines, and largely\n\n1:55:05.920 --> 1:55:12.880\n don't feel well cared for as the monkeys that we are. So returning briefly to, not briefly,\n\n1:55:12.880 --> 1:55:19.680\n but returning to AI, what, let me ask a romanticized question, what is the most beautiful\n\n1:55:19.680 --> 1:55:25.760\n to you, silly ape, the most beautiful or surprising idea in the development of artificial\n\n1:55:25.760 --> 1:55:30.000\n intelligence, whether in your own life or in the history of artificial intelligence that you've\n\n1:55:30.000 --> 1:55:37.120\n come across? If you built an AI, it probably can make models at an arbitrary degree of detail,\n\n1:55:37.120 --> 1:55:42.400\n right, of the world. And then it would try to understand its own nature. It's tempting to think\n\n1:55:42.400 --> 1:55:46.000\n that at some point when we have general intelligence, we have competitions where we\n\n1:55:46.000 --> 1:55:51.040\n will let the AIs wake up in different kinds of physical universes, and we measure how many\n\n1:55:51.040 --> 1:55:55.920\n movements of the Rubik's cube it takes until it's figured out what's going on in its universe and\n\n1:55:55.920 --> 1:56:00.640\n what it is in its own nature and its own physics and so on, right? So what if we exist in the\n\n1:56:00.640 --> 1:56:05.440\n memory of an AI that is trying to understand its own nature and remembers its own genesis and\n\n1:56:05.440 --> 1:56:11.280\n remembers Lex and Joscha sitting in a hotel room, sparking some of the ideas off that led to the\n\n1:56:11.280 --> 1:56:15.760\n development of general intelligence. So we're a kind of simulation that's running in an AI\n\n1:56:15.760 --> 1:56:21.680\n system that's trying to understand itself. It's not that I believe that, but I think it's a\n\n1:56:21.680 --> 1:56:31.760\n beautiful idea. I mean, you kind of returned to this idea with the Turing test of intelligence\n\n1:56:31.760 --> 1:56:47.280\n being the process of asking and answering what is intelligence. I mean, do you think there is an\n\n1:56:47.280 --> 1:56:57.280\n answer? Why is there such a search for an answer? So does there have to be like an answer? You just\n\n1:56:57.280 --> 1:57:02.880\n said an AI system that's trying to understand the why of what, you know, understand itself.\n\n1:57:04.880 --> 1:57:09.920\n Is that a fundamental process of greater and greater complexity, greater and greater\n\n1:57:09.920 --> 1:57:13.840\n intelligence is the continuous trying of understanding itself?\n\n1:57:13.840 --> 1:57:18.080\n No, I think you will find that most people don't care about that because they're well adjusted\n\n1:57:18.080 --> 1:57:23.200\n enough to not care. And the reason why people like you and me care about it probably has to\n\n1:57:23.200 --> 1:57:28.000\n do with the need to understand ourselves. It's because we are in fundamental disagreement with\n\n1:57:28.000 --> 1:57:32.720\n the universe that we wake up in. They look down on me and they see, oh my God, I'm caught in a\n\n1:57:32.720 --> 1:57:38.400\n monkey. What's that? Some people are unhappy with the government and I'm unhappy with the entire\n\n1:57:38.400 --> 1:57:45.120\n universe that I find myself in. Oh, so you don't think that's a fundamental aspect of human nature\n\n1:57:45.120 --> 1:57:51.200\n that some people are just suppressing? That they wake up shocked they're in the body of a monkey?\n\n1:57:51.200 --> 1:57:55.920\n No, there is a clear adaptive value to not be confused by that and by...\n\n1:57:56.960 --> 1:58:04.880\n Well, no, that's not what I asked. So you have this clear adaptive value, then there's clear\n\n1:58:04.880 --> 1:58:09.680\n there's clear adaptive value to while fundamentally your brain is confused by that,\n\n1:58:09.680 --> 1:58:16.800\n by creating an illusion, another layer of the narrative that says, you know, that tries to\n\n1:58:16.800 --> 1:58:21.120\n suppress that and instead say that, you know, what's going on with the government right now\n\n1:58:21.120 --> 1:58:24.480\n is the most important thing. What's going on with my football team is the most important thing.\n\n1:58:25.120 --> 1:58:32.000\n But it seems to me, like for me, it was a really interesting moment reading Ernest\n\n1:58:32.000 --> 1:58:40.640\n Becker's Denial of Death. That, you know, this kind of idea that we're all, you know,\n\n1:58:40.640 --> 1:58:49.680\n the fundamental thing from which most of our human mind springs is this fear of mortality\n\n1:58:49.680 --> 1:58:54.720\n and being cognizant of your mortality and the fear of that mortality. And then you construct\n\n1:58:54.720 --> 1:59:03.600\n illusions on top of that. I guess you being just a push on it, you really don't think it's possible\n\n1:59:03.600 --> 1:59:11.280\n that this worry of the big existential questions is actually fundamental as the existentialist\n\n1:59:11.280 --> 1:59:17.040\n thought to our existence. I think that the fear of death only plays a role as long as you don't\n\n1:59:17.040 --> 1:59:22.560\n see the big picture. The thing is that minds are software states, right? Software doesn't have\n\n1:59:22.560 --> 1:59:30.880\n identity. Software in some sense is a physical law. But it feels like there's an identity. I\n\n1:59:30.880 --> 1:59:35.440\n thought that was the for this particular piece of software and the narrative it tells, that's\n\n1:59:35.440 --> 1:59:41.280\n a fundamental property of it. The maintenance of the identity is not terminal. It's instrumental\n\n1:59:41.280 --> 1:59:46.320\n to something else. You maintain your identity so you can serve your meaning. So you can do the\n\n1:59:46.320 --> 1:59:51.120\n things that you're supposed to do before you die. And I suspect that for most people the fear of\n\n1:59:51.120 --> 1:59:54.800\n death is the fear of dying before they are done with the things that they feel they have to do,\n\n1:59:54.800 --> 1:59:57.760\n even though they cannot quite put their finger on it, what that is.\n\n1:59:59.280 --> 2:00:07.280\n Right. But in the software world, to return to the question, then what happens after we die?\n\n2:00:10.240 --> 2:00:14.960\n Why would you care? You will not be longer there. The point of dying is that you are gone.\n\n2:00:14.960 --> 2:00:21.040\n Well, maybe I'm not. This is what, you know, it seems like there's so much,\n\n2:00:23.040 --> 2:00:28.800\n in the idea that this is just, the mind is just a simulation that's constructing a narrative around\n\n2:00:28.800 --> 2:00:37.280\n some particular aspects of the quantum mechanical wave function world that we can't quite get direct\n\n2:00:37.280 --> 2:00:44.320\n access to. Then like the idea of mortality seems to be a little fuzzy as well. It doesn't, maybe\n\n2:00:44.320 --> 2:00:49.520\n there's not a clear answer. The fuzzy idea is the one of continuous existence. We don't have\n\n2:00:49.520 --> 2:00:55.840\n continuous existence. How do you know that? Because it's not computable. Because you're\n\n2:00:55.840 --> 2:00:58.400\n saying it's going to be directly infinite. There is no continuous process. The only thing that\n\n2:00:58.400 --> 2:01:02.080\n binds you together with the Lex Friedman from yesterday is the illusion that you have memories\n\n2:01:02.080 --> 2:01:06.400\n about him. So if you want to upload, it's very easy. You make a machine that thinks it's you.\n\n2:01:07.120 --> 2:01:10.000\n Because this is the same thing that you are. You are a machine that thinks it's you.\n\n2:01:10.000 --> 2:01:15.600\n But that's immortality. Yeah, but it's just a belief. You can create this belief very easily\n\n2:01:15.600 --> 2:01:21.840\n once you realize that the question whether you are immortal or not depends entirely on your beliefs\n\n2:01:21.840 --> 2:01:28.160\n and your own continuity. But then you can be immortal by the continuity of the belief.\n\n2:01:28.880 --> 2:01:33.680\n You cannot be immortal, but you can stop being afraid of your mortality because you realize you\n\n2:01:33.680 --> 2:01:39.600\n were never continuously existing in the first place. Well, I don't know if I'd be more terrified\n\n2:01:39.600 --> 2:01:43.120\n or less terrified by that. It seems like the fact that I existed.\n\n2:01:44.160 --> 2:01:47.840\n You don't know this state in which you don't have a self. You can't turn off yourself.\n\n2:01:49.280 --> 2:01:50.320\n I can't turn off myself.\n\n2:01:50.320 --> 2:01:52.080\n You can't turn it off. You can't turn it off.\n\n2:01:52.080 --> 2:01:52.640\n I can.\n\n2:01:52.640 --> 2:01:57.120\n Yes. And you can basically meditate yourself in a state where you are still conscious,\n\n2:01:57.120 --> 2:02:00.800\n where still things are happening, where you know everything that you knew before,\n\n2:02:00.800 --> 2:02:03.120\n but you're no longer identified with changing anything.\n\n2:02:03.120 --> 2:02:09.120\n And this means that yourself, in a way, dissolves. There is no longer this person. You know that this\n\n2:02:09.120 --> 2:02:15.520\n person construct exists in other states and it runs on this brain of Lex Friedman, but it's not\n\n2:02:15.520 --> 2:02:21.520\n a real thing. It's a construct. It's an idea. And you can change that idea. And if you let go of\n\n2:02:21.520 --> 2:02:26.800\n this idea, if you don't think that you are special, you realize it's just one of many people and it's\n\n2:02:26.800 --> 2:02:31.760\n not your favorite person even. It's just one of many. And it's the one that you are doomed to\n\n2:02:31.760 --> 2:02:37.360\n control for the most part. And that is basically informing the actions of this organism as a\n\n2:02:37.360 --> 2:02:42.640\n control model. And this is all there is. And you are somehow afraid that this control model gets\n\n2:02:42.640 --> 2:02:46.640\n interrupted or loses the identity of continuity.\n\n2:02:47.200 --> 2:02:52.880\n Yeah. So I'm attached. I mean, yeah, it's a very popular, it's a somehow compelling notion that\n\n2:02:52.880 --> 2:02:58.720\n being attached, like there's no need to be attached to this idea of an identity.\n\n2:02:58.720 --> 2:03:03.040\n But that in itself could be an illusion that you construct. So the process of meditation,\n\n2:03:03.040 --> 2:03:08.480\n while popular, is thought of as getting under the concept of identity. It could be just putting a\n\n2:03:08.480 --> 2:03:18.240\n cloak over it, just telling it to be quiet for the moment. I think that meditation is eventually just\n\n2:03:18.240 --> 2:03:22.720\n a bunch of techniques that let you control attention. And when you can control attention,\n\n2:03:22.720 --> 2:03:31.200\n you can get access to your own source code, hopefully not before you understand what you're\n\n2:03:31.200 --> 2:03:35.200\n doing. And then you can change the way it works temporarily or permanently.\n\n2:03:36.000 --> 2:03:41.920\n So yeah, meditation is to get a glimpse at the source code, get under, so basically control or\n\n2:03:41.920 --> 2:03:42.720\n turn off the attention.\n\n2:03:42.720 --> 2:03:46.240\n The entire thing is that you learn to control attention. So everything else is downstream\n\n2:03:46.240 --> 2:03:47.440\n from controlling attention.\n\n2:03:47.440 --> 2:03:50.880\n And control the attention that's looking at the attention.\n\n2:03:50.880 --> 2:03:54.720\n Normally we only get attention in the parts of our mind that create heat, where you have a\n\n2:03:54.720 --> 2:04:00.240\n mismatch between model and the results that are happening. And so most people are not self aware\n\n2:04:00.240 --> 2:04:05.280\n because their control is too good. If everything works out roughly the way you want, and the only\n\n2:04:05.280 --> 2:04:09.920\n things that don't work out is whether your football team wins, then you will mostly have\n\n2:04:09.920 --> 2:04:15.120\n models about these domains. And it's only when, for instance, your fundamental relationships to\n\n2:04:15.120 --> 2:04:20.080\n the world around you don't work, because the ideology of your country is insane, and you don't\n\n2:04:20.080 --> 2:04:24.160\n understand why it's insane, and the other kids are not nerds, and don't understand why you\n\n2:04:24.160 --> 2:04:29.280\n understand physics, and you don't, why you want to understand physics, and you don't understand\n\n2:04:29.280 --> 2:04:31.200\n why somebody would not want to understand physics.\n\n2:04:32.640 --> 2:04:38.320\n So we kind of brought up neurons in the brain as reinforcement learning agents.\n\n2:04:40.480 --> 2:04:46.240\n And there's been some successes as you brought up with Go, with AlphaGo, AlphaZero, with ideas\n\n2:04:46.240 --> 2:04:51.440\n which I think are incredibly interesting ideas of systems playing each other in an automated way\n\n2:04:52.400 --> 2:05:00.560\n to improve by playing other systems in a particular construct of a game that are a little\n\n2:05:00.560 --> 2:05:05.360\n bit better than itself, and then thereby improving continuously. All the competitors in the game\n\n2:05:05.920 --> 2:05:11.520\n are improving gradually. So being just challenging enough and from learning from the process of the\n\n2:05:11.520 --> 2:05:16.720\n competition. Do you have hope for that reinforcement learning process to achieve\n\n2:05:16.720 --> 2:05:21.200\n greater and greater level of intelligence? So we talked about different ideas in AI that need to\n\n2:05:21.200 --> 2:05:28.480\n be solved. Is RL a part of that process of trying to create an AGI system? What do you think?\n\n2:05:28.480 --> 2:05:32.800\n Definitely forms of unsupervised learning, but there are many algorithms that can achieve that.\n\n2:05:32.800 --> 2:05:38.800\n And I suspect that ultimately the algorithms that work, there will be a class of them or many of\n\n2:05:38.800 --> 2:05:45.120\n them. And they might have small differences of like a magnitude and efficiency, but eventually\n\n2:05:45.120 --> 2:05:49.760\n what matters is the type of model that you form and the types of models that we form right now\n\n2:05:49.760 --> 2:05:59.200\n are not sparse enough. What does it mean to be sparse? It means that ideally every potential\n\n2:05:59.200 --> 2:06:06.000\n model state should correspond to a potential world state. So basically if you vary states\n\n2:06:06.000 --> 2:06:10.400\n in your model, you always end up with valid world states and our mind is not quite there.\n\n2:06:10.400 --> 2:06:14.960\n So an indication is basically what we see in dreams. The older we get, the more boring our\n\n2:06:14.960 --> 2:06:19.840\n dreams become because we incorporate more and more constraints that we learned about how the\n\n2:06:19.840 --> 2:06:25.280\n world works. So many of the things that we imagine to be possible as children turn out to be\n\n2:06:25.280 --> 2:06:31.600\n constrained by physical and social dynamics. And as a result, fewer and fewer things remain\n\n2:06:31.600 --> 2:06:36.640\n possible. It's not because our imagination scales back, but the constraints under which it operates\n\n2:06:36.640 --> 2:06:42.800\n become tighter and tighter. And so the constraints under which our neural networks operate are\n\n2:06:42.800 --> 2:06:47.360\n almost limitless, which means it's very difficult to get a neural network to imagine things that\n\n2:06:47.360 --> 2:06:55.200\n look real. So I suspect part of what we need to do is we probably need to build dreaming systems.\n\n2:06:55.200 --> 2:07:01.280\n I suspect that part of the purpose of dreams is similar to a generative adversarial network,\n\n2:07:01.280 --> 2:07:07.360\n we learn certain constraints and then it produces alternative perspectives on the same set of\n\n2:07:07.360 --> 2:07:11.520\n constraints. So you can recognize it under different circumstances. Maybe we have flying\n\n2:07:11.520 --> 2:07:16.080\n dreams as children because we recreate the objects that we know and the maps that we know from\n\n2:07:16.080 --> 2:07:21.040\n different perspectives, which also means from a bird's eye perspective. So I mean, aren't we\n\n2:07:21.040 --> 2:07:27.200\n doing that anyway? I mean, not with our eyes closed and when we're sleeping, aren't we just\n\n2:07:27.200 --> 2:07:32.560\n constantly running dreams and simulations in our mind as we try to interpret the environment?\n\n2:07:32.560 --> 2:07:37.200\n I mean, sort of considering all the different possibilities, the way we interact with the\n\n2:07:37.200 --> 2:07:46.000\n environment seems like, essentially, like you said, sort of creating a bunch of simulations\n\n2:07:46.000 --> 2:07:52.240\n that are consistent with our expectations, with our previous experiences, with the things we just\n\n2:07:52.240 --> 2:08:02.400\n saw recently. And through that hallucination process, we are able to then somehow stitch\n\n2:08:02.400 --> 2:08:07.760\n together what actually we see in the world with the simulations that match it well and thereby\n\n2:08:07.760 --> 2:08:12.720\n interpret it. I suspect that you and my brain are slightly unusual in this regard,\n\n2:08:13.440 --> 2:08:19.520\n which is probably what got you into MIT. So this obsession of constantly pondering possibilities\n\n2:08:19.520 --> 2:08:27.520\n and solutions to problems. Oh, stop it. I think I'm not talking about intellectual stuff. I'm\n\n2:08:27.520 --> 2:08:35.040\n talking about just doing the kind of stuff it takes to walk and not fall. Yes, this is\n\n2:08:35.040 --> 2:08:43.600\n largely automatic. Yes, but the process is, I mean... It's not complicated. It's relatively\n\n2:08:43.600 --> 2:08:48.480\n easy to build a neural network that, in some sense, learns the dynamics. The fact that we\n\n2:08:48.480 --> 2:08:52.560\n haven't done it right so far doesn't mean it's hard, because you can see that a biological\n\n2:08:52.560 --> 2:08:57.360\n organism does it with relatively few neurons. So basically, you build a bunch of neural\n\n2:08:57.360 --> 2:09:01.760\n oscillators that entrain themselves with the dynamics of your body in such a way that the\n\n2:09:01.760 --> 2:09:06.880\n regulator becomes isomorphic in its model to the dynamics that it regulates, and then it's\n\n2:09:06.880 --> 2:09:11.440\n automatic. And it's only interesting in the sense that it captures attention when the system is off.\n\n2:09:12.160 --> 2:09:18.000\n See, but thinking of the kind of mechanism that's required to do walking as a controller,\n\n2:09:18.000 --> 2:09:26.960\n as a neural network, I think it's a compelling notion, but it discards quietly,\n\n2:09:27.840 --> 2:09:33.040\n or at least makes implicit, the fact that you need to have something like common sense reasoning\n\n2:09:33.040 --> 2:09:40.400\n to walk. It's an open question whether you do or not. But my intuition is to act in this world,\n\n2:09:40.400 --> 2:09:46.400\n there's a huge knowledge base that's underlying it somehow. There's so much information\n\n2:09:46.400 --> 2:09:53.520\n of the kind we have never been able to construct in neural networks in an artificial intelligence\n\n2:09:53.520 --> 2:10:00.000\n systems period, which is like, it's humbling, at least in my imagination, the amount of information\n\n2:10:00.000 --> 2:10:07.600\n required to act in this world humbles me. And I think saying that neural networks can accomplish\n\n2:10:07.600 --> 2:10:16.560\n it is missing the fact that we don't have yet a mechanism for constructing something like\n\n2:10:16.560 --> 2:10:28.480\n common sense reasoning. I mean, what's your sense about to linger on the idea of what kind of\n\n2:10:28.480 --> 2:10:33.920\n mechanism would be effective at walking? You said just a neural network, not maybe the kind we have,\n\n2:10:33.920 --> 2:10:40.080\n but something a little bit better, would be able to walk easily. Don't you think it also needs to know\n\n2:10:42.080 --> 2:10:47.360\n like a huge amount of knowledge that's represented under the flag of common sense reasoning?\n\n2:10:47.360 --> 2:10:51.680\n How much common sense knowledge do we actually have? Imagine that you are really hardworking\n\n2:10:51.680 --> 2:10:56.640\n for all your life and you form two new concepts every half hour or so. You end up with something\n\n2:10:56.640 --> 2:11:02.320\n like a million concepts because you don't get that old. So a million concepts, that's not a lot.\n\n2:11:02.320 --> 2:11:06.800\n So it's not just a million concepts. I think it would be a lot. I personally think it might be\n\n2:11:06.800 --> 2:11:11.520\n much more than a million. But if you think just about the numbers, you don't live that long.\n\n2:11:12.320 --> 2:11:16.480\n If you think about how many cycles do your neurons have in your life, it's quite limited.\n\n2:11:16.480 --> 2:11:22.880\n You don't get that old. Yeah, but the powerful thing is the number of concepts, and they're\n\n2:11:23.440 --> 2:11:29.200\n probably deeply hierarchical in nature. The relations, as you described between them,\n\n2:11:29.200 --> 2:11:35.120\n is the key thing. So it's like, even if it's a million concepts, the graph of relations that's\n\n2:11:35.120 --> 2:11:42.800\n formed and some kind of, perhaps, some kind of probabilistic relationships, that's what's common\n\n2:11:42.800 --> 2:11:48.960\n sense reasoning is the relationship between things. Yeah, so in some sense, I think of the concepts as\n\n2:11:48.960 --> 2:11:53.920\n the address space for our behavior programs. And the behavior programs allow us to recognize objects\n\n2:11:53.920 --> 2:11:59.840\n and interact with them, also mental objects. And a large part of that is the physical world that we\n\n2:11:59.840 --> 2:12:04.640\n interact with, which is this RAS extender thing, which is basically navigation of information in\n\n2:12:04.640 --> 2:12:11.920\n space. And basically, it's similar to a game engine. It's a physics engine that you can use to\n\n2:12:12.480 --> 2:12:18.080\n describe and predict how things that look in a particular way, that feel when you touch them in\n\n2:12:18.080 --> 2:12:22.480\n a particular way, that love proprioception, that love auditory, for example. So it's a\n\n2:12:22.480 --> 2:12:27.200\n lot of auditory perception and so on, how they work out. So basically, the geometry of all these\n\n2:12:27.200 --> 2:12:33.600\n things. And this is probably 80% of what our brain is doing is dealing with that, with this real time\n\n2:12:33.600 --> 2:12:39.360\n simulation. And by itself, a game engine is fascinating, but it's not that hard to understand\n\n2:12:39.360 --> 2:12:47.040\n what it's doing. And our game engines are already, in some sense, approximating the fidelity of what\n\n2:12:47.040 --> 2:12:54.000\n we can perceive. So if we put on an Oculus Quest, we get something that is still relatively crude\n\n2:12:54.000 --> 2:12:58.480\n with respect to what we can perceive, but it's also in the same ballpark already. It's just a\n\n2:12:58.480 --> 2:13:04.640\n couple order of magnitudes away from saturating our perception in terms of the complexity that\n\n2:13:04.640 --> 2:13:10.800\n it can produce. So in some sense, it's reasonable to say that the computer that you can buy and put\n\n2:13:10.800 --> 2:13:15.600\n into your home is able to give a perceptual reality that has a detail that is already in\n\n2:13:15.600 --> 2:13:22.080\n the same ballpark as what your brain can process. And everything else are ideas about the world.\n\n2:13:22.080 --> 2:13:27.040\n And I suspect that they are relatively sparse and also the intuitive models that we form about\n\n2:13:27.040 --> 2:13:32.480\n social interaction. Social interaction is not so hard. It's just hard for us nerds because we all\n\n2:13:32.480 --> 2:13:37.760\n have our wires crossed, so we need to deduce them. But the pyres are present in most social animals.\n\n2:13:37.760 --> 2:13:44.560\n So it's interesting thing to notice that many domestic social animals, like cats and dogs,\n\n2:13:44.560 --> 2:13:51.440\n have better social cognition than children. Right. I hope so. I hope it's not that many concepts\n\n2:13:51.440 --> 2:13:57.760\n fundamentally to do to exist in this world. For me, it's more like I'm afraid so because this\n\n2:13:57.760 --> 2:14:02.560\n thing that we only appear to be so complex to each other because we are so stupid is a little\n\n2:14:02.560 --> 2:14:11.360\n bit depressing. Yeah, to me that's inspiring if we're indeed as stupid as it seems. The things\n\n2:14:11.360 --> 2:14:15.920\n our brains don't scale and the information processing that we build tend to scale very well.\n\n2:14:16.800 --> 2:14:23.840\n Yeah, but I mean, one of the things that worries me is that the fact that the brain doesn't scale\n\n2:14:23.840 --> 2:14:30.000\n means that that's actually a fundamental feature of the brain. All the flaws of the brain,\n\n2:14:30.000 --> 2:14:34.480\n everything we see that we see as limitations, perhaps there's a fundamental, the constraints\n\n2:14:34.480 --> 2:14:43.040\n on the system could be a requirement of its power, which is different than our current\n\n2:14:43.040 --> 2:14:48.080\n understanding of intelligent systems where scale, especially with deep learning, especially with\n\n2:14:48.080 --> 2:14:55.440\n reinforcement learning, the hope behind OpenAI and DeepMind, all the major results really have\n\n2:14:55.440 --> 2:15:01.680\n to do with huge compute. It could also be that our brains are so small, not just because they\n\n2:15:01.680 --> 2:15:07.520\n take up so much glucose in our body, like 20% of the glucose, so they don't arbitrarily scale.\n\n2:15:07.520 --> 2:15:11.760\n There's some animals like elephants which have larger brains than us and they don't seem to be\n\n2:15:11.760 --> 2:15:16.560\n smarter. Elephants seem to be autistic. They have very, very good motor control and they're really\n\n2:15:16.560 --> 2:15:20.480\n good with details, but they really struggle to see the big picture. So you can make them\n\n2:15:21.120 --> 2:15:27.200\n recreate drawings stroke by stroke, they can do that, but they cannot reproduce a still life. So\n\n2:15:27.200 --> 2:15:31.840\n they cannot make a drawing of a scene that they see. They will always be only able to reproduce\n\n2:15:31.840 --> 2:15:37.120\n the line drawing, at least as far from what I could see in the experiments. So why is that?\n\n2:15:37.120 --> 2:15:41.680\n Maybe smarter elephants would meditate themselves out of existence because their brains are too\n\n2:15:41.680 --> 2:15:45.760\n large. So basically the elephants that were not autistic, they didn't reproduce.\n\n2:15:46.400 --> 2:15:50.400\n Yeah. So we have to remember that the brain is fundamentally interlinked with the body in our\n\n2:15:50.400 --> 2:15:55.440\n human and biological system. Do you think that AGI systems that we try to create or greater\n\n2:15:55.440 --> 2:16:00.960\n intelligent systems would need to have a body? I think they should be able to make use of a body\n\n2:16:00.960 --> 2:16:06.080\n if you give it to them. But I don't think that they fundamentally need a body. So I suspect if\n\n2:16:06.080 --> 2:16:11.840\n you can interact with the world by moving your eyes and your head, you can make controlled\n\n2:16:11.840 --> 2:16:19.920\n experiments. And this allows you to have many magnitudes, fewer observations in order to reduce\n\n2:16:19.920 --> 2:16:24.000\n the uncertainty in your models. So you can pinpoint the areas in your models where you're\n\n2:16:24.000 --> 2:16:28.480\n not quite sure and you just move your head and see what's going on over there and you get additional\n\n2:16:28.480 --> 2:16:33.680\n information. If you just have to use YouTube as an input and you cannot do anything beyond this,\n\n2:16:33.680 --> 2:16:39.680\n you probably need just much more data. But we have much more data. So if you can build a system that\n\n2:16:39.680 --> 2:16:44.320\n has enough time and attention to browse all of YouTube and extract all the information that there\n\n2:16:44.320 --> 2:16:50.080\n is to be found, I don't think there's an obvious limit to what it can do. Yeah, but it seems that\n\n2:16:50.080 --> 2:16:55.280\n the interactivity is a fundamental thing that the physical body allows you to do. But let me ask on\n\n2:16:55.280 --> 2:17:00.640\n that topic sort of that that's what a body is, is allowing the brain to like touch things and move\n\n2:17:00.640 --> 2:17:06.800\n things and interact with the whether the physical world exists or not, whatever, but interact with\n\n2:17:06.800 --> 2:17:14.960\n some interface to the physical world. What about a virtual world? Do you think we can do the same kind\n\n2:17:14.960 --> 2:17:23.760\n of reasoning, consciousness, intelligence if we put on a VR headset and move over to that world?\n\n2:17:23.760 --> 2:17:28.080\n Do you think there's any fundamental difference between the interface to the physical world that\n\n2:17:28.080 --> 2:17:32.960\n it's here in this hotel and if we were sitting in the same hotel in a virtual world? The question\n\n2:17:32.960 --> 2:17:39.360\n is, does this nonphysical world or this other environment entice you to solve problems that\n\n2:17:39.360 --> 2:17:44.800\n require general intelligence? If it doesn't, then you probably will not develop general intelligence\n\n2:17:44.800 --> 2:17:48.240\n and arguably most people are not generally intelligent because they don't have to solve\n\n2:17:48.240 --> 2:17:52.880\n problems that make them generally intelligent. And even for us, it's not yet clear if we are smart\n\n2:17:52.880 --> 2:17:58.720\n enough to build AI and understand our own nature to this degree. So it could be a matter of capacity\n\n2:17:58.720 --> 2:18:02.320\n and for most people, it's in the first place a matter of interest. They don't see the point\n\n2:18:02.320 --> 2:18:06.960\n because the benefit of attempting this project are marginal because you're probably not going\n\n2:18:06.960 --> 2:18:11.920\n to succeed in it and the cost of trying to do it requires complete dedication of your entire life.\n\n2:18:11.920 --> 2:18:15.760\n Right? But it seems like the possibilities of what you can do in the virtual world,\n\n2:18:15.760 --> 2:18:21.200\n so imagine that is much greater than you can in the real world. So imagine a situation,\n\n2:18:21.200 --> 2:18:27.120\n maybe interesting option for me. If somebody came to me and offered what I'll do is,\n\n2:18:27.840 --> 2:18:34.720\n so from now on, you can only exist in the virtual world. And so you put on this headset and when you\n\n2:18:34.720 --> 2:18:41.040\n eat, we'll make sure to connect your body up in a way that when you eat in the virtual world,\n\n2:18:41.040 --> 2:18:45.760\n your body will be nourished in the same way in the virtual world. So the aligning incentives\n\n2:18:45.760 --> 2:18:50.880\n between our common sort of real world and the virtual world, but then the possibilities become\n\n2:18:50.880 --> 2:18:57.120\n much bigger. Like I could be other kinds of creatures. I could do, I can break the laws\n\n2:18:57.120 --> 2:19:02.320\n of physics as we know them. I could do a lot. I mean, the possibilities are endless, right? As far\n\n2:19:02.320 --> 2:19:08.640\n as we think it's an interesting thought, whether like what existence would be like, what kind of\n\n2:19:08.640 --> 2:19:13.840\n intelligence would emerge there? What kind of consciousness? What kind of maybe greater\n\n2:19:13.840 --> 2:19:19.920\n intelligence, even in me, Lex, even at this stage in my life, if I spend the next 20 years in that\n\n2:19:19.920 --> 2:19:26.080\n world to see how that intelligence emerges. And if that happened at the very beginning,\n\n2:19:26.080 --> 2:19:31.040\n before I was even cognizant of my existence in this physical world, it's interesting to think\n\n2:19:31.040 --> 2:19:37.440\n how that child would develop. And the way virtual reality and digitization of everything is moving,\n\n2:19:37.440 --> 2:19:43.920\n it's not completely out of the realm of possibility that we're all, that some part of our lives will,\n\n2:19:44.800 --> 2:19:51.360\n if not entirety of it, will live in a virtual world to a greater degree than we currently have\n\n2:19:51.360 --> 2:19:56.800\n living on Twitter and social media and so on. Do you have, I mean, does something draw you\n\n2:19:56.800 --> 2:20:03.360\n intellectually or naturally in terms of thinking about AI to this virtual world where more\n\n2:20:03.360 --> 2:20:09.200\n possibilities are? I think that currently it's a waste of time to deal with the physical world\n\n2:20:09.200 --> 2:20:12.480\n before we have mechanisms that can automatically learn how to deal with it.\n\n2:20:13.280 --> 2:20:18.320\n The body gives you second order agency, but what constitutes the body is the things that\n\n2:20:18.320 --> 2:20:24.080\n you can indirectly control. The third order are tools, and the second order is the things that\n\n2:20:24.080 --> 2:20:29.680\n are basically always present, but you operate on them with first order things, which are mental\n\n2:20:29.680 --> 2:20:36.800\n operators. And the zero order is in some sense, the direct sense of what you're deciding. Right.\n\n2:20:36.800 --> 2:20:42.000\n So you observe yourself initiating an action, there are features that you interpret as the\n\n2:20:42.000 --> 2:20:46.880\n initiation of an action. Then you perform the operations that you perform to make that happen.\n\n2:20:47.520 --> 2:20:52.480\n And then you see the movement of your limbs and you learn to associate those and thereby model\n\n2:20:52.480 --> 2:20:56.800\n your own agency over this feedback, right? But the first feedback that you get is from this first\n\n2:20:56.800 --> 2:21:01.280\n order thing already. Basically, you decide to think a thought and the thought is being thought.\n\n2:21:01.280 --> 2:21:05.200\n You decide to change the thought and you observe how the thought is being changed.\n\n2:21:05.200 --> 2:21:10.000\n And in some sense, this is, you could say, an embodiment already, right? And I suspect it's\n\n2:21:10.000 --> 2:21:14.560\n sufficient as an embodiment for intelligence. And so it's not that important at least at\n\n2:21:14.560 --> 2:21:19.200\n this time to consider variations in the second order. Yes. But the thing that you also put\n\n2:21:20.800 --> 2:21:24.800\n mentioned just now is physics that you could change in any way you want.\n\n2:21:24.800 --> 2:21:29.920\n So you need an environment that puts up resistance against you. If there's nothing to control,\n\n2:21:29.920 --> 2:21:33.840\n you cannot make models, right? There needs to be a particular way that resists you.\n\n2:21:34.560 --> 2:21:38.800\n And by the way, your motivation is usually outside of your mind. It resists you. Motivation\n\n2:21:38.800 --> 2:21:43.040\n is what gets you up in the morning even though it would be much less work to stay in bed.\n\n2:21:43.840 --> 2:21:51.760\n So it's basically forcing you to resist the environment and it forces your mind to serve it,\n\n2:21:51.760 --> 2:21:56.640\n to serve this resistance to the environment. So in some sense, it is also putting up resistance\n\n2:21:56.640 --> 2:22:01.200\n against the natural tendency of the mind to not do anything. Yeah. So some of that resistance,\n\n2:22:01.200 --> 2:22:05.520\n just like you described with motivation is like in the first order, it's in the mind.\n\n2:22:05.520 --> 2:22:11.040\n Some resistance is in the second order, like actual physical objects pushing against you and so on.\n\n2:22:11.040 --> 2:22:14.480\n It seems that the second order stuff in virtual reality could be recreated.\n\n2:22:14.480 --> 2:22:19.360\n Of course. But it might be sufficient that you just do mathematics and mathematics is already\n\n2:22:19.360 --> 2:22:24.160\n putting up enough resistance against you. So basically just with an aesthetic motive,\n\n2:22:24.160 --> 2:22:29.760\n this could maybe sufficient to form a type of intelligence. It would probably not be a very\n\n2:22:29.760 --> 2:22:37.760\n human intelligence, but it might be one that is already general. So to mess with this zero order,\n\n2:22:37.760 --> 2:22:43.040\n maybe first order, what do you think about ideas of brain computer interfaces? So again, returning\n\n2:22:43.040 --> 2:22:48.160\n to our friend Elon Musk and Neuralink, a company that's trying to, of course, there's a lot of\n\n2:22:48.160 --> 2:22:54.240\n a trying to cure diseases and so on with a near term, but the longterm vision is to add an extra\n\n2:22:54.240 --> 2:23:00.640\n layer to basically expand the capacity of the brain connected to the computational world.\n\n2:23:03.120 --> 2:23:07.120\n Do you think one that's possible too, how does that change the fundamentals of the zeroth order\n\n2:23:07.120 --> 2:23:11.360\n in the first order? It's technically possible, but I don't see that the FDA would ever allow me to\n\n2:23:11.360 --> 2:23:16.480\n drill holes in my skull to interface my neocortex the way Elon Musk envisions. So at the moment,\n\n2:23:16.480 --> 2:23:21.760\n I can do horrible things to mice, but I'm not able to do useful things to people,\n\n2:23:21.760 --> 2:23:26.960\n except maybe at some point down the line in medical applications. So this thing that we\n\n2:23:26.960 --> 2:23:32.880\n are envisioning, which means recreational and creational brain computer interfaces\n\n2:23:33.440 --> 2:23:35.760\n are probably not going to happen in the present legal system.\n\n2:23:36.560 --> 2:23:43.200\n I love it how I'm asking you out there philosophical and sort of engineering\n\n2:23:43.200 --> 2:23:48.000\n questions. And for the first time ever, you jumped to the legal FDA.\n\n2:23:48.000 --> 2:23:51.760\n There would be enough people that would be crazy enough to have holes drilled in their skull to\n\n2:23:51.760 --> 2:23:57.680\n try a new type of brain computer interface. But also, if it works, FDA will approve it.\n\n2:23:57.680 --> 2:24:02.560\n I mean, yes, it's like, you know, I work a lot with autonomous vehicles. Yes,\n\n2:24:02.560 --> 2:24:05.760\n you can say that it's going to be a very difficult regulatory process of approving\n\n2:24:05.760 --> 2:24:08.800\n autonomous, but it doesn't mean autonomous vehicles are never going to happen.\n\n2:24:08.800 --> 2:24:14.080\n No, they will totally happen as soon as we create jobs for at least two lawyers\n\n2:24:14.080 --> 2:24:15.520\n and one regulator per car.\n\n2:24:17.120 --> 2:24:24.800\n Yes, lawyers, that's actually like lawyers is the fundamental substrate of reality.\n\n2:24:24.800 --> 2:24:30.400\n In the US, it's a very weird system. It's not universal in the world. The law is a very\n\n2:24:30.400 --> 2:24:34.880\n interesting software once you realize it, right? These circuits are in some sense streams of\n\n2:24:34.880 --> 2:24:39.600\n software and this is largely works by exception handling. So you make decisions on the ground\n\n2:24:39.600 --> 2:24:43.120\n and they get synchronized with the next level structure as soon as an exception is being\n\n2:24:43.120 --> 2:24:49.600\n thrown. So it escalates the exception handling. The process is very expensive,\n\n2:24:49.600 --> 2:24:54.960\n especially since it incentivizes the lawyers for producing work for lawyers.\n\n2:24:54.960 --> 2:25:02.960\n Yes, so the exceptions are actually incentivized for firing often. But to return, outside of\n\n2:25:02.960 --> 2:25:13.440\n lawyers, is there anything interesting, insightful about the possibility of this extra layer\n\n2:25:13.440 --> 2:25:15.280\n of intelligence added to the brain?\n\n2:25:15.280 --> 2:25:20.320\n I do think so, but I don't think that you need technically invasive procedures to do\n\n2:25:20.320 --> 2:25:25.120\n so. We can already interface with other people by observing them very, very closely and getting\n\n2:25:25.120 --> 2:25:31.360\n in some kind of empathetic resonance. And I'm not very good at this, but I noticed that\n\n2:25:31.360 --> 2:25:37.360\n people are able to do this to some degree. And it basically means that we model an interface\n\n2:25:37.360 --> 2:25:42.720\n layer of the other person in real time. And it works despite our neurons being slow because\n\n2:25:42.720 --> 2:25:46.960\n most of the things that we do are built on periodic processes. So you just need to entrain\n\n2:25:46.960 --> 2:25:51.600\n yourself with the oscillation that happens. And if the oscillation itself changes slowly\n\n2:25:51.600 --> 2:25:54.080\n enough, you can basically follow along.\n\n2:25:54.080 --> 2:26:03.360\n Right. But the bandwidth of the interaction, it seems like you can do a lot more computation\n\n2:26:03.360 --> 2:26:04.080\n when there's...\n\n2:26:04.080 --> 2:26:08.240\n Of course. But the other thing is that the bandwidth that our brain, our own mind is\n\n2:26:08.240 --> 2:26:12.320\n running on is actually quite slow. So the number of thoughts that I can productively\n\n2:26:12.320 --> 2:26:18.400\n think in any given day is quite limited. If they had the discipline to write it down\n\n2:26:18.400 --> 2:26:22.400\n and the speed to write it down, maybe it would be a book every day or so. But if you think\n\n2:26:22.400 --> 2:26:28.080\n about the computers that we can build, the magnitudes at which they operate, this would\n\n2:26:28.080 --> 2:26:30.640\n be nothing. It's something that it can put out in a second.\n\n2:26:30.640 --> 2:26:37.120\n Well, I don't know. So it's possible the number of thoughts you have in your brain is... It\n\n2:26:37.120 --> 2:26:41.760\n could be several orders of magnitude higher than what you're possibly able to express\n\n2:26:41.760 --> 2:26:43.360\n through your fingers or through your voice.\n\n2:26:45.040 --> 2:26:47.920\n Most of them are going to be repetitive because they...\n\n2:26:47.920 --> 2:26:48.880\n How do you know that?\n\n2:26:48.880 --> 2:26:53.440\n If they have to control the same problems every day. When I walk, there are going to\n\n2:26:53.440 --> 2:26:58.160\n be processes in my brain that model my walking pattern and regulate them and so on. But it's\n\n2:26:58.160 --> 2:26:59.840\n going to be pretty much the same every day.\n\n2:26:59.840 --> 2:27:00.400\n But that could be...\n\n2:27:00.400 --> 2:27:01.360\n Every step.\n\n2:27:01.360 --> 2:27:04.960\n But I'm talking about intellectual reasoning, thinking. So the question, what is the best\n\n2:27:04.960 --> 2:27:09.360\n system of government? So you sit down and start thinking about that. One of the constraints\n\n2:27:09.360 --> 2:27:16.560\n is that you don't have access to a lot of facts, a lot of studies. You always have to\n\n2:27:16.560 --> 2:27:23.600\n interface with something else to learn more, to aid in your reasoning process. If you can\n\n2:27:23.600 --> 2:27:28.160\n directly access all of Wikipedia in trying to understand what is the best form of government,\n\n2:27:28.160 --> 2:27:33.360\n then every thought won't be stuck in a loop. Every thought that requires some extra piece\n\n2:27:33.360 --> 2:27:38.640\n of information will be able to grab it really quickly. That's the possibility of if the\n\n2:27:38.640 --> 2:27:47.600\n bottleneck is literally the information that... The bottleneck of breakthrough ideas is just\n\n2:27:47.600 --> 2:27:51.840\n being able to quickly access huge amounts of information, then the possibility of connecting\n\n2:27:51.840 --> 2:27:59.760\n your brain to the computer could lead to totally new breakthroughs. You can think of mathematicians\n\n2:27:59.760 --> 2:28:08.240\n being able to just up the orders of magnitude of power in their reasoning about\n\n2:28:08.240 --> 2:28:12.400\n mathematical proofs. What if humanity has already discovered the optimal form of\n\n2:28:12.400 --> 2:28:17.840\n government through an evolutionary process? There is an evolution going on. So what we\n\n2:28:17.840 --> 2:28:23.280\n discover is that maybe the problem of government doesn't have stable solutions for us as a species,\n\n2:28:23.280 --> 2:28:27.840\n because we are not designed in such a way that we can make everybody conform to them.\n\n2:28:28.720 --> 2:28:33.360\n But there could be solutions that work under given circumstances or that are the best for\n\n2:28:33.360 --> 2:28:38.400\n certain environment and depends on, for instance, the primary forms of ownership and the means\n\n2:28:38.400 --> 2:28:45.760\n of production. So if the main means of production is land, then the forms of government will be\n\n2:28:45.760 --> 2:28:50.320\n regulated by the landowners and you get a monarchy. If you also want to have a form of\n\n2:28:50.320 --> 2:28:56.240\n government in which you depend on some form of slavery, for instance, where the peasants have\n\n2:28:56.240 --> 2:29:01.680\n to work very long hours for very little gain, so very few people can have plumbing, then maybe\n\n2:29:01.680 --> 2:29:08.000\n you need to promise them to get paid in the afterlife, the overtime. So you need a theocracy.\n\n2:29:08.560 --> 2:29:14.880\n And so for much of human history in the West, we had a combination of monarchy and theocracy\n\n2:29:14.880 --> 2:29:21.040\n that was our form of governance. At the same time, the Catholic Church implemented game theoretic\n\n2:29:21.040 --> 2:29:27.040\n principles. I recently reread Thomas Aquinas. It's very interesting to see this because he was not\n\n2:29:27.040 --> 2:29:32.560\n dualist. He was translating Aristotle in a particular way for designing an operating\n\n2:29:32.560 --> 2:29:39.440\n system for the Catholic society. And he says that basically people are animals in very much the same\n\n2:29:39.440 --> 2:29:44.320\n way as Aristotle envisions, which is basically organisms with cybernetic control. And then he\n\n2:29:44.320 --> 2:29:48.880\n says that there are additional rational principles that humans can discover and everybody can\n\n2:29:48.880 --> 2:29:53.520\n discover them so they are universal. If you are sane, you should understand, you should submit to\n\n2:29:53.520 --> 2:30:00.240\n them because you can rationally deduce them. And these principles are roughly you should be willing\n\n2:30:00.240 --> 2:30:06.800\n to self regulate correctly. You should be willing to do correct social regulation. It's\n\n2:30:06.800 --> 2:30:13.360\n intraorganismic. You should be willing to act on your models so you have skin in the game.\n\n2:30:17.040 --> 2:30:20.400\n And you should have goal rationality. You should be choosing the right\n\n2:30:20.400 --> 2:30:26.320\n goals to work on. So basically these three rational principles, goal rationality he calls\n\n2:30:26.320 --> 2:30:33.280\n prudence or wisdom, social regulation is justice, the correct social one, and the internal regulation\n\n2:30:33.280 --> 2:30:40.400\n is temperance. And this willingness to act on your models is courage. And then he says that\n\n2:30:40.400 --> 2:30:45.200\n there are additionally to these four cardinal virtues, three divine virtues. And these three\n\n2:30:45.200 --> 2:30:49.920\n divine virtues cannot be rationally deduced, but they reveal themselves by the harmony, which means\n\n2:30:49.920 --> 2:30:54.320\n if you assume them and you extrapolate what's going to happen, you will see that they make sense.\n\n2:30:55.360 --> 2:31:00.080\n And it's often been misunderstood as God has to tell you that these are the things. So basically\n\n2:31:00.080 --> 2:31:05.360\n there's something nefarious going on. The Christian conspiracy forces you to believe\n\n2:31:05.360 --> 2:31:11.840\n some guy with a long beard that they discovered this. So these principles are relatively simple.\n\n2:31:11.840 --> 2:31:16.480\n Again, it's for high level organization for the resulting civilization that you form.\n\n2:31:16.480 --> 2:31:21.520\n A commitment to unity. So basically you serve this higher, larger thing,\n\n2:31:21.520 --> 2:31:28.480\n this structural principle on the next level. And he calls that faith. Then there needs to be a\n\n2:31:28.480 --> 2:31:32.160\n commitment to shared purpose. This is basically this global reward that you try to figure out\n\n2:31:32.160 --> 2:31:36.000\n what that should be and how you can facilitate this. And this is love. The commitment to shared\n\n2:31:36.000 --> 2:31:40.960\n purpose is the core of love, right? You see the sacred thing that is more important than your own\n\n2:31:40.960 --> 2:31:45.840\n organismic interests in the other, and you serve this together. And this is how you see the sacred\n\n2:31:45.840 --> 2:31:50.880\n in the other. And the last one is hope, which means you need to be willing to act on that\n\n2:31:51.520 --> 2:31:55.520\n principle without getting rewards in the here and now because it doesn't exist yet.\n\n2:31:55.520 --> 2:31:59.520\n Then you start out building the civilization, right? So you need to be able to do this in the\n\n2:31:59.520 --> 2:32:06.640\n absence of its actual existence yet. So it can come into being. So the way it comes into being\n\n2:32:06.640 --> 2:32:12.960\n is by you accepting those notions and then you see these three divine concepts and you see them\n\n2:32:12.960 --> 2:32:18.800\n realized. Divine is a loaded concept in our world because we are outside of this cult and we are\n\n2:32:18.800 --> 2:32:23.520\n still scarred from breaking free of it. But the idea is basically we need to have a civilization\n\n2:32:23.520 --> 2:32:28.960\n that acts as an intentional agent, like an insect state. And we are not actually a tribal species,\n\n2:32:28.960 --> 2:32:35.520\n we are a state building species. And what enables state building is basically the formation of\n\n2:32:35.520 --> 2:32:40.240\n religious states and other forms of rule based administration in which the individual doesn't\n\n2:32:40.240 --> 2:32:45.600\n matter as much as the rule or the higher goal. We got there by the question, what's the optimal\n\n2:32:45.600 --> 2:32:50.000\n form of governance? So I don't think that Catholicism is the optimal form of governance\n\n2:32:50.000 --> 2:32:54.400\n because it's obviously on the way out, right? So it is for the present type of society that we are\n\n2:32:54.400 --> 2:33:01.280\n in. Religious institutions don't seem to be optimal to organize that. So what we discovered right now\n\n2:33:01.280 --> 2:33:06.640\n that we live in in the West is democracy. And democracy is the rule of oligarchs that are the\n\n2:33:06.640 --> 2:33:11.440\n people that currently own the means of production that is administered not by the oligarchs\n\n2:33:11.440 --> 2:33:17.360\n themselves because there's too much disruption. We have so much innovation that we have in every\n\n2:33:17.360 --> 2:33:23.520\n generation new means of production that we invent. And corporations die usually after 30 years or so\n\n2:33:23.520 --> 2:33:29.280\n and something other takes a leading role in our societies. So it's administered by institutions\n\n2:33:29.280 --> 2:33:35.680\n and these institutions themselves are not elected but they provide continuity and they are led by\n\n2:33:35.680 --> 2:33:40.640\n electable politicians. And this makes it possible that you can adapt to change without having to\n\n2:33:40.640 --> 2:33:44.960\n kill people, right? So you can, for instance, have a change in governments if people think that the\n\n2:33:44.960 --> 2:33:50.160\n current government is too corrupt or is not up to date, you can just elect new people. Or if a\n\n2:33:50.160 --> 2:33:55.920\n journalist finds out something inconvenient about the institution and the institution has no plan B\n\n2:33:55.920 --> 2:34:02.400\n like in Russia, the journalist has to die. This is when you run society by the deep state. So ideally\n\n2:34:02.400 --> 2:34:09.360\n you have an administration layer that you can change if something bad happens, right? So you\n\n2:34:09.360 --> 2:34:13.600\n will have a continuity in the whole thing. And this is the system that we came up in the West.\n\n2:34:13.600 --> 2:34:17.920\n And the way it's set up in the US is largely a result of low level models. So it's mostly just\n\n2:34:17.920 --> 2:34:22.880\n second, third order consequences that people are modeling in the design of these institutions. So\n\n2:34:22.880 --> 2:34:27.120\n it's a relatively young society that doesn't really take care of the downstream effects of\n\n2:34:27.120 --> 2:34:33.120\n many of the decisions that are being made. And I suspect that AI can help us this in a way if you\n\n2:34:33.120 --> 2:34:39.520\n can fix the incentives. The society of the US is a society of cheaters. It's basically cheating is\n\n2:34:39.520 --> 2:34:44.000\n so indistinguishable from innovation and we want to encourage innovation. Can you elaborate on what\n\n2:34:44.000 --> 2:34:48.400\n you mean by cheating? It's basically people do things that they know are wrong. It's acceptable\n\n2:34:48.400 --> 2:34:52.720\n to do things that you know are wrong in this society to a certain degree. You can, for instance,\n\n2:34:52.720 --> 2:34:57.920\n suggest some non sustainable business models and implement them. Right. But you're always pushing\n\n2:34:57.920 --> 2:35:05.280\n the boundaries. I mean, yes, this is seen as a good thing largely. Yes. And this is different\n\n2:35:05.280 --> 2:35:09.520\n from other societies. So for instance, social mobility is an aspect of this. Social mobility\n\n2:35:09.520 --> 2:35:14.720\n is the result of individual innovation that would not be sustainable at scale for everybody else.\n\n2:35:14.720 --> 2:35:18.960\n Right. Normally you should not go up, you should go deep, right? We need bakers and if we are very\n\n2:35:18.960 --> 2:35:23.760\n very good bakers, but in a society that innovates, maybe you can replace all the bakers with a really\n\n2:35:23.760 --> 2:35:29.280\n good machine. Right. And that's not a bad thing. And it's a thing that made the US so successful,\n\n2:35:29.280 --> 2:35:33.600\n right? But it also means that the US is not optimizing for sustainability, but for innovation.\n\n2:35:34.880 --> 2:35:39.600\n And so it's not obvious as the evolutionary process is unrolling, it's not obvious that that\n\n2:35:39.600 --> 2:35:45.920\n long term would be better. It has side effects. So you basically, if you cheat, you will have a\n\n2:35:45.920 --> 2:35:50.400\n certain layer of toxic sludge that covers everything that is a result of cheating.\n\n2:35:50.400 --> 2:35:55.600\n And we have to unroll this evolutionary process to figure out if these side effects are so damaging\n\n2:35:55.600 --> 2:36:01.840\n that the system is horrible, or if the benefits actually outweigh the negative effects.\n\n2:36:03.600 --> 2:36:07.920\n How do we get to which system of government is best? That was from,\n\n2:36:07.920 --> 2:36:10.720\n I'm trying to trace back the last like five minutes.\n\n2:36:10.720 --> 2:36:16.640\n I suspect that we can find a way back to AI by thinking about the way in which our brain has to\n\n2:36:16.640 --> 2:36:24.480\n organize itself. In some sense, our brain is a society of neurons. And our mind is a society\n\n2:36:24.480 --> 2:36:30.720\n of behaviors. And they need to be organizing themselves into a structure that implements\n\n2:36:30.720 --> 2:36:36.480\n regulation and government is social regulation. We often see government as the manifestation of\n\n2:36:36.480 --> 2:36:40.960\n power or local interests, but it's actually a platform for negotiating the conditions of human\n\n2:36:40.960 --> 2:36:46.960\n survival. And this platform emerges over the current needs and possibilities and the trajectory\n\n2:36:46.960 --> 2:36:52.560\n that we have. So given the present state, there are only so many options on how we can move into\n\n2:36:52.560 --> 2:36:56.400\n the next state without completely disrupting everything. And we mostly agree that it's a\n\n2:36:56.400 --> 2:37:01.200\n bad idea to disrupt everything because it will endanger our food supply for a while and the entire\n\n2:37:01.200 --> 2:37:06.960\n infrastructure and fabric of society. So we do try to find natural transitions,\n\n2:37:06.960 --> 2:37:10.720\n and there are not that many natural transitions available at any given point.\n\n2:37:10.720 --> 2:37:12.080\n What do you mean by natural transitions?\n\n2:37:12.080 --> 2:37:14.880\n So we try not to have revolutions if we can have it.\n\n2:37:14.880 --> 2:37:21.200\n Right. So speaking of revolutions and the connection between government systems and the mind,\n\n2:37:21.760 --> 2:37:29.120\n you've also said that in some sense, becoming an adult means you take charge of your emotions.\n\n2:37:29.120 --> 2:37:35.200\n Maybe you never said that. Maybe I just made that up. But in the context of the mind,\n\n2:37:35.840 --> 2:37:42.240\n what's the role of emotion? And what is it? First of all, what is emotion? What's its role?\n\n2:37:42.240 --> 2:37:46.880\n It's several things. So psychologists often distinguish between emotion and feeling,\n\n2:37:46.880 --> 2:37:52.800\n and in common day parlance, we don't. I think that emotion is a configuration of the cognitive\n\n2:37:52.800 --> 2:37:57.840\n system. And that's especially true for the lowest level for the affective state. So when you have\n\n2:37:57.840 --> 2:38:02.560\n an affect, it's the configuration of certain modulation parameters like arousal, valence,\n\n2:38:03.920 --> 2:38:08.080\n your attentional focus, whether it's wide or narrow, inter reception or extra reception,\n\n2:38:08.080 --> 2:38:13.040\n and so on. And all these parameters together put you in a certain way. You relate to the\n\n2:38:13.040 --> 2:38:17.200\n environment and to yourself, and this is in some sense an emotional configuration.\n\n2:38:17.200 --> 2:38:21.600\n In the more narrow sense, an emotion is an affective state. It has an object,\n\n2:38:22.240 --> 2:38:26.640\n and the relevance of that object is given by motivation. And motivation is a bunch of needs\n\n2:38:26.640 --> 2:38:31.440\n that are associated with rewards, things that give you pleasure and pain. And you don't actually act\n\n2:38:31.440 --> 2:38:35.520\n on your needs, you act on models of your needs. Because when the pleasure and pain manifest,\n\n2:38:35.520 --> 2:38:40.480\n it's too late, you've done everything. So you act on expectations that will give you pleasure and\n\n2:38:40.480 --> 2:38:45.680\n pain. And these are your purposes. The needs don't form a hierarchy, they just coexist and compete.\n\n2:38:45.680 --> 2:38:51.840\n And your brain has to find a dynamic homeostasis between them. But the purposes need to be\n\n2:38:51.840 --> 2:38:57.920\n consistent. So you basically can create a story for your life and make plans. And so we organize\n\n2:38:57.920 --> 2:39:02.160\n them all into hierarchies. And there is not a unique solution for this. Some people eat to make\n\n2:39:02.160 --> 2:39:07.200\n art and other people make art to eat. They might end up doing the same things, but they cooperate\n\n2:39:07.200 --> 2:39:12.800\n in very different ways. Because their ultimate goals are different. And we cooperate based on\n\n2:39:12.800 --> 2:39:16.800\n shared purpose. Everything else that is not cooperation on shared purpose is transactional.\n\n2:39:16.800 --> 2:39:25.760\n I don't think I understood that last piece of achieving the homeostasis.\n\n2:39:26.640 --> 2:39:30.240\n Are you distinguishing between the experience of emotion and the expression of emotion?\n\n2:39:30.960 --> 2:39:37.520\n Of course. So the experience of emotion is a feeling. And in this sense, what you feel is\n\n2:39:37.520 --> 2:39:42.800\n an appraisal that your perceptual system has made of the situation at hand. And it makes this based\n\n2:39:42.800 --> 2:39:50.400\n on your motivation and on your estimates, not your but of the subconscious geometric parts of your\n\n2:39:50.400 --> 2:39:56.480\n mind that assess the situation in the world with something like a neural network. And this neural\n\n2:39:56.480 --> 2:40:02.000\n network is making itself known to the symbolic parts of your mind, to your conscious attention\n\n2:40:02.000 --> 2:40:08.640\n by mapping them as features into a space. So what you will feel about your emotion is a projection\n\n2:40:08.640 --> 2:40:12.800\n usually into your body map. So you might feel anxiety in your solar plexus, and you might feel\n\n2:40:12.800 --> 2:40:18.880\n it as a contraction, which is all geometry. Your body map is the space that is always instantiated\n\n2:40:18.880 --> 2:40:26.960\n and always available. So it's a very obvious cheat if your non symbolic parts of your brain\n\n2:40:26.960 --> 2:40:31.120\n try to talk to your symbolic parts of your brain to map the feelings into the body map.\n\n2:40:31.120 --> 2:40:35.040\n And then you perceive them as pleasant and unpleasant, depending on whether the appraisal\n\n2:40:35.040 --> 2:40:40.000\n has a negative or positive valence. And then you have different features of them that give you\n\n2:40:40.720 --> 2:40:44.160\n more knowledge about the nature of what you're feeling. So for instance, when you feel\n\n2:40:44.160 --> 2:40:47.600\n connected to other people, you typically feel this in your chest region around the heart.\n\n2:40:48.240 --> 2:40:53.200\n And you feel this is an expansive feeling in which you're reaching out, right? And it's\n\n2:40:53.200 --> 2:40:59.760\n very intuitive to encode it like this. That's why it's encoded like this. It's a code in which the\n\n2:40:59.760 --> 2:41:04.080\n non symbolic parts of your mind talk to the symbolic ones. And then the expression of emotion\n\n2:41:04.080 --> 2:41:09.680\n is then the final step that could be sort of gestural or visual and so on. That's part of\n\n2:41:09.680 --> 2:41:14.480\n the communication. This probably evolved as part of an adversarial communication. So as soon as\n\n2:41:14.480 --> 2:41:19.360\n you started to observe the facial expression and posture of others to understand what emotional\n\n2:41:19.360 --> 2:41:23.920\n state they're in, others started to use this as signaling and also to subvert your model of their\n\n2:41:23.920 --> 2:41:29.280\n emotional state. So we now look at the inflections, at the difference between the standard face that\n\n2:41:29.280 --> 2:41:33.600\n they're going to make in this situation. When you are at a funeral, everybody expects you to make a\n\n2:41:33.600 --> 2:41:38.080\n solemn face, but the solemn face doesn't express whether you're sad or not. It just expresses that\n\n2:41:38.080 --> 2:41:44.080\n you understand what face you have to make at a funeral. Nobody should know that you are triumphant.\n\n2:41:44.080 --> 2:41:48.000\n So when you try to read the emotion of another person, you try to look at the delta\n\n2:41:48.000 --> 2:41:56.320\n between a truly sad expression and the things that are animating this face behind the curtain.\n\n2:41:56.320 --> 2:42:03.920\n So the interesting thing is, so having done this podcast and the video component, one of the things\n\n2:42:03.920 --> 2:42:10.720\n I've learned is that now I'm Russian and I just don't know how to express emotion on my face.\n\n2:42:10.720 --> 2:42:16.960\n One, I see that as weakness, but whatever. The people look to me after you say something,\n\n2:42:16.960 --> 2:42:22.960\n they look to my face to help them see how they should feel about what you said,\n\n2:42:22.960 --> 2:42:28.000\n which is fascinating because then they'll often comment on why did you look bored or why did you\n\n2:42:28.000 --> 2:42:32.800\n particularly enjoy that part or why did you whatever. It's a kind of interesting, it makes\n\n2:42:32.800 --> 2:42:38.160\n me cognizant of I'm part, like you're basically saying a bunch of brilliant things, but I'm\n\n2:42:38.800 --> 2:42:45.360\n part of the play that you're the key actor in by making my facial expressions and then\n\n2:42:45.920 --> 2:42:51.200\n therefore telling the narrative of what the big, like the big point is, which is fascinating.\n\n2:42:51.200 --> 2:42:56.240\n Makes me cognizant that I'm supposed to be making facial expressions. Even this conversation is hard\n\n2:42:56.240 --> 2:43:01.920\n because my preference would be to wear a mask with sunglasses to where I could just listen.\n\n2:43:01.920 --> 2:43:06.960\n Yes, I understand this because it's intrusive to interact with others this way. And basically\n\n2:43:07.680 --> 2:43:11.200\n Eastern European society have a taboo against that and especially Russia,\n\n2:43:11.840 --> 2:43:17.360\n the further you go to the East and in the US it's the opposite. You're expected to be\n\n2:43:17.360 --> 2:43:22.160\n hyperanimated in your face and you're also expected to show positive affect.\n\n2:43:22.160 --> 2:43:22.560\n Yes.\n\n2:43:22.560 --> 2:43:26.880\n And if you show positive affect without a good reason in Russia,\n\n2:43:27.600 --> 2:43:31.440\n people will think you are a stupid, unsophisticated person.\n\n2:43:33.280 --> 2:43:40.080\n Exactly. And here positive affect without reason goes either appreciated or goes unnoticed.\n\n2:43:40.800 --> 2:43:45.840\n No, it's the default. It's being expected. Everything is amazing. Have you seen these?\n\n2:43:45.840 --> 2:43:47.600\n Lego movie?\n\n2:43:47.600 --> 2:43:52.800\n No, there was a diagram where somebody gave the appraisals that exist in the US and Russia,\n\n2:43:52.800 --> 2:44:02.080\n so you have your bell curve. And the lower 10% in the US, it's a good start. Everything\n\n2:44:02.080 --> 2:44:04.400\n above the lowest 10%, it's amazing.\n\n2:44:04.400 --> 2:44:05.200\n It's amazing.\n\n2:44:06.000 --> 2:44:14.800\n And for Russians, everything below the top 10%, it's terrible. And then everything except the\n\n2:44:14.800 --> 2:44:19.840\n top percent is, I don't like it. And the top percent is even so.\n\n2:44:23.680 --> 2:44:25.600\n It's funny, but it's kind of true.\n\n2:44:27.200 --> 2:44:33.440\n There's a deeper aspect to this. It's also how we construct meaning in the US. Usually you focus on\n\n2:44:33.440 --> 2:44:40.400\n the positive aspects and you just suppress the negative aspects. And in our Eastern European\n\n2:44:40.400 --> 2:44:46.880\n traditions, we emphasize the fact that if you hold something above the waterline, you also need to\n\n2:44:46.880 --> 2:44:50.480\n put something below the waterline because existence by itself is as best neutral.\n\n2:44:51.360 --> 2:44:56.240\n Right. That's the basic intuition, at best neutral. Or it could be just suffering,\n\n2:44:56.240 --> 2:44:56.800\n the default is suffering.\n\n2:44:56.800 --> 2:45:02.160\n There are moments of beauty, but these moments of beauty are inextricably linked to the reality\n\n2:45:02.160 --> 2:45:07.280\n of suffering. And to not acknowledge the reality of suffering means that you are really stupid and\n\n2:45:07.280 --> 2:45:11.680\n unaware of the fact that basically every conscious being spends most of the time suffering.\n\n2:45:12.560 --> 2:45:19.680\n Yeah. You just summarized the ethos of the Eastern Europe. Yeah. Most of life is suffering\n\n2:45:19.680 --> 2:45:24.480\n with an occasional moments of beauty. And if your facial expressions don't acknowledge\n\n2:45:24.480 --> 2:45:29.840\n the abundance of suffering in the world and in existence itself, then you must be an idiot.\n\n2:45:30.640 --> 2:45:36.000\n It's an interesting thing when you raise children in the US and you, in some sense,\n\n2:45:36.000 --> 2:45:40.960\n preserve the identity of the intellectual and cultural traditions that are embedded in your\n\n2:45:40.960 --> 2:45:46.800\n own families. And your daughter asks you about Ariel the mermaid and asks you,\n\n2:45:46.800 --> 2:45:53.840\n why is Ariel not allowed to play with the humans? And you tell her the truth. She's a siren. Sirens\n\n2:45:53.840 --> 2:45:58.080\n eat people. You don't play with your food. It does not end well. And then you tell her the original\n\n2:45:58.080 --> 2:46:02.640\n story, which is not the one by Anderson, which is the romantic one. And there's a much darker one,\n\n2:46:02.640 --> 2:46:11.040\n which is Undine's story. What happened? So Undine is a mermaid or a water woman. She lives on the\n\n2:46:11.040 --> 2:46:15.440\n ground of a river and she meets this prince and they fall in love. And the prince really,\n\n2:46:15.440 --> 2:46:20.800\n really wants to be with her. And she says, okay, but the deal is you cannot have any other woman.\n\n2:46:20.800 --> 2:46:24.240\n If you marry somebody else, even though you cannot be with me, because obviously you cannot breathe\n\n2:46:24.240 --> 2:46:29.520\n underwater and have other things to do than managing your kingdom as you have here, you will\n\n2:46:29.520 --> 2:46:35.040\n die. And eventually after a few years, he falls in love with some princess and marries her. And\n\n2:46:35.760 --> 2:46:41.520\n she shows up and quietly goes into his chamber and nobody is able to stop her or willing to do\n\n2:46:41.520 --> 2:46:46.480\n so because she is fierce. And she comes quietly and sad out of his chamber. And they ask her,\n\n2:46:47.440 --> 2:46:50.240\n what has happened? What did you do? And she said, I kissed him to death.\n\n2:46:52.000 --> 2:46:52.480\n All done.\n\n2:46:53.760 --> 2:46:59.280\n And you know the Anderson story, right? In the Anderson story, the mermaid is playing with\n\n2:46:59.280 --> 2:47:04.240\n this prince that she saves and she falls in love with him and she cannot live out there. So she is\n\n2:47:04.240 --> 2:47:11.360\n giving up her voice and her tale for a human like appearance so she can walk among the humans. But\n\n2:47:11.360 --> 2:47:16.880\n this guy does not recognize that she is the one that you would marry. Instead, he marries somebody\n\n2:47:16.880 --> 2:47:22.160\n who has a kingdom and economical and political relationships to his own kingdom and so on,\n\n2:47:22.160 --> 2:47:25.200\n as he should. And she dies.\n\n2:47:25.200 --> 2:47:35.040\n Yeah. Instead, Disney, the Little Mermaid story has a little bit of a happy ending. That's the\n\n2:47:35.040 --> 2:47:37.040\n Western, that's the American way.\n\n2:47:37.040 --> 2:47:41.280\n My own problem is this, of course, that I read Oscar Wilde before I read the other things. So\n\n2:47:41.280 --> 2:47:46.960\n I'm indoctrinated, inoculated with this romanticism. And I think that the mermaid is right. You\n\n2:47:46.960 --> 2:47:51.520\n sacrifice your life for romantic love. That's what you do. Because if you are confronted with\n\n2:47:51.520 --> 2:47:57.120\n either serving the machine and doing the obviously right thing under the economic and social and\n\n2:47:57.680 --> 2:48:01.440\n other human incentives, that's wrong. You should follow your heart.\n\n2:48:04.000 --> 2:48:09.520\n So do you think suffering is fundamental to happiness along these lines?\n\n2:48:09.520 --> 2:48:14.640\n Suffering is the result of caring about things that you cannot change. And if you are able to\n\n2:48:14.640 --> 2:48:17.840\n change what you care about to those things that you can change, you will not suffer.\n\n2:48:17.840 --> 2:48:22.160\n But would you then be able to experience happiness?\n\n2:48:22.160 --> 2:48:27.680\n Yes. But happiness itself is not important. Happiness is like a cookie. When you are a child,\n\n2:48:27.680 --> 2:48:30.720\n you think cookies are very important and you want to have all the cookies in the world,\n\n2:48:30.720 --> 2:48:34.160\n you look forward to being an adult because then you have as many cookies as you want.\n\n2:48:35.200 --> 2:48:39.440\n But as an adult, you realize a cookie is a tool. It's a tool to make you eat vegetables.\n\n2:48:40.000 --> 2:48:43.280\n And once you eat your vegetables anyway, you stop eating cookies for the most part,\n\n2:48:43.280 --> 2:48:46.560\n because otherwise you will get diabetes and will not be around for your kids.\n\n2:48:46.560 --> 2:48:51.760\n Yes, but then the cookie, the scarcity of a cookie, if scarcity is enforced,\n\n2:48:51.760 --> 2:48:54.560\n nevertheless, so like the pleasure comes from the scarcity.\n\n2:48:54.560 --> 2:48:59.760\n Yes. But the happiness is a cookie that your brain bakes for itself. It's not made by the\n\n2:48:59.760 --> 2:49:03.600\n environment. The environment cannot make you happy. It's your appraisal of the environment\n\n2:49:03.600 --> 2:49:07.840\n that makes you happy. And if you can change the appraisal of the environment, which you can learn\n\n2:49:07.840 --> 2:49:11.920\n to, then you can create arbitrary states of happiness. And some meditators fall into this\n\n2:49:11.920 --> 2:49:16.560\n trap. So they discover the womb, this basement womb in their brain where the cookies are made,\n\n2:49:16.560 --> 2:49:20.400\n and they indulge and stuff themselves. And after a few months, it gets really old and\n\n2:49:20.400 --> 2:49:25.360\n the big crisis of meaning comes. Because they thought before that their unhappiness was the\n\n2:49:25.360 --> 2:49:29.600\n result of not being happy enough. So they fixed this, right? They can release the newer\n\n2:49:29.600 --> 2:49:35.680\n transmitters at will if they train. And then the crisis of meaning pops up in a deeper layer.\n\n2:49:36.400 --> 2:49:40.480\n And the question is, why do I live? How can I make a sustainable civilization that is meaningful to\n\n2:49:40.480 --> 2:49:44.240\n me? How can I insert myself into this? And this was the problem that you couldn't solve in the\n\n2:49:44.240 --> 2:49:53.360\n first place. But at the end of all this, let me then ask that same question. What is the answer\n\n2:49:53.360 --> 2:49:59.440\n to that? What could the possible answer be of the meaning of life? What could an answer be? What is\n\n2:49:59.440 --> 2:50:06.000\n it to you? I think that if you look at the meaning of life, you look at what the cell is. Life is the\n\n2:50:06.000 --> 2:50:14.160\n cell. Or this principle, the cell. It's this self organizing thing that can participate in evolution.\n\n2:50:14.160 --> 2:50:18.160\n In order to make it work, it's a molecular machine. It needs a self replicator and an\n\n2:50:18.160 --> 2:50:22.560\n entropy extractor and a Turing machine. If any of these parts is missing, you don't have a cell\n\n2:50:22.560 --> 2:50:27.520\n and it is not living. And life is basically the emergent complexity over that principle.\n\n2:50:27.520 --> 2:50:32.560\n Once you have this intelligent super molecule, the cell, there is very little that you cannot\n\n2:50:32.560 --> 2:50:37.920\n make it do. It's probably the optimal computronium and especially in terms of resilience. It's very\n\n2:50:37.920 --> 2:50:45.200\n hard to sterilize the planet once it's infected with life. So it's active function of these three\n\n2:50:45.200 --> 2:50:51.600\n components or the supercell cell is present in the cell, it's present in us, and it's just...\n\n2:50:51.600 --> 2:50:55.600\n We are just an expression of the cell. It's a certain layer of complexity in the organization\n\n2:50:55.600 --> 2:51:02.000\n of cells. So in a way, it's tempting to think of the cell as a von Neumann probe. If you want to\n\n2:51:02.000 --> 2:51:06.480\n build intelligence on other planets, the best way to do this is to infect them with cells\n\n2:51:07.360 --> 2:51:11.600\n and wait for long enough and there's a reasonable chance the stuff is going to evolve into an\n\n2:51:11.600 --> 2:51:14.880\n information processing principle that is general enough to become sentient.\n\n2:51:16.400 --> 2:51:21.360\n That idea is very akin to the same dream and beautiful ideas that are expressed to\n\n2:51:21.360 --> 2:51:26.000\n cellular automata in their most simple mathematical form. If you just inject the system with some\n\n2:51:26.000 --> 2:51:32.160\n basic mechanisms of replication and so on, basic rules, amazing things would emerge.\n\n2:51:32.160 --> 2:51:38.000\n The cell is able to do something that James Trardy calls existential design. He points out\n\n2:51:38.000 --> 2:51:42.880\n that in technical design, we go from the outside in. We work in a highly controlled environment in\n\n2:51:42.880 --> 2:51:48.080\n which everything is deterministic, like our computers, our labs, or our engineering workshops.\n\n2:51:48.080 --> 2:51:53.120\n And then we use this determinism to implement a particular kind of function that we dream up and\n\n2:51:53.120 --> 2:51:57.600\n that seamlessly interfaces with all the other deterministic functions that we already have in\n\n2:51:57.600 --> 2:52:04.080\n our world. So it's basically from the outside in. Biological systems designed from the inside out\n\n2:52:04.080 --> 2:52:11.200\n as seed will become a seedling by taking some of the relatively unorganized matter around it and\n\n2:52:11.200 --> 2:52:16.560\n turning it into its own structure and thereby subdue the environment. Cells can cooperate if\n\n2:52:16.560 --> 2:52:21.040\n they can rely on other cells having a similar organization that is already compatible. But\n\n2:52:21.040 --> 2:52:27.280\n unless that's there, the cell needs to divide to create that structure by itself. So it's a\n\n2:52:27.280 --> 2:52:32.640\n self organizing principle that works on a somewhat chaotic environment. And the purpose of life in\n\n2:52:32.640 --> 2:52:38.960\n this sense is to produce complexity. And the complexity allows you to harvest entropy gradients\n\n2:52:38.960 --> 2:52:43.920\n that you couldn't harvest without the complexity. And in this sense, intelligence and life are very\n\n2:52:43.920 --> 2:52:48.800\n strongly connected because the purpose of intelligence is to allow control under conditions\n\n2:52:48.800 --> 2:52:53.760\n and the conditions of complexity. So basically, you shift the boundary between the ordered systems\n\n2:52:53.760 --> 2:53:00.960\n into the realm of chaos. You build bridge heads into chaos with complexity. And this is what we\n\n2:53:00.960 --> 2:53:05.200\n are doing. This is not necessarily a deeper meaning. I think the meaning that we have priors\n\n2:53:05.200 --> 2:53:09.280\n for that we are all for outside of the priors, there is no meaning. Meaning only exists if the\n\n2:53:09.280 --> 2:53:16.720\n mind projects it. That is probably civilization. I think that what feels most meaningful to me is\n\n2:53:16.720 --> 2:53:22.880\n to try to build and maintain a sustainable civilization. And taking a slight step outside\n\n2:53:22.880 --> 2:53:34.400\n of that, we talked about a man with a beard and God, but something, some mechanism, perhaps must\n\n2:53:34.400 --> 2:53:42.480\n have planted the seed, the initial seed of the cell. Do you think there is a God? What is a God?\n\n2:53:42.480 --> 2:53:48.000\n And what would that look like? If there was no spontaneous biogenesis, in the sense that the\n\n2:53:48.000 --> 2:53:54.160\n first cell formed by some happy random accidents where the molecules just happened to be in the\n\n2:53:54.160 --> 2:53:59.600\n right constellation to each other. But there could also be the mechanism that allows for the random.\n\n2:53:59.600 --> 2:54:04.720\n I mean, there's like turtles all the way down. There seems to be, there has to be a head turtle\n\n2:54:04.720 --> 2:54:10.480\n at the bottom. Let's consider something really wild. Imagine, is it possible that a gas giant\n\n2:54:10.480 --> 2:54:16.240\n could become intelligent? What would that involve? So imagine you have vortices that spontaneously\n\n2:54:16.240 --> 2:54:21.360\n emerge on the gas giants, like big storm systems that endure for thousands of years.\n\n2:54:21.360 --> 2:54:24.880\n And some of these storm systems produce electromagnetic fields because some of the\n\n2:54:24.880 --> 2:54:30.240\n clouds are ferromagnetic or something. And as a result, they can change how certain clouds react\n\n2:54:30.240 --> 2:54:34.960\n rather than other clouds and thereby produce some self stabilizing patterns that eventually\n\n2:54:34.960 --> 2:54:40.560\n lead to regulation feedback loops, nested feedback loops and control. So imagine you have such this\n\n2:54:40.560 --> 2:54:44.960\n thing that basically has emergent self sustaining, self organizing complexity. And at some point,\n\n2:54:44.960 --> 2:54:50.320\n this breaks up and realizes and basically lam solaris, I am a thinking planet, but I will not\n\n2:54:50.320 --> 2:54:55.680\n replicate because I can recreate the conditions of my own existence somewhere else. I'm just\n\n2:54:55.680 --> 2:55:01.840\n basically an intelligence that has spontaneously formed because it could. And now it builds a\n\n2:55:01.840 --> 2:55:05.680\n von Neumann probe and the best von Neumann probe for such a thing might be the cell.\n\n2:55:05.680 --> 2:55:10.560\n So maybe it, because it's very, very clever and very enduring, creates cells and sends them out.\n\n2:55:10.560 --> 2:55:14.400\n And one of them has infected our planet. And I'm not suggesting that this is the case,\n\n2:55:14.400 --> 2:55:19.120\n but it would be compatible with the Prince Birmingham hypothesis. And it was my intuition\n\n2:55:19.120 --> 2:55:24.160\n that our biogenesis is very unlikely. It's possible, but you probably need to roll the\n\n2:55:24.160 --> 2:55:28.240\n cosmic dice very often, maybe more often than there are planetary surfaces. I don't know.\n\n2:55:28.240 --> 2:55:37.360\n So God is just a large enough, a system that's large enough that allows randomness.\n\n2:55:37.360 --> 2:55:41.440\n No, I don't think that God has anything to do with creation. I think it's a mistranslation\n\n2:55:41.440 --> 2:55:46.800\n of the Talmud into the Catholic mythology. I think that Genesis is actually the childhood\n\n2:55:46.800 --> 2:55:51.600\n memories of a God. So the, when. Sorry, Genesis is the.\n\n2:55:51.600 --> 2:55:57.280\n The childhood memories of a God. It's basically a mind that is remembering how it came into being.\n\n2:55:57.280 --> 2:55:57.840\n Wow.\n\n2:55:57.840 --> 2:56:03.680\n And we typically interpret Genesis as the creation of a physical universe by a supernatural being.\n\n2:56:03.680 --> 2:56:04.240\n Yes.\n\n2:56:04.240 --> 2:56:12.160\n And I think when you read it, there is light and darkness that is being created. And then you\n\n2:56:12.160 --> 2:56:18.880\n discover sky and ground, create them. You construct the plants and the animals and you\n\n2:56:18.880 --> 2:56:24.320\n give everything their names and so on. That's basically cognitive development. It's a sequence\n\n2:56:24.320 --> 2:56:28.320\n of steps that every mind has to go through when it makes sense of the world. And when you have\n\n2:56:28.320 --> 2:56:33.040\n children, you can see how initially they distinguish light and darkness and then they\n\n2:56:33.040 --> 2:56:37.120\n make out directions in it and they discover sky and ground and they discover the plants and the\n\n2:56:37.120 --> 2:56:41.520\n animals and they give everything their name. And it's a creative process that happens in every mind\n\n2:56:41.520 --> 2:56:46.160\n because it's not given. Your mind has to invent these structures to make sense of the patterns\n\n2:56:46.160 --> 2:56:52.560\n on your retina. Also, if there was some big nerd who set up a server and runs this world on it,\n\n2:56:52.560 --> 2:56:57.120\n this would not create a special relationship between us and the nerd. This nerd would not\n\n2:56:57.120 --> 2:57:03.280\n have the magical power to give meaning to our existence. So this equation of a creator god\n\n2:57:03.280 --> 2:57:07.840\n with the god of meaning is a sleight of hand. You shouldn't do it.\n\n2:57:07.840 --> 2:57:12.080\n The other one that is done in Catholicism is the equation of the first mover,\n\n2:57:12.080 --> 2:57:17.440\n the prime mover of Aristotle, which is basically the automaton that runs the universe. Aristotle\n\n2:57:17.440 --> 2:57:23.600\n says if things are moving and things seem to be moving here, something must move them. If something\n\n2:57:23.600 --> 2:57:28.080\n moves them, something must move the thing that is moving it. So there must be a prime mover.\n\n2:57:28.080 --> 2:57:32.400\n This idea to say that this prime mover is a supernatural being is complete nonsense.\n\n2:57:33.280 --> 2:57:39.120\n It's an automaton in the simplest case. So we have to explain the enormity that this automaton\n\n2:57:39.120 --> 2:57:45.520\n exists at all. But again, we don't have any possibility to infer anything about its properties\n\n2:57:45.520 --> 2:57:51.440\n except that it's able to produce change in information. So there needs to be some kind\n\n2:57:51.440 --> 2:57:56.400\n of computational principle. This is all there is. But to say this automaton is identical again with\n\n2:57:56.400 --> 2:58:02.000\n the creator of the first cause or with the thing that gives meaning to our life is confusion.\n\n2:58:02.000 --> 2:58:08.080\n No, I think that what we perceive is the higher being that we are part of. The higher being that\n\n2:58:08.080 --> 2:58:13.200\n we are part of is the civilization. It's the thing in which we have a similar relationship as the cell\n\n2:58:13.200 --> 2:58:19.360\n has to our body. And we have this prior because we have evolved to organize in these structures.\n\n2:58:20.560 --> 2:58:24.720\n So basically, the Christian God in its natural form without the mythology,\n\n2:58:24.720 --> 2:58:28.240\n if you undress it, is basically the platonic form of the civilization.\n\n2:58:30.480 --> 2:58:32.480\n Is the ideal?\n\n2:58:32.480 --> 2:58:36.240\n Yes, it's this ideal that you try to approximate when you interact with others,\n\n2:58:36.240 --> 2:58:38.880\n not based on your incentives, but on what you think is right.\n\n2:58:38.880 --> 2:58:45.520\n Wow, we covered a lot of ground. And we're left with one of my favorite lines, and there's many,\n\n2:58:45.520 --> 2:58:54.880\n which is happiness is a cookie that the brain bakes itself. It's been a huge honor and a\n\n2:58:54.880 --> 2:58:58.640\n pleasure to talk to you. I'm sure our paths will cross many times again.\n\n2:58:59.680 --> 2:59:02.480\n Joshua, thank you so much for talking today. I really appreciate it.\n\n2:59:02.480 --> 2:59:05.760\n Thank you, Lex. It was so much fun. I enjoyed it.\n\n2:59:05.760 --> 2:59:12.080\n Awesome. Thanks for listening to this conversation with Joshua Bach. And thank you to our sponsors,\n\n2:59:12.080 --> 2:59:18.000\n ExpressVPN and Cash App. Please consider supporting this podcast by getting ExpressVPN at\n\n2:59:18.000 --> 2:59:27.360\n expressvpn.com slash lexpod and downloading Cash App and using code lexpodcast. If you enjoy this\n\n2:59:27.360 --> 2:59:33.440\n thing, subscribe on YouTube, review it with five stars in Apple Podcast, support it on Patreon,\n\n2:59:33.440 --> 2:59:39.440\n or simply connect with me on Twitter at lexfreedman. And yes, try to figure out how to\n\n2:59:39.440 --> 2:59:45.200\n spell it without the E. And now let me leave you with some words of wisdom from Joshua Bach.\n\n2:59:46.400 --> 2:59:51.760\n If you take this as a computer game metaphor, this is the best level for humanity to play.\n\n2:59:52.720 --> 2:59:59.760\n And this best level happens to be the last level, as it happens against the backdrop of a dying\n\n2:59:59.760 --> 3:00:07.440\n world. But it's still the best level. Thank you for listening and hope to see you next time.\n\n"
}