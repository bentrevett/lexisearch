{
  "title": "Pamela McCorduck: Machines Who Think and the Early Days of AI | Lex Fridman Podcast #34",
  "id": "i6rnzk8VU24",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:04.800\n The following is a conversation with Pamela McCordick. She's an author who has written on\n\n00:04.800 --> 00:10.400\n the history and the philosophical significance of artificial intelligence. Her books include\n\n00:10.400 --> 00:18.160\n Machines Who Think in 1979, The Fifth Generation in 1983 with Ed Feigenbaum, who's considered to\n\n00:18.160 --> 00:24.000\n be the father of expert systems, The Edge of Chaos that features women, and many more books.\n\n00:24.000 --> 00:29.520\n I came across her work in an unusual way by stumbling in a quote from Machines Who Think\n\n00:29.520 --> 00:36.240\n that is something like, artificial intelligence began with the ancient wish to forge the gods.\n\n00:37.040 --> 00:42.320\n That was a beautiful way to draw a connecting line between our societal relationship with AI\n\n00:42.960 --> 00:48.560\n from the grounded day to day science, math and engineering, to popular stories and science\n\n00:48.560 --> 00:54.800\n fiction and myths of automatons that go back for centuries. Through her literary work,\n\n00:54.800 --> 01:00.480\n she has spent a lot of time with the seminal figures of artificial intelligence, including\n\n01:00.480 --> 01:07.920\n the founding fathers of AI from the 1956 Dartmouth summer workshop where the field was launched.\n\n01:08.480 --> 01:13.760\n I reached out to Pamela for a conversation in hopes of getting a sense of what those early\n\n01:13.760 --> 01:19.200\n days were like, and how their dreams continue to reverberate through the work of our community\n\n01:19.200 --> 01:25.600\n today. I often don't know where the conversation may take us, but I jump in and see. Having no\n\n01:25.600 --> 01:31.760\n constraints, rules, or goals is a wonderful way to discover new ideas. This is the Artificial\n\n01:31.760 --> 01:37.840\n Intelligence Podcast. If you enjoy it, subscribe on YouTube, give it five stars on iTunes,\n\n01:37.840 --> 01:44.720\n support it on Patreon, or simply connect with me on Twitter, at Lex Friedman, spelled F R I D M\n\n01:44.720 --> 01:53.840\n A N. And now, here's my conversation with Pamela McCordick. In 1979, your book Machines Who Think\n\n01:55.040 --> 02:00.720\n was published. In it, you interview some of the early AI pioneers and explore the idea that\n\n02:00.720 --> 02:10.400\n AI was born not out of maybe math and computer science, but out of myth and legend. So, tell me\n\n02:10.400 --> 02:17.520\n if you could the story of how you first arrived at the book, the journey of beginning to write it.\n\n02:19.040 --> 02:29.120\n I had been a novelist. I'd published two novels, and I was sitting under the portal at Stanford\n\n02:29.120 --> 02:33.920\n one day, the house we were renting for the summer. And I thought, I should write a novel about these\n\n02:33.920 --> 02:41.360\n weird people in AI, I know. And then I thought, ah, don't write a novel, write a history. Simple.\n\n02:41.360 --> 02:48.240\n Just go around, interview them, splice it together, voila, instant book. Ha, ha, ha. It was\n\n02:48.240 --> 02:54.400\n much harder than that. But nobody else was doing it. And so, I thought, well, this is a great\n\n02:54.400 --> 03:03.760\n opportunity. And there were people who, John McCarthy, for example, thought it was a nutty\n\n03:03.760 --> 03:11.040\n idea. The field had not evolved yet, so on. And he had some mathematical thing he thought I should\n\n03:11.040 --> 03:17.840\n write instead. And I said, no, John, I am not a woman in search of a project. This is what I want\n\n03:17.840 --> 03:24.560\n to do. I hope you'll cooperate. And he said, oh, mutter, mutter, well, okay, it's your time.\n\n03:24.560 --> 03:30.800\n What was the pitch for the, I mean, such a young field at that point. How do you write\n\n03:30.800 --> 03:37.040\n a personal history of a field that's so young? I said, this is wonderful. The founders of the\n\n03:37.040 --> 03:42.720\n field are alive and kicking and able to talk about what they're doing. Did they sound or feel like\n\n03:42.720 --> 03:48.000\n founders at the time? Did they know that they have founded something?\n\n03:48.000 --> 03:55.520\n Oh, yeah. They knew what they were doing was very important. Very. What I now see in retrospect\n\n03:56.160 --> 04:04.320\n is that they were at the height of their research careers. And it's humbling to me that they took\n\n04:04.320 --> 04:11.440\n time out from all the things that they had to do as a consequence of being there. And to talk to\n\n04:11.440 --> 04:16.400\n this woman who said, I think I'm going to write a book about you. No, it was amazing. Just amazing.\n\n04:17.040 --> 04:25.040\n So who stands out to you? Maybe looking 63 years ago, the Dartmouth conference,\n\n04:26.480 --> 04:32.960\n so Marvin Minsky was there, McCarthy was there, Claude Shannon, Alan Newell, Herb Simon,\n\n04:32.960 --> 04:40.080\n some of the folks you've mentioned. Then there's other characters, right? One of your coauthors\n\n04:40.080 --> 04:43.120\n He wasn't at Dartmouth.\n\n04:43.120 --> 04:43.920\n He wasn't at Dartmouth.\n\n04:43.920 --> 04:46.800\n No. He was, I think, an undergraduate then.\n\n04:47.680 --> 04:56.000\n And of course, Joe Traub. All of these are players, not at Dartmouth, but in that era.\n\n04:56.000 --> 04:56.500\n Right.\n\n04:57.600 --> 05:02.960\n CMU and so on. So who are the characters, if you could paint a picture, that stand out to you\n\n05:02.960 --> 05:07.200\n from memory? Those people you've interviewed and maybe not, people that were just in the\n\n05:08.400 --> 05:09.920\n In the atmosphere.\n\n05:09.920 --> 05:10.720\n In the atmosphere.\n\n05:11.840 --> 05:15.920\n Of course, the four founding fathers were extraordinary guys. They really were.\n\n05:15.920 --> 05:17.040\n Who are the founding fathers?\n\n05:18.560 --> 05:24.800\n Alan Newell, Herbert Simon, Marvin Minsky, John McCarthy. They were the four who were not only\n\n05:24.800 --> 05:28.880\n at the Dartmouth conference, but Newell and Simon arrived there with a working program\n\n05:29.600 --> 05:34.960\n called The Logic Theorist. Everybody else had great ideas about how they might do it, but\n\n05:34.960 --> 05:38.480\n But they weren't going to do it yet.\n\n05:41.040 --> 05:48.720\n And you mentioned Joe Traub, my husband. I was immersed in AI before I met Joe\n\n05:50.080 --> 05:55.040\n because I had been Ed Feigenbaum's assistant at Stanford. And before that,\n\n05:55.040 --> 06:04.320\n I had worked on a book edited by Feigenbaum and Julian Feldman called Computers and Thought.\n\n06:04.320 --> 06:10.480\n It was the first textbook of readings of AI. And they only did it because they were trying to teach\n\n06:10.480 --> 06:15.040\n AI to people at Berkeley. And there was nothing, you'd have to send them to this journal and that\n\n06:15.040 --> 06:22.240\n journal. This was not the internet where you could go look at an article. So I was fascinated from\n\n06:22.240 --> 06:30.960\n the get go by AI. I was an English major. What did I know? And yet I was fascinated. And that's\n\n06:30.960 --> 06:38.080\n why you saw that historical, that literary background, which I think is very much a part\n\n06:38.080 --> 06:47.600\n of the continuum of AI, that AI grew out of that same impulse. That traditional, what was,\n\n06:47.600 --> 06:54.880\n what drew you to AI? How did you even think of it back then? What was the possibilities,\n\n06:54.880 --> 07:03.200\n the dreams? What was interesting to you? The idea of intelligence outside the human cranium,\n\n07:03.200 --> 07:08.000\n this was a phenomenal idea. And even when I finished Machines Who Think,\n\n07:08.960 --> 07:15.120\n I didn't know if they were going to succeed. In fact, the final chapter is very wishy washy,\n\n07:15.120 --> 07:25.760\n frankly. Succeed, the field did. Yeah. So was there the idea that AI began with the wish to\n\n07:25.760 --> 07:33.760\n forge the gods? So the spiritual component that we crave to create this other thing greater than\n\n07:33.760 --> 07:42.320\n ourselves. For those guys, I don't think so. Newell and Simon were cognitive psychologists.\n\n07:42.320 --> 07:49.040\n What they wanted was to simulate aspects of human intelligence,\n\n07:49.040 --> 07:57.280\n and they found they could do it on the computer. Minsky just thought it was a really cool thing\n\n07:57.280 --> 08:06.160\n to do. Likewise, McCarthy. McCarthy had got the idea in 1949 when he was a Caltech student.\n\n08:06.160 --> 08:15.520\n And he listened to somebody's lecture. It's in my book. I forget who it was. And he thought,\n\n08:15.520 --> 08:20.560\n oh, that would be fun to do. How do we do that? And he took a very mathematical approach.\n\n08:21.520 --> 08:29.440\n Minsky was hybrid, and Newell and Simon were very much cognitive psychology. How can we simulate\n\n08:29.440 --> 08:37.120\n various things about human cognition? What happened over the many years is, of course,\n\n08:37.120 --> 08:44.800\n our definition of intelligence expanded tremendously. These days, biologists are\n\n08:44.800 --> 08:49.240\n comfortable talking about the intelligence of the cell, the intelligence of the brain,\n\n08:49.240 --> 09:00.560\n not just human brain, but the intelligence of any kind of brain. Cephalopods, I mean, an octopus is\n\n09:00.560 --> 09:06.880\n really intelligent by any amount. We wouldn't have thought of that in the 60s, even the 70s.\n\n09:06.880 --> 09:16.320\n So all these things have worked in. And I did hear one behavioral primatologist, Franz De Waal,\n\n09:16.320 --> 09:26.240\n say, AI taught us the questions to ask. Yeah, this is what happens, right? When you try to build it,\n\n09:26.240 --> 09:32.400\n is when you start to actually ask questions. It puts a mirror to ourselves. Yeah, right. So you\n\n09:32.400 --> 09:38.880\n were there in the middle of it. It seems like not many people were asking the questions that\n\n09:38.880 --> 09:45.920\n you were, or just trying to look at this field the way you were. I was so low. When I went to\n\n09:45.920 --> 09:53.800\n get funding for this because I needed somebody to transcribe the interviews and I needed travel\n\n09:53.800 --> 10:07.160\n expenses, I went to everything you could think of, the NSF, the DARPA. There was an Air Force\n\n10:07.160 --> 10:15.480\n place that doled out money. And each of them said, well, that's a very interesting idea.\n\n10:15.480 --> 10:23.960\n But we'll think about it. And the National Science Foundation actually said to me in plain English,\n\n10:23.960 --> 10:30.480\n hey, you're only a writer. You're not a historian of science. And I said, yeah, that's true. But\n\n10:30.480 --> 10:35.400\n the historians of science will be crawling all over this field. I'm writing for the general\n\n10:35.400 --> 10:43.880\n audience, so I thought. And they still wouldn't budge. I finally got a private grant without\n\n10:43.880 --> 10:51.400\n knowing who it was from, from Ed Fredkin at MIT. He was a wealthy man, and he liked what he called\n\n10:51.400 --> 10:58.680\n crackpot ideas. And he considered this a crackpot idea, and he was willing to support it. I am ever\n\n10:58.680 --> 11:06.720\n grateful, let me say that. Some would say that a history of science approach to AI, or even just a\n\n11:06.720 --> 11:13.760\n history, or anything like the book that you've written, hasn't been written since. Maybe I'm\n\n11:13.760 --> 11:20.240\n not familiar, but it's certainly not many. If we think about bigger than just these couple of\n\n11:20.240 --> 11:30.640\n decades, few decades, what are the roots of AI? Oh, they go back so far. Yes, of course, there's\n\n11:30.640 --> 11:41.240\n all the legendary stuff, the Golem and the early robots of the 20th century. But they go back much\n\n11:41.240 --> 11:49.680\n further than that. If you read Homer, Homer has robots in the Iliad. And a classical scholar was\n\n11:49.680 --> 11:54.120\n pointing out to me just a few months ago, well, you said you just read the Odyssey. The Odyssey\n\n11:54.120 --> 12:00.800\n is full of robots. It is, I said? Yeah. How do you think Odysseus's ship gets from one place to\n\n12:00.800 --> 12:07.320\n another? He doesn't have the crew people to do that, the crewmen. Yeah, it's magic. It's robots.\n\n12:07.320 --> 12:17.240\n Oh, I thought, how interesting. So we've had this notion of AI for a long time. And then toward the\n\n12:17.240 --> 12:23.080\n end of the 19th century, the beginning of the 20th century, there were scientists who actually\n\n12:23.080 --> 12:29.520\n tried to make this happen some way or another, not successfully. They didn't have the technology for\n\n12:29.520 --> 12:40.080\n it. And of course, Babbage in the 1850s and 60s, he saw that what he was building was capable of\n\n12:40.080 --> 12:47.080\n intelligent behavior. And when he ran out of funding, the British government finally said,\n\n12:47.080 --> 12:55.880\n that's enough. He and Lady Lovelace decided, oh, well, why don't we play the ponies with this? He\n\n12:55.880 --> 13:02.400\n had other ideas for raising money too. But if we actually reach back once again, I think people\n\n13:02.400 --> 13:09.160\n don't actually really know that robots do appear and ideas of robots. You talk about the Hellenic\n\n13:09.160 --> 13:16.760\n and the Hebraic points of view. Oh, yes. Can you tell me about each? I defined it this way. The\n\n13:16.760 --> 13:25.160\n Hellenic point of view is robots are great. They are party help. They help this guy Hephaestus,\n\n13:25.160 --> 13:32.560\n this god Hephaestus in his forge. I presume he made them to help him and so on and so forth.\n\n13:32.560 --> 13:40.120\n And they welcome the whole idea of robots. The Hebraic view has to do with, I think it's the\n\n13:40.120 --> 13:47.280\n second commandment, thou shalt not make any graven image. In other words, you better not\n\n13:47.280 --> 13:55.200\n start imitating humans because that's just forbidden. It's the second commandment. And\n\n13:55.200 --> 14:08.800\n a lot of the reaction to artificial intelligence has been a sense that this is somehow wicked,\n\n14:08.800 --> 14:17.600\n this is somehow blasphemous. We shouldn't be going there. Now, you can say, yeah, but there are going\n\n14:17.600 --> 14:21.840\n to be some downsides. And I say, yes, there are, but blasphemy is not one of them.\n\n14:21.840 --> 14:29.800\n You know, there is a kind of fear that feels to be almost primal. Is there religious roots to that?\n\n14:29.800 --> 14:36.280\n Because so much of our society has religious roots. And so there is a feeling of, like you\n\n14:36.280 --> 14:43.800\n said, blasphemy of creating the other, of creating something, you know, it doesn't have to be\n\n14:43.800 --> 14:48.640\n artificial intelligence. It's creating life in general. It's the Frankenstein idea.\n\n14:48.640 --> 14:56.080\n There's the annotated Frankenstein on my coffee table. It's a tremendous novel. It really is just\n\n14:56.080 --> 15:03.880\n beautifully perceptive. Yes, we do fear this and we have good reason to fear it,\n\n15:03.880 --> 15:08.760\n but because it can get out of hand. Maybe you can speak to that fear,\n\n15:08.760 --> 15:12.960\n the psychology, if you've thought about it. You know, there's a practical set of fears,\n\n15:12.960 --> 15:17.800\n concerns in the short term. You can think if we actually think about artificial intelligence\n\n15:17.800 --> 15:29.160\n systems, you can think about bias of discrimination in algorithms. You can think about their social\n\n15:29.160 --> 15:35.520\n networks have algorithms that recommend the content you see, thereby these algorithms control\n\n15:35.520 --> 15:40.320\n the behavior of the masses. There's these concerns. But to me, it feels like the fear\n\n15:40.320 --> 15:46.280\n that people have is deeper than that. So have you thought about the psychology of it?\n\n15:46.280 --> 15:57.240\n I think in a superficial way I have. There is this notion that if we produce a machine that\n\n15:57.240 --> 16:01.240\n can think, it will outthink us and therefore replace us.\n\n16:01.240 --> 16:11.960\n I guess that's a primal fear of almost kind of a kind of mortality. So around the time you said\n\n16:11.960 --> 16:21.760\n you worked at Stanford with Ed Feigenbaum. So let's look at that one person. Throughout his\n\n16:21.760 --> 16:31.240\n history, clearly a key person, one of the many in the history of AI. How has he changed in general\n\n16:31.240 --> 16:36.440\n around him? How has Stanford changed in the last, how many years are we talking about here?\n\n16:36.440 --> 16:38.400\n Oh, since 65.\n\n16:38.400 --> 16:45.000\n 65. So maybe it doesn't have to be about him. It could be bigger. But because he was a key\n\n16:45.000 --> 16:54.160\n person in expert systems, for example, how is that, how are these folks who you've interviewed in the\n\n16:54.160 --> 16:58.360\n 70s, 79 changed through the decades?\n\n16:58.360 --> 17:12.240\n In Ed's case, I know him well. We are dear friends. We see each other every month or so. He told me\n\n17:12.240 --> 17:17.040\n that when Machines Who Think first came out, he really thought all the front matter was kind of\n\n17:17.040 --> 17:27.040\n bologna. And 10 years later, he said, no, I see what you're getting at. Yes, this is an impulse\n\n17:27.040 --> 17:34.800\n that has been a human impulse for thousands of years to create something outside the human\n\n17:34.800 --> 17:46.000\n cranium that has intelligence. I think it's very hard when you're down at the algorithmic level,\n\n17:46.000 --> 17:53.000\n and you're just trying to make something work, which is hard enough to step back and think of\n\n17:53.000 --> 17:59.720\n the big picture. It reminds me of when I was in Santa Fe, I knew a lot of archaeologists,\n\n17:59.720 --> 18:07.920\n which was a hobby of mine. And I would say, yeah, yeah, well, you can look at the shards and say,\n\n18:07.920 --> 18:14.080\n oh, this came from this tribe and this came from this trade route and so on. But what about the big\n\n18:14.080 --> 18:21.840\n picture? And a very distinguished archaeologist said to me, they don't think that way. No,\n\n18:21.840 --> 18:30.520\n they're trying to match the shard to where it came from. Where did the remainder of this corn\n\n18:30.520 --> 18:37.360\n come from? Was it grown here? Was it grown elsewhere? And I think this is part of any\n\n18:37.360 --> 18:46.800\n scientific field. You're so busy doing the hard work, and it is hard work, that you don't step\n\n18:46.800 --> 18:53.120\n back and say, oh, well, now let's talk about the general meaning of all this. Yes.\n\n18:53.120 --> 18:58.320\n So none of the even Minsky and McCarthy, they...\n\n18:58.320 --> 19:01.840\n Oh, those guys did. Yeah. The founding fathers did.\n\n19:01.840 --> 19:03.920\n Early on or later?\n\n19:03.920 --> 19:11.200\n Pretty early on. But in a different way from how I looked at it. The two cognitive psychologists,\n\n19:11.200 --> 19:20.960\n Newell and Simon, they wanted to imagine reforming cognitive psychology so that we would really,\n\n19:20.960 --> 19:31.520\n really understand the brain. Minsky was more speculative. And John McCarthy saw it as,\n\n19:32.960 --> 19:40.320\n I think I'm doing him right by this, he really saw it as a great boon for human beings to have\n\n19:40.320 --> 19:48.000\n this technology. And that was reason enough to do it. And he had wonderful, wonderful\n\n19:48.880 --> 19:56.800\n fables about how if you do the mathematics, you will see that these things are really good for\n\n19:56.800 --> 20:03.440\n human beings. And if you had a technological objection, he had an answer, a technological\n\n20:03.440 --> 20:10.320\n answer. But here's how we could get over that and then blah, blah, blah. And one of his favorite things\n\n20:10.320 --> 20:15.680\n was what he called the literary problem, which of course he presented to me several times.\n\n20:16.400 --> 20:23.680\n That is everything in literature, there are conventions in literature. One of the conventions\n\n20:23.680 --> 20:36.160\n is that you have a villain and a hero. And the hero in most literature is human,\n\n20:36.160 --> 20:41.680\n and the villain in most literature is a machine. And he said, that's just not the way it's going\n\n20:41.680 --> 20:47.600\n to be. But that's the way we're used to it. So when we tell stories about AI, it's always\n\n20:47.600 --> 20:57.040\n with this paradigm. I thought, yeah, he's right. Looking back, the classics RUR is certainly the\n\n20:57.040 --> 21:04.000\n machines trying to overthrow the humans. Frankenstein is different. Frankenstein is\n\n21:06.400 --> 21:13.440\n a creature. He never has a name. Frankenstein, of course, is the guy who created him, the human,\n\n21:13.440 --> 21:22.320\n Dr. Frankenstein. This creature wants to be loved, wants to be accepted. And it is only when\n\n21:22.320 --> 21:32.800\n Frankenstein turns his head, in fact, runs the other way. And the creature is without love,\n\n21:34.480 --> 21:38.560\n that he becomes the monster that he later becomes.\n\n21:38.560 --> 21:43.840\n So who's the villain in Frankenstein? It's unclear, right?\n\n21:43.840 --> 21:45.520\n Oh, it is unclear, yeah.\n\n21:45.520 --> 21:54.240\n It's really the people who drive him. By driving him away, they bring out the worst.\n\n21:54.240 --> 22:00.800\n That's right. They give him no human solace. And he is driven away, you're right.\n\n22:00.800 --> 22:08.160\n He becomes, at one point, the friend of a blind man. And he serves this blind man,\n\n22:08.160 --> 22:14.880\n and they become very friendly. But when the sighted people of the blind man's family come in,\n\n22:14.880 --> 22:23.040\n ah, you've got a monster here. So it's very didactic in its way. And what I didn't know\n\n22:23.040 --> 22:31.120\n is that Mary Shelley and Percy Shelley were great readers of the literature surrounding abolition\n\n22:31.120 --> 22:38.720\n in the United States, the abolition of slavery. And they picked that up wholesale. You are making\n\n22:38.720 --> 22:44.000\n monsters of these people because you won't give them the respect and love that they deserve.\n\n22:44.000 --> 22:52.000\n Do you have, if we get philosophical for a second, do you worry that once we create\n\n22:52.000 --> 22:56.960\n machines that are a little bit more intelligent, let's look at Roomba, the vacuums, the cleaner,\n\n22:58.080 --> 23:08.080\n that this darker part of human nature where we abuse the other, the somebody who's different,\n\n23:08.800 --> 23:09.600\n will come out?\n\n23:09.600 --> 23:18.560\n I don't worry about it. I could imagine it happening. But I think that what AI has to offer\n\n23:18.560 --> 23:24.800\n the human race will be so attractive that people will be won over.\n\n23:25.760 --> 23:32.480\n So you have looked deep into these people, had deep conversations, and it's interesting to get\n\n23:32.480 --> 23:42.720\n a sense of stories of the way they were thinking and the way it was changed, the way your own\n\n23:42.720 --> 23:51.840\n thinking about AI has changed. So you mentioned McCarthy. What about the years at CMU, Carnegie\n\n23:51.840 --> 24:02.800\n Mellon, with Joe? Sure. Joe was not in AI. He was in algorithmic complexity.\n\n24:03.440 --> 24:07.280\n Was there always a line between AI and computer science, for example?\n\n24:07.280 --> 24:10.880\n Is AI its own place of outcasts? Was that the feeling?\n\n24:10.880 --> 24:24.560\n There was a kind of outcast period for AI. For instance, in 1974, the new field was hardly 10\n\n24:24.560 --> 24:31.680\n years old. The new field of computer science was asked by the National Science Foundation,\n\n24:31.680 --> 24:34.400\n I believe, but it may have been the National Academies, I can't remember,\n\n24:34.400 --> 24:43.200\n to tell your fellow scientists where computer science is and what it means.\n\n24:44.160 --> 24:53.520\n And they wanted to leave out AI. And they only agreed to put it in because Don Knuth said,\n\n24:53.520 --> 24:57.280\n hey, this is important. You can't just leave that out.\n\n24:57.280 --> 24:58.240\n Really? Don, dude?\n\n24:58.240 --> 24:59.680\n Don Knuth, yes.\n\n24:59.680 --> 25:02.960\n I talked to him recently, too. Out of all the people.\n\n25:02.960 --> 25:08.640\n Yes. But you see, an AI person couldn't have made that argument. He wouldn't have been believed.\n\n25:08.640 --> 25:10.800\n But Knuth was believed. Yes.\n\n25:10.800 --> 25:15.200\n So Joe Traub worked on the real stuff.\n\n25:15.200 --> 25:22.160\n Joe was working on algorithmic complexity. But he would say in plain English again and again,\n\n25:22.160 --> 25:24.720\n the smartest people I know are in AI.\n\n25:24.720 --> 25:25.280\n Really?\n\n25:25.280 --> 25:35.120\n Oh, yes. No question. Anyway, Joe loved these guys. What happened was that I guess it was\n\n25:35.760 --> 25:40.480\n as I started to write Machines Who Think, Herb Simon and I became very close friends.\n\n25:41.360 --> 25:47.200\n He would walk past our house on Northumberland Street every day after work. And I would just\n\n25:47.200 --> 25:52.160\n be putting my cover on my typewriter. And I would lean out the door and say,\n\n25:52.160 --> 25:58.880\n Herb, would you like a sherry? And Herb almost always would like a sherry. So he'd stop in\n\n25:59.440 --> 26:06.000\n and we'd talk for an hour, two hours. My journal says we talked this afternoon for three hours.\n\n26:06.720 --> 26:11.680\n What was on his mind at the time in terms of on the AI side of things?\n\n26:11.680 --> 26:14.640\n Oh, we didn't talk too much about AI. We talked about other things.\n\n26:14.640 --> 26:15.680\n Just life.\n\n26:15.680 --> 26:24.000\n We both love literature. And Herb had read Proust in the original French twice all the\n\n26:24.000 --> 26:30.480\n way through. I can't. I've read it in English in translation. So we talked about literature.\n\n26:30.480 --> 26:36.240\n We talked about languages. We talked about music because he loved music. We talked about\n\n26:36.240 --> 26:44.960\n art because he was actually enough of a painter that he had to give it up because he was afraid\n\n26:44.960 --> 26:50.960\n it was interfering with his research and so on. So no, it was really just chat, chat.\n\n26:51.520 --> 26:59.360\n But it was very warm. So one summer I said to Herb, my students have all the really\n\n26:59.360 --> 27:03.920\n interesting conversations. I was teaching at the University of Pittsburgh then in the English\n\n27:03.920 --> 27:09.920\n department. They get to talk about the meaning of life and that kind of thing. And what do I have?\n\n27:09.920 --> 27:17.040\n I have university meetings where we talk about the photocopying budget and whether the course\n\n27:17.040 --> 27:23.040\n on romantic poetry should be one semester or two. So Herb laughed. He said, yes, I know what you\n\n27:23.040 --> 27:30.640\n mean. He said, but you could do something about that. Dot, that was his wife, Dot and I used to\n\n27:30.640 --> 27:38.560\n have a salon at the University of Chicago every Sunday night. And we would have essentially an\n\n27:38.560 --> 27:45.360\n open house and people knew. It wasn't for a small talk. It was really for some topic of\n\n27:47.600 --> 27:54.480\n depth. He said, but my advice would be that you choose the topic ahead of time. Fine, I said.\n\n27:54.480 --> 28:01.680\n So we exchanged mail over the summer. That was US Post in those days because\n\n28:01.680 --> 28:11.360\n you didn't have personal email. And I decided I would organize it and there would be eight of us,\n\n28:12.000 --> 28:21.200\n Alan Noland, his wife, Herb Simon and his wife Dorothea. There was a novelist in town,\n\n28:21.200 --> 28:29.680\n a man named Mark Harris. He had just arrived and his wife Josephine. Mark was most famous then for\n\n28:29.680 --> 28:35.920\n a novel called Bang the Drum Slowly, which was about baseball. And Joe and me, so eight people.\n\n28:36.720 --> 28:45.760\n And we met monthly and we just sank our teeth into really hard topics and it was great fun.\n\n28:45.760 --> 28:52.080\n TK How have your own views around artificial intelligence changed\n\n28:53.600 --> 28:57.440\n through the process of writing Machines Who Think and afterwards, the ripple effects?\n\n28:57.440 --> 29:04.160\n RL I was a little skeptical that this whole thing would work out. It didn't matter. To me,\n\n29:04.160 --> 29:16.800\n it was so audacious. AI generally. And in some ways, it hasn't worked out the way I expected\n\n29:16.800 --> 29:26.880\n so far. That is to say, there's this wonderful lot of apps, thanks to deep learning and so on.\n\n29:26.880 --> 29:37.920\n But those are algorithmic. And in the part of symbolic processing, there's very little yet.\n\n29:39.120 --> 29:45.600\n And that's a field that lies waiting for industrious graduate students.\n\n29:45.600 --> 29:53.040\n TK Maybe you can tell me some figures that popped up in your life in the 80s with expert systems\n\n29:53.040 --> 30:00.320\n where there was the symbolic AI possibilities of what most people think of as AI,\n\n30:00.960 --> 30:07.520\n if you dream of the possibilities of AI, it's really expert systems. And those hit a few walls\n\n30:07.520 --> 30:12.080\n and there was challenges there. And I think, yes, they will reemerge again with some new\n\n30:12.080 --> 30:17.760\n breakthroughs and so on. But what did that feel like, both the possibility and the winter that\n\n30:17.760 --> 30:24.480\n followed the slowdown in research? BG Ah, you know, this whole thing about AI winter is to me\n\n30:25.040 --> 30:26.960\n a crock. TK Snow winters.\n\n30:26.960 --> 30:33.760\n BG Because I look at the basic research that was being done in the 80s, which is supposed to be,\n\n30:34.480 --> 30:40.320\n my God, it was really important. It was laying down things that nobody had thought about before,\n\n30:40.320 --> 30:44.880\n but it was basic research. You couldn't monetize it. Hence the winter.\n\n30:44.880 --> 30:49.120\n TK That's the winter. BG You know, research,\n\n30:49.120 --> 30:53.680\n scientific research goes and fits and starts. It isn't this nice smooth,\n\n30:54.240 --> 30:59.040\n oh, this follows this follows this. No, it just doesn't work that way.\n\n30:59.040 --> 31:03.600\n TK The interesting thing, the way winters happen, it's never the fault of the researchers.\n\n31:05.760 --> 31:12.000\n It's the some source of hype over promising. Well, no, let me take that back. Sometimes it\n\n31:12.000 --> 31:17.200\n is the fault of the researchers. Sometimes certain researchers might over promise the\n\n31:17.200 --> 31:23.520\n possibilities. They themselves believe that we're just a few years away. Sort of just recently\n\n31:23.520 --> 31:28.160\n talked to Elon Musk and he believes he'll have an autonomous vehicle, will have autonomous vehicles\n\n31:28.160 --> 31:30.640\n in a year. And he believes it. BG A year?\n\n31:30.640 --> 31:33.360\n TK A year. Yeah. With mass deployment of a time.\n\n31:33.360 --> 31:38.640\n BG For the record, this is 2019 right now. So he's talking 2020.\n\n31:38.640 --> 31:44.480\n TK To do the impossible, you really have to believe it. And I think what's going to happen\n\n31:44.480 --> 31:47.360\n when you believe it, because there's a lot of really brilliant people around him,\n\n31:48.240 --> 31:53.840\n is some good stuff will come out of it. Some unexpected brilliant breakthroughs will come out\n\n31:53.840 --> 31:58.480\n of it when you really believe it, when you work that hard. BG I believe that. And I believe\n\n31:58.480 --> 32:02.640\n autonomous vehicles will come. I just don't believe it'll be in a year. I wish.\n\n32:02.640 --> 32:09.120\n TK But nevertheless, there's, autonomous vehicles is a good example. There's a feeling\n\n32:09.120 --> 32:16.640\n many companies have promised by 2021, by 2022, Ford, GM, basically every single automotive\n\n32:16.640 --> 32:21.440\n company has promised they'll have autonomous vehicles. So that kind of over promise is what\n\n32:21.440 --> 32:26.720\n leads to the winter. Because we'll come to those dates, there won't be autonomous vehicles.\n\n32:26.720 --> 32:32.080\n BG And there'll be a feeling, well, wait a minute, if we took your word at that time,\n\n32:32.080 --> 32:39.680\n that means we just spent billions of dollars, had made no money, and there's a counter response to\n\n32:39.680 --> 32:46.880\n where everybody gives up on it. Sort of intellectually, at every level, the hope just\n\n32:46.880 --> 32:52.960\n dies. And all that's left is a few basic researchers. So you're uncomfortable with\n\n32:52.960 --> 32:58.400\n some aspects of this idea. TK Well, it's the difference between science and commerce.\n\n32:58.400 --> 33:04.160\n BG So you think science goes on the way it does?\n\n33:04.160 --> 33:12.000\n TK Oh, science can really be killed by not getting proper funding or timely funding.\n\n33:14.160 --> 33:19.440\n I think Great Britain was a perfect example of that. The Lighthill report in,\n\n33:19.440 --> 33:26.560\n I can't remember the year, essentially said, there's no use Great Britain putting any money\n\n33:26.560 --> 33:35.600\n into this, it's going nowhere. And this was all about social factions in Great Britain.\n\n33:37.040 --> 33:44.720\n Edinburgh hated Cambridge and Cambridge hated Manchester. Somebody else can write that story.\n\n33:44.720 --> 33:54.400\n But it really did have a hard effect on research there. Now, they've come roaring back with Deep\n\n33:54.400 --> 34:03.760\n Mind. But that's one guy and his visionaries around him. BG But just to push on that,\n\n34:03.760 --> 34:08.320\n it's kind of interesting. You have this dislike of the idea of an AI winter.\n\n34:08.320 --> 34:15.440\n Where's that coming from? Where were you? TK Oh, because I just don't think it's true.\n\n34:15.440 --> 34:21.280\n BG There was a particular period of time. It's a romantic notion, certainly.\n\n34:21.280 --> 34:33.280\n TK Yeah, well. No, I admire science, perhaps more than I admire commerce. Commerce is fine. Hey,\n\n34:33.280 --> 34:45.920\n you know, we all gotta live. But science has a much longer view than commerce and continues\n\n34:46.720 --> 34:56.400\n almost regardless. It can't continue totally regardless, but almost regardless of what's\n\n34:56.400 --> 35:01.680\n saleable and what's not, what's monetizable and what's not. BG So the winter is just something\n\n35:01.680 --> 35:10.960\n that happens on the commerce side, and the science marches. That's a beautifully optimistic\n\n35:10.960 --> 35:16.400\n and inspiring message. I agree with you. I think if we look at the key people that work in AI,\n\n35:16.400 --> 35:22.160\n that work in key scientists in most disciplines, they continue working out of the love for science.\n\n35:22.160 --> 35:30.480\n You can always scrape up some funding to stay alive, and they continue working diligently.\n\n35:31.680 --> 35:38.080\n But there certainly is a huge amount of funding now, and there's a concern on the AI side and\n\n35:38.080 --> 35:44.160\n deep learning. There's a concern that we might, with over promising, hit another slowdown in\n\n35:44.160 --> 35:47.520\n funding, which does affect the number of students, you know, that kind of thing.\n\n35:47.520 --> 35:52.080\n RG Yeah, it does. BG So the kind of ideas you had in Machines Who Think,\n\n35:52.640 --> 35:56.240\n did you continue that curiosity through the decades that followed?\n\n35:56.240 --> 36:03.840\n RG Yes, I did. BG And what was your view, historical view of how AI community evolved,\n\n36:03.840 --> 36:09.280\n the conversations about it, the work? Has it persisted the same way from its birth?\n\n36:09.280 --> 36:19.760\n RG No, of course not. It's just as we were just talking, the symbolic AI really kind of dried up\n\n36:19.760 --> 36:26.640\n and it all became algorithmic. I remember a young AI student telling me what he was doing,\n\n36:27.200 --> 36:33.200\n and I had been away from the field long enough. I'd gotten involved with complexity at the Santa\n\n36:33.200 --> 36:40.960\n Fe Institute. I thought, algorithms, yeah, they're in the service of, but they're not the main event.\n\n36:41.680 --> 36:49.440\n No, they became the main event. That surprised me. And we all know the downside of this. We all\n\n36:49.440 --> 36:58.240\n know that if you're using an algorithm to make decisions based on a gazillion human decisions,\n\n36:58.240 --> 37:04.480\n baked into it are all the mistakes that humans make, the bigotries, the short sightedness,\n\n37:05.440 --> 37:13.280\n and so on and so on. BG So you mentioned Santa Fe Institute. So you've written the novel\n\n37:13.280 --> 37:20.720\n Edge of Chaos, but it's inspired by the ideas of complexity, a lot of which have been extensively\n\n37:20.720 --> 37:31.200\n explored at the Santa Fe Institute. It's another fascinating topic, just sort of emergent\n\n37:31.200 --> 37:37.440\n complexity from chaos. Nobody knows how it happens really, but it seems to where all the interesting\n\n37:37.440 --> 37:44.480\n stuff does happen. So how did first, not your novel, but just complexity in general and the\n\n37:44.480 --> 37:50.960\n work at Santa Fe, fit into the bigger puzzle of the history of AI? Or maybe even your personal\n\n37:51.600 --> 37:56.320\n journey through that? RG One of the last projects I did\n\n37:57.760 --> 38:06.080\n concerning AI in particular was looking at the work of Harold Cohen, the painter. And Harold was\n\n38:06.080 --> 38:17.920\n deeply involved with AI. He was a painter first. And what his project, ARIN, which was a lifelong\n\n38:17.920 --> 38:30.480\n project, did was reflect his own cognitive processes. Okay. Harold and I, even though I wrote\n\n38:30.480 --> 38:39.120\n a book about it, we had a lot of friction between us. And I went, I thought, this is it. The book\n\n38:39.120 --> 38:47.760\n died. It was published and fell into a ditch. This is it. I'm finished. It's time for me to\n\n38:47.760 --> 38:55.840\n do something different. By chance, this was a sabbatical year for my husband. And we spent two\n\n38:55.840 --> 39:03.120\n months at the Santa Fe Institute and two months at Caltech. And then the spring semester in Munich,\n\n39:03.120 --> 39:15.040\n Germany. Okay. Those two months at the Santa Fe Institute were so restorative for me. And I began\n\n39:15.040 --> 39:22.560\n to, the Institute was very small then. It was in some kind of office complex on old Santa Fe trail.\n\n39:22.560 --> 39:29.840\n Everybody kept their door open. So you could crack your head on a problem. And if you finally didn't\n\n39:29.840 --> 39:39.040\n get it, you could walk in to see Stuart Kaufman or any number of people and say, I don't get this.\n\n39:39.040 --> 39:46.880\n Can you explain? And one of the people that I was talking to about complex adaptive systems\n\n39:46.880 --> 39:55.200\n was Murray Gelman. And I told Murray what Harold Cohen had done. And I said, you know,\n\n39:55.200 --> 40:02.240\n this sounds to me like a complex adaptive system. And he said, yeah, it is. Well, what do you know?\n\n40:02.240 --> 40:09.120\n Harold Aaron had all these kids and cousins all over the world in science and in economics and\n\n40:09.120 --> 40:16.480\n so on and so forth. I was so relieved. I thought, okay, your instincts are okay. You're doing the\n\n40:16.480 --> 40:21.760\n right thing. I didn't have the vocabulary. And that was one of the things that the Santa Fe\n\n40:21.760 --> 40:26.880\n Institute gave me. If I could have rewritten that book, no, it had just come out. I couldn't rewrite\n\n40:26.880 --> 40:34.480\n it. I would have had a vocabulary to explain what Aaron was doing. Okay. So I got really interested\n\n40:34.480 --> 40:44.080\n in what was going on at the Institute. The people were, again, bright and funny and willing to\n\n40:44.080 --> 40:51.600\n explain anything to this amateur. George Cowan, who was then the head of the Institute, said he\n\n40:51.600 --> 40:58.800\n thought it might be a nice idea if I wrote a book about the Institute. And I thought about it and I\n\n40:58.800 --> 41:05.920\n had my eye on some other project, God knows what. And I said, I'm sorry, George. Yeah, I'd really\n\n41:05.920 --> 41:11.440\n love to do it, but just not going to work for me at this moment. He said, oh, too bad. I think it\n\n41:11.440 --> 41:17.120\n would make an interesting book. Well, he was right and I was wrong. I wish I'd done it. But that's\n\n41:17.120 --> 41:22.080\n interesting. I hadn't thought about that, that that was a road not taken that I wish I'd taken.\n\n41:22.080 --> 41:31.680\n Well, you know what? Just on that point, it's quite brave for you as a writer, as sort of\n\n41:31.680 --> 41:37.120\n coming from a world of literature and the literary thinking and historical thinking. I mean, just\n\n41:37.120 --> 41:49.600\n from that world and bravely talking to quite, I assume, large egos in AI or in complexity.\n\n41:49.600 --> 41:59.040\n Yeah, in AI or in complexity and so on. How'd you do it? I mean, I suppose they could be\n\n41:59.040 --> 42:03.120\n intimidated of you as well because it's two different worlds coming together.\n\n42:03.120 --> 42:06.080\n I never picked up that anybody was intimidated by me.\n\n42:06.080 --> 42:08.640\n But how were you brave enough? Where did you find the guts to sort of...\n\n42:08.640 --> 42:14.000\n God, just dumb luck. I mean, this is an interesting rock to turn over. I'm going\n\n42:14.000 --> 42:18.880\n to write a book about it. And you know, people have enough patience with writers\n\n42:18.880 --> 42:24.240\n if they think they're going to end up in a book that they let you flail around and so on.\n\n42:24.800 --> 42:27.360\n Well, but they also look if the writer has,\n\n42:28.320 --> 42:31.120\n if there's a sparkle in their eye, if they get it.\n\n42:31.120 --> 42:31.680\n Yeah, sure.\n\n42:32.640 --> 42:34.720\n When were you at the Santa Fe Institute?\n\n42:35.920 --> 42:46.240\n The time I'm talking about is 1990, 1991, 1992. But we then, because Joe was an external faculty\n\n42:46.240 --> 42:52.640\n member, were in Santa Fe every summer. We bought a house there and I didn't have that much to do\n\n42:52.640 --> 42:57.920\n with the Institute anymore. I was writing my novels. I was doing whatever I was doing.\n\n43:00.560 --> 43:04.320\n But I loved the Institute and I loved\n\n43:08.400 --> 43:12.960\n again, the audacity of the ideas. That really appeals to me.\n\n43:12.960 --> 43:22.160\n I think that there's this feeling, much like in great institutes of neuroscience, for example,\n\n43:23.040 --> 43:29.840\n that they're in it for the long game of understanding something fundamental about\n\n43:29.840 --> 43:36.800\n reality and nature. And that's really exciting. So if we start now to look a little bit more recently,\n\n43:36.800 --> 43:46.480\n how, you know, AI is really popular today. How is this world, you mentioned algorithmic,\n\n43:46.480 --> 43:51.680\n but in general, is the spirit of the people, the kind of conversations you hear through the\n\n43:51.680 --> 43:55.360\n grapevine and so on, is that different than the roots that you remember?\n\n43:55.360 --> 44:01.200\n No. The same kind of excitement, the same kind of, this is really going to make a difference\n\n44:01.200 --> 44:07.920\n in the world. And it will. It has. You know, a lot of folks, especially young, 20 years old or\n\n44:07.920 --> 44:14.000\n something, they think we've just found something special here. We're going to change the world\n\n44:14.000 --> 44:24.240\n tomorrow. On a time scale, do you have a sense of what, of the time scale at which breakthroughs\n\n44:24.240 --> 44:31.440\n of the time scale at which breakthroughs in AI happen? I really don't. Because look at Deep Learning.\n\n44:32.240 --> 44:44.720\n That was, Jeffrey Hinton came up with the algorithm in 86. But it took all these years\n\n44:44.720 --> 44:56.400\n for the technology to be good enough to actually be applicable. So no, I can't predict that at all.\n\n44:56.400 --> 45:02.480\n I can't. I wouldn't even try. Well, let me ask you to, not to try to predict, but to speak to the,\n\n45:03.760 --> 45:09.440\n you know, I'm sure in the 60s, as it continues now, there's people that think, let's call it,\n\n45:09.440 --> 45:16.160\n we can call it this fun word, the singularity. When there's a phase shift, there's some profound\n\n45:16.160 --> 45:22.720\n feeling where we're all really surprised by what's able to be achieved. I'm sure those dreams are\n\n45:22.720 --> 45:29.200\n there. I remember reading quotes in the 60s and those continued. How have your own views,\n\n45:29.200 --> 45:34.960\n maybe if you look back, about the timeline of a singularity changed?\n\n45:34.960 --> 45:45.760\n Well, I'm not a big fan of the singularity as Ray Kurzweil has presented it.\n\n45:46.640 --> 45:53.120\n How would you define the Ray Kurzweil? How do you think of singularity in those?\n\n45:53.120 --> 45:59.280\n If I understand Kurzweil's view, it's sort of, there's going to be this moment when machines\n\n45:59.280 --> 46:07.120\n are smarter than humans and, you know, game over. However, the game over is. I mean, do they put us\n\n46:07.120 --> 46:15.680\n on a reservation? Do they, et cetera, et cetera. And first of all, machines are smarter than humans\n\n46:15.680 --> 46:21.440\n in some ways all over the place. And they have been since adding machines were invented.\n\n46:21.440 --> 46:28.640\n So it's not, it's not going to come like some great eatable crossroads, you know, where\n\n46:29.440 --> 46:37.360\n they meet each other and our offspring, Oedipus says, you're dead. It's just not going to happen.\n\n46:37.920 --> 46:44.000\n Yeah. So it's already game over with calculators, right? They're already out to do much better at\n\n46:44.000 --> 46:51.920\n basic arithmetic than us. But you know, there's a human like intelligence. And it's not the ones\n\n46:51.920 --> 46:57.920\n that destroy us, but you know, somebody that you can have as a, as a friend, you can have deep\n\n46:57.920 --> 47:04.640\n connections with that kind of passing the touring test and beyond those kinds of ideas. Have you\n\n47:04.640 --> 47:10.560\n dreamt of those? Oh yes, yes, yes. Those possibilities. In a book I wrote with Ed Feigenbaum,\n\n47:10.560 --> 47:16.160\n a book I wrote with Ed Feigenbaum, there's a little story called the geriatric robot.\n\n47:17.280 --> 47:24.880\n And how I came up with the geriatric robot is a story in itself. But here's what the geriatric\n\n47:24.880 --> 47:29.520\n robot does. It doesn't just clean you up and feed you and wheel you out into the sun.\n\n47:29.520 --> 47:44.480\n It's great advantages. It listens. It says, tell me again about the great coup of 73. Tell me again\n\n47:45.280 --> 47:52.080\n about how awful or how wonderful your grandchildren are and so on and so forth.\n\n47:52.960 --> 47:59.440\n And it isn't hanging around to inherit your money. It isn't hanging around because it can't get\n\n47:59.440 --> 48:08.320\n any other job. This is his job. And so on and so forth. Well, I would love something like that.\n\n48:09.120 --> 48:15.680\n Yeah. I mean, for me, that deeply excites me. So I think there's a lot of us.\n\n48:15.680 --> 48:20.880\n Lex, you gotta know, it was a joke. I dreamed it up because I needed to talk to college students\n\n48:20.880 --> 48:26.960\n and I needed to give them some idea of what AI might be. And they were rolling in the aisles as\n\n48:26.960 --> 48:35.680\n I elaborated and elaborated and elaborated. When it went into the book, they took my hide off\n\n48:36.320 --> 48:41.280\n in the New York Review of Books. This is just what we have thought about these people in AI.\n\n48:41.280 --> 48:47.280\n They're inhuman. Come on, get over it. Don't you think that's a good thing for\n\n48:47.280 --> 48:52.000\n the world that AI could potentially do? I do. Absolutely. And furthermore,\n\n48:52.000 --> 49:02.560\n I'm pushing 80 now. By the time I need help like that, I also want it to roll itself in a corner\n\n49:02.560 --> 49:09.360\n and shut the fuck up. Let me linger on that point. Do you really though?\n\n49:09.360 --> 49:12.720\n Yeah, I do. Here's why. Don't you want it to push back a little bit?\n\n49:13.360 --> 49:20.240\n A little. But I have watched my friends go through the whole issue around having help\n\n49:20.240 --> 49:28.880\n in the house. And some of them have been very lucky and had fabulous help. And some of them\n\n49:28.880 --> 49:34.000\n have had people in the house who want to keep the television going on all day, who want to talk on\n\n49:34.000 --> 49:41.360\n their phones all day. No. Just roll yourself in the corner and shut the fuck up. Unfortunately,\n\n49:41.360 --> 49:47.040\n us humans, when we're assistants, we're still, even when we're assisting others,\n\n49:47.040 --> 49:54.800\n we care about ourselves more. Of course. And so you create more frustration. And a robot AI\n\n49:54.800 --> 50:01.520\n assistant can really optimize the experience for you. I was just speaking to the point,\n\n50:01.520 --> 50:05.360\n you actually bring up a very, very good point. But I was speaking to the fact that\n\n50:05.360 --> 50:11.120\n us humans are a little complicated, that we don't necessarily want a perfect servant.\n\n50:11.120 --> 50:20.800\n I don't, maybe you disagree with that, but there's a, I think there's a push and pull with humans.\n\n50:20.800 --> 50:21.360\n You're right.\n\n50:21.360 --> 50:27.680\n A little tension, a little mystery that, of course, that's really difficult for AI to get right. But\n\n50:27.680 --> 50:34.800\n I do sense, especially today with social media, that people are getting more and more lonely,\n\n50:34.800 --> 50:42.080\n even young folks, and sometimes especially young folks, that loneliness, there's a longing for\n\n50:42.080 --> 50:49.840\n connection and AI can help alleviate some of that loneliness. Some, just somebody who listens,\n\n50:50.800 --> 51:03.200\n like in person. So to speak. So to speak, yeah. So to speak. Yeah, that to me is really exciting.\n\n51:03.200 --> 51:08.880\n That is really exciting. But so if we look at that, that level of intelligence, which is\n\n51:08.880 --> 51:15.520\n exceptionally difficult to achieve actually, as the singularity or whatever, that's the human level\n\n51:15.520 --> 51:23.920\n bar, that people have dreamt of that too. Turing dreamt of it. He had a date timeline. Do you have,\n\n51:23.920 --> 51:27.840\n how have your own timeline evolved on past?\n\n51:27.840 --> 51:28.960\n I don't even think about it.\n\n51:28.960 --> 51:29.680\n You don't even think?\n\n51:29.680 --> 51:37.200\n No. Just this field has been so full of surprises for me.\n\n51:38.080 --> 51:42.080\n You're just taking in and see the fun about the basic science.\n\n51:42.080 --> 51:48.160\n Yeah. I just can't. Maybe that's because I've been around the field long enough to think,\n\n51:48.960 --> 51:54.720\n you know, don't go that way. Herb Simon was terrible about making these predictions of\n\n51:54.720 --> 52:00.640\n when this and that would happen. And he was a sensible guy.\n\n52:00.640 --> 52:03.360\n His quotes are often used, right?\n\n52:03.360 --> 52:04.880\n As a legend, yeah.\n\n52:04.880 --> 52:14.800\n Yeah. Do you have concerns about AI, the existential threats that many people\n\n52:14.800 --> 52:18.800\n like Elon Musk and Sam Harris and others are thinking about?\n\n52:18.800 --> 52:26.560\n Yeah. That takes up half a chapter in my book. I call it the male gaze.\n\n52:29.600 --> 52:35.200\n Well, you hear me out. The male gaze is actually a term from film criticism.\n\n52:36.240 --> 52:44.240\n And I'm blocking on the women who dreamed this up. But she pointed out how most movies were\n\n52:44.240 --> 52:52.240\n made from the male point of view, that women were objects, not subjects. They didn't have any\n\n52:53.760 --> 53:00.800\n agency and so on and so forth. So when Elon and his pals Hawking and so on came,\n\n53:01.520 --> 53:07.360\n AI is going to eat our lunch and our dinner and our midnight snack too, I thought, what?\n\n53:08.000 --> 53:13.120\n And I said to Ed Feigenbaum, oh, this is the first guy. First, these guys have always been\n\n53:13.120 --> 53:18.800\n the smartest guy on the block. And here comes something that might be smarter. Oh, let's stamp\n\n53:18.800 --> 53:23.360\n it out before it takes over. And Ed laughed. He said, I didn't think about it that way.\n\n53:24.080 --> 53:34.480\n But I did. I did. And it is the male gaze. Okay, suppose these things do have agency.\n\n53:34.480 --> 53:43.920\n Well, let's wait and see what happens. Can we imbue them with ethics? Can we imbue them\n\n53:43.920 --> 53:54.480\n with a sense of empathy? Or are they just going to be, I don't know, we've had centuries of guys\n\n53:54.480 --> 54:05.280\n like that. That's interesting that the ego, the male gaze is immediately threatened. And so you\n\n54:05.280 --> 54:16.240\n can't think in a patient, calm way of how the tech could evolve. Speaking of which, your 96 book,\n\n54:16.240 --> 54:23.840\n The Future of Women, I think at the time and now, certainly now, I mean, I'm sorry, maybe at the\n\n54:23.840 --> 54:30.800\n time, but I'm more cognizant of now, is extremely relevant. You and Nancy Ramsey talk about four\n\n54:30.800 --> 54:38.960\n possible futures of women in science and tech. So if we look at the decades before and after\n\n54:38.960 --> 54:46.800\n the book was released, can you tell a history, sorry, of women in science and tech and how it\n\n54:46.800 --> 54:54.320\n has evolved? How have things changed? Where do we stand? Not enough. They have not changed enough.\n\n54:54.320 --> 55:05.840\n The way that women are ground down in computing is simply unbelievable. But what are the four\n\n55:05.840 --> 55:13.520\n possible futures for women in tech from the book? What you're really looking at are various aspects\n\n55:13.520 --> 55:20.880\n of the present. So for each of those, you could say, oh yeah, we do have backlash. Look at what's\n\n55:20.880 --> 55:26.640\n happening with abortion and so on and so forth. We have one step forward, one step back.\n\n55:28.400 --> 55:33.440\n The golden age of equality was the hardest chapter to write. And I used something from\n\n55:33.440 --> 55:41.760\n the Santa Fe Institute, which is the sandpile effect, that you drop sand very slowly onto a pile\n\n55:41.760 --> 55:47.680\n and it grows and it grows and it grows until suddenly it just breaks apart. And\n\n55:50.240 --> 55:58.240\n in a way, Me Too has done that. That was the last drop of sand that broke everything apart.\n\n55:58.240 --> 56:03.760\n That was a perfect example of the sandpile effect. And that made me feel good. It didn't\n\n56:03.760 --> 56:10.480\n change all of society, but it really woke a lot of people up. But are you in general optimistic\n\n56:10.480 --> 56:17.120\n about maybe after Me Too? I mean, Me Too is about a very specific kind of thing.\n\n56:17.120 --> 56:18.800\n Boy, solve that and you solve everything.\n\n56:19.920 --> 56:23.200\n But are you in general optimistic about the future?\n\n56:23.200 --> 56:27.600\n Yes. I'm a congenital optimistic. I can't help it.\n\n56:28.400 --> 56:33.440\n What about AI? What are your thoughts about the future of AI?\n\n56:34.560 --> 56:40.080\n Of course, I get asked, what do you worry about? And the one thing I worry about is the things\n\n56:40.080 --> 56:47.440\n we can't anticipate. There's going to be something out of left field that we will just say,\n\n56:47.440 --> 56:56.800\n we weren't prepared for that. I am generally optimistic. When I first took up\n\n56:58.240 --> 57:05.760\n being interested in AI, like most people in the field, more intelligence was like more virtue.\n\n57:05.760 --> 57:13.520\n You know, what could be bad? And in a way, I still believe that. But I realize that my\n\n57:13.520 --> 57:19.440\n notion of intelligence has broadened. There are many kinds of intelligence,\n\n57:19.440 --> 57:22.640\n and we need to imbue our machines with those many kinds.\n\n57:24.720 --> 57:32.560\n So you've now just finished or in the process of finishing the book that you've been working\n\n57:32.560 --> 57:39.440\n on, the memoir, how have you changed? I know it's just writing, but how have you changed\n\n57:39.440 --> 57:46.800\n the process? If you look back, what kind of stuff did it bring up to you that surprised you,\n\n57:47.600 --> 57:55.840\n looking at the entirety of it all? The biggest thing, and it really wasn't a surprise,\n\n57:55.840 --> 58:07.520\n is how lucky I was. Oh, my. To have access to the beginning of a scientific field that is going to\n\n58:07.520 --> 58:20.240\n change the world. How did I luck out? And yes, of course, my view of things has widened a lot.\n\n58:20.240 --> 58:28.640\n If I can get back to one feminist part of our conversation. Without knowing it,\n\n58:28.640 --> 58:36.320\n it really was subconscious. I wanted AI to succeed because I was so tired of hearing\n\n58:36.320 --> 58:43.280\n that intelligence was inside the male cranium. And I thought if there was something out there\n\n58:43.280 --> 58:53.040\n that wasn't a male thinking and doing well, then that would put a lie to this whole notion of\n\n58:53.040 --> 59:01.600\n intelligence resides in the male cranium. I did not know that until one night Harold Cohen and I\n\n59:01.600 --> 59:09.600\n were having a glass of wine, maybe two, and he said, what drew you to AI? And I said, oh,\n\n59:09.600 --> 59:14.720\n you know, smartest people I knew, great project, blah, blah, blah. And I said, and I wanted\n\n59:14.720 --> 59:24.160\n something besides male smarts. And it just bubbled up out of me like, what?\n\n59:24.160 --> 59:32.000\n It's kind of brilliant, actually. So AI really humbles all of us and humbles the people that\n\n59:32.000 --> 59:35.360\n need to be humbled the most. Let's hope.\n\n59:35.360 --> 59:40.800\n Wow. That is so beautiful. Pamela, thank you so much for talking to me. It's really a huge honor.\n\n59:40.800 --> 59:41.840\n It's been a great pleasure.\n\n59:41.840 --> 1:00:05.840\n Thank you.\n\n"
}