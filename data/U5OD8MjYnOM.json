{
  "title": "Wojciech Zaremba: OpenAI Codex, GPT-3, Robotics, and the Future of AI | Lex Fridman Podcast #215",
  "id": "U5OD8MjYnOM",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:05.260\n The following is a conversation with Wojciech Zaremba, cofounder of OpenAI,\n\n00:05.520 --> 00:09.900\n which is one of the top organizations in the world doing artificial intelligence\n\n00:09.900 --> 00:11.520\n research and development.\n\n00:12.360 --> 00:17.820\n Wojciech is the head of language and cogeneration teams, building and doing\n\n00:17.820 --> 00:27.080\n research on GitHub Copilot, OpenAI Codex, and GPT 3, and who knows, 4, 5, 6,\n\n00:27.080 --> 00:33.800\n and, and, and plus one, and he also previously led OpenAI's robotic efforts.\n\n00:34.400 --> 00:39.320\n These are incredibly exciting projects to me that deeply challenge and expand\n\n00:39.600 --> 00:43.160\n our understanding of the structure and nature of intelligence.\n\n00:43.720 --> 00:49.040\n The 21st century, I think, may very well be remembered for a handful of\n\n00:49.040 --> 00:52.440\n revolutionary AI systems and their implementations.\n\n00:52.440 --> 00:57.520\n GPT, Codex, and applications of language models and transformers in general\n\n00:57.760 --> 01:03.680\n to the language and visual domains may very well be at the core of these AI\n\n01:03.680 --> 01:08.160\n systems. To support this podcast, please check out our sponsors.\n\n01:08.640 --> 01:10.360\n They're listed in the description.\n\n01:11.000 --> 01:14.880\n This is the Lex Friedman podcast, and here is my conversation\n\n01:15.040 --> 01:16.560\n with Wojciech Zaremba.\n\n01:16.560 --> 01:22.960\n You mentioned that Sam Altman asked about the Fermi Paradox, and the people\n\n01:22.960 --> 01:27.200\n at OpenAI had really sophisticated, interesting answers, so that's when you\n\n01:27.200 --> 01:29.200\n knew this is the right team to be working with.\n\n01:29.680 --> 01:33.280\n So let me ask you about the Fermi Paradox, about aliens.\n\n01:34.600 --> 01:38.560\n Why have we not found overwhelming evidence for aliens visiting Earth?\n\n01:39.520 --> 01:42.920\n I don't have a conviction in the answer, but rather kind of probabilistic\n\n01:42.920 --> 01:46.160\n perspective on what might be, let's say, possible answers.\n\n01:46.200 --> 01:51.480\n It's also interesting that the question itself even can touch on the, you\n\n01:51.480 --> 01:54.800\n know, your typical question of what's the meaning of life, because if you\n\n01:54.800 --> 01:58.280\n assume that, like, we don't see aliens because they destroy themselves, that\n\n01:58.280 --> 02:04.160\n kind of upweights the focus on making sure that we won't destroy ourselves.\n\n02:04.400 --> 02:10.160\n At the moment, the place where I am actually with my belief, and these\n\n02:10.160 --> 02:15.200\n things also change over the time, is I think that we might be alone in\n\n02:15.200 --> 02:19.880\n the universe, which actually makes life more, or let's say, consciousness\n\n02:19.880 --> 02:24.360\n life, more kind of valuable, and that means that we should more appreciate it.\n\n02:24.840 --> 02:26.000\n Have we always been alone?\n\n02:26.000 --> 02:29.000\n So what's your intuition about our galaxy, our universe?\n\n02:29.040 --> 02:34.160\n Is it just sprinkled with graveyards of intelligent civilizations, or are\n\n02:34.160 --> 02:37.920\n we truly, is life, intelligent life, truly unique?\n\n02:37.920 --> 02:41.880\n At the moment, my belief that it is unique, but I would say I could also,\n\n02:42.320 --> 02:47.640\n you know, there was like some footage released with UFO objects, which makes\n\n02:47.640 --> 02:49.480\n me actually doubt my own belief.\n\n02:49.560 --> 02:50.040\n Yes.\n\n02:51.040 --> 02:53.760\n Yeah, I can tell you one crazy answer that I have heard.\n\n02:53.960 --> 02:54.280\n Yes.\n\n02:55.280 --> 03:00.520\n So, apparently, when you look actually at the limits of computation, you\n\n03:00.520 --> 03:06.080\n can compute more if the temperature of the universe would drop.\n\n03:06.080 --> 03:08.360\n Temperature of the universe would drop down.\n\n03:09.720 --> 03:15.160\n So one of the things that aliens might want to do if they are truly optimizing\n\n03:15.160 --> 03:18.800\n to maximize amount of compute, which, you know, maybe can lead to, or let's\n\n03:18.800 --> 03:24.120\n say simulations or so, it's instead of wasting current entropy of the\n\n03:24.120 --> 03:27.080\n universe, because, you know, we, by living, we are actually somewhat\n\n03:27.080 --> 03:32.080\n wasting entropy, then you can wait for the universe to cool down such that\n\n03:32.080 --> 03:33.160\n you have more computation.\n\n03:33.360 --> 03:34.560\n So that's kind of a funny answer.\n\n03:34.560 --> 03:37.400\n I'm not sure if I believe in it, but that would be one of the\n\n03:37.400 --> 03:39.520\n reasons why you don't see aliens.\n\n03:39.760 --> 03:44.680\n It's also possible to some people say that maybe there is not that much\n\n03:44.680 --> 03:48.600\n point in actually going to other galaxies if you can go inwards.\n\n03:49.400 --> 03:54.440\n So there is no limits of what could be an experience if we could, you\n\n03:54.440 --> 03:58.600\n know, connect machines to our brains while there are still some limits\n\n03:58.600 --> 03:59.880\n if we want to explore the universe.\n\n03:59.880 --> 04:04.280\n Yeah, there could be a lot of ways to go inwards too.\n\n04:04.960 --> 04:08.720\n Once you figure out some aspect of physics, we haven't figured out yet.\n\n04:08.960 --> 04:11.000\n Maybe you can travel to different dimensions.\n\n04:11.000 --> 04:18.720\n I mean, travel in three dimensional space may not be the most fun kind of travel.\n\n04:19.080 --> 04:22.680\n There may be like just a huge amount of different ways to travel and it\n\n04:22.680 --> 04:28.200\n doesn't require a spaceship going slowly in 3d space to space time.\n\n04:28.200 --> 04:31.920\n It also feels, you know, one of the problems is that speed of light\n\n04:31.920 --> 04:34.200\n is low and the universe is vast.\n\n04:34.760 --> 04:40.680\n And it seems that actually most likely if we want to travel very far, then\n\n04:42.360 --> 04:46.640\n we would, instead of actually sending spaceships with humans that weight a\n\n04:46.640 --> 04:50.720\n lot, we would send something similar to what Yuri Miller is working on.\n\n04:51.000 --> 04:56.200\n These are like a huge sail, which is at first powered or there is a shot of\n\n04:56.200 --> 05:02.400\n laser from an air and it can propel it to quarter of speed of light and sail\n\n05:02.400 --> 05:06.720\n itself contains a few grams of equipment.\n\n05:07.200 --> 05:12.120\n And that might be the way to actually transport matter through universe.\n\n05:12.280 --> 05:16.160\n But then when you think what would it mean for humans, it means that we would\n\n05:16.160 --> 05:20.080\n need to actually put their 3d printer and, you know, 3d print the human on\n\n05:20.080 --> 05:24.400\n other planet, I don't know, play them YouTube or let's say, or like a 3d\n\n05:24.400 --> 05:28.120\n print like huge human right away, or maybe a womb or so, um, yeah.\n\n05:28.960 --> 05:34.080\n With our current techniques of archeology, if, if, if a civilization\n\n05:34.080 --> 05:39.560\n was born and died, uh, long, long enough ago on earth, we wouldn't be able to\n\n05:39.560 --> 05:42.440\n tell, and so that makes me really sad.\n\n05:43.280 --> 05:45.480\n And so I think about earth in that same way.\n\n05:45.520 --> 05:49.640\n How can we leave some remnants if we do destroy ourselves?\n\n05:50.040 --> 05:53.320\n How can we leave remnants for aliens in the future to discover?\n\n05:53.320 --> 05:57.400\n Like, here's some nice stuff we've done, like Wikipedia and YouTube.\n\n05:57.600 --> 06:02.520\n Do we have it like in a satellite orbiting earth with a hard drive?\n\n06:02.920 --> 06:07.080\n Like, how, how do we say, how do we back up human civilization?\n\n06:07.560 --> 06:13.440\n Uh, the good parts or all of it is good parts so that, uh, it can be\n\n06:13.440 --> 06:15.480\n preserved longer than our bodies can.\n\n06:15.920 --> 06:19.720\n That's a, that's kind of, um, it's a difficult question.\n\n06:19.720 --> 06:23.800\n It also requires the difficult acceptance of the fact that we may die.\n\n06:24.040 --> 06:28.040\n And if we die, we may die suddenly as a civilization.\n\n06:28.680 --> 06:32.160\n So let's see, I think it kind of depends on the cataclysm.\n\n06:32.480 --> 06:37.680\n We have observed in other parts of the universe that birds of gamma rays, uh,\n\n06:38.160 --> 06:42.720\n these are, uh, high energy, uh, rays of light that actually can\n\n06:42.800 --> 06:44.440\n apparently kill entire galaxy.\n\n06:44.440 --> 06:48.560\n So there might be actually nothing, even to, nothing to protect us from it.\n\n06:48.960 --> 06:51.560\n I'm also, and I'm looking actually at the past civilizations.\n\n06:51.560 --> 06:56.800\n So it's like Aztecs or so they disappear from the surface of the earth.\n\n06:56.880 --> 06:59.800\n And one can ask, why is it the case?\n\n07:00.240 --> 07:06.520\n And the way I'm thinking about it is, you know, that definitely they had some\n\n07:06.520 --> 07:10.720\n problem that they couldn't solve and maybe there was a flood and all of a\n\n07:10.720 --> 07:14.520\n sudden they couldn't drink, uh, there was no potable water and they all died.\n\n07:15.040 --> 07:24.200\n And, um, I think that, uh, so far the best solution to such a problems is I\n\n07:24.200 --> 07:27.960\n guess, technology, so, I mean, if they would know that you can just boil\n\n07:27.960 --> 07:31.680\n water and then drink it after, then that would save their civilization.\n\n07:31.920 --> 07:36.040\n And even now, when we look actually at the current pandemic, it seems\n\n07:36.040 --> 07:38.680\n that there, once again, actually science comes to rest.\n\n07:38.680 --> 07:42.040\n And somehow science increases size of the action space.\n\n07:42.440 --> 07:44.000\n And I think that's a good thing.\n\n07:44.600 --> 07:44.800\n Yeah.\n\n07:44.800 --> 07:51.440\n But nature has a vastly larger action space, but still it might be a good thing\n\n07:51.440 --> 07:53.200\n for us to keep on increasing action space.\n\n07:55.080 --> 07:55.720\n Okay.\n\n07:56.200 --> 07:58.000\n Uh, looking at past civilizations.\n\n07:58.000 --> 07:58.520\n Yes.\n\n07:58.920 --> 08:04.560\n But looking at the destruction of human civilization, perhaps expanding the\n\n08:04.560 --> 08:12.880\n action space will add, um, actions that are easily acted upon, easily executed\n\n08:12.920 --> 08:15.400\n and as a result, destroy us.\n\n08:15.960 --> 08:21.800\n So let's see, I was pondering, uh, why actually even, uh, we have\n\n08:21.800 --> 08:23.880\n negative impact on the, uh, globe.\n\n08:24.200 --> 08:27.680\n Because, you know, if you ask every single individual, they\n\n08:27.680 --> 08:28.880\n would like to have clean air.\n\n08:29.720 --> 08:32.320\n They would like healthy planet, but somehow it's not.\n\n08:32.320 --> 08:35.800\n It's not the case that as a collective, we are not going in this direction.\n\n08:36.840 --> 08:41.040\n I think that there exists very powerful system to describe what we value.\n\n08:41.080 --> 08:41.960\n That's capitalism.\n\n08:42.000 --> 08:45.280\n It assigns actually monetary values to various activities.\n\n08:45.760 --> 08:49.000\n At the moment, the problem in the current system is that there's\n\n08:49.000 --> 08:50.360\n some things which we value.\n\n08:50.680 --> 08:52.320\n There is no cost assigned to it.\n\n08:52.320 --> 09:00.240\n So even though we value clean air, or maybe we also, uh, value, uh,\n\n09:00.240 --> 09:06.000\n value lack of destruction on, let's say internet or so at the moment, these\n\n09:06.000 --> 09:10.680\n quantities, you know, companies, corporations can pollute them, uh, for free.\n\n09:11.680 --> 09:19.640\n So in some sense, I wished or like, and that's, I guess, purpose of politics\n\n09:20.040 --> 09:22.960\n to, to align the incentive systems.\n\n09:23.000 --> 09:25.680\n And we are kind of maybe even moving in this direction.\n\n09:25.680 --> 09:28.920\n The first issue is even to be able to measure the things that we value.\n\n09:28.920 --> 09:32.080\n Then we can actually assign the monetary value to them.\n\n09:32.720 --> 09:32.840\n Yeah.\n\n09:32.840 --> 09:38.000\n And that's, so it's getting the data and also probably through technology,\n\n09:38.040 --> 09:44.640\n enabling people to vote and to move money around in a way that is aligned\n\n09:44.640 --> 09:48.320\n with their values, and that's very much a technology question.\n\n09:48.720 --> 09:55.880\n So like having one president and Congress and voting that happens every four years\n\n09:55.880 --> 09:59.720\n or something like that, that's a very outdated idea that could be some\n\n09:59.720 --> 10:01.840\n technological improvements to that kind of idea.\n\n10:02.080 --> 10:06.400\n So I'm thinking from time to time about these topics, but it's also feels to me\n\n10:06.400 --> 10:10.240\n that it's, it's a little bit like, uh, it's hard for me to actually make\n\n10:10.240 --> 10:11.160\n correct predictions.\n\n10:11.160 --> 10:12.920\n What is the appropriate thing to do?\n\n10:13.120 --> 10:20.480\n I extremely trust, uh, Sam Altman, our CEO on these topics here, um, like, uh,\n\n10:20.560 --> 10:24.200\n I'm more on the side of being, I guess, naive hippie.\n\n10:24.200 --> 10:29.400\n That, uh, yeah, that's your life philosophy.\n\n10:29.400 --> 10:37.200\n Um, well, like I think self doubt and, uh, I think hippie implies optimism.\n\n10:37.480 --> 10:40.520\n Those, those two things are pretty, pretty good way to operate.\n\n10:41.080 --> 10:46.920\n I mean, still, it is hard for me to actually understand how the politics\n\n10:46.920 --> 10:51.440\n works or like, uh, how this, like, uh, exactly how the things would play out.\n\n10:51.440 --> 10:53.720\n And Sam is, uh, really excellent with it.\n\n10:54.120 --> 10:56.480\n What do you think is rarest in the universe?\n\n10:56.560 --> 10:57.920\n You said we might be alone.\n\n10:58.560 --> 11:02.880\n What's hardest to build is another engineering way to ask that life,\n\n11:03.360 --> 11:05.480\n intelligence or consciousness.\n\n11:05.680 --> 11:11.000\n So like you said that we might be alone, which is the thing that's hardest to get\n\n11:11.000 --> 11:13.280\n to, is it just the origin of life?\n\n11:13.680 --> 11:15.320\n Is it the origin of intelligence?\n\n11:15.640 --> 11:17.800\n Is it the origin of consciousness?\n\n11:17.800 --> 11:23.080\n So, um, let me at first explain to you my kind of mental model, what I think\n\n11:23.080 --> 11:24.760\n is needed for life to appear.\n\n11:25.560 --> 11:32.680\n Um, so I imagine that at some point there was this primordial, uh, soup of, uh,\n\n11:32.840 --> 11:38.240\n amino acids and maybe some proteins in the ocean and, uh, you know, some\n\n11:38.240 --> 11:42.640\n proteins were turning into some other proteins through reaction and, uh, you\n\n11:42.640 --> 11:46.480\n can also, uh, you know, you can, you know, you can, you know, you can\n\n11:46.480 --> 11:52.160\n and, uh, you can almost think about this, uh, cycle of what, uh, turns into what\n\n11:52.200 --> 11:55.680\n as there is a graph essentially describing which substance turns into\n\n11:55.840 --> 12:00.120\n some other substance and essentially life means that all of a sudden in the graph\n\n12:00.480 --> 12:04.640\n has been created that cycle such that the same thing keeps on happening over\n\n12:04.640 --> 12:07.280\n and over again, that's what is needed for life to happen.\n\n12:07.280 --> 12:11.840\n And in some sense, you can think almost that you have this gigantic graph and it\n\n12:12.000 --> 12:15.400\n needs like a sufficient number of edges for the cycle to appear.\n\n12:15.400 --> 12:21.760\n Um, then, um, from perspective of intelligence and consciousness, uh, my\n\n12:21.760 --> 12:25.960\n current intuition is that they might be quite intertwined.\n\n12:26.280 --> 12:29.160\n First of all, it might not be that it's like a binary thing that you\n\n12:29.160 --> 12:30.560\n have intelligence or consciousness.\n\n12:30.720 --> 12:36.360\n It seems to be, uh, uh, more, uh, continuous component.\n\n12:36.800 --> 12:41.320\n Let's see, if we look for instance on the event networks, uh, recognizing\n\n12:41.320 --> 12:45.920\n images and people are able to show that the activations of these networks\n\n12:46.120 --> 12:51.680\n correlate very strongly, uh, with activations in visual cortex, uh, of\n\n12:51.760 --> 12:55.520\n some monkeys, the same seems to be true about language models.\n\n12:56.080 --> 13:04.320\n Um, also if you, for instance, um, look, um, if you train agent in, um, 3d\n\n13:04.320 --> 13:09.880\n world, um, at first, you know, it, it, it, it barely recognizes what is going\n\n13:09.880 --> 13:14.640\n on over the time, it kind of recognizes foreground from a background over the\n\n13:14.640 --> 13:18.520\n time, it kind of knows where there is a foot, uh, and it just follows it.\n\n13:18.880 --> 13:22.640\n Um, over the time it actually starts having a 3d perception.\n\n13:22.800 --> 13:27.480\n So it is possible for instance, to look inside of the head of an agent and ask,\n\n13:27.480 --> 13:29.280\n what would it see if it looks to the right?\n\n13:29.760 --> 13:33.600\n And the crazy thing is, you know, initially when the agents are barely\n\n13:33.600 --> 13:37.200\n trained, that these predictions are pretty bad over the time they become\n\n13:37.200 --> 13:42.840\n better and better, you can still see that if you ask what happens when the\n\n13:42.840 --> 13:47.400\n head is turned by 360 degrees for some time, they think that the different\n\n13:47.400 --> 13:51.440\n thing appears and then at some stage they understand actually that the same\n\n13:51.440 --> 13:52.440\n thing supposed to appear.\n\n13:52.640 --> 13:55.520\n So they get that understanding of 3d structure.\n\n13:55.760 --> 14:01.960\n It's also, you know, very likely that they have inside some level of, of like\n\n14:01.960 --> 14:06.720\n a symbolic reasoning, like a particular, these symbols for other agents.\n\n14:06.720 --> 14:13.600\n So when you look at DOTA agents, they collaborate together and, uh, and, uh,\n\n14:13.800 --> 14:17.880\n no, they, they, they, they have some anticipation of, uh, if, if they would\n\n14:17.880 --> 14:21.720\n win battle, they have some, some expectations with respect to other\n\n14:21.720 --> 14:22.120\n agents.\n\n14:22.360 --> 14:26.160\n I might be, you know, too much anthropomorphizing, um, the, the, the,\n\n14:26.160 --> 14:31.400\n how the things look, look, look for me, but then the fact that they have a\n\n14:31.400 --> 14:37.160\n symbol for other agents, uh, makes me believe that, uh, at some stage as the,\n\n14:37.440 --> 14:41.400\n uh, you know, as they are optimizing for skills, they would have also symbol to\n\n14:41.400 --> 14:42.400\n describe themselves.\n\n14:43.360 --> 14:46.040\n Uh, this is like a very useful symbol to have.\n\n14:46.400 --> 14:50.280\n And this particularity, I would call it like a self consciousness or self\n\n14:50.280 --> 14:55.120\n awareness, uh, and, uh, still it might be different from the consciousness.\n\n14:55.280 --> 14:59.800\n So I guess the, the way how I'm understanding the word consciousness,\n\n14:59.800 --> 15:03.120\n I'd say the experience of drinking a coffee or let's say experience of being\n\n15:03.120 --> 15:06.280\n a bat, that's the meaning of the word consciousness.\n\n15:06.280 --> 15:07.400\n It doesn't mean to be awake.\n\n15:07.760 --> 15:13.480\n Uh, yeah, it feels, it might be also somewhat related to memory and\n\n15:13.480 --> 15:14.560\n recurrent connections.\n\n15:14.840 --> 15:21.480\n So, um, it's kind of like, if you look at anesthetic drugs, they might be, uh,\n\n15:21.480 --> 15:30.200\n uh, like, uh, that they essentially, they, they disturb, uh, uh, brainwaves, uh, such\n\n15:30.200 --> 15:33.320\n that, um, maybe memories, not, not form.\n\n15:33.960 --> 15:37.160\n And so there's a lessening of consciousness when you do that.\n\n15:37.280 --> 15:37.680\n Correct.\n\n15:37.840 --> 15:40.720\n And so that's the one way to intuit what is consciousness.\n\n15:41.040 --> 15:45.040\n There's also kind of another element here.\n\n15:45.360 --> 15:49.480\n It could be that it's, you know, this kind of self awareness\n\n15:49.480 --> 15:56.320\n module that you described, plus the actual subjective experience is a\n\n15:56.360 --> 16:04.600\n storytelling module that tells us a story about, uh, what we're experiencing.\n\n16:05.160 --> 16:06.960\n The crazy thing.\n\n16:06.960 --> 16:11.200\n So let's say, I mean, in meditation, they teach people not to speak\n\n16:11.200 --> 16:12.280\n story inside of their head.\n\n16:12.800 --> 16:17.280\n And there is also some fraction of population who doesn't have actually\n\n16:17.280 --> 16:22.120\n a narrator, I know people who don't have a narrator and, you know, they have\n\n16:22.120 --> 16:27.760\n to use external people in order to, um, kind of solve tasks that\n\n16:27.760 --> 16:29.920\n require internal narrator.\n\n16:30.360 --> 16:35.040\n Um, so it seems that it's possible to have the experience without the talk.\n\n16:37.440 --> 16:40.680\n What are we talking about when we talk about the internal narrator?\n\n16:41.080 --> 16:44.000\n Is that the voice when you're like, yeah, I thought that that's what you are\n\n16:44.000 --> 16:49.960\n referring to while I was referring more on the, like, not an actual voice.\n\n16:51.120 --> 16:58.200\n I meant like, there's some kind of like subjective experience feels like it's.\n\n17:00.400 --> 17:03.840\n It's fundamentally about storytelling to ourselves.\n\n17:04.560 --> 17:13.760\n It feels like, like the feeling is a story that is much, uh, much\n\n17:13.760 --> 17:16.800\n simpler abstraction than the raw sensory information.\n\n17:17.400 --> 17:23.360\n So there feels like it's a very high level of abstraction that, uh, is useful\n\n17:23.960 --> 17:27.000\n for me to feel like entity in this world.\n\n17:27.280 --> 17:35.920\n M most useful aspect of it is that because I'm conscious, I think there's\n\n17:35.920 --> 17:39.400\n an intricate connection to me, not wanting to die.\n\n17:39.400 --> 17:46.160\n So like, it's a useful hack to really prioritize not dying, like those\n\n17:46.160 --> 17:47.520\n seem to be somehow connected.\n\n17:47.560 --> 17:50.440\n So I'm telling the story of like, it's rich.\n\n17:50.440 --> 17:55.200\n He feels like something to be me and the fact that me exists in this world.\n\n17:55.200 --> 17:56.280\n I want to preserve me.\n\n17:56.920 --> 17:58.840\n And so that makes it a useful agent hack.\n\n17:59.280 --> 18:03.720\n So I will just refer maybe to that first part, as you said, about that kind\n\n18:03.720 --> 18:05.800\n of story of describing who you are.\n\n18:05.800 --> 18:13.600\n Um, I was, uh, thinking about that even, so, you know, obviously I'm, I, I like\n\n18:13.600 --> 18:18.280\n thinking about consciousness, uh, I like thinking about AI as well, and I'm trying\n\n18:18.280 --> 18:21.840\n to see analogies of these things in AI, what would it correspond to?\n\n18:22.520 --> 18:34.360\n So, um, um, you know, open AI train, uh, uh, a model called GPT, uh, which, uh,\n\n18:34.360 --> 18:42.280\n can generate, uh, pretty, I'm using texts on arbitrary topic and, um, um, and one\n\n18:42.280 --> 18:49.400\n way to control GPT is, uh, by putting into prefix at the beginning of the text, some\n\n18:49.400 --> 18:55.120\n information, what would be the story about, uh, you can have even chat with, uh, uh,\n\n18:55.400 --> 19:01.200\n you know, with GPT by saying that the chat is with Lex or Elon Musk or so, and, uh,\n\n19:01.200 --> 19:08.720\n GPT would just pretend to be you or Elon Musk or so, and, uh, uh, it almost feels\n\n19:08.720 --> 19:14.960\n that this, uh, story that we give ourselves to describe our life, it's almost like, uh,\n\n19:15.360 --> 19:17.320\n things that you put into context of GPT.\n\n19:17.360 --> 19:17.640\n Yeah.\n\n19:17.680 --> 19:25.400\n The primary, it's the, and so, but the context we provide to GPT is, uh, is multimodal.\n\n19:25.520 --> 19:27.760\n It's more so GPT itself is multimodal.\n\n19:27.760 --> 19:33.120\n GPT itself, uh, hasn't learned actually from experience of single human, but from the\n\n19:33.120 --> 19:35.360\n experience of humanity, it's a chameleon.\n\n19:35.520 --> 19:42.000\n You can turn it into anything and in some sense, by providing context, um, it, you\n\n19:42.000 --> 19:44.920\n know, behaves as the thing that you wanted it to be.\n\n19:45.160 --> 19:49.840\n Um, it's interesting that the, you know, people have a stories of who they are.\n\n19:50.520 --> 19:54.400\n And, uh, as you said, these stories, they help them to operate in the world.\n\n19:54.400 --> 19:59.800\n Um, but it's also, you know, interesting, I guess, various people find it out through\n\n19:59.800 --> 20:05.400\n meditation or so that, uh, there might be some patterns that you have learned when\n\n20:05.400 --> 20:07.800\n you were a kid that actually are not serving you anymore.\n\n20:08.600 --> 20:13.320\n And you also might be thinking that that's who you are and that's actually just a story.\n\n20:13.320 --> 20:13.840\n Mm hmm.\n\n20:15.040 --> 20:15.240\n Yeah.\n\n20:15.240 --> 20:18.160\n So it's a useful hack, but sometimes it gets us into trouble.\n\n20:18.240 --> 20:19.280\n It's a local optima.\n\n20:19.360 --> 20:20.200\n It's a local optima.\n\n20:20.200 --> 20:24.880\n You wrote that Stephen Hawking, he tweeted, Stephen Hawking asked what\n\n20:24.880 --> 20:29.440\n breathes fire into equations, which meant what makes given mathematical\n\n20:29.440 --> 20:32.720\n equations realize the physics of a universe.\n\n20:33.120 --> 20:37.200\n Similarly, I wonder what breathes fire into computation.\n\n20:37.520 --> 20:39.920\n What makes given computation conscious?\n\n20:40.600 --> 20:41.000\n Okay.\n\n20:41.240 --> 20:44.360\n So how do we engineer consciousness?\n\n20:44.400 --> 20:47.280\n How do you breathe fire and magic?\n\n20:47.280 --> 20:51.240\n How do you breathe fire and magic into the machine?\n\n20:51.800 --> 20:57.200\n So, um, it seems clear to me that not every computation is conscious.\n\n20:57.280 --> 21:01.520\n I mean, you can, let's say, just keep on multiplying one matrix over and over\n\n21:01.520 --> 21:03.920\n again and might be gigantic matrix.\n\n21:03.920 --> 21:05.400\n You can put a lot of computation.\n\n21:05.480 --> 21:06.880\n I don't think it would be conscious.\n\n21:07.080 --> 21:13.000\n So in some sense, the question is, uh, what are the computations which could be\n\n21:13.000 --> 21:18.280\n conscious, uh, I mean, so, so one assumption is that it has to do purely\n\n21:18.280 --> 21:21.960\n with computation that you can abstract away matter and other possibilities\n\n21:22.160 --> 21:25.400\n that it's very important was the realization of computation that it has\n\n21:25.400 --> 21:30.200\n to do with some, uh, uh, force fields or so, and they bring consciousness.\n\n21:30.520 --> 21:33.440\n At the moment, my intuition is that it can be fully abstracted away.\n\n21:33.680 --> 21:38.240\n So in case of computation, you can ask yourself, what are the mathematical\n\n21:38.280 --> 21:41.440\n objects or so that could bring such a properties?\n\n21:41.440 --> 21:49.000\n So for instance, if we think about the models, uh, AI models, the, what they\n\n21:49.000 --> 21:57.000\n truly try to do, uh, or like a models like GPT is, uh, uh, you know, they try\n\n21:57.000 --> 22:00.320\n to predict, uh, next word or so.\n\n22:00.480 --> 22:05.920\n And this turns out to be equivalent to, uh, compressing, uh, text.\n\n22:05.920 --> 22:11.120\n Um, and, uh, because in some sense, compression means that, uh, you learn\n\n22:11.120 --> 22:16.040\n the model of reality and you have just to, uh, remember where are your mistakes.\n\n22:16.320 --> 22:20.640\n The better you are in predicting the, and, and, and in some sense, when we\n\n22:20.640 --> 22:24.040\n look at our experience, also, when you look, for instance, at the car driving,\n\n22:24.080 --> 22:27.400\n you know, in which direction it will go, you are good like in prediction.\n\n22:27.720 --> 22:32.880\n And, um, you know, it might be the case that the consciousness is intertwined\n\n22:32.880 --> 22:38.080\n with, uh, compression, it might be also the case that self consciousness, uh,\n\n22:38.400 --> 22:41.000\n has to do with compress or trying to compress itself.\n\n22:41.280 --> 22:43.360\n So, um, okay.\n\n22:43.600 --> 22:47.640\n I was just wondering, what are the objects in, you know, mathematics or\n\n22:47.640 --> 22:52.360\n computer science, which are mysterious that could, uh, that, that, that could\n\n22:52.360 --> 22:53.520\n have to do with consciousness.\n\n22:53.520 --> 22:59.680\n And then I thought, um, you know, you, you see in mathematics, there is\n\n22:59.680 --> 23:03.720\n something called Gadel theorem, uh, which means, okay, you have, if you have\n\n23:03.720 --> 23:08.440\n sufficiently complicated mathematical system, it is possible to point the\n\n23:08.440 --> 23:10.600\n mathematical system back on itself.\n\n23:10.800 --> 23:14.040\n In computer science, there is, uh, something called helping problem.\n\n23:14.280 --> 23:16.480\n It's, it's somewhat similar construction.\n\n23:16.800 --> 23:22.960\n So I thought that, you know, if we believe that, uh, that, uh, that under\n\n23:22.960 --> 23:28.320\n assumption that consciousness has to do with, uh, with compression, uh, then\n\n23:28.320 --> 23:32.760\n you could imagine that the, that the, as you keep on compressing things, then\n\n23:32.760 --> 23:36.640\n at some point, it actually makes sense for the compressor to compress itself.\n\n23:36.720 --> 23:40.080\n Metacompression consciousness is metacompression.\n\n23:40.760 --> 23:43.480\n That's a, that's an I, an, an, an idea.\n\n23:44.360 --> 23:47.280\n And in some sense, you know, the crazy, thank you.\n\n23:47.280 --> 23:52.280\n So, uh, but do you think if we think of a Turing machine, a universal\n\n23:52.280 --> 23:55.880\n Turing machine, can that achieve consciousness?\n\n23:55.880 --> 24:00.240\n So is there some thing beyond our traditional definition\n\n24:00.240 --> 24:02.000\n of computation that's required?\n\n24:02.200 --> 24:03.920\n So it's a specific computation.\n\n24:03.920 --> 24:08.760\n And I said, this computation has to do with compression and, uh, the compression\n\n24:08.760 --> 24:13.040\n itself, maybe other way of putting it is like, uh, you are internally creating\n\n24:13.040 --> 24:18.040\n the model of reality in order, like, uh, it's like a, you try inside to simplify\n\n24:18.040 --> 24:20.200\n reality in order to predict what's going to happen.\n\n24:20.200 --> 24:25.080\n And, um, that also feels somewhat similar to how I think actually about my own\n\n24:25.200 --> 24:28.680\n conscious experience, though clearly I don't have access to reality.\n\n24:29.040 --> 24:33.240\n The only access to reality is through, you know, cable going to my brain and my\n\n24:33.240 --> 24:37.320\n brain is creating a simulation of reality and I have access to the simulation of\n\n24:37.320 --> 24:37.840\n reality.\n\n24:38.400 --> 24:43.520\n Are you by any chance, uh, aware of, uh, the Hutter prize, Marcus Hutter?\n\n24:44.200 --> 24:48.160\n He, uh, he made this prize for compression.\n\n24:48.160 --> 24:53.080\n Uh, Wikipedia pages, and, uh, there's a few qualities to it.\n\n24:53.560 --> 24:57.640\n One, I think has to be perfect compression, which makes, I think that\n\n24:57.640 --> 25:03.520\n little cork makes it much less, um, applicable to the general task of\n\n25:03.520 --> 25:06.920\n intelligence, because it feels like intelligence is always going to be messy.\n\n25:07.720 --> 25:14.280\n Uh, like perfect compression is feels like it's not the right goal, but\n\n25:14.280 --> 25:19.280\n it's nevertheless a very interesting goal.\n\n25:19.280 --> 25:22.000\n So for him, intelligence equals compression.\n\n25:22.680 --> 25:29.240\n And so the smaller you make the file, given a large Wikipedia page, the\n\n25:29.240 --> 25:31.000\n more intelligent the system has to be.\n\n25:31.200 --> 25:31.920\n Yeah, that makes sense.\n\n25:31.920 --> 25:34.520\n So you can make perfect compression if you store errors.\n\n25:34.920 --> 25:37.960\n And I think that actually what he meant is you have algorithm plus errors.\n\n25:37.960 --> 25:44.720\n Uh, by the way, Hutter, Hutter is, uh, he was a PhD advisor of Sean\n\n25:44.720 --> 25:48.360\n Leck, who is a DeepMind, uh, uh, DeepMind cofounder.\n\n25:48.600 --> 25:48.920\n Yeah.\n\n25:49.080 --> 25:49.360\n Yeah.\n\n25:49.360 --> 25:53.600\n So there's an interesting, uh, and now he's a DeepMind, there's an\n\n25:53.600 --> 25:55.720\n interesting, uh, network of people.\n\n25:55.720 --> 26:02.360\n And he's one of the people that I think seriously took on the task of\n\n26:02.680 --> 26:04.960\n what would an AGI system look like?\n\n26:04.960 --> 26:11.760\n Uh, I think for a longest time, the question of AGI was not taken\n\n26:12.680 --> 26:14.960\n seriously or rather rigorously.\n\n26:15.640 --> 26:19.800\n And he did just that, like mathematically speaking, what\n\n26:19.800 --> 26:23.440\n would the model look like if you remove the constraints of it, having to be,\n\n26:23.440 --> 26:31.880\n uh, um, having to have a reasonable amount of memory, reasonable amount\n\n26:31.880 --> 26:36.400\n of, uh, running time, complexity, uh, computation time, what would it look\n\n26:36.400 --> 26:41.760\n like and essentially it's, it's a half math, half philosophical discussion\n\n26:41.760 --> 26:45.000\n of, uh, how would it like a reinforcement learning type of\n\n26:45.240 --> 26:47.360\n framework look like for an AGI?\n\n26:47.520 --> 26:47.800\n Yeah.\n\n26:47.800 --> 26:51.640\n So he developed the framework even to describe what's optimal with\n\n26:51.640 --> 26:53.240\n respect to reinforcement learning.\n\n26:53.240 --> 26:57.040\n Like there is a theoretical framework, which is, as you said, under assumption,\n\n26:57.040 --> 26:59.000\n there is infinite amount of memory and compute.\n\n26:59.000 --> 27:03.560\n Um, there was actually one person before his name is Solomonov, who\n\n27:03.560 --> 27:07.840\n there extended, uh, Solomonov work to reinforcement learning, but there\n\n27:07.840 --> 27:13.640\n exists the, uh, theoretical algorithm, which is optimal algorithm to build\n\n27:13.840 --> 27:16.480\n intelligence and I can actually explain you the algorithm.\n\n27:16.560 --> 27:16.880\n Yes.\n\n27:18.080 --> 27:18.520\n Let's go.\n\n27:18.960 --> 27:19.400\n Let's go.\n\n27:19.880 --> 27:26.680\n So the task itself, can I just pause how absurd it is for brain in a\n\n27:26.680 --> 27:30.800\n skull, trying to explain the algorithm for intelligence, just go ahead.\n\n27:31.120 --> 27:32.160\n It is pretty crazy.\n\n27:32.160 --> 27:34.640\n It is pretty crazy that, you know, the brain itself is actually so\n\n27:34.640 --> 27:40.960\n small and it can ponder, uh, how to design algorithms that optimally\n\n27:40.960 --> 27:42.520\n solve the problem of intelligence.\n\n27:42.560 --> 27:42.840\n Okay.\n\n27:43.440 --> 27:43.640\n All right.\n\n27:43.640 --> 27:44.640\n So what's the algorithm?\n\n27:44.920 --> 27:46.080\n So let's see.\n\n27:46.120 --> 27:51.560\n So first of all, the task itself is, uh, described as, uh, you have infinite\n\n27:51.560 --> 27:52.960\n sequence of zeros and ones.\n\n27:53.560 --> 27:53.840\n Okay.\n\n27:53.840 --> 27:58.600\n Okay. You read, uh, N bits and they are about to predict N plus one bit.\n\n27:59.120 --> 28:00.160\n So that's the task.\n\n28:00.160 --> 28:04.120\n And you could imagine that every task could be casted as such a task.\n\n28:04.440 --> 28:08.800\n So if for instance, you have images and labels, you can just turn every image\n\n28:08.800 --> 28:12.960\n into a sequence of zeros and ones, then label, you concatenate labels and\n\n28:12.960 --> 28:16.680\n you, and that that's actually the, the, and you could, you could start by\n\n28:16.680 --> 28:20.480\n having training data first, and then afterwards you have test data.\n\n28:20.480 --> 28:25.480\n So theoretically any problem could be casted as a problem of predicting\n\n28:25.480 --> 28:28.120\n zeros and ones on this, uh, infinite tape.\n\n28:28.320 --> 28:35.240\n So, um, so let's say you read already N bits and you want to predict N plus\n\n28:35.240 --> 28:42.160\n one bit, and I will ask you to write every possible program that generates\n\n28:42.160 --> 28:42.880\n these N bits.\n\n28:43.560 --> 28:43.760\n Okay.\n\n28:43.760 --> 28:47.880\n So, um, and you can have, you, you choose programming language.\n\n28:47.880 --> 28:49.720\n It can be Python or C plus plus.\n\n28:49.720 --> 28:53.480\n And the difference between programming languages, uh, might be, there is\n\n28:53.480 --> 28:57.760\n a difference by constant asymptotically, your predictions will be equivalent.\n\n28:59.160 --> 29:04.080\n So you read N bits, you enumerate all the programs that produce\n\n29:04.080 --> 29:06.040\n these N bits in their output.\n\n29:06.680 --> 29:13.160\n And then in order to predict N plus one bit, you actually weight the programs\n\n29:13.480 --> 29:14.600\n according to their length.\n\n29:15.440 --> 29:18.480\n And there is like a, some specific formula, how you weight them.\n\n29:18.480 --> 29:24.120\n And then the N plus, uh, one bit prediction is the prediction, uh, from each\n\n29:24.120 --> 29:26.040\n of these program, according to that weight.\n\n29:27.040 --> 29:31.880\n Like statistically, you pick, so the smaller the program, the more likely\n\n29:31.880 --> 29:34.680\n you, you are to pick the, its output.\n\n29:35.480 --> 29:42.280\n So, uh, that's, that algorithm is grounded in the hope or the intuition\n\n29:42.280 --> 29:43.960\n that the simple answer is the right one.\n\n29:44.600 --> 29:46.000\n It's a formalization of it.\n\n29:46.000 --> 29:52.040\n Um, it also means like, if you would ask the question after how many years\n\n29:52.600 --> 29:58.080\n would, you know, sun explode, uh, you can say, hmm, it's more likely\n\n29:58.080 --> 30:01.560\n the answer is due to some power because they're shorter program.\n\n30:02.160 --> 30:02.440\n Yeah.\n\n30:02.920 --> 30:08.240\n Um, then other, well, I don't have a good intuition about, uh, how different\n\n30:08.240 --> 30:12.120\n the space of short programs are from the space of large programs.\n\n30:12.120 --> 30:17.600\n Like, what is the universe where short programs, uh, like run things?\n\n30:18.520 --> 30:21.960\n Uh, so, so I said, the things have to agree with N bits.\n\n30:22.160 --> 30:25.600\n So even if you have, you, you need to start, okay.\n\n30:25.640 --> 30:29.920\n If, if you have very short program and they're like a steel, some has, if, if\n\n30:29.920 --> 30:33.520\n it's not perfectly prediction of N bits, you have to start errors.\n\n30:33.760 --> 30:34.520\n What are the errors?\n\n30:34.520 --> 30:37.120\n And that gives you the full program that agrees on N bits.\n\n30:38.200 --> 30:40.160\n Oh, so you don't agree with the N bits.\n\n30:40.160 --> 30:44.280\n And you store, that's like a longer, a longer program, slightly longer program\n\n30:45.000 --> 30:47.120\n because it can take these extra bits of errors.\n\n30:47.440 --> 30:48.440\n That's fascinating.\n\n30:48.480 --> 30:55.920\n What's what's your intuition about the, the programs that are able to do cool\n\n30:55.920 --> 31:02.560\n stuff like intelligence and consciousness, are they, uh, perfectly like, is, is it,\n\n31:02.560 --> 31:05.400\n uh, is there if then statements in them?\n\n31:05.680 --> 31:08.920\n So like, is there a lot of a good, uh, if then statements in them?\n\n31:08.920 --> 31:11.240\n So like, is there a lot of exceptions that they're storing?\n\n31:11.480 --> 31:15.800\n So, um, you could imagine if there would be tremendous amount of if statements,\n\n31:16.280 --> 31:17.560\n then they wouldn't be that short.\n\n31:17.720 --> 31:23.520\n In case of neural networks, you could imagine that, um, what happens is, uh,\n\n31:24.280 --> 31:29.840\n they, uh, when you start with an initialized neural network, uh, it stores\n\n31:29.840 --> 31:34.840\n internally many possibilities, how the, uh, how the problem can be solved.\n\n31:34.840 --> 31:42.320\n And SGD is kind of magnifying some, some, uh, some, uh, paths, which are slightly\n\n31:42.720 --> 31:44.000\n similar to the correct answer.\n\n31:44.000 --> 31:45.960\n So it's kind of magnifying correct programs.\n\n31:46.280 --> 31:50.960\n And in some sense, SGD is a search algorithm in the program space and the\n\n31:50.960 --> 31:56.440\n program space is represented by, uh, you know, kind of the wiring inside of the\n\n31:56.440 --> 32:00.760\n neural network and there's like an insane number of ways how the features can be\n\n32:00.760 --> 32:01.280\n computed.\n\n32:01.280 --> 32:05.080\n Let me ask you the high level, basic question that's not so basic.\n\n32:05.720 --> 32:07.760\n What is deep learning?\n\n32:08.480 --> 32:11.960\n Is there a way you'd like to think of it that is different than like\n\n32:11.960 --> 32:13.640\n a generic textbook definition?\n\n32:14.360 --> 32:19.160\n The thing that I hinted just a second ago is maybe that, uh, closest to how I'm\n\n32:19.160 --> 32:21.200\n thinking these days about deep learning.\n\n32:21.600 --> 32:29.240\n So, uh, now the statement is, uh, neural networks can represent some programs.\n\n32:29.240 --> 32:33.320\n Uh, it seems that various modules that we are actually adding up to, or like, uh,\n\n32:33.600 --> 32:37.520\n you know, we, we want networks to be deep because we, we want multiple\n\n32:37.520 --> 32:45.160\n steps of the computation and, uh, uh, and deep learning provides the way to\n\n32:45.160 --> 32:48.920\n represent space of programs, which is searchable and it's searchable with,\n\n32:48.920 --> 32:50.800\n uh, stochastic gradient descent.\n\n32:50.840 --> 32:56.600\n So we have an algorithm to search over humongous number of programs and\n\n32:56.600 --> 33:01.160\n gradient descent kind of bubbles up the things that are, uh, tend to give correct\n\n33:01.160 --> 33:01.640\n answers.\n\n33:01.800 --> 33:09.800\n So a neural network with a, with fixed weights that's optimized, do you think\n\n33:09.800 --> 33:11.160\n of that as a single program?\n\n33:11.400 --> 33:18.280\n Um, so there is a, uh, work by Christopher Olaj where he, uh, so he works on\n\n33:18.360 --> 33:24.800\n interpretability of neural networks and he was able to, uh, to identify the\n\n33:24.800 --> 33:29.920\n neural network, for instance, a detector of a wheel for a car, or the detector of\n\n33:29.920 --> 33:35.120\n a mask for a car, and then he was able to separate them out and assemble them, uh,\n\n33:35.280 --> 33:39.960\n together using a simple program, uh, for the detector, for a car detector.\n\n33:40.400 --> 33:44.440\n That's like, uh, if you think of traditionally defined programs, that's\n\n33:44.440 --> 33:48.240\n like a function within a program that this particular neural network was able\n\n33:48.240 --> 33:53.000\n to find and you can tear that out, just like you can copy and paste it into a\n\n33:53.000 --> 33:59.960\n stack overflow that, so, uh, any program is a composition of smaller programs.\n\n34:00.520 --> 34:00.760\n Yeah.\n\n34:00.760 --> 34:04.880\n I mean, the nice thing about the neural networks is that it allows the things\n\n34:04.880 --> 34:07.040\n to be more fuzzy than in case of programs.\n\n34:07.360 --> 34:11.440\n Uh, in case of programs, you have this, like a branching this way or that way.\n\n34:11.760 --> 34:16.240\n And the neural networks, they, they have an easier way to, to be somewhere in\n\n34:16.240 --> 34:18.080\n between or to share things.\n\n34:18.080 --> 34:23.360\n What is the most beautiful or surprising idea in deep learning and the utilization\n\n34:23.360 --> 34:27.080\n of these neural networks, which by the way, for people who are not familiar,\n\n34:27.800 --> 34:32.840\n neural networks is a bunch of, uh, what would you say it's inspired by the human\n\n34:32.840 --> 34:37.080\n brain, there's neurons, there's connection between those neurons, there's inputs and\n\n34:37.080 --> 34:41.840\n there's outputs and there's millions or billions of those neurons and the\n\n34:41.840 --> 34:44.800\n learning happens in the neural network.\n\n34:44.800 --> 34:52.000\n Neurons and the learning happens, uh, by adjusting the weights on the\n\n34:52.000 --> 34:53.800\n edges that connect these neurons.\n\n34:54.160 --> 34:58.320\n Thank you for giving definition that I supposed to do it, but I guess you have\n\n34:58.320 --> 35:02.680\n enough empathy to listeners to actually know that the, that that might be useful.\n\n35:02.760 --> 35:07.480\n No, that's like, so I'm asking Plato of like, what is the meaning of life?\n\n35:07.480 --> 35:08.800\n He's not going to answer.\n\n35:09.320 --> 35:13.320\n You're being philosophical and deep and quite profound talking about the space\n\n35:13.320 --> 35:17.120\n of programs, which is, which is very interesting, but also for people who\n\n35:17.120 --> 35:20.360\n just not familiar with the hell we're talking about when we talk about deep\n\n35:20.360 --> 35:25.840\n learning anyway, sorry, what is the most beautiful or surprising idea to you in,\n\n35:25.920 --> 35:29.560\n in, um, in all the time you've worked at deep learning and you worked on a lot of.\n\n35:30.040 --> 35:34.480\n Fascinating projects, applications of neural networks.\n\n35:35.240 --> 35:36.920\n It doesn't have to be big and profound.\n\n35:36.920 --> 35:38.000\n It can be a cool trick.\n\n35:38.240 --> 35:38.760\n Yeah.\n\n35:38.920 --> 35:42.520\n I mean, I'm thinking about the trick, but like, uh, it's still, uh, I'm using\n\n35:42.520 --> 35:47.360\n to me that it works at all that let's say that the extremely simple algorithm\n\n35:47.360 --> 35:52.120\n stochastic gradient descent, which is something that I would be able to derive\n\n35:52.120 --> 35:58.120\n on the piece of paper to high school student, uh, when put at the, at the\n\n35:58.120 --> 36:03.360\n scale of, you know, thousands of machines actually, uh, can create the.\n\n36:03.880 --> 36:07.520\n Behaviors we, which we called kind of human like behaviors.\n\n36:07.960 --> 36:11.760\n So in general, any application is stochastic gradient descent\n\n36:11.760 --> 36:14.560\n to neural networks is, is amazing to you.\n\n36:14.600 --> 36:19.920\n So that, or is there a particular application in natural language\n\n36:20.320 --> 36:29.160\n reinforcement learning, uh, and also what do you attribute that success to?\n\n36:29.200 --> 36:30.400\n Is it just scale?\n\n36:31.320 --> 36:36.200\n What profound insight can we take from the fact that the thing works\n\n36:36.200 --> 36:39.880\n for gigantic, uh, sets of variables?\n\n36:39.880 --> 36:44.360\n I mean, the interesting thing is this algorithms, they were invented decades\n\n36:44.360 --> 36:52.680\n ago and, uh, people actually, uh, gave up on the idea and, um, you know, back\n\n36:52.680 --> 36:58.040\n then they thought that we need profoundly different algorithms and they spent a lot\n\n36:58.040 --> 36:59.920\n of cycles on very different algorithms.\n\n37:00.240 --> 37:05.040\n And I believe that, uh, you know, we have seen that various, uh, various innovations\n\n37:05.040 --> 37:11.400\n that say like transformer or, or dropout or so they can, uh, you know, pass the\n\n37:11.400 --> 37:17.880\n help, but it's also remarkable to me that this algorithm from sixties or so, uh, or,\n\n37:18.000 --> 37:22.680\n I mean, you can even say that the gradient descent was invented by Leibniz in, I\n\n37:22.680 --> 37:29.200\n guess, 18th century or so that actually is the core of learning in the past.\n\n37:29.200 --> 37:35.400\n In the past people are, it's almost like a, out of the, maybe an ego, people are\n\n37:35.400 --> 37:39.640\n saying that it cannot be the case that such a simple algorithm is there, you\n\n37:39.640 --> 37:43.640\n know, uh, could solve complicated problems.\n\n37:44.480 --> 37:48.280\n So they were in search for the other algorithms.\n\n37:48.560 --> 37:51.560\n And as I'm saying, like, I believe that actually we are in the game where there\n\n37:51.560 --> 37:54.000\n is, there are actually frankly three levers.\n\n37:54.040 --> 37:56.960\n There is compute, there are algorithms and there is data.\n\n37:56.960 --> 38:01.480\n And, uh, if we want to build intelligent systems, we have to pull, uh, all three\n\n38:01.480 --> 38:04.520\n levers and they are actually multiplicative.\n\n38:05.440 --> 38:06.800\n Um, it's also interesting.\n\n38:06.800 --> 38:08.400\n So you ask, is it only compute?\n\n38:08.920 --> 38:14.200\n Uh, people internally, they did the studies to determine how much gains they\n\n38:14.200 --> 38:15.560\n were coming from different levers.\n\n38:16.040 --> 38:20.520\n And so far we have seen that more gains came from compute than algorithms, but\n\n38:20.520 --> 38:24.200\n also we are in the world that in case of compute, there is a kind of, you know,\n\n38:24.200 --> 38:28.640\n exponential increase in funding and at some point it's impossible to, uh, invest\n\n38:28.640 --> 38:32.800\n more, it's impossible to, you know, invest $10 trillion as we are speaking about\n\n38:32.800 --> 38:35.840\n the, let's say all taxes in us.\n\n38:36.600 --> 38:41.880\n Uh, but you're talking about money that could be innovation in the compute.\n\n38:42.000 --> 38:42.960\n That's that's true as well.\n\n38:43.680 --> 38:45.760\n Uh, so I mean, they're like a few pieces.\n\n38:45.760 --> 38:51.800\n So one piece is human brain is an incredible supercomputer and they're like\n\n38:51.800 --> 39:01.360\n a, it, it, it has a hundred trillion parameters or like a, if you try to count\n\n39:01.360 --> 39:05.720\n the various quantities in the brain, they're like a neuron synapses that small\n\n39:05.720 --> 39:10.600\n number of neurons, there is a lot of synapses it's unclear even how to map, uh,\n\n39:10.760 --> 39:16.880\n synapses to, uh, to parameters of neural networks, but it's clear that there are\n\n39:16.880 --> 39:17.400\n many more.\n\n39:17.400 --> 39:22.400\n Yeah. Um, so it might be the case that our networks are still somewhat small.\n\n39:22.880 --> 39:27.040\n Uh, it also might be the case that they are more efficient than brain or less\n\n39:27.040 --> 39:29.280\n efficient by some, by some huge factor.\n\n39:29.680 --> 39:33.960\n Um, I also believe that there will be like a, you know, at the moment we are at\n\n39:33.960 --> 39:39.000\n the stage that the, these neural networks, they require thousand X or, or like a\n\n39:39.000 --> 39:41.920\n huge factor of more data than humans do.\n\n39:41.920 --> 39:48.560\n And it will be a matter of, uh, um, there will be algorithms that vastly decrease\n\n39:48.560 --> 39:52.960\n sample complexity, I believe so, but that place where we are heading today is\n\n39:53.280 --> 39:57.920\n there are domains which contains million X more data.\n\n39:58.080 --> 40:02.320\n And even though computers might be 1000 times slower than humans in learning,\n\n40:02.640 --> 40:03.520\n that's not a problem.\n\n40:03.560 --> 40:09.640\n Like, uh, for instance, uh, I believe that, uh, it should be possible to create\n\n40:09.640 --> 40:15.560\n super human therapist, uh, by, uh, and, and the, the, like, uh, even simple\n\n40:15.560 --> 40:18.520\n steps of, of, of doing what, of, of doing it.\n\n40:18.880 --> 40:23.560\n And, you know, the, the core reason is there is just machine will be able to\n\n40:23.560 --> 40:27.760\n read way more transcripts of therapies, and then it should be able to speak\n\n40:27.760 --> 40:31.720\n simultaneously with many more people and it should be possible to optimize it,\n\n40:31.960 --> 40:33.160\n uh, all in parallel.\n\n40:33.760 --> 40:37.760\n And, uh, well, there's now you're touching on something I deeply care about\n\n40:37.760 --> 40:39.960\n and think is way harder than we imagine.\n\n40:40.360 --> 40:43.360\n Um, what's the goal of a therapist?\n\n40:43.480 --> 40:44.520\n What's the goal of therapies?\n\n40:45.520 --> 40:50.600\n So, okay, so one goal now this is terrifying to me, but there's a lot of\n\n40:50.600 --> 40:57.320\n people that, uh, contemplate suicide, suffer from depression, uh, and they\n\n40:57.320 --> 41:03.640\n could significantly be helped with therapy and the idea that an AI algorithm\n\n41:03.640 --> 41:08.240\n might be in charge of that, it's like a life and death task.\n\n41:08.480 --> 41:11.440\n It's, uh, the stakes are high.\n\n41:12.000 --> 41:19.400\n So one goal for a therapist, whether human or AI is to prevent suicide\n\n41:19.400 --> 41:21.640\n ideation to prevent suicide.\n\n41:21.960 --> 41:23.040\n How do you achieve that?\n\n41:23.800 --> 41:25.240\n So let's see.\n\n41:25.800 --> 41:31.160\n So to be clear, I don't think that the current models are good enough for such\n\n41:31.160 --> 41:35.280\n a task because it requires insane amount of understanding, empathy, and the\n\n41:35.280 --> 41:38.360\n models are far from this place, but it's.\n\n41:38.640 --> 41:43.120\n But do you think that understanding empathy, that signal is in the data?\n\n41:43.560 --> 41:45.520\n Um, I think there is some signal in the data.\n\n41:45.520 --> 41:45.800\n Yes.\n\n41:45.800 --> 41:50.840\n I mean, there are plenty of transcripts of conversations and it is possible to,\n\n41:51.680 --> 41:54.320\n it is possible from it to understand personalities.\n\n41:54.480 --> 41:59.720\n It is possible from it to understand, uh, if conversation is, uh,\n\n41:59.720 --> 42:05.720\n friendly, uh, amicable, uh, uh, antagonistic, it is, I believe that the,\n\n42:05.760 --> 42:12.440\n you know, given the fact that the models that we train now, they can, uh, they\n\n42:12.440 --> 42:17.000\n can have, they are chameleons that they can have any personality, they might\n\n42:17.000 --> 42:21.520\n turn out to be better in understanding, uh, personality of other people than\n\n42:21.520 --> 42:24.720\n anyone else and they empathetic to be empathetic.\n\n42:24.760 --> 42:25.040\n Yeah.\n\n42:25.840 --> 42:26.520\n Interesting.\n\n42:26.520 --> 42:34.960\n Yeah, interesting. Uh, but I wonder if there's some level of, uh, multiple\n\n42:34.960 --> 42:41.560\n modalities required to be able to, um, be empathetic of the human experience,\n\n42:42.000 --> 42:46.080\n whether language is not enough to understand death, to understand fear,\n\n42:46.080 --> 42:54.240\n to understand, uh, childhood trauma, to understand, uh, wit and humor required\n\n42:54.240 --> 42:59.040\n when you're dancing with a person who might be depressed or suffering both\n\n42:59.040 --> 43:02.320\n humor and hope and love and all those kinds of things.\n\n43:02.760 --> 43:07.480\n So there's another underlying question, which is self supervised versus\n\n43:07.480 --> 43:08.320\n supervised.\n\n43:09.440 --> 43:16.280\n So can you get that from the data by just reading a huge number of transcripts?\n\n43:16.320 --> 43:20.400\n I actually, so I think that reading huge number of transcripts is a step one.\n\n43:20.400 --> 43:25.200\n It's like at the same way as you cannot learn to dance if just from YouTube by\n\n43:25.200 --> 43:27.720\n watching it, you have to actually try it out yourself.\n\n43:28.160 --> 43:31.280\n And so I think that here that's a similar situation.\n\n43:31.520 --> 43:36.400\n I also wouldn't deploy the system in the high stakes situations right away, but\n\n43:36.400 --> 43:39.280\n kind of see gradually where it goes.\n\n43:39.600 --> 43:45.680\n And, uh, obviously initially, uh, it would have to go hand in hand with humans.\n\n43:45.680 --> 43:50.480\n But, uh, at the moment we are in the situation that actually there is many\n\n43:50.480 --> 43:55.400\n more people who actually would like to have a therapy or, or speak with, with\n\n43:55.400 --> 43:57.080\n someone than there are therapies out there.\n\n43:57.400 --> 44:02.320\n I can, you know, I was so, so fundamentally I was thinking, what are\n\n44:02.320 --> 44:08.760\n the things that, uh, can vastly increase people's well being therapy is one of\n\n44:08.760 --> 44:13.160\n them being meditation is other one, I guess maybe human connection is a third\n\n44:13.160 --> 44:17.840\n one, and I guess pharmacologically it's also possible, maybe direct brain\n\n44:17.840 --> 44:19.160\n stimulation or something like that.\n\n44:19.160 --> 44:21.120\n But these are pretty much options out there.\n\n44:21.440 --> 44:26.040\n Then let's say the way I'm thinking about the AGI endeavor is by default,\n\n44:26.040 --> 44:29.520\n that's an endeavor to, uh, increase amount of wealth.\n\n44:29.960 --> 44:34.000\n And I believe that we can invest the increase amount of wealth for everyone\n\n44:34.400 --> 44:35.840\n and simultaneously.\n\n44:35.880 --> 44:39.040\n So, I mean, there are like a two endeavors that make sense to me.\n\n44:39.320 --> 44:41.760\n One is like essentially increase amount of wealth.\n\n44:41.760 --> 44:45.560\n And second one is, uh, increase overall human wellbeing.\n\n44:46.200 --> 44:49.280\n And those are coupled together and they, they can, like, uh, I would\n\n44:49.280 --> 44:51.080\n say these are different topics.\n\n44:51.080 --> 44:56.760\n One can help another and, uh, you know, therapist is a, is a funny word\n\n44:57.080 --> 44:59.520\n because I see friendship and love as therapy.\n\n44:59.520 --> 45:04.000\n I mean, so therapist broadly defined as just friendship as a friend.\n\n45:04.640 --> 45:10.160\n So like therapist is, has a very kind of clinical sense to it, but what\n\n45:10.160 --> 45:17.800\n is human connection you're like, uh, not to get all Camus and Dostoevsky on you,\n\n45:17.800 --> 45:23.880\n but you know, life is suffering and we draw, we seek connection with the\n\n45:23.880 --> 45:29.760\n humans as we, uh, desperately try to make sense of this world in a deep\n\n45:30.040 --> 45:33.080\n overwhelming loneliness that we feel inside.\n\n45:34.040 --> 45:36.680\n So I think connection has to do with understanding.\n\n45:36.680 --> 45:40.120\n And I think that almost like a lack of understanding causes suffering.\n\n45:40.160 --> 45:45.440\n If you speak with someone and do you, do you feel ignored that actually causes pain?\n\n45:45.480 --> 45:50.720\n If you are feeling deeply understood that actually they, they, they might\n\n45:50.720 --> 45:54.720\n not even tell you what to do in life, but like a pure understanding\n\n45:54.800 --> 45:59.240\n or just being heard, understanding is a kind of, that's a lot, you know,\n\n45:59.480 --> 46:04.840\n just being heard, feel like you're being heard, like somehow that's a\n\n46:04.840 --> 46:10.720\n alleviation temporarily of the loneliness that if somebody knows\n\n46:10.720 --> 46:15.960\n you're here with their body language, with the way they are, with the way\n\n46:15.960 --> 46:20.400\n they look at you, with the way they talk, do you feel less alone for a brief moment?\n\n46:22.080 --> 46:23.200\n Yeah, very much agree.\n\n46:23.320 --> 46:28.000\n So I thought in the past about, um, somewhat similar question to yours,\n\n46:28.000 --> 46:31.320\n which is what is love, uh, rather what is connection.\n\n46:31.320 --> 46:36.240\n Yes. And, um, and obviously I think about these things from AI perspective.\n\n46:36.240 --> 46:36.960\n What would it mean?\n\n46:37.480 --> 46:43.080\n Um, so I said that, um, you know, intelligence has to do with some compression,\n\n46:43.120 --> 46:46.720\n which is more or less like I can say, almost understanding of what is going around.\n\n46:47.200 --> 46:52.720\n It seems to me that, uh, other aspect is there seem to be reward functions and you\n\n46:52.720 --> 46:59.040\n can have, uh, uh, you know, reward for, uh, food, for maybe human connection, for,\n\n46:59.040 --> 47:02.840\n uh, let's say warmth, uh, sex and so on.\n\n47:03.480 --> 47:09.880\n And, um, and it turns out that the various people might be optimizing slightly\n\n47:09.880 --> 47:11.320\n different, uh, reward functions.\n\n47:11.320 --> 47:13.520\n They essentially might care about different things.\n\n47:14.120 --> 47:20.840\n And, uh, uh, in case of, uh, love at least the love between two people, you can say\n\n47:20.840 --> 47:25.560\n that the, um, you know, boundary between people dissolves to such extent that, uh,\n\n47:25.560 --> 47:33.160\n they end up optimizing each other reward functions and yeah, oh, that's interesting.\n\n47:33.200 --> 47:36.800\n Um, celebrate the success of each other.\n\n47:36.880 --> 47:37.160\n Yeah.\n\n47:37.200 --> 47:42.760\n In some sense, I would say love means, uh, helping others to optimize their, uh,\n\n47:42.800 --> 47:45.840\n reward functions, not your reward functions, not the things that you think are\n\n47:45.840 --> 47:51.080\n important, but the things that the person cares about, you try to help them to,\n\n47:51.120 --> 47:51.920\n uh, optimize it.\n\n47:51.920 --> 47:56.840\n So love is, uh, if you think of two reward functions, you just, it's a condition.\n\n47:56.880 --> 48:00.840\n You combine them together, pretty much maybe like with a weight and it depends\n\n48:00.840 --> 48:02.640\n like the dynamic of the relationship.\n\n48:02.760 --> 48:03.080\n Yeah.\n\n48:03.080 --> 48:06.560\n I mean, you could imagine that if you're fully, uh, optimizing someone's reward\n\n48:06.560 --> 48:10.360\n function without yours, then, then maybe are creating codependency or something\n\n48:10.360 --> 48:14.600\n like that, but I'm not sure what's the appropriate weight, but the interesting\n\n48:14.600 --> 48:19.920\n thing is I even, I even think that the, uh, individual reward function is\n\n48:19.920 --> 48:27.480\n saying that the individual person, uh, uh, we ourselves, we are actually less\n\n48:27.480 --> 48:29.640\n of a unified insight.\n\n48:29.720 --> 48:33.520\n So for instance, if you look at, at the donut on the one level, you might think,\n\n48:33.560 --> 48:35.080\n oh, this is like, it looks tasty.\n\n48:35.080 --> 48:36.600\n I would like to eat it on other level.\n\n48:36.840 --> 48:41.600\n You might tell yourself, I shouldn't be doing it because I want to gain muscles.\n\n48:42.000 --> 48:45.920\n So, and you know, you might do it regardless kind of against yourself.\n\n48:45.920 --> 48:50.520\n So it seems that even within ourselves, they're almost like a kind of intertwined\n\n48:50.520 --> 48:57.440\n personas and, um, I believe that the self love means that, uh, the love between all\n\n48:57.440 --> 49:03.880\n these personas, which also means being able to love, love yourself when we are\n\n49:04.280 --> 49:08.400\n angry or stressed or so combining all those reward functions of the different\n\n49:08.400 --> 49:12.000\n selves you have and accepting that they are there, like, uh, you know, often\n\n49:12.000 --> 49:16.320\n people, they have a negative self talk or they say, I don't like when I'm angry.\n\n49:16.720 --> 49:23.840\n And like, I try to imagine, try to imagine if there would be like a small\n\n49:23.840 --> 49:29.640\n baby Lex, like a five years old, angry, and then they are like, you shouldn't\n\n49:29.640 --> 49:30.080\n be angry.\n\n49:30.080 --> 49:31.240\n Like stop being angry.\n\n49:31.280 --> 49:31.640\n Yeah.\n\n49:31.720 --> 49:35.920\n But like an instant, actually you want the Lex to come over, give him a hug and\n\n49:35.920 --> 49:37.240\n just like, I say, it's fine.\n\n49:37.560 --> 49:37.920\n Okay.\n\n49:37.920 --> 49:39.960\n It's going to be angry as long as you want.\n\n49:39.960 --> 49:45.240\n And then he would stop or, or maybe not, or maybe not, but you cannot expect it\n\n49:45.240 --> 49:45.520\n even.\n\n49:45.800 --> 49:46.200\n Yeah.\n\n49:46.800 --> 49:49.280\n But still, that doesn't explain the why of love.\n\n49:49.280 --> 49:51.720\n Like why is love part of the human condition?\n\n49:51.720 --> 49:55.560\n Why is it useful to combine the reward functions?\n\n49:56.160 --> 50:01.080\n It seems like that doesn't, I mean, I don't think reinforcement learning\n\n50:01.080 --> 50:06.800\n frameworks can give us answers to why even, even the Hutter framework has\n\n50:06.800 --> 50:08.920\n an objective function that's static.\n\n50:08.920 --> 50:13.640\n So we came to existence as a consequence of evolutionary process.\n\n50:13.960 --> 50:16.800\n And in some sense, the purpose of evolution is survival.\n\n50:17.080 --> 50:23.720\n And then the, this complicated optimization objective baked into us, let's\n\n50:23.720 --> 50:27.960\n say compression, which might help us operate in the real world and it baked\n\n50:27.960 --> 50:29.600\n into us various reward functions.\n\n50:29.680 --> 50:30.040\n Yeah.\n\n50:31.080 --> 50:35.360\n Then to be clear at the moment we are operating in the regime, which is somewhat\n\n50:35.360 --> 50:38.040\n out of distribution, where they even evolution optimized us.\n\n50:38.040 --> 50:42.640\n It's almost like love is a consequence of a cooperation that we've discovered is\n\n50:42.640 --> 50:43.160\n useful.\n\n50:43.240 --> 50:43.640\n Correct.\n\n50:43.880 --> 50:45.800\n In some way it's even the case.\n\n50:45.800 --> 50:49.680\n If you, I just love the idea that love is like the out of distribution.\n\n50:50.560 --> 50:51.720\n Or it's not out of distribution.\n\n50:51.720 --> 50:53.960\n It's like, as you said, it evolved for cooperation.\n\n50:54.600 --> 50:55.000\n Yes.\n\n50:55.000 --> 50:58.960\n And I believe that the cop, like in some sense, cooperation ends up helping each\n\n50:58.960 --> 51:03.400\n of us individually, so it makes sense evolutionary and there is a, in some\n\n51:03.400 --> 51:08.000\n sense, and, you know, love means there is this dissolution of boundaries that you\n\n51:08.000 --> 51:12.640\n have a shared reward function and we evolve to actually identify ourselves with\n\n51:12.640 --> 51:18.160\n larger groups, so we can identify ourselves, you know, with a family, we can\n\n51:18.160 --> 51:22.240\n identify ourselves with a country to such extent that people are willing to give\n\n51:22.240 --> 51:23.520\n away their life for country.\n\n51:24.880 --> 51:29.000\n So there is, we are wired actually even for love.\n\n51:29.000 --> 51:36.440\n And at the moment, I guess, the, maybe it would be somewhat more beneficial if you\n\n51:36.440 --> 51:40.200\n will, if we would identify ourselves with all the humanity as a whole.\n\n51:40.520 --> 51:44.440\n So you can clearly see when people travel around the world, when they run into\n\n51:44.440 --> 51:48.720\n person from the same country, they say, oh, which CPR and all this, like all the\n\n51:48.720 --> 51:50.920\n sudden they find all these similarities.\n\n51:50.920 --> 51:55.040\n They find some, they befriended those folks earlier than others.\n\n51:55.040 --> 51:58.840\n So there is like a sense, some sense of the belonging. And I would say, I think\n\n51:58.840 --> 52:05.720\n it would be overall good thing to the world for people to move towards, I think\n\n52:05.720 --> 52:11.320\n it's even called open individualism, move toward the mindset of a larger and\n\n52:11.320 --> 52:12.160\n larger groups.\n\n52:12.400 --> 52:17.520\n So the challenge there, that's a beautiful vision and I share it to expand\n\n52:17.520 --> 52:21.960\n that circle of empathy, that circle of love towards the entirety of humanity.\n\n52:21.960 --> 52:24.520\n But then you start to ask, well, where do you draw the line?\n\n52:25.120 --> 52:28.200\n Because why not expand it to other conscious beings?\n\n52:28.520 --> 52:34.360\n And then finally, for our discussion, something I think about is why not\n\n52:34.360 --> 52:36.320\n expand it to AI systems?\n\n52:37.200 --> 52:42.080\n Like we, we start respecting each other when the, the person, the entity on the\n\n52:42.080 --> 52:45.000\n other side has the capacity to suffer.\n\n52:45.360 --> 52:49.320\n Cause then we develop a capacity to sort of empathize.\n\n52:49.320 --> 52:54.640\n And so I could see AI systems that are interacting with humans more and more\n\n52:54.640 --> 52:58.200\n having conscious, like displays.\n\n52:58.480 --> 53:02.400\n So like they display consciousness through language and through other means.\n\n53:02.880 --> 53:06.280\n And so then the question is like, well, is that consciousness?\n\n53:06.840 --> 53:08.440\n Because they're acting conscious.\n\n53:08.960 --> 53:15.920\n And so, you know, the reason we don't like torturing animals is because\n\n53:15.920 --> 53:21.240\n they look like they're suffering when they're tortured and if AI looks like\n\n53:21.240 --> 53:30.560\n it's suffering when it's tortured, how is that not requiring of the same kind\n\n53:30.560 --> 53:35.880\n of empathy from us and respect and rights that animals do and other humans do?\n\n53:35.920 --> 53:37.600\n I think it requires empathy as well.\n\n53:37.600 --> 53:42.520\n I mean, I would like, I guess us or humanity or so make a progress in\n\n53:42.520 --> 53:46.040\n understanding what consciousness is, because I don't want just to be speaking\n\n53:46.040 --> 53:50.800\n about that, the philosophy, but rather actually make a scientific, uh, to have\n\n53:50.800 --> 53:56.280\n a, like, you know, there was a time that people thought that there is a force of\n\n53:56.280 --> 54:01.560\n life and, uh, the things that have this force, they are alive.\n\n54:03.040 --> 54:08.280\n And, um, I think that there is actually a path to understand exactly what\n\n54:08.280 --> 54:10.560\n consciousness is and how it works.\n\n54:10.560 --> 54:13.160\n Understand exactly what consciousness is.\n\n54:13.840 --> 54:19.440\n And, uh, um, in some sense, it might require essentially putting\n\n54:19.440 --> 54:23.760\n probes inside of a human brain, uh, what Neuralink, uh, does.\n\n54:23.800 --> 54:26.440\n So the goal there, I mean, there's several things with consciousness\n\n54:26.440 --> 54:30.240\n that make it a real discipline, which is one is rigorous\n\n54:30.240 --> 54:31.640\n measurement of consciousness.\n\n54:32.480 --> 54:34.680\n And then the other is the engineering of consciousness,\n\n54:34.680 --> 54:36.240\n which may or may not be related.\n\n54:36.520 --> 54:38.840\n I mean, you could also run into trouble.\n\n54:38.840 --> 54:43.200\n Like, for example, in the United States for the department, DOT,\n\n54:43.200 --> 54:46.680\n department of transportation, and a lot of different places\n\n54:46.720 --> 54:48.200\n put a value on human life.\n\n54:48.720 --> 54:53.040\n I think DOT is, uh, values $9 million per person.\n\n54:54.200 --> 54:56.760\n Sort of in that same way, you can get into trouble.\n\n54:57.840 --> 55:01.960\n If you put a number on how conscious of being is, because then you can start\n\n55:01.960 --> 55:11.960\n making policy, if a cow is a 0.1 or like, um, 10% as conscious as a human,\n\n55:12.400 --> 55:15.360\n then you can start making calculations and it might get you into trouble.\n\n55:15.360 --> 55:17.720\n But then again, that might be a very good way to do it.\n\n55:18.920 --> 55:23.360\n I would like, uh, to move to that place that actually we have scientific\n\n55:23.360 --> 55:24.720\n understanding what consciousness is.\n\n55:25.160 --> 55:27.400\n And then we'll be able to actually assign value.\n\n55:27.400 --> 55:32.000\n And I believe that there is even the path for the experimentation in it.\n\n55:32.440 --> 55:37.760\n So, uh, you know, w we said that, you know, you could put the\n\n55:37.800 --> 55:38.960\n probes inside of the brain.\n\n55:39.280 --> 55:42.640\n There is actually a few other things that you could do with\n\n55:42.640 --> 55:44.120\n devices like Neuralink.\n\n55:44.400 --> 55:48.800\n So you could imagine that the way even to measure if AI system is conscious\n\n55:49.360 --> 55:51.920\n is by literally just plugging into the brain.\n\n55:52.760 --> 55:56.040\n Um, I mean, that, that seems like it's kind of easy, but the plugging\n\n55:56.040 --> 55:59.240\n into the brain and asking person if they feel that their consciousness\n\n55:59.240 --> 56:02.880\n expanded, um, this direction of course has some issues.\n\n56:02.880 --> 56:05.880\n You can say, you know, if someone takes a psychedelic drug, they might\n\n56:05.880 --> 56:08.920\n feel that their consciousness expanded, even though that drug\n\n56:08.920 --> 56:09.960\n itself is not conscious.\n\n56:10.840 --> 56:11.280\n Right.\n\n56:11.520 --> 56:15.480\n So like, you can't fully trust the self report of a person saying their,\n\n56:15.800 --> 56:18.040\n their consciousness is expanded or not.\n\n56:20.280 --> 56:23.160\n Let me ask you a little bit about psychedelics is, uh, there've been\n\n56:23.160 --> 56:26.960\n a lot of excellent research on, uh, different psychedelics, psilocybin,\n\n56:26.960 --> 56:32.200\n MDMA, even DMT drugs in general, marijuana too.\n\n56:33.280 --> 56:36.360\n Uh, what do you think psychedelics do to the human mind?\n\n56:36.800 --> 56:40.840\n It seems they take the human mind to some interesting places.\n\n56:41.760 --> 56:46.760\n Is that just a little, uh, hack, a visual hack, or is there some\n\n56:46.760 --> 56:48.480\n profound expansion of the mind?\n\n56:49.160 --> 56:52.120\n So let's see, I don't believe in magic.\n\n56:52.120 --> 57:00.000\n I believe in, uh, I believe in, uh, in science in, in causality, um, still,\n\n57:00.000 --> 57:06.000\n let's say, and then as I said, like, I think that the brain, that the, our\n\n57:06.000 --> 57:12.120\n subjective experience of reality is, uh, we live in the simulation run by our\n\n57:12.120 --> 57:17.200\n brain and the simulation that our brain runs, they can be very pleasant or very\n\n57:17.200 --> 57:22.680\n hellish drugs, they are changing some hyper parameters of the simulation.\n\n57:23.040 --> 57:27.920\n It is possible thanks to change of these hyper parameters to actually look back\n\n57:27.920 --> 57:32.160\n on your experience and even see that the given things that we took for\n\n57:32.160 --> 57:34.360\n granted, they are changeable.\n\n57:35.320 --> 57:38.880\n So they allow to have a amazing perspective.\n\n57:39.160 --> 57:44.280\n There is also, for instance, the fact that after DMT people can see the\n\n57:44.280 --> 57:51.480\n full movie inside of their head, gives me further belief that the brain can generate\n\n57:51.480 --> 57:57.000\n that full movie, that the brain is actually learning the model of reality\n\n57:57.000 --> 57:59.840\n to such extent that it tries to predict what's going to happen next.\n\n58:00.080 --> 58:00.280\n Yeah.\n\n58:00.280 --> 58:01.560\n Very high resolution.\n\n58:01.560 --> 58:02.880\n So it can replay reality.\n\n58:03.400 --> 58:04.960\n Extremely high resolution.\n\n58:05.640 --> 58:05.920\n Yeah.\n\n58:05.960 --> 58:11.040\n It's also kind of interesting to me that somehow there seems to be some similarity\n\n58:11.040 --> 58:15.920\n between these, uh, drugs and meditation itself.\n\n58:16.440 --> 58:20.760\n And I actually started even these days to think about meditation as a psychedelic.\n\n58:22.240 --> 58:23.600\n Do you practice meditation?\n\n58:24.160 --> 58:25.800\n I practice meditation.\n\n58:26.080 --> 58:31.520\n I mean, I went a few times on the retreats and it feels after like after\n\n58:31.520 --> 58:39.080\n second or third day of meditation, uh, there is a, there is almost like a\n\n58:39.080 --> 58:43.520\n sense of, you know, tripping what, what does the meditation retreat entail?\n\n58:44.320 --> 58:50.520\n So you w you wake up early in the morning and you meditate for extended\n\n58:50.520 --> 58:56.480\n period of time, uh, and yeah, so it's optimized, even though there are other\n\n58:56.480 --> 58:59.440\n people, it's optimized for isolation.\n\n58:59.600 --> 59:01.040\n So you don't speak with anyone.\n\n59:01.040 --> 59:06.360\n You don't actually look into other people's eyes and, uh, you know, you sit\n\n59:06.360 --> 59:13.160\n on the chair and say Vipassana meditation tells you, uh, to focus on the breath.\n\n59:13.200 --> 59:18.600\n So you try to put, uh, all the, all attention into breathing and, uh,\n\n59:18.640 --> 59:19.920\n breathing in and breathing out.\n\n59:20.440 --> 59:26.760\n And the crazy thing is that as you focus attention like that, uh, after some\n\n59:26.760 --> 59:33.080\n time, their stems starts coming back, like some memories that you completely\n\n59:33.080 --> 59:39.320\n forgotten, it almost feels like, uh, that you'll have a mailbox and then you know,\n\n59:39.320 --> 59:42.320\n you are just like a archiving email one by one.\n\n59:43.080 --> 59:48.640\n And at some point, at some point there is this like a amazing feeling of getting\n\n59:48.640 --> 59:50.960\n to mailbox zero, zero emails.\n\n59:51.040 --> 59:52.880\n And, uh, it's very pleasant.\n\n59:53.080 --> 1:00:02.400\n It's, it's kind of, it's, it's, it's crazy to me that, um, that once you\n\n1:00:02.400 --> 1:00:08.960\n resolve these, uh, inner store stories or like inner traumas, then once there is\n\n1:00:08.960 --> 1:00:16.040\n nothing, uh, left that default, uh, state of human mind is extremely peaceful and\n\n1:00:16.040 --> 1:00:24.520\n happy, extreme, like, uh, some sense it, it feels that the, it feels at least to\n\n1:00:24.520 --> 1:00:30.400\n me that way, how, when I was a child that I can look at any object and it's very\n\n1:00:30.400 --> 1:00:34.680\n beautiful, I have a lot of curiosity about the simple things and that's where\n\n1:00:34.680 --> 1:00:36.080\n the usual meditation takes me.\n\n1:00:37.440 --> 1:00:40.000\n Are you, what are you experiencing?\n\n1:00:40.040 --> 1:00:45.560\n Are you just taking in simple sensory information and they're just enjoying\n\n1:00:45.560 --> 1:00:47.840\n the rawness of that sensory information?\n\n1:00:48.120 --> 1:00:52.000\n So there's no, there's no memories or all that kind of stuff.\n\n1:00:52.000 --> 1:00:54.400\n You're just enjoying being.\n\n1:00:54.960 --> 1:00:55.920\n Yeah, pretty much.\n\n1:00:55.920 --> 1:01:00.760\n I mean, still there is, uh, that it's, it's thoughts are slowing down.\n\n1:01:00.880 --> 1:01:06.080\n Sometimes they pop up, but it's also somehow the extended meditation takes you\n\n1:01:06.080 --> 1:01:11.080\n to the space that they are way more friendly, way more positive.\n\n1:01:11.400 --> 1:01:19.240\n Um, there is also this, uh, this thing that, uh, we've, it almost feels that the.\n\n1:01:19.240 --> 1:01:24.240\n It almost feels that the, we are constantly getting a little bit of a reward\n\n1:01:24.240 --> 1:01:28.240\n function and we are just spreading this reward function on various activities.\n\n1:01:28.560 --> 1:01:33.000\n But if you'll stay still for extended period of time, it kind of accumulates,\n\n1:01:33.000 --> 1:01:38.800\n accumulates, accumulates, and, uh, there is a, there is a sense, there is a sense\n\n1:01:38.800 --> 1:01:46.080\n that some point it passes some threshold and it feels as drop is falling into kind\n\n1:01:46.080 --> 1:01:49.920\n of ocean of love and this, and that's like, uh, this is like a very pleasant.\n\n1:01:49.920 --> 1:01:54.440\n And that's, I'm saying like, uh, that corresponds to the subjective experience.\n\n1:01:54.920 --> 1:02:01.440\n Some people, uh, I guess in spiritual community, they describe it that that's\n\n1:02:01.440 --> 1:02:04.840\n the reality, and I would say, I believe that they're like, uh, all sorts of\n\n1:02:04.840 --> 1:02:06.920\n subjective experience that one can have.\n\n1:02:07.320 --> 1:02:11.720\n And, uh, I believe that for instance, meditation might take you to the\n\n1:02:11.720 --> 1:02:13.640\n subjective experiences with the subject.\n\n1:02:13.640 --> 1:02:16.480\n Vision might take you to the subjective experiences, which are\n\n1:02:16.480 --> 1:02:17.880\n very pleasant, collaborative.\n\n1:02:18.080 --> 1:02:23.400\n And I would like a word to move toward a more collaborative, uh, uh, place.\n\n1:02:24.880 --> 1:02:25.200\n Yeah.\n\n1:02:25.240 --> 1:02:28.240\n I would say that's very pleasant and I enjoy doing stuff like that.\n\n1:02:28.440 --> 1:02:34.800\n I, um, I wonder how that maps to your, uh, mathematical model of love with, uh,\n\n1:02:35.040 --> 1:02:42.280\n the reward function, combining a bunch of things, it seems like our life then is\n\n1:02:42.280 --> 1:02:46.120\n just, we have this reward function and we're accumulating a bunch of stuff\n\n1:02:46.120 --> 1:02:55.000\n in it with weights, it's like, um, like multi objective and what meditation\n\n1:02:55.000 --> 1:03:01.040\n is, is you just remove them, remove them until the weight on one, uh, or\n\n1:03:01.040 --> 1:03:04.960\n just a few is very high and that's where the pleasure comes from.\n\n1:03:05.200 --> 1:03:05.480\n Yeah.\n\n1:03:05.480 --> 1:03:08.200\n So something similar, how I'm thinking about this.\n\n1:03:08.200 --> 1:03:13.240\n So I told you that there is this like, uh, that there is a story of who you are.\n\n1:03:14.120 --> 1:03:19.000\n And I think almost about it as a, you know, text prepended to GPT.\n\n1:03:20.400 --> 1:03:20.720\n Yeah.\n\n1:03:21.000 --> 1:03:23.720\n And, uh, some people refer to it as ego.\n\n1:03:24.120 --> 1:03:24.480\n Okay.\n\n1:03:24.600 --> 1:03:27.560\n There's like a story who, who, who you are.\n\n1:03:27.560 --> 1:03:27.880\n Okay.\n\n1:03:28.000 --> 1:03:31.320\n So ego is the prompt for GPT three or GPT.\n\n1:03:31.360 --> 1:03:31.600\n Yes.\n\n1:03:31.600 --> 1:03:31.760\n Yes.\n\n1:03:31.760 --> 1:03:32.720\n And that's description of you.\n\n1:03:32.960 --> 1:03:37.080\n And then with meditation, you can get to the point that actually you experience\n\n1:03:37.080 --> 1:03:42.480\n things without the prompt and you experience things like as they are, you\n\n1:03:42.480 --> 1:03:47.040\n are not biased over the description, how they supposed to be, uh, that's very\n\n1:03:47.040 --> 1:03:47.480\n pleasant.\n\n1:03:47.480 --> 1:03:49.560\n And then we've respected the reward function.\n\n1:03:50.000 --> 1:03:54.960\n Uh, it's possible to get to the point that the, there is the solution of self.\n\n1:03:55.480 --> 1:03:59.480\n And therefore you can say that the, or you're having a, your, or like a, your\n\n1:03:59.480 --> 1:04:03.320\n brain attempts to simulate the reward function of everyone else or like\n\n1:04:03.320 --> 1:04:07.120\n everything that's that there is this like a love, which feels like a oneness with\n\n1:04:07.120 --> 1:04:07.560\n everything.\n\n1:04:08.760 --> 1:04:11.440\n And that's also, you know, very beautiful, very pleasant.\n\n1:04:11.440 --> 1:04:16.120\n At some point you might have a lot of altruistic thoughts during that moment.\n\n1:04:16.120 --> 1:04:18.840\n And then the self, uh, always comes back.\n\n1:04:19.240 --> 1:04:23.480\n How would you recommend if somebody is interested in meditation, like a big\n\n1:04:23.480 --> 1:04:27.360\n thing to take on as a project, would you recommend a meditation retreat?\n\n1:04:27.400 --> 1:04:29.840\n How many days, what kind of thing would you recommend?\n\n1:04:30.160 --> 1:04:32.560\n I think that actually retreat is the way to go.\n\n1:04:32.560 --> 1:04:38.800\n Um, it almost feels that, uh, um, as I said, like a meditation is a psychedelic,\n\n1:04:39.000 --> 1:04:43.000\n but, uh, when you take it in the small dose, you might barely feel it.\n\n1:04:43.280 --> 1:04:46.160\n Once you get the high dose, actually you're going to feel it.\n\n1:04:46.880 --> 1:04:51.800\n Um, so even cold turkey, if you haven't really seriously meditated for a long\n\n1:04:51.800 --> 1:04:53.800\n period of time, just go to a retreat.\n\n1:04:53.920 --> 1:04:54.280\n Yeah.\n\n1:04:54.280 --> 1:04:55.560\n How many days, how many days?\n\n1:04:55.560 --> 1:04:57.600\n Start weekend one weekend.\n\n1:04:57.600 --> 1:04:58.800\n So like two, three days.\n\n1:04:58.800 --> 1:05:03.480\n And it's like, uh, it's interesting that first or second day, it's hard.\n\n1:05:03.520 --> 1:05:05.040\n And at some point it becomes easy.\n\n1:05:06.560 --> 1:05:08.200\n There's a lot of seconds in a day.\n\n1:05:08.520 --> 1:05:12.360\n How hard is the meditation retreat just sitting there in a chair?\n\n1:05:13.040 --> 1:05:20.480\n So the thing is actually, it literally just depends on your, uh, on the,\n\n1:05:20.800 --> 1:05:24.560\n your own framing, like if you are in the mindset that you are waiting for it to\n\n1:05:24.560 --> 1:05:28.720\n be over, or you are waiting for a Nirvana to happen, you are waiting\n\n1:05:28.720 --> 1:05:30.200\n it will be very unpleasant.\n\n1:05:30.680 --> 1:05:36.480\n And in some sense, even the difficulty, it's not even in the lack of being\n\n1:05:36.480 --> 1:05:40.360\n able to speak with others, like, uh, you're sitting there, your legs\n\n1:05:40.360 --> 1:05:44.440\n will hurt from sitting in terms of like the practical things.\n\n1:05:44.480 --> 1:05:48.160\n Do you experience kind of discomfort, like physical discomfort of just\n\n1:05:48.160 --> 1:05:53.720\n sitting, like your, your butt being numb, your legs being sore, all that kind of\n\n1:05:53.720 --> 1:05:54.040\n stuff?\n\n1:05:54.160 --> 1:05:54.520\n Yes.\n\n1:05:54.520 --> 1:05:55.360\n You experience it.\n\n1:05:55.360 --> 1:05:59.320\n And then the, the, they teach you to observe it rather.\n\n1:05:59.320 --> 1:06:03.280\n And it's like, uh, the crazy thing is you at first might have a feeling\n\n1:06:03.280 --> 1:06:07.520\n toward trying to escape it and that becomes very apparent that that's\n\n1:06:07.560 --> 1:06:08.640\n extremely unpleasant.\n\n1:06:09.120 --> 1:06:11.400\n And then you just, just observe it.\n\n1:06:11.840 --> 1:06:18.720\n And then at some point it just becomes, uh, it just is, it's like, uh, I remember\n\n1:06:18.720 --> 1:06:22.680\n that we've, Ilya told me some time ago that, uh, you know, he takes a cold\n\n1:06:22.680 --> 1:06:28.200\n shower and he's the mindset of taking a cold shower was to embrace suffering.\n\n1:06:28.360 --> 1:06:28.680\n Yeah.\n\n1:06:28.960 --> 1:06:29.440\n Excellent.\n\n1:06:29.680 --> 1:06:30.320\n I do the same.\n\n1:06:30.320 --> 1:06:31.160\n This is your style?\n\n1:06:31.240 --> 1:06:32.320\n Yeah, it's my style.\n\n1:06:32.880 --> 1:06:33.480\n I like this.\n\n1:06:34.200 --> 1:06:38.520\n So my style is actually, I also sometimes take cold showers.\n\n1:06:38.960 --> 1:06:43.480\n It is purely observing how the water goes through my body, like a purely being\n\n1:06:43.480 --> 1:06:45.000\n present, not trying to escape from there.\n\n1:06:46.040 --> 1:06:46.360\n Yeah.\n\n1:06:46.800 --> 1:06:49.360\n And I would say then it actually becomes pleasant.\n\n1:06:49.360 --> 1:06:52.000\n It's not like, ah, well, that that's interesting.\n\n1:06:52.200 --> 1:06:57.520\n Um, I I'm also that mean that's, that's the way to deal with anything really\n\n1:06:57.520 --> 1:07:03.200\n difficult, especially in the physical space is to observe it to say it's pleasant.\n\n1:07:04.880 --> 1:07:05.200\n Hmm.\n\n1:07:05.600 --> 1:07:07.680\n It's a D I would use a different word.\n\n1:07:08.480 --> 1:07:14.160\n You're, um, you're accepting of the full beauty of reality.\n\n1:07:14.480 --> 1:07:16.600\n I would say, cause say pleasant.\n\n1:07:16.600 --> 1:07:19.520\n But yeah, I mean, in some sense it is pleasant.\n\n1:07:19.560 --> 1:07:24.200\n That's the only way to deal with a cold shower is to, to, uh, become an\n\n1:07:24.200 --> 1:07:27.000\n observer and to find joy in it.\n\n1:07:28.440 --> 1:07:32.920\n Um, same with like really difficult, physical, um, exercise or like running\n\n1:07:32.920 --> 1:07:37.680\n for a really long time, endurance events, just anytime you're, any kind of pain.\n\n1:07:38.040 --> 1:07:41.680\n I think the only way to survive it is not to resist it is to observe it.\n\n1:07:43.120 --> 1:07:46.520\n You mentioned, you mentioned, um, you mentioned, um, you mentioned\n\n1:07:46.520 --> 1:07:51.920\n Ilya, Ilya says, it's very, he's our chief scientist, but also\n\n1:07:51.920 --> 1:07:53.160\n he's very close friend of mine.\n\n1:07:53.600 --> 1:07:55.360\n He cofounded open air with you.\n\n1:07:56.280 --> 1:07:58.400\n I've spoken with him a few times.\n\n1:07:58.440 --> 1:07:59.160\n He's brilliant.\n\n1:07:59.160 --> 1:08:00.440\n I really enjoy talking to him.\n\n1:08:02.960 --> 1:08:06.040\n His mind, just like yours works in fascinating ways.\n\n1:08:06.960 --> 1:08:10.000\n Now, both of you are not able to define deep learning simply.\n\n1:08:10.000 --> 1:08:15.840\n Uh, what's it like having him as somebody you have technical discussions with on\n\n1:08:15.880 --> 1:08:20.360\n in the space of machine learning, deep learning, AI, but also life.\n\n1:08:21.200 --> 1:08:28.480\n What's it like when these two, um, agents get into a self play situation in a room?\n\n1:08:29.000 --> 1:08:30.280\n What's it like collaborating with him?\n\n1:08:30.840 --> 1:08:35.320\n So I believe that we have, uh, extreme, uh, respect to each other.\n\n1:08:35.320 --> 1:08:43.400\n So, uh, in, I love Ilya's insight, both like, uh, I guess about\n\n1:08:43.720 --> 1:08:49.480\n consciousness, uh, life AI, but, uh, in terms of the, it's interesting to\n\n1:08:49.480 --> 1:08:56.080\n me, cause you're a brilliant, uh, Thinker in the space of machine\n\n1:08:56.080 --> 1:09:01.840\n learning, like intuition, like digging deep in what works, what doesn't,\n\n1:09:01.840 --> 1:09:04.720\n why it works, why it doesn't, and so is Ilya.\n\n1:09:05.200 --> 1:09:09.600\n I'm wondering if there's interesting deep discussions you've had with him in the\n\n1:09:09.600 --> 1:09:12.040\n past or disagreements that were very productive.\n\n1:09:12.280 --> 1:09:18.000\n So I can say, I also understood over the time, where are my strengths?\n\n1:09:18.000 --> 1:09:24.240\n So obviously we have plenty of AI discussions and, um, um, and do you\n\n1:09:24.240 --> 1:09:29.440\n know, I myself have plenty of ideas, but like I consider Ilya, uh, what\n\n1:09:29.440 --> 1:09:32.440\n of the most prolific AI scientists in the entire world.\n\n1:09:33.160 --> 1:09:39.760\n And, uh, I think that, um, I realized that maybe my super skill, um, is, uh,\n\n1:09:40.000 --> 1:09:43.640\n being able to bring people to collaborate together, that I have some level of\n\n1:09:43.800 --> 1:09:46.320\n empathy that is unique in AI world.\n\n1:09:46.760 --> 1:09:50.800\n And that might come, you know, from either meditation, psychedelics, or\n\n1:09:50.800 --> 1:09:53.080\n let's say I read just hundreds of books on this topic.\n\n1:09:53.080 --> 1:09:56.920\n So, and I also went through a journey of, you know, I developed a\n\n1:09:56.920 --> 1:10:04.840\n lot of, uh, algorithms, so I think that maybe I can, that's my super human skill.\n\n1:10:05.320 --> 1:10:11.200\n Uh, Ilya is, uh, one of the best AI scientists, but then I'm pretty\n\n1:10:11.200 --> 1:10:14.920\n good in assembling teams and I'm also not holding to people.\n\n1:10:14.920 --> 1:10:18.080\n Like I'm growing people and then people become managers at OpenAI.\n\n1:10:18.400 --> 1:10:20.680\n I grew many of them, like a research managers.\n\n1:10:20.680 --> 1:10:27.240\n So you, you find, you find places where you're excellent and he finds like his,\n\n1:10:27.240 --> 1:10:31.800\n his, his deep scientific insights is where he is and you find ways you can,\n\n1:10:31.840 --> 1:10:33.600\n the puzzle pieces fit together.\n\n1:10:33.600 --> 1:10:33.920\n Correct.\n\n1:10:33.920 --> 1:10:37.680\n Like, uh, you know, ultimately, for instance, let's say Ilya, he doesn't\n\n1:10:37.680 --> 1:10:41.960\n manage people, uh, that's not what he likes or so.\n\n1:10:42.280 --> 1:10:45.680\n Um, I like, I like hanging out with people.\n\n1:10:45.680 --> 1:10:48.200\n By default, I'm an extrovert and I care about people.\n\n1:10:48.200 --> 1:10:50.840\n Oh, interesting. Okay. All right. Okay, cool.\n\n1:10:50.880 --> 1:10:52.880\n So that, that fits perfectly together.\n\n1:10:52.920 --> 1:10:56.600\n But I mean, uh, I also just like your intuition about various\n\n1:10:56.600 --> 1:10:57.920\n problems in machine learning.\n\n1:10:58.160 --> 1:11:00.480\n He's definitely one I really enjoy.\n\n1:11:01.440 --> 1:11:06.800\n I remember talking to him about something I was struggling with, which\n\n1:11:06.800 --> 1:11:12.920\n is coming up with a good model for pedestrians, for human beings across\n\n1:11:12.920 --> 1:11:16.800\n the street in the context of autonomous vehicles, and I was like, okay,\n\n1:11:16.800 --> 1:11:18.600\n in the context of autonomous vehicles.\n\n1:11:19.840 --> 1:11:24.400\n And he immediately started to like formulate a framework within which you\n\n1:11:24.400 --> 1:11:29.040\n can evolve a model for pedestrians, like through self play, all that kind of\n\n1:11:29.040 --> 1:11:35.040\n mechanisms, the depth of thought on a particular problem, especially problems\n\n1:11:35.040 --> 1:11:38.120\n he doesn't know anything about is, is fascinating to watch.\n\n1:11:38.560 --> 1:11:46.000\n It makes you realize like, um, yeah, the, the, the limits of the, that the human\n\n1:11:46.000 --> 1:11:50.560\n intellect may be limitless, or it's just impressive to see a descendant of\n\n1:11:50.560 --> 1:11:52.440\n ape come up with clever ideas.\n\n1:11:52.640 --> 1:11:53.000\n Yeah.\n\n1:11:53.000 --> 1:11:56.920\n I mean, so even in the space of deep learning, when you look at various\n\n1:11:56.920 --> 1:12:03.680\n people, there are people now who invented some breakthroughs once, but\n\n1:12:03.680 --> 1:12:06.120\n there are very few people who did it multiple times.\n\n1:12:06.280 --> 1:12:10.880\n And you can think if someone invented it once, that might be just a sheer luck.\n\n1:12:11.680 --> 1:12:15.160\n And if someone invented it multiple times, you know, if a probability of\n\n1:12:15.160 --> 1:12:19.080\n inventing it once is one over a million, then probability of inventing it twice\n\n1:12:19.080 --> 1:12:22.200\n or three times would be one over a million square or, or to the power of\n\n1:12:22.200 --> 1:12:24.800\n three, which, which would be just impossible.\n\n1:12:25.040 --> 1:12:30.280\n So it literally means that it's, it's given that, uh, it's not the luck.\n\n1:12:30.680 --> 1:12:30.920\n Yeah.\n\n1:12:30.920 --> 1:12:36.680\n And Ilya is one of these few people who, uh, uh, who have, uh, a lot of\n\n1:12:36.680 --> 1:12:38.400\n these inventions in his arsenal.\n\n1:12:38.640 --> 1:12:42.800\n It also feels that, um, you know, for instance, if you think about folks\n\n1:12:42.800 --> 1:12:49.760\n like Gauss or Euler, uh, you know, at first they read a lot of books and then\n\n1:12:49.760 --> 1:12:55.280\n they did thinking and then they figure out math and that's how it feels with\n\n1:12:55.280 --> 1:13:00.320\n Ilya, you know, at first he read stuff and then like he spent his thinking cycles.\n\n1:13:01.000 --> 1:13:04.120\n And that's a really good way to put it.\n\n1:13:05.680 --> 1:13:11.320\n When I talk to him, I, I see thinking.\n\n1:13:11.320 --> 1:13:15.960\n He's actually thinking, like, he makes me realize that there's like deep\n\n1:13:15.960 --> 1:13:17.800\n thinking that the human mind can do.\n\n1:13:18.280 --> 1:13:20.480\n Like most of us are not thinking deeply.\n\n1:13:21.440 --> 1:13:24.560\n Uh, like you really have to put in a lot of effort to think deeply.\n\n1:13:24.760 --> 1:13:29.040\n Like I have to really put myself in a place where I think deeply about a\n\n1:13:29.040 --> 1:13:30.640\n problem, it takes a lot of effort.\n\n1:13:30.960 --> 1:13:33.680\n It's like, uh, it's like an airplane taking off or something.\n\n1:13:33.680 --> 1:13:35.320\n You have to achieve deep focus.\n\n1:13:35.640 --> 1:13:38.560\n He he's just, uh, he's what is it?\n\n1:13:38.560 --> 1:13:43.600\n He said, what does it, his brain is like a vertical takeoff in\n\n1:13:43.600 --> 1:13:45.080\n terms of airplane analogy.\n\n1:13:45.320 --> 1:13:49.520\n So it's interesting, but it, I mean, Cal Newport talks about\n\n1:13:49.520 --> 1:13:51.080\n this as ideas of deep work.\n\n1:13:51.880 --> 1:13:57.400\n It's, you know, most of us don't work much at all in terms of like, like deeply\n\n1:13:57.400 --> 1:14:01.400\n think about particular problems, whether it's a math engineering, all that kind\n\n1:14:01.400 --> 1:14:06.480\n of stuff, you want to go to that place often and that's real hard work.\n\n1:14:06.480 --> 1:14:08.520\n And some of us are better than others at that.\n\n1:14:08.760 --> 1:14:13.040\n So I think that the big piece has to do with actually even engineering\n\n1:14:13.040 --> 1:14:15.640\n your environment that says that it's conducive to that.\n\n1:14:15.840 --> 1:14:16.040\n Yeah.\n\n1:14:16.040 --> 1:14:22.480\n So, um, see both Ilya and I, uh, on the frequent basis, we kind of disconnect\n\n1:14:22.480 --> 1:14:26.760\n ourselves from the world in order to be able to do extensive amount of thinking.\n\n1:14:26.920 --> 1:14:27.200\n Yes.\n\n1:14:27.480 --> 1:14:33.320\n So Ilya usually, he just, uh, leaves iPad at hand.\n\n1:14:33.400 --> 1:14:34.400\n He loves his iPad.\n\n1:14:34.400 --> 1:14:39.320\n And, uh, for me, I'm even sometimes, you know, just going for a few days\n\n1:14:39.320 --> 1:14:44.520\n to different location to Airbnb, I'm turning off my phone and there is no\n\n1:14:44.520 --> 1:14:51.040\n access to me and, uh, that's extremely important for me to be able to actually\n\n1:14:51.040 --> 1:14:55.040\n just formulate new thoughts, to do deep work rather than to be reactive.\n\n1:14:55.400 --> 1:15:00.440\n And the, the, the older I am, the more of these random tasks are at hand.\n\n1:15:00.440 --> 1:15:05.600\n Before I go on to that, uh, thread, let me return to our friend, GPT.\n\n1:15:06.400 --> 1:15:08.720\n And let me ask you another ridiculously big question.\n\n1:15:09.440 --> 1:15:13.840\n Can you give an overview of what GPT three is, or like you say in\n\n1:15:13.840 --> 1:15:20.320\n your Twitter bio, GPT N plus one, how it works and why it works.\n\n1:15:21.120 --> 1:15:25.640\n So, um, GPT three is a humongous neural network.\n\n1:15:25.640 --> 1:15:30.760\n Um, let's assume that we know what is neural network, the definition, and it\n\n1:15:30.760 --> 1:15:36.000\n is trained on the entire internet and just to predict next word.\n\n1:15:36.000 --> 1:15:41.400\n So let's say it sees part of the, uh, article and it, the only task that it\n\n1:15:41.400 --> 1:15:45.680\n has at hand, it is to say what would be the next word and what would be the next\n\n1:15:45.680 --> 1:15:51.800\n word and it becomes a really exceptional at the task of figuring out what's the\n\n1:15:51.800 --> 1:15:57.640\n next word. So you might ask, why would, uh, this be an important, uh, task?\n\n1:15:57.640 --> 1:16:00.560\n Why would it be important to predict what's the next word?\n\n1:16:01.280 --> 1:16:07.920\n And it turns out that a lot of problems, uh, can be formulated, uh, as a text\n\n1:16:07.920 --> 1:16:08.840\n completion problem.\n\n1:16:08.840 --> 1:16:12.560\n So GPT is purely, uh, learning to complete the text.\n\n1:16:13.120 --> 1:16:17.240\n And you could imagine, for instance, if you are asking a question, uh, who is\n\n1:16:17.240 --> 1:16:21.840\n the president of the United States, then GPT can give you an answer to it.\n\n1:16:22.160 --> 1:16:25.720\n It turns out that many more things can be formulated this way.\n\n1:16:25.720 --> 1:16:29.880\n You can format text in the way that you have sentence in English.\n\n1:16:30.920 --> 1:16:35.600\n You make it even look like some content of a website, uh, elsewhere, which would\n\n1:16:35.600 --> 1:16:38.440\n be teaching people how to translate things between languages.\n\n1:16:38.440 --> 1:16:43.720\n So it would be EN colon, uh, text in English, FR colon, and then you'll\n\n1:16:43.720 --> 1:16:48.560\n uh, uh, and then you'll ask people and then you ask model to, to continue.\n\n1:16:48.560 --> 1:16:52.720\n And it turns out that the, such a model is predicting translation from English\n\n1:16:52.720 --> 1:16:53.240\n to French.\n\n1:16:53.640 --> 1:17:00.840\n The crazy thing is that this model can be used for way more sophisticated tasks.\n\n1:17:00.840 --> 1:17:05.240\n So you can format text such that it looks like a conversation between two people.\n\n1:17:05.640 --> 1:17:08.920\n And that might be a conversation between you and Elon Musk.\n\n1:17:08.920 --> 1:17:13.960\n And because the model read all the texts about Elon Musk, it will be able to\n\n1:17:13.960 --> 1:17:16.480\n predict Elon Musk words as it would be Elon Musk.\n\n1:17:16.480 --> 1:17:22.160\n It will speak about colonization of Mars, about sustainable future and so on.\n\n1:17:22.560 --> 1:17:29.200\n And it's also possible to, to even give arbitrary personality to the model.\n\n1:17:29.200 --> 1:17:32.640\n You can say, here is a conversation that we've a friendly AI bot.\n\n1:17:32.640 --> 1:17:37.480\n And the model, uh, will complete the text as a friendly AI bot.\n\n1:17:37.520 --> 1:17:43.680\n So, I mean, how do I express how amazing this is?\n\n1:17:43.920 --> 1:17:49.760\n So just to clarify, uh, a conversation, generating a conversation between me and\n\n1:17:49.760 --> 1:17:56.320\n Elon Musk, it wouldn't just generate good examples of what Elon would say.\n\n1:17:56.800 --> 1:18:01.080\n It would get the same results as the conversation between Elon Musk and me.\n\n1:18:01.080 --> 1:18:03.960\n Say it would get the syntax all correct.\n\n1:18:04.200 --> 1:18:09.000\n So like interview style, it would say like Elon call and Lex call, like it,\n\n1:18:09.280 --> 1:18:17.240\n it's not just like, uh, inklings of, um, semantic correctness.\n\n1:18:17.720 --> 1:18:25.520\n It's like the whole thing, grammatical, syntactic, semantic, it's just really,\n\n1:18:25.520 --> 1:18:29.080\n really impressive, uh, generalization.\n\n1:18:30.000 --> 1:18:30.280\n Yeah.\n\n1:18:30.280 --> 1:18:34.680\n I mean, I also want to, you know, provide some caveats so it can generate\n\n1:18:34.680 --> 1:18:38.880\n few paragraphs of coherent text, but as you go to, uh, longer pieces,\n\n1:18:38.880 --> 1:18:41.120\n it, uh, it actually goes off the rails.\n\n1:18:41.360 --> 1:18:41.480\n Okay.\n\n1:18:41.480 --> 1:18:45.360\n If you try to write a book, it won't work out this way.\n\n1:18:45.680 --> 1:18:47.840\n What way does it go off the rails, by the way?\n\n1:18:47.840 --> 1:18:50.320\n Is there interesting ways in which it goes off the rails?\n\n1:18:50.560 --> 1:18:53.160\n Like what falls apart first?\n\n1:18:54.040 --> 1:18:58.720\n So the model is trained on the, all the existing data, uh, that is out there,\n\n1:18:58.720 --> 1:19:01.920\n which means that it is not trained on its own mistakes.\n\n1:19:02.040 --> 1:19:06.160\n So for instance, if it would make a mistake, then, uh, I kept,\n\n1:19:06.360 --> 1:19:07.960\n so to give you, give you an example.\n\n1:19:08.160 --> 1:19:13.880\n So let's say I have a conversation with a model pretending that is Elon Musk.\n\n1:19:14.360 --> 1:19:18.960\n And then I start putting some, uh, I'm start actually making up\n\n1:19:19.000 --> 1:19:20.320\n things which are not factual.\n\n1:19:21.360 --> 1:19:25.680\n Um, I would say like Twitter, but I got you.\n\n1:19:25.680 --> 1:19:26.040\n Sorry.\n\n1:19:26.120 --> 1:19:26.440\n Yeah.\n\n1:19:26.440 --> 1:19:28.960\n Um, like, uh, I don't know.\n\n1:19:28.960 --> 1:19:35.440\n I would say that Elon is my wife and the model will just keep on carrying it on.\n\n1:19:35.440 --> 1:19:36.560\n And as if it's true.\n\n1:19:37.120 --> 1:19:37.600\n Yes.\n\n1:19:38.000 --> 1:19:41.680\n And in some sense, if you would have a normal conversation with Elon,\n\n1:19:41.720 --> 1:19:42.720\n he would be what the fuck.\n\n1:19:43.160 --> 1:19:43.600\n Yeah.\n\n1:19:43.760 --> 1:19:48.480\n There'll be some feedback between, so the model is trained on things\n\n1:19:48.480 --> 1:19:52.280\n that humans have written, but through the generation process, there's\n\n1:19:52.280 --> 1:19:54.000\n no human in the loop feedback.\n\n1:19:54.200 --> 1:19:54.600\n Correct.\n\n1:19:55.360 --> 1:19:56.240\n That's fascinating.\n\n1:19:56.240 --> 1:19:56.920\n Makes sense.\n\n1:19:57.000 --> 1:19:57.960\n So it's magnified.\n\n1:19:57.960 --> 1:20:03.160\n It's like the errors get magnified and magnified and it's also interesting.\n\n1:20:04.880 --> 1:20:06.760\n I mean, first of all, humans have the same problem.\n\n1:20:06.760 --> 1:20:13.600\n It's just that we, uh, we'll make fewer errors and magnify the errors slower.\n\n1:20:13.960 --> 1:20:17.400\n I think that actually what happens with humans is if you have a wrong\n\n1:20:17.400 --> 1:20:21.720\n belief about the world as a kid, then very quickly we'll learn that it's\n\n1:20:21.720 --> 1:20:25.320\n not correct because they are grounded in reality and they are learning\n\n1:20:25.320 --> 1:20:26.400\n from your new experience.\n\n1:20:26.400 --> 1:20:26.680\n Yes.\n\n1:20:27.520 --> 1:20:30.240\n But do you think the model can correct itself too?\n\n1:20:30.960 --> 1:20:34.040\n Won't it through the power of the representation.\n\n1:20:34.840 --> 1:20:40.560\n And so the absence of Elon Musk being your wife information on the\n\n1:20:40.560 --> 1:20:42.880\n internet, won't it correct itself?\n\n1:20:43.720 --> 1:20:45.280\n There won't be examples like that.\n\n1:20:45.760 --> 1:20:47.960\n So the errors will be subtle at first.\n\n1:20:48.320 --> 1:20:49.200\n Subtle at first.\n\n1:20:49.200 --> 1:20:54.440\n And in some sense, you can also say that the data that is not out there is\n\n1:20:54.440 --> 1:21:00.400\n the data, which would represent how the human learns and maybe model would\n\n1:21:00.400 --> 1:21:01.800\n be learned, trained on such a data.\n\n1:21:01.800 --> 1:21:03.120\n Then it would be better off.\n\n1:21:03.480 --> 1:21:06.440\n How intelligent is GPT3 do you think?\n\n1:21:06.480 --> 1:21:10.080\n Like when you think about the nature of intelligence, it\n\n1:21:10.080 --> 1:21:12.320\n seems exceptionally impressive.\n\n1:21:14.440 --> 1:21:18.040\n But then if you think about the big AGI problem, is this\n\n1:21:18.040 --> 1:21:20.120\n footsteps along the way to AGI?\n\n1:21:20.120 --> 1:21:25.480\n So let's see, it seems that intelligence itself is, there are multiple axis of it.\n\n1:21:25.920 --> 1:21:33.240\n And I would expect that the systems that we are building, they might end up being\n\n1:21:33.280 --> 1:21:37.360\n superhuman on some axis and subhuman on some other axis.\n\n1:21:37.360 --> 1:21:41.400\n It would be surprising to me on all axis simultaneously, they would become superhuman.\n\n1:21:43.040 --> 1:21:48.560\n Of course, people ask this question, is GPT a spaceship that would take us to\n\n1:21:48.560 --> 1:21:52.360\n the moon or are we putting a, building a ladder to heaven that we are just\n\n1:21:52.360 --> 1:21:53.960\n building bigger and bigger ladder.\n\n1:21:54.520 --> 1:21:58.880\n And we don't know in some sense, which one of these two.\n\n1:21:59.080 --> 1:21:59.840\n Which one is better?\n\n1:22:02.240 --> 1:22:04.120\n I'm trying to, I like stairway to heaven.\n\n1:22:04.120 --> 1:22:04.840\n It's a good song.\n\n1:22:04.840 --> 1:22:08.120\n So I'm not exactly sure which one is better, but you're saying like the\n\n1:22:08.120 --> 1:22:10.040\n spaceship to the moon is actually effective.\n\n1:22:10.680 --> 1:22:11.080\n Correct.\n\n1:22:11.080 --> 1:22:17.960\n So people who criticize GPT, they say, you guys just building a\n\n1:22:17.960 --> 1:22:20.960\n taller, a ladder, and it will never reach the moon.\n\n1:22:22.320 --> 1:22:28.080\n And at the moment, I would say the way I'm thinking is, is like a scientific question.\n\n1:22:28.480 --> 1:22:35.040\n And I'm also in heart, I'm a builder creator and like, I'm thinking, let's try out, let's\n\n1:22:35.040 --> 1:22:36.200\n see how far it goes.\n\n1:22:36.840 --> 1:22:40.480\n And so far we see constantly that there is a progress.\n\n1:22:40.800 --> 1:22:41.320\n Yeah.\n\n1:22:41.320 --> 1:22:52.320\n So do you think GPT four, GPT five, GPT N plus one will, um, there'll be a phase\n\n1:22:52.320 --> 1:22:56.760\n shift, like a transition to a, to a place where we'll be truly surprised.\n\n1:22:56.960 --> 1:23:00.360\n Then again, like GPT three is already very like truly surprising.\n\n1:23:00.880 --> 1:23:04.600\n The people that criticize GPT three as a stair, as a, what is it?\n\n1:23:04.600 --> 1:23:05.560\n Ladder to heaven.\n\n1:23:06.240 --> 1:23:09.880\n I think too quickly get accustomed to how impressive it is that they're\n\n1:23:09.880 --> 1:23:15.080\n impressive, it is that the prediction of the next word can achieve such depth of\n\n1:23:15.080 --> 1:23:19.640\n semantics, accuracy of syntax, grammar, and semantics.\n\n1:23:20.680 --> 1:23:26.720\n Um, do you, do you think GPT four and five and six will continue to surprise us?\n\n1:23:28.120 --> 1:23:31.320\n I mean, definitely there will be more impressive models that there is a\n\n1:23:31.320 --> 1:23:38.560\n question of course, if there will be a phase shift and, uh, the, also even the\n\n1:23:38.560 --> 1:23:42.880\n way I'm thinking about the, about these models is that when we build these\n\n1:23:42.880 --> 1:23:47.560\n models, you know, we see some level of the capabilities, but we don't even fully\n\n1:23:47.560 --> 1:23:50.000\n understand everything that the model can do.\n\n1:23:50.280 --> 1:23:55.880\n And actually one of the best things to do is to allow other people to probe the\n\n1:23:55.880 --> 1:23:57.560\n model to even see what is possible.\n\n1:23:58.880 --> 1:24:05.240\n Hence the, the using GPT as an API and opening it up to the world.\n\n1:24:05.320 --> 1:24:05.600\n Yeah.\n\n1:24:05.600 --> 1:24:10.680\n I mean, so when I'm thinking from perspective of like, uh, obviously\n\n1:24:10.680 --> 1:24:14.280\n various people are, that have concerns about AGI, including myself.\n\n1:24:14.840 --> 1:24:18.920\n Um, and then when I'm thinking from perspective, what's the strategy even to\n\n1:24:18.960 --> 1:24:23.880\n deploy these things to the world, then the one strategy that I have seen many\n\n1:24:23.880 --> 1:24:29.360\n times working is that iterative deployment that you deploy, um, slightly\n\n1:24:29.360 --> 1:24:32.520\n better versions and you allow other people to criticize you.\n\n1:24:32.520 --> 1:24:36.720\n So you actually, or try it out, you see where are their fundamental issues.\n\n1:24:37.200 --> 1:24:42.280\n And it's almost, you don't want to be in that situation that you are holding\n\n1:24:42.320 --> 1:24:48.320\n into powerful system and there's like a huge overhang, then you deploy it and it\n\n1:24:48.320 --> 1:24:50.960\n might have a random chaotic impact on the world.\n\n1:24:50.960 --> 1:24:53.800\n So you actually want to be in the situation that they are\n\n1:24:53.800 --> 1:24:55.520\n gradually deploying systems.\n\n1:24:56.560 --> 1:25:00.680\n I asked this question of Illya, let me ask you, uh, you this question.\n\n1:25:00.680 --> 1:25:04.680\n I've been reading a lot about Stalin and power.\n\n1:25:09.360 --> 1:25:14.480\n If you're in possession of a system that's like AGI, that's exceptionally\n\n1:25:14.480 --> 1:25:20.640\n powerful, do you think your character and integrity might become corrupted?\n\n1:25:21.040 --> 1:25:23.920\n Like famously power corrupts and absolute power corrupts.\n\n1:25:23.920 --> 1:25:24.440\n Absolutely.\n\n1:25:24.440 --> 1:25:30.640\n So I believe that the, you want at some point to work toward distributing the power.\n\n1:25:31.440 --> 1:25:36.360\n I think that the, you want to be in the situation that actually AGI is not\n\n1:25:36.360 --> 1:25:42.680\n controlled by a small number of people, uh, but, uh, essentially, uh, by a larger\n\n1:25:42.680 --> 1:25:43.200\n collective.\n\n1:25:43.560 --> 1:25:50.360\n So the thing is that requires a George Washington style move in the ascent to\n\n1:25:50.360 --> 1:25:55.280\n power, there's always a moment when somebody gets a lot of power and they\n\n1:25:55.280 --> 1:26:01.040\n have to have the integrity and, uh, the moral compass to give away that power.\n\n1:26:01.920 --> 1:26:06.480\n That humans have been good and bad throughout history at this particular\n\n1:26:06.480 --> 1:26:07.000\n step.\n\n1:26:07.400 --> 1:26:13.120\n And I wonder, I wonder we like blind ourselves in a, for example, between\n\n1:26:13.120 --> 1:26:20.440\n nations, a race, uh, towards, um, they, yeah, AI race between nations, we might\n\n1:26:20.440 --> 1:26:25.240\n blind ourselves and justify to ourselves the development of AI without distributing\n\n1:26:25.240 --> 1:26:29.440\n the power because we want to defend ourselves against China, against Russia,\n\n1:26:29.920 --> 1:26:31.480\n that kind of, that kind of logic.\n\n1:26:32.360 --> 1:26:40.160\n And, um, I wonder how we, um, how we design governance mechanisms that, um,\n\n1:26:40.160 --> 1:26:45.360\n prevent us from becoming power hungry and in the process, destroying ourselves.\n\n1:26:46.280 --> 1:26:50.600\n So let's see, I have been thinking about this topic quite a bit, but I also want\n\n1:26:50.600 --> 1:26:55.800\n to admit that, uh, once again, I actually want to rely way more on Sam Altman on it.\n\n1:26:55.840 --> 1:27:01.280\n He wrote an excellent blog on how even to distribute wealth.\n\n1:27:01.280 --> 1:27:08.720\n Um, and he's proper, he proposed in his blog, uh, to tax, uh, equity of the companies\n\n1:27:08.720 --> 1:27:10.680\n rather than profit and to distribute it.\n\n1:27:11.000 --> 1:27:15.680\n And this is, this is an example of, uh, Washington move.\n\n1:27:17.680 --> 1:27:24.040\n I guess I personally have insane trust in some here already spent plenty of money\n\n1:27:24.320 --> 1:27:28.360\n running, uh, universal basic income, uh, project.\n\n1:27:28.360 --> 1:27:33.280\n That like, uh, gives me, I guess, maybe some level of trust to him, but I also,\n\n1:27:34.480 --> 1:27:37.480\n I guess love him as a friend.\n\n1:27:37.720 --> 1:27:38.220\n Yeah.\n\n1:27:38.920 --> 1:27:43.120\n I wonder because we're sort of summoning a new set of technologies.\n\n1:27:44.280 --> 1:27:50.280\n I wonder if we'll be, um, cognizant, like you're describing the process of open AI,\n\n1:27:50.680 --> 1:27:54.360\n but it could also be at other places like in the U S government, right?\n\n1:27:54.360 --> 1:28:00.680\n Uh, both China and the U S are now full steam ahead on autonomous\n\n1:28:00.680 --> 1:28:02.160\n weapons systems development.\n\n1:28:03.200 --> 1:28:09.680\n And that's really worrying to me because in the framework of something being a\n\n1:28:09.680 --> 1:28:14.880\n national security danger or military danger, you can do a lot of pretty dark\n\n1:28:14.880 --> 1:28:18.720\n things that blind our moral compass.\n\n1:28:18.720 --> 1:28:23.920\n And I think AI will be one of those things, um, in some sense, the, the mission\n\n1:28:24.320 --> 1:28:28.440\n and the work you're doing in open AI is like the counterbalance to that.\n\n1:28:28.840 --> 1:28:32.760\n So you want to have more open AI and less autonomous weapons systems.\n\n1:28:33.200 --> 1:28:37.200\n I, I, I, I like these statements, like to be clear, like this interesting and I'm\n\n1:28:37.200 --> 1:28:43.760\n thinking about it myself, but, uh, this is a place that I, I, I put my trust\n\n1:28:43.760 --> 1:28:48.440\n actually in Sam's hands, because it's extremely hard for me to reason about it.\n\n1:28:48.760 --> 1:28:49.080\n Yeah.\n\n1:28:49.200 --> 1:28:54.200\n I mean, one important statement to make is, um, it's good to think about this.\n\n1:28:54.280 --> 1:28:54.600\n Yeah.\n\n1:28:54.640 --> 1:28:55.520\n No question about it.\n\n1:28:55.520 --> 1:29:02.680\n No question, even like low level quote unquote engineer, like there's such a,\n\n1:29:02.680 --> 1:29:10.080\n um, I remember I, I programmed a car, uh, our RC car, um, and it was, it was\n\n1:29:10.080 --> 1:29:17.760\n programmed a car, uh, our RC car, they went really fast, like 30, 40 miles an hour.\n\n1:29:18.480 --> 1:29:21.040\n And I remember I was like sleep deprived.\n\n1:29:21.080 --> 1:29:26.440\n So I programmed it pretty crappily and it like, uh, the, the, the code froze.\n\n1:29:26.440 --> 1:29:30.040\n So it's doing some basic computer vision and it's going around on track,\n\n1:29:30.280 --> 1:29:31.600\n but it's going full speed.\n\n1:29:32.640 --> 1:29:39.280\n And, uh, there was a bug in the code that, uh, the car just went, it didn't turn.\n\n1:29:39.280 --> 1:29:42.040\n Went straight full speed and smash into the wall.\n\n1:29:42.520 --> 1:29:49.480\n And I remember thinking the seriousness with which you need to approach the\n\n1:29:49.480 --> 1:29:53.240\n design of artificial intelligence systems and the programming of artificial\n\n1:29:53.240 --> 1:29:58.520\n intelligence systems is high because the consequences are high, like that\n\n1:29:58.520 --> 1:30:00.320\n little car smashing into the wall.\n\n1:30:00.880 --> 1:30:04.480\n For some reason, I immediately thought of like an algorithm that controls\n\n1:30:04.480 --> 1:30:07.160\n nuclear weapons, having the same kind of bug.\n\n1:30:07.160 --> 1:30:11.840\n And so like the lowest level engineer and the CEO of a company all need to\n\n1:30:11.840 --> 1:30:15.240\n have the seriousness, uh, in approaching this problem and thinking\n\n1:30:15.240 --> 1:30:16.840\n about the worst case consequences.\n\n1:30:17.000 --> 1:30:18.560\n So I think that is true.\n\n1:30:18.800 --> 1:30:24.840\n I mean, the, what I also recognize in myself and others even asking this\n\n1:30:24.840 --> 1:30:29.680\n question is that it evokes a lot of fear and fear itself ends up being\n\n1:30:29.680 --> 1:30:31.400\n actually quite debilitating.\n\n1:30:31.400 --> 1:30:38.680\n The place where I arrived at the moment might sound cheesy or so, but it's\n\n1:30:38.680 --> 1:30:48.680\n almost to build things out of love rather than fear, like a focus on how, uh, I can,\n\n1:30:48.720 --> 1:30:54.000\n you know, maximize the value, how the systems that I'm building might be, uh,\n\n1:30:54.280 --> 1:30:54.800\n useful.\n\n1:30:55.800 --> 1:31:00.400\n I'm not saying that the fear doesn't exist out there and like it totally\n\n1:31:00.400 --> 1:31:04.920\n makes sense to minimize it, but I don't want to be working because, uh, I'm\n\n1:31:04.920 --> 1:31:10.320\n scared, I want to be working out of passion, out of curiosity, out of the,\n\n1:31:10.640 --> 1:31:13.120\n you know, uh, looking forward for the positive future.\n\n1:31:13.840 --> 1:31:19.480\n With, uh, the definition of love arising from a rigorous practice of empathy.\n\n1:31:19.800 --> 1:31:23.560\n So not just like your own conception of what is good for the world, but\n\n1:31:23.600 --> 1:31:24.840\n always listening to others.\n\n1:31:25.160 --> 1:31:25.560\n Correct.\n\n1:31:25.560 --> 1:31:29.160\n Like the love where I'm considering reward functions of others.\n\n1:31:29.160 --> 1:31:35.280\n Others to limit to infinity is like a sum of like one to N where N is, uh,\n\n1:31:35.280 --> 1:31:36.680\n 7 billion or whatever it is.\n\n1:31:36.680 --> 1:31:38.880\n Not, not projecting my reward functions on others.\n\n1:31:38.920 --> 1:31:39.720\n Yeah, exactly.\n\n1:31:40.440 --> 1:31:40.920\n Okay.\n\n1:31:41.360 --> 1:31:43.760\n Can we just take a step back to something else?\n\n1:31:43.760 --> 1:31:46.240\n Super cool, which is, uh, OpenAI Codex.\n\n1:31:47.240 --> 1:31:53.200\n Can you give an overview of what OpenAI Codex and GitHub Copilot is, how it works\n\n1:31:53.680 --> 1:31:55.280\n and why the hell it works so well?\n\n1:31:55.280 --> 1:32:00.960\n So with GPT tree, we noticed that the system, uh, you know, that system train\n\n1:32:00.960 --> 1:32:05.400\n on all the language out there started having some rudimentary coding capabilities.\n\n1:32:05.440 --> 1:32:10.880\n So we're able to ask it, you know, to implement addition function between\n\n1:32:10.880 --> 1:32:14.680\n two numbers and indeed it can write item or JavaScript code for that.\n\n1:32:15.320 --> 1:32:20.520\n And then we thought, uh, we might as well just go full steam ahead and try to\n\n1:32:20.520 --> 1:32:25.320\n create a system that is actually good at what we are doing every day ourselves,\n\n1:32:25.800 --> 1:32:26.680\n which is programming.\n\n1:32:27.320 --> 1:32:31.440\n We optimize models for proficiency in coding.\n\n1:32:31.600 --> 1:32:38.040\n We actually even created models that both have a comprehension of language and code.\n\n1:32:38.840 --> 1:32:42.200\n And Codex is API for these models.\n\n1:32:42.600 --> 1:32:48.840\n So it's first pre trained on language and then codex.\n\n1:32:48.840 --> 1:32:54.080\n Then I don't know if you can say fine tuned because there's a lot of code,\n\n1:32:54.600 --> 1:32:56.320\n but it's language and code.\n\n1:32:56.400 --> 1:32:57.320\n It's language and code.\n\n1:32:58.320 --> 1:33:00.200\n It's also optimized for various things.\n\n1:33:00.200 --> 1:33:01.960\n I can, let's say low latency and so on.\n\n1:33:02.600 --> 1:33:05.520\n Codex is the API, the similar to GPT tree.\n\n1:33:06.000 --> 1:33:10.560\n We expect that there will be proliferation of the potential products that can use\n\n1:33:10.560 --> 1:33:14.640\n coding capabilities and I can, I can speak about it in a second.\n\n1:33:14.920 --> 1:33:18.200\n Copilot is a first product and developed by GitHub.\n\n1:33:18.200 --> 1:33:22.000\n So as we're building, uh, models, we wanted to make sure that these\n\n1:33:22.000 --> 1:33:26.840\n models are useful and we work together with GitHub on building the first product.\n\n1:33:27.320 --> 1:33:32.040\n Copilot is actually, as you code, it suggests you code completions.\n\n1:33:32.240 --> 1:33:36.520\n And we have seen in the past, there are like a various tools that can suggest\n\n1:33:36.760 --> 1:33:40.600\n how to like a few characters of the code or a line of code.\n\n1:33:41.000 --> 1:33:44.600\n Then the thing about Copilot is it can generate 10 lines of code.\n\n1:33:44.600 --> 1:33:49.160\n You, it's often the way how it works is you often write in the comment\n\n1:33:49.480 --> 1:33:53.720\n what you want to happen because people in comments, they describe what happens next.\n\n1:33:53.960 --> 1:34:00.200\n So, um, these days when I code, instead of going to Google to search, uh, for\n\n1:34:00.200 --> 1:34:06.200\n the appropriate code to solve my problem, I say, Oh, for this area, could you\n\n1:34:06.200 --> 1:34:10.520\n smooth it and then, you know, it imports some appropriate libraries and say it\n\n1:34:10.520 --> 1:34:15.000\n uses NumPy convolution or so I, that I was not even aware that exists and\n\n1:34:15.000 --> 1:34:16.240\n it does the appropriate thing.\n\n1:34:16.840 --> 1:34:21.440\n Um, so you, uh, you write a comment, maybe the header of a function\n\n1:34:21.440 --> 1:34:22.680\n and it completes the function.\n\n1:34:23.320 --> 1:34:27.200\n Of course, you don't know what is the space of all the possible small\n\n1:34:27.200 --> 1:34:28.440\n programs that can generate.\n\n1:34:28.840 --> 1:34:30.360\n What are the failure cases?\n\n1:34:30.360 --> 1:34:34.880\n How many edge cases, how many subtle errors there are, how many big errors\n\n1:34:34.880 --> 1:34:38.840\n there are, it's hard to know, but the fact that it works at all in a large\n\n1:34:38.840 --> 1:34:40.680\n number of cases is incredible.\n\n1:34:41.000 --> 1:34:45.920\n It's like, uh, it's a kind of search engine into code that's\n\n1:34:45.920 --> 1:34:47.120\n been written on the internet.\n\n1:34:47.720 --> 1:34:48.120\n Correct.\n\n1:34:48.120 --> 1:34:53.240\n So for instance, when you search things online, then usually you get to the,\n\n1:34:53.720 --> 1:34:58.920\n some particular case, like if you go to stack overflow and people describe\n\n1:34:58.920 --> 1:35:03.040\n that one particular situation, uh, and then they seek for a solution.\n\n1:35:03.040 --> 1:35:08.040\n But in case of a copilot, it's aware of your entire context and in\n\n1:35:08.040 --> 1:35:10.320\n context is, Oh, these are the libraries that they are using.\n\n1:35:10.480 --> 1:35:13.640\n That's the set of the variables that is initialized.\n\n1:35:14.120 --> 1:35:16.520\n And on the spot, it can actually tell you what to do.\n\n1:35:17.280 --> 1:35:21.280\n So the interesting thing is, and we think that the copilot is one\n\n1:35:21.280 --> 1:35:25.080\n possible product using codecs, but there is a place for many more.\n\n1:35:25.080 --> 1:35:29.480\n So internally we tried out, you know, to create other fun products.\n\n1:35:29.760 --> 1:35:33.880\n So it turns out that a lot of tools out there, let's say Google\n\n1:35:33.880 --> 1:35:38.480\n calendar or Microsoft word or so, they all have a internal API\n\n1:35:38.480 --> 1:35:40.480\n to build plugins around them.\n\n1:35:41.240 --> 1:35:47.000\n So there is a way in the sophisticated way to control calendar or Microsoft word.\n\n1:35:47.520 --> 1:35:51.160\n Today, if you want, if you want more complicated behaviors from these\n\n1:35:51.160 --> 1:35:54.000\n programs, you have to add the new button for every behavior.\n\n1:35:55.040 --> 1:36:00.440\n But it is possible to use codecs and tell for instance, to calendar, uh,\n\n1:36:00.440 --> 1:36:06.200\n could you schedule an appointment with Lex next week after 2 PM and it\n\n1:36:06.200 --> 1:36:07.800\n writes corresponding piece of code.\n\n1:36:08.920 --> 1:36:10.760\n And that's the thing that actually you want.\n\n1:36:10.800 --> 1:36:11.440\n So interesting.\n\n1:36:11.440 --> 1:36:15.000\n So you figure out is there's a lot of programs with which\n\n1:36:15.000 --> 1:36:16.440\n you can interact through code.\n\n1:36:17.080 --> 1:36:21.960\n And so there you can generate that code from natural language.\n\n1:36:22.480 --> 1:36:23.400\n That's fascinating.\n\n1:36:23.440 --> 1:36:28.880\n And that's somewhat like also closest to what was the promise of Siri or Alexa.\n\n1:36:28.880 --> 1:36:33.680\n So previously all these behaviors, they were hard coded and it seems\n\n1:36:33.680 --> 1:36:39.000\n that codecs on the fly can pick up the API of let's say, given software.\n\n1:36:39.360 --> 1:36:42.320\n And then it can turn language into use of this API.\n\n1:36:42.320 --> 1:36:46.560\n So without hard coding, you can find, it can translate to machine language.\n\n1:36:46.640 --> 1:36:47.040\n Correct.\n\n1:36:47.040 --> 1:36:51.840\n To, uh, so for example, this would be really exciting for me, like for, um,\n\n1:36:51.880 --> 1:36:57.320\n Adobe products, like Photoshop, uh, which I think action scripted, I think\n\n1:36:57.320 --> 1:37:00.080\n there's a scripting language that communicates with them, same with Premier.\n\n1:37:00.440 --> 1:37:05.960\n And do you could imagine that that allows even to do coding by voice on your phone?\n\n1:37:06.480 --> 1:37:08.880\n So for instance, in the past, okay.\n\n1:37:09.000 --> 1:37:13.760\n As of today, I'm not editing Word documents on my phone because it's\n\n1:37:13.760 --> 1:37:15.360\n just the keyboard is too small.\n\n1:37:15.480 --> 1:37:20.520\n But if I would be able to tell, uh, to my phone, you know, uh, make the\n\n1:37:20.520 --> 1:37:25.040\n header large, then move the paragraphs around and that's actually what I want.\n\n1:37:25.040 --> 1:37:29.080\n So I can tell you one more cool thing, or even how I'm thinking about codecs.\n\n1:37:29.720 --> 1:37:36.280\n So if you look actually at the evolution of, uh, of computers, we started with\n\n1:37:36.320 --> 1:37:40.160\n a very primitive interfaces, which is a punch card and punch card.\n\n1:37:40.320 --> 1:37:46.280\n So Charlie, you make a holes in the, in the plastic card to indicate zeros and ones.\n\n1:37:47.040 --> 1:37:50.720\n And, uh, during that time, there was a small number of specialists\n\n1:37:50.720 --> 1:37:52.040\n who were able to use computers.\n\n1:37:52.040 --> 1:37:55.000\n And by the way, people even suspected that there is no need for many\n\n1:37:55.000 --> 1:37:56.320\n more people to use computers.\n\n1:37:56.960 --> 1:38:03.880\n Um, but then we moved from punch cards to at first assembly and see, and\n\n1:38:03.920 --> 1:38:07.040\n at these programming languages, they were slightly higher level.\n\n1:38:07.200 --> 1:38:11.920\n They allowed many more people to code and they also, uh, led to more\n\n1:38:11.920 --> 1:38:13.600\n of a proliferation of technology.\n\n1:38:14.040 --> 1:38:19.960\n And, uh, you know, further on, there was a jump to say from C++ to Java and Python.\n\n1:38:19.960 --> 1:38:23.560\n And every time it has happened, more people are able to code\n\n1:38:23.600 --> 1:38:25.840\n and we build more technology.\n\n1:38:26.200 --> 1:38:31.200\n And it's even, you know, hard to imagine now, if someone will tell you that you\n\n1:38:31.200 --> 1:38:36.760\n should write code in assembly instead of let's say, Python or Java or JavaScript.\n\n1:38:37.160 --> 1:38:41.520\n And codecs is yet another step toward kind of bringing computers closer to\n\n1:38:41.520 --> 1:38:47.120\n humans such that you communicate with a computer with your own language rather\n\n1:38:47.120 --> 1:38:52.600\n than with a specialized language, and, uh, I think that it will lead to an\n\n1:38:52.600 --> 1:38:54.600\n increase of number of people who can code.\n\n1:38:55.280 --> 1:38:55.440\n Yeah.\n\n1:38:55.440 --> 1:39:00.160\n And then, and the kind of technologies that those people will create is it's\n\n1:39:00.160 --> 1:39:03.760\n innumerable, it could, you know, it could be a huge number of technologies.\n\n1:39:03.760 --> 1:39:07.560\n We're not predicting at all because that's less and less requirement\n\n1:39:07.600 --> 1:39:13.480\n of having a technical mind, a programming mind, you're not opening it to the world\n\n1:39:13.480 --> 1:39:19.360\n of, um, other kinds of minds, creative minds, artistic minds, all that kind of stuff.\n\n1:39:19.400 --> 1:39:23.720\n I would like, for instance, biologists who work on DNA to be able to program\n\n1:39:23.800 --> 1:39:26.720\n and not to need to spend a lot of time learning it.\n\n1:39:26.720 --> 1:39:28.600\n And I, I believe that's a good thing to the world.\n\n1:39:29.080 --> 1:39:33.720\n And I would actually add, I would add, so at the moment I'm a managing codecs\n\n1:39:33.800 --> 1:39:37.800\n team and also language team, and I believe that there is like a plenty\n\n1:39:37.800 --> 1:39:41.640\n of brilliant people out there and they should have a lot of experience.\n\n1:39:41.640 --> 1:39:43.800\n There and they should apply.\n\n1:39:44.360 --> 1:39:45.080\n Oh, okay.\n\n1:39:45.080 --> 1:39:45.320\n Yeah.\n\n1:39:45.320 --> 1:39:45.720\n Awesome.\n\n1:39:45.880 --> 1:39:48.440\n So what's the language and the codecs is, so those are kind of,\n\n1:39:48.960 --> 1:39:50.760\n they're overlapping teams.\n\n1:39:50.760 --> 1:39:56.640\n It's like GPT, the raw language, and then the codecs is like applied to programming.\n\n1:39:57.120 --> 1:39:57.480\n Correct.\n\n1:39:57.480 --> 1:39:59.280\n And they are quite intertwined.\n\n1:40:00.000 --> 1:40:03.880\n There are many more things involved making this, uh, models,\n\n1:40:03.960 --> 1:40:06.240\n uh, extremely efficient and deployable.\n\n1:40:06.480 --> 1:40:06.600\n Okay.\n\n1:40:06.600 --> 1:40:10.800\n For instance, there are people who are working to, you know, make our data\n\n1:40:10.800 --> 1:40:14.960\n centers, uh, amazing, or there are people who work on putting these\n\n1:40:14.960 --> 1:40:20.120\n models into production or, uh, or even pushing it at the very limit of the scale.\n\n1:40:21.640 --> 1:40:25.240\n So all aspects from, from the infrastructure to the actual machine.\n\n1:40:25.240 --> 1:40:29.640\n So I'm just saying there are multiple teams while the, and the team working\n\n1:40:29.640 --> 1:40:33.440\n on codecs and language, uh, I guess I'm, I'm directly managing them.\n\n1:40:33.560 --> 1:40:37.560\n I would like, I would love to hire more interested in machine learning.\n\n1:40:37.560 --> 1:40:41.960\n This is probably one of the most exciting problems and like systems\n\n1:40:41.960 --> 1:40:45.520\n to be working on is it's actually, it's, it's, it's pretty cool.\n\n1:40:45.560 --> 1:40:48.760\n Like what, what, uh, the program synthesis, like generating a\n\n1:40:48.760 --> 1:40:53.480\n programs is very interesting, very interesting problem that has echoes\n\n1:40:53.480 --> 1:40:56.520\n of reasoning and intelligence in it.\n\n1:40:57.080 --> 1:41:00.520\n It's and I think there's a lot of fundamental questions that you might\n\n1:41:00.520 --> 1:41:05.480\n be able to sneak, uh, sneak up to by generating programs.\n\n1:41:05.480 --> 1:41:09.600\n Yeah, that one more exciting thing about the programs is that, so I said\n\n1:41:09.600 --> 1:41:13.720\n that the, um, you know, the, in case of language, that one of the travels\n\n1:41:13.720 --> 1:41:15.160\n is even evaluating language.\n\n1:41:15.200 --> 1:41:20.600\n So when the things are made up, you, you need somehow either a human to,\n\n1:41:20.840 --> 1:41:25.360\n to say that this doesn't make sense or so in case of program, there is one extra\n\n1:41:25.360 --> 1:41:29.400\n lever that we can actually execute programs and see what they evaluate to.\n\n1:41:29.400 --> 1:41:35.800\n So that process might be somewhat, uh, more automated in, in order to improve\n\n1:41:35.800 --> 1:41:38.040\n the, uh, qualities of generations.\n\n1:41:38.440 --> 1:41:39.160\n Oh, that's fascinating.\n\n1:41:39.160 --> 1:41:41.800\n So like the, wow, that's really interesting.\n\n1:41:42.120 --> 1:41:45.680\n So, so for the language, the, you know, the simulation to actually\n\n1:41:45.680 --> 1:41:47.200\n execute it as a human mind.\n\n1:41:47.440 --> 1:41:47.940\n Yeah.\n\n1:41:48.280 --> 1:41:52.400\n For programs, there is a, there is a computer on which you can evaluate it.\n\n1:41:53.760 --> 1:41:54.720\n Wow.\n\n1:41:54.960 --> 1:41:58.400\n That's a brilliant little insight.\n\n1:41:58.400 --> 1:42:04.640\n Insight that the thing compiles and runs that's first and second, you can evaluate\n\n1:42:04.880 --> 1:42:11.000\n on a, like do automated unit testing and in some sense, it seems to me that we'll\n\n1:42:11.000 --> 1:42:12.920\n be able to make a tremendous progress.\n\n1:42:12.920 --> 1:42:17.320\n You know, we are in the paradigm that there is way more data.\n\n1:42:17.320 --> 1:42:23.440\n There is like a transcription of millions of, uh, of, uh, software engineers.\n\n1:42:23.520 --> 1:42:24.020\n Yeah.\n\n1:42:24.320 --> 1:42:24.820\n Yeah.\n\n1:42:24.820 --> 1:42:29.020\n So, uh, I mean, you just mean, cause I was going to ask you about reliability.\n\n1:42:29.300 --> 1:42:35.260\n The thing about programs is you don't know if they're going to, like a program\n\n1:42:35.260 --> 1:42:38.860\n that's controlling a nuclear power plant has to be very reliable.\n\n1:42:39.140 --> 1:42:43.060\n So I wouldn't start with controlling nuclear power plant maybe one day,\n\n1:42:43.140 --> 1:42:46.380\n but that's not actually, that's not on the current roadmap.\n\n1:42:46.420 --> 1:42:48.060\n That's not the step one.\n\n1:42:48.540 --> 1:42:50.460\n And you know, it's the Russian thing.\n\n1:42:50.460 --> 1:42:53.500\n You just want to go to the most powerful, destructive, most powerful\n\n1:42:53.500 --> 1:42:57.620\n the most powerful, destructive thing right away run by JavaScript.\n\n1:42:57.660 --> 1:42:58.300\n But I got you.\n\n1:42:58.300 --> 1:43:01.020\n So this is a lower impact, but nevertheless, when you make me\n\n1:43:01.020 --> 1:43:05.380\n realize it is possible to achieve some levels of reliability by doing testing.\n\n1:43:06.620 --> 1:43:09.820\n And you could, you could imagine that, you know, maybe there are ways for\n\n1:43:09.820 --> 1:43:15.260\n model to write event code for testing itself and so on, and there exists\n\n1:43:15.340 --> 1:43:19.260\n a ways to create the feedback loops that the model could keep on improving.\n\n1:43:19.260 --> 1:43:25.420\n Yeah. By writing programs that generate tests for the instance, for instance.\n\n1:43:26.940 --> 1:43:30.460\n And that's how we get consciousness, because it's metacompression.\n\n1:43:30.660 --> 1:43:31.540\n That's what you're going to write.\n\n1:43:31.540 --> 1:43:32.460\n That's the comment.\n\n1:43:32.460 --> 1:43:34.380\n That's the prompt that generates consciousness.\n\n1:43:34.900 --> 1:43:36.780\n Compressor of compressors.\n\n1:43:36.780 --> 1:43:37.540\n You just write that.\n\n1:43:38.500 --> 1:43:41.300\n Do you think the code that generates consciousness will be simple?\n\n1:43:42.300 --> 1:43:44.140\n So let's see.\n\n1:43:44.140 --> 1:43:48.060\n I mean, ultimately, the core idea behind will be simple,\n\n1:43:48.060 --> 1:43:53.380\n but there will be also decent amount of engineering involved.\n\n1:43:53.380 --> 1:43:58.580\n Like in some sense, it seems that, you know, spreading these models\n\n1:43:58.580 --> 1:44:01.780\n on many machines, it's not that trivial.\n\n1:44:01.860 --> 1:44:02.260\n Yeah.\n\n1:44:02.260 --> 1:44:07.820\n And we find all sorts of innovations that make our models more efficient.\n\n1:44:08.460 --> 1:44:14.460\n I believe that first models that I guess are conscious or like a truly intelligent,\n\n1:44:14.460 --> 1:44:21.580\n they will have all sorts of tricks, but then again, there's a Richard Sutton\n\n1:44:21.620 --> 1:44:25.780\n argument that maybe the tricks are temporary things that they might be\n\n1:44:25.780 --> 1:44:32.300\n temporary things and in some sense, it's also even important to, to know\n\n1:44:32.300 --> 1:44:33.780\n that even the cost of a trick.\n\n1:44:33.780 --> 1:44:38.220\n So sometimes people are eager to put the trick while forgetting that\n\n1:44:38.220 --> 1:44:43.300\n there is a cost of maintenance or like a long term cost, long term cost\n\n1:44:43.300 --> 1:44:48.980\n or maintenance, or maybe even flexibility of code to actually implement new ideas.\n\n1:44:48.980 --> 1:44:52.740\n So even if you have something that gives you 2x, but it requires, you know,\n\n1:44:53.100 --> 1:44:55.860\n 1000 lines of code, I'm not sure if it's actually worth it.\n\n1:44:56.300 --> 1:45:00.980\n So in some sense, you know, if it's five lines of code and 2x, I would take it.\n\n1:45:02.060 --> 1:45:07.620\n And we see many of this, but also, you know, that requires some level of,\n\n1:45:07.620 --> 1:45:12.180\n I guess, lack of attachment to code that we are willing to remove it.\n\n1:45:12.540 --> 1:45:13.020\n Yeah.\n\n1:45:14.620 --> 1:45:17.340\n So you led the OpenAI robotics team.\n\n1:45:17.580 --> 1:45:20.460\n Can you give an overview of the cool things you were able to\n\n1:45:20.460 --> 1:45:21.980\n accomplish, what are you most proud of?\n\n1:45:22.780 --> 1:45:26.060\n So when we started robotics, we knew that actually reinforcement learning works\n\n1:45:26.060 --> 1:45:29.620\n and it is possible to solve fairly complicated problems.\n\n1:45:29.940 --> 1:45:36.020\n Like for instance, AlphaGo is an evidence that it is possible to build superhuman\n\n1:45:36.020 --> 1:45:44.060\n Go players, DOTA2 is an evidence that it's possible to build superhuman agents\n\n1:45:44.060 --> 1:45:48.500\n playing DOTA, so I asked myself a question, you know, what about robots out there?\n\n1:45:48.820 --> 1:45:52.940\n Could we train machines to solve arbitrary tasks in the physical world?\n\n1:45:53.820 --> 1:45:59.620\n Our approach was, I guess, let's pick a complicated problem that if we would\n\n1:45:59.620 --> 1:46:04.260\n solve it, that means that we made some significant progress in the domain.\n\n1:46:04.260 --> 1:46:07.780\n And if can progress the domain, and then we went after the problem.\n\n1:46:08.220 --> 1:46:13.740\n So we noticed that actually the robots out there, they are kind of at the moment\n\n1:46:13.780 --> 1:46:18.420\n optimized per task, so you can have a robot that it's like, if you have a robot\n\n1:46:18.420 --> 1:46:22.460\n opening a bottle, it's very likely that the end factor is that bottle opener.\n\n1:46:24.060 --> 1:46:27.740\n And the, and in some sense, that's a hack to be able to solve a task,\n\n1:46:27.780 --> 1:46:33.180\n which makes any task easier and ask myself, so what would be a robot that\n\n1:46:33.180 --> 1:46:34.540\n can actually solve many tasks?\n\n1:46:35.300 --> 1:46:42.900\n And we conclude that human hands have such a quality that indeed they are, you\n\n1:46:42.900 --> 1:46:48.060\n know, you have five kind of tiny arms attached individually.\n\n1:46:48.060 --> 1:46:51.420\n They can manipulate pretty broad spectrum of objects.\n\n1:46:51.860 --> 1:46:57.140\n So we went after a single hand, like trying to solve Rubik's cube single handed.\n\n1:46:57.420 --> 1:47:01.740\n We picked this task because we thought that there is no way to hard code it.\n\n1:47:01.740 --> 1:47:05.380\n And it's also, we picked the robot on which it would be hard to hard code it.\n\n1:47:05.700 --> 1:47:11.140\n And we went after the solution such that it could generalize to other problems.\n\n1:47:11.180 --> 1:47:15.900\n And just to clarify, it's one robotic hand solving the Rubik's cube.\n\n1:47:16.300 --> 1:47:20.580\n The hard part isn't the solution to the Rubik's cube is the manipulation of the,\n\n1:47:21.180 --> 1:47:27.100\n of like having it not fall out of the hand, having it use the, uh, five baby\n\n1:47:27.100 --> 1:47:32.020\n arms to, uh, what is it like rotate different parts of the Rubik's cube to\n\n1:47:32.020 --> 1:47:32.980\n achieve the solution.\n\n1:47:33.140 --> 1:47:33.540\n Correct.\n\n1:47:33.940 --> 1:47:34.260\n Yeah.\n\n1:47:34.660 --> 1:47:37.740\n So what, uh, what was the hardest part about that?\n\n1:47:38.380 --> 1:47:39.980\n What was the approach taken there?\n\n1:47:40.180 --> 1:47:41.260\n What are you most proud of?\n\n1:47:41.460 --> 1:47:44.300\n Obviously we have like a strong belief in reinforcement learning.\n\n1:47:44.980 --> 1:47:49.660\n And, uh, you know, one path it is to do reinforcement learning, the real\n\n1:47:49.660 --> 1:47:55.860\n world other path is to, uh, uh, that simulation in some sense, the tricky\n\n1:47:55.860 --> 1:47:59.620\n part about the real world is at the moment, our models, they require a lot\n\n1:47:59.620 --> 1:48:01.660\n of data and there is essentially no data.\n\n1:48:02.220 --> 1:48:07.060\n And, uh, I did, we decided to go through the path of the simulation.\n\n1:48:07.060 --> 1:48:09.420\n And in simulation, you can have infinite amount of data.\n\n1:48:09.780 --> 1:48:12.380\n The tricky part is the fidelity of the simulation.\n\n1:48:12.740 --> 1:48:16.780\n And also can you in simulation represent everything that you represent\n\n1:48:16.780 --> 1:48:18.140\n otherwise in the real world.\n\n1:48:18.940 --> 1:48:22.900\n And, you know, it turned out that, uh, that, you know, because there is\n\n1:48:22.900 --> 1:48:29.820\n lack of fidelity, it is possible to what we, what we arrived at is training\n\n1:48:29.820 --> 1:48:33.820\n a model that doesn't solve one simulation, but it actually solves the\n\n1:48:34.180 --> 1:48:39.260\n entire range of simulations, which, uh, uh, in terms of like, uh, what's\n\n1:48:39.260 --> 1:48:45.260\n the, exactly the friction of the cube or the weight or so, and the single AI\n\n1:48:45.260 --> 1:48:48.860\n that can solve all of them ends up working well with the reality.\n\n1:48:49.220 --> 1:48:51.300\n How do you generate the different simulations?\n\n1:48:51.300 --> 1:48:54.220\n So, uh, you know, there's plenty of parameters out there.\n\n1:48:54.260 --> 1:48:55.540\n We just pick them randomly.\n\n1:48:55.820 --> 1:49:01.740\n And, uh, and in simulation model just goes for thousands of years and keeps\n\n1:49:01.740 --> 1:49:03.460\n on solving Rubik's cube in each of them.\n\n1:49:03.780 --> 1:49:08.660\n And the thing is that neural network that we used, it has a memory.\n\n1:49:09.260 --> 1:49:15.580\n And as it presses, for instance, the side of the, of the cube, it can sense,\n\n1:49:15.620 --> 1:49:19.620\n oh, that's actually, this side was, uh, difficult to press.\n\n1:49:19.620 --> 1:49:24.540\n I should press it stronger and throughout this process kind of, uh, learn it's even\n\n1:49:24.540 --> 1:49:29.060\n how to, uh, how to solve this particular instance of the Rubik's cube, like even\n\n1:49:29.060 --> 1:49:34.620\n mass, it's kind of like, uh, you know, sometimes when you go to a gym and after,\n\n1:49:34.660 --> 1:49:44.020\n um, after bench press, you try to leave the class and you kind of forgot, uh, and,\n\n1:49:44.060 --> 1:49:48.900\n and your head goes like up right away because kind of you got used to maybe\n\n1:49:48.900 --> 1:49:54.940\n different weight and it takes a second to adjust and this kind of, of a memory,\n\n1:49:54.940 --> 1:49:58.180\n the model gained through the process of interacting with the cube in the\n\n1:49:58.180 --> 1:50:02.580\n simulation, I appreciate you speaking to the audience with the bench press,\n\n1:50:02.660 --> 1:50:05.780\n all the bros in the audience, probably working out right now.\n\n1:50:05.780 --> 1:50:08.540\n There's probably somebody listening to this actually doing bench press.\n\n1:50:09.300 --> 1:50:13.900\n Um, so maybe, uh, put the bar down and pick up the water bottle and you'll\n\n1:50:13.900 --> 1:50:17.020\n know exactly what, uh, what Jack is talking about.\n\n1:50:17.060 --> 1:50:17.540\n Okay.\n\n1:50:17.540 --> 1:50:18.500\n Okay.\n\n1:50:18.500 --> 1:50:24.620\n So what, uh, what was the hardest part of getting the whole thing to work?\n\n1:50:24.780 --> 1:50:31.660\n So the hardest part is at the moment when it comes to, uh, physical work, when it\n\n1:50:31.660 --> 1:50:36.740\n comes to robots, uh, they require maintenance, it's hard to replicate a\n\n1:50:36.740 --> 1:50:41.620\n million times it's, uh, it's also, it's hard to replay things exactly.\n\n1:50:41.620 --> 1:50:48.460\n I remember this situation that one guy at our company, he had like a model that\n\n1:50:48.460 --> 1:50:52.180\n performs way better than other models in solving Rubik's cube.\n\n1:50:52.580 --> 1:50:57.540\n And, uh, you know, we kind of didn't know what's going on, why it's that.\n\n1:50:58.420 --> 1:51:04.420\n And, uh, it turned out, uh, that, you know, he was running it from his laptop\n\n1:51:04.420 --> 1:51:09.540\n that had better CPU or better, maybe local GPU as well.\n\n1:51:09.540 --> 1:51:14.140\n And, uh, because of that, there was less of a latency and the model was the same.\n\n1:51:14.780 --> 1:51:18.540\n And that actually made solving Rubik's cube more reliable.\n\n1:51:18.820 --> 1:51:22.300\n So in some sense, there might be some subtle bugs like that when it comes\n\n1:51:22.300 --> 1:51:24.060\n to running things in the real world.\n\n1:51:24.700 --> 1:51:29.420\n Even hinting on that, you could imagine that the initial models you would like\n\n1:51:29.420 --> 1:51:34.140\n to have models, which are insanely huge neural networks, and you would like to\n\n1:51:34.140 --> 1:51:36.460\n give them even more time for thinking.\n\n1:51:36.460 --> 1:51:41.980\n And when you have these real time systems, uh, then you might be constrained\n\n1:51:41.980 --> 1:51:43.700\n actually by the amount of latency.\n\n1:51:44.660 --> 1:51:50.940\n And, uh, ultimately I would like to build a system that it is worth for you to wait\n\n1:51:50.940 --> 1:51:55.220\n five minutes because it gives you the answer that you're willing to wait for\n\n1:51:55.220 --> 1:51:55.900\n five minutes.\n\n1:51:56.260 --> 1:51:59.620\n So latency is a very unpleasant constraint under which to operate.\n\n1:51:59.820 --> 1:52:00.320\n Correct.\n\n1:52:00.620 --> 1:52:04.260\n And also there is actually one more thing, which is tricky about robots.\n\n1:52:04.260 --> 1:52:08.060\n Uh, there is actually, uh, no, uh, not much data.\n\n1:52:08.060 --> 1:52:13.380\n So the data that I'm speaking about would be a data of, uh, first person\n\n1:52:13.380 --> 1:52:17.660\n experience from the robot and like a gigabytes of data like that, if we would\n\n1:52:17.660 --> 1:52:21.900\n have gigabytes of data like that, of robots solving various problems, it would\n\n1:52:21.900 --> 1:52:24.060\n be very easy to make a progress on robotics.\n\n1:52:24.420 --> 1:52:28.660\n And you can see that in case of text or code, there is a lot of data, like a\n\n1:52:28.660 --> 1:52:31.980\n first person perspective, they don't writing code.\n\n1:52:31.980 --> 1:52:37.740\n Yeah. So you had this, you mentioned this really interesting idea that if you were\n\n1:52:37.740 --> 1:52:42.100\n to build like a successful robotics company, so open as mission is much\n\n1:52:42.100 --> 1:52:46.500\n bigger than robotics, this is one of the, one of the things you've worked on, but\n\n1:52:46.500 --> 1:52:51.260\n if it was a robotics company, they, you wouldn't so quickly dismiss supervised\n\n1:52:51.260 --> 1:52:58.300\n learning, uh, correct that you would build a robot that, uh, was perhaps what\n\n1:52:58.300 --> 1:53:04.180\n like, um, an empty shell, like dumb, and they would operate under teleoperation.\n\n1:53:04.660 --> 1:53:09.660\n So you would invest, that's just one way to do it, invest in human supervision,\n\n1:53:09.700 --> 1:53:14.740\n like direct human control of the robots as it's learning and over time, add\n\n1:53:14.740 --> 1:53:15.820\n more and more automation.\n\n1:53:16.380 --> 1:53:16.860\n That's correct.\n\n1:53:16.860 --> 1:53:20.220\n So let's say that's how I would build a robotics company today.\n\n1:53:20.780 --> 1:53:23.620\n If I would be building a robotics company, which is, you know, spend 10\n\n1:53:23.620 --> 1:53:28.660\n million dollars or so recording human trajectories, controlling a robot.\n\n1:53:29.100 --> 1:53:34.860\n After you find a thing that the robot should be doing, that there's a market\n\n1:53:34.860 --> 1:53:37.340\n fit for, like you can make a lot of money with that product.\n\n1:53:37.380 --> 1:53:37.700\n Correct.\n\n1:53:37.700 --> 1:53:38.100\n Correct.\n\n1:53:38.100 --> 1:53:38.500\n Yeah.\n\n1:53:38.500 --> 1:53:43.420\n Uh, so I would record data and then I would essentially train supervised\n\n1:53:43.500 --> 1:53:44.460\n learning model on it.\n\n1:53:45.020 --> 1:53:46.540\n That might be the path today.\n\n1:53:47.220 --> 1:53:47.860\n Long term.\n\n1:53:47.860 --> 1:53:52.340\n I think that actually what is needed is to have a robot that can\n\n1:53:52.340 --> 1:53:54.780\n train powerful models over video.\n\n1:53:55.580 --> 1:54:02.740\n So, um, you have seen maybe a models that can generate images like Dali and people\n\n1:54:02.740 --> 1:54:06.260\n are looking into models, generating videos, they're like, uh, bodies,\n\n1:54:06.300 --> 1:54:08.260\n algorithmic questions, even how to do it.\n\n1:54:08.500 --> 1:54:13.220\n And it's unclear if there is enough compute for this purpose, but, uh, I, I\n\n1:54:13.220 --> 1:54:19.300\n suspect that the models that which would have a level of understanding of video,\n\n1:54:19.300 --> 1:54:25.620\n same as GPT has a level of understanding of text, could be used, uh, to train\n\n1:54:25.620 --> 1:54:26.540\n robots to solve tasks.\n\n1:54:26.580 --> 1:54:28.300\n They would have a lot of common sense.\n\n1:54:29.780 --> 1:54:36.420\n If one day, I'm pretty sure one day there will be a robotics company by robotics\n\n1:54:36.420 --> 1:54:42.740\n company, I mean, the primary source of income is, is from robots that is worth\n\n1:54:42.740 --> 1:54:44.740\n over $1 trillion.\n\n1:54:44.740 --> 1:54:49.940\n What do you think that company will do?\n\n1:54:49.940 --> 1:54:51.620\n I think self driving cars.\n\n1:54:51.620 --> 1:54:53.260\n No, it's interesting.\n\n1:54:53.260 --> 1:54:56.500\n Cause my mind went to personal robotics, robots in the home.\n\n1:54:57.220 --> 1:54:59.820\n It seems like there's much more market opportunity there.\n\n1:55:00.300 --> 1:55:03.380\n I think it's very difficult to achieve.\n\n1:55:04.420 --> 1:55:09.460\n I mean, this, this, this might speak to something important, which is I understand\n\n1:55:09.460 --> 1:55:12.180\n self driving much better than understand robotics in the home.\n\n1:55:12.180 --> 1:55:17.500\n So I understand how difficult it is to actually solve self driving to a, to a\n\n1:55:17.500 --> 1:55:22.060\n level, not just the actual computer vision and the control problem and just the\n\n1:55:22.060 --> 1:55:27.540\n basic problem of self driving, but creating a product that would undeniably\n\n1:55:28.100 --> 1:55:31.260\n be, um, that will cost less money.\n\n1:55:31.300 --> 1:55:34.020\n Like it will save you a lot of money, like orders of magnitude, less money\n\n1:55:34.220 --> 1:55:36.300\n that could replace Uber drivers, for example.\n\n1:55:36.780 --> 1:55:41.380\n So car sharing that's autonomous, that creates a similar or better\n\n1:55:41.380 --> 1:55:46.220\n experience in terms of how quickly you get from A to B or just whatever, the\n\n1:55:46.220 --> 1:55:50.260\n pleasantness of the experience, the efficiency of the experience, the value\n\n1:55:50.260 --> 1:55:54.580\n of the experience, and at the same time, the car itself costs cheaper.\n\n1:55:55.300 --> 1:55:57.300\n I think that's very difficult to achieve.\n\n1:55:57.340 --> 1:56:02.140\n I think there's a lot more, um, low hanging fruit in the home.\n\n1:56:03.780 --> 1:56:08.340\n That, that, that could be, I also want to give you a perspective on like how\n\n1:56:08.340 --> 1:56:12.900\n challenging it would be at home or like it maybe kind of depends on that exact\n\n1:56:12.900 --> 1:56:14.100\n problem that you'd be solving.\n\n1:56:14.100 --> 1:56:20.220\n Like if we're speaking about these robotic arms and hands, these things,\n\n1:56:20.220 --> 1:56:27.540\n they cost tens of thousands of dollars or maybe a hundred K and, um, you know,\n\n1:56:27.580 --> 1:56:30.260\n maybe, obviously, maybe there would be economy of scale.\n\n1:56:30.260 --> 1:56:34.460\n These things would be cheaper, but actually for any household to buy it,\n\n1:56:34.540 --> 1:56:37.340\n the price would have to go down to maybe a thousand bucks.\n\n1:56:37.340 --> 1:56:38.340\n Yeah.\n\n1:56:38.340 --> 1:56:44.220\n I personally think that, uh, so self driving car, it provides a clear service.\n\n1:56:44.500 --> 1:56:48.180\n I don't think robots in the home, there'll be a trillion dollar company\n\n1:56:48.180 --> 1:56:53.260\n will just be all about service, meaning it will not necessarily be about like\n\n1:56:53.260 --> 1:56:56.060\n a robotic arm that's helps you.\n\n1:56:56.100 --> 1:57:02.580\n I don't know, open a bottle or wash the dishes or, uh, any of that kind of stuff.\n\n1:57:02.580 --> 1:57:05.940\n It has to be able to take care of that whole, the therapist thing.\n\n1:57:05.940 --> 1:57:10.700\n You mentioned, I think that's, um, of course there's a line between what\n\n1:57:10.700 --> 1:57:14.420\n is a robot and what is not like, does it really need a body?\n\n1:57:14.460 --> 1:57:19.780\n But you know, some, um, uh, AI system with some embodiment, I think.\n\n1:57:20.340 --> 1:57:23.860\n So the tricky part when you think actually what's the difficult part is,\n\n1:57:24.260 --> 1:57:29.940\n um, when the robot has like, when there is a diversity of the environment\n\n1:57:29.940 --> 1:57:31.980\n with which the robot has to interact, that becomes hard.\n\n1:57:31.980 --> 1:57:36.740\n So, you know, on the one spectrum, you have, uh, industrial robots as they\n\n1:57:36.740 --> 1:57:40.900\n are doing over and over the same thing, it is possible to some extent to\n\n1:57:40.900 --> 1:57:46.220\n prescribe the movements and we've very small amount of intelligence, the, the\n\n1:57:46.300 --> 1:57:48.100\n movement can be repeated millions of times.\n\n1:57:48.100 --> 1:57:52.700\n Um, the, it, there are also, you know, various pieces of industrial robots\n\n1:57:52.700 --> 1:57:54.340\n where it becomes harder and harder.\n\n1:57:54.500 --> 1:57:59.460\n I can, for instance, in case of Tesla, it might be a matter of putting a, a\n\n1:57:59.460 --> 1:58:03.860\n rack inside of a car and, you know, because the rack kind of moves around,\n\n1:58:03.860 --> 1:58:05.580\n it's, uh, it's not that easy.\n\n1:58:05.580 --> 1:58:07.940\n It's not exactly the same every time.\n\n1:58:08.100 --> 1:58:10.460\n That's not being the case that you need actually humans to do it.\n\n1:58:11.500 --> 1:58:15.580\n Uh, while, you know, welding cars together, it's a very repetitive process.\n\n1:58:16.100 --> 1:58:23.300\n Um, then in case of self driving itself, uh, that difficulty has to do with the\n\n1:58:23.460 --> 1:58:27.860\n diversity of the environment, but still the car itself, um, the problem\n\n1:58:27.860 --> 1:58:32.300\n that they are solving is you try to avoid even interacting with things.\n\n1:58:32.540 --> 1:58:35.900\n You are not touching anything around because touching itself is hard.\n\n1:58:36.140 --> 1:58:40.580\n And then if you would have in the home, uh, robot that, you know, has to\n\n1:58:40.580 --> 1:58:44.140\n touch things and like if these things, they change the shape, if there is a huge\n\n1:58:44.140 --> 1:58:46.660\n variety of things to be touched, then that's difficult.\n\n1:58:46.860 --> 1:58:50.300\n If you are speaking about the robot, which there is, you know, head that\n\n1:58:50.300 --> 1:58:54.380\n is smiling in some way with cameras that either doesn't, you know, touch things.\n\n1:58:54.660 --> 1:58:55.900\n That's relatively simple.\n\n1:58:55.900 --> 1:58:59.260\n Okay. So to both agree and to push back.\n\n1:59:00.060 --> 1:59:07.260\n So you're referring to touch, like soft robotics, like the actual touch, but.\n\n1:59:08.060 --> 1:59:13.860\n I would argue that you could formulate just basic interaction between, um, like\n\n1:59:13.900 --> 1:59:18.660\n non contact interaction is also a kind of touch and that might be very difficult\n\n1:59:18.660 --> 1:59:22.620\n to solve that's the basic, this not disagreement, but that's the basic open\n\n1:59:22.620 --> 1:59:27.540\n question to me with self driving cars and this agreement with Elon, which\n\n1:59:27.540 --> 1:59:31.260\n is how much interaction is required to solve self driving cars.\n\n1:59:31.260 --> 1:59:32.820\n How much touch is required?\n\n1:59:33.180 --> 1:59:36.900\n You said that in your intuition, touch is not required.\n\n1:59:37.380 --> 1:59:41.820\n And my intuition to create a product that's compelling to use, you're going\n\n1:59:41.820 --> 1:59:46.740\n to have to, uh, interact with pedestrians, not just avoid pedestrians,\n\n1:59:46.740 --> 1:59:49.980\n but interact with them when we drive around.\n\n1:59:49.980 --> 1:59:54.100\n In major cities, we're constantly threatening everybody's life with\n\n1:59:54.100 --> 1:59:57.740\n our movements, um, and that's how they respect us.\n\n1:59:57.740 --> 2:00:02.940\n There's a game to ready going out with pedestrians and I'm afraid you can't\n\n2:00:02.940 --> 2:00:08.460\n just formulate autonomous driving as a collision avoidance problem.\n\n2:00:08.820 --> 2:00:12.380\n So I think it goes beyond like a collision avoidance is the\n\n2:00:12.380 --> 2:00:13.700\n first order approximation.\n\n2:00:14.180 --> 2:00:18.420\n Uh, but then at least in case of Tesla, you can't just\n\n2:00:18.420 --> 2:00:22.500\n at least in case of Tesla, they are gathering data from people driving their\n\n2:00:22.500 --> 2:00:27.220\n cars and I believe that's an example of supervised data that they can train\n\n2:00:27.220 --> 2:00:32.900\n their models, uh, on, and they are doing it, uh, which, you know, can give\n\n2:00:32.900 --> 2:00:38.900\n a model dislike, uh, another level of, uh, of, uh, behavior that is needed\n\n2:00:38.900 --> 2:00:40.900\n to actually interact with the real world.\n\n2:00:41.140 --> 2:00:41.340\n Yeah.\n\n2:00:41.340 --> 2:00:45.340\n It's interesting how much data is required to achieve that.\n\n2:00:45.340 --> 2:00:49.380\n Um, w what do you think of the whole Tesla autopilot approach, the computer\n\n2:00:49.380 --> 2:00:53.380\n vision based approach with multiple cameras and there's a data engine.\n\n2:00:53.380 --> 2:00:57.820\n It's a multitask, multiheaded neural network, and it's this fascinating\n\n2:00:57.820 --> 2:01:02.740\n process of, uh, similar to what you're talking about with the robotics\n\n2:01:02.780 --> 2:01:06.540\n approach, uh, which is, you know, you deploy in your own network and\n\n2:01:06.540 --> 2:01:10.940\n then there's humans that use it and then it runs into trouble in a bunch\n\n2:01:10.940 --> 2:01:12.780\n of places and that stuff is sent back.\n\n2:01:12.780 --> 2:01:17.740\n So like the deployment discovers a bunch of edge cases and those edge\n\n2:01:17.740 --> 2:01:22.140\n cases are sent back for supervised annotation, thereby improving the\n\n2:01:22.140 --> 2:01:23.980\n neural network and that's deployed again.\n\n2:01:24.540 --> 2:01:29.340\n It goes over and over until the network becomes really good at the task of\n\n2:01:29.340 --> 2:01:31.340\n driving becomes safer and safer.\n\n2:01:31.580 --> 2:01:34.460\n What do you think of that kind of approach to robotics?\n\n2:01:34.700 --> 2:01:36.060\n I believe that's the way to go.\n\n2:01:36.100 --> 2:01:39.660\n So in some sense, even when I was speaking about, you know, collecting\n\n2:01:39.660 --> 2:01:43.180\n trajectories from humans, that's like a first step and then you deploy\n\n2:01:43.180 --> 2:01:46.620\n the system and then you have humans revising the, all the issues.\n\n2:01:46.620 --> 2:01:51.580\n And in some sense, like at this approach converges to system that doesn't make\n\n2:01:51.580 --> 2:01:54.700\n mistakes because for the cases where there are mistakes, you got their\n\n2:01:54.700 --> 2:01:57.660\n data, how to fix them and the system will keep on improving.\n\n2:01:58.220 --> 2:02:02.380\n So there's a very, to me, difficult question of how hard that, you know,\n\n2:02:02.460 --> 2:02:04.940\n how long that converging takes, how hard it is.\n\n2:02:04.940 --> 2:02:09.180\n The other aspect of autonomous vehicles, this probably applies to certain\n\n2:02:09.180 --> 2:02:12.620\n robotics applications is society, right?\n\n2:02:12.700 --> 2:02:17.580\n They put as, as the quality of the system converges.\n\n2:02:18.220 --> 2:02:21.820\n So one, there's a human factors perspective of psychology of humans being\n\n2:02:21.820 --> 2:02:25.420\n able to supervise those even with teleoperation, those robots.\n\n2:02:25.740 --> 2:02:28.380\n And the other is society willing to accept robots.\n\n2:02:29.100 --> 2:02:32.540\n Currently society is much harsher on self driving cars than it is on human\n\n2:02:32.540 --> 2:02:35.340\n driven cars in terms of the expectation of safety.\n\n2:02:35.660 --> 2:02:38.620\n So the bar is set much higher than for humans.\n\n2:02:39.100 --> 2:02:43.740\n And so if there's a death in an autonomous vehicle, that's seen as a much more,\n\n2:02:47.180 --> 2:02:50.220\n much more dramatic than a death in the human driven vehicle.\n\n2:02:50.940 --> 2:02:55.260\n Part of the success of deployment of robots is figuring out how to make robots\n\n2:02:55.260 --> 2:03:01.100\n part of society, both on the, just the human side, on the media side, on the\n\n2:03:01.100 --> 2:03:04.460\n media journalist side, and also on the policy government side.\n\n2:03:04.780 --> 2:03:08.620\n And that seems to be, maybe you can put that into the objective function to\n\n2:03:08.620 --> 2:03:14.860\n optimize, but that is, that is definitely a tricky one.\n\n2:03:14.860 --> 2:03:18.460\n And I wonder if that is actually the trickiest part for self driving cars or\n\n2:03:18.460 --> 2:03:20.300\n any system that's safety critical.\n\n2:03:21.340 --> 2:03:24.460\n It's not the algorithm, it's the society accepting it.\n\n2:03:24.460 --> 2:03:31.020\n Yeah, I would say, I believe that the part of the process of deployment is actually\n\n2:03:31.020 --> 2:03:36.860\n showing people that the given things can be trusted and, you know, trust is also\n\n2:03:36.860 --> 2:03:42.620\n like a glass that is actually really easy to crack it and damage it.\n\n2:03:43.100 --> 2:03:52.300\n And I think that's actually very common with, with innovation, that there's\n\n2:03:52.300 --> 2:03:56.620\n some resistance toward it and it's just the natural progression.\n\n2:03:56.620 --> 2:04:00.140\n So in some sense, people will have to keep on proving that indeed these\n\n2:04:00.140 --> 2:04:02.300\n systems are worth being used.\n\n2:04:02.780 --> 2:04:08.940\n And I would say, I also found out that often the best way to convince people\n\n2:04:09.420 --> 2:04:11.420\n is by letting them experience it.\n\n2:04:11.660 --> 2:04:12.540\n Yeah, absolutely.\n\n2:04:12.540 --> 2:04:17.180\n That's the case with Tesla autopilot, for example, that's the case with, yeah,\n\n2:04:17.180 --> 2:04:18.940\n with basically robots in general.\n\n2:04:18.940 --> 2:04:22.220\n It's kind of funny to hear people talk about robots.\n\n2:04:22.220 --> 2:04:27.420\n Like there's a lot of fear, even with like legged robots, but when they\n\n2:04:27.420 --> 2:04:30.780\n actually interact with them, there's joy.\n\n2:04:31.420 --> 2:04:32.700\n I love interacting with them.\n\n2:04:32.780 --> 2:04:38.860\n And the same with the car, with a robot, if it starts being useful, I think\n\n2:04:38.860 --> 2:04:40.380\n people immediately understand.\n\n2:04:40.460 --> 2:04:43.100\n And if the product is designed well, they fall in love.\n\n2:04:43.340 --> 2:04:43.820\n You're right.\n\n2:04:44.300 --> 2:04:46.940\n It's actually even similar when I'm thinking about the car.\n\n2:04:46.940 --> 2:04:51.180\n It's actually even similar when I'm thinking about Copilot, the GitHub Copilot.\n\n2:04:51.260 --> 2:04:53.980\n There was a spectrum of responses that people had.\n\n2:04:54.460 --> 2:04:59.900\n And ultimately the important piece was to let people try it out.\n\n2:05:00.140 --> 2:05:02.300\n And then many people just loved it.\n\n2:05:02.620 --> 2:05:04.700\n Especially like programmers.\n\n2:05:05.020 --> 2:05:07.820\n Yeah, programmers, but like some of them, you know, they came with a fear.\n\n2:05:08.300 --> 2:05:08.540\n Yeah.\n\n2:05:08.860 --> 2:05:11.260\n But then you try it out and you think, actually, that's cool.\n\n2:05:11.820 --> 2:05:15.180\n And, you know, you can try to resist the same way as, you know, you could\n\n2:05:15.180 --> 2:05:20.060\n resist moving from punch cards to, let's say, C++ or so.\n\n2:05:20.860 --> 2:05:22.860\n And it's a little bit futile.\n\n2:05:23.980 --> 2:05:30.060\n So we talked about generation of program, generation of language, even\n\n2:05:30.540 --> 2:05:33.820\n self supervised learning in the visual space for robotics and then\n\n2:05:33.820 --> 2:05:34.780\n reinforcement learning.\n\n2:05:35.100 --> 2:05:40.700\n What do you, in like this whole beautiful spectrum of AI, do you think is a\n\n2:05:40.700 --> 2:05:47.340\n good benchmark, a good test to strive for to achieve intelligence?\n\n2:05:47.740 --> 2:05:49.580\n That's a strong test of intelligence.\n\n2:05:49.820 --> 2:05:52.380\n You know, it started with Alan Turing and the Turing test.\n\n2:05:53.260 --> 2:05:56.700\n Maybe you think natural language conversation is a good test.\n\n2:05:57.100 --> 2:06:01.260\n So, you know, it would be nice if, for instance, machine would be able to\n\n2:06:01.340 --> 2:06:03.180\n solve Riemann hypothesis in math.\n\n2:06:04.540 --> 2:06:07.420\n That would be, I think that would be very impressive.\n\n2:06:07.420 --> 2:06:12.940\n So theorem proving, is that to you, proving theorems is a good, oh, oh,\n\n2:06:12.940 --> 2:06:15.820\n like one thing that the machine did, you would say, damn.\n\n2:06:16.460 --> 2:06:17.020\n Exactly.\n\n2:06:18.460 --> 2:06:18.780\n Okay.\n\n2:06:19.420 --> 2:06:22.380\n That would be quite, quite impressive.\n\n2:06:22.460 --> 2:06:26.940\n I mean, the tricky part about the benchmarks is, you know, as we are\n\n2:06:26.940 --> 2:06:29.340\n getting closer with them, we have to invent new benchmarks.\n\n2:06:29.340 --> 2:06:31.180\n There is actually no ultimate benchmark out there.\n\n2:06:31.660 --> 2:06:31.820\n Yeah.\n\n2:06:31.820 --> 2:06:36.140\n See, my thought with the Riemann hypothesis would be the moment the\n\n2:06:36.140 --> 2:06:39.820\n machine proves it, we would say, okay, well then the problem was easy.\n\n2:06:40.860 --> 2:06:41.660\n That's what happens.\n\n2:06:42.060 --> 2:06:46.140\n And I mean, in some sense, that's actually what happens over the years\n\n2:06:46.140 --> 2:06:49.660\n in AI that like, we get used to things very quickly.\n\n2:06:50.380 --> 2:06:52.300\n You know something, I talked to Rodney Brooks.\n\n2:06:52.300 --> 2:06:53.340\n I don't know if you know who that is.\n\n2:06:54.380 --> 2:06:57.020\n He called AlphaZero homework problem.\n\n2:06:57.020 --> 2:06:59.740\n Cause he was saying like, there's nothing special about it.\n\n2:06:59.740 --> 2:07:00.780\n It's not a big leap.\n\n2:07:00.780 --> 2:07:05.260\n And I didn't, well, he's coming from one of the aspects that we referred\n\n2:07:05.260 --> 2:07:10.140\n to is he was part of the founding of iRobot, which deployed now tens\n\n2:07:10.140 --> 2:07:11.900\n of millions of robot in the home.\n\n2:07:11.900 --> 2:07:18.540\n So if you see robots that are actually in the homes of people as the\n\n2:07:18.540 --> 2:07:23.340\n legitimate instantiation of artificial intelligence, then yes, maybe an AI\n\n2:07:23.340 --> 2:07:26.460\n that plays a silly game like go and chess is not a real accomplishment,\n\n2:07:26.460 --> 2:07:29.180\n but to me it's a fundamental leap.\n\n2:07:29.180 --> 2:07:33.740\n But I think we as humans then say, okay, well then that that game of\n\n2:07:33.740 --> 2:07:37.660\n chess or go wasn't that difficult compared to the thing that's currently\n\n2:07:37.660 --> 2:07:38.220\n unsolved.\n\n2:07:38.220 --> 2:07:44.940\n So my intuition is that from perspective of the evolution of these AI\n\n2:07:44.940 --> 2:07:49.820\n systems will at first seen the tremendous progress in digital space.\n\n2:07:49.820 --> 2:07:52.700\n And the, you know, the main thing about digital space is also that you\n\n2:07:52.700 --> 2:07:56.300\n can, everything is that there is a lot of recorded data.\n\n2:07:56.300 --> 2:07:59.900\n Plus you can very rapidly deploy things to billions of people.\n\n2:07:59.900 --> 2:08:05.260\n While in case of a physical space, the deployment part takes multiple\n\n2:08:05.260 --> 2:08:05.500\n years.\n\n2:08:05.500 --> 2:08:10.300\n You have to manufacture things and, you know, delivering it to actual\n\n2:08:10.300 --> 2:08:11.900\n people, it's very hard.\n\n2:08:13.580 --> 2:08:19.980\n So I'm expecting that the first and that prices in digital space of\n\n2:08:19.980 --> 2:08:24.220\n goods, they would go, you know, down to the, let's say marginal costs\n\n2:08:24.220 --> 2:08:25.020\n are two zero.\n\n2:08:25.020 --> 2:08:28.780\n And also the question is how much of our life will be in digital because\n\n2:08:28.780 --> 2:08:31.980\n it seems like we're heading towards more and more of our lives being in\n\n2:08:31.980 --> 2:08:33.260\n the digital space.\n\n2:08:33.260 --> 2:08:37.100\n So like innovation in the physical space might become less and less\n\n2:08:37.100 --> 2:08:38.060\n significant.\n\n2:08:38.060 --> 2:08:42.700\n Like why do you need to drive anywhere if most of your life is spent in\n\n2:08:42.700 --> 2:08:44.060\n virtual reality?\n\n2:08:44.060 --> 2:08:47.980\n I still would like, you know, to at least at the moment, my impression\n\n2:08:47.980 --> 2:08:51.020\n is that I would like to have a physical contact with other people.\n\n2:08:51.020 --> 2:08:52.220\n And that's very important to me.\n\n2:08:52.940 --> 2:08:55.180\n We don't have a way to replicate it in the computer.\n\n2:08:55.180 --> 2:08:57.260\n It might be the case that over the time it will change.\n\n2:08:57.260 --> 2:09:02.380\n Like in 10 years from now, why not have like an arbitrary infinite number\n\n2:09:02.380 --> 2:09:04.060\n of people you can interact with?\n\n2:09:04.060 --> 2:09:09.740\n Some of them are real, some are not with arbitrary characteristics that\n\n2:09:09.740 --> 2:09:12.700\n you can define based on your own preferences.\n\n2:09:12.700 --> 2:09:15.900\n I think that's maybe where we are heading and maybe I'm resisting the\n\n2:09:15.900 --> 2:09:16.460\n future.\n\n2:09:16.460 --> 2:09:25.100\n Yeah, I'm telling you, if I got to choose, if I could live in Elder\n\n2:09:25.100 --> 2:09:29.820\n Scrolls Skyrim versus the real world, I'm not so sure I would stay with\n\n2:09:29.820 --> 2:09:30.380\n the real world.\n\n2:09:31.420 --> 2:09:35.900\n Yeah, I mean, the question is, so will VR be sufficient to get us there\n\n2:09:35.900 --> 2:09:38.940\n or do you need to, you know, plug electrodes in the brain?\n\n2:09:40.140 --> 2:09:43.500\n And it would be nice if these electrodes wouldn't be invasive.\n\n2:09:45.020 --> 2:09:47.500\n Or at least like provably non destructive.\n\n2:09:49.020 --> 2:09:53.420\n But in the digital space, do you think we'll be able to solve the\n\n2:09:53.420 --> 2:09:57.020\n Turing test, the spirit of the Turing test, which is, do you think we'll\n\n2:09:57.020 --> 2:10:02.380\n be able to achieve compelling natural language conversation between\n\n2:10:02.380 --> 2:10:06.300\n people, like have friends that are AI systems on the internet?\n\n2:10:07.100 --> 2:10:08.780\n I totally think it's doable.\n\n2:10:08.780 --> 2:10:12.460\n Do you think the current approach of GPT will take us there?\n\n2:10:12.460 --> 2:10:16.700\n So there is, you know, the part of at first learning all the content\n\n2:10:16.700 --> 2:10:20.060\n out there and I think that Steel System should keep on learning as\n\n2:10:20.060 --> 2:10:21.260\n it speaks with you.\n\n2:10:21.260 --> 2:10:21.500\n Yeah.\n\n2:10:21.500 --> 2:10:23.900\n Yeah, and I think that should work.\n\n2:10:23.900 --> 2:10:25.660\n The question is how exactly to do it.\n\n2:10:25.660 --> 2:10:29.740\n And, you know, obviously we have people at OpenAI asking these\n\n2:10:29.740 --> 2:10:35.100\n questions and kind of at first pre training on all existing content\n\n2:10:35.100 --> 2:10:37.580\n is like a backbone and is a decent backbone.\n\n2:10:39.340 --> 2:10:45.820\n Do you think AI needs a body connecting to our robotics question to\n\n2:10:45.820 --> 2:10:49.100\n truly connect with humans or can most of the connection be in the\n\n2:10:49.100 --> 2:10:49.820\n digital space?\n\n2:10:49.820 --> 2:10:55.260\n So let's see, we know that there are people who met each other online\n\n2:10:55.260 --> 2:10:56.300\n and they fell in love.\n\n2:10:57.740 --> 2:10:57.980\n Yeah.\n\n2:10:58.620 --> 2:11:03.740\n So it seems that it's conceivable to establish connection, which is\n\n2:11:03.740 --> 2:11:06.060\n purely through internet.\n\n2:11:07.340 --> 2:11:10.540\n Of course, it might be more compelling the more modalities you add.\n\n2:11:12.140 --> 2:11:16.620\n So it would be like you're proposing like a Tinder, but for AI, you\n\n2:11:16.620 --> 2:11:20.220\n like swipe right and left and half the systems are AI and the other is\n\n2:11:21.100 --> 2:11:22.700\n humans and you don't know which is which.\n\n2:11:24.380 --> 2:11:27.180\n That would be our formulation of Turing test.\n\n2:11:27.980 --> 2:11:32.460\n The moment AI is able to achieve more swipe right or left, whatever,\n\n2:11:33.260 --> 2:11:36.940\n the moment it's able to be more attractive than other humans, it\n\n2:11:36.940 --> 2:11:38.060\n passes the Turing test.\n\n2:11:38.060 --> 2:11:40.620\n Then you would pass the Turing test in attractiveness.\n\n2:11:40.620 --> 2:11:41.100\n That's right.\n\n2:11:41.100 --> 2:11:42.940\n Well, no, like attractiveness just to clarify.\n\n2:11:42.940 --> 2:11:44.060\n There will be conversation.\n\n2:11:44.060 --> 2:11:44.780\n Not just visual.\n\n2:11:44.780 --> 2:11:45.260\n Right, right.\n\n2:11:45.260 --> 2:11:51.660\n It's also attractiveness with wit and humor and whatever makes\n\n2:11:51.660 --> 2:11:53.340\n conversation is pleasant for humans.\n\n2:11:56.060 --> 2:11:56.700\n Okay.\n\n2:11:56.700 --> 2:11:57.100\n All right.\n\n2:11:58.780 --> 2:12:02.620\n So you're saying it's possible to achieve in the digital space.\n\n2:12:02.620 --> 2:12:04.620\n In some sense, I would almost ask that question.\n\n2:12:05.180 --> 2:12:06.620\n Why wouldn't that be possible?\n\n2:12:07.980 --> 2:12:11.180\n Well, I have this argument with my dad all the time.\n\n2:12:11.180 --> 2:12:13.820\n He thinks that touch and smell are really important.\n\n2:12:13.820 --> 2:12:16.700\n So they can be very important.\n\n2:12:16.700 --> 2:12:19.260\n And I'm saying the initial systems, they won't have it.\n\n2:12:20.380 --> 2:12:28.380\n Still, there are people being born without these senses and I believe\n\n2:12:28.380 --> 2:12:31.580\n that they can still fall in love and have meaningful life.\n\n2:12:32.140 --> 2:12:32.460\n Yeah.\n\n2:12:32.460 --> 2:12:37.500\n I wonder if it's possible to go close to all the way by just training\n\n2:12:37.500 --> 2:12:39.740\n on transcripts of conversations.\n\n2:12:40.620 --> 2:12:42.220\n I wonder how far that takes us.\n\n2:12:42.220 --> 2:12:45.980\n So I think that actually still you want images like I would like.\n\n2:12:45.980 --> 2:12:50.620\n So I don't have kids, but like I could imagine having AI Tutor.\n\n2:12:50.620 --> 2:12:55.100\n It has to see, you know, kids drawing some pictures on the paper.\n\n2:12:56.300 --> 2:12:58.460\n And also facial expressions, all that kind of stuff.\n\n2:12:58.460 --> 2:13:04.060\n We use dogs and humans use their eyes to communicate with each other.\n\n2:13:04.060 --> 2:13:07.500\n I think that's a really powerful mechanism of communication.\n\n2:13:07.500 --> 2:13:12.540\n Body language too, that words are much lower bandwidth.\n\n2:13:12.540 --> 2:13:15.340\n And for body language, we still, you know, we kind of have a system\n\n2:13:15.340 --> 2:13:19.420\n that displays an image of its or facial expression on the computer.\n\n2:13:19.980 --> 2:13:23.420\n Doesn't have to move, you know, mechanical pieces or so.\n\n2:13:23.420 --> 2:13:27.420\n So I think that, you know, that there is like kind of a progression.\n\n2:13:27.420 --> 2:13:31.100\n You can imagine that text might be the simplest to tackle.\n\n2:13:31.660 --> 2:13:36.700\n But this is not a complete human experience at all.\n\n2:13:36.700 --> 2:13:41.260\n You expand it to, let's say images, both for input and output.\n\n2:13:41.260 --> 2:13:45.900\n And what you describe is actually the final, I guess, frontier.\n\n2:13:45.900 --> 2:13:50.060\n What makes us human, the fact that we can touch each other or smell or so.\n\n2:13:50.060 --> 2:13:53.420\n And it's the hardest from perspective of data and deployment.\n\n2:13:54.140 --> 2:13:58.380\n And I believe that these things might happen gradually.\n\n2:13:59.660 --> 2:14:01.340\n Are you excited by that possibility?\n\n2:14:01.340 --> 2:14:07.820\n This particular application of human to AI friendship and interaction?\n\n2:14:07.820 --> 2:14:08.700\n So let's see.\n\n2:14:09.660 --> 2:14:11.500\n Like would you, do you look forward to a world?\n\n2:14:12.380 --> 2:14:15.340\n You said you're living with a few folks and you're very close friends with them.\n\n2:14:16.060 --> 2:14:19.580\n Do you look forward to a day where one or two of those friends are AI systems?\n\n2:14:19.580 --> 2:14:25.180\n So if the system would be truly wishing me well, rather than being in the situation\n\n2:14:25.180 --> 2:14:28.460\n that it optimizes for my time to interact with the system.\n\n2:14:28.460 --> 2:14:33.500\n The line between those is, it's a gray area.\n\n2:14:33.500 --> 2:14:38.060\n I think that's the distinction between love and possession.\n\n2:14:39.340 --> 2:14:46.620\n And these things, they might be often correlated for humans, but you might find that there are\n\n2:14:46.620 --> 2:14:48.860\n some friends with whom you haven't spoke for months.\n\n2:14:49.660 --> 2:14:50.060\n Yeah.\n\n2:14:50.060 --> 2:14:54.620\n And then you pick up the phone, it's as the time hasn't passed.\n\n2:14:54.620 --> 2:14:55.820\n They are not holding to you.\n\n2:14:55.820 --> 2:15:02.300\n And I will, I wouldn't like to have AI system that, you know, it's trying to convince me\n\n2:15:02.300 --> 2:15:03.420\n to spend time with it.\n\n2:15:03.420 --> 2:15:10.700\n I would like the system to optimize for what I care about and help me in achieving my own goals.\n\n2:15:12.300 --> 2:15:17.900\n But there's some, I mean, I don't know, there's some manipulation, there's some possessiveness,\n\n2:15:17.900 --> 2:15:23.340\n there's some insecurities, this fragility, all those things are necessary to form a close\n\n2:15:23.340 --> 2:15:29.260\n friendship over time, to go through some dark shit together, some bliss and happiness together.\n\n2:15:29.740 --> 2:15:33.900\n I feel like there's a lot of greedy self centered behavior within that process.\n\n2:15:35.020 --> 2:15:41.340\n My intuition, but I might be wrong, is that human computer interaction doesn't have to\n\n2:15:41.340 --> 2:15:46.140\n go through a computer being greedy, possessive, and so on.\n\n2:15:46.140 --> 2:15:50.700\n It is possible to train systems, maybe, that they actually\n\n2:15:50.700 --> 2:15:57.020\n they are, I guess, prompted or fine tuned or so to truly optimize for what you care about.\n\n2:15:57.020 --> 2:16:01.980\n And you could imagine that, you know, the way how the process would look like is at\n\n2:16:01.980 --> 2:16:08.860\n some point, we as humans, we look at the transcript of the conversation or like an entire\n\n2:16:08.860 --> 2:16:14.700\n interaction and we say, actually here, there was more loving way to go about it.\n\n2:16:14.700 --> 2:16:20.540\n And we supervise system toward being more loving, or maybe we train the system such\n\n2:16:20.540 --> 2:16:23.180\n that it has a reward function toward being more loving.\n\n2:16:23.180 --> 2:16:23.740\n Yeah.\n\n2:16:23.740 --> 2:16:29.820\n Or maybe the possibility of the system being an asshole and manipulative and possessive\n\n2:16:29.820 --> 2:16:32.940\n every once in a while is a feature, not a bug.\n\n2:16:33.580 --> 2:16:40.860\n Because some of the happiness that we experience when two souls meet each other, when two humans\n\n2:16:40.860 --> 2:16:44.620\n meet each other, is a kind of break from the assholes in the world.\n\n2:16:45.420 --> 2:16:52.060\n And so you need assholes in AI as well, because, like, it'll be like a breath of fresh air\n\n2:16:52.060 --> 2:17:00.540\n to discover an AI that the three previous AIs you had are too friendly or no, or cruel\n\n2:17:00.540 --> 2:17:01.340\n or whatever.\n\n2:17:01.340 --> 2:17:03.020\n It's like some kind of mix.\n\n2:17:03.020 --> 2:17:07.420\n And then this one is just right, but you need to experience the full spectrum.\n\n2:17:07.420 --> 2:17:10.940\n Like, I think you need to be able to engineer assholes.\n\n2:17:11.500 --> 2:17:12.860\n So let's see.\n\n2:17:14.380 --> 2:17:20.220\n Because there's some level to us being appreciated to appreciate the human experience.\n\n2:17:21.180 --> 2:17:24.300\n We need the dark and the light.\n\n2:17:24.300 --> 2:17:25.820\n So that kind of reminds me.\n\n2:17:27.100 --> 2:17:35.820\n I met a while ago at the meditation retreat, one woman, and she told me, you know,\n\n2:17:35.820 --> 2:17:41.260\n beautiful, beautiful woman, and she had a she had a crutch.\n\n2:17:41.260 --> 2:17:41.980\n Okay.\n\n2:17:41.980 --> 2:17:44.940\n She had the trouble walking on one leg.\n\n2:17:44.940 --> 2:17:46.460\n I asked her what has happened.\n\n2:17:47.340 --> 2:17:55.820\n And she said that five years ago she was in Maui, Hawaii, and she was eating a salad and\n\n2:17:55.820 --> 2:17:57.980\n some snail fell into the salad.\n\n2:17:57.980 --> 2:18:01.820\n And apparently there are neurotoxic snails over there.\n\n2:18:02.380 --> 2:18:04.380\n And she got into coma for a year.\n\n2:18:04.380 --> 2:18:05.740\n Okay.\n\n2:18:05.740 --> 2:18:09.660\n And apparently there is, you know, high chance of even just dying.\n\n2:18:09.660 --> 2:18:10.860\n But she was in the coma.\n\n2:18:10.860 --> 2:18:14.860\n At some point, she regained partially consciousness.\n\n2:18:14.860 --> 2:18:16.780\n She was able to hear people in the room.\n\n2:18:18.380 --> 2:18:19.900\n People behave as she wouldn't be there.\n\n2:18:21.100 --> 2:18:25.900\n You know, at some point she started being able to speak, but she was mumbling like a\n\n2:18:25.900 --> 2:18:28.460\n barely able to express herself.\n\n2:18:28.460 --> 2:18:30.700\n Then at some point she got into wheelchair.\n\n2:18:30.700 --> 2:18:38.140\n Then at some point she actually noticed that she can move her toe and then she knew that\n\n2:18:38.140 --> 2:18:39.180\n she will be able to walk.\n\n2:18:40.220 --> 2:18:42.620\n And then, you know, that's where she was five years after.\n\n2:18:42.620 --> 2:18:47.100\n And she said that since then she appreciates the fact that she can move her toe.\n\n2:18:48.460 --> 2:18:53.580\n And I was thinking, hmm, do I need to go through such experience to appreciate that I have\n\n2:18:53.580 --> 2:18:55.020\n I can move my toe?\n\n2:18:55.020 --> 2:18:58.300\n Wow, that's a really good story and really deep example.\n\n2:18:58.300 --> 2:18:58.780\n Yeah.\n\n2:18:58.780 --> 2:19:05.420\n And in some sense, it might be the case that we don't see light if we haven't went through\n\n2:19:05.420 --> 2:19:06.380\n the darkness.\n\n2:19:06.380 --> 2:19:07.900\n But I wouldn't say that we should.\n\n2:19:08.780 --> 2:19:14.460\n We shouldn't assume that that's the case, which it may be able to engineer shortcuts.\n\n2:19:14.460 --> 2:19:15.180\n Yeah.\n\n2:19:15.180 --> 2:19:22.220\n Ilya had this, you know, belief that maybe one has to go for a week or six months to\n\n2:19:22.220 --> 2:19:29.660\n do some challenging camp to just experience, you know, a lot of difficulties and then comes\n\n2:19:29.660 --> 2:19:33.500\n back and actually everything is bright, everything is beautiful.\n\n2:19:33.500 --> 2:19:34.460\n I'm with Ilya on this.\n\n2:19:34.460 --> 2:19:35.500\n It must be a Russian thing.\n\n2:19:35.500 --> 2:19:36.940\n Where are you from originally?\n\n2:19:36.940 --> 2:19:37.900\n I'm Polish.\n\n2:19:37.900 --> 2:19:38.400\n Polish.\n\n2:19:39.740 --> 2:19:40.240\n Okay.\n\n2:19:41.500 --> 2:19:43.500\n I'm tempted to say that explains a lot.\n\n2:19:43.500 --> 2:19:47.820\n But yeah, there's something about the Russian, the necessity of suffering.\n\n2:19:47.820 --> 2:19:52.700\n I believe suffering or rather struggle is necessary.\n\n2:19:52.700 --> 2:19:54.300\n I believe that struggle is necessary.\n\n2:19:54.300 --> 2:20:00.380\n I mean, in some sense, you even look at the story of any superhero in the movie.\n\n2:20:00.380 --> 2:20:03.340\n It's not that it was like everything goes easy, easy, easy, easy.\n\n2:20:03.340 --> 2:20:07.820\n I like how that's your ground truth is the story of superheroes.\n\n2:20:07.820 --> 2:20:08.320\n Okay.\n\n2:20:09.260 --> 2:20:13.420\n You mentioned that you used to do research at night and go to bed at like 6 a.m.\n\n2:20:13.420 --> 2:20:14.140\n or 7 a.m.\n\n2:20:14.140 --> 2:20:17.260\n I still do that often.\n\n2:20:18.860 --> 2:20:23.180\n What sleep schedules have you tried to make for a productive and happy life?\n\n2:20:23.180 --> 2:20:29.500\n Like, is there is there some interesting wild sleeping patterns that you engaged that you\n\n2:20:29.500 --> 2:20:30.860\n found that works really well for you?\n\n2:20:31.420 --> 2:20:37.180\n I tried at some point decreasing number of hours of sleep like a gradually like a half\n\n2:20:37.180 --> 2:20:38.540\n an hour every few days to this.\n\n2:20:39.100 --> 2:20:41.260\n You know, I was hoping to just save time.\n\n2:20:41.980 --> 2:20:43.500\n That clearly didn't work for me.\n\n2:20:43.500 --> 2:20:48.380\n Like at some point, there's like a phase shift and I felt tired all the time.\n\n2:20:50.380 --> 2:20:53.980\n You know, there was a time that I used to work during the nights.\n\n2:20:53.980 --> 2:20:57.100\n The nice thing about the nights is that no one disturbs you.\n\n2:20:57.740 --> 2:21:04.620\n And even I remember when I was meeting for the first time with Greg Brockman, his\n\n2:21:04.620 --> 2:21:08.860\n CTO and chairman of OpenAI, our meeting was scheduled to 5 p.m.\n\n2:21:09.660 --> 2:21:11.740\n And I overstepped for the meeting.\n\n2:21:11.740 --> 2:21:14.060\n Over slept for the meeting at 5 p.m.\n\n2:21:14.060 --> 2:21:15.740\n Yeah, now you sound like me.\n\n2:21:15.740 --> 2:21:16.540\n That's hilarious.\n\n2:21:16.540 --> 2:21:17.660\n OK, yeah.\n\n2:21:17.660 --> 2:21:23.820\n And at the moment, in some sense, my sleeping schedule also has to do with the fact that\n\n2:21:23.820 --> 2:21:26.060\n I'm interacting with people.\n\n2:21:26.780 --> 2:21:28.140\n I sleep without an alarm.\n\n2:21:28.620 --> 2:21:35.900\n So, yeah, the the team thing you mentioned, the extrovert thing, because most humans operate\n\n2:21:35.900 --> 2:21:41.660\n during a certain set of hours, you're forced to then operate at the same set of hours.\n\n2:21:42.220 --> 2:21:45.660\n But I'm not quite there yet.\n\n2:21:46.460 --> 2:21:51.420\n I found a lot of joy, just like you said, working through the night because it's quiet\n\n2:21:51.900 --> 2:21:53.660\n because the world doesn't disturb you.\n\n2:21:53.660 --> 2:21:57.580\n And there's some aspect counter to everything you're saying.\n\n2:21:57.580 --> 2:22:03.660\n There's some joyful aspect to sleeping through the mess of the day because people are having\n\n2:22:03.660 --> 2:22:07.500\n people are having meetings and sending emails and there's drama meetings.\n\n2:22:08.060 --> 2:22:09.980\n I can sleep through all the meetings.\n\n2:22:09.980 --> 2:22:14.140\n You know, I have meetings every day and they prevent me from having sufficient amount of\n\n2:22:14.140 --> 2:22:15.820\n time for focused work.\n\n2:22:16.780 --> 2:22:23.980\n And then I modified my calendar and I said that I'm out of office Wednesday, Thursday\n\n2:22:23.980 --> 2:22:27.500\n and Friday every day and I'm having meetings only Monday and Tuesday.\n\n2:22:27.500 --> 2:22:33.420\n And that busty positively influenced my mood that I have literally like at three days for\n\n2:22:33.420 --> 2:22:34.380\n fully focused work.\n\n2:22:34.380 --> 2:22:34.940\n Yeah.\n\n2:22:35.580 --> 2:22:39.580\n So there's better solutions to this problem than staying awake all night.\n\n2:22:39.980 --> 2:22:45.420\n OK, you've been part of development of some of the greatest ideas in artificial intelligence.\n\n2:22:45.420 --> 2:22:48.860\n What would you say is your process for developing good novel ideas?\n\n2:22:49.820 --> 2:22:53.820\n You have to be aware that clearly there are many other brilliant people around.\n\n2:22:53.820 --> 2:23:02.780\n So you have to ask yourself a question, why the given idea, let's say, wasn't tried by\n\n2:23:02.780 --> 2:23:10.140\n someone else and in some sense, it has to do with, you know, kind of simple.\n\n2:23:10.140 --> 2:23:12.940\n It might sound simple, but like a thinking outside of the box.\n\n2:23:12.940 --> 2:23:14.300\n And what do I mean here?\n\n2:23:14.780 --> 2:23:23.260\n So, for instance, for a while, people in academia, they assumed that you have a feeling that\n\n2:23:23.260 --> 2:23:30.220\n you have a fixed data set and then you optimize the algorithms in order to get the best performance.\n\n2:23:31.500 --> 2:23:39.580\n And that was so in great assumption that no one thought about training models on\n\n2:23:39.580 --> 2:23:42.700\n anti internet or like that.\n\n2:23:42.700 --> 2:23:48.540\n Maybe some people thought about it, but it felt to many as unfair.\n\n2:23:48.540 --> 2:23:53.180\n And in some sense, that's almost like a it's not my idea or so, but that's an example of\n\n2:23:53.180 --> 2:23:54.940\n breaking at the typical assumption.\n\n2:23:55.740 --> 2:23:59.740\n So you want to be in the paradigm that you're breaking at the typical assumption.\n\n2:24:00.540 --> 2:24:06.540\n In the context of the community, getting to pick your data set is cheating.\n\n2:24:06.540 --> 2:24:07.020\n Correct.\n\n2:24:07.020 --> 2:24:11.260\n And in some sense, so that was that was assumption that many people had out there.\n\n2:24:11.260 --> 2:24:19.020\n And then if you free yourself from assumptions, then they are likely to achieve something\n\n2:24:19.020 --> 2:24:20.380\n that others cannot do.\n\n2:24:20.380 --> 2:24:24.940\n And in some sense, if you are trying to do exactly the same things as others, it's very\n\n2:24:24.940 --> 2:24:26.940\n likely that you're going to have the same results.\n\n2:24:26.940 --> 2:24:33.660\n Yeah, I but there's also that kind of tension, which is asking yourself the question, why\n\n2:24:34.220 --> 2:24:35.660\n haven't others done this?\n\n2:24:35.660 --> 2:24:44.620\n Because, I mean, I get a lot of good ideas, but I think probably most of them suck when\n\n2:24:44.620 --> 2:24:45.900\n they meet reality.\n\n2:24:45.900 --> 2:24:53.500\n So so actually, I think the other big piece is getting into habit of generating ideas,\n\n2:24:53.500 --> 2:25:00.860\n training your brain towards generating ideas and not even suspending judgment of the ideas.\n\n2:25:00.860 --> 2:25:06.380\n So in some sense, I noticed myself that even if I'm in the process of generating ideas,\n\n2:25:06.380 --> 2:25:12.860\n if I tell myself, oh, that was a bad idea, then that actually interrupts the process\n\n2:25:12.860 --> 2:25:17.180\n and I cannot generate more ideas because I'm actually focused on the negative part, why\n\n2:25:17.180 --> 2:25:17.980\n it won't work.\n\n2:25:17.980 --> 2:25:18.480\n Yes.\n\n2:25:19.020 --> 2:25:25.020\n But I created also environment in the way that it's very easy for me to store new ideas.\n\n2:25:25.020 --> 2:25:31.900\n So, for instance, next to my bed, I have a voice recorder and it happens to me often\n\n2:25:31.900 --> 2:25:35.020\n like I wake up during the night and I have some idea.\n\n2:25:35.020 --> 2:25:40.380\n In the past, I was writing them down on my phone, but that means, you know, turning on\n\n2:25:40.380 --> 2:25:45.500\n the screen and that wakes me up or like pulling a paper, which requires, you know, turning\n\n2:25:45.500 --> 2:25:47.500\n on the light.\n\n2:25:47.500 --> 2:25:49.660\n These days, I just start recording it.\n\n2:25:49.660 --> 2:25:55.740\n What do you think, I don't know if you know who Jim Keller is.\n\n2:25:55.740 --> 2:25:57.740\n I know Jim Keller.\n\n2:25:57.740 --> 2:26:03.580\n He's a big proponent of thinking harder on a problem right before sleep so that he can\n\n2:26:03.580 --> 2:26:08.700\n sleep through it and solve it in his sleep or like come up with radical stuff in his\n\n2:26:08.700 --> 2:26:11.180\n sleep that's trying to get me to do this.\n\n2:26:11.180 --> 2:26:19.020\n So it happened from my experience perspective, it happened to me many times during the high\n\n2:26:19.020 --> 2:26:25.180\n school days when I was doing mathematics that I had a solution to my problem as I woke up.\n\n2:26:27.260 --> 2:26:33.420\n At the moment, regarding thinking hard about the given problem is I'm trying to actually\n\n2:26:33.420 --> 2:26:37.500\n devote substantial amount of time to think about important problems, not just before\n\n2:26:37.500 --> 2:26:37.900\n the sleep.\n\n2:26:39.020 --> 2:26:44.060\n I'm organizing amount of the huge chunks of time such that I'm not constantly working\n\n2:26:44.060 --> 2:26:48.220\n on the urgent problems, but I actually have time to think about the important one.\n\n2:26:48.220 --> 2:26:49.740\n So you do it naturally.\n\n2:26:49.740 --> 2:26:56.060\n But his idea is that you kind of prime your brain to make sure that that's the focus.\n\n2:26:56.060 --> 2:27:00.700\n Oftentimes people have other worries in their life that's not fundamentally deep problems\n\n2:27:00.700 --> 2:27:06.860\n like I don't know, just stupid drama in your life and even at work, all that kind of stuff.\n\n2:27:06.860 --> 2:27:12.620\n He wants to kind of pick the most important problem that you're thinking about and go\n\n2:27:12.620 --> 2:27:13.820\n to bed on that.\n\n2:27:13.820 --> 2:27:14.940\n I think that's wise.\n\n2:27:14.940 --> 2:27:19.820\n I mean, the other thing that comes to my mind is also I feel the most fresh in the morning.\n\n2:27:20.380 --> 2:27:25.900\n So during the morning, I try to work on the most important things rather than just being\n\n2:27:25.900 --> 2:27:28.540\n pulled by urgent things or checking email or so.\n\n2:27:29.740 --> 2:27:30.620\n What do you do with the...\n\n2:27:30.620 --> 2:27:35.020\n Because I've been doing the voice recorder thing too, but I end up recording so many\n\n2:27:35.020 --> 2:27:36.700\n messages it's hard to organize.\n\n2:27:37.260 --> 2:27:38.540\n I have the same problem.\n\n2:27:38.540 --> 2:27:44.380\n Now I have heard that Google Pixel is really good in transcribing text and I might get\n\n2:27:44.380 --> 2:27:47.020\n a Google Pixel just for the sake of transcribing text.\n\n2:27:47.020 --> 2:27:50.940\n Yeah, people listening to this, if you have a good voice recorder suggestion that transcribe,\n\n2:27:50.940 --> 2:27:51.580\n please let me know.\n\n2:27:52.780 --> 2:27:57.900\n Some of it has to do with OpenAI codecs too.\n\n2:27:57.900 --> 2:28:01.180\n Like some of it is simply like the friction.\n\n2:28:01.900 --> 2:28:08.940\n I need apps that remove that friction between voice and the organization of the resulting\n\n2:28:08.940 --> 2:28:10.300\n transcripts and all that kind of stuff.\n\n2:28:11.980 --> 2:28:12.940\n But yes, you're right.\n\n2:28:12.940 --> 2:28:20.460\n Absolutely, like during, for me it's walking, sleep too, but walking and running, especially\n\n2:28:20.460 --> 2:28:25.500\n running, get a lot of thoughts during running and there's no good mechanism for recording\n\n2:28:25.500 --> 2:28:25.980\n thoughts.\n\n2:28:25.980 --> 2:28:32.780\n So one more thing that I do, I have a separate phone which has no apps.\n\n2:28:33.660 --> 2:28:37.180\n Maybe it has like audible or let's say Kindle.\n\n2:28:37.180 --> 2:28:40.060\n No one has this phone number, this kind of my meditation phone.\n\n2:28:40.060 --> 2:28:40.620\n Yeah.\n\n2:28:40.620 --> 2:28:46.380\n And I try to expand the amount of time that that's the phone that I'm having.\n\n2:28:47.180 --> 2:28:52.220\n It has also Google Maps if I need to go somewhere and I also use this phone to write down ideas.\n\n2:28:52.860 --> 2:28:54.860\n Ah, that's a really good idea.\n\n2:28:55.660 --> 2:28:57.020\n That's a really good idea.\n\n2:28:57.020 --> 2:29:01.740\n Often actually what I end up doing is even sending a message from that phone to the other\n\n2:29:01.740 --> 2:29:02.380\n phone.\n\n2:29:02.380 --> 2:29:06.780\n So that's actually my way of recording messages or I just put them into notes.\n\n2:29:06.780 --> 2:29:07.340\n I love it.\n\n2:29:07.340 --> 2:29:15.660\n What advice would you give to a young person, high school, college, about how to be successful?\n\n2:29:15.660 --> 2:29:20.940\n You've done a lot of incredible things in the past decade, so maybe, maybe have some.\n\n2:29:20.940 --> 2:29:22.540\n There's something, there might be something.\n\n2:29:22.540 --> 2:29:23.580\n There might be something.\n\n2:29:25.020 --> 2:29:33.020\n I mean, might sound like a simplistic or so, but I would say literally just follow your\n\n2:29:33.020 --> 2:29:34.140\n passion, double down on it.\n\n2:29:34.140 --> 2:29:38.460\n And if you don't know what's your passion, just figure out what could be a, what could\n\n2:29:38.460 --> 2:29:39.100\n be a passion.\n\n2:29:39.100 --> 2:29:40.940\n So that might be an exploration.\n\n2:29:41.900 --> 2:29:45.500\n When I was in elementary school was math and chemistry.\n\n2:29:46.300 --> 2:29:52.300\n And I remember for some time I gave up on math because my school teacher, she told me\n\n2:29:52.300 --> 2:29:53.180\n that I'm dumb.\n\n2:29:54.940 --> 2:30:00.140\n And I guess maybe an advice would be just ignore people if they tell you that you're\n\n2:30:00.140 --> 2:30:00.860\n dumb.\n\n2:30:00.860 --> 2:30:01.420\n You're dumb.\n\n2:30:01.420 --> 2:30:06.300\n You're dumb. You mentioned something offline about chemistry and explosives.\n\n2:30:08.540 --> 2:30:09.660\n What was that about?\n\n2:30:09.660 --> 2:30:10.460\n So let's see.\n\n2:30:11.900 --> 2:30:13.420\n So a story goes like that.\n\n2:30:16.860 --> 2:30:18.300\n I got into chemistry.\n\n2:30:18.300 --> 2:30:22.620\n Maybe I was like a second grade of my elementary school, third grade.\n\n2:30:23.500 --> 2:30:25.900\n I started going to chemistry classes.\n\n2:30:27.740 --> 2:30:30.060\n I really love building stuff.\n\n2:30:30.060 --> 2:30:35.740\n And I did all the experiments that they describe in the book, like, you know, how to create\n\n2:30:35.740 --> 2:30:39.660\n oxygen with vinegar and baking soda or so.\n\n2:30:39.660 --> 2:30:40.160\n Okay.\n\n2:30:40.780 --> 2:30:45.740\n So I did all the experiments and at some point I was, you know, so what's next?\n\n2:30:45.740 --> 2:30:46.460\n What can I do?\n\n2:30:47.260 --> 2:30:53.180\n And explosives, they also, it's like a, you have a clear reward signal, you know, if the\n\n2:30:53.180 --> 2:30:54.140\n thing worked or not.\n\n2:30:54.140 --> 2:31:00.780\n So I remember at first I got interested in producing hydrogen.\n\n2:31:00.780 --> 2:31:03.260\n That was kind of funny experiment from school.\n\n2:31:03.260 --> 2:31:04.380\n You can just burn it.\n\n2:31:04.380 --> 2:31:07.420\n And then I moved to nitroglycerin.\n\n2:31:07.420 --> 2:31:10.220\n So that's also relatively easy to synthesize.\n\n2:31:11.260 --> 2:31:16.540\n I started producing essentially dynamite and detonating it with a friend.\n\n2:31:16.540 --> 2:31:20.860\n I remember there was a, you know, there was at first like maybe two attempts that I went\n\n2:31:20.860 --> 2:31:25.020\n with a friend to detonate what we built and it didn't work out.\n\n2:31:25.020 --> 2:31:27.660\n And like a third time he was like, ah, it won't work.\n\n2:31:27.660 --> 2:31:29.740\n Like, let's don't waste time.\n\n2:31:30.220 --> 2:31:38.700\n And, you know, we were, I was carrying this, this, you know, that tube with dynamite, I\n\n2:31:38.700 --> 2:31:45.260\n don't know, pound or so, dynamite in my backpack, we're like riding on the bike to the edges\n\n2:31:45.260 --> 2:31:45.820\n of the city.\n\n2:31:45.820 --> 2:31:51.340\n Yeah, and attempt number three, this was be attempt number three.\n\n2:31:51.340 --> 2:31:52.300\n Attempt number three.\n\n2:31:52.860 --> 2:31:57.420\n And now we dig a hole to put it inside.\n\n2:31:57.420 --> 2:32:01.020\n It actually had the, you know, electrical detonator.\n\n2:32:02.220 --> 2:32:04.860\n We draw a cable behind the tree.\n\n2:32:05.660 --> 2:32:10.140\n I even, I never had, I haven't ever seen like a explosion before.\n\n2:32:10.140 --> 2:32:15.580\n So I thought that there would be a lot of, you know, a lot of, you know, a lot of, you\n\n2:32:15.580 --> 2:32:17.100\n know, there will be a lot of sound.\n\n2:32:17.980 --> 2:32:22.380\n But, you know, we're like laying down and I'm holding the cable and the battery.\n\n2:32:22.380 --> 2:32:28.380\n At some point, you know, we kind of like a three to one and I just connected it and it\n\n2:32:28.380 --> 2:32:30.300\n felt like the ground shake.\n\n2:32:30.300 --> 2:32:32.860\n It was like more like a sound.\n\n2:32:32.860 --> 2:32:37.740\n And then the soil started kind of lifting up and started falling on us.\n\n2:32:37.740 --> 2:32:38.380\n Yeah.\n\n2:32:38.380 --> 2:32:39.180\n Wow.\n\n2:32:39.180 --> 2:32:45.580\n And then, you know, the friend said, let's make sure the next time we have helmets.\n\n2:32:45.580 --> 2:32:48.940\n But also, you know, I'm happy that nothing happened to me.\n\n2:32:48.940 --> 2:32:52.300\n It could have been the case that I lost the limbo or so.\n\n2:32:52.300 --> 2:33:01.900\n Yeah, but that's childhood of an engineering mind with a strong reward signal of an\n\n2:33:01.900 --> 2:33:02.460\n explosion.\n\n2:33:03.660 --> 2:33:04.140\n I love it.\n\n2:33:04.140 --> 2:33:10.140\n My there's some aspect of chemists that the chemists I know, like my dad with plasma\n\n2:33:10.140 --> 2:33:13.740\n chemistry, plasma physics, he was very much into explosives, too.\n\n2:33:13.740 --> 2:33:18.300\n It's a worrying quality of people that work in chemistry that they love.\n\n2:33:18.300 --> 2:33:23.500\n I think it is that exactly is the strong signal that the thing worked.\n\n2:33:23.500 --> 2:33:24.620\n There is no doubt.\n\n2:33:24.620 --> 2:33:25.660\n There's no doubt.\n\n2:33:25.660 --> 2:33:26.860\n There's some magic.\n\n2:33:26.860 --> 2:33:31.420\n It's almost like a reminder that physics works, that chemistry works.\n\n2:33:31.420 --> 2:33:32.220\n It's cool.\n\n2:33:32.220 --> 2:33:36.540\n It's almost like a little glimpse at nature that you yourself engineer.\n\n2:33:36.540 --> 2:33:43.420\n I that's why I really like artificial intelligence, especially robotics, is you create a little\n\n2:33:43.420 --> 2:33:49.020\n piece of nature and in some sense, even for me with explosives, the motivation was creation\n\n2:33:49.020 --> 2:33:50.060\n rather than destruction.\n\n2:33:50.060 --> 2:33:50.860\n Yes, exactly.\n\n2:33:51.740 --> 2:33:57.180\n In terms of advice, I forgot to ask about just machine learning and deep learning for\n\n2:33:57.180 --> 2:34:01.980\n people who are specifically interested in machine learning, how would you recommend\n\n2:34:01.980 --> 2:34:03.020\n they get into the field?\n\n2:34:03.580 --> 2:34:07.900\n So I would say re implement everything and also there is plenty of courses.\n\n2:34:08.620 --> 2:34:09.660\n So like from scratch?\n\n2:34:10.380 --> 2:34:14.780\n So on different levels of abstraction in some sense, but I would say re implement something\n\n2:34:14.780 --> 2:34:19.100\n from scratch, re implement something from a paper, re implement something, you know,\n\n2:34:19.100 --> 2:34:20.860\n from podcasts that you have heard about.\n\n2:34:21.420 --> 2:34:23.820\n I would say that's a powerful way to understand things.\n\n2:34:23.820 --> 2:34:30.220\n So it's often the case that you read the description and you think you understand, but you truly\n\n2:34:30.220 --> 2:34:35.500\n understand once you build it, then you actually know what really matter in the description.\n\n2:34:36.220 --> 2:34:40.300\n Is there a particular topics that you find people just fall in love with?\n\n2:34:41.020 --> 2:34:41.980\n I've seen.\n\n2:34:44.220 --> 2:34:51.500\n I tend to really enjoy reinforcement learning because it's much more, it's much easier\n\n2:34:51.500 --> 2:34:57.260\n to get to a point where you feel like you created something special, like fun games\n\n2:34:57.260 --> 2:34:58.620\n kind of things that are rewarding.\n\n2:34:58.620 --> 2:34:59.100\n It's rewarding.\n\n2:34:59.100 --> 2:34:59.600\n Yeah.\n\n2:35:01.100 --> 2:35:07.740\n As opposed to like re implementing from scratch, more like supervised learning kind of things.\n\n2:35:07.740 --> 2:35:08.940\n It's yeah.\n\n2:35:08.940 --> 2:35:15.260\n So, you know, if someone would optimize for things to be rewarding, then it feels that\n\n2:35:15.260 --> 2:35:18.460\n the things that are somewhat generative, they have such a property.\n\n2:35:18.460 --> 2:35:23.580\n So you have, for instance, adversarial networks, or do you have just even generative language\n\n2:35:23.580 --> 2:35:24.080\n models?\n\n2:35:24.700 --> 2:35:30.780\n And you can even see, internally, we have seen this thing with our releases.\n\n2:35:30.780 --> 2:35:33.820\n So we have, we released recently two models.\n\n2:35:33.820 --> 2:35:39.340\n There is one model called Dali that generates images, and there is other model called Clip\n\n2:35:39.340 --> 2:35:45.500\n that actually you provide various possibilities, what could be the answer to what is on the\n\n2:35:45.500 --> 2:35:48.700\n picture, and it can tell you which one is the most likely.\n\n2:35:48.700 --> 2:35:56.220\n And in some sense, in case of the first one, Dali, it is very easy for you to understand\n\n2:35:56.220 --> 2:35:58.220\n that actually there is magic going on.\n\n2:35:59.740 --> 2:36:04.860\n And in the case of the second one, even though it is insanely powerful, and you know, people\n\n2:36:04.860 --> 2:36:10.540\n from a vision community, they, as they started probing it inside, they actually understood\n\n2:36:12.540 --> 2:36:13.740\n how far it goes.\n\n2:36:13.740 --> 2:36:19.980\n How far it goes, it's difficult for a person at first to see how well it works.\n\n2:36:21.500 --> 2:36:25.260\n And that's the same, as you said, that in case of supervised learning models, you might\n\n2:36:25.260 --> 2:36:30.300\n not kind of see, or it's not that easy for you to understand the strength.\n\n2:36:31.180 --> 2:36:33.820\n Even though you don't believe in magic, to see the magic.\n\n2:36:33.820 --> 2:36:35.020\n To see the magic, yeah.\n\n2:36:35.020 --> 2:36:36.220\n It's a generative.\n\n2:36:36.220 --> 2:36:37.340\n That's really brilliant.\n\n2:36:37.340 --> 2:36:42.860\n So anything that's generative, because then you are at the core of the creation.\n\n2:36:42.860 --> 2:36:46.620\n You get to experience creation without much effort.\n\n2:36:46.620 --> 2:36:48.540\n Unless you have to do it from scratch, but.\n\n2:36:48.540 --> 2:36:51.900\n And it feels that, you know, humans are wired.\n\n2:36:51.900 --> 2:36:54.700\n There is some level of reward for creating stuff.\n\n2:36:54.700 --> 2:36:54.940\n Yeah.\n\n2:36:56.380 --> 2:36:59.100\n Of course, different people have a different weight on this reward.\n\n2:36:59.100 --> 2:36:59.340\n Yeah.\n\n2:37:00.460 --> 2:37:01.740\n In the big objective function.\n\n2:37:01.740 --> 2:37:03.980\n In the big objective function of a person.\n\n2:37:03.980 --> 2:37:04.620\n Of a person.\n\n2:37:05.420 --> 2:37:10.860\n You wrote that beautiful is what you intensely pay attention to.\n\n2:37:10.860 --> 2:37:12.380\n Even a cockroach is beautiful.\n\n2:37:12.380 --> 2:37:16.300\n If you look very closely, can you expand on this?\n\n2:37:16.300 --> 2:37:17.980\n What is beauty?\n\n2:37:18.620 --> 2:37:26.060\n So what I'm, I wrote here actually corresponds to my subjective experience that I had through\n\n2:37:26.060 --> 2:37:27.740\n extended periods of meditation.\n\n2:37:28.540 --> 2:37:34.380\n It's, it's pretty crazy that at some point the meditation gets you to the place that\n\n2:37:34.380 --> 2:37:39.820\n you have really increased focus, increased attention.\n\n2:37:39.820 --> 2:37:40.940\n Increased attention.\n\n2:37:40.940 --> 2:37:45.580\n And then you look at the very simple objects that were all the time around you can look\n\n2:37:45.580 --> 2:37:48.540\n at the table or on the pen or at the nature.\n\n2:37:49.260 --> 2:37:55.500\n And you notice more and more details and it becomes very pleasant to look at it.\n\n2:37:56.780 --> 2:37:59.660\n And it, once again, it kind of reminds me of my childhood.\n\n2:38:01.260 --> 2:38:03.900\n Like I just pure joy of being.\n\n2:38:03.900 --> 2:38:11.580\n It's also, I have seen even the reverse effect that by default, regardless of what we possess,\n\n2:38:11.580 --> 2:38:13.260\n we very quickly get used to it.\n\n2:38:14.300 --> 2:38:20.700\n And you know, you can have a very beautiful house and if you don't put sufficient effort,\n\n2:38:21.500 --> 2:38:25.500\n you're just going to get used to it and it doesn't bring any more joy,\n\n2:38:25.500 --> 2:38:26.700\n regardless of what you have.\n\n2:38:27.180 --> 2:38:27.680\n Yeah.\n\n2:38:27.680 --> 2:38:36.960\n Well, I actually, I find that material possessions get in the way of that experience of pure\n\n2:38:36.960 --> 2:38:37.460\n joy.\n\n2:38:38.720 --> 2:38:45.360\n So I've always, I've been very fortunate to just find joy in simple things.\n\n2:38:45.360 --> 2:38:50.800\n Just, just like you're saying, just like, I don't know, objects in my life, just stupid\n\n2:38:50.800 --> 2:38:55.440\n objects like this cup, like thing, you know, just objects sounds okay.\n\n2:38:55.440 --> 2:39:00.880\n I'm not being eloquent, but literally objects in the world, they're just full of joy.\n\n2:39:00.880 --> 2:39:07.360\n Cause it's like, I can't believe when I can't believe that I'm fortunate enough to be alive\n\n2:39:07.360 --> 2:39:09.200\n to experience these objects.\n\n2:39:09.680 --> 2:39:14.320\n And then two, I can't believe humans are clever enough to have built these objects.\n\n2:39:15.120 --> 2:39:19.520\n The hierarchy of pleasure that that provides is infinite.\n\n2:39:19.520 --> 2:39:24.000\n I mean, even if you look at the cup of water, so, you know, you see first like a level of\n\n2:39:24.000 --> 2:39:28.320\n like a reflection of light, but then you think, you know, man, there's like a trillions upon\n\n2:39:28.320 --> 2:39:32.000\n of trillions of particles bouncing against each other.\n\n2:39:32.000 --> 2:39:38.560\n There is also the tension on the surface that, you know, if the back, back could like a stand\n\n2:39:38.560 --> 2:39:40.000\n on it and move around.\n\n2:39:40.000 --> 2:39:44.800\n And you think it also has this like a magical property that as you decrease temperature,\n\n2:39:45.440 --> 2:39:51.680\n it actually expands in volume, which allows for the, you know, legs to freeze on the,\n\n2:39:51.680 --> 2:39:58.080\n on the surface and at the bottom to have actually not freeze, which allows for life like a crazy.\n\n2:39:58.080 --> 2:39:58.560\n Yeah.\n\n2:39:58.560 --> 2:40:03.520\n You look in detail at some object and you think actually, you know, this table, that\n\n2:40:03.520 --> 2:40:06.400\n was just a figment of someone's imagination at some point.\n\n2:40:06.400 --> 2:40:10.560\n And then there was like a thousands of people involved to actually manufacture it and put\n\n2:40:10.560 --> 2:40:11.120\n it here.\n\n2:40:11.120 --> 2:40:13.280\n And by default, no one cares.\n\n2:40:15.280 --> 2:40:19.360\n And then you can start thinking about evolution, how it all started from single cell organisms\n\n2:40:19.360 --> 2:40:21.280\n that led to this table.\n\n2:40:21.280 --> 2:40:27.360\n And these thoughts, they give me life appreciation and even lack of thoughts, just the pure raw\n\n2:40:27.360 --> 2:40:29.920\n signal also gives the life appreciation.\n\n2:40:29.920 --> 2:40:37.360\n See, the thing is, and then that's coupled for me with the sadness that the whole ride\n\n2:40:37.360 --> 2:40:43.440\n ends and perhaps is deeply coupled in that the fact that this experience, this moment\n\n2:40:43.440 --> 2:40:49.680\n ends, gives it, gives it an intensity that I'm not sure I would otherwise have.\n\n2:40:50.160 --> 2:40:53.600\n So in that same way, I tried to meditate on my own death.\n\n2:40:53.600 --> 2:40:54.160\n Often.\n\n2:40:54.880 --> 2:40:56.720\n Do you think about your mortality?\n\n2:40:58.160 --> 2:40:59.120\n Are you afraid of death?\n\n2:41:01.840 --> 2:41:07.840\n So fear of death is like one of the most fundamental fears that each of us has.\n\n2:41:07.840 --> 2:41:09.680\n We might be not even aware of it.\n\n2:41:09.680 --> 2:41:15.440\n It requires to look inside, to even recognize that it's out there and there is still, let's\n\n2:41:15.440 --> 2:41:22.960\n say, this property of nature that if things would last forever, then they would be also\n\n2:41:22.960 --> 2:41:23.760\n boring to us.\n\n2:41:24.880 --> 2:41:29.520\n The fact that the things change in some way gives any meaning to them.\n\n2:41:29.520 --> 2:41:40.800\n I also, you know, found out that it seems to be very healing to people to have these\n\n2:41:40.800 --> 2:41:49.440\n short experiences, like, I guess, psychedelic experiences in which they experience death\n\n2:41:49.440 --> 2:41:58.160\n of self in which they let go of this fear and then maybe can even increase the intensity\n\n2:41:58.160 --> 2:42:00.720\n can even increase the appreciation of the moment.\n\n2:42:01.520 --> 2:42:11.440\n It seems that many people, they can easily comprehend the fact that the money is finite\n\n2:42:12.160 --> 2:42:14.720\n while they don't see that time is finite.\n\n2:42:15.680 --> 2:42:18.640\n I have this like a discussion with Ilya from time to time.\n\n2:42:18.640 --> 2:42:23.520\n He's like, you know, man, like the life will pass very fast.\n\n2:42:23.520 --> 2:42:26.640\n At some point I will be 40, 50, 60, 70 and then it's over.\n\n2:42:26.640 --> 2:42:33.120\n This is true, which also makes me believe that, you know, that every single moment it\n\n2:42:33.120 --> 2:42:37.600\n is so unique that should be appreciated.\n\n2:42:37.600 --> 2:42:44.560\n And this also makes me think that I should be acting on my life because otherwise it\n\n2:42:44.560 --> 2:42:45.200\n will pass.\n\n2:42:46.240 --> 2:42:53.280\n I also like this framework of thinking from Jeff Bezos on regret minimization that like\n\n2:42:53.280 --> 2:43:01.520\n I would like if I will be at that deathbed to look back on my life and not regret that\n\n2:43:01.520 --> 2:43:03.280\n I haven't done something.\n\n2:43:03.280 --> 2:43:07.680\n It's usually you might regret that you haven't tried.\n\n2:43:07.680 --> 2:43:08.880\n I'm fine with failing.\n\n2:43:10.640 --> 2:43:11.440\n I haven't tried.\n\n2:43:13.120 --> 2:43:15.360\n What's the Nietzsche eternal occurrence?\n\n2:43:15.360 --> 2:43:20.480\n Try to live a life that if you had to live it infinitely many times, that would be the\n\n2:43:20.480 --> 2:43:23.920\n you'd be okay with that kind of life.\n\n2:43:24.640 --> 2:43:26.160\n So try to live it optimally.\n\n2:43:27.120 --> 2:43:30.800\n I can say that it's almost like I'm.\n\n2:43:33.280 --> 2:43:36.640\n I'm available to me where I am in my life.\n\n2:43:36.640 --> 2:43:40.480\n I'm extremely grateful for actually people whom I met.\n\n2:43:40.480 --> 2:43:44.320\n I would say I think that I'm decently smart and so on.\n\n2:43:44.320 --> 2:43:50.160\n But I think that actually to a great extent where I am has to do with the people who I\n\n2:43:50.160 --> 2:43:50.720\n met.\n\n2:43:52.320 --> 2:43:55.600\n Would you be okay if after this conversation you died?\n\n2:43:56.320 --> 2:44:00.800\n So if I'm dead, then it kind of I don't have a choice anymore.\n\n2:44:01.600 --> 2:44:05.440\n So in some sense, there's like plenty of things that I would like to try out in my life.\n\n2:44:07.040 --> 2:44:10.480\n I feel that I'm gradually going one by one and I'm just doing them.\n\n2:44:10.480 --> 2:44:13.120\n I think that the list will be always infinite.\n\n2:44:13.120 --> 2:44:15.680\n Yeah, so might as well go today.\n\n2:44:16.800 --> 2:44:20.000\n Yeah, I mean, to be clear, I'm not looking forward to die.\n\n2:44:20.800 --> 2:44:23.520\n I would say if there is no choice, I would accept it.\n\n2:44:24.320 --> 2:44:30.480\n But like in some sense, I'm if there would be a choice, if there would be a possibility\n\n2:44:30.480 --> 2:44:32.080\n to leave, I would fight for leaving.\n\n2:44:33.680 --> 2:44:37.120\n I find it's more.\n\n2:44:37.120 --> 2:44:44.560\n I find it's more honest and real to think about, you know, dying today at the end of\n\n2:44:44.560 --> 2:44:45.040\n the day.\n\n2:44:46.080 --> 2:44:52.960\n That seems to me, at least to my brain, more honest slap in the face as opposed to I still\n\n2:44:52.960 --> 2:44:59.520\n have 10 years like today, then I'm much more about appreciating the cup and the table and\n\n2:44:59.520 --> 2:45:04.960\n so on and less about like silly worldly accomplishments and all those kinds of things.\n\n2:45:04.960 --> 2:45:11.760\n But we have in the company a person who say at some point found out that they have cancer\n\n2:45:11.760 --> 2:45:16.000\n and that also gives, you know, huge perspective with respect to what matters now.\n\n2:45:16.000 --> 2:45:16.560\n Yeah.\n\n2:45:16.560 --> 2:45:20.320\n And, you know, often people in situations like that, they conclude that actually what\n\n2:45:20.320 --> 2:45:21.680\n matters is human connection.\n\n2:45:22.720 --> 2:45:28.720\n And love and that's people conclude also if you have kids, kids as family.\n\n2:45:28.720 --> 2:45:35.120\n You, I think, tweeted, we don't assign the minus infinity reward to our death.\n\n2:45:35.440 --> 2:45:38.640\n Such a reward would prevent us from taking any risk.\n\n2:45:38.640 --> 2:45:42.480\n We wouldn't be able to cross the road in fear of being hit by a car.\n\n2:45:42.480 --> 2:45:46.400\n So in the objective function, you mentioned fear of death might be fundamental to the\n\n2:45:46.400 --> 2:45:47.440\n human condition.\n\n2:45:48.400 --> 2:45:52.640\n So, as I said, let's assume that they're like a reward functions in our brain.\n\n2:45:52.640 --> 2:46:01.840\n And the interesting thing is even realization, how different reward functions can play with\n\n2:46:01.840 --> 2:46:02.640\n your behavior.\n\n2:46:03.440 --> 2:46:09.280\n As a matter of fact, I wouldn't say that you should assign infinite negative reward to\n\n2:46:09.280 --> 2:46:11.360\n anything because that messes up the math.\n\n2:46:12.400 --> 2:46:13.600\n The math doesn't work out.\n\n2:46:13.600 --> 2:46:14.320\n It doesn't work out.\n\n2:46:14.320 --> 2:46:19.440\n And as you said, even, you know, government or some insurance companies, you said they\n\n2:46:19.440 --> 2:46:22.720\n assign $9 million to human life.\n\n2:46:22.720 --> 2:46:29.600\n And I'm just saying it with respect to, that might be a hard statement to ourselves, but\n\n2:46:29.600 --> 2:46:32.480\n in some sense that there is a finite value of our own life.\n\n2:46:34.640 --> 2:46:43.440\n I'm trying to put it from perspective of being less, of being more egoless and realizing\n\n2:46:43.440 --> 2:46:44.800\n fragility of my own life.\n\n2:46:44.800 --> 2:46:53.760\n And in some sense, the fear of death might prevent you from acting because anything can\n\n2:46:53.760 --> 2:46:54.560\n cause death.\n\n2:46:56.080 --> 2:46:56.560\n Yeah.\n\n2:46:56.560 --> 2:47:00.800\n And I'm sure actually, if you were to put death in the objective function, there's probably\n\n2:47:00.800 --> 2:47:06.960\n so many aspects to death and fear of death and realization of death and mortality.\n\n2:47:06.960 --> 2:47:13.600\n There's just whole components of finiteness of not just your life, but every experience\n\n2:47:13.600 --> 2:47:17.840\n and so on that you're going to have to formalize mathematically.\n\n2:47:18.320 --> 2:47:27.040\n And also, you know, that might lead to you spending a lot of compute cycles on this like\n\n2:47:27.040 --> 2:47:32.480\n a deliberating this terrible future instead of experiencing now.\n\n2:47:32.480 --> 2:47:36.480\n And then in some sense, it's also kind of unpleasant simulation to run in your head.\n\n2:47:36.480 --> 2:47:37.040\n Yeah.\n\n2:47:39.040 --> 2:47:45.120\n Do you think there's an objective function that describes the entirety of human life?\n\n2:47:45.920 --> 2:47:49.680\n So, you know, usually the way you ask that is what is the meaning of life?\n\n2:47:50.560 --> 2:47:55.760\n Is there a universal objective functions that captures the why of life?\n\n2:47:55.760 --> 2:48:03.440\n So, yeah, I mean, I suspect that they will ask this question, but it's also a question\n\n2:48:03.440 --> 2:48:05.280\n that I ask myself many, many times.\n\n2:48:06.320 --> 2:48:10.320\n See, I can tell you a framework that I have these days to think about this question.\n\n2:48:10.320 --> 2:48:16.480\n So I think that fundamentally, meaning of life has to do with some of our reward actions\n\n2:48:16.480 --> 2:48:21.680\n that we have in brain and they might have to do with, let's say, for instance, curiosity\n\n2:48:21.680 --> 2:48:25.760\n or human connection, which might mean understanding others.\n\n2:48:27.760 --> 2:48:32.080\n It's also possible for a person to slightly modify their reward function.\n\n2:48:32.080 --> 2:48:37.280\n Usually they mostly stay fixed, but it's possible to modify reward function and you can pretty\n\n2:48:37.280 --> 2:48:38.080\n much choose.\n\n2:48:38.080 --> 2:48:42.480\n So in some sense, the reward functions, optimizing reward functions, they will give you a life\n\n2:48:42.480 --> 2:48:43.120\n satisfaction.\n\n2:48:44.000 --> 2:48:45.920\n Is there some randomness in the function?\n\n2:48:45.920 --> 2:48:48.000\n I think when you are born, there is some randomness.\n\n2:48:48.000 --> 2:48:53.840\n You can see that some people, for instance, they care more about building stuff.\n\n2:48:53.840 --> 2:48:56.160\n Some people care more about caring for others.\n\n2:48:56.880 --> 2:49:00.880\n Some people, there are all sorts of default reward functions.\n\n2:49:00.880 --> 2:49:08.400\n And then in some sense, you can ask yourself, what is the satisfying way for you to go after\n\n2:49:08.400 --> 2:49:09.680\n this reward function?\n\n2:49:09.680 --> 2:49:11.280\n And you just go after this reward function.\n\n2:49:11.280 --> 2:49:15.120\n And, you know, some people also ask, are you satisfied with your life?\n\n2:49:15.120 --> 2:49:19.120\n And, you know, some people also ask, are these reward functions real?\n\n2:49:19.840 --> 2:49:27.680\n I almost think about it as, let's say, if you would have to discover mathematics, in\n\n2:49:27.680 --> 2:49:34.640\n mathematics, you are likely to run into various objects like complex numbers or differentiation,\n\n2:49:34.640 --> 2:49:35.680\n some other objects.\n\n2:49:35.680 --> 2:49:38.320\n And these are very natural objects that arise.\n\n2:49:38.320 --> 2:49:42.480\n And similarly, the reward functions that we are having in our brain, they are somewhat\n\n2:49:42.480 --> 2:49:50.240\n very natural, that, you know, there is a reward function for understanding, like a comprehension,\n\n2:49:52.080 --> 2:49:53.280\n curiosity, and so on.\n\n2:49:53.280 --> 2:49:59.040\n So in some sense, they are in the same way natural as their natural objects in mathematics.\n\n2:49:59.040 --> 2:49:59.680\n Interesting.\n\n2:49:59.680 --> 2:50:05.600\n So, you know, there's the old sort of debate, is mathematics invented or discovered?\n\n2:50:05.600 --> 2:50:07.840\n You're saying reward functions are discovered.\n\n2:50:07.840 --> 2:50:08.880\n So nature.\n\n2:50:08.880 --> 2:50:12.960\n So nature provided some, you can still, let's say, expand it throughout the life.\n\n2:50:12.960 --> 2:50:15.360\n Some of the reward functions, they might be futile.\n\n2:50:15.360 --> 2:50:19.600\n Like, for instance, there might be a reward function, maximize amount of wealth.\n\n2:50:20.320 --> 2:50:20.800\n Yeah.\n\n2:50:20.800 --> 2:50:24.160\n And this is more like a learned reward function.\n\n2:50:25.520 --> 2:50:30.560\n But we know also that some reward functions, if you optimize them, you won't be quite satisfied.\n\n2:50:32.240 --> 2:50:37.040\n Well, I don't know which part of your reward function resulted in you coming today, but\n\n2:50:37.040 --> 2:50:40.960\n I am deeply appreciative that you did spend your valuable time with me.\n\n2:50:40.960 --> 2:50:43.360\n Wojtek is really fun talking to you.\n\n2:50:43.920 --> 2:50:45.200\n You're brilliant.\n\n2:50:45.200 --> 2:50:46.320\n You're a good human being.\n\n2:50:46.320 --> 2:50:48.880\n And it's an honor to meet you and an honor to talk to you.\n\n2:50:48.880 --> 2:50:50.080\n Thanks for talking today, brother.\n\n2:50:50.720 --> 2:50:51.600\n Thank you, Lex a lot.\n\n2:50:51.600 --> 2:50:54.240\n I appreciated your questions, curiosity.\n\n2:50:54.240 --> 2:50:55.680\n I had a lot of time being here.\n\n2:50:57.120 --> 2:51:00.480\n Thanks for listening to this conversation with Wojtek Zaremba.\n\n2:51:00.480 --> 2:51:04.480\n To support this podcast, please check out our sponsors in the description.\n\n2:51:04.480 --> 2:51:10.000\n And now, let me leave you with some words from Arthur C. Clarke, who is the author of\n\n2:51:10.000 --> 2:51:11.840\n 2001 A Space Odyssey.\n\n2:51:12.800 --> 2:51:18.800\n It may be that our role on this planet is not to worship God, but to create him.\n\n2:51:18.800 --> 2:51:34.160\n Thank you for listening, and I hope to see you next time.\n\n"
}