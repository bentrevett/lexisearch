{
  "title": "Chris Urmson: Self-Driving Cars at Aurora, Google, CMU, and DARPA | Lex Fridman Podcast #28",
  "id": "Tj6NOfdfa4o",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:03.120\n The following is a conversation with Chris Sampson.\n\n00:03.120 --> 00:06.000\n He was a CTO of the Google self driving car team,\n\n00:06.000 --> 00:08.880\n a key engineer and leader behind the Carnegie Mellon\n\n00:08.880 --> 00:12.000\n University autonomous vehicle entries in the DARPA Grand\n\n00:12.000 --> 00:16.160\n Challenges and the winner of the DARPA Urban Challenge.\n\n00:16.160 --> 00:20.100\n Today, he's the CEO of Aurora Innovation, an autonomous\n\n00:20.100 --> 00:21.360\n vehicle software company.\n\n00:21.360 --> 00:23.600\n He started with Sterling Anderson,\n\n00:23.600 --> 00:25.960\n who was the former director of Tesla Autopilot,\n\n00:25.960 --> 00:30.120\n and drew back now, Uber's former autonomy and perception lead.\n\n00:30.120 --> 00:32.880\n Chris is one of the top roboticists and autonomous\n\n00:32.880 --> 00:36.320\n vehicle experts in the world, and a longtime voice\n\n00:36.320 --> 00:38.840\n of reason in a space that is shrouded\n\n00:38.840 --> 00:41.320\n in both mystery and hype.\n\n00:41.320 --> 00:43.600\n He both acknowledges the incredible challenges\n\n00:43.600 --> 00:46.480\n involved in solving the problem of autonomous driving\n\n00:46.480 --> 00:49.760\n and is working hard to solve it.\n\n00:49.760 --> 00:52.400\n This is the Artificial Intelligence podcast.\n\n00:52.400 --> 00:54.720\n If you enjoy it, subscribe on YouTube,\n\n00:54.720 --> 00:57.920\n give it five stars on iTunes, support it on Patreon,\n\n00:57.920 --> 00:59.720\n or simply connect with me on Twitter\n\n00:59.720 --> 01:03.240\n at Lex Friedman, spelled F R I D M A N.\n\n01:03.240 --> 01:09.120\n And now, here's my conversation with Chris Sampson.\n\n01:09.120 --> 01:11.960\n You were part of both the DARPA Grand Challenge\n\n01:11.960 --> 01:13.880\n and the DARPA Urban Challenge teams\n\n01:13.880 --> 01:17.040\n at CMU with Red Whitaker.\n\n01:17.040 --> 01:19.720\n What technical or philosophical things\n\n01:19.720 --> 01:22.240\n have you learned from these races?\n\n01:22.240 --> 01:26.600\n I think the high order bit was that it could be done.\n\n01:26.600 --> 01:30.200\n I think that was the thing that was\n\n01:30.200 --> 01:34.880\n incredible about the first of the Grand Challenges,\n\n01:34.880 --> 01:38.160\n that I remember I was a grad student at Carnegie Mellon,\n\n01:38.160 --> 01:45.360\n and there was kind of this dichotomy of it\n\n01:45.360 --> 01:46.720\n seemed really hard, so that would\n\n01:46.720 --> 01:48.800\n be cool and interesting.\n\n01:48.800 --> 01:52.800\n But at the time, we were the only robotics institute around,\n\n01:52.800 --> 01:55.560\n and so if we went into it and fell on our faces,\n\n01:55.560 --> 01:58.360\n that would be embarrassing.\n\n01:58.360 --> 02:01.120\n So I think just having the will to go do it,\n\n02:01.120 --> 02:02.880\n to try to do this thing that at the time\n\n02:02.880 --> 02:05.000\n was marked as darn near impossible,\n\n02:05.000 --> 02:06.960\n and then after a couple of tries,\n\n02:06.960 --> 02:08.420\n be able to actually make it happen,\n\n02:08.420 --> 02:12.320\n I think that was really exciting.\n\n02:12.320 --> 02:15.040\n But at which point did you believe it was possible?\n\n02:15.040 --> 02:16.960\n Did you from the very beginning?\n\n02:16.960 --> 02:18.000\n Did you personally?\n\n02:18.000 --> 02:19.800\n Because you're one of the lead engineers.\n\n02:19.800 --> 02:21.800\n You actually had to do a lot of the work.\n\n02:21.800 --> 02:23.880\n Yeah, I was the technical director there,\n\n02:23.880 --> 02:26.120\n and did a lot of the work, along with a bunch\n\n02:26.120 --> 02:28.420\n of other really good people.\n\n02:28.420 --> 02:29.760\n Did I believe it could be done?\n\n02:29.760 --> 02:31.080\n Yeah, of course.\n\n02:31.080 --> 02:32.760\n Why would you go do something you thought\n\n02:32.760 --> 02:34.800\n was completely impossible?\n\n02:34.800 --> 02:36.260\n We thought it was going to be hard.\n\n02:36.260 --> 02:37.800\n We didn't know how we were going to be able to do it.\n\n02:37.800 --> 02:42.880\n We didn't know if we'd be able to do it the first time.\n\n02:42.880 --> 02:45.960\n Turns out we couldn't.\n\n02:45.960 --> 02:48.400\n That, yeah, I guess you have to.\n\n02:48.400 --> 02:52.960\n I think there's a certain benefit to naivete, right?\n\n02:52.960 --> 02:55.440\n That if you don't know how hard something really is,\n\n02:55.440 --> 02:59.600\n you try different things, and it gives you an opportunity\n\n02:59.600 --> 03:04.120\n that others who are wiser maybe don't have.\n\n03:04.120 --> 03:05.720\n What were the biggest pain points?\n\n03:05.720 --> 03:08.880\n Mechanical, sensors, hardware, software,\n\n03:08.880 --> 03:11.800\n algorithms for mapping, localization,\n\n03:11.800 --> 03:13.680\n just general perception, control?\n\n03:13.680 --> 03:15.320\n Like hardware, software, first of all?\n\n03:15.320 --> 03:20.120\n I think that's the joy of this field, is that it's all hard\n\n03:20.120 --> 03:25.360\n and that you have to be good at each part of it.\n\n03:25.360 --> 03:32.360\n So for the urban challenges, if I look back at it from today,\n\n03:32.360 --> 03:38.960\n it should be easy today, that it was a static world.\n\n03:38.960 --> 03:40.800\n There weren't other actors moving through it,\n\n03:40.800 --> 03:42.480\n is what that means.\n\n03:42.480 --> 03:47.080\n It was out in the desert, so you get really good GPS.\n\n03:47.080 --> 03:51.400\n So that went, and we could map it roughly.\n\n03:51.400 --> 03:55.160\n And so in retrospect now, it's within the realm of things\n\n03:55.160 --> 03:57.840\n we could do back then.\n\n03:57.840 --> 03:59.720\n Just actually getting the vehicle and the,\n\n03:59.720 --> 04:00.680\n there's a bunch of engineering work\n\n04:00.680 --> 04:04.760\n to get the vehicle so that we could control it and drive it.\n\n04:04.760 --> 04:09.600\n That's still a pain today, but it was even more so back then.\n\n04:09.600 --> 04:14.280\n And then the uncertainty of exactly what they wanted us to do\n\n04:14.280 --> 04:17.040\n was part of the challenge as well.\n\n04:17.040 --> 04:19.440\n Right, you didn't actually know the track heading in here.\n\n04:19.440 --> 04:21.480\n You knew approximately, but you didn't actually\n\n04:21.480 --> 04:23.520\n know the route that was going to be taken.\n\n04:23.520 --> 04:24.920\n That's right, we didn't know the route.\n\n04:24.920 --> 04:28.600\n We didn't even really, the way the rules had been described,\n\n04:28.600 --> 04:29.800\n you had to kind of guess.\n\n04:29.800 --> 04:33.360\n So if you think back to that challenge,\n\n04:33.360 --> 04:36.960\n the idea was that the government would give us,\n\n04:36.960 --> 04:40.320\n the DARPA would give us a set of waypoints\n\n04:40.320 --> 04:43.520\n and kind of the width that you had to stay within\n\n04:43.520 --> 04:46.800\n between the line that went between each of those waypoints.\n\n04:46.800 --> 04:49.280\n And so the most devious thing they could have done\n\n04:49.280 --> 04:53.280\n is set a kilometer wide corridor across a field\n\n04:53.280 --> 04:58.280\n of scrub brush and rocks and said, go figure it out.\n\n04:58.520 --> 05:01.920\n Fortunately, it really, it turned into basically driving\n\n05:01.920 --> 05:05.000\n along a set of trails, which is much more relevant\n\n05:05.000 --> 05:07.920\n to the application they were looking for.\n\n05:08.760 --> 05:12.080\n But no, it was a hell of a thing back in the day.\n\n05:12.080 --> 05:16.640\n So the legend, Red, was kind of leading that effort\n\n05:16.640 --> 05:19.120\n in terms of just broadly speaking.\n\n05:19.120 --> 05:22.040\n So you're a leader now.\n\n05:22.040 --> 05:25.000\n What have you learned from Red about leadership?\n\n05:25.000 --> 05:26.200\n I think there's a couple things.\n\n05:26.200 --> 05:31.080\n One is go and try those really hard things.\n\n05:31.080 --> 05:34.480\n That's where there is an incredible opportunity.\n\n05:34.480 --> 05:36.560\n I think the other big one, though,\n\n05:36.560 --> 05:40.680\n is to see people for who they can be, not who they are.\n\n05:41.720 --> 05:43.720\n It's one of the things that I actually,\n\n05:43.720 --> 05:46.080\n one of the deepest lessons I learned from Red\n\n05:46.080 --> 05:50.200\n was that he would look at undergraduates\n\n05:50.200 --> 05:55.200\n or graduate students and empower them to be leaders,\n\n05:56.120 --> 06:00.320\n to have responsibility, to do great things\n\n06:00.320 --> 06:04.480\n that I think another person might look at them\n\n06:04.480 --> 06:06.600\n and think, oh, well, that's just an undergraduate student.\n\n06:06.600 --> 06:07.720\n What could they know?\n\n06:08.680 --> 06:12.720\n And so I think that kind of trust but verify,\n\n06:12.720 --> 06:14.480\n have confidence in what people can become,\n\n06:14.480 --> 06:16.680\n I think is a really powerful thing.\n\n06:16.680 --> 06:20.440\n So through that, let's just fast forward through the history.\n\n06:20.440 --> 06:24.160\n Can you maybe talk through the technical evolution\n\n06:24.160 --> 06:26.200\n of autonomous vehicle systems\n\n06:26.200 --> 06:29.960\n from the first two Grand Challenges to the Urban Challenge\n\n06:29.960 --> 06:33.560\n to today, are there major shifts in your mind\n\n06:33.560 --> 06:37.240\n or is it the same kind of technology just made more robust?\n\n06:37.240 --> 06:39.840\n I think there's been some big, big steps.\n\n06:40.880 --> 06:43.720\n So for the Grand Challenge,\n\n06:43.720 --> 06:48.720\n the real technology that unlocked that was HD mapping.\n\n06:51.400 --> 06:54.200\n Prior to that, a lot of the off road robotics work\n\n06:55.160 --> 06:58.480\n had been done without any real prior model\n\n06:58.480 --> 07:01.400\n of what the vehicle was going to encounter.\n\n07:01.400 --> 07:04.880\n And so that innovation that the fact that we could get\n\n07:05.960 --> 07:10.960\n decimeter resolution models was really a big deal.\n\n07:13.440 --> 07:18.200\n And that allowed us to kind of bound the complexity\n\n07:18.200 --> 07:19.680\n of the driving problem the vehicle had\n\n07:19.680 --> 07:21.040\n and allowed it to operate at speed\n\n07:21.040 --> 07:23.800\n because we could assume things about the environment\n\n07:23.800 --> 07:25.360\n that it was going to encounter.\n\n07:25.360 --> 07:29.720\n So that was the big step there.\n\n07:31.280 --> 07:35.280\n For the Urban Challenge,\n\n07:37.240 --> 07:39.280\n one of the big technological innovations there\n\n07:39.280 --> 07:41.040\n was the multi beam LIDAR\n\n07:41.960 --> 07:45.760\n and being able to generate high resolution,\n\n07:45.760 --> 07:48.680\n mid to long range 3D models of the world\n\n07:48.680 --> 07:53.680\n and use that for understanding the world around the vehicle.\n\n07:53.680 --> 07:56.600\n And that was really kind of a game changing technology.\n\n07:58.600 --> 08:00.000\n In parallel with that,\n\n08:00.000 --> 08:04.360\n we saw a bunch of other technologies\n\n08:04.360 --> 08:06.120\n that had been kind of converging\n\n08:06.120 --> 08:08.440\n half their day in the sun.\n\n08:08.440 --> 08:12.560\n So Bayesian estimation had been,\n\n08:12.560 --> 08:17.560\n SLAM had been a big field in robotics.\n\n08:17.840 --> 08:20.760\n You would go to a conference a couple of years before that\n\n08:20.760 --> 08:24.880\n and every paper would effectively have SLAM somewhere in it.\n\n08:24.880 --> 08:29.320\n And so seeing that the Bayesian estimation techniques\n\n08:30.720 --> 08:33.400\n play out on a very visible stage,\n\n08:33.400 --> 08:36.520\n I thought that was pretty exciting to see.\n\n08:38.080 --> 08:41.560\n And mostly SLAM was done based on LIDAR at that time.\n\n08:41.560 --> 08:44.560\n Yeah, and in fact, we weren't really doing SLAM per se\n\n08:45.600 --> 08:47.480\n in real time because we had a model ahead of time,\n\n08:47.480 --> 08:51.040\n we had a roadmap, but we were doing localization.\n\n08:51.040 --> 08:53.560\n And we were using the LIDAR or the cameras\n\n08:53.560 --> 08:55.400\n depending on who exactly was doing it\n\n08:55.400 --> 08:57.560\n to localize to a model of the world.\n\n08:57.560 --> 09:00.160\n And I thought that was a big step\n\n09:00.160 --> 09:05.160\n from kind of naively trusting GPS, INS before that.\n\n09:06.640 --> 09:09.840\n And again, lots of work had been going on in this field.\n\n09:09.840 --> 09:13.040\n Certainly this was not doing anything\n\n09:13.040 --> 09:16.840\n particularly innovative in SLAM or in localization,\n\n09:16.840 --> 09:20.200\n but it was seeing that technology necessary\n\n09:20.200 --> 09:21.800\n in a real application on a big stage,\n\n09:21.800 --> 09:23.080\n I thought was very cool.\n\n09:23.080 --> 09:24.000\n So for the urban challenge,\n\n09:24.000 --> 09:28.600\n those are already maps constructed offline in general.\n\n09:28.600 --> 09:30.920\n And did people do that individually,\n\n09:30.920 --> 09:33.600\n did individual teams do it individually\n\n09:33.600 --> 09:36.440\n so they had their own different approaches there\n\n09:36.440 --> 09:41.440\n or did everybody kind of share that information\n\n09:41.720 --> 09:42.880\n at least intuitively?\n\n09:42.880 --> 09:47.880\n So DARPA gave all the teams a model of the world, a map.\n\n09:49.640 --> 09:53.240\n And then one of the things that we had to figure out\n\n09:53.240 --> 09:56.080\n back then was, and it's still one of these things\n\n09:56.080 --> 09:57.280\n that trips people up today\n\n09:57.280 --> 10:00.280\n is actually the coordinate system.\n\n10:00.280 --> 10:03.080\n So you get a latitude longitude\n\n10:03.080 --> 10:05.040\n and to so many decimal places,\n\n10:05.040 --> 10:07.360\n you don't really care about kind of the ellipsoid\n\n10:07.360 --> 10:09.560\n of the earth that's being used.\n\n10:09.560 --> 10:12.240\n But when you want to get to 10 centimeter\n\n10:12.240 --> 10:14.400\n or centimeter resolution,\n\n10:14.400 --> 10:18.520\n you care whether the coordinate system is NADS 83\n\n10:18.520 --> 10:23.520\n or WGS 84 or these are different ways to describe\n\n10:24.200 --> 10:26.760\n both the kind of non sphericalness of the earth,\n\n10:26.760 --> 10:31.080\n but also kind of the, I think,\n\n10:31.080 --> 10:32.080\n I can't remember which one,\n\n10:32.080 --> 10:33.600\n the tectonic shifts that are happening\n\n10:33.600 --> 10:37.000\n and how to transform the global datum as a function of that.\n\n10:37.000 --> 10:41.020\n So getting a map and then actually matching it to reality\n\n10:41.020 --> 10:42.880\n to centimeter resolution, that was kind of interesting\n\n10:42.880 --> 10:44.040\n and fun back then.\n\n10:44.040 --> 10:46.760\n So how much work was the perception doing there?\n\n10:46.760 --> 10:51.760\n So how much were you relying on localization based on maps\n\n10:52.480 --> 10:55.760\n without using perception to register to the maps?\n\n10:55.760 --> 10:58.000\n And I guess the question is how advanced\n\n10:58.000 --> 10:59.800\n was perception at that point?\n\n10:59.800 --> 11:01.960\n It's certainly behind where we are today, right?\n\n11:01.960 --> 11:05.840\n We're more than a decade since the urban challenge.\n\n11:05.840 --> 11:08.640\n But the core of it was there.\n\n11:08.640 --> 11:13.120\n That we were tracking vehicles.\n\n11:13.120 --> 11:15.640\n We had to do that at 100 plus meter range\n\n11:15.640 --> 11:18.320\n because we had to merge with other traffic.\n\n11:18.320 --> 11:21.240\n We were using, again, Bayesian estimates\n\n11:21.240 --> 11:23.860\n for state of these vehicles.\n\n11:23.860 --> 11:25.580\n We had to deal with a bunch of the problems\n\n11:25.580 --> 11:26.920\n that you think of today,\n\n11:26.920 --> 11:29.820\n of predicting where that vehicle's going to be\n\n11:29.820 --> 11:31.060\n a few seconds into the future.\n\n11:31.060 --> 11:32.380\n We had to deal with the fact\n\n11:32.380 --> 11:35.320\n that there were multiple hypotheses for that\n\n11:35.320 --> 11:37.660\n because a vehicle at an intersection might be going right\n\n11:37.660 --> 11:38.780\n or it might be going straight\n\n11:38.780 --> 11:40.620\n or it might be making a left turn.\n\n11:41.500 --> 11:44.120\n And we had to deal with the challenge of the fact\n\n11:44.120 --> 11:47.600\n that our behavior was going to impact the behavior\n\n11:47.600 --> 11:48.960\n of that other operator.\n\n11:48.960 --> 11:53.480\n And we did a lot of that in relatively naive ways,\n\n11:53.480 --> 11:54.820\n but it kind of worked.\n\n11:54.820 --> 11:57.080\n Still had to have some kind of solution.\n\n11:57.080 --> 11:59.960\n And so where does that, 10 years later,\n\n11:59.960 --> 12:01.520\n where does that take us today\n\n12:01.520 --> 12:04.260\n from that artificial city construction\n\n12:04.260 --> 12:07.000\n to real cities to the urban environment?\n\n12:07.000 --> 12:09.160\n Yeah, I think the biggest thing\n\n12:09.160 --> 12:14.160\n is that the actors are truly unpredictable.\n\n12:15.720 --> 12:18.800\n That most of the time, the drivers on the road,\n\n12:18.800 --> 12:23.800\n the other road users are out there behaving well,\n\n12:24.080 --> 12:25.880\n but every once in a while they're not.\n\n12:27.080 --> 12:32.080\n The variety of other vehicles is, you have all of them.\n\n12:32.080 --> 12:35.840\n In terms of behavior, in terms of perception, or both?\n\n12:35.840 --> 12:36.680\n Both.\n\n12:38.740 --> 12:40.520\n Back then we didn't have to deal with cyclists,\n\n12:40.520 --> 12:42.800\n we didn't have to deal with pedestrians,\n\n12:42.800 --> 12:44.800\n didn't have to deal with traffic lights.\n\n12:46.260 --> 12:49.400\n The scale over which that you have to operate is now\n\n12:49.400 --> 12:51.120\n is much larger than the air base\n\n12:51.120 --> 12:52.720\n that we were thinking about back then.\n\n12:52.720 --> 12:55.420\n So what, easy question,\n\n12:56.280 --> 12:59.720\n what do you think is the hardest part about driving?\n\n12:59.720 --> 13:00.560\n Easy question.\n\n13:00.560 --> 13:02.560\n Yeah, no, I'm joking.\n\n13:02.560 --> 13:07.440\n I'm sure nothing really jumps out at you as one thing,\n\n13:07.440 --> 13:12.440\n but in the jump from the urban challenge to the real world,\n\n13:12.920 --> 13:15.320\n is there something that's a particular,\n\n13:15.320 --> 13:18.480\n you foresee as very serious, difficult challenge?\n\n13:18.480 --> 13:21.080\n I think the most fundamental difference\n\n13:21.080 --> 13:25.340\n is that we're doing it for real.\n\n13:26.760 --> 13:28.960\n That in that environment,\n\n13:28.960 --> 13:31.880\n it was both a limited complexity environment\n\n13:31.880 --> 13:33.240\n because certain actors weren't there,\n\n13:33.240 --> 13:35.380\n because the roads were maintained,\n\n13:35.380 --> 13:37.360\n there were barriers keeping people separate\n\n13:37.360 --> 13:39.400\n from robots at the time,\n\n13:40.840 --> 13:43.300\n and it only had to work for 60 miles.\n\n13:43.300 --> 13:46.160\n Which, looking at it from 2006,\n\n13:46.160 --> 13:48.960\n it had to work for 60 miles, right?\n\n13:48.960 --> 13:50.940\n Looking at it from now,\n\n13:51.880 --> 13:53.720\n we want things that will go and drive\n\n13:53.720 --> 13:57.160\n for half a million miles,\n\n13:57.160 --> 14:00.020\n and it's just a different game.\n\n14:00.940 --> 14:03.480\n So how important,\n\n14:03.480 --> 14:06.080\n you said LiDAR came into the game early on,\n\n14:06.080 --> 14:07.880\n and it's really the primary driver\n\n14:07.880 --> 14:10.240\n of autonomous vehicles today as a sensor.\n\n14:10.240 --> 14:11.920\n So how important is the role of LiDAR\n\n14:11.920 --> 14:14.800\n in the sensor suite in the near term?\n\n14:14.800 --> 14:16.740\n So I think it's essential.\n\n14:17.920 --> 14:20.480\n I believe, but I also believe that cameras are essential,\n\n14:20.480 --> 14:22.120\n and I believe the radar is essential.\n\n14:22.120 --> 14:26.280\n I think that you really need to use\n\n14:26.280 --> 14:28.720\n the composition of data from these different sensors\n\n14:28.720 --> 14:32.640\n if you want the thing to really be robust.\n\n14:32.640 --> 14:34.360\n The question I wanna ask,\n\n14:34.360 --> 14:35.600\n let's see if we can untangle it,\n\n14:35.600 --> 14:39.320\n is what are your thoughts on the Elon Musk\n\n14:39.320 --> 14:42.340\n provocative statement that LiDAR is a crutch,\n\n14:42.340 --> 14:47.340\n that it's a kind of, I guess, growing pains,\n\n14:47.760 --> 14:49.920\n and that much of the perception task\n\n14:49.920 --> 14:52.120\n can be done with cameras?\n\n14:52.120 --> 14:55.440\n So I think it is undeniable\n\n14:55.440 --> 14:59.360\n that people walk around without lasers in their foreheads,\n\n14:59.360 --> 15:01.880\n and they can get into vehicles and drive them,\n\n15:01.880 --> 15:05.600\n and so there's an existence proof\n\n15:05.600 --> 15:09.600\n that you can drive using passive vision.\n\n15:10.880 --> 15:12.720\n No doubt, can't argue with that.\n\n15:12.720 --> 15:14.680\n In terms of sensors, yeah, so there's proof.\n\n15:14.680 --> 15:16.000\n Yeah, in terms of sensors, right?\n\n15:16.000 --> 15:20.200\n So there's an example that we all go do it,\n\n15:20.200 --> 15:21.380\n many of us every day.\n\n15:21.380 --> 15:26.380\n In terms of LiDAR being a crutch, sure.\n\n15:28.180 --> 15:33.100\n But in the same way that the combustion engine\n\n15:33.100 --> 15:35.260\n was a crutch on the path to an electric vehicle,\n\n15:35.260 --> 15:39.300\n in the same way that any technology ultimately gets\n\n15:40.840 --> 15:44.380\n replaced by some superior technology in the future,\n\n15:44.380 --> 15:47.740\n and really the way that I look at this\n\n15:47.740 --> 15:51.460\n is that the way we get around on the ground,\n\n15:51.460 --> 15:53.920\n the way that we use transportation is broken,\n\n15:55.280 --> 15:59.740\n and that we have this, I think the number I saw this morning,\n\n15:59.740 --> 16:04.060\n 37,000 Americans killed last year on our roads,\n\n16:04.060 --> 16:05.380\n and that's just not acceptable.\n\n16:05.380 --> 16:09.460\n And so any technology that we can bring to bear\n\n16:09.460 --> 16:12.860\n that accelerates this self driving technology\n\n16:12.860 --> 16:14.640\n coming to market and saving lives\n\n16:14.640 --> 16:17.320\n is technology we should be using.\n\n16:18.280 --> 16:20.840\n And it feels just arbitrary to say,\n\n16:20.840 --> 16:25.840\n well, I'm not okay with using lasers\n\n16:26.240 --> 16:27.820\n because that's whatever,\n\n16:27.820 --> 16:30.720\n but I am okay with using an eight megapixel camera\n\n16:30.720 --> 16:32.880\n or a 16 megapixel camera.\n\n16:32.880 --> 16:34.640\n These are just bits of technology,\n\n16:34.640 --> 16:36.360\n and we should be taking the best technology\n\n16:36.360 --> 16:41.360\n from the tool bin that allows us to go and solve a problem.\n\n16:41.360 --> 16:45.160\n The question I often talk to, well, obviously you do as well,\n\n16:45.160 --> 16:48.280\n to sort of automotive companies,\n\n16:48.280 --> 16:51.360\n and if there's one word that comes up more often\n\n16:51.360 --> 16:55.280\n than anything, it's cost, and trying to drive costs down.\n\n16:55.280 --> 17:00.280\n So while it's true that it's a tragic number, the 37,000,\n\n17:01.400 --> 17:04.880\n the question is, and I'm not the one asking this question\n\n17:04.880 --> 17:05.820\n because I hate this question,\n\n17:05.820 --> 17:09.960\n but we want to find the cheapest sensor suite\n\n17:09.960 --> 17:13.280\n that creates a safe vehicle.\n\n17:13.280 --> 17:18.220\n So in that uncomfortable trade off,\n\n17:18.220 --> 17:23.220\n do you foresee LiDAR coming down in cost in the future,\n\n17:23.680 --> 17:26.680\n or do you see a day where level four autonomy\n\n17:26.680 --> 17:29.880\n is possible without LiDAR?\n\n17:29.880 --> 17:32.880\n I see both of those, but it's really a matter of time.\n\n17:32.880 --> 17:36.040\n And I think really, maybe I would talk to the question\n\n17:36.040 --> 17:37.840\n you asked about the cheapest sensor.\n\n17:37.840 --> 17:40.360\n I don't think that's actually what you want.\n\n17:40.360 --> 17:45.360\n What you want is a sensor suite that is economically viable.\n\n17:45.680 --> 17:49.440\n And then after that, everything is about margin\n\n17:49.440 --> 17:52.120\n and driving costs out of the system.\n\n17:52.120 --> 17:55.360\n What you also want is a sensor suite that works.\n\n17:55.360 --> 17:58.200\n And so it's great to tell a story about\n\n17:59.600 --> 18:03.260\n how it would be better to have a self driving system\n\n18:03.260 --> 18:08.040\n with a $50 sensor instead of a $500 sensor.\n\n18:08.040 --> 18:10.520\n But if the $500 sensor makes it work\n\n18:10.520 --> 18:14.760\n and the $50 sensor doesn't work, who cares?\n\n18:15.680 --> 18:20.020\n So long as you can actually have an economic opportunity,\n\n18:20.020 --> 18:21.520\n there's an economic opportunity there.\n\n18:21.520 --> 18:23.760\n And the economic opportunity is important\n\n18:23.760 --> 18:27.760\n because that's how you actually have a sustainable business\n\n18:27.760 --> 18:31.120\n and that's how you can actually see this come to scale\n\n18:31.120 --> 18:32.400\n and be out in the world.\n\n18:32.400 --> 18:34.780\n And so when I look at LiDAR,\n\n18:35.960 --> 18:38.880\n I see a technology that has no underlying\n\n18:38.880 --> 18:42.420\n fundamentally expense to it, fundamental expense to it.\n\n18:42.420 --> 18:46.080\n It's going to be more expensive than an imager\n\n18:46.080 --> 18:50.360\n because CMOS processes or FAP processes\n\n18:51.360 --> 18:55.080\n are dramatically more scalable than mechanical processes.\n\n18:56.200 --> 18:58.320\n But we still should be able to drive costs down\n\n18:58.320 --> 19:00.120\n substantially on that side.\n\n19:00.120 --> 19:04.840\n And then I also do think that with the right business model\n\n19:05.880 --> 19:07.560\n you can absorb more,\n\n19:07.560 --> 19:09.480\n certainly more cost on the bill of materials.\n\n19:09.480 --> 19:12.600\n Yeah, if the sensor suite works, extra value is provided,\n\n19:12.600 --> 19:15.480\n thereby you don't need to drive costs down to zero.\n\n19:15.480 --> 19:17.100\n It's the basic economics.\n\n19:17.100 --> 19:18.820\n You've talked about your intuition\n\n19:18.820 --> 19:22.200\n that level two autonomy is problematic\n\n19:22.200 --> 19:25.920\n because of the human factor of vigilance,\n\n19:25.920 --> 19:28.040\n decrement, complacency, over trust and so on,\n\n19:28.040 --> 19:29.600\n just us being human.\n\n19:29.600 --> 19:31.120\n We over trust the system,\n\n19:31.120 --> 19:34.240\n we start doing even more so partaking\n\n19:34.240 --> 19:37.180\n in the secondary activities like smartphones and so on.\n\n19:38.680 --> 19:43.000\n Have your views evolved on this point in either direction?\n\n19:43.000 --> 19:44.800\n Can you speak to it?\n\n19:44.800 --> 19:47.480\n So, and I want to be really careful\n\n19:47.480 --> 19:50.380\n because sometimes this gets twisted in a way\n\n19:50.380 --> 19:53.040\n that I certainly didn't intend.\n\n19:53.040 --> 19:58.040\n So active safety systems are a really important technology\n\n19:58.040 --> 20:00.680\n that we should be pursuing and integrating into vehicles.\n\n20:02.080 --> 20:04.280\n And there's an opportunity in the near term\n\n20:04.280 --> 20:06.520\n to reduce accidents, reduce fatalities,\n\n20:06.520 --> 20:10.320\n and we should be pushing on that.\n\n20:11.960 --> 20:14.680\n Level two systems are systems\n\n20:14.680 --> 20:18.080\n where the vehicle is controlling two axes.\n\n20:18.080 --> 20:21.720\n So braking and throttle slash steering.\n\n20:23.480 --> 20:25.680\n And I think there are variants of level two systems\n\n20:25.680 --> 20:27.280\n that are supporting the driver.\n\n20:27.280 --> 20:31.080\n That absolutely we should encourage to be out there.\n\n20:31.080 --> 20:32.880\n Where I think there's a real challenge\n\n20:32.880 --> 20:37.640\n is in the human factors part around this\n\n20:37.640 --> 20:41.240\n and the misconception from the public\n\n20:41.240 --> 20:43.600\n around the capability set that that enables\n\n20:43.600 --> 20:45.640\n and the trust that they should have in it.\n\n20:46.640 --> 20:50.000\n And that is where I kind of,\n\n20:50.000 --> 20:52.920\n I'm actually incrementally more concerned\n\n20:52.920 --> 20:54.440\n around level three systems\n\n20:54.440 --> 20:58.440\n and how exactly a level two system is marketed and delivered\n\n20:58.440 --> 21:01.840\n and how much effort people have put into those human factors.\n\n21:01.840 --> 21:05.640\n So I still believe several things around this.\n\n21:05.640 --> 21:09.440\n One is people will overtrust the technology.\n\n21:09.440 --> 21:11.440\n We've seen over the last few weeks\n\n21:11.440 --> 21:14.040\n a spate of people sleeping in their Tesla.\n\n21:14.920 --> 21:19.920\n I watched an episode last night of Trevor Noah\n\n21:19.920 --> 21:23.920\n talking about this and him,\n\n21:23.920 --> 21:26.720\n this is a smart guy who has a lot of resources\n\n21:26.720 --> 21:30.720\n at his disposal describing a Tesla as a self driving car\n\n21:30.720 --> 21:33.480\n and that why shouldn't people be sleeping in their Tesla?\n\n21:33.480 --> 21:36.560\n And it's like, well, because it's not a self driving car\n\n21:36.560 --> 21:38.840\n and it is not intended to be\n\n21:38.840 --> 21:43.840\n and these people will almost certainly die at some point\n\n21:46.400 --> 21:48.040\n or hurt other people.\n\n21:48.040 --> 21:50.080\n And so we need to really be thoughtful\n\n21:50.080 --> 21:51.840\n about how that technology is described\n\n21:51.840 --> 21:53.280\n and brought to market.\n\n21:54.240 --> 21:59.240\n I also think that because of the economic challenges\n\n21:59.240 --> 22:01.240\n we were just talking about,\n\n22:01.240 --> 22:05.160\n that these level two driver assistance systems,\n\n22:05.160 --> 22:07.280\n that technology path will diverge\n\n22:07.280 --> 22:10.200\n from the technology path that we need to be on\n\n22:10.200 --> 22:14.080\n to actually deliver truly self driving vehicles,\n\n22:14.080 --> 22:16.920\n ones where you can get in it and drive it.\n\n22:16.920 --> 22:20.800\n Can get in it and sleep and have the equivalent\n\n22:20.800 --> 22:24.680\n or better safety than a human driver behind the wheel.\n\n22:24.680 --> 22:27.520\n Because again, the economics are very different\n\n22:28.480 --> 22:30.880\n in those two worlds and so that leads\n\n22:30.880 --> 22:32.800\n to divergent technology.\n\n22:32.800 --> 22:34.680\n So you just don't see the economics\n\n22:34.680 --> 22:38.560\n of gradually increasing from level two\n\n22:38.560 --> 22:41.600\n and doing so quickly enough\n\n22:41.600 --> 22:44.480\n to where it doesn't cause safety, critical safety concerns.\n\n22:44.480 --> 22:47.680\n You believe that it needs to diverge at this point\n\n22:48.680 --> 22:50.800\n into basically different routes.\n\n22:50.800 --> 22:55.560\n And really that comes back to what are those L2\n\n22:55.560 --> 22:57.080\n and L1 systems doing?\n\n22:57.080 --> 22:59.840\n And they are driver assistance functions\n\n22:59.840 --> 23:04.400\n where the people that are marketing that responsibly\n\n23:04.400 --> 23:08.000\n are being very clear and putting human factors in place\n\n23:08.000 --> 23:12.440\n such that the driver is actually responsible for the vehicle\n\n23:12.440 --> 23:15.160\n and that the technology is there to support the driver.\n\n23:15.160 --> 23:19.880\n And the safety cases that are built around those\n\n23:19.880 --> 23:24.040\n are dependent on that driver attention and attentiveness.\n\n23:24.040 --> 23:28.000\n And at that point, you can kind of give up\n\n23:29.160 --> 23:31.240\n to some degree for economic reasons,\n\n23:31.240 --> 23:33.480\n you can give up on say false negatives.\n\n23:34.800 --> 23:36.200\n And the way to think about this\n\n23:36.200 --> 23:39.320\n is for a four collision mitigation braking system,\n\n23:39.320 --> 23:43.960\n if it half the times the driver missed a vehicle\n\n23:43.960 --> 23:46.080\n in front of it, it hit the brakes\n\n23:46.080 --> 23:47.680\n and brought the vehicle to a stop,\n\n23:47.680 --> 23:51.640\n that would be an incredible, incredible advance\n\n23:51.640 --> 23:53.040\n in safety on our roads, right?\n\n23:53.040 --> 23:55.000\n That would be equivalent to seat belts.\n\n23:55.000 --> 23:56.600\n But it would mean that if that vehicle\n\n23:56.600 --> 23:59.440\n wasn't being monitored, it would hit one out of two cars.\n\n24:00.600 --> 24:05.120\n And so economically, that's a perfectly good solution\n\n24:05.120 --> 24:06.280\n for a driver assistance system.\n\n24:06.280 --> 24:07.240\n What you should do at that point,\n\n24:07.240 --> 24:09.240\n if you can get it to work 50% of the time,\n\n24:09.240 --> 24:10.520\n is drive the cost out of that\n\n24:10.520 --> 24:13.320\n so you can get it on as many vehicles as possible.\n\n24:13.320 --> 24:14.760\n But driving the cost out of it\n\n24:14.760 --> 24:18.800\n doesn't drive up performance on the false negative case.\n\n24:18.800 --> 24:21.440\n And so you'll continue to not have a technology\n\n24:21.440 --> 24:25.680\n that could really be available for a self driven vehicle.\n\n24:25.680 --> 24:28.440\n So clearly the communication,\n\n24:28.440 --> 24:31.600\n and this probably applies to all four vehicles as well,\n\n24:31.600 --> 24:34.440\n the marketing and communication\n\n24:34.440 --> 24:37.040\n of what the technology is actually capable of,\n\n24:37.040 --> 24:38.400\n how hard it is, how easy it is,\n\n24:38.400 --> 24:41.000\n all that kind of stuff is highly problematic.\n\n24:41.000 --> 24:45.640\n So say everybody in the world was perfectly communicated\n\n24:45.640 --> 24:48.400\n and were made to be completely aware\n\n24:48.400 --> 24:50.000\n of every single technology out there,\n\n24:50.000 --> 24:52.840\n what it's able to do.\n\n24:52.840 --> 24:54.120\n What's your intuition?\n\n24:54.120 --> 24:56.880\n And now we're maybe getting into philosophical ground.\n\n24:56.880 --> 25:00.000\n Is it possible to have a level two vehicle\n\n25:00.000 --> 25:03.280\n where we don't over trust it?\n\n25:04.680 --> 25:05.800\n I don't think so.\n\n25:05.800 --> 25:10.800\n If people truly understood the risks and internalized it,\n\n25:11.160 --> 25:14.320\n then sure, you could do that safely.\n\n25:14.320 --> 25:16.160\n But that's a world that doesn't exist.\n\n25:16.160 --> 25:17.520\n The people are going to,\n\n25:18.720 --> 25:20.760\n if the facts are put in front of them,\n\n25:20.760 --> 25:24.440\n they're gonna then combine that with their experience.\n\n25:24.440 --> 25:28.360\n And let's say they're using an L2 system\n\n25:28.360 --> 25:30.800\n and they go up and down the 101 every day\n\n25:30.800 --> 25:32.720\n and they do that for a month.\n\n25:32.720 --> 25:36.200\n And it just worked every day for a month.\n\n25:36.200 --> 25:39.000\n Like that's pretty compelling at that point,\n\n25:39.000 --> 25:41.800\n just even if you know the statistics,\n\n25:41.800 --> 25:43.400\n you're like, well, I don't know,\n\n25:43.400 --> 25:44.760\n maybe there's something funny about those.\n\n25:44.760 --> 25:46.920\n Maybe they're driving in difficult places.\n\n25:46.920 --> 25:49.840\n Like I've seen it with my own eyes, it works.\n\n25:49.840 --> 25:52.400\n And the problem is that that sample size that they have,\n\n25:52.400 --> 25:53.880\n so it's 30 miles up and down,\n\n25:53.880 --> 25:56.360\n so 60 miles times 30 days,\n\n25:56.360 --> 25:58.720\n so 60, 180, 1,800 miles.\n\n25:58.720 --> 26:03.280\n Like that's a drop in the bucket\n\n26:03.280 --> 26:07.640\n compared to the, what, 85 million miles between fatalities.\n\n26:07.640 --> 26:11.400\n And so they don't really have a true estimate\n\n26:11.400 --> 26:14.440\n based on their personal experience of the real risks,\n\n26:14.440 --> 26:15.640\n but they're gonna trust it anyway,\n\n26:15.640 --> 26:16.480\n because it's hard not to.\n\n26:16.480 --> 26:18.640\n It worked for a month, what's gonna change?\n\n26:18.640 --> 26:21.640\n So even if you start a perfect understanding of the system,\n\n26:21.640 --> 26:24.160\n your own experience will make it drift.\n\n26:24.160 --> 26:25.920\n I mean, that's a big concern.\n\n26:25.920 --> 26:28.160\n Over a year, over two years even,\n\n26:28.160 --> 26:29.440\n it doesn't have to be months.\n\n26:29.440 --> 26:32.920\n And I think that as this technology moves\n\n26:32.920 --> 26:37.760\n from what I would say is kind of the more technology savvy\n\n26:37.760 --> 26:40.880\n ownership group to the mass market,\n\n26:42.640 --> 26:44.600\n you may be able to have some of those folks\n\n26:44.600 --> 26:46.280\n who are really familiar with technology,\n\n26:46.280 --> 26:48.840\n they may be able to internalize it better.\n\n26:48.840 --> 26:50.800\n And your kind of immunization\n\n26:50.800 --> 26:53.360\n against this kind of false risk assessment\n\n26:53.360 --> 26:54.280\n might last longer,\n\n26:54.280 --> 26:58.680\n but as folks who aren't as savvy about that\n\n26:58.680 --> 27:00.880\n read the material and they compare that\n\n27:00.880 --> 27:02.160\n to their personal experience,\n\n27:02.160 --> 27:07.160\n I think there it's going to move more quickly.\n\n27:08.160 --> 27:11.280\n So your work, the program that you've created at Google\n\n27:11.280 --> 27:16.280\n and now at Aurora is focused more on the second path\n\n27:16.600 --> 27:18.480\n of creating full autonomy.\n\n27:18.480 --> 27:20.880\n So it's such a fascinating,\n\n27:20.880 --> 27:24.560\n I think it's one of the most interesting AI problems\n\n27:24.560 --> 27:25.600\n of the century, right?\n\n27:25.600 --> 27:28.280\n It's, I just talked to a lot of people,\n\n27:28.280 --> 27:29.440\n just regular people, I don't know,\n\n27:29.440 --> 27:31.720\n my mom, about autonomous vehicles,\n\n27:31.720 --> 27:34.520\n and you begin to grapple with ideas\n\n27:34.520 --> 27:38.080\n of giving your life control over to a machine.\n\n27:38.080 --> 27:40.040\n It's philosophically interesting,\n\n27:40.040 --> 27:41.760\n it's practically interesting.\n\n27:41.760 --> 27:43.720\n So let's talk about safety.\n\n27:43.720 --> 27:46.240\n How do you think we demonstrate,\n\n27:46.240 --> 27:47.880\n you've spoken about metrics in the past,\n\n27:47.880 --> 27:51.880\n how do you think we demonstrate to the world\n\n27:51.880 --> 27:56.160\n that an autonomous vehicle, an Aurora system is safe?\n\n27:56.160 --> 27:57.320\n This is one where it's difficult\n\n27:57.320 --> 27:59.280\n because there isn't a soundbite answer.\n\n27:59.280 --> 28:04.280\n That we have to show a combination of work\n\n28:05.960 --> 28:08.360\n that was done diligently and thoughtfully,\n\n28:08.360 --> 28:10.840\n and this is where something like a functional safety process\n\n28:10.840 --> 28:11.680\n is part of that.\n\n28:11.680 --> 28:14.360\n It's like here's the way we did the work,\n\n28:15.280 --> 28:17.160\n that means that we were very thorough.\n\n28:17.160 --> 28:20.040\n So if you believe that what we said\n\n28:20.040 --> 28:21.440\n about this is the way we did it,\n\n28:21.440 --> 28:22.720\n then you can have some confidence\n\n28:22.720 --> 28:25.200\n that we were thorough in the engineering work\n\n28:25.200 --> 28:26.920\n we put into the system.\n\n28:26.920 --> 28:28.920\n And then on top of that,\n\n28:28.920 --> 28:32.000\n to kind of demonstrate that we weren't just thorough,\n\n28:32.000 --> 28:33.800\n we were actually good at what we did,\n\n28:35.280 --> 28:38.200\n there'll be a kind of a collection of evidence\n\n28:38.200 --> 28:40.440\n in terms of demonstrating that the capabilities\n\n28:40.440 --> 28:42.920\n worked the way we thought they did,\n\n28:42.920 --> 28:45.320\n statistically and to whatever degree\n\n28:45.320 --> 28:47.280\n we can demonstrate that,\n\n28:48.160 --> 28:50.320\n both in some combination of simulations,\n\n28:50.320 --> 28:53.080\n some combination of unit testing\n\n28:53.080 --> 28:54.640\n and decomposition testing,\n\n28:54.640 --> 28:57.000\n and then some part of it will be on road data.\n\n28:58.160 --> 29:02.680\n And I think the way we'll ultimately\n\n29:02.680 --> 29:04.000\n convey this to the public\n\n29:04.000 --> 29:06.760\n is there'll be clearly some conversation\n\n29:06.760 --> 29:08.200\n with the public about it,\n\n29:08.200 --> 29:12.040\n but we'll kind of invoke the kind of the trusted nodes\n\n29:12.040 --> 29:13.880\n and that we'll spend more time\n\n29:13.880 --> 29:17.280\n being able to go into more depth with folks like NHTSA\n\n29:17.280 --> 29:19.720\n and other federal and state regulatory bodies\n\n29:19.720 --> 29:22.080\n and kind of given that they are\n\n29:22.080 --> 29:25.200\n operating in the public interest and they're trusted,\n\n29:26.240 --> 29:28.640\n that if we can show enough work to them\n\n29:28.640 --> 29:30.000\n that they're convinced,\n\n29:30.000 --> 29:33.800\n then I think we're in a pretty good place.\n\n29:33.800 --> 29:35.000\n That means you work with people\n\n29:35.000 --> 29:36.920\n that are essentially experts at safety\n\n29:36.920 --> 29:39.000\n to try to discuss and show.\n\n29:39.000 --> 29:41.720\n Do you think, the answer's probably no,\n\n29:41.720 --> 29:42.920\n but just in case,\n\n29:42.920 --> 29:44.360\n do you think there exists a metric?\n\n29:44.360 --> 29:46.320\n So currently people have been using\n\n29:46.320 --> 29:48.200\n number of disengagements.\n\n29:48.200 --> 29:50.120\n And it quickly turns into a marketing scheme\n\n29:50.120 --> 29:54.280\n to sort of you alter the experiments you run to adjust.\n\n29:54.280 --> 29:56.280\n I think you've spoken that you don't like.\n\n29:56.280 --> 29:57.120\n Don't love it.\n\n29:57.120 --> 29:59.680\n No, in fact, I was on the record telling DMV\n\n29:59.680 --> 30:01.960\n that I thought this was not a great metric.\n\n30:01.960 --> 30:05.280\n Do you think it's possible to create a metric,\n\n30:05.280 --> 30:09.440\n a number that could demonstrate safety\n\n30:09.440 --> 30:12.320\n outside of fatalities?\n\n30:12.320 --> 30:13.440\n So I do.\n\n30:13.440 --> 30:16.560\n And I think that it won't be just one number.\n\n30:17.600 --> 30:21.280\n So as we are internally grappling with this,\n\n30:21.280 --> 30:23.560\n and at some point we'll be able to talk\n\n30:23.560 --> 30:25.040\n more publicly about it,\n\n30:25.040 --> 30:28.520\n is how do we think about human performance\n\n30:28.520 --> 30:29.840\n in different tasks,\n\n30:29.840 --> 30:32.160\n say detecting traffic lights\n\n30:32.160 --> 30:36.200\n or safely making a left turn across traffic?\n\n30:37.680 --> 30:40.080\n And what do we think the failure rates are\n\n30:40.080 --> 30:42.520\n for those different capabilities for people?\n\n30:42.520 --> 30:44.760\n And then demonstrating to ourselves\n\n30:44.760 --> 30:48.480\n and then ultimately folks in the regulatory role\n\n30:48.480 --> 30:50.760\n and then ultimately the public\n\n30:50.760 --> 30:52.400\n that we have confidence that our system\n\n30:52.400 --> 30:54.760\n will work better than that.\n\n30:54.760 --> 30:57.040\n And so these individual metrics\n\n30:57.040 --> 31:00.720\n will kind of tell a compelling story ultimately.\n\n31:01.760 --> 31:03.920\n I do think at the end of the day\n\n31:03.920 --> 31:06.640\n what we care about in terms of safety\n\n31:06.640 --> 31:11.320\n is life saved and injuries reduced.\n\n31:12.160 --> 31:15.280\n And then ultimately kind of casualty dollars\n\n31:16.440 --> 31:19.360\n that people aren't having to pay to get their car fixed.\n\n31:19.360 --> 31:22.680\n And I do think that in aviation\n\n31:22.680 --> 31:25.880\n they look at a kind of an event pyramid\n\n31:25.880 --> 31:28.600\n where a crash is at the top of that\n\n31:28.600 --> 31:30.440\n and that's the worst event obviously\n\n31:30.440 --> 31:34.240\n and then there's injuries and near miss events and whatnot\n\n31:34.240 --> 31:37.320\n and violation of operating procedures\n\n31:37.320 --> 31:40.160\n and you kind of build a statistical model\n\n31:40.160 --> 31:44.440\n of the relevance of the low severity things\n\n31:44.440 --> 31:45.280\n or the high severity things.\n\n31:45.280 --> 31:46.120\n And I think that's something\n\n31:46.120 --> 31:48.200\n where we'll be able to look at as well\n\n31:48.200 --> 31:51.840\n because an event per 85 million miles\n\n31:51.840 --> 31:54.440\n is statistically a difficult thing\n\n31:54.440 --> 31:56.800\n even at the scale of the U.S.\n\n31:56.800 --> 31:59.360\n to kind of compare directly.\n\n31:59.360 --> 32:02.240\n And that event fatality that's connected\n\n32:02.240 --> 32:07.240\n to an autonomous vehicle is significantly\n\n32:07.440 --> 32:09.160\n at least currently magnified\n\n32:09.160 --> 32:12.320\n in the amount of attention it gets.\n\n32:12.320 --> 32:15.080\n So that speaks to public perception.\n\n32:15.080 --> 32:16.720\n I think the most popular topic\n\n32:16.720 --> 32:19.480\n about autonomous vehicles in the public\n\n32:19.480 --> 32:23.080\n is the trolley problem formulation, right?\n\n32:23.080 --> 32:27.000\n Which has, let's not get into that too much\n\n32:27.000 --> 32:29.600\n but is misguided in many ways.\n\n32:29.600 --> 32:32.320\n But it speaks to the fact that people are grappling\n\n32:32.320 --> 32:36.160\n with this idea of giving control over to a machine.\n\n32:36.160 --> 32:41.160\n So how do you win the hearts and minds of the people\n\n32:41.560 --> 32:44.600\n that autonomy is something that could be a part\n\n32:44.600 --> 32:45.520\n of their lives?\n\n32:45.520 --> 32:47.640\n I think you let them experience it, right?\n\n32:47.640 --> 32:50.440\n I think it's right.\n\n32:50.440 --> 32:52.800\n I think people should be skeptical.\n\n32:52.800 --> 32:55.680\n I think people should ask questions.\n\n32:55.680 --> 32:57.000\n I think they should doubt\n\n32:57.000 --> 33:00.120\n because this is something new and different.\n\n33:00.120 --> 33:01.880\n They haven't touched it yet.\n\n33:01.880 --> 33:03.640\n And I think that's perfectly reasonable.\n\n33:03.640 --> 33:07.320\n And, but at the same time,\n\n33:07.320 --> 33:09.320\n it's clear there's an opportunity to make the road safer.\n\n33:09.320 --> 33:12.440\n It's clear that we can improve access to mobility.\n\n33:12.440 --> 33:14.960\n It's clear that we can reduce the cost of mobility.\n\n33:16.640 --> 33:19.480\n And that once people try that\n\n33:19.480 --> 33:22.720\n and understand that it's safe\n\n33:22.720 --> 33:24.440\n and are able to use in their daily lives,\n\n33:24.440 --> 33:25.280\n I think it's one of these things\n\n33:25.280 --> 33:28.040\n that will just be obvious.\n\n33:28.040 --> 33:32.240\n And I've seen this practically in demonstrations\n\n33:32.240 --> 33:35.560\n that I've given where I've had people come in\n\n33:35.560 --> 33:38.840\n and they're very skeptical.\n\n33:38.840 --> 33:40.440\n Again, in a vehicle, my favorite one\n\n33:40.440 --> 33:42.560\n is taking somebody out on the freeway\n\n33:42.560 --> 33:46.000\n and we're on the 101 driving at 65 miles an hour.\n\n33:46.000 --> 33:48.400\n And after 10 minutes, they kind of turn and ask,\n\n33:48.400 --> 33:49.480\n is that all it does?\n\n33:49.480 --> 33:52.080\n And you're like, it's a self driving car.\n\n33:52.080 --> 33:54.840\n I'm not sure exactly what you thought it would do, right?\n\n33:54.840 --> 33:57.920\n But it becomes mundane,\n\n33:58.840 --> 34:01.480\n which is exactly what you want a technology\n\n34:01.480 --> 34:02.720\n like this to be, right?\n\n34:02.720 --> 34:07.280\n We don't really, when I turn the light switch on in here,\n\n34:07.280 --> 34:12.000\n I don't think about the complexity of those electrons\n\n34:12.000 --> 34:14.200\n being pushed down a wire from wherever it was\n\n34:14.200 --> 34:15.240\n and being generated.\n\n34:15.240 --> 34:19.080\n It's like, I just get annoyed if it doesn't work, right?\n\n34:19.080 --> 34:21.400\n And what I value is the fact\n\n34:21.400 --> 34:23.080\n that I can do other things in this space.\n\n34:23.080 --> 34:24.560\n I can see my colleagues.\n\n34:24.560 --> 34:26.160\n I can read stuff on a paper.\n\n34:26.160 --> 34:29.200\n I can not be afraid of the dark.\n\n34:30.360 --> 34:33.320\n And I think that's what we want this technology to be like\n\n34:33.320 --> 34:34.640\n is it's in the background\n\n34:34.640 --> 34:37.120\n and people get to have those life experiences\n\n34:37.120 --> 34:38.440\n and do so safely.\n\n34:38.440 --> 34:42.160\n So putting this technology in the hands of people\n\n34:42.160 --> 34:46.320\n speaks to scale of deployment, right?\n\n34:46.320 --> 34:50.880\n So what do you think the dreaded question about the future\n\n34:50.880 --> 34:53.560\n because nobody can predict the future,\n\n34:53.560 --> 34:57.240\n but just maybe speak poetically\n\n34:57.240 --> 35:00.880\n about when do you think we'll see a large scale deployment\n\n35:00.880 --> 35:05.880\n of autonomous vehicles, 10,000, those kinds of numbers?\n\n35:06.680 --> 35:08.240\n We'll see that within 10 years.\n\n35:09.240 --> 35:10.240\n I'm pretty confident.\n\n35:14.040 --> 35:16.040\n What's an impressive scale?\n\n35:16.040 --> 35:19.200\n What moment, so you've done the DARPA challenge\n\n35:19.200 --> 35:20.440\n where there's one vehicle.\n\n35:20.440 --> 35:23.960\n At which moment does it become, wow, this is serious scale?\n\n35:23.960 --> 35:26.520\n So I think the moment it gets serious\n\n35:26.520 --> 35:31.520\n is when we really do have a driverless vehicle\n\n35:32.240 --> 35:34.120\n operating on public roads\n\n35:35.000 --> 35:37.960\n and that we can do that kind of continuously.\n\n35:37.960 --> 35:38.880\n Without a safety driver.\n\n35:38.880 --> 35:40.440\n Without a safety driver in the vehicle.\n\n35:40.440 --> 35:41.560\n I think at that moment,\n\n35:41.560 --> 35:44.400\n we've kind of crossed the zero to one threshold.\n\n35:45.920 --> 35:50.200\n And then it is about how do we continue to scale that?\n\n35:50.200 --> 35:53.960\n How do we build the right business models?\n\n35:53.960 --> 35:56.320\n How do we build the right customer experience around it\n\n35:56.320 --> 35:59.960\n so that it is actually a useful product out in the world?\n\n36:00.960 --> 36:03.600\n And I think that is really,\n\n36:03.600 --> 36:05.920\n at that point it moves from\n\n36:05.920 --> 36:09.200\n what is this kind of mixed science engineering project\n\n36:09.200 --> 36:12.360\n into engineering and commercialization\n\n36:12.360 --> 36:15.840\n and really starting to deliver on the value\n\n36:15.840 --> 36:20.680\n that we all see here and actually making that real in the world.\n\n36:20.680 --> 36:22.240\n What do you think that deployment looks like?\n\n36:22.240 --> 36:26.440\n Where do we first see the inkling of no safety driver,\n\n36:26.440 --> 36:28.600\n one or two cars here and there?\n\n36:28.600 --> 36:29.800\n Is it on the highway?\n\n36:29.800 --> 36:33.160\n Is it in specific routes in the urban environment?\n\n36:33.160 --> 36:36.920\n I think it's gonna be urban, suburban type environments.\n\n36:37.880 --> 36:41.560\n Yeah, with Aurora, when we thought about how to tackle this,\n\n36:41.560 --> 36:45.040\n it was kind of in vogue to think about trucking\n\n36:46.040 --> 36:47.800\n as opposed to urban driving.\n\n36:47.800 --> 36:51.280\n And again, the human intuition around this\n\n36:51.280 --> 36:55.400\n is that freeways are easier to drive on\n\n36:57.080 --> 36:59.280\n because everybody's kind of going in the same direction\n\n36:59.280 --> 37:01.560\n and lanes are a little wider, et cetera.\n\n37:01.560 --> 37:03.320\n And I think that that intuition is pretty good,\n\n37:03.320 --> 37:06.040\n except we don't really care about most of the time.\n\n37:06.040 --> 37:08.400\n We care about all of the time.\n\n37:08.400 --> 37:10.880\n And when you're driving on a freeway with a truck,\n\n37:10.880 --> 37:13.440\n say 70 miles an hour,\n\n37:14.600 --> 37:16.240\n and you've got 70,000 pound load with you,\n\n37:16.240 --> 37:18.880\n that's just an incredible amount of kinetic energy.\n\n37:18.880 --> 37:21.440\n And so when that goes wrong, it goes really wrong.\n\n37:22.640 --> 37:27.640\n And those challenges that you see occur more rarely,\n\n37:27.800 --> 37:31.120\n so you don't get to learn as quickly.\n\n37:31.120 --> 37:34.720\n And they're incrementally more difficult than urban driving,\n\n37:34.720 --> 37:37.440\n but they're not easier than urban driving.\n\n37:37.440 --> 37:41.640\n And so I think this happens in moderate speed\n\n37:41.640 --> 37:45.280\n urban environments because if two vehicles crash\n\n37:45.280 --> 37:48.120\n at 25 miles per hour, it's not good,\n\n37:48.120 --> 37:50.120\n but probably everybody walks away.\n\n37:51.080 --> 37:53.720\n And those events where there's the possibility\n\n37:53.720 --> 37:55.800\n for that occurring happen frequently.\n\n37:55.800 --> 37:58.000\n So we get to learn more rapidly.\n\n37:58.000 --> 38:01.360\n We get to do that with lower risk for everyone.\n\n38:02.520 --> 38:04.360\n And then we can deliver value to people\n\n38:04.360 --> 38:05.880\n that need to get from one place to another.\n\n38:05.880 --> 38:08.160\n And once we've got that solved,\n\n38:08.160 --> 38:11.320\n then the freeway driving part of this just falls out.\n\n38:11.320 --> 38:13.080\n But we're able to learn more safely,\n\n38:13.080 --> 38:15.200\n more quickly in the urban environment.\n\n38:15.200 --> 38:18.760\n So 10 years and then scale 20, 30 year,\n\n38:18.760 --> 38:22.040\n who knows if a sufficiently compelling experience\n\n38:22.040 --> 38:24.400\n is created, it could be faster and slower.\n\n38:24.400 --> 38:27.160\n Do you think there could be breakthroughs\n\n38:27.160 --> 38:29.920\n and what kind of breakthroughs might there be\n\n38:29.920 --> 38:32.400\n that completely change that timeline?\n\n38:32.400 --> 38:35.360\n Again, not only am I asking you to predict the future,\n\n38:35.360 --> 38:37.360\n I'm asking you to predict breakthroughs\n\n38:37.360 --> 38:38.360\n that haven't happened yet.\n\n38:38.360 --> 38:41.440\n So what's the, I think another way to ask that\n\n38:41.440 --> 38:44.320\n would be if I could wave a magic wand,\n\n38:44.320 --> 38:46.720\n what part of the system would I make work today\n\n38:46.720 --> 38:49.480\n to accelerate it as quickly as possible?\n\n38:52.120 --> 38:54.200\n Don't say infrastructure, please don't say infrastructure.\n\n38:54.200 --> 38:56.320\n No, it's definitely not infrastructure.\n\n38:56.320 --> 39:00.600\n It's really that perception forecasting capability.\n\n39:00.600 --> 39:04.840\n So if tomorrow you could give me a perfect model\n\n39:04.840 --> 39:06.960\n of what's happened, what is happening\n\n39:06.960 --> 39:09.200\n and what will happen for the next five seconds\n\n39:10.360 --> 39:13.040\n around a vehicle on the roadway,\n\n39:13.040 --> 39:15.360\n that would accelerate things pretty dramatically.\n\n39:15.360 --> 39:17.600\n Are you, in terms of staying up at night,\n\n39:17.600 --> 39:21.760\n are you mostly bothered by cars, pedestrians or cyclists?\n\n39:21.760 --> 39:25.960\n So I worry most about the vulnerable road users\n\n39:25.960 --> 39:28.480\n about the combination of cyclists and cars, right?\n\n39:28.480 --> 39:31.960\n Or cyclists and pedestrians because they're not in armor.\n\n39:31.960 --> 39:36.480\n The cars, they're bigger, they've got protection\n\n39:36.480 --> 39:39.440\n for the people and so the ultimate risk is lower there.\n\n39:41.080 --> 39:43.240\n Whereas a pedestrian or a cyclist,\n\n39:43.240 --> 39:46.480\n they're out on the road and they don't have any protection\n\n39:46.480 --> 39:49.720\n and so we need to pay extra attention to that.\n\n39:49.720 --> 39:54.120\n Do you think about a very difficult technical challenge\n\n39:55.720 --> 39:58.520\n of the fact that pedestrians,\n\n39:58.520 --> 40:00.240\n if you try to protect pedestrians\n\n40:00.240 --> 40:04.560\n by being careful and slow, they'll take advantage of that.\n\n40:04.560 --> 40:09.040\n So the game theoretic dance, does that worry you\n\n40:09.040 --> 40:12.480\n of how, from a technical perspective, how we solve that?\n\n40:12.480 --> 40:14.560\n Because as humans, the way we solve that\n\n40:14.560 --> 40:17.240\n is kind of nudge our way through the pedestrians\n\n40:17.240 --> 40:20.000\n which doesn't feel, from a technical perspective,\n\n40:20.000 --> 40:22.300\n as a appropriate algorithm.\n\n40:23.200 --> 40:25.920\n But do you think about how we solve that problem?\n\n40:25.920 --> 40:30.920\n Yeah, I think there's two different concepts there.\n\n40:31.360 --> 40:35.820\n So one is, am I worried that because these vehicles\n\n40:35.820 --> 40:37.600\n are self driving, people will kind of step in the road\n\n40:37.600 --> 40:38.640\n and take advantage of them?\n\n40:38.640 --> 40:43.640\n And I've heard this and I don't really believe it\n\n40:43.760 --> 40:45.960\n because if I'm driving down the road\n\n40:45.960 --> 40:48.400\n and somebody steps in front of me, I'm going to stop.\n\n40:50.600 --> 40:53.660\n Even if I'm annoyed, I'm not gonna just drive\n\n40:53.660 --> 40:56.400\n through a person stood in the road.\n\n40:56.400 --> 41:00.400\n And so I think today people can take advantage of this\n\n41:00.400 --> 41:02.560\n and you do see some people do it.\n\n41:02.560 --> 41:04.180\n I guess there's an incremental risk\n\n41:04.180 --> 41:05.880\n because maybe they have lower confidence\n\n41:05.880 --> 41:07.720\n that I'm gonna see them than they might have\n\n41:07.720 --> 41:10.400\n for an automated vehicle and so maybe that shifts\n\n41:10.400 --> 41:12.040\n it a little bit.\n\n41:12.040 --> 41:14.360\n But I think people don't wanna get hit by cars.\n\n41:14.360 --> 41:17.080\n And so I think that I'm not that worried\n\n41:17.080 --> 41:18.760\n about people walking out of the 101\n\n41:18.760 --> 41:23.760\n and creating chaos more than they would today.\n\n41:24.400 --> 41:27.040\n Regarding kind of the nudging through a big stream\n\n41:27.040 --> 41:30.040\n of pedestrians leaving a concert or something,\n\n41:30.040 --> 41:33.520\n I think that is further down the technology pipeline.\n\n41:33.520 --> 41:36.960\n I think that you're right, that's tricky.\n\n41:36.960 --> 41:38.620\n I don't think it's necessarily,\n\n41:40.360 --> 41:43.600\n I think the algorithm people use for this is pretty simple.\n\n41:43.600 --> 41:44.800\n It's kind of just move forward slowly\n\n41:44.800 --> 41:46.800\n and if somebody's really close then stop.\n\n41:46.800 --> 41:50.880\n And I think that that probably can be replicated\n\n41:50.880 --> 41:54.040\n pretty easily and particularly given that\n\n41:54.040 --> 41:55.720\n you don't do this at 30 miles an hour,\n\n41:55.720 --> 41:59.080\n you do it at one, that even in those situations\n\n41:59.080 --> 42:01.200\n the risk is relatively minimal.\n\n42:01.200 --> 42:03.640\n But it's not something we're thinking about\n\n42:03.640 --> 42:04.560\n in any serious way.\n\n42:04.560 --> 42:07.920\n And probably that's less an algorithm problem\n\n42:07.920 --> 42:10.160\n and more creating a human experience.\n\n42:10.160 --> 42:14.300\n So the HCI people that create a visual display\n\n42:14.300 --> 42:16.260\n that you're pleasantly as a pedestrian\n\n42:16.260 --> 42:20.760\n nudged out of the way, that's an experience problem,\n\n42:20.760 --> 42:22.000\n not an algorithm problem.\n\n42:22.880 --> 42:25.480\n Who's the main competitor to Aurora today?\n\n42:25.480 --> 42:28.640\n And how do you outcompete them in the long run?\n\n42:28.640 --> 42:31.200\n So we really focus a lot on what we're doing here.\n\n42:31.200 --> 42:34.480\n I think that, I've said this a few times,\n\n42:34.480 --> 42:37.960\n that this is a huge difficult problem\n\n42:37.960 --> 42:40.320\n and it's great that a bunch of companies are tackling it\n\n42:40.320 --> 42:42.320\n because I think it's so important for society\n\n42:42.320 --> 42:43.800\n that somebody gets there.\n\n42:43.800 --> 42:48.800\n So we don't spend a whole lot of time\n\n42:49.120 --> 42:51.600\n thinking tactically about who's out there\n\n42:51.600 --> 42:55.240\n and how do we beat that person individually.\n\n42:55.240 --> 42:58.720\n What are we trying to do to go faster ultimately?\n\n42:59.760 --> 43:02.640\n Well part of it is the leadership team we have\n\n43:02.640 --> 43:04.200\n has got pretty tremendous experience.\n\n43:04.200 --> 43:06.440\n And so we kind of understand the landscape\n\n43:06.440 --> 43:09.160\n and understand where the cul de sacs are to some degree\n\n43:09.160 --> 43:10.980\n and we try and avoid those.\n\n43:10.980 --> 43:14.260\n I think there's a part of it,\n\n43:14.260 --> 43:16.260\n just this great team we've built.\n\n43:16.260 --> 43:19.080\n People, this is a technology and a company\n\n43:19.080 --> 43:22.320\n that people believe in the mission of\n\n43:22.320 --> 43:23.740\n and so it allows us to attract\n\n43:23.740 --> 43:25.740\n just awesome people to go work.\n\n43:26.800 --> 43:29.320\n We've got a culture I think that people appreciate\n\n43:29.320 --> 43:30.460\n that allows them to focus,\n\n43:30.460 --> 43:33.120\n allows them to really spend time solving problems.\n\n43:33.120 --> 43:35.900\n And I think that keeps them energized.\n\n43:35.900 --> 43:38.940\n And then we've invested hard,\n\n43:38.940 --> 43:43.500\n invested heavily in the infrastructure\n\n43:43.500 --> 43:46.540\n and architectures that we think will ultimately accelerate us.\n\n43:46.540 --> 43:50.660\n So because of the folks we're able to bring in early on,\n\n43:50.660 --> 43:53.540\n because of the great investors we have,\n\n43:53.540 --> 43:56.780\n we don't spend all of our time doing demos\n\n43:56.780 --> 43:58.660\n and kind of leaping from one demo to the next.\n\n43:58.660 --> 44:02.820\n We've been given the freedom to invest in\n\n44:03.940 --> 44:05.500\n infrastructure to do machine learning,\n\n44:05.500 --> 44:08.600\n infrastructure to pull data from our on road testing,\n\n44:08.600 --> 44:11.500\n infrastructure to use that to accelerate engineering.\n\n44:11.500 --> 44:14.480\n And I think that early investment\n\n44:14.480 --> 44:17.340\n and continuing investment in those kind of tools\n\n44:17.340 --> 44:19.780\n will ultimately allow us to accelerate\n\n44:19.780 --> 44:21.940\n and do something pretty incredible.\n\n44:21.940 --> 44:23.420\n Chris, beautifully put.\n\n44:23.420 --> 44:24.660\n It's a good place to end.\n\n44:24.660 --> 44:26.500\n Thank you so much for talking today.\n\n44:26.500 --> 44:47.940\n Thank you very much. Really enjoyed it.\n\n"
}