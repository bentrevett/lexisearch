{
  "title": "Michael Kearns: Algorithmic Fairness, Privacy & Ethics | Lex Fridman Podcast #50",
  "id": "AzdxbzHtjgs",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:03.120\n The following is a conversation with Michael Kearns.\n\n00:03.120 --> 00:06.240\n He's a professor at the University of Pennsylvania\n\n00:06.240 --> 00:09.520\n and a coauthor of the new book, Ethical Algorithm,\n\n00:09.520 --> 00:11.840\n that is the focus of much of this conversation.\n\n00:12.640 --> 00:16.640\n It includes algorithmic fairness, bias, privacy,\n\n00:16.640 --> 00:18.080\n and ethics in general.\n\n00:18.080 --> 00:20.000\n But that is just one of many fields\n\n00:20.000 --> 00:22.480\n that Michael is a world class researcher in,\n\n00:22.480 --> 00:24.880\n some of which we touch on quickly,\n\n00:24.880 --> 00:26.240\n including learning theory\n\n00:26.240 --> 00:29.120\n or the theoretical foundation of machine learning,\n\n00:29.120 --> 00:31.280\n game theory, quantitative finance,\n\n00:31.280 --> 00:33.440\n computational social science, and much more.\n\n00:34.000 --> 00:35.600\n But on a personal note,\n\n00:35.600 --> 00:38.320\n when I was an undergrad, early on,\n\n00:38.320 --> 00:39.520\n I worked with Michael\n\n00:39.520 --> 00:41.280\n on an algorithmic trading project\n\n00:41.280 --> 00:42.560\n and competition that he led.\n\n00:43.120 --> 00:44.720\n That's when I first fell in love\n\n00:44.720 --> 00:46.160\n with algorithmic game theory.\n\n00:46.800 --> 00:48.400\n While most of my research life\n\n00:48.400 --> 00:49.520\n has been in machine learning\n\n00:49.520 --> 00:50.880\n and human robot interaction,\n\n00:51.440 --> 00:53.600\n the systematic way that game theory\n\n00:53.600 --> 00:55.120\n reveals the beautiful structure\n\n00:55.120 --> 00:58.480\n in our competitive and cooperating world of humans\n\n00:58.480 --> 01:00.480\n has been a continued inspiration to me.\n\n01:01.120 --> 01:03.520\n So for that and other things,\n\n01:03.520 --> 01:05.600\n I'm deeply thankful to Michael\n\n01:05.600 --> 01:07.920\n and really enjoyed having this conversation\n\n01:07.920 --> 01:10.160\n again in person after so many years.\n\n01:11.040 --> 01:13.760\n This is the Artificial Intelligence Podcast.\n\n01:13.760 --> 01:16.080\n If you enjoy it, subscribe on YouTube,\n\n01:16.080 --> 01:18.320\n give it five stars on Apple Podcast,\n\n01:18.320 --> 01:19.760\n support on Patreon,\n\n01:19.760 --> 01:21.680\n or simply connect with me on Twitter\n\n01:21.680 --> 01:24.800\n at Lex Friedman, spelled F R I D M A N.\n\n01:25.760 --> 01:27.280\n This episode is supported\n\n01:27.280 --> 01:30.320\n by an amazing podcast called Pessimists Archive.\n\n01:31.040 --> 01:32.720\n Jason, the host of the show,\n\n01:32.720 --> 01:35.360\n reached out to me looking to support this podcast,\n\n01:35.360 --> 01:37.600\n and so I listened to it, to check it out.\n\n01:38.160 --> 01:40.720\n And by listened, I mean I went through it,\n\n01:40.720 --> 01:43.680\n Netflix binge style, at least five episodes in a row.\n\n01:44.800 --> 01:46.480\n It's not one of my favorite podcasts,\n\n01:46.480 --> 01:48.720\n and I think it should be one of the top podcasts\n\n01:48.720 --> 01:49.680\n in the world, frankly.\n\n01:50.480 --> 01:51.760\n It's a history show\n\n01:51.760 --> 01:54.080\n about why people resist new things.\n\n01:54.080 --> 01:56.400\n Each episode looks at a moment in history\n\n01:56.400 --> 01:59.040\n when something new was introduced,\n\n01:59.040 --> 02:01.680\n something that today we think of as commonplace,\n\n02:01.680 --> 02:04.560\n like recorded music, umbrellas, bicycles, cars,\n\n02:04.560 --> 02:06.400\n chess, coffee, the elevator,\n\n02:06.960 --> 02:09.600\n and the show explores why it freaked everyone out.\n\n02:10.160 --> 02:12.800\n The latest episode on mirrors and vanity\n\n02:12.800 --> 02:15.360\n still stays with me as I think about vanity\n\n02:15.360 --> 02:17.680\n in the modern day of the Twitter world.\n\n02:18.640 --> 02:20.880\n That's the fascinating thing about the show,\n\n02:20.880 --> 02:22.640\n is that stuff that happened long ago,\n\n02:22.640 --> 02:25.040\n especially in terms of our fear of new things,\n\n02:25.040 --> 02:26.880\n repeats itself in the modern day,\n\n02:26.880 --> 02:29.280\n and so has many lessons for us to think about\n\n02:29.280 --> 02:31.040\n in terms of human psychology\n\n02:31.040 --> 02:33.120\n and the role of technology in our society.\n\n02:34.160 --> 02:35.600\n Anyway, you should subscribe\n\n02:35.600 --> 02:37.200\n and listen to Pessimist Archive.\n\n02:37.760 --> 02:39.200\n I highly recommend it.\n\n02:39.840 --> 02:43.680\n And now, here's my conversation with Michael Kearns.\n\n02:44.880 --> 02:47.840\n You mentioned reading Fear and Loathing in Las Vegas\n\n02:47.840 --> 02:49.840\n in high school, and having a more,\n\n02:50.800 --> 02:52.560\n or a bit more of a literary mind.\n\n02:52.560 --> 02:56.160\n So, what books, non technical, non computer science,\n\n02:56.880 --> 02:59.360\n would you say had the biggest impact on your life,\n\n02:59.360 --> 03:02.400\n either intellectually or emotionally?\n\n03:02.400 --> 03:04.640\n You've dug deep into my history, I see.\n\n03:04.640 --> 03:05.680\n Went deep.\n\n03:05.680 --> 03:08.240\n Yeah, I think, well, my favorite novel is\n\n03:08.240 --> 03:10.960\n Infinite Jest by David Foster Wallace,\n\n03:10.960 --> 03:13.360\n which actually, coincidentally,\n\n03:13.360 --> 03:15.840\n much of it takes place in the halls of buildings\n\n03:15.840 --> 03:17.200\n right around us here at MIT.\n\n03:18.000 --> 03:20.000\n So that certainly had a big influence on me.\n\n03:20.000 --> 03:22.320\n And as you noticed, like, when I was in high school,\n\n03:22.320 --> 03:25.440\n I actually even started college as an English major.\n\n03:25.440 --> 03:29.120\n So, I was very influenced by sort of that genre of journalism\n\n03:29.120 --> 03:30.880\n at the time, and thought I wanted to be a writer,\n\n03:30.880 --> 03:33.840\n and then realized that an English major teaches you to read,\n\n03:33.840 --> 03:35.360\n but it doesn't teach you how to write,\n\n03:35.360 --> 03:36.960\n and then I became interested in math\n\n03:36.960 --> 03:38.560\n and computer science instead.\n\n03:38.560 --> 03:41.040\n Well, in your new book, Ethical Algorithm,\n\n03:41.600 --> 03:45.920\n you kind of sneak up from an algorithmic perspective\n\n03:45.920 --> 03:48.400\n on these deep, profound philosophical questions\n\n03:48.400 --> 03:53.520\n of fairness, of privacy.\n\n03:55.120 --> 03:56.400\n In thinking about these topics,\n\n03:56.400 --> 04:01.440\n how often do you return to that literary mind that you had?\n\n04:01.440 --> 04:04.320\n Yeah, I'd like to claim there was a deeper connection,\n\n04:05.200 --> 04:07.920\n but, you know, I think both Aaron and I\n\n04:07.920 --> 04:10.320\n kind of came at these topics first and foremost\n\n04:10.320 --> 04:11.280\n from a technical angle.\n\n04:11.280 --> 04:14.720\n I mean, you know, I kind of consider myself primarily\n\n04:14.720 --> 04:17.600\n and originally a machine learning researcher,\n\n04:17.600 --> 04:20.400\n and I think as we just watched, like the rest of the society,\n\n04:20.400 --> 04:23.440\n the field technically advance, and then quickly on the heels\n\n04:23.440 --> 04:27.120\n of that kind of the buzzkill of all of the antisocial behavior\n\n04:27.120 --> 04:29.360\n by algorithms, just kind of realized\n\n04:29.360 --> 04:31.840\n there was an opportunity for us to do something about it\n\n04:31.840 --> 04:33.360\n from a research perspective.\n\n04:34.000 --> 04:36.320\n You know, more to the point in your question,\n\n04:36.320 --> 04:41.600\n I mean, I do have an uncle who is literally a moral philosopher,\n\n04:41.600 --> 04:43.280\n and so in the early days of my life,\n\n04:43.280 --> 04:46.000\n he was a philosopher, and so in the early days\n\n04:46.000 --> 04:48.800\n of our technical work on fairness topics,\n\n04:48.800 --> 04:51.280\n I would occasionally, you know, run ideas behind him.\n\n04:51.280 --> 04:53.440\n So, I mean, I remember an early email I sent to him\n\n04:53.440 --> 04:55.040\n in which I said, like, oh, you know,\n\n04:55.040 --> 04:57.840\n here's a specific definition of algorithmic fairness\n\n04:57.840 --> 05:01.600\n that we think is some sort of variant of Rawlsian fairness.\n\n05:02.320 --> 05:03.120\n What do you think?\n\n05:03.920 --> 05:06.880\n And I thought I was asking a yes or no question,\n\n05:06.880 --> 05:09.120\n and I got back your kind of classical philosopher's\n\n05:09.120 --> 05:10.800\n response saying, well, it depends.\n\n05:10.800 --> 05:14.400\n Hey, then you might conclude this, and that's when I realized\n\n05:14.400 --> 05:19.680\n that there was a real kind of rift between the ways\n\n05:19.680 --> 05:21.840\n philosophers and others had thought about things\n\n05:21.840 --> 05:25.680\n like fairness, you know, from sort of a humanitarian perspective\n\n05:25.680 --> 05:27.840\n and the way that you needed to think about it\n\n05:27.840 --> 05:30.720\n as a computer scientist if you were going to kind of\n\n05:30.720 --> 05:32.960\n implement actual algorithmic solutions.\n\n05:34.080 --> 05:39.200\n But I would say the algorithmic solutions take care\n\n05:39.200 --> 05:41.360\n of some of the low hanging fruit.\n\n05:41.360 --> 05:44.560\n Sort of the problem is a lot of algorithms,\n\n05:44.560 --> 05:46.160\n when they don't consider fairness,\n\n05:47.280 --> 05:50.240\n they are just terribly unfair.\n\n05:50.240 --> 05:51.680\n And when they don't consider privacy,\n\n05:51.680 --> 05:55.200\n they're terribly, they violate privacy.\n\n05:55.200 --> 05:59.680\n Sort of the algorithmic approach fixes big problems.\n\n05:59.680 --> 06:04.000\n But there's still, when you start pushing into the gray area,\n\n06:04.000 --> 06:06.240\n that's when you start getting into this philosophy\n\n06:06.240 --> 06:09.680\n of what it means to be fair, starting from Plato,\n\n06:09.680 --> 06:11.760\n what is justice kind of questions?\n\n06:12.400 --> 06:13.280\n Yeah, I think that's right.\n\n06:13.280 --> 06:16.960\n And I mean, I would even not go as far as you want to say\n\n06:16.960 --> 06:19.520\n that sort of the algorithmic work in these areas\n\n06:19.520 --> 06:21.520\n is solving like the biggest problems.\n\n06:22.720 --> 06:24.320\n And, you know, we discuss in the book,\n\n06:24.320 --> 06:27.520\n the fact that really we are, there's a sense in which\n\n06:27.520 --> 06:29.760\n we're kind of looking where the light is in that,\n\n06:30.640 --> 06:34.880\n you know, for example, if police are racist\n\n06:34.880 --> 06:36.960\n in who they decide to stop and frisk,\n\n06:37.760 --> 06:39.120\n and that goes into the data,\n\n06:39.120 --> 06:41.520\n there's sort of no undoing that downstream\n\n06:41.520 --> 06:44.560\n by kind of clever algorithmic methods.\n\n06:45.760 --> 06:47.520\n And I think, especially in fairness,\n\n06:47.520 --> 06:49.840\n I mean, I think less so in privacy,\n\n06:49.840 --> 06:52.880\n where we feel like the community kind of really has settled\n\n06:52.880 --> 06:56.320\n on the right definition, which is differential privacy.\n\n06:56.320 --> 06:58.960\n If you just look at the algorithmic fairness literature\n\n06:58.960 --> 07:01.600\n already, you can see it's going to be much more of a mess.\n\n07:01.600 --> 07:03.360\n And, you know, you've got these theorems saying,\n\n07:03.360 --> 07:06.640\n here are three entirely reasonable,\n\n07:06.640 --> 07:09.760\n desirable notions of fairness.\n\n07:09.760 --> 07:12.720\n And, you know, here's a proof that you cannot simultaneously\n\n07:12.720 --> 07:13.840\n have all three of them.\n\n07:14.560 --> 07:17.680\n So I think we know that algorithmic fairness\n\n07:17.680 --> 07:19.280\n compared to algorithmic privacy\n\n07:19.280 --> 07:21.600\n is going to be kind of a harder problem.\n\n07:21.600 --> 07:23.440\n And it will have to revisit, I think,\n\n07:23.440 --> 07:26.080\n things that have been thought about by,\n\n07:26.080 --> 07:28.560\n you know, many generations of scholars before us.\n\n07:29.520 --> 07:32.240\n So it's very early days for fairness, I think.\n\n07:32.240 --> 07:34.560\n TK So before we get into the details\n\n07:34.560 --> 07:37.280\n of differential privacy, and on the fairness side,\n\n07:37.280 --> 07:39.520\n let me linger on the philosophy a bit.\n\n07:39.520 --> 07:42.640\n Do you think most people are fundamentally good?\n\n07:43.600 --> 07:46.640\n Or do most of us have both the capacity\n\n07:46.640 --> 07:48.320\n for good and evil within us?\n\n07:48.320 --> 07:50.240\n SB I mean, I'm an optimist.\n\n07:50.240 --> 07:52.480\n I tend to think that most people are good\n\n07:52.480 --> 07:54.960\n and want to do right.\n\n07:55.600 --> 07:58.480\n And that deviations from that are, you know,\n\n07:58.480 --> 08:00.240\n kind of usually due to circumstance,\n\n08:00.240 --> 08:02.960\n not due to people being bad at heart.\n\n08:02.960 --> 08:04.720\n TK With people with power,\n\n08:05.520 --> 08:08.160\n are people at the heads of governments,\n\n08:08.160 --> 08:09.680\n people at the heads of companies,\n\n08:10.320 --> 08:13.920\n people at the heads of, maybe, so financial power markets,\n\n08:15.040 --> 08:19.040\n do you think the distribution there is also,\n\n08:19.040 --> 08:21.040\n most people are good and have good intent?\n\n08:21.040 --> 08:22.800\n SB Yeah, I do.\n\n08:22.800 --> 08:26.560\n I mean, my statement wasn't qualified to people\n\n08:26.560 --> 08:28.560\n not in positions of power.\n\n08:28.560 --> 08:30.640\n I mean, I think what happens in a lot of the, you know,\n\n08:30.640 --> 08:34.080\n the cliche about absolute power corrupts absolutely.\n\n08:34.080 --> 08:36.880\n I mean, you know, I think even short of that,\n\n08:37.680 --> 08:39.840\n you know, having spent a lot of time on Wall Street,\n\n08:40.720 --> 08:44.160\n and also in arenas very, very different from Wall Street,\n\n08:44.160 --> 08:46.960\n like academia, you know, one of the things\n\n08:47.920 --> 08:50.960\n I think I've benefited from by moving between\n\n08:50.960 --> 08:53.760\n two very different worlds is you become aware\n\n08:53.760 --> 08:57.120\n that, you know, these worlds kind of develop\n\n08:57.120 --> 08:59.040\n their own social norms, and they develop\n\n08:59.040 --> 09:02.080\n their own rationales for, you know,\n\n09:02.080 --> 09:03.920\n behavior, for instance, that might look\n\n09:03.920 --> 09:05.280\n unusual to outsiders.\n\n09:05.280 --> 09:07.120\n But when you're in that world,\n\n09:07.120 --> 09:08.640\n it doesn't feel unusual at all.\n\n09:09.600 --> 09:11.680\n And I think this is true of a lot of,\n\n09:11.680 --> 09:13.760\n you know, professional cultures, for instance.\n\n09:15.440 --> 09:18.480\n And, you know, so then your maybe slippery slope\n\n09:18.480 --> 09:19.600\n is too strong of a word.\n\n09:19.600 --> 09:21.040\n But, you know, you're in some world\n\n09:21.040 --> 09:23.200\n where you're mainly around other people\n\n09:23.200 --> 09:25.520\n with the same kind of viewpoints and training\n\n09:25.520 --> 09:27.360\n and worldview as you.\n\n09:27.360 --> 09:30.640\n And I think that's more of a source of,\n\n09:30.640 --> 09:33.280\n of, you know, kind of abuses of power\n\n09:34.560 --> 09:36.720\n than sort of, you know, there being good people\n\n09:36.720 --> 09:40.640\n and evil people, and that somehow the evil people\n\n09:40.640 --> 09:43.040\n are the ones that somehow rise to power.\n\n09:43.040 --> 09:44.160\n Oh, that's really interesting.\n\n09:44.160 --> 09:46.880\n So it's the, within the social norms\n\n09:46.880 --> 09:49.760\n constructed by that particular group of people,\n\n09:50.400 --> 09:51.920\n you're all trying to do good.\n\n09:52.720 --> 09:54.640\n But because as a group, you might be,\n\n09:54.640 --> 09:56.080\n you might drift into something\n\n09:56.080 --> 09:57.600\n that for the broader population,\n\n09:58.160 --> 10:00.480\n it does not align with the values of society.\n\n10:00.480 --> 10:01.680\n That kind of, that's the word.\n\n10:01.680 --> 10:03.520\n Yeah, I mean, or not that you drift,\n\n10:03.520 --> 10:07.440\n but even the things that don't make sense\n\n10:07.440 --> 10:10.080\n to the outside world don't seem unusual to you.\n\n10:11.280 --> 10:13.360\n So it's not sort of like a good or a bad thing,\n\n10:13.360 --> 10:14.800\n but, you know, like, so for instance,\n\n10:14.800 --> 10:18.160\n you know, on, in the world of finance, right?\n\n10:18.160 --> 10:21.280\n There's a lot of complicated types of activity\n\n10:21.280 --> 10:22.960\n that if you are not immersed in that world,\n\n10:22.960 --> 10:25.760\n you cannot see why the purpose of that,\n\n10:25.760 --> 10:27.440\n you know, that activity exists at all.\n\n10:27.440 --> 10:30.640\n It just seems like, you know, completely useless\n\n10:30.640 --> 10:33.440\n and people just like, you know, pushing money around.\n\n10:33.440 --> 10:34.800\n And when you're in that world, right,\n\n10:34.800 --> 10:36.640\n you're, and you learn more,\n\n10:36.640 --> 10:39.600\n your view does become more nuanced, right?\n\n10:39.600 --> 10:41.680\n You realize, okay, there is actually a function\n\n10:41.680 --> 10:43.120\n to this activity.\n\n10:43.840 --> 10:46.640\n And in some cases, you would conclude that actually,\n\n10:46.640 --> 10:50.240\n if magically we could eradicate this activity tomorrow,\n\n10:50.240 --> 10:52.720\n it would come back because it actually is like\n\n10:52.720 --> 10:54.400\n serving some useful purpose.\n\n10:54.400 --> 10:56.880\n It's just a useful purpose that's very difficult\n\n10:56.880 --> 10:58.560\n for outsiders to see.\n\n10:59.200 --> 11:02.720\n And so I think, you know, lots of professional work\n\n11:02.720 --> 11:05.280\n environments or cultures, as I might put it,\n\n11:06.320 --> 11:08.800\n kind of have these social norms that, you know,\n\n11:08.800 --> 11:10.160\n don't make sense to the outside world.\n\n11:10.160 --> 11:11.280\n Academia is the same, right?\n\n11:11.280 --> 11:13.600\n I mean, lots of people look at academia and say,\n\n11:13.600 --> 11:15.920\n you know, what the hell are all of you people doing?\n\n11:16.480 --> 11:18.960\n Why are you paid so much in some cases\n\n11:18.960 --> 11:21.840\n at taxpayer expenses to do, you know,\n\n11:21.840 --> 11:24.400\n to publish papers that nobody reads?\n\n11:24.400 --> 11:25.920\n You know, but when you're in that world,\n\n11:25.920 --> 11:27.680\n you come to see the value for it.\n\n11:27.680 --> 11:30.240\n And, but even though you might not be able to explain it\n\n11:30.240 --> 11:31.840\n to, you know, the person in the street.\n\n11:33.040 --> 11:33.360\n Right.\n\n11:33.360 --> 11:36.000\n And in the case of the financial sector,\n\n11:36.000 --> 11:39.200\n tools like credit might not make sense to people.\n\n11:39.200 --> 11:41.600\n Like, it's a good example of something that does seem\n\n11:41.600 --> 11:45.120\n to pop up and be useful or just the power of markets\n\n11:45.120 --> 11:47.120\n and just in general capitalism.\n\n11:47.120 --> 11:47.360\n Yeah.\n\n11:47.360 --> 11:49.360\n In finance, I think the primary example\n\n11:49.360 --> 11:51.040\n I would give is leverage, right?\n\n11:51.040 --> 11:56.320\n So being allowed to borrow, to sort of use ten times\n\n11:56.320 --> 11:58.480\n as much money as you've actually borrowed, right?\n\n11:58.480 --> 12:00.720\n So that's an example of something that before I had\n\n12:00.720 --> 12:02.320\n any experience in financial markets,\n\n12:02.320 --> 12:03.440\n I might have looked at and said,\n\n12:03.440 --> 12:05.280\n well, what is the purpose of that?\n\n12:05.280 --> 12:08.400\n That just seems very dangerous and it is dangerous\n\n12:08.400 --> 12:10.480\n and it has proven dangerous.\n\n12:10.480 --> 12:13.280\n But, you know, if the fact of the matter is that,\n\n12:13.280 --> 12:15.920\n you know, sort of on some particular time scale,\n\n12:16.560 --> 12:19.680\n you are holding positions that are,\n\n12:19.680 --> 12:23.040\n you know, very unlikely to, you know,\n\n12:23.040 --> 12:26.800\n lose, you know, your value at risk or variance\n\n12:26.800 --> 12:30.320\n is like one or five percent, then it kind of makes sense\n\n12:30.320 --> 12:32.160\n that you would be allowed to use a little bit more\n\n12:32.160 --> 12:35.120\n than you have because you have, you know,\n\n12:35.120 --> 12:37.760\n some confidence that you're not going to lose\n\n12:37.760 --> 12:38.880\n it all in a single day.\n\n12:39.840 --> 12:41.600\n Now, of course, when that happens,\n\n12:42.960 --> 12:45.920\n we've seen what happens, you know, not too long ago.\n\n12:45.920 --> 12:48.800\n But, you know, but the idea that it serves\n\n12:48.800 --> 12:52.800\n no useful economic purpose under any circumstances\n\n12:52.800 --> 12:54.800\n is definitely not true.\n\n12:54.800 --> 12:57.680\n We'll return to the other side of the coast,\n\n12:57.680 --> 13:02.560\n Silicon Valley, and the problems there as we talk about privacy,\n\n13:02.560 --> 13:03.760\n as we talk about fairness.\n\n13:05.360 --> 13:09.360\n At the high level, and I'll ask some sort of basic questions\n\n13:09.360 --> 13:12.560\n with the hope to get at the fundamental nature of reality.\n\n13:12.560 --> 13:18.160\n But from a very high level, what is an ethical algorithm?\n\n13:18.160 --> 13:20.960\n So I can say that an algorithm has a running time\n\n13:20.960 --> 13:23.680\n of using big O notation n log n.\n\n13:24.400 --> 13:27.760\n I can say that a machine learning algorithm\n\n13:27.760 --> 13:31.440\n classified cat versus dog with 97 percent accuracy.\n\n13:31.440 --> 13:35.040\n Do you think there will one day be a way to measure\n\n13:36.320 --> 13:39.920\n sort of in the same compelling way as the big O notation\n\n13:39.920 --> 13:44.000\n of this algorithm is 97 percent ethical?\n\n13:44.000 --> 13:48.800\n First of all, let me riff for a second on your specific n log n example.\n\n13:48.800 --> 13:51.920\n So because early in the book when we're just kind of trying to describe\n\n13:51.920 --> 13:54.640\n algorithms period, we say like, okay, you know,\n\n13:54.640 --> 13:58.560\n what's an example of an algorithm or an algorithmic problem?\n\n13:58.560 --> 14:00.160\n First of all, like it's sorting, right?\n\n14:00.160 --> 14:02.240\n You have a bunch of index cards with numbers on them\n\n14:02.240 --> 14:03.760\n and you want to sort them.\n\n14:03.760 --> 14:07.360\n And we describe, you know, an algorithm that sweeps all the way through,\n\n14:07.360 --> 14:09.680\n finds the smallest number, puts it at the front,\n\n14:09.680 --> 14:12.640\n then sweeps through again, finds the second smallest number.\n\n14:12.640 --> 14:14.800\n So we make the point that this is an algorithm\n\n14:14.800 --> 14:17.760\n and it's also a bad algorithm in the sense that, you know,\n\n14:17.760 --> 14:20.640\n it's quadratic rather than n log n,\n\n14:20.640 --> 14:23.200\n which we know is kind of optimal for sorting.\n\n14:23.920 --> 14:26.080\n And we make the point that sort of like, you know,\n\n14:26.080 --> 14:30.880\n so even within the confines of a very precisely specified problem,\n\n14:31.680 --> 14:35.200\n there, you know, there might be many, many different algorithms\n\n14:35.200 --> 14:37.520\n for the same problem with different properties.\n\n14:37.520 --> 14:40.400\n Like some might be faster in terms of running time,\n\n14:40.400 --> 14:43.520\n some might use less memory, some might have, you know,\n\n14:43.520 --> 14:45.440\n better distributed implementations.\n\n14:46.240 --> 14:50.560\n And so the point is that already we're used to, you know,\n\n14:50.560 --> 14:53.520\n in computer science thinking about trade offs\n\n14:53.520 --> 14:56.800\n between different types of quantities and resources\n\n14:56.800 --> 14:59.920\n and there being, you know, better and worse algorithms.\n\n15:00.960 --> 15:08.480\n And our book is about that part of algorithmic ethics\n\n15:08.480 --> 15:13.520\n that we know how to kind of put on that same kind of quantitative footing right now.\n\n15:13.520 --> 15:17.440\n So, you know, just to say something that our book is not about,\n\n15:17.440 --> 15:22.400\n our book is not about kind of broad, fuzzy notions of fairness.\n\n15:22.400 --> 15:25.120\n It's about very specific notions of fairness.\n\n15:25.840 --> 15:27.200\n There's more than one of them.\n\n15:28.240 --> 15:30.880\n There are tensions between them, right?\n\n15:30.880 --> 15:35.680\n But if you pick one of them, you can do something akin to saying\n\n15:35.680 --> 15:38.480\n that this algorithm is 97% ethical.\n\n15:39.200 --> 15:44.080\n You can say, for instance, the, you know, for this lending model,\n\n15:44.080 --> 15:51.040\n the false rejection rate on black people and white people is within 3%, right?\n\n15:51.040 --> 15:57.040\n So we might call that a 97% ethical algorithm and a 100% ethical algorithm\n\n15:57.040 --> 15:59.200\n would mean that that difference is 0%.\n\n15:59.920 --> 16:04.640\n In that case, fairness is specified when two groups, however,\n\n16:04.640 --> 16:06.000\n they're defined are given to you.\n\n16:06.720 --> 16:07.280\n That's right.\n\n16:07.280 --> 16:11.280\n So the, and then you can sort of mathematically start describing the algorithm.\n\n16:11.760 --> 16:18.880\n But nevertheless, the part where the two groups are given to you,\n\n16:20.080 --> 16:24.480\n I mean, unlike running time, you know, we don't in computer science\n\n16:24.480 --> 16:29.200\n talk about how fast an algorithm feels like when it runs.\n\n16:29.200 --> 16:29.760\n True.\n\n16:29.760 --> 16:33.040\n We measure it and ethical starts getting into feelings.\n\n16:33.040 --> 16:38.160\n So, for example, an algorithm runs, you know, if it runs in the background,\n\n16:38.160 --> 16:40.480\n it doesn't disturb the performance of my system.\n\n16:40.480 --> 16:41.600\n It'll feel nice.\n\n16:41.600 --> 16:42.560\n I'll be okay with it.\n\n16:42.560 --> 16:45.280\n But if it overloads the system, it'll feel unpleasant.\n\n16:45.280 --> 16:50.320\n So in that same way, ethics, there's a feeling of how socially acceptable it is.\n\n16:50.320 --> 16:55.200\n How does it represent the moral standards of our society today?\n\n16:55.200 --> 16:59.040\n So in that sense, and sorry to linger on that first of high,\n\n16:59.040 --> 17:00.640\n low philosophical questions.\n\n17:00.640 --> 17:05.120\n Do you have a sense we'll be able to measure how ethical an algorithm is?\n\n17:05.920 --> 17:09.680\n First of all, I didn't, certainly didn't mean to give the impression that you can kind of\n\n17:09.680 --> 17:16.320\n measure, you know, memory speed trade offs, you know, and that there's a complete mapping from\n\n17:16.320 --> 17:22.240\n that onto kind of fairness, for instance, or ethics and accuracy, for example.\n\n17:22.880 --> 17:28.960\n In the type of fairness definitions that are largely the objects of study today and starting\n\n17:28.960 --> 17:35.360\n to be deployed, you as the user of the definitions, you need to make some hard decisions before you\n\n17:35.360 --> 17:39.200\n even get to the point of designing fair algorithms.\n\n17:40.240 --> 17:45.840\n One of them, for instance, is deciding who it is that you're worried about protecting,\n\n17:45.840 --> 17:50.560\n who you're worried about being harmed by, for instance, some notion of discrimination or\n\n17:50.560 --> 17:51.200\n unfairness.\n\n17:52.160 --> 17:55.520\n And then you need to also decide what constitutes harm.\n\n17:55.520 --> 18:02.320\n So, for instance, in a lending application, maybe you decide that, you know, falsely rejecting\n\n18:02.320 --> 18:08.960\n a creditworthy individual, you know, sort of a false negative, is the real harm and that false\n\n18:08.960 --> 18:14.560\n positives, i.e. people that are not creditworthy or are not gonna repay your loan, that get a loan,\n\n18:14.560 --> 18:16.320\n you might think of them as lucky.\n\n18:17.120 --> 18:22.720\n And so that's not a harm, although it's not clear that if you don't have the means to repay a loan,\n\n18:22.720 --> 18:26.320\n that being given a loan is not also a harm.\n\n18:26.880 --> 18:33.600\n So, you know, the literature is sort of so far quite limited in that you sort of need to say,\n\n18:33.600 --> 18:36.960\n who do you want to protect and what would constitute harm to that group?\n\n18:37.920 --> 18:42.080\n And when you ask questions like, will algorithms feel ethical?\n\n18:42.080 --> 18:47.440\n One way in which they won't, under the definitions that I'm describing, is if, you know, if you are\n\n18:47.440 --> 18:54.320\n an individual who is falsely denied a loan, incorrectly denied a loan, all of these definitions\n\n18:54.320 --> 19:00.240\n basically say like, well, you know, your compensation is the knowledge that we are also\n\n19:00.240 --> 19:05.120\n falsely denying loans to other people, you know, in other groups at the same rate that we're doing\n\n19:05.120 --> 19:05.680\n it to you.\n\n19:05.680 --> 19:12.800\n And, you know, and so there is actually this interesting even technical tension in the field\n\n19:12.800 --> 19:18.400\n right now between these sort of group notions of fairness and notions of fairness that might\n\n19:18.400 --> 19:22.160\n actually feel like real fairness to individuals, right?\n\n19:22.160 --> 19:27.360\n They might really feel like their particular interests are being protected or thought about\n\n19:27.360 --> 19:32.800\n by the algorithm rather than just, you know, the groups that they happen to be members of.\n\n19:33.360 --> 19:37.920\n Is there parallels to the big O notation of worst case analysis?\n\n19:37.920 --> 19:45.760\n So, is it important to looking at the worst violation of fairness for an individual?\n\n19:45.760 --> 19:48.080\n Is it important to minimize that one individual?\n\n19:48.080 --> 19:52.320\n So like worst case analysis, is that something you think about or?\n\n19:52.320 --> 19:56.960\n I mean, I think we're not even at the point where we can sensibly think about that.\n\n19:56.960 --> 20:03.280\n So first of all, you know, we're talking here both about fairness applied at the group level,\n\n20:03.280 --> 20:08.000\n which is a relatively weak thing, but it's better than nothing.\n\n20:08.000 --> 20:14.960\n And also the more ambitious thing of trying to give some individual promises, but even\n\n20:14.960 --> 20:18.640\n that doesn't incorporate, I think something that you're hinting at here is what I might\n\n20:18.640 --> 20:20.720\n call subjective fairness, right?\n\n20:20.720 --> 20:25.200\n So a lot of the definitions, I mean, all of the definitions in the algorithmic fairness\n\n20:25.200 --> 20:28.400\n literature are what I would kind of call received wisdom definitions.\n\n20:28.400 --> 20:33.440\n It's sort of, you know, somebody like me sits around and things like, okay, you know, I\n\n20:33.440 --> 20:37.840\n think here's a technical definition of fairness that I think people should want or that they\n\n20:37.840 --> 20:41.840\n should, you know, think of as some notion of fairness, maybe not the only one, maybe\n\n20:41.840 --> 20:44.320\n not the best one, maybe not the last one.\n\n20:44.320 --> 20:52.480\n But we really actually don't know from a subjective standpoint, like what people really\n\n20:52.480 --> 20:53.360\n think is fair.\n\n20:53.360 --> 21:01.120\n You know, we just started doing a little bit of work in our group at actually doing kind\n\n21:01.120 --> 21:09.120\n of human subject experiments in which we, you know, ask people about, you know, we ask\n\n21:09.120 --> 21:15.120\n them questions about fairness, we survey them, we, you know, we show them pairs of individuals\n\n21:15.120 --> 21:20.320\n in, let's say, a criminal recidivism prediction setting, and we ask them, do you think these\n\n21:20.320 --> 21:24.320\n two individuals should be treated the same as a matter of fairness?\n\n21:24.320 --> 21:31.760\n And to my knowledge, there's not a large literature in which ordinary people are asked\n\n21:31.760 --> 21:37.040\n about, you know, they have sort of notions of their subjective fairness elicited from\n\n21:37.040 --> 21:37.540\n them.\n\n21:38.160 --> 21:43.840\n It's mainly, you know, kind of scholars who think about fairness kind of making up their\n\n21:43.840 --> 21:44.400\n own definitions.\n\n21:44.400 --> 21:50.320\n And I think this needs to change actually for many social norms, not just for fairness,\n\n21:50.320 --> 21:50.560\n right?\n\n21:50.560 --> 21:56.560\n So there's a lot of discussion these days in the AI community about interpretable AI\n\n21:56.560 --> 21:57.920\n or understandable AI.\n\n21:58.560 --> 22:04.880\n And as far as I can tell, everybody agrees that deep learning or at least the outputs\n\n22:04.880 --> 22:11.840\n of deep learning are not very understandable, and people might agree that sparse linear\n\n22:11.840 --> 22:15.520\n models with integer coefficients are more understandable.\n\n22:15.520 --> 22:17.440\n But nobody's really asked people.\n\n22:17.440 --> 22:21.280\n You know, there's very little literature on, you know, sort of showing people models\n\n22:21.280 --> 22:24.080\n and asking them, do they understand what the model is doing?\n\n22:25.280 --> 22:32.560\n And I think that in all these topics, as these fields mature, we need to start doing more\n\n22:32.560 --> 22:33.520\n behavioral work.\n\n22:34.400 --> 22:38.160\n Yeah, which is one of my deep passions is psychology.\n\n22:38.160 --> 22:44.480\n And I always thought computer scientists will be the best future psychologists in a sense\n\n22:44.480 --> 22:51.680\n that data is, especially in this modern world, the data is a really powerful way to understand\n\n22:51.680 --> 22:53.360\n and study human behavior.\n\n22:53.360 --> 22:56.720\n And you've explored that with your game theory side of work as well.\n\n22:56.720 --> 23:01.680\n Yeah, I'd like to think that what you say is true about computer scientists and psychology\n\n23:02.240 --> 23:07.520\n from my own limited wandering into human subject experiments.\n\n23:07.520 --> 23:11.600\n We have a great deal to learn, not just computer science, but AI and machine learning more\n\n23:11.600 --> 23:17.040\n specifically, I kind of think of as imperialist research communities in that, you know, kind\n\n23:17.040 --> 23:22.800\n of like physicists in an earlier generation, computer scientists kind of don't think of\n\n23:22.800 --> 23:25.440\n any scientific topic that's off limits to them.\n\n23:25.440 --> 23:30.880\n They will like freely wander into areas that others have been thinking about for decades\n\n23:30.880 --> 23:31.440\n or longer.\n\n23:31.440 --> 23:37.840\n And, you know, we usually tend to embarrass ourselves in those efforts for some amount\n\n23:37.840 --> 23:38.320\n of time.\n\n23:38.320 --> 23:41.840\n Like, you know, I think reinforcement learning is a good example, right?\n\n23:41.840 --> 23:48.160\n So a lot of the early work in reinforcement learning, I have complete sympathy for the\n\n23:48.160 --> 23:53.120\n control theorists that looked at this and said like, okay, you are reinventing stuff\n\n23:53.120 --> 23:55.600\n that we've known since like the forties, right?\n\n23:55.600 --> 24:01.120\n But, you know, in my view, eventually this sort of, you know, computer scientists have\n\n24:01.120 --> 24:06.320\n made significant contributions to that field, even though we kind of embarrassed ourselves\n\n24:06.320 --> 24:07.520\n for the first decade.\n\n24:07.520 --> 24:12.080\n So I think if computer scientists are gonna start engaging in kind of psychology, human\n\n24:12.080 --> 24:18.080\n subjects type of research, we should expect to be embarrassing ourselves for a good 10\n\n24:18.080 --> 24:23.600\n years or so, and then hope that it turns out as well as, you know, some other areas that\n\n24:23.600 --> 24:25.600\n we've waded into.\n\n24:25.600 --> 24:30.400\n So you kind of mentioned this, just to linger on the idea of an ethical algorithm, of idea\n\n24:30.400 --> 24:33.760\n of groups, sort of group thinking and individual thinking.\n\n24:33.760 --> 24:35.040\n And we're struggling that.\n\n24:35.040 --> 24:39.280\n One of the amazing things about algorithms and your book and just this field of study\n\n24:39.280 --> 24:46.640\n is it gets us to ask, like forcing machines, converting these ideas into algorithms is\n\n24:46.640 --> 24:50.160\n forcing us to ask questions of ourselves as a human civilization.\n\n24:50.160 --> 24:58.320\n So there's a lot of people now in public discourse doing sort of group thinking, thinking like\n\n24:58.320 --> 25:02.000\n there's particular sets of groups that we don't wanna discriminate against and so on.\n\n25:02.000 --> 25:08.560\n And then there is individuals, sort of in the individual life stories, the struggles\n\n25:08.560 --> 25:10.000\n they went through and so on.\n\n25:10.000 --> 25:16.480\n Now, like in philosophy, it's easier to do group thinking because you don't, it's very\n\n25:16.480 --> 25:17.920\n hard to think about individuals.\n\n25:17.920 --> 25:23.840\n There's so much variability, but with data, you can start to actually say, you know what\n\n25:23.840 --> 25:26.400\n group thinking is too crude.\n\n25:26.400 --> 25:30.880\n You're actually doing more discrimination by thinking in terms of groups and individuals.\n\n25:30.880 --> 25:36.720\n Can you linger on that kind of idea of group versus individual and ethics?\n\n25:36.720 --> 25:41.680\n And is it good to continue thinking in terms of groups in algorithms?\n\n25:41.680 --> 25:49.360\n So let me start by answering a very good high level question with a slightly narrow technical\n\n25:49.360 --> 25:54.480\n response, which is these group definitions of fairness, like here's a few groups, like\n\n25:54.480 --> 25:59.440\n different racial groups, maybe gender groups, maybe age, what have you.\n\n25:59.440 --> 26:06.480\n And let's make sure that for none of these groups, do we have a false negative rate,\n\n26:06.480 --> 26:09.200\n which is much higher than any other one of these groups.\n\n26:09.200 --> 26:13.760\n Okay, so these are kind of classic group aggregate notions of fairness.\n\n26:13.760 --> 26:18.000\n And you know, but at the end of the day, an individual you can think of as a combination\n\n26:18.000 --> 26:19.360\n of all of their attributes, right?\n\n26:19.360 --> 26:26.800\n They're a member of a racial group, they have a gender, they have an age, and many other\n\n26:26.800 --> 26:33.840\n demographic properties that are not biological, but that are still very strong determinants\n\n26:33.840 --> 26:36.720\n of outcome and personality and the like.\n\n26:36.720 --> 26:43.920\n So one, I think, useful spectrum is to sort of think about that array between the group\n\n26:43.920 --> 26:49.600\n and the specific individual, and to realize that in some ways, asking for fairness at\n\n26:49.600 --> 26:56.800\n the individual level is to sort of ask for group fairness simultaneously for all possible\n\n26:56.800 --> 26:57.840\n combinations of groups.\n\n26:57.840 --> 27:06.480\n So in particular, you know, if I build a predictive model that meets some definition of fairness,\n\n27:06.480 --> 27:14.160\n definition of fairness by race, by gender, by age, by what have you, marginally, to get\n\n27:14.160 --> 27:20.960\n it slightly technical, sort of independently, I shouldn't expect that model to not discriminate\n\n27:20.960 --> 27:27.440\n against disabled Hispanic women over age 55, making less than $50,000 a year annually,\n\n27:27.440 --> 27:32.480\n even though I might have protected each one of those attributes marginally.\n\n27:32.480 --> 27:35.680\n So the optimization, actually, that's a fascinating way to put it.\n\n27:35.680 --> 27:42.160\n So you're just optimizing, the one way to achieve the optimizing fairness for individuals\n\n27:42.160 --> 27:46.080\n is just to add more and more definitions of groups that each individual belongs to.\n\n27:46.080 --> 27:47.080\n That's right.\n\n27:47.080 --> 27:50.320\n So, you know, at the end of the day, we could think of all of ourselves as groups of size\n\n27:50.320 --> 27:55.400\n one because eventually there's some attribute that separates you from me and everybody else\n\n27:55.400 --> 27:57.020\n in the world, okay?\n\n27:57.020 --> 28:03.560\n And so it is possible to put, you know, these incredibly coarse ways of thinking about fairness\n\n28:03.560 --> 28:09.960\n and these very, very individualistic specific ways on a common scale.\n\n28:09.960 --> 28:14.160\n And you know, one of the things we've worked on from a research perspective is, you know,\n\n28:14.160 --> 28:20.520\n so we sort of know how to, you know, in relative terms, we know how to provide fairness guarantees\n\n28:20.520 --> 28:22.760\n at the core system of the scale.\n\n28:22.760 --> 28:28.240\n We don't know how to provide kind of sensible, tractable, realistic fairness guarantees at\n\n28:28.240 --> 28:33.120\n the individual level, but maybe we could start creeping towards that by dealing with more\n\n28:33.120 --> 28:35.040\n refined subgroups.\n\n28:35.040 --> 28:41.000\n I mean, we gave a name to this phenomenon where, you know, you protect, you enforce\n\n28:41.000 --> 28:46.580\n some definition of fairness for a bunch of marginal attributes or features, but then\n\n28:46.580 --> 28:49.980\n you find yourself discriminating against a combination of them.\n\n28:49.980 --> 28:55.480\n We call that fairness gerrymandering because like political gerrymandering, you know, you're\n\n28:55.480 --> 29:01.400\n giving some guarantee at the aggregate level, but when you kind of look in a more granular\n\n29:01.400 --> 29:06.440\n way at what's going on, you realize that you're achieving that aggregate guarantee by sort\n\n29:06.440 --> 29:10.880\n of favoring some groups and discriminating against other ones.\n\n29:10.880 --> 29:15.940\n And so there are, you know, it's early days, but there are algorithmic approaches that\n\n29:15.940 --> 29:22.440\n let you start creeping towards that, you know, individual end of the spectrum.\n\n29:22.440 --> 29:30.740\n Does there need to be human input in the form of weighing the value of the importance of\n\n29:30.740 --> 29:33.000\n each kind of group?\n\n29:33.000 --> 29:42.400\n So for example, is it like, so gender, say crudely speaking, male and female, and then\n\n29:42.400 --> 29:51.980\n different races, are we as humans supposed to put value on saying gender is 0.6 and race\n\n29:51.980 --> 29:59.200\n is 0.4 in terms of in the big optimization of achieving fairness?\n\n29:59.200 --> 30:01.720\n Is that kind of what humans are supposed to do here?\n\n30:01.720 --> 30:05.320\n I mean, of course, you know, I don't need to tell you that, of course, technically one\n\n30:05.320 --> 30:10.720\n could incorporate such weights if you wanted to into a definition of fairness.\n\n30:10.720 --> 30:19.680\n You know, fairness is an interesting topic in that having worked in the book being about\n\n30:19.680 --> 30:24.820\n both fairness, privacy, and many other social norms, fairness, of course, is a much, much\n\n30:24.820 --> 30:27.160\n more loaded topic.\n\n30:27.160 --> 30:32.180\n So privacy, I mean, people want privacy, people don't like violations of privacy, violations\n\n30:32.180 --> 30:40.680\n of privacy cause damage, angst, and bad publicity for the companies that are victims of them.\n\n30:40.680 --> 30:48.020\n But sort of everybody agrees more data privacy would be better than less data privacy.\n\n30:48.020 --> 30:53.780\n And you don't have these, somehow the discussions of fairness don't become politicized along\n\n30:53.780 --> 31:01.900\n other dimensions like race and about gender and, you know, whether we, and, you know,\n\n31:01.900 --> 31:10.760\n you quickly find yourselves kind of revisiting topics that have been kind of unresolved forever,\n\n31:10.760 --> 31:12.560\n like affirmative action, right?\n\n31:12.560 --> 31:16.400\n Sort of, you know, like, why are you protecting, and some people will say, why are you protecting\n\n31:16.400 --> 31:20.320\n this particular racial group?\n\n31:20.320 --> 31:26.240\n And others will say, well, we need to do that as a matter of retribution.\n\n31:26.240 --> 31:30.040\n Other people will say, it's a matter of economic opportunity.\n\n31:30.040 --> 31:34.920\n And I don't know which of, you know, whether any of these are the right answers, but you\n\n31:34.920 --> 31:39.840\n sort of, fairness is sort of special in that as soon as you start talking about it, you\n\n31:39.840 --> 31:46.360\n inevitably have to participate in debates about fair to whom, at what expense to who\n\n31:46.360 --> 31:47.360\n else.\n\n31:47.360 --> 31:56.180\n I mean, even in criminal justice, right, you know, where people talk about fairness in\n\n31:56.180 --> 32:02.840\n criminal sentencing or, you know, predicting failures to appear or making parole decisions\n\n32:02.840 --> 32:08.340\n or the like, they will, you know, they'll point out that, well, these definitions of\n\n32:08.340 --> 32:13.640\n fairness are all about fairness for the criminals.\n\n32:13.640 --> 32:16.120\n And what about fairness for the victims, right?\n\n32:16.120 --> 32:22.840\n So when I basically say something like, well, the false incarceration rate for black people\n\n32:22.840 --> 32:28.300\n and white people needs to be roughly the same, you know, there's no mention of potential\n\n32:28.300 --> 32:33.180\n victims of criminals in such a fairness definition.\n\n32:33.180 --> 32:34.960\n And that's the realm of public discourse.\n\n32:34.960 --> 32:41.200\n I should actually recommend, I just listened to people listening, Intelligence Squares\n\n32:41.200 --> 32:45.080\n debates, US edition just had a debate.\n\n32:45.080 --> 32:50.080\n They have this structure where you have old Oxford style or whatever they're called, debates,\n\n32:50.080 --> 32:55.680\n you know, it's two versus two and they talked about affirmative action and it was incredibly\n\n32:55.680 --> 33:03.000\n interesting that there's really good points on every side of this issue, which is fascinating\n\n33:03.000 --> 33:04.000\n to listen to.\n\n33:04.000 --> 33:05.680\n Yeah, yeah, I agree.\n\n33:05.680 --> 33:12.400\n And so it's interesting to be a researcher trying to do, for the most part, technical\n\n33:12.400 --> 33:17.980\n algorithmic work, but Aaron and I both quickly learned you cannot do that and then go out\n\n33:17.980 --> 33:22.640\n and talk about it and expect people to take it seriously if you're unwilling to engage\n\n33:22.640 --> 33:28.160\n in these broader debates that are entirely extra algorithmic, right?\n\n33:28.160 --> 33:31.200\n They're not about, you know, algorithms and making algorithms better.\n\n33:31.200 --> 33:35.160\n They're sort of, you know, as you said, sort of like, what should society be protecting\n\n33:35.160 --> 33:36.160\n in the first place?\n\n33:36.160 --> 33:42.320\n When you discuss the fairness, an algorithm that achieves fairness, whether in the constraints\n\n33:42.320 --> 33:48.520\n and the objective function, there's an immediate kind of analysis you can perform, which is\n\n33:48.520 --> 33:56.520\n saying, if you care about fairness in gender, this is the amount that you have to pay for\n\n33:56.520 --> 33:59.280\n it in terms of the performance of the system.\n\n33:59.280 --> 34:03.960\n Like do you, is there a role for statements like that in a table, in a paper, or do you\n\n34:03.960 --> 34:06.680\n want to really not touch that?\n\n34:06.680 --> 34:09.800\n No, no, we want to touch that and we do touch it.\n\n34:09.800 --> 34:16.680\n So I mean, just again, to make sure I'm not promising your viewers more than we know how\n\n34:16.680 --> 34:21.760\n to provide, but if you pick a definition of fairness, like I'm worried about gender discrimination\n\n34:21.760 --> 34:27.100\n and you pick a notion of harm, like false rejection for a loan, for example, and you\n\n34:27.100 --> 34:30.960\n give me a model, I can definitely, first of all, go audit that model.\n\n34:30.960 --> 34:36.640\n It's easy for me to go, you know, from data to kind of say like, okay, your false rejection\n\n34:36.640 --> 34:41.880\n rate on women is this much higher than it is on men, okay?\n\n34:41.880 --> 34:47.240\n But once you also put the fairness into your objective function, I mean, I think the table\n\n34:47.240 --> 34:51.640\n that you're talking about is what we would call the Pareto curve, right?\n\n34:51.640 --> 34:58.740\n You can literally trace out, and we give examples of such plots on real data sets in the book,\n\n34:58.740 --> 34:59.760\n you have two axes.\n\n34:59.760 --> 35:06.360\n On the X axis is your error, on the Y axis is unfairness by whatever, you know, if it's\n\n35:06.360 --> 35:12.240\n like the disparity between false rejection rates between two groups.\n\n35:12.240 --> 35:17.080\n And you know, your algorithm now has a knob that basically says, how strongly do I want\n\n35:17.080 --> 35:19.400\n to enforce fairness?\n\n35:19.400 --> 35:24.680\n And the less unfair, you know, if the two axes are error and unfairness, we'd like to\n\n35:24.680 --> 35:26.260\n be at zero, zero.\n\n35:26.260 --> 35:31.280\n We'd like zero error and zero unfairness simultaneously.\n\n35:31.280 --> 35:34.840\n Anybody who works in machine learning knows that you're generally not going to get to\n\n35:34.840 --> 35:38.840\n zero error period without any fairness constraint whatsoever.\n\n35:38.840 --> 35:41.060\n So that's not going to happen.\n\n35:41.060 --> 35:46.480\n But in general, you know, you'll get this, you'll get some kind of convex curve that\n\n35:46.480 --> 35:49.960\n specifies the numerical trade off you face.\n\n35:49.960 --> 35:57.920\n You know, if I want to go from 17% error down to 16% error, what will be the increase in\n\n35:57.920 --> 36:02.960\n unfairness that I experienced as a result of that?\n\n36:02.960 --> 36:09.520\n And so this curve kind of specifies the, you know, kind of undominated models.\n\n36:09.520 --> 36:14.480\n Models that are off that curve are, you know, can be strictly improved in one or both dimensions.\n\n36:14.480 --> 36:18.840\n You can, you know, either make the error better or the unfairness better or both.\n\n36:18.840 --> 36:26.000\n And I think our view is that not only are these objects, these Pareto curves, you know,\n\n36:26.000 --> 36:34.360\n with efficient frontiers as you might call them, not only are they valuable scientific\n\n36:34.360 --> 36:41.320\n objects, I actually think that they in the near term might need to be the interface between\n\n36:41.320 --> 36:46.180\n researchers working in the field and stakeholders in given problems.\n\n36:46.180 --> 36:55.320\n So you know, you could really imagine telling a criminal jurisdiction, look, if you're concerned\n\n36:55.320 --> 36:58.820\n about racial fairness, but you're also concerned about accuracy.\n\n36:58.820 --> 37:05.200\n You want to, you know, you want to release on parole people that are not going to recommit\n\n37:05.200 --> 37:08.600\n a violent crime and you don't want to release the ones who are.\n\n37:08.600 --> 37:10.600\n So you know, that's accuracy.\n\n37:10.600 --> 37:15.120\n But if you also care about those, you know, the mistakes you make not being disproportionately\n\n37:15.120 --> 37:19.160\n on one racial group or another, you can show this curve.\n\n37:19.160 --> 37:23.980\n I'm hoping that in the near future, it'll be possible to explain these curves to non\n\n37:23.980 --> 37:29.520\n technical people that are the ones that have to make the decision, where do we want to\n\n37:29.520 --> 37:30.520\n be on this curve?\n\n37:30.520 --> 37:38.440\n Like, what are the relative merits or value of having lower error versus lower unfairness?\n\n37:38.440 --> 37:43.560\n You know, that's not something computer scientists should be deciding for society, right?\n\n37:43.560 --> 37:49.400\n That, you know, the people in the field, so to speak, the policymakers, the regulators,\n\n37:49.400 --> 37:51.680\n that's who should be making these decisions.\n\n37:51.680 --> 37:56.600\n But I think and hope that they can be made to understand that these trade offs generally\n\n37:56.600 --> 38:03.280\n exist and that you need to pick a point and like, and ignoring the trade off, you know,\n\n38:03.280 --> 38:06.760\n you're implicitly picking a point anyway, right?\n\n38:06.760 --> 38:09.400\n You just don't know it and you're not admitting it.\n\n38:09.400 --> 38:12.740\n Just to linger on the point of trade offs, I think that's a really important thing to\n\n38:12.740 --> 38:15.400\n sort of think about.\n\n38:15.400 --> 38:22.360\n So you think when we start to optimize for fairness, there's almost always in most system\n\n38:22.360 --> 38:25.080\n going to be trade offs.\n\n38:25.080 --> 38:30.200\n Can you like, what's the trade off between just to clarify, there have been some sort\n\n38:30.200 --> 38:39.240\n of technical terms thrown around, but sort of a perfectly fair world.\n\n38:39.240 --> 38:40.760\n Why is that?\n\n38:40.760 --> 38:43.760\n Why will somebody be upset about that?\n\n38:43.760 --> 38:47.400\n The specific trade off I talked about just in order to make things very concrete was\n\n38:47.400 --> 38:53.360\n between numerical error and some numerical measure of unfairness.\n\n38:53.360 --> 38:56.400\n What is numerical error in the case of...\n\n38:56.400 --> 39:01.000\n Just like say predictive error, like, you know, the probability or frequency with which\n\n39:01.000 --> 39:08.480\n you release somebody on parole who then goes on to recommit a violent crime or keep incarcerated\n\n39:08.480 --> 39:10.920\n somebody who would not have recommitted a violent crime.\n\n39:10.920 --> 39:17.480\n So in the case of awarding somebody parole or giving somebody parole or letting them\n\n39:17.480 --> 39:21.480\n out on parole, you don't want them to recommit a crime.\n\n39:21.480 --> 39:26.600\n So it's your system failed in prediction if they happen to do a crime.\n\n39:26.600 --> 39:30.280\n Okay, so that's one axis.\n\n39:30.280 --> 39:31.800\n And what's the fairness axis?\n\n39:31.800 --> 39:39.640\n So then the fairness axis might be the difference between racial groups in the kind of false\n\n39:39.640 --> 39:47.840\n positive predictions, namely people that I kept incarcerated predicting that they would\n\n39:47.840 --> 39:51.200\n recommit a violent crime when in fact they wouldn't have.\n\n39:51.200 --> 39:52.200\n Right.\n\n39:52.200 --> 40:00.840\n And the unfairness of that, just to linger it and allow me to in eloquently to try to\n\n40:00.840 --> 40:06.360\n sort of describe why that's unfair, why unfairness is there.\n\n40:06.360 --> 40:13.280\n The unfairness you want to get rid of is that in the judge's mind, the bias of having being\n\n40:13.280 --> 40:18.480\n brought up to society, the slight racial bias, the racism that exists in the society, you\n\n40:18.480 --> 40:21.760\n want to remove that from the system.\n\n40:21.760 --> 40:28.720\n Another way that's been debated is sort of equality of opportunity versus equality of\n\n40:28.720 --> 40:30.440\n outcome.\n\n40:30.440 --> 40:35.120\n And there's a weird dance there that's really difficult to get right.\n\n40:35.120 --> 40:40.200\n And we don't, affirmative action is exploring that space.\n\n40:40.200 --> 40:41.200\n Right.\n\n40:41.200 --> 40:48.840\n And then this also quickly bleeds into questions like, well, maybe if one group really does\n\n40:48.840 --> 40:55.240\n recommit crimes at a higher rate, the reason for that is that at some earlier point in\n\n40:55.240 --> 41:00.200\n the pipeline or earlier in their lives, they didn't receive the same resources that the\n\n41:00.200 --> 41:02.560\n other group did.\n\n41:02.560 --> 41:08.480\n And so there's always in kind of fairness discussions, the possibility that the real\n\n41:08.480 --> 41:11.040\n injustice came earlier, right?\n\n41:11.040 --> 41:16.360\n Earlier in this individual's life, earlier in this group's history, et cetera, et cetera.\n\n41:16.360 --> 41:20.840\n And so a lot of the fairness discussion is almost, the goal is for it to be a corrective\n\n41:20.840 --> 41:25.440\n mechanism to account for the injustice earlier in life.\n\n41:25.440 --> 41:29.640\n By some definitions of fairness or some theories of fairness, yeah.\n\n41:29.640 --> 41:35.120\n Others would say like, look, it's not to correct that injustice, it's just to kind of level\n\n41:35.120 --> 41:40.720\n the playing field right now and not falsely incarcerate more people of one group than\n\n41:40.720 --> 41:41.720\n another group.\n\n41:41.720 --> 41:46.960\n But I mean, I think just it might be helpful just to demystify a little bit about the many\n\n41:46.960 --> 41:54.940\n ways in which bias or unfairness can come into algorithms, especially in the machine\n\n41:54.940 --> 41:55.940\n learning era, right?\n\n41:55.940 --> 42:00.680\n I think many of your viewers have probably heard these examples before, but let's say\n\n42:00.680 --> 42:04.160\n I'm building a face recognition system, right?\n\n42:04.160 --> 42:12.000\n And so I'm kind of gathering lots of images of faces and trying to train the system to\n\n42:12.000 --> 42:17.340\n recognize new faces of those individuals from training on a training set of those faces\n\n42:17.340 --> 42:19.080\n of individuals.\n\n42:19.080 --> 42:24.860\n And it shouldn't surprise anybody or certainly not anybody in the field of machine learning\n\n42:24.860 --> 42:34.960\n if my training data set was primarily white males and I'm training the model to maximize\n\n42:34.960 --> 42:44.060\n the overall accuracy on my training data set, that the model can reduce its error most by\n\n42:44.060 --> 42:48.800\n getting things right on the white males that constitute the majority of the data set, even\n\n42:48.800 --> 42:53.640\n if that means that on other groups, they will be less accurate, okay?\n\n42:53.640 --> 42:57.720\n Now, there's a bunch of ways you could think about addressing this.\n\n42:57.720 --> 43:05.760\n One is to deliberately put into the objective of the algorithm not to optimize the error\n\n43:05.760 --> 43:09.060\n at the expense of this discrimination, and then you're kind of back in the land of these\n\n43:09.060 --> 43:13.140\n kind of two dimensional numerical trade offs.\n\n43:13.140 --> 43:18.660\n A valid counter argument is to say like, well, no, you don't have to, there's no, you know,\n\n43:18.660 --> 43:22.840\n the notion of the tension between error and accuracy here is a false one.\n\n43:22.840 --> 43:27.760\n You could instead just go out and get much more data on these other groups that are in\n\n43:27.760 --> 43:34.580\n the minority and, you know, equalize your data set, or you could train a separate model\n\n43:34.580 --> 43:38.800\n on those subgroups and, you know, have multiple models.\n\n43:38.800 --> 43:43.120\n The point I think we would, you know, we tried to make in the book is that those things have\n\n43:43.120 --> 43:45.160\n cost too, right?\n\n43:45.160 --> 43:51.200\n Going out and gathering more data on groups that are relatively rare compared to your\n\n43:51.200 --> 43:55.520\n plurality or more majority group that, you know, it may not cost you in the accuracy\n\n43:55.520 --> 43:59.460\n of the model, but it's going to cost, you know, it's going to cost the company developing\n\n43:59.460 --> 44:04.460\n this model more money to develop that, and it also costs more money to build separate\n\n44:04.460 --> 44:07.500\n predictive models and to implement and deploy them.\n\n44:07.500 --> 44:14.100\n So even if you can find a way to avoid the tension between error and accuracy in training\n\n44:14.100 --> 44:20.720\n a model, you might push the cost somewhere else, like money, like development time, research\n\n44:20.720 --> 44:22.920\n time and the like.\n\n44:22.920 --> 44:30.200\n There are fundamentally difficult philosophical questions, in fairness, and we live in a very\n\n44:30.200 --> 44:34.160\n divisive political climate, outraged culture.\n\n44:34.160 --> 44:38.560\n There is alt right folks on 4chan, trolls.\n\n44:38.560 --> 44:43.320\n There is social justice warriors on Twitter.\n\n44:43.320 --> 44:49.920\n There's very divisive, outraged folks on all sides of every kind of system.\n\n44:49.920 --> 44:57.280\n How do you, how do we as engineers build ethical algorithms in such divisive culture?\n\n44:57.280 --> 44:59.540\n Do you think they could be disjoint?\n\n44:59.540 --> 45:04.700\n The human has to inject your values, and then you can optimize over those values.\n\n45:04.700 --> 45:09.560\n But in our times, when you start actually applying these systems, things get a little\n\n45:09.560 --> 45:13.100\n bit challenging for the public discourse.\n\n45:13.100 --> 45:14.920\n How do you think we can proceed?\n\n45:14.920 --> 45:21.000\n Yeah, I mean, for the most part in the book, a point that we try to take some pains to\n\n45:21.000 --> 45:29.560\n make is that we don't view ourselves or people like us as being in the position of deciding\n\n45:29.560 --> 45:34.960\n for society what the right social norms are, what the right definitions of fairness are.\n\n45:34.960 --> 45:41.660\n Our main point is to just show that if society or the relevant stakeholders in a particular\n\n45:41.660 --> 45:47.160\n domain can come to agreement on those sorts of things, there's a way of encoding that\n\n45:47.160 --> 45:50.720\n into algorithms in many cases, not in all cases.\n\n45:50.720 --> 45:55.640\n One other misconception that hopefully we definitely dispel is sometimes people read\n\n45:55.640 --> 46:00.880\n the title of the book and I think not unnaturally fear that what we're suggesting is that the\n\n46:00.880 --> 46:05.760\n algorithms themselves should decide what those social norms are and develop their own notions\n\n46:05.760 --> 46:10.160\n of fairness and privacy or ethics, and we're definitely not suggesting that.\n\n46:10.160 --> 46:13.920\n The title of the book is Ethical Algorithm, by the way, and I didn't think of that interpretation\n\n46:13.920 --> 46:14.920\n of the title.\n\n46:14.920 --> 46:15.920\n That's interesting.\n\n46:15.920 --> 46:16.920\n Yeah, yeah.\n\n46:16.920 --> 46:21.080\n I mean, especially these days where people are concerned about the robots becoming our\n\n46:21.080 --> 46:25.980\n overlords, the idea that the robots would also sort of develop their own social norms\n\n46:25.980 --> 46:29.360\n is just one step away from that.\n\n46:29.360 --> 46:35.240\n But I do think, obviously, despite disclaimer that people like us shouldn't be making those\n\n46:35.240 --> 46:40.880\n decisions for society, we are kind of living in a world where in many ways computer scientists\n\n46:40.880 --> 46:46.820\n have made some decisions that have fundamentally changed the nature of our society and democracy\n\n46:46.820 --> 46:53.240\n and sort of civil discourse and deliberation in ways that I think most people generally\n\n46:53.240 --> 46:55.720\n feel are bad these days, right?\n\n46:55.720 --> 47:01.120\n But they had to make, so if we look at people at the heads of companies and so on, they\n\n47:01.120 --> 47:02.800\n had to make those decisions, right?\n\n47:02.800 --> 47:08.440\n There has to be decisions, so there's two options, either you kind of put your head\n\n47:08.440 --> 47:14.000\n in the sand and don't think about these things and just let the algorithm do what it does,\n\n47:14.000 --> 47:19.320\n or you make decisions about what you value, you know, of injecting moral values into the\n\n47:19.320 --> 47:20.320\n algorithm.\n\n47:20.320 --> 47:26.760\n Look, I never mean to be an apologist for the tech industry, but I think it's a little\n\n47:26.760 --> 47:31.120\n bit too far to sort of say that explicit decisions were made about these things.\n\n47:31.120 --> 47:34.920\n So let's, for instance, take social media platforms, right?\n\n47:34.920 --> 47:40.160\n So like many inventions in technology and computer science, a lot of these platforms\n\n47:40.160 --> 47:45.120\n that we now use regularly kind of started as curiosities, right?\n\n47:45.120 --> 47:49.240\n I remember when things like Facebook came out and its predecessors like Friendster,\n\n47:49.240 --> 47:55.620\n which nobody even remembers now, people really wonder, like, why would anybody want to spend\n\n47:55.620 --> 47:56.620\n time doing that?\n\n47:56.620 --> 48:01.480\n I mean, even the web when it first came out, when it wasn't populated with much content\n\n48:01.480 --> 48:07.100\n and it was largely kind of hobbyists building their own kind of ramshackle websites, a lot\n\n48:07.100 --> 48:09.960\n of people looked at this and said, well, what is the purpose of this thing?\n\n48:09.960 --> 48:11.000\n Why is this interesting?\n\n48:11.000 --> 48:12.880\n Who would want to do this?\n\n48:12.880 --> 48:18.120\n And so even things like Facebook and Twitter, yes, technical decisions were made by engineers,\n\n48:18.120 --> 48:23.520\n by scientists, by executives in the design of those platforms, but, you know, I don't\n\n48:23.520 --> 48:32.240\n think 10 years ago anyone anticipated that those platforms, for instance, might kind\n\n48:32.240 --> 48:42.200\n of acquire undue, you know, influence on political discourse or on the outcomes of elections.\n\n48:42.200 --> 48:47.600\n And I think the scrutiny that these companies are getting now is entirely appropriate, but\n\n48:47.600 --> 48:53.080\n I think it's a little too harsh to kind of look at history and sort of say like, oh,\n\n48:53.080 --> 48:56.320\n you should have been able to anticipate that this would happen with your platform.\n\n48:56.320 --> 49:00.600\n And in this sort of gaming chapter of the book, one of the points we're making is that,\n\n49:00.600 --> 49:05.200\n you know, these platforms, right, they don't operate in isolation.\n\n49:05.200 --> 49:09.360\n So unlike the other topics we're discussing, like fairness and privacy, like those are\n\n49:09.360 --> 49:13.600\n really cases where algorithms can operate on your data and make decisions about you\n\n49:13.600 --> 49:16.300\n and you're not even aware of it, okay?\n\n49:16.300 --> 49:20.280\n Things like Facebook and Twitter, these are, you know, these are systems, right?\n\n49:20.280 --> 49:25.960\n These are social systems and their evolution, even their technical evolution because machine\n\n49:25.960 --> 49:31.680\n learning is involved, is driven in no small part by the behavior of the users themselves\n\n49:31.680 --> 49:35.680\n and how the users decide to adopt them and how to use them.\n\n49:35.680 --> 49:44.600\n And so, you know, I'm kind of like who really knew that, you know, until we saw it happen,\n\n49:44.600 --> 49:48.340\n who knew that these things might be able to influence the outcome of elections?\n\n49:48.340 --> 49:55.120\n Who knew that, you know, they might polarize political discourse because of the ability\n\n49:55.120 --> 50:00.840\n to, you know, decide who you interact with on the platform and also with the platform\n\n50:00.840 --> 50:05.080\n naturally using machine learning to optimize for your own interest that they would further\n\n50:05.080 --> 50:10.080\n isolate us from each other and, you know, like feed us all basically just the stuff\n\n50:10.080 --> 50:12.080\n that we already agreed with.\n\n50:12.080 --> 50:18.120\n So I think, you know, we've come to that outcome, I think, largely, but I think it's\n\n50:18.120 --> 50:24.240\n something that we all learned together, including the companies as these things happen.\n\n50:24.240 --> 50:29.940\n You asked like, well, are there algorithmic remedies to these kinds of things?\n\n50:29.940 --> 50:35.360\n And again, these are big problems that are not going to be solved with, you know, somebody\n\n50:35.360 --> 50:40.040\n going in and changing a few lines of code somewhere in a social media platform.\n\n50:40.040 --> 50:44.960\n But I do think in many ways, there are definitely ways of making things better.\n\n50:44.960 --> 50:49.360\n I mean, like an obvious recommendation that we make at some point in the book is like,\n\n50:49.360 --> 50:55.280\n look, you know, to the extent that we think that machine learning applied for personalization\n\n50:55.280 --> 51:03.480\n purposes in things like newsfeed, you know, or other platforms has led to polarization\n\n51:03.480 --> 51:07.940\n and intolerance of opposing viewpoints.\n\n51:07.940 --> 51:11.880\n As you know, right, these algorithms have models, right, and they kind of place people\n\n51:11.880 --> 51:17.700\n in some kind of metric space, and they place content in that space, and they sort of know\n\n51:17.700 --> 51:22.180\n the extent to which I have an affinity for a particular type of content.\n\n51:22.180 --> 51:26.400\n And by the same token, they also probably have that same model probably gives you a\n\n51:26.400 --> 51:32.760\n good idea of the stuff I'm likely to violently disagree with or be offended by, okay?\n\n51:32.760 --> 51:37.440\n So you know, in this case, there really is some knob you could tune that says like, instead\n\n51:37.440 --> 51:43.040\n of showing people only what they like and what they want, let's show them some stuff\n\n51:43.040 --> 51:46.160\n that we think that they don't like, or that's a little bit further away.\n\n51:46.160 --> 51:51.680\n And you could even imagine users being able to control this, you know, just like everybody\n\n51:51.680 --> 51:58.240\n gets a slider, and that slider says like, you know, how much stuff do you want to see\n\n51:58.240 --> 52:02.960\n that's kind of, you know, you might disagree with, or is at least further from your interest.\n\n52:02.960 --> 52:05.720\n It's almost like an exploration button.\n\n52:05.720 --> 52:08.360\n So just get your intuition.\n\n52:08.360 --> 52:15.160\n Do you think engagement, so like you staying on the platform, you're staying engaged.\n\n52:15.160 --> 52:19.920\n Do you think fairness, ideas of fairness won't emerge?\n\n52:19.920 --> 52:23.740\n Like how bad is it to just optimize for engagement?\n\n52:23.740 --> 52:28.440\n Do you think we'll run into big trouble if we're just optimizing for how much you love\n\n52:28.440 --> 52:29.440\n the platform?\n\n52:29.440 --> 52:34.800\n Well, I mean, optimizing for engagement kind of got us where we are.\n\n52:34.800 --> 52:39.960\n So do you, one, have faith that it's possible to do better?\n\n52:39.960 --> 52:44.240\n And two, if it is, how do we do better?\n\n52:44.240 --> 52:47.060\n I mean, it's definitely possible to do different, right?\n\n52:47.060 --> 52:51.700\n And again, you know, it's not as if I think that doing something different than optimizing\n\n52:51.700 --> 52:57.880\n for engagement won't cost these companies in real ways, including revenue and profitability\n\n52:57.880 --> 52:58.880\n potentially.\n\n52:58.880 --> 53:00.600\n In the short term at least.\n\n53:00.600 --> 53:01.600\n Yeah.\n\n53:01.600 --> 53:02.600\n In the short term.\n\n53:02.600 --> 53:03.600\n Right.\n\n53:03.600 --> 53:08.920\n And again, you know, if I worked at these companies, I'm sure that it would have seemed\n\n53:08.920 --> 53:12.640\n like the most natural thing in the world also to want to optimize engagement, right?\n\n53:12.640 --> 53:14.600\n And that's good for users in some sense.\n\n53:14.600 --> 53:19.600\n You want them to be, you know, vested in the platform and enjoying it and finding it useful,\n\n53:19.600 --> 53:21.660\n interesting, and or productive.\n\n53:21.660 --> 53:27.080\n But you know, my point is, is that the idea that there is, that it's sort of out of their\n\n53:27.080 --> 53:31.560\n hands as you said, or that there's nothing to do about it, never say never, but that\n\n53:31.560 --> 53:34.560\n strikes me as implausible as a machine learning person, right?\n\n53:34.560 --> 53:39.600\n I mean, these companies are driven by machine learning and this optimization of engagement\n\n53:39.600 --> 53:42.040\n is essentially driven by machine learning, right?\n\n53:42.040 --> 53:47.120\n It's driven by not just machine learning, but you know, very, very large scale A, B\n\n53:47.120 --> 53:53.080\n experimentation where you kind of tweak some element of the user interface or tweak some\n\n53:53.080 --> 53:59.520\n component of an algorithm or tweak some component or feature of your click through prediction\n\n53:59.520 --> 54:01.200\n model.\n\n54:01.200 --> 54:06.360\n And my point is, is that anytime you know how to optimize for something, you, you know,\n\n54:06.360 --> 54:10.600\n by def, almost by definition, that solution tells you how not to optimize for it or to\n\n54:10.600 --> 54:13.240\n do something different.\n\n54:13.240 --> 54:16.200\n Engagement can be measured.\n\n54:16.200 --> 54:25.320\n So sort of optimizing for sort of minimizing divisiveness or maximizing intellectual growth\n\n54:25.320 --> 54:30.160\n over the lifetime of a human being are very difficult to measure.\n\n54:30.160 --> 54:31.160\n That's right.\n\n54:31.160 --> 54:38.240\n And I'm not claiming that doing something different will immediately make it apparent\n\n54:38.240 --> 54:42.320\n that this is a good thing for society and in particular, I mean, I think one way of\n\n54:42.320 --> 54:47.400\n thinking about where we are on some of these social media platforms is that, you know,\n\n54:47.400 --> 54:50.880\n it kind of feels a bit like we're in a bad equilibrium, right?\n\n54:50.880 --> 54:55.920\n That these systems are helping us all kind of optimize something myopically and selfishly\n\n54:55.920 --> 55:02.280\n for ourselves and of course, from an individual standpoint at any given moment, like why would\n\n55:02.280 --> 55:07.280\n I want to see things in my newsfeed that I found irrelevant, offensive or, you know,\n\n55:07.280 --> 55:09.240\n or the like, okay?\n\n55:09.240 --> 55:15.320\n But you know, maybe by all of us, you know, having these platforms myopically optimized\n\n55:15.320 --> 55:20.920\n in our interests, we have reached a collective outcome as a society that we're unhappy with\n\n55:20.920 --> 55:21.920\n in different ways.\n\n55:21.920 --> 55:26.360\n Let's say with respect to things like, you know, political discourse and tolerance of\n\n55:26.360 --> 55:28.160\n opposing viewpoints.\n\n55:28.160 --> 55:34.840\n And if Mark Zuckerberg gave you a call and said, I'm thinking of taking a sabbatical,\n\n55:34.840 --> 55:37.440\n could you run Facebook for me for six months?\n\n55:37.440 --> 55:39.240\n What would you, how?\n\n55:39.240 --> 55:45.720\n I think no thanks would be my first response, but there are many aspects of being the head\n\n55:45.720 --> 55:51.200\n of the entire company that are kind of entirely exogenous to many of the things that we're\n\n55:51.200 --> 55:52.200\n discussing here.\n\n55:52.200 --> 55:53.200\n Yes.\n\n55:53.200 --> 55:58.680\n And so I don't really think I would need to be CEO of Facebook to kind of implement the,\n\n55:58.680 --> 56:02.720\n you know, more limited set of solutions that I might imagine.\n\n56:02.720 --> 56:08.940\n But I think one concrete thing they could do is they could experiment with letting people\n\n56:08.940 --> 56:17.020\n who chose to, to see more stuff in their newsfeed that is not entirely kind of chosen to optimize\n\n56:17.020 --> 56:22.500\n for their particular interests, beliefs, et cetera.\n\n56:22.500 --> 56:27.240\n So the, the kind of thing, so I could speak to YouTube, but I think Facebook probably\n\n56:27.240 --> 56:34.880\n does something similar is they're quite effective at automatically finding what sorts of groups\n\n56:34.880 --> 56:40.260\n you belong to, not based on race or gender or so on, but based on the kind of stuff you\n\n56:40.260 --> 56:43.120\n enjoy watching in the case of YouTube.\n\n56:43.120 --> 56:50.160\n Sort of, it's a, it's a difficult thing for Facebook or YouTube to then say, well, you\n\n56:50.160 --> 56:51.160\n know what?\n\n56:51.160 --> 56:54.700\n We're going to show you something from a very different cluster.\n\n56:54.700 --> 57:00.020\n Even though we believe algorithmically, you're unlikely to enjoy that thing sort of that's\n\n57:00.020 --> 57:02.340\n a weird jump to make.\n\n57:02.340 --> 57:07.020\n There has to be a human, like at the very top of that system that says, well, that will\n\n57:07.020 --> 57:09.840\n be longterm healthy for you.\n\n57:09.840 --> 57:11.880\n That's more than an algorithmic decision.\n\n57:11.880 --> 57:18.460\n Or that same person could say that'll be longterm healthy for the platform or for the platform's\n\n57:18.460 --> 57:22.560\n influence on society outside of the platform, right?\n\n57:22.560 --> 57:27.020\n And it, you know, it's easy for me to sit here and say these things, but conceptually\n\n57:27.020 --> 57:32.920\n I do not think that these are kind of totally or should, they shouldn't be kind of completely\n\n57:32.920 --> 57:34.800\n alien ideas, right?\n\n57:34.800 --> 57:40.880\n That, you know, you could try things like this and it wouldn't be, you know, we wouldn't\n\n57:40.880 --> 57:45.820\n have to invent entirely new science to do it because if we're all already embedded in\n\n57:45.820 --> 57:50.520\n some metric space and there's a notion of distance between you and me and every other,\n\n57:50.520 --> 57:56.060\n every piece of content, then, you know, we know exactly, you know, the same model that\n\n57:56.060 --> 58:03.340\n tells, you know, dictates how to make me really happy also tells how to make me as unhappy\n\n58:03.340 --> 58:04.960\n as possible as well.\n\n58:04.960 --> 58:05.960\n Right.\n\n58:05.960 --> 58:11.000\n The focus in your book and algorithmic fairness research today in general is on machine learning,\n\n58:11.000 --> 58:16.800\n like we said, is data, but, and just even the entire AI field right now is captivated\n\n58:16.800 --> 58:19.720\n with machine learning, with deep learning.\n\n58:19.720 --> 58:25.480\n Do you think ideas in symbolic AI or totally other kinds of approaches are interesting,\n\n58:25.480 --> 58:31.400\n useful in the space, have some promising ideas in terms of fairness?\n\n58:31.400 --> 58:35.240\n I haven't thought about that question specifically in the context of fairness.\n\n58:35.240 --> 58:39.040\n I definitely would agree with that statement in the large, right?\n\n58:39.040 --> 58:46.840\n I mean, I am, you know, one of many machine learning researchers who do believe that the\n\n58:46.840 --> 58:51.400\n great successes that have been shown in machine learning recently are great successes, but\n\n58:51.400 --> 58:53.280\n they're on a pretty narrow set of tasks.\n\n58:53.280 --> 59:00.480\n I mean, I don't, I don't think we're kind of notably closer to general artificial intelligence\n\n59:00.480 --> 59:03.360\n now than we were when I started my career.\n\n59:03.360 --> 59:08.640\n I mean, there's been progress and I do think that we are kind of as a community, maybe\n\n59:08.640 --> 59:12.000\n looking a bit where the light is, but the light is shining pretty bright there right\n\n59:12.000 --> 59:13.760\n now and we're finding a lot of stuff.\n\n59:13.760 --> 59:18.520\n So I don't want to like argue with the progress that's been made in areas like deep learning,\n\n59:18.520 --> 59:19.880\n for example.\n\n59:19.880 --> 59:25.080\n This touches another sort of related thing that you mentioned and that people might misinterpret\n\n59:25.080 --> 59:27.420\n from the title of your book, ethical algorithm.\n\n59:27.420 --> 59:31.800\n Is it possible for the algorithm to automate some of those decisions?\n\n59:31.800 --> 59:37.400\n Sort of a higher level decisions of what kind of, like what, what should be fair, what should\n\n59:37.400 --> 59:38.720\n be fair.\n\n59:38.720 --> 59:43.400\n The more you know about a field, the more aware you are of its limitations.\n\n59:43.400 --> 59:47.840\n And so I'm a, I'm pretty leery of sort of trying, you know, there's, there's so much\n\n59:47.840 --> 59:53.760\n we don't all, we already don't know in fairness, even when we're the ones picking the fairness\n\n59:53.760 --> 59:58.960\n definitions and, you know, comparing alternatives and thinking about the tensions between different\n\n59:58.960 --> 1:00:05.160\n definitions that the idea of kind of letting the algorithm start exploring as well.\n\n1:00:05.160 --> 1:00:08.560\n I definitely think, you know, this is a much narrower statement.\n\n1:00:08.560 --> 1:00:12.440\n I definitely think that kind of algorithmic auditing for different types of unfairness,\n\n1:00:12.440 --> 1:00:13.440\n right?\n\n1:00:13.440 --> 1:00:18.680\n So like in this gerrymandering example where I might want to prevent not just discrimination\n\n1:00:18.680 --> 1:00:23.960\n against very broad categories, but against combinations of broad categories.\n\n1:00:23.960 --> 1:00:27.700\n You know, you quickly get to a point where there's a lot of, a lot of categories.\n\n1:00:27.700 --> 1:00:33.520\n There's a lot of combinations of end features and, you know, you can use algorithmic techniques\n\n1:00:33.520 --> 1:00:38.000\n to sort of try to find the subgroups on which you're discriminating the most and try to\n\n1:00:38.000 --> 1:00:39.000\n fix that.\n\n1:00:39.000 --> 1:00:42.460\n That's actually kind of the form of one of the algorithms we developed for this fairness\n\n1:00:42.460 --> 1:00:44.240\n gerrymandering problem.\n\n1:00:44.240 --> 1:00:49.440\n But I'm, I'm, you know, partly because of our technological, you know, our sort of our\n\n1:00:49.440 --> 1:00:53.400\n scientific ignorance on these topics right now.\n\n1:00:53.400 --> 1:00:58.360\n And also partly just because these topics are so loaded emotionally for people that\n\n1:00:58.360 --> 1:01:00.440\n I just don't see the value.\n\n1:01:00.440 --> 1:01:03.920\n I mean, again, never say never, but I just don't think we're at a moment where it's\n\n1:01:03.920 --> 1:01:08.600\n a great time for computer scientists to be rolling out the idea like, hey, you know,\n\n1:01:08.600 --> 1:01:12.520\n you know, not only have we kind of figured fairness out, but, you know, we think the\n\n1:01:12.520 --> 1:01:16.880\n algorithm should start deciding what's fair or giving input on that decision.\n\n1:01:16.880 --> 1:01:22.080\n I just don't, it's like the cost benefit analysis to the field of kind of going there\n\n1:01:22.080 --> 1:01:24.520\n right now just doesn't seem worth it to me.\n\n1:01:24.520 --> 1:01:29.200\n That said, I should say that I think computer scientists should be more philosophically,\n\n1:01:29.200 --> 1:01:32.280\n like should enrich their thinking about these kinds of things.\n\n1:01:32.280 --> 1:01:38.020\n I think it's been too often used as an excuse for roboticists working on autonomous vehicles,\n\n1:01:38.020 --> 1:01:43.720\n for example, to not think about the human factor or psychology or safety in the same\n\n1:01:43.720 --> 1:01:47.440\n way like computer science design algorithms that have been sort of using it as an excuse.\n\n1:01:47.440 --> 1:01:51.640\n And I think it's time for basically everybody to become a computer scientist.\n\n1:01:51.640 --> 1:01:54.440\n I was about to agree with everything you said except that last point.\n\n1:01:54.440 --> 1:01:59.760\n I think that the other way of looking at it is that I think computer scientists, you know,\n\n1:01:59.760 --> 1:02:06.120\n and many of us are, but we need to weigh it out into the world more, right?\n\n1:02:06.120 --> 1:02:12.520\n I mean, just the influence that computer science and therefore computer scientists have had\n\n1:02:12.520 --> 1:02:21.520\n on society at large just like has exponentially magnified in the last 10 or 20 years or so.\n\n1:02:21.520 --> 1:02:26.560\n And you know, before when we were just tinkering around amongst ourselves and it didn't matter\n\n1:02:26.560 --> 1:02:32.360\n that much, there was no need for sort of computer scientists to be citizens of the world more\n\n1:02:32.360 --> 1:02:33.440\n broadly.\n\n1:02:33.440 --> 1:02:36.760\n And I think those days need to be over very, very fast.\n\n1:02:36.760 --> 1:02:40.720\n And I'm not saying everybody needs to do it, but to me, like the right way of doing it\n\n1:02:40.720 --> 1:02:44.120\n is to not to sort of think that everybody else is going to become a computer scientist.\n\n1:02:44.120 --> 1:02:49.200\n But you know, I think people are becoming more sophisticated about computer science,\n\n1:02:49.200 --> 1:02:50.200\n even lay people.\n\n1:02:50.200 --> 1:02:55.520\n You know, I think one of the reasons we decided to write this book is we thought 10 years\n\n1:02:55.520 --> 1:03:00.400\n ago I wouldn't have tried this just because I just didn't think that sort of people's\n\n1:03:00.400 --> 1:03:06.240\n awareness of algorithms and machine learning, you know, the general population would have\n\n1:03:06.240 --> 1:03:07.240\n been high.\n\n1:03:07.240 --> 1:03:12.060\n I mean, you would have had to first, you know, write one of the many books kind of just explicating\n\n1:03:12.060 --> 1:03:14.720\n that topic to a lay audience first.\n\n1:03:14.720 --> 1:03:18.900\n Now I think we're at the point where like lots of people without any technical training\n\n1:03:18.900 --> 1:03:22.800\n at all know enough about algorithms and machine learning that you can start getting to these\n\n1:03:22.800 --> 1:03:26.000\n nuances of things like ethical algorithms.\n\n1:03:26.000 --> 1:03:31.780\n I think we agree that there needs to be much more mixing, but I think a lot of the onus\n\n1:03:31.780 --> 1:03:35.360\n of that mixing needs to be on the computer science community.\n\n1:03:35.360 --> 1:03:36.360\n Yeah.\n\n1:03:36.360 --> 1:03:41.920\n So just to linger on the disagreement, because I do disagree with you on the point that I\n\n1:03:41.920 --> 1:03:50.780\n think if you're a biologist, if you're a chemist, if you're an MBA business person, all of those\n\n1:03:50.780 --> 1:03:57.160\n things you can, like if you learned a program, and not only program, if you learned to do\n\n1:03:57.160 --> 1:04:02.160\n machine learning, if you learned to do data science, you immediately become much more\n\n1:04:02.160 --> 1:04:04.200\n powerful in the kinds of things you can do.\n\n1:04:04.200 --> 1:04:11.600\n And therefore literature, like library sciences, like, so you were speaking, I think, I think\n\n1:04:11.600 --> 1:04:14.760\n it holds true what you're saying for the next few years.\n\n1:04:14.760 --> 1:04:21.520\n But long term, if you're interested to me, if you're interested in philosophy, you should\n\n1:04:21.520 --> 1:04:27.700\n learn a program, because then you can scrape data and study what people are thinking about\n\n1:04:27.700 --> 1:04:33.760\n on Twitter, and then start making philosophical conclusions about the meaning of life.\n\n1:04:33.760 --> 1:04:41.440\n I just feel like the access to data, the digitization of whatever problem you're trying to solve,\n\n1:04:41.440 --> 1:04:44.200\n will fundamentally change what it means to be a computer scientist.\n\n1:04:44.200 --> 1:04:51.200\n I mean, a computer scientist in 20, 30 years will go back to being Donald Knuth style theoretical\n\n1:04:51.200 --> 1:04:56.560\n computer science, and everybody would be doing basically, exploring the kinds of ideas that\n\n1:04:56.560 --> 1:04:57.560\n you explore in your book.\n\n1:04:57.560 --> 1:04:58.880\n It won't be a computer science major.\n\n1:04:58.880 --> 1:05:05.000\n Yeah, I mean, I don't think I disagree enough, but I think that that trend of more and more\n\n1:05:05.000 --> 1:05:11.600\n people in more and more disciplines adopting ideas from computer science, learning how\n\n1:05:11.600 --> 1:05:14.560\n to code, I think that that trend seems firmly underway.\n\n1:05:14.560 --> 1:05:21.000\n I mean, you know, like an interesting digressive question along these lines is maybe in 50\n\n1:05:21.000 --> 1:05:27.080\n years, there won't be computer science departments anymore, because the field will just sort\n\n1:05:27.080 --> 1:05:30.840\n of be ambient in all of the different disciplines.\n\n1:05:30.840 --> 1:05:35.720\n And people will look back and having a computer science department will look like having an\n\n1:05:35.720 --> 1:05:39.480\n electricity department or something that's like, you know, everybody uses this, it's\n\n1:05:39.480 --> 1:05:40.480\n just out there.\n\n1:05:40.480 --> 1:05:45.180\n I mean, I do think there will always be that kind of Knuth style core to it, but it's not\n\n1:05:45.180 --> 1:05:50.180\n an implausible path that we kind of get to the point where the academic discipline of\n\n1:05:50.180 --> 1:05:56.160\n computer science becomes somewhat marginalized because of its very success in kind of infiltrating\n\n1:05:56.160 --> 1:06:00.720\n all of science and society and the humanities, etcetera.\n\n1:06:00.720 --> 1:06:07.720\n What is differential privacy, or more broadly, algorithmic privacy?\n\n1:06:07.720 --> 1:06:15.040\n Algorithmic privacy more broadly is just the study or the notion of privacy definitions\n\n1:06:15.040 --> 1:06:19.580\n or norms being encoded inside of algorithms.\n\n1:06:19.580 --> 1:06:27.520\n And so, you know, I think we count among this body of work just, you know, the literature\n\n1:06:27.520 --> 1:06:33.980\n and practice of things like data anonymization, which we kind of at the beginning of our discussion\n\n1:06:33.980 --> 1:06:38.600\n of privacy say like, okay, this is sort of a notion of algorithmic privacy.\n\n1:06:38.600 --> 1:06:44.840\n It kind of tells you, you know, something to go do with data, but, you know, our view\n\n1:06:44.840 --> 1:06:50.120\n is that it's, and I think this is now, you know, quite widespread, that it's, you know,\n\n1:06:50.120 --> 1:06:57.320\n despite the fact that those notions of anonymization kind of redacting and coarsening are the most\n\n1:06:57.320 --> 1:07:03.700\n widely adopted technical solutions for data privacy, they are like deeply fundamentally\n\n1:07:03.700 --> 1:07:05.120\n flawed.\n\n1:07:05.120 --> 1:07:11.240\n And so, you know, to your first question, what is differential privacy?\n\n1:07:11.240 --> 1:07:16.680\n Differential privacy seems to be a much, much better notion of privacy that kind of avoids\n\n1:07:16.680 --> 1:07:24.520\n a lot of the weaknesses of anonymization notions while still letting us do useful stuff with\n\n1:07:24.520 --> 1:07:25.520\n data.\n\n1:07:25.520 --> 1:07:27.480\n What is anonymization of data?\n\n1:07:27.480 --> 1:07:34.000\n So by anonymization, I'm, you know, kind of referring to techniques like I have a database.\n\n1:07:34.000 --> 1:07:40.240\n The rows of that database are, let's say, individual people's medical records, okay?\n\n1:07:40.240 --> 1:07:43.840\n And I want to let people use that data.\n\n1:07:43.840 --> 1:07:49.480\n Maybe I want to let researchers access that data to build predictive models for some disease,\n\n1:07:49.480 --> 1:07:56.200\n but I'm worried that that will leak, you know, sensitive information about specific people's\n\n1:07:56.200 --> 1:07:57.640\n medical records.\n\n1:07:57.640 --> 1:08:01.680\n So anonymization broadly refers to the set of techniques where I say like, okay, I'm\n\n1:08:01.680 --> 1:08:06.160\n first going to like, I'm going to delete the column with people's names.\n\n1:08:06.160 --> 1:08:09.760\n I'm going to not put, you know, so that would be like a redaction, right?\n\n1:08:09.760 --> 1:08:12.040\n I'm just redacting that information.\n\n1:08:12.040 --> 1:08:17.040\n I am going to take ages and I'm not going to like say your exact age.\n\n1:08:17.040 --> 1:08:23.120\n I'm going to say whether you're, you know, zero to 10, 10 to 20, 20 to 30, I might put\n\n1:08:23.120 --> 1:08:27.520\n the first three digits of your zip code, but not the last two, et cetera, et cetera.\n\n1:08:27.520 --> 1:08:31.800\n And so the idea is that through some series of operations like this on the data, I anonymize\n\n1:08:31.800 --> 1:08:32.800\n it.\n\n1:08:32.800 --> 1:08:38.880\n You know, another term of art that's used is removing personally identifiable information.\n\n1:08:38.880 --> 1:08:45.600\n And you know, this is basically the most common way of providing data privacy, but that it's\n\n1:08:45.600 --> 1:08:50.240\n in a way that still lets people access the, some variant form of the data.\n\n1:08:50.240 --> 1:08:56.080\n So at a slightly broader picture, as you talk about what does anonymization mean when you\n\n1:08:56.080 --> 1:09:01.440\n have multiple database, like with a Netflix prize, when you can start combining stuff\n\n1:09:01.440 --> 1:09:02.440\n together.\n\n1:09:02.440 --> 1:09:05.400\n So this is exactly the problem with these notions, right?\n\n1:09:05.400 --> 1:09:10.900\n Is that notions of a anonymization, removing personally identifiable information, the kind\n\n1:09:10.900 --> 1:09:16.000\n of fundamental conceptual flaw is that, you know, these definitions kind of pretend as\n\n1:09:16.000 --> 1:09:21.240\n if the data set in question is the only data set that exists in the world or that ever\n\n1:09:21.240 --> 1:09:23.640\n will exist in the future.\n\n1:09:23.640 --> 1:09:28.080\n And of course, things like the Netflix prize and many, many other examples since the Netflix\n\n1:09:28.080 --> 1:09:33.320\n prize, I think that was one of the earliest ones though, you know, you can reidentify\n\n1:09:33.320 --> 1:09:38.540\n people that were, you know, that were anonymized in the data set by taking that anonymized\n\n1:09:38.540 --> 1:09:43.240\n data set and combining it with other allegedly anonymized data sets and maybe publicly available\n\n1:09:43.240 --> 1:09:44.480\n information about you.\n\n1:09:44.480 --> 1:09:45.480\n You know,\n\n1:09:45.480 --> 1:09:50.880\n for people who don't know the Netflix prize was, was being publicly released this data.\n\n1:09:50.880 --> 1:09:55.640\n So the names from those rows were removed, but what was released is the preference or\n\n1:09:55.640 --> 1:09:58.720\n the ratings of what movies you like and you don't like.\n\n1:09:58.720 --> 1:10:03.360\n And from that combined with other things, I think forum posts and so on, you can start\n\n1:10:03.360 --> 1:10:04.360\n to figure out\n\n1:10:04.360 --> 1:10:10.400\n I guess it was specifically the internet movie database where, where lots of Netflix users\n\n1:10:10.400 --> 1:10:15.280\n publicly rate their movie, you know, their movie preferences.\n\n1:10:15.280 --> 1:10:21.840\n And so the anonymized data and Netflix, when it's just this phenomenon, I think that we've\n\n1:10:21.840 --> 1:10:29.920\n all come to realize in the last decade or so is that just knowing a few apparently irrelevant\n\n1:10:29.920 --> 1:10:33.100\n innocuous things about you can often act as a fingerprint.\n\n1:10:33.100 --> 1:10:39.000\n Like if I know, you know, what, what rating you gave to these 10 movies and the date on\n\n1:10:39.000 --> 1:10:43.480\n which you entered these movies, this is almost like a fingerprint for you in the sea of all\n\n1:10:43.480 --> 1:10:44.480\n Netflix users.\n\n1:10:44.480 --> 1:10:49.760\n There were just another paper on this in science or nature of about a month ago that, you know,\n\n1:10:49.760 --> 1:10:51.240\n kind of 18 attributes.\n\n1:10:51.240 --> 1:10:57.120\n I mean, my favorite example of this is, was actually a paper from several years ago now\n\n1:10:57.120 --> 1:11:03.400\n where it was shown that just from your likes on Facebook, just from the time, you know,\n\n1:11:03.400 --> 1:11:09.520\n the things on which you clicked on the thumbs up button on the platform, not using any information,\n\n1:11:09.520 --> 1:11:14.720\n demographic information, nothing about who your friends are, just knowing the content\n\n1:11:14.720 --> 1:11:20.680\n that you had liked was enough to, you know, in the aggregate accurately predict things\n\n1:11:20.680 --> 1:11:27.280\n like sexual orientation, drug and alcohol use, whether you were the child of divorced parents.\n\n1:11:27.280 --> 1:11:32.080\n So we live in this era where, you know, even the apparently irrelevant data that we offer\n\n1:11:32.080 --> 1:11:38.760\n about ourselves on public platforms and forums often unbeknownst to us, more or less acts\n\n1:11:38.760 --> 1:11:42.480\n as signature or, you know, fingerprint.\n\n1:11:42.480 --> 1:11:46.980\n And that if you can kind of, you know, do a join between that kind of data and allegedly\n\n1:11:46.980 --> 1:11:50.720\n anonymized data, you have real trouble.\n\n1:11:50.720 --> 1:11:58.380\n So is there hope for any kind of privacy in a world where a few likes can identify you?\n\n1:11:58.380 --> 1:12:00.380\n So there is differential privacy, right?\n\n1:12:00.380 --> 1:12:01.380\n What is differential privacy?\n\n1:12:01.380 --> 1:12:06.100\n Yeah, so differential privacy basically is a kind of alternate, much stronger notion\n\n1:12:06.100 --> 1:12:10.280\n of privacy than these anonymization ideas.\n\n1:12:10.280 --> 1:12:18.760\n And, you know, it's a technical definition, but like the spirit of it is we compare two\n\n1:12:18.760 --> 1:12:20.320\n alternate worlds, okay?\n\n1:12:20.320 --> 1:12:26.120\n So let's suppose I'm a researcher and I want to do, you know, there's a database of medical\n\n1:12:26.120 --> 1:12:31.600\n records and one of them is yours, and I want to use that database of medical records to\n\n1:12:31.600 --> 1:12:33.800\n build a predictive model for some disease.\n\n1:12:33.800 --> 1:12:39.440\n So based on people's symptoms and test results and the like, I want to, you know, build a\n\n1:12:39.440 --> 1:12:42.180\n probably model predicting the probability that people have disease.\n\n1:12:42.180 --> 1:12:46.400\n So, you know, this is the type of scientific research that we would like to be allowed\n\n1:12:46.400 --> 1:12:48.060\n to continue.\n\n1:12:48.060 --> 1:12:53.400\n And in differential privacy, you ask a very particular counterfactual question.\n\n1:12:53.400 --> 1:12:57.480\n We basically compare two alternatives.\n\n1:12:57.480 --> 1:13:04.760\n One is when I do this, I build this model on the database of medical records, including\n\n1:13:04.760 --> 1:13:07.200\n your medical record.\n\n1:13:07.200 --> 1:13:15.320\n And the other one is where I do the same exercise with the same database with just your medical\n\n1:13:15.320 --> 1:13:16.320\n record removed.\n\n1:13:16.320 --> 1:13:22.280\n So basically, you know, it's two databases, one with N records in it and one with N minus\n\n1:13:22.280 --> 1:13:23.840\n one records in it.\n\n1:13:23.840 --> 1:13:27.960\n The N minus one records are the same, and the only one that's missing in the second\n\n1:13:27.960 --> 1:13:30.420\n case is your medical record.\n\n1:13:30.420 --> 1:13:40.580\n So differential privacy basically says that any harms that might come to you from the\n\n1:13:40.580 --> 1:13:47.640\n analysis in which your data was included are essentially nearly identical to the harms\n\n1:13:47.640 --> 1:13:52.720\n that would have come to you if the same analysis had been done without your medical record\n\n1:13:52.720 --> 1:13:53.720\n included.\n\n1:13:53.720 --> 1:13:58.280\n So in other words, this doesn't say that bad things cannot happen to you as a result of\n\n1:13:58.280 --> 1:13:59.760\n data analysis.\n\n1:13:59.760 --> 1:14:05.080\n It just says that these bad things were going to happen to you already, even if your data\n\n1:14:05.080 --> 1:14:06.080\n wasn't included.\n\n1:14:06.080 --> 1:14:12.360\n And to give a very concrete example, right, you know, like we discussed at some length,\n\n1:14:12.360 --> 1:14:17.800\n the study that, you know, in the 50s that was done that established the link between\n\n1:14:17.800 --> 1:14:19.960\n smoking and lung cancer.\n\n1:14:19.960 --> 1:14:25.200\n And we make the point that, like, well, if your data was used in that analysis and, you\n\n1:14:25.200 --> 1:14:28.980\n know, the world kind of knew that you were a smoker because, you know, there was no stigma\n\n1:14:28.980 --> 1:14:35.160\n associated with smoking before those findings, real harm might have come to you as a result\n\n1:14:35.160 --> 1:14:37.760\n of that study that your data was included in.\n\n1:14:37.760 --> 1:14:42.440\n In particular, your insurer now might have a higher posterior belief that you might have\n\n1:14:42.440 --> 1:14:44.360\n lung cancer and raise your premium.\n\n1:14:44.360 --> 1:14:47.820\n So you've suffered economic damage.\n\n1:14:47.820 --> 1:14:54.960\n But the point is, is that if the same analysis has been done with all the other N minus one\n\n1:14:54.960 --> 1:14:58.800\n medical records and just yours missing, the outcome would have been the same.\n\n1:14:58.800 --> 1:15:05.560\n Or your data wasn't idiosyncratically crucial to establishing the link between smoking and\n\n1:15:05.560 --> 1:15:10.440\n lung cancer because the link between smoking and lung cancer is like a fact about the world\n\n1:15:10.440 --> 1:15:14.820\n that can be discovered with any sufficiently large database of medical records.\n\n1:15:14.820 --> 1:15:17.320\n But that's a very low value of harm.\n\n1:15:17.320 --> 1:15:18.320\n Yeah.\n\n1:15:18.320 --> 1:15:20.560\n So that's showing that very little harm is done.\n\n1:15:20.560 --> 1:15:21.560\n Great.\n\n1:15:21.560 --> 1:15:24.760\n But how what is the mechanism of differential privacy?\n\n1:15:24.760 --> 1:15:27.600\n So that's the kind of beautiful statement of it.\n\n1:15:27.600 --> 1:15:30.440\n It's the mechanism by which privacy is preserved.\n\n1:15:30.440 --> 1:15:31.440\n Yeah.\n\n1:15:31.440 --> 1:15:34.600\n So it's basically by adding noise to computations, right?\n\n1:15:34.600 --> 1:15:40.400\n So the basic idea is that every differentially private algorithm, first of all, or every\n\n1:15:40.400 --> 1:15:45.380\n good differentially private algorithm, every useful one, is a probabilistic algorithm.\n\n1:15:45.380 --> 1:15:51.000\n So it doesn't, on a given input, if you gave the algorithm the same input multiple times,\n\n1:15:51.000 --> 1:15:55.760\n it would give different outputs each time from some distribution.\n\n1:15:55.760 --> 1:15:59.820\n And the way you achieve differential privacy algorithmically is by kind of carefully and\n\n1:15:59.820 --> 1:16:05.400\n tastefully adding noise to a computation in the right places.\n\n1:16:05.400 --> 1:16:11.600\n And to give a very concrete example, if I wanna compute the average of a set of numbers,\n\n1:16:11.600 --> 1:16:17.220\n the non private way of doing that is to take those numbers and average them and release\n\n1:16:17.220 --> 1:16:21.880\n like a numerically precise value for the average.\n\n1:16:21.880 --> 1:16:24.200\n In differential privacy, you wouldn't do that.\n\n1:16:24.200 --> 1:16:29.520\n You would first compute that average to numerical precisions, and then you'd add some noise\n\n1:16:29.520 --> 1:16:30.520\n to it, right?\n\n1:16:30.520 --> 1:16:37.520\n You'd add some kind of zero mean, Gaussian or exponential noise to it so that the actual\n\n1:16:37.520 --> 1:16:44.120\n value you output is not the exact mean, but it'll be close to the mean, but it'll be close...\n\n1:16:44.120 --> 1:16:50.560\n The noise that you add will sort of prove that nobody can kind of reverse engineer any\n\n1:16:50.560 --> 1:16:53.440\n particular value that went into the average.\n\n1:16:53.440 --> 1:16:56.200\n So noise is a savior.\n\n1:16:56.200 --> 1:17:01.640\n How many algorithms can be aided by adding noise?\n\n1:17:01.640 --> 1:17:07.040\n Yeah, so I'm a relatively recent member of the differential privacy community.\n\n1:17:07.040 --> 1:17:12.440\n My co author, Aaron Roth is really one of the founders of the field and has done a great\n\n1:17:12.440 --> 1:17:15.520\n deal of work and I've learned a tremendous amount working with him on it.\n\n1:17:15.520 --> 1:17:17.240\n It's a pretty grown up field already.\n\n1:17:17.240 --> 1:17:18.480\n Yeah, but now it's pretty mature.\n\n1:17:18.480 --> 1:17:22.080\n But I must admit, the first time I saw the definition of differential privacy, my reaction\n\n1:17:22.080 --> 1:17:28.360\n was like, wow, that is a clever definition and it's really making very strong promises.\n\n1:17:28.360 --> 1:17:34.920\n And I first saw the definition in much earlier days and my first reaction was like, well,\n\n1:17:34.920 --> 1:17:38.960\n my worry about this definition would be that it's a great definition of privacy, but that\n\n1:17:38.960 --> 1:17:43.180\n it'll be so restrictive that we won't really be able to use it.\n\n1:17:43.180 --> 1:17:47.200\n We won't be able to compute many things in a differentially private way.\n\n1:17:47.200 --> 1:17:51.920\n So that's one of the great successes of the field, I think, is in showing that the opposite\n\n1:17:51.920 --> 1:18:00.980\n is true and that most things that we know how to compute, absent any privacy considerations,\n\n1:18:00.980 --> 1:18:02.920\n can be computed in a differentially private way.\n\n1:18:02.920 --> 1:18:08.240\n So for example, pretty much all of statistics and machine learning can be done differentially\n\n1:18:08.240 --> 1:18:09.320\n privately.\n\n1:18:09.320 --> 1:18:15.120\n So pick your favorite machine learning algorithm, back propagation and neural networks, cart\n\n1:18:15.120 --> 1:18:21.060\n for decision trees, support vector machines, boosting, you name it, as well as classic\n\n1:18:21.060 --> 1:18:24.920\n hypothesis testing and the like in statistics.\n\n1:18:24.920 --> 1:18:29.720\n None of those algorithms are differentially private in their original form.\n\n1:18:29.720 --> 1:18:35.700\n All of them have modifications that add noise to the computation in different places in\n\n1:18:35.700 --> 1:18:39.120\n different ways that achieve differential privacy.\n\n1:18:39.120 --> 1:18:47.460\n So this really means that to the extent that we've become a scientific community very dependent\n\n1:18:47.460 --> 1:18:53.400\n on the use of machine learning and statistical modeling and data analysis, we really do have\n\n1:18:53.400 --> 1:19:02.760\n a path to provide privacy guarantees to those methods and so we can still enjoy the benefits\n\n1:19:02.760 --> 1:19:10.760\n of the data science era while providing rather robust privacy guarantees to individuals.\n\n1:19:10.760 --> 1:19:16.160\n So perhaps a slightly crazy question, but if we take the ideas of differential privacy\n\n1:19:16.160 --> 1:19:20.680\n and take it to the nature of truth that's being explored currently.\n\n1:19:20.680 --> 1:19:24.880\n So what's your most favorite and least favorite food?\n\n1:19:24.880 --> 1:19:25.880\n Hmm.\n\n1:19:25.880 --> 1:19:29.880\n I'm not a real foodie, so I'm a big fan of spaghetti.\n\n1:19:29.880 --> 1:19:30.880\n Spaghetti?\n\n1:19:30.880 --> 1:19:31.880\n Yeah.\n\n1:19:31.880 --> 1:19:35.840\n What do you really don't like?\n\n1:19:35.840 --> 1:19:37.280\n I really don't like cauliflower.\n\n1:19:37.280 --> 1:19:39.280\n Wow, I love cauliflower.\n\n1:19:39.280 --> 1:19:40.280\n Okay.\n\n1:19:40.280 --> 1:19:46.400\n Is there one way to protect your preference for spaghetti by having an information campaign\n\n1:19:46.400 --> 1:19:51.280\n bloggers and so on of bots saying that you like cauliflower?\n\n1:19:51.280 --> 1:19:56.640\n So like this kind of the same kind of noise ideas, I mean if you think of in our politics\n\n1:19:56.640 --> 1:20:01.920\n today there's this idea of Russia hacking our elections.\n\n1:20:01.920 --> 1:20:07.200\n What's meant there I believe is bots spreading different kinds of information.\n\n1:20:07.200 --> 1:20:10.480\n Is that a kind of privacy or is that too much of a stretch?\n\n1:20:10.480 --> 1:20:12.160\n No it's not a stretch.\n\n1:20:12.160 --> 1:20:19.320\n I've not seen those ideas, you know, that is not a technique that to my knowledge will\n\n1:20:19.320 --> 1:20:24.400\n provide differential privacy, but to give an example like one very specific example\n\n1:20:24.400 --> 1:20:30.240\n about what you're discussing is there was a very interesting project at NYU I think\n\n1:20:30.240 --> 1:20:38.720\n led by Helen Nissenbaum there in which they basically built a browser plugin that tried\n\n1:20:38.720 --> 1:20:41.640\n to essentially obfuscate your Google searches.\n\n1:20:41.640 --> 1:20:46.440\n So to the extent that you're worried that Google is using your searches to build, you\n\n1:20:46.440 --> 1:20:51.480\n know, predictive models about you to decide what ads to show you which they might very\n\n1:20:51.480 --> 1:20:56.040\n reasonably want to do, but if you object to that they built this widget you could plug\n\n1:20:56.040 --> 1:21:01.280\n in and basically whenever you put in a query into Google it would send that query to Google,\n\n1:21:01.280 --> 1:21:06.200\n but in the background all of the time from your browser it would just be sending this\n\n1:21:06.200 --> 1:21:11.800\n torrent of irrelevant queries to the search engine.\n\n1:21:11.800 --> 1:21:16.840\n So you know it's like a weed and chaff thing so you know out of every thousand queries\n\n1:21:16.840 --> 1:21:21.560\n let's say that Google was receiving from your browser one of them was one that you put in\n\n1:21:21.560 --> 1:21:27.300\n but the other 999 were not okay so it's the same kind of idea kind of you know privacy\n\n1:21:27.300 --> 1:21:29.680\n by obfuscation.\n\n1:21:29.680 --> 1:21:34.920\n So I think that's an interesting idea, doesn't give you differential privacy.\n\n1:21:34.920 --> 1:21:39.260\n It's also I was actually talking to somebody at one of the large tech companies recently\n\n1:21:39.260 --> 1:21:45.560\n about the fact that you know just this kind of thing that there are some times when the\n\n1:21:45.560 --> 1:21:53.120\n response to my data needs to be very specific to my data right like I type mountain biking\n\n1:21:53.120 --> 1:21:58.420\n into Google, I want results on mountain biking and I really want Google to know that I typed\n\n1:21:58.420 --> 1:22:01.880\n in mountain biking, I don't want noise added to that.\n\n1:22:01.880 --> 1:22:06.180\n And so I think there's sort of maybe even interesting technical questions around notions\n\n1:22:06.180 --> 1:22:10.800\n of privacy that are appropriate where you know it's not that my data is part of some\n\n1:22:10.800 --> 1:22:15.800\n aggregate like medical records and that we're trying to discover important correlations\n\n1:22:15.800 --> 1:22:20.960\n and facts about the world at large but rather you know there's a service that I really want\n\n1:22:20.960 --> 1:22:26.120\n to you know pay attention to my specific data yet I still want some kind of privacy guarantee\n\n1:22:26.120 --> 1:22:30.200\n and I think these kind of obfuscation ideas are sort of one way of getting at that but\n\n1:22:30.200 --> 1:22:32.160\n maybe there are others as well.\n\n1:22:32.160 --> 1:22:36.520\n So where do you think we'll land in this algorithm driven society in terms of privacy?\n\n1:22:36.520 --> 1:22:44.960\n So sort of China like Kai Fuli describes you know it's collecting a lot of data on its\n\n1:22:44.960 --> 1:22:52.360\n citizens but in the best form it's actually able to provide a lot of sort of protect human\n\n1:22:52.360 --> 1:22:57.320\n rights and provide a lot of amazing services and it's worst forms that can violate those\n\n1:22:57.320 --> 1:23:01.080\n human rights and limit services.\n\n1:23:01.080 --> 1:23:08.400\n So where do you think we'll land because algorithms are powerful when they use data.\n\n1:23:08.400 --> 1:23:12.900\n So as a society do you think we'll give over more data?\n\n1:23:12.900 --> 1:23:16.400\n Is it possible to protect the privacy of that data?\n\n1:23:16.400 --> 1:23:24.400\n So I'm optimistic about the possibility of you know balancing the desire for individual\n\n1:23:24.400 --> 1:23:32.360\n privacy and individual control of privacy with kind of societally and commercially beneficial\n\n1:23:32.360 --> 1:23:37.840\n uses of data not unrelated to differential privacy or suggestions that say like well\n\n1:23:37.840 --> 1:23:40.560\n individuals should have control of their data.\n\n1:23:40.560 --> 1:23:43.600\n They should be able to limit the uses of that data.\n\n1:23:43.600 --> 1:23:48.200\n They should even you know there's you know fledgling discussions going on in research\n\n1:23:48.200 --> 1:23:54.680\n circles about allowing people selective use of their data and being compensated for it.\n\n1:23:54.680 --> 1:23:59.480\n And then you get to sort of very interesting economic questions like pricing right.\n\n1:23:59.480 --> 1:24:05.360\n And one interesting idea is that maybe differential privacy would also you know be a conceptual\n\n1:24:05.360 --> 1:24:09.120\n framework in which you could talk about the relative value of different people's data\n\n1:24:09.120 --> 1:24:12.080\n like you know to demystify this a little bit.\n\n1:24:12.080 --> 1:24:17.320\n If I'm trying to build a predictive model for some rare disease and I'm trying to use\n\n1:24:17.320 --> 1:24:22.480\n machine learning to do it, it's easy to get negative examples because the disease is rare\n\n1:24:22.480 --> 1:24:23.740\n right.\n\n1:24:23.740 --> 1:24:30.880\n But I really want to have lots of people with the disease in my data set okay.\n\n1:24:30.880 --> 1:24:35.380\n And so somehow those people's data with respect to this application is much more valuable\n\n1:24:35.380 --> 1:24:37.840\n to me than just like the background population.\n\n1:24:37.840 --> 1:24:43.160\n And so maybe they should be compensated more for it.\n\n1:24:43.160 --> 1:24:48.800\n And so you know I think these are kind of very, very fledgling conceptual questions\n\n1:24:48.800 --> 1:24:54.000\n that maybe we'll have kind of technical thought on them sometime in the coming years.\n\n1:24:54.000 --> 1:24:56.760\n But I do think we'll you know to kind of get more directly answer your question.\n\n1:24:56.760 --> 1:25:02.760\n I think I'm optimistic at this point from what I've seen that we will land at some you\n\n1:25:02.760 --> 1:25:08.640\n know better compromise than we're at right now where again you know privacy guarantees\n\n1:25:08.640 --> 1:25:15.400\n are few far between and weak and users have very, very little control.\n\n1:25:15.400 --> 1:25:20.320\n And I'm optimistic that we'll land in something that you know provides better privacy overall\n\n1:25:20.320 --> 1:25:22.820\n and more individual control of data and privacy.\n\n1:25:22.820 --> 1:25:27.740\n But you know I think to get there it's again just like fairness it's not going to be enough\n\n1:25:27.740 --> 1:25:29.560\n to propose algorithmic solutions.\n\n1:25:29.560 --> 1:25:34.880\n There's going to have to be a whole kind of regulatory legal process that prods companies\n\n1:25:34.880 --> 1:25:38.880\n and other parties to kind of adopt solutions.\n\n1:25:38.880 --> 1:25:43.040\n And I think you've mentioned the word control a lot and I think giving people control that's\n\n1:25:43.040 --> 1:25:48.200\n something that people don't quite have in a lot of these algorithms and that's a really\n\n1:25:48.200 --> 1:25:50.540\n interesting idea of giving them control.\n\n1:25:50.540 --> 1:25:57.920\n Some of that is actually literally an interface design question sort of just enabling because\n\n1:25:57.920 --> 1:26:00.440\n I think it's good for everybody to give users control.\n\n1:26:00.440 --> 1:26:06.160\n It's almost not a trade off except that you have to hire people that are good at interface\n\n1:26:06.160 --> 1:26:07.160\n design.\n\n1:26:07.160 --> 1:26:08.160\n Yeah.\n\n1:26:08.160 --> 1:26:13.080\n I mean the other thing that has to be said right is that you know it's a cliche but you\n\n1:26:13.080 --> 1:26:21.720\n know we as the users of many systems platforms and apps you know we are the product.\n\n1:26:21.720 --> 1:26:23.120\n We are not the customer.\n\n1:26:23.120 --> 1:26:26.760\n The customer are advertisers and our data is the product.\n\n1:26:26.760 --> 1:26:27.760\n Okay.\n\n1:26:27.760 --> 1:26:32.640\n So it's one thing to kind of suggest more individual control of data and privacy and\n\n1:26:32.640 --> 1:26:40.480\n uses but this you know if this happens in sufficient degree it will upend the entire\n\n1:26:40.480 --> 1:26:44.520\n economic model that has supported the internet to date.\n\n1:26:44.520 --> 1:26:50.040\n And so some other economic model will have to be you know we'll have to replace it.\n\n1:26:50.040 --> 1:26:56.480\n So the idea of markets you mentioned by exposing the economic model to the people they will\n\n1:26:56.480 --> 1:26:57.920\n then become a market.\n\n1:26:57.920 --> 1:27:00.280\n They could be participants in it.\n\n1:27:00.280 --> 1:27:04.680\n And you know this isn't you know this is not a weird idea right because there are markets\n\n1:27:04.680 --> 1:27:05.720\n for data already.\n\n1:27:05.720 --> 1:27:10.080\n It's just that consumers are not participants and there's like you know there's sort of\n\n1:27:10.080 --> 1:27:14.780\n you know publishers and content providers on one side that have inventory and then their\n\n1:27:14.780 --> 1:27:19.680\n advertisers on the others and you know you know Google and Facebook are running you know\n\n1:27:19.680 --> 1:27:25.540\n they're pretty much their entire revenue stream is by running two sided markets between those\n\n1:27:25.540 --> 1:27:27.380\n parties right.\n\n1:27:27.380 --> 1:27:32.800\n And so it's not a crazy idea that there would be like a three sided market or that you know\n\n1:27:32.800 --> 1:27:37.080\n that on one side of the market or the other we would have proxies representing our interest.\n\n1:27:37.080 --> 1:27:43.080\n It's not you know it's not a crazy idea but it would it's not a crazy technical idea but\n\n1:27:43.080 --> 1:27:49.920\n it would have pretty extreme economic consequences.\n\n1:27:49.920 --> 1:27:55.520\n Speaking of markets a lot of fascinating aspects of this world arise not from individual human\n\n1:27:55.520 --> 1:27:59.880\n beings but from the interaction of human beings.\n\n1:27:59.880 --> 1:28:02.080\n You've done a lot of work in game theory.\n\n1:28:02.080 --> 1:28:07.360\n First can you say what is game theory and how does it help us model and study?\n\n1:28:07.360 --> 1:28:11.080\n Yeah game theory of course let us give credit where it's due.\n\n1:28:11.080 --> 1:28:16.300\n You know it comes from the economist first and foremost but as I've mentioned before\n\n1:28:16.300 --> 1:28:22.000\n like you know computer scientists never hesitate to wander into other people's turf and so\n\n1:28:22.000 --> 1:28:26.520\n there is now this 20 year old field called algorithmic game theory.\n\n1:28:26.520 --> 1:28:33.240\n But you know game theory first and foremost is a mathematical framework for reasoning\n\n1:28:33.240 --> 1:28:40.240\n about collective outcomes in systems of interacting individuals.\n\n1:28:40.240 --> 1:28:46.040\n You know so you need at least two people to get started in game theory and many people\n\n1:28:46.040 --> 1:28:50.560\n are probably familiar with Prisoner's Dilemma as kind of a classic example of game theory\n\n1:28:50.560 --> 1:28:57.000\n and a classic example where everybody looking out for their own individual interests leads\n\n1:28:57.000 --> 1:29:02.560\n to a collective outcome that's kind of worse for everybody than what might be possible\n\n1:29:02.560 --> 1:29:05.200\n if they cooperated for example.\n\n1:29:05.200 --> 1:29:09.780\n But cooperation is not an equilibrium in Prisoner's Dilemma.\n\n1:29:09.780 --> 1:29:16.120\n And so my work in the field of algorithmic game theory more generally in these areas\n\n1:29:16.120 --> 1:29:24.720\n kind of looks at settings in which the number of actors is potentially extraordinarily large\n\n1:29:24.720 --> 1:29:31.160\n and their incentives might be quite complicated and kind of hard to model directly but you\n\n1:29:31.160 --> 1:29:36.120\n still want kind of algorithmic ways of kind of predicting what will happen or influencing\n\n1:29:36.120 --> 1:29:39.880\n what will happen in the design of platforms.\n\n1:29:39.880 --> 1:29:47.160\n So what to you is the most beautiful idea that you've encountered in game theory?\n\n1:29:47.160 --> 1:29:48.160\n There's a lot of them.\n\n1:29:48.160 --> 1:29:50.760\n I'm a big fan of the field.\n\n1:29:50.760 --> 1:29:56.400\n I mean you know I mean technical answers to that of course would include Nash's work just\n\n1:29:56.400 --> 1:30:02.640\n establishing that you know there is a competitive equilibrium under very very general circumstances\n\n1:30:02.640 --> 1:30:09.840\n which in many ways kind of put the field on a firm conceptual footing because if you don't\n\n1:30:09.840 --> 1:30:14.280\n have equilibrium it's kind of hard to ever reason about what might happen since you know\n\n1:30:14.280 --> 1:30:16.200\n there's just no stability.\n\n1:30:16.200 --> 1:30:20.680\n So just the idea that stability can emerge when there's multiple.\n\n1:30:20.680 --> 1:30:23.840\n Not that it will necessarily emerge just that it's possible right.\n\n1:30:23.840 --> 1:30:28.580\n Like the existence of equilibrium doesn't mean that sort of natural iterative behavior\n\n1:30:28.580 --> 1:30:30.640\n will necessarily lead to it.\n\n1:30:30.640 --> 1:30:31.640\n In the real world.\n\n1:30:31.640 --> 1:30:32.640\n Yeah.\n\n1:30:32.640 --> 1:30:35.960\n Maybe answering a slightly less personally than you asked the question I think within\n\n1:30:35.960 --> 1:30:43.760\n the field of algorithmic game theory perhaps the single most important kind of technical\n\n1:30:43.760 --> 1:30:49.640\n contribution that's been made is the realization between close connections between machine\n\n1:30:49.640 --> 1:30:53.840\n learning and game theory and in particular between game theory and the branch of machine\n\n1:30:53.840 --> 1:31:00.600\n learning that's known as no regret learning and this sort of provides a very general framework\n\n1:31:00.600 --> 1:31:07.460\n in which a bunch of players interacting in a game or a system each one kind of doing\n\n1:31:07.460 --> 1:31:12.440\n something that's in their self interest will actually kind of reach an equilibrium and\n\n1:31:12.440 --> 1:31:18.960\n actually reach an equilibrium in a you know a pretty you know a rather you know short\n\n1:31:18.960 --> 1:31:21.400\n amount of steps.\n\n1:31:21.400 --> 1:31:30.120\n So you kind of mentioned acting greedily can somehow end up pretty good for everybody.\n\n1:31:30.120 --> 1:31:31.320\n Or pretty bad.\n\n1:31:31.320 --> 1:31:32.320\n Or pretty bad.\n\n1:31:32.320 --> 1:31:33.320\n Yeah.\n\n1:31:33.320 --> 1:31:34.320\n It will end up stable.\n\n1:31:34.320 --> 1:31:35.320\n Yeah.\n\n1:31:35.320 --> 1:31:36.320\n Right.\n\n1:31:36.320 --> 1:31:41.500\n And and you know stability or equilibrium by itself is neither is not necessarily either\n\n1:31:41.500 --> 1:31:43.220\n a good thing or a bad thing.\n\n1:31:43.220 --> 1:31:45.840\n So what's the connection between machine learning and the ideas.\n\n1:31:45.840 --> 1:31:50.960\n Well I think we kind of talked about these ideas already in kind of a non technical way\n\n1:31:50.960 --> 1:31:57.200\n which is maybe the more interesting way of understanding them first which is you know\n\n1:31:57.200 --> 1:32:04.840\n we have many systems platforms and apps these days that work really hard to use our data\n\n1:32:04.840 --> 1:32:12.120\n and the data of everybody else on the platform to selfishly optimize on behalf of each user.\n\n1:32:12.120 --> 1:32:13.120\n OK.\n\n1:32:13.120 --> 1:32:17.960\n So you know let me let me give I think the cleanest example which is just driving apps\n\n1:32:17.960 --> 1:32:24.040\n navigation apps like you know Google Maps and Waze where you know miraculously compared\n\n1:32:24.040 --> 1:32:28.680\n to when I was growing up at least you know the objective would be the same when you wanted\n\n1:32:28.680 --> 1:32:33.440\n to drive from point A to point B spend the least time driving not necessarily minimize\n\n1:32:33.440 --> 1:32:35.840\n the distance but minimize the time.\n\n1:32:35.840 --> 1:32:36.840\n Right.\n\n1:32:36.840 --> 1:32:41.080\n And when I was growing up like the only resources you had to do that were like maps in the car\n\n1:32:41.080 --> 1:32:46.540\n which literally just told you what roads were available and then you might have like half\n\n1:32:46.540 --> 1:32:51.680\n hourly traffic reports just about the major freeways but not about side roads.\n\n1:32:51.680 --> 1:32:54.040\n So you were pretty much on your own.\n\n1:32:54.040 --> 1:32:57.920\n And now we've got these apps you pull it out and you say I want to go from point A to point\n\n1:32:57.920 --> 1:33:03.360\n B and in response kind of to what everybody else is doing if you like what all the other\n\n1:33:03.360 --> 1:33:09.280\n players in this game are doing right now here's the you know the route that minimizes your\n\n1:33:09.280 --> 1:33:10.280\n driving time.\n\n1:33:10.280 --> 1:33:16.560\n So it is really kind of computing a selfish best response for each of us in response to\n\n1:33:16.560 --> 1:33:20.240\n what all of the rest of us are doing at any given moment.\n\n1:33:20.240 --> 1:33:26.280\n And so you know I think it's quite fair to think of these apps as driving or nudging\n\n1:33:26.280 --> 1:33:32.560\n us all towards the competitive or Nash equilibrium of that game.\n\n1:33:32.560 --> 1:33:36.320\n Now you might ask like well that sounds great why is that a bad thing.\n\n1:33:36.320 --> 1:33:45.400\n Well you know it's known both in theory and with some limited studies from actual like\n\n1:33:45.400 --> 1:33:52.660\n traffic data that all of us being in this competitive equilibrium might cause our collective\n\n1:33:52.660 --> 1:33:59.760\n driving time to be higher maybe significantly higher than it would be under other solutions.\n\n1:33:59.760 --> 1:34:04.320\n And then you have to talk about what those other solutions might be and what the algorithms\n\n1:34:04.320 --> 1:34:07.760\n to implement them are which we do discuss in the kind of game theory chapter of the\n\n1:34:07.760 --> 1:34:09.880\n book.\n\n1:34:09.880 --> 1:34:17.040\n But similarly you know on social media platforms or on Amazon you know all these algorithms\n\n1:34:17.040 --> 1:34:22.000\n that are essentially trying to optimize our behalf they're driving us in a colloquial\n\n1:34:22.000 --> 1:34:26.920\n sense towards some kind of competitive equilibrium and you know one of the most important lessons\n\n1:34:26.920 --> 1:34:30.360\n of game theory is that just because we're at equilibrium doesn't mean that there's not\n\n1:34:30.360 --> 1:34:35.720\n a solution in which some or maybe even all of us might be better off.\n\n1:34:35.720 --> 1:34:39.080\n And then the connection to machine learning of course is that in all these platforms I've\n\n1:34:39.080 --> 1:34:44.320\n mentioned the optimization that they're doing on our behalf is driven by machine learning\n\n1:34:44.320 --> 1:34:48.040\n you know like predicting where the traffic will be predicting what products I'm going\n\n1:34:48.040 --> 1:34:52.220\n to like predicting what would make me happy in my newsfeed.\n\n1:34:52.220 --> 1:34:56.720\n Now in terms of the stability and the promise of that I have to ask just out of curiosity\n\n1:34:56.720 --> 1:35:02.600\n how stable are these mechanisms that you game theory is just the economist came up with\n\n1:35:02.600 --> 1:35:08.040\n and we all know that economists don't live in the real world just kidding sort of what's\n\n1:35:08.040 --> 1:35:15.720\n do you think when we look at the fact that we haven't blown ourselves up from the from\n\n1:35:15.720 --> 1:35:21.000\n a game theoretic concept of mutually shared destruction what are the odds that we destroy\n\n1:35:21.000 --> 1:35:28.400\n ourselves with nuclear weapons as one example of a stable game theoretic system?\n\n1:35:28.400 --> 1:35:32.080\n Just to prime your viewers a little bit I mean I think you're referring to the fact\n\n1:35:32.080 --> 1:35:38.400\n that game theory was taken quite seriously back in the 60s as a tool for reasoning about\n\n1:35:38.400 --> 1:35:45.160\n kind of Soviet US nuclear armament disarmament detente things like that.\n\n1:35:45.160 --> 1:35:52.320\n I'll be honest as huge of a fan as I am of game theory and its kind of rich history it\n\n1:35:52.320 --> 1:35:57.700\n still surprises me that you know you had people at the RAND Corporation back in those days\n\n1:35:57.700 --> 1:36:02.800\n kind of drawing up you know two by two tables and one the row player is you know the US\n\n1:36:02.800 --> 1:36:08.240\n and the column player is Russia and that they were taking seriously you know I'm sure if\n\n1:36:08.240 --> 1:36:12.840\n I was there maybe it wouldn't have seemed as naive as it does at the time you know.\n\n1:36:12.840 --> 1:36:15.440\n Seems to have worked which is why it seems naive.\n\n1:36:15.440 --> 1:36:16.440\n Well we're still here.\n\n1:36:16.440 --> 1:36:17.960\n We're still here in that sense.\n\n1:36:17.960 --> 1:36:22.600\n Yeah even though I kind of laugh at those efforts they were more sensible then than\n\n1:36:22.600 --> 1:36:26.540\n they would be now right because there were sort of only two nuclear powers at the time\n\n1:36:26.540 --> 1:36:32.480\n and you didn't have to worry about deterring new entrants and who was developing the capacity\n\n1:36:32.480 --> 1:36:39.120\n and so we have many you know it's definitely a game with more players now and more potential\n\n1:36:39.120 --> 1:36:40.120\n entrants.\n\n1:36:40.120 --> 1:36:46.200\n I'm not in general somebody who advocates using kind of simple mathematical models when\n\n1:36:46.200 --> 1:36:51.840\n the stakes are as high as things like that and the complexities are very political and\n\n1:36:51.840 --> 1:36:55.760\n social but we are still here.\n\n1:36:55.760 --> 1:37:00.600\n So you've worn many hats one of which the one that first caused me to become a big fan\n\n1:37:00.600 --> 1:37:04.460\n of your work many years ago is algorithmic trading.\n\n1:37:04.460 --> 1:37:08.520\n So I have to just ask a question about this because you have so much fascinating work\n\n1:37:08.520 --> 1:37:15.820\n there in the 21st century what role do you think algorithms have in space of trading\n\n1:37:15.820 --> 1:37:19.080\n investment in the financial sector?\n\n1:37:19.080 --> 1:37:27.160\n Yeah it's a good question I mean in the time I've spent on Wall Street and in finance you\n\n1:37:27.160 --> 1:37:31.320\n know I've seen a clear progression and I think it's a progression that kind of models the\n\n1:37:31.320 --> 1:37:38.640\n use of algorithms and automation more generally in society which is you know the things that\n\n1:37:38.640 --> 1:37:44.240\n kind of get taken over by the algos first are sort of the things that computers are\n\n1:37:44.240 --> 1:37:50.320\n obviously better at than people right so you know so first of all there needed to be this\n\n1:37:50.320 --> 1:37:56.200\n era of automation right where just you know financial exchanges became largely electronic\n\n1:37:56.200 --> 1:38:01.720\n which then enabled the possibility of you know trading becoming more algorithmic because\n\n1:38:01.720 --> 1:38:06.720\n once you know that exchanges are electronic an algorithm can submit an order through an\n\n1:38:06.720 --> 1:38:11.800\n API just as well as a human can do at a monitor quickly can read all the data so yeah and\n\n1:38:11.800 --> 1:38:18.800\n so you know I think the places where algorithmic trading have had the greatest inroads and\n\n1:38:18.800 --> 1:38:24.560\n had the first inroads were in kind of execution problems kind of optimized execution problems\n\n1:38:24.560 --> 1:38:30.440\n so what I mean by that is at a large brokerage firm for example one of the lines of business\n\n1:38:30.440 --> 1:38:36.320\n might be on behalf of large institutional clients taking you know what we might consider\n\n1:38:36.320 --> 1:38:40.280\n difficult trade so it's not like a mom and pop investor saying I want to buy a hundred\n\n1:38:40.280 --> 1:38:45.940\n shares of Microsoft it's a large hedge fund saying you know I want to buy a very very\n\n1:38:45.940 --> 1:38:52.760\n large stake in Apple and I want to do it over the span of a day and it's such a large volume\n\n1:38:52.760 --> 1:38:57.260\n that if you're not clever about how you break that trade up not just over time but over\n\n1:38:57.260 --> 1:39:02.560\n perhaps multiple different electronic exchanges that all let you trade Apple on their platform\n\n1:39:02.560 --> 1:39:07.760\n you know you will you will move you'll push prices around in a way that hurts your your\n\n1:39:07.760 --> 1:39:11.600\n execution so you know this is the kind of you know this is an optimization problem this\n\n1:39:11.600 --> 1:39:19.800\n is a control problem right and so machines are better we we know how to design algorithms\n\n1:39:19.800 --> 1:39:23.600\n you know that are better at that kind of thing than a person is going to be able to do because\n\n1:39:23.600 --> 1:39:29.320\n we can take volumes of historical and real time data to kind of optimize the schedule\n\n1:39:29.320 --> 1:39:35.080\n with which we trade and you know similarly high frequency trading you know which is closely\n\n1:39:35.080 --> 1:39:41.480\n related but not the same as optimized execution where you're just trying to spot very very\n\n1:39:41.480 --> 1:39:48.520\n temporary you know mispricings between exchanges or within an asset itself or just predict\n\n1:39:48.520 --> 1:39:54.800\n directional movement of a stock because of the kind of very very low level granular buying\n\n1:39:54.800 --> 1:40:00.440\n and selling data in the in the exchange machines are good at this kind of stuff it's kind of\n\n1:40:00.440 --> 1:40:08.080\n like the mechanics of trading what about the can machines do long terms of prediction yeah\n\n1:40:08.080 --> 1:40:13.280\n so I think we are in an era where you know clearly there have been some very successful\n\n1:40:13.280 --> 1:40:19.760\n you know quant hedge funds that are you know in what we would traditionally call you know\n\n1:40:19.760 --> 1:40:24.520\n still in this the stat arb regime like so you know what's that stat arb referring to\n\n1:40:24.520 --> 1:40:28.920\n statistical arbitrage but but for the purposes of this conversation what it really means\n\n1:40:28.920 --> 1:40:35.840\n is making directional predictions in asset price movement or returns your prediction\n\n1:40:35.840 --> 1:40:42.100\n about that directional movement is good for you know you you have a view that it's valid\n\n1:40:42.100 --> 1:40:48.440\n for some period of time between a few seconds and a few days and that's the amount of time\n\n1:40:48.440 --> 1:40:51.920\n that you're going to kind of get into the position hold it and then hopefully be right\n\n1:40:51.920 --> 1:40:57.360\n about the directional movement and you know buy low and sell high as the cliche goes.\n\n1:40:57.360 --> 1:41:04.300\n So that is a you know kind of a sweet spot I think for quant trading and investing right\n\n1:41:04.300 --> 1:41:11.920\n now and has been for some time when you really get to kind of more Warren Buffett style timescales\n\n1:41:11.920 --> 1:41:16.800\n right like you know my cartoon of Warren Buffett is that you know Warren Buffett sits and thinks\n\n1:41:16.800 --> 1:41:22.360\n what the long term value of Apple really should be and he doesn't even look at what Apple\n\n1:41:22.360 --> 1:41:27.400\n is doing today he just decides you know you know I think that this is what its long term\n\n1:41:27.400 --> 1:41:31.960\n value is and it's far from that right now and so I'm going to buy some Apple or you\n\n1:41:31.960 --> 1:41:37.880\n know short some Apple and I'm going to I'm going to sit on that for 10 or 20 years okay.\n\n1:41:37.880 --> 1:41:45.600\n So when you're at that kind of timescale or even more than just a few days all kinds of\n\n1:41:45.600 --> 1:41:51.600\n other sources of risk and information you know so now you're talking about holding things\n\n1:41:51.600 --> 1:41:56.080\n through recessions and economic cycles, wars can break out.\n\n1:41:56.080 --> 1:41:59.080\n So there you have to understand human nature at a level that.\n\n1:41:59.080 --> 1:42:03.820\n Yeah and you need to just be able to ingest many many more sources of data that are on\n\n1:42:03.820 --> 1:42:06.380\n wildly different timescales right.\n\n1:42:06.380 --> 1:42:13.120\n So if I'm an HFT I'm a high frequency trader like I don't I don't I really my main source\n\n1:42:13.120 --> 1:42:18.000\n of data is just the data from the exchanges themselves about the activity in the exchanges\n\n1:42:18.000 --> 1:42:22.880\n right and maybe I need to pay you know I need to keep an eye on the news right because you\n\n1:42:22.880 --> 1:42:29.040\n know that can cause sudden you know the CEO gets caught in a scandal or you know gets\n\n1:42:29.040 --> 1:42:33.720\n run over by a bus or something that can cause very sudden changes but you know I don't need\n\n1:42:33.720 --> 1:42:38.480\n to understand economic cycles I don't need to understand recessions I don't need to worry\n\n1:42:38.480 --> 1:42:43.600\n about the political situation or war breaking out in this part of the world because you\n\n1:42:43.600 --> 1:42:49.720\n know all I need to know is as long as that's not going to happen in the next 500 milliseconds\n\n1:42:49.720 --> 1:42:52.440\n then you know my model is good.\n\n1:42:52.440 --> 1:42:55.760\n When you get to these longer timescales you really have to worry about that kind of stuff\n\n1:42:55.760 --> 1:42:59.280\n and people in the machine learning community are starting to think about this.\n\n1:42:59.280 --> 1:43:06.760\n We held a we jointly sponsored a workshop at Penn with the Federal Reserve Bank of Philadelphia\n\n1:43:06.760 --> 1:43:10.960\n a little more than a year ago on you know I think the title is something like machine\n\n1:43:10.960 --> 1:43:14.120\n learning for macroeconomic prediction.\n\n1:43:14.120 --> 1:43:19.440\n You know macroeconomic referring specifically to these longer timescales and you know it\n\n1:43:19.440 --> 1:43:26.800\n was an interesting conference but it you know my it left me with greater confidence that\n\n1:43:26.800 --> 1:43:32.440\n we have a long way to go to you know and so I think that people that you know in the grand\n\n1:43:32.440 --> 1:43:37.480\n scheme of things you know if somebody asked me like well whose job on Wall Street is safe\n\n1:43:37.480 --> 1:43:42.840\n from the bots I think people that are at that longer you know timescale and have that appetite\n\n1:43:42.840 --> 1:43:49.320\n for all the risks involved in long term investing and that really need kind of not just algorithms\n\n1:43:49.320 --> 1:43:54.480\n that can optimize from data but they need views on stuff they need views on the political\n\n1:43:54.480 --> 1:44:01.000\n landscape economic cycles and the like and I think you know they're they're they're pretty\n\n1:44:01.000 --> 1:44:02.680\n safe for a while as far as I can tell.\n\n1:44:02.680 --> 1:44:08.320\n So Warren Buffett's job is not seeing you know a robo Warren Buffett anytime soon.\n\n1:44:08.320 --> 1:44:10.080\n Give him comfort.\n\n1:44:10.080 --> 1:44:11.160\n Last question.\n\n1:44:11.160 --> 1:44:18.320\n If you could go back to if there's a day in your life you could relive because it made\n\n1:44:18.320 --> 1:44:21.280\n you truly happy.\n\n1:44:21.280 --> 1:44:29.100\n Maybe you outside family what otherwise you know what what what day would it be.\n\n1:44:29.100 --> 1:44:40.720\n But can you look back you remember just being profoundly transformed in some way or blissful.\n\n1:44:40.720 --> 1:44:44.840\n I'll answer a slightly different question which is like what's a day in my my life or\n\n1:44:44.840 --> 1:44:49.040\n my career that was kind of a watershed moment.\n\n1:44:49.040 --> 1:44:55.760\n I went straight from undergrad to doctoral studies and you know that's not at all atypical\n\n1:44:55.760 --> 1:45:00.440\n and I'm also from an academic family like my my dad was a professor my uncle on his\n\n1:45:00.440 --> 1:45:03.440\n side as a professor both my grandfathers were professors.\n\n1:45:03.440 --> 1:45:05.640\n All kinds of majors to philosophy.\n\n1:45:05.640 --> 1:45:10.820\n Yeah they're kind of all over the map yeah and I was a grad student here just up the\n\n1:45:10.820 --> 1:45:15.840\n river at Harvard and came to study with Les Valiant which was a wonderful experience.\n\n1:45:15.840 --> 1:45:21.600\n But you know I remember my first year of graduate school I was generally pretty unhappy and\n\n1:45:21.600 --> 1:45:25.720\n I was unhappy because you know at Berkeley as an undergraduate you know yeah I studied\n\n1:45:25.720 --> 1:45:29.960\n a lot of math and computer science but it was a huge school first of all and I took\n\n1:45:29.960 --> 1:45:34.020\n a lot of other courses as we've discussed I started as an English major and took history\n\n1:45:34.020 --> 1:45:40.200\n courses and art history classes and had friends you know that did all kinds of different things.\n\n1:45:40.200 --> 1:45:44.840\n And you know Harvard's a much smaller institution than Berkeley and its computer science department\n\n1:45:44.840 --> 1:45:48.720\n especially at that time was was a much smaller place than it is now.\n\n1:45:48.720 --> 1:45:54.000\n And I suddenly just felt very you know like I'd gone from this very big world to this\n\n1:45:54.000 --> 1:45:59.400\n highly specialized world and now all of the classes I was taking were computer science\n\n1:45:59.400 --> 1:46:04.600\n classes and I was only in classes with math and computer science people.\n\n1:46:04.600 --> 1:46:09.960\n And so I was you know I thought often in that first year of grad school about whether I\n\n1:46:09.960 --> 1:46:14.760\n really wanted to stick with it or not and you know I thought like oh I could you know\n\n1:46:14.760 --> 1:46:19.860\n stop with a master's I could go back to the Bay Area and to California and you know this\n\n1:46:19.860 --> 1:46:23.660\n was in one of the early periods where there was you know like you could definitely get\n\n1:46:23.660 --> 1:46:28.680\n a relatively good job paying job at one of the one of the tech companies back you know\n\n1:46:28.680 --> 1:46:31.440\n that were the big tech companies back then.\n\n1:46:31.440 --> 1:46:36.880\n And so I distinctly remember like kind of a late spring day when I was kind of you know\n\n1:46:36.880 --> 1:46:40.440\n sitting in Boston Common and kind of really just kind of chewing over what I wanted to\n\n1:46:40.440 --> 1:46:45.220\n do with my life and I realized like okay and I think this is where my academic background\n\n1:46:45.220 --> 1:46:46.220\n helped me a great deal.\n\n1:46:46.220 --> 1:46:50.420\n I sort of realized you know yeah you're not having a great time right now this feels really\n\n1:46:50.420 --> 1:46:56.320\n narrowing but you know that you're here for research eventually and to do something original\n\n1:46:56.320 --> 1:47:02.320\n and to try to you know carve out a career where you kind of you know choose what you\n\n1:47:02.320 --> 1:47:06.260\n want to think about you know and have a great deal of independence.\n\n1:47:06.260 --> 1:47:10.920\n And so you know at that point I really didn't have any real research experience yet I mean\n\n1:47:10.920 --> 1:47:15.840\n it was trying to think about some problems with very little success but I knew that like\n\n1:47:15.840 --> 1:47:23.320\n I hadn't really tried to do the thing that I knew I'd come to do and so I thought you\n\n1:47:23.320 --> 1:47:30.080\n know I'm going to stick through it for the summer and you know and that was very formative\n\n1:47:30.080 --> 1:47:37.160\n because I went from kind of contemplating quitting to you know a year later it being\n\n1:47:37.160 --> 1:47:42.360\n very clear to me I was going to finish because I still had a ways to go but I kind of started\n\n1:47:42.360 --> 1:47:46.520\n doing research it was going well it was really interesting and it was sort of a complete\n\n1:47:46.520 --> 1:47:52.400\n transformation you know it's just that transition that I think every doctoral student makes\n\n1:47:52.400 --> 1:48:00.040\n at some point which is to sort of go from being like a student of what's been done before\n\n1:48:00.040 --> 1:48:04.120\n to doing you know your own thing and figure out what makes you interested in what your\n\n1:48:04.120 --> 1:48:09.280\n strengths and weaknesses are as a researcher and once you know I kind of made that decision\n\n1:48:09.280 --> 1:48:15.120\n on that particular day at that particular moment in Boston Common you know I'm glad\n\n1:48:15.120 --> 1:48:16.240\n I made that decision.\n\n1:48:16.240 --> 1:48:19.400\n And also just accepting the painful nature of that journey.\n\n1:48:19.400 --> 1:48:21.400\n Yeah exactly exactly.\n\n1:48:21.400 --> 1:48:26.880\n In that moment said I'm gonna I'm gonna stick it out yeah I'm gonna stick around for a while.\n\n1:48:26.880 --> 1:48:30.880\n Well Michael I've looked off do you work for a long time it's really nice to talk to you\n\n1:48:30.880 --> 1:48:31.880\n thank you so much.\n\n1:48:31.880 --> 1:48:34.360\n It's great to get back in touch with you too and see how great you're doing as well.\n\n1:48:34.360 --> 1:48:35.360\n Thanks a lot.\n\n1:48:35.360 --> 1:48:57.320\n Thank you.\n\n"
}