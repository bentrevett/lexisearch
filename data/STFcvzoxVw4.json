{
  "title": "Vladimir Vapnik: Statistical Learning | Lex Fridman Podcast #5",
  "id": "STFcvzoxVw4",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:03.000\n The following is a conversation with Vladimir Vapnik.\n\n00:03.000 --> 00:05.200\n He's the co inventor of support vector machines,\n\n00:05.200 --> 00:07.840\n support vector clustering, VC theory,\n\n00:07.840 --> 00:11.120\n and many foundational ideas in statistical learning.\n\n00:11.120 --> 00:13.080\n He was born in the Soviet Union\n\n00:13.080 --> 00:16.240\n and worked at the Institute of Control Sciences in Moscow.\n\n00:16.240 --> 00:19.280\n Then in the United States, he worked at AT&T,\n\n00:19.280 --> 00:22.200\n NEC Labs, Facebook Research,\n\n00:22.200 --> 00:25.880\n and now is a professor at Columbia University.\n\n00:25.880 --> 00:30.120\n His work has been cited over 170,000 times.\n\n00:30.120 --> 00:31.800\n He has some very interesting ideas\n\n00:31.800 --> 00:34.760\n about artificial intelligence and the nature of learning,\n\n00:34.760 --> 00:37.560\n especially on the limits of our current approaches\n\n00:37.560 --> 00:39.320\n and the open problems in the field.\n\n00:40.360 --> 00:42.440\n This conversation is part of MIT course\n\n00:42.440 --> 00:44.360\n on artificial general intelligence\n\n00:44.360 --> 00:46.800\n and the artificial intelligence podcast.\n\n00:46.800 --> 00:49.520\n If you enjoy it, please subscribe on YouTube\n\n00:49.520 --> 00:52.960\n or rate it on iTunes or your podcast provider of choice,\n\n00:52.960 --> 00:55.240\n or simply connect with me on Twitter\n\n00:55.240 --> 01:00.120\n or other social networks at Lex Friedman spelled F R I D.\n\n01:00.120 --> 01:03.760\n And now here's my conversation with Vladimir Vapnik.\n\n01:04.760 --> 01:08.800\n Einstein famously said that God doesn't play dice.\n\n01:08.800 --> 01:09.920\n Yeah.\n\n01:09.920 --> 01:12.800\n You have studied the world through the eyes of statistics.\n\n01:12.800 --> 01:17.280\n So let me ask you in terms of the nature of reality,\n\n01:17.280 --> 01:21.320\n fundamental nature of reality, does God play dice?\n\n01:21.320 --> 01:25.400\n We don't know some factors.\n\n01:25.400 --> 01:28.160\n And because we don't know some factors,\n\n01:28.160 --> 01:30.520\n which could be important,\n\n01:30.520 --> 01:35.040\n it looks like God plays dice.\n\n01:35.040 --> 01:38.000\n But we should describe it.\n\n01:38.000 --> 01:42.080\n In philosophy, they distinguish between two positions,\n\n01:42.080 --> 01:44.920\n positions of instrumentalism,\n\n01:44.920 --> 01:48.720\n where you're creating theory for prediction\n\n01:48.720 --> 01:50.960\n and position of realism,\n\n01:50.960 --> 01:54.640\n where you're trying to understand what God did.\n\n01:54.640 --> 01:58.400\n Can you describe instrumentalism and realism a little bit?\n\n01:58.400 --> 02:03.400\n For example, if you have some mechanical laws,\n\n02:04.200 --> 02:06.280\n what is that?\n\n02:06.280 --> 02:11.280\n Is it law which is true always and everywhere?\n\n02:11.480 --> 02:14.880\n Or it is law which allow you to predict\n\n02:14.880 --> 02:19.880\n position of moving element?\n\n02:19.880 --> 02:23.000\n What you believe.\n\n02:23.000 --> 02:25.520\n You believe that it is God's law,\n\n02:25.520 --> 02:28.520\n that God created the world,\n\n02:28.520 --> 02:33.200\n which obey to this physical law.\n\n02:33.200 --> 02:36.280\n Or it is just law for predictions.\n\n02:36.280 --> 02:38.440\n And which one is instrumentalism?\n\n02:38.440 --> 02:39.960\n For predictions.\n\n02:39.960 --> 02:43.680\n If you believe that this is law of God,\n\n02:43.680 --> 02:47.560\n and it's always true everywhere,\n\n02:47.560 --> 02:50.080\n that means that you're realist.\n\n02:50.080 --> 02:55.080\n So you're trying to really understand God's thought.\n\n02:55.520 --> 02:58.640\n So the way you see the world is as an instrumentalist?\n\n03:00.080 --> 03:03.280\n You know, I'm working for some models,\n\n03:03.280 --> 03:07.000\n model of machine learning.\n\n03:07.000 --> 03:11.680\n So in this model, we can see setting,\n\n03:12.840 --> 03:15.360\n and we try to solve,\n\n03:15.360 --> 03:18.320\n resolve the setting to solve the problem.\n\n03:18.320 --> 03:20.840\n And you can do in two different way.\n\n03:20.840 --> 03:23.880\n From the point of view of instrumentalist,\n\n03:23.880 --> 03:27.160\n and that's what everybody does now.\n\n03:27.160 --> 03:31.640\n Because they say that goal of machine learning\n\n03:31.640 --> 03:36.640\n is to find the rule for classification.\n\n03:36.880 --> 03:38.360\n That is true.\n\n03:38.360 --> 03:41.000\n But it is instrument for prediction.\n\n03:41.000 --> 03:46.000\n But I can say the goal of machine learning\n\n03:46.240 --> 03:50.080\n is to learn about conditional probability.\n\n03:50.080 --> 03:54.520\n So how God played use, and if he play,\n\n03:54.520 --> 03:56.000\n what is probability for one,\n\n03:56.000 --> 04:00.000\n what is probability for another, given situation.\n\n04:00.000 --> 04:02.680\n But for prediction, I don't need this.\n\n04:02.680 --> 04:04.320\n I need the rule.\n\n04:04.320 --> 04:08.520\n But for understanding, I need conditional probability.\n\n04:08.520 --> 04:11.840\n So let me just step back a little bit first to talk about,\n\n04:11.840 --> 04:14.000\n you mentioned, which I read last night,\n\n04:14.000 --> 04:19.000\n the parts of the 1960 paper by Eugene Wigner,\n\n04:21.360 --> 04:23.560\n Unreasonable Effectiveness of Mathematics\n\n04:23.560 --> 04:24.960\n and Natural Sciences.\n\n04:24.960 --> 04:29.400\n Such a beautiful paper, by the way.\n\n04:29.400 --> 04:32.640\n Made me feel, to be honest,\n\n04:32.640 --> 04:35.560\n to confess my own work in the past few years\n\n04:35.560 --> 04:38.480\n on deep learning, heavily applied.\n\n04:38.480 --> 04:40.440\n Made me feel that I was missing out\n\n04:40.440 --> 04:43.480\n on some of the beauty of nature\n\n04:43.480 --> 04:45.640\n in the way that math can uncover.\n\n04:45.640 --> 04:50.440\n So let me just step away from the poetry of that for a second.\n\n04:50.440 --> 04:53.120\n How do you see the role of math in your life?\n\n04:53.120 --> 04:55.640\n Is it a tool, is it poetry?\n\n04:55.640 --> 04:57.040\n Where does it sit?\n\n04:57.040 --> 05:01.480\n And does math for you have limits of what it can describe?\n\n05:01.480 --> 05:06.480\n Some people say that math is language which use God.\n\n05:06.480 --> 05:07.320\n Use God.\n\n05:08.280 --> 05:10.320\n So I believe that...\n\n05:10.320 --> 05:12.280\n Speak to God or use God or...\n\n05:12.280 --> 05:13.120\n Use God.\n\n05:13.120 --> 05:14.080\n Use God.\n\n05:14.080 --> 05:15.560\n Yeah.\n\n05:15.560 --> 05:20.560\n So I believe that this article\n\n05:23.920 --> 05:27.840\n about effectiveness, unreasonable effectiveness of math,\n\n05:27.840 --> 05:32.120\n is that if you're looking at mathematical structures,\n\n05:32.120 --> 05:36.120\n they know something about reality.\n\n05:36.120 --> 05:41.120\n And the most scientists from Natural Science,\n\n05:41.120 --> 05:46.120\n they're looking on equation and trying to understand reality.\n\n05:47.120 --> 05:50.120\n So the same in machine learning.\n\n05:50.120 --> 05:56.120\n If you try very carefully look on all equations\n\n05:56.120 --> 05:59.120\n which define conditional probability,\n\n05:59.120 --> 06:04.120\n you can understand something about reality\n\n06:04.120 --> 06:07.120\n more than from your fantasy.\n\n06:07.120 --> 06:13.120\n So math can reveal the simple underlying principles of reality perhaps.\n\n06:13.120 --> 06:16.120\n You know what means simple?\n\n06:16.120 --> 06:19.120\n It is very hard to discover them.\n\n06:19.120 --> 06:23.120\n But then when you discover them and look at them,\n\n06:23.120 --> 06:26.120\n you see how beautiful they are.\n\n06:26.120 --> 06:33.120\n And it is surprising why people did not see that before.\n\n06:33.120 --> 06:37.120\n You're looking on equation and derive it from equations.\n\n06:37.120 --> 06:43.120\n For example, I talked yesterday about least square method.\n\n06:43.120 --> 06:48.120\n And people had a lot of fantasy how to improve least square method.\n\n06:48.120 --> 06:52.120\n But if you're going step by step by solving some equations,\n\n06:52.120 --> 06:59.120\n you suddenly will get some term which after thinking,\n\n06:59.120 --> 07:04.120\n you understand that it describes position of observation point.\n\n07:04.120 --> 07:08.120\n In least square method, we throw out a lot of information.\n\n07:08.120 --> 07:11.120\n We don't look in composition of point of observations,\n\n07:11.120 --> 07:14.120\n we're looking only on residuals.\n\n07:14.120 --> 07:19.120\n But when you understood that, that's very simple idea,\n\n07:19.120 --> 07:22.120\n but it's not too simple to understand.\n\n07:22.120 --> 07:26.120\n And you can derive this just from equations.\n\n07:26.120 --> 07:31.120\n So some simple algebra, a few steps will take you to something surprising\n\n07:31.120 --> 07:34.120\n that when you think about, you understand.\n\n07:34.120 --> 07:42.120\n And that is proof that human intuition is not too rich and very primitive.\n\n07:42.120 --> 07:48.120\n And it does not see very simple situations.\n\n07:48.120 --> 07:50.120\n So let me take a step back.\n\n07:50.120 --> 07:54.120\n In general, yes.\n\n07:54.120 --> 08:01.120\n But what about human, as opposed to intuition, ingenuity?\n\n08:01.120 --> 08:06.120\n Moments of brilliance.\n\n08:06.120 --> 08:09.120\n Do you have to be so hard on human intuition?\n\n08:09.120 --> 08:12.120\n Are there moments of brilliance in human intuition?\n\n08:12.120 --> 08:17.120\n They can leap ahead of math and then the math will catch up?\n\n08:17.120 --> 08:19.120\n I don't think so.\n\n08:19.120 --> 08:26.120\n I think that the best human intuition, it is putting in axioms.\n\n08:26.120 --> 08:28.120\n And then it is technical.\n\n08:28.120 --> 08:31.120\n See where the axioms take you.\n\n08:31.120 --> 08:35.120\n But if they correctly take axioms.\n\n08:35.120 --> 08:41.120\n But it axiom polished during generations of scientists.\n\n08:41.120 --> 08:45.120\n And this is integral wisdom.\n\n08:45.120 --> 08:47.120\n That is beautifully put.\n\n08:47.120 --> 08:56.120\n But if you maybe look at, when you think of Einstein and special relativity,\n\n08:56.120 --> 09:04.120\n what is the role of imagination coming first there in the moment of discovery of an idea?\n\n09:04.120 --> 09:10.120\n So there is obviously a mix of math and out of the box imagination there.\n\n09:10.120 --> 09:12.120\n That I don't know.\n\n09:12.120 --> 09:17.120\n Whatever I did, I exclude any imagination.\n\n09:17.120 --> 09:22.120\n Because whatever I saw in machine learning that comes from imagination,\n\n09:22.120 --> 09:29.120\n like features, like deep learning, they are not relevant to the problem.\n\n09:29.120 --> 09:34.120\n When you are looking very carefully from mathematical equations,\n\n09:34.120 --> 09:39.120\n you are deriving very simple theory, which goes far beyond theoretically\n\n09:39.120 --> 09:42.120\n than whatever people can imagine.\n\n09:42.120 --> 09:44.120\n Because it is not good fantasy.\n\n09:44.120 --> 09:46.120\n It is just interpretation.\n\n09:46.120 --> 09:48.120\n It is just fantasy.\n\n09:48.120 --> 09:51.120\n But it is not what you need.\n\n09:51.120 --> 09:59.120\n You don't need any imagination to derive the main principle of machine learning.\n\n09:59.120 --> 10:02.120\n When you think about learning and intelligence,\n\n10:02.120 --> 10:06.120\n maybe thinking about the human brain and trying to describe mathematically\n\n10:06.120 --> 10:13.120\n the process of learning, that is something like what happens in the human brain.\n\n10:13.120 --> 10:17.120\n Do you think we have the tools currently?\n\n10:17.120 --> 10:21.120\n Do you think we will ever have the tools to try to describe that process of learning?\n\n10:21.120 --> 10:25.120\n It is not description what is going on.\n\n10:25.120 --> 10:27.120\n It is interpretation.\n\n10:27.120 --> 10:29.120\n It is your interpretation.\n\n10:29.120 --> 10:32.120\n Your vision can be wrong.\n\n10:32.120 --> 10:39.120\n You know, one guy invented microscope, Levenhuk, for the first time.\n\n10:39.120 --> 10:45.120\n Only he got this instrument and he kept secret about microscope.\n\n10:45.120 --> 10:49.120\n But he wrote a report in London Academy of Science.\n\n10:49.120 --> 10:52.120\n In his report, when he was looking at the blood,\n\n10:52.120 --> 10:56.120\n he looked everywhere, on the water, on the blood, on the sperm.\n\n10:56.120 --> 11:04.120\n But he described blood like fight between queen and king.\n\n11:04.120 --> 11:12.120\n So, he saw blood cells, red cells, and he imagined that it is army fighting each other.\n\n11:12.120 --> 11:17.120\n And it was his interpretation of situation.\n\n11:17.120 --> 11:20.120\n And he sent this report in Academy of Science.\n\n11:20.120 --> 11:24.120\n They very carefully looked because they believed that he is right.\n\n11:24.120 --> 11:25.120\n He saw something.\n\n11:25.120 --> 11:26.120\n Yes.\n\n11:26.120 --> 11:28.120\n But he gave wrong interpretation.\n\n11:28.120 --> 11:32.120\n And I believe the same can happen with brain.\n\n11:32.120 --> 11:33.120\n With brain, yeah.\n\n11:33.120 --> 11:35.120\n The most important part.\n\n11:35.120 --> 11:39.120\n You know, I believe in human language.\n\n11:39.120 --> 11:43.120\n In some proverbs, there is so much wisdom.\n\n11:43.120 --> 11:54.120\n For example, people say that it is better than thousand days of diligent studies one day with great teacher.\n\n11:54.120 --> 11:59.120\n But if I will ask you what teacher does, nobody knows.\n\n11:59.120 --> 12:01.120\n And that is intelligence.\n\n12:01.120 --> 12:12.120\n But we know from history and now from math and machine learning that teacher can do a lot.\n\n12:12.120 --> 12:16.120\n So, what from a mathematical point of view is the great teacher?\n\n12:16.120 --> 12:17.120\n I don't know.\n\n12:17.120 --> 12:18.120\n That's an open question.\n\n12:18.120 --> 12:25.120\n No, but we can say what teacher can do.\n\n12:25.120 --> 12:32.120\n He can introduce some invariants, some predicate for creating invariants.\n\n12:32.120 --> 12:33.120\n How he doing it?\n\n12:33.120 --> 12:41.120\n I don't know because teacher knows reality and can describe from this reality a predicate, invariants.\n\n12:41.120 --> 12:49.120\n But he knows that when you're using invariant, you can decrease number of observations hundred times.\n\n12:49.120 --> 12:53.120\n So, but maybe try to pull that apart a little bit.\n\n12:53.120 --> 12:59.120\n I think you mentioned like a piano teacher saying to the student, play like a butterfly.\n\n12:59.120 --> 13:00.120\n Yeah.\n\n13:00.120 --> 13:01.120\n I play piano.\n\n13:01.120 --> 13:03.120\n I play guitar for a long time.\n\n13:03.120 --> 13:12.120\n Yeah, maybe it's romantic, poetic, but it feels like there's a lot of truth in that statement.\n\n13:12.120 --> 13:15.120\n Like there is a lot of instruction in that statement.\n\n13:15.120 --> 13:17.120\n And so, can you pull that apart?\n\n13:17.120 --> 13:19.120\n What is that?\n\n13:19.120 --> 13:22.120\n The language itself may not contain this information.\n\n13:22.120 --> 13:24.120\n It is not blah, blah, blah.\n\n13:24.120 --> 13:25.120\n It is not blah, blah, blah.\n\n13:25.120 --> 13:26.120\n It affects you.\n\n13:26.120 --> 13:27.120\n It's what?\n\n13:27.120 --> 13:28.120\n It affects you.\n\n13:28.120 --> 13:29.120\n It affects your playing.\n\n13:29.120 --> 13:34.120\n Yes, it does, but it's not the laying.\n\n13:34.120 --> 13:38.120\n It feels like what is the information being exchanged there?\n\n13:38.120 --> 13:39.120\n What is the nature of information?\n\n13:39.120 --> 13:41.120\n What is the representation of that information?\n\n13:41.120 --> 13:45.120\n I believe that it is sort of predicate, but I don't know.\n\n13:45.120 --> 13:49.120\n That is exactly what intelligence and machine learning should be.\n\n13:49.120 --> 13:50.120\n Yes.\n\n13:50.120 --> 13:53.120\n Because the rest is just mathematical technique.\n\n13:53.120 --> 14:03.120\n I think that what was discovered recently is that there is two mechanism of learning.\n\n14:03.120 --> 14:08.120\n One called strong convergence mechanism and weak convergence mechanism.\n\n14:08.120 --> 14:11.120\n Before, people use only one convergence.\n\n14:11.120 --> 14:16.120\n In weak convergence mechanism, you can use predicate.\n\n14:16.120 --> 14:23.120\n That's what play like butterfly and it will immediately affect your playing.\n\n14:23.120 --> 14:27.120\n You know, there is English proverb, great.\n\n14:27.120 --> 14:35.120\n If it looks like a duck, swims like a duck, and quack like a duck, then it is probably duck.\n\n14:35.120 --> 14:36.120\n Yes.\n\n14:36.120 --> 14:40.120\n But this is exact about predicate.\n\n14:40.120 --> 14:43.120\n Looks like a duck, what it means.\n\n14:43.120 --> 14:47.120\n You saw many ducks that you're training data.\n\n14:47.120 --> 14:56.120\n So, you have description of how looks integral looks ducks.\n\n14:56.120 --> 14:57.120\n Yeah.\n\n14:57.120 --> 14:59.120\n The visual characteristics of a duck.\n\n14:59.120 --> 15:00.120\n Yeah.\n\n15:00.120 --> 15:04.120\n But you want and you have model for recognition.\n\n15:04.120 --> 15:12.120\n So, you would like so that theoretical description from model coincide with empirical description,\n\n15:12.120 --> 15:14.120\n which you saw on territory.\n\n15:14.120 --> 15:18.120\n So, about looks like a duck, it is general.\n\n15:18.120 --> 15:21.120\n But what about swims like a duck?\n\n15:21.120 --> 15:23.120\n You should know that duck swims.\n\n15:23.120 --> 15:26.120\n You can say it play chess like a duck.\n\n15:26.120 --> 15:27.120\n Okay.\n\n15:27.120 --> 15:29.120\n Duck doesn't play chess.\n\n15:29.120 --> 15:35.120\n And it is completely legal predicate, but it is useless.\n\n15:35.120 --> 15:41.120\n So, half teacher can recognize not useless predicate.\n\n15:41.120 --> 15:47.120\n So, up to now, we don't use this predicate in existing machine learning.\n\n15:47.120 --> 15:50.120\n So, why we need zillions of data.\n\n15:50.120 --> 15:55.120\n But in this English proverb, they use only three predicate.\n\n15:55.120 --> 15:59.120\n Looks like a duck, swims like a duck, and quack like a duck.\n\n15:59.120 --> 16:08.120\n So, you can't deny the fact that swims like a duck and quacks like a duck has humor in it, has ambiguity.\n\n16:08.120 --> 16:12.120\n Let's talk about swim like a duck.\n\n16:12.120 --> 16:16.120\n It doesn't say jump like a duck.\n\n16:16.120 --> 16:17.120\n Why?\n\n16:17.120 --> 16:18.120\n Because...\n\n16:18.120 --> 16:20.120\n It's not relevant.\n\n16:20.120 --> 16:27.120\n But that means that you know ducks, you know different birds, you know animals.\n\n16:27.120 --> 16:32.120\n And you derive from this that it is relevant to say swim like a duck.\n\n16:32.120 --> 16:42.120\n So, underneath, in order for us to understand swims like a duck, it feels like we need to know millions of other little pieces of information.\n\n16:42.120 --> 16:44.120\n Which we pick up along the way.\n\n16:44.120 --> 16:45.120\n You don't think so.\n\n16:45.120 --> 16:55.120\n There doesn't need to be this knowledge base in those statements carries some rich information that helps us understand the essence of duck.\n\n16:55.120 --> 16:57.120\n Yeah.\n\n16:57.120 --> 17:01.120\n How far are we from integrating predicates?\n\n17:01.120 --> 17:07.120\n You know that when you consider complete theory of machine learning.\n\n17:07.120 --> 17:12.120\n So, what it does, you have a lot of functions.\n\n17:12.120 --> 17:17.120\n And then you're talking it looks like a duck.\n\n17:17.120 --> 17:20.120\n You see your training data.\n\n17:20.120 --> 17:31.120\n From training data you recognize like expected duck should look.\n\n17:31.120 --> 17:40.120\n Then you remove all functions which does not look like you think it should look from training data.\n\n17:40.120 --> 17:46.120\n So, you decrease amount of function from which you pick up one.\n\n17:46.120 --> 17:52.120\n Then you give a second predicate and again decrease the set of function.\n\n17:52.120 --> 17:56.120\n And after that you pick up the best function you can find.\n\n17:56.120 --> 17:58.120\n It is standard machine learning.\n\n17:58.120 --> 18:03.120\n So, why you need not too many examples?\n\n18:03.120 --> 18:06.120\n Because your predicates aren't very good?\n\n18:06.120 --> 18:17.120\n That means that predicates are very good because every predicate is invented to decrease admissible set of function.\n\n18:17.120 --> 18:22.120\n So, you talk about admissible set of functions and you talk about good functions.\n\n18:22.120 --> 18:24.120\n So, what makes a good function?\n\n18:24.120 --> 18:35.120\n So, admissible set of function is set of function which has small capacity or small diversity, small VC dimension example.\n\n18:35.120 --> 18:37.120\n Which contain good function inside.\n\n18:37.120 --> 18:45.120\n So, by the way for people who don't know VC, you're the V in the VC.\n\n18:45.120 --> 18:50.120\n So, how would you describe to lay person what VC theory is?\n\n18:50.120 --> 18:52.120\n How would you describe VC?\n\n18:52.120 --> 18:54.120\n So, when you have a machine.\n\n18:54.120 --> 19:02.120\n So, machine capable to pick up one function from the admissible set of function.\n\n19:02.120 --> 19:07.120\n But set of admissible function can be big.\n\n19:07.120 --> 19:11.120\n So, it contain all continuous functions and it's useless.\n\n19:11.120 --> 19:15.120\n You don't have so many examples to pick up function.\n\n19:15.120 --> 19:17.120\n But it can be small.\n\n19:17.120 --> 19:24.120\n Small, we call it capacity but maybe better called diversity.\n\n19:24.120 --> 19:27.120\n So, not very different function in the set.\n\n19:27.120 --> 19:31.120\n It's infinite set of function but not very diverse.\n\n19:31.120 --> 19:34.120\n So, it is small VC dimension.\n\n19:34.120 --> 19:41.120\n When VC dimension is small, you need small amount of training data.\n\n19:41.120 --> 19:53.120\n So, the goal is to create admissible set of functions which is have small VC dimension and contain good function.\n\n19:53.120 --> 20:02.120\n Then you will be able to pick up the function using small amount of observations.\n\n20:02.120 --> 20:06.120\n So, that is the task of learning?\n\n20:06.120 --> 20:07.120\n Yeah.\n\n20:07.120 --> 20:17.120\n Is creating a set of admissible functions that has a small VC dimension and then you've figure out a clever way of picking up?\n\n20:17.120 --> 20:22.120\n No, that is goal of learning which I formulated yesterday.\n\n20:22.120 --> 20:30.120\n Statistical learning theory does not involve in creating admissible set of function.\n\n20:30.120 --> 20:39.120\n In classical learning theory, everywhere, 100% in textbook, the set of function, admissible set of function is given.\n\n20:39.120 --> 20:47.120\n But this is science about nothing because the most difficult problem to create admissible set of functions\n\n20:47.120 --> 20:55.120\n given, say, a lot of functions, continuum set of function, create admissible set of functions.\n\n20:55.120 --> 21:02.120\n That means that it has finite VC dimension, small VC dimension and contain good function.\n\n21:02.120 --> 21:05.120\n So, this was out of consideration.\n\n21:05.120 --> 21:07.120\n So, what's the process of doing that?\n\n21:07.120 --> 21:08.120\n I mean, it's fascinating.\n\n21:08.120 --> 21:13.120\n What is the process of creating this admissible set of functions?\n\n21:13.120 --> 21:15.120\n That is invariant.\n\n21:15.120 --> 21:16.120\n That's invariant.\n\n21:16.120 --> 21:30.120\n Yeah, you're looking of properties of training data and properties means that you have some function\n\n21:30.120 --> 21:39.120\n and you just count what is value, average value of function on training data.\n\n21:39.120 --> 21:46.120\n You have model and what is expectation of this function on the model and they should coincide.\n\n21:46.120 --> 21:51.120\n So, the problem is about how to pick up functions.\n\n21:51.120 --> 21:54.120\n It can be any function.\n\n21:54.120 --> 22:00.120\n In fact, it is true for all functions.\n\n22:00.120 --> 22:11.120\n But because when we're talking, say, duck does not jumping, so you don't ask question jump like a duck\n\n22:11.120 --> 22:13.120\n because it is trivial.\n\n22:13.120 --> 22:16.120\n It does not jumping and doesn't help you to recognize jump.\n\n22:16.120 --> 22:24.120\n But you know something, which question to ask and you're asking it seems like a duck,\n\n22:24.120 --> 22:28.120\n but looks like a duck at this general situation.\n\n22:28.120 --> 22:36.120\n Looks like, say, guy who have this illness, this disease.\n\n22:36.120 --> 22:39.120\n It is legal.\n\n22:39.120 --> 22:47.120\n So, there is a general type of predicate looks like and special type of predicate,\n\n22:47.120 --> 22:51.120\n which related to this specific problem.\n\n22:51.120 --> 22:56.120\n And that is intelligence part of all this business and that where teacher is involved.\n\n22:56.120 --> 23:01.120\n Incorporating the specialized predicates.\n\n23:01.120 --> 23:08.120\n What do you think about deep learning as neural networks, these arbitrary architectures\n\n23:08.120 --> 23:13.120\n as helping accomplish some of the tasks you're thinking about?\n\n23:13.120 --> 23:15.120\n Their effectiveness or lack thereof?\n\n23:15.120 --> 23:20.120\n What are the weaknesses and what are the possible strengths?\n\n23:20.120 --> 23:29.120\n You know, I think that this is fantasy, everything which like deep learning, like features.\n\n23:29.120 --> 23:34.120\n Let me give you this example.\n\n23:34.120 --> 23:39.120\n One of the greatest books is Churchill book about history of Second World War.\n\n23:39.120 --> 23:53.120\n And he started this book describing that in old time when war is over, so the great kings,\n\n23:53.120 --> 24:00.120\n they gathered together, almost all of them were relatives, and they discussed what should\n\n24:00.120 --> 24:03.120\n be done, how to create peace.\n\n24:03.120 --> 24:05.120\n And they came to agreement.\n\n24:05.120 --> 24:13.120\n And when happened First World War, the general public came in power.\n\n24:13.120 --> 24:18.120\n And they were so greedy that robbed Germany.\n\n24:18.120 --> 24:24.120\n And it was clear for everybody that it is not peace, that peace will last only 20 years\n\n24:24.120 --> 24:28.120\n because they were not professionals.\n\n24:28.120 --> 24:32.120\n And the same I see in machine learning.\n\n24:32.120 --> 24:38.120\n There are mathematicians who are looking for the problem from a very deep point of view,\n\n24:38.120 --> 24:40.120\n mathematical point of view.\n\n24:40.120 --> 24:46.120\n And there are computer scientists who mostly does not know mathematics.\n\n24:46.120 --> 24:49.120\n They just have interpretation of that.\n\n24:49.120 --> 24:54.120\n And they invented a lot of blah, blah, blah interpretations like deep learning.\n\n24:54.120 --> 24:55.120\n Why you need deep learning?\n\n24:55.120 --> 24:57.120\n Mathematic does not know deep learning.\n\n24:57.120 --> 25:00.120\n Mathematic does not know neurons.\n\n25:00.120 --> 25:02.120\n It is just function.\n\n25:02.120 --> 25:09.120\n If you like to say piecewise linear function, say that and do in class of piecewise linear\n\n25:09.120 --> 25:10.120\n function.\n\n25:10.120 --> 25:12.120\n But they invent something.\n\n25:12.120 --> 25:22.120\n And then they try to prove advantage of that through interpretations, which mostly wrong.\n\n25:22.120 --> 25:27.120\n And when it's not enough, they appeal to brain, which they know nothing about that.\n\n25:27.120 --> 25:30.120\n Nobody knows what's going on in the brain.\n\n25:30.120 --> 25:34.120\n So, I think that more reliable work on math.\n\n25:34.120 --> 25:36.120\n This is a mathematical problem.\n\n25:36.120 --> 25:39.120\n Do your best to solve this problem.\n\n25:39.120 --> 25:45.120\n Try to understand that there is not only one way of convergence, which is strong way of\n\n25:45.120 --> 25:46.120\n convergence.\n\n25:46.120 --> 25:49.120\n There is a weak way of convergence, which requires predicate.\n\n25:49.120 --> 25:56.120\n And if you will go through all this stuff, you will see that you don't need deep learning.\n\n25:56.120 --> 26:03.120\n Even more, I would say one of the theory, which called represented theory.\n\n26:03.120 --> 26:16.120\n It says that optimal solution of mathematical problem, which is described learning is on\n\n26:16.120 --> 26:21.120\n shadow network, not on deep learning.\n\n26:21.120 --> 26:22.120\n And a shallow network.\n\n26:22.120 --> 26:23.120\n Yeah.\n\n26:23.120 --> 26:24.120\n The ultimate problem is there.\n\n26:24.120 --> 26:25.120\n Absolutely.\n\n26:25.120 --> 26:29.120\n In the end, what you're saying is exactly right.\n\n26:29.120 --> 26:37.120\n The question is you have no value for throwing something on the table, playing with it, not\n\n26:37.120 --> 26:38.120\n math.\n\n26:38.120 --> 26:43.120\n It's like a neural network where you said throwing something in the bucket or the biological\n\n26:43.120 --> 26:47.120\n example and looking at kings and queens or the cells or the microscope.\n\n26:47.120 --> 26:55.120\n You don't see value in imagining the cells or kings and queens and using that as inspiration\n\n26:55.120 --> 26:59.120\n and imagination for where the math will eventually lead you.\n\n26:59.120 --> 27:06.120\n You think that interpretation basically deceives you in a way that's not productive.\n\n27:06.120 --> 27:17.120\n I think that if you're trying to analyze this business of learning and especially discussion\n\n27:17.120 --> 27:24.120\n about deep learning, it is discussion about interpretation, not about things, about what\n\n27:24.120 --> 27:26.120\n you can say about things.\n\n27:26.120 --> 27:27.120\n That's right.\n\n27:27.120 --> 27:29.120\n But aren't you surprised by the beauty of it?\n\n27:29.120 --> 27:38.200\n So not mathematical beauty, but the fact that it works at all or are you criticizing that\n\n27:38.200 --> 27:47.880\n very beauty, our human desire to interpret, to find our silly interpretations in these\n\n27:47.880 --> 27:49.840\n constructs?\n\n27:49.840 --> 27:51.320\n Let me ask you this.\n\n27:51.320 --> 27:57.100\n Are you surprised and does it inspire you?\n\n27:57.100 --> 28:03.520\n How do you feel about the success of a system like AlphaGo at beating the game of Go?\n\n28:03.520 --> 28:11.600\n Using neural networks to estimate the quality of a board and the quality of the position.\n\n28:11.600 --> 28:14.600\n That is your interpretation, quality of the board.\n\n28:14.600 --> 28:15.600\n Yeah, yes.\n\n28:15.600 --> 28:16.600\n Yeah.\n\n28:16.600 --> 28:20.320\n So it's not our interpretation.\n\n28:20.320 --> 28:25.920\n The fact is a neural network system, it doesn't matter, a learning system that we don't I\n\n28:25.920 --> 28:30.160\n think mathematically understand that well, beats the best human player, does something\n\n28:30.160 --> 28:31.160\n that was thought impossible.\n\n28:31.160 --> 28:35.160\n That means that it's not a very difficult problem.\n\n28:35.160 --> 28:40.200\n So you empirically, we've empirically have discovered that this is not a very difficult\n\n28:40.200 --> 28:41.200\n problem.\n\n28:41.200 --> 28:42.200\n Yeah.\n\n28:42.200 --> 28:44.080\n It's true.\n\n28:44.080 --> 28:48.720\n So maybe, can't argue.\n\n28:48.720 --> 28:56.680\n So even more I would say that if they use deep learning, it is not the most effective\n\n28:56.680 --> 29:00.320\n way of learning theory.\n\n29:00.320 --> 29:08.800\n And usually when people use deep learning, they're using zillions of training data.\n\n29:08.800 --> 29:10.480\n Yeah.\n\n29:10.480 --> 29:13.520\n But you don't need this.\n\n29:13.520 --> 29:23.240\n So I describe challenge, can we do some problems which do well deep learning method, this deep\n\n29:23.240 --> 29:28.400\n net, using hundred times less training data.\n\n29:28.400 --> 29:38.560\n Even more, some problems deep learning cannot solve because it's not necessary they create\n\n29:38.560 --> 29:40.840\n admissible set of function.\n\n29:40.840 --> 29:45.840\n To create deep architecture means to create admissible set of functions.\n\n29:45.840 --> 29:50.680\n You cannot say that you're creating good admissible set of functions.\n\n29:50.680 --> 29:52.760\n You just, it's your fantasy.\n\n29:52.760 --> 29:54.960\n It does not come from us.\n\n29:54.960 --> 30:00.280\n But it is possible to create admissible set of functions because you have your training\n\n30:00.280 --> 30:01.280\n data.\n\n30:01.280 --> 30:10.600\n That actually for mathematicians, when you consider a variant, you need to use law of\n\n30:10.600 --> 30:11.600\n large numbers.\n\n30:11.600 --> 30:20.840\n When you're making training in existing algorithm, you need uniform law of large numbers, which\n\n30:20.840 --> 30:25.300\n is much more difficult, it requires VC dimension and all this stuff.\n\n30:25.300 --> 30:33.480\n But nevertheless, if you use both weak and strong way of convergence, you can decrease\n\n30:33.480 --> 30:35.240\n a lot of training data.\n\n30:35.240 --> 30:41.360\n You could do the three, the swims like a duck and quacks like a duck.\n\n30:41.360 --> 30:48.820\n So let's step back and think about human intelligence in general.\n\n30:48.820 --> 30:54.120\n Clearly that has evolved in a non mathematical way.\n\n30:54.120 --> 31:04.280\n It wasn't, as far as we know, God or whoever didn't come up with a model and place in our\n\n31:04.280 --> 31:05.880\n brain of admissible functions.\n\n31:05.880 --> 31:06.880\n It kind of evolved.\n\n31:06.880 --> 31:09.720\n I don't know, maybe you have a view on this.\n\n31:09.720 --> 31:16.920\n So Alan Turing in the 50s, in his paper, asked and rejected the question, can machines think?\n\n31:16.920 --> 31:23.960\n It's not a very useful question, but can you briefly entertain this useful, useless question?\n\n31:23.960 --> 31:25.720\n Can machines think?\n\n31:25.720 --> 31:28.560\n So talk about intelligence and your view of it.\n\n31:28.560 --> 31:29.880\n I don't know that.\n\n31:29.880 --> 31:35.560\n I know that Turing described imitation.\n\n31:35.560 --> 31:43.060\n If computer can imitate human being, let's call it intelligent.\n\n31:43.060 --> 31:46.720\n And he understands that it is not thinking computer.\n\n31:46.720 --> 31:49.480\n He completely understands what he's doing.\n\n31:49.480 --> 31:53.840\n But he set up problem of imitation.\n\n31:53.840 --> 31:58.000\n So now we understand that the problem is not in imitation.\n\n31:58.000 --> 32:04.360\n I'm not sure that intelligence is just inside of us.\n\n32:04.360 --> 32:06.680\n It may be also outside of us.\n\n32:06.680 --> 32:09.440\n I have several observations.\n\n32:09.440 --> 32:20.360\n So when I prove some theorem, it's very difficult theorem, in couple of years, in several places,\n\n32:20.360 --> 32:27.140\n people prove the same theorem, say, Sawyer Lemma, after us was done, then another guys\n\n32:27.140 --> 32:28.960\n proved the same theorem.\n\n32:28.960 --> 32:32.280\n In the history of science, it's happened all the time.\n\n32:32.280 --> 32:40.600\n For example, geometry, it's happened simultaneously, first it did Lobachevsky and then Gauss and\n\n32:40.600 --> 32:48.800\n Boyai and another guys, and it's approximately in 10 times period, 10 years period of time.\n\n32:48.800 --> 32:51.760\n And I saw a lot of examples like that.\n\n32:51.760 --> 32:57.800\n And many mathematicians think that when they develop something, they develop something\n\n32:57.800 --> 33:01.600\n in general which affect everybody.\n\n33:01.600 --> 33:07.320\n So maybe our model that intelligence is only inside of us is incorrect.\n\n33:07.320 --> 33:09.320\n It's our interpretation.\n\n33:09.320 --> 33:15.800\n It might be there exists some connection with world intelligence.\n\n33:15.800 --> 33:16.800\n I don't know.\n\n33:16.800 --> 33:19.040\n You're almost like plugging in into...\n\n33:19.040 --> 33:21.240\n Yeah, exactly.\n\n33:21.240 --> 33:22.640\n And contributing to this...\n\n33:22.640 --> 33:24.360\n Into a big network.\n\n33:24.360 --> 33:28.360\n Into a big, maybe in your own network.\n\n33:28.360 --> 33:37.400\n On the flip side of that, maybe you can comment on big O complexity and how you see classifying\n\n33:37.400 --> 33:42.240\n algorithms by worst case running time in relation to their input.\n\n33:42.240 --> 33:47.840\n So that way of thinking about functions, do you think p equals np, do you think that's\n\n33:47.840 --> 33:49.120\n an interesting question?\n\n33:49.120 --> 33:52.000\n Yeah, it is an interesting question.\n\n33:52.000 --> 34:00.000\n But let me talk about complexity in about worst case scenario.\n\n34:00.000 --> 34:04.320\n There is a mathematical setting.\n\n34:04.320 --> 34:11.160\n When I came to United States in 1990, people did not know, they did not know statistical\n\n34:11.160 --> 34:13.040\n learning theory.\n\n34:13.040 --> 34:19.400\n So in Russia, it was published to monographs, our monographs, but in America they didn't\n\n34:19.400 --> 34:20.400\n know.\n\n34:20.400 --> 34:26.640\n Then they learned and somebody told me that it is worst case theory and they will create\n\n34:26.640 --> 34:30.800\n real case theory, but till now it did not.\n\n34:30.800 --> 34:34.100\n Because it is mathematical too.\n\n34:34.100 --> 34:38.680\n You can do only what you can do using mathematics.\n\n34:38.680 --> 34:45.920\n And which has a clear understanding and clear description.\n\n34:45.920 --> 34:52.640\n And for this reason, we introduce complexity.\n\n34:52.640 --> 35:01.720\n And we need this because using, actually it is diversity, I like this one more.\n\n35:01.720 --> 35:05.220\n You see the mention, you can prove some theorems.\n\n35:05.220 --> 35:12.680\n But we also create theory for case when you know probability measure.\n\n35:12.680 --> 35:18.080\n And that is the best case which can happen, it is entropy theory.\n\n35:18.080 --> 35:24.080\n So from mathematical point of view, you know the best possible case and the worst possible\n\n35:24.080 --> 35:25.080\n case.\n\n35:25.080 --> 35:30.480\n You can derive different model in medium, but it's not so interesting.\n\n35:30.480 --> 35:33.440\n You think the edges are interesting?\n\n35:33.440 --> 35:44.720\n The edges are interesting because it is not so easy to get good bound, exact bound.\n\n35:44.720 --> 35:49.280\n It's not many cases where you have the bound is not exact.\n\n35:49.280 --> 35:54.840\n But interesting principles which discover the mass.\n\n35:54.840 --> 36:00.340\n Do you think it's interesting because it's challenging and reveals interesting principles\n\n36:00.340 --> 36:02.700\n that allow you to get those bounds?\n\n36:02.700 --> 36:06.700\n Or do you think it's interesting because it's actually very useful for understanding the\n\n36:06.700 --> 36:11.080\n essence of a function of an algorithm?\n\n36:11.080 --> 36:17.680\n So it's like me judging your life as a human being by the worst thing you did and the best\n\n36:17.680 --> 36:21.840\n thing you did versus all the stuff in the middle.\n\n36:21.840 --> 36:24.520\n It seems not productive.\n\n36:24.520 --> 36:31.520\n I don't think so because you cannot describe situation in the middle.\n\n36:31.520 --> 36:34.600\n So it will be not general.\n\n36:34.600 --> 36:44.120\n So you can describe edges cases and it is clear it has some model, but you cannot describe\n\n36:44.120 --> 36:47.720\n model for every new case.\n\n36:47.720 --> 36:53.400\n So you will be never accurate when you're using model.\n\n36:53.400 --> 36:59.360\n But from a statistical point of view, the way you've studied functions and the nature\n\n36:59.360 --> 37:07.760\n of learning in the world, don't you think that the real world has a very long tail?\n\n37:07.760 --> 37:19.520\n That the edge cases are very far away from the mean, the stuff in the middle or no?\n\n37:19.520 --> 37:21.520\n I don't know that.\n\n37:21.520 --> 37:36.920\n I think that from my point of view, if you will use formal statistic, you need uniform\n\n37:36.920 --> 37:40.300\n law of large numbers.\n\n37:40.300 --> 37:52.240\n If you will use this invariance business, you will need just law of large numbers.\n\n37:52.240 --> 37:56.760\n And there's this huge difference between uniform law of large numbers and large numbers.\n\n37:56.760 --> 38:01.880\n Is it useful to describe that a little more or should we just take it to...\n\n38:01.880 --> 38:09.800\n For example, when I'm talking about duck, I give three predicates and that was enough.\n\n38:09.800 --> 38:19.760\n But if you will try to do formal distinguish, you will need a lot of observations.\n\n38:19.760 --> 38:27.400\n So that means that information about looks like a duck contain a lot of bit of information,\n\n38:27.400 --> 38:29.860\n formal bits of information.\n\n38:29.860 --> 38:39.880\n So we don't know that how much bit of information contain things from artificial and from intelligence.\n\n38:39.880 --> 38:42.440\n And that is the subject of analysis.\n\n38:42.440 --> 38:54.780\n Till now, all business, I don't like how people consider artificial intelligence.\n\n38:54.780 --> 39:01.240\n They consider us some codes which imitate activity of human being.\n\n39:01.240 --> 39:03.960\n It is not science, it is applications.\n\n39:03.960 --> 39:09.760\n You would like to imitate go ahead, it is very useful and a good problem.\n\n39:09.760 --> 39:15.960\n But you need to learn something more.\n\n39:15.960 --> 39:25.960\n How people try to do, how people can to develop, say, predicates seems like a duck or play\n\n39:25.960 --> 39:29.960\n like butterfly or something like that.\n\n39:29.960 --> 39:37.000\n Not the teacher says you, how it came in his mind, how he choose this image.\n\n39:37.000 --> 39:38.000\n So that process...\n\n39:38.000 --> 39:39.960\n That is problem of intelligence.\n\n39:39.960 --> 39:44.720\n That is the problem of intelligence and you see that connected to the problem of learning?\n\n39:44.720 --> 39:45.720\n Absolutely.\n\n39:45.720 --> 39:52.240\n Because you immediately give this predicate like specific predicate seems like a duck\n\n39:52.240 --> 39:54.840\n or quack like a duck.\n\n39:54.840 --> 39:57.560\n It was chosen somehow.\n\n39:57.560 --> 40:01.400\n So what is the line of work, would you say?\n\n40:01.400 --> 40:08.680\n If you were to formulate as a set of open problems, that will take us there, to play\n\n40:08.680 --> 40:09.680\n like a butterfly.\n\n40:09.680 --> 40:12.200\n We'll get a system to be able to...\n\n40:12.200 --> 40:14.520\n Let's separate two stories.\n\n40:14.520 --> 40:20.480\n One mathematical story that if you have predicate, you can do something.\n\n40:20.480 --> 40:23.840\n And another story how to get predicate.\n\n40:23.840 --> 40:32.280\n It is intelligence problem and people even did not start to understand intelligence.\n\n40:32.280 --> 40:39.440\n Because to understand intelligence, first of all, try to understand what do teachers.\n\n40:39.440 --> 40:43.960\n How teacher teach, why one teacher better than another one.\n\n40:43.960 --> 40:44.960\n Yeah.\n\n40:44.960 --> 40:50.400\n And so you think we really even haven't started on the journey of generating the predicates?\n\n40:50.400 --> 40:51.400\n No.\n\n40:51.400 --> 40:52.400\n We don't understand.\n\n40:52.400 --> 40:56.880\n We even don't understand that this problem exists.\n\n40:56.880 --> 40:57.880\n Because did you hear...\n\n40:57.880 --> 40:58.880\n You do.\n\n40:58.880 --> 41:02.720\n No, I just know name.\n\n41:02.720 --> 41:13.440\n I want to understand why one teacher better than another and how affect teacher, student.\n\n41:13.440 --> 41:18.520\n It is not because he repeating the problem which is in textbook.\n\n41:18.520 --> 41:20.920\n He makes some remarks.\n\n41:20.920 --> 41:23.040\n He makes some philosophy of reasoning.\n\n41:23.040 --> 41:24.600\n Yeah, that's a beautiful...\n\n41:24.600 --> 41:31.400\n So it is a formulation of a question that is the open problem.\n\n41:31.400 --> 41:34.200\n Why is one teacher better than another?\n\n41:34.200 --> 41:35.320\n Right.\n\n41:35.320 --> 41:37.360\n What he does better.\n\n41:37.360 --> 41:38.360\n Yeah.\n\n41:38.360 --> 41:39.360\n What...\n\n41:39.360 --> 41:40.360\n What...\n\n41:40.360 --> 41:41.360\n Why in...\n\n41:41.360 --> 41:42.360\n At every level?\n\n41:42.360 --> 41:45.080\n How do they get better?\n\n41:45.080 --> 41:48.560\n What does it mean to be better?\n\n41:48.560 --> 41:49.560\n The whole...\n\n41:49.560 --> 41:50.560\n Yeah.\n\n41:50.560 --> 41:51.560\n Yeah.\n\n41:51.560 --> 41:56.800\n From whatever model I have, one teacher can give a very good predicate.\n\n41:56.800 --> 42:03.880\n One teacher can say swims like a dog and another can say jump like a dog.\n\n42:03.880 --> 42:09.400\n And jump like a dog carries zero information.\n\n42:09.400 --> 42:14.400\n So what is the most exciting problem in statistical learning you've ever worked on or are working\n\n42:14.400 --> 42:17.600\n on now?\n\n42:17.600 --> 42:24.560\n I just finished this invariant story and I'm happy that...\n\n42:24.560 --> 42:30.600\n I believe that it is ultimate learning story.\n\n42:30.600 --> 42:38.120\n At least I can show that there are no another mechanism, only two mechanisms.\n\n42:38.120 --> 42:46.760\n But they separate statistical part from intelligent part and I know nothing about intelligent\n\n42:46.760 --> 42:47.760\n part.\n\n42:47.760 --> 42:59.160\n And if you will know this intelligent part, so it will help us a lot in teaching, in learning.\n\n42:59.160 --> 43:00.160\n In learning.\n\n43:00.160 --> 43:01.160\n Yeah.\n\n43:01.160 --> 43:02.920\n You will know it when we see it?\n\n43:02.920 --> 43:07.100\n So for example, in my talk, the last slide was a challenge.\n\n43:07.100 --> 43:14.680\n So you have say NIST digit recognition problem and deep learning claims that they did it\n\n43:14.680 --> 43:22.100\n very well, say 99.5% of correct answers.\n\n43:22.100 --> 43:25.280\n But they use 60,000 observations.\n\n43:25.280 --> 43:29.560\n Can you do the same using hundred times less?\n\n43:29.560 --> 43:35.280\n But incorporating invariants, what it means, you know, digit one, two, three.\n\n43:35.280 --> 43:44.040\n But looking on that, explain to me which invariant I should keep to use hundred examples or say\n\n43:44.040 --> 43:47.800\n hundred times less examples to do the same job.\n\n43:47.800 --> 43:56.520\n Yeah, that last slide, unfortunately your talk ended quickly, but that last slide was\n\n43:56.520 --> 44:01.960\n a powerful open challenge and a formulation of the essence here.\n\n44:01.960 --> 44:06.300\n What is the exact problem of intelligence?\n\n44:06.300 --> 44:15.040\n Because everybody, when machine learning started and it was developed by mathematicians, they\n\n44:15.040 --> 44:22.540\n immediately recognized that we use much more training data than humans needed.\n\n44:22.540 --> 44:27.640\n But now again, we came to the same story, have to decrease.\n\n44:27.640 --> 44:30.660\n That is the problem of learning.\n\n44:30.660 --> 44:37.320\n It is not like in deep learning, they use zillions of training data because maybe zillions\n\n44:37.320 --> 44:44.720\n are not enough if you have a good invariants.\n\n44:44.720 --> 44:49.520\n Maybe you will never collect some number of observations.\n\n44:49.520 --> 44:56.080\n But now it is a question to intelligence, how to do that?\n\n44:56.080 --> 45:03.200\n Because statistical part is ready, as soon as you supply us with predicate, we can do\n\n45:03.200 --> 45:06.880\n good job with small amount of observations.\n\n45:06.880 --> 45:11.040\n And the very first challenge is well known digit recognition.\n\n45:11.040 --> 45:15.560\n And you know digits, and please tell me invariants.\n\n45:15.560 --> 45:25.760\n I think about that, I can say for digit three, I would introduce concept of horizontal symmetry.\n\n45:25.760 --> 45:32.440\n So the digit three has horizontal symmetry, say more than, say, digit two or something\n\n45:32.440 --> 45:33.440\n like that.\n\n45:33.440 --> 45:40.480\n But as soon as I get the idea of horizontal symmetry, I can mathematically invent a lot\n\n45:40.480 --> 45:47.360\n of measure of horizontal symmetry, or then vertical symmetry, or diagonal symmetry, whatever,\n\n45:47.360 --> 45:49.980\n if I have idea of symmetry.\n\n45:49.980 --> 45:52.800\n But what else?\n\n45:52.800 --> 46:07.600\n I think on digit I see that it is meta predicate, which is not shape, it is something like symmetry,\n\n46:07.600 --> 46:16.240\n like how dark is whole picture, something like that, which can self rise a predicate.\n\n46:16.240 --> 46:29.800\n You think such a predicate could rise out of something that is not general, meaning\n\n46:29.800 --> 46:35.640\n it feels like for me to be able to understand the difference between two and three, I would\n\n46:35.640 --> 46:48.080\n need to have had a childhood of 10 to 15 years playing with kids, going to school, being\n\n46:48.080 --> 46:57.880\n yelled by parents, all of that, walking, jumping, looking at ducks, and then I would be able\n\n46:57.880 --> 47:03.120\n to generate the right predicate for telling the difference between two and a three.\n\n47:03.120 --> 47:05.720\n Or do you think there's a more efficient way?\n\n47:05.720 --> 47:06.720\n I don't know.\n\n47:06.720 --> 47:12.200\n I know for sure that you must know something more than digits.\n\n47:12.200 --> 47:13.200\n Yes.\n\n47:13.200 --> 47:15.000\n And that's a powerful statement.\n\n47:15.000 --> 47:16.000\n Yeah.\n\n47:16.000 --> 47:24.600\n But maybe there are several languages of description, these elements of digits.\n\n47:24.600 --> 47:32.000\n So I'm talking about symmetry, about some properties of geometry, I'm talking about\n\n47:32.000 --> 47:33.000\n something abstract.\n\n47:33.000 --> 47:34.780\n I don't know that.\n\n47:34.780 --> 47:38.900\n But this is a problem of intelligence.\n\n47:38.900 --> 47:47.160\n So in one of our articles, it is trivial to show that every example can carry not more\n\n47:47.160 --> 47:50.240\n than one bit of information in real.\n\n47:50.240 --> 48:00.660\n Because when you show example and you say this is one, you can remove, say, a function\n\n48:00.660 --> 48:05.080\n which does not tell you one, say, is the best strategy.\n\n48:05.080 --> 48:10.160\n If you can do it perfectly, it's remove half of the functions.\n\n48:10.160 --> 48:17.080\n But when you use one predicate, which looks like a duck, you can remove much more functions\n\n48:17.080 --> 48:18.920\n than half.\n\n48:18.920 --> 48:26.160\n And that means that it contains a lot of bit of information from formal point of view.\n\n48:26.160 --> 48:34.640\n But when you have a general picture of what you want to recognize and general picture\n\n48:34.640 --> 48:40.960\n of the world, can you invent this predicate?\n\n48:40.960 --> 48:47.560\n And that predicate carries a lot of information.\n\n48:47.560 --> 48:48.960\n Beautifully put.\n\n48:48.960 --> 48:56.000\n Maybe just me, but in all the math you show, in your work, which is some of the most profound\n\n48:56.000 --> 49:02.320\n mathematical work in the field of learning AI and just math in general, I hear a lot\n\n49:02.320 --> 49:04.400\n of poetry and philosophy.\n\n49:04.400 --> 49:09.920\n You really kind of talk about philosophy of science.\n\n49:09.920 --> 49:13.320\n There's a poetry and music to a lot of the work you're doing and the way you're thinking\n\n49:13.320 --> 49:14.320\n about it.\n\n49:14.320 --> 49:16.680\n So do you, where does that come from?\n\n49:16.680 --> 49:18.880\n Do you escape to poetry?\n\n49:18.880 --> 49:21.360\n Do you escape to music or not?\n\n49:21.360 --> 49:23.840\n I think that there exists ground truth.\n\n49:23.840 --> 49:25.760\n There exists ground truth?\n\n49:25.760 --> 49:26.760\n Yeah.\n\n49:26.760 --> 49:30.720\n And that can be seen everywhere.\n\n49:30.720 --> 49:39.000\n The smart guy, philosopher, sometimes I'm surprised how they deep see.\n\n49:39.000 --> 49:45.560\n Sometimes I see that some of them are completely out of subject.\n\n49:45.560 --> 49:50.960\n But the ground truth I see in music.\n\n49:50.960 --> 49:51.960\n Music is the ground truth?\n\n49:51.960 --> 49:52.960\n Yeah.\n\n49:52.960 --> 50:01.880\n And in poetry, many poets, they believe, they take dictation.\n\n50:01.880 --> 50:12.360\n So what piece of music as a piece of empirical evidence gave you a sense that they are touching\n\n50:12.360 --> 50:14.560\n something in the ground truth?\n\n50:14.560 --> 50:16.720\n It is structure.\n\n50:16.720 --> 50:17.720\n The structure of the math of music.\n\n50:17.720 --> 50:22.360\n Yeah, because when you're listening to Bach, you see the structure.\n\n50:22.360 --> 50:31.160\n Very clear, very classic, very simple, and the same in math when you have axioms in geometry,\n\n50:31.160 --> 50:32.160\n you have the same feeling.\n\n50:32.160 --> 50:38.360\n And in poetry, sometimes you see the same.\n\n50:38.360 --> 50:44.580\n And if you look back at your childhood, you grew up in Russia, you maybe were born as\n\n50:44.580 --> 50:48.680\n a researcher in Russia, you've developed as a researcher in Russia, you've came to United\n\n50:48.680 --> 50:51.800\n States and a few places.\n\n50:51.800 --> 51:00.000\n If you look back, what was some of your happiest moments as a researcher, some of the most\n\n51:00.000 --> 51:09.960\n profound moments, not in terms of their impact on society, but in terms of their impact on\n\n51:09.960 --> 51:15.400\n how damn good you feel that day and you remember that moment?\n\n51:15.400 --> 51:26.600\n You know, every time when you found something, it is great in the life, every simple things.\n\n51:26.600 --> 51:32.160\n But my general feeling is that most of my time was wrong.\n\n51:32.160 --> 51:39.520\n You should go again and again and again and try to be honest in front of yourself, not\n\n51:39.520 --> 51:47.840\n to make interpretation, but try to understand that it's related to ground truth, it is not\n\n51:47.840 --> 51:52.640\n my blah, blah, blah interpretation and something like that.\n\n51:52.640 --> 51:56.720\n But you're allowed to get excited at the possibility of discovery.\n\n51:56.720 --> 51:57.720\n Oh yeah.\n\n51:57.720 --> 51:59.840\n You have to double check it.\n\n51:59.840 --> 52:10.880\n No, but how it's related to another ground truth, is it just temporary or it is for forever?\n\n52:10.880 --> 52:19.880\n You know, you always have a feeling when you found something, how big is that?\n\n52:19.880 --> 52:26.560\n So 20 years ago when we discovered statistical learning theory, nobody believed, except for\n\n52:26.560 --> 52:37.640\n one guy, Dudley from MIT, and then in 20 years it became fashion, and the same with support\n\n52:37.640 --> 52:41.480\n vector machines, that is kernel machines.\n\n52:41.480 --> 52:49.240\n So with support vector machines and learning theory, when you were working on it, you had\n\n52:49.240 --> 52:59.600\n a sense, you had a sense of the profundity of it, how this seems to be right, this seems\n\n52:59.600 --> 53:00.600\n to be powerful.\n\n53:00.600 --> 53:01.600\n Right.\n\n53:01.600 --> 53:02.600\n Absolutely.\n\n53:02.600 --> 53:03.600\n Immediately.\n\n53:03.600 --> 53:18.480\n I recognized that it will last forever, and now when I found this invariant story, I have\n\n53:18.480 --> 53:24.720\n a feeling that it is complete learning, because I have proof that there are no different mechanisms.\n\n53:24.720 --> 53:35.480\n You can have some cosmetic improvement you can do, but in terms of invariants, you need\n\n53:35.480 --> 53:41.660\n both invariants and statistical learning, and they should work together.\n\n53:41.660 --> 53:52.920\n But also I'm happy that we can formulate what is intelligence from that, and to separate\n\n53:52.920 --> 53:57.240\n from technical part, and that is completely different.\n\n53:57.240 --> 53:58.240\n Absolutely.\n\n53:58.240 --> 54:00.280\n Well, Vladimir, thank you so much for talking today.\n\n54:00.280 --> 54:01.280\n Thank you.\n\n54:01.280 --> 54:02.280\n It's an honor.\n\n"
}