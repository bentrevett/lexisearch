{
  "title": "Elon Musk: Tesla Autopilot | Lex Fridman Podcast #18",
  "id": "dEv99vxKjVI",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:02.440\n The following is a conversation with Elon Musk.\n\n00:03.000 --> 00:08.700\n He's the CEO of Tesla, SpaceX, Neuralink, and a cofounder of several other companies.\n\n00:09.200 --> 00:12.580\n This conversation is part of the Artificial Intelligence podcast.\n\n00:13.180 --> 00:18.540\n The series includes leading researchers in academia and industry, including CEOs\n\n00:18.540 --> 00:23.120\n and CTOs of automotive, robotics, AI, and technology companies.\n\n00:24.060 --> 00:28.260\n This conversation happened after the release of the paper from our group at MIT\n\n00:28.260 --> 00:31.900\n on Driver Functional Vigilance, during use of Tesla's Autopilot.\n\n00:32.540 --> 00:36.140\n The Tesla team reached out to me offering a podcast conversation with Mr.\n\n00:36.140 --> 00:36.540\n Musk.\n\n00:37.140 --> 00:41.240\n I accepted, with full control of questions I could ask and the choice\n\n00:41.280 --> 00:42.680\n of what is released publicly.\n\n00:43.220 --> 00:45.880\n I ended up editing out nothing of substance.\n\n00:46.480 --> 00:50.780\n I've never spoken with Elon before this conversation, publicly or privately.\n\n00:51.380 --> 00:56.220\n Neither he nor his companies have any influence on my opinion, nor on the rigor\n\n00:56.220 --> 01:00.660\n and integrity of the scientific method that I practice in my position at MIT.\n\n01:01.360 --> 01:05.940\n Tesla has never financially supported my research, and I've never owned a Tesla\n\n01:05.940 --> 01:08.880\n vehicle, and I've never owned Tesla stock.\n\n01:09.700 --> 01:12.080\n This podcast is not a scientific paper.\n\n01:12.340 --> 01:13.440\n It is a conversation.\n\n01:13.880 --> 01:17.640\n I respect Elon as I do all other leaders and engineers I've spoken with.\n\n01:18.220 --> 01:20.540\n We agree on some things and disagree on others.\n\n01:20.980 --> 01:24.560\n My goal is always with these conversations is to understand the way\n\n01:24.560 --> 01:26.220\n the guest sees the world.\n\n01:26.860 --> 01:30.700\n One particular point of disagreement in this conversation was the extent to\n\n01:30.700 --> 01:35.700\n which camera based driver monitoring will improve outcomes and for how long\n\n01:36.040 --> 01:38.900\n it will remain relevant for AI assisted driving.\n\n01:39.900 --> 01:44.080\n As someone who works on and is fascinated by human centered artificial\n\n01:44.080 --> 01:48.340\n intelligence, I believe that if implemented and integrated effectively,\n\n01:48.640 --> 01:52.880\n camera based driver monitoring is likely to be of benefit in both the short\n\n01:52.880 --> 01:54.860\n term and the long term.\n\n01:55.580 --> 02:01.520\n In contrast, Elon and Tesla's focus is on the improvement of autopilot such\n\n02:01.520 --> 02:06.700\n that it's statistical safety benefits override any concern of human behavior\n\n02:06.960 --> 02:08.260\n and psychology.\n\n02:09.000 --> 02:13.840\n Elon and I may not agree on everything, but I deeply respect the engineering\n\n02:13.880 --> 02:16.340\n and innovation behind the efforts that he leads.\n\n02:16.800 --> 02:21.920\n My goal here is to catalyze a rigorous nuanced and objective discussion in\n\n02:21.920 --> 02:25.580\n industry and academia on AI assisted driving.\n\n02:26.180 --> 02:29.980\n One that ultimately makes for a safer and better world.\n\n02:30.820 --> 02:34.420\n And now here's my conversation with Elon Musk.\n\n02:35.560 --> 02:40.140\n What was the vision, the dream of autopilot when, in the beginning, the\n\n02:40.140 --> 02:44.440\n big picture system level, when it was first conceived and started being\n\n02:44.440 --> 02:48.900\n installed in 2014, the hardware and the cars, what was the vision, the dream?\n\n02:48.900 --> 02:52.200\n I wouldn't characterize the vision or dream, simply that there are obviously\n\n02:52.200 --> 02:58.840\n two massive revolutions in, in the automobile industry.\n\n02:59.280 --> 03:05.480\n One is the transition to electrification and then the other is autonomy.\n\n03:06.920 --> 03:14.420\n And it became obvious to me that in the future, any car that does not have\n\n03:14.420 --> 03:19.360\n autonomy would be about as useful as a horse, which is not to say that\n\n03:19.360 --> 03:23.540\n there's no use, it's just rare and somewhat idiosyncratic if somebody\n\n03:23.540 --> 03:24.440\n has a horse at this point.\n\n03:24.900 --> 03:27.300\n It's just obvious that cars will drive themselves completely.\n\n03:27.480 --> 03:28.540\n It's just a question of time.\n\n03:29.040 --> 03:37.340\n And if we did not participate in the autonomy revolution, then our cars\n\n03:37.340 --> 03:42.260\n would not be useful to people relative to cars that are autonomous.\n\n03:42.260 --> 03:48.600\n I mean, an autonomous car is arguably worth five to 10 times more than\n\n03:49.100 --> 03:51.440\n a car which is not autonomous.\n\n03:52.480 --> 03:53.240\n In the long term.\n\n03:53.780 --> 03:57.140\n Turns out what you mean by long term, but let's say at least for the\n\n03:57.140 --> 03:59.080\n next five years, perhaps 10 years.\n\n04:00.240 --> 04:04.020\n So there are a lot of very interesting design choices with autopilot early on.\n\n04:04.520 --> 04:10.120\n First is showing on the instrument cluster or in the Model 3 on the\n\n04:10.120 --> 04:15.200\n center stack display, what the combined sensor suite sees, what was the\n\n04:15.200 --> 04:16.460\n thinking behind that choice?\n\n04:16.600 --> 04:17.560\n Was there a debate?\n\n04:17.600 --> 04:18.600\n What was the process?\n\n04:19.160 --> 04:25.140\n The whole point of the display is to provide a health check on the\n\n04:25.480 --> 04:26.800\n vehicle's perception of reality.\n\n04:26.800 --> 04:30.440\n So the vehicle's taking information from a bunch of sensors, primarily\n\n04:30.440 --> 04:34.540\n cameras, but also radar and ultrasonics, GPS, and so forth.\n\n04:34.540 --> 04:41.680\n And then that, that information is then rendered into vector space and that,\n\n04:41.720 --> 04:46.480\n you know, with a bunch of objects with, with properties like lane lines and\n\n04:46.480 --> 04:47.980\n traffic lights and other cars.\n\n04:48.460 --> 04:53.160\n And then in vector space that is rerendered onto a display.\n\n04:53.400 --> 04:56.560\n So you can confirm whether the car knows what's going on or not\n\n04:58.060 --> 04:58.860\n by looking out the window.\n\n04:59.860 --> 05:00.200\n Right.\n\n05:00.200 --> 05:04.600\n I think that's an extremely powerful thing for people to get an understanding.\n\n05:04.940 --> 05:06.960\n So it become one with the system and understanding what\n\n05:06.960 --> 05:08.100\n the system is capable of.\n\n05:08.840 --> 05:11.400\n Now, have you considered showing more?\n\n05:11.840 --> 05:16.040\n So if we look at the computer vision, you know, like road segmentation,\n\n05:16.040 --> 05:19.440\n lane detection, vehicle detection, object detection, underlying the system,\n\n05:19.920 --> 05:22.120\n there is at the edges, some uncertainty.\n\n05:22.680 --> 05:28.280\n Have you considered revealing the parts that the vehicle is\n\n05:28.280 --> 05:34.220\n in, the parts that the, the uncertainty in the system, the sort of probabilities\n\n05:34.260 --> 05:36.740\n associated with, with say image recognition or something like that?\n\n05:36.760 --> 05:37.160\n Yeah.\n\n05:37.160 --> 05:41.660\n So right now it shows like the vehicles in the vicinity, a very clean, crisp image.\n\n05:41.840 --> 05:45.140\n And people do confirm that there's a car in front of me and the system\n\n05:45.140 --> 05:49.000\n sees there's a car in front of me, but to help people build an intuition\n\n05:49.000 --> 05:52.280\n of what computer vision is by showing some of the uncertainty.\n\n05:53.040 --> 05:57.440\n Well, I think it's, in my car, I always look at the sort of the debug view.\n\n05:57.440 --> 05:59.140\n And there's, there's two debug views.\n\n05:59.440 --> 06:04.620\n Uh, one is augmented vision, uh, where, which I'm sure you've seen where it's\n\n06:04.620 --> 06:09.620\n basically, we draw boxes and labels around objects that are recognized.\n\n06:10.760 --> 06:15.960\n And then there's a work called the visualizer, which is basically vector\n\n06:15.960 --> 06:22.340\n space representation, summing up the input from all sensors that doesn't,\n\n06:22.340 --> 06:28.180\n that doesn't, does not show any pictures, but it shows, uh, all of the, it's\n\n06:28.180 --> 06:32.140\n basically shows the car's view of, of, of the world in vector space.\n\n06:32.480 --> 06:36.320\n Um, but I think this is very difficult for people to know, normal people to\n\n06:36.320 --> 06:38.360\n understand, they would not know what they're looking at.\n\n06:39.460 --> 06:42.660\n So it's almost an HMI challenge to the current things that are being\n\n06:42.660 --> 06:47.200\n displayed is optimized for the general public understanding of\n\n06:47.200 --> 06:48.560\n what the system is capable of.\n\n06:48.720 --> 06:51.600\n It's like, if you have no idea what, how computer vision works or anything,\n\n06:51.600 --> 06:54.560\n you can sort of look at the screen and see if the car knows what's going on.\n\n06:55.740 --> 06:59.280\n And then if you're, you know, if you're a development engineer or if you're,\n\n06:59.320 --> 07:02.720\n you know, if you're, if you have the development build like I do, then you\n\n07:02.720 --> 07:07.520\n can see, uh, you know, all the debug information, but those would just be\n\n07:07.560 --> 07:10.360\n like total diverse to most people.\n\n07:11.200 --> 07:13.720\n What's your view on how to best distribute effort.\n\n07:14.200 --> 07:17.560\n So there's three, I would say technical aspects of autopilot\n\n07:17.560 --> 07:18.800\n that are really important.\n\n07:18.800 --> 07:21.680\n So it's the underlying algorithms, like the neural network architecture,\n\n07:22.000 --> 07:25.680\n there's the data, so that the strain on, and then there's a hardware development.\n\n07:26.000 --> 07:32.400\n There may be others, but so look, algorithm, data, hardware, you don't, you\n\n07:32.400 --> 07:35.960\n only have so much money, only have so much time, what do you think is the most\n\n07:35.960 --> 07:40.800\n important thing to, to, uh, allocate resources to, or do you see it as pretty\n\n07:40.800 --> 07:43.440\n evenly distributed between those three?\n\n07:43.440 --> 07:51.040\n We automatically get a fast amounts of data because all of our cars have eight\n\n07:51.040 --> 07:58.560\n external facing cameras and radar, and usually 12 ultrasonic sensors, uh, GPS,\n\n07:58.560 --> 08:01.320\n obviously, um, and, uh, IMU.\n\n08:02.920 --> 08:10.400\n And so we basically have a fleet that has, uh, and we've got about 400,000\n\n08:10.400 --> 08:13.880\n cars on the road that have that level of data, I think you keep quite\n\n08:13.880 --> 08:14.840\n close track of it actually.\n\n08:14.840 --> 08:15.340\n Yes.\n\n08:15.520 --> 08:15.800\n Yeah.\n\n08:15.800 --> 08:20.720\n So we're, we're approaching half a million cars on the road that have the full sensor\n\n08:20.720 --> 08:21.220\n suite.\n\n08:21.520 --> 08:27.720\n Um, so this is, I'm, I'm not sure how many other cars on the road have the sensor\n\n08:27.720 --> 08:32.400\n suite, but I would be surprised if it's more than 5,000, which means that we\n\n08:32.400 --> 08:33.920\n have 99% of all the data.\n\n08:35.200 --> 08:36.920\n So there's this huge inflow of data.\n\n08:37.400 --> 08:37.920\n Absolutely.\n\n08:37.920 --> 08:43.800\n Massive inflow of data, and then we, it's, it's taken us about three years, but now\n\n08:43.800 --> 08:51.120\n we've finally developed our full self driving computer, which can process, uh,\n\n08:51.160 --> 08:54.720\n and in order of magnitude as much as the Nvidia system that we currently have in\n\n08:54.720 --> 08:59.000\n the, in the cars, and it's really just a, to use it, you've unplugged the Nvidia\n\n08:59.000 --> 09:01.600\n computer and plug the Tesla computer in and that's it.\n\n09:01.600 --> 09:06.400\n And it's, it's, uh, in fact, we're not even, we're still exploring the boundaries\n\n09:06.400 --> 09:10.080\n of capabilities, uh, but we're able to run the cameras at full frame rate, full\n\n09:10.080 --> 09:16.600\n resolution, uh, not even crop the images and it's still got headroom even on one\n\n09:16.600 --> 09:20.840\n of the systems, the harder full self driving computer is really two computers,\n\n09:21.240 --> 09:23.680\n two systems on a chip that are fully redundant.\n\n09:23.840 --> 09:27.320\n So you could put a bolt through basically any part of that system and it still\n\n09:27.320 --> 09:27.820\n works.\n\n09:27.820 --> 09:33.100\n The redundancy, are they perfect copies of each other or also it's purely for\n\n09:33.100 --> 09:37.140\n redundancy as opposed to an argue machine kind of architecture where they're both\n\n09:37.140 --> 09:37.740\n making decisions.\n\n09:37.780 --> 09:39.220\n This is purely for redundancy.\n\n09:39.540 --> 09:43.620\n I think it would more like it's, if you have a twin engine aircraft, uh, commercial\n\n09:43.620 --> 09:51.140\n aircraft, the system will operate best if both systems are operating, but it's,\n\n09:51.180 --> 09:53.140\n it's capable of operating safely on one.\n\n09:53.140 --> 09:59.020\n So, but as it is right now, we can just run, we're, we haven't even hit the, the,\n\n09:59.020 --> 10:01.020\n the edge of performance.\n\n10:01.020 --> 10:09.980\n So there's no need to actually distribute functionality across both SOCs.\n\n10:10.020 --> 10:13.540\n We can actually just run a full duplicate on, on, on each one.\n\n10:13.660 --> 10:17.100\n Do you haven't really explored or hit the limit of this?\n\n10:17.100 --> 10:18.220\n Not yet at the limiter.\n\n10:18.220 --> 10:22.740\n So the magic of deep learning is that it gets better with data.\n\n10:22.900 --> 10:28.340\n You said there's a huge inflow of data, but the thing about driving the really\n\n10:28.340 --> 10:31.460\n valuable data to learn from is the edge cases.\n\n10:32.260 --> 10:39.100\n So how do you, I mean, I've, I've heard you talk somewhere about, uh, autopilot\n\n10:39.100 --> 10:42.140\n disengagements being an important moment of time to use.\n\n10:42.460 --> 10:46.660\n Is there other edge cases where you can, you know, you can, you can, you can\n\n10:46.660 --> 10:52.540\n drive, is there other edge cases or perhaps can you speak to those edge cases?\n\n10:53.060 --> 10:56.900\n What aspects of that might be valuable or if you have other ideas, how to\n\n10:56.900 --> 10:59.580\n discover more and more and more edge cases in driving?\n\n11:00.780 --> 11:02.220\n Well, there's a lot of things that are learned.\n\n11:02.860 --> 11:06.940\n There are certainly edge cases where I say somebody is on autopilot and they,\n\n11:06.980 --> 11:12.580\n they take over and then, okay, that, that, that, that's a trigger that goes to our\n\n11:12.580 --> 11:16.220\n system that says, okay, did they take over for convenience or do they take\n\n11:16.220 --> 11:19.020\n over because the autopilot wasn't working properly.\n\n11:19.380 --> 11:22.980\n There's also like, let's say we're, we're trying to figure out what is the optimal\n\n11:23.700 --> 11:26.180\n spline for traversing an intersection.\n\n11:27.420 --> 11:33.540\n Um, then then the ones where there are no interventions and are the right ones.\n\n11:33.660 --> 11:37.260\n So you then say, okay, when it looks like this, do the following.\n\n11:38.300 --> 11:42.020\n And then, and then you get the optimal spline for a complex, uh,\n\n11:42.060 --> 11:44.660\n navigating a complex, uh, intersection.\n\n11:44.660 --> 11:46.260\n So that's for this.\n\n11:46.260 --> 11:50.780\n So there's kind of the common case you're trying to, uh, capture a huge amount of\n\n11:50.780 --> 11:54.500\n samples of a particular intersection, how, when things went right, and then\n\n11:54.500 --> 11:59.420\n there's the edge case where, uh, as you said, not for convenience, but\n\n11:59.420 --> 12:01.140\n something didn't go exactly right.\n\n12:01.140 --> 12:03.900\n Somebody took over, somebody asserted manual control from autopilot.\n\n12:05.020 --> 12:08.100\n And really like the way to look at this as view all input is error.\n\n12:08.900 --> 12:12.700\n If the user had to do input, it does something all input is error.\n\n12:12.980 --> 12:13.940\n That's a powerful line.\n\n12:13.940 --> 12:17.460\n That's a powerful line to think of it that way, because they may very well be\n\n12:17.460 --> 12:21.380\n error, but if you want to exit the highway, or if you want to, uh, it's\n\n12:21.380 --> 12:25.340\n a navigation decision that all autopilot is not currently designed to do.\n\n12:25.380 --> 12:27.180\n Then the driver takes over.\n\n12:27.540 --> 12:28.380\n How do you know the difference?\n\n12:28.380 --> 12:31.180\n That's going to change with navigate an autopilot, which we were just\n\n12:31.180 --> 12:33.580\n released and without still confirm.\n\n12:33.820 --> 12:38.340\n So the navigation, like lane change based, like a certain control in\n\n12:38.340 --> 12:42.780\n order to change, do a lane change or exit a freeway or, or doing a highway\n\n12:42.780 --> 12:47.580\n under change, the vast majority of that will go away with, um, the\n\n12:47.580 --> 12:48.500\n release that just went out.\n\n12:48.900 --> 12:49.140\n Yeah.\n\n12:49.140 --> 12:54.540\n So that, that I don't think people quite understand how big of a step that is.\n\n12:54.580 --> 12:55.100\n Yeah, they don't.\n\n12:55.900 --> 12:57.700\n So if you drive the car, then you do.\n\n12:58.260 --> 13:00.980\n So you still have to keep your hands on the steering wheel currently when\n\n13:00.980 --> 13:02.500\n it does the automatic lane change.\n\n13:03.420 --> 13:07.780\n What are, so there's, there's these big leaps through the development of\n\n13:07.780 --> 13:13.580\n autopilot through its history and what stands out to you as the big leaps?\n\n13:13.580 --> 13:18.540\n I would say this one, navigate an autopilot without, uh, confirm\n\n13:18.580 --> 13:21.060\n without having to confirm is a huge leap.\n\n13:21.100 --> 13:22.020\n It is a huge leap.\n\n13:22.500 --> 13:24.380\n It also automatically overtakes low cars.\n\n13:24.900 --> 13:30.100\n So it's, it's both navigation, um, and seeking the fastest lane.\n\n13:31.060 --> 13:36.420\n So it'll, it'll, it'll, you know, overtake a slow cause, um, and exit the\n\n13:36.420 --> 13:38.540\n freeway and take highway interchanges.\n\n13:38.540 --> 13:47.380\n And, and then, uh, we have, uh, traffic lights, uh, recognition, which\n\n13:47.380 --> 13:49.780\n introduced initially as a, as a warning.\n\n13:50.220 --> 13:53.460\n I mean, on the development version that I'm driving, the car fully, fully\n\n13:53.460 --> 13:55.980\n stops and goes at traffic lights.\n\n13:56.900 --> 13:58.500\n So those are the steps, right?\n\n13:58.500 --> 14:02.220\n You've just mentioned something sort of inkling a step towards full autonomy.\n\n14:02.220 --> 14:06.860\n What would you say are the biggest technological roadblocks\n\n14:06.860 --> 14:08.100\n to full self driving?\n\n14:08.900 --> 14:11.860\n Actually, I don't think, I think we just, the full self driving computer that we\n\n14:11.860 --> 14:17.660\n just, uh, that the Tesla, what we call the FSD computer, uh, that that's now in\n\n14:17.660 --> 14:18.340\n production.\n\n14:20.540 --> 14:26.300\n Uh, so if you order, uh, any model SRX or any model three that has the full self\n\n14:26.300 --> 14:29.700\n driving package, you'll get the FSD computer.\n\n14:29.700 --> 14:34.980\n That, that was, that's important to have enough, uh, base computation, uh, then\n\n14:34.980 --> 14:39.820\n refining the neural net and the control software, uh, which, but all of that can\n\n14:39.820 --> 14:41.340\n just be provided as an over there update.\n\n14:42.660 --> 14:47.460\n The thing that's really profound and where I'll be emphasizing at the, uh, sort\n\n14:47.460 --> 14:51.100\n of what that investor day that we're having focused on autonomy is that the\n\n14:51.100 --> 14:55.820\n cars currently being produced with the hardware currently being produced is\n\n14:55.820 --> 15:01.940\n capable of full self driving, but capable is an interesting word because, um, like\n\n15:01.940 --> 15:07.980\n the hardware is, and as we refine the software, the capabilities will increase\n\n15:07.980 --> 15:11.740\n dramatically, um, and then the reliability will increase dramatically, and then it\n\n15:11.740 --> 15:13.140\n will receive regulatory approval.\n\n15:13.420 --> 15:16.100\n So essentially buying a car today is an investment in the future.\n\n15:16.420 --> 15:21.580\n You're essentially buying a car, you're buying the, I think the most profound\n\n15:21.580 --> 15:26.860\n thing is that if you buy a Tesla today, I believe you are buying an appreciating\n\n15:26.860 --> 15:29.180\n asset, not a depreciating asset.\n\n15:30.140 --> 15:33.940\n So that's a really important statement there because if hardware is capable\n\n15:33.940 --> 15:37.220\n enough, that's the hard thing to upgrade usually.\n\n15:37.260 --> 15:37.700\n Exactly.\n\n15:37.700 --> 15:40.820\n So then the rest is a software problem.\n\n15:40.820 --> 15:41.300\n Yes.\n\n15:41.500 --> 15:43.660\n Software has no marginal cost really.\n\n15:44.940 --> 15:48.420\n But what's your intuition on the software side?\n\n15:48.420 --> 15:57.500\n How hard are the remaining steps to, to get it to where, um, you know, uh, the,\n\n15:57.500 --> 16:03.260\n the experience, uh, not just the safety, but the full experience is something\n\n16:03.260 --> 16:05.540\n that people would, uh, enjoy.\n\n16:06.220 --> 16:09.500\n Well, I think people enjoy it very much so on, on, on the highways.\n\n16:09.500 --> 16:15.180\n It's, it's a total game changer for quality of life for using, you know,\n\n16:15.180 --> 16:19.340\n Tesla autopilot on the highways, uh, so it's really just extending that\n\n16:19.340 --> 16:26.180\n functionality to city streets, adding in the traffic light recognition, uh,\n\n16:26.220 --> 16:32.420\n navigating complex intersections and, um, and then, uh, being able to navigate\n\n16:32.540 --> 16:37.860\n complicated parking lots so the car can, uh, exit a parking space and come and\n\n16:37.860 --> 16:43.740\n find you, even if it's in a complete maze of a parking lot, um, and, and, and,\n\n16:43.740 --> 16:46.300\n and then if, and then you can just, it can just drop you off and find a\n\n16:46.300 --> 16:48.180\n parking spot by itself.\n\n16:48.940 --> 16:49.140\n Yeah.\n\n16:49.140 --> 16:53.860\n In terms of enjoyability and something that people would, uh, would actually\n\n16:53.860 --> 16:58.300\n find a lot of use from the parking lot is a, is a really, you know, it's, it's\n\n16:58.300 --> 17:00.700\n rich of annoyance when you have to do it manually.\n\n17:00.700 --> 17:03.620\n So there's a lot of benefit to be gained from automation there.\n\n17:04.580 --> 17:07.780\n So let me start injecting the human into this discussion a little bit.\n\n17:08.380 --> 17:13.620\n Uh, so let's talk about, uh, the, the, the, the, the, the, the, the, the, the,\n\n17:13.620 --> 17:14.700\n about full autonomy.\n\n17:15.660 --> 17:18.060\n If you look at the current level four vehicles being tested on\n\n17:18.060 --> 17:23.340\n road, like Waymo and so on, they're only technically autonomous.\n\n17:23.380 --> 17:28.860\n They're really level two systems with just the different design philosophy,\n\n17:28.860 --> 17:31.660\n because there's always a safety driver in almost all cases and\n\n17:31.660 --> 17:32.820\n they're monitoring the system.\n\n17:33.060 --> 17:33.300\n Right.\n\n17:33.340 --> 17:42.580\n Do you see Tesla's full self driving as still for a time to come requiring\n\n17:42.580 --> 17:44.380\n supervision of the human being.\n\n17:44.780 --> 17:48.580\n So it's capabilities are powerful enough to drive, but nevertheless requires\n\n17:48.580 --> 17:53.140\n the human to still be supervising, just like a safety driver is in a\n\n17:54.820 --> 17:56.420\n other fully autonomous vehicles.\n\n17:57.340 --> 18:05.900\n I think it will require detecting hands on wheel for at least, uh, six months\n\n18:05.900 --> 18:07.660\n or something like that from here.\n\n18:07.660 --> 18:15.060\n It really is a question of like, from a regulatory standpoint, uh, what, how much\n\n18:15.060 --> 18:20.900\n safer than a person does autopilot need to be for it to be okay to not monitor\n\n18:20.900 --> 18:25.460\n the car, you know, and, and this is a debate that one can have it.\n\n18:25.460 --> 18:30.340\n And then if you, but you need, you know, a large sample, a large amount of data.\n\n18:30.980 --> 18:36.340\n Um, so you can prove with high confidence, statistically speaking, that the car is\n\n18:36.340 --> 18:40.940\n dramatically safer than a person, um, and that adding in the person monitoring\n\n18:40.940 --> 18:43.660\n does not materially affect the safety.\n\n18:44.060 --> 18:47.340\n So it might need to be like two or 300% safer than a person.\n\n18:48.300 --> 18:52.780\n And how do you prove that incidents per mile incidents per mile crashes and\n\n18:53.460 --> 18:58.100\n fatalities, fatalities would be a factor, but there, there are just not enough\n\n18:58.100 --> 19:03.060\n fatalities to be statistically significant at scale, but there are enough.\n\n19:03.060 --> 19:06.500\n Crashes, you know, there are far more crashes than there are fatalities.\n\n19:08.180 --> 19:14.460\n So you can assess what is the probability of a crash that then there's another step\n\n19:14.460 --> 19:19.140\n which probability of injury and probability of permanent injury, the\n\n19:19.140 --> 19:24.380\n probability of death, and all of those need to be a much better than a person,\n\n19:24.660 --> 19:28.900\n uh, by at least perhaps 200%.\n\n19:28.900 --> 19:33.500\n And you think there's, uh, the ability to have a healthy discourse with the\n\n19:33.500 --> 19:35.420\n regulatory bodies on this topic?\n\n19:36.020 --> 19:41.140\n I mean, there's no question that, um, but, um, regulators pay just disproportionate\n\n19:41.140 --> 19:44.260\n amount of attention to that, which generates press.\n\n19:44.700 --> 19:46.060\n This is just an objective fact.\n\n19:46.420 --> 19:48.940\n Um, and Tesla generates a lot of press.\n\n19:49.260 --> 19:55.660\n So the, you know, in the United States, this, I think almost, you know,\n\n19:55.660 --> 20:01.020\n uh, in the United States, this, I think almost 40,000 automotive deaths per year.\n\n20:01.820 --> 20:06.100\n Uh, but if there are four in Tesla, they'll probably receive a thousand\n\n20:06.100 --> 20:07.820\n times more press than anyone else.\n\n20:08.780 --> 20:11.420\n So the, the psychology of that is actually fascinating.\n\n20:11.460 --> 20:14.820\n I don't think we'll have enough time to talk about that, but I have to talk to\n\n20:14.820 --> 20:16.540\n you about the human side of things.\n\n20:16.980 --> 20:21.860\n So myself and our team at MIT recently released the paper on functional\n\n20:21.860 --> 20:23.980\n vigilance of drivers while using autopilot.\n\n20:23.980 --> 20:28.500\n This is work we've been doing since autopilot was first released publicly\n\n20:28.580 --> 20:33.140\n over three years ago, collecting video of driver faces and driver body.\n\n20:34.020 --> 20:40.980\n So I saw that you tweeted a quote from the abstract, so I can at least, uh,\n\n20:40.980 --> 20:42.780\n guess that you've glanced at it.\n\n20:42.820 --> 20:43.300\n Yeah, I read it.\n\n20:43.940 --> 20:45.700\n Can I talk you through what we found?\n\n20:45.740 --> 20:46.020\n Sure.\n\n20:46.140 --> 20:46.420\n Okay.\n\n20:46.420 --> 20:53.620\n So it appears that in the data that we've collected, that drivers are maintaining\n\n20:53.620 --> 20:57.260\n functional vigilance such that we're looking at 18,000 disengagement from\n\n20:57.260 --> 21:04.420\n autopilot, 18,900 and annotating, were they able to take over control in a timely\n\n21:04.420 --> 21:04.860\n manner?\n\n21:05.100 --> 21:09.020\n So they were there present looking at the road, uh, to take over control.\n\n21:09.500 --> 21:09.860\n Okay.\n\n21:09.860 --> 21:15.500\n So this, uh, goes against what, what many would predict from the body of literature\n\n21:15.500 --> 21:17.180\n on vigilance with automation.\n\n21:18.060 --> 21:22.260\n Now, the question is, do you think these results hold across the broader\n\n21:22.260 --> 21:23.060\n population?\n\n21:23.300 --> 21:25.420\n So ours is just a small subset.\n\n21:25.780 --> 21:30.700\n Do you think, uh, one of the criticism is that, you know, there's a small\n\n21:30.700 --> 21:35.380\n minority of drivers that may be highly responsible where their vigilance\n\n21:35.420 --> 21:38.180\n decrement would increase with autopilot use?\n\n21:38.180 --> 21:40.220\n I think this is all really going to be swept.\n\n21:40.260 --> 21:46.660\n I mean, the system's improving so much, so fast that this is going to be a mood\n\n21:46.660 --> 21:55.860\n point very soon where vigilance is like, if something's many times safer than a\n\n21:55.860 --> 22:01.620\n person, then adding a person, uh, does the, the, the effect on safety is, is\n\n22:01.620 --> 22:02.100\n limited.\n\n22:02.100 --> 22:09.580\n Um, and in fact, uh, it could be negative.\n\n22:09.580 --> 22:10.420\n That's really interesting.\n\n22:10.420 --> 22:16.660\n So the, uh, the, so the fact that a human may, some percent of the population may,\n\n22:16.660 --> 22:20.980\n uh, exhibit a vigilance decrement will not affect overall statistics numbers of\n\n22:20.980 --> 22:21.340\n safety.\n\n22:21.380 --> 22:27.460\n No, in fact, I think it will become, uh, very, very quickly, maybe even towards\n\n22:27.460 --> 22:30.860\n the end of this year, but I'd say I'd be shocked if it's not next year.\n\n22:30.860 --> 22:35.260\n At the latest, that, um, having the person, having a human intervene will\n\n22:35.300 --> 22:38.940\n decrease safety decrease.\n\n22:38.980 --> 22:42.220\n It's like, imagine if you're an elevator and it used to be that there were\n\n22:42.220 --> 22:46.780\n elevator operators, um, and, and you couldn't go on an elevator by yourself\n\n22:46.780 --> 22:49.460\n and work the lever to move between floors.\n\n22:49.940 --> 22:56.900\n Um, and now, uh, nobody wants it an elevator operator because the automated\n\n22:56.900 --> 23:00.500\n elevator that stops the floors is much safer than the elevator operator.\n\n23:01.940 --> 23:05.340\n And in fact, it would be quite dangerous to have someone with a lever that can\n\n23:05.420 --> 23:06.940\n move the elevator between floors.\n\n23:07.740 --> 23:11.900\n So that's a, that's a really powerful statement and really interesting one.\n\n23:12.500 --> 23:16.140\n But I also have to ask from a user experience and from a safety perspective,\n\n23:16.620 --> 23:22.580\n one of the passions for me algorithmically is a camera based detection of, uh,\n\n23:22.580 --> 23:26.380\n of just sensing the human, but detecting what the driver is looking at, cognitive\n\n23:26.380 --> 23:30.140\n load, body pose on the computer vision side, that's a fascinating problem.\n\n23:30.140 --> 23:33.620\n But do you, and there's many in industry believe you have to have\n\n23:33.620 --> 23:35.220\n camera based driver monitoring.\n\n23:35.540 --> 23:38.860\n Do you think there could be benefit gained from driver monitoring?\n\n23:39.700 --> 23:44.660\n If you have a system that's, that's at, that's at or below a human level\n\n23:44.660 --> 23:46.700\n reliability, then driver monitoring makes sense.\n\n23:48.220 --> 23:51.540\n But if your system is dramatically better, more likely to be\n\n23:51.540 --> 23:55.740\n better, more liable than, than a human, then drive monitoring monitoring\n\n23:55.780 --> 23:58.540\n is not just not help much.\n\n23:59.420 --> 24:03.500\n And, uh, like I said, you, you, just like, as an, you wouldn't want someone\n\n24:03.500 --> 24:06.580\n into like, you wouldn't want someone in the elevator, if you're in an elevator,\n\n24:06.580 --> 24:09.660\n do you really want someone with a big lever, some, some random person\n\n24:09.780 --> 24:11.380\n operating the elevator between floors?\n\n24:12.940 --> 24:15.300\n I wouldn't trust that or rather have the buttons.\n\n24:17.420 --> 24:17.860\n Okay.\n\n24:17.860 --> 24:21.900\n You're optimistic about the pace of improvement of the system that from\n\n24:21.900 --> 24:25.780\n what you've seen with the full self driving car computer, the rate\n\n24:25.780 --> 24:26.980\n of improvement is exponential.\n\n24:28.300 --> 24:32.900\n So one of the other very interesting design choices early on that connects\n\n24:32.900 --> 24:38.020\n to this is the operational design domain of autopilot.\n\n24:38.020 --> 24:44.820\n So where autopilot is able to be turned on the, so contrast another vehicle\n\n24:44.820 --> 24:48.780\n system that we're studying is the Cadillac SuperCrew system.\n\n24:48.860 --> 24:53.620\n That's in terms of ODD, very constrained to particular kinds of highways, well\n\n24:53.620 --> 24:58.340\n mapped, tested, but it's much narrower than the ODD of Tesla vehicles.\n\n24:58.940 --> 25:00.660\n What's there's, there's pros and...\n\n25:00.660 --> 25:01.540\n It's like ADD.\n\n25:02.580 --> 25:02.860\n Yeah.\n\n25:04.300 --> 25:04.740\n That's good.\n\n25:04.740 --> 25:06.100\n That's a, that's a good line.\n\n25:06.660 --> 25:13.060\n Uh, what was the design decision, uh, what, in that different philosophy\n\n25:13.060 --> 25:18.820\n of thinking where there's pros and cons, what we see with, uh, a wide ODD\n\n25:18.820 --> 25:22.900\n is drive Tesla drivers are able to explore more the limitations of the\n\n25:22.900 --> 25:26.860\n system, at least early on, and they understand together with the instrument\n\n25:26.860 --> 25:29.980\n cluster display, they start to understand what are the capabilities.\n\n25:30.180 --> 25:31.220\n So that's a benefit.\n\n25:31.740 --> 25:36.740\n The con is you go, you're letting drivers use it basically anywhere.\n\n25:38.180 --> 25:41.100\n So anyway, that could detect lanes with confidence.\n\n25:41.100 --> 25:45.940\n Was there a philosophy, uh, design decisions that were challenging\n\n25:46.020 --> 25:51.260\n that were being made there or from the very beginning, was that, uh,\n\n25:51.300 --> 25:53.620\n done on purpose with intent?\n\n25:54.100 --> 25:57.380\n Well, I mean, I think it's frankly, it's pretty crazy giving it, letting people\n\n25:57.380 --> 26:00.820\n drive a two ton death machine manually.\n\n26:01.340 --> 26:03.180\n Uh, that's crazy.\n\n26:03.580 --> 26:07.740\n Like, like in the future of people who are like, I can't believe anyone was\n\n26:07.740 --> 26:12.780\n just allowed to drive for one of these two ton death machines and they\n\n26:12.780 --> 26:14.020\n just drive wherever they wanted.\n\n26:14.100 --> 26:14.980\n Just like elevators.\n\n26:14.980 --> 26:17.780\n He was like, move the elevator with that lever, wherever you want.\n\n26:17.780 --> 26:19.500\n It can stop at halfway between floors if you want.\n\n26:22.060 --> 26:23.020\n It's pretty crazy.\n\n26:24.140 --> 26:31.260\n So it's going to seem like a mad thing in the future that people were driving cars.\n\n26:32.500 --> 26:36.380\n So I have a bunch of questions about the human psychology, about behavior and so\n\n26:36.380 --> 26:46.140\n on that would become that because, uh, you have faith in the AI system, uh, not\n\n26:46.140 --> 26:51.180\n faith, but, uh, the, both on the hardware side and the deep learning approach of\n\n26:51.180 --> 26:55.100\n learning from data will make it just far safer than humans.\n\n26:55.260 --> 26:56.060\n Yeah, exactly.\n\n26:56.900 --> 27:00.780\n Recently, there are a few hackers who, uh, tricked autopilot to act in\n\n27:00.780 --> 27:03.020\n unexpected ways with adversarial examples.\n\n27:03.020 --> 27:06.900\n So we all know that neural network systems are very sensitive to minor\n\n27:06.900 --> 27:09.700\n disturbances to these adversarial examples on input.\n\n27:10.420 --> 27:13.700\n Do you think it's possible to defend against something like this for the\n\n27:13.700 --> 27:15.140\n broader, for the industry?\n\n27:15.140 --> 27:15.640\n Sure.\n\n27:15.860 --> 27:21.700\n So can you elaborate on the, on the confidence behind that answer?\n\n27:22.900 --> 27:27.100\n Um, well the, you know, neural net is just like a basic bunch of matrix math.\n\n27:27.820 --> 27:31.620\n Or you have to be like a very sophisticated, somebody who really\n\n27:31.620 --> 27:36.620\n understands neural nets and like basically reverse engineer how the matrix\n\n27:36.620 --> 27:42.700\n is being built and then create a little thing that's just exactly, um, causes\n\n27:42.700 --> 27:44.340\n the matrix math to be slightly off.\n\n27:44.740 --> 27:49.540\n But it's very easy to then block it, block that by, by having basically\n\n27:49.540 --> 27:51.100\n anti negative recognition.\n\n27:51.100 --> 27:55.460\n It's like if you, if the system sees something that looks like a matrix hack,\n\n27:55.460 --> 28:00.860\n uh, exclude it, so it's such an easy thing to do.\n\n28:01.860 --> 28:05.300\n So learn both on the, the valid data and the invalid data.\n\n28:05.340 --> 28:08.940\n So basically learn on the adversarial examples to be able to exclude them.\n\n28:08.980 --> 28:09.480\n Yeah.\n\n28:09.480 --> 28:13.020\n Like you basically want to both know what is, what is a car and\n\n28:13.020 --> 28:14.700\n what is definitely not a car.\n\n28:15.260 --> 28:18.300\n And you train for this is a car and this is definitely not a car.\n\n28:18.340 --> 28:19.300\n Those are two different things.\n\n28:20.180 --> 28:23.020\n People have no idea neural nets really.\n\n28:23.020 --> 28:26.140\n They probably think neural nets are both like, you know, fishing net only.\n\n28:28.460 --> 28:36.260\n So as you know, so taking a step beyond just Tesla and autopilot, uh, current\n\n28:36.260 --> 28:42.660\n deep learning approaches still seem in some ways to be far from general\n\n28:42.660 --> 28:43.660\n intelligence systems.\n\n28:43.940 --> 28:49.820\n Do you think the current approaches will take us to general intelligence or do\n\n28:49.820 --> 28:52.500\n totally new ideas need to be invented?\n\n28:54.500 --> 28:59.740\n I think we're missing a few key ideas for general intelligence, general artificial\n\n28:59.740 --> 29:06.100\n general intelligence, but it's going to be upon us very quickly.\n\n29:07.700 --> 29:12.580\n And then we'll need to figure out what shall we do if we even have that choice?\n\n29:14.580 --> 29:18.700\n But it's amazing how people can't differentiate between say the narrow\n\n29:18.700 --> 29:24.140\n AI that, you know, allows a car to figure out what a lane line is and, and, and,\n\n29:24.140 --> 29:29.180\n you know, and navigate streets versus general intelligence.\n\n29:29.420 --> 29:31.140\n Like these are just very different things.\n\n29:32.020 --> 29:35.340\n Like your toaster and your computer are both machines, but one's much\n\n29:35.340 --> 29:36.460\n more sophisticated than another.\n\n29:37.460 --> 29:39.340\n You're confident with Tesla.\n\n29:39.340 --> 29:41.700\n You can create the world's best toaster.\n\n29:42.580 --> 29:43.420\n The world's best toaster.\n\n29:43.420 --> 29:43.920\n Yes.\n\n29:43.920 --> 29:50.640\n The world's best toaster. Yes. The world's best self driving. I'm, I, yes.\n\n29:52.240 --> 29:54.240\n To me right now, this seems game set match.\n\n29:54.880 --> 29:57.760\n I don't, I mean, that sounds, I don't want to be complacent or overconfident,\n\n29:57.760 --> 29:58.800\n but that's what it appears.\n\n29:58.880 --> 30:02.400\n That is just literally what it, how it appears right now.\n\n30:02.600 --> 30:08.960\n I could be wrong, but it appears to be the case that Tesla is vastly ahead of\n\n30:08.960 --> 30:09.480\n everyone.\n\n30:09.480 --> 30:14.960\n Do you think we will ever create an AI system that we can love and loves us back\n\n30:14.960 --> 30:15.960\n in a deep, meaningful way?\n\n30:15.960 --> 30:22.360\n Like in the movie, her, I think AI will be capable of convincing you to fall in\n\n30:22.360 --> 30:24.000\n love with it very well.\n\n30:24.360 --> 30:25.920\n And that's different than us humans.\n\n30:27.840 --> 30:31.560\n You know, we start getting into a metaphysical question of like, do emotions\n\n30:31.560 --> 30:33.800\n and thoughts exist in a different realm than the physical?\n\n30:34.160 --> 30:35.000\n And maybe they do.\n\n30:35.040 --> 30:35.560\n Maybe they don't.\n\n30:35.600 --> 30:36.100\n I don't know.\n\n30:36.100 --> 30:42.740\n But from a physics standpoint, I tend to think of things, you know, like physics\n\n30:42.740 --> 30:50.100\n was my main sort of training and from a physics standpoint, essentially, if it\n\n30:50.100 --> 30:53.940\n loves you in a way that is, that you can't tell whether it's real or not, it is\n\n30:53.940 --> 30:54.440\n real.\n\n30:55.940 --> 30:57.340\n That's a physics view of love.\n\n30:57.380 --> 30:57.880\n Yeah.\n\n30:59.180 --> 31:04.780\n If there's no, if you cannot just, if you cannot prove that it does not, if there's\n\n31:04.780 --> 31:14.900\n no, if there's no test that you can apply that would make it, allow you to tell the\n\n31:14.900 --> 31:17.340\n difference, then there is no difference.\n\n31:17.340 --> 31:17.840\n Right.\n\n31:17.860 --> 31:21.420\n And it's similar to seeing our world as simulation.\n\n31:21.420 --> 31:24.900\n There may not be a test to tell the difference between what the real world\n\n31:24.900 --> 31:28.780\n and the simulation, and therefore from a physics perspective, it might as well be\n\n31:28.780 --> 31:29.460\n the same thing.\n\n31:29.540 --> 31:30.040\n Yes.\n\n31:30.540 --> 31:33.220\n And there may be ways to test whether it's a simulation.\n\n31:33.220 --> 31:36.420\n There might be, I'm not saying there aren't, but you could certainly imagine\n\n31:36.420 --> 31:40.900\n that a simulation could correct that once an entity in the simulation found a way\n\n31:40.900 --> 31:45.820\n to detect the simulation, it could either restart, you know, pause the simulation,\n\n31:46.620 --> 31:50.340\n start a new simulation, or do one of many other things that then corrects for that\n\n31:50.340 --> 31:50.840\n error.\n\n31:52.380 --> 32:00.260\n So when maybe you or somebody else creates an AGI system and you get to ask\n\n32:00.260 --> 32:03.220\n her one question, what would that question be?\n\n32:16.260 --> 32:17.700\n What's outside the simulation?\n\n32:20.900 --> 32:22.620\n Elon, thank you so much for talking today.\n\n32:22.660 --> 32:23.160\n It was a pleasure.\n\n32:23.500 --> 32:24.000\n All right.\n\n32:24.000 --> 32:34.000\n Thank you.\n\n"
}