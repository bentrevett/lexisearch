{
  "title": "David Patterson: Computer Architecture and Data Storage | Lex Fridman Podcast #104",
  "id": "naed4C4hfAg",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:05.440\n The following is a conversation with David Patterson, touring award winner and professor\n\n00:05.440 --> 00:10.800\n of computer science at Berkeley. He's known for pioneering contributions to RISC processor\n\n00:10.800 --> 00:18.800\n architecture used by 99% of new chips today and for co creating RAID storage. The impact that\n\n00:18.800 --> 00:25.040\n these two lines of research and development have had in our world is immeasurable. He's also one of\n\n00:25.040 --> 00:30.160\n the great educators of computer science in the world. His book with John Hennessy is how I first\n\n00:30.160 --> 00:58.880\n learned about and was humbled by the inner workings of machines at the lowest level.\n\n01:21.120 --> 01:24.400\n This episode is supported by the Jordan Harbinger Show.\n\n01:24.400 --> 01:30.800\n Go to Jordan Harbinger.com slash Lex. It's how he knows I sent you on that page. There's links\n\n01:30.800 --> 01:36.560\n to subscribe to it on Apple podcast, Spotify, and everywhere else. I've been binging on this podcast.\n\n01:36.560 --> 01:41.600\n It's amazing. Jordan is a great human being. He gets the best out of his guests, dives deep,\n\n01:41.600 --> 01:46.080\n calls them out when it's needed, and makes the whole thing fun to listen to. He's interviewed\n\n01:46.080 --> 01:52.560\n Kobe Bryant, Mark Cuban, Neil deGrasse Tyson, Garry Kasparov, and many more. I recently listened\n\n01:52.560 --> 01:58.080\n to his conversation with Frank Abagnale, author of Catch Me If You Can, and one of the world's\n\n01:58.080 --> 02:05.120\n most famous con men. Perfect podcast length and topic for a recent long distance run that I did.\n\n02:05.840 --> 02:12.720\n Again, go to Jordan Harbinger.com slash Lex to give him my love and to support this podcast.\n\n02:13.520 --> 02:17.040\n Subscribe also on Apple podcast, Spotify, and everywhere else.\n\n02:17.040 --> 02:23.280\n This show is presented by Cash App, the greatest sponsor of this podcast ever, and the number one\n\n02:23.280 --> 02:29.200\n finance app in the App Store. When you get it, use code LEX PODCAST. Cash App lets you send money\n\n02:29.200 --> 02:35.040\n to friends, buy Bitcoin, and invest in the stock market with as little as $1. Since Cash App allows\n\n02:35.040 --> 02:39.040\n you to buy Bitcoin, let me mention that cryptocurrency in the context of the history\n\n02:39.040 --> 02:44.080\n of money is fascinating. I recommend Ascent of Money as a great book on this history.\n\n02:44.080 --> 02:50.240\n Also, the audiobook is amazing. Debits and credits on Ledger started around 30,000 years ago.\n\n02:50.240 --> 02:55.760\n The US dollar created over 200 years ago, and the first decentralized cryptocurrency released just\n\n02:55.760 --> 03:00.720\n over 10 years ago. So given that history, cryptocurrency is still very much in its early\n\n03:00.720 --> 03:06.160\n days of development, but it's still aiming to and just might redefine the nature of money.\n\n03:06.880 --> 03:12.640\n So again, if you get Cash App from the App Store or Google Play, and use the code LEX PODCAST,\n\n03:12.640 --> 03:19.440\n you get $10, and Cash App will also donate $10 to FIRST, an organization that is helping to\n\n03:19.440 --> 03:25.360\n advance robotics and STEM education for young people around the world. And now, here's my\n\n03:25.360 --> 03:32.480\n conversation with David Patterson. Let's start with the big historical question. How have computers\n\n03:32.480 --> 03:38.400\n changed in the past 50 years at both the fundamental architectural level and in general, in your eyes?\n\n03:38.400 --> 03:42.960\n David Patterson Well, the biggest thing that happened was the invention of the microprocessor.\n\n03:42.960 --> 03:52.240\n So computers that used to fill up several rooms could fit inside your cell phone. And not only\n\n03:52.240 --> 03:57.600\n did they get smaller, they got a lot faster. So they're a million times faster than they were\n\n03:58.160 --> 04:06.320\n 50 years ago, and they're much cheaper, and they're ubiquitous. There's 7.8 billion people\n\n04:06.320 --> 04:10.960\n on this planet. Probably half of them have cell phones right now, which is remarkable.\n\n04:10.960 --> 04:14.800\n Soterios Johnson That's probably more microprocessors than there are people.\n\n04:14.800 --> 04:16.800\n David Patterson Sure. I don't know what the ratio is,\n\n04:16.800 --> 04:21.520\n but I'm sure it's above one. Maybe it's 10 to 1 or some number like that.\n\n04:21.520 --> 04:23.760\n Soterios Johnson What is a microprocessor?\n\n04:23.760 --> 04:27.520\n David Patterson So a way to say what a microprocessor is,\n\n04:27.520 --> 04:32.240\n is to tell you what's inside a computer. So a computer forever has classically had\n\n04:32.240 --> 04:38.640\n five pieces. There's input and output, which kind of naturally, as you'd expect, is input is like\n\n04:38.640 --> 04:48.880\n speech or typing, and output is displays. There's a memory, and like the name sounds, it remembers\n\n04:48.880 --> 04:54.480\n things. So it's integrated circuits whose job is you put information in, then when you ask for it,\n\n04:54.480 --> 05:00.400\n it comes back out. That's memory. And the third part is the processor, where the microprocessor\n\n05:00.400 --> 05:07.120\n comes from. And that has two pieces as well. And that is the control, which is kind of the brain\n\n05:07.920 --> 05:15.440\n of the processor. And what's called the arithmetic unit, it's kind of the brawn of the computer. So\n\n05:15.440 --> 05:19.680\n if you think of the, as a human body, the arithmetic unit, the thing that does the\n\n05:19.680 --> 05:25.280\n number crunching is the body and the control is the brain. So those five pieces, input, output,\n\n05:25.280 --> 05:34.080\n memory, arithmetic unit, and control are, have been in computers since the very dawn. And the\n\n05:34.080 --> 05:39.440\n last two are considered the processor. So a microprocessor simply means a processor that\n\n05:39.440 --> 05:46.240\n fits on a microchip. And that was invented about, you know, 40 years ago, was the first microprocessor.\n\n05:46.240 --> 05:52.320\n It's interesting that you refer to the arithmetic unit as the, like you connected to the body and\n\n05:52.320 --> 05:57.440\n the controllers of the brain. So I guess, I never thought of it that way. It's a nice way to think\n\n05:57.440 --> 06:05.120\n of it because most of the actions the microprocessor does in terms of literally sort of computation,\n\n06:05.120 --> 06:10.160\n but the microprocessor does computation. It processes information. And most of the thing it\n\n06:10.160 --> 06:16.080\n does is basic arithmetic operations. What are the operations, by the way?\n\n06:16.080 --> 06:22.720\n It's a lot like a calculator. So there are add instructions, subtract instructions,\n\n06:22.720 --> 06:32.240\n multiply and divide. And kind of the brilliance of the invention of the computer or the processor\n\n06:32.960 --> 06:39.120\n is that it performs very trivial operations, but it just performs billions of them per second.\n\n06:39.120 --> 06:44.880\n And what we're capable of doing is writing software that can take these very trivial instructions\n\n06:44.880 --> 06:49.440\n and have them create tasks that can do things better than human beings can do today.\n\n06:49.440 --> 06:55.360\n Just looking back through your career, did you anticipate the kind of how good we would be able\n\n06:55.360 --> 07:03.280\n to get at doing these small, basic operations? How many surprises along the way where you just\n\n07:03.280 --> 07:09.200\n kind of sat back and said, wow, I didn't expect it to go this fast, this good?\n\n07:09.200 --> 07:17.280\n MG Well, the fundamental driving force is what's called Moore's law, which was named after Gordon\n\n07:17.280 --> 07:23.360\n Moore, who's a Berkeley alumnus. And he made this observation very early in what are called\n\n07:23.360 --> 07:29.120\n semiconductors. And semiconductors are these ideas, you can build these very simple switches,\n\n07:29.120 --> 07:34.240\n and you can put them on these microchips. And he made this observation over 50 years ago.\n\n07:34.240 --> 07:38.320\n He looked at a few years and said, I think what's going to happen is the number of these little\n\n07:38.320 --> 07:44.960\n switches called transistors is going to double every year for the next decade. And he said this\n\n07:44.960 --> 07:51.760\n in 1965. And in 1975, he said, well, maybe it's going to double every two years. And that what\n\n07:51.760 --> 07:58.720\n other people since named that Moore's law guided the industry. And when Gordon Moore made that\n\n07:58.720 --> 08:08.720\n prediction, he wrote a paper back in, I think, in the 70s and said, not only did this going to happen,\n\n08:08.720 --> 08:13.120\n he wrote, what would be the implications of that? And in this article from 1965,\n\n08:13.120 --> 08:21.520\n he shows ideas like computers being in cars and computers being in something that you would buy\n\n08:21.520 --> 08:26.800\n in the grocery store and stuff like that. So he kind of not only called his shot, he called the\n\n08:26.800 --> 08:33.280\n implications of it. So if you were in the computing field, and if you believed Moore's prediction,\n\n08:33.280 --> 08:41.360\n he kind of said what would be happening in the future. So it's not kind of, it's at one sense,\n\n08:41.360 --> 08:46.320\n this is what was predicted. And you could imagine it was easy to believe that Moore's law was going\n\n08:46.320 --> 08:51.200\n to continue. And so this would be the implications. On the other side, there are these kind of\n\n08:51.200 --> 08:59.520\n shocking events in your life. Like I remember driving in Marin across the Bay in San Francisco\n\n08:59.520 --> 09:06.560\n and seeing a bulletin board at a local civic center and it had a URL on it. And it was like,\n\n09:07.680 --> 09:13.760\n for the people at the time, these first URLs and that's the, you know, www select stuff with the\n\n09:13.760 --> 09:22.960\n HTTP. People thought it looked like alien writing, right? You'd see these advertisements and\n\n09:22.960 --> 09:26.560\n commercials or bulletin boards that had this alien writing on it. So for the lay people, it's like,\n\n09:26.560 --> 09:30.720\n what the hell is going on here? And for those people in the industry, it was, oh my God,\n\n09:32.000 --> 09:37.200\n this stuff is getting so popular, it's actually leaking out of our nerdy world into the real\n\n09:37.200 --> 09:42.320\n world. So that, I mean, there was events like that. I think another one was, I remember in the\n\n09:42.320 --> 09:46.800\n early days of the personal computer, when we started seeing advertisements in magazines\n\n09:46.800 --> 09:52.480\n for personal computers, like it's so popular that it's made the newspapers. So at one hand,\n\n09:52.480 --> 09:56.720\n you know, Gordon Moore predicted it and you kind of expected it to happen, but when it really hit\n\n09:56.720 --> 10:05.200\n and you saw it affecting society, it was shocking. So maybe taking a step back and looking both\n\n10:05.200 --> 10:11.520\n the engineering and philosophical perspective, what do you see as the layers of abstraction\n\n10:11.520 --> 10:16.080\n in the computer? Do you see a computer as a set of layers of abstractions?\n\n10:16.080 --> 10:19.600\n Dr. Justin Marchegiani Yeah, I think that's one of the things that computer science\n\n10:20.880 --> 10:26.880\n fundamentals is the, these things are really complicated in the way we cope with complicated\n\n10:26.880 --> 10:32.880\n software and complicated hardware is these layers of abstraction. And that simply means that we,\n\n10:33.840 --> 10:39.520\n you know, suspend disbelief and pretend that the only thing you know is that layer,\n\n10:39.520 --> 10:44.240\n and you don't know anything about the layer below it. And that's the way we can make very complicated\n\n10:44.240 --> 10:50.720\n things. And probably it started with hardware that that's the way it was done, but it's been\n\n10:50.720 --> 10:56.400\n proven extremely useful. And, you know, I would say in a modern computer today, there might be\n\n10:56.400 --> 11:02.880\n 10 or 20 layers of abstraction, and they're all trying to kind of enforce this contract is all\n\n11:02.880 --> 11:09.840\n you know is this interface. There's a set of commands that you can, are allowed to use,\n\n11:09.840 --> 11:13.840\n and you stick to those commands, and we will faithfully execute that. And it's like peeling\n\n11:13.840 --> 11:19.200\n the air layers of a London, of an onion, you get down, there's a new set of layers and so forth.\n\n11:19.200 --> 11:26.320\n So for people who want to study computer science, the exciting part about it is you can\n\n11:27.040 --> 11:32.000\n keep peeling those layers. You take your first course, and you might learn to program in Python,\n\n11:32.000 --> 11:37.680\n and then you can take a follow on course, and you can get it down to a lower level language like C,\n\n11:37.680 --> 11:42.000\n and you know, you can go and then you can, if you want to, you can start getting into the hardware\n\n11:42.000 --> 11:47.840\n layers, and you keep getting down all the way to that transistor that I talked about that Gordon\n\n11:47.840 --> 11:53.680\n Moore predicted. And you can understand all those layers all the way up to the highest level\n\n11:53.680 --> 12:02.320\n application software. So it's a very kind of magnetic field. If you're interested, you can go\n\n12:02.320 --> 12:07.600\n into any depth and keep going. In particular, what's happening right now, or it's happened\n\n12:08.240 --> 12:12.480\n in software the last 20 years and recently in hardware, there's getting to be open source\n\n12:12.480 --> 12:18.960\n versions of all of these things. So what open source means is what the engineer, the programmer\n\n12:18.960 --> 12:26.320\n designs, it's not secret, the belonging to a company, it's out there on the worldwide web,\n\n12:26.320 --> 12:33.600\n so you can see it. So you can look at, for lots of pieces of software that you use, you can see\n\n12:33.600 --> 12:39.280\n exactly what the programmer does if you want to get involved. That used to stop at the hardware.\n\n12:39.920 --> 12:46.240\n Recently, there's been an effort to make open source hardware and those interfaces open,\n\n12:46.240 --> 12:50.240\n so you can see that. So instead of before you had to stop at the hardware, you can now start going\n\n12:51.120 --> 12:56.480\n layer by layer below that and see what's inside there. So it's a remarkable time that for the\n\n12:56.480 --> 13:01.920\n interested individual can really see in great depth what's really going on in the computers\n\n13:01.920 --> 13:07.680\n that power everything that we see around us. Are you thinking also when you say open source at\n\n13:07.680 --> 13:14.560\n the hardware level, is this going to the design architecture instruction set level or is it going\n\n13:14.560 --> 13:24.960\n to literally the manufacturer of the actual hardware, of the actual chips, whether that's ASIC\n\n13:24.960 --> 13:30.000\n specialized to a particular domain or the general? Yeah, so let's talk about that a little bit.\n\n13:30.000 --> 13:38.640\n So when you get down to the bottom layer of software, the way software talks to hardware\n\n13:38.640 --> 13:45.120\n is in a vocabulary. And what we call that vocabulary, we call that, the words of that\n\n13:45.120 --> 13:50.720\n vocabulary are called instructions. And the technical term for the vocabulary is instruction\n\n13:50.720 --> 13:55.600\n set. So those instructions are like we talked about earlier, that can be instructions like\n\n13:55.600 --> 14:01.920\n add, subtract and multiply, divide. There's instructions to put data into memory, which\n\n14:01.920 --> 14:05.520\n is called a store instruction and to get data back, which is called the load instructions.\n\n14:05.520 --> 14:12.800\n And those simple instructions go back to the very dawn of computing in 1950, the commercial\n\n14:12.800 --> 14:16.800\n computer had these instructions. So that's the instruction set that we're talking about.\n\n14:17.600 --> 14:23.440\n So up until, I'd say 10 years ago, these instruction sets were all proprietary. So\n\n14:23.440 --> 14:29.840\n a very popular one is owned by Intel, the one that's in the cloud and in all the PCs in the\n\n14:29.840 --> 14:36.320\n world. Intel owns that instruction set. It's referred to as the x86. There've been a sequence\n\n14:36.320 --> 14:41.920\n of ones that the first number was called 8086. And since then, there's been a lot of numbers,\n\n14:41.920 --> 14:47.920\n but they all end in 86. So there's been that kind of family of instruction sets.\n\n14:47.920 --> 14:49.440\n And that's proprietary.\n\n14:49.440 --> 14:55.920\n That's proprietary. The other one that's very popular is from ARM. That kind of powers all\n\n14:55.920 --> 15:02.000\n the cell phones in the world, all the iPads in the world, and a lot of things that are so called\n\n15:02.000 --> 15:09.600\n Internet of Things devices. ARM and that one is also proprietary. ARM will license it to people\n\n15:09.600 --> 15:16.160\n for a fee, but they own that. So the new idea that got started at Berkeley kind of unintentionally\n\n15:16.160 --> 15:25.680\n 10 years ago is early in my career, we pioneered a way to do these vocabularies instruction sets\n\n15:25.680 --> 15:31.600\n that was very controversial at the time. At the time in the 1980s, conventional wisdom was these\n\n15:32.320 --> 15:38.800\n vocabularies instruction sets should have powerful instructions. So polysyllabic kind of words,\n\n15:38.800 --> 15:44.560\n you can think of that. And so instead of just add, subtract, and multiply, they would have\n\n15:44.560 --> 15:51.200\n polynomial, divide, or sort a list. And the hope was of those powerful vocabularies,\n\n15:51.200 --> 15:57.760\n that'd make it easier for software. So we thought that didn't make sense for microprocessors. There\n\n15:57.760 --> 16:03.600\n was people at Berkeley and Stanford and IBM who argued the opposite. And what we called that was\n\n16:03.600 --> 16:10.480\n a reduced instruction set computer. And the abbreviation was RISC. And typical for computer\n\n16:10.480 --> 16:15.920\n people, we use the abbreviation start pronouncing it. So risk was the thing. So we said for\n\n16:15.920 --> 16:21.440\n microprocessors, which with Gordon's Moore is changing really fast, we think it's better to have\n\n16:21.440 --> 16:28.480\n a pretty simple set of instructions, reduced set of instructions. That that would be a better way\n\n16:28.480 --> 16:32.800\n to build microprocessors since they're going to be changing so fast due to Moore's law. And then\n\n16:32.800 --> 16:41.120\n we'll just use standard software to cover the use, generate more of those simple instructions. And\n\n16:41.120 --> 16:45.760\n one of the pieces of software that's in that software stack going between these layers of\n\n16:45.760 --> 16:50.400\n abstractions is called a compiler. And it's basically translates, it's a translator between\n\n16:50.400 --> 16:57.200\n levels. We said the translator will handle that. So the technical question was, well, since there\n\n16:57.200 --> 17:01.920\n are these reduced instructions, you have to execute more of them. Yeah, that's right. But\n\n17:01.920 --> 17:05.840\n maybe you could execute them faster. Yeah, that's right. They're simpler so they could go faster,\n\n17:05.840 --> 17:12.080\n but you have to do more of them. So what's that trade off look like? And it ended up that we ended\n\n17:12.080 --> 17:19.040\n up executing maybe 50% more instructions, maybe a third more instructions, but they ran four times\n\n17:19.040 --> 17:26.000\n faster. So this risk, controversial risk ideas proved to be maybe factors of three or four\n\n17:26.000 --> 17:33.280\n better. I love that this idea was controversial and almost kind of like rebellious. So that's\n\n17:33.280 --> 17:39.280\n in the context of what was more conventional is the complex instructional set computing. So\n\n17:40.080 --> 17:45.840\n how would you pronounce that? CISC. CISC versus risk. Risk versus CISC. And believe it or not,\n\n17:46.400 --> 17:54.720\n this sounds very, who cares about this? It was violently debated at several conferences. It's\n\n17:54.720 --> 18:01.280\n like, what's the right way to go? And people thought risk was a deevolution. We're going to\n\n18:01.280 --> 18:07.200\n make software worse by making those instructions simpler. And there are fierce debates at several\n\n18:07.200 --> 18:14.400\n conferences in the 1980s. And then later in the 80s, it kind of settled to these benefits.\n\n18:14.400 --> 18:18.400\n It's not completely intuitive to me why risk has, for the most part, won.\n\n18:19.600 --> 18:24.240\n Yeah. So why did that happen? Yeah. Yeah. And maybe I can sort of say a bunch of dumb things\n\n18:24.240 --> 18:30.640\n that could lay the land for further commentary. So to me, this is kind of an interesting thing.\n\n18:30.640 --> 18:36.960\n If you look at C++ versus C, with modern compilers, you really could write faster code\n\n18:36.960 --> 18:44.160\n with C++. So relying on the compiler to reduce your complicated code into something simple and\n\n18:44.160 --> 18:53.360\n fast. So to me, comparing risk, maybe this is a dumb question, but why is it that focusing the\n\n18:53.360 --> 19:00.240\n definition of the design of the instruction set on very few simple instructions in the long run\n\n19:00.240 --> 19:09.920\n provide faster execution versus coming up with, like you said, a ton of complicated instructions\n\n19:09.920 --> 19:16.160\n that over time, you know, years, maybe decades, you come up with compilers that can reduce those\n\n19:16.160 --> 19:21.600\n into simple instructions for you. Yeah. So let's try and split that into two pieces.\n\n19:22.640 --> 19:29.120\n So if the compiler can do that for you, if the compiler can take, you know, a complicated program\n\n19:29.120 --> 19:37.120\n and produce simpler instructions, then the programmer doesn't care, right? I don't care just\n\n19:37.120 --> 19:43.520\n how fast is the computer I'm using, how much does it cost? And so what happened kind of in the\n\n19:43.520 --> 19:50.560\n software industry is right around before the 1980s, critical pieces of software were still written\n\n19:50.560 --> 19:57.600\n not in languages like C or C++, they were written in what's called assembly language, where there's\n\n19:57.600 --> 20:04.320\n this kind of humans writing exactly at the instructions at the level that a computer can\n\n20:04.320 --> 20:10.400\n understand. So they were writing add, subtract, multiply, you know, instructions. It's very tedious.\n\n20:11.040 --> 20:17.440\n But the belief was to write this lowest level of software that people use, which are called operating\n\n20:17.440 --> 20:22.000\n systems, they had to be written in assembly language because these high level languages were just too\n\n20:22.000 --> 20:31.600\n inefficient. They were too slow, or the programs would be too big. So that changed with a famous\n\n20:31.600 --> 20:37.040\n operating system called Unix, which is kind of the grandfather of all the operating systems today.\n\n20:37.680 --> 20:43.280\n So Unix demonstrated that you could write something as complicated as an operating system in a\n\n20:43.280 --> 20:51.760\n language like C. So once that was true, then that meant we could hide the instruction set from the\n\n20:51.760 --> 20:58.000\n programmer. And so that meant then it didn't really matter. The programmer didn't have to write\n\n20:58.480 --> 21:02.960\n lots of these simple instructions, that was up to the compiler. So that was part of our arguments\n\n21:02.960 --> 21:07.440\n for risk is, if you were still writing assembly language, there's maybe a better case for CISC\n\n21:07.440 --> 21:13.840\n instructions. But if the compiler can do that, it's going to be, you know, that's done once the\n\n21:13.840 --> 21:19.280\n computer translates at once. And then every time you run the program, it runs at this potentially\n\n21:19.280 --> 21:26.960\n simpler instructions. And so that was the debate, right? And people would acknowledge that the\n\n21:26.960 --> 21:33.040\n simpler instructions could lead to a faster computer. You can think of monosyllabic instructions,\n\n21:33.040 --> 21:36.720\n you could say them, you know, if you think of reading, you can probably read them faster or say\n\n21:36.720 --> 21:41.440\n them faster than long instructions. The same thing, that analogy works pretty well for hardware.\n\n21:42.080 --> 21:46.880\n And as long as you didn't have to read a lot more of those instructions, you could win. So that's\n\n21:46.880 --> 21:53.600\n kind of, that's the basic idea for risk. But it's interesting that in that discussion of Unix and C,\n\n21:54.160 --> 22:02.080\n that there's only one step of levels of abstraction from the code that's really the closest to the\n\n22:02.080 --> 22:09.840\n machine to the code that's written by human. It's, at least to me again, perhaps a dumb intuition,\n\n22:09.840 --> 22:16.000\n but it feels like there might've been more layers, sort of different kinds of humans stacked on top\n\n22:16.000 --> 22:25.680\n of each other. So what's true and not true about what you said is several of the layers of software,\n\n22:27.120 --> 22:32.560\n like, so the, if you, two layers would be, suppose we just talked about two layers,\n\n22:32.560 --> 22:38.240\n that would be the operating system, like you get from Microsoft or from Apple, like iOS,\n\n22:38.240 --> 22:44.560\n or the Windows operating system. And let's say applications that run on top of it, like Word\n\n22:44.560 --> 22:52.400\n or Excel. So both the operating system could be written in C and the application could be written\n\n22:52.400 --> 22:58.880\n in C. But you could construct those two layers and the applications absolutely do call upon the\n\n22:58.880 --> 23:04.720\n operating system. And the change was that both of them could be written in higher level languages.\n\n23:04.720 --> 23:10.160\n So it's one step of a translation, but you can still build many layers of abstraction\n\n23:10.160 --> 23:16.960\n of software on top of that. And that's how things are done today. So still today,\n\n23:17.520 --> 23:23.600\n many of the layers that you'll deal with, you may deal with debuggers, you may deal with linkers,\n\n23:25.600 --> 23:34.080\n there's libraries. Many of those today will be written in C++, say, even though that language is\n\n23:34.080 --> 23:41.120\n pretty ancient. And even the Python interpreter is probably written in C or C++. So lots of\n\n23:41.120 --> 23:47.280\n layers there are probably written in these, some old fashioned efficient languages that\n\n23:48.080 --> 23:56.240\n still take one step to produce these instructions, produce RISC instructions, but they're composed,\n\n23:56.240 --> 24:02.800\n each layer of software invokes one another through these interfaces. And you can get 10 layers of\n\n24:02.800 --> 24:08.720\n software that way. So in general, the RISC was developed here at Berkeley? It was kind of the\n\n24:08.720 --> 24:14.480\n three places that were these radicals that advocated for this against the rest of community\n\n24:14.480 --> 24:24.400\n were IBM, Berkeley, and Stanford. You're one of these radicals. And how radical did you feel?\n\n24:24.400 --> 24:31.680\n How confident did you feel? How doubtful were you that RISC might be the right approach? Because\n\n24:31.680 --> 24:37.440\n it may, you can also intuit that is kind of taking a step back into simplicity, not forward into\n\n24:37.440 --> 24:44.080\n simplicity. Yeah, no, it was easy to make, yeah, it was easy to make the argument against it. Well,\n\n24:44.880 --> 24:50.000\n this was my colleague, John Hennessy at Stanford Nine. We were both assistant professors. And\n\n24:50.640 --> 24:57.040\n for me, I just believed in the power of our ideas. I thought what we were saying made sense.\n\n24:57.040 --> 25:03.200\n Moore's law is going to move fast. The other thing that I didn't mention is one of the surprises of\n\n25:03.200 --> 25:08.240\n these complex instruction sets. You could certainly write these complex instructions\n\n25:08.240 --> 25:13.440\n if the programmer is writing them themselves. It turned out to be kind of difficult for the\n\n25:13.440 --> 25:18.240\n compiler to generate those complex instructions. Kind of ironically, you'd have to find the right\n\n25:18.240 --> 25:22.880\n circumstances that just exactly fit this complex instruction. It was actually easier for the\n\n25:22.880 --> 25:28.720\n compiler to generate these simple instructions. So not only did these complex instructions make\n\n25:28.720 --> 25:34.160\n the hardware more difficult to build, often the compiler wouldn't even use them. And so\n\n25:35.280 --> 25:40.320\n it's harder to build. The compiler doesn't use them that much. The simple instructions go better\n\n25:40.320 --> 25:46.320\n with Moore's law. The number of transistors is doubling every two years. So we're going to have,\n\n25:46.320 --> 25:51.360\n you want to reduce the time to design the microprocessor, that may be more important\n\n25:51.360 --> 25:58.160\n than these number of instructions. So I think we believed that we were right, that this was\n\n25:58.160 --> 26:03.840\n the best idea. Then the question became in these debates, well, yeah, that's a good technical idea,\n\n26:03.840 --> 26:08.000\n but in the business world, this doesn't matter. There's other things that matter. It's like\n\n26:08.720 --> 26:14.640\n arguing that if there's a standard with the railroad tracks and you've come up with a better\n\n26:14.640 --> 26:20.240\n width, but the whole world is covered in railroad tracks, so your ideas have no chance of success.\n\n26:20.240 --> 26:25.440\n Right. Commercial success. It was technically right, but commercially it'll be insignificant.\n\n26:25.440 --> 26:32.640\n Yeah, it's kind of sad that this world, the history of human civilization is full of good ideas that\n\n26:33.280 --> 26:39.040\n lost because somebody else came along first with a worse idea. And it's good that in the\n\n26:39.040 --> 26:43.600\n computing world, at least some of these have, well, you could, I mean, there's probably still\n\n26:43.600 --> 26:50.640\n CISC people that say, yeah, there still are. And what happened was, what was interesting, Intel,\n\n26:50.640 --> 26:57.360\n a bunch of the CISC companies with CISC instruction sets of vocabulary, they gave up,\n\n26:57.360 --> 27:06.320\n but not Intel. What Intel did to its credit, because Intel's vocabulary was in the personal\n\n27:06.320 --> 27:11.760\n computer. And so that was a very valuable vocabulary because the way we distribute software\n\n27:11.760 --> 27:15.840\n is in those actual instructions. It's in the instructions of that instruction set. So\n\n27:17.280 --> 27:22.480\n you don't get that source code, what the programmers wrote. You get, after it's been translated into\n\n27:22.480 --> 27:27.440\n the lowest level, that's if you were to get a floppy disk or download software, it's in the\n\n27:27.440 --> 27:33.680\n instructions of that instruction set. So the x86 instruction set was very valuable. So what Intel\n\n27:33.680 --> 27:40.960\n did cleverly and amazingly is they had their chips in hardware do a translation step.\n\n27:40.960 --> 27:45.280\n They would take these complex instructions and translate them into essentially in RISC instructions\n\n27:45.280 --> 27:52.720\n in hardware on the fly, at gigahertz clock speeds. And then any good idea that RISC people had,\n\n27:52.720 --> 28:01.200\n they could use, and they could still be compatible with this really valuable PC software base,\n\n28:01.200 --> 28:09.040\n which also had very high volumes, 100 million personal computers per year. So the CISC architecture\n\n28:09.040 --> 28:19.280\n in the business world was actually won in this PC era. So just going back to the\n\n28:20.480 --> 28:27.680\n time of designing RISC, when you design an instruction set architecture, do you think\n\n28:27.680 --> 28:33.200\n like a programmer? Do you think like a microprocessor engineer? Do you think like a\n\n28:33.200 --> 28:40.240\n artist, a philosopher? Do you think in software and hardware? I mean, is it art? Is it science?\n\n28:40.240 --> 28:46.720\n Yeah, I'd say, I think designing a good instruction set is an art. And I think you're trying to\n\n28:47.760 --> 28:57.600\n balance the simplicity and speed of execution with how well easy it will be for compilers\n\n28:57.600 --> 29:03.120\n to use it. You're trying to create an instruction set that everything in there can be used by\n\n29:03.120 --> 29:09.280\n compilers. There's not things that are missing that'll make it difficult for the program to run.\n\n29:10.720 --> 29:16.160\n They run efficiently, but you want it to be easy to build as well. So I'd say you're thinking\n\n29:16.160 --> 29:24.320\n hardware, trying to find a hardware software compromise that'll work well. And it's a matter\n\n29:24.320 --> 29:30.880\n of taste. It's kind of fun to build instruction sets. It's not that hard to build an instruction\n\n29:30.880 --> 29:38.000\n set, but to build one that catches on and people use, you have to be fortunate to be\n\n29:38.000 --> 29:43.200\n the right place in the right time or have a design that people really like. Are you using metrics?\n\n29:43.200 --> 29:49.200\n So is it quantifiable? Because you kind of have to anticipate the kind of programs that people\n\n29:49.200 --> 29:56.080\n write ahead of time. So can you use numbers? Can you use metrics? Can you quantify something ahead\n\n29:56.080 --> 30:00.960\n of time? Or is this, again, that's the art part where you're kind of anticipating? No, it's a big\n\n30:00.960 --> 30:07.040\n change. Kind of what happened, I think from Hennessy's and my perspective in the 1980s,\n\n30:07.040 --> 30:17.040\n what happened was going from kind of really, you know, taste and hunches to quantifiable. And in\n\n30:17.040 --> 30:22.880\n fact, he and I wrote a textbook at the end of the 1980s called Computer Architecture, A Quantitative\n\n30:22.880 --> 30:30.320\n Approach. I heard of that. And it's the thing, it had a pretty big impact in the field because we\n\n30:30.320 --> 30:36.320\n went from textbooks that kind of listed, so here's what this computer does, and here's the pros and\n\n30:36.320 --> 30:40.560\n cons, and here's what this computer does and pros and cons to something where there were formulas\n\n30:40.560 --> 30:47.680\n and equations where you could measure things. So specifically for instruction sets, what we do\n\n30:47.680 --> 30:52.960\n and some other fields do is we agree upon a set of programs, which we call benchmarks,\n\n30:53.680 --> 31:00.960\n and a suite of programs, and then you develop both the hardware and the compiler and you get\n\n31:00.960 --> 31:09.360\n numbers on how well your computer does given its instruction set and how well you implemented it in\n\n31:09.360 --> 31:14.960\n your microprocessor and how good your compilers are. In computer architecture, you know, using\n\n31:14.960 --> 31:19.760\n professor's terms, we grade on a curve rather than grade on an absolute scale. So when you say,\n\n31:20.400 --> 31:24.640\n you know, these programs run this fast, well, that's kind of interesting, but how do you know\n\n31:24.640 --> 31:29.840\n it's better? Well, you compare it to other computers at the same time. So the best way we\n\n31:29.840 --> 31:37.680\n know how to turn it into a kind of more science and experimental and quantitative is to compare\n\n31:37.680 --> 31:41.920\n yourself to other computers of the same era that have the same access to the same kind of technology\n\n31:41.920 --> 31:44.940\n on commonly agreed benchmark programs.\n\n31:44.940 --> 31:51.520\n So maybe to toss up two possible directions we can go. One is what are the different tradeoffs\n\n31:51.520 --> 31:56.640\n in designing architectures? We've been already talking about SISC and RISC, but maybe a little\n\n31:56.640 --> 32:03.120\n bit more detail in terms of specific features that you were thinking about. And the other side is\n\n32:03.680 --> 32:08.160\n what are the metrics that you're thinking about when looking at these tradeoffs?\n\n32:08.160 --> 32:14.160\n Yeah, let's talk about the metrics. So during these debates, we actually had kind of a hard\n\n32:14.160 --> 32:20.240\n time explaining, convincing people the ideas, and partly we didn't have a formula to explain it.\n\n32:20.240 --> 32:26.000\n And a few years into it, we hit upon a formula that helped explain what was going on. And\n\n32:27.600 --> 32:34.080\n I think if we can do this, see how it works orally to do this. So if I can do a formula\n\n32:34.080 --> 32:40.720\n orally, let's see. So fundamentally, the way you measure performance is how long does it take a\n\n32:40.720 --> 32:47.360\n program to run? A program, if you have 10 programs, and typically these benchmarks were sweet because\n\n32:47.360 --> 32:51.600\n you'd want to have 10 programs so they could represent lots of different applications. So for\n\n32:51.600 --> 32:56.080\n these 10 programs, how long does it take to run? Well now, when you're trying to explain why it\n\n32:56.080 --> 33:01.120\n took so long, you could factor how long it takes a program to run into three factors.\n\n33:01.120 --> 33:06.240\n One of the first one is how many instructions did it take to execute? So that's the what we've been\n\n33:06.240 --> 33:11.360\n talking about, you know, the instructions of Alchemy. How many did it take? All right. The\n\n33:11.360 --> 33:17.840\n next question is how long did each instruction take to run on average? So you multiply the number\n\n33:17.840 --> 33:23.520\n of instructions times how long it took to run, and that gets you time. Okay, so that's, but now let's\n\n33:23.520 --> 33:28.240\n look at this metric of how long did it take the instruction to run. Well, it turns out,\n\n33:28.240 --> 33:33.280\n the way we could build computers today is they all have a clock, and you've seen this when you,\n\n33:33.280 --> 33:39.760\n if you buy a microprocessor, it'll say 3.1 gigahertz or 2.5 gigahertz, and more gigahertz is\n\n33:39.760 --> 33:47.600\n good. Well, what that is is the speed of the clock. So 2.5 gigahertz turns out to be 4 billionths of\n\n33:47.600 --> 33:54.160\n instruction or 4 nanoseconds. So that's the clock cycle time. But there's another factor, which is\n\n33:54.160 --> 33:59.840\n what's the average number of clock cycles it takes per instruction? So it's number of instructions,\n\n33:59.840 --> 34:05.760\n average number of clock cycles, and the clock cycle time. So in these risk sis debates, they\n\n34:05.760 --> 34:12.800\n would concentrate on, but risk needs to take more instructions, and we'd argue maybe the clock cycle\n\n34:12.800 --> 34:17.680\n is faster, but what the real big difference was was the number of clock cycles per instruction.\n\n34:17.680 --> 34:25.920\n Per instruction, that's fascinating. What about the mess of, the beautiful mess of parallelism in the\n\n34:25.920 --> 34:31.280\n whole picture? Parallelism, which has to do with, say, how many instructions could execute in parallel\n\n34:31.280 --> 34:35.440\n and things like that, you could think of that as affecting the clock cycles per instruction,\n\n34:35.440 --> 34:39.280\n because it's the average clock cycles per instruction. So when you're running a program,\n\n34:39.280 --> 34:45.840\n if it took 100 billion instructions, and on average it took two clock cycles per instruction,\n\n34:45.840 --> 34:49.840\n and they were four nanoseconds, you could multiply that out and see how long it took to run.\n\n34:49.840 --> 34:53.520\n And there's all kinds of tricks to try and reduce the number of clock cycles per instruction.\n\n34:55.440 --> 35:00.400\n But it turned out that the way they would do these complex instructions is they would actually\n\n35:00.400 --> 35:05.840\n build what we would call an interpreter in a simpler, a very simple hardware interpreter.\n\n35:05.840 --> 35:10.720\n But it turned out that for the sis constructions, if you had to use one of those interpreters,\n\n35:10.720 --> 35:16.160\n it would be like 10 clock cycles per instruction, where the risk constructions could be two. So\n\n35:16.160 --> 35:21.280\n there'd be this factor of five advantage in clock cycles per instruction. We have to execute, say,\n\n35:21.280 --> 35:25.440\n 25 or 50 percent more instructions, so that's where the win would come. And then you could\n\n35:25.440 --> 35:30.080\n make an argument whether the clock cycle times are the same or not. But pointing out that we\n\n35:30.080 --> 35:36.000\n could divide the benchmark results time per program into three factors, and the biggest\n\n35:36.000 --> 35:40.640\n difference between RISC and SIS was the clock cycles per, you execute a few more instructions,\n\n35:40.640 --> 35:46.000\n but the clock cycles per instruction is much less. And that was what this debate, once we\n\n35:46.000 --> 35:53.280\n made that argument, then people said, oh, okay, I get it. And so we went from, it was outrageously\n\n35:53.280 --> 35:59.520\n controversial in, you know, 1982 that maybe probably by 1984 or so, people said, oh, yeah,\n\n35:59.520 --> 36:05.680\n technically, they've got a good argument. What are the instructions in the RISC instruction set,\n\n36:05.680 --> 36:13.600\n just to get an intuition? Okay. 1995, I was asked to predict the future of what microprocessor\n\n36:13.600 --> 36:20.240\n could future. So I, and I'd seen these predictions and usually people predict something outrageous\n\n36:20.240 --> 36:25.920\n just to be entertaining, right? And so my prediction for 2020 was, you know, things are\n\n36:25.920 --> 36:30.080\n going to be pretty much, they're going to look very familiar to what they are. And they are,\n\n36:30.080 --> 36:34.400\n and if you were to read the article, you know, the things I said are pretty much true. The\n\n36:34.400 --> 36:38.880\n instructions that have been around forever are kind of the same. And that's the outrageous\n\n36:38.880 --> 36:42.160\n prediction, actually. Yeah. Given how fast computers have been going. Well, and you know,\n\n36:42.160 --> 36:47.840\n Moore's law was going to go on, we thought for 25 more years, you know, who knows, but kind of the\n\n36:47.840 --> 36:55.120\n surprising thing, in fact, you know, Hennessy and I, you know, won the ACM, AM, Turing award for\n\n36:55.120 --> 37:00.160\n both the RISC instruction set contributions and for that textbook I mentioned. But, you know,\n\n37:00.160 --> 37:10.400\n we're surprised that here we are 35, 40 years later after we did our work and the conventionalism\n\n37:10.400 --> 37:15.120\n of the best way to do instruction sets is still those RISC instruction sets that looked very\n\n37:15.120 --> 37:21.200\n similar to what we looked like, you know, we did in the 1980s. So those, surprisingly, there hasn't\n\n37:21.200 --> 37:26.880\n been some radical new idea, even though we have, you know, a million times as many transistors as\n\n37:26.880 --> 37:32.720\n we had back then. But what are the basic constructions and how do they change over the\n\n37:32.720 --> 37:39.200\n years? So we're talking about addition, subtraction, these are the specific. So the things that are in\n\n37:39.200 --> 37:44.640\n a calculator are in a computer. So any of the buttons that are in the calculator in the computer,\n\n37:44.640 --> 37:50.160\n so the, so if there's a memory function key, and like I said, those are, turns into putting\n\n37:50.160 --> 37:54.720\n something in memory is called a store, bring something back to load. Just a quick tangent.\n\n37:54.720 --> 38:00.400\n When you say memory, what does memory mean? Well, I told you there were five pieces of a computer.\n\n38:00.400 --> 38:04.720\n And if you remember in a calculator, there's a memory key. So you want to have intermediate\n\n38:04.720 --> 38:09.680\n calculation and bring it back later. So you'd hit the memory plus key M plus maybe, and it would\n\n38:09.680 --> 38:14.320\n put that into memory and then you'd hit an RM like recurrence section and then bring it back\n\n38:14.320 --> 38:17.360\n on the display. So you don't have to type it. You don't have to write it down and bring it back\n\n38:17.360 --> 38:22.880\n again. So that's exactly what memory is. You can put things into it as temporary storage and bring\n\n38:22.880 --> 38:28.160\n it back when you need it later. So that's memory and loads and stores. But the big thing, the\n\n38:28.160 --> 38:35.440\n difference between a computer and a calculator is that the computer can make decisions. And\n\n38:35.440 --> 38:41.600\n amazingly, decisions are as simple as, is this value less than zero? Or is this value bigger\n\n38:41.600 --> 38:47.440\n than that value? And those instructions, which are called conditional branch instructions,\n\n38:47.440 --> 38:53.200\n is what give computers all its power. If you were in the early days of computing before\n\n38:53.200 --> 38:58.560\n what's called the general purpose microprocessor, people would write these instructions kind of in\n\n38:58.560 --> 39:04.320\n hardware, but it couldn't make decisions. It would do the same thing over and over again.\n\n39:05.520 --> 39:09.680\n With the power of having branch instructions, it can look at things and make decisions\n\n39:09.680 --> 39:15.200\n automatically. And it can make these decisions billions of times per second. And amazingly\n\n39:15.200 --> 39:20.640\n enough, we can get, thanks to advanced machine learning, we can create programs that can do\n\n39:20.640 --> 39:25.040\n something smarter than human beings can do. But if you go down that very basic level, it's the\n\n39:25.600 --> 39:30.960\n instructions are the keys on the calculator, plus the ability to make decisions, these conditional\n\n39:30.960 --> 39:34.960\n branch instructions. And all decisions fundamentally can be reduced down to these\n\n39:34.960 --> 39:42.240\n branch instructions. Yeah. So in fact, and so going way back in the stack back to,\n\n39:42.240 --> 39:47.200\n we did four RISC projects at Berkeley in the 1980s. They did a couple at Stanford\n\n39:47.200 --> 39:54.800\n in the 1980s. In 2010, we decided we wanted to do a new instruction set learning from the mistakes\n\n39:54.800 --> 40:00.640\n of those RISC architectures in the 1980s. And that was done here at Berkeley almost exactly\n\n40:00.640 --> 40:07.200\n 10 years ago. And the people who did it, I participated, but Krzysztof Sanowicz and others\n\n40:07.200 --> 40:13.200\n drove it. They called it RISC 5 to honor those RISC, the four RISC projects of the 1980s.\n\n40:13.840 --> 40:21.200\n So what does RISC 5 involve? So RISC 5 is another instruction set of vocabulary. It's learned from\n\n40:21.200 --> 40:25.680\n the mistakes of the past, but it still has, if you look at the, there's a core set of instructions\n\n40:25.680 --> 40:31.280\n that's very similar to the simplest architectures from the 1980s. And the big difference about RISC\n\n40:31.280 --> 40:41.920\n 5 is it's open. So I talked early about proprietary versus open software. So this is an instruction\n\n40:41.920 --> 40:47.920\n set. So it's a vocabulary, it's not hardware, but by having an open instruction set, we can have\n\n40:47.920 --> 40:54.960\n open source implementations, open source processors that people can use. Where do you see that\n\n40:54.960 --> 41:00.080\n going? It's a really exciting possibility, but you're just like in the scientific American,\n\n41:00.080 --> 41:07.040\n if you were to predict 10, 20, 30 years from now, that kind of ability to utilize open source\n\n41:07.840 --> 41:13.600\n instruction set architectures like RISC 5, what kind of possibilities might that unlock?\n\n41:13.600 --> 41:20.320\n Yeah. And so just to make it clear, because this is confusing, the specification of RISC 5 is\n\n41:20.320 --> 41:26.320\n something that's like in a textbook, there's books about it. So that's defining an interface.\n\n41:27.520 --> 41:33.280\n There's also the way you build hardware is you write it in languages that are kind of like C,\n\n41:33.280 --> 41:38.880\n but they're specialized for hardware that gets translated into hardware. And so these\n\n41:39.440 --> 41:44.960\n implementations of this specification are the open source. So they're written in something\n\n41:44.960 --> 41:52.560\n that's called Verilog or VHDL, but it's put up on the web, just like you can see the C++ code for\n\n41:53.120 --> 42:00.560\n Linux on the web. So that's the open instruction set enables open source implementations of RISC 5.\n\n42:00.560 --> 42:03.360\n So you can literally build a processor using this instruction set.\n\n42:04.080 --> 42:09.360\n People are, people are. So what happened to us that the story was this was developed here for\n\n42:09.360 --> 42:15.440\n our use to do our research. And we made it, we licensed under the Berkeley Software Distribution\n\n42:15.440 --> 42:19.760\n License, like a lot of things get licensed here. So other academics use it, they wouldn't be afraid\n\n42:19.760 --> 42:27.200\n to use it. And then about 2014, we started getting complaints that we were using it in our research\n\n42:27.200 --> 42:32.160\n and in our courses. And we got complaints from people in industries, why did you change your\n\n42:32.160 --> 42:37.840\n instruction set between the fall and the spring semester? And well, we get complaints from\n\n42:37.840 --> 42:42.400\n industrial time. Why the hell do you care what we do with our instruction set? And then when we\n\n42:42.400 --> 42:46.960\n talked to him, we found out there was this thirst for this idea of an open instruction set\n\n42:46.960 --> 42:51.600\n architecture. And they had been looking for one. They stumbled upon ours at Berkeley, thought it\n\n42:51.600 --> 42:58.400\n was, boy, this looks great. We should use this one. And so once we realized there is this need\n\n42:58.400 --> 43:02.960\n for an open instruction set architecture, we thought that's a great idea. And then we started\n\n43:02.960 --> 43:08.720\n supporting it and tried to make it happen. So this was kind of, we accidentally stumbled into this\n\n43:09.600 --> 43:14.800\n and to this need and our timing was good. And so it's really taking off. There's,\n\n43:16.480 --> 43:20.800\n you know, universities are good at starting things, but they're not good at sustaining things. So like\n\n43:20.800 --> 43:26.640\n Linux has a Linux foundation, there's a RISC 5 foundation that we started. There's an annual\n\n43:26.640 --> 43:32.720\n conferences. And the first one was done, I think, January of 2015. And the one that was just last\n\n43:32.720 --> 43:38.080\n December in it, you know, it had 50 people at it. And this one last December had, I don't know,\n\n43:38.880 --> 43:44.880\n 1700 people were at it and the companies excited all over the world. So if predicting into the\n\n43:44.880 --> 43:51.040\n future, you know, if we were doing 25 years, I would predict that RISC 5 will be, you know,\n\n43:51.040 --> 43:57.120\n possibly the most popular instruction set architecture out there, because it's a pretty\n\n43:57.120 --> 44:03.440\n good instruction set architecture and it's open and free. And there's no reason lots of people\n\n44:03.440 --> 44:10.000\n shouldn't use it. And there's benefits just like Linux is so popular today compared to 20 years\n\n44:10.000 --> 44:17.360\n ago. And, you know, the fact that you can get access to it for free, you can modify it, you can\n\n44:17.360 --> 44:22.480\n improve it for all those same arguments. And so people collaborate to make it a better system\n\n44:22.480 --> 44:26.640\n for everybody to use. And that works in software. And I expect the same thing will happen in\n\n44:26.640 --> 44:33.040\n hardware. So if you look at ARM, Intel, MIPS, if you look at just the lay of the land,\n\n44:34.080 --> 44:42.000\n and what do you think, just for me, because I'm not familiar how difficult this kind of transition\n\n44:42.000 --> 44:48.720\n would, how much challenges this kind of transition would entail, do you see,\n\n44:50.400 --> 44:52.240\n let me ask my dumb question in another way.\n\n44:52.240 --> 44:57.280\n No, that's, I know where you're headed. Well, there's a bunch, I think the thing you point out,\n\n44:57.280 --> 45:02.400\n there's these very popular proprietary instruction sets, the x86.\n\n45:02.400 --> 45:09.040\n And so how do we move to RISC 5 potentially in sort of in the span of 5, 10, 20 years,\n\n45:09.040 --> 45:15.360\n a kind of unification, given that the devices, the kind of way we use devices,\n\n45:15.360 --> 45:20.080\n IoT, mobile devices, and the cloud keeps changing?\n\n45:20.080 --> 45:27.920\n Well, part of it, a big piece of it is the software stack. And right now, looking forward,\n\n45:27.920 --> 45:34.720\n there seem to be three important markets. There's the cloud. And the cloud is simply\n\n45:34.720 --> 45:42.720\n companies like Alibaba and Amazon and Google, Microsoft, having these giant data centers with\n\n45:42.720 --> 45:48.800\n tens of thousands of servers in maybe a hundred of these data centers all over the world.\n\n45:48.800 --> 45:54.560\n And that's what the cloud is. So the computer that dominates the cloud is the x86 instruction set.\n\n45:54.560 --> 46:03.040\n So the instruction sets used in the cloud are the x86, almost 100% of that today is x86.\n\n46:03.040 --> 46:08.640\n The other big thing are cell phones and laptops. Those are the big things today.\n\n46:08.640 --> 46:14.480\n I mean, the PC is also dominated by the x86 instruction set, but those sales are dwindling.\n\n46:14.480 --> 46:21.600\n You know, there's maybe 200 million PCs a year, and there's one and a half billion phones a year.\n\n46:21.600 --> 46:26.800\n There's numbers like that. So for the phones, that's dominated by ARM.\n\n46:26.800 --> 46:33.920\n And now, and a reason that I talked about the software stacks, and the third category is\n\n46:33.920 --> 46:38.160\n Internet of Things, which is basically embedded devices, things in your cars and your microwaves\n\n46:38.160 --> 46:45.360\n everywhere. So what's different about those three categories is for the cloud, the software that\n\n46:45.360 --> 46:51.600\n runs in the cloud is determined by these companies, Alibaba, Amazon, Google, Microsoft. So they\n\n46:51.600 --> 46:58.320\n control that software stack. For the cell phones, there's both for Android and Apple, the software\n\n46:58.320 --> 47:03.760\n they supply, but both of them have marketplaces where anybody in the world can build software.\n\n47:03.760 --> 47:11.680\n And that software is translated or, you know, compiled down and shipped in the vocabulary of ARM.\n\n47:11.680 --> 47:18.560\n So that's what's referred to as binary compatible because the actual, it's the instructions are\n\n47:18.560 --> 47:21.760\n turned into numbers, binary numbers, and shipped around the world.\n\n47:21.760 --> 47:29.120\n And sorry, just a quick interruption. So ARM, what is ARM? ARM is an instruction set, like a risk based...\n\n47:29.120 --> 47:35.600\n Yeah, it's a risk based instruction set. It's a proprietary one. ARM stands for Advanced Risk\n\n47:36.400 --> 47:41.040\n Machine. ARM is the name where the company is. So it's a proprietary risk architecture.\n\n47:41.040 --> 47:50.000\n So, and it's been around for a while and it's, you know, the, surely the most popular instruction set\n\n47:50.000 --> 47:56.800\n in the world right now. They, every year, billions of chips are using the ARM design in this post PC\n\n47:56.800 --> 48:03.120\n era. Was it one of the early risk adopters of the risk idea? Yeah. The first ARM goes back,\n\n48:03.120 --> 48:09.840\n I don't know, 86 or so. So Berkeley instead did their work in the early 80s. The ARM guys needed\n\n48:09.840 --> 48:17.360\n an instruction set and they read our papers and it heavily influenced them. So getting back to my\n\n48:17.360 --> 48:21.600\n story, what about Internet of Things? Well, software is not shipped in Internet of Things. It's the\n\n48:22.960 --> 48:29.680\n embedded device people control that software stack. So the opportunities for risk five,\n\n48:29.680 --> 48:34.640\n everybody thinks, is in the Internet of Things embedded things because there's no dominant\n\n48:34.640 --> 48:41.920\n player like there is in the cloud or the smartphones. And, you know, it's, it's,\n\n48:41.920 --> 48:46.720\n doesn't have a lot of licenses associated with, and you can enhance the instruction set if you want.\n\n48:46.720 --> 48:52.800\n And it's, and people have looked at instruction sets and think it's a very good instruction set.\n\n48:52.800 --> 48:59.840\n So it appears to be very popular there. It's possible that in the cloud people,\n\n48:59.840 --> 49:05.920\n those companies control their software stacks. So it's possible that they would decide to use\n\n49:05.920 --> 49:10.880\n risk five if we're talking about 10 and 20 years in the future. The one that would be harder would\n\n49:10.880 --> 49:16.160\n be the cell phones. Since people ship software in the ARM instruction set that you'd think be\n\n49:16.160 --> 49:20.720\n the more difficult one. But if risk five really catches on and, you know, you could,\n\n49:20.720 --> 49:25.920\n in a period of a decade, you can imagine that's changing over too. Do you have a sense why risk\n\n49:25.920 --> 49:31.200\n five or ARM has dominated? You mentioned these three categories. Why has, why did ARM dominate,\n\n49:31.200 --> 49:38.560\n why does it dominate the mobile device space? And maybe my naive intuition is that there are some\n\n49:38.560 --> 49:44.000\n aspects of power efficiency that are important that somehow come along with risk. Well, part of it is\n\n49:44.000 --> 49:55.920\n for these old CIS construction sets, like in the x86, it was more expensive to these for, you know,\n\n49:55.920 --> 50:01.760\n they're older, so they have disadvantages in them because they were designed 40 years ago. But also\n\n50:01.760 --> 50:06.720\n they have to translate in hardware from CIS constructions to risk constructions on the fly.\n\n50:06.720 --> 50:12.240\n And that costs both silicon area that the chips are bigger to be able to do that.\n\n50:12.240 --> 50:17.520\n And it uses more power. So ARM has, which has, you know, followed this risk philosophy is\n\n50:18.240 --> 50:23.040\n seen to be much more energy efficient. And in today's computer world, both in the cloud\n\n50:23.760 --> 50:29.920\n and the cell phone and, you know, things, it isn't, the limiting resource isn't the number of\n\n50:29.920 --> 50:34.560\n transistors you can fit in the chip. It's what, how much power can you dissipate for your\n\n50:34.560 --> 50:42.080\n application? So by having a reduced instruction set, that's possible to have a simpler hardware,\n\n50:42.080 --> 50:46.480\n which is more energy efficient. And energy efficiency is incredibly important in the cloud.\n\n50:46.480 --> 50:51.040\n When you have tens of thousands of computers in a data center, you want to have the most energy\n\n50:51.040 --> 50:54.880\n efficient ones there as well. And of course, for embedded things running off of batteries,\n\n50:54.880 --> 51:00.400\n you want those to be energy efficient and the cell phones too. So I think it's believed that\n\n51:00.400 --> 51:06.400\n there's a energy disadvantage of using these more complex instruction set architectures.\n\n51:08.400 --> 51:14.880\n So the other aspect of this is if we look at Apple, Qualcomm, Samsung, Huawei, all use the\n\n51:14.880 --> 51:20.320\n ARM architecture, and yet the performance of the systems varies. I mean, I don't know\n\n51:20.320 --> 51:26.000\n whose opinion you take on, but you know, Apple for some reason seems to perform better in terms of\n\n51:26.000 --> 51:30.480\n these implementation, these architectures. So where's the magic and show the picture.\n\n51:30.480 --> 51:35.120\n How's that happen? Yeah. So what ARM pioneered was a new business model. As they said, well,\n\n51:35.120 --> 51:39.840\n here's our proprietary instruction set, and we'll give you two ways to do it.\n\n51:41.040 --> 51:46.800\n We'll give you one of these implementations written in things like C called Verilog,\n\n51:46.800 --> 51:51.840\n and you can just use ours. Well, you have to pay money for that. Not only you pay,\n\n51:51.840 --> 51:57.360\n we'll give you their, you know, we'll license you to do that, or you could design your own. And so\n\n51:57.360 --> 52:02.400\n we're talking about numbers like tens of millions of dollars to have the right to design your own,\n\n52:02.400 --> 52:08.960\n since they, it's the instruction set belongs to them. So Apple got one of those, the right to\n\n52:08.960 --> 52:15.440\n build their own. Most of the other people who build like Android phones just get one of the designs\n\n52:15.440 --> 52:23.760\n from ARM to do it themselves. So Apple developed a really good microprocessor design team. They,\n\n52:24.800 --> 52:30.640\n you know, acquired a very good team that had, was building other microprocessors and brought them\n\n52:30.640 --> 52:35.760\n into the company to build their designs. So the instruction sets are the same, the specifications\n\n52:35.760 --> 52:40.800\n are the same, but their hardware design is much more efficient than I think everybody else's.\n\n52:40.800 --> 52:48.480\n And that's given Apple an advantage in the marketplace in that the iPhones tend to be the\n\n52:49.520 --> 52:55.680\n faster than most everybody else's phones that are there. It'd be nice to be able to jump around and\n\n52:55.680 --> 53:01.280\n kind of explore different little sides of this, but let me ask one sort of romanticized question.\n\n53:01.280 --> 53:07.120\n What to you is the most beautiful aspect or idea of RISC instruction set?\n\n53:07.120 --> 53:13.920\n Most beautiful aspect or idea of RISC instruction set or instruction sets or this work that you've\n\n53:13.920 --> 53:20.400\n done? You know, I'm, you know, I was always attracted to the idea of, you know, small is\n\n53:20.400 --> 53:26.640\n beautiful, right? Is that the temptation in engineering, it's kind of easy to make things\n\n53:26.640 --> 53:32.160\n more complicated. It's harder to come up with a, it's more difficult, surprisingly, to come up with\n\n53:32.160 --> 53:39.120\n a simple, elegant solution. And I think that there's a bunch of small features of RISC in general\n\n53:39.120 --> 53:45.520\n that, you know, where you can see this examples of keeping it simpler makes it more elegant.\n\n53:45.520 --> 53:50.160\n Specifically in RISC 5, which, you know, I was kind of the mentor in the program, but it was\n\n53:50.160 --> 53:56.480\n really driven by Krzysztof Sanovi\u0107 and two grad students, Andrew Waterman and Yen Tsip Li, is they\n\n53:56.480 --> 54:05.200\n hit upon this idea of having a subset of instructions, a nice, simple subset instructions,\n\n54:05.200 --> 54:12.880\n like 40ish instructions that all software, the software staff RISC 5 can run just on those 40\n\n54:12.880 --> 54:20.080\n instructions. And then they provide optional features that could accelerate the performance\n\n54:20.080 --> 54:24.160\n instructions that if you needed them could be very helpful, but you don't need to have them.\n\n54:24.160 --> 54:31.840\n And that's a new, really a new idea. So RISC 5 has right now maybe five optional subsets that\n\n54:31.840 --> 54:37.360\n you can pull in, but the software runs without them. If you just want to build the, just the core\n\n54:37.360 --> 54:43.760\n 40 instructions, that's fine. You can do that. So this is fantastic educationally is you can\n\n54:43.760 --> 54:48.960\n explain computers. You only have to explain 40 instructions and not thousands of them. Also,\n\n54:48.960 --> 54:56.000\n if you invent some wild and crazy new technology like, you know, biological computing, you'd like\n\n54:56.000 --> 55:02.000\n a nice, simple instruction set and you can, RISC 5, if you implement those core instructions, you\n\n55:02.000 --> 55:07.360\n can run, you know, really interesting programs on top of that. So this idea of a core set of\n\n55:07.360 --> 55:13.280\n instructions that the software stack runs on and then optional features that if you turn them on,\n\n55:13.280 --> 55:18.720\n the compilers were used, but you don't have to, I think is a powerful idea. What's happened in\n\n55:18.720 --> 55:25.680\n the past for the proprietary instruction sets is when they add new instructions, it becomes\n\n55:25.680 --> 55:33.520\n required piece. And so that all microprocessors in the future have to use those instructions. So\n\n55:33.520 --> 55:39.840\n it's kind of like, for a lot of people as they get older, they gain weight, right? That weight and\n\n55:39.840 --> 55:44.320\n age are correlated. And so you can see these instruction sets getting bigger and bigger as\n\n55:44.320 --> 55:50.640\n they get older. So RISC 5, you know, lets you be as slim as you as a teenager. And you only have to\n\n55:50.640 --> 55:55.760\n add these extra features if you're really going to use them rather than you have no choice. You have\n\n55:55.760 --> 56:00.320\n to keep growing with the instruction set. I don't know if the analogy holds up, but that's a beautiful\n\n56:00.320 --> 56:06.560\n notion that there's, it's almost like a nudge towards here's the simple core. That's the\n\n56:06.560 --> 56:12.000\n essential. Yeah. And I think the surprising thing is still if we brought back, you know,\n\n56:12.000 --> 56:16.480\n the pioneers from the 1950s and showed them the instruction set architectures, they'd understand\n\n56:16.480 --> 56:21.920\n it. They'd say, wow, that doesn't look that different. Well, you know, I'm surprised. And\n\n56:21.920 --> 56:26.400\n it's, there's, it may be something, you know, to talk about philosophical things. I mean, there may\n\n56:26.400 --> 56:35.840\n be something powerful about those, you know, 40 or 50 instructions that all you need is these\n\n56:35.840 --> 56:42.160\n commands like these instructions that we talked about. And that is sufficient to build, to bring\n\n56:42.160 --> 56:50.000\n up on, you know, artificial intelligence. And so it's a remarkable, surprising to me that as\n\n56:50.640 --> 56:58.800\n complicated as it is to build these things, you know, microprocessors where the line widths are\n\n56:58.800 --> 57:05.200\n are narrower than the wavelength of light, you know, is this amazing technologies at some\n\n57:05.200 --> 57:10.160\n fundamental level. The commands that software executes are really pretty straightforward and\n\n57:10.160 --> 57:18.080\n haven't changed that much in decades. What a surprising outcome. So underlying all computation,\n\n57:18.080 --> 57:23.760\n all Turing machines, all artificial intelligence systems, perhaps might be a very simple instruction\n\n57:23.760 --> 57:30.800\n set like a RISC5 or it's. Yeah. I mean, that's kind of what I said. I was interested to see,\n\n57:30.800 --> 57:36.480\n I had another more senior faculty colleague and he had written something in Scientific American\n\n57:36.480 --> 57:43.040\n and, you know, his 25 years in the future and his turned out about when I was a young professor and\n\n57:43.040 --> 57:48.400\n he said, yep, I checked it. And so I was interested to see how that was going to turn out for me. And\n\n57:48.400 --> 57:54.080\n it's pretty held up pretty well, but yeah, so there's, there's probably, there's some, you know,\n\n57:54.080 --> 58:00.000\n there's, there must be something fundamental about those instructions that we're capable of\n\n58:01.040 --> 58:07.920\n creating, you know, intelligence from pretty primitive operations and just doing them really\n\n58:07.920 --> 58:14.560\n fast. You kind of mentioned a different, maybe radical computational medium like biological,\n\n58:14.560 --> 58:20.320\n and there's other ideas. So there's a lot of spaces in ASIC, domain specific, and then there\n\n58:20.320 --> 58:25.440\n could be quantum computers. And so we can think of all of those different mediums and types of\n\n58:25.440 --> 58:33.120\n computation. What's the connection between swapping out different hardware systems and the\n\n58:33.120 --> 58:37.920\n instruction set? Do you see those as disjoint or are they fundamentally coupled? Yeah. So what's,\n\n58:37.920 --> 58:45.440\n so kind of, if we go back to the history, you know, when Moore's Law is in full effect and\n\n58:45.440 --> 58:51.760\n you're getting twice as many transistors every couple of years, you know, kind of the challenge\n\n58:51.760 --> 58:56.800\n for computer designers is how can we take advantage of that? How can we turn those transistors into\n\n58:56.800 --> 59:04.400\n better computers faster typically? And so there was an era, I guess in the 80s and 90s where\n\n59:04.400 --> 59:10.960\n computers were doubling performance every 18 months. And if you weren't around then,\n\n59:11.520 --> 59:18.480\n what would happen is you had your computer and your friend's computer, which was like a year,\n\n59:18.480 --> 59:23.760\n a year and a half newer, and it was much faster than your computer. And he or she could get their\n\n59:23.760 --> 59:27.680\n work done much faster than your computer because it was newer. So people took their computers,\n\n59:27.680 --> 59:33.920\n perfectly good computers, and threw them away to buy a newer computer because the computer\n\n59:33.920 --> 59:39.040\n one or two years later was so much faster. So that's what the world was like in the 80s and\n\n59:39.040 --> 59:46.560\n 90s. Well, with the slowing down of Moore's Law, that's no longer true, right? Now with, you know,\n\n59:46.560 --> 59:51.440\n not desk side computers with the laptops, I only get a new desk laptop when it breaks,\n\n59:51.440 --> 59:56.480\n right? Oh damn, the disk broke or this display broke, I gotta buy a new computer. But before\n\n59:56.480 --> 1:00:01.520\n you would throw them away because it just, they were just so sluggish compared to the latest\n\n1:00:01.520 --> 1:00:11.840\n computers. So that's, you know, that's a huge change of what's gone on. So, but since this\n\n1:00:11.840 --> 1:00:18.720\n lasted for decades, kind of programmers and maybe all of society is used to computers getting faster\n\n1:00:18.720 --> 1:00:24.640\n regularly. We now believe, those of us who are in computer design, it's called computer\n\n1:00:24.640 --> 1:00:33.680\n architecture, that the path forward is instead is to add accelerators that only work well for\n\n1:00:33.680 --> 1:00:41.600\n certain applications. So since Moore's Law is slowing down, we don't think general purpose\n\n1:00:41.600 --> 1:00:46.560\n computers are going to get a lot faster. So the Intel processors of the world are not going to,\n\n1:00:46.560 --> 1:00:51.680\n haven't been getting a lot faster. They've been barely improving, like a few percent a year.\n\n1:00:51.680 --> 1:00:56.640\n It used to be doubling every 18 months and now it's doubling every 20 years. So it was just\n\n1:00:56.640 --> 1:01:01.840\n shocking. So to be able to deliver on what Moore's Law used to do, we think what's going to happen,\n\n1:01:02.480 --> 1:01:09.280\n what is happening right now is people adding accelerators to their microprocessors that only\n\n1:01:09.280 --> 1:01:17.120\n work well for some domains. And by sheer coincidence, at the same time that this is happening,\n\n1:01:17.120 --> 1:01:23.840\n has been this revolution in artificial intelligence called machine learning. So with,\n\n1:01:23.840 --> 1:01:31.040\n as I'm sure your other guests have said, you know, AI had these two competing schools of thought is\n\n1:01:31.040 --> 1:01:36.480\n that we could figure out artificial intelligence by just writing the rules top down, or that was\n\n1:01:36.480 --> 1:01:41.600\n wrong. You had to look at data and infer what the rules are, the machine learning, and what's\n\n1:01:41.600 --> 1:01:48.560\n happened in the last decade or eight years as machine learning has won. And it turns out that\n\n1:01:48.560 --> 1:01:55.440\n machine learning, the hardware you build for machine learning is pretty much multiply. The\n\n1:01:55.440 --> 1:02:03.040\n matrix multiply is a key feature for the way machine learning is done. So that's a godsend\n\n1:02:03.040 --> 1:02:08.640\n for computer designers. We know how to make matrix multiply run really fast. So general purpose\n\n1:02:08.640 --> 1:02:13.360\n microprocessors are slowing down. We're adding accelerators for machine learning that fundamentally\n\n1:02:13.360 --> 1:02:17.360\n are doing matrix multiplies much more efficiently than general purpose computers have done.\n\n1:02:17.920 --> 1:02:23.120\n So we have to come up with a new way to accelerate things. The danger of only accelerating one\n\n1:02:23.120 --> 1:02:28.640\n application is how important is that application. Turns out machine learning gets used for all\n\n1:02:28.640 --> 1:02:36.320\n kinds of things. So serendipitously, we found something to accelerate that's widely applicable.\n\n1:02:36.320 --> 1:02:40.400\n And we don't even, we're in the middle of this revolution of machine learning. We're not sure\n\n1:02:40.400 --> 1:02:46.000\n what the limits of machine learning are. So this has been a kind of a godsend. If you're going to\n\n1:02:46.000 --> 1:02:53.680\n be able to excel, deliver on improved performance, as long as people are moving their programs to be\n\n1:02:54.080 --> 1:02:58.880\n embracing more machine learning, we know how to give them more performance even as Moore's law\n\n1:02:58.880 --> 1:03:06.800\n is slowing down. And counterintuitively, the machine learning mechanism you can say is domain\n\n1:03:06.800 --> 1:03:13.200\n specific, but because it's leveraging data, it's actually could be very broad in terms of\n\n1:03:15.360 --> 1:03:21.040\n in terms of the domains it could be applied in. Yeah, that's exactly right. Sort of, it's almost\n\n1:03:21.040 --> 1:03:27.520\n sort of people sometimes talk about the idea of software 2.0. We're almost taking another step\n\n1:03:27.520 --> 1:03:34.080\n up in the abstraction layer in designing machine learning systems, because now you're programming\n\n1:03:34.080 --> 1:03:38.480\n in the space of data, in the space of hyperparameters, it's changing fundamentally\n\n1:03:38.480 --> 1:03:45.120\n the nature of programming. And so the specialized devices that accelerate the performance, especially\n\n1:03:45.120 --> 1:03:52.400\n neural network based machine learning systems might become the new general. Yeah. So the thing\n\n1:03:52.400 --> 1:03:59.680\n that's interesting point out these are not coral, these are not tied together. The enthusiasm about\n\n1:03:59.680 --> 1:04:05.040\n machine learning about creating programs driven from data that we should figure out the answers\n\n1:04:05.040 --> 1:04:10.160\n from data rather than kind of top down, which classically the way most programming is done\n\n1:04:10.160 --> 1:04:14.640\n and the way artificial intelligence used to be done. That's a movement that's going on at the\n\n1:04:14.640 --> 1:04:21.440\n same time. Coincidentally, and the first word machine learning is machines, right? So that's\n\n1:04:21.440 --> 1:04:27.360\n going to increase the demand for computing, because instead of programmers being smart, writing those\n\n1:04:27.360 --> 1:04:31.760\n those things down, we're going to instead use computers to examine a lot of data to kind of\n\n1:04:31.760 --> 1:04:38.560\n create the programs. That's the idea. And remarkably, this gets used for all kinds of\n\n1:04:38.560 --> 1:04:43.200\n things very successfully. The image recognition, the language translation, the game playing,\n\n1:04:43.200 --> 1:04:50.320\n and you know, it gets into pieces of the software stack like databases and stuff like that. We're\n\n1:04:50.320 --> 1:04:54.880\n not quite sure how general purpose is, but that's going on independent of this hardware stuff.\n\n1:04:54.880 --> 1:04:59.040\n What's happening on the hardware side is Moore's law is slowing down right when we need a lot more\n\n1:04:59.040 --> 1:05:03.840\n cycles. It's failing us, it's failing us right when we need it because there's going to be a\n\n1:05:03.840 --> 1:05:09.680\n greater increase in computing. And then this idea that we're going to do so called domain\n\n1:05:09.680 --> 1:05:16.800\n specific. Here's a domain that your greatest fear is you'll make this one thing work and that'll\n\n1:05:16.800 --> 1:05:22.160\n help, you know, five percent of the people in the world. Well, this looks like it's a very\n\n1:05:22.160 --> 1:05:28.640\n general purpose thing. So the timing is fortuitous that if we can perhaps, if we can keep building\n\n1:05:28.640 --> 1:05:36.080\n hardware that will accelerate machine learning, the neural networks, that'll beat the timing will\n\n1:05:36.080 --> 1:05:42.160\n be right. That neural network revolution will transform your software, the so called software\n\n1:05:42.160 --> 1:05:47.680\n 2.0. And the software of the future will be very different from the software of the past. And just\n\n1:05:47.680 --> 1:05:52.480\n as our microprocessors, even though we're still going to have that same basic RISC instructions\n\n1:05:53.200 --> 1:05:57.360\n to run a big pieces of the software stack like user interfaces and stuff like that,\n\n1:05:58.080 --> 1:06:02.880\n we can accelerate the kind of the small piece that's computationally intensive. It's not lots\n\n1:06:02.880 --> 1:06:08.160\n of lines of code, but it takes a lot of cycles to run that code that that's going to be the\n\n1:06:08.160 --> 1:06:14.800\n accelerator piece. And so that's what makes this from a computer designers perspective a really\n\n1:06:14.800 --> 1:06:20.320\n interesting decade. What Hennessy and I talked about in the title of our Turing Warrant speech\n\n1:06:20.320 --> 1:06:28.160\n is a new golden age. We see this as a very exciting decade, much like when we were assistant\n\n1:06:28.160 --> 1:06:32.720\n professors and the RISC stuff was going on. That was a very exciting time was where we were changing\n\n1:06:32.720 --> 1:06:39.280\n what was going on. We see this happening again. Tremendous opportunities of people because we're\n\n1:06:39.280 --> 1:06:43.760\n fundamentally changing how software is built and how we're running it. So which layer of the\n\n1:06:43.760 --> 1:06:49.360\n abstraction do you think most of the acceleration might be happening? If you look in the next 10\n\n1:06:49.360 --> 1:06:54.880\n years, Google is working on a lot of exciting stuff with the TPU. Sort of there's a closer to\n\n1:06:54.880 --> 1:07:00.640\n the hardware that could be optimizations around the closer to the instruction set.\n\n1:07:00.640 --> 1:07:05.040\n There could be optimization at the compiler level. It could be even at the higher level software\n\n1:07:05.040 --> 1:07:10.480\n stack. Yeah, it's got to be, I mean, if you think about the old RISC Sys debate, it was both,\n\n1:07:11.840 --> 1:07:18.080\n it was software hardware. It was the compilers improving as well as the architecture improving.\n\n1:07:18.080 --> 1:07:24.240\n And that's likely to be the way things are now. With machine learning, they're using\n\n1:07:24.240 --> 1:07:30.960\n domain specific languages. The languages like TensorFlow and PyTorch are very popular with\n\n1:07:30.960 --> 1:07:35.280\n the machine learning people. Those are the raising the level of abstraction. It's easier\n\n1:07:35.280 --> 1:07:41.920\n for people to write machine learning in these domain specific languages like PyTorch and\n\n1:07:41.920 --> 1:07:47.760\n TensorFlow. So where the most optimization might be happening. Yeah. And so there'll be both the\n\n1:07:47.760 --> 1:07:53.680\n compiler piece and the hardware piece underneath it. So as you kind of the fatal flaw for hardware\n\n1:07:53.680 --> 1:07:59.360\n people is to create really great hardware, but not have brought along the compilers. And what we're\n\n1:07:59.360 --> 1:08:04.560\n seeing right now in the marketplace because of this enthusiasm around hardware for machine\n\n1:08:04.560 --> 1:08:10.400\n learning is getting, you know, probably billions of dollars invested in startup companies. We're\n\n1:08:10.400 --> 1:08:15.920\n seeing startup companies go belly up because they focus on the hardware, but didn't bring the\n\n1:08:15.920 --> 1:08:23.440\n software stack along. We talked about benchmarks earlier. So I participated in machine learning\n\n1:08:23.440 --> 1:08:27.520\n didn't really have a set of benchmarks. I think just two years ago, they didn't have a set of\n\n1:08:27.520 --> 1:08:33.280\n benchmarks. And we've created something called ML Perf, which is machine learning benchmark suite.\n\n1:08:33.280 --> 1:08:39.840\n And pretty much the companies who didn't invest in the software stack couldn't run ML Perf very\n\n1:08:39.840 --> 1:08:45.040\n well. And the ones who did invest in software stack did. And we're seeing, you know, like kind\n\n1:08:45.040 --> 1:08:48.800\n of in computer architecture, this is what happens. You have these arguments about risk versus this.\n\n1:08:48.800 --> 1:08:54.720\n People spend billions of dollars in the marketplace to see who wins. It's not a perfect comparison,\n\n1:08:54.720 --> 1:08:59.680\n but it kind of sorts things out. And we're seeing companies go out of business and then companies\n\n1:08:59.680 --> 1:09:07.200\n like there's a company in Israel called Habana. They came up with machine learning accelerators.\n\n1:09:08.160 --> 1:09:14.560\n They had good ML Perf scores. Intel had acquired a company earlier called Nirvana a couple of years\n\n1:09:14.560 --> 1:09:21.120\n ago. They didn't reveal their ML Perf scores, which was suspicious. But a month ago, Intel\n\n1:09:21.120 --> 1:09:25.760\n announced that they're canceling the Nirvana product line and they've bought Habana for $2\n\n1:09:25.760 --> 1:09:32.560\n billion. And Intel's going to be shipping Habana chips, which have hardware and software and run\n\n1:09:32.560 --> 1:09:36.160\n the ML Perf programs pretty well. And that's going to be their product line in the future.\n\n1:09:36.800 --> 1:09:42.560\n Brilliant. So maybe just to linger briefly on ML Perf. I love metrics. I love standards that\n\n1:09:42.560 --> 1:09:48.800\n everyone can gather around. What are some interesting aspects of that portfolio of metrics?\n\n1:09:48.800 --> 1:09:55.680\n Well, one of the interesting metrics is what we thought. I was involved in the start.\n\n1:09:57.440 --> 1:10:00.880\n Peter Mattson is leading the effort from Google. Google got it off the ground,\n\n1:10:00.880 --> 1:10:07.360\n but we had to reach out to competitors and say, there's no benchmarks here. We think this is\n\n1:10:07.360 --> 1:10:11.120\n bad for the field. It'll be much better if we look at examples like in the risk days,\n\n1:10:11.120 --> 1:10:16.400\n there was an effort to create a... For the people in the risk community got together,\n\n1:10:16.400 --> 1:10:19.520\n competitors got together building risk microprocessors to agree on a set of\n\n1:10:19.520 --> 1:10:24.720\n benchmarks that were called spec. And that was good for the industry. It's rather before\n\n1:10:24.720 --> 1:10:28.160\n the different risk architectures were arguing, well, you can believe my performance others,\n\n1:10:28.160 --> 1:10:34.400\n but those other guys are liars. And that didn't do any good. So we agreed on a set of benchmarks\n\n1:10:34.400 --> 1:10:37.920\n and then we could figure out who was faster between the various risk architectures. But\n\n1:10:37.920 --> 1:10:42.800\n it was a little bit faster, but that grew the market rather than people were afraid to buy\n\n1:10:42.800 --> 1:10:48.880\n anything. So we argued the same thing would happen with MLPerf. Companies like Nvidia were maybe\n\n1:10:48.880 --> 1:10:53.120\n worried that it was some kind of trap, but eventually we all got together to create a\n\n1:10:53.120 --> 1:10:59.760\n set of benchmarks and do the right thing. And we agree on the results. And so we can see whether\n\n1:11:00.320 --> 1:11:06.560\n TPUs or GPUs or CPUs are really faster and how much the faster. And I think from an engineer's\n\n1:11:06.560 --> 1:11:12.800\n perspective, as long as the results are fair, you can live with it. Okay, you kind of tip your hat\n\n1:11:12.800 --> 1:11:18.240\n to your colleagues at another institution, boy, they did a better job than us. What you hate is\n\n1:11:18.240 --> 1:11:23.600\n if it's false, right? They're making claims and it's just marketing bullshit and that's affecting\n\n1:11:23.600 --> 1:11:28.640\n sales. So from an engineer's perspective, as long as it's a fair comparison and we don't come in\n\n1:11:28.640 --> 1:11:33.600\n first place, that's too bad, but it's fair. So we wanted to create that environment for MLPerf.\n\n1:11:33.600 --> 1:11:40.880\n And so now there's 10 companies, I mean, 10 universities and 50 companies involved. So pretty\n\n1:11:40.880 --> 1:11:50.640\n much MLPerf is the way you measure machine learning performance. And it didn't exist even\n\n1:11:50.640 --> 1:11:56.720\n two years ago. One of the cool things that I enjoy about the internet has a few downsides, but one of\n\n1:11:56.720 --> 1:12:02.080\n the nice things is people can see through BS a little better with the presence of these kinds\n\n1:12:02.080 --> 1:12:08.080\n of metrics. So it's really nice companies like Google and Facebook and Twitter. Now, it's the\n\n1:12:08.080 --> 1:12:13.280\n cool thing to do is to put your engineers forward and to actually show off how well you do on these\n\n1:12:13.280 --> 1:12:22.560\n metrics. There's less of a desire to do marketing, less so. In my sort of naive viewpoint.\n\n1:12:22.560 --> 1:12:29.280\n I was trying to understand what's changed from the 80s in this era. I think because of things\n\n1:12:29.280 --> 1:12:36.560\n like social networking, Twitter and stuff like that, if you put up bullshit stuff that's just\n\n1:12:38.080 --> 1:12:45.920\n purposely misleading, you can get a violent reaction in social media pointing out the flaws\n\n1:12:45.920 --> 1:12:51.840\n in your arguments. And so from a marketing perspective, you have to be careful today that\n\n1:12:51.840 --> 1:12:57.120\n you didn't have to be careful that there'll be people who put out the flaw. You can get the\n\n1:12:57.120 --> 1:13:02.960\n word out about the flaws in what you're saying much more easily today than in the past. It used\n\n1:13:02.960 --> 1:13:08.000\n to be easier to get away with it. And the other thing that's been happening in terms of showing\n\n1:13:08.000 --> 1:13:14.880\n off engineers is just in the software side, people have largely embraced open source software.\n\n1:13:16.800 --> 1:13:22.000\n 20 years ago, it was a dirty word at Microsoft. And today Microsoft is one of the big proponents\n\n1:13:22.000 --> 1:13:28.160\n of open source software. That's the standard way most software gets built, which really shows off\n\n1:13:28.160 --> 1:13:34.320\n your engineers because you can see if you look at the source code, you can see who are making the\n\n1:13:34.320 --> 1:13:38.960\n commits, who's making the improvements, who are the engineers at all these companies who are\n\n1:13:41.440 --> 1:13:47.120\n really great programmers and engineers and making really solid contributions,\n\n1:13:47.120 --> 1:13:50.080\n which enhances their reputations and the reputation of the companies.\n\n1:13:50.080 --> 1:13:56.320\n LR But that's, of course, not everywhere. Like in the space that I work more in is autonomous\n\n1:13:56.320 --> 1:14:02.080\n vehicles. And there's still the machinery of hype and marketing is still very strong there. And\n\n1:14:02.080 --> 1:14:06.960\n there's less willingness to be open in this kind of open source way and sort of benchmark. So\n\n1:14:07.600 --> 1:14:12.480\n MLPerf represents the machine learning world is much better being open source about holding\n\n1:14:12.480 --> 1:14:18.240\n itself to standards of different, the amount of incredible benchmarks in terms of the different\n\n1:14:18.240 --> 1:14:23.120\n computer vision, natural language processing tasks is incredible.\n\n1:14:23.120 --> 1:14:26.240\n LR Historically, it wasn't always that way.\n\n1:14:26.800 --> 1:14:31.680\n I had a graduate student working with me, David Martin. So in computer, in some fields,\n\n1:14:32.480 --> 1:14:40.560\n benchmarking has been around forever. So computer architecture, databases, maybe operating systems,\n\n1:14:40.560 --> 1:14:47.440\n benchmarks are the way you measure progress. But he was working with me and then started working\n\n1:14:47.440 --> 1:14:53.040\n with Jitendra Malik. And Jitendra Malik in computer vision space, I guess you've interviewed\n\n1:14:53.040 --> 1:14:59.360\n Jitendra. And David Martin told me, they don't have benchmarks. Everybody has their own vision\n\n1:14:59.360 --> 1:15:04.960\n algorithm and the way, here's my image, look at how well I do. And everybody had their own image.\n\n1:15:04.960 --> 1:15:10.640\n So David Martin, back when he did his dissertation, figured out a way to do benchmarks. He had a bunch\n\n1:15:10.640 --> 1:15:17.200\n of graduate students identify images and then ran benchmarks to see which algorithms run well. And\n\n1:15:17.200 --> 1:15:24.480\n that was, as far as I know, kind of the first time people did benchmarks in computer vision, which\n\n1:15:24.480 --> 1:15:29.600\n was predated all the things that eventually led to ImageNet and stuff like that. But then the vision\n\n1:15:29.600 --> 1:15:37.520\n community got religion. And then once we got as far as ImageNet, then that let the guys in Toronto\n\n1:15:38.720 --> 1:15:42.560\n be able to win the ImageNet competition. And then that changed the whole world.\n\n1:15:42.560 --> 1:15:47.840\n It's a scary step actually, because when you enter the world of benchmarks, you actually have to be\n\n1:15:47.840 --> 1:15:54.160\n good to participate as opposed to... Yeah, you can just, you just believe you're the best in the\n\n1:15:54.160 --> 1:16:01.280\n world. I think the people, I think they weren't purposely misleading. I think if you don't have\n\n1:16:01.280 --> 1:16:06.320\n benchmarks, I mean, how do you know? Your intuition is kind of like the way we did just\n\n1:16:06.320 --> 1:16:11.040\n do computer architecture. Your intuition is that this is the right instruction set to do this job.\n\n1:16:11.040 --> 1:16:18.160\n I believe in my experience, my hunch is that's true. We had to get to make things more quantitative\n\n1:16:18.160 --> 1:16:23.840\n to make progress. And so I just don't know how, you know, in fields that don't have benchmarks,\n\n1:16:23.840 --> 1:16:26.880\n I don't understand how they figure out how they're making progress.\n\n1:16:28.400 --> 1:16:34.480\n We're kind of in the vacuum tube days of quantum computing. What are your thoughts in this wholly\n\n1:16:34.480 --> 1:16:40.400\n different kind of space of architectures? You know, I actually, you know, quantum computing\n\n1:16:41.120 --> 1:16:46.240\n is, idea has been around for a while and I actually thought, well, I sure hope I retire\n\n1:16:46.240 --> 1:16:53.520\n before I have to start teaching this. I'd say because I talk about, give these talks about the\n\n1:16:53.520 --> 1:17:01.040\n slowing of Moore's law and, you know, when we need to change by doing domain specific accelerators,\n\n1:17:01.040 --> 1:17:04.480\n common questions say, what about quantum computing? The reason that comes up,\n\n1:17:04.480 --> 1:17:08.880\n it's in the news all the time. So I think to keep in, the third thing to keep in mind is\n\n1:17:08.880 --> 1:17:14.080\n quantum computing is not right around the corner. There've been two national reports,\n\n1:17:14.080 --> 1:17:18.800\n one by the National Academy of Engineering and other by the Computing Consortium, where they\n\n1:17:18.800 --> 1:17:25.440\n did a frank assessment of quantum computing. And both of those reports said, you know,\n\n1:17:25.440 --> 1:17:31.200\n as far as we can tell, before you get error corrected quantum computing, it's a decade away.\n\n1:17:31.200 --> 1:17:35.680\n So I think of it like nuclear fusion, right? There've been people who've been excited about\n\n1:17:35.680 --> 1:17:39.760\n nuclear fusion a long time. If we ever get nuclear fusion, it's going to be fantastic\n\n1:17:39.760 --> 1:17:43.840\n for the world. I'm glad people are working on it, but, you know, it's not right around the corner.\n\n1:17:45.120 --> 1:17:52.640\n Those two reports to me say probably it'll be 2030 before quantum computing is something\n\n1:17:52.640 --> 1:17:58.000\n that could happen. And when it does happen, you know, this is going to be big science stuff. This\n\n1:17:58.000 --> 1:18:04.880\n is, you know, micro Kelvin, almost absolute zero things that if they vibrate, if truck goes by,\n\n1:18:04.880 --> 1:18:09.200\n it won't work, right? So this will be in data center stuff. We're not going to have a quantum\n\n1:18:09.200 --> 1:18:16.080\n cell phone. And it's probably a 2030 kind of thing. So I'm happy that our people are working on it,\n\n1:18:16.080 --> 1:18:21.040\n but just, you know, it's hard with all the news about it, not to think that it's right around the\n\n1:18:21.040 --> 1:18:27.040\n corner. And that's why we need to do something as Moore's Law is slowing down to provide the\n\n1:18:27.040 --> 1:18:32.560\n computing, keep computing getting better for this next decade. And, you know, we shouldn't\n\n1:18:32.560 --> 1:18:39.680\n be betting on quantum computing or expecting quantum computing to deliver in the next few\n\n1:18:39.680 --> 1:18:44.480\n years. It's probably further off. You know, I'd be happy to be wrong. It'd be great if quantum\n\n1:18:44.480 --> 1:18:49.600\n computing is going to commercially viable, but it will be a set of applications. It's not a general\n\n1:18:49.600 --> 1:18:54.640\n purpose computation. So it's going to do some amazing things, but there'll be a lot of things\n\n1:18:54.640 --> 1:18:59.360\n that probably, you know, the old fashioned computers are going to keep doing better for\n\n1:18:59.360 --> 1:19:04.240\n quite a while. And there'll be a teenager 50 years from now watching this video saying,\n\n1:19:04.240 --> 1:19:09.920\n look how silly David Patterson was saying. No, I just said, I said 2030. I didn't say,\n\n1:19:09.920 --> 1:19:14.560\n I didn't say never. We're not going to have quantum cell phones. So he's going to be watching it.\n\n1:19:14.560 --> 1:19:21.920\n Well, I mean, I think this is such a, you know, given that we've had Moore's Law, I just, I feel\n\n1:19:21.920 --> 1:19:27.360\n comfortable trying to do projects that are thinking about the next decade. I admire people who are\n\n1:19:27.360 --> 1:19:32.880\n trying to do things that are 30 years out, but it's such a fast moving field. I just don't know\n\n1:19:32.880 --> 1:19:38.800\n how to, I'm not good enough to figure out what's the problem is going to be in 30 years. You know,\n\n1:19:38.800 --> 1:19:44.160\n 10 years is hard enough for me. So maybe if it's possible to untangle your intuition a little bit,\n\n1:19:44.160 --> 1:19:50.320\n I spoke with Jim Keller. I don't know if you're familiar with Jim. And he is trying to sort of\n\n1:19:50.320 --> 1:19:57.200\n be a little bit rebellious and to try to think that he quotes me as being wrong. Yeah. So this,\n\n1:19:57.200 --> 1:20:04.400\n this is what you're doing for the record. Jim talks about that. He has an intuition that Moore's\n\n1:20:04.400 --> 1:20:09.840\n Law is not in fact, in fact dead yet. And then it may continue for some time to come.\n\n1:20:10.720 --> 1:20:16.080\n What are your thoughts about Jim's ideas in this space? Yeah, this is just, this is just marketing.\n\n1:20:16.080 --> 1:20:22.720\n So what Gordon Moore said is a quantitative prediction. We can check the facts, right? Which\n\n1:20:22.720 --> 1:20:29.200\n is doubling the number of transistors every two years. So we can look back at Intel for the last\n\n1:20:29.200 --> 1:20:38.320\n five years and ask him, let's look at DRAM chips six years ago. So that would be three, two year\n\n1:20:38.320 --> 1:20:44.160\n periods. So then our DRAM chips have eight times as many transistors as they did six years ago.\n\n1:20:44.160 --> 1:20:50.320\n We can look up Intel microprocessors six years ago. If Moore's Law is continuing, it should have\n\n1:20:50.320 --> 1:20:56.800\n eight times as many transistors as six years ago. The answer in both those cases is no.\n\n1:20:57.760 --> 1:21:05.440\n The problem has been because Moore's Law was kind of genuinely embraced by the semiconductor\n\n1:21:05.440 --> 1:21:10.480\n industry as they would make investments in similar equipment to make Moore's Law come true.\n\n1:21:10.480 --> 1:21:17.520\n Semiconductor improving and Moore's Law in many people's minds are the same thing. So when I say,\n\n1:21:17.520 --> 1:21:24.080\n and I'm factually correct, that Moore's Law is no longer holds, we are not doubling transistors\n\n1:21:24.080 --> 1:21:31.840\n every year's years. The downside for a company like Intel is people think that means it's stopped,\n\n1:21:31.840 --> 1:21:36.160\n that technology has no longer improved. And so Jim is trying to,\n\n1:21:36.160 --> 1:21:46.240\n counteract the impression that semiconductors are frozen in 2019 are never going to get better.\n\n1:21:46.240 --> 1:21:53.120\n So I never said that. All I said was Moore's Law is no more. And I'm strictly looking at the number\n\n1:21:53.120 --> 1:22:01.440\n of transistors. That's what Moore's Law is. There's the, I don't know, there's been this aura\n\n1:22:01.440 --> 1:22:07.520\n associated with Moore's Law that they've enjoyed for 50 years about, look at the field we're in,\n\n1:22:07.520 --> 1:22:12.160\n we're doubling transistors every two years. What an amazing field, which is an amazing thing that\n\n1:22:12.160 --> 1:22:16.000\n they were able to pull off. But even as Gordon Moore said, you know, no exponential can last\n\n1:22:16.000 --> 1:22:22.080\n forever. It lasted for 50 years, which is amazing. And this is a huge impact on the industry because\n\n1:22:22.080 --> 1:22:28.000\n of these changes that we've been talking about. So he claims, and I'm not going to go into the\n\n1:22:28.000 --> 1:22:33.280\n that we've been talking about. So he claims, because he's trying to act on it, he claims,\n\n1:22:33.280 --> 1:22:38.560\n you know, Patterson says Moore's Law is no more and look at all, look at it, it's still going.\n\n1:22:38.560 --> 1:22:44.800\n And TSMC, they say it's no longer, but there's quantitative evidence that Moore's Law is not\n\n1:22:44.800 --> 1:22:51.520\n continuing. So what I say now to try and, okay, I understand the perception problem when I say\n\n1:22:51.520 --> 1:22:58.640\n Moore's Law has stopped. Okay. So now I say Moore's Law is slowing down. And I think Jim, which is\n\n1:22:58.640 --> 1:23:03.760\n another way of, if he's, if it's predicting every two years and I say it's slowing down, then that's\n\n1:23:03.760 --> 1:23:09.520\n another way of saying it doesn't hold anymore. And, and I think Jim wouldn't disagree that it's\n\n1:23:09.520 --> 1:23:14.560\n slowing down because that sounds like it's, things are still getting better and just not as fast,\n\n1:23:14.560 --> 1:23:18.000\n which is another way of saying Moore's Law isn't working anymore.\n\n1:23:18.000 --> 1:23:22.720\n TG. It's still good for marketing. But what's your, you're not,\n\n1:23:22.720 --> 1:23:27.520\n you don't like expanding the definition of Moore's Law, sort of naturally.\n\n1:23:27.520 --> 1:23:33.600\n CM. Well, as an educator, you know, is this like modern politics? Does everybody get their own facts?\n\n1:23:34.880 --> 1:23:41.200\n Or do we have, you know, Moore's Law was a crisp, you know, it was Carver Mead looked at his\n\n1:23:41.840 --> 1:23:47.680\n Moore's Conversations drawing on a log log scale, a straight line. And that's what the definition of\n\n1:23:47.680 --> 1:23:54.720\n Moore's Law is. There's this other, what Intel did for a while, interestingly, before Jim joined\n\n1:23:54.720 --> 1:23:58.400\n them, they said, oh, no, Moore's Law isn't the number of doubling, isn't really doubling\n\n1:23:58.400 --> 1:24:03.600\n transistors every two years. Moore's Law is the cost of the individual transistor going down,\n\n1:24:04.400 --> 1:24:10.080\n cutting in half every two years. Now, that's not what he said, but they reinterpreted it\n\n1:24:10.080 --> 1:24:15.520\n because they believed that the cost of transistors was continuing to drop,\n\n1:24:15.520 --> 1:24:20.560\n even if they couldn't get twice as many chips. Many people in industry have told me that's not\n\n1:24:20.560 --> 1:24:26.000\n true anymore, that basically in more recent technologies, they got more complicated,\n\n1:24:26.000 --> 1:24:32.400\n the actual cost of transistor went up. So even the, a corollary might not be true,\n\n1:24:32.400 --> 1:24:38.400\n but certainly, you know, Moore's Law, that was the beauty of Moore's Law. It was a very simple,\n\n1:24:38.400 --> 1:24:44.000\n it's like E equals MC squared, right? It was like, wow, what an amazing prediction. It's so easy\n\n1:24:44.000 --> 1:24:50.000\n to understand, the implications are amazing, and that's why it was so famous as a prediction.\n\n1:24:50.000 --> 1:24:56.160\n And this reinterpretation of what it meant and changing is, you know, is revisionist history.\n\n1:24:56.160 --> 1:25:03.520\n And I'd be happy, and they're not claiming there's a new Moore's Law. They're not saying,\n\n1:25:04.160 --> 1:25:10.480\n by the way, instead of every two years, it's every three years. I don't think they want to\n\n1:25:10.480 --> 1:25:14.400\n say that. I think what's going to happen is new technology generations, each one is going to get\n\n1:25:14.400 --> 1:25:21.840\n a little bit slower. So it is slowing down, the improvements won't be as great, and that's why we\n\n1:25:21.840 --> 1:25:27.200\n need to do new things. Yeah, I don't like that the idea of Moore's Law is tied up with marketing.\n\n1:25:28.240 --> 1:25:34.560\n It would be nice if... Whether it's marketing or it's, well, it could be affecting business,\n\n1:25:34.560 --> 1:25:40.720\n but it could also be affecting the imagination of engineers. If Intel employees actually believe\n\n1:25:40.720 --> 1:25:47.840\n that we're frozen in 2019, well, that would be bad for Intel. Not just Intel, but everybody.\n\n1:25:49.040 --> 1:25:56.320\n Moore's Law is inspiring to everybody. But what's happening right now, talking to people\n\n1:25:57.200 --> 1:26:02.800\n who have working in national offices and stuff like that, a lot of the computer science community\n\n1:26:02.800 --> 1:26:09.280\n is unaware that this is going on, that we are in an era that's going to need radical change at lower\n\n1:26:09.280 --> 1:26:18.400\n levels that could affect the whole software stack. If you're using cloud stuff and the\n\n1:26:18.960 --> 1:26:23.040\n servers that you get next year are basically only a little bit faster than the servers you got this\n\n1:26:23.040 --> 1:26:30.240\n year, you need to know that, and we need to start innovating to start delivering on it. If you're\n\n1:26:30.240 --> 1:26:34.400\n counting on your software going to have a lot more features, assuming the computers are going to get\n\n1:26:34.400 --> 1:26:38.640\n faster, that's not true. So are you going to have to start making your software stack more efficient?\n\n1:26:38.640 --> 1:26:45.440\n Are you going to have to start learning about machine learning? So it's a warning or call\n\n1:26:45.440 --> 1:26:51.040\n for arms that the world is changing right now. And a lot of computer science PhDs are unaware\n\n1:26:51.040 --> 1:26:56.800\n of that. So a way to try and get their attention is to say that Moore's Law is slowing down and\n\n1:26:56.800 --> 1:27:02.160\n that's going to affect your assumptions. And we're trying to get the word out. And when companies\n\n1:27:02.160 --> 1:27:08.080\n like TSMC and Intel say, oh, no, no, no, Moore's Law is fine, then people think, oh, hey, I don't\n\n1:27:08.080 --> 1:27:13.600\n have to change my behavior. I'll just get the next servers. And if they start doing measurements,\n\n1:27:13.600 --> 1:27:18.800\n they'll realize what's going on. It'd be nice to have some transparency on metrics for the lay\n\n1:27:18.800 --> 1:27:24.720\n person to be able to know if computers are getting faster and not to forget Moore's Law.\n\n1:27:24.720 --> 1:27:31.920\n Yeah. There are a bunch of, most people kind of use clock rate as a measure of performance.\n\n1:27:31.920 --> 1:27:36.640\n It's not a perfect one, but if you've noticed clock rates are more or less the same as they were\n\n1:27:37.200 --> 1:27:42.960\n five years ago, computers are a little better than they are. They haven't made zero progress,\n\n1:27:42.960 --> 1:27:47.200\n but they've made small progress. So there's some indications out there. And then our behavior,\n\n1:27:47.200 --> 1:27:52.800\n right? Nobody buys the next laptop because it's so much faster than the laptop from the past.\n\n1:27:52.800 --> 1:28:00.480\n For cell phones, I think, I don't know why people buy new cell phones, you know, because\n\n1:28:00.480 --> 1:28:04.560\n the new ones announced. The cameras are better, but that's kind of domain specific, right? They're\n\n1:28:04.560 --> 1:28:10.560\n putting special purpose hardware to make the processing of images go much better. So that's\n\n1:28:10.560 --> 1:28:15.840\n the way they're doing it. They're not particularly, it's not that the ARM processor in there is twice\n\n1:28:15.840 --> 1:28:22.800\n as fast as much as they've added accelerators to help the experience of the phone. Can we talk a\n\n1:28:22.800 --> 1:28:30.720\n little bit about one other exciting space, arguably the same level of impact as your work with RISC\n\n1:28:30.720 --> 1:28:41.920\n is RAID. In 1988, you coauthored a paper, A Case for Redundant Arrays of Inexpensive Disks, hence\n\n1:28:41.920 --> 1:28:48.640\n RAID RAID. So that's where you introduced the idea of RAID. Incredible that that little,\n\n1:28:49.840 --> 1:28:55.760\n I mean little, that paper kind of had this ripple effect and had a really a revolutionary effect.\n\n1:28:55.760 --> 1:29:01.920\n So first, what is RAID? What is RAID? So this is work I did with my colleague Randy Katz and\n\n1:29:01.920 --> 1:29:08.160\n a star graduate student, Garth Gibson. So we had just done the fourth generation RISC project\n\n1:29:08.160 --> 1:29:17.280\n and Randy Katz, which had an early Apple Macintosh computer. At this time, everything was done with\n\n1:29:17.280 --> 1:29:26.160\n floppy disks, which are old technologies that could store things that didn't have much capacity\n\n1:29:26.160 --> 1:29:31.360\n and you had to get any work done, you're always sticking your little floppy disk in and out because\n\n1:29:31.360 --> 1:29:36.400\n they didn't have much capacity. But they started building what are called hard disk drives, which\n\n1:29:36.400 --> 1:29:44.320\n is magnetic material that can remember information storage for the Mac. And Randy asked the question\n\n1:29:44.320 --> 1:29:51.760\n when he saw this disk next to his Mac, gee, these are brand new small things. Before that,\n\n1:29:51.760 --> 1:29:57.520\n for the big computers, the disk would be the size of washing machines. And here's something\n\n1:29:57.520 --> 1:30:02.720\n the size of a, kind of the size of a book or so. He says, I wonder what we could do with that? Well,\n\n1:30:02.720 --> 1:30:11.200\n Randy was involved in the fourth generation RISC project here at Berkeley in the 80s. So we figured\n\n1:30:11.200 --> 1:30:15.680\n out a way how to make the computation part, the processor part go a lot faster, but what about\n\n1:30:15.680 --> 1:30:22.960\n the storage part? Can we do something to make it faster? So we hit upon the idea of taking a lot of\n\n1:30:22.960 --> 1:30:27.600\n these disks developed for personal computers and Macintoshes and putting many of them together\n\n1:30:27.600 --> 1:30:32.640\n instead of one of these washing machine size things. And so we wrote the first draft of the\n\n1:30:32.640 --> 1:30:38.640\n paper and we'd have 40 of these little PC disks instead of one of these washing machine size\n\n1:30:38.640 --> 1:30:42.960\n things. And they would be much cheaper because they're made for PCs and they could actually kind\n\n1:30:42.960 --> 1:30:47.360\n of be faster because there was 40 of them rather than one of them. And so we wrote a paper like\n\n1:30:47.360 --> 1:30:51.520\n that and sent it to one of our former Berkeley students at IBM. And he said, well, this is all\n\n1:30:51.520 --> 1:30:56.960\n great and good, but what about the reliability of these things? Now you have 40 of these things\n\n1:30:56.960 --> 1:31:03.120\n and 40 of these devices, each of which are kind of PC quality. So they're not as good as these\n\n1:31:03.120 --> 1:31:10.160\n IBM washing machines. IBM dominated the storage businesses. So the reliability is going to be\n\n1:31:10.160 --> 1:31:16.240\n awful. And so when we calculated it out, instead of it breaking on average once a year, it would\n\n1:31:16.240 --> 1:31:22.480\n break every two weeks. So we thought about the idea and said, well, we got to address the\n\n1:31:22.480 --> 1:31:27.120\n reliability. So we did it originally performance, but we had to do reliability. So the name\n\n1:31:27.120 --> 1:31:33.440\n redundant array of inexpensive disks is array of these disks inexpensive like for PCs, but we have\n\n1:31:33.440 --> 1:31:40.400\n extra copies. So if one breaks, we won't lose all the information. We'll have enough redundancy that\n\n1:31:40.400 --> 1:31:44.560\n we could let some break and we can still preserve the information. So the name is an array of\n\n1:31:44.560 --> 1:31:51.200\n inexpensive disks. This is a collection of these PCs and the R part of the name was the redundancy\n\n1:31:51.200 --> 1:31:55.760\n so they'd be reliable. And it turns out if you put a modest number of extra disks in one of\n\n1:31:55.760 --> 1:32:00.720\n these arrays, it could actually not only be as faster and cheaper than one of these washing\n\n1:32:00.720 --> 1:32:05.360\n machine disks, it could be actually more reliable because you could have a couple of breaks even\n\n1:32:05.360 --> 1:32:09.760\n with these cheap disks. Whereas one failure with the washing machine thing would knock it out.\n\n1:32:10.480 --> 1:32:17.360\n Did you have a sense just like with risk that in the 30 years that followed,\n\n1:32:17.360 --> 1:32:26.240\n RAID would take over as a mechanism for storage? I think I'm naturally an optimist,\n\n1:32:27.280 --> 1:32:33.840\n but I thought our ideas were right. I thought kind of like Moore's law, it seemed to me,\n\n1:32:33.840 --> 1:32:38.000\n if you looked at the history of the disk drives, they went from washing machine size things and\n\n1:32:38.000 --> 1:32:43.360\n they were getting smaller and smaller and the volumes were with the smaller disk drives because\n\n1:32:43.360 --> 1:32:51.120\n that's where the PCs were. So we thought that was a technological trend that the volume of disk\n\n1:32:51.120 --> 1:32:55.920\n drives was going to be getting smaller and smaller devices, which were true. They were the size of,\n\n1:32:56.480 --> 1:33:00.800\n I don't know, eight inches diameter, then five inches, then three inches in diameters.\n\n1:33:01.440 --> 1:33:06.640\n And so that it made sense to figure out how to deal things with an array of disks. So I think\n\n1:33:06.640 --> 1:33:13.440\n it was one of those things where logically, we think the technological forces were on our side,\n\n1:33:13.440 --> 1:33:19.120\n that it made sense. So we expected it to catch on, but there was that same kind of business question.\n\n1:33:19.920 --> 1:33:25.840\n IBM was the big pusher of these disk drives in the real world where the technical advantage\n\n1:33:25.840 --> 1:33:32.560\n get turned into a business advantage or not. It proved to be true. And so we thought we were\n\n1:33:32.560 --> 1:33:38.320\n sound technically and it was unclear whether the business side, but we kind of, as academics,\n\n1:33:38.320 --> 1:33:43.920\n we believe that technology should win and it did. And if you look at those 30 years,\n\n1:33:44.720 --> 1:33:48.240\n just from your perspective, are there interesting developments in the space of storage\n\n1:33:48.800 --> 1:33:53.520\n that have happened in that time? Yeah. The big thing that happened, well, a couple of things\n\n1:33:53.520 --> 1:34:00.720\n that happened, what we did had a modest amount of storage. So as redundancy, as people built bigger\n\n1:34:00.720 --> 1:34:05.840\n and bigger storage systems, they've added more redundancy so they could add more failures. And\n\n1:34:05.840 --> 1:34:13.360\n the biggest thing that happened in storage is for decades, it was based on things physically spinning\n\n1:34:14.240 --> 1:34:18.400\n called hard disk drives where you used to turn on your computer and it would make a noise.\n\n1:34:18.400 --> 1:34:25.200\n What that noise was, was the disk drives spinning and they were rotating at like 60 revolutions per\n\n1:34:25.200 --> 1:34:31.680\n second. And it's like, if you remember the vinyl records, if you've ever seen those,\n\n1:34:31.680 --> 1:34:36.160\n that's what it looked like. And there was like a needle like on a vinyl record that was reading it.\n\n1:34:36.160 --> 1:34:41.440\n So the big drive change is switching that over to a semiconductor technology called flash.\n\n1:34:41.440 --> 1:34:47.200\n So within the last, I'd say about decade is increasing fraction of all the computers in the\n\n1:34:47.200 --> 1:34:54.880\n world are using semiconductor for storage, the flash drive, instead of being magnetic,\n\n1:34:54.880 --> 1:35:02.640\n they're optical, well, they're a semiconductor writing of information very densely.\n\n1:35:04.080 --> 1:35:08.000\n And that's been a huge difference. So all the cell phones in the world use flash.\n\n1:35:08.000 --> 1:35:13.520\n Most of the laptops use flash. All the embedded devices use flash instead of storage. Still in\n\n1:35:13.520 --> 1:35:20.160\n the cloud, magnetic disks are more economical than flash, but they use both in the cloud.\n\n1:35:20.160 --> 1:35:26.880\n So it's been a huge change in the storage industry, the switching from primarily disk\n\n1:35:26.880 --> 1:35:31.040\n to being primarily semiconductor. For the individual disk, but still the RAID mechanism\n\n1:35:31.040 --> 1:35:35.920\n applies to those different kinds of disks. Yes. The people will still use RAID ideas\n\n1:35:35.920 --> 1:35:41.120\n because it's kind of what's different, kind of interesting kind of psychologically,\n\n1:35:41.120 --> 1:35:46.160\n if you think about it. People have always worried about the reliability of computing since the\n\n1:35:46.160 --> 1:35:52.240\n earliest days. So kind of, but if we're talking about computation, if your computer makes a\n\n1:35:52.240 --> 1:35:59.120\n mistake and the computer says, the computer has ways to check and say, Oh, we screwed up.\n\n1:35:59.120 --> 1:36:04.160\n We made a mistake. What happens is that program that was running, you have to redo it,\n\n1:36:04.160 --> 1:36:12.320\n which is a hassle for storage. If you've sent important information away and it loses that\n\n1:36:12.320 --> 1:36:18.240\n information, you go nuts. This is the worst. Oh my God. So if you have a laptop and you're not\n\n1:36:18.240 --> 1:36:24.240\n backing it up on the cloud or something like this, and your disk drive breaks, which it can do,\n\n1:36:24.880 --> 1:36:29.760\n you'll lose all that information and you just go crazy. So the importance of reliability\n\n1:36:29.760 --> 1:36:34.160\n for storage is tremendously higher than the importance of reliability for computation\n\n1:36:34.160 --> 1:36:39.440\n because of the consequences of it. So yes, so RAID ideas are still very popular, even with\n\n1:36:39.440 --> 1:36:45.200\n the switch of the technology. Although flash drives are more reliable, if you're not doing\n\n1:36:45.200 --> 1:36:51.280\n anything like backing it up to get some redundancy so they handle it, you're taking great risks.\n\n1:36:53.680 --> 1:36:57.600\n You said that for you and possibly for many others, teaching and research don't\n\n1:36:58.800 --> 1:37:03.840\n conflict with each other as one might suspect. And in fact, they kind of complement each other. So\n\n1:37:03.840 --> 1:37:10.480\n maybe a question I have is how has teaching helped you in your research or just in your\n\n1:37:10.480 --> 1:37:17.040\n entirety as a person who both teaches and does research and just thinks and creates new ideas\n\n1:37:17.040 --> 1:37:22.880\n in this world? Yes, I think what happens is when you're a college student, you know there's this\n\n1:37:22.880 --> 1:37:30.400\n kind of tenure system in doing research. So kind of this model that is popular in America, I think\n\n1:37:30.400 --> 1:37:35.440\n America really made it happen, is we can attract these really great faculty to research universities\n\n1:37:36.000 --> 1:37:40.640\n because they get to do research as well as teach. And that, especially in fast moving fields,\n\n1:37:40.640 --> 1:37:44.800\n this means people are up to date and they're teaching those kinds of things. But when you run\n\n1:37:44.800 --> 1:37:50.480\n into a really bad professor, a really bad teacher, I think the students think, well, this guy must be\n\n1:37:50.480 --> 1:37:57.280\n a great researcher because why else could he be here? So after 40 years at Berkeley, we had a\n\n1:37:57.280 --> 1:38:02.400\n retirement party and I got a chance to reflect and I looked back at some things. That is not my\n\n1:38:02.400 --> 1:38:09.600\n experience. I saw a photograph of five of us in the department who won the Distinguished Teaching\n\n1:38:09.600 --> 1:38:14.480\n Award from campus, a very high honor. I've got one of those, one of the highest honors. So there are\n\n1:38:14.480 --> 1:38:23.360\n five of us on that picture. There's Manuel Blum, Richard Karp, me, Randy Kass, and John Osterhaupt,\n\n1:38:23.360 --> 1:38:27.920\n contemporaries of mine. I mentioned Randy already. All of us are in the National Academy of\n\n1:38:27.920 --> 1:38:34.160\n Engineering. We've all run the Distinguished Teaching Award. Blum, Karp, and I all have\n\n1:38:34.160 --> 1:38:45.120\n Turing Awards. The highest award in computing. So that's the opposite. What's happened is they're\n\n1:38:45.120 --> 1:38:51.280\n highly correlated. So the other way to think of it, if you're very successful people or maybe\n\n1:38:51.280 --> 1:38:56.160\n successful at everything they do, it's not an either or. But it's an interesting question\n\n1:38:56.160 --> 1:39:00.880\n whether specifically, that's probably true, but specifically for teaching, if there's something\n\n1:39:00.880 --> 1:39:06.720\n in teaching that, it's the Richard Feynman idea, is there something about teaching that actually\n\n1:39:06.720 --> 1:39:12.640\n makes your research, makes you think deeper and more outside the box and more insightful?\n\n1:39:12.640 --> 1:39:16.400\n Absolutely. I was going to bring up Feynman. I mean, he criticized the Institute of Advanced\n\n1:39:16.400 --> 1:39:21.760\n Studies. So the Institute of Advanced Studies was this thing that was created near Princeton\n\n1:39:21.760 --> 1:39:26.240\n where Einstein and all these smart people went. And when he was invited, he thought it was a\n\n1:39:26.240 --> 1:39:31.040\n terrible idea. This is a university. It was supposed to be heaven, right? A university\n\n1:39:31.040 --> 1:39:35.600\n without any teaching. But he thought it was a mistake. It's getting up in the classroom and\n\n1:39:35.600 --> 1:39:40.640\n having to explain things to students and having them ask questions like, well, why is that true,\n\n1:39:40.640 --> 1:39:47.600\n makes you stop and think. So he thought, and I agree, I think that interaction between a great\n\n1:39:47.600 --> 1:39:52.400\n research university and having students with bright young minds asking hard questions the\n\n1:39:52.400 --> 1:40:00.880\n whole time is synergistic. And a university without teaching wouldn't be as vital and\n\n1:40:00.880 --> 1:40:07.920\n exciting a place. And I think it helps stimulate the research. Another romanticized question,\n\n1:40:07.920 --> 1:40:14.960\n but what's your favorite concept or idea to teach? What inspires you or you see inspire the students?\n\n1:40:15.680 --> 1:40:19.440\n Is there something that pops to mind or puts the fear of God in them? I don't know,\n\n1:40:19.440 --> 1:40:24.640\n whichever is most effective. I mean, in general, I think people are surprised.\n\n1:40:25.200 --> 1:40:31.200\n I've seen a lot of people who don't think they like teaching come give guest lectures or teach\n\n1:40:31.200 --> 1:40:37.200\n a course and get hooked on seeing the lights turn on, right? You can explain something to\n\n1:40:37.200 --> 1:40:44.240\n people that they don't understand. And suddenly they get something that's important and difficult.\n\n1:40:44.240 --> 1:40:50.240\n And just seeing the lights turn on is a real satisfaction there. I don't think there's any\n\n1:40:51.920 --> 1:40:58.320\n specific example of that. It's just the general joy of seeing them understand.\n\n1:40:58.320 --> 1:41:10.560\n SL. I have to talk about this because I've wrestled. I do martial arts. Of course, I love wrestling. I'm a huge, I'm Russian. So I've talked to Dan Gable on the podcast.\n\n1:41:11.520 --> 1:41:20.640\n So you wrestled at UCLA among many other things you've done in your life, competitively in sports\n\n1:41:20.640 --> 1:41:26.800\n and science and so on. You've wrestled. Maybe, again, continue with the romanticized questions,\n\n1:41:26.800 --> 1:41:32.080\n but what have you learned about life and maybe even science from wrestling or from?\n\n1:41:32.080 --> 1:41:39.520\n CB. Yeah, in fact, I wrestled at UCLA, but also at El Camino Community College. And just right now,\n\n1:41:39.520 --> 1:41:44.400\n we were in the state of California, we were state champions at El Camino. And in fact, I was talking\n\n1:41:44.400 --> 1:41:52.000\n to my mom and I got into UCLA, but I decided to go to the community college, which is, it's much\n\n1:41:52.000 --> 1:41:56.400\n harder to go to UCLA than the community college. And I asked, why did I make that decision? Because I\n\n1:41:56.400 --> 1:41:59.920\n thought it was because of my girlfriend. She said, well, it was the girlfriend and you thought the\n\n1:41:59.920 --> 1:42:04.880\n wrestling team was really good. And we were right. We had a great wrestling team. We actually\n\n1:42:06.000 --> 1:42:12.320\n wrestled against UCLA at a tournament and we beat UCLA as a community college, which just freshmen\n\n1:42:12.320 --> 1:42:17.440\n and sophomores. And part of the reason I brought this up is I'm going to go, they've invited me back\n\n1:42:17.440 --> 1:42:27.440\n at El Camino to give a lecture next month. And so, my friend who was on the wrestling team that\n\n1:42:27.440 --> 1:42:31.680\n we're still together, we're right now reaching out to other members of the wrestling team if we can\n\n1:42:31.680 --> 1:42:40.480\n get together for a reunion. But in terms of me, it was a huge difference. The age cut off, it was\n\n1:42:40.480 --> 1:42:47.520\n December 1st. And so, I was almost always the youngest person in my class and I matured later\n\n1:42:47.520 --> 1:42:54.560\n on, our family matured later. So, I was almost always the smallest guy. So, I took kind of\n\n1:42:54.560 --> 1:43:02.480\n nerdy courses, but I was wrestling. So, wrestling was huge for my self confidence in high school.\n\n1:43:02.480 --> 1:43:08.560\n And then, I kind of got bigger at El Camino and in college. And so, I had this kind of physical\n\n1:43:08.560 --> 1:43:18.800\n self confidence and it's translated into research self confidence. And also kind of, I've had this\n\n1:43:18.800 --> 1:43:27.280\n feeling even today in my 70s, if something going on in the streets that is bad physically, I'm not\n\n1:43:27.280 --> 1:43:31.200\n going to ignore it. I'm going to stand up and try and straighten that out.\n\n1:43:31.200 --> 1:43:34.320\n And that kind of confidence just carries through the entirety of your life.\n\n1:43:34.320 --> 1:43:39.040\n Yeah. And the same things happens intellectually. If there's something going on where people are\n\n1:43:39.040 --> 1:43:44.240\n saying something that's not true, I feel it's my job to stand up just like I would in the street.\n\n1:43:44.240 --> 1:43:49.120\n If there's something going on, somebody attacking some woman or something, I'm not standing by and\n\n1:43:49.120 --> 1:43:54.720\n letting that get away. So, I feel it's my job to stand up. So, it's kind of ironically translates.\n\n1:43:54.720 --> 1:44:00.560\n The other things that turned out for both, I had really great college and high school coaches and\n\n1:44:00.560 --> 1:44:05.280\n they believed, even though wrestling is an individual sport, that we would be more successful\n\n1:44:05.280 --> 1:44:10.880\n as a team if we bonded together, do things that we would support each other rather than everybody,\n\n1:44:10.880 --> 1:44:15.200\n you know, in wrestling it's a one on one and you could be everybody's on their own, but he felt if\n\n1:44:15.200 --> 1:44:21.200\n we bonded as a team, we'd succeed. So, I kind of picked up those skills of how to form successful\n\n1:44:21.200 --> 1:44:27.280\n teams and how to, from wrestling. And so, I think one of, most people would say one of my strengths\n\n1:44:27.280 --> 1:44:33.200\n is I can create teams of faculty, large teams of faculty grad students, pull all together for a\n\n1:44:33.200 --> 1:44:41.360\n common goal and often be successful at it. But I got both of those things from wrestling. Also,\n\n1:44:41.360 --> 1:44:49.040\n I think I heard this line about if people are in kind of collision, sports with physical contact\n\n1:44:49.040 --> 1:44:54.800\n like wrestling or football and stuff like that, people are a little bit more assertive or something.\n\n1:44:54.800 --> 1:45:02.160\n And so, I think that also comes through as, you know, and I didn't shy away from the\n\n1:45:02.160 --> 1:45:07.360\n racist debates, you know, I enjoyed taking on the arguments and stuff like that. So,\n\n1:45:08.800 --> 1:45:13.520\n I'm really glad I did wrestling. I think it was really good for my self image and I learned a lot\n\n1:45:13.520 --> 1:45:19.440\n from it. So, I think that's, you know, sports done well, you know, there's really lots of positives\n\n1:45:19.440 --> 1:45:26.240\n you can take about it, of leadership, you know, how to form teams and how to be successful.\n\n1:45:26.880 --> 1:45:30.880\n So, we've talked about metrics a lot. There's a really cool, in terms of bench press and\n\n1:45:30.880 --> 1:45:34.640\n weightlifting, pound years metric that you've developed that we don't have time to talk about,\n\n1:45:34.640 --> 1:45:39.040\n but it's a really cool one that people should look into. It's rethinking the way we think about\n\n1:45:39.040 --> 1:45:43.600\n metrics and weightlifting. But let me talk about metrics more broadly, since that appeals to you\n\n1:45:43.600 --> 1:45:49.600\n in all forms. Let's look at the most ridiculous, the biggest question of the meaning of life.\n\n1:45:50.480 --> 1:45:55.040\n If you were to try to put metrics on a life well lived, what would those metrics be?\n\n1:45:56.800 --> 1:46:03.920\n Yeah, a friend of mine, Randy Katz, said this. He said, you know, when it's time to sign off,\n\n1:46:06.000 --> 1:46:09.920\n the measure isn't the number of zeros in your bank account, it's the number of inches\n\n1:46:09.920 --> 1:46:15.280\n in the obituary in the New York Times, was he said it. I think, you know, having,\n\n1:46:17.040 --> 1:46:21.840\n and you know, this is a cliche, is that people don't die wishing they'd spent more time in the\n\n1:46:21.840 --> 1:46:29.360\n office, right? As I reflect upon my career, there have been, you know, a half a dozen, a dozen things\n\n1:46:29.360 --> 1:46:35.440\n say I've been proud of. A lot of them aren't papers or scientific results. Certainly, my family,\n\n1:46:35.440 --> 1:46:41.120\n my wife, we've been married more than 50 years, kids and grandkids, that's really precious.\n\n1:46:42.880 --> 1:46:50.240\n Education things I've done, I'm very proud of, you know, books and courses. I did some help\n\n1:46:50.240 --> 1:46:55.200\n with underrepresented groups that was effective. So it was interesting to see what were the things\n\n1:46:55.200 --> 1:47:00.960\n I reflected. You know, I had hundreds of papers, but some of them were the papers, like the risk\n\n1:47:00.960 --> 1:47:06.480\n rate stuff that I'm proud of, but a lot of them were not those things. So people who are, just\n\n1:47:06.480 --> 1:47:11.040\n spend their lives, you know, going after the dollars or going after all the papers in the\n\n1:47:11.040 --> 1:47:15.760\n world, you know, that's probably not the things that are afterwards you're going to care about.\n\n1:47:15.760 --> 1:47:22.320\n When I was, just when I got the offer from Berkeley before I showed up, I read a book where\n\n1:47:22.320 --> 1:47:27.200\n they interviewed a lot of people in all works of life. And what I got out of that book was the\n\n1:47:27.200 --> 1:47:31.520\n people who felt good about what they did was the people who affected people, as opposed to things\n\n1:47:31.520 --> 1:47:36.320\n that were more transitory. So I came into this job assuming that it wasn't going to be the papers,\n\n1:47:36.320 --> 1:47:42.000\n it was going to be relationships with the people over time that I would value, and that was a\n\n1:47:42.000 --> 1:47:47.120\n correct assessment, right? It's the people you work with, the people you can influence, the people\n\n1:47:47.120 --> 1:47:50.640\n you can help, it's the things that you feel good about towards the end of your career. It's not\n\n1:47:51.920 --> 1:47:53.200\n the stuff that's more transitory.\n\n1:47:53.200 --> 1:47:58.480\n Trey Lockerbie I don't think there's a better way to end it than talking about your family,\n\n1:47:58.480 --> 1:48:02.320\n the over 50 years of being married to your childhood sweetheart.\n\n1:48:02.320 --> 1:48:03.920\n Richard Averbeck What I think I can add is,\n\n1:48:05.040 --> 1:48:07.280\n when you tell people you've been married 50 years, they want to know why.\n\n1:48:07.280 --> 1:48:08.800\n Trey Lockerbie How? Why?\n\n1:48:08.800 --> 1:48:10.400\n Richard Averbeck Yeah, I can tell you the nine\n\n1:48:10.400 --> 1:48:16.560\n magic words that you need to say to your partner to keep a good relationship. And the nine magic\n\n1:48:16.560 --> 1:48:22.960\n words are, I was wrong. You were right. I love you. Okay. And you got to say all nine. You can't\n\n1:48:22.960 --> 1:48:28.160\n say, I was wrong. You were right. You're a jerk. You know, you can't say that. So yeah, freely\n\n1:48:28.160 --> 1:48:33.200\n acknowledging that you made a mistake, the other person was right, and that you love them really\n\n1:48:34.640 --> 1:48:37.760\n gets over a lot of bumps in the road. So that's what I pass along.\n\n1:48:37.760 --> 1:48:39.840\n Trey Lockerbie Beautifully put. David,\n\n1:48:39.840 --> 1:48:43.840\n it's a huge honor. Thank you so much for the book you've written, for the research you've done,\n\n1:48:43.840 --> 1:48:45.760\n for changing the world. Thank you for talking today.\n\n1:48:45.760 --> 1:48:46.880\n Richard Averbeck Thanks for the interview.\n\n1:48:46.880 --> 1:48:48.880\n Trey Lockerbie Thanks for listening to this\n\n1:48:48.880 --> 1:48:55.440\n conversation with David Patterson. And thank you to our sponsors, The Jordan Harbinger Show, and\n\n1:48:55.440 --> 1:49:02.320\n Cash App. Please consider supporting this podcast by going to JordanHarbinger.com slash Lex and\n\n1:49:02.320 --> 1:49:08.640\n downloading Cash App and using code LexPodcast. Click the links, buy the stuff. It's the best way\n\n1:49:08.640 --> 1:49:14.800\n to support this podcast and the journey I'm on. If you enjoy this thing, subscribe on YouTube,\n\n1:49:14.800 --> 1:49:19.680\n review it with five stars in a podcast, support it on Patreon, or connect with me on Twitter at\n\n1:49:19.680 --> 1:49:26.640\n Lex Friedman, spelled without the E, try to figure out how to do that. It's just F R I D M A N.\n\n1:49:27.280 --> 1:49:31.280\n And now let me leave you with some words from Henry David Thoreau.\n\n1:49:32.240 --> 1:49:40.560\n Our life is faded away by detail. Simplify, simplify. Thank you for listening and hope to\n\n1:49:40.560 --> 1:49:46.560\n see you next time.\n\n"
}