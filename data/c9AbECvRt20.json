{
  "title": "Michael Littman: Reinforcement Learning and the Future of AI | Lex Fridman Podcast #144",
  "id": "c9AbECvRt20",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:04.560\n The following is a conversation with Michael Littman, a computer science professor at Brown\n\n00:04.560 --> 00:10.320\n University doing research on and teaching machine learning, reinforcement learning,\n\n00:10.320 --> 00:16.400\n and artificial intelligence. He enjoys being silly and lighthearted in conversation,\n\n00:16.400 --> 00:20.640\n so this was definitely a fun one. Quick mention of each sponsor,\n\n00:20.640 --> 00:26.800\n followed by some thoughts related to the episode. Thank you to SimplySafe, a home security company\n\n00:26.800 --> 00:32.480\n I use to monitor and protect my apartment, ExpressVPN, the VPN I've used for many years\n\n00:32.480 --> 00:38.000\n to protect my privacy on the internet, MasterClass, online courses that I enjoy from\n\n00:38.000 --> 00:43.760\n some of the most amazing humans in history, and BetterHelp, online therapy with a licensed\n\n00:43.760 --> 00:49.200\n professional. Please check out these sponsors in the description to get a discount and to support\n\n00:49.200 --> 00:55.440\n this podcast. As a side note, let me say that I may experiment with doing some solo episodes\n\n00:55.440 --> 01:02.080\n in the coming month or two. The three ideas I have floating in my head currently is to use one,\n\n01:02.720 --> 01:10.240\n a particular moment in history, two, a particular movie, or three, a book to drive a conversation\n\n01:10.240 --> 01:17.120\n about a set of related concepts. For example, I could use 2001, A Space Odyssey, or Ex Machina\n\n01:17.120 --> 01:26.000\n to talk about AGI for one, two, three hours. Or I could do an episode on the, yes, rise and fall of\n\n01:26.000 --> 01:32.560\n Hitler and Stalin, each in a separate episode, using relevant books and historical moments\n\n01:32.560 --> 01:38.800\n for reference. I find the format of a solo episode very uncomfortable and challenging,\n\n01:38.800 --> 01:44.080\n but that just tells me that it's something I definitely need to do and learn from the experience.\n\n01:44.080 --> 01:48.960\n Of course, I hope you come along for the ride. Also, since we have all this momentum built up\n\n01:49.280 --> 01:54.240\n on announcements, I'm giving a few lectures on machine learning at MIT this January.\n\n01:54.240 --> 02:01.600\n In general, if you have ideas for the episodes, for the lectures, or for just short videos on\n\n02:01.600 --> 02:10.080\n YouTube, let me know in the comments that I still definitely read, despite my better judgment,\n\n02:10.080 --> 02:17.200\n and the wise sage advice of the great Joe Rogan. If you enjoy this thing, subscribe on YouTube,\n\n02:17.200 --> 02:22.400\n review it with Five Stars and Apple Podcast, follow on Spotify, support on Patreon, or connect\n\n02:22.400 --> 02:28.640\n with me on Twitter at Lex Friedman. And now, here's my conversation with Michael Littman.\n\n02:29.920 --> 02:35.680\n I saw a video of you talking to Charles Isbell about Westworld, the TV series. You guys were\n\n02:35.680 --> 02:40.560\n doing the kind of thing where you're watching new things together, but let's rewind back.\n\n02:41.360 --> 02:50.560\n Is there a sci fi movie or book or shows that was profound, that had an impact on you philosophically,\n\n02:50.560 --> 02:54.000\n or just specifically something you enjoyed nerding out about?\n\n02:55.200 --> 03:00.640\n Yeah, interesting. I think a lot of us have been inspired by robots in movies. One that I really\n\n03:00.640 --> 03:05.760\n like is, there's a movie called Robot and Frank, which I think is really interesting because it's\n\n03:05.760 --> 03:15.200\n very near term future, where robots are being deployed as helpers in people's homes. And we\n\n03:15.200 --> 03:19.200\n don't know how to make robots like that at this point, but it seemed very plausible. It seemed\n\n03:19.200 --> 03:25.280\n very realistic or imaginable. And I thought that was really cool because they're awkward,\n\n03:25.280 --> 03:29.040\n they do funny things that raise some interesting issues, but it seemed like something that would\n\n03:29.040 --> 03:31.600\n ultimately be helpful and good if we could do it right.\n\n03:31.600 --> 03:33.760\n Yeah, he was an older cranky gentleman, right?\n\n03:33.760 --> 03:35.920\n He was an older cranky jewel thief, yeah.\n\n03:36.800 --> 03:42.240\n It's kind of funny little thing, which is, you know, he's a jewel thief and so he pulls the\n\n03:42.240 --> 03:49.520\n robot into his life, which is like, which is something you could imagine taking a home robotics\n\n03:49.520 --> 03:54.800\n thing and pulling into whatever quirky thing that's involved in your existence.\n\n03:54.800 --> 04:00.000\n It's meaningful to you. Exactly so. Yeah. And I think from that perspective, I mean,\n\n04:00.000 --> 04:05.680\n not all of us are jewel thieves. And so when we bring our robots into our lives, it explains a\n\n04:05.680 --> 04:12.400\n lot about this apartment, actually. But no, the idea that people should have the ability to make\n\n04:12.400 --> 04:18.400\n this technology their own, that it becomes part of their lives. And I think it's hard for us\n\n04:18.400 --> 04:22.720\n as technologists to make that kind of technology. It's easier to mold people into what we need them\n\n04:22.720 --> 04:28.080\n to be. And just that opposite vision, I think, is really inspiring. And then there's a\n\n04:28.080 --> 04:32.640\n anthropomorphization where we project certain things on them, because I think the robot was\n\n04:32.640 --> 04:38.240\n kind of dumb. But I have a bunch of Roombas I play with and you immediately project stuff onto\n\n04:38.240 --> 04:43.920\n them. Much greater level of intelligence. We'll probably do that with each other too. Much greater\n\n04:43.920 --> 04:47.760\n degree of compassion. That's right. One of the things we're learning from AI is where we are\n\n04:47.760 --> 04:55.760\n smart and where we are not smart. Yeah. You also enjoy, as people can see, and I enjoyed\n\n04:55.760 --> 05:01.600\n myself watching you sing and even dance a little bit, a little bit, a little bit of dancing.\n\n05:02.160 --> 05:07.920\n A little bit of dancing. That's not quite my thing. As a method of education or just in life,\n\n05:08.800 --> 05:15.920\n you know, in general. So easy question. What's the definitive, objectively speaking,\n\n05:15.920 --> 05:22.000\n top three songs of all time? Maybe something that, you know, to walk that back a little bit,\n\n05:22.000 --> 05:27.920\n maybe something that others might be surprised by the three songs that you kind of enjoy.\n\n05:28.480 --> 05:32.560\n That is a great question that I cannot answer. But instead, let me tell you a story.\n\n05:32.560 --> 05:36.480\n So pick a question you do want to answer. That's right. I've been watching the\n\n05:36.480 --> 05:39.440\n presidential debates and vice presidential debates. And it turns out, yeah, it's really,\n\n05:39.440 --> 05:46.640\n you can just answer any question you want. So it's a related question. Well said.\n\n05:47.280 --> 05:51.760\n I really like pop music. I've enjoyed pop music ever since I was very young. So 60s music,\n\n05:51.760 --> 05:56.560\n 70s music, 80s music. This is all awesome. And then I had kids and I think I stopped listening\n\n05:56.560 --> 06:01.440\n to music and I was starting to realize that my musical taste had sort of frozen out.\n\n06:01.440 --> 06:08.240\n And so I decided in 2011, I think, to start listening to the top 10 billboard songs each week.\n\n06:08.240 --> 06:11.920\n So I'd be on the on the treadmill and I would listen to that week's top 10 songs\n\n06:11.920 --> 06:17.280\n so I could find out what was popular now. And what I discovered is that I have no musical\n\n06:17.280 --> 06:22.960\n taste whatsoever. I like what I'm familiar with. And so the first time I'd hear a song\n\n06:22.960 --> 06:26.880\n is the first week that was on the charts, I'd be like, and then the second week,\n\n06:26.880 --> 06:30.640\n I was into it a little bit. And the third week, I was loving it. And by the fourth week is like,\n\n06:30.640 --> 06:36.720\n just part of me. And so I'm afraid that I can't tell you the most my favorite song of all time,\n\n06:36.720 --> 06:42.240\n because it's whatever I heard most recently. Yeah, that's interesting. People have told me that\n\n06:44.240 --> 06:48.800\n there's an art to listening to music as well. And you can start to, if you listen to a song,\n\n06:48.800 --> 06:53.520\n just carefully, like explicitly, just force yourself to really listen. You start to,\n\n06:54.080 --> 07:01.200\n I did this when I was part of jazz band and fusion band in college. You start to hear the layers\n\n07:01.200 --> 07:04.720\n of the instruments. You start to hear the individual instruments and you start to,\n\n07:04.720 --> 07:08.240\n you can listen to classical music or to orchestra this way. You can listen to jazz this way.\n\n07:08.240 --> 07:16.240\n I mean, it's funny to imagine you now to walking that forward to listening to pop hits now as like\n\n07:16.240 --> 07:22.160\n a scholar, listening to like Cardi B or something like that, or Justin Timberlake. Is he? No,\n\n07:22.160 --> 07:26.640\n not Timberlake, Bieber. They've both been in the top 10 since I've been listening.\n\n07:26.640 --> 07:29.520\n They're still up there. Oh my God, I'm so cool.\n\n07:29.520 --> 07:33.440\n If you haven't heard Justin Timberlake's top 10 in the last few years, there was one\n\n07:33.440 --> 07:38.000\n song that he did where the music video was set at essentially NeurIPS.\n\n07:38.720 --> 07:42.400\n Oh, wow. Oh, the one with the robotics. Yeah, yeah, yeah, yeah, yeah.\n\n07:42.400 --> 07:45.520\n Yeah, yeah. It's like at an academic conference and he's doing a demo.\n\n07:45.520 --> 07:46.640\n He was presenting, right?\n\n07:46.640 --> 07:51.920\n It was sort of a cross between the Apple, like Steve Jobs kind of talk and NeurIPS.\n\n07:51.920 --> 07:52.420\n Yeah.\n\n07:53.120 --> 07:56.560\n So, you know, it's always fun when AI shows up in pop culture.\n\n07:56.560 --> 08:01.840\n I wonder if he consulted somebody for that. That's really interesting. So maybe on that topic,\n\n08:01.840 --> 08:08.000\n I've seen your celebrity multiple dimensions, but one of them is you've done cameos in different\n\n08:08.000 --> 08:16.720\n places. I've seen you in a TurboTax commercial as like, I guess, the brilliant Einstein character.\n\n08:16.720 --> 08:23.840\n And the point is that TurboTax doesn't need somebody like you. It doesn't need a brilliant\n\n08:23.840 --> 08:24.340\n person.\n\n08:24.340 --> 08:28.000\n Very few things need someone like me. But yes, they were specifically emphasizing the\n\n08:28.000 --> 08:32.080\n idea that you don't need to be like a computer expert to be able to use their software.\n\n08:32.080 --> 08:33.680\n How did you end up in that world?\n\n08:33.680 --> 08:38.560\n I think it's an interesting story. So I was teaching my class. It was an intro computer\n\n08:38.560 --> 08:45.440\n science class for non concentrators, non majors. And sometimes when people would visit campus,\n\n08:45.440 --> 08:48.960\n they would check in to say, hey, we want to see what a class is like. Can we sit on your class?\n\n08:48.960 --> 09:02.800\n So a person came to my class who was the daughter of the brother of the husband of the best friend\n\n09:02.800 --> 09:11.200\n of my wife. Anyway, basically a family friend came to campus to check out Brown and asked to\n\n09:11.200 --> 09:16.800\n come to my class and came with her dad. Her dad is, who I've known from various\n\n09:16.800 --> 09:21.360\n kinds of family events and so forth, but he also does advertising. And he said that he was\n\n09:21.360 --> 09:31.200\n recruiting scientists for this ad, this TurboTax set of ads. And he said, we wrote the ad with the\n\n09:31.200 --> 09:36.720\n idea that we get like the most brilliant researchers, but they all said no. So can you\n\n09:36.720 --> 09:44.800\n help us find like B level scientists? And I'm like, sure, that's who I hang out with.\n\n09:44.800 --> 09:49.840\n So that should be fine. So I put together a list and I did what some people call the Dick Cheney.\n\n09:49.840 --> 09:55.040\n So I included myself on the list of possible candidates, with a little blurb about each one\n\n09:55.040 --> 09:59.200\n and why I thought that would make sense for them to do it. And they reached out to a handful of\n\n09:59.200 --> 10:02.560\n them, but then they ultimately, they YouTube stalked me a little bit and they thought,\n\n10:03.120 --> 10:07.600\n oh, I think he could do this. And they said, okay, we're going to offer you the commercial.\n\n10:07.600 --> 10:14.320\n I'm like, what? So it was such an interesting experience because they have another world, the\n\n10:14.320 --> 10:21.760\n people who do like nationwide kind of ad campaigns and television shows and movies and so forth.\n\n10:21.760 --> 10:28.400\n It's quite a remarkable system that they have going because they have a set. Yeah. So I went to,\n\n10:28.400 --> 10:35.680\n it was just somebody's house that they rented in New Jersey. But in the commercial, it's just me\n\n10:35.680 --> 10:41.680\n and this other woman. In reality, there were 50 people in that room and another, I don't know,\n\n10:41.680 --> 10:46.400\n half a dozen kind of spread out around the house in various ways. There were people whose job it\n\n10:46.400 --> 10:53.440\n was to control the sun. They were in the backyard on ladders, putting filters up to try to make sure\n\n10:53.440 --> 10:57.120\n that the sun didn't glare off the window in a way that would wreck the shot. So there was like\n\n10:57.120 --> 11:02.160\n six people out there doing that. There was three people out there giving snacks, the craft table.\n\n11:02.160 --> 11:05.840\n There was another three people giving healthy snacks because that was a separate craft table.\n\n11:05.840 --> 11:12.720\n There was one person whose job it was to keep me from getting lost. And I think the reason for all\n\n11:12.720 --> 11:16.560\n this is because so many people are in one place at one time. They have to be time efficient. They\n\n11:16.560 --> 11:20.640\n have to get it done. The morning they were going to do my commercial. In the afternoon, they were\n\n11:20.640 --> 11:27.600\n going to do a commercial of a mathematics professor from Princeton. They had to get it done. No wasted\n\n11:27.600 --> 11:32.320\n time or energy. And so there's just a fleet of people all working as an organism. And it was\n\n11:32.320 --> 11:36.880\n fascinating. I was just the whole time just looking around like, this is so neat. Like one person\n\n11:36.880 --> 11:43.760\n whose job it was to take the camera off of the cameraman so that someone else whose job it was\n\n11:43.760 --> 11:48.720\n to remove the film canister. Because every couple's takes, they had to replace the film because film\n\n11:48.720 --> 11:53.520\n gets used up. It was just, I don't know. I was geeking out the whole time. It was so fun.\n\n11:53.520 --> 11:57.920\n How many takes did it take? It looked the opposite. There was more than two people there. It was very\n\n11:57.920 --> 12:06.320\n relaxed. Right. Yeah. The person who I was in the scene with is a professional. She's an improv\n\n12:06.320 --> 12:11.040\n comedian from New York City. And when I got there, they had given me a script as such as it was. And\n\n12:11.040 --> 12:15.280\n then I got there and they said, we're going to do this as improv. I'm like, I don't know how to\n\n12:15.280 --> 12:21.600\n improv. I don't know what you're telling me to do here. Don't worry. She knows. I'm like, okay.\n\n12:21.600 --> 12:26.320\n I'll go see how this goes. I guess I got pulled into the story because like, where the heck did\n\n12:26.320 --> 12:31.440\n you come from? I guess in the scene. Like, how did you show up in this random person's house?\n\n12:32.480 --> 12:36.320\n Yeah. Well, I mean, the reality of it is I stood outside in the blazing sun. There was someone\n\n12:36.320 --> 12:41.440\n whose job it was to keep an umbrella over me because I started to sweat. And so I would wreck\n\n12:41.440 --> 12:45.600\n the shot because my face was all shiny with sweat. So there was one person who would dab me off,\n\n12:45.600 --> 12:51.600\n had an umbrella. But yeah, like the reality of it, like, why is this strange stalkery person hanging\n\n12:51.600 --> 12:54.960\n around outside somebody's house? We're not sure when you have to look in,\n\n12:54.960 --> 13:00.400\n what the ways for the book, but are you, so you make, you make, like you said, YouTube,\n\n13:00.400 --> 13:07.760\n you make videos yourself, you make awesome parody, sort of parody songs that kind of focus on a\n\n13:07.760 --> 13:13.360\n particular aspect of computer science. How much those seem really interesting to you?\n\n13:13.360 --> 13:18.000\n How much those seem really natural? How much production value goes into that?\n\n13:18.000 --> 13:22.480\n Do you also have a team of 50 people? The videos, almost all the videos,\n\n13:22.480 --> 13:26.880\n except for the ones that people would have actually seen, are just me. I write the lyrics,\n\n13:26.880 --> 13:34.400\n I sing the song. I generally find a, like a backing track online because I'm like you,\n\n13:34.400 --> 13:39.120\n can't really play an instrument. And then I do, in some cases I'll do visuals using just like\n\n13:39.120 --> 13:43.600\n PowerPoint. Lots and lots of PowerPoint to make it sort of like an animation.\n\n13:44.240 --> 13:49.120\n The most produced one is the one that people might have seen, which is the overfitting video\n\n13:49.120 --> 13:55.760\n that I did with Charles Isbell. And that was produced by the Georgia Tech and Udacity people\n\n13:55.760 --> 13:59.680\n because we were doing a class together. It was kind of, I usually do parody songs kind of to\n\n13:59.680 --> 14:04.560\n cap off a class at the end of a class. So that one you're wearing, so it was just a\n\n14:04.560 --> 14:09.920\n thriller. You're wearing the Michael Jackson, the red leather jacket. The interesting thing\n\n14:09.920 --> 14:20.160\n with podcasting that you're also into is that I really enjoy is that there's not a team of people.\n\n14:21.040 --> 14:29.040\n It's kind of more, because you know, there's something that happens when there's more people\n\n14:29.040 --> 14:36.400\n involved than just one person that just the way you start acting, I don't know. There's a censorship.\n\n14:36.400 --> 14:42.480\n You're not given, especially for like slow thinkers like me, you're not. And I think most of us are,\n\n14:42.480 --> 14:50.640\n if we're trying to actually think we're a little bit slow and careful, it kind of large teams get\n\n14:50.640 --> 14:56.480\n in the way of that. And I don't know what to do with that. Like that's the, to me, like if,\n\n14:56.480 --> 15:00.160\n yeah, it's very popular to criticize quote unquote mainstream media.\n\n15:01.760 --> 15:06.880\n But there is legitimacy to criticizing them the same. I love listening to NPR, for example,\n\n15:06.880 --> 15:11.440\n but every, it's clear that there's a team behind it. There's a commercial,\n\n15:11.440 --> 15:14.800\n there's constant commercial breaks. There's this kind of like rush of like,\n\n15:16.080 --> 15:20.320\n okay, I have to interrupt you now because we have to go to commercial. Just this whole,\n\n15:20.320 --> 15:28.640\n it creates, it destroys the possibility of nuanced conversation. Yeah, exactly. Evian,\n\n15:29.280 --> 15:36.800\n which Charles Isbell, who I talked to yesterday told me that Evian is naive backwards, which\n\n15:36.800 --> 15:42.240\n the fact that his mind thinks this way is quite brilliant. Anyway, there's a freedom to this\n\n15:42.240 --> 15:46.960\n podcast. He's Dr. Awkward, which by the way, is a palindrome. That's a palindrome that I happen to\n\n15:46.960 --> 15:54.640\n know from other parts of my life. And I just, well, you know, use it against Charles. Dr. Awkward.\n\n15:54.640 --> 16:00.000\n So what was the most challenging parody song to make? Was it the Thriller one?\n\n16:00.800 --> 16:06.080\n No, that one was really fun. I wrote the lyrics really quickly and then I gave it over to the\n\n16:06.080 --> 16:11.920\n production team. They recruited a acapella group to sing. That went really smoothly. It's great\n\n16:11.920 --> 16:15.520\n having a team because then you can just focus on the part that you really love, which in my case\n\n16:15.520 --> 16:21.040\n is writing the lyrics. For me, the most challenging one, not challenging in a bad way, but challenging\n\n16:21.040 --> 16:27.520\n in a really fun way, was I did one of the parody songs I did is about the halting problem in\n\n16:27.520 --> 16:34.480\n computer science. The fact that you can't create a program that can tell for any other arbitrary\n\n16:34.480 --> 16:38.080\n program whether it actually going to get stuck in infinite loop or whether it's going to eventually\n\n16:38.080 --> 16:46.000\n stop. And so I did it to an 80's song because I hadn't started my new thing of learning current\n\n16:46.000 --> 16:55.600\n songs. And it was Billy Joel's The Piano Man. Nice. Which is a great song. Sing me a song.\n\n16:56.560 --> 17:04.560\n You're the piano man. Yeah. So the lyrics are great because first of all, it rhymes. Not all\n\n17:04.560 --> 17:09.760\n songs rhyme. I've done Rolling Stones songs which turn out to have no rhyme scheme whatsoever. They're\n\n17:09.760 --> 17:14.640\n just sort of yelling and having a good time, which makes it not fun from a parody perspective because\n\n17:14.640 --> 17:18.960\n like you can say anything. But the lines rhymed and there was a lot of internal rhymes as well.\n\n17:18.960 --> 17:24.720\n And so figuring out how to sing with internal rhymes, a proof of the halting problem was really\n\n17:24.720 --> 17:30.960\n challenging. And I really enjoyed that process. What about, last question on this topic, what\n\n17:30.960 --> 17:36.800\n about the dancing in the Thriller video? How many takes that take? So I wasn't planning to dance.\n\n17:36.800 --> 17:40.560\n They had me in the studio and they gave me the jacket and it's like, well, you can't,\n\n17:40.560 --> 17:46.080\n if you have the jacket and the glove, like there's not much you can do. Yeah. So I think I just\n\n17:46.080 --> 17:49.600\n danced around and then they said, why don't you dance a little bit? There was a scene with me\n\n17:49.600 --> 17:55.920\n and Charles dancing together. They did not use it in the video, but we recorded it. Yeah. Yeah. No,\n\n17:55.920 --> 18:02.720\n it was pretty funny. And Charles, who has this beautiful, wonderful voice doesn't really sing.\n\n18:02.720 --> 18:07.520\n He's not really a singer. And so that was why I designed the song with him doing a spoken section\n\n18:07.520 --> 18:12.320\n and me doing the singing. It's very like Barry White. Yeah. Smooth baritone. Yeah. Yeah. It's\n\n18:12.320 --> 18:19.200\n great. That was awesome. So one of the other things Charles said is that, you know, everyone\n\n18:19.200 --> 18:26.400\n knows you as like a super nice guy, super passionate about teaching and so on. What he said,\n\n18:27.040 --> 18:34.000\n don't know if it's true, that despite the fact that you're, you are. Okay. I will admit this\n\n18:34.000 --> 18:39.360\n finally for the first time. That was, that was me. It's the Johnny Cash song. Kill the Manorino just\n\n18:39.360 --> 18:46.880\n to watch him die. That you actually do have some strong opinions on some topics. So if this in fact\n\n18:46.880 --> 18:55.120\n is true, what strong opinions would you say you have? Is there ideas you think maybe in artificial\n\n18:55.120 --> 19:01.200\n intelligence and machine learning, maybe in life that you believe is true that others might,\n\n19:02.640 --> 19:08.400\n you know, some number of people might disagree with you on? So I try very hard to see things\n\n19:08.400 --> 19:15.680\n from multiple perspectives. There's this great Calvin and Hobbes cartoon where, do you know?\n\n19:15.680 --> 19:21.440\n Yeah. Okay. So Calvin's dad is always kind of a bit of a foil and he talked Calvin into,\n\n19:21.440 --> 19:25.440\n Calvin had done something wrong. The dad talks him into like seeing it from another perspective\n\n19:25.440 --> 19:30.880\n and Calvin, like this breaks Calvin because he's like, oh my gosh, now I can see the opposite sides\n\n19:30.880 --> 19:35.920\n of things. And so the, it's, it becomes like a Cubist cartoon where there is no front and back.\n\n19:35.920 --> 19:39.680\n Everything's just exposed and it really freaks him out. And finally he settles back down. It's\n\n19:39.680 --> 19:44.160\n like, oh good. No, I can make that go away. But like, I'm that, I'm that I live in that world where\n\n19:44.160 --> 19:48.400\n I'm trying to see everything from every perspective all the time. So there are some things that I've\n\n19:48.400 --> 19:56.160\n formed opinions about that I would be harder, I think, to disavow me of. One is the super\n\n19:56.160 --> 20:02.640\n intelligence argument and the existential threat of AI is one where I feel pretty confident in my\n\n20:02.640 --> 20:07.840\n feeling about that one. Like I'm willing to hear other arguments, but like, I am not particularly\n\n20:07.840 --> 20:13.600\n moved by the idea that if we're not careful, we will accidentally create a super intelligence\n\n20:13.600 --> 20:17.600\n that will destroy human life. Let's talk about that. Let's get you in trouble and record your\n\n20:17.600 --> 20:24.800\n video. It's like Bill Gates, I think he said like some quote about the internet that that's just\n\n20:24.800 --> 20:29.360\n going to be a small thing. It's not going to really go anywhere. And then I think Steve\n\n20:29.360 --> 20:35.280\n Ballmer said, I don't know why I'm sticking on Microsoft. That's something that like smartphones\n\n20:36.080 --> 20:40.400\n are useless. There's no reason why Microsoft should get into smartphones, that kind of.\n\n20:40.400 --> 20:45.200\n So let's get, let's talk about AGI. As AGI is destroying the world, we'll look back at this\n\n20:45.200 --> 20:49.920\n video and see. No, I think it's really interesting to actually talk about because nobody really\n\n20:49.920 --> 20:54.080\n knows the future. So you have to use your best intuition. It's very difficult to predict it,\n\n20:54.080 --> 21:01.760\n but you have spoken about AGI and the existential risks around it and sort of basing your intuition\n\n21:01.760 --> 21:08.960\n that we're quite far away from that being a serious concern relative to the other concerns\n\n21:08.960 --> 21:15.840\n we have. Can you maybe unpack that a little bit? Yeah, sure, sure, sure. So as I understand it,\n\n21:15.840 --> 21:22.320\n that for example, I read Bostrom's book and a bunch of other reading material about this sort\n\n21:22.320 --> 21:26.720\n of general way of thinking about the world. And I think the story goes something like this, that we\n\n21:27.520 --> 21:35.840\n will at some point create computers that are smart enough that they can help design the next version\n\n21:35.840 --> 21:42.160\n of themselves, which itself will be smarter than the previous version of themselves and eventually\n\n21:42.160 --> 21:49.120\n bootstrapped up to being smarter than us. At which point we are essentially at the mercy of this sort\n\n21:49.120 --> 21:56.720\n of more powerful intellect, which in principle we don't have any control over what its goals are.\n\n21:56.720 --> 22:04.480\n And so if its goals are at all out of sync with our goals, for example, the continued existence\n\n22:04.480 --> 22:11.920\n of humanity, we won't be able to stop it. It'll be way more powerful than us and we will be toast.\n\n22:12.640 --> 22:18.800\n So there's some, I don't know, very smart people who have signed on to that story. And it's a\n\n22:18.800 --> 22:25.360\n compelling story. Now I can really get myself in trouble. I once wrote an op ed about this,\n\n22:25.360 --> 22:30.640\n specifically responding to some quotes from Elon Musk, who has been on this very podcast\n\n22:30.640 --> 22:38.480\n more than once. AI summoning the demon. But then he came to Providence, Rhode Island,\n\n22:38.480 --> 22:45.360\n which is where I live, and said to the governors of all the states, you know, you're worried about\n\n22:45.360 --> 22:49.360\n entirely the wrong thing. You need to be worried about AI. You need to be very, very worried about\n\n22:49.360 --> 22:56.240\n AI. And journalists kind of reacted to that and they wanted to get people's take. And I was like,\n\n22:56.240 --> 23:03.440\n OK, my my my belief is that one of the things that makes Elon Musk so successful and so remarkable\n\n23:03.440 --> 23:08.880\n as an individual is that he believes in the power of ideas. He believes that you can have you can\n\n23:08.880 --> 23:12.960\n if you know, if you have a really good idea for getting into space, you can get into space.\n\n23:12.960 --> 23:17.440\n If you have a really good idea for a company or for how to change the way that people drive,\n\n23:18.080 --> 23:23.840\n you just have to do it and it can happen. It's really natural to apply that same idea to AI.\n\n23:23.840 --> 23:30.720\n You see these systems that are doing some pretty remarkable computational tricks, demonstrations,\n\n23:30.720 --> 23:35.760\n and then to take that idea and just push it all the way to the limit and think, OK, where does\n\n23:35.760 --> 23:40.160\n this go? Where is this going to take us next? And if you're a deep believer in the power of ideas,\n\n23:40.720 --> 23:46.320\n then it's really natural to believe that those ideas could be taken to the extreme and kill us.\n\n23:47.760 --> 23:52.720\n So I think, you know, his strength is also his undoing, because that doesn't mean it's true.\n\n23:52.720 --> 23:56.160\n Like, it doesn't mean that that has to happen, but it's natural for him to think that.\n\n23:56.720 --> 24:04.160\n So another way to phrase the way he thinks, and I find it very difficult to argue with that line\n\n24:04.160 --> 24:09.360\n of thinking. So Sam Harris is another person from neuroscience perspective that thinks like that\n\n24:09.920 --> 24:18.080\n is saying, well, is there something fundamental in the physics of the universe that prevents this\n\n24:18.080 --> 24:24.320\n from eventually happening? And Nick Bostrom thinks in the same way, that kind of zooming out, yeah,\n\n24:24.320 --> 24:32.400\n OK, we humans now are existing in this like time scale of minutes and days. And so our intuition\n\n24:32.400 --> 24:38.400\n is in this time scale of minutes, hours and days. But if you look at the span of human history,\n\n24:39.200 --> 24:47.520\n is there any reason you can't see this in 100 years? And like, is there something fundamental\n\n24:47.520 --> 24:52.320\n about the laws of physics that prevent this? And if it doesn't, then it eventually will happen\n\n24:52.320 --> 24:57.200\n or will we will destroy ourselves in some other way. And it's very difficult, I find,\n\n24:57.200 --> 25:01.280\n to actually argue against that. Yeah, me too.\n\n25:03.680 --> 25:11.600\n And not sound like. Not sound like you're just like rolling your eyes like I have like science\n\n25:11.600 --> 25:16.000\n fiction, we don't have to think about it, but even even worse than that, which is like, I don't have\n\n25:16.000 --> 25:20.400\n kids, but like I got to pick up my kids now like this. OK, I see there's more pressing short. Yeah,\n\n25:20.400 --> 25:25.440\n there's more pressing short term things that like stop over the next national crisis. We have much,\n\n25:25.440 --> 25:30.000\n much shorter things like now, especially this year, there's covid. So like any kind of discussion\n\n25:30.000 --> 25:37.520\n like that is like there's this, you know, this pressing things today is. And then so the Sam\n\n25:37.520 --> 25:45.200\n Harris argument, well, like any day the exponential singularity can can occur is very difficult to\n\n25:45.200 --> 25:50.160\n argue against. I mean, I don't know. But part of his story is also he's not going to put a date on\n\n25:50.160 --> 25:53.680\n it. It could be in a thousand years, it could be in a hundred years, it could be in two years. It's\n\n25:53.680 --> 25:58.560\n just that as long as we keep making this kind of progress, it's ultimately has to become a concern.\n\n25:59.680 --> 26:03.920\n I kind of am on board with that. But the thing that the piece that I feel like is missing from\n\n26:03.920 --> 26:09.600\n that that way of extrapolating from the moment that we're in, is that I believe that in the\n\n26:09.600 --> 26:14.560\n process of actually developing technology that can really get around in the world and really process\n\n26:14.560 --> 26:20.960\n and do things in the world in a sophisticated way, we're going to learn a lot about what that means,\n\n26:20.960 --> 26:23.600\n which that we don't know now because we don't know how to do this right now.\n\n26:24.240 --> 26:28.160\n If you believe that you can just turn on a deep learning network and eventually give it enough\n\n26:28.160 --> 26:32.320\n compute and eventually get there. Well, sure, that seems really scary because we won't we won't be\n\n26:32.320 --> 26:38.640\n in the loop at all. We won't we won't be helping to design or target these kinds of systems.\n\n26:38.640 --> 26:43.840\n But I don't I don't see that. That feels like it is against the laws of physics,\n\n26:43.840 --> 26:49.760\n because these systems need help. Right. They need they need to surpass the the the difficulty,\n\n26:49.760 --> 26:54.560\n the wall of complexity that happens in arranging something in the form that that will happen.\n\n26:55.520 --> 27:00.880\n Yeah, like I believe in evolution, like I believe that that that there's an argument. Right. So\n\n27:00.880 --> 27:04.400\n there's another argument, just to look at it from a different perspective, that people say,\n\n27:04.400 --> 27:10.000\n why don't believe in evolution? How could evolution? It's it's sort of like a random set of\n\n27:10.000 --> 27:15.680\n parts assemble themselves into a 747. And that could just never happen. So it's like,\n\n27:15.680 --> 27:20.480\n OK, that's maybe hard to argue against. But clearly, 747 do get assembled. They get assembled\n\n27:20.480 --> 27:26.480\n by us. Basically, the idea being that there's a process by which we will get to the point of\n\n27:26.480 --> 27:31.920\n making technology that has that kind of awareness. And in that process, we're going to learn a lot\n\n27:31.920 --> 27:37.760\n about that process and we'll have more ability to control it or to shape it or to build it in our\n\n27:37.760 --> 27:43.680\n own image. It's not something that is going to spring into existence like that 747. And we're\n\n27:43.680 --> 27:49.440\n just going to have to contend with it completely unprepared. That's very possible that in the\n\n27:49.440 --> 27:55.200\n context of the long arc of human history, it will, in fact, spring into existence.\n\n27:55.200 --> 28:02.640\n But that springing might take like if you look at nuclear weapons, like even 20 years is a springing\n\n28:02.640 --> 28:07.760\n in in the context of human history. And it's very possible, just like with nuclear weapons,\n\n28:07.760 --> 28:13.040\n that we could have I don't know what percentage you want to put at it, but the possibility could\n\n28:13.040 --> 28:17.520\n have knocked ourselves out. Yeah. The possibility of human beings destroying themselves in the 20th\n\n28:17.520 --> 28:23.200\n century with nuclear weapons. I don't know. You can if you really think through it, you could\n\n28:23.200 --> 28:28.400\n really put it close to, like, I don't know, 30, 40 percent, given like the certain moments of\n\n28:28.400 --> 28:37.680\n crisis that happen. So, like, I think one, like, fear in the shadows that's not being acknowledged\n\n28:38.240 --> 28:43.440\n is it's not so much the A.I. will run away is is that as it's running away,\n\n28:44.240 --> 28:52.080\n we won't have enough time to think through how to stop it. Right. Fast takeoff or FOOM. Yeah.\n\n28:52.080 --> 28:55.760\n I mean, my much bigger concern, I wonder what you think about it, which is\n\n28:55.760 --> 29:05.760\n we won't know it's happening. So I kind of think that there's an A.G.I. situation already happening\n\n29:05.760 --> 29:11.840\n with social media that our minds, our collective intelligence of human civilization is already\n\n29:11.840 --> 29:19.520\n being controlled by an algorithm. And like we're we're already super like the level of a collective\n\n29:19.520 --> 29:23.840\n intelligence, thanks to Wikipedia, people should donate to Wikipedia to feed the A.G.I.\n\n29:23.840 --> 29:30.320\n. Man, if we had a super intelligence that that was in line with Wikipedia's values,\n\n29:31.920 --> 29:36.160\n that it's a lot better than a lot of other things I could imagine. I trust Wikipedia more than I\n\n29:36.160 --> 29:41.520\n trust Facebook or YouTube as far as trying to do the right thing from a rational perspective.\n\n29:41.520 --> 29:45.120\n Yeah. Now, that's not where you were going. I understand that. But it does strike me that\n\n29:45.120 --> 29:51.200\n there's sort of smarter and less smart ways of exposing ourselves to each other on the Internet.\n\n29:51.200 --> 29:55.360\n Yeah. The interesting thing is that Wikipedia and social media have very different forces.\n\n29:55.360 --> 30:02.160\n You're right. I mean, Wikipedia, if A.G.I. was Wikipedia, it'd be just like this cranky, overly\n\n30:02.160 --> 30:07.920\n competent editor of articles. You know, there's something to that. But the social\n\n30:08.480 --> 30:16.240\n media aspect is not. So the vision of A.G.I. is as a separate system that's super intelligent.\n\n30:17.120 --> 30:20.880\n That's super intelligent. That's one key little thing. I mean, there's the paperclip argument\n\n30:20.880 --> 30:27.200\n that's super dumb, but super powerful systems. But with social media, you have a relatively like\n\n30:27.200 --> 30:35.040\n algorithms we may talk about today, very simple algorithms that when something Charles talks a\n\n30:35.040 --> 30:40.640\n lot about, which is interactive A.I., when they start like having at scale, like tiny little\n\n30:40.640 --> 30:45.200\n interactions with human beings, they can start controlling these human beings. So a single\n\n30:45.200 --> 30:51.040\n algorithm can control the minds of human beings slowly to what we might not realize. It could\n\n30:51.040 --> 30:56.960\n start wars. It could start. It could change the way we think about things. It feels like\n\n30:57.840 --> 31:03.680\n in the long arc of history, if I were to sort of zoom out from all the outrage and all the tension\n\n31:03.680 --> 31:11.840\n on social media, that it's progressing us towards better and better things. It feels like chaos and\n\n31:11.840 --> 31:17.040\n toxic and all that kind of stuff. It's chaos and toxic. Yeah. But it feels like actually\n\n31:17.040 --> 31:22.000\n the chaos and toxic is similar to the kind of debates we had from the founding of this country.\n\n31:22.000 --> 31:28.000\n You know, there was a civil war that happened over that period. And ultimately it was all about\n\n31:28.000 --> 31:33.280\n this tension of like something doesn't feel right about our implementation of the core values we\n\n31:33.280 --> 31:38.720\n hold as human beings. And they're constantly struggling with this. And that results in people\n\n31:38.720 --> 31:47.680\n calling each other, just being shady to each other on Twitter. But ultimately the algorithm is\n\n31:47.680 --> 31:51.760\n managing all that. And it feels like there's a possible future in which that algorithm\n\n31:53.120 --> 31:58.640\n controls us into the direction of self destruction and whatever that looks like.\n\n31:59.200 --> 32:05.200\n Yeah. So, all right. I do believe in the power of social media to screw us up royally. I do believe\n\n32:05.200 --> 32:12.160\n in the power of social media to benefit us too. I do think that we're in a, yeah, it's sort of\n\n32:12.160 --> 32:16.000\n almost got dropped on top of us. And now we're trying to, as a culture, figure out how to cope\n\n32:16.000 --> 32:22.560\n with it. There's a sense in which, I don't know, there's some arguments that say that, for example,\n\n32:23.600 --> 32:27.840\n I guess college age students now, late college age students now, people who were in middle school\n\n32:27.840 --> 32:34.720\n when social media started to really take off, may be really damaged. Like this may have really hurt\n\n32:34.720 --> 32:40.000\n their development in a way that we don't have all the implications of quite yet. That's the generation\n\n32:40.000 --> 32:46.880\n who, and I hate to make it somebody else's responsibility, but like they're the ones who\n\n32:46.880 --> 32:53.280\n can fix it. They're the ones who can figure out how do we keep the good of this kind of technology\n\n32:53.280 --> 33:01.920\n without letting it eat us alive. And if they're successful, we move on to the next phase, the next\n\n33:01.920 --> 33:06.080\n level of the game. If they're not successful, then yeah, then we're going to wreck each other. We're\n\n33:06.080 --> 33:11.360\n going to destroy society. So you're going to, in your old age, sit on a porch and watch the world\n\n33:11.360 --> 33:17.040\n burn because of the TikTok generation that... I believe, well, so this is my kid's age,\n\n33:17.040 --> 33:21.520\n right? And that's certainly my daughter's age. And she's very tapped in to social stuff, but she's\n\n33:21.520 --> 33:26.720\n also, she's trying to find that balance, right? Of participating in it and in getting the positives\n\n33:26.720 --> 33:33.120\n of it, but without letting it eat her alive. And I think sometimes she ventures, I hope she doesn't\n\n33:33.120 --> 33:39.440\n watch this. Sometimes I think she ventures a little too far and is consumed by it. And other\n\n33:39.440 --> 33:46.320\n times she gets a little distance. And if there's enough people like her out there, they're going to\n\n33:46.320 --> 33:52.960\n navigate this choppy waters. That's an interesting skill actually to develop. I talked to my dad\n\n33:52.960 --> 34:01.920\n about it. I've now, somehow this podcast in particular, but other reasons has received a\n\n34:01.920 --> 34:07.600\n little bit of attention. And with that, apparently in this world, even though I don't shut up about\n\n34:07.600 --> 34:15.040\n love and I'm just all about kindness, I have now a little mini army of trolls. It's kind of hilarious\n\n34:15.040 --> 34:23.920\n actually, but it also doesn't feel good, but it's a skill to learn to not look at that, like to\n\n34:23.920 --> 34:27.840\n moderate actually how much you look at that. The discussion I have with my dad, it's similar to,\n\n34:28.800 --> 34:33.840\n it doesn't have to be about trolls. It could be about checking email, which is like, if you're\n\n34:33.840 --> 34:39.840\n anticipating, you know, there's a, my dad runs a large Institute at Drexel University and there\n\n34:39.840 --> 34:45.120\n could be stressful like emails you're waiting, like there's drama of some kinds. And so like,\n\n34:45.680 --> 34:49.200\n there's a temptation to check the email. If you send an email and you kind of,\n\n34:49.200 --> 34:56.320\n and that pulls you in into, it doesn't feel good. And it's a skill that he actually complains that\n\n34:56.320 --> 35:00.880\n he hasn't learned. I mean, he grew up without it. So he hasn't learned the skill of how to\n\n35:01.440 --> 35:05.840\n shut off the internet and walk away. And I think young people, while they're also being\n\n35:05.840 --> 35:12.000\n quote unquote damaged by like, you know, being bullied online, all of those stories, which are\n\n35:12.000 --> 35:17.200\n very like horrific, you basically can't escape your bullies these days when you're growing up.\n\n35:17.200 --> 35:23.920\n But at the same time, they're also learning that skill of how to be able to shut off the,\n\n35:23.920 --> 35:29.040\n like disconnect with it, be able to laugh at it, not take it too seriously. It's fascinating. Like\n\n35:29.040 --> 35:32.320\n we're all trying to figure this out. Just like you said, it's been dropped on us and we're trying to\n\n35:32.320 --> 35:37.280\n figure it out. Yeah. I think that's really interesting. And I guess I've become a believer\n\n35:37.280 --> 35:42.720\n in the human design, which I feel like I don't completely understand. Like how do you make\n\n35:42.720 --> 35:48.960\n something as robust as us? Like we're so flawed in so many ways. And yet, and yet, you know,\n\n35:48.960 --> 35:56.720\n we dominate the planet and we do seem to manage to get ourselves out of scrapes eventually,\n\n35:57.680 --> 36:02.160\n not necessarily the most elegant possible way, but somehow we get, we get to the next step.\n\n36:02.160 --> 36:09.600\n And I don't know how I'd make a machine do that. Generally speaking, like if I train one of my\n\n36:09.600 --> 36:13.760\n reinforcement learning agents to play a video game and it works really hard on that first stage\n\n36:13.760 --> 36:16.880\n over and over and over again, and it makes it through, it succeeds on that first level.\n\n36:17.680 --> 36:21.520\n And then the new level comes and it's just like, okay, I'm back to the drawing board. And somehow\n\n36:21.520 --> 36:26.800\n humanity, we keep leveling up and then somehow managing to put together the skills necessary to\n\n36:26.800 --> 36:33.760\n achieve success, some semblance of success in that next level too. And, you know,\n\n36:33.760 --> 36:35.360\n I hope we can keep doing that.\n\n36:36.320 --> 36:42.880\n You mentioned reinforcement learning. So you've had a couple of years in the field. No, quite,\n\n36:42.880 --> 36:50.160\n you know, quite a few, quite a long career in artificial intelligence broadly, but reinforcement\n\n36:50.160 --> 36:57.760\n learning specifically, can you maybe give a hint about your sense of the history of the field?\n\n36:58.320 --> 37:04.560\n And in some ways it's changed with the advent of deep learning, but as a long roots, like how is it\n\n37:05.280 --> 37:09.840\n weaved in and out of your own life? How have you seen the community change or maybe the ideas that\n\n37:09.840 --> 37:16.080\n it's playing with change? I've had the privilege, the pleasure of being, of having almost a front\n\n37:16.080 --> 37:21.040\n row seat to a lot of this stuff. And it's been really, really fun and interesting. So when I was\n\n37:21.040 --> 37:28.560\n in college in the eighties, early eighties, the neural net thing was starting to happen.\n\n37:29.280 --> 37:34.000\n And I was taking a lot of psychology classes and a lot of computer science classes as a college\n\n37:34.000 --> 37:38.720\n student. And I thought, you know, something that can play tic tac toe and just like learn to get\n\n37:38.720 --> 37:43.440\n better at it. That ought to be a really easy thing. So I spent almost, almost all of my, what would\n\n37:43.440 --> 37:48.640\n have been vacations during college, like hacking on my home computer, trying to teach it how to\n\n37:48.640 --> 37:53.520\n play tic tac toe and programming language. Basic. Oh yeah. That's, that's, I was, I that's my first\n\n37:53.520 --> 37:57.760\n language. That's my native language. Is that when you first fell in love with computer science,\n\n37:57.760 --> 38:02.880\n just like programming basic on that? Uh, what was, what was the computer? Do you remember? I had,\n\n38:02.880 --> 38:08.000\n I had a TRS 80 model one before they were called model ones. Cause there was nothing else. Uh,\n\n38:08.000 --> 38:18.960\n I got my computer in 1979, uh, instead. So I was, I was, I would have been bar mitzvahed,\n\n38:18.960 --> 38:23.440\n but instead of having a big party that my parents threw on my behalf, they just got me a computer.\n\n38:23.440 --> 38:26.960\n Cause that's what I really, really, really wanted. I saw them in the, in the, in the mall and\n\n38:26.960 --> 38:32.080\n radio shack. And I thought, what, how are they doing that? I would try to stump them. I would\n\n38:32.080 --> 38:37.280\n give them math problems like one plus and then in parentheses, two plus one. And I would always get\n\n38:37.280 --> 38:42.640\n it right. I'm like, how do you know so much? Like I've had to go to algebra class for the last few\n\n38:42.640 --> 38:48.000\n years to learn this stuff and you just seem to know. So I was, I was, I was smitten and, uh,\n\n38:48.000 --> 38:55.520\n got a computer and I think ages 13 to 15. I have no memory of those years. I think I just was in\n\n38:55.520 --> 38:59.920\n my room with the computer, listening to Billy Joel, communing, possibly listening to the radio,\n\n38:59.920 --> 39:06.480\n listening to Billy Joel. That was the one album I had, uh, on vinyl at that time. And, um, and then\n\n39:06.480 --> 39:09.920\n I got it on cassette tape and that was really helpful because then I could play it. I didn't\n\n39:09.920 --> 39:16.320\n have to go down to my parents, wifi or hi fi sorry. Uh, and at age 15, I remember kind of\n\n39:16.320 --> 39:20.480\n walking out and like, okay, I'm ready to talk to people again. Like I've learned what I need to\n\n39:20.480 --> 39:26.240\n learn here. And, um, so yeah, so, so that was, that was my home computer. And so I went to college\n\n39:26.240 --> 39:30.400\n and I was like, oh, I'm totally going to study computer science. And I opted the college I chose\n\n39:30.400 --> 39:34.720\n specifically had a computer science major. The one that I really wanted the college I really wanted\n\n39:34.720 --> 39:41.840\n to go to didn't so bye bye to them. So I went to Yale, uh, Princeton would have been way more\n\n39:41.840 --> 39:45.760\n convenient and it was just beautiful campus and it was close enough to home. And I was really\n\n39:45.760 --> 39:50.240\n excited about Princeton. And I visited, I said, so computer science majors like, well, we have\n\n39:50.240 --> 39:55.920\n computer engineering. I'm like, Oh, I don't like that word engineering. I like computer science.\n\n39:55.920 --> 39:59.360\n I really, I want to do like, you're saying hardware and software. They're like, yeah.\n\n39:59.360 --> 40:02.240\n I'm like, I just want to do software. I couldn't care less about hardware. And you grew up in\n\n40:02.240 --> 40:07.280\n Philadelphia. I grew up outside Philly. Yeah. Yeah. Uh, so the, you know, local schools were\n\n40:07.280 --> 40:12.800\n like Penn and Drexel and, uh, temple. Like everyone in my family went to temple at least at\n\n40:12.800 --> 40:18.400\n one point in their lives, except for me. So yeah, Philly, Philly family, Yale had a computer science\n\n40:18.400 --> 40:22.560\n department. And that's when you, it's kind of interesting. You said eighties and neural\n\n40:22.560 --> 40:27.760\n networks. That's when the neural networks was a hot new thing or a hot thing period. Uh, so what\n\n40:27.760 --> 40:31.760\n is that in college when you first learned about neural networks or when she learned, like how did\n\n40:31.760 --> 40:36.960\n it was in a psychology class, not in a CS. Yeah. Was it psychology or cognitive science or like,\n\n40:36.960 --> 40:42.320\n do you remember like what context it was? Yeah. Yeah. Yeah. So, so I was a, I've always been a\n\n40:42.320 --> 40:47.600\n bit of a cognitive psychology groupie. So like I'm, I studied computer science, but I like,\n\n40:47.600 --> 40:52.640\n I like to hang around where the cognitive scientists are. Cause I don't know brains, man.\n\n40:52.640 --> 40:57.920\n They're like, they're wacky. Cool. And they have a bigger picture view of things. They're a little\n\n40:57.920 --> 41:03.120\n less engineering. I would say they're more, they're more interested in the nature of cognition and\n\n41:03.120 --> 41:07.440\n intelligence and perception and how like the vision system work. Like they're asking always\n\n41:07.440 --> 41:12.880\n bigger questions. Now with the deep learning community there, I think more, there's a lot of\n\n41:12.880 --> 41:21.920\n intersections, but I do find that the neuroscience folks actually in cognitive psychology, cognitive\n\n41:21.920 --> 41:27.760\n science folks are starting to learn how to program, how to use neural, artificial neural networks.\n\n41:27.760 --> 41:31.840\n And they are actually approaching problems in like totally new, interesting ways. It's fun to\n\n41:31.840 --> 41:37.200\n watch that grad students from those departments, like approach a problem of machine learning.\n\n41:37.200 --> 41:40.640\n Right. They come in with a different perspective. Yeah. They don't care about like your\n\n41:40.640 --> 41:47.440\n image net data set or whatever they want, like to understand the, the, the, like the basic\n\n41:47.440 --> 41:53.760\n mechanisms at the, at the neuronal level and the functional level of intelligence. It's kind of,\n\n41:53.760 --> 41:58.720\n it's kind of cool to see them work, but yeah. Okay. So you always love, you're always a groupie\n\n41:58.720 --> 42:04.800\n of cognitive psychology. Yeah. Yeah. And so, so it was in a class by Richard Garrig. He was kind of\n\n42:04.800 --> 42:10.560\n like my favorite psych professor in college. And I took like three different classes with him\n\n42:11.600 --> 42:15.840\n and yeah. So they were talking specifically the class, I think was kind of a,\n\n42:17.440 --> 42:22.560\n there was a big paper that was written by Steven Pinker and Prince. I don't, I'm blanking on\n\n42:22.560 --> 42:28.480\n Prince's first name, but Prince and Pinker and Prince, they wrote kind of a, they were at that\n\n42:28.480 --> 42:36.240\n time kind of like, ah, I'm blanking on the names of the current people. The cognitive scientists\n\n42:36.240 --> 42:44.720\n who are complaining a lot about deep networks. Oh, Gary, Gary Marcus, Marcus and who else? I mean,\n\n42:44.720 --> 42:49.280\n there's a few, but Gary, Gary's the most feisty. Sure. Gary's very feisty. And with this, with his\n\n42:49.280 --> 42:52.880\n coauthor, they, they, you know, they're kind of doing these kinds of take downs where they say,\n\n42:52.880 --> 42:56.960\n okay, well, yeah, it does all these amazing, amazing things, but here's a shortcoming. Here's\n\n42:56.960 --> 43:00.960\n a shortcoming. Here's a shortcoming. And so the Pinker Prince paper is kind of like the,\n\n43:01.600 --> 43:07.360\n that generation's version of Marcus and Davis, right? Where they're, they're trained as cognitive\n\n43:07.360 --> 43:12.480\n scientists, but they're looking skeptically at the results in the, in the artificial intelligence,\n\n43:12.480 --> 43:16.720\n neural net kind of world and saying, yeah, it can do this and this and this, but low,\n\n43:16.720 --> 43:20.640\n it can't do that. And it can't do that. And it can't do that maybe in principle or maybe just\n\n43:20.640 --> 43:26.000\n in practice at this point. But, but the fact of the matter is you're, you've narrowed your focus\n\n43:26.000 --> 43:30.720\n too far to be impressed. You know, you're impressed with the things within that circle,\n\n43:30.720 --> 43:34.800\n but you need to broaden that circle a little bit. You need to look at a wider set of problems.\n\n43:34.800 --> 43:40.720\n And so, so we had, so I was in this seminar in college that was basically a close reading of\n\n43:40.720 --> 43:46.720\n the Pinker Prince paper, which was like really thick. There was a lot going on in there. And,\n\n43:47.920 --> 43:51.120\n and it, you know, and it talked about the reinforcement learning idea a little bit.\n\n43:51.120 --> 43:55.120\n I'm like, oh, that sounds really cool because behavior is what is really interesting to me\n\n43:55.120 --> 44:00.640\n about psychology anyway. So making programs that, I mean, programs are things that behave.\n\n44:00.640 --> 44:04.640\n People are things that behave. Like I want to make learning that learns to behave.\n\n44:05.360 --> 44:09.760\n And which way was reinforcement learning presented? Is this talking about human and\n\n44:09.760 --> 44:12.960\n animal behavior or are we talking about actual mathematical construct?\n\n44:12.960 --> 44:17.760\n Ah, that's right. So that's a good question. Right. So this is, I think it wasn't actually\n\n44:17.760 --> 44:22.000\n talked about as behavior in the paper that I was reading. I think that it just talked about\n\n44:22.000 --> 44:27.120\n learning. And to me, learning is about learning to behave, but really neural nets at that point\n\n44:27.120 --> 44:31.360\n were about learning like supervised learning. So learning to produce outputs from inputs.\n\n44:31.360 --> 44:36.800\n So I kind of tried to invent reinforcement learning. When I graduated, I joined a research\n\n44:36.800 --> 44:42.240\n group at Bellcore, which had spun out of Bell Labs recently at that time because of the divestiture\n\n44:42.240 --> 44:49.840\n of the long distance and local phone service in the 1980s, 1984. And I was in a group with\n\n44:50.400 --> 44:56.240\n Dave Ackley, who was the first author of the Boltzmann machine paper. So the very first neural\n\n44:56.240 --> 45:02.000\n net paper that could handle XOR, right? So XOR sort of killed neural nets. The very first,\n\n45:02.000 --> 45:10.320\n the zero with the first winter. Yeah. Um, the, the perceptrons paper and Hinton along with his\n\n45:10.320 --> 45:14.480\n student, Dave Ackley, and I think there was other authors as well showed that no, no, no,\n\n45:14.480 --> 45:19.600\n with Boltzmann machines, we can actually learn nonlinear concepts. And so everything's back on\n\n45:19.600 --> 45:24.240\n the table again. And that kind of started that second wave of neural networks. So Dave Ackley\n\n45:24.240 --> 45:30.320\n was, he became my mentor at, at Bellcore and we talked a lot about learning and life and\n\n45:30.320 --> 45:34.720\n computation and how all these things fit together. Now Dave and I have a podcast together. So,\n\n45:35.440 --> 45:42.320\n so I get to kind of enjoy that sort of his, his perspective once again, even, even all these years\n\n45:42.320 --> 45:48.240\n later. And so I said, so I said, I was really interested in learning, but in the concept of\n\n45:48.240 --> 45:52.640\n behavior and he's like, oh, well that's reinforcement learning here. And he gave me\n\n45:52.640 --> 45:58.880\n Rich Sutton's 1984 TD paper. So I read that paper. I honestly didn't get all of it,\n\n45:58.880 --> 46:04.000\n but I got the idea. I got that they were using, that he was using ideas that I was familiar with\n\n46:04.000 --> 46:09.920\n in the context of neural nets and, and like sort of back prop. But with this idea of making\n\n46:09.920 --> 46:13.200\n predictions over time, I'm like, this is so interesting, but I don't really get all the\n\n46:13.200 --> 46:17.920\n details I said to Dave. And Dave said, oh, well, why don't we have him come and give a talk?\n\n46:18.560 --> 46:23.040\n And I was like, wait, what, you can do that? Like, these are real people. I thought they\n\n46:23.040 --> 46:28.240\n were just words. I thought it was just like ideas that somehow magically seeped into paper. He's\n\n46:28.240 --> 46:35.680\n like, no, I, I, I know Rich like, we'll just have him come down and he'll give a talk. And so I was,\n\n46:35.680 --> 46:41.440\n you know, my mind was blown. And so Rich came and he gave a talk at Bellcore and he talked about\n\n46:41.440 --> 46:48.880\n what he was super excited, which was they had just figured out at the time Q learning. So Watkins\n\n46:48.880 --> 46:55.760\n had visited the Rich Sutton's lab at, at UMass or Andy Bartow's lab that Rich was a part of.\n\n46:55.760 --> 47:00.560\n And, um, he was really excited about this because it resolved a whole bunch of problems that he\n\n47:00.560 --> 47:05.040\n didn't know how to resolve in the, in the earlier paper. And so, um,\n\n47:05.040 --> 47:09.200\n For people who don't know TD, temporal difference, these are all just algorithms\n\n47:09.200 --> 47:10.320\n for reinforcement learning.\n\n47:10.320 --> 47:15.520\n Right. And TD, temporal difference in particular is about making predictions over time. And you can\n\n47:15.520 --> 47:19.840\n try to use it for making decisions, right? Cause if you can predict how good a future action or an\n\n47:19.840 --> 47:24.960\n action outcomes will be in the future, you can choose one that has better and, or, but the thing\n\n47:24.960 --> 47:29.040\n that's really cool about Q learning is it was off policy, which meant that you could actually be\n\n47:29.040 --> 47:33.840\n learning about the environment and what the value of different actions would be while actually\n\n47:33.840 --> 47:38.160\n figuring out how to behave optimally. So that was a revelation.\n\n47:38.160 --> 47:41.280\n Yeah. And the proof of that is kind of interesting. I mean, that's really surprising\n\n47:41.280 --> 47:46.400\n to me when I first read that paper. I mean, it's, it's, it's, it's, it's, it's, it's, it's,\n\n47:46.400 --> 47:51.840\n it's, it's, it's, it's, it's, it's, it's, it's, it's, it's, it's, it's, it's, it's, it's, it's, it's,\n\n47:51.840 --> 47:55.840\n it's interesting. I mean, that's really surprising to me when I first read that and then in Richard,\n\n47:55.840 --> 48:01.120\n Rich Sutton's book on the matter, it's, it's kind of a beautiful that a single equation can\n\n48:01.120 --> 48:06.160\n capture all one line of code and like, you can learn anything. Yeah. Like enough time.\n\n48:06.160 --> 48:13.600\n So equation and code, you're right. Like you can the code that you can arguably, at least\n\n48:13.600 --> 48:17.180\n if you like squint your eyes can say,\n\n48:17.180 --> 48:21.880\n this is all of intelligence is that you can implement\n\n48:21.880 --> 48:22.720\n that in a single one.\n\n48:22.720 --> 48:26.720\n I think I started with Lisp, which is a shout out to Lisp\n\n48:26.720 --> 48:29.860\n with like a single line of code, key piece of code,\n\n48:29.860 --> 48:32.200\n maybe a couple that you could do that.\n\n48:32.200 --> 48:33.480\n It's kind of magical.\n\n48:33.480 --> 48:37.040\n It's feels too good to be true.\n\n48:37.040 --> 48:38.400\n Well, and it sort of is.\n\n48:38.400 --> 48:40.360\n Yeah, kind of.\n\n48:40.360 --> 48:41.980\n It seems to require an awful lot\n\n48:41.980 --> 48:43.400\n of extra stuff supporting it.\n\n48:43.400 --> 48:46.500\n But nonetheless, the idea is really good.\n\n48:46.500 --> 48:50.480\n And as far as we know, it is a very reasonable way\n\n48:50.480 --> 48:52.480\n of trying to create adaptive behavior,\n\n48:52.480 --> 48:55.460\n behavior that gets better at something over time.\n\n48:56.840 --> 49:00.240\n Did you find the idea of optimal at all compelling\n\n49:00.240 --> 49:02.040\n that you could prove that it's optimal?\n\n49:02.040 --> 49:04.920\n So like one part of computer science\n\n49:04.920 --> 49:08.240\n that it makes people feel warm and fuzzy inside\n\n49:08.240 --> 49:10.440\n is when you can prove something like\n\n49:10.440 --> 49:13.000\n that a sorting algorithm worst case runs\n\n49:13.000 --> 49:16.220\n and N log N, and it makes everybody feel so good.\n\n49:16.220 --> 49:18.200\n Even though in reality, it doesn't really matter\n\n49:18.200 --> 49:20.080\n what the worst case is, what matters is like,\n\n49:20.080 --> 49:22.500\n does this thing actually work in practice\n\n49:22.500 --> 49:26.000\n on this particular actual set of data that I enjoy?\n\n49:26.000 --> 49:26.840\n Did you?\n\n49:26.840 --> 49:29.880\n So here's a place where I have maybe a strong opinion,\n\n49:29.880 --> 49:34.040\n which is like, you're right, of course, but no, no.\n\n49:34.040 --> 49:37.760\n Like, so what makes worst case so great, right?\n\n49:37.760 --> 49:39.520\n If you have a worst case analysis so great\n\n49:39.520 --> 49:41.040\n is that you get modularity.\n\n49:41.040 --> 49:44.320\n You can take that thing and plug it into another thing\n\n49:44.320 --> 49:47.400\n and still have some understanding of what's gonna happen\n\n49:47.400 --> 49:49.320\n when you click them together, right?\n\n49:49.320 --> 49:51.600\n If it just works well in practice, in other words,\n\n49:51.600 --> 49:54.640\n with respect to some distribution that you care about,\n\n49:54.640 --> 49:56.300\n when you go plug it into another thing,\n\n49:56.300 --> 49:58.560\n that distribution can shift, it can change,\n\n49:58.560 --> 50:00.480\n and your thing may not work well anymore.\n\n50:00.480 --> 50:02.620\n And you want it to, and you wish it does,\n\n50:02.620 --> 50:04.960\n and you hope that it will, but it might not,\n\n50:04.960 --> 50:06.560\n and then, ah.\n\n50:06.560 --> 50:11.080\n So you're saying you don't like machine learning.\n\n50:13.220 --> 50:15.680\n But we have some positive theoretical results\n\n50:15.680 --> 50:16.600\n for these things.\n\n50:17.680 --> 50:20.460\n You can come back at me with,\n\n50:20.460 --> 50:21.520\n yeah, but they're really weak,\n\n50:21.520 --> 50:22.960\n and yeah, they're really weak.\n\n50:22.960 --> 50:25.520\n And you can even say that sorting algorithms,\n\n50:25.520 --> 50:27.200\n like if you do the optimal sorting algorithm,\n\n50:27.200 --> 50:29.000\n it's not really the one that you want,\n\n50:30.000 --> 50:31.860\n and that might be true as well.\n\n50:31.860 --> 50:34.200\n But it is, the modularity is a really powerful statement.\n\n50:34.200 --> 50:35.040\n I really like that.\n\n50:35.040 --> 50:36.880\n If you're an engineer, you can then assemble\n\n50:36.880 --> 50:39.240\n different things, you can count on them to be,\n\n50:39.240 --> 50:42.040\n I mean, it's interesting.\n\n50:42.040 --> 50:45.280\n It's a balance, like with everything else in life,\n\n50:45.280 --> 50:47.300\n you don't want to get too obsessed.\n\n50:47.300 --> 50:48.760\n I mean, this is what computer scientists do,\n\n50:48.760 --> 50:51.440\n which they tend to get obsessed,\n\n50:51.440 --> 50:53.560\n and they overoptimize things,\n\n50:53.560 --> 50:56.560\n or they start by optimizing, and then they overoptimize.\n\n50:56.560 --> 51:00.960\n So it's easy to get really granular about this thing,\n\n51:00.960 --> 51:05.960\n but like the step from an n squared to an n log n\n\n51:06.160 --> 51:10.480\n sorting algorithm is a big leap for most real world systems.\n\n51:10.480 --> 51:13.560\n No matter what the actual behavior of the system is,\n\n51:13.560 --> 51:14.760\n that's a big leap.\n\n51:14.760 --> 51:17.400\n And the same can probably be said\n\n51:17.400 --> 51:20.800\n for other kind of first leaps\n\n51:20.800 --> 51:22.380\n that you would take on a particular problem.\n\n51:22.380 --> 51:25.680\n Like it's picking the low hanging fruit,\n\n51:25.680 --> 51:29.120\n or whatever the equivalent of doing the,\n\n51:29.120 --> 51:32.560\n not the dumbest thing, but the next to the dumbest thing.\n\n51:32.560 --> 51:34.760\n Picking the most delicious reachable fruit.\n\n51:34.760 --> 51:36.440\n Yeah, most delicious reachable fruit.\n\n51:36.440 --> 51:38.920\n I don't know why that's not a saying.\n\n51:38.920 --> 51:39.960\n Yeah.\n\n51:39.960 --> 51:44.000\n Okay, so then this is the 80s,\n\n51:44.000 --> 51:47.680\n and this kind of idea starts to percolate of learning.\n\n51:47.680 --> 51:50.680\n At that point, I got to meet Rich Sutton,\n\n51:50.680 --> 51:52.240\n so everything was sort of downhill from there,\n\n51:52.240 --> 51:55.280\n and that was really the pinnacle of everything.\n\n51:55.280 --> 51:58.020\n But then I felt like I was kind of on the inside.\n\n51:58.020 --> 52:00.080\n So then as interesting results were happening,\n\n52:00.080 --> 52:03.560\n I could like check in with Rich or with Jerry Tesaro,\n\n52:03.560 --> 52:06.920\n who had a huge impact on kind of early thinking\n\n52:06.920 --> 52:10.200\n in temporal difference learning and reinforcement learning\n\n52:10.200 --> 52:11.700\n and showed that you could do,\n\n52:11.700 --> 52:12.720\n you could solve problems\n\n52:12.720 --> 52:15.080\n that we didn't know how to solve any other way.\n\n52:16.120 --> 52:17.240\n And so that was really cool.\n\n52:17.240 --> 52:18.780\n So as good things were happening,\n\n52:18.780 --> 52:20.720\n I would hear about it from either the people\n\n52:20.720 --> 52:21.560\n who were doing it,\n\n52:21.560 --> 52:23.080\n or the people who were talking to the people\n\n52:23.080 --> 52:23.920\n who were doing it.\n\n52:23.920 --> 52:25.800\n And so I was able to track things pretty well\n\n52:25.800 --> 52:28.240\n through the 90s.\n\n52:28.240 --> 52:32.000\n So what wasn't most of the excitement\n\n52:32.000 --> 52:34.640\n on reinforcement learning in the 90s era\n\n52:34.640 --> 52:37.100\n with, what is it, TD Gamma?\n\n52:37.100 --> 52:40.560\n Like what's the role of these kind of little\n\n52:40.560 --> 52:43.360\n like fun game playing things and breakthroughs\n\n52:43.360 --> 52:46.840\n about exciting the community?\n\n52:46.840 --> 52:48.720\n Was that, like what were your,\n\n52:48.720 --> 52:50.720\n because you've also built across,\n\n52:50.720 --> 52:55.720\n or part of building across a puzzle solver,\n\n52:56.680 --> 53:00.000\n solving program called proverb.\n\n53:00.000 --> 53:05.000\n So you were interested in this as a problem,\n\n53:05.600 --> 53:09.660\n like in forming, using games to understand\n\n53:09.660 --> 53:12.480\n how to build intelligence systems.\n\n53:12.480 --> 53:14.240\n So like, what did you think about TD Gamma?\n\n53:14.240 --> 53:16.560\n Like what did you think about that whole thing in the 90s?\n\n53:16.560 --> 53:19.000\n Yeah, I mean, I found the TD Gamma result\n\n53:19.000 --> 53:20.320\n really just remarkable.\n\n53:20.320 --> 53:22.280\n So I had known about some of Jerry's stuff\n\n53:22.280 --> 53:24.840\n before he did TD Gamma and he did a system,\n\n53:24.840 --> 53:27.840\n just more vanilla, well, not entirely vanilla,\n\n53:27.840 --> 53:31.320\n but a more classical back proppy kind of network\n\n53:31.320 --> 53:32.720\n for playing backgammon,\n\n53:32.720 --> 53:35.200\n where he was training it on expert moves.\n\n53:35.200 --> 53:37.280\n So it was kind of supervised,\n\n53:37.280 --> 53:41.100\n but the way that it worked was not to mimic the actions,\n\n53:41.100 --> 53:44.040\n but to learn internally an evaluation function.\n\n53:44.040 --> 53:47.440\n So to learn, well, if the expert chose this over this,\n\n53:47.440 --> 53:50.480\n that must mean that the expert values this more than this.\n\n53:50.480 --> 53:52.280\n And so let me adjust my weights to make it\n\n53:52.280 --> 53:54.760\n so that the network evaluates this\n\n53:54.760 --> 53:56.240\n as being better than this.\n\n53:56.240 --> 53:59.940\n So it could learn from human preferences,\n\n53:59.940 --> 54:02.080\n it could learn its own preferences.\n\n54:02.080 --> 54:04.480\n And then when he took the step from that\n\n54:04.480 --> 54:06.520\n to actually doing it\n\n54:06.520 --> 54:08.580\n as a full on reinforcement learning problem,\n\n54:08.580 --> 54:10.080\n where you didn't need a trainer,\n\n54:10.080 --> 54:13.840\n you could just let it play, that was remarkable, right?\n\n54:13.840 --> 54:17.920\n And so I think as humans often do,\n\n54:17.920 --> 54:20.960\n as we've done in the recent past as well,\n\n54:20.960 --> 54:22.000\n people extrapolate.\n\n54:22.000 --> 54:23.460\n It's like, oh, well, if you can do that,\n\n54:23.460 --> 54:24.960\n which is obviously very hard,\n\n54:24.960 --> 54:27.960\n then obviously you could do all these other problems\n\n54:27.960 --> 54:31.560\n that we wanna solve that we know are also really hard.\n\n54:31.560 --> 54:35.320\n And it turned out very few of them ended up being practical,\n\n54:35.320 --> 54:38.000\n partly because I think neural nets,\n\n54:38.000 --> 54:39.100\n certainly at the time,\n\n54:39.100 --> 54:42.740\n were struggling to be consistent and reliable.\n\n54:42.740 --> 54:45.020\n And so training them in a reinforcement learning setting\n\n54:45.020 --> 54:46.720\n was a bit of a mess.\n\n54:46.720 --> 54:50.120\n I had, I don't know, generation after generation\n\n54:50.120 --> 54:51.880\n of like master students\n\n54:51.880 --> 54:55.700\n who wanted to do value function approximation,\n\n54:55.700 --> 54:59.380\n basically reinforcement learning with neural nets.\n\n54:59.380 --> 55:03.620\n And over and over and over again, we were failing.\n\n55:03.620 --> 55:06.160\n We couldn't get the good results that Jerry Tesaro got.\n\n55:06.160 --> 55:09.680\n I now believe that Jerry is a neural net whisperer.\n\n55:09.680 --> 55:14.080\n He has a particular ability to get neural networks\n\n55:14.080 --> 55:18.040\n to do things that other people would find impossible.\n\n55:18.040 --> 55:19.640\n And it's not the technology,\n\n55:19.640 --> 55:22.700\n it's the technology and Jerry together.\n\n55:22.700 --> 55:27.200\n Which I think speaks to the role of the human expert\n\n55:27.200 --> 55:28.760\n in the process of machine learning.\n\n55:28.760 --> 55:30.060\n Right, it's so easy.\n\n55:30.060 --> 55:32.860\n We're so drawn to the idea that it's the technology\n\n55:32.860 --> 55:36.000\n that is where the power is coming from\n\n55:36.000 --> 55:38.000\n that I think we lose sight of the fact\n\n55:38.000 --> 55:39.440\n that sometimes you need a really good,\n\n55:39.440 --> 55:40.800\n just like, I mean, no one would think,\n\n55:40.800 --> 55:42.240\n hey, here's this great piece of software.\n\n55:42.240 --> 55:44.800\n Here's like, I don't know, GNU Emacs or whatever.\n\n55:44.800 --> 55:48.380\n And doesn't that prove that computers are super powerful\n\n55:48.380 --> 55:49.960\n and basically gonna take over the world?\n\n55:49.960 --> 55:52.640\n It's like, no, Stalman is a hell of a hacker, right?\n\n55:52.640 --> 55:55.880\n So he was able to make the code do these amazing things.\n\n55:55.880 --> 55:57.520\n He couldn't have done it without the computer,\n\n55:57.520 --> 55:59.160\n but the computer couldn't have done it without him.\n\n55:59.160 --> 56:02.360\n And so I think people discount the role of people\n\n56:02.360 --> 56:07.360\n like Jerry who have just a particular set of skills.\n\n56:07.360 --> 56:10.620\n On that topic, by the way, as a small side note,\n\n56:10.620 --> 56:14.620\n I tweeted Emacs is greater than Vim yesterday\n\n56:14.620 --> 56:18.020\n and deleted the tweet 10 minutes later\n\n56:18.020 --> 56:21.860\n when I realized it started a war.\n\n56:21.860 --> 56:24.340\n I was like, oh, I was just kidding.\n\n56:24.340 --> 56:29.340\n I was just being, and I'm gonna walk back and forth.\n\n56:29.340 --> 56:30.980\n So people still feel passionately\n\n56:30.980 --> 56:32.940\n about that particular piece of good stuff.\n\n56:32.940 --> 56:33.780\n Yeah, I don't get that\n\n56:33.780 --> 56:37.380\n because Emacs is clearly so much better, I don't understand.\n\n56:37.380 --> 56:38.220\n But why do I say that?\n\n56:38.220 --> 56:43.220\n Because I spent a block of time in the 80s\n\n56:43.220 --> 56:46.180\n making my fingers know the Emacs keys\n\n56:46.180 --> 56:49.060\n and now that's part of the thought process for me.\n\n56:49.060 --> 56:51.460\n Like I need to express, and if you take that,\n\n56:51.460 --> 56:54.620\n if you take my Emacs key bindings away, I become...\n\n56:57.660 --> 56:58.820\n I can't express myself.\n\n56:58.820 --> 56:59.660\n I'm the same way with the,\n\n56:59.660 --> 57:01.060\n I don't know if you know what it is,\n\n57:01.060 --> 57:05.100\n but it's a Kinesis keyboard, which is this butt shaped keyboard.\n\n57:05.100 --> 57:06.940\n Yes, I've seen them.\n\n57:06.940 --> 57:10.540\n They're very, I don't know, sexy, elegant?\n\n57:10.540 --> 57:11.700\n They're just beautiful.\n\n57:11.700 --> 57:14.460\n Yeah, they're gorgeous, way too expensive.\n\n57:14.460 --> 57:19.220\n But the problem with them, similar with Emacs,\n\n57:19.220 --> 57:22.700\n is once you learn to use it.\n\n57:23.860 --> 57:24.860\n It's harder to use other things.\n\n57:24.860 --> 57:26.100\n It's hard to use other things.\n\n57:26.100 --> 57:29.060\n There's this absurd thing where I have like small, elegant,\n\n57:29.060 --> 57:31.500\n lightweight, beautiful little laptops\n\n57:31.500 --> 57:33.180\n and I'm sitting there in a coffee shop\n\n57:33.180 --> 57:36.340\n with a giant Kinesis keyboard and a sexy little laptop.\n\n57:36.340 --> 57:40.460\n It's absurd, but I used to feel bad about it,\n\n57:40.460 --> 57:42.900\n but at the same time, you just kind of have to,\n\n57:42.900 --> 57:44.780\n sometimes it's back to the Billy Joel thing.\n\n57:44.780 --> 57:47.220\n You just have to throw that Billy Joel record\n\n57:47.220 --> 57:51.380\n and throw Taylor Swift and Justin Bieber to the wind.\n\n57:51.380 --> 57:52.220\n So...\n\n57:52.220 --> 57:54.820\n See, but I like them now because again,\n\n57:54.820 --> 57:55.740\n I have no musical taste.\n\n57:55.740 --> 57:57.900\n Like now that I've heard Justin Bieber enough,\n\n57:57.900 --> 57:59.980\n I'm like, I really like his songs.\n\n57:59.980 --> 58:02.980\n And Taylor Swift, not only do I like her songs,\n\n58:02.980 --> 58:04.820\n but my daughter's convinced that she's a genius.\n\n58:04.820 --> 58:07.020\n And so now I basically have signed onto that.\n\n58:07.020 --> 58:08.100\n So...\n\n58:08.100 --> 58:10.060\n So yeah, that speaks to the,\n\n58:10.060 --> 58:11.700\n back to the robustness of the human brain.\n\n58:11.700 --> 58:13.300\n That speaks to the neuroplasticity\n\n58:13.300 --> 58:17.980\n that you can just like a mouse teach yourself to,\n\n58:17.980 --> 58:21.500\n or probably a dog teach yourself to enjoy Taylor Swift.\n\n58:21.500 --> 58:22.340\n I'll try it out.\n\n58:22.340 --> 58:23.660\n I don't know.\n\n58:23.660 --> 58:25.300\n I try, you know what?\n\n58:25.300 --> 58:28.060\n It has to do with just like acclimation, right?\n\n58:28.060 --> 58:29.660\n Just like you said, a couple of weeks.\n\n58:29.660 --> 58:30.500\n Yeah.\n\n58:30.500 --> 58:31.340\n That's an interesting experiment.\n\n58:31.340 --> 58:32.180\n I'll actually try that.\n\n58:32.180 --> 58:33.020\n Like I'll listen to it.\n\n58:33.020 --> 58:33.860\n That wasn't the intent of the experiment?\n\n58:33.860 --> 58:34.700\n Just like social media,\n\n58:34.700 --> 58:36.100\n it wasn't intended as an experiment\n\n58:36.100 --> 58:38.220\n to see what we can take as a society,\n\n58:38.220 --> 58:39.540\n but it turned out that way.\n\n58:39.540 --> 58:40.860\n I don't think I'll be the same person\n\n58:40.860 --> 58:43.300\n on the other side of the week listening to Taylor Swift,\n\n58:43.300 --> 58:44.140\n but let's try.\n\n58:44.140 --> 58:45.820\n No, it's more compartmentalized.\n\n58:45.820 --> 58:46.860\n Don't be so worried.\n\n58:46.860 --> 58:48.980\n Like it's, like I get that you can be worried,\n\n58:48.980 --> 58:49.820\n but don't be so worried\n\n58:49.820 --> 58:51.420\n because we compartmentalize really well.\n\n58:51.420 --> 58:53.860\n And so it won't bleed into other parts of your life.\n\n58:53.860 --> 58:55.340\n You won't start, I don't know,\n\n58:56.220 --> 58:57.260\n wearing red lipstick or whatever.\n\n58:57.260 --> 58:58.260\n Like it's fine.\n\n58:58.260 --> 58:59.100\n It's fine.\n\n58:59.100 --> 58:59.940\n It changed fashion and everything.\n\n58:59.940 --> 59:00.780\n It's fine.\n\n59:00.780 --> 59:01.620\n But you know what?\n\n59:01.620 --> 59:02.460\n The thing you have to watch out for\n\n59:02.460 --> 59:03.860\n is you'll walk into a coffee shop\n\n59:03.860 --> 59:05.180\n once we can do that again.\n\n59:05.180 --> 59:06.220\n And recognize the song?\n\n59:06.220 --> 59:07.060\n And you'll be, no,\n\n59:07.060 --> 59:09.220\n you won't know that you're singing along\n\n59:09.220 --> 59:11.540\n until everybody in the coffee shop is looking at you.\n\n59:11.540 --> 59:14.140\n And then you're like, that wasn't me.\n\n59:16.060 --> 59:17.140\n Yeah, that's the, you know,\n\n59:17.140 --> 59:18.300\n people are afraid of AGI.\n\n59:18.300 --> 59:21.020\n I'm afraid of the Taylor Swift.\n\n59:21.020 --> 59:22.300\n The Taylor Swift takeover.\n\n59:22.300 --> 59:26.940\n Yeah, and I mean, people should know that TD Gammon was,\n\n59:26.940 --> 59:28.300\n I get, would you call it,\n\n59:28.300 --> 59:31.300\n do you like the terminology of self play by any chance?\n\n59:31.300 --> 59:35.300\n So like systems that learn by playing themselves.\n\n59:35.300 --> 59:38.060\n Just, I don't know if it's the best word, but.\n\n59:38.060 --> 59:39.900\n So what's the problem with that term?\n\n59:41.180 --> 59:42.020\n I don't know.\n\n59:42.020 --> 59:43.540\n So it's like the big bang,\n\n59:43.540 --> 59:46.780\n like it's like talking to a serious physicist.\n\n59:46.780 --> 59:47.980\n Do you like the term big bang?\n\n59:47.980 --> 59:49.740\n And when it was early,\n\n59:49.740 --> 59:51.620\n I feel like it's the early days of self play.\n\n59:51.620 --> 59:53.220\n I don't know, maybe it was used previously,\n\n59:53.220 --> 59:57.660\n but I think it's been used by only a small group of people.\n\n59:57.660 --> 59:59.660\n And so like, I think we're still deciding\n\n59:59.660 --> 1:00:02.860\n is this ridiculously silly name a good name\n\n1:00:02.860 --> 1:00:05.860\n for potentially one of the most important concepts\n\n1:00:05.860 --> 1:00:07.140\n in artificial intelligence?\n\n1:00:07.140 --> 1:00:09.020\n Okay, it depends how broadly you apply the term.\n\n1:00:09.020 --> 1:00:12.980\n So I used the term in my 1996 PhD dissertation.\n\n1:00:12.980 --> 1:00:14.660\n Wow, the actual terms of self play.\n\n1:00:14.660 --> 1:00:18.540\n Yeah, because Tesoro's paper was something like\n\n1:00:18.540 --> 1:00:21.660\n training up an expert backgammon player through self play.\n\n1:00:21.660 --> 1:00:24.060\n So I think it was in the title of his paper.\n\n1:00:24.060 --> 1:00:27.140\n If not in the title, it was definitely a term that he used.\n\n1:00:27.140 --> 1:00:29.740\n There's another term that we got from that work is rollout.\n\n1:00:29.740 --> 1:00:32.020\n So I don't know if you, do you ever hear the term rollout?\n\n1:00:32.020 --> 1:00:35.180\n That's a backgammon term that has now applied\n\n1:00:35.180 --> 1:00:38.380\n generally in computers, well, at least in AI\n\n1:00:38.380 --> 1:00:39.700\n because of TD gammon.\n\n1:00:40.740 --> 1:00:41.580\n That's fascinating.\n\n1:00:41.580 --> 1:00:43.140\n So how is self play being used now?\n\n1:00:43.140 --> 1:00:44.380\n And like, why is it,\n\n1:00:44.380 --> 1:00:46.460\n does it feel like a more general powerful concept\n\n1:00:46.460 --> 1:00:47.860\n is sort of the idea of,\n\n1:00:47.860 --> 1:00:50.020\n well, the machine's just gonna teach itself to be smart.\n\n1:00:50.020 --> 1:00:53.740\n Yeah, so that's where maybe you can correct me,\n\n1:00:53.740 --> 1:00:56.740\n but that's where the continuation of the spirit\n\n1:00:56.740 --> 1:01:00.220\n and actually like literally the exact algorithms\n\n1:01:00.220 --> 1:01:03.980\n of TD gammon are applied by DeepMind and OpenAI\n\n1:01:03.980 --> 1:01:07.220\n to learn games that are a little bit more complex\n\n1:01:07.220 --> 1:01:09.060\n that when I was learning artificial intelligence,\n\n1:01:09.060 --> 1:01:10.780\n Go was presented to me\n\n1:01:10.780 --> 1:01:13.900\n with artificial intelligence, the modern approach.\n\n1:01:13.900 --> 1:01:16.180\n I don't know if they explicitly pointed to Go\n\n1:01:16.180 --> 1:01:20.900\n in those books as like unsolvable kind of thing,\n\n1:01:20.900 --> 1:01:24.340\n like implying that these approaches hit their limit\n\n1:01:24.340 --> 1:01:26.380\n in this, with these particular kind of games.\n\n1:01:26.380 --> 1:01:29.460\n So something, I don't remember if the book said it or not,\n\n1:01:29.460 --> 1:01:31.140\n but something in my head,\n\n1:01:31.140 --> 1:01:34.380\n or if it was the professors instilled in me the idea\n\n1:01:34.380 --> 1:01:37.060\n like this is the limits of artificial intelligence\n\n1:01:37.060 --> 1:01:38.300\n of the field.\n\n1:01:38.300 --> 1:01:40.780\n Like it instilled in me the idea\n\n1:01:40.780 --> 1:01:44.900\n that if we can create a system that can solve the game of Go\n\n1:01:44.900 --> 1:01:46.180\n we've achieved AGI.\n\n1:01:46.180 --> 1:01:49.580\n That was kind of, I didn't explicitly like say this,\n\n1:01:49.580 --> 1:01:51.180\n but that was the feeling.\n\n1:01:51.180 --> 1:01:54.140\n And so from, I was one of the people that it seemed magical\n\n1:01:54.140 --> 1:01:57.660\n when a learning system was able to beat\n\n1:01:59.340 --> 1:02:02.340\n a human world champion at the game of Go\n\n1:02:02.340 --> 1:02:06.740\n and even more so from that, that was AlphaGo,\n\n1:02:06.740 --> 1:02:08.380\n even more so with AlphaGo Zero\n\n1:02:08.380 --> 1:02:11.900\n than kind of renamed and advanced into AlphaZero\n\n1:02:11.900 --> 1:02:15.940\n beating a world champion or world class player\n\n1:02:16.940 --> 1:02:21.420\n without any supervised learning on expert games.\n\n1:02:21.420 --> 1:02:24.580\n We're doing only through by playing itself.\n\n1:02:24.580 --> 1:02:29.020\n So that is, I don't know what to make of it.\n\n1:02:29.020 --> 1:02:31.300\n I think it would be interesting to hear\n\n1:02:31.300 --> 1:02:35.180\n what your opinions are on just how exciting,\n\n1:02:35.180 --> 1:02:40.180\n surprising, profound, interesting, or boring\n\n1:02:40.180 --> 1:02:45.180\n the breakthrough performance of AlphaZero was.\n\n1:02:45.180 --> 1:02:48.380\n Okay, so AlphaGo knocked my socks off.\n\n1:02:48.380 --> 1:02:50.780\n That was so remarkable.\n\n1:02:50.780 --> 1:02:51.780\n Which aspect of it?\n\n1:02:52.940 --> 1:02:55.020\n That they got it to work,\n\n1:02:55.020 --> 1:02:57.540\n that they actually were able to leverage\n\n1:02:57.540 --> 1:02:58.980\n a whole bunch of different ideas,\n\n1:02:58.980 --> 1:03:01.060\n integrate them into one giant system.\n\n1:03:01.060 --> 1:03:04.220\n Just the software engineering aspect of it is mind blowing.\n\n1:03:04.220 --> 1:03:06.760\n I don't, I've never been a part of a program\n\n1:03:06.760 --> 1:03:09.660\n as complicated as the program that they built for that.\n\n1:03:09.660 --> 1:03:14.660\n And just the, like Jerry Tesaro is a neural net whisperer,\n\n1:03:14.660 --> 1:03:17.420\n like David Silver is a kind of neural net whisperer too.\n\n1:03:17.420 --> 1:03:19.300\n He was able to coax these networks\n\n1:03:19.300 --> 1:03:22.380\n and these new way out there architectures\n\n1:03:22.380 --> 1:03:25.980\n to do these, solve these problems that,\n\n1:03:25.980 --> 1:03:29.940\n as you said, when we were learning from AI,\n\n1:03:31.220 --> 1:03:32.780\n no one had an idea how to make it work.\n\n1:03:32.780 --> 1:03:35.780\n It was remarkable that these techniques\n\n1:03:35.780 --> 1:03:40.140\n that were so good at playing chess\n\n1:03:40.140 --> 1:03:42.020\n and that could beat the world champion in chess\n\n1:03:42.020 --> 1:03:46.660\n couldn't beat your typical Go playing teenager in Go.\n\n1:03:46.660 --> 1:03:49.740\n So the fact that in a very short number of years,\n\n1:03:49.740 --> 1:03:54.180\n we kind of ramped up to trouncing people in Go\n\n1:03:54.180 --> 1:03:55.980\n just blew me away.\n\n1:03:55.980 --> 1:03:58.500\n So you're kind of focusing on the engineering aspect,\n\n1:03:58.500 --> 1:04:00.060\n which is also very surprising.\n\n1:04:00.060 --> 1:04:02.580\n I mean, there's something different\n\n1:04:02.580 --> 1:04:05.260\n about large, well funded companies.\n\n1:04:05.260 --> 1:04:07.940\n I mean, there's a compute aspect to it too.\n\n1:04:07.940 --> 1:04:11.500\n Like that, of course, I mean, that's similar\n\n1:04:11.500 --> 1:04:14.300\n to Deep Blue, right, with IBM.\n\n1:04:14.300 --> 1:04:16.660\n Like there's something important to be learned\n\n1:04:16.660 --> 1:04:19.500\n and remembered about a large company\n\n1:04:19.500 --> 1:04:22.020\n taking the ideas that are already out there\n\n1:04:22.020 --> 1:04:26.180\n and investing a few million dollars into it or more.\n\n1:04:26.180 --> 1:04:29.820\n And so you're kind of saying the engineering\n\n1:04:29.820 --> 1:04:32.060\n is kind of fascinating, both on the,\n\n1:04:32.060 --> 1:04:35.300\n with AlphaGo is probably just gathering all the data,\n\n1:04:35.300 --> 1:04:38.860\n right, of the expert games, like organizing everything,\n\n1:04:38.860 --> 1:04:42.780\n actually doing distributed supervised learning.\n\n1:04:42.780 --> 1:04:47.780\n And to me, see the engineering I kind of took for granted,\n\n1:04:49.420 --> 1:04:53.540\n to me philosophically being able to persist\n\n1:04:55.100 --> 1:04:57.940\n in the face of like long odds,\n\n1:04:57.940 --> 1:05:00.180\n because it feels like for me,\n\n1:05:00.180 --> 1:05:02.260\n I would be one of the skeptical people in the room\n\n1:05:02.260 --> 1:05:05.140\n thinking that you can learn your way to beat Go.\n\n1:05:05.140 --> 1:05:08.500\n Like it sounded like, especially with David Silver,\n\n1:05:08.500 --> 1:05:11.780\n it sounded like David was not confident at all.\n\n1:05:11.780 --> 1:05:14.780\n So like it was, like not,\n\n1:05:15.780 --> 1:05:18.540\n it's funny how confidence works.\n\n1:05:18.540 --> 1:05:23.540\n It's like, you're not like cocky about it, like, but.\n\n1:05:24.860 --> 1:05:26.140\n Right, because if you're cocky about it,\n\n1:05:26.140 --> 1:05:28.660\n you kind of stop and stall and don't get anywhere.\n\n1:05:28.660 --> 1:05:31.620\n But there's like a hope that's unbreakable.\n\n1:05:31.620 --> 1:05:33.280\n Maybe that's better than confidence.\n\n1:05:33.280 --> 1:05:36.380\n It's a kind of wishful hope and a little dream.\n\n1:05:36.380 --> 1:05:38.980\n And you almost don't want to do anything else.\n\n1:05:38.980 --> 1:05:40.900\n You kind of keep doing it.\n\n1:05:40.900 --> 1:05:43.660\n That's, that seems to be the story and.\n\n1:05:43.660 --> 1:05:45.660\n But with enough skepticism that you're looking\n\n1:05:45.660 --> 1:05:48.420\n for where the problems are and fighting through them.\n\n1:05:48.420 --> 1:05:51.100\n Cause you know, there's gotta be a way out of this thing.\n\n1:05:51.100 --> 1:05:52.500\n And for him, it was probably,\n\n1:05:52.500 --> 1:05:55.980\n there's a bunch of little factors that come into play.\n\n1:05:55.980 --> 1:05:57.780\n It's funny how these stories just all come together.\n\n1:05:57.780 --> 1:06:00.660\n Like everything he did in his life came into play,\n\n1:06:00.660 --> 1:06:02.940\n which is like a love for video games\n\n1:06:02.940 --> 1:06:05.380\n and also a connection to,\n\n1:06:05.380 --> 1:06:09.020\n so the nineties had to happen with TD Gammon and so on.\n\n1:06:09.020 --> 1:06:10.900\n In some ways it's surprising,\n\n1:06:10.900 --> 1:06:13.700\n maybe you can provide some intuition to it\n\n1:06:13.700 --> 1:06:16.300\n that not much more than TD Gammon was done\n\n1:06:16.300 --> 1:06:19.840\n for quite a long time on the reinforcement learning front.\n\n1:06:19.840 --> 1:06:21.140\n Is that weird to you?\n\n1:06:21.140 --> 1:06:24.180\n I mean, like I said, the students who I worked with,\n\n1:06:24.180 --> 1:06:27.140\n we tried to get, basically apply that architecture\n\n1:06:27.140 --> 1:06:30.700\n to other problems and we consistently failed.\n\n1:06:30.700 --> 1:06:33.900\n There were a couple of really nice demonstrations\n\n1:06:33.900 --> 1:06:35.100\n that ended up being in the literature.\n\n1:06:35.100 --> 1:06:38.700\n There was a paper about controlling elevators, right?\n\n1:06:38.700 --> 1:06:42.260\n Where it's like, okay, can we modify the heuristic\n\n1:06:42.260 --> 1:06:43.620\n that elevators use for deciding,\n\n1:06:43.620 --> 1:06:46.160\n like a bank of elevators for deciding which floors\n\n1:06:46.160 --> 1:06:50.260\n we should be stopping on to maximize throughput essentially.\n\n1:06:50.260 --> 1:06:52.320\n And you can set that up as a reinforcement learning problem\n\n1:06:52.320 --> 1:06:55.580\n and you can have a neural net represent the value function\n\n1:06:55.580 --> 1:06:57.680\n so that it's taking where all the elevators,\n\n1:06:57.680 --> 1:07:00.580\n where the button pushes, you know, this high dimensional,\n\n1:07:00.580 --> 1:07:02.800\n well, at the time high dimensional input,\n\n1:07:03.700 --> 1:07:05.620\n you know, a couple of dozen dimensions\n\n1:07:05.620 --> 1:07:07.980\n and turn that into a prediction as to,\n\n1:07:07.980 --> 1:07:10.620\n oh, is it gonna be better if I stop at this floor or not?\n\n1:07:10.620 --> 1:07:13.460\n And ultimately it appeared as though\n\n1:07:13.460 --> 1:07:16.780\n for the standard simulation distribution\n\n1:07:16.780 --> 1:07:18.280\n for people trying to leave the building\n\n1:07:18.280 --> 1:07:19.300\n at the end of the day,\n\n1:07:19.300 --> 1:07:21.160\n that the neural net learned a better strategy\n\n1:07:21.160 --> 1:07:22.740\n than the standard one that's implemented\n\n1:07:22.740 --> 1:07:24.860\n in elevator controllers.\n\n1:07:24.860 --> 1:07:26.540\n So that was nice.\n\n1:07:26.540 --> 1:07:28.820\n There was some work that Satyendra Singh et al\n\n1:07:28.820 --> 1:07:33.200\n did on handoffs with cell phones,\n\n1:07:34.060 --> 1:07:36.680\n you know, deciding when should you hand off\n\n1:07:36.680 --> 1:07:38.100\n from this cell tower to this cell tower.\n\n1:07:38.100 --> 1:07:39.980\n Oh, okay, communication networks, yeah.\n\n1:07:39.980 --> 1:07:42.700\n Yeah, and so a couple of things\n\n1:07:42.700 --> 1:07:44.180\n seemed like they were really promising.\n\n1:07:44.180 --> 1:07:46.780\n None of them made it into production that I'm aware of.\n\n1:07:46.780 --> 1:07:48.420\n And neural nets as a whole started\n\n1:07:48.420 --> 1:07:50.300\n to kind of implode around then.\n\n1:07:50.300 --> 1:07:53.800\n And so there just wasn't a lot of air in the room\n\n1:07:53.800 --> 1:07:55.020\n for people to try to figure out,\n\n1:07:55.020 --> 1:07:58.420\n okay, how do we get this to work in the RL setting?\n\n1:07:58.420 --> 1:08:03.140\n And then they found their way back in 10 plus years.\n\n1:08:03.140 --> 1:08:05.180\n So you said AlphaGo was impressive,\n\n1:08:05.180 --> 1:08:06.540\n like it's a big spectacle.\n\n1:08:06.540 --> 1:08:07.860\n Is there, is that?\n\n1:08:07.860 --> 1:08:09.120\n Right, so then AlphaZero.\n\n1:08:09.120 --> 1:08:11.460\n So I think I may have a slightly different opinion\n\n1:08:11.460 --> 1:08:12.440\n on this than some people.\n\n1:08:12.440 --> 1:08:15.540\n So I talked to Satyendra Singh in particular about this.\n\n1:08:15.540 --> 1:08:18.400\n So Satyendra was like Rich Sutton,\n\n1:08:18.400 --> 1:08:19.660\n a student of Andy Bartow.\n\n1:08:19.660 --> 1:08:21.280\n So they came out of the same lab,\n\n1:08:21.280 --> 1:08:23.940\n very influential machine learning,\n\n1:08:23.940 --> 1:08:26.100\n reinforcement learning researcher.\n\n1:08:26.100 --> 1:08:28.860\n Now at DeepMind, as is Rich.\n\n1:08:29.900 --> 1:08:31.940\n Though different sites, the two of them.\n\n1:08:31.940 --> 1:08:33.020\n He's in Alberta.\n\n1:08:33.020 --> 1:08:36.340\n Rich is in Alberta and Satyendra would be in England,\n\n1:08:36.340 --> 1:08:39.620\n but I think he's in England from Michigan at the moment.\n\n1:08:39.620 --> 1:08:41.860\n But the, but he was, yes,\n\n1:08:41.860 --> 1:08:46.780\n he was much more impressed with AlphaGo Zero,\n\n1:08:46.780 --> 1:08:50.100\n which is didn't get a kind of a bootstrap\n\n1:08:50.100 --> 1:08:51.660\n in the beginning with human trained games.\n\n1:08:51.660 --> 1:08:53.300\n It just was purely self play.\n\n1:08:53.300 --> 1:08:55.740\n Though the first one AlphaGo\n\n1:08:55.740 --> 1:08:58.080\n was also a tremendous amount of self play, right?\n\n1:08:58.080 --> 1:09:01.060\n They started off, they kickstarted the action network\n\n1:09:01.060 --> 1:09:02.540\n that was making decisions,\n\n1:09:02.540 --> 1:09:04.460\n but then they trained it for a really long time\n\n1:09:04.460 --> 1:09:07.140\n using more traditional temporal difference methods.\n\n1:09:08.220 --> 1:09:09.860\n So as a result, I didn't,\n\n1:09:09.860 --> 1:09:11.860\n it didn't seem that different to me.\n\n1:09:11.860 --> 1:09:15.940\n Like, it seems like, yeah, why wouldn't that work?\n\n1:09:15.940 --> 1:09:17.780\n Like once you, once it works, it works.\n\n1:09:17.780 --> 1:09:21.420\n So what, but he found that removal\n\n1:09:21.420 --> 1:09:23.780\n of that extra information to be breathtaking.\n\n1:09:23.780 --> 1:09:25.940\n Like that's a game changer.\n\n1:09:25.940 --> 1:09:27.860\n To me, the first thing was more of a game changer.\n\n1:09:27.860 --> 1:09:29.420\n But the open question, I mean,\n\n1:09:29.420 --> 1:09:32.980\n I guess that's the assumption is the expert games\n\n1:09:32.980 --> 1:09:37.980\n might contain within them a humongous amount of information.\n\n1:09:39.180 --> 1:09:41.140\n But we know that it went beyond that, right?\n\n1:09:41.140 --> 1:09:43.740\n We know that it somehow got away from that information\n\n1:09:43.740 --> 1:09:45.140\n because it was learning strategies.\n\n1:09:45.140 --> 1:09:48.540\n I don't think AlphaGo is just better\n\n1:09:48.540 --> 1:09:50.260\n at implementing human strategies.\n\n1:09:50.260 --> 1:09:52.540\n I think it actually developed its own strategies\n\n1:09:52.540 --> 1:09:54.500\n that were more effective.\n\n1:09:54.500 --> 1:09:56.780\n And so from that perspective, okay, well,\n\n1:09:56.780 --> 1:10:00.220\n so it made at least one quantum leap\n\n1:10:00.220 --> 1:10:02.460\n in terms of strategic knowledge.\n\n1:10:02.460 --> 1:10:05.460\n Okay, so now maybe it makes three, like, okay.\n\n1:10:05.460 --> 1:10:07.540\n But that first one is the doozy, right?\n\n1:10:07.540 --> 1:10:11.660\n Getting it to work reliably and for the networks\n\n1:10:11.660 --> 1:10:13.500\n to hold onto the value well enough.\n\n1:10:13.500 --> 1:10:16.100\n Like that was a big step.\n\n1:10:16.100 --> 1:10:17.820\n Well, maybe you could speak to this\n\n1:10:17.820 --> 1:10:19.140\n on the reinforcement learning front.\n\n1:10:19.140 --> 1:10:24.140\n So starting from scratch and learning to do something,\n\n1:10:25.260 --> 1:10:29.140\n like the first like random behavior\n\n1:10:29.140 --> 1:10:34.140\n to like crappy behavior to like somewhat okay behavior.\n\n1:10:34.860 --> 1:10:39.860\n It's not obvious to me that that's not like impossible\n\n1:10:39.860 --> 1:10:41.420\n to take those steps.\n\n1:10:41.420 --> 1:10:43.900\n Like if you just think about the intuition,\n\n1:10:43.900 --> 1:10:46.780\n like how the heck does random behavior\n\n1:10:46.780 --> 1:10:51.100\n become somewhat basic intelligent behavior?\n\n1:10:51.100 --> 1:10:55.180\n Not human level, not superhuman level, but just basic.\n\n1:10:55.180 --> 1:10:58.100\n But you're saying to you kind of the intuition is like,\n\n1:10:58.100 --> 1:11:01.060\n if you can go from human to superhuman level intelligence\n\n1:11:01.060 --> 1:11:04.060\n on this particular task of game playing,\n\n1:11:04.060 --> 1:11:07.020\n then so you're good at taking leaps.\n\n1:11:07.020 --> 1:11:08.580\n So you can take many of them.\n\n1:11:08.580 --> 1:11:10.020\n That the system, I believe that the system\n\n1:11:10.020 --> 1:11:12.140\n can take that kind of leap.\n\n1:11:12.140 --> 1:11:17.060\n Yeah, and also I think that beginner knowledge in go,\n\n1:11:17.060 --> 1:11:19.700\n like you can start to get a feel really quickly\n\n1:11:19.700 --> 1:11:24.700\n for the idea that being in certain parts of the board\n\n1:11:25.180 --> 1:11:28.460\n seems to be more associated with winning, right?\n\n1:11:28.460 --> 1:11:32.060\n Cause it's not stumbling upon the concept of winning.\n\n1:11:32.060 --> 1:11:34.660\n It's told that it wins or that it loses.\n\n1:11:34.660 --> 1:11:35.500\n Well, it's self play.\n\n1:11:35.500 --> 1:11:36.700\n So it both wins and loses.\n\n1:11:36.700 --> 1:11:39.540\n It's told which side won.\n\n1:11:39.540 --> 1:11:41.900\n And the information is kind of there\n\n1:11:41.900 --> 1:11:45.420\n to start percolating around to make a difference as to,\n\n1:11:46.460 --> 1:11:48.860\n well, these things have a better chance of helping you win.\n\n1:11:48.860 --> 1:11:50.660\n And these things have a worse chance of helping you win.\n\n1:11:50.660 --> 1:11:54.340\n And so it can get to basic play, I think pretty quickly.\n\n1:11:54.340 --> 1:11:55.980\n Then once it has basic play,\n\n1:11:55.980 --> 1:11:58.580\n well now it's kind of forced to do some search\n\n1:11:58.580 --> 1:12:00.100\n to actually experiment with, okay,\n\n1:12:00.100 --> 1:12:04.140\n well what gets me that next increment of improvement?\n\n1:12:04.140 --> 1:12:07.180\n How far do you think, okay, this is where you kind of\n\n1:12:07.180 --> 1:12:10.500\n bring up the Elon Musk and the Sam Harris, right?\n\n1:12:10.500 --> 1:12:13.140\n How far is your intuition about these kinds\n\n1:12:13.140 --> 1:12:16.020\n of self play mechanisms being able to take us?\n\n1:12:16.020 --> 1:12:21.020\n Cause it feels, one of the ominous but stated calmly things\n\n1:12:23.060 --> 1:12:25.500\n that when I talked to David Silver, he said,\n\n1:12:25.500 --> 1:12:29.180\n is that they have not yet discovered a ceiling\n\n1:12:29.180 --> 1:12:32.660\n for Alpha Zero, for example, in the game of Go or chess.\n\n1:12:32.660 --> 1:12:35.540\n Like it keeps, no matter how much they compute,\n\n1:12:35.540 --> 1:12:37.620\n they throw at it, it keeps improving.\n\n1:12:37.620 --> 1:12:42.620\n So it's possible, it's very possible that if you throw,\n\n1:12:43.100 --> 1:12:46.540\n you know, some like 10 X compute that it will improve\n\n1:12:46.540 --> 1:12:48.660\n by five X or something like that.\n\n1:12:48.660 --> 1:12:53.660\n And when stated calmly, it's so like, oh yeah, I guess so.\n\n1:12:54.580 --> 1:12:56.300\n But like, and then you think like,\n\n1:12:56.300 --> 1:13:00.900\n well, can we potentially have like continuations\n\n1:13:00.900 --> 1:13:02.860\n of Moore's law in totally different way,\n\n1:13:02.860 --> 1:13:04.980\n like broadly defined Moore's law,\n\n1:13:04.980 --> 1:13:08.500\n not the exponential improvement, like,\n\n1:13:08.500 --> 1:13:11.460\n are we going to have an Alpha Zero that swallows the world?\n\n1:13:13.180 --> 1:13:15.140\n But notice it's not getting better at other things.\n\n1:13:15.140 --> 1:13:16.820\n It's getting better at Go.\n\n1:13:16.820 --> 1:13:19.460\n And I think that's a big leap to say,\n\n1:13:19.460 --> 1:13:22.820\n okay, well, therefore it's better at other things.\n\n1:13:22.820 --> 1:13:26.500\n Well, I mean, the question is how much of the game of life\n\n1:13:26.500 --> 1:13:27.700\n can be turned into.\n\n1:13:27.700 --> 1:13:30.100\n Right, so that I think is a really good question.\n\n1:13:30.100 --> 1:13:32.460\n And I think that we don't, I don't think we as a,\n\n1:13:32.460 --> 1:13:34.860\n I don't know, community really know the answer to this,\n\n1:13:34.860 --> 1:13:39.060\n but so, okay, so I went to a talk\n\n1:13:39.060 --> 1:13:43.260\n by some experts on computer chess.\n\n1:13:43.260 --> 1:13:45.980\n So in particular, computer chess is really interesting\n\n1:13:45.980 --> 1:13:49.340\n because for, of course, for a thousand years,\n\n1:13:49.340 --> 1:13:52.460\n humans were the best chess playing things on the planet.\n\n1:13:52.460 --> 1:13:56.420\n And then computers like edged ahead of the best person.\n\n1:13:56.420 --> 1:13:57.620\n And they've been ahead ever since.\n\n1:13:57.620 --> 1:14:01.160\n It's not like people have overtaken computers.\n\n1:14:01.160 --> 1:14:05.020\n But computers and people together\n\n1:14:05.020 --> 1:14:07.100\n have overtaken computers.\n\n1:14:07.100 --> 1:14:09.060\n So at least last time I checked,\n\n1:14:09.060 --> 1:14:10.340\n I don't know what the very latest is,\n\n1:14:10.340 --> 1:14:14.220\n but last time I checked that there were teams of people\n\n1:14:14.220 --> 1:14:16.100\n who could work with computer programs\n\n1:14:16.100 --> 1:14:17.980\n to defeat the best computer programs.\n\n1:14:17.980 --> 1:14:18.820\n In the game of Go?\n\n1:14:18.820 --> 1:14:19.740\n In the game of chess.\n\n1:14:19.740 --> 1:14:20.580\n In the game of chess.\n\n1:14:20.580 --> 1:14:24.480\n Right, and so using the information about how,\n\n1:14:25.740 --> 1:14:27.080\n these things called ELO scores,\n\n1:14:27.080 --> 1:14:30.320\n this sort of notion of how strong a player are you.\n\n1:14:30.320 --> 1:14:32.540\n There's kind of a range of possible scores.\n\n1:14:32.540 --> 1:14:35.500\n And you increment in score,\n\n1:14:35.500 --> 1:14:37.820\n basically if you can beat another player\n\n1:14:37.820 --> 1:14:41.760\n of that lower score 62% of the time or something like that.\n\n1:14:41.760 --> 1:14:42.900\n Like there's some threshold\n\n1:14:42.900 --> 1:14:46.220\n of if you can somewhat consistently beat someone,\n\n1:14:46.220 --> 1:14:48.800\n then you are of a higher score than that person.\n\n1:14:48.800 --> 1:14:50.820\n And there's a question as to how many times\n\n1:14:50.820 --> 1:14:52.700\n can you do that in chess, right?\n\n1:14:52.700 --> 1:14:55.460\n And so we know that there's a range of human ability levels\n\n1:14:55.460 --> 1:14:57.820\n that cap out with the best playing humans.\n\n1:14:57.820 --> 1:15:00.140\n And the computers went a step beyond that.\n\n1:15:00.140 --> 1:15:03.100\n And computers and people together have not gone,\n\n1:15:03.100 --> 1:15:05.200\n I think a full step beyond that.\n\n1:15:05.200 --> 1:15:07.540\n It feels, the estimates that they have\n\n1:15:07.540 --> 1:15:09.160\n is that it's starting to asymptote.\n\n1:15:09.160 --> 1:15:11.000\n That we've reached kind of the maximum,\n\n1:15:11.000 --> 1:15:13.940\n the best possible chess playing.\n\n1:15:13.940 --> 1:15:15.460\n And so that means that there's kind of\n\n1:15:15.460 --> 1:15:18.500\n a finite strategic depth, right?\n\n1:15:18.500 --> 1:15:21.700\n At some point you just can't get any better at this game.\n\n1:15:21.700 --> 1:15:25.740\n Yeah, I mean, I don't, so I'll actually check that.\n\n1:15:25.740 --> 1:15:29.660\n I think it's interesting because if you have somebody\n\n1:15:29.660 --> 1:15:34.660\n like Magnus Carlsen, who's using these chess programs\n\n1:15:34.980 --> 1:15:37.940\n to train his mind, like to learn about chess.\n\n1:15:37.940 --> 1:15:38.900\n To become a better chess player, yeah.\n\n1:15:38.900 --> 1:15:41.820\n And so like, that's a very interesting thing\n\n1:15:41.820 --> 1:15:43.980\n because we're not static creatures.\n\n1:15:43.980 --> 1:15:45.180\n We're learning together.\n\n1:15:45.180 --> 1:15:47.820\n I mean, just like we're talking about social networks,\n\n1:15:47.820 --> 1:15:49.540\n those algorithms are teaching us\n\n1:15:49.540 --> 1:15:51.540\n just like we're teaching those algorithms.\n\n1:15:51.540 --> 1:15:52.500\n So that's a fascinating thing.\n\n1:15:52.500 --> 1:15:57.140\n But I think the best chess playing programs\n\n1:15:57.140 --> 1:15:58.700\n are now better than the pairs.\n\n1:15:58.700 --> 1:16:00.700\n Like they have competition between pairs,\n\n1:16:00.700 --> 1:16:03.620\n but it's still, even if they weren't,\n\n1:16:03.620 --> 1:16:06.020\n it's an interesting question, where's the ceiling?\n\n1:16:06.020 --> 1:16:09.420\n So the David, the ominous David Silver kind of statement\n\n1:16:09.420 --> 1:16:12.180\n is like, we have not found the ceiling.\n\n1:16:12.180 --> 1:16:14.260\n Right, so the question is, okay,\n\n1:16:14.260 --> 1:16:16.540\n so I don't know his analysis on that.\n\n1:16:16.540 --> 1:16:20.060\n My, from talking to Go experts,\n\n1:16:20.060 --> 1:16:22.620\n the depth, the strategic depth of Go\n\n1:16:22.620 --> 1:16:25.180\n seems to be substantially greater than that of chess.\n\n1:16:25.180 --> 1:16:27.920\n That there's more kind of steps of improvement\n\n1:16:27.920 --> 1:16:29.700\n that you can make, getting better and better\n\n1:16:29.700 --> 1:16:30.540\n and better and better.\n\n1:16:30.540 --> 1:16:32.100\n But there's no reason to think that it's infinite.\n\n1:16:32.100 --> 1:16:33.420\n Infinite, yeah.\n\n1:16:33.420 --> 1:16:37.060\n And so it could be that what David is seeing\n\n1:16:37.060 --> 1:16:39.780\n is a kind of asymptoting that you can keep getting better,\n\n1:16:39.780 --> 1:16:41.140\n but with diminishing returns.\n\n1:16:41.140 --> 1:16:43.620\n And at some point you hit optimal play.\n\n1:16:43.620 --> 1:16:47.620\n Like in theory, all these finite games, they're finite.\n\n1:16:47.620 --> 1:16:49.280\n They have an optimal strategy.\n\n1:16:49.280 --> 1:16:51.820\n There's a strategy that is the minimax optimal strategy.\n\n1:16:51.820 --> 1:16:54.780\n And so at that point, you can't get any better.\n\n1:16:54.780 --> 1:16:56.460\n You can't beat that strategy.\n\n1:16:56.460 --> 1:16:58.220\n Now that strategy may be,\n\n1:16:58.220 --> 1:17:02.380\n from an information processing perspective, intractable.\n\n1:17:02.380 --> 1:17:06.260\n Right, you need, all the situations\n\n1:17:06.260 --> 1:17:08.460\n are sufficiently different that you can't compress it at all.\n\n1:17:08.460 --> 1:17:12.220\n It's this giant mess of hardcoded rules.\n\n1:17:12.220 --> 1:17:13.740\n And we can never achieve that.\n\n1:17:14.720 --> 1:17:17.740\n But that still puts a cap on how many levels of improvement\n\n1:17:17.740 --> 1:17:19.020\n that we can actually make.\n\n1:17:19.020 --> 1:17:23.260\n But the thing about self play is if you put it,\n\n1:17:23.260 --> 1:17:24.540\n although I don't like doing that,\n\n1:17:24.540 --> 1:17:28.420\n in the broader category of self supervised learning,\n\n1:17:28.420 --> 1:17:31.780\n is that it doesn't require too much or any human input.\n\n1:17:31.780 --> 1:17:32.700\n Human labeling, yeah.\n\n1:17:32.700 --> 1:17:34.900\n Yeah, human label or just human effort.\n\n1:17:34.900 --> 1:17:37.940\n The human involvement passed a certain point.\n\n1:17:37.940 --> 1:17:41.100\n And the same thing you could argue is true\n\n1:17:41.100 --> 1:17:44.820\n for the recent breakthroughs in natural language processing\n\n1:17:44.820 --> 1:17:45.860\n with language models.\n\n1:17:45.860 --> 1:17:47.780\n Oh, this is how you get to GPT3.\n\n1:17:47.780 --> 1:17:49.780\n Yeah, see how that did the.\n\n1:17:49.780 --> 1:17:51.300\n That was a good transition.\n\n1:17:51.300 --> 1:17:55.480\n Yeah, I practiced that for days leading up to this now.\n\n1:17:56.460 --> 1:17:59.680\n But like that's one of the questions is,\n\n1:17:59.680 --> 1:18:03.400\n can we find ways to formulate problems in this world\n\n1:18:03.400 --> 1:18:05.520\n that are important to us humans,\n\n1:18:05.520 --> 1:18:08.260\n like more important than the game of chess,\n\n1:18:08.260 --> 1:18:12.540\n that to which self supervised kinds of approaches\n\n1:18:12.540 --> 1:18:13.380\n could be applied?\n\n1:18:13.380 --> 1:18:15.540\n Whether it's self play, for example,\n\n1:18:15.540 --> 1:18:19.260\n for like maybe you could think of like autonomous vehicles\n\n1:18:19.260 --> 1:18:22.340\n in simulation, that kind of stuff,\n\n1:18:22.340 --> 1:18:25.720\n or just robotics applications and simulation,\n\n1:18:25.720 --> 1:18:29.440\n or in the self supervised learning,\n\n1:18:29.440 --> 1:18:33.660\n where unannotated data,\n\n1:18:33.660 --> 1:18:37.460\n or data that's generated by humans naturally\n\n1:18:37.460 --> 1:18:41.420\n without extra costs, like Wikipedia,\n\n1:18:41.420 --> 1:18:44.060\n or like all of the internet can be used\n\n1:18:44.060 --> 1:18:46.300\n to learn something about,\n\n1:18:46.300 --> 1:18:49.300\n to create intelligent systems that do something\n\n1:18:49.300 --> 1:18:52.380\n really powerful, that pass the Turing test,\n\n1:18:52.380 --> 1:18:56.500\n or that do some kind of superhuman level performance.\n\n1:18:56.500 --> 1:18:58.820\n So what's your intuition,\n\n1:18:58.820 --> 1:19:01.600\n like trying to stitch all of it together\n\n1:19:01.600 --> 1:19:05.180\n about our discussion of AGI,\n\n1:19:05.180 --> 1:19:07.260\n the limits of self play,\n\n1:19:07.260 --> 1:19:10.420\n and your thoughts about maybe the limits of neural networks\n\n1:19:10.420 --> 1:19:13.100\n in the context of language models.\n\n1:19:13.100 --> 1:19:14.540\n Is there some intuition in there\n\n1:19:14.540 --> 1:19:17.020\n that might be useful to think about?\n\n1:19:17.020 --> 1:19:17.860\n Yeah, yeah, yeah.\n\n1:19:17.860 --> 1:19:21.860\n So first of all, the whole Transformer network\n\n1:19:22.820 --> 1:19:26.620\n family of things is really cool.\n\n1:19:26.620 --> 1:19:28.140\n It's really, really cool.\n\n1:19:28.140 --> 1:19:30.260\n I mean, if you've ever,\n\n1:19:30.260 --> 1:19:31.780\n back in the day you played with,\n\n1:19:31.780 --> 1:19:34.020\n I don't know, Markov models for generating texts,\n\n1:19:34.020 --> 1:19:35.820\n and you've seen the kind of texts that they spit out,\n\n1:19:35.820 --> 1:19:37.960\n and you compare it to what's happening now,\n\n1:19:37.960 --> 1:19:41.820\n it's amazing, it's so amazing.\n\n1:19:41.820 --> 1:19:43.980\n Now, it doesn't take very long interacting\n\n1:19:43.980 --> 1:19:47.340\n with one of these systems before you find the holes, right?\n\n1:19:47.340 --> 1:19:52.340\n It's not smart in any kind of general way.\n\n1:19:53.100 --> 1:19:55.300\n It's really good at a bunch of things.\n\n1:19:55.300 --> 1:19:56.540\n And it does seem to understand\n\n1:19:56.540 --> 1:19:59.980\n a lot of the statistics of language extremely well.\n\n1:19:59.980 --> 1:20:01.860\n And that turns out to be very powerful.\n\n1:20:01.860 --> 1:20:04.040\n You can answer many questions with that.\n\n1:20:04.040 --> 1:20:06.580\n But it doesn't make it a good conversationalist, right?\n\n1:20:06.580 --> 1:20:08.460\n And it doesn't make it a good storyteller.\n\n1:20:08.460 --> 1:20:10.040\n It just makes it good at imitating\n\n1:20:10.040 --> 1:20:12.620\n of things that is seen in the past.\n\n1:20:12.620 --> 1:20:14.540\n The exact same thing could be said\n\n1:20:14.540 --> 1:20:16.620\n by people who are voting for Donald Trump\n\n1:20:16.620 --> 1:20:18.060\n about Joe Biden supporters,\n\n1:20:18.060 --> 1:20:19.420\n and people voting for Joe Biden\n\n1:20:19.420 --> 1:20:22.900\n about Donald Trump supporters is, you know.\n\n1:20:22.900 --> 1:20:25.100\n That they're not intelligent, they're just following the.\n\n1:20:25.100 --> 1:20:27.420\n Yeah, they're following things they've seen in the past.\n\n1:20:27.420 --> 1:20:31.220\n And it doesn't take long to find the flaws\n\n1:20:31.220 --> 1:20:36.220\n in their natural language generation abilities.\n\n1:20:36.380 --> 1:20:37.220\n Yes, yes.\n\n1:20:37.220 --> 1:20:38.060\n So we're being very.\n\n1:20:38.060 --> 1:20:39.500\n That's interesting.\n\n1:20:39.500 --> 1:20:41.260\n Critical of AI systems.\n\n1:20:41.260 --> 1:20:43.420\n Right, so I've had a similar thought,\n\n1:20:43.420 --> 1:20:48.420\n which was that the stories that GPT3 spits out\n\n1:20:48.700 --> 1:20:52.420\n are amazing and very humanlike.\n\n1:20:52.420 --> 1:20:55.940\n And it doesn't mean that computers are smarter\n\n1:20:55.940 --> 1:20:57.500\n than we realize necessarily.\n\n1:20:57.500 --> 1:21:00.280\n It partly means that people are dumber than we realize.\n\n1:21:00.280 --> 1:21:04.520\n Or that much of what we do day to day is not that deep.\n\n1:21:04.520 --> 1:21:07.300\n Like we're just kind of going with the flow.\n\n1:21:07.300 --> 1:21:09.360\n We're saying whatever feels like the natural thing\n\n1:21:09.360 --> 1:21:10.380\n to say next.\n\n1:21:10.380 --> 1:21:15.380\n Not a lot of it is creative or meaningful or intentional.\n\n1:21:17.060 --> 1:21:20.460\n But enough is that we actually get by, right?\n\n1:21:20.460 --> 1:21:22.280\n We do come up with new ideas sometimes,\n\n1:21:22.280 --> 1:21:24.860\n and we do manage to talk each other into things sometimes.\n\n1:21:24.860 --> 1:21:28.180\n And we do sometimes vote for reasonable people sometimes.\n\n1:21:29.420 --> 1:21:32.660\n But it's really hard to see in the statistics\n\n1:21:32.660 --> 1:21:35.620\n because so much of what we're saying is kind of rote.\n\n1:21:35.620 --> 1:21:38.160\n And so our metrics that we use to measure\n\n1:21:38.160 --> 1:21:41.700\n how these systems are doing don't reveal that\n\n1:21:41.700 --> 1:21:46.700\n because it's in the interstices that is very hard to detect.\n\n1:21:47.100 --> 1:21:49.020\n But is your, do you have an intuition\n\n1:21:49.020 --> 1:21:53.380\n that with these language models, if they grow in size,\n\n1:21:53.380 --> 1:21:57.460\n it's already surprising when you go from GPT2 to GPT3\n\n1:21:57.460 --> 1:21:59.540\n that there is a noticeable improvement.\n\n1:21:59.540 --> 1:22:02.560\n So the question now goes back to the ominous David Silver\n\n1:22:02.560 --> 1:22:03.420\n and the ceiling.\n\n1:22:03.420 --> 1:22:04.980\n Right, so maybe there's just no ceiling.\n\n1:22:04.980 --> 1:22:06.140\n We just need more compute.\n\n1:22:06.140 --> 1:22:10.340\n Now, I mean, okay, so now I'm speculating.\n\n1:22:10.340 --> 1:22:11.180\n Yes.\n\n1:22:11.180 --> 1:22:13.860\n As opposed to before when I was completely on firm ground.\n\n1:22:13.860 --> 1:22:17.300\n All right, I don't believe that you can get something\n\n1:22:17.300 --> 1:22:21.940\n that really can do language and use language as a thing\n\n1:22:21.940 --> 1:22:24.360\n that doesn't interact with people.\n\n1:22:24.360 --> 1:22:25.940\n Like I think that it's not enough\n\n1:22:25.940 --> 1:22:28.300\n to just take everything that we've said written down\n\n1:22:28.300 --> 1:22:29.840\n and just say, that's enough.\n\n1:22:29.840 --> 1:22:32.020\n You can just learn from that and you can be intelligent.\n\n1:22:32.020 --> 1:22:35.360\n I think you really need to be pushed back at.\n\n1:22:35.360 --> 1:22:36.780\n I think that conversations,\n\n1:22:36.780 --> 1:22:38.940\n even people who are pretty smart,\n\n1:22:38.940 --> 1:22:40.720\n maybe the smartest thing that we know,\n\n1:22:40.720 --> 1:22:43.020\n maybe not the smartest thing we can imagine,\n\n1:22:43.020 --> 1:22:44.700\n but we get so much benefit\n\n1:22:44.700 --> 1:22:48.620\n out of talking to each other and interacting.\n\n1:22:48.620 --> 1:22:51.260\n That's presumably why you have conversations live with guests\n\n1:22:51.260 --> 1:22:53.900\n is that there's something in that interaction\n\n1:22:53.900 --> 1:22:55.920\n that would not be exposed by,\n\n1:22:55.920 --> 1:22:57.180\n oh, I'll just write you a story\n\n1:22:57.180 --> 1:22:58.340\n and then you can read it later.\n\n1:22:58.340 --> 1:23:00.300\n And I think because these systems\n\n1:23:00.300 --> 1:23:01.800\n are just learning from our stories,\n\n1:23:01.800 --> 1:23:05.200\n they're not learning from being pushed back at by us,\n\n1:23:05.200 --> 1:23:06.540\n that they're fundamentally limited\n\n1:23:06.540 --> 1:23:08.860\n into what they can actually become on this route.\n\n1:23:08.860 --> 1:23:12.300\n They have to get shut down.\n\n1:23:12.300 --> 1:23:14.940\n Like we have to have an argument,\n\n1:23:14.940 --> 1:23:15.980\n they have to have an argument with us\n\n1:23:15.980 --> 1:23:17.540\n and lose a couple of times\n\n1:23:17.540 --> 1:23:20.540\n before they start to realize, oh, okay, wait,\n\n1:23:20.540 --> 1:23:23.240\n there's some nuance here that actually matters.\n\n1:23:23.240 --> 1:23:25.820\n Yeah, that's actually subtle sounding,\n\n1:23:25.820 --> 1:23:30.020\n but quite profound that the interaction with humans\n\n1:23:30.020 --> 1:23:34.240\n is essential and the limitation within that\n\n1:23:34.240 --> 1:23:37.380\n is profound as well because the timescale,\n\n1:23:37.380 --> 1:23:40.520\n like the bandwidth at which you can really interact\n\n1:23:40.520 --> 1:23:43.500\n with humans is very low.\n\n1:23:43.500 --> 1:23:44.460\n So it's costly.\n\n1:23:44.460 --> 1:23:47.700\n So you can't, one of the underlying things about self plays,\n\n1:23:47.700 --> 1:23:52.700\n it has to do a very large number of interactions.\n\n1:23:53.100 --> 1:23:56.660\n And so you can't really deploy reinforcement learning systems\n\n1:23:56.660 --> 1:23:58.140\n into the real world to interact.\n\n1:23:58.140 --> 1:24:01.340\n Like you couldn't deploy a language model\n\n1:24:01.340 --> 1:24:04.580\n into the real world to interact with humans\n\n1:24:04.580 --> 1:24:06.780\n because it was just not getting enough data\n\n1:24:06.780 --> 1:24:09.860\n relative to the cost it takes to interact.\n\n1:24:09.860 --> 1:24:12.820\n Like the time of humans is expensive,\n\n1:24:12.820 --> 1:24:13.700\n which is really interesting.\n\n1:24:13.700 --> 1:24:16.300\n That takes us back to reinforcement learning\n\n1:24:16.300 --> 1:24:18.700\n and trying to figure out if there's ways\n\n1:24:18.700 --> 1:24:22.500\n to make algorithms that are more efficient at learning,\n\n1:24:22.500 --> 1:24:24.660\n keep the spirit in reinforcement learning\n\n1:24:24.660 --> 1:24:26.300\n and become more efficient.\n\n1:24:26.300 --> 1:24:28.220\n In some sense, that seems to be the goal.\n\n1:24:28.220 --> 1:24:31.380\n I'd love to hear what your thoughts are.\n\n1:24:31.380 --> 1:24:33.380\n I don't know if you got a chance to see\n\n1:24:33.380 --> 1:24:35.140\n the blog post called Bitter Lesson.\n\n1:24:35.140 --> 1:24:35.980\n Oh yes.\n\n1:24:37.060 --> 1:24:39.620\n By Rich Sutton that makes an argument,\n\n1:24:39.620 --> 1:24:41.620\n hopefully I can summarize it.\n\n1:24:41.620 --> 1:24:43.460\n Perhaps you can.\n\n1:24:43.460 --> 1:24:44.660\n Yeah, but do you want?\n\n1:24:44.660 --> 1:24:45.500\n Okay.\n\n1:24:45.500 --> 1:24:47.380\n So I mean, I could try and you can correct me,\n\n1:24:47.380 --> 1:24:50.340\n which is he makes an argument that it seems\n\n1:24:50.340 --> 1:24:52.940\n if we look at the long arc of the history\n\n1:24:52.940 --> 1:24:55.020\n of the artificial intelligence field,\n\n1:24:55.020 --> 1:24:58.380\n he calls 70 years that the algorithms\n\n1:24:58.380 --> 1:25:02.900\n from which we've seen the biggest improvements in practice\n\n1:25:02.900 --> 1:25:05.980\n are the very simple, like dumb algorithms\n\n1:25:05.980 --> 1:25:08.660\n that are able to leverage computation.\n\n1:25:08.660 --> 1:25:11.420\n And you just wait for the computation to improve.\n\n1:25:11.420 --> 1:25:13.660\n Like all of the academics and so on have fun\n\n1:25:13.660 --> 1:25:15.020\n by finding little tricks\n\n1:25:15.020 --> 1:25:17.460\n and congratulate themselves on those tricks.\n\n1:25:17.460 --> 1:25:20.060\n And sometimes those tricks can be like big,\n\n1:25:20.060 --> 1:25:22.700\n that feel in the moment like big spikes and breakthroughs,\n\n1:25:22.700 --> 1:25:25.660\n but in reality over the decades,\n\n1:25:25.660 --> 1:25:27.620\n it's still the same dumb algorithm\n\n1:25:27.620 --> 1:25:31.700\n that just waits for the compute to get faster and faster.\n\n1:25:31.700 --> 1:25:36.300\n Do you find that to be an interesting argument\n\n1:25:36.300 --> 1:25:39.540\n against the entirety of the field of machine learning\n\n1:25:39.540 --> 1:25:41.020\n as an academic discipline?\n\n1:25:41.020 --> 1:25:44.380\n That we're really just a subfield of computer architecture.\n\n1:25:44.380 --> 1:25:45.500\n We're just kind of waiting around\n\n1:25:45.500 --> 1:25:46.340\n for them to do their next thing.\n\n1:25:46.340 --> 1:25:48.140\n Who really don't want to do hardware work.\n\n1:25:48.140 --> 1:25:48.980\n So like.\n\n1:25:48.980 --> 1:25:49.820\n That's right.\n\n1:25:49.820 --> 1:25:50.660\n I really don't want to think about it.\n\n1:25:50.660 --> 1:25:51.500\n We're procrastinating.\n\n1:25:51.500 --> 1:25:53.740\n Yes, that's right, just waiting for them to do their jobs\n\n1:25:53.740 --> 1:25:55.180\n so that we can pretend to have done ours.\n\n1:25:55.180 --> 1:26:00.180\n So yeah, I mean, the argument reminds me a lot of,\n\n1:26:00.180 --> 1:26:02.300\n I think it was a Fred Jelinek quote,\n\n1:26:02.300 --> 1:26:04.740\n early computational linguist who said,\n\n1:26:04.740 --> 1:26:07.260\n we're building these computational linguistic systems\n\n1:26:07.260 --> 1:26:11.100\n and every time we fire a linguist performance goes up\n\n1:26:11.100 --> 1:26:13.060\n by 10%, something like that.\n\n1:26:13.060 --> 1:26:16.060\n And so the idea of us building the knowledge in,\n\n1:26:16.060 --> 1:26:19.100\n in that case was much less,\n\n1:26:19.100 --> 1:26:20.980\n he was finding it to be much less successful\n\n1:26:20.980 --> 1:26:25.020\n than get rid of the people who know about language as a,\n\n1:26:25.020 --> 1:26:29.700\n from a kind of scholastic academic kind of perspective\n\n1:26:29.700 --> 1:26:32.180\n and replace them with more compute.\n\n1:26:32.180 --> 1:26:34.380\n And so I think this is kind of a modern version\n\n1:26:34.380 --> 1:26:35.620\n of that story, which is, okay,\n\n1:26:35.620 --> 1:26:38.420\n we want to do better on machine vision.\n\n1:26:38.420 --> 1:26:39.820\n You could build in all these,\n\n1:26:41.940 --> 1:26:45.420\n motivated part based models that,\n\n1:26:45.420 --> 1:26:47.420\n that just feel like obviously the right thing\n\n1:26:47.420 --> 1:26:48.500\n that you have to have,\n\n1:26:48.500 --> 1:26:49.980\n or we can throw a lot of data at it\n\n1:26:49.980 --> 1:26:52.100\n and guess what we're doing better with a lot of data.\n\n1:26:52.100 --> 1:26:57.100\n So I hadn't thought about it until this moment in this way,\n\n1:26:57.460 --> 1:27:00.620\n but what I believe, well, I've thought about what I believe.\n\n1:27:00.620 --> 1:27:05.620\n What I believe is that, you know, compositionality\n\n1:27:05.780 --> 1:27:08.820\n and what's the right way to say it,\n\n1:27:08.820 --> 1:27:12.180\n the complexity grows rapidly\n\n1:27:12.180 --> 1:27:14.580\n as you consider more and more possibilities,\n\n1:27:14.580 --> 1:27:16.740\n like explosively.\n\n1:27:16.740 --> 1:27:20.180\n And so far Moore's law has also been growing explosively\n\n1:27:20.180 --> 1:27:21.020\n exponentially.\n\n1:27:21.020 --> 1:27:23.020\n And so it really does seem like, well,\n\n1:27:23.020 --> 1:27:27.140\n we don't have to think really hard about the algorithm\n\n1:27:27.140 --> 1:27:29.340\n design or the way that we build the systems,\n\n1:27:29.340 --> 1:27:32.740\n because the best benefit we could get is exponential.\n\n1:27:32.740 --> 1:27:34.700\n And the best benefit that we can get from waiting\n\n1:27:34.700 --> 1:27:35.860\n is exponential.\n\n1:27:35.860 --> 1:27:36.860\n So we can just wait.\n\n1:27:38.180 --> 1:27:39.940\n It's got, that's gotta end, right?\n\n1:27:39.940 --> 1:27:41.100\n And there's hints now that,\n\n1:27:41.100 --> 1:27:44.740\n that Moore's law is starting to feel some friction,\n\n1:27:44.740 --> 1:27:47.340\n starting to, the world is pushing back a little bit.\n\n1:27:48.380 --> 1:27:50.940\n One thing that I don't know, do lots of people know this?\n\n1:27:50.940 --> 1:27:54.020\n I didn't know this, I was trying to write an essay\n\n1:27:54.020 --> 1:27:56.940\n and yeah, Moore's law has been amazing\n\n1:27:56.940 --> 1:27:58.580\n and it's enabled all sorts of things,\n\n1:27:58.580 --> 1:28:01.380\n but there's also a kind of counter Moore's law,\n\n1:28:01.380 --> 1:28:03.260\n which is that the development cost\n\n1:28:03.260 --> 1:28:07.660\n for each successive generation of chips also is doubling.\n\n1:28:07.660 --> 1:28:09.380\n So it's costing twice as much money.\n\n1:28:09.380 --> 1:28:12.900\n So the amount of development money per cycle or whatever\n\n1:28:12.900 --> 1:28:14.860\n is actually sort of constant.\n\n1:28:14.860 --> 1:28:17.180\n And at some point we run out of money.\n\n1:28:17.180 --> 1:28:19.540\n So, or we have to come up with an entirely different way\n\n1:28:19.540 --> 1:28:22.100\n of doing the development process.\n\n1:28:22.100 --> 1:28:25.980\n So like, I guess I always a bit skeptical of the look,\n\n1:28:25.980 --> 1:28:28.700\n it's an exponential curve, therefore it has no end.\n\n1:28:28.700 --> 1:28:30.500\n Soon the number of people going to NeurIPS\n\n1:28:30.500 --> 1:28:32.660\n will be greater than the population of the earth.\n\n1:28:32.660 --> 1:28:35.460\n That means we're gonna discover life on other planets.\n\n1:28:35.460 --> 1:28:36.300\n No, it doesn't.\n\n1:28:36.300 --> 1:28:40.340\n It means that we're in a sigmoid curve on the front half,\n\n1:28:40.340 --> 1:28:42.700\n which looks a lot like an exponential.\n\n1:28:42.700 --> 1:28:46.140\n The second half is gonna look a lot like diminishing returns.\n\n1:28:46.140 --> 1:28:48.980\n Yeah, I mean, but the interesting thing about Moore's law,\n\n1:28:48.980 --> 1:28:52.220\n if you actually like look at the technologies involved,\n\n1:28:52.220 --> 1:28:55.620\n it's hundreds, if not thousands of S curves\n\n1:28:55.620 --> 1:28:56.700\n stacked on top of each other.\n\n1:28:56.700 --> 1:28:58.700\n It's not actually an exponential curve,\n\n1:28:58.700 --> 1:29:01.100\n it's constant breakthroughs.\n\n1:29:01.100 --> 1:29:04.140\n And then what becomes useful to think about,\n\n1:29:04.140 --> 1:29:05.500\n which is exactly what you're saying,\n\n1:29:05.500 --> 1:29:08.100\n the cost of development, like the size of teams,\n\n1:29:08.100 --> 1:29:10.220\n the amount of resources that are invested\n\n1:29:10.220 --> 1:29:14.300\n in continuing to find new S curves, new breakthroughs.\n\n1:29:14.300 --> 1:29:19.100\n And yeah, it's an interesting idea.\n\n1:29:19.100 --> 1:29:22.860\n If we live in the moment, if we sit here today,\n\n1:29:22.860 --> 1:29:25.820\n it seems to be the reasonable thing\n\n1:29:25.820 --> 1:29:29.180\n to say that exponentials end.\n\n1:29:29.180 --> 1:29:31.420\n And yet in the software realm,\n\n1:29:31.420 --> 1:29:34.740\n they just keep appearing to be happening.\n\n1:29:34.740 --> 1:29:39.700\n And it's so, I mean, it's so hard to disagree\n\n1:29:39.700 --> 1:29:41.060\n with Elon Musk on this.\n\n1:29:41.060 --> 1:29:45.980\n Because it like, I've, you know,\n\n1:29:45.980 --> 1:29:47.740\n I used to be one of those folks,\n\n1:29:47.740 --> 1:29:49.980\n I'm still one of those folks that studied\n\n1:29:49.980 --> 1:29:52.180\n autonomous vehicles, that's what I worked on.\n\n1:29:52.180 --> 1:29:56.260\n And it's like, you look at what Elon Musk is saying\n\n1:29:56.260 --> 1:29:58.100\n about autonomous vehicles, well, obviously,\n\n1:29:58.100 --> 1:30:01.580\n in a couple of years, or in a year, or next month,\n\n1:30:01.580 --> 1:30:03.220\n we'll have fully autonomous vehicles.\n\n1:30:03.220 --> 1:30:04.700\n Like there's no reason why we can't.\n\n1:30:04.700 --> 1:30:07.980\n Driving is pretty simple, like it's just a learning problem\n\n1:30:07.980 --> 1:30:11.060\n and you just need to convert all the driving\n\n1:30:11.060 --> 1:30:13.140\n that we're doing into data and just having you all know\n\n1:30:13.140 --> 1:30:14.660\n with the trains on that data.\n\n1:30:14.660 --> 1:30:18.620\n And like, we use only our eyes, so you can use cameras\n\n1:30:18.620 --> 1:30:20.380\n and you can train on it.\n\n1:30:20.380 --> 1:30:25.380\n And it's like, yeah, that should work.\n\n1:30:26.180 --> 1:30:29.100\n And then you put that hat on, like the philosophical hat,\n\n1:30:29.100 --> 1:30:31.540\n and but then you put the pragmatic hat and it's like,\n\n1:30:31.540 --> 1:30:33.900\n this is what the flaws of computer vision are.\n\n1:30:33.900 --> 1:30:35.980\n Like, this is what it means to train at scale.\n\n1:30:35.980 --> 1:30:40.940\n And then you put the human factors, the psychology hat on,\n\n1:30:40.940 --> 1:30:43.620\n which is like, it's actually driving us a lot,\n\n1:30:43.620 --> 1:30:44.900\n the cognitive science or cognitive,\n\n1:30:44.900 --> 1:30:48.180\n whatever the heck you call it, it's really hard,\n\n1:30:48.180 --> 1:30:50.900\n it's much harder to drive than we realize,\n\n1:30:50.900 --> 1:30:53.420\n there's a much larger number of edge cases.\n\n1:30:53.420 --> 1:30:55.740\n So building up an intuition around this is,\n\n1:30:57.460 --> 1:30:59.380\n around exponentials is really difficult.\n\n1:30:59.380 --> 1:31:03.180\n And on top of that, the pandemic is making us think\n\n1:31:03.180 --> 1:31:06.980\n about exponentials, making us realize that like,\n\n1:31:06.980 --> 1:31:08.900\n we don't understand anything about it,\n\n1:31:08.900 --> 1:31:11.060\n we're not able to intuit exponentials,\n\n1:31:11.060 --> 1:31:15.540\n we're either ultra terrified, some part of the population\n\n1:31:15.540 --> 1:31:20.260\n and some part is like the opposite of whatever\n\n1:31:20.260 --> 1:31:24.620\n the different carefree and we're not managing it very well.\n\n1:31:24.620 --> 1:31:27.180\n Blase, well, wow, is that French?\n\n1:31:28.260 --> 1:31:29.780\n I assume so, it's got an accent.\n\n1:31:29.780 --> 1:31:34.780\n So it's fascinating to think what the limits\n\n1:31:35.460 --> 1:31:40.460\n of this exponential growth of technology,\n\n1:31:41.060 --> 1:31:44.460\n not just Moore's law, it's technology,\n\n1:31:44.460 --> 1:31:49.460\n how that rubs up against the bitter lesson\n\n1:31:49.460 --> 1:31:53.700\n and GPT three and self play mechanisms.\n\n1:31:53.700 --> 1:31:56.980\n Like it's not obvious, I used to be much more skeptical\n\n1:31:56.980 --> 1:31:58.220\n about neural networks.\n\n1:31:58.220 --> 1:32:00.980\n Now I at least give a slither of possibility\n\n1:32:00.980 --> 1:32:04.420\n that we'll be very much surprised\n\n1:32:04.420 --> 1:32:09.420\n and also caught in a way that like,\n\n1:32:10.900 --> 1:32:14.140\n we are not prepared for.\n\n1:32:14.140 --> 1:32:19.140\n Like in applications of social networks, for example,\n\n1:32:19.420 --> 1:32:23.460\n cause it feels like really good transformer models\n\n1:32:23.460 --> 1:32:28.460\n that are able to do some kind of like very good\n\n1:32:28.460 --> 1:32:31.220\n natural language generation of the same kind of models\n\n1:32:31.220 --> 1:32:33.860\n that can be used to learn human behavior\n\n1:32:33.860 --> 1:32:35.940\n and then manipulate that human behavior\n\n1:32:35.940 --> 1:32:38.980\n to gain advertisers dollars and all those kinds of things\n\n1:32:38.980 --> 1:32:41.380\n through the capitalist system.\n\n1:32:41.380 --> 1:32:45.060\n And they arguably already are manipulating human behavior.\n\n1:32:46.420 --> 1:32:51.220\n But not for self preservation, which I think is a big,\n\n1:32:51.220 --> 1:32:52.340\n that would be a big step.\n\n1:32:52.340 --> 1:32:54.020\n Like if they were trying to manipulate us\n\n1:32:54.020 --> 1:32:56.100\n to convince us not to shut them off,\n\n1:32:57.020 --> 1:32:58.580\n I would be very freaked out.\n\n1:32:58.580 --> 1:33:01.780\n But I don't see a path to that from where we are now.\n\n1:33:01.780 --> 1:33:05.820\n They don't have any of those abilities.\n\n1:33:05.820 --> 1:33:07.660\n That's not what they're trying to do.\n\n1:33:07.660 --> 1:33:10.100\n They're trying to keep people on the site.\n\n1:33:10.100 --> 1:33:13.020\n But see the thing is, this is the thing about life on earth\n\n1:33:13.020 --> 1:33:16.860\n is they might be borrowing our consciousness\n\n1:33:16.860 --> 1:33:20.940\n and sentience like, so like in a sense they do\n\n1:33:20.940 --> 1:33:23.740\n because the creators of the algorithms have,\n\n1:33:23.740 --> 1:33:26.940\n like they're not, if you look at our body,\n\n1:33:26.940 --> 1:33:28.540\n we're not a single organism.\n\n1:33:28.540 --> 1:33:30.340\n We're a huge number of organisms\n\n1:33:30.340 --> 1:33:31.700\n with like tiny little motivations\n\n1:33:31.700 --> 1:33:33.300\n were built on top of each other.\n\n1:33:33.300 --> 1:33:36.220\n In the same sense, the AI algorithms that are,\n\n1:33:36.220 --> 1:33:37.060\n they're not like.\n\n1:33:37.060 --> 1:33:40.260\n It's a system that includes companies and corporations,\n\n1:33:40.260 --> 1:33:42.100\n because corporations are funny organisms\n\n1:33:42.100 --> 1:33:44.380\n in and of themselves that really do seem\n\n1:33:44.380 --> 1:33:45.780\n to have self preservation built in.\n\n1:33:45.780 --> 1:33:48.180\n And I think that's at the design level.\n\n1:33:48.180 --> 1:33:50.020\n I think they're designed to have self preservation\n\n1:33:50.020 --> 1:33:52.540\n to be a focus.\n\n1:33:52.540 --> 1:33:53.380\n So you're right.\n\n1:33:53.380 --> 1:33:58.380\n In that broader system that we're also a part of\n\n1:33:58.620 --> 1:34:00.300\n and can have some influence on,\n\n1:34:02.460 --> 1:34:04.780\n it is much more complicated, much more powerful.\n\n1:34:04.780 --> 1:34:05.980\n Yeah, I agree with that.\n\n1:34:06.980 --> 1:34:09.380\n So people really love it when I ask,\n\n1:34:09.380 --> 1:34:13.500\n what three books, technical, philosophical, fiction\n\n1:34:13.500 --> 1:34:14.860\n had a big impact on your life?\n\n1:34:14.860 --> 1:34:16.180\n Maybe you can recommend.\n\n1:34:16.180 --> 1:34:21.180\n We went with movies, we went with Billy Joe\n\n1:34:21.260 --> 1:34:24.460\n and I forgot what music you recommended, but.\n\n1:34:24.460 --> 1:34:26.580\n I didn't, I just said I have no taste in music.\n\n1:34:26.580 --> 1:34:27.740\n I just like pop music.\n\n1:34:27.740 --> 1:34:30.020\n That was actually really skillful\n\n1:34:30.020 --> 1:34:30.860\n the way you avoided that question.\n\n1:34:30.860 --> 1:34:31.700\n Thank you, thanks.\n\n1:34:31.700 --> 1:34:33.780\n I'm gonna try to do the same with the books.\n\n1:34:33.780 --> 1:34:37.300\n So do you have a skillful way to avoid answering\n\n1:34:37.300 --> 1:34:39.820\n the question about three books you would recommend?\n\n1:34:39.820 --> 1:34:41.220\n I'd like to tell you a story.\n\n1:34:42.900 --> 1:34:45.900\n So my first job out of college was at Bellcore.\n\n1:34:45.900 --> 1:34:48.180\n I mentioned that before, where I worked with Dave Ackley.\n\n1:34:48.180 --> 1:34:50.180\n The head of the group was a guy named Tom Landauer.\n\n1:34:50.180 --> 1:34:53.580\n And I don't know how well known he's known now,\n\n1:34:53.580 --> 1:34:56.260\n but arguably he's the inventor\n\n1:34:56.260 --> 1:34:59.100\n and the first proselytizer of word embeddings.\n\n1:34:59.100 --> 1:35:02.500\n So they developed a system shortly before I got to the group\n\n1:35:04.740 --> 1:35:07.700\n that was called latent semantic analysis\n\n1:35:07.700 --> 1:35:09.300\n that would take words of English\n\n1:35:09.300 --> 1:35:12.780\n and embed them in multi hundred dimensional space\n\n1:35:12.780 --> 1:35:15.740\n and then use that as a way of assessing\n\n1:35:15.740 --> 1:35:17.860\n similarity and basically doing reinforcement learning,\n\n1:35:17.860 --> 1:35:20.940\n I'm sorry, not reinforcement, information retrieval,\n\n1:35:20.940 --> 1:35:23.460\n sort of pre Google information retrieval.\n\n1:35:23.460 --> 1:35:28.060\n And he was trained as an anthropologist,\n\n1:35:28.060 --> 1:35:29.780\n but then became a cognitive scientist.\n\n1:35:29.780 --> 1:35:32.020\n So I was in the cognitive science research group.\n\n1:35:32.020 --> 1:35:34.980\n Like I said, I'm a cognitive science groupie.\n\n1:35:34.980 --> 1:35:37.100\n At the time I thought I'd become a cognitive scientist,\n\n1:35:37.100 --> 1:35:38.740\n but then I realized in that group,\n\n1:35:38.740 --> 1:35:40.380\n no, I'm a computer scientist,\n\n1:35:40.380 --> 1:35:41.780\n but I'm a computer scientist who really loves\n\n1:35:41.780 --> 1:35:43.660\n to hang out with cognitive scientists.\n\n1:35:43.660 --> 1:35:48.660\n And he said, he studied language acquisition in particular.\n\n1:35:48.660 --> 1:35:51.500\n He said, you know, humans have about this number of words\n\n1:35:51.500 --> 1:35:55.540\n of vocabulary and most of that is learned from reading.\n\n1:35:55.540 --> 1:35:57.260\n And I said, that can't be true\n\n1:35:57.260 --> 1:36:00.580\n because I have a really big vocabulary and I don't read.\n\n1:36:00.580 --> 1:36:01.420\n He's like, you must.\n\n1:36:01.420 --> 1:36:03.020\n I'm like, I don't think I do.\n\n1:36:03.020 --> 1:36:05.740\n I mean like stop signs, I definitely read stop signs,\n\n1:36:05.740 --> 1:36:08.900\n but like reading books is not a thing that I do a lot of.\n\n1:36:08.900 --> 1:36:09.860\n Do you really though?\n\n1:36:09.860 --> 1:36:12.260\n It might be just visual, maybe the red color.\n\n1:36:12.260 --> 1:36:14.340\n Do I read stop signs?\n\n1:36:14.340 --> 1:36:15.900\n No, it's just pattern recognition at this point.\n\n1:36:15.900 --> 1:36:16.900\n I don't sound it out.\n\n1:36:19.740 --> 1:36:20.580\n So now I do.\n\n1:36:21.780 --> 1:36:25.140\n I wonder what that, oh yeah, stop the guns.\n\n1:36:25.140 --> 1:36:26.620\n So.\n\n1:36:26.620 --> 1:36:27.460\n That's fascinating.\n\n1:36:27.460 --> 1:36:28.300\n So you don't.\n\n1:36:28.300 --> 1:36:29.700\n So I don't read very, I mean, obviously I read\n\n1:36:29.700 --> 1:36:31.980\n and I've read plenty of books,\n\n1:36:31.980 --> 1:36:34.020\n but like some people like Charles,\n\n1:36:34.020 --> 1:36:35.940\n my friend Charles and others,\n\n1:36:35.940 --> 1:36:38.620\n like a lot of people in my field, a lot of academics,\n\n1:36:38.620 --> 1:36:42.260\n like reading was really a central topic to them\n\n1:36:42.260 --> 1:36:45.100\n in development and I'm not that guy.\n\n1:36:45.100 --> 1:36:49.420\n In fact, I used to joke that when I got into college,\n\n1:36:49.420 --> 1:36:53.740\n that it was on kind of a help out the illiterate\n\n1:36:53.740 --> 1:36:55.180\n kind of program because I got to,\n\n1:36:55.180 --> 1:36:57.260\n like in my house, I wasn't a particularly bad\n\n1:36:57.260 --> 1:36:58.740\n or good reader, but when I got to college,\n\n1:36:58.740 --> 1:37:01.900\n I was surrounded by these people that were just voracious\n\n1:37:01.900 --> 1:37:03.380\n in their reading appetite.\n\n1:37:03.380 --> 1:37:04.900\n And they would like, have you read this?\n\n1:37:04.900 --> 1:37:05.740\n Have you read this?\n\n1:37:05.740 --> 1:37:06.580\n Have you read this?\n\n1:37:06.580 --> 1:37:09.060\n And I'm like, no, I'm clearly not qualified\n\n1:37:09.060 --> 1:37:10.220\n to be at this school.\n\n1:37:10.220 --> 1:37:11.700\n Like there's no way I should be here.\n\n1:37:11.700 --> 1:37:14.780\n Now I've discovered books on tape, like audio books.\n\n1:37:14.780 --> 1:37:17.580\n And so I'm much better.\n\n1:37:17.580 --> 1:37:18.420\n I'm more caught up.\n\n1:37:18.420 --> 1:37:20.260\n I read a lot of books.\n\n1:37:20.260 --> 1:37:22.140\n The small tangent on that,\n\n1:37:22.140 --> 1:37:24.620\n it is a fascinating open question to me\n\n1:37:24.620 --> 1:37:27.020\n on the topic of driving.\n\n1:37:27.020 --> 1:37:30.980\n Whether, you know, supervised learning people,\n\n1:37:30.980 --> 1:37:33.860\n machine learning people think you have to like drive\n\n1:37:33.860 --> 1:37:35.900\n to learn how to drive.\n\n1:37:35.900 --> 1:37:40.020\n To me, it's very possible that just by us humans,\n\n1:37:40.020 --> 1:37:41.500\n by first of all, walking,\n\n1:37:41.500 --> 1:37:44.140\n but also by watching other people drive,\n\n1:37:44.140 --> 1:37:46.500\n not even being inside cars as a passenger,\n\n1:37:46.500 --> 1:37:49.260\n but let's say being inside the car as a passenger,\n\n1:37:49.260 --> 1:37:53.340\n but even just like being a pedestrian and crossing the road,\n\n1:37:53.340 --> 1:37:56.260\n you learn so much about driving from that.\n\n1:37:56.260 --> 1:37:58.660\n It's very possible that you can,\n\n1:37:58.660 --> 1:38:01.300\n without ever being inside of a car,\n\n1:38:01.300 --> 1:38:04.420\n be okay at driving once you get in it.\n\n1:38:04.420 --> 1:38:06.380\n Or like watching a movie, for example.\n\n1:38:06.380 --> 1:38:08.100\n I don't know, something like that.\n\n1:38:08.100 --> 1:38:11.140\n Have you taught anyone to drive?\n\n1:38:11.140 --> 1:38:13.460\n No, except myself.\n\n1:38:13.460 --> 1:38:15.020\n I have two children.\n\n1:38:15.020 --> 1:38:18.740\n And I learned a lot about car driving\n\n1:38:18.740 --> 1:38:21.060\n because my wife doesn't want to be the one in the car\n\n1:38:21.060 --> 1:38:21.900\n while they're learning.\n\n1:38:21.900 --> 1:38:22.980\n So that's my job.\n\n1:38:22.980 --> 1:38:25.860\n So I sit in the passenger seat and it's really scary.\n\n1:38:27.260 --> 1:38:29.380\n You know, I have wishes to live\n\n1:38:30.460 --> 1:38:32.260\n and they're figuring things out.\n\n1:38:32.260 --> 1:38:37.140\n Now, they start off very much better\n\n1:38:37.140 --> 1:38:39.700\n than I imagine like a neural network would, right?\n\n1:38:39.700 --> 1:38:41.660\n They get that they're seeing the world.\n\n1:38:41.660 --> 1:38:44.100\n They get that there's a road that they're trying to be on.\n\n1:38:44.100 --> 1:38:45.420\n They get that there's a relationship\n\n1:38:45.420 --> 1:38:47.020\n between the angle of the steering,\n\n1:38:47.020 --> 1:38:49.540\n but it takes a while to not be very jerky.\n\n1:38:51.020 --> 1:38:52.340\n And so that happens pretty quickly.\n\n1:38:52.340 --> 1:38:55.100\n Like the ability to stay in lane at speed,\n\n1:38:55.100 --> 1:38:56.940\n that happens relatively fast.\n\n1:38:56.940 --> 1:39:00.140\n It's not zero shot learning, but it's pretty fast.\n\n1:39:00.140 --> 1:39:01.900\n The thing that's remarkably hard,\n\n1:39:01.900 --> 1:39:03.860\n and this is I think partly why self driving cars\n\n1:39:03.860 --> 1:39:04.780\n are really hard,\n\n1:39:04.780 --> 1:39:06.700\n is the degree to which driving\n\n1:39:06.700 --> 1:39:09.460\n is a social interaction activity.\n\n1:39:09.460 --> 1:39:10.380\n And that blew me away.\n\n1:39:10.380 --> 1:39:11.940\n I was completely unaware of it\n\n1:39:11.940 --> 1:39:14.260\n until I watched my son learning to drive.\n\n1:39:14.260 --> 1:39:17.780\n And I was realizing that he was sending signals\n\n1:39:17.780 --> 1:39:19.420\n to all the cars around him.\n\n1:39:19.420 --> 1:39:20.980\n And those in his case,\n\n1:39:20.980 --> 1:39:25.940\n he's always had social communication challenges.\n\n1:39:25.940 --> 1:39:28.220\n He was sending very mixed confusing signals\n\n1:39:28.220 --> 1:39:29.060\n to the other cars.\n\n1:39:29.060 --> 1:39:30.460\n And that was causing the other cars\n\n1:39:30.460 --> 1:39:32.540\n to drive weirdly and erratically.\n\n1:39:32.540 --> 1:39:34.300\n And there was no question in my mind\n\n1:39:34.300 --> 1:39:36.620\n that he would have an accident\n\n1:39:36.620 --> 1:39:39.860\n because they didn't know how to read him.\n\n1:39:39.860 --> 1:39:42.220\n There's things you do with the speed that you drive,\n\n1:39:42.220 --> 1:39:43.740\n the positioning of your car,\n\n1:39:43.740 --> 1:39:46.220\n that you're constantly like in the head\n\n1:39:46.220 --> 1:39:47.580\n of the other drivers.\n\n1:39:47.580 --> 1:39:50.740\n And seeing him not knowing how to do that\n\n1:39:50.740 --> 1:39:52.220\n and having to be taught explicitly,\n\n1:39:52.220 --> 1:39:53.420\n okay, you have to be thinking\n\n1:39:53.420 --> 1:39:55.980\n about what the other driver is thinking,\n\n1:39:55.980 --> 1:39:57.460\n was a revelation to me.\n\n1:39:57.460 --> 1:39:58.780\n I was stunned.\n\n1:39:58.780 --> 1:40:02.980\n So creating kind of theories of mind of the other.\n\n1:40:02.980 --> 1:40:04.740\n Theories of mind of the other cars.\n\n1:40:04.740 --> 1:40:05.580\n Yeah, yeah.\n\n1:40:05.580 --> 1:40:07.260\n Which I just hadn't heard discussed\n\n1:40:07.260 --> 1:40:09.700\n in the self driving car talks that I've been to.\n\n1:40:09.700 --> 1:40:13.620\n Since then, there's some people who do consider\n\n1:40:13.620 --> 1:40:14.460\n those kinds of issues,\n\n1:40:14.460 --> 1:40:16.140\n but it's way more subtle than I think\n\n1:40:16.140 --> 1:40:19.140\n there's a little bit of work involved with that\n\n1:40:19.140 --> 1:40:21.340\n when you realize like when you especially focus\n\n1:40:21.340 --> 1:40:24.260\n not on other cars, but on pedestrians, for example,\n\n1:40:24.260 --> 1:40:27.620\n it's literally staring you in the face.\n\n1:40:27.620 --> 1:40:28.700\n So then when you're just like,\n\n1:40:28.700 --> 1:40:30.740\n how do I interact with pedestrians?\n\n1:40:32.020 --> 1:40:33.340\n Pedestrians, you're practically talking\n\n1:40:33.340 --> 1:40:34.460\n to an octopus at that point.\n\n1:40:34.460 --> 1:40:36.180\n They've got all these weird degrees of freedom.\n\n1:40:36.180 --> 1:40:37.140\n You don't know what they're gonna do.\n\n1:40:37.140 --> 1:40:38.420\n They can turn around any second.\n\n1:40:38.420 --> 1:40:42.020\n But the point is, we humans know what they're gonna do.\n\n1:40:42.020 --> 1:40:43.860\n Like we have a good theory of mind.\n\n1:40:43.860 --> 1:40:46.740\n We have a good mental model of what they're doing.\n\n1:40:46.740 --> 1:40:50.460\n And we have a good model of the model they have a view\n\n1:40:50.460 --> 1:40:52.020\n and the model of the model of the model.\n\n1:40:52.020 --> 1:40:55.540\n Like we're able to kind of reason about this kind of,\n\n1:40:55.540 --> 1:40:59.980\n the social like game of it all.\n\n1:40:59.980 --> 1:41:03.180\n The hope is that it's quite simple actually,\n\n1:41:03.180 --> 1:41:04.340\n that it could be learned.\n\n1:41:04.340 --> 1:41:06.180\n That's why I just talked to the Waymo.\n\n1:41:06.180 --> 1:41:07.540\n I don't know if you know that company.\n\n1:41:07.540 --> 1:41:09.340\n It's Google South Africa.\n\n1:41:09.340 --> 1:41:12.900\n They, I talked to their CTO about this podcast\n\n1:41:12.900 --> 1:41:15.340\n and they like, I rode in their car\n\n1:41:15.340 --> 1:41:17.820\n and it's quite aggressive and it's quite fast\n\n1:41:17.820 --> 1:41:20.060\n and it's good and it feels great.\n\n1:41:20.060 --> 1:41:21.860\n It also, just like Tesla,\n\n1:41:21.860 --> 1:41:24.580\n Waymo made me change my mind about like,\n\n1:41:24.580 --> 1:41:27.540\n maybe driving is easier than I thought.\n\n1:41:27.540 --> 1:41:32.540\n Maybe I'm just being speciest, human centric, maybe.\n\n1:41:33.260 --> 1:41:35.100\n It's a speciest argument.\n\n1:41:35.100 --> 1:41:36.620\n Yeah, so I don't know.\n\n1:41:36.620 --> 1:41:40.060\n But it's fascinating to think about like the same\n\n1:41:41.220 --> 1:41:43.860\n as with reading, which I think you just said.\n\n1:41:43.860 --> 1:41:45.380\n You avoided the question,\n\n1:41:45.380 --> 1:41:47.100\n though I still hope you answered it somewhat.\n\n1:41:47.100 --> 1:41:48.620\n You avoided it brilliantly.\n\n1:41:48.620 --> 1:41:52.140\n It is, there's blind spots as artificial intelligence,\n\n1:41:52.140 --> 1:41:55.140\n that artificial intelligence researchers have\n\n1:41:55.140 --> 1:41:58.820\n about what it actually takes to learn to solve a problem.\n\n1:41:58.820 --> 1:41:59.660\n That's fascinating.\n\n1:41:59.660 --> 1:42:00.820\n Have you had Anca Dragan on?\n\n1:42:00.820 --> 1:42:01.660\n Yeah.\n\n1:42:01.660 --> 1:42:02.500\n Okay.\n\n1:42:02.500 --> 1:42:03.320\n She's one of my favorites.\n\n1:42:03.320 --> 1:42:04.160\n So much energy.\n\n1:42:04.160 --> 1:42:05.000\n She's right.\n\n1:42:05.000 --> 1:42:05.820\n Oh, yeah.\n\n1:42:05.820 --> 1:42:06.660\n She's amazing.\n\n1:42:06.660 --> 1:42:07.500\n Fantastic.\n\n1:42:07.500 --> 1:42:10.380\n And in particular, she thinks a lot about this kind of,\n\n1:42:10.380 --> 1:42:12.820\n I know that you know that I know kind of planning.\n\n1:42:12.820 --> 1:42:14.820\n And the last time I spoke with her,\n\n1:42:14.820 --> 1:42:17.340\n she was very articulate about the ways\n\n1:42:17.340 --> 1:42:20.060\n in which self driving cars are not solved.\n\n1:42:20.060 --> 1:42:22.100\n Like what's still really, really hard.\n\n1:42:22.100 --> 1:42:23.780\n But even her intuition is limited.\n\n1:42:23.780 --> 1:42:26.060\n Like we're all like new to this.\n\n1:42:26.060 --> 1:42:27.900\n So in some sense, the Elon Musk approach\n\n1:42:27.900 --> 1:42:30.300\n of being ultra confident and just like plowing.\n\n1:42:30.300 --> 1:42:31.140\n Put it out there.\n\n1:42:31.140 --> 1:42:32.180\n Putting it out there.\n\n1:42:32.180 --> 1:42:35.340\n Like some people say it's reckless and dangerous and so on.\n\n1:42:35.340 --> 1:42:39.060\n But like, partly it's like, it seems to be one\n\n1:42:39.060 --> 1:42:40.500\n of the only ways to make progress\n\n1:42:40.500 --> 1:42:41.540\n in artificial intelligence.\n\n1:42:41.540 --> 1:42:45.540\n So it's, you know, these are difficult things.\n\n1:42:45.540 --> 1:42:48.420\n You know, democracy is messy.\n\n1:42:49.360 --> 1:42:51.940\n Implementation of artificial intelligence systems\n\n1:42:51.940 --> 1:42:53.980\n in the real world is messy.\n\n1:42:53.980 --> 1:42:56.260\n So many years ago, before self driving cars\n\n1:42:56.260 --> 1:42:58.500\n were an actual thing you could have a discussion about,\n\n1:42:58.500 --> 1:43:01.820\n somebody asked me, like, what if we could use\n\n1:43:01.820 --> 1:43:04.780\n that robotic technology and use it to drive cars around?\n\n1:43:04.780 --> 1:43:06.580\n Like, isn't that, aren't people gonna be killed?\n\n1:43:06.580 --> 1:43:08.060\n And then it's not, you know, blah, blah, blah.\n\n1:43:08.060 --> 1:43:09.700\n I'm like, that's not what's gonna happen.\n\n1:43:09.700 --> 1:43:12.200\n I said with confidence, incorrectly, obviously.\n\n1:43:13.320 --> 1:43:15.820\n What I think is gonna happen is we're gonna have a lot more,\n\n1:43:15.820 --> 1:43:17.580\n like a very gradual kind of rollout\n\n1:43:17.580 --> 1:43:22.540\n where people have these cars in like closed communities,\n\n1:43:22.540 --> 1:43:24.480\n right, where it's somewhat realistic,\n\n1:43:24.480 --> 1:43:26.660\n but it's still in a box, right?\n\n1:43:26.660 --> 1:43:28.980\n So that we can really get a sense of what,\n\n1:43:28.980 --> 1:43:30.620\n what are the weird things that can happen?\n\n1:43:30.620 --> 1:43:34.580\n How do we, how do we have to change the way we behave\n\n1:43:34.580 --> 1:43:35.700\n around these vehicles?\n\n1:43:35.700 --> 1:43:39.500\n Like, it's obviously requires a kind of co evolution\n\n1:43:39.500 --> 1:43:42.720\n that you can't just plop them in and see what happens.\n\n1:43:42.720 --> 1:43:44.240\n But of course, we're basically popping them in\n\n1:43:44.240 --> 1:43:45.080\n and see what happens.\n\n1:43:45.080 --> 1:43:46.860\n So I was wrong, but I do think that would have been\n\n1:43:46.860 --> 1:43:47.900\n a better plan.\n\n1:43:47.900 --> 1:43:50.600\n So that's, but your intuition, that's funny,\n\n1:43:50.600 --> 1:43:54.180\n just zooming out and looking at the forces of capitalism.\n\n1:43:54.180 --> 1:43:57.700\n And it seems that capitalism rewards risk takers\n\n1:43:57.700 --> 1:44:00.860\n and rewards and punishes risk takers, like,\n\n1:44:00.860 --> 1:44:03.900\n and like, try it out.\n\n1:44:03.900 --> 1:44:08.900\n The academic approach to let's try a small thing\n\n1:44:11.200 --> 1:44:13.980\n and try to understand slowly the fundamentals\n\n1:44:13.980 --> 1:44:14.820\n of the problem.\n\n1:44:14.820 --> 1:44:18.420\n And let's start with one, then do two, and then see that.\n\n1:44:18.420 --> 1:44:21.900\n And then do the three, you know, the capitalist\n\n1:44:21.900 --> 1:44:26.180\n like startup entrepreneurial dream is let's build a thousand\n\n1:44:26.180 --> 1:44:27.020\n and let's.\n\n1:44:27.020 --> 1:44:28.820\n Right, and 500 of them fail, but whatever,\n\n1:44:28.820 --> 1:44:30.680\n the other 500, we learned from them.\n\n1:44:30.680 --> 1:44:33.340\n But if you're good enough, I mean, one thing is like,\n\n1:44:33.340 --> 1:44:35.740\n your intuition would say like, that's gonna be\n\n1:44:35.740 --> 1:44:37.940\n hugely destructive to everything.\n\n1:44:37.940 --> 1:44:42.260\n But actually, it's kind of the forces of capitalism,\n\n1:44:42.260 --> 1:44:44.940\n like people are quite, it's easy to be critical,\n\n1:44:44.940 --> 1:44:47.780\n but if you actually look at the data at the way\n\n1:44:47.780 --> 1:44:50.660\n our world has progressed in terms of the quality of life,\n\n1:44:50.660 --> 1:44:54.700\n it seems like the competent good people rise to the top.\n\n1:44:54.700 --> 1:44:58.500\n This is coming from me from the Soviet Union and so on.\n\n1:44:58.500 --> 1:45:03.500\n It's like, it's interesting that somebody like Elon Musk\n\n1:45:03.540 --> 1:45:08.060\n is the way you push progress in artificial intelligence.\n\n1:45:08.060 --> 1:45:11.580\n Like it's forcing Waymo to step their stuff up\n\n1:45:11.580 --> 1:45:16.580\n and Waymo is forcing Elon Musk to step up.\n\n1:45:17.020 --> 1:45:21.180\n It's fascinating, because I have this tension in my heart\n\n1:45:21.180 --> 1:45:26.100\n and just being upset by the lack of progress\n\n1:45:26.100 --> 1:45:29.760\n in autonomous vehicles within academia.\n\n1:45:29.760 --> 1:45:33.580\n So there's a huge progress in the early days\n\n1:45:33.580 --> 1:45:35.620\n of the DARPA challenges.\n\n1:45:35.620 --> 1:45:39.260\n And then it just kind of stopped like at MIT,\n\n1:45:39.260 --> 1:45:43.060\n but it's true everywhere else with an exception\n\n1:45:43.060 --> 1:45:46.940\n of a few sponsors here and there is like,\n\n1:45:46.940 --> 1:45:50.260\n it's not seen as a sexy problem, Thomas.\n\n1:45:50.260 --> 1:45:53.900\n Like the moment artificial intelligence starts approaching\n\n1:45:53.900 --> 1:45:56.180\n the problems of the real world,\n\n1:45:56.180 --> 1:46:00.300\n like academics kind of like, all right, let the...\n\n1:46:00.300 --> 1:46:01.860\n They get really hard in a different way.\n\n1:46:01.860 --> 1:46:03.260\n In a different way, that's right.\n\n1:46:03.260 --> 1:46:05.880\n I think, yeah, right, some of us are not excited\n\n1:46:05.880 --> 1:46:07.220\n about that other way.\n\n1:46:07.220 --> 1:46:09.540\n But I still think there's fundamentals problems\n\n1:46:09.540 --> 1:46:12.140\n to be solved in those difficult things.\n\n1:46:12.140 --> 1:46:14.700\n It's not, it's still publishable, I think.\n\n1:46:14.700 --> 1:46:17.100\n Like we just need to, it's the same criticism\n\n1:46:17.100 --> 1:46:20.300\n you could have of all these conferences in Europe, CVPR,\n\n1:46:20.300 --> 1:46:24.340\n where application papers are often as powerful\n\n1:46:24.340 --> 1:46:27.420\n and as important as like a theory paper.\n\n1:46:27.420 --> 1:46:31.300\n Even like theory just seems much more respectable and so on.\n\n1:46:31.300 --> 1:46:32.860\n I mean, machine learning community is changing\n\n1:46:32.860 --> 1:46:33.820\n that a little bit.\n\n1:46:33.820 --> 1:46:35.380\n I mean, at least in statements,\n\n1:46:35.380 --> 1:46:40.300\n but it's still not seen as the sexiest of pursuits,\n\n1:46:40.300 --> 1:46:42.060\n which is like, how do I actually make this thing\n\n1:46:42.060 --> 1:46:45.280\n work in practice as opposed to on this toy data set?\n\n1:46:47.060 --> 1:46:49.860\n All that to say, are you still avoiding\n\n1:46:49.860 --> 1:46:50.900\n the three books question?\n\n1:46:50.900 --> 1:46:54.620\n Is there something on audio book that you can recommend?\n\n1:46:54.620 --> 1:46:58.740\n Oh, yeah, I mean, yeah, I've read a lot of really fun stuff.\n\n1:46:58.740 --> 1:47:02.140\n In terms of books that I find myself thinking back on\n\n1:47:02.140 --> 1:47:03.460\n that I read a while ago,\n\n1:47:03.460 --> 1:47:06.380\n like that stood the test of time to some degree.\n\n1:47:06.380 --> 1:47:09.200\n I find myself thinking of program or be programmed a lot\n\n1:47:09.200 --> 1:47:13.980\n by Douglas Roschkopf, which was,\n\n1:47:13.980 --> 1:47:15.780\n it basically put out the premise\n\n1:47:15.780 --> 1:47:19.180\n that we all need to become programmers\n\n1:47:19.180 --> 1:47:21.200\n in one form or another.\n\n1:47:21.200 --> 1:47:24.180\n And it was an analogy to once upon a time\n\n1:47:24.180 --> 1:47:26.500\n we all had to become readers.\n\n1:47:26.500 --> 1:47:27.600\n We had to become literate.\n\n1:47:27.600 --> 1:47:28.860\n And there was a time before that\n\n1:47:28.860 --> 1:47:30.060\n when not everybody was literate,\n\n1:47:30.060 --> 1:47:31.740\n but once literacy was possible,\n\n1:47:31.740 --> 1:47:36.080\n the people who were literate had more of a say in society\n\n1:47:36.080 --> 1:47:37.660\n than the people who weren't.\n\n1:47:37.660 --> 1:47:39.700\n And so we made a big effort to get everybody up to speed.\n\n1:47:39.700 --> 1:47:44.000\n And now it's not 100% universal, but it's quite widespread.\n\n1:47:44.000 --> 1:47:47.220\n Like the assumption is generally that people can read.\n\n1:47:48.340 --> 1:47:50.600\n The analogy that he makes is that programming\n\n1:47:50.600 --> 1:47:51.760\n is a similar kind of thing,\n\n1:47:51.760 --> 1:47:56.760\n that we need to have a say in, right?\n\n1:47:57.100 --> 1:47:59.780\n So being a reader, being literate, being a reader means\n\n1:47:59.780 --> 1:48:01.900\n you can receive all this information,\n\n1:48:01.900 --> 1:48:04.260\n but you don't get to put it out there.\n\n1:48:04.260 --> 1:48:06.720\n And programming is the way that we get to put it out there.\n\n1:48:06.720 --> 1:48:07.740\n And that was the argument that he made.\n\n1:48:07.740 --> 1:48:11.780\n I think he specifically has now backed away from this idea.\n\n1:48:11.780 --> 1:48:14.880\n He doesn't think it's happening quite this way.\n\n1:48:14.880 --> 1:48:17.500\n And that might be true that it didn't,\n\n1:48:17.500 --> 1:48:20.740\n society didn't sort of play forward quite that way.\n\n1:48:20.740 --> 1:48:22.220\n I still believe in the premise.\n\n1:48:22.220 --> 1:48:24.460\n I still believe that at some point,\n\n1:48:24.460 --> 1:48:26.420\n the relationship that we have to these machines\n\n1:48:26.420 --> 1:48:29.260\n and these networks has to be one of each individual\n\n1:48:29.260 --> 1:48:34.260\n can, has the wherewithal to make the machines help them.\n\n1:48:34.940 --> 1:48:37.140\n Do the things that that person wants done.\n\n1:48:37.140 --> 1:48:40.140\n And as software people, we know how to do that.\n\n1:48:40.140 --> 1:48:41.500\n And when we have a problem, we're like, okay,\n\n1:48:41.500 --> 1:48:43.380\n I'll just, I'll hack up a Pearl script or something\n\n1:48:43.380 --> 1:48:44.900\n and make it so.\n\n1:48:44.900 --> 1:48:47.260\n If we lived in a world where everybody could do that,\n\n1:48:47.260 --> 1:48:49.260\n that would be a better world.\n\n1:48:49.260 --> 1:48:53.780\n And computers would be, have, I think less sway over us.\n\n1:48:53.780 --> 1:48:56.920\n And other people's software would have less sway over us\n\n1:48:56.920 --> 1:48:57.760\n as a group.\n\n1:48:57.760 --> 1:49:00.860\n In some sense, software engineering, programming is power.\n\n1:49:00.860 --> 1:49:03.100\n Programming is power, right?\n\n1:49:03.100 --> 1:49:04.220\n Yeah, it's like magic.\n\n1:49:04.220 --> 1:49:05.420\n It's like magic spells.\n\n1:49:05.420 --> 1:49:09.220\n And it's not out of reach of everyone.\n\n1:49:09.220 --> 1:49:11.780\n But at the moment, it's just a sliver of the population\n\n1:49:11.780 --> 1:49:15.300\n who can commune with machines in this way.\n\n1:49:15.300 --> 1:49:18.460\n So I don't know, so that book had a big impact on me.\n\n1:49:18.460 --> 1:49:20.900\n Currently, I'm reading The Alignment Problem,\n\n1:49:20.900 --> 1:49:22.180\n actually by Brian Christian.\n\n1:49:22.180 --> 1:49:23.660\n So I don't know if you've seen this out there yet.\n\n1:49:23.660 --> 1:49:25.380\n Is this similar to Stuart Russell's work\n\n1:49:25.380 --> 1:49:27.040\n with the control problem?\n\n1:49:27.040 --> 1:49:28.860\n It's in that same general neighborhood.\n\n1:49:28.860 --> 1:49:31.320\n I mean, they have different emphases\n\n1:49:31.320 --> 1:49:32.540\n that they're concentrating on.\n\n1:49:32.540 --> 1:49:36.380\n I think Stuart's book did a remarkably good job,\n\n1:49:36.380 --> 1:49:38.940\n like just a celebratory good job\n\n1:49:38.940 --> 1:49:43.220\n at describing AI technology and sort of how it works.\n\n1:49:43.220 --> 1:49:44.180\n I thought that was great.\n\n1:49:44.180 --> 1:49:46.500\n It was really cool to see that in a book.\n\n1:49:46.500 --> 1:49:49.540\n I think he has some experience writing some books.\n\n1:49:49.540 --> 1:49:52.100\n You know, that's probably a possible thing.\n\n1:49:52.100 --> 1:49:53.620\n He's maybe thought a thing or two\n\n1:49:53.620 --> 1:49:56.200\n about how to explain AI to people.\n\n1:49:56.200 --> 1:49:57.820\n Yeah, that's a really good point.\n\n1:49:57.820 --> 1:50:00.720\n This book so far has been remarkably good\n\n1:50:00.720 --> 1:50:04.860\n at telling the story of sort of the history,\n\n1:50:04.860 --> 1:50:07.060\n the recent history of some of the things\n\n1:50:07.060 --> 1:50:08.420\n that have happened.\n\n1:50:08.420 --> 1:50:09.600\n I'm in the first third.\n\n1:50:09.600 --> 1:50:10.980\n He said this book is in three thirds.\n\n1:50:10.980 --> 1:50:14.540\n The first third is essentially AI fairness\n\n1:50:14.540 --> 1:50:16.860\n and implications of AI on society\n\n1:50:16.860 --> 1:50:18.420\n that we're seeing right now.\n\n1:50:18.420 --> 1:50:19.720\n And that's been great.\n\n1:50:19.720 --> 1:50:21.220\n I mean, he's telling the stories really well.\n\n1:50:21.220 --> 1:50:23.700\n He went out and talked to the frontline people\n\n1:50:23.700 --> 1:50:26.620\n whose names were associated with some of these ideas\n\n1:50:26.620 --> 1:50:28.220\n and it's been terrific.\n\n1:50:28.220 --> 1:50:29.420\n He says the second half of the book\n\n1:50:29.420 --> 1:50:30.700\n is on reinforcement learning.\n\n1:50:30.700 --> 1:50:33.220\n So maybe that'll be fun.\n\n1:50:33.220 --> 1:50:36.420\n And then the third half, third third,\n\n1:50:36.420 --> 1:50:39.980\n is on the super intelligence alignment problem.\n\n1:50:39.980 --> 1:50:43.360\n And I suspect that that part will be less fun\n\n1:50:43.360 --> 1:50:44.320\n for me to read.\n\n1:50:44.320 --> 1:50:45.160\n Yeah.\n\n1:50:46.260 --> 1:50:48.940\n Yeah, it's an interesting problem to talk about.\n\n1:50:48.940 --> 1:50:50.740\n I find it to be the most interesting,\n\n1:50:50.740 --> 1:50:52.560\n just like thinking about whether we live\n\n1:50:52.560 --> 1:50:54.060\n in a simulation or not,\n\n1:50:54.060 --> 1:50:58.280\n as a thought experiment to think about our own existence.\n\n1:50:58.280 --> 1:50:59.700\n So in the same way,\n\n1:50:59.700 --> 1:51:02.260\n talking about alignment problem with AGI\n\n1:51:02.260 --> 1:51:04.180\n is a good way to think similar\n\n1:51:04.180 --> 1:51:06.660\n to like the trolley problem with autonomous vehicles.\n\n1:51:06.660 --> 1:51:08.520\n It's a useless thing for engineering,\n\n1:51:08.520 --> 1:51:10.900\n but it's a nice little thought experiment\n\n1:51:10.900 --> 1:51:13.580\n for actually thinking about what are like\n\n1:51:13.580 --> 1:51:17.180\n our own human ethical systems, our moral systems\n\n1:51:17.180 --> 1:51:22.180\n to by thinking how we engineer these things,\n\n1:51:23.100 --> 1:51:24.780\n you start to understand yourself.\n\n1:51:25.660 --> 1:51:27.180\n So sci fi can be good at that too.\n\n1:51:27.180 --> 1:51:29.020\n So one sci fi book to recommend\n\n1:51:29.020 --> 1:51:31.900\n is Exhalations by Ted Chiang,\n\n1:51:31.900 --> 1:51:33.880\n bunch of short stories.\n\n1:51:33.880 --> 1:51:35.940\n This Ted Chiang is the guy who wrote the short story\n\n1:51:35.940 --> 1:51:37.780\n that became the movie Arrival.\n\n1:51:38.660 --> 1:51:41.660\n And all of his stories just from a,\n\n1:51:41.660 --> 1:51:43.340\n he was a computer scientist,\n\n1:51:43.340 --> 1:51:44.740\n actually he studied at Brown.\n\n1:51:44.740 --> 1:51:49.140\n And they all have this sort of really insightful bit\n\n1:51:49.140 --> 1:51:52.260\n of science or computer science that drives them.\n\n1:51:52.260 --> 1:51:54.940\n And so it's just a romp, right?\n\n1:51:54.940 --> 1:51:57.420\n To just like, he creates these artificial worlds\n\n1:51:57.420 --> 1:51:59.840\n with these by extrapolating on these ideas\n\n1:51:59.840 --> 1:52:01.460\n that we know about,\n\n1:52:01.460 --> 1:52:02.780\n but hadn't really thought through\n\n1:52:02.780 --> 1:52:04.120\n to this kind of conclusion.\n\n1:52:04.120 --> 1:52:06.460\n And so his stuff is, it's really fun to read,\n\n1:52:06.460 --> 1:52:08.620\n it's mind warping.\n\n1:52:08.620 --> 1:52:10.820\n So I'm not sure if you're familiar,\n\n1:52:10.820 --> 1:52:13.820\n I seem to mention this every other word\n\n1:52:13.820 --> 1:52:16.600\n is I'm from the Soviet Union and I'm Russian.\n\n1:52:17.820 --> 1:52:18.940\n Way too much to see us.\n\n1:52:18.940 --> 1:52:20.220\n My roots are Russian too,\n\n1:52:20.220 --> 1:52:22.580\n but a couple generations back.\n\n1:52:22.580 --> 1:52:24.240\n Well, it's probably in there somewhere.\n\n1:52:24.240 --> 1:52:28.740\n So maybe we can pull at that thread a little bit\n\n1:52:28.740 --> 1:52:31.500\n of the existential dread that we all feel.\n\n1:52:31.500 --> 1:52:32.740\n You mentioned that you,\n\n1:52:32.740 --> 1:52:34.540\n I think somewhere in the conversation you mentioned\n\n1:52:34.540 --> 1:52:38.120\n that you don't really pretty much like dying.\n\n1:52:38.120 --> 1:52:39.540\n I forget in which context,\n\n1:52:39.540 --> 1:52:41.560\n it might've been a reinforcement learning perspective.\n\n1:52:41.560 --> 1:52:42.400\n I don't know.\n\n1:52:42.400 --> 1:52:43.220\n No, you know what it was?\n\n1:52:43.220 --> 1:52:45.220\n It was in teaching my kids to drive.\n\n1:52:47.100 --> 1:52:49.860\n That's how you face your mortality, yes.\n\n1:52:49.860 --> 1:52:52.820\n From a human beings perspective\n\n1:52:52.820 --> 1:52:55.420\n or from a reinforcement learning researchers perspective,\n\n1:52:55.420 --> 1:52:57.340\n let me ask you the most absurd question.\n\n1:52:57.340 --> 1:53:00.640\n What do you think is the meaning of this whole thing?\n\n1:53:01.660 --> 1:53:05.300\n The meaning of life on this spinning rock.\n\n1:53:06.660 --> 1:53:08.980\n I mean, I think reinforcement learning researchers\n\n1:53:08.980 --> 1:53:11.380\n maybe think about this from a science perspective\n\n1:53:11.380 --> 1:53:13.680\n more often than a lot of other people, right?\n\n1:53:13.680 --> 1:53:14.940\n As a supervised learning person,\n\n1:53:14.940 --> 1:53:18.500\n you're probably not thinking about the sweep of a lifetime,\n\n1:53:18.500 --> 1:53:20.180\n but reinforcement learning agents\n\n1:53:20.180 --> 1:53:22.860\n are having little lifetimes, little weird little lifetimes.\n\n1:53:22.860 --> 1:53:25.420\n And it's hard not to project yourself\n\n1:53:25.420 --> 1:53:26.880\n into their world sometimes.\n\n1:53:27.740 --> 1:53:30.300\n But as far as the meaning of life,\n\n1:53:30.300 --> 1:53:34.060\n so when I turned 42, you may know from,\n\n1:53:34.060 --> 1:53:35.700\n that is a book I read,\n\n1:53:35.700 --> 1:53:38.940\n The Hitchhiker's Guide to the Galaxy,\n\n1:53:38.940 --> 1:53:40.100\n that that is the meaning of life.\n\n1:53:40.100 --> 1:53:43.660\n So when I turned 42, I had a meaning of life party\n\n1:53:43.660 --> 1:53:45.300\n where I invited people over\n\n1:53:45.300 --> 1:53:48.980\n and everyone shared their meaning of life.\n\n1:53:48.980 --> 1:53:50.860\n We had slides made up.\n\n1:53:50.860 --> 1:53:54.660\n And so we all sat down and did a slide presentation\n\n1:53:54.660 --> 1:53:56.700\n to each other about the meaning of life.\n\n1:53:56.700 --> 1:54:00.500\n And mine was balance.\n\n1:54:00.500 --> 1:54:02.100\n I think that life is balance.\n\n1:54:02.100 --> 1:54:06.740\n And so the activity at the party,\n\n1:54:06.740 --> 1:54:09.180\n for a 42 year old, maybe this is a little bit nonstandard,\n\n1:54:09.180 --> 1:54:12.380\n but I found all the little toys and devices that I had\n\n1:54:12.380 --> 1:54:13.620\n where you had to balance on them.\n\n1:54:13.620 --> 1:54:15.740\n You had to like stand on it and balance,\n\n1:54:15.740 --> 1:54:17.500\n or a pogo stick I brought,\n\n1:54:17.500 --> 1:54:21.480\n a rip stick, which is like a weird two wheeled skateboard.\n\n1:54:23.180 --> 1:54:26.860\n I got a unicycle, but I didn't know how to do it.\n\n1:54:26.860 --> 1:54:28.280\n I now can do it.\n\n1:54:28.280 --> 1:54:29.540\n I would love watching you try.\n\n1:54:29.540 --> 1:54:31.820\n Yeah, I'll send you a video.\n\n1:54:31.820 --> 1:54:35.460\n I'm not great, but I managed.\n\n1:54:35.460 --> 1:54:37.220\n And so balance, yeah.\n\n1:54:37.220 --> 1:54:42.220\n So my wife has a really good one that she sticks to\n\n1:54:42.460 --> 1:54:43.700\n and is probably pretty accurate.\n\n1:54:43.700 --> 1:54:47.060\n And it has to do with healthy relationships\n\n1:54:47.060 --> 1:54:51.440\n with people that you love and working hard for good causes.\n\n1:54:51.440 --> 1:54:53.700\n But to me, yeah, balance, balance in a word.\n\n1:54:53.700 --> 1:54:56.080\n That works for me.\n\n1:54:56.080 --> 1:54:57.220\n Not too much of anything,\n\n1:54:57.220 --> 1:55:00.340\n because too much of anything is iffy.\n\n1:55:00.340 --> 1:55:02.300\n That feels like a Rolling Stones song.\n\n1:55:02.300 --> 1:55:03.420\n I feel like they must be.\n\n1:55:03.420 --> 1:55:05.020\n You can't always get what you want,\n\n1:55:05.020 --> 1:55:08.460\n but if you try sometimes, you can strike a balance.\n\n1:55:09.620 --> 1:55:12.860\n Yeah, I think that's how it goes, Michael.\n\n1:55:12.860 --> 1:55:14.620\n I'll write you a parody.\n\n1:55:14.620 --> 1:55:16.220\n It's a huge honor to talk to you.\n\n1:55:16.220 --> 1:55:17.060\n This is really fun.\n\n1:55:17.060 --> 1:55:17.880\n Oh, no, the honor's mine.\n\n1:55:17.880 --> 1:55:18.800\n I've been a big fan of yours,\n\n1:55:18.800 --> 1:55:23.800\n so can't wait to see what you do next\n\n1:55:24.460 --> 1:55:27.160\n in the world of education, in the world of parody,\n\n1:55:27.160 --> 1:55:28.420\n in the world of reinforcement learning.\n\n1:55:28.420 --> 1:55:29.340\n Thanks for talking to me.\n\n1:55:29.340 --> 1:55:30.840\n My pleasure.\n\n1:55:30.840 --> 1:55:32.340\n Thank you for listening to this conversation\n\n1:55:32.340 --> 1:55:35.140\n with Michael Littman, and thank you to our sponsors,\n\n1:55:35.140 --> 1:55:37.780\n SimpliSafe, a home security company I use\n\n1:55:37.780 --> 1:55:41.680\n to monitor and protect my apartment, ExpressVPN,\n\n1:55:41.680 --> 1:55:43.420\n the VPN I've used for many years\n\n1:55:43.420 --> 1:55:45.700\n to protect my privacy on the internet,\n\n1:55:45.700 --> 1:55:48.540\n Masterclass, online courses that I enjoy\n\n1:55:48.540 --> 1:55:51.400\n from some of the most amazing humans in history,\n\n1:55:51.400 --> 1:55:55.640\n and BetterHelp, online therapy with a licensed professional.\n\n1:55:55.640 --> 1:55:58.180\n Please check out these sponsors in the description\n\n1:55:58.180 --> 1:56:00.900\n to get a discount and to support this podcast.\n\n1:56:00.900 --> 1:56:03.540\n If you enjoy this thing, subscribe on YouTube,\n\n1:56:03.540 --> 1:56:05.860\n review it with five stars on Apple Podcast,\n\n1:56:05.860 --> 1:56:08.660\n follow on Spotify, support it on Patreon,\n\n1:56:08.660 --> 1:56:12.220\n or connect with me on Twitter at Lex Friedman.\n\n1:56:12.220 --> 1:56:14.660\n And now, let me leave you with some words\n\n1:56:14.660 --> 1:56:16.760\n from Groucho Marx.\n\n1:56:16.760 --> 1:56:20.700\n If you're not having fun, you're doing something wrong.\n\n1:56:20.700 --> 1:56:32.300\n Thank you for listening, and hope to see you next time.\n\n"
}