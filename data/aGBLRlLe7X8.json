{
  "title": "Oriol Vinyals: Deep Learning and Artificial General Intelligence | Lex Fridman Podcast #306",
  "id": "aGBLRlLe7X8",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:05.000\n at which point is the neural network a being versus a tool?\n\n00:08.400 --> 00:11.360\n The following is a conversation with Oriel Veniales,\n\n00:11.360 --> 00:13.440\n his second time on the podcast.\n\n00:13.440 --> 00:15.920\n Oriel is the research director\n\n00:15.920 --> 00:18.000\n and deep learning lead at DeepMind\n\n00:18.000 --> 00:20.960\n and one of the most brilliant thinkers and researchers\n\n00:20.960 --> 00:24.320\n in the history of artificial intelligence.\n\n00:24.320 --> 00:26.640\n This is the Lex Friedman podcast.\n\n00:26.640 --> 00:28.840\n To support it, please check out our sponsors\n\n00:28.840 --> 00:30.160\n in the description.\n\n00:30.160 --> 00:33.560\n And now, dear friends, here's Oriel Veniales.\n\n00:34.480 --> 00:37.020\n You are one of the most brilliant researchers\n\n00:37.020 --> 00:38.440\n in the history of AI,\n\n00:38.440 --> 00:40.560\n working across all kinds of modalities.\n\n00:40.560 --> 00:42.680\n Probably the one common theme is\n\n00:42.680 --> 00:45.000\n it's always sequences of data.\n\n00:45.000 --> 00:46.960\n So we're talking about languages, images,\n\n00:46.960 --> 00:50.240\n even biology and games, as we talked about last time.\n\n00:50.240 --> 00:53.360\n So you're a good person to ask this.\n\n00:53.360 --> 00:57.320\n In your lifetime, will we be able to build an AI system\n\n00:57.320 --> 01:00.740\n that's able to replace me as the interviewer\n\n01:00.740 --> 01:02.580\n in this conversation,\n\n01:02.580 --> 01:04.460\n in terms of ability to ask questions\n\n01:04.460 --> 01:06.600\n that are compelling to somebody listening?\n\n01:06.600 --> 01:10.640\n And then further question is, are we close?\n\n01:10.640 --> 01:13.880\n Will we be able to build a system that replaces you\n\n01:13.880 --> 01:16.080\n as the interviewee\n\n01:16.080 --> 01:18.100\n in order to create a compelling conversation?\n\n01:18.100 --> 01:20.020\n How far away are we, do you think?\n\n01:20.020 --> 01:21.800\n It's a good question.\n\n01:21.800 --> 01:24.680\n I think partly I would say, do we want that?\n\n01:24.680 --> 01:29.400\n I really like when we start now with very powerful models,\n\n01:29.400 --> 01:32.160\n interacting with them and thinking of them\n\n01:32.160 --> 01:34.080\n more closer to us.\n\n01:34.080 --> 01:37.020\n The question is, if you remove the human side\n\n01:37.020 --> 01:42.020\n of the conversation, is that an interesting artifact?\n\n01:42.320 --> 01:44.440\n And I would say, probably not.\n\n01:44.440 --> 01:47.400\n I've seen, for instance, last time we spoke,\n\n01:47.400 --> 01:50.320\n like we were talking about StarCraft,\n\n01:50.320 --> 01:54.920\n and creating agents that play games involves self play,\n\n01:54.920 --> 01:57.660\n but ultimately what people care about was,\n\n01:57.660 --> 01:59.080\n how does this agent behave\n\n01:59.080 --> 02:02.700\n when the opposite side is a human?\n\n02:02.700 --> 02:04.720\n So without a doubt,\n\n02:04.720 --> 02:08.560\n we will probably be more empowered by AI.\n\n02:08.560 --> 02:12.480\n Maybe you can source some questions from an AI system.\n\n02:12.480 --> 02:15.020\n I mean, that even today, I would say it's quite plausible\n\n02:15.020 --> 02:17.060\n that with your creativity,\n\n02:17.060 --> 02:19.400\n you might actually find very interesting questions\n\n02:19.400 --> 02:20.740\n that you can filter.\n\n02:20.740 --> 02:22.420\n We call this cherry picking sometimes\n\n02:22.420 --> 02:24.400\n in the field of language.\n\n02:24.400 --> 02:27.540\n And likewise, if I had now the tools on my side,\n\n02:27.540 --> 02:30.660\n I could say, look, you're asking this interesting question.\n\n02:30.660 --> 02:33.240\n From this answer, I like the words chosen\n\n02:33.240 --> 02:36.600\n by this particular system that created a few words.\n\n02:36.600 --> 02:41.280\n Completely replacing it feels not exactly exciting to me.\n\n02:41.280 --> 02:43.780\n Although in my lifetime, I think way,\n\n02:43.780 --> 02:45.520\n I mean, given the trajectory,\n\n02:45.520 --> 02:48.020\n I think it's possible that perhaps\n\n02:48.020 --> 02:49.880\n there could be interesting,\n\n02:49.880 --> 02:53.040\n maybe self play interviews as you're suggesting\n\n02:53.040 --> 02:56.160\n that would look or sound quite interesting\n\n02:56.160 --> 02:57.720\n and probably would educate\n\n02:57.720 --> 03:00.160\n or you could learn a topic through listening\n\n03:00.160 --> 03:03.200\n to one of these interviews at a basic level at least.\n\n03:03.200 --> 03:04.800\n So you said it doesn't seem exciting to you,\n\n03:04.800 --> 03:07.520\n but what if exciting is part of the objective function\n\n03:07.520 --> 03:09.120\n the thing is optimized over?\n\n03:09.120 --> 03:12.840\n So there's probably a huge amount of data of humans\n\n03:12.840 --> 03:16.080\n if you look correctly, of humans communicating online,\n\n03:16.080 --> 03:19.280\n and there's probably ways to measure the degree of,\n\n03:19.280 --> 03:21.920\n you know, as they talk about engagement.\n\n03:21.920 --> 03:24.140\n So you can probably optimize the question\n\n03:24.140 --> 03:28.680\n that's most created an engaging conversation in the past.\n\n03:28.680 --> 03:31.560\n So actually, if you strictly use the word exciting,\n\n03:33.200 --> 03:37.240\n there is probably a way to create\n\n03:37.240 --> 03:40.320\n a optimally exciting conversations\n\n03:40.320 --> 03:42.160\n that involve AI systems.\n\n03:42.160 --> 03:44.600\n At least one side is AI.\n\n03:44.600 --> 03:46.560\n Yeah, that makes sense, I think,\n\n03:46.560 --> 03:50.240\n maybe looping back a bit to games and the game industry,\n\n03:50.240 --> 03:53.040\n when you design algorithms,\n\n03:53.040 --> 03:55.800\n you're thinking about winning as the objective, right?\n\n03:55.800 --> 03:57.320\n Or the reward function.\n\n03:57.320 --> 04:00.080\n But in fact, when we discussed this with Blizzard,\n\n04:00.080 --> 04:02.320\n the creators of StarCraft in this case,\n\n04:02.320 --> 04:05.340\n I think what's exciting, fun,\n\n04:05.340 --> 04:09.160\n if you could measure that and optimize for that,\n\n04:09.160 --> 04:11.720\n that's probably why we play video games\n\n04:11.720 --> 04:14.640\n or why we interact or listen or look at cat videos\n\n04:14.640 --> 04:16.460\n or whatever on the internet.\n\n04:16.460 --> 04:19.500\n So it's true that modeling reward\n\n04:19.500 --> 04:21.320\n beyond the obvious reward functions\n\n04:21.320 --> 04:23.720\n we've used to in reinforcement learning\n\n04:23.720 --> 04:25.560\n is definitely very exciting.\n\n04:25.560 --> 04:28.240\n And again, there is some progress actually\n\n04:28.240 --> 04:32.140\n into a particular aspect of AI, which is quite critical,\n\n04:32.140 --> 04:36.120\n which is, for instance, is a conversation\n\n04:36.120 --> 04:38.200\n or is the information truthful, right?\n\n04:38.200 --> 04:41.640\n So you could start trying to evaluate these\n\n04:41.640 --> 04:44.440\n from accepts from the internet, right?\n\n04:44.440 --> 04:45.840\n That has lots of information.\n\n04:45.840 --> 04:50.220\n And then if you can learn a function automated ideally,\n\n04:50.220 --> 04:52.920\n so you can also optimize it more easily,\n\n04:52.920 --> 04:54.920\n then you could actually have conversations\n\n04:54.920 --> 04:59.440\n that optimize for non obvious things such as excitement.\n\n04:59.440 --> 05:01.100\n So yeah, that's quite possible.\n\n05:01.100 --> 05:03.620\n And then I would say in that case,\n\n05:03.620 --> 05:05.960\n it would definitely be fun exercise\n\n05:05.960 --> 05:08.120\n and quite unique to have at least one site\n\n05:08.120 --> 05:12.840\n that is fully driven by an excitement reward function.\n\n05:12.840 --> 05:16.960\n But obviously, there would be still quite a lot of humanity\n\n05:16.960 --> 05:20.800\n in the system, both from who is building the system,\n\n05:20.800 --> 05:23.600\n of course, and also, ultimately,\n\n05:23.600 --> 05:26.040\n if we think of labeling for excitement,\n\n05:26.040 --> 05:28.480\n that those labels must come from us\n\n05:28.480 --> 05:32.560\n because it's just hard to have a computational measure\n\n05:32.560 --> 05:33.520\n of excitement.\n\n05:33.520 --> 05:36.160\n As far as I understand, there's no such thing.\n\n05:36.160 --> 05:39.280\n Well, as you mentioned truth also,\n\n05:39.280 --> 05:41.840\n I would actually venture to say that excitement\n\n05:41.840 --> 05:44.160\n is easier to label than truth,\n\n05:44.160 --> 05:49.000\n or is perhaps has lower consequences of failure.\n\n05:49.920 --> 05:54.920\n But there is perhaps the humanness that you mentioned,\n\n05:55.760 --> 05:58.280\n that's perhaps part of a thing that could be labeled.\n\n05:58.280 --> 06:02.520\n And that could mean an AI system that's doing dialogue,\n\n06:02.520 --> 06:07.520\n that's doing conversations should be flawed, for example.\n\n06:07.720 --> 06:09.440\n Like that's the thing you optimize for,\n\n06:09.440 --> 06:13.280\n which is have inherent contradictions by design,\n\n06:13.280 --> 06:15.080\n have flaws by design.\n\n06:15.080 --> 06:18.760\n Maybe it also needs to have a strong sense of identity.\n\n06:18.760 --> 06:22.680\n So it has a backstory it told itself that it sticks to.\n\n06:22.680 --> 06:26.900\n It has memories, not in terms of how the system is designed,\n\n06:26.900 --> 06:30.440\n but it's able to tell stories about its past.\n\n06:30.440 --> 06:35.440\n It's able to have mortality and fear of mortality\n\n06:36.040 --> 06:39.080\n in the following way that it has an identity.\n\n06:39.080 --> 06:41.200\n And if it says something stupid\n\n06:41.200 --> 06:44.680\n and gets canceled on Twitter, that's the end of that system.\n\n06:44.680 --> 06:47.320\n So it's not like you get to rebrand yourself.\n\n06:47.320 --> 06:49.320\n That system is, that's it.\n\n06:49.320 --> 06:52.080\n So maybe the high stakes nature of it,\n\n06:52.080 --> 06:54.520\n because you can't say anything stupid now,\n\n06:54.520 --> 06:57.680\n or because you'd be canceled on Twitter.\n\n06:57.680 --> 06:59.720\n And there's stakes to that.\n\n06:59.720 --> 07:01.120\n And that I think part of the reason\n\n07:01.120 --> 07:03.480\n that makes it interesting.\n\n07:03.480 --> 07:04.680\n And then you have a perspective,\n\n07:04.680 --> 07:07.680\n like you've built up over time that you stick with,\n\n07:07.680 --> 07:09.100\n and then people can disagree with you.\n\n07:09.100 --> 07:11.760\n So holding that perspective strongly,\n\n07:11.760 --> 07:14.000\n holding sort of maybe a controversial,\n\n07:14.000 --> 07:16.280\n at least a strong opinion.\n\n07:16.280 --> 07:18.800\n All of those elements, it feels like they can be learned\n\n07:18.800 --> 07:21.720\n because it feels like there's a lot of data\n\n07:21.720 --> 07:24.520\n on the internet of people having an opinion.\n\n07:24.520 --> 07:27.800\n And then combine that with a metric of excitement,\n\n07:27.800 --> 07:30.020\n you can start to create something that,\n\n07:30.020 --> 07:31.680\n as opposed to trying to optimize\n\n07:31.680 --> 07:36.680\n for sort of grammatical clarity and truthfulness,\n\n07:38.120 --> 07:42.000\n the factual consistency over many sentences,\n\n07:42.000 --> 07:45.320\n you optimize for the humanness.\n\n07:45.320 --> 07:48.860\n And there's obviously data for humanness on the internet.\n\n07:48.860 --> 07:53.760\n So I wonder if there's a future where that's part,\n\n07:53.760 --> 07:56.400\n or I mean, I sometimes wonder that about myself.\n\n07:56.400 --> 07:58.120\n I'm a huge fan of podcasts,\n\n07:58.120 --> 08:00.760\n and I listen to some podcasts,\n\n08:00.760 --> 08:03.240\n and I think like, what is interesting about this?\n\n08:03.240 --> 08:04.260\n What is compelling?\n\n08:05.960 --> 08:07.440\n The same way you watch other games.\n\n08:07.440 --> 08:09.160\n Like you said, watch, play StarCraft,\n\n08:09.160 --> 08:13.040\n or have Magnus Carlsen play chess.\n\n08:13.040 --> 08:14.920\n So I'm not a chess player,\n\n08:14.920 --> 08:16.120\n but it's still interesting to me.\n\n08:16.120 --> 08:16.960\n What is that?\n\n08:16.960 --> 08:19.440\n That's the stakes of it,\n\n08:19.440 --> 08:23.400\n maybe the end of a domination of a series of wins.\n\n08:23.400 --> 08:25.440\n I don't know, there's all those elements\n\n08:25.440 --> 08:28.000\n somehow connect to a compelling conversation.\n\n08:28.000 --> 08:30.200\n And I wonder how hard is that to replace,\n\n08:30.200 --> 08:31.840\n because ultimately all of that connects\n\n08:31.840 --> 08:35.480\n to the initial proposition of how to test,\n\n08:35.480 --> 08:38.640\n whether an AI is intelligent or not with the Turing test,\n\n08:38.640 --> 08:41.760\n which I guess my question comes from a place\n\n08:41.760 --> 08:43.680\n of the spirit of that test.\n\n08:43.680 --> 08:45.440\n Yes, I actually recall,\n\n08:45.440 --> 08:47.920\n I was just listening to our first podcast\n\n08:47.920 --> 08:50.380\n where we discussed Turing test.\n\n08:50.380 --> 08:54.760\n So I would say from a neural network,\n\n08:54.760 --> 08:57.640\n AI builder perspective,\n\n08:57.640 --> 09:01.360\n there's usually you try to map\n\n09:01.360 --> 09:05.200\n many of these interesting topics you discuss to benchmarks,\n\n09:05.200 --> 09:08.140\n and then also to actual architectures\n\n09:08.140 --> 09:10.640\n on the how these systems are currently built,\n\n09:10.640 --> 09:13.080\n how they learn, what data they learn from,\n\n09:13.080 --> 09:14.300\n what are they learning, right?\n\n09:14.300 --> 09:17.800\n We're talking about weights of a mathematical function,\n\n09:17.800 --> 09:21.560\n and then looking at the current state of the game,\n\n09:21.560 --> 09:26.000\n maybe what do we need leaps forward\n\n09:26.000 --> 09:30.660\n to get to the ultimate stage of all these experiences,\n\n09:30.660 --> 09:32.880\n lifetime experience, fears,\n\n09:32.880 --> 09:34.800\n like words that currently,\n\n09:34.800 --> 09:38.020\n barely we're seeing progress\n\n09:38.020 --> 09:40.120\n just because what's happening today\n\n09:40.120 --> 09:44.020\n is you take all these human interactions,\n\n09:44.020 --> 09:47.960\n it's a large vast variety of human interactions online,\n\n09:47.960 --> 09:51.640\n and then you're distilling these sequences, right?\n\n09:51.640 --> 09:53.000\n Going back to my passion,\n\n09:53.000 --> 09:56.920\n like sequences of words, letters, images, sound,\n\n09:56.920 --> 09:59.840\n there's more modalities here to be at play.\n\n09:59.840 --> 10:03.360\n And then you're trying to just learn a function\n\n10:03.360 --> 10:04.400\n that will be happy,\n\n10:04.400 --> 10:08.840\n that maximizes the likelihood of seeing all these\n\n10:08.840 --> 10:10.880\n through a neural network.\n\n10:10.880 --> 10:14.200\n Now, I think there's a few places\n\n10:14.200 --> 10:17.240\n where the way currently we train these models\n\n10:17.240 --> 10:20.000\n would clearly lack to be able to develop\n\n10:20.000 --> 10:22.120\n the kinds of capabilities you save.\n\n10:22.120 --> 10:23.560\n I'll tell you maybe a couple.\n\n10:23.560 --> 10:27.640\n One is the lifetime of an agent or a model.\n\n10:27.640 --> 10:30.820\n So you learn from this data offline, right?\n\n10:30.820 --> 10:33.880\n So you're just passively observing and maximizing these,\n\n10:33.880 --> 10:35.360\n it's almost like a mountains,\n\n10:35.360 --> 10:37.600\n like a landscape of mountains,\n\n10:37.600 --> 10:39.140\n and then everywhere there's data\n\n10:39.140 --> 10:41.040\n that humans interacted in this way,\n\n10:41.040 --> 10:43.000\n you're trying to make that higher\n\n10:43.000 --> 10:45.720\n and then lower where there's no data.\n\n10:45.720 --> 10:48.480\n And then these models generally\n\n10:48.480 --> 10:51.160\n don't then experience themselves.\n\n10:51.160 --> 10:52.520\n They just are observers, right?\n\n10:52.520 --> 10:54.600\n They're passive observers of the data.\n\n10:54.600 --> 10:57.440\n And then we're putting them to then generate data\n\n10:57.440 --> 10:59.180\n when we interact with them,\n\n10:59.180 --> 11:00.900\n but that's very limiting.\n\n11:00.900 --> 11:03.480\n The experience they actually experience\n\n11:03.480 --> 11:05.680\n when they could maybe be optimizing\n\n11:05.680 --> 11:07.440\n or further optimizing the weights,\n\n11:07.440 --> 11:08.640\n we're not even doing that.\n\n11:08.640 --> 11:13.640\n So to be clear, and again, mapping to AlphaGo, AlphaStar,\n\n11:14.080 --> 11:15.280\n we train the model.\n\n11:15.280 --> 11:18.260\n And when we deploy it to play against humans,\n\n11:18.260 --> 11:20.400\n or in this case interact with humans,\n\n11:20.400 --> 11:21.840\n like language models,\n\n11:21.840 --> 11:23.560\n they don't even keep training, right?\n\n11:23.560 --> 11:26.220\n They're not learning in the sense of the weights\n\n11:26.220 --> 11:28.240\n that you've learned from the data,\n\n11:28.240 --> 11:29.820\n they don't keep changing.\n\n11:29.820 --> 11:33.540\n Now, there's something a bit more feels magical,\n\n11:33.540 --> 11:36.240\n but it's understandable if you're into Neuronet,\n\n11:36.240 --> 11:39.180\n which is, well, they might not learn\n\n11:39.180 --> 11:40.520\n in the strict sense of the words,\n\n11:40.520 --> 11:41.520\n the weights changing,\n\n11:41.520 --> 11:44.400\n maybe that's mapping to how neurons interconnect\n\n11:44.400 --> 11:46.680\n and how we learn over our lifetime.\n\n11:46.680 --> 11:50.320\n But it's true that the context of the conversation\n\n11:50.320 --> 11:55.020\n that takes place when you talk to these systems,\n\n11:55.020 --> 11:57.280\n it's held in their working memory, right?\n\n11:57.280 --> 12:00.160\n It's almost like you start the computer,\n\n12:00.160 --> 12:02.880\n it has a hard drive that has a lot of information,\n\n12:02.880 --> 12:04.040\n you have access to the internet,\n\n12:04.040 --> 12:06.360\n which has probably all the information,\n\n12:06.360 --> 12:08.520\n but there's also a working memory\n\n12:08.520 --> 12:11.120\n where these agents, as we call them,\n\n12:11.120 --> 12:13.880\n or start calling them, build upon.\n\n12:13.880 --> 12:16.640\n Now, this memory is very limited.\n\n12:16.640 --> 12:19.240\n I mean, right now we're talking, to be concrete,\n\n12:19.240 --> 12:21.780\n about 2,000 words that we hold,\n\n12:21.780 --> 12:24.880\n and then beyond that, we start forgetting what we've seen.\n\n12:24.880 --> 12:28.080\n So you can see that there's some short term coherence\n\n12:28.080 --> 12:29.880\n already, right, with what you said.\n\n12:29.880 --> 12:32.340\n I mean, it's a very interesting topic.\n\n12:32.340 --> 12:37.340\n Having sort of a mapping, an agent to have consistency,\n\n12:37.440 --> 12:40.800\n then if you say, oh, what's your name,\n\n12:40.800 --> 12:42.280\n it could remember that,\n\n12:42.280 --> 12:45.020\n but then it might forget beyond 2,000 words,\n\n12:45.020 --> 12:47.520\n which is not that long of context\n\n12:47.520 --> 12:51.800\n if we think even of these podcast books are much longer.\n\n12:51.800 --> 12:55.160\n So technically speaking, there's a limitation there,\n\n12:55.160 --> 12:58.220\n super exciting from people that work on deep learning\n\n12:58.220 --> 13:03.080\n to be working on, but I would say we lack maybe benchmarks\n\n13:03.080 --> 13:07.880\n and the technology to have this lifetime like experience\n\n13:07.880 --> 13:10.900\n of memory that keeps building up.\n\n13:10.900 --> 13:13.200\n However, the way it learns offline\n\n13:13.200 --> 13:14.920\n is clearly very powerful, right?\n\n13:14.920 --> 13:17.840\n So you asked me three years ago, I would say,\n\n13:17.840 --> 13:18.680\n oh, we're very far.\n\n13:18.680 --> 13:22.240\n I think we've seen the power of this imitation,\n\n13:22.240 --> 13:26.280\n again, on the internet scale that has enabled this\n\n13:26.280 --> 13:28.800\n to feel like at least the knowledge,\n\n13:28.800 --> 13:30.720\n the basic knowledge about the world now\n\n13:30.720 --> 13:33.160\n is incorporated into the weights,\n\n13:33.160 --> 13:36.600\n but then this experience is lacking.\n\n13:36.600 --> 13:39.360\n And in fact, as I said, we don't even train them\n\n13:39.360 --> 13:41.200\n when we're talking to them,\n\n13:41.200 --> 13:44.800\n other than their working memory, of course, is affected.\n\n13:44.800 --> 13:46.600\n So that's the dynamic part,\n\n13:46.600 --> 13:48.300\n but they don't learn in the same way\n\n13:48.300 --> 13:50.640\n that you and I have learned, right?\n\n13:50.640 --> 13:54.080\n From basically when we were born and probably before.\n\n13:54.080 --> 13:57.440\n So lots of fascinating, interesting questions you asked there.\n\n13:57.440 --> 14:01.720\n I think the one I mentioned is this idea of memory\n\n14:01.720 --> 14:05.540\n and experience versus just kind of observe the world\n\n14:05.540 --> 14:08.040\n and learn its knowledge, which I think for that,\n\n14:08.040 --> 14:10.400\n I would argue lots of recent advancements\n\n14:10.400 --> 14:13.480\n that make me very excited about the field.\n\n14:13.480 --> 14:18.240\n And then the second maybe issue that I see is\n\n14:18.240 --> 14:21.320\n all these models, we train them from scratch.\n\n14:21.320 --> 14:24.100\n That's something I would have complained three years ago\n\n14:24.100 --> 14:26.480\n or six years ago or 10 years ago.\n\n14:26.480 --> 14:31.440\n And it feels if we take inspiration from how we got here,\n\n14:31.440 --> 14:35.340\n how the universe evolved us and we keep evolving,\n\n14:35.340 --> 14:37.920\n it feels that is a missing piece,\n\n14:37.920 --> 14:41.400\n that we should not be training models from scratch\n\n14:41.400 --> 14:42.560\n every few months,\n\n14:42.560 --> 14:45.320\n that there should be some sort of way\n\n14:45.320 --> 14:49.040\n in which we can grow models much like as a species\n\n14:49.040 --> 14:51.560\n and many other elements in the universe\n\n14:51.560 --> 14:55.080\n is building from the previous sort of iterations.\n\n14:55.080 --> 14:59.600\n And that from a just purely neural network perspective,\n\n14:59.600 --> 15:02.360\n even though we would like to make it work,\n\n15:02.360 --> 15:06.300\n it's proven very hard to not throw away\n\n15:06.300 --> 15:07.720\n the previous weights, right?\n\n15:07.720 --> 15:09.720\n This landscape we learn from the data\n\n15:09.720 --> 15:13.400\n and refresh it with a brand new set of weights,\n\n15:13.400 --> 15:17.020\n given maybe a recent snapshot of these data sets\n\n15:17.020 --> 15:20.000\n we train on, et cetera, or even a new game we're learning.\n\n15:20.000 --> 15:24.200\n So that feels like something is missing fundamentally.\n\n15:24.200 --> 15:27.480\n We might find it, but it's not very clear\n\n15:27.480 --> 15:28.460\n how it will look like.\n\n15:28.460 --> 15:30.860\n There's many ideas and it's super exciting as well.\n\n15:30.860 --> 15:32.480\n Yes, just for people who don't know,\n\n15:32.480 --> 15:35.760\n when you're approaching a new problem in machine learning,\n\n15:35.760 --> 15:38.240\n you're going to come up with an architecture\n\n15:38.240 --> 15:41.000\n that has a bunch of weights\n\n15:41.000 --> 15:43.400\n and then you initialize them somehow,\n\n15:43.400 --> 15:47.320\n which in most cases is some version of random.\n\n15:47.320 --> 15:49.020\n So that's what you mean by starting from scratch.\n\n15:49.020 --> 15:52.920\n And it seems like it's a waste every time you solve\n\n15:54.480 --> 15:59.480\n the game of Go and chess, StarCraft, protein folding,\n\n15:59.720 --> 16:03.200\n like surely there's some way to reuse the weights\n\n16:03.200 --> 16:08.200\n as we grow this giant database of neural networks\n\n16:08.400 --> 16:10.760\n that have solved some of the toughest problems in the world.\n\n16:10.760 --> 16:15.240\n And so some of that is, what is that?\n\n16:15.240 --> 16:19.080\n Methods, how to reuse weights,\n\n16:19.080 --> 16:22.480\n how to learn, extract what's generalizable\n\n16:22.480 --> 16:25.160\n or at least has a chance to be\n\n16:25.160 --> 16:26.900\n and throw away the other stuff.\n\n16:27.840 --> 16:29.580\n And maybe the neural network itself\n\n16:29.580 --> 16:31.640\n should be able to tell you that.\n\n16:31.640 --> 16:34.400\n Like what, yeah, how do you,\n\n16:34.400 --> 16:37.520\n what ideas do you have for better initialization of weights?\n\n16:37.520 --> 16:38.760\n Maybe stepping back,\n\n16:38.760 --> 16:41.720\n if we look at the field of machine learning,\n\n16:41.720 --> 16:44.040\n but especially deep learning, right?\n\n16:44.040 --> 16:45.240\n At the core of deep learning,\n\n16:45.240 --> 16:49.240\n there's this beautiful idea that is a single algorithm\n\n16:49.240 --> 16:50.920\n can solve any task, right?\n\n16:50.920 --> 16:54.400\n So it's been proven over and over\n\n16:54.400 --> 16:56.420\n with more increasing set of benchmarks\n\n16:56.420 --> 16:58.580\n and things that were thought impossible\n\n16:58.580 --> 17:01.960\n that are being cracked by this basic principle\n\n17:01.960 --> 17:05.800\n that is you take a neural network of uninitialized weights,\n\n17:05.800 --> 17:09.620\n so like a blank computational brain,\n\n17:09.620 --> 17:12.580\n then you give it, in the case of supervised learning,\n\n17:12.580 --> 17:14.960\n a lot ideally of examples of,\n\n17:14.960 --> 17:17.120\n hey, here is what the input looks like\n\n17:17.120 --> 17:19.560\n and the desired output should look like this.\n\n17:19.560 --> 17:22.360\n I mean, image classification is very clear example,\n\n17:22.360 --> 17:25.560\n images to maybe one of a thousand categories,\n\n17:25.560 --> 17:26.840\n that's what ImageNet is like,\n\n17:26.840 --> 17:30.720\n but many, many, if not all problems can be mapped this way.\n\n17:30.720 --> 17:33.840\n And then there's a generic recipe, right?\n\n17:33.840 --> 17:35.240\n That you can use.\n\n17:35.240 --> 17:38.600\n And this recipe with very little change,\n\n17:38.600 --> 17:41.520\n and I think that's the core of deep learning research, right?\n\n17:41.520 --> 17:44.420\n That what is the recipe that is universal?\n\n17:44.420 --> 17:46.400\n That for any new given task,\n\n17:46.400 --> 17:48.460\n I'll be able to use without thinking,\n\n17:48.460 --> 17:51.740\n without having to work very hard on the problem at stake.\n\n17:52.600 --> 17:54.400\n We have not found this recipe,\n\n17:54.400 --> 17:59.400\n but I think the field is excited to find less tweaks\n\n18:00.160 --> 18:02.640\n or tricks that people find when they work\n\n18:02.640 --> 18:05.280\n on important problems specific to those\n\n18:05.280 --> 18:07.540\n and more of a general algorithm, right?\n\n18:07.540 --> 18:09.300\n So at an algorithmic level,\n\n18:09.300 --> 18:11.780\n I would say we have something general already,\n\n18:11.780 --> 18:14.520\n which is this formula of training a very powerful model,\n\n18:14.520 --> 18:17.000\n a neural network on a lot of data.\n\n18:17.000 --> 18:21.200\n And in many cases, you need some specificity\n\n18:21.200 --> 18:23.400\n to the actual problem you're solving,\n\n18:23.400 --> 18:26.080\n protein folding being such an important problem\n\n18:26.080 --> 18:30.800\n has some basic recipe that is learned from before, right?\n\n18:30.800 --> 18:34.140\n Like transformer models, graph neural networks,\n\n18:34.140 --> 18:38.600\n ideas coming from NLP, like something called BERT,\n\n18:38.600 --> 18:41.280\n that is a kind of loss that you can emplace\n\n18:41.280 --> 18:45.460\n to help the knowledge distillation is another technique,\n\n18:45.460 --> 18:46.300\n right?\n\n18:46.300 --> 18:47.120\n So this is the formula.\n\n18:47.120 --> 18:50.560\n We still had to find some particular things\n\n18:50.560 --> 18:53.600\n that were specific to alpha fold, right?\n\n18:53.600 --> 18:55.860\n That's very important because protein folding\n\n18:55.860 --> 18:59.120\n is such a high value problem that as humans,\n\n18:59.120 --> 19:00.840\n we should solve it no matter\n\n19:00.840 --> 19:02.880\n if we need to be a bit specific.\n\n19:02.880 --> 19:04.940\n And it's possible that some of these learnings\n\n19:04.940 --> 19:07.380\n will apply then to the next iteration of this recipe\n\n19:07.380 --> 19:09.340\n that deep learners are about.\n\n19:09.340 --> 19:13.200\n But it is true that so far, the recipe is what's common,\n\n19:13.200 --> 19:15.880\n but the weights you generally throw away,\n\n19:15.880 --> 19:17.800\n which feels very sad.\n\n19:17.800 --> 19:20.440\n Although, maybe in the last,\n\n19:20.440 --> 19:22.280\n especially in the last two, three years,\n\n19:22.280 --> 19:23.560\n and when we last spoke,\n\n19:23.560 --> 19:25.560\n I mentioned this area of meta learning,\n\n19:25.560 --> 19:27.560\n which is the idea of learning to learn.\n\n19:28.560 --> 19:32.040\n That idea and some progress has been had starting,\n\n19:32.040 --> 19:36.040\n I would say, mostly from GPT3 on the language domain only,\n\n19:36.040 --> 19:41.040\n in which you could conceive a model that is trained once.\n\n19:41.040 --> 19:44.720\n And then this model is not narrow in that it only knows\n\n19:44.720 --> 19:47.760\n how to translate a pair of languages or even a set of\n\n19:47.760 --> 19:51.520\n or it only knows how to assign sentiment to a sentence.\n\n19:51.520 --> 19:55.040\n These actually, you could teach it by a prompting,\n\n19:55.040 --> 19:56.880\n it's called, and this prompting is essentially\n\n19:56.880 --> 19:59.920\n just showing it a few more examples,\n\n19:59.920 --> 20:03.040\n almost like you do show examples, input, output examples,\n\n20:03.040 --> 20:04.900\n algorithmically speaking to the process\n\n20:04.900 --> 20:06.320\n of creating this model.\n\n20:06.320 --> 20:07.840\n But now you're doing it through language,\n\n20:07.840 --> 20:11.080\n which is very natural way for us to learn from one another.\n\n20:11.080 --> 20:13.180\n I tell you, hey, you should do this new task.\n\n20:13.180 --> 20:14.600\n I'll tell you a bit more.\n\n20:14.600 --> 20:16.080\n Maybe you ask me some questions\n\n20:16.080 --> 20:17.840\n and now you know the task, right?\n\n20:17.840 --> 20:20.320\n You didn't need to retrain it from scratch.\n\n20:20.320 --> 20:23.200\n And we've seen these magical moments almost\n\n20:24.080 --> 20:26.960\n in this way to do few shot promptings through language\n\n20:26.960 --> 20:28.560\n on language only domain.\n\n20:28.560 --> 20:30.960\n And then in the last two years,\n\n20:30.960 --> 20:34.640\n we've seen these expanded to beyond language,\n\n20:34.640 --> 20:38.040\n adding vision, adding actions and games,\n\n20:38.040 --> 20:39.480\n lots of progress to be had.\n\n20:39.480 --> 20:42.160\n But this is maybe, if you ask me like about\n\n20:42.160 --> 20:43.720\n how are we gonna crack this problem?\n\n20:43.720 --> 20:47.800\n This is perhaps one way in which you have a single model.\n\n20:48.760 --> 20:52.160\n The problem of this model is it's hard to grow\n\n20:52.160 --> 20:54.320\n in weights or capacity,\n\n20:54.320 --> 20:56.400\n but the model is certainly so powerful\n\n20:56.400 --> 20:58.960\n that you can teach it some tasks, right?\n\n20:58.960 --> 21:00.600\n In this way that I teach you,\n\n21:00.600 --> 21:02.000\n I could teach you a new task now,\n\n21:02.000 --> 21:05.120\n if we were all at a text based task\n\n21:05.120 --> 21:08.440\n or a classification of vision style task.\n\n21:08.440 --> 21:12.860\n But it still feels like more breakthroughs should be had,\n\n21:12.860 --> 21:14.040\n but it's a great beginning, right?\n\n21:14.040 --> 21:15.440\n We have a good baseline.\n\n21:15.440 --> 21:18.160\n We have an idea that this maybe is the way we want\n\n21:18.160 --> 21:20.800\n to benchmark progress towards AGI.\n\n21:20.800 --> 21:22.880\n And I think in my view, that's critical\n\n21:22.880 --> 21:25.760\n to always have a way to benchmark the community\n\n21:25.760 --> 21:27.840\n sort of converging to these overall,\n\n21:27.840 --> 21:29.240\n which is good to see.\n\n21:29.240 --> 21:33.520\n And then this is actually what excites me\n\n21:33.520 --> 21:36.640\n in terms of also next steps for deep learning\n\n21:36.640 --> 21:39.120\n is how to make these models more powerful,\n\n21:39.120 --> 21:41.760\n how do you train them, how to grow them\n\n21:41.760 --> 21:44.520\n if they must grow, should they change their weights\n\n21:44.520 --> 21:46.120\n as you teach it task or not?\n\n21:46.120 --> 21:48.560\n There's some interesting questions, many to be answered.\n\n21:48.560 --> 21:49.760\n Yeah, you've opened the door\n\n21:49.760 --> 21:52.320\n about to a bunch of questions I want to ask,\n\n21:52.320 --> 21:55.720\n but let's first return to your tweet\n\n21:55.720 --> 21:57.160\n and read it like a Shakespeare.\n\n21:57.160 --> 22:01.280\n You wrote, God is not the end, it's the beginning.\n\n22:01.280 --> 22:05.000\n And then you wrote meow and then an emoji of a cat.\n\n22:06.200 --> 22:07.740\n So first two questions.\n\n22:07.740 --> 22:10.080\n First, can you explain the meow and the cat emoji?\n\n22:10.080 --> 22:13.680\n And second, can you explain what Godot is and how it works?\n\n22:13.680 --> 22:14.640\n Right, indeed.\n\n22:14.640 --> 22:16.520\n I mean, thanks for reminding me\n\n22:16.520 --> 22:19.920\n that we're all exposing on Twitter and.\n\n22:19.920 --> 22:20.960\n Permanently there.\n\n22:20.960 --> 22:21.920\n Yes, permanently there.\n\n22:21.920 --> 22:25.120\n One of the greatest AI researchers of all time,\n\n22:25.120 --> 22:27.200\n meow and cat emoji.\n\n22:27.200 --> 22:28.280\n Yes. There you go.\n\n22:28.280 --> 22:29.120\n Right, so.\n\n22:29.120 --> 22:32.720\n Can you imagine like touring, tweeting, meow and cat,\n\n22:32.720 --> 22:34.360\n probably he would, probably would.\n\n22:34.360 --> 22:35.200\n Probably.\n\n22:35.200 --> 22:38.020\n So yeah, the tweet is important actually.\n\n22:38.020 --> 22:40.720\n You know, I put thought on the tweets, I hope people.\n\n22:40.720 --> 22:41.720\n Which part do you think?\n\n22:41.720 --> 22:44.840\n Okay, so there's three sentences.\n\n22:44.840 --> 22:48.640\n Godot is not the end, Godot is the beginning,\n\n22:48.640 --> 22:50.120\n meow, cat emoji.\n\n22:50.120 --> 22:51.720\n Okay, which is the important part?\n\n22:51.720 --> 22:53.120\n The meow, no, no.\n\n22:53.120 --> 22:56.080\n Definitely that it is the beginning.\n\n22:56.080 --> 23:00.340\n I mean, I probably was just explaining a bit\n\n23:00.340 --> 23:03.760\n where the field is going, but let me tell you about Godot.\n\n23:03.760 --> 23:08.120\n So first the name Godot comes from maybe a sequence\n\n23:08.120 --> 23:11.820\n of releases that DeepMind had that named,\n\n23:11.820 --> 23:15.100\n like used animal names to name some of their models\n\n23:15.100 --> 23:19.120\n that are based on this idea of large sequence models.\n\n23:19.120 --> 23:20.620\n Initially they're only language,\n\n23:20.620 --> 23:23.180\n but we are expanding to other modalities.\n\n23:23.180 --> 23:28.180\n So we had, you know, we had Gopher, Chinchilla,\n\n23:28.800 --> 23:29.960\n these were language only.\n\n23:29.960 --> 23:32.720\n And then more recently we released Flamingo,\n\n23:32.720 --> 23:35.460\n which adds vision to the equation.\n\n23:35.460 --> 23:38.160\n And then Godot, which adds vision\n\n23:38.160 --> 23:41.660\n and then also actions in the mix, right?\n\n23:41.660 --> 23:44.520\n As we discuss actually actions,\n\n23:44.520 --> 23:47.600\n especially discrete actions like up, down, left, right.\n\n23:47.600 --> 23:49.520\n I just told you the actions, but they're words.\n\n23:49.520 --> 23:52.800\n So you can kind of see how actions naturally map\n\n23:52.800 --> 23:54.560\n to sequence modeling of words,\n\n23:54.560 --> 23:57.100\n which these models are very powerful.\n\n23:57.100 --> 24:01.720\n So Godot was named after, I believe,\n\n24:01.720 --> 24:03.640\n I can only from memory, right?\n\n24:03.640 --> 24:06.080\n These, you know, these things always happen\n\n24:06.080 --> 24:08.520\n with an amazing team of researchers behind.\n\n24:08.520 --> 24:12.200\n So before the release, we had a discussion\n\n24:12.200 --> 24:14.240\n about which animal would we pick, right?\n\n24:14.240 --> 24:18.380\n And I think because of the word general agent, right?\n\n24:18.380 --> 24:21.920\n And this is a property quite unique to Godot.\n\n24:21.920 --> 24:24.760\n We kind of were playing with the GA words\n\n24:24.760 --> 24:26.040\n and then, you know, Godot.\n\n24:26.040 --> 24:26.960\n Rhymes with cat.\n\n24:26.960 --> 24:28.080\n Yes.\n\n24:28.080 --> 24:30.280\n And Godot is obviously a Spanish version of cat.\n\n24:30.280 --> 24:32.280\n I had nothing to do with it, although I'm from Spain.\n\n24:32.280 --> 24:33.320\n Oh, how do you, wait, sorry.\n\n24:33.320 --> 24:34.700\n How do you say cat in Spanish?\n\n24:34.700 --> 24:35.540\n Gato.\n\n24:35.540 --> 24:36.360\n Oh, gato, okay.\n\n24:36.360 --> 24:37.200\n Now it all makes sense.\n\n24:37.200 --> 24:38.160\n Okay, okay, I see, I see, I see.\n\n24:38.160 --> 24:39.120\n Now it all makes sense.\n\n24:39.120 --> 24:39.960\n Okay, so.\n\n24:39.960 --> 24:40.840\n How do you say meow in Spanish?\n\n24:40.840 --> 24:41.960\n No, that's probably the same.\n\n24:41.960 --> 24:44.440\n I think you say it the same way,\n\n24:44.440 --> 24:48.120\n but you write it as M, I, A, U.\n\n24:48.120 --> 24:49.240\n Okay, it's universal.\n\n24:49.240 --> 24:50.080\n Yes.\n\n24:50.080 --> 24:51.680\n All right, so then how does the thing work?\n\n24:51.680 --> 24:56.680\n So you said general is, so you said language, vision.\n\n24:57.520 --> 24:59.240\n And action. Action.\n\n24:59.240 --> 25:01.840\n How does this, can you explain\n\n25:01.840 --> 25:04.240\n what kind of neural networks are involved?\n\n25:04.240 --> 25:06.360\n What does the training look like?\n\n25:06.360 --> 25:09.380\n And maybe what do you,\n\n25:09.380 --> 25:11.840\n are some beautiful ideas within the system?\n\n25:11.840 --> 25:16.060\n Yeah, so maybe the basics of Gato\n\n25:16.060 --> 25:19.920\n are not that dissimilar from many, many work that come.\n\n25:19.920 --> 25:22.880\n So here is where the sort of the recipe,\n\n25:22.880 --> 25:24.200\n I mean, hasn't changed too much.\n\n25:24.200 --> 25:25.600\n There is a transformer model\n\n25:25.600 --> 25:28.640\n that's the kind of recurrent neural network\n\n25:28.640 --> 25:33.320\n that essentially takes a sequence of modalities,\n\n25:33.320 --> 25:36.360\n observations that could be words,\n\n25:36.360 --> 25:38.800\n could be vision or could be actions.\n\n25:38.800 --> 25:42.120\n And then its own objective that you train it to do\n\n25:42.120 --> 25:46.360\n when you train it is to predict what the next anything is.\n\n25:46.360 --> 25:48.760\n And anything means what's the next action.\n\n25:48.760 --> 25:51.220\n If this sequence that I'm showing you to train\n\n25:51.220 --> 25:53.500\n is a sequence of actions and observations,\n\n25:53.500 --> 25:55.600\n then you're predicting what's the next action\n\n25:55.600 --> 25:57.100\n and the next observation, right?\n\n25:57.100 --> 26:00.880\n So you think of these really as a sequence of bites, right?\n\n26:00.880 --> 26:04.220\n So take any sequence of words,\n\n26:04.220 --> 26:07.000\n a sequence of interleaved words and images,\n\n26:07.000 --> 26:11.280\n a sequence of maybe observations that are images\n\n26:11.280 --> 26:14.280\n and moves in Atari up, down, left, right.\n\n26:14.280 --> 26:17.640\n And these you just think of them as bites\n\n26:17.640 --> 26:20.580\n and you're modeling what's the next bite gonna be like.\n\n26:20.580 --> 26:23.440\n And you might interpret that as an action\n\n26:23.440 --> 26:25.880\n and then play it in a game,\n\n26:25.880 --> 26:27.720\n or you could interpret it as a word\n\n26:27.720 --> 26:29.120\n and then write it down\n\n26:29.120 --> 26:31.400\n if you're chatting with the system and so on.\n\n26:32.480 --> 26:36.600\n So Gato basically can be thought as inputs,\n\n26:36.600 --> 26:41.480\n images, text, video, actions.\n\n26:41.480 --> 26:45.800\n It also actually inputs some sort of proprioception sensors\n\n26:45.800 --> 26:48.280\n from robotics because robotics is one of the tasks\n\n26:48.280 --> 26:49.860\n that it's been trained to do.\n\n26:49.860 --> 26:51.920\n And then at the output, similarly,\n\n26:51.920 --> 26:53.720\n it outputs words, actions.\n\n26:53.720 --> 26:57.440\n It does not output images, that's just by design,\n\n26:57.440 --> 26:59.900\n we decided not to go that way for now.\n\n27:00.880 --> 27:02.760\n That's also in part why it's the beginning\n\n27:02.760 --> 27:04.920\n because there's more to do clearly.\n\n27:04.920 --> 27:06.440\n But that's kind of what the Gato is,\n\n27:06.440 --> 27:09.200\n is this brain that essentially you give it any sequence\n\n27:09.200 --> 27:11.940\n of these observations and modalities\n\n27:11.940 --> 27:13.760\n and it outputs the next step.\n\n27:13.760 --> 27:17.380\n And then off you go, you feed the next step into\n\n27:17.380 --> 27:20.060\n and predict the next one and so on.\n\n27:20.060 --> 27:24.160\n Now, it is more than a language model\n\n27:24.160 --> 27:26.780\n because even though you can chat with Gato,\n\n27:26.780 --> 27:29.520\n like you can chat with Chinchilla or Flamingo,\n\n27:30.520 --> 27:33.200\n it also is an agent, right?\n\n27:33.200 --> 27:37.200\n So that's why we call it A of Gato,\n\n27:37.200 --> 27:41.340\n like the letter A and also it's general.\n\n27:41.340 --> 27:43.960\n It's not an agent that's been trained to be good\n\n27:43.960 --> 27:47.860\n at only StarCraft or only Atari or only Go.\n\n27:47.860 --> 27:51.640\n It's been trained on a vast variety of datasets.\n\n27:51.640 --> 27:53.840\n What makes it an agent, if I may interrupt,\n\n27:53.840 --> 27:56.000\n the fact that it can generate actions?\n\n27:56.000 --> 28:00.080\n Yes, so when we call it, I mean, it's a good question, right?\n\n28:00.080 --> 28:02.760\n When do we call a model?\n\n28:02.760 --> 28:03.840\n I mean, everything is a model,\n\n28:03.840 --> 28:07.360\n but what is an agent in my view is indeed the capacity\n\n28:07.360 --> 28:11.680\n to take actions in an environment that you then send to it\n\n28:11.680 --> 28:13.480\n and then the environment might return\n\n28:13.480 --> 28:15.040\n with a new observation\n\n28:15.040 --> 28:17.560\n and then you generate the next action.\n\n28:17.560 --> 28:20.440\n This actually, this reminds me of the question\n\n28:20.440 --> 28:23.000\n from the side of biology, what is life?\n\n28:23.000 --> 28:25.380\n Which is actually a very difficult question as well.\n\n28:25.380 --> 28:29.200\n What is living, what is living when you think about life\n\n28:29.200 --> 28:31.000\n here on this planet Earth?\n\n28:31.000 --> 28:33.420\n And a question interesting to me about aliens,\n\n28:33.420 --> 28:35.720\n what is life when we visit another planet?\n\n28:35.720 --> 28:37.200\n Would we be able to recognize it?\n\n28:37.200 --> 28:40.220\n And this feels like, it sounds perhaps silly,\n\n28:40.220 --> 28:41.360\n but I don't think it is.\n\n28:41.360 --> 28:48.260\n At which point is the neural network a being versus a tool?\n\n28:48.260 --> 28:52.400\n And it feels like action, ability to modify its environment\n\n28:52.400 --> 28:54.560\n is that fundamental leap.\n\n28:54.560 --> 28:57.440\n Yeah, I think it certainly feels like action\n\n28:57.440 --> 29:01.960\n is a necessary condition to be more alive,\n\n29:01.960 --> 29:04.400\n but probably not sufficient either.\n\n29:04.400 --> 29:05.240\n So sadly I...\n\n29:05.240 --> 29:06.880\n It's a soul consciousness thing, whatever.\n\n29:06.880 --> 29:09.080\n Yeah, yeah, we can get back to that later.\n\n29:09.080 --> 29:12.320\n But anyways, going back to the meow and the gato, right?\n\n29:12.320 --> 29:17.640\n So one of the leaps forward and what took the team a lot\n\n29:17.640 --> 29:21.280\n of effort and time was, as you were asking,\n\n29:21.280 --> 29:23.080\n how has gato been trained?\n\n29:23.080 --> 29:26.080\n So I told you gato is this transformer neural network,\n\n29:26.080 --> 29:30.840\n models actions, sequences of actions, words, et cetera.\n\n29:30.840 --> 29:35.520\n And then the way we train it is by essentially pulling\n\n29:35.520 --> 29:39.400\n data sets of observations, right?\n\n29:39.400 --> 29:42.600\n So it's a massive imitation learning algorithm\n\n29:42.600 --> 29:45.320\n that it imitates obviously to what\n\n29:45.320 --> 29:48.520\n is the next word that comes next from the usual data\n\n29:48.520 --> 29:50.160\n sets we use before, right?\n\n29:50.160 --> 29:54.600\n So these are these web scale style data sets of people\n\n29:54.600 --> 29:58.520\n writing on webs or chatting or whatnot, right?\n\n29:58.520 --> 30:02.000\n So that's an obvious source that we use on all language work.\n\n30:02.000 --> 30:05.640\n But then we also took a lot of agents\n\n30:05.640 --> 30:06.720\n that we have at DeepMind.\n\n30:06.720 --> 30:10.920\n I mean, as you know, DeepMind, we're quite interested\n\n30:10.920 --> 30:14.960\n in learning reinforcement learning and learning agents\n\n30:14.960 --> 30:17.040\n that play in different environments.\n\n30:17.040 --> 30:20.760\n So we kind of created a data set of these trajectories,\n\n30:20.760 --> 30:23.120\n as we call them, or agent experiences.\n\n30:23.120 --> 30:25.240\n So in a way, there are other agents\n\n30:25.240 --> 30:29.560\n we train for a single mind purpose to, let's say,\n\n30:29.560 --> 30:33.320\n control a 3D game environment and navigate a maze.\n\n30:33.320 --> 30:35.320\n So we had all the experience that\n\n30:35.320 --> 30:38.160\n was created through one agent interacting\n\n30:38.160 --> 30:39.480\n with that environment.\n\n30:39.480 --> 30:41.800\n And we added this to the data set, right?\n\n30:41.800 --> 30:44.400\n And as I said, we just see all the data,\n\n30:44.400 --> 30:46.440\n all these sequences of words or sequences\n\n30:46.440 --> 30:51.120\n of this agent interacting with that environment or agents\n\n30:51.120 --> 30:52.200\n playing Atari and so on.\n\n30:52.200 --> 30:54.920\n We see it as the same kind of data.\n\n30:54.920 --> 30:57.440\n And so we mix these data sets together.\n\n30:57.440 --> 31:00.120\n And we train Gato.\n\n31:00.120 --> 31:01.600\n That's the G part, right?\n\n31:01.600 --> 31:05.240\n It's general because it really has mixed.\n\n31:05.240 --> 31:07.560\n It doesn't have different brains for each modality\n\n31:07.560 --> 31:09.040\n or each narrow task.\n\n31:09.040 --> 31:10.480\n It has a single brain.\n\n31:10.480 --> 31:12.120\n It's not that big of a brain compared\n\n31:12.120 --> 31:14.760\n to most of the neural networks we see these days.\n\n31:14.760 --> 31:18.200\n It has 1 billion parameters.\n\n31:18.200 --> 31:21.120\n Some models we're seeing get in the trillions these days.\n\n31:21.120 --> 31:25.080\n And certainly, 100 billion feels like a size\n\n31:25.080 --> 31:29.000\n that is very common from when you train these jobs.\n\n31:29.000 --> 31:32.640\n So the actual agent is relatively small.\n\n31:32.640 --> 31:36.280\n But it's been trained on a very challenging, diverse data set,\n\n31:36.280 --> 31:38.000\n not only containing all of the internet\n\n31:38.000 --> 31:40.680\n but containing all these agent experience playing\n\n31:40.680 --> 31:43.240\n very different, distinct environments.\n\n31:43.240 --> 31:46.720\n So this brings us to the part of the tweet of this\n\n31:46.720 --> 31:48.880\n is not the end, it's the beginning.\n\n31:48.880 --> 31:53.120\n It feels very cool to see Gato, in principle,\n\n31:53.120 --> 31:57.360\n is able to control any sort of environments, especially\n\n31:57.360 --> 32:00.360\n the ones that it's been trained to do, these 3D games, Atari\n\n32:00.360 --> 32:04.600\n games, all sorts of robotics tasks, and so on.\n\n32:04.600 --> 32:07.760\n But obviously, it's not as proficient\n\n32:07.760 --> 32:10.520\n as the teachers it learned from on these environments.\n\n32:10.520 --> 32:11.680\n Not obvious.\n\n32:11.680 --> 32:15.040\n It's not obvious that it wouldn't be more proficient.\n\n32:15.040 --> 32:17.960\n It's just the current beginning part\n\n32:17.960 --> 32:21.760\n is that the performance is such that it's not as good\n\n32:21.760 --> 32:23.360\n as if it's specialized to that task.\n\n32:23.360 --> 32:23.840\n Right.\n\n32:23.840 --> 32:28.040\n So it's not as good, although I would argue size matters here.\n\n32:28.040 --> 32:31.360\n So the fact that I would argue always size always matters.\n\n32:31.360 --> 32:33.320\n That's a different conversation.\n\n32:33.320 --> 32:36.200\n But for neural networks, certainly size does matter.\n\n32:36.200 --> 32:39.600\n So it's the beginning because it's relatively small.\n\n32:39.600 --> 32:42.560\n So obviously, scaling this idea up\n\n32:42.560 --> 32:48.240\n might make the connections that exist between text\n\n32:48.240 --> 32:51.560\n on the internet and playing Atari and so on more\n\n32:51.560 --> 32:53.320\n synergistic with one another.\n\n32:53.320 --> 32:54.240\n And you might gain.\n\n32:54.240 --> 32:56.320\n And that moment, we didn't quite see.\n\n32:56.320 --> 32:58.600\n But obviously, that's why it's the beginning.\n\n32:58.600 --> 33:00.920\n That synergy might emerge with scale.\n\n33:00.920 --> 33:02.160\n Right, might emerge with scale.\n\n33:02.160 --> 33:05.160\n And also, I believe there's some new research or ways\n\n33:05.160 --> 33:08.480\n in which you prepare the data that you\n\n33:08.480 --> 33:11.560\n might need to make it more clear to the model\n\n33:11.560 --> 33:14.120\n that you're not only playing Atari,\n\n33:14.120 --> 33:16.240\n and you start from a screen.\n\n33:16.240 --> 33:18.400\n And here is up and a screen and down.\n\n33:18.400 --> 33:20.640\n Maybe you can think of playing Atari\n\n33:20.640 --> 33:23.880\n as there's some sort of context that is needed for the agent\n\n33:23.880 --> 33:26.920\n before it starts seeing, oh, this is an Atari screen.\n\n33:26.920 --> 33:28.640\n I'm going to start playing.\n\n33:28.640 --> 33:33.360\n You might require, for instance, to be told in words,\n\n33:33.360 --> 33:36.840\n hey, in this sequence that I'm showing,\n\n33:36.840 --> 33:39.080\n you're going to be playing an Atari game.\n\n33:39.080 --> 33:44.400\n So text might actually be a good driver to enhance the data.\n\n33:44.400 --> 33:46.960\n So then these connections might be made more easily.\n\n33:46.960 --> 33:51.200\n So that's an idea that we start seeing in language.\n\n33:51.200 --> 33:55.080\n But obviously, beyond, this is going to be effective.\n\n33:55.080 --> 33:57.400\n It's not like I don't show you a screen,\n\n33:57.400 --> 34:01.000\n and you, from scratch, you're supposed to learn a game.\n\n34:01.000 --> 34:03.360\n There is a lot of context we might set.\n\n34:03.360 --> 34:05.840\n So there might be some work needed as well\n\n34:05.840 --> 34:07.720\n to set that context.\n\n34:07.720 --> 34:10.640\n But anyways, there's a lot of work.\n\n34:10.640 --> 34:13.520\n So that context puts all the different modalities\n\n34:13.520 --> 34:16.640\n on the same level ground to provide the context best.\n\n34:16.640 --> 34:19.880\n So maybe on that point, so there's\n\n34:19.880 --> 34:25.480\n this task, which may not seem trivial, of tokenizing the data,\n\n34:25.480 --> 34:28.480\n of converting the data into pieces,\n\n34:28.480 --> 34:34.400\n into basic atomic elements that then could cross modalities\n\n34:34.400 --> 34:35.240\n somehow.\n\n34:35.240 --> 34:37.840\n So what's tokenization?\n\n34:37.840 --> 34:39.640\n How do you tokenize text?\n\n34:39.640 --> 34:42.160\n How do you tokenize images?\n\n34:42.160 --> 34:47.080\n How do you tokenize games and actions and robotics tasks?\n\n34:47.080 --> 34:48.320\n Yeah, that's a great question.\n\n34:48.320 --> 34:52.840\n So tokenization is the entry point\n\n34:52.840 --> 34:55.600\n to actually make all the data look like a sequence,\n\n34:55.600 --> 34:59.480\n because tokens then are just these little puzzle pieces.\n\n34:59.480 --> 35:01.760\n We break down anything into these puzzle pieces,\n\n35:01.760 --> 35:05.560\n and then we just model, what's this puzzle look like when\n\n35:05.560 --> 35:09.600\n you make it lay down in a line, so to speak, in a sequence?\n\n35:09.600 --> 35:15.440\n So in Gato, the text, there's a lot of work.\n\n35:15.440 --> 35:17.400\n You tokenize text usually by looking\n\n35:17.400 --> 35:20.040\n at commonly used substrings, right?\n\n35:20.040 --> 35:23.680\n So there's ING in English is a very common substring,\n\n35:23.680 --> 35:25.480\n so that becomes a token.\n\n35:25.480 --> 35:29.080\n There's quite a well studied problem on tokenizing text.\n\n35:29.080 --> 35:31.560\n And Gato just used the standard techniques\n\n35:31.560 --> 35:34.280\n that have been developed from many years,\n\n35:34.280 --> 35:38.000\n even starting from ngram models in the 1950s and so on.\n\n35:38.000 --> 35:40.440\n Just for context, how many tokens,\n\n35:40.440 --> 35:42.640\n what order, magnitude, number of tokens\n\n35:42.640 --> 35:45.120\n is required for a word, usually?\n\n35:45.120 --> 35:46.200\n What are we talking about?\n\n35:46.200 --> 35:48.880\n Yeah, for a word in English, I mean,\n\n35:48.880 --> 35:51.120\n every language is very different.\n\n35:51.120 --> 35:53.920\n The current level or granularity of tokenization\n\n35:53.920 --> 35:57.840\n generally means it's maybe two to five.\n\n35:57.840 --> 36:00.200\n I mean, I don't know the statistics exactly,\n\n36:00.200 --> 36:03.000\n but to give you an idea, we don't tokenize\n\n36:03.000 --> 36:04.160\n at the level of letters.\n\n36:04.160 --> 36:05.720\n Then it would probably be, I don't\n\n36:05.720 --> 36:08.080\n know what the average length of a word is in English,\n\n36:08.080 --> 36:11.400\n but that would be the minimum set of tokens you could use.\n\n36:11.400 --> 36:13.200\n It was bigger than letters, smaller than words.\n\n36:13.200 --> 36:13.880\n Yes, yes.\n\n36:13.880 --> 36:16.840\n And you could think of very, very common words like the.\n\n36:16.840 --> 36:18.760\n I mean, that would be a single token,\n\n36:18.760 --> 36:22.360\n but very quickly you're talking two, three, four tokens or so.\n\n36:22.360 --> 36:24.680\n Have you ever tried to tokenize emojis?\n\n36:24.680 --> 36:30.080\n Emojis are actually just sequences of letters, so.\n\n36:30.080 --> 36:33.000\n Maybe to you, but to me they mean so much more.\n\n36:33.000 --> 36:35.080\n Yeah, you can render the emoji, but you\n\n36:35.080 --> 36:36.840\n might if you actually just.\n\n36:36.840 --> 36:39.360\n Yeah, this is a philosophical question.\n\n36:39.360 --> 36:43.320\n Is emojis an image or a text?\n\n36:43.320 --> 36:46.640\n The way we do these things is they're actually\n\n36:46.640 --> 36:49.520\n mapped to small sequences of characters.\n\n36:49.520 --> 36:52.600\n So you can actually play with these models\n\n36:52.600 --> 36:55.760\n and input emojis, it will output emojis back,\n\n36:55.760 --> 36:57.960\n which is actually quite a fun exercise.\n\n36:57.960 --> 37:02.240\n You probably can find other tweets about these out there.\n\n37:02.240 --> 37:04.440\n But yeah, so anyways, text.\n\n37:04.440 --> 37:06.720\n It's very clear how this is done.\n\n37:06.720 --> 37:10.560\n And then in Gato, what we did for images\n\n37:10.560 --> 37:14.880\n is we map images to essentially we compressed images,\n\n37:14.880 --> 37:19.120\n so to speak, into something that looks more like less\n\n37:19.120 --> 37:21.320\n like every pixel with every intensity.\n\n37:21.320 --> 37:23.840\n That would mean we have a very long sequence, right?\n\n37:23.840 --> 37:27.320\n Like if we were talking about 100 by 100 pixel images,\n\n37:27.320 --> 37:30.000\n that would make the sequences far too long.\n\n37:30.000 --> 37:32.520\n So what was done there is you just\n\n37:32.520 --> 37:35.760\n use a technique that essentially compresses an image\n\n37:35.760 --> 37:40.160\n into maybe 16 by 16 patches of pixels,\n\n37:40.160 --> 37:42.720\n and then that is mapped, again, tokenized.\n\n37:42.720 --> 37:45.360\n You just essentially quantize this space\n\n37:45.360 --> 37:48.720\n into a special word that actually\n\n37:48.720 --> 37:51.720\n maps to these little sequence of pixels.\n\n37:51.720 --> 37:55.120\n And then you put the pixels together in some raster order,\n\n37:55.120 --> 37:59.360\n and then that's how you get out or in the image\n\n37:59.360 --> 38:00.720\n that you're processing.\n\n38:00.720 --> 38:04.040\n But there's no semantic aspect to that,\n\n38:04.040 --> 38:05.840\n so you're doing some kind of,\n\n38:05.840 --> 38:07.760\n you don't need to understand anything about the image\n\n38:07.760 --> 38:09.640\n in order to tokenize it currently.\n\n38:09.640 --> 38:12.600\n No, you're only using this notion of compression.\n\n38:12.600 --> 38:15.080\n So you're trying to find common,\n\n38:15.080 --> 38:17.640\n it's like JPG or all these algorithms.\n\n38:17.640 --> 38:20.520\n It's actually very similar at the tokenization level.\n\n38:20.520 --> 38:23.320\n All we're doing is finding common patterns\n\n38:23.320 --> 38:27.200\n and then making sure in a lossy way we compress these images\n\n38:27.200 --> 38:29.480\n given the statistics of the images\n\n38:29.480 --> 38:31.840\n that are contained in all the data we deal with.\n\n38:31.840 --> 38:34.200\n Although you could probably argue that JPEG\n\n38:34.200 --> 38:36.600\n does have some understanding of images.\n\n38:38.720 --> 38:42.920\n Because visual information, maybe color,\n\n38:44.000 --> 38:46.920\n compressing crudely based on color\n\n38:46.920 --> 38:51.160\n does capture something important about an image\n\n38:51.160 --> 38:54.640\n that's about its meaning, not just about some statistics.\n\n38:54.640 --> 38:56.640\n Yeah, I mean, JP, as I said,\n\n38:56.640 --> 38:58.640\n the algorithms look actually very similar\n\n38:58.640 --> 39:02.800\n to they use the cosine transform in JPG.\n\n39:04.120 --> 39:07.120\n The approach we usually do in machine learning\n\n39:07.120 --> 39:10.120\n when we deal with images and we do this quantization step\n\n39:10.120 --> 39:11.400\n is a bit more data driven.\n\n39:11.400 --> 39:14.120\n So rather than have some sort of Fourier basis\n\n39:14.120 --> 39:18.880\n for how frequencies appear in the natural world,\n\n39:18.880 --> 39:23.840\n we actually just use the statistics of the images\n\n39:23.840 --> 39:27.000\n and then quantize them based on the statistics,\n\n39:27.000 --> 39:28.280\n much like you do in words, right?\n\n39:28.280 --> 39:32.400\n So common substrings are allocated a token\n\n39:32.400 --> 39:34.400\n and images is very similar.\n\n39:34.400 --> 39:36.960\n But there's no connection.\n\n39:36.960 --> 39:39.240\n The token space, if you think of,\n\n39:39.240 --> 39:41.080\n oh, like the tokens are an integer\n\n39:41.080 --> 39:42.440\n and in the end of the day.\n\n39:42.440 --> 39:46.200\n So now like we work on, maybe we have about,\n\n39:46.200 --> 39:48.000\n let's say, I don't know the exact numbers,\n\n39:48.000 --> 39:51.160\n but let's say 10,000 tokens for text, right?\n\n39:51.160 --> 39:52.840\n Certainly more than characters\n\n39:52.840 --> 39:55.320\n because we have groups of characters and so on.\n\n39:55.320 --> 39:58.280\n So from one to 10,000, those are representing\n\n39:58.280 --> 40:01.000\n all the language and the words we'll see.\n\n40:01.000 --> 40:04.160\n And then images occupy the next set of integers.\n\n40:04.160 --> 40:05.800\n So they're completely independent, right?\n\n40:05.800 --> 40:08.920\n So from 10,001 to 20,000,\n\n40:08.920 --> 40:10.640\n those are the tokens that represent\n\n40:10.640 --> 40:12.760\n these other modality images.\n\n40:12.760 --> 40:17.760\n And that is an interesting aspect that makes it orthogonal.\n\n40:18.640 --> 40:21.600\n So what connects these concepts is the data, right?\n\n40:21.600 --> 40:23.760\n Once you have a data set,\n\n40:23.760 --> 40:26.880\n for instance, that captions images that tells you,\n\n40:26.880 --> 40:30.480\n oh, this is someone playing a frisbee on a green field.\n\n40:30.480 --> 40:34.560\n Now the model will need to predict the tokens\n\n40:34.560 --> 40:37.800\n from the text green field to then the pixels.\n\n40:37.800 --> 40:39.760\n And that will start making the connections\n\n40:39.760 --> 40:40.600\n between the tokens.\n\n40:40.600 --> 40:43.640\n So these connections happen as the algorithm learns.\n\n40:43.640 --> 40:45.840\n And then the last, if we think of these integers,\n\n40:45.840 --> 40:48.760\n the first few are words, the next few are images.\n\n40:48.760 --> 40:53.760\n In Gato, we also allocated the highest order of integers\n\n40:55.240 --> 40:56.240\n to actions, right?\n\n40:56.240 --> 40:59.920\n Which we discretize and actions are very diverse, right?\n\n40:59.920 --> 41:04.120\n In Atari, there's, I don't know if 17 discrete actions.\n\n41:04.120 --> 41:06.960\n In robotics, actions might be torques\n\n41:06.960 --> 41:08.240\n and forces that we apply.\n\n41:08.240 --> 41:11.200\n So we just use kind of similar ideas\n\n41:11.200 --> 41:14.320\n to compress these actions into tokens.\n\n41:14.320 --> 41:18.000\n And then we just, that's how we map now\n\n41:18.000 --> 41:20.800\n all the space to these sequence of integers.\n\n41:20.800 --> 41:22.480\n But they occupy different space\n\n41:22.480 --> 41:24.840\n and what connects them is then the learning algorithm.\n\n41:24.840 --> 41:26.320\n That's where the magic happens.\n\n41:26.320 --> 41:28.840\n So the modalities are orthogonal\n\n41:28.840 --> 41:30.760\n to each other in token space.\n\n41:30.760 --> 41:35.760\n So in the input, everything you add, you add extra tokens.\n\n41:35.760 --> 41:40.440\n And then you're shoving all of that into one place.\n\n41:40.440 --> 41:41.640\n Yes, the transformer.\n\n41:41.640 --> 41:46.400\n And that transformer, that transformer tries\n\n41:46.400 --> 41:49.360\n to look at this gigantic token space\n\n41:49.360 --> 41:52.240\n and tries to form some kind of representation,\n\n41:52.240 --> 41:56.760\n some kind of unique wisdom\n\n41:56.760 --> 41:59.240\n about all of these different modalities.\n\n41:59.240 --> 42:02.120\n How's that possible?\n\n42:02.120 --> 42:06.520\n If you were to sort of like put your psychoanalysis hat on\n\n42:06.520 --> 42:09.400\n and try to psychoanalyze this neural network,\n\n42:09.400 --> 42:11.760\n is it schizophrenic?\n\n42:11.760 --> 42:16.760\n Does it try to, given this very few weights,\n\n42:17.160 --> 42:19.560\n represent multiple disjoint things\n\n42:19.560 --> 42:22.800\n and somehow have them not interfere with each other?\n\n42:22.800 --> 42:27.800\n Or is it somehow building on the joint strength,\n\n42:27.960 --> 42:31.800\n on whatever is common to all the different modalities?\n\n42:31.800 --> 42:34.520\n Like what, if you were to ask a question,\n\n42:34.520 --> 42:38.720\n is it schizophrenic or is it of one mind?\n\n42:38.720 --> 42:42.640\n I mean, it is one mind and it's actually\n\n42:42.640 --> 42:46.800\n the simplest algorithm, which that's kind of in a way\n\n42:46.800 --> 42:49.840\n how it feels like the field hasn't changed\n\n42:49.840 --> 42:52.600\n since back propagation and gradient descent\n\n42:52.600 --> 42:55.760\n was purpose for learning neural networks.\n\n42:55.760 --> 42:58.720\n So there is obviously details on the architecture.\n\n42:58.720 --> 42:59.640\n This has evolved.\n\n42:59.640 --> 43:03.080\n The current iteration is still the transformer,\n\n43:03.080 --> 43:07.440\n which is a powerful sequence modeling architecture.\n\n43:07.440 --> 43:11.000\n But then the goal of this, you know,\n\n43:11.000 --> 43:13.840\n setting these weights to predict the data\n\n43:13.840 --> 43:17.240\n is essentially the same as basically I could describe.\n\n43:17.240 --> 43:18.680\n I mean, we described a few years ago,\n\n43:18.680 --> 43:21.600\n Alpha star language modeling and so on, right?\n\n43:21.600 --> 43:24.600\n We take, let's say an Atari game,\n\n43:24.600 --> 43:27.640\n we map it to a string of numbers\n\n43:27.640 --> 43:30.360\n that will all be probably image space\n\n43:30.360 --> 43:32.440\n and action space interleaved.\n\n43:32.440 --> 43:37.280\n And all we're gonna do is say, okay, given the numbers,\n\n43:37.280 --> 43:40.400\n you know, 10,001, 10,004, 10,005,\n\n43:40.400 --> 43:43.280\n the next number that comes is 20,006,\n\n43:43.280 --> 43:45.400\n which is in the action space.\n\n43:45.400 --> 43:48.880\n And you're just optimizing these weights\n\n43:48.880 --> 43:51.720\n via very simple gradients.\n\n43:51.720 --> 43:53.520\n Like, you know, mathematical is almost\n\n43:53.520 --> 43:55.880\n the most boring algorithm you could imagine.\n\n43:55.880 --> 43:57.800\n We settle the weights so that\n\n43:57.800 --> 44:00.200\n given this particular instance,\n\n44:00.200 --> 44:04.080\n these weights are set to maximize the probability\n\n44:04.080 --> 44:07.280\n of having seen this particular sequence of integers\n\n44:07.280 --> 44:09.120\n for this particular game.\n\n44:09.120 --> 44:11.640\n And then the algorithm does this\n\n44:11.640 --> 44:14.800\n for many, many, many iterations,\n\n44:14.800 --> 44:17.920\n looking at different modalities, different games, right?\n\n44:17.920 --> 44:20.480\n That's the mixture of the dataset we discussed.\n\n44:20.480 --> 44:24.040\n So in a way, it's a very simple algorithm\n\n44:24.040 --> 44:27.560\n and the weights, right, they're all shared, right?\n\n44:27.560 --> 44:30.920\n So in terms of, is it focusing on one modality or not?\n\n44:30.920 --> 44:33.240\n The intermediate weights that are converting\n\n44:33.240 --> 44:35.160\n from these input of integers\n\n44:35.160 --> 44:37.720\n to the target integer you're predicting next,\n\n44:37.720 --> 44:40.320\n those weights certainly are common.\n\n44:40.320 --> 44:43.400\n And then the way that tokenization happens,\n\n44:43.400 --> 44:45.840\n there is a special place in the neural network,\n\n44:45.840 --> 44:49.800\n which is we map this integer, like number 10,001,\n\n44:49.800 --> 44:51.920\n to a vector of real numbers.\n\n44:51.920 --> 44:54.760\n Like real numbers, we can optimize them\n\n44:54.760 --> 44:56.120\n with gradient descent, right?\n\n44:56.120 --> 44:57.080\n The functions we learn\n\n44:57.080 --> 44:59.720\n are actually surprisingly differentiable.\n\n44:59.720 --> 45:01.720\n That's why we compute gradients.\n\n45:01.720 --> 45:03.920\n So this step is the only one\n\n45:03.920 --> 45:06.560\n that this orthogonality you mentioned applies.\n\n45:06.560 --> 45:11.560\n So mapping a certain token for text or image or actions,\n\n45:12.520 --> 45:15.040\n each of these tokens gets its own little vector\n\n45:15.040 --> 45:17.200\n of real numbers that represents this.\n\n45:17.200 --> 45:19.560\n If you look at the field back many years ago,\n\n45:19.560 --> 45:23.480\n people were talking about word vectors or word embeddings.\n\n45:23.480 --> 45:24.320\n These are the same.\n\n45:24.320 --> 45:26.040\n We have word vectors or embeddings.\n\n45:26.040 --> 45:28.880\n We have image vector or embeddings\n\n45:28.880 --> 45:30.880\n and action vector of embeddings.\n\n45:30.880 --> 45:33.920\n And the beauty here is that as you train this model,\n\n45:33.920 --> 45:36.640\n if you visualize these little vectors,\n\n45:36.640 --> 45:38.480\n it might be that they start aligning\n\n45:38.480 --> 45:41.400\n even though they're independent parameters.\n\n45:41.400 --> 45:42.840\n There could be anything,\n\n45:42.840 --> 45:47.480\n but then it might be that you take the word gato or cat,\n\n45:47.480 --> 45:48.520\n which maybe is common enough\n\n45:48.520 --> 45:50.200\n that it actually has its own token.\n\n45:50.200 --> 45:52.400\n And then you take pixels that have a cat\n\n45:52.400 --> 45:53.960\n and you might start seeing\n\n45:53.960 --> 45:57.400\n that these vectors look like they align, right?\n\n45:57.400 --> 46:00.640\n So by learning from this vast amount of data,\n\n46:00.640 --> 46:03.920\n the model is realizing the potential connections\n\n46:03.920 --> 46:05.640\n between these modalities.\n\n46:05.640 --> 46:07.840\n Now, I will say there will be another way,\n\n46:07.840 --> 46:12.840\n at least in part, to not have these different vectors\n\n46:13.160 --> 46:15.520\n for each different modality.\n\n46:15.520 --> 46:18.360\n For instance, when I tell you about actions\n\n46:18.360 --> 46:22.800\n in certain space, I'm defining actions by words, right?\n\n46:22.800 --> 46:26.520\n So you could imagine a world in which I'm not learning\n\n46:26.520 --> 46:31.240\n that the action app in Atari is its own number.\n\n46:31.240 --> 46:34.400\n The action app in Atari maybe is literally the word\n\n46:34.400 --> 46:37.320\n or the sentence app in Atari, right?\n\n46:37.320 --> 46:39.400\n And that would mean we now leverage\n\n46:39.400 --> 46:41.040\n much more from the language.\n\n46:41.040 --> 46:42.520\n This is not what we did here,\n\n46:42.520 --> 46:45.680\n but certainly it might make these connections\n\n46:45.680 --> 46:49.080\n much easier to learn and also to teach the model\n\n46:49.080 --> 46:51.280\n to correct its own actions and so on, right?\n\n46:51.280 --> 46:55.840\n So all these to say that gato is indeed the beginning,\n\n46:55.840 --> 46:59.400\n that it is a radical idea to do this this way,\n\n46:59.400 --> 47:02.320\n but there's probably a lot more to be done\n\n47:02.320 --> 47:04.440\n and the results to be more impressive,\n\n47:04.440 --> 47:07.920\n not only through scale, but also through some new research\n\n47:07.920 --> 47:10.480\n that will come hopefully in the years to come.\n\n47:10.480 --> 47:12.280\n So just to elaborate quickly,\n\n47:12.280 --> 47:16.680\n you mean one possible next step\n\n47:16.680 --> 47:20.200\n or one of the paths that you might take next\n\n47:20.200 --> 47:25.200\n is doing the tokenization fundamentally\n\n47:25.200 --> 47:28.240\n as a kind of linguistic communication.\n\n47:28.240 --> 47:31.320\n So like you convert even images into language.\n\n47:31.320 --> 47:35.520\n So doing something like a crude semantic segmentation,\n\n47:35.520 --> 47:38.360\n trying to just assign a bunch of words to an image\n\n47:38.360 --> 47:42.280\n that like have almost like a dumb entity\n\n47:42.280 --> 47:45.320\n explaining as much as it can about the image.\n\n47:45.320 --> 47:46.920\n And so you convert that into words\n\n47:46.920 --> 47:49.280\n and then you convert games into words\n\n47:49.280 --> 47:52.120\n and then you provide the context in words and all of it.\n\n47:52.120 --> 47:56.320\n And eventually getting to a point\n\n47:56.320 --> 47:58.080\n where everybody agrees with Noam Chomsky\n\n47:58.080 --> 48:00.920\n that language is actually at the core of everything.\n\n48:00.920 --> 48:04.240\n That's it's the base layer of intelligence\n\n48:04.240 --> 48:07.520\n and consciousness and all that kind of stuff, okay.\n\n48:07.520 --> 48:11.240\n You mentioned early on like size, it's hard to grow.\n\n48:11.240 --> 48:12.800\n What did you mean by that?\n\n48:12.800 --> 48:15.680\n Because we're talking about scale might change.\n\n48:17.000 --> 48:18.960\n There might be, and we'll talk about this too,\n\n48:18.960 --> 48:23.880\n like there's a emergent, there's certain things\n\n48:23.880 --> 48:25.640\n about these neural networks that are emergent.\n\n48:25.640 --> 48:28.960\n So certain like performance we can see only with scale\n\n48:28.960 --> 48:30.960\n and there's some kind of threshold of scale.\n\n48:30.960 --> 48:35.960\n So why is it hard to grow something like this Meow network?\n\n48:36.640 --> 48:41.120\n So the Meow network, it's not hard to grow\n\n48:41.120 --> 48:42.600\n if you retrain it.\n\n48:42.600 --> 48:46.840\n What's hard is, well, we have now 1 billion parameters.\n\n48:46.840 --> 48:48.120\n We train them for a while.\n\n48:48.120 --> 48:53.120\n We spend some amount of work towards building these weights\n\n48:53.120 --> 48:55.840\n that are an amazing initial brain\n\n48:55.840 --> 48:58.800\n for doing these kinds of tasks we care about.\n\n48:58.800 --> 49:03.800\n Could we reuse the weights and expand to a larger brain?\n\n49:03.880 --> 49:06.680\n And that is extraordinarily hard,\n\n49:06.680 --> 49:10.040\n but also exciting from a research perspective\n\n49:10.040 --> 49:12.520\n and a practical perspective point of view, right?\n\n49:12.520 --> 49:17.520\n So there's this notion of modularity in software engineering\n\n49:17.520 --> 49:20.360\n and we starting to see some examples\n\n49:20.360 --> 49:23.160\n and work that leverages modularity.\n\n49:23.160 --> 49:26.200\n In fact, if we go back one step from Gato\n\n49:26.200 --> 49:29.560\n to a work that I would say train much larger,\n\n49:29.560 --> 49:32.400\n much more capable network called Flamingo.\n\n49:32.400 --> 49:34.160\n Flamingo did not deal with actions,\n\n49:34.160 --> 49:38.280\n but it definitely dealt with images in an interesting way,\n\n49:38.280 --> 49:40.120\n kind of akin to what Gato did,\n\n49:40.120 --> 49:42.840\n but slightly different technique for tokenizing,\n\n49:42.840 --> 49:45.280\n but we don't need to go into that detail.\n\n49:45.280 --> 49:49.240\n But what Flamingo also did, which Gato didn't do,\n\n49:49.240 --> 49:51.560\n and that just happens because these projects,\n\n49:51.560 --> 49:55.760\n they're different, it's a bit of like the exploratory nature\n\n49:55.760 --> 49:57.120\n of research, which is great.\n\n49:57.120 --> 50:00.480\n The research behind these projects is also modular.\n\n50:00.480 --> 50:01.720\n Yes, exactly.\n\n50:01.720 --> 50:02.640\n And it has to be, right?\n\n50:02.640 --> 50:05.480\n We need to have creativity\n\n50:05.480 --> 50:09.120\n and sometimes you need to protect pockets of people,\n\n50:09.120 --> 50:10.200\n researchers and so on.\n\n50:10.200 --> 50:11.760\n By we, you mean humans.\n\n50:11.760 --> 50:12.720\n Yes.\n\n50:12.720 --> 50:14.480\n And also in particular researchers\n\n50:14.480 --> 50:18.720\n and maybe even further DeepMind or other such labs.\n\n50:18.720 --> 50:20.880\n And then the neural networks themselves.\n\n50:20.880 --> 50:23.480\n So it's modularity all the way down.\n\n50:23.480 --> 50:24.320\n All the way down.\n\n50:24.320 --> 50:27.400\n So the way that we did modularity very beautifully\n\n50:27.400 --> 50:30.000\n in Flamingo is we took Chinchilla,\n\n50:30.000 --> 50:33.440\n which is a language only model, not an agent,\n\n50:33.440 --> 50:36.600\n if we think of actions being necessary for agency.\n\n50:36.600 --> 50:40.840\n So we took Chinchilla, we took the weights of Chinchilla\n\n50:40.840 --> 50:42.640\n and then we froze them.\n\n50:42.640 --> 50:44.680\n We said, these don't change.\n\n50:44.680 --> 50:47.400\n We train them to be very good at predicting the next word.\n\n50:47.400 --> 50:50.120\n It's a very good language model, state of the art\n\n50:50.120 --> 50:52.800\n at the time you release it, et cetera, et cetera.\n\n50:52.800 --> 50:55.360\n We're going to add a capability to see, right?\n\n50:55.360 --> 50:56.800\n We are going to add the ability to see\n\n50:56.800 --> 50:58.200\n to this language model.\n\n50:58.200 --> 51:01.800\n So we're going to attach small pieces of neural networks\n\n51:01.800 --> 51:03.760\n at the right places in the model.\n\n51:03.760 --> 51:07.760\n It's almost like I'm injecting the network\n\n51:07.760 --> 51:10.640\n with some weights and some substructures\n\n51:10.640 --> 51:12.760\n in a good way, right?\n\n51:12.760 --> 51:15.160\n So you need the research to say, what is effective?\n\n51:15.160 --> 51:16.600\n How do you add this capability\n\n51:16.600 --> 51:18.720\n without destroying others, et cetera.\n\n51:18.720 --> 51:23.720\n So we created a small sub network initialized,\n\n51:24.280 --> 51:28.680\n not from random, but actually from self supervised learning\n\n51:28.680 --> 51:32.720\n that a model that understands vision in general.\n\n51:32.720 --> 51:37.160\n And then we took data sets that connect the two modalities,\n\n51:37.160 --> 51:38.680\n vision and language.\n\n51:38.680 --> 51:41.120\n And then we froze the main part,\n\n51:41.120 --> 51:43.640\n the largest portion of the network, which was Chinchilla,\n\n51:43.640 --> 51:45.880\n that is 70 billion parameters.\n\n51:45.880 --> 51:49.160\n And then we added a few more parameters on top,\n\n51:49.160 --> 51:51.360\n trained from scratch, and then some others\n\n51:51.360 --> 51:55.200\n that were pre trained with the capacity to see,\n\n51:55.200 --> 51:57.320\n like it was not tokenization\n\n51:57.320 --> 52:01.360\n in the way I described for Gato, but it's a similar idea.\n\n52:01.360 --> 52:03.560\n And then we trained the whole system.\n\n52:03.560 --> 52:06.520\n Parts of it were frozen, parts of it were new.\n\n52:06.520 --> 52:09.640\n And all of a sudden, we developed Flamingo,\n\n52:09.640 --> 52:12.520\n which is an amazing model that is essentially,\n\n52:12.520 --> 52:14.960\n I mean, describing it is a chatbot\n\n52:14.960 --> 52:16.920\n where you can also upload images\n\n52:16.920 --> 52:19.880\n and start conversing about images.\n\n52:19.880 --> 52:23.680\n But it's also kind of a dialogue style chatbot.\n\n52:23.680 --> 52:26.600\n So the input is images and text and the output is text.\n\n52:26.600 --> 52:27.440\n Exactly.\n\n52:28.480 --> 52:31.760\n How many parameters, you said 70 billion for Chinchilla?\n\n52:31.760 --> 52:33.200\n Yeah, Chinchilla is 70 billion.\n\n52:33.200 --> 52:34.600\n And then the ones we add on top,\n\n52:34.600 --> 52:38.000\n which kind of almost is almost like a way\n\n52:38.000 --> 52:40.920\n to overwrite its little activations\n\n52:40.920 --> 52:42.400\n so that when it sees vision,\n\n52:42.400 --> 52:45.280\n it does kind of a correct computation of what it's seeing,\n\n52:45.280 --> 52:47.920\n mapping it back towards, so to speak.\n\n52:47.920 --> 52:50.800\n That adds an extra 10 billion parameters, right?\n\n52:50.800 --> 52:53.920\n So it's total 80 billion, the largest one we released.\n\n52:53.920 --> 52:57.320\n And then you train it on a few datasets\n\n52:57.320 --> 52:59.280\n that contain vision and language.\n\n52:59.280 --> 53:01.120\n And once you interact with the model,\n\n53:01.120 --> 53:04.160\n you start seeing that you can upload an image\n\n53:04.160 --> 53:07.960\n and start sort of having a dialogue about the image,\n\n53:07.960 --> 53:09.480\n which is actually not something,\n\n53:09.480 --> 53:12.520\n it's very similar and akin to what we saw in language only.\n\n53:12.520 --> 53:15.240\n These prompting abilities that it has,\n\n53:15.240 --> 53:17.720\n you can teach it a new vision task, right?\n\n53:17.720 --> 53:20.440\n It does things beyond the capabilities\n\n53:20.440 --> 53:24.480\n that in theory the datasets provided in themselves,\n\n53:24.480 --> 53:27.080\n but because it leverages a lot of the language knowledge\n\n53:27.080 --> 53:28.880\n acquired from Chinchilla,\n\n53:28.880 --> 53:31.760\n it actually has this few shot learning ability\n\n53:31.760 --> 53:33.080\n and these emerging abilities\n\n53:33.080 --> 53:34.640\n that we didn't even measure\n\n53:34.640 --> 53:36.400\n once we were developing the model,\n\n53:36.400 --> 53:40.080\n but once developed, then as you play with the interface,\n\n53:40.080 --> 53:42.320\n you can start seeing, wow, okay, yeah, it's cool.\n\n53:42.320 --> 53:45.000\n We can upload, I think one of the tweets\n\n53:45.000 --> 53:47.840\n talking about Twitter was this image from Obama\n\n53:47.840 --> 53:49.840\n that is placing a weight\n\n53:49.840 --> 53:52.400\n and someone is kind of weighting themselves\n\n53:52.400 --> 53:54.880\n and it's kind of a joke style image.\n\n53:54.880 --> 53:57.840\n And it's notable because I think Andrew Carpati\n\n53:57.840 --> 53:59.400\n a few years ago said,\n\n53:59.400 --> 54:02.320\n no computer vision system can understand\n\n54:02.320 --> 54:04.720\n the subtlety of this joke in this image,\n\n54:04.720 --> 54:06.360\n all the things that go on.\n\n54:06.360 --> 54:09.600\n And so what we try to do, and it's very anecdotally,\n\n54:09.600 --> 54:12.120\n I mean, this is not a proof that we solved this issue,\n\n54:12.120 --> 54:15.720\n but it just shows that you can upload now this image\n\n54:15.720 --> 54:17.560\n and start conversing with the model,\n\n54:17.560 --> 54:21.360\n trying to make out if it gets that there's a joke\n\n54:21.360 --> 54:23.520\n because the person weighting themselves\n\n54:23.520 --> 54:25.040\n doesn't see that someone behind\n\n54:25.040 --> 54:27.840\n is making the weight higher and so on and so forth.\n\n54:27.840 --> 54:30.760\n So it's a fascinating capability\n\n54:30.760 --> 54:33.240\n and it comes from this key idea of modularity\n\n54:33.240 --> 54:34.800\n where we took a frozen brain\n\n54:34.800 --> 54:37.760\n and we just added a new capability.\n\n54:37.760 --> 54:40.600\n So the question is, should we,\n\n54:40.600 --> 54:42.720\n so in a way you can see even from DeepMind,\n\n54:42.720 --> 54:46.280\n we have Flamingo that this moderate approach\n\n54:46.280 --> 54:49.040\n and thus could leverage the scale a bit more reasonably\n\n54:49.040 --> 54:52.200\n because we didn't need to retrain a system from scratch.\n\n54:52.200 --> 54:54.080\n And on the other hand, we had Gato,\n\n54:54.080 --> 54:55.800\n which used the same data sets,\n\n54:55.800 --> 54:57.400\n but then he trained it from scratch, right?\n\n54:57.400 --> 55:00.480\n And so I guess big question for the community\n\n55:00.480 --> 55:02.760\n is should we train from scratch\n\n55:02.760 --> 55:04.640\n or should we embrace modularity?\n\n55:04.640 --> 55:08.640\n And this lies, like this goes back to modularity\n\n55:08.640 --> 55:12.040\n as a way to grow, but reuse seems like natural\n\n55:12.040 --> 55:14.920\n and it was very effective, certainly.\n\n55:14.920 --> 55:18.960\n The next question is, if you go the way of modularity,\n\n55:18.960 --> 55:22.680\n is there a systematic way of freezing weights\n\n55:22.680 --> 55:27.040\n and joining different modalities across,\n\n55:27.040 --> 55:29.200\n you know, not just two or three or four networks,\n\n55:29.200 --> 55:32.280\n but hundreds of networks from all different kinds of places,\n\n55:32.280 --> 55:36.280\n maybe open source network that looks at weather patterns\n\n55:36.280 --> 55:37.880\n and you shove that in somehow\n\n55:37.880 --> 55:40.360\n and then you have networks that, I don't know,\n\n55:40.360 --> 55:42.000\n do all kinds of stuff, play StarCraft\n\n55:42.000 --> 55:43.960\n and play all the other video games\n\n55:43.960 --> 55:48.960\n and you can keep adding them in without significant effort,\n\n55:49.480 --> 55:53.160\n like maybe the effort scales linearly or something like that\n\n55:53.160 --> 55:54.880\n as opposed to like the more network you add,\n\n55:54.880 --> 55:57.840\n the more you have to worry about the instabilities created.\n\n55:57.840 --> 55:59.840\n Yeah, so that vision is beautiful.\n\n55:59.840 --> 56:03.400\n I think there's still the question\n\n56:03.400 --> 56:06.720\n about within single modalities, like Chinchilla was reused,\n\n56:06.720 --> 56:10.120\n but now if we train a next iteration of language models,\n\n56:10.120 --> 56:11.720\n are we gonna use Chinchilla or not?\n\n56:11.720 --> 56:13.040\n Yeah, how do you swap out Chinchilla?\n\n56:13.040 --> 56:15.840\n Right, so there's still big questions,\n\n56:15.840 --> 56:19.280\n but that idea is actually really akin to software engineering,\n\n56:19.280 --> 56:22.280\n which we're not reimplementing libraries from scratch,\n\n56:22.280 --> 56:25.280\n we're reusing and then building ever more amazing things,\n\n56:25.280 --> 56:28.880\n including neural networks with software that we're reusing.\n\n56:28.880 --> 56:32.120\n So I think this idea of modularity, I like it,\n\n56:32.120 --> 56:33.800\n I think it's here to stay\n\n56:33.800 --> 56:35.840\n and that's also why I mentioned\n\n56:35.840 --> 56:38.160\n it's just the beginning, not the end.\n\n56:38.160 --> 56:39.360\n You've mentioned meta learning,\n\n56:39.360 --> 56:42.760\n so given this promise of Gato,\n\n56:42.760 --> 56:45.960\n can we try to redefine this term\n\n56:45.960 --> 56:47.560\n that's almost akin to consciousness\n\n56:47.560 --> 56:50.120\n because it means different things to different people\n\n56:50.120 --> 56:52.360\n throughout the history of artificial intelligence,\n\n56:52.360 --> 56:56.600\n but what do you think meta learning is\n\n56:56.600 --> 57:00.040\n and looks like now in the five years, 10 years,\n\n57:00.040 --> 57:03.160\n will it look like the system like Gato, but scaled?\n\n57:03.160 --> 57:07.000\n What's your sense of, what does meta learning look like?\n\n57:07.000 --> 57:10.480\n Do you think with all the wisdom we've learned so far?\n\n57:10.480 --> 57:11.520\n Yeah, great question.\n\n57:11.520 --> 57:14.480\n Maybe it's good to give another data point\n\n57:14.480 --> 57:16.160\n looking backwards rather than forward.\n\n57:16.160 --> 57:22.880\n So when we talk in 2019,\n\n57:22.880 --> 57:26.480\n meta learning meant something that has changed\n\n57:26.480 --> 57:31.120\n mostly through the revolution of GPT3 and beyond.\n\n57:31.120 --> 57:35.000\n So what meta learning meant at the time\n\n57:35.000 --> 57:37.640\n was driven by what benchmarks people care about\n\n57:37.640 --> 57:38.800\n in meta learning.\n\n57:38.800 --> 57:42.560\n And the benchmarks were about a capability\n\n57:42.560 --> 57:44.960\n to learn about object identities.\n\n57:44.960 --> 57:48.480\n So it was very much overfitted to vision\n\n57:48.480 --> 57:50.360\n and object classification.\n\n57:50.360 --> 57:52.880\n And the part that was meta about that was that,\n\n57:52.880 --> 57:55.320\n oh, we're not just learning a thousand categories\n\n57:55.320 --> 57:57.040\n that ImageNet tells us to learn.\n\n57:57.040 --> 57:59.200\n We're going to learn object categories\n\n57:59.200 --> 58:03.280\n that can be defined when we interact with the model.\n\n58:03.280 --> 58:06.640\n So it's interesting to see the evolution, right?\n\n58:06.640 --> 58:10.720\n The way this started was we have a special language\n\n58:10.720 --> 58:13.200\n that was a data set, a small data set\n\n58:13.200 --> 58:15.920\n that we prompted the model with saying,\n\n58:15.920 --> 58:18.960\n hey, here is a new classification task.\n\n58:18.960 --> 58:21.720\n I'll give you one image and the name,\n\n58:21.720 --> 58:24.320\n which was an integer at the time of the image\n\n58:24.320 --> 58:25.920\n and a different image and so on.\n\n58:25.920 --> 58:30.000\n So you have a small prompt in the form of a data set,\n\n58:30.000 --> 58:31.600\n a machine learning data set.\n\n58:31.600 --> 58:35.480\n And then you got then a system that could then predict\n\n58:35.480 --> 58:37.440\n or classify these objects that you just\n\n58:37.440 --> 58:40.280\n defined kind of on the fly.\n\n58:40.280 --> 58:46.480\n So fast forward, it was revealed that language models\n\n58:46.480 --> 58:47.440\n are few shot learners.\n\n58:47.440 --> 58:49.120\n That's the title of the paper.\n\n58:49.120 --> 58:50.080\n So very good title.\n\n58:50.080 --> 58:51.480\n Sometimes titles are really good.\n\n58:51.480 --> 58:53.520\n So this one is really, really good.\n\n58:53.520 --> 58:58.800\n Because that's the point of GPT3 that showed that, look, sure,\n\n58:58.800 --> 59:00.960\n we can focus on object classification\n\n59:00.960 --> 59:04.160\n and what meta learning means within the space of learning\n\n59:04.160 --> 59:05.400\n object categories.\n\n59:05.400 --> 59:07.440\n This goes beyond, or before rather,\n\n59:07.440 --> 59:10.120\n to also Omniglot, before ImageNet and so on.\n\n59:10.120 --> 59:11.560\n So there's a few benchmarks.\n\n59:11.560 --> 59:15.240\n To now, all of a sudden, we're a bit unlocked from benchmarks.\n\n59:15.240 --> 59:17.960\n And through language, we can define tasks.\n\n59:17.960 --> 59:20.280\n So we're literally telling the model\n\n59:20.280 --> 59:23.920\n some logical task or a little thing that we wanted to do.\n\n59:23.920 --> 59:26.000\n We prompt it much like we did before,\n\n59:26.000 --> 59:28.440\n but now we prompt it through natural language.\n\n59:28.440 --> 59:32.280\n And then not perfectly, I mean, these models have failure modes\n\n59:32.280 --> 59:35.560\n and that's fine, but these models then\n\n59:35.560 --> 59:37.240\n are now doing a new task.\n\n59:37.240 --> 59:40.520\n And so they meta learn this new capability.\n\n59:40.520 --> 59:43.480\n Now, that's where we are now.\n\n59:43.480 --> 59:47.320\n Flamingo expanded this to visual and language,\n\n59:47.320 --> 59:49.440\n but it basically has the same abilities.\n\n59:49.440 --> 59:52.720\n You can teach it, for instance, an emergent property\n\n59:52.720 --> 59:55.320\n was that you can take pictures of numbers\n\n59:55.320 --> 59:59.040\n and then do arithmetic with the numbers just by teaching it,\n\n59:59.040 --> 1:00:03.720\n oh, when I show you 3 plus 6, I want you to output 9.\n\n1:00:03.720 --> 1:00:06.880\n And you show it a few examples, and now it does that.\n\n1:00:06.880 --> 1:00:12.800\n So it went way beyond the image net categorization of images\n\n1:00:12.800 --> 1:00:17.280\n that we were a bit stuck maybe before this revelation\n\n1:00:17.280 --> 1:00:19.200\n moment that happened in 2000.\n\n1:00:19.200 --> 1:00:21.960\n I believe it was 19, but it was after we checked.\n\n1:00:21.960 --> 1:00:24.400\n In that way, it has solved meta learning\n\n1:00:24.400 --> 1:00:26.160\n as was previously defined.\n\n1:00:26.160 --> 1:00:27.880\n Yes, it expanded what it meant.\n\n1:00:27.880 --> 1:00:29.680\n So that's what you say, what does it mean?\n\n1:00:29.680 --> 1:00:31.400\n So it's an evolving term.\n\n1:00:31.400 --> 1:00:35.320\n But here is maybe now looking forward,\n\n1:00:35.320 --> 1:00:38.080\n looking at what's happening, obviously,\n\n1:00:38.080 --> 1:00:42.560\n in the community with more modalities, what we can expect.\n\n1:00:42.560 --> 1:00:45.040\n And I would certainly hope to see the following.\n\n1:00:45.040 --> 1:00:48.400\n And this is a pretty drastic hope.\n\n1:00:48.400 --> 1:00:51.200\n But in five years, maybe we chat again.\n\n1:00:51.200 --> 1:00:55.920\n And we have a system, a set of weights\n\n1:00:55.920 --> 1:00:59.840\n that we can teach it to play StarCraft.\n\n1:00:59.840 --> 1:01:01.480\n Maybe not at the level of AlphaStar,\n\n1:01:01.480 --> 1:01:03.720\n but play StarCraft, a complex game,\n\n1:01:03.720 --> 1:01:06.920\n we teach it through interactions to prompting.\n\n1:01:06.920 --> 1:01:08.600\n You can certainly prompt a system.\n\n1:01:08.600 --> 1:01:11.880\n That's what Gata shows to play some simple Atari games.\n\n1:01:11.880 --> 1:01:15.360\n So imagine if you start talking to a system,\n\n1:01:15.360 --> 1:01:17.280\n teaching it a new game, showing it\n\n1:01:17.280 --> 1:01:20.960\n examples of in this particular game,\n\n1:01:20.960 --> 1:01:22.720\n this user did something good.\n\n1:01:22.720 --> 1:01:25.440\n Maybe the system can even play and ask you questions.\n\n1:01:25.440 --> 1:01:27.000\n Say, hey, I played this game.\n\n1:01:27.000 --> 1:01:28.040\n I just played this game.\n\n1:01:28.040 --> 1:01:29.040\n Did I do well?\n\n1:01:29.040 --> 1:01:30.440\n Can you teach me more?\n\n1:01:30.440 --> 1:01:34.720\n So five, maybe to 10 years, these capabilities,\n\n1:01:34.720 --> 1:01:36.400\n or what meta learning means, will\n\n1:01:36.400 --> 1:01:38.800\n be much more interactive, much more rich,\n\n1:01:38.800 --> 1:01:41.640\n and through domains that we were specializing.\n\n1:01:41.640 --> 1:01:42.920\n So you see the difference.\n\n1:01:42.920 --> 1:01:47.000\n We built AlphaStar Specialized to play StarCraft.\n\n1:01:47.000 --> 1:01:50.400\n The algorithms were general, but the weights were specialized.\n\n1:01:50.400 --> 1:01:54.160\n And what we're hoping is that we can teach a network\n\n1:01:54.160 --> 1:01:58.560\n to play games, to play any game, just using games as an example,\n\n1:01:58.560 --> 1:02:01.440\n through interacting with it, teaching it,\n\n1:02:01.440 --> 1:02:04.000\n uploading the Wikipedia page of StarCraft.\n\n1:02:04.000 --> 1:02:06.120\n This is in the horizon.\n\n1:02:06.120 --> 1:02:09.360\n And obviously, there are details that need to be filled\n\n1:02:09.360 --> 1:02:11.000\n and research needs to be done.\n\n1:02:11.000 --> 1:02:13.200\n But that's how I see meta learning above,\n\n1:02:13.200 --> 1:02:15.360\n which is going to be beyond prompting.\n\n1:02:15.360 --> 1:02:18.080\n It's going to be a bit more interactive.\n\n1:02:18.080 --> 1:02:20.720\n The system might tell us to give it feedback\n\n1:02:20.720 --> 1:02:24.080\n after it maybe makes mistakes or it loses a game.\n\n1:02:24.080 --> 1:02:26.240\n But it's nonetheless very exciting\n\n1:02:26.240 --> 1:02:28.960\n because if you think about this this way,\n\n1:02:28.960 --> 1:02:30.600\n the benchmarks are already there.\n\n1:02:30.600 --> 1:02:33.120\n We just repurposed the benchmarks.\n\n1:02:33.120 --> 1:02:38.440\n So in a way, I like to map the space of what\n\n1:02:38.440 --> 1:02:45.480\n maybe AGI means to say, OK, we went 101% performance in Go,\n\n1:02:45.480 --> 1:02:47.920\n in Chess, in StarCraft.\n\n1:02:47.920 --> 1:02:51.920\n The next iteration might be 20% performance\n\n1:02:51.920 --> 1:02:54.720\n across, quote unquote, all tasks.\n\n1:02:54.720 --> 1:02:57.720\n And even if it's not as good, it's fine.\n\n1:02:57.720 --> 1:02:59.960\n We have ways to also measure progress\n\n1:02:59.960 --> 1:03:04.320\n because we have those specialized agents and so on.\n\n1:03:04.320 --> 1:03:06.160\n So this is, to me, very exciting.\n\n1:03:06.160 --> 1:03:10.080\n And these next iteration models are definitely\n\n1:03:10.080 --> 1:03:13.360\n hinting at that direction of progress,\n\n1:03:13.360 --> 1:03:14.720\n which hopefully we can have.\n\n1:03:14.720 --> 1:03:16.440\n There are obviously some things that\n\n1:03:16.440 --> 1:03:20.120\n could go wrong in terms of we might not have the tools.\n\n1:03:20.120 --> 1:03:22.600\n Maybe transformers are not enough.\n\n1:03:22.600 --> 1:03:24.920\n There are some breakthroughs to come, which\n\n1:03:24.920 --> 1:03:27.560\n makes the field more exciting to people like me as well,\n\n1:03:27.560 --> 1:03:28.600\n of course.\n\n1:03:28.600 --> 1:03:32.040\n But that's, if you ask me, five to 10 years,\n\n1:03:32.040 --> 1:03:33.800\n you might see these models that start\n\n1:03:33.800 --> 1:03:36.880\n to look more like weights that are already trained.\n\n1:03:36.880 --> 1:03:40.520\n And then it's more about teaching or make\n\n1:03:40.520 --> 1:03:44.040\n their meta learn what you're trying\n\n1:03:44.040 --> 1:03:47.000\n to induce in terms of tasks and so on,\n\n1:03:47.000 --> 1:03:49.920\n well beyond the simple now tasks we're\n\n1:03:49.920 --> 1:03:53.200\n starting to see emerge like small arithmetic tasks\n\n1:03:53.200 --> 1:03:54.280\n and so on.\n\n1:03:54.280 --> 1:03:55.720\n So a few questions around that.\n\n1:03:55.720 --> 1:03:57.200\n This is fascinating.\n\n1:03:57.200 --> 1:04:01.440\n So that kind of teaching, interactive,\n\n1:04:01.440 --> 1:04:02.760\n so it's beyond prompting.\n\n1:04:02.760 --> 1:04:05.240\n So it's interacting with the neural network.\n\n1:04:05.240 --> 1:04:08.520\n That's different than the training process.\n\n1:04:08.520 --> 1:04:12.440\n So it's different than the optimization\n\n1:04:12.440 --> 1:04:15.920\n over differentiable functions.\n\n1:04:15.920 --> 1:04:17.240\n This is already trained.\n\n1:04:17.240 --> 1:04:21.800\n And now you're teaching, I mean, it's\n\n1:04:21.800 --> 1:04:25.560\n almost akin to the brain, the neurons already\n\n1:04:25.560 --> 1:04:26.960\n set with their connections.\n\n1:04:26.960 --> 1:04:30.000\n On top of that, you're now using that infrastructure\n\n1:04:30.000 --> 1:04:33.640\n to build up further knowledge.\n\n1:04:33.640 --> 1:04:37.200\n So that's a really interesting distinction that's actually\n\n1:04:37.200 --> 1:04:40.320\n not obvious from a software engineering perspective,\n\n1:04:40.320 --> 1:04:42.560\n that there's a line to be drawn.\n\n1:04:42.560 --> 1:04:44.880\n Because you always think for a neural network to learn,\n\n1:04:44.880 --> 1:04:49.880\n it has to be retrained, trained and retrained.\n\n1:04:49.880 --> 1:04:54.000\n And prompting is a way of teaching.\n\n1:04:54.000 --> 1:04:55.920\n And you'll now work a little bit of context\n\n1:04:55.920 --> 1:04:57.960\n about whatever the heck you're trying it to do.\n\n1:04:57.960 --> 1:05:00.440\n So you can maybe expand this prompting capability\n\n1:05:00.440 --> 1:05:03.320\n by making it interact.\n\n1:05:03.320 --> 1:05:04.680\n That's really, really interesting.\n\n1:05:04.680 --> 1:05:08.080\n By the way, this is not, if you look at way back\n\n1:05:08.080 --> 1:05:11.840\n at different ways to tackle even classification tasks.\n\n1:05:11.840 --> 1:05:16.440\n So this comes from longstanding literature\n\n1:05:16.440 --> 1:05:18.240\n in machine learning.\n\n1:05:18.240 --> 1:05:20.800\n What I'm suggesting could sound to some\n\n1:05:20.800 --> 1:05:23.400\n like a bit like nearest neighbor.\n\n1:05:23.400 --> 1:05:27.120\n So nearest neighbor is almost the simplest algorithm\n\n1:05:27.120 --> 1:05:30.200\n that does not require learning.\n\n1:05:30.200 --> 1:05:32.640\n So it has this interesting, you don't\n\n1:05:32.640 --> 1:05:34.320\n need to compute gradients.\n\n1:05:34.320 --> 1:05:37.560\n And what nearest neighbor does is you, quote unquote,\n\n1:05:37.560 --> 1:05:39.960\n have a data set or upload a data set.\n\n1:05:39.960 --> 1:05:42.040\n And then all you need to do is a way\n\n1:05:42.040 --> 1:05:44.720\n to measure distance between points.\n\n1:05:44.720 --> 1:05:46.680\n And then to classify a new point,\n\n1:05:46.680 --> 1:05:48.360\n you're just simply computing, what's\n\n1:05:48.360 --> 1:05:51.240\n the closest point in this massive amount of data?\n\n1:05:51.240 --> 1:05:52.720\n And that's my answer.\n\n1:05:52.720 --> 1:05:55.440\n So you can think of prompting in a way\n\n1:05:55.440 --> 1:05:58.680\n as you're uploading not just simple points.\n\n1:05:58.680 --> 1:06:02.480\n And the metric is not the distance between the images\n\n1:06:02.480 --> 1:06:03.320\n or something simple.\n\n1:06:03.320 --> 1:06:06.040\n It's something that you compute that's much more advanced.\n\n1:06:06.040 --> 1:06:09.040\n But in a way, it's very similar.\n\n1:06:09.040 --> 1:06:12.600\n You simply are uploading some knowledge\n\n1:06:12.600 --> 1:06:15.040\n to this pre trained system in nearest neighbor.\n\n1:06:15.040 --> 1:06:17.280\n Maybe the metric is learned or not,\n\n1:06:17.280 --> 1:06:19.400\n but you don't need to further train it.\n\n1:06:19.400 --> 1:06:23.680\n And then now you immediately get a classifier out of this.\n\n1:06:23.680 --> 1:06:25.840\n Now it's just an evolution of that concept,\n\n1:06:25.840 --> 1:06:28.080\n very classical concept in machine learning, which\n\n1:06:28.080 --> 1:06:32.640\n is just learning through what's the closest point, closest\n\n1:06:32.640 --> 1:06:34.720\n by some distance, and that's it.\n\n1:06:34.720 --> 1:06:36.120\n It's an evolution of that.\n\n1:06:36.120 --> 1:06:39.400\n And I will say how I saw meta learning when\n\n1:06:39.400 --> 1:06:44.760\n we worked on a few ideas in 2016 was precisely\n\n1:06:44.760 --> 1:06:47.520\n through the lens of nearest neighbor, which\n\n1:06:47.520 --> 1:06:50.160\n is very common in computer vision community.\n\n1:06:50.160 --> 1:06:52.160\n There's a very active area of research\n\n1:06:52.160 --> 1:06:55.600\n about how do you compute the distance between two images.\n\n1:06:55.600 --> 1:06:57.560\n But if you have a good distance metric,\n\n1:06:57.560 --> 1:06:59.920\n you also have a good classifier.\n\n1:06:59.920 --> 1:07:02.680\n All I'm saying is now these distances and the points\n\n1:07:02.680 --> 1:07:03.800\n are not just images.\n\n1:07:03.800 --> 1:07:08.560\n They're like words or sequences of words and images\n\n1:07:08.560 --> 1:07:10.400\n and actions that teach you something new.\n\n1:07:10.400 --> 1:07:14.680\n But it might be that technique wise those come back.\n\n1:07:14.680 --> 1:07:18.240\n And I will say that it's not necessarily true\n\n1:07:18.240 --> 1:07:21.760\n that you might not ever train the weights a bit further.\n\n1:07:21.760 --> 1:07:24.800\n Some aspect of meta learning, some techniques\n\n1:07:24.800 --> 1:07:28.280\n in meta learning do actually do a bit of fine tuning\n\n1:07:28.280 --> 1:07:29.080\n as it's called.\n\n1:07:29.080 --> 1:07:32.960\n They train the weights a little bit when they get a new task.\n\n1:07:32.960 --> 1:07:37.960\n So as I call the how or how we're going to achieve this,\n\n1:07:37.960 --> 1:07:39.840\n as a deep learner, I'm very skeptic.\n\n1:07:39.840 --> 1:07:41.840\n We're going to try a few things, whether it's\n\n1:07:41.840 --> 1:07:44.200\n a bit of training, adding a few parameters,\n\n1:07:44.200 --> 1:07:45.960\n thinking of these as nearest neighbor,\n\n1:07:45.960 --> 1:07:49.200\n or just simply thinking of there's a sequence of words,\n\n1:07:49.200 --> 1:07:50.440\n it's a prefix.\n\n1:07:50.440 --> 1:07:53.000\n And that's the new classifier.\n\n1:07:53.000 --> 1:07:53.680\n We'll see.\n\n1:07:53.680 --> 1:07:55.480\n There's the beauty of research.\n\n1:07:55.480 --> 1:08:00.160\n But what's important is that is a good goal in itself\n\n1:08:00.160 --> 1:08:03.800\n that I see as very worthwhile pursuing for the next stages\n\n1:08:03.800 --> 1:08:05.720\n of not only meta learning.\n\n1:08:05.720 --> 1:08:10.160\n I think this is basically what's exciting about machine learning\n\n1:08:10.160 --> 1:08:11.400\n period to me.\n\n1:08:11.400 --> 1:08:13.760\n Well, and the interactive aspect of that\n\n1:08:13.760 --> 1:08:16.400\n is also very interesting, the interactive version\n\n1:08:16.400 --> 1:08:22.160\n of nearest neighbor to help you pull out the classifier\n\n1:08:22.160 --> 1:08:23.760\n from this giant thing.\n\n1:08:23.760 --> 1:08:31.040\n OK, is this the way we can go in 5, 10 plus years\n\n1:08:31.040 --> 1:08:38.200\n from any task, sorry, from many tasks to any task?\n\n1:08:38.200 --> 1:08:39.400\n And what does that mean?\n\n1:08:39.400 --> 1:08:42.760\n What does it need to be actually trained on?\n\n1:08:42.760 --> 1:08:45.400\n Which point is the network had enough?\n\n1:08:45.400 --> 1:08:50.360\n So what does a network need to learn about this world\n\n1:08:50.360 --> 1:08:52.440\n in order to be able to perform any task?\n\n1:08:52.440 --> 1:08:57.880\n Is it just as simple as language, image, and action?\n\n1:08:57.880 --> 1:09:02.680\n Or do you need some set of representative images?\n\n1:09:02.680 --> 1:09:05.200\n Like if you only see land images,\n\n1:09:05.200 --> 1:09:06.760\n will you know anything about underwater?\n\n1:09:06.760 --> 1:09:08.720\n Is that some fundamentally different?\n\n1:09:08.720 --> 1:09:09.800\n I don't know.\n\n1:09:09.800 --> 1:09:12.080\n I mean, those are open questions, I would say.\n\n1:09:12.080 --> 1:09:15.280\n I mean, the way you put, let me maybe further your example.\n\n1:09:15.280 --> 1:09:18.920\n If all you see is land images but you're\n\n1:09:18.920 --> 1:09:21.560\n reading all about land and water worlds\n\n1:09:21.560 --> 1:09:25.360\n but in books, imagine, would that be enough?\n\n1:09:25.360 --> 1:09:26.400\n Good question.\n\n1:09:26.400 --> 1:09:27.120\n We don't know.\n\n1:09:27.120 --> 1:09:30.440\n But I guess maybe you can join us\n\n1:09:30.440 --> 1:09:32.120\n if you want in our quest to find this.\n\n1:09:32.120 --> 1:09:33.440\n That's precisely.\n\n1:09:33.440 --> 1:09:34.360\n Water world, yeah.\n\n1:09:34.360 --> 1:09:37.640\n Yes, that's precisely, I mean, the beauty of research.\n\n1:09:37.640 --> 1:09:42.680\n And that's the research business we're in,\n\n1:09:42.680 --> 1:09:46.160\n I guess, is to figure this out and ask the right questions\n\n1:09:46.160 --> 1:09:49.520\n and then iterate with the whole community,\n\n1:09:49.520 --> 1:09:52.640\n publishing findings and so on.\n\n1:09:52.640 --> 1:09:55.160\n But yeah, this is a question.\n\n1:09:55.160 --> 1:09:58.640\n It's not the only question, but it's certainly, as you ask,\n\n1:09:58.640 --> 1:10:00.080\n on my mind constantly.\n\n1:10:00.080 --> 1:10:03.920\n And so we'll need to wait for maybe the, let's say, five\n\n1:10:03.920 --> 1:10:09.400\n years, let's hope it's not 10, to see what are the answers.\n\n1:10:09.400 --> 1:10:12.800\n Some people will largely believe in unsupervised or\n\n1:10:12.800 --> 1:10:15.640\n self supervised learning of single modalities\n\n1:10:15.640 --> 1:10:18.000\n and then crossing them.\n\n1:10:18.000 --> 1:10:21.640\n Some people might think end to end learning is the answer.\n\n1:10:21.640 --> 1:10:23.760\n Modularity is maybe the answer.\n\n1:10:23.760 --> 1:10:27.040\n So we don't know, but we're just definitely excited\n\n1:10:27.040 --> 1:10:27.560\n to find out.\n\n1:10:27.560 --> 1:10:29.280\n But it feels like this is the right time\n\n1:10:29.280 --> 1:10:31.680\n and we're at the beginning of this journey.\n\n1:10:31.680 --> 1:10:36.040\n We're finally ready to do these kind of general big models\n\n1:10:36.040 --> 1:10:37.960\n and agents.\n\n1:10:37.960 --> 1:10:42.480\n What do you sort of specific technical thing\n\n1:10:42.480 --> 1:10:48.040\n about Gato, Flamingo, Chinchilla, Gopher, any of these\n\n1:10:48.040 --> 1:10:51.640\n that is especially beautiful, that was surprising, maybe?\n\n1:10:51.640 --> 1:10:55.200\n Is there something that just jumps out at you?\n\n1:10:55.200 --> 1:10:57.600\n Of course, there's the general thing of like,\n\n1:10:57.600 --> 1:11:00.600\n you didn't think it was possible and then you\n\n1:11:00.600 --> 1:11:03.560\n realize it's possible in terms of the generalizability\n\n1:11:03.560 --> 1:11:05.640\n across modalities and all that kind of stuff.\n\n1:11:05.640 --> 1:11:08.920\n Or maybe how small of a network, relatively speaking,\n\n1:11:08.920 --> 1:11:10.440\n Gato is, all that kind of stuff.\n\n1:11:10.440 --> 1:11:15.160\n But is there some weird little things that were surprising?\n\n1:11:15.160 --> 1:11:18.200\n Look, I'll give you an answer that's very important\n\n1:11:18.200 --> 1:11:22.560\n because maybe people don't quite realize this,\n\n1:11:22.560 --> 1:11:27.200\n but the teams behind these efforts, the actual humans,\n\n1:11:27.200 --> 1:11:31.640\n that's maybe the surprising in an obviously positive way.\n\n1:11:31.640 --> 1:11:34.560\n So anytime you see these breakthroughs,\n\n1:11:34.560 --> 1:11:37.080\n I mean, it's easy to map it to a few people.\n\n1:11:37.080 --> 1:11:39.680\n There's people that are great at explaining things and so on.\n\n1:11:39.680 --> 1:11:40.720\n And that's very nice.\n\n1:11:40.720 --> 1:11:44.720\n But maybe the learnings or the method learnings\n\n1:11:44.720 --> 1:11:50.480\n that I get as a human about this is, sure, we can move forward.\n\n1:11:50.480 --> 1:11:55.640\n But the surprising bit is how important\n\n1:11:55.640 --> 1:11:58.720\n are all the pieces of these projects,\n\n1:11:58.720 --> 1:12:00.080\n how do they come together?\n\n1:12:00.080 --> 1:12:04.440\n So I'll give you maybe some of the ingredients of success\n\n1:12:04.440 --> 1:12:07.680\n that are common across these, but not the obvious ones\n\n1:12:07.680 --> 1:12:08.480\n on machine learning.\n\n1:12:08.480 --> 1:12:11.320\n I can always also give you those.\n\n1:12:11.320 --> 1:12:17.280\n But basically, there is engineering is critical.\n\n1:12:17.280 --> 1:12:21.120\n So very good engineering because ultimately we're\n\n1:12:21.120 --> 1:12:23.720\n collecting data sets, right?\n\n1:12:23.720 --> 1:12:26.640\n So the engineering of data and then\n\n1:12:26.640 --> 1:12:31.160\n of deploying the models at scale into some compute cluster\n\n1:12:31.160 --> 1:12:36.800\n that cannot go understated, that is a huge factor of success.\n\n1:12:36.800 --> 1:12:41.560\n And it's hard to believe that details matter so much.\n\n1:12:41.560 --> 1:12:43.760\n We would like to believe that it's\n\n1:12:43.760 --> 1:12:47.360\n true that there is more and more of a standard formula,\n\n1:12:47.360 --> 1:12:49.360\n as I was saying, like this recipe that\n\n1:12:49.360 --> 1:12:50.520\n works for everything.\n\n1:12:50.520 --> 1:12:53.680\n But then when you zoom into each of these projects,\n\n1:12:53.680 --> 1:12:57.760\n then you realize the devil is indeed in the details.\n\n1:12:57.760 --> 1:13:03.040\n And then the teams have to work together towards these goals.\n\n1:13:03.040 --> 1:13:07.520\n So engineering of data and obviously clusters\n\n1:13:07.520 --> 1:13:09.280\n and large scale is very important.\n\n1:13:09.280 --> 1:13:15.080\n And then one that is often not, maybe nowadays it is more clear\n\n1:13:15.080 --> 1:13:17.160\n is benchmark progress, right?\n\n1:13:17.160 --> 1:13:20.840\n So we're talking here about multiple months of tens\n\n1:13:20.840 --> 1:13:24.520\n of researchers and people that are\n\n1:13:24.520 --> 1:13:28.080\n trying to organize the research and so on working together.\n\n1:13:28.080 --> 1:13:32.120\n And you don't know that you can get there.\n\n1:13:32.120 --> 1:13:34.520\n I mean, this is the beauty.\n\n1:13:34.520 --> 1:13:37.320\n If you're not risking to trying to do something\n\n1:13:37.320 --> 1:13:41.600\n that feels impossible, you're not going to get there.\n\n1:13:41.600 --> 1:13:43.920\n But you need a way to measure progress.\n\n1:13:43.920 --> 1:13:47.680\n So the benchmarks that you build are critical.\n\n1:13:47.680 --> 1:13:50.520\n I've seen this beautifully play out in many projects.\n\n1:13:50.520 --> 1:13:53.840\n I mean, maybe the one I've seen it more consistently,\n\n1:13:53.840 --> 1:13:56.760\n which means we establish the metric,\n\n1:13:56.760 --> 1:13:58.240\n actually the community did.\n\n1:13:58.240 --> 1:14:01.520\n And then we leverage that massively is alpha fold.\n\n1:14:01.520 --> 1:14:05.160\n This is a project where the data, the metrics\n\n1:14:05.160 --> 1:14:06.040\n were all there.\n\n1:14:06.040 --> 1:14:09.080\n And all it took was, and it's easier said than done,\n\n1:14:09.080 --> 1:14:12.840\n an amazing team working not to try\n\n1:14:12.840 --> 1:14:14.720\n to find some incremental improvement\n\n1:14:14.720 --> 1:14:17.920\n and publish, which is one way to do research that is valid,\n\n1:14:17.920 --> 1:14:22.440\n but aim very high and work literally for years\n\n1:14:22.440 --> 1:14:24.080\n to iterate over that process.\n\n1:14:24.080 --> 1:14:25.640\n And working for years with the team,\n\n1:14:25.640 --> 1:14:30.120\n I mean, it is tricky that also happened to happen partly\n\n1:14:30.120 --> 1:14:32.280\n during a pandemic and so on.\n\n1:14:32.280 --> 1:14:34.200\n So I think my meta learning from all this\n\n1:14:34.200 --> 1:14:37.960\n is the teams are critical to the success.\n\n1:14:37.960 --> 1:14:40.200\n And then if now going to the machine learning,\n\n1:14:40.200 --> 1:14:46.880\n the part that's surprising is so we like architectures\n\n1:14:46.880 --> 1:14:48.680\n like neural networks.\n\n1:14:48.680 --> 1:14:53.040\n And I would say this was a very rapidly evolving field\n\n1:14:53.040 --> 1:14:54.920\n until the transformer came.\n\n1:14:54.920 --> 1:14:58.280\n So attention might indeed be all you need,\n\n1:14:58.280 --> 1:15:00.280\n which is the title, also a good title,\n\n1:15:00.280 --> 1:15:02.040\n although in hindsight is good.\n\n1:15:02.040 --> 1:15:03.440\n I don't think at the time I thought\n\n1:15:03.440 --> 1:15:05.040\n this is a great title for a paper.\n\n1:15:05.040 --> 1:15:10.960\n But that architecture is proving that the dream of modeling\n\n1:15:10.960 --> 1:15:15.320\n sequences of any bytes, there is something there that will stick.\n\n1:15:15.320 --> 1:15:18.280\n And I think these advance in architectures\n\n1:15:18.280 --> 1:15:21.000\n in how neural networks are architecture\n\n1:15:21.000 --> 1:15:23.080\n to do what they do.\n\n1:15:23.080 --> 1:15:26.080\n It's been hard to find one that has been so stable\n\n1:15:26.080 --> 1:15:28.880\n and relatively has changed very little\n\n1:15:28.880 --> 1:15:33.080\n since it was invented five or so years ago.\n\n1:15:33.080 --> 1:15:35.840\n So that is a surprising, is a surprise\n\n1:15:35.840 --> 1:15:38.280\n that keeps recurring into other projects.\n\n1:15:38.280 --> 1:15:43.320\n Try to, on a philosophical or technical level, introspect,\n\n1:15:43.320 --> 1:15:45.440\n what is the magic of attention?\n\n1:15:45.440 --> 1:15:47.280\n What is attention?\n\n1:15:47.280 --> 1:15:50.120\n That's attention in people that study cognition,\n\n1:15:50.120 --> 1:15:52.040\n so human attention.\n\n1:15:52.040 --> 1:15:55.760\n I think there's giant wars over what attention means,\n\n1:15:55.760 --> 1:15:57.440\n how it works in the human mind.\n\n1:15:57.440 --> 1:16:00.480\n So there's very simple looks at what\n\n1:16:00.480 --> 1:16:03.840\n attention is in a neural network from the days of attention\n\n1:16:03.840 --> 1:16:04.440\n is all you need.\n\n1:16:04.440 --> 1:16:07.520\n But do you think there's a general principle that's\n\n1:16:07.520 --> 1:16:08.760\n really powerful here?\n\n1:16:08.760 --> 1:16:13.400\n Yeah, so a distinction between transformers and LSTMs,\n\n1:16:13.400 --> 1:16:15.360\n which were what came before.\n\n1:16:15.360 --> 1:16:17.880\n And there was a transitional period\n\n1:16:17.880 --> 1:16:19.720\n where you could use both.\n\n1:16:19.720 --> 1:16:22.000\n In fact, when we talked about AlphaStar,\n\n1:16:22.000 --> 1:16:24.240\n we used transformers and LSTMs.\n\n1:16:24.240 --> 1:16:26.400\n So it was still the beginning of transformers.\n\n1:16:26.400 --> 1:16:27.400\n They were very powerful.\n\n1:16:27.400 --> 1:16:31.480\n But LSTMs were also very powerful sequence models.\n\n1:16:31.480 --> 1:16:35.400\n So the power of the transformer is\n\n1:16:35.400 --> 1:16:38.440\n that it has built in what we call\n\n1:16:38.440 --> 1:16:43.040\n an inductive bias of attention that makes the model.\n\n1:16:43.040 --> 1:16:45.720\n When you think of a sequence of integers,\n\n1:16:45.720 --> 1:16:50.400\n like we discussed this before, this is a sequence of words.\n\n1:16:50.400 --> 1:16:54.800\n When you have to do very hard tasks over these words,\n\n1:16:54.800 --> 1:16:57.840\n this could be we're going to translate a whole paragraph\n\n1:16:57.840 --> 1:17:00.320\n or we're going to predict the next paragraph given\n\n1:17:00.320 --> 1:17:01.320\n 10 paragraphs before.\n\n1:17:04.280 --> 1:17:10.360\n There's some loose intuition from how we do it as a human\n\n1:17:10.360 --> 1:17:15.400\n that is very nicely mimicked and replicated structurally\n\n1:17:15.400 --> 1:17:16.840\n speaking in the transformer, which\n\n1:17:16.840 --> 1:17:21.160\n is this idea of you're looking for something.\n\n1:17:21.160 --> 1:17:25.760\n So you're sort of when you just read a piece of text,\n\n1:17:25.760 --> 1:17:27.880\n now you're thinking what comes next.\n\n1:17:27.880 --> 1:17:31.800\n You might want to relook at the text or look it from scratch.\n\n1:17:31.800 --> 1:17:35.040\n I mean, literally is because there's no recurrence.\n\n1:17:35.040 --> 1:17:37.240\n You're just thinking what comes next.\n\n1:17:37.240 --> 1:17:40.040\n And it's almost hypothesis driven.\n\n1:17:40.040 --> 1:17:46.600\n So if I'm thinking the next word that I write is cat or dog,\n\n1:17:46.600 --> 1:17:49.880\n the way the transformer works almost philosophically\n\n1:17:49.880 --> 1:17:52.840\n is it has these two hypotheses.\n\n1:17:52.840 --> 1:17:55.640\n Is it going to be cat or is it going to be dog?\n\n1:17:55.640 --> 1:17:58.360\n And then it says, OK, if it's cat,\n\n1:17:58.360 --> 1:17:59.920\n I'm going to look for certain words.\n\n1:17:59.920 --> 1:18:01.920\n Not necessarily cat, although cat is an obvious word\n\n1:18:01.920 --> 1:18:03.520\n you would look in the past to see\n\n1:18:03.520 --> 1:18:05.960\n whether it makes more sense to output cat or dog.\n\n1:18:05.960 --> 1:18:09.480\n And then it does some very deep computation\n\n1:18:09.480 --> 1:18:11.480\n over the words and beyond.\n\n1:18:11.480 --> 1:18:16.200\n So it combines the words, but it has the query\n\n1:18:16.200 --> 1:18:18.440\n as we call it that is cat.\n\n1:18:18.440 --> 1:18:20.680\n And then similarly for dog.\n\n1:18:20.680 --> 1:18:24.760\n And so it's a very computational way to think about, look,\n\n1:18:24.760 --> 1:18:27.000\n if I'm thinking deeply about text,\n\n1:18:27.000 --> 1:18:30.600\n I need to go back to look at all of the text, attend over it.\n\n1:18:30.600 --> 1:18:32.200\n But it's not just attention.\n\n1:18:32.200 --> 1:18:34.000\n What is guiding the attention?\n\n1:18:34.000 --> 1:18:36.680\n And that was the key insight from an earlier paper\n\n1:18:36.680 --> 1:18:39.120\n is not how far away is it?\n\n1:18:39.120 --> 1:18:40.840\n I mean, how far away is it is important?\n\n1:18:40.840 --> 1:18:42.720\n What did I just write about?\n\n1:18:42.720 --> 1:18:44.120\n That's critical.\n\n1:18:44.120 --> 1:18:46.760\n But what you wrote about 10 pages ago\n\n1:18:46.760 --> 1:18:48.480\n might also be critical.\n\n1:18:48.480 --> 1:18:53.160\n So you're looking not positionally, but content wise.\n\n1:18:53.160 --> 1:18:56.120\n And transformers have this beautiful way\n\n1:18:56.120 --> 1:18:59.480\n to query for certain content and pull it out\n\n1:18:59.480 --> 1:19:00.440\n in a compressed way.\n\n1:19:00.440 --> 1:19:02.960\n So then you can make a more informed decision.\n\n1:19:02.960 --> 1:19:05.920\n I mean, that's one way to explain transformers.\n\n1:19:05.920 --> 1:19:10.000\n But I think it's a very powerful inductive bias.\n\n1:19:10.000 --> 1:19:12.480\n There might be some details that might change over time,\n\n1:19:12.480 --> 1:19:17.360\n but I think that is what makes transformers so much more\n\n1:19:17.360 --> 1:19:20.080\n powerful than the recurrent networks that\n\n1:19:20.080 --> 1:19:23.600\n were more recency bias based, which obviously works\n\n1:19:23.600 --> 1:19:26.720\n in some tasks, but it has major flaws.\n\n1:19:26.720 --> 1:19:29.240\n Transformer itself has flaws.\n\n1:19:29.240 --> 1:19:31.680\n And I think the main one, the main challenge\n\n1:19:31.680 --> 1:19:35.760\n is these prompts that we just were talking about,\n\n1:19:35.760 --> 1:19:38.040\n they can be 1,000 words long.\n\n1:19:38.040 --> 1:19:40.440\n But if I'm teaching you StarGraph,\n\n1:19:40.440 --> 1:19:41.880\n I'll have to show you videos.\n\n1:19:41.880 --> 1:19:44.600\n I'll have to point you to whole Wikipedia articles\n\n1:19:44.600 --> 1:19:46.120\n about the game.\n\n1:19:46.120 --> 1:19:48.040\n We'll have to interact probably as you play.\n\n1:19:48.040 --> 1:19:49.480\n You'll ask me questions.\n\n1:19:49.480 --> 1:19:52.320\n The context required for us to achieve\n\n1:19:52.320 --> 1:19:54.720\n me being a good teacher to you on the game\n\n1:19:54.720 --> 1:19:58.920\n as you would want to do it with a model, I think\n\n1:19:58.920 --> 1:20:01.720\n goes well beyond the current capabilities.\n\n1:20:01.720 --> 1:20:03.920\n So the question is, how do we benchmark this?\n\n1:20:03.920 --> 1:20:07.320\n And then how do we change the structure of the architectures?\n\n1:20:07.320 --> 1:20:08.840\n I think there's ideas on both sides,\n\n1:20:08.840 --> 1:20:11.800\n but we'll have to see empirically, obviously,\n\n1:20:11.800 --> 1:20:13.320\n what ends up working.\n\n1:20:13.320 --> 1:20:15.280\n And as you talked about, some of the ideas\n\n1:20:15.280 --> 1:20:19.440\n could be keeping the constraint of that length in place,\n\n1:20:19.440 --> 1:20:23.000\n but then forming hierarchical representations\n\n1:20:23.000 --> 1:20:26.600\n to where you can start being much clever in how\n\n1:20:26.600 --> 1:20:28.800\n you use those 1,000 tokens.\n\n1:20:28.800 --> 1:20:30.920\n Indeed.\n\n1:20:30.920 --> 1:20:32.240\n Yeah, that's really interesting.\n\n1:20:32.240 --> 1:20:34.840\n But it also is possible that this attentional mechanism\n\n1:20:34.840 --> 1:20:37.560\n where you basically, you don't have a recency bias,\n\n1:20:37.560 --> 1:20:42.000\n but you look more generally, you make it learnable.\n\n1:20:42.000 --> 1:20:45.240\n The mechanism in which way you look back into the past,\n\n1:20:45.240 --> 1:20:46.760\n you make that learnable.\n\n1:20:46.760 --> 1:20:50.160\n It's also possible we're at the very beginning of that\n\n1:20:50.160 --> 1:20:54.400\n because that, you might become smarter and smarter\n\n1:20:54.400 --> 1:20:58.400\n in the way you query the past.\n\n1:20:58.400 --> 1:21:01.800\n So recent past and distant past and maybe very, very distant\n\n1:21:01.800 --> 1:21:02.320\n past.\n\n1:21:02.320 --> 1:21:04.960\n So almost like the attention mechanism\n\n1:21:04.960 --> 1:21:11.280\n will have to improve and evolve as good as the tokenization\n\n1:21:11.280 --> 1:21:14.960\n mechanism so you can represent long term memory somehow.\n\n1:21:14.960 --> 1:21:16.080\n Yes.\n\n1:21:16.080 --> 1:21:18.240\n And I mean, hierarchies are very,\n\n1:21:18.240 --> 1:21:22.160\n I mean, it's a very nice word that sounds appealing.\n\n1:21:22.160 --> 1:21:25.920\n There's lots of work adding hierarchy to the memories.\n\n1:21:25.920 --> 1:21:29.480\n In practice, it does seem like we keep coming back\n\n1:21:29.480 --> 1:21:33.880\n to the main formula or main architecture.\n\n1:21:33.880 --> 1:21:35.320\n That sometimes tells us something.\n\n1:21:35.320 --> 1:21:38.560\n There is such a sentence that a friend of mine told me,\n\n1:21:38.560 --> 1:21:41.000\n like, whether it wants to work or not.\n\n1:21:41.000 --> 1:21:44.920\n So Transformer was clearly an idea that wanted to work.\n\n1:21:44.920 --> 1:21:47.520\n And then I think there's some principles\n\n1:21:47.520 --> 1:21:49.080\n we believe will be needed.\n\n1:21:49.080 --> 1:21:52.880\n But finding the exact details, details matter so much.\n\n1:21:52.880 --> 1:21:54.200\n That's going to be tricky.\n\n1:21:54.200 --> 1:21:59.440\n I love the idea that there's like you as a human being,\n\n1:21:59.440 --> 1:22:01.280\n you want some ideas to work.\n\n1:22:01.280 --> 1:22:03.800\n And then there's the model that wants some ideas\n\n1:22:03.800 --> 1:22:05.960\n to work and you get to have a conversation\n\n1:22:05.960 --> 1:22:10.520\n to see which more likely the model will win in the end.\n\n1:22:10.520 --> 1:22:12.800\n Because it's the one, you don't have to do any work.\n\n1:22:12.800 --> 1:22:14.360\n The model is the one that has to do the work.\n\n1:22:14.360 --> 1:22:15.840\n So you should listen to the model.\n\n1:22:15.840 --> 1:22:17.440\n And I really love this idea that you\n\n1:22:17.440 --> 1:22:19.160\n talked about the humans in this picture.\n\n1:22:19.160 --> 1:22:21.840\n If I could just briefly ask, one is you're\n\n1:22:21.840 --> 1:22:28.960\n saying the benchmarks about the modular humans working on this,\n\n1:22:28.960 --> 1:22:32.160\n the benchmarks providing a sturdy ground of a wish\n\n1:22:32.160 --> 1:22:34.680\n to do these things that seem impossible.\n\n1:22:34.680 --> 1:22:37.880\n They give you, in the darkest of times,\n\n1:22:37.880 --> 1:22:41.520\n give you hope because little signs of improvement.\n\n1:22:41.520 --> 1:22:42.000\n Yes.\n\n1:22:42.000 --> 1:22:46.560\n Like somehow you're not lost if you have metrics\n\n1:22:46.560 --> 1:22:48.680\n to measure your improvement.\n\n1:22:48.680 --> 1:22:50.800\n And then there's other aspect.\n\n1:22:50.800 --> 1:22:56.560\n You said elsewhere and here today, like titles matter.\n\n1:22:56.560 --> 1:23:01.280\n I wonder how much humans matter in the evolution\n\n1:23:01.280 --> 1:23:03.760\n of all of this, meaning individual humans.\n\n1:23:06.760 --> 1:23:08.160\n Something about their interactions,\n\n1:23:08.160 --> 1:23:11.200\n something about their ideas, how much they change\n\n1:23:11.200 --> 1:23:12.920\n the direction of all of this.\n\n1:23:12.920 --> 1:23:15.440\n Like if you change the humans in this picture,\n\n1:23:15.440 --> 1:23:18.160\n is it that the model is sitting there\n\n1:23:18.160 --> 1:23:22.480\n and it wants some idea to work?\n\n1:23:22.480 --> 1:23:25.000\n Or is it the humans, or maybe the model\n\n1:23:25.000 --> 1:23:27.000\n is providing you 20 ideas that could work.\n\n1:23:27.000 --> 1:23:29.080\n And depending on the humans you pick,\n\n1:23:29.080 --> 1:23:33.160\n they're going to be able to hear some of those ideas.\n\n1:23:33.160 --> 1:23:35.920\n Because you're now directing all of deep learning and deep mind,\n\n1:23:35.920 --> 1:23:37.720\n you get to interact with a lot of projects,\n\n1:23:37.720 --> 1:23:40.600\n a lot of brilliant researchers.\n\n1:23:40.600 --> 1:23:44.080\n How much variability is created by the humans in all of this?\n\n1:23:44.080 --> 1:23:47.320\n Yeah, I mean, I do believe humans matter a lot,\n\n1:23:47.320 --> 1:23:53.360\n at the very least at the time scale of years\n\n1:23:53.360 --> 1:23:56.880\n on when things are happening and what's the sequencing of it.\n\n1:23:56.880 --> 1:24:00.800\n So you get to interact with people that, I mean,\n\n1:24:00.800 --> 1:24:02.200\n you mentioned this.\n\n1:24:02.200 --> 1:24:05.080\n Some people really want some idea to work\n\n1:24:05.080 --> 1:24:07.040\n and they'll persist.\n\n1:24:07.040 --> 1:24:09.400\n And then some other people might be more practical,\n\n1:24:09.400 --> 1:24:12.840\n like I don't care what idea works.\n\n1:24:12.840 --> 1:24:16.800\n I care about cracking protein folding.\n\n1:24:16.800 --> 1:24:21.240\n And at least these two kind of seem opposite sides.\n\n1:24:21.240 --> 1:24:22.400\n We need both.\n\n1:24:22.400 --> 1:24:25.680\n And we've clearly had both historically,\n\n1:24:25.680 --> 1:24:28.960\n and that made certain things happen earlier or later.\n\n1:24:28.960 --> 1:24:33.400\n So definitely humans involved in all of this endeavor\n\n1:24:33.400 --> 1:24:38.640\n have had, I would say, years of change or of ordering\n\n1:24:38.640 --> 1:24:41.840\n how things have happened, which breakthroughs came before,\n\n1:24:41.840 --> 1:24:43.280\n which other breakthroughs, and so on.\n\n1:24:43.280 --> 1:24:45.800\n So certainly that does happen.\n\n1:24:45.800 --> 1:24:50.600\n And so one other, maybe one other axis of distinction\n\n1:24:50.600 --> 1:24:53.840\n is what I called, and this is most commonly used\n\n1:24:53.840 --> 1:24:56.920\n in reinforcement learning, is the exploration exploitation\n\n1:24:56.920 --> 1:24:57.800\n trade off as well.\n\n1:24:57.800 --> 1:25:00.960\n It's not exactly what I meant, although quite related.\n\n1:25:00.960 --> 1:25:07.000\n So when you start trying to help others,\n\n1:25:07.000 --> 1:25:11.440\n like you become a bit more of a mentor\n\n1:25:11.440 --> 1:25:14.600\n to a large group of people, be it a project or the deep\n\n1:25:14.600 --> 1:25:17.440\n learning team or something, or even in the community\n\n1:25:17.440 --> 1:25:20.760\n when you interact with people in conferences and so on,\n\n1:25:20.760 --> 1:25:26.040\n you're identifying quickly some things that are explorative\n\n1:25:26.040 --> 1:25:27.080\n or exploitative.\n\n1:25:27.080 --> 1:25:30.720\n And it's tempting to try to guide people, obviously.\n\n1:25:30.720 --> 1:25:33.160\n I mean, that's what makes our experience.\n\n1:25:33.160 --> 1:25:36.760\n We bring it, and we try to shape things sometimes wrongly.\n\n1:25:36.760 --> 1:25:39.560\n And there's many times that I've been wrong in the past.\n\n1:25:39.560 --> 1:25:40.800\n That's great.\n\n1:25:40.800 --> 1:25:47.800\n But it would be wrong to dismiss any sort of the research\n\n1:25:47.800 --> 1:25:49.880\n styles that I'm observing.\n\n1:25:49.880 --> 1:25:52.760\n And I often get asked, well, you're in industry, right?\n\n1:25:52.760 --> 1:25:55.640\n So we do have access to large compute scale and so on.\n\n1:25:55.640 --> 1:25:57.360\n So there are certain kinds of research\n\n1:25:57.360 --> 1:26:01.640\n I almost feel like we need to do responsibly and so on.\n\n1:26:01.640 --> 1:26:05.160\n But it is, Carlos, we have the particle accelerator here,\n\n1:26:05.160 --> 1:26:06.280\n so to speak, in physics.\n\n1:26:06.280 --> 1:26:07.480\n So we need to use it.\n\n1:26:07.480 --> 1:26:09.240\n We need to answer the questions that we\n\n1:26:09.240 --> 1:26:12.320\n should be answering right now for the scientific progress.\n\n1:26:12.320 --> 1:26:15.200\n But then at the same time, I look at many advances,\n\n1:26:15.200 --> 1:26:19.280\n including attention, which was discovered in Montreal\n\n1:26:19.280 --> 1:26:22.440\n initially because of lack of compute, right?\n\n1:26:22.440 --> 1:26:24.920\n So we were working on sequence to sequence\n\n1:26:24.920 --> 1:26:27.840\n with my friends over at Google Brain at the time.\n\n1:26:27.840 --> 1:26:30.400\n And we were using, I think, eight GPUs,\n\n1:26:30.400 --> 1:26:32.360\n which was somehow a lot at the time.\n\n1:26:32.360 --> 1:26:36.080\n And then I think Montreal was a bit more limited in the scale.\n\n1:26:36.080 --> 1:26:38.800\n But then they discovered this content based attention\n\n1:26:38.800 --> 1:26:42.240\n concept that then has obviously triggered things\n\n1:26:42.240 --> 1:26:43.320\n like Transformer.\n\n1:26:43.320 --> 1:26:46.280\n Not everything obviously starts Transformer.\n\n1:26:46.280 --> 1:26:49.920\n There's always a history that is important to recognize\n\n1:26:49.920 --> 1:26:53.680\n because then you can make sure that then those who might feel\n\n1:26:53.680 --> 1:26:56.320\n now, well, we don't have so much compute,\n\n1:26:56.320 --> 1:27:00.320\n you need to then help them optimize\n\n1:27:00.320 --> 1:27:02.320\n that kind of research that might actually\n\n1:27:02.320 --> 1:27:04.240\n produce amazing change.\n\n1:27:04.240 --> 1:27:07.960\n Perhaps it's not as short term as some of these advancements\n\n1:27:07.960 --> 1:27:09.720\n or perhaps it's a different time scale.\n\n1:27:09.720 --> 1:27:13.040\n But the people and the diversity of the field\n\n1:27:13.040 --> 1:27:15.680\n is quite critical that we maintain it.\n\n1:27:15.680 --> 1:27:19.800\n And at times, especially mixed a bit with hype or other things,\n\n1:27:19.800 --> 1:27:23.600\n it's a bit tricky to be observing maybe\n\n1:27:23.600 --> 1:27:27.760\n too much of the same thinking across the board.\n\n1:27:27.760 --> 1:27:30.520\n But the humans definitely are critical.\n\n1:27:30.520 --> 1:27:33.960\n And I can think of quite a few personal examples\n\n1:27:33.960 --> 1:27:36.640\n where also someone told me something\n\n1:27:36.640 --> 1:27:40.320\n that had a huge effect onto some idea.\n\n1:27:40.320 --> 1:27:43.360\n And then that's why I'm saying at least in terms of years,\n\n1:27:43.360 --> 1:27:44.920\n probably some things do happen.\n\n1:27:44.920 --> 1:27:46.040\n Yeah, it's fascinating.\n\n1:27:46.040 --> 1:27:48.240\n And it's also fascinating how constraints somehow\n\n1:27:48.240 --> 1:27:51.040\n are essential for innovation.\n\n1:27:51.040 --> 1:27:53.440\n And the other thing you mentioned about engineering,\n\n1:27:53.440 --> 1:27:54.960\n I have a sneaking suspicion.\n\n1:27:54.960 --> 1:28:00.040\n Maybe I over, my love is with engineering.\n\n1:28:00.040 --> 1:28:04.480\n So I have a sneaky suspicion that all the genius,\n\n1:28:04.480 --> 1:28:06.600\n a large percentage of the genius is\n\n1:28:06.600 --> 1:28:09.320\n in the tiny details of engineering.\n\n1:28:09.320 --> 1:28:14.000\n So I think we like to think our genius,\n\n1:28:14.000 --> 1:28:17.600\n the genius is in the big ideas.\n\n1:28:17.600 --> 1:28:20.600\n I have a sneaking suspicion that because I've\n\n1:28:20.600 --> 1:28:24.440\n seen the genius of details, of engineering details,\n\n1:28:24.440 --> 1:28:28.840\n make the night and day difference.\n\n1:28:28.840 --> 1:28:32.960\n And I wonder if those kind of have a ripple effect over time.\n\n1:28:32.960 --> 1:28:36.360\n So that too, so that's sort of taking the engineering\n\n1:28:36.360 --> 1:28:39.400\n perspective that sometimes that quiet innovation\n\n1:28:39.400 --> 1:28:41.800\n at the level of an individual engineer\n\n1:28:41.800 --> 1:28:44.680\n or maybe at the small scale of a few engineers\n\n1:28:44.680 --> 1:28:46.840\n can make all the difference.\n\n1:28:46.840 --> 1:28:50.200\n Because we're working on computers that\n\n1:28:50.200 --> 1:28:55.080\n are scaled across large groups, that one engineering decision\n\n1:28:55.080 --> 1:28:57.320\n can lead to ripple effects.\n\n1:28:57.320 --> 1:28:59.000\n It's interesting to think about.\n\n1:28:59.000 --> 1:29:01.160\n Yeah, I mean, engineering, there's\n\n1:29:01.160 --> 1:29:06.360\n also kind of a historical, it might be a bit random.\n\n1:29:06.360 --> 1:29:10.240\n Because if you think of the history of how especially\n\n1:29:10.240 --> 1:29:12.360\n deep learning and neural networks took off,\n\n1:29:12.360 --> 1:29:16.600\n feels like a bit random because GPUs happened\n\n1:29:16.600 --> 1:29:19.120\n to be there at the right time for a different purpose, which\n\n1:29:19.120 --> 1:29:20.640\n was to play video games.\n\n1:29:20.640 --> 1:29:24.920\n So even the engineering that goes into the hardware\n\n1:29:24.920 --> 1:29:27.160\n and it might have a time, the time frame\n\n1:29:27.160 --> 1:29:28.160\n might be very different.\n\n1:29:28.160 --> 1:29:31.640\n I mean, the GPUs were evolved throughout many years\n\n1:29:31.640 --> 1:29:33.920\n where we didn't even were looking at that.\n\n1:29:33.920 --> 1:29:38.680\n So even at that level, that revolution, so to speak,\n\n1:29:38.680 --> 1:29:42.200\n the ripples are like, we'll see when they stop.\n\n1:29:42.200 --> 1:29:46.960\n But in terms of thinking of why is this happening,\n\n1:29:46.960 --> 1:29:49.760\n I think that when I try to categorize it\n\n1:29:49.760 --> 1:29:52.720\n in sort of things that might not be so obvious,\n\n1:29:52.720 --> 1:29:54.920\n I mean, clearly there's a hardware revolution.\n\n1:29:54.920 --> 1:29:58.360\n We are surfing thanks to that.\n\n1:29:58.360 --> 1:29:59.720\n Data centers as well.\n\n1:29:59.720 --> 1:30:02.680\n I mean, data centers are like, I mean, at Google,\n\n1:30:02.680 --> 1:30:04.840\n for instance, obviously they're serving Google.\n\n1:30:04.840 --> 1:30:06.920\n But there's also now thanks to that\n\n1:30:06.920 --> 1:30:09.680\n and to have built such amazing data centers,\n\n1:30:09.680 --> 1:30:11.720\n we can train these models.\n\n1:30:11.720 --> 1:30:13.400\n Software is an important one.\n\n1:30:13.400 --> 1:30:16.640\n I think if I look at the state of how\n\n1:30:16.640 --> 1:30:20.040\n I had to implement things to implement my ideas,\n\n1:30:20.040 --> 1:30:22.080\n how I discarded ideas because they were too hard\n\n1:30:22.080 --> 1:30:23.120\n to implement.\n\n1:30:23.120 --> 1:30:25.280\n Yeah, clearly the times have changed.\n\n1:30:25.280 --> 1:30:28.440\n And thankfully, we are in a much better software position\n\n1:30:28.440 --> 1:30:29.400\n as well.\n\n1:30:29.400 --> 1:30:31.680\n And then, I mean, obviously there's\n\n1:30:31.680 --> 1:30:34.360\n research that happens at scale and more people\n\n1:30:34.360 --> 1:30:35.120\n enter the field.\n\n1:30:35.120 --> 1:30:35.920\n That's great to see.\n\n1:30:35.920 --> 1:30:38.200\n But it's almost enabled by these other things.\n\n1:30:38.200 --> 1:30:40.560\n And last but not least is also data, right?\n\n1:30:40.560 --> 1:30:43.120\n Curating data sets, labeling data sets,\n\n1:30:43.120 --> 1:30:44.920\n these benchmarks we think about.\n\n1:30:44.920 --> 1:30:48.880\n Maybe we'll want to have all the benchmarks in one system.\n\n1:30:48.880 --> 1:30:51.240\n But it's still very valuable that someone\n\n1:30:51.240 --> 1:30:53.600\n put the thought and the time and the vision\n\n1:30:53.600 --> 1:30:54.880\n to build certain benchmarks.\n\n1:30:54.880 --> 1:30:56.640\n We've seen progress thanks to.\n\n1:30:56.640 --> 1:30:59.280\n But we're going to repurpose the benchmarks.\n\n1:30:59.280 --> 1:31:04.160\n That's the beauty of Atari is like we solved it in a way.\n\n1:31:04.160 --> 1:31:06.000\n But we use it in Gato.\n\n1:31:06.000 --> 1:31:06.840\n It was critical.\n\n1:31:06.840 --> 1:31:09.080\n And I'm sure there's still a lot more\n\n1:31:09.080 --> 1:31:10.960\n to do thanks to that amazing benchmark\n\n1:31:10.960 --> 1:31:13.160\n that someone took the time to put,\n\n1:31:13.160 --> 1:31:15.560\n even though at the time maybe, oh, you\n\n1:31:15.560 --> 1:31:19.480\n have to think what's the next iteration of architectures.\n\n1:31:19.480 --> 1:31:21.440\n That's what maybe the field recognizes.\n\n1:31:21.440 --> 1:31:24.040\n But that's another thing we need to balance\n\n1:31:24.040 --> 1:31:25.760\n in terms of humans behind.\n\n1:31:25.760 --> 1:31:27.960\n We need to recognize all these aspects\n\n1:31:27.960 --> 1:31:29.440\n because they're all critical.\n\n1:31:29.440 --> 1:31:33.600\n And we tend to think of the genius, the scientist,\n\n1:31:33.600 --> 1:31:34.080\n and so on.\n\n1:31:34.080 --> 1:31:38.000\n But I'm glad I know you have a strong engineering background.\n\n1:31:38.000 --> 1:31:40.040\n But also, I'm a lover of data.\n\n1:31:40.040 --> 1:31:43.200\n And the pushback on the engineering comment\n\n1:31:43.200 --> 1:31:46.120\n ultimately could be the creators of benchmarks\n\n1:31:46.120 --> 1:31:47.400\n who have the most impact.\n\n1:31:47.400 --> 1:31:49.160\n Andrej Karpathy, who you mentioned,\n\n1:31:49.160 --> 1:31:52.240\n has recently been talking a lot of trash about ImageNet, which\n\n1:31:52.240 --> 1:31:54.960\n he has the right to do because of how critical he is about\n\n1:31:54.960 --> 1:31:57.760\n ImageNet, how essential he is to the development\n\n1:31:57.760 --> 1:32:01.480\n and the success of deep learning around ImageNet.\n\n1:32:01.480 --> 1:32:02.960\n And he's saying that that's actually\n\n1:32:02.960 --> 1:32:05.520\n that benchmark is holding back the field.\n\n1:32:05.520 --> 1:32:09.000\n Because I mean, especially in his context on Tesla Autopilot,\n\n1:32:09.000 --> 1:32:11.680\n that's looking at real world behavior of a system.\n\n1:32:14.280 --> 1:32:16.280\n There's something fundamentally missing\n\n1:32:16.280 --> 1:32:17.920\n about ImageNet that doesn't capture\n\n1:32:17.920 --> 1:32:20.400\n the real worldness of things.\n\n1:32:20.400 --> 1:32:23.560\n That we need to have data sets, benchmarks that\n\n1:32:23.560 --> 1:32:27.600\n have the unpredictability, the edge cases, whatever\n\n1:32:27.600 --> 1:32:30.760\n the heck it is that makes the real world so\n\n1:32:30.760 --> 1:32:32.280\n difficult to operate in.\n\n1:32:32.280 --> 1:32:34.640\n We need to have benchmarks of that.\n\n1:32:34.640 --> 1:32:37.760\n But just to think about the impact of ImageNet\n\n1:32:37.760 --> 1:32:42.120\n as a benchmark, and that really puts a lot of emphasis\n\n1:32:42.120 --> 1:32:43.720\n on the importance of a benchmark,\n\n1:32:43.720 --> 1:32:46.640\n both sort of internally a deep mind and as a community.\n\n1:32:46.640 --> 1:32:50.120\n So one is coming in from within, like,\n\n1:32:50.120 --> 1:32:55.280\n how do I create a benchmark for me to mark and make progress?\n\n1:32:55.280 --> 1:32:58.120\n And how do I make benchmark for the community\n\n1:32:58.120 --> 1:33:02.520\n to mark and push progress?\n\n1:33:02.520 --> 1:33:05.840\n You have this amazing paper you coauthored,\n\n1:33:05.840 --> 1:33:08.600\n a survey paper called Emergent Abilities\n\n1:33:08.600 --> 1:33:10.480\n of Large Language Models.\n\n1:33:10.480 --> 1:33:12.520\n It has, again, the philosophy here\n\n1:33:12.520 --> 1:33:14.520\n that I'd love to ask you about.\n\n1:33:14.520 --> 1:33:17.320\n What's the intuition about the phenomena of emergence\n\n1:33:17.320 --> 1:33:20.600\n in neural networks transformed as language models?\n\n1:33:20.600 --> 1:33:24.160\n Is there a magic threshold beyond which\n\n1:33:24.160 --> 1:33:27.080\n we start to see certain performance?\n\n1:33:27.080 --> 1:33:29.880\n And is that different from task to task?\n\n1:33:29.880 --> 1:33:32.640\n Is that us humans just being poetic and romantic?\n\n1:33:32.640 --> 1:33:36.160\n Or is there literally some level at which we start\n\n1:33:36.160 --> 1:33:38.120\n to see breakthrough performance?\n\n1:33:38.120 --> 1:33:43.520\n Yeah, I mean, this is a property that we start seeing in systems\n\n1:33:43.520 --> 1:33:48.160\n that actually tend to be so in machine learning,\n\n1:33:48.160 --> 1:33:51.680\n traditionally, again, going to benchmarks.\n\n1:33:51.680 --> 1:33:54.840\n I mean, if you have some input, output, right,\n\n1:33:54.840 --> 1:33:58.200\n like that is just a single input and a single output,\n\n1:33:58.200 --> 1:34:01.200\n you generally, when you train these systems,\n\n1:34:01.200 --> 1:34:04.760\n you see reasonably smooth curves when\n\n1:34:04.760 --> 1:34:10.040\n you analyze how much the data set size affects\n\n1:34:10.040 --> 1:34:12.280\n the performance, or how the model size affects\n\n1:34:12.280 --> 1:34:18.200\n the performance, or how long you train the system for affects\n\n1:34:18.200 --> 1:34:19.280\n the performance, right?\n\n1:34:19.280 --> 1:34:23.080\n So if we think of ImageNet, the training curves\n\n1:34:23.080 --> 1:34:28.080\n look fairly smooth and predictable in a way.\n\n1:34:28.080 --> 1:34:31.520\n And I would say that's probably because it's\n\n1:34:31.520 --> 1:34:36.520\n kind of a one hop reasoning task, right?\n\n1:34:36.520 --> 1:34:39.160\n It's like, here is an input, and you\n\n1:34:39.160 --> 1:34:42.760\n think for a few milliseconds or 100 milliseconds, 300\n\n1:34:42.760 --> 1:34:44.560\n as a human, and then you tell me,\n\n1:34:44.560 --> 1:34:47.840\n yeah, there's an alpaca in this image.\n\n1:34:47.840 --> 1:34:55.560\n So in language, we are seeing benchmarks that require more\n\n1:34:55.560 --> 1:34:58.200\n pondering and more thought in a way, right?\n\n1:34:58.200 --> 1:35:02.440\n This is just kind of you need to look for some subtleties.\n\n1:35:02.440 --> 1:35:05.840\n It involves inputs that you might think of,\n\n1:35:05.840 --> 1:35:08.360\n even if the input is a sentence describing\n\n1:35:08.360 --> 1:35:13.080\n a mathematical problem, there is a bit more processing\n\n1:35:13.080 --> 1:35:15.640\n required as a human and more introspection.\n\n1:35:15.640 --> 1:35:20.440\n So I think how these benchmarks work\n\n1:35:20.440 --> 1:35:24.720\n means that there is actually a threshold.\n\n1:35:24.720 --> 1:35:26.480\n Just going back to how transformers\n\n1:35:26.480 --> 1:35:29.520\n work in this way of querying for the right questions\n\n1:35:29.520 --> 1:35:31.760\n to get the right answers, that might\n\n1:35:31.760 --> 1:35:35.400\n mean that performance becomes random\n\n1:35:35.400 --> 1:35:37.720\n until the right question is asked\n\n1:35:37.720 --> 1:35:40.920\n by the querying system of a transformer or of a language\n\n1:35:40.920 --> 1:35:42.760\n model like a transformer.\n\n1:35:42.760 --> 1:35:46.240\n And then only then you might start\n\n1:35:46.240 --> 1:35:50.000\n seeing performance going from random to nonrandom.\n\n1:35:50.000 --> 1:35:53.080\n And this is more empirical.\n\n1:35:53.080 --> 1:35:56.320\n There's no formalism or theory behind this yet,\n\n1:35:56.320 --> 1:35:57.760\n although it might be quite important.\n\n1:35:57.760 --> 1:36:00.320\n But we are seeing these phase transitions\n\n1:36:00.320 --> 1:36:03.200\n of random performance until some,\n\n1:36:03.200 --> 1:36:04.880\n let's say, scale of a model.\n\n1:36:04.880 --> 1:36:06.680\n And then it goes beyond that.\n\n1:36:06.680 --> 1:36:10.440\n And it might be that you need to fit\n\n1:36:10.440 --> 1:36:16.040\n a few low order bits of thought before you can make progress\n\n1:36:16.040 --> 1:36:17.200\n on the whole task.\n\n1:36:17.200 --> 1:36:19.720\n And if you could measure, actually,\n\n1:36:19.720 --> 1:36:22.240\n those breakdown of the task, maybe you\n\n1:36:22.240 --> 1:36:25.320\n would see more smooth, like, yeah,\n\n1:36:25.320 --> 1:36:27.760\n once you get these and these and these and these and these,\n\n1:36:27.760 --> 1:36:30.240\n then you start making progress in the task.\n\n1:36:30.240 --> 1:36:35.240\n But it's somehow a bit annoying because then it\n\n1:36:35.240 --> 1:36:40.240\n means that certain questions we might ask about architectures\n\n1:36:40.240 --> 1:36:42.960\n possibly can only be done at a certain scale.\n\n1:36:42.960 --> 1:36:46.320\n And one thing that, conversely, I've\n\n1:36:46.320 --> 1:36:49.200\n seen great progress on in the last couple of years\n\n1:36:49.200 --> 1:36:53.120\n is this notion of science of deep learning and science\n\n1:36:53.120 --> 1:36:55.000\n of scale in particular.\n\n1:36:55.000 --> 1:36:57.520\n So on the negative is that there are\n\n1:36:57.520 --> 1:37:01.000\n some benchmarks for which progress might\n\n1:37:01.000 --> 1:37:04.000\n need to be measured at minimum at a certain scale\n\n1:37:04.000 --> 1:37:07.040\n until you see then what details of the model\n\n1:37:07.040 --> 1:37:09.960\n matter to make that performance better.\n\n1:37:09.960 --> 1:37:11.880\n So that's a bit of a con.\n\n1:37:11.880 --> 1:37:17.960\n But what we've also seen is that you can empirically\n\n1:37:17.960 --> 1:37:22.880\n analyze behavior of models at scales that are smaller.\n\n1:37:22.880 --> 1:37:25.920\n So let's say, to put an example, we\n\n1:37:25.920 --> 1:37:30.080\n had this Chinchilla paper that revised the so called scaling\n\n1:37:30.080 --> 1:37:31.320\n laws of models.\n\n1:37:31.320 --> 1:37:35.000\n And that whole study is done at a reasonably small scale,\n\n1:37:35.000 --> 1:37:38.600\n that may be hundreds of millions up to 1 billion parameters.\n\n1:37:38.600 --> 1:37:41.880\n And then the cool thing is that you create some loss,\n\n1:37:41.880 --> 1:37:45.840\n some loss that some trends, you extract trends from data\n\n1:37:45.840 --> 1:37:49.400\n that you see, OK, it looks like the amount of data required\n\n1:37:49.400 --> 1:37:52.080\n to train now a 10x larger model would be this.\n\n1:37:52.080 --> 1:37:55.200\n And these laws so far, these extrapolations\n\n1:37:55.200 --> 1:37:59.880\n have helped us save compute and just get to a better place\n\n1:37:59.880 --> 1:38:02.520\n in terms of the science of how should we\n\n1:38:02.520 --> 1:38:05.640\n run these models at scale, how much data, how much depth,\n\n1:38:05.640 --> 1:38:07.360\n and all sorts of questions we start\n\n1:38:07.360 --> 1:38:10.560\n asking extrapolating from a small scale.\n\n1:38:10.560 --> 1:38:13.720\n But then these emergence is sadly that not everything\n\n1:38:13.720 --> 1:38:16.920\n can be extrapolated from scale depending on the benchmark.\n\n1:38:16.920 --> 1:38:19.840\n And maybe the harder benchmarks are not\n\n1:38:19.840 --> 1:38:21.920\n so good for extracting these laws.\n\n1:38:21.920 --> 1:38:24.160\n But we have a variety of benchmarks at least.\n\n1:38:24.160 --> 1:38:29.240\n So I wonder to which degree the threshold, the phase shift\n\n1:38:29.240 --> 1:38:32.440\n scale is a function of the benchmark.\n\n1:38:32.440 --> 1:38:35.120\n So some of the science of scale might\n\n1:38:35.120 --> 1:38:40.400\n be engineering benchmarks where that threshold is low,\n\n1:38:40.400 --> 1:38:46.160\n sort of taking a main benchmark and reducing it somehow\n\n1:38:46.160 --> 1:38:48.480\n where the essential difficulty is left\n\n1:38:48.480 --> 1:38:51.880\n but the scale of which the emergence happens\n\n1:38:51.880 --> 1:38:54.320\n is lower just for the science aspect of it\n\n1:38:54.320 --> 1:38:56.960\n versus the actual real world aspect.\n\n1:38:56.960 --> 1:38:59.920\n Yeah, so luckily we have quite a few benchmarks, some of which\n\n1:38:59.920 --> 1:39:02.640\n are simpler or maybe they're more like I think people might\n\n1:39:02.640 --> 1:39:05.920\n call these systems one versus systems two style.\n\n1:39:05.920 --> 1:39:09.920\n So I think what we're not seeing luckily\n\n1:39:09.920 --> 1:39:14.080\n is that extrapolations from maybe slightly more smooth\n\n1:39:14.080 --> 1:39:18.560\n or simpler benchmarks are translating to the harder ones.\n\n1:39:18.560 --> 1:39:21.480\n But that is not to say that this extrapolation will\n\n1:39:21.480 --> 1:39:22.560\n hit its limits.\n\n1:39:22.560 --> 1:39:27.560\n And when it does, then how much we scale or how we scale\n\n1:39:27.560 --> 1:39:31.760\n will sadly be a bit suboptimal until we find better laws.\n\n1:39:31.760 --> 1:39:33.840\n And these laws, again, are very empirical laws.\n\n1:39:33.840 --> 1:39:35.960\n They're not like physical laws of models,\n\n1:39:35.960 --> 1:39:39.520\n although I wish there would be better theory about these\n\n1:39:39.520 --> 1:39:40.240\n things as well.\n\n1:39:40.240 --> 1:39:43.040\n But so far, I would say empirical theory,\n\n1:39:43.040 --> 1:39:46.000\n as I call it, is way ahead than actual theory\n\n1:39:46.000 --> 1:39:47.800\n of machine learning.\n\n1:39:47.800 --> 1:39:50.560\n Let me ask you almost for fun.\n\n1:39:50.560 --> 1:39:55.840\n So this is not, Oriol, as a deep mind person or anything\n\n1:39:55.840 --> 1:39:59.080\n to do with deep mind or Google, just as a human being,\n\n1:39:59.080 --> 1:40:04.320\n looking at these news of a Google engineer who claimed\n\n1:40:04.320 --> 1:40:11.120\n that, I guess, the lambda language model was sentient.\n\n1:40:11.120 --> 1:40:14.080\n And you still need to look into the details of this.\n\n1:40:14.080 --> 1:40:19.440\n But making an official report and the claim\n\n1:40:19.440 --> 1:40:23.880\n that he believes there's evidence that this system has\n\n1:40:23.880 --> 1:40:25.160\n achieved sentience.\n\n1:40:25.160 --> 1:40:29.480\n And I think this is a really interesting case\n\n1:40:29.480 --> 1:40:31.720\n on a human level, on a psychological level,\n\n1:40:31.720 --> 1:40:37.240\n on a technical machine learning level of how language models\n\n1:40:37.240 --> 1:40:39.840\n transform our world, and also just philosophical level\n\n1:40:39.840 --> 1:40:44.200\n of the role of AI systems in a human world.\n\n1:40:44.200 --> 1:40:48.080\n So what do you find interesting?\n\n1:40:48.080 --> 1:40:51.080\n What's your take on all of this as a machine learning\n\n1:40:51.080 --> 1:40:54.240\n engineer and a researcher and also as a human being?\n\n1:40:54.240 --> 1:40:57.440\n Yeah, I mean, a few reactions.\n\n1:40:57.440 --> 1:40:58.680\n Quite a few, actually.\n\n1:40:58.680 --> 1:41:02.560\n Have you ever briefly thought, is this thing sentient?\n\n1:41:02.560 --> 1:41:04.800\n Right, so never, absolutely never.\n\n1:41:04.800 --> 1:41:06.240\n Like even with Alpha Star?\n\n1:41:06.240 --> 1:41:08.080\n Wait a minute.\n\n1:41:08.080 --> 1:41:11.840\n Sadly, though, I think, yeah, sadly, I have not.\n\n1:41:11.840 --> 1:41:15.280\n Yeah, I think the current, any of the current models,\n\n1:41:15.280 --> 1:41:18.880\n although very useful and very good,\n\n1:41:18.880 --> 1:41:22.320\n yeah, I think we're quite far from that.\n\n1:41:22.320 --> 1:41:25.320\n And there's kind of a converse side story.\n\n1:41:25.320 --> 1:41:30.320\n So one of my passions is about science in general.\n\n1:41:30.320 --> 1:41:34.440\n And I think I feel I'm a bit of a failed scientist.\n\n1:41:34.440 --> 1:41:36.520\n That's why I came to machine learning,\n\n1:41:36.520 --> 1:41:40.080\n because you always feel, and you start seeing this,\n\n1:41:40.080 --> 1:41:43.320\n that machine learning is maybe the science that\n\n1:41:43.320 --> 1:41:46.400\n can help other sciences, as we've seen.\n\n1:41:46.400 --> 1:41:48.640\n It's such a powerful tool.\n\n1:41:48.640 --> 1:41:52.480\n So thanks to that angle, that, OK, I love science.\n\n1:41:52.480 --> 1:41:53.880\n I love, I mean, I love astronomy.\n\n1:41:53.880 --> 1:41:54.880\n I love biology.\n\n1:41:54.880 --> 1:41:56.000\n But I'm not an expert.\n\n1:41:56.000 --> 1:41:58.600\n And I decided, well, the thing I can do better\n\n1:41:58.600 --> 1:41:59.960\n at is computers.\n\n1:41:59.960 --> 1:42:04.720\n But having, especially with when I was a bit more involved\n\n1:42:04.720 --> 1:42:07.400\n in AlphaFold, learning a bit about proteins\n\n1:42:07.400 --> 1:42:11.440\n and about biology and about life,\n\n1:42:11.440 --> 1:42:14.840\n the complexity, it feels like it really is.\n\n1:42:14.840 --> 1:42:19.200\n I mean, if you start looking at the things that are going on\n\n1:42:19.200 --> 1:42:26.360\n at the atomic level, and also, I mean, there's obviously the,\n\n1:42:26.360 --> 1:42:29.280\n we are maybe inclined to try to think of neural networks\n\n1:42:29.280 --> 1:42:30.400\n as like the brain.\n\n1:42:30.400 --> 1:42:33.760\n But the complexities and the amount of magic\n\n1:42:33.760 --> 1:42:37.080\n that it feels when, I mean, I'm not an expert,\n\n1:42:37.080 --> 1:42:38.560\n so it naturally feels more magic.\n\n1:42:38.560 --> 1:42:40.800\n But looking at biological systems,\n\n1:42:40.800 --> 1:42:46.640\n as opposed to these computational brains,\n\n1:42:46.640 --> 1:42:50.320\n just makes me like, wow, there's such a level of complexity\n\n1:42:50.320 --> 1:42:54.840\n difference still, like orders of magnitude complexity that,\n\n1:42:54.840 --> 1:42:56.640\n sure, these weights, I mean, we train them\n\n1:42:56.640 --> 1:42:58.040\n and they do nice things.\n\n1:42:58.040 --> 1:43:04.320\n But they're not at the level of biological entities, brains,\n\n1:43:04.320 --> 1:43:06.000\n cells.\n\n1:43:06.000 --> 1:43:09.680\n It just feels like it's just not possible to achieve\n\n1:43:09.680 --> 1:43:12.400\n the same level of complexity behavior.\n\n1:43:12.400 --> 1:43:16.240\n And my belief, when I talk to other beings,\n\n1:43:16.240 --> 1:43:20.360\n is certainly shaped by this amazement of biology\n\n1:43:20.360 --> 1:43:22.360\n that, maybe because I know too much,\n\n1:43:22.360 --> 1:43:23.800\n I don't have about machine learning,\n\n1:43:23.800 --> 1:43:28.120\n but I certainly feel it's very far fetched and far\n\n1:43:28.120 --> 1:43:31.720\n in the future to be calling or to be thinking,\n\n1:43:31.720 --> 1:43:35.640\n well, this mathematical function that is differentiable\n\n1:43:35.640 --> 1:43:39.200\n is, in fact, sentient and so on.\n\n1:43:39.200 --> 1:43:42.000\n There's something on that point that is very interesting.\n\n1:43:42.000 --> 1:43:46.120\n So you know enough about machines and enough\n\n1:43:46.120 --> 1:43:47.760\n about biology to know that there's\n\n1:43:47.760 --> 1:43:51.880\n many orders of magnitude of difference and complexity.\n\n1:43:51.880 --> 1:43:56.080\n But you know how machine learning works.\n\n1:43:56.080 --> 1:43:58.160\n So the interesting question for human beings\n\n1:43:58.160 --> 1:44:00.080\n that are interacting with a system that don't know\n\n1:44:00.080 --> 1:44:02.280\n about the underlying complexity.\n\n1:44:02.280 --> 1:44:05.240\n And I've seen people, probably including myself,\n\n1:44:05.240 --> 1:44:08.400\n that have fallen in love with things that are quite simple.\n\n1:44:08.400 --> 1:44:11.520\n And so maybe the complexity is one part of the picture,\n\n1:44:11.520 --> 1:44:18.840\n but maybe that's not a necessary condition for sentience,\n\n1:44:18.840 --> 1:44:24.760\n for perception or emulation of sentience.\n\n1:44:24.760 --> 1:44:25.280\n Right.\n\n1:44:25.280 --> 1:44:27.560\n So I mean, I guess the other side of this\n\n1:44:27.560 --> 1:44:29.560\n is that's how I feel personally.\n\n1:44:29.560 --> 1:44:32.360\n I mean, you asked me about the person, right?\n\n1:44:32.360 --> 1:44:35.560\n Now, it's very interesting to see how other humans feel\n\n1:44:35.560 --> 1:44:37.080\n about things, right?\n\n1:44:37.080 --> 1:44:41.640\n We are, again, I'm not as amazed about things\n\n1:44:41.640 --> 1:44:44.560\n that I feel this is not as magical as this other thing\n\n1:44:44.560 --> 1:44:48.040\n because of maybe how I got to learn about it\n\n1:44:48.040 --> 1:44:50.480\n and how I see the curve a bit more smooth\n\n1:44:50.480 --> 1:44:54.000\n because I've just seen the progress of language models\n\n1:44:54.000 --> 1:44:56.000\n since Shannon in the 50s.\n\n1:44:56.000 --> 1:44:58.920\n And actually looking at that time scale,\n\n1:44:58.920 --> 1:45:00.880\n we're not that fast progress, right?\n\n1:45:00.880 --> 1:45:06.040\n I mean, what we were thinking at the time almost 100 years ago\n\n1:45:06.040 --> 1:45:08.880\n is not that dissimilar to what we're doing now.\n\n1:45:08.880 --> 1:45:11.440\n But at the same time, yeah, obviously others,\n\n1:45:11.440 --> 1:45:14.440\n my experience, the personal experience,\n\n1:45:14.440 --> 1:45:20.680\n I think no one should tell others how they should feel.\n\n1:45:20.680 --> 1:45:22.920\n I mean, the feelings are very personal, right?\n\n1:45:22.920 --> 1:45:26.080\n So how others might feel about the models and so on.\n\n1:45:26.080 --> 1:45:27.840\n That's one part of the story that\n\n1:45:27.840 --> 1:45:31.960\n is important to understand for me personally as a researcher.\n\n1:45:31.960 --> 1:45:35.200\n And then when I maybe disagree or I\n\n1:45:35.200 --> 1:45:38.200\n don't understand or see that, yeah, maybe this is not\n\n1:45:38.200 --> 1:45:39.920\n something I think right now is reasonable,\n\n1:45:39.920 --> 1:45:42.840\n knowing all that I know, one of the other things\n\n1:45:42.840 --> 1:45:46.480\n and perhaps partly why it's great to be talking to you\n\n1:45:46.480 --> 1:45:49.200\n and reaching out to the world about machine learning\n\n1:45:49.200 --> 1:45:53.440\n is, hey, let's demystify a bit the magic\n\n1:45:53.440 --> 1:45:56.200\n and try to see a bit more of the math\n\n1:45:56.200 --> 1:45:59.800\n and the fact that literally to create these models,\n\n1:45:59.800 --> 1:46:03.520\n if we had the right software, it would be 10 lines of code\n\n1:46:03.520 --> 1:46:06.760\n and then just a dump of the internet.\n\n1:46:06.760 --> 1:46:11.600\n Versus then the complexity of the creation of humans\n\n1:46:11.600 --> 1:46:13.520\n from their inception, right?\n\n1:46:13.520 --> 1:46:17.600\n And also the complexity of evolution of the whole universe\n\n1:46:17.600 --> 1:46:21.040\n to where we are that feels orders of magnitude\n\n1:46:21.040 --> 1:46:23.400\n more complex and fascinating to me.\n\n1:46:23.400 --> 1:46:26.520\n So I think, yeah, maybe part of the only thing\n\n1:46:26.520 --> 1:46:30.240\n I'm thinking about trying to tell you is, yeah, I think\n\n1:46:30.240 --> 1:46:32.560\n explaining a bit of the magic.\n\n1:46:32.560 --> 1:46:33.600\n There is a bit of magic.\n\n1:46:33.600 --> 1:46:35.240\n It's good to be in love, obviously,\n\n1:46:35.240 --> 1:46:36.920\n with what you do at work.\n\n1:46:36.920 --> 1:46:41.320\n And I'm certainly fascinated and surprised quite often as well.\n\n1:46:41.320 --> 1:46:45.040\n But I think, hopefully, as experts in biology,\n\n1:46:45.040 --> 1:46:47.080\n hopefully will tell me this is not as magic.\n\n1:46:47.080 --> 1:46:50.840\n And I'm happy to learn that through interactions\n\n1:46:50.840 --> 1:46:54.000\n with the larger community, we can also\n\n1:46:54.000 --> 1:46:56.000\n have a certain level of education\n\n1:46:56.000 --> 1:46:58.920\n that in practice also will matter because, I mean,\n\n1:46:58.920 --> 1:47:00.800\n one question is how you feel about this.\n\n1:47:00.800 --> 1:47:03.000\n But then the other very important is\n\n1:47:03.000 --> 1:47:06.960\n you starting to interact with these in products and so on.\n\n1:47:06.960 --> 1:47:09.240\n It's good to understand a bit what's going on,\n\n1:47:09.240 --> 1:47:12.280\n what's not going on, what's safe, what's not safe,\n\n1:47:12.280 --> 1:47:13.000\n and so on, right?\n\n1:47:13.000 --> 1:47:15.280\n Otherwise, the technology will not\n\n1:47:15.280 --> 1:47:18.120\n be used properly for good, which is obviously\n\n1:47:18.120 --> 1:47:20.480\n the goal of all of us, I hope.\n\n1:47:20.480 --> 1:47:22.920\n So let me then ask the next question.\n\n1:47:22.920 --> 1:47:25.760\n Do you think in order to solve intelligence\n\n1:47:25.760 --> 1:47:29.480\n or to replace the leg spot that does interviews\n\n1:47:29.480 --> 1:47:31.480\n as we started this conversation with,\n\n1:47:31.480 --> 1:47:34.880\n do you think the system needs to be sentient?\n\n1:47:34.880 --> 1:47:38.720\n Do you think it needs to achieve something like consciousness?\n\n1:47:38.720 --> 1:47:41.120\n And do you think about what consciousness\n\n1:47:41.120 --> 1:47:45.360\n is in the human mind that could be instructive for creating AI\n\n1:47:45.360 --> 1:47:46.720\n systems?\n\n1:47:46.720 --> 1:47:47.760\n Yeah.\n\n1:47:47.760 --> 1:47:53.480\n Honestly, I think probably not to the degree of intelligence\n\n1:47:53.480 --> 1:47:58.760\n that there's this brain that can learn,\n\n1:47:58.760 --> 1:48:02.960\n can be extremely useful, can challenge you, can teach you.\n\n1:48:02.960 --> 1:48:05.600\n Conversely, you can teach it to do things.\n\n1:48:05.600 --> 1:48:09.080\n I'm not sure it's necessary, personally speaking.\n\n1:48:09.080 --> 1:48:15.680\n But if consciousness or any other biological or evolutionary\n\n1:48:15.680 --> 1:48:20.880\n lesson can be repurposed to then influence\n\n1:48:20.880 --> 1:48:24.360\n our next set of algorithms, that is a great way\n\n1:48:24.360 --> 1:48:25.680\n to actually make progress, right?\n\n1:48:25.680 --> 1:48:28.240\n And the same way I try to explain transformers a bit\n\n1:48:28.240 --> 1:48:33.360\n how it feels we operate when we look at text specifically,\n\n1:48:33.360 --> 1:48:36.000\n these insights are very important, right?\n\n1:48:36.000 --> 1:48:41.240\n So there's a distinction between details of how the brain might\n\n1:48:41.240 --> 1:48:43.200\n be doing computation.\n\n1:48:43.200 --> 1:48:46.560\n I think my understanding is, sure, there's neurons\n\n1:48:46.560 --> 1:48:48.520\n and there's some resemblance to neural networks,\n\n1:48:48.520 --> 1:48:52.200\n but we don't quite understand enough of the brain in detail,\n\n1:48:52.200 --> 1:48:55.240\n right, to be able to replicate it.\n\n1:48:55.240 --> 1:49:01.320\n But then if you zoom out a bit, our thought process,\n\n1:49:01.320 --> 1:49:05.560\n how memory works, maybe even how evolution got us here,\n\n1:49:05.560 --> 1:49:07.280\n what's exploration, exploitation,\n\n1:49:07.280 --> 1:49:09.080\n like how these things happen, I think\n\n1:49:09.080 --> 1:49:12.960\n these clearly can inform algorithmic level research.\n\n1:49:12.960 --> 1:49:17.040\n And I've seen some examples of this\n\n1:49:17.040 --> 1:49:19.720\n being quite useful to then guide the research,\n\n1:49:19.720 --> 1:49:21.640\n even it might be for the wrong reasons, right?\n\n1:49:21.640 --> 1:49:26.080\n So I think biology and what we know about ourselves\n\n1:49:26.080 --> 1:49:30.000\n can help a whole lot to build, essentially,\n\n1:49:30.000 --> 1:49:34.480\n what we call AGI, this general, the real ghetto, right?\n\n1:49:34.480 --> 1:49:36.480\n The last step of the chain, hopefully.\n\n1:49:36.480 --> 1:49:40.760\n But consciousness in particular, I don't myself\n\n1:49:40.760 --> 1:49:44.760\n at least think too hard about how to add that to the system.\n\n1:49:44.760 --> 1:49:47.840\n But maybe my understanding is also very personal\n\n1:49:47.840 --> 1:49:48.840\n about what it means, right?\n\n1:49:48.840 --> 1:49:51.760\n I think even that in itself is a long debate\n\n1:49:51.760 --> 1:49:55.240\n that I know people have often.\n\n1:49:55.240 --> 1:49:57.720\n And maybe I should learn more about this.\n\n1:49:57.720 --> 1:50:01.680\n Yeah, and I personally, I notice the magic often\n\n1:50:01.680 --> 1:50:04.960\n on a personal level, especially with physical systems\n\n1:50:04.960 --> 1:50:06.120\n like robots.\n\n1:50:06.120 --> 1:50:10.440\n I have a lot of legged robots now in Austin\n\n1:50:10.440 --> 1:50:11.680\n that I play with.\n\n1:50:11.680 --> 1:50:13.480\n And even when you program them, when\n\n1:50:13.480 --> 1:50:15.560\n they do things you didn't expect,\n\n1:50:15.560 --> 1:50:18.560\n there's an immediate anthropomorphization.\n\n1:50:18.560 --> 1:50:19.960\n And you notice the magic, and you\n\n1:50:19.960 --> 1:50:22.600\n start to think about things like sentience\n\n1:50:22.600 --> 1:50:26.000\n that has to do more with effective communication\n\n1:50:26.000 --> 1:50:30.160\n and less with any of these kind of dramatic things.\n\n1:50:30.160 --> 1:50:32.840\n It seems like a useful part of communication.\n\n1:50:32.840 --> 1:50:36.560\n Having the perception of consciousness\n\n1:50:36.560 --> 1:50:38.800\n seems like useful for us humans.\n\n1:50:38.800 --> 1:50:40.840\n We treat each other more seriously.\n\n1:50:40.840 --> 1:50:46.000\n We are able to do a nearest neighbor shoving of that entity\n\n1:50:46.000 --> 1:50:48.640\n into your memory correctly, all that kind of stuff.\n\n1:50:48.640 --> 1:50:50.800\n It seems useful, at least to fake it,\n\n1:50:50.800 --> 1:50:52.440\n even if you never make it.\n\n1:50:52.440 --> 1:50:55.560\n So maybe, like, yeah, mirroring the question.\n\n1:50:55.560 --> 1:50:57.440\n And since you talked to a few people,\n\n1:50:57.440 --> 1:50:59.880\n then you do think that we'll need\n\n1:50:59.880 --> 1:51:04.560\n to figure something out in order to achieve intelligence\n\n1:51:04.560 --> 1:51:06.520\n in a grander sense of the word.\n\n1:51:06.520 --> 1:51:09.360\n Yeah, I personally believe yes, but I don't even\n\n1:51:09.360 --> 1:51:14.160\n think it'll be like a separate island we'll have to travel to.\n\n1:51:14.160 --> 1:51:16.400\n I think it will emerge quite naturally.\n\n1:51:16.400 --> 1:51:19.040\n OK, that's easier for us then.\n\n1:51:19.040 --> 1:51:20.080\n Thank you.\n\n1:51:20.080 --> 1:51:22.760\n But the reason I think it's important to think about\n\n1:51:22.760 --> 1:51:25.800\n is you will start, I believe, like with this Google\n\n1:51:25.800 --> 1:51:29.320\n engineer, you will start seeing this a lot more, especially\n\n1:51:29.320 --> 1:51:31.600\n when you have AI systems that are actually interacting\n\n1:51:31.600 --> 1:51:35.120\n with human beings that don't have an engineering background.\n\n1:51:35.120 --> 1:51:38.520\n And we have to prepare for that.\n\n1:51:38.520 --> 1:51:41.160\n Because I do believe there will be a civil rights\n\n1:51:41.160 --> 1:51:44.520\n movement for robots, as silly as it is to say.\n\n1:51:44.520 --> 1:51:46.720\n There's going to be a large number of people\n\n1:51:46.720 --> 1:51:49.760\n that realize there's these intelligent entities with whom\n\n1:51:49.760 --> 1:51:53.160\n I have a deep relationship, and I don't want to lose them.\n\n1:51:53.160 --> 1:51:55.920\n They've come to be a part of my life, and they mean a lot.\n\n1:51:55.920 --> 1:51:57.120\n They have a name.\n\n1:51:57.120 --> 1:51:58.040\n They have a story.\n\n1:51:58.040 --> 1:51:59.120\n They have a memory.\n\n1:51:59.120 --> 1:52:01.240\n And we start to ask questions about ourselves.\n\n1:52:01.240 --> 1:52:07.520\n Well, this thing sure seems like it's capable of suffering,\n\n1:52:07.520 --> 1:52:09.800\n because it tells all these stories of suffering.\n\n1:52:09.800 --> 1:52:11.960\n It doesn't want to die and all those kinds of things.\n\n1:52:11.960 --> 1:52:14.400\n And we have to start to ask ourselves questions.\n\n1:52:14.400 --> 1:52:16.960\n What is the difference between a human being and this thing?\n\n1:52:16.960 --> 1:52:20.120\n And so when you engineer, I believe\n\n1:52:20.120 --> 1:52:23.400\n from an engineering perspective, from a deep mind or anybody\n\n1:52:23.400 --> 1:52:26.440\n that builds systems, there might be laws in the future\n\n1:52:26.440 --> 1:52:29.120\n where you're not allowed to engineer systems\n\n1:52:29.120 --> 1:52:35.120\n with displays of sentience, unless they're explicitly\n\n1:52:35.120 --> 1:52:37.320\n designed to be that, unless it's a pet.\n\n1:52:37.320 --> 1:52:41.160\n So if you have a system that's just doing customer support,\n\n1:52:41.160 --> 1:52:44.160\n you're legally not allowed to display sentience.\n\n1:52:44.160 --> 1:52:47.200\n We'll start to ask ourselves that question.\n\n1:52:47.200 --> 1:52:49.920\n And then so that's going to be part of the software\n\n1:52:49.920 --> 1:52:52.080\n engineering process.\n\n1:52:52.080 --> 1:52:53.320\n Which features do we have?\n\n1:52:53.320 --> 1:52:56.440\n And one of them is communications of the sentience.\n\n1:52:56.440 --> 1:52:58.680\n But it's important to start thinking about that stuff,\n\n1:52:58.680 --> 1:53:01.640\n especially how much it captivates public attention.\n\n1:53:01.640 --> 1:53:03.120\n Yeah, absolutely.\n\n1:53:03.120 --> 1:53:06.360\n It's definitely a topic that is important.\n\n1:53:06.360 --> 1:53:07.880\n We think about.\n\n1:53:07.880 --> 1:53:12.560\n And I think in a way, I always see not every movie\n\n1:53:12.560 --> 1:53:16.080\n is equally on point with certain things.\n\n1:53:16.080 --> 1:53:19.000\n But certainly science fiction in this sense\n\n1:53:19.000 --> 1:53:22.120\n at least has prepared society to start\n\n1:53:22.120 --> 1:53:25.360\n thinking about certain topics that even if it's\n\n1:53:25.360 --> 1:53:29.400\n too early to talk about, as long as we are reasonable,\n\n1:53:29.400 --> 1:53:33.840\n it's certainly going to prepare us for both the research\n\n1:53:33.840 --> 1:53:34.920\n to come and how to.\n\n1:53:34.920 --> 1:53:38.080\n I mean, there's many important challenges and topics\n\n1:53:38.080 --> 1:53:43.200\n that come with building an intelligent system, many of\n\n1:53:43.200 --> 1:53:44.640\n which you just mentioned.\n\n1:53:44.640 --> 1:53:49.880\n So I think we're never going to be fully ready\n\n1:53:49.880 --> 1:53:51.360\n unless we talk about these.\n\n1:53:51.360 --> 1:53:58.840\n And we start also, as I said, just expanding the people\n\n1:53:58.840 --> 1:54:03.240\n we talk to not include only our own researchers and so on.\n\n1:54:03.240 --> 1:54:06.480\n And in fact, places like DeepMind but elsewhere,\n\n1:54:06.480 --> 1:54:10.320\n there's more interdisciplinary groups forming up\n\n1:54:10.320 --> 1:54:12.880\n to start asking and really working\n\n1:54:12.880 --> 1:54:14.880\n with us on these questions.\n\n1:54:14.880 --> 1:54:17.400\n Because obviously, this is not initially\n\n1:54:17.400 --> 1:54:19.360\n what your passion is when you do your PhD,\n\n1:54:19.360 --> 1:54:21.440\n but certainly it is coming.\n\n1:54:21.440 --> 1:54:23.120\n So it's fascinating.\n\n1:54:23.120 --> 1:54:27.160\n It's the thing that brings me to one of my passions\n\n1:54:27.160 --> 1:54:28.000\n that is learning.\n\n1:54:28.000 --> 1:54:31.680\n So in this sense, this is a new area\n\n1:54:31.680 --> 1:54:35.120\n that, as a learning system myself,\n\n1:54:35.120 --> 1:54:36.640\n I want to keep exploring.\n\n1:54:36.640 --> 1:54:41.000\n And I think it's great to see parts of the debate.\n\n1:54:41.000 --> 1:54:43.720\n And even I've seen a level of maturity\n\n1:54:43.720 --> 1:54:46.400\n in the conferences that deal with AI.\n\n1:54:46.400 --> 1:54:49.840\n If you look five years ago to now,\n\n1:54:49.840 --> 1:54:53.040\n just the amount of workshops and so on has changed so much.\n\n1:54:53.040 --> 1:54:58.520\n It's impressive to see how much topics of safety, ethics,\n\n1:54:58.520 --> 1:55:01.720\n and so on come to the surface, which is great.\n\n1:55:01.720 --> 1:55:03.800\n And if it were too early, clearly it's fine.\n\n1:55:03.800 --> 1:55:05.920\n I mean, it's a big field, and there's\n\n1:55:05.920 --> 1:55:09.040\n lots of people with lots of interests\n\n1:55:09.040 --> 1:55:11.880\n that will do progress or make progress.\n\n1:55:11.880 --> 1:55:14.160\n And obviously, I don't believe we're too late.\n\n1:55:14.160 --> 1:55:16.440\n So in that sense, I think it's great\n\n1:55:16.440 --> 1:55:18.160\n that we're doing this already.\n\n1:55:18.160 --> 1:55:20.200\n It better be too early than too late\n\n1:55:20.200 --> 1:55:22.720\n when it comes to super intelligent AI systems.\n\n1:55:22.720 --> 1:55:25.480\n Let me ask, speaking of sentient AIs,\n\n1:55:25.480 --> 1:55:28.680\n you gave props to your friend Ilyas Etzgever\n\n1:55:28.680 --> 1:55:31.960\n for being elected the fellow of the Royal Society.\n\n1:55:31.960 --> 1:55:34.680\n So just as a shout out to a fellow researcher\n\n1:55:34.680 --> 1:55:38.240\n and a friend, what's the secret to the genius of Ilyas\n\n1:55:38.240 --> 1:55:39.400\n Etzgever?\n\n1:55:39.400 --> 1:55:42.640\n And also, do you believe that his tweets,\n\n1:55:42.640 --> 1:55:46.000\n as you've hypothesized and Andrej Karpathy did as well,\n\n1:55:46.000 --> 1:55:48.680\n are generated by a language model?\n\n1:55:48.680 --> 1:55:49.360\n Yeah.\n\n1:55:49.360 --> 1:55:54.240\n So I strongly believe Ilya is going to visit in a few weeks,\n\n1:55:54.240 --> 1:55:54.720\n actually.\n\n1:55:54.720 --> 1:55:58.000\n So I'll ask him in person.\n\n1:55:58.000 --> 1:55:59.160\n Will he tell you the truth?\n\n1:55:59.160 --> 1:56:00.720\n Yes, of course, hopefully.\n\n1:56:00.720 --> 1:56:04.040\n I mean, ultimately, we all have shared paths,\n\n1:56:04.040 --> 1:56:08.280\n and there's friendships that go beyond, obviously,\n\n1:56:08.280 --> 1:56:09.960\n institutions and so on.\n\n1:56:09.960 --> 1:56:11.680\n So I hope he tells me the truth.\n\n1:56:11.680 --> 1:56:14.400\n Well, maybe the AI system is holding him hostage somehow.\n\n1:56:14.400 --> 1:56:16.920\n Maybe he has some videos that he doesn't want to release.\n\n1:56:16.920 --> 1:56:19.720\n So maybe it has taken control over him.\n\n1:56:19.720 --> 1:56:20.960\n So he can't tell the truth.\n\n1:56:20.960 --> 1:56:23.920\n Well, if I see him in person, then I think he will know.\n\n1:56:23.920 --> 1:56:33.920\n But I think Ilya's personality, just knowing him for a while,\n\n1:56:33.920 --> 1:56:36.640\n everyone in Twitter, I guess, gets a different persona.\n\n1:56:36.640 --> 1:56:40.920\n And I think Ilya's one does not surprise me.\n\n1:56:40.920 --> 1:56:43.600\n So I think knowing Ilya from before social media\n\n1:56:43.600 --> 1:56:46.000\n and before AI was so prevalent, I\n\n1:56:46.000 --> 1:56:47.560\n recognize a lot of his character.\n\n1:56:47.560 --> 1:56:49.200\n So that's something for me that I\n\n1:56:49.200 --> 1:56:52.520\n feel good about a friend that hasn't changed\n\n1:56:52.520 --> 1:56:55.960\n or is still true to himself.\n\n1:56:55.960 --> 1:56:58.960\n Obviously, there is, though, a fact\n\n1:56:58.960 --> 1:57:02.080\n that your field becomes more popular,\n\n1:57:02.080 --> 1:57:05.440\n and he is obviously one of the main figures in the field,\n\n1:57:05.440 --> 1:57:07.040\n having done a lot of advancement.\n\n1:57:07.040 --> 1:57:09.080\n So I think that the tricky bit here\n\n1:57:09.080 --> 1:57:12.200\n is how to balance your true self with the responsibility\n\n1:57:12.200 --> 1:57:13.560\n that your words carry.\n\n1:57:13.560 --> 1:57:19.360\n So in this sense, I appreciate the style, and I understand it.\n\n1:57:19.360 --> 1:57:24.160\n But it created debates on some of his tweets\n\n1:57:24.160 --> 1:57:27.920\n that maybe it's good we have them early anyways.\n\n1:57:27.920 --> 1:57:31.040\n But yeah, then the reactions are usually polarizing.\n\n1:57:31.040 --> 1:57:34.160\n I think we're just seeing the reality of social media\n\n1:57:34.160 --> 1:57:38.120\n be there as well, reflected on that particular topic\n\n1:57:38.120 --> 1:57:40.200\n or set of topics he's tweeting about.\n\n1:57:40.200 --> 1:57:42.960\n Yeah, I mean, it's funny that he used to speak to this tension.\n\n1:57:42.960 --> 1:57:46.160\n He was one of the early seminal figures\n\n1:57:46.160 --> 1:57:47.800\n in the field of deep learning, so there's\n\n1:57:47.800 --> 1:57:48.960\n a responsibility with that.\n\n1:57:48.960 --> 1:57:53.200\n But he's also, from having interacted with him quite a bit,\n\n1:57:53.200 --> 1:58:01.280\n he's just a brilliant thinker about ideas, which, as are you.\n\n1:58:01.280 --> 1:58:03.120\n And there's a tension between becoming\n\n1:58:03.120 --> 1:58:06.960\n the manager versus the actual thinking\n\n1:58:06.960 --> 1:58:13.640\n through very novel ideas, the scientist versus the manager.\n\n1:58:13.640 --> 1:58:17.680\n And he's one of the great scientists of our time.\n\n1:58:17.680 --> 1:58:18.960\n So this was quite interesting.\n\n1:58:18.960 --> 1:58:20.840\n And also, people tell me quite silly,\n\n1:58:20.840 --> 1:58:23.200\n which I haven't quite detected yet.\n\n1:58:23.200 --> 1:58:26.000\n But in private, we'll have to see about that.\n\n1:58:26.000 --> 1:58:27.480\n Yeah, yeah.\n\n1:58:27.480 --> 1:58:30.000\n I mean, just on the point of, I mean,\n\n1:58:30.000 --> 1:58:33.360\n Ilya has been an inspiration.\n\n1:58:33.360 --> 1:58:35.480\n I mean, quite a few colleagues, I can think,\n\n1:58:35.480 --> 1:58:38.080\n shaped the person you are.\n\n1:58:38.080 --> 1:58:42.320\n Like, Ilya certainly gets probably the top spot,\n\n1:58:42.320 --> 1:58:43.800\n if not close to the top.\n\n1:58:43.800 --> 1:58:47.960\n And if we go back to the question about people in the field,\n\n1:58:47.960 --> 1:58:51.680\n like how their role would have changed the field or not,\n\n1:58:51.680 --> 1:58:54.000\n I think Ilya's case is interesting\n\n1:58:54.000 --> 1:58:58.760\n because he really has a deep belief in the scaling up\n\n1:58:58.760 --> 1:58:59.640\n of neural networks.\n\n1:58:59.640 --> 1:59:03.680\n There was a talk that is still famous to this day\n\n1:59:03.680 --> 1:59:07.720\n from the Sequence to Sequence paper, where he was just\n\n1:59:07.720 --> 1:59:10.560\n claiming, just give me supervised data\n\n1:59:10.560 --> 1:59:12.800\n and a large neural network, and then you'll\n\n1:59:12.800 --> 1:59:16.240\n solve basically all the problems.\n\n1:59:16.240 --> 1:59:19.800\n That vision was already there many years ago.\n\n1:59:19.800 --> 1:59:22.880\n So it's good to see someone who is, in this case,\n\n1:59:22.880 --> 1:59:27.160\n very deeply into this style of research\n\n1:59:27.160 --> 1:59:32.800\n and clearly has had a tremendous track record of successes\n\n1:59:32.800 --> 1:59:34.160\n and so on.\n\n1:59:34.160 --> 1:59:37.520\n The funny bit about that talk is that we rehearsed the talk\n\n1:59:37.520 --> 1:59:42.040\n in a hotel room before, and the original version of that talk\n\n1:59:42.040 --> 1:59:44.000\n would have been even more controversial.\n\n1:59:44.000 --> 1:59:46.760\n So maybe I'm the only person that\n\n1:59:46.760 --> 1:59:49.520\n has seen the unfiltered version of the talk.\n\n1:59:49.520 --> 1:59:52.160\n And maybe when the time comes, maybe we\n\n1:59:52.160 --> 1:59:55.120\n should revisit some of the skip slides\n\n1:59:55.120 --> 1:59:57.560\n from the talk from Ilya.\n\n1:59:57.560 --> 2:00:01.040\n But I really think the deep belief\n\n2:00:01.040 --> 2:00:03.240\n into some certain style of research\n\n2:00:03.240 --> 2:00:06.400\n pays out, is good to be practical sometimes.\n\n2:00:06.400 --> 2:00:09.400\n And I actually think Ilya and myself are practical,\n\n2:00:09.400 --> 2:00:10.440\n but it's also good.\n\n2:00:10.440 --> 2:00:14.840\n There's some sort of long term belief and trajectory.\n\n2:00:14.840 --> 2:00:16.720\n Obviously, there's a bit of lack involved,\n\n2:00:16.720 --> 2:00:18.840\n but it might be that that's the right path.\n\n2:00:18.840 --> 2:00:22.320\n Then you clearly are ahead and hugely influential to the field\n\n2:00:22.320 --> 2:00:23.560\n as he has been.\n\n2:00:23.560 --> 2:00:26.440\n Do you agree with that intuition that maybe\n\n2:00:26.440 --> 2:00:33.600\n was written about by Rich Sutton in The Bitter Lesson,\n\n2:00:33.600 --> 2:00:36.480\n that the biggest lesson that can be read from 70 years of AI\n\n2:00:36.480 --> 2:00:40.080\n research is that general methods that leverage computation\n\n2:00:40.080 --> 2:00:42.800\n are ultimately the most effective?\n\n2:00:42.800 --> 2:00:48.560\n Do you think that intuition is ultimately correct?\n\n2:00:48.560 --> 2:00:52.240\n General methods that leverage computation,\n\n2:00:52.240 --> 2:00:54.360\n allowing the scaling of computation\n\n2:00:54.360 --> 2:00:56.240\n to do a lot of the work.\n\n2:00:56.240 --> 2:00:59.640\n And so the basic task of us humans\n\n2:00:59.640 --> 2:01:01.440\n is to design methods that are more\n\n2:01:01.440 --> 2:01:05.960\n and more general versus more and more specific to the tasks\n\n2:01:05.960 --> 2:01:07.040\n at hand.\n\n2:01:07.040 --> 2:01:10.320\n I certainly think this essentially mimics\n\n2:01:10.320 --> 2:01:14.680\n a bit of the deep learning research,\n\n2:01:14.680 --> 2:01:18.840\n almost like philosophy, that on the one hand,\n\n2:01:18.840 --> 2:01:20.480\n we want to be data agnostic.\n\n2:01:20.480 --> 2:01:22.160\n We don't want to preprocess data sets.\n\n2:01:22.160 --> 2:01:25.560\n We want to see the bytes, the true data as it is,\n\n2:01:25.560 --> 2:01:27.440\n and then learn everything on top.\n\n2:01:27.440 --> 2:01:30.120\n So very much agree with that.\n\n2:01:30.120 --> 2:01:33.360\n And I think scaling up feels, at the very least, again,\n\n2:01:33.360 --> 2:01:38.960\n necessary for building incredible complex systems.\n\n2:01:38.960 --> 2:01:42.880\n It's possibly not sufficient, barring that we\n\n2:01:42.880 --> 2:01:45.080\n need a couple of breakthroughs.\n\n2:01:45.080 --> 2:01:47.960\n I think Reed Sutton mentioned search\n\n2:01:47.960 --> 2:01:52.200\n being part of the equation of scale and search.\n\n2:01:52.200 --> 2:01:55.720\n I think search, I've seen it, that's\n\n2:01:55.720 --> 2:01:57.400\n been more mixed in my experience.\n\n2:01:57.400 --> 2:01:59.320\n So from that lesson in particular,\n\n2:01:59.320 --> 2:02:02.480\n search is a bit more tricky because it\n\n2:02:02.480 --> 2:02:05.320\n is very appealing to search in domains like Go,\n\n2:02:05.320 --> 2:02:08.080\n where you have a clear reward function that you can then\n\n2:02:08.080 --> 2:02:10.560\n discard some search traces.\n\n2:02:10.560 --> 2:02:13.160\n But then in some other tasks, it's\n\n2:02:13.160 --> 2:02:15.160\n not very clear how you would do that,\n\n2:02:15.160 --> 2:02:19.320\n although recently one of our recent works, which actually\n\n2:02:19.320 --> 2:02:22.120\n was mostly mimicking or a continuation,\n\n2:02:22.120 --> 2:02:25.840\n and even the team and the people involved were pretty much very\n\n2:02:25.840 --> 2:02:28.400\n intersecting with AlphaStar, was AlphaCode,\n\n2:02:28.400 --> 2:02:31.440\n in which we actually saw the bitter lesson how\n\n2:02:31.440 --> 2:02:34.240\n scale of the models and then a massive amount of search\n\n2:02:34.240 --> 2:02:36.760\n yielded this kind of very interesting result\n\n2:02:36.760 --> 2:02:41.280\n of being able to have human level code competition.\n\n2:02:41.280 --> 2:02:43.640\n So I've seen examples of it being\n\n2:02:43.640 --> 2:02:46.320\n literally mapped to search and scale.\n\n2:02:46.320 --> 2:02:48.120\n I'm not so convinced about the search bit,\n\n2:02:48.120 --> 2:02:51.000\n but certainly I'm convinced scale will be needed.\n\n2:02:51.000 --> 2:02:52.600\n So we need general methods.\n\n2:02:52.600 --> 2:02:54.080\n We need to test them, and maybe we\n\n2:02:54.080 --> 2:02:57.080\n need to make sure that we can scale them given the hardware\n\n2:02:57.080 --> 2:02:59.080\n that we have in practice.\n\n2:02:59.080 --> 2:03:01.920\n But then maybe we should also shape how the hardware looks\n\n2:03:01.920 --> 2:03:05.640\n like based on which methods might be needed to scale.\n\n2:03:05.640 --> 2:03:11.600\n And that's an interesting contrast of these GPU comments\n\n2:03:11.600 --> 2:03:14.280\n that is we got it for free almost because games\n\n2:03:14.280 --> 2:03:15.080\n were using these.\n\n2:03:15.080 --> 2:03:19.440\n But maybe now if sparsity is required,\n\n2:03:19.440 --> 2:03:20.560\n we don't have the hardware.\n\n2:03:20.560 --> 2:03:22.840\n Although in theory, many people are\n\n2:03:22.840 --> 2:03:24.800\n building different kinds of hardware these days.\n\n2:03:24.800 --> 2:03:27.760\n But there's a bit of this notion of hardware lottery\n\n2:03:27.760 --> 2:03:31.560\n for scale that might actually have an impact at least\n\n2:03:31.560 --> 2:03:35.240\n on the scale of years on how fast we will make progress\n\n2:03:35.240 --> 2:03:37.680\n to maybe a version of neural nets\n\n2:03:37.680 --> 2:03:41.920\n or whatever comes next that might enable\n\n2:03:41.920 --> 2:03:44.360\n truly intelligent agents.\n\n2:03:44.360 --> 2:03:50.520\n Do you think in your lifetime we will build an AGI system that\n\n2:03:50.520 --> 2:03:55.640\n would undeniably be a thing that achieves human level\n\n2:03:55.640 --> 2:03:58.480\n intelligence and goes far beyond?\n\n2:03:58.480 --> 2:04:03.720\n I definitely think it's possible that it will go far beyond.\n\n2:04:03.720 --> 2:04:05.520\n But I'm definitely convinced that it will\n\n2:04:05.520 --> 2:04:08.480\n be human level intelligence.\n\n2:04:08.480 --> 2:04:11.000\n And I'm hypothesizing about the beyond\n\n2:04:11.000 --> 2:04:16.520\n because the beyond bit is a bit tricky to define,\n\n2:04:16.520 --> 2:04:21.280\n especially when we look at the current formula of starting\n\n2:04:21.280 --> 2:04:23.760\n from this imitation learning standpoint.\n\n2:04:23.760 --> 2:04:30.760\n So we can certainly imitate humans at language and beyond.\n\n2:04:30.760 --> 2:04:33.440\n So getting at human level through imitation\n\n2:04:33.440 --> 2:04:34.920\n feels very possible.\n\n2:04:34.920 --> 2:04:39.120\n Going beyond will require reinforcement learning\n\n2:04:39.120 --> 2:04:39.880\n and other things.\n\n2:04:39.880 --> 2:04:43.600\n And I think in some areas that certainly already has paid out.\n\n2:04:43.600 --> 2:04:46.000\n I mean, Go being an example that's\n\n2:04:46.000 --> 2:04:48.240\n my favorite so far in terms of going\n\n2:04:48.240 --> 2:04:50.440\n beyond human capabilities.\n\n2:04:50.440 --> 2:04:55.600\n But in general, I'm not sure we can define reward functions\n\n2:04:55.600 --> 2:04:59.360\n that from a seed of imitating human level\n\n2:04:59.360 --> 2:05:02.920\n intelligence that is general and then going beyond.\n\n2:05:02.920 --> 2:05:05.280\n That bit is not so clear in my lifetime.\n\n2:05:05.280 --> 2:05:08.240\n But certainly, human level, yes.\n\n2:05:08.240 --> 2:05:11.000\n And I mean, that in itself is already quite powerful,\n\n2:05:11.000 --> 2:05:11.520\n I think.\n\n2:05:11.520 --> 2:05:14.560\n So going beyond, I think it's obviously not.\n\n2:05:14.560 --> 2:05:17.680\n We're not going to not try that if then we\n\n2:05:17.680 --> 2:05:20.760\n get to superhuman scientists and discovery\n\n2:05:20.760 --> 2:05:22.160\n and advancing the world.\n\n2:05:22.160 --> 2:05:25.600\n But at least human level in general\n\n2:05:25.600 --> 2:05:27.560\n is also very, very powerful.\n\n2:05:27.560 --> 2:05:31.560\n Well, especially if human level or slightly beyond\n\n2:05:31.560 --> 2:05:33.760\n is integrated deeply with human society\n\n2:05:33.760 --> 2:05:36.520\n and there's billions of agents like that,\n\n2:05:36.520 --> 2:05:39.960\n do you think there's a singularity moment beyond which\n\n2:05:39.960 --> 2:05:44.200\n our world will be just very deeply transformed\n\n2:05:44.200 --> 2:05:45.640\n by these kinds of systems?\n\n2:05:45.640 --> 2:05:47.840\n Because now you're talking about intelligence systems\n\n2:05:47.840 --> 2:05:53.040\n that are just, I mean, this is no longer just going\n\n2:05:53.040 --> 2:05:56.440\n from horse and buggy to the car.\n\n2:05:56.440 --> 2:05:59.760\n It feels like a very different kind of shift\n\n2:05:59.760 --> 2:06:03.280\n in what it means to be a living entity on Earth.\n\n2:06:03.280 --> 2:06:04.240\n Are you afraid?\n\n2:06:04.240 --> 2:06:06.280\n Are you excited of this world?\n\n2:06:06.280 --> 2:06:09.360\n I'm afraid if there's a lot more.\n\n2:06:09.360 --> 2:06:13.680\n So I think maybe we'll need to think about if we truly\n\n2:06:13.680 --> 2:06:18.400\n get there just thinking of limited resources\n\n2:06:18.400 --> 2:06:21.480\n like humanity clearly hit some limits\n\n2:06:21.480 --> 2:06:23.440\n and then there's some balance, hopefully,\n\n2:06:23.440 --> 2:06:26.320\n that biologically the planet is imposing.\n\n2:06:26.320 --> 2:06:28.600\n And we should actually try to get better at this.\n\n2:06:28.600 --> 2:06:31.600\n As we know, there's quite a few issues\n\n2:06:31.600 --> 2:06:35.840\n with having too many people coexisting\n\n2:06:35.840 --> 2:06:37.720\n in a resource limited way.\n\n2:06:37.720 --> 2:06:40.360\n So for digital entities, it's an interesting question.\n\n2:06:40.360 --> 2:06:43.520\n I think such a limit maybe should exist.\n\n2:06:43.520 --> 2:06:47.680\n But maybe it's going to be imposed by energy availability\n\n2:06:47.680 --> 2:06:49.760\n because this also consumes energy.\n\n2:06:49.760 --> 2:06:53.560\n In fact, most systems are more inefficient\n\n2:06:53.560 --> 2:06:56.720\n than we are in terms of energy required.\n\n2:06:56.720 --> 2:06:59.480\n But definitely, I think as a society,\n\n2:06:59.480 --> 2:07:03.520\n we'll need to just work together to find\n\n2:07:03.520 --> 2:07:06.400\n what would be reasonable in terms of growth\n\n2:07:06.400 --> 2:07:11.400\n or how we coexist if that is to happen.\n\n2:07:11.400 --> 2:07:14.640\n I am very excited about, obviously,\n\n2:07:14.640 --> 2:07:17.720\n the aspects of automation that make people\n\n2:07:17.720 --> 2:07:20.120\n that obviously don't have access to certain resources\n\n2:07:20.120 --> 2:07:23.920\n or knowledge, for them to have that access.\n\n2:07:23.920 --> 2:07:26.280\n I think those are the applications in a way\n\n2:07:26.280 --> 2:07:30.960\n that I'm most excited to see and to personally work towards.\n\n2:07:30.960 --> 2:07:32.640\n Yeah, there's going to be significant improvements\n\n2:07:32.640 --> 2:07:34.320\n in productivity and the quality of life\n\n2:07:34.320 --> 2:07:36.960\n across the whole population, which is very interesting.\n\n2:07:36.960 --> 2:07:39.200\n But I'm looking even far beyond\n\n2:07:39.200 --> 2:07:42.680\n us becoming a multiplanetary species.\n\n2:07:42.680 --> 2:07:45.360\n And just as a quick bet, last question.\n\n2:07:45.360 --> 2:07:49.200\n Do you think as humans become multiplanetary species,\n\n2:07:49.200 --> 2:07:52.480\n go outside our solar system, all that kind of stuff,\n\n2:07:52.480 --> 2:07:54.440\n do you think there will be more humans\n\n2:07:54.440 --> 2:07:57.200\n or more robots in that future world?\n\n2:07:57.200 --> 2:08:02.200\n So will humans be the quirky, intelligent being of the past\n\n2:08:04.480 --> 2:08:07.000\n or is there something deeply fundamental\n\n2:08:07.000 --> 2:08:09.560\n to human intelligence that's truly special,\n\n2:08:09.560 --> 2:08:12.120\n where we will be part of those other planets,\n\n2:08:12.120 --> 2:08:13.920\n not just AI systems?\n\n2:08:13.920 --> 2:08:18.640\n I think we're all excited to build AGI\n\n2:08:18.640 --> 2:08:23.640\n to empower or make us more powerful as human species.\n\n2:08:25.080 --> 2:08:27.560\n Not to say there might be some hybridization.\n\n2:08:27.560 --> 2:08:29.680\n I mean, this is obviously speculation,\n\n2:08:29.680 --> 2:08:32.480\n but there are companies also trying to,\n\n2:08:32.480 --> 2:08:35.640\n the same way medicine is making us better.\n\n2:08:35.640 --> 2:08:39.080\n Maybe there are other things that are yet to happen on that.\n\n2:08:39.080 --> 2:08:43.320\n But if the ratio is not at most one to one,\n\n2:08:43.320 --> 2:08:44.520\n I would not be happy.\n\n2:08:44.520 --> 2:08:49.200\n So I would hope that we are part of the equation,\n\n2:08:49.200 --> 2:08:53.280\n but maybe there's maybe a one to one ratio feels\n\n2:08:53.280 --> 2:08:56.200\n like possible, constructive and so on,\n\n2:08:56.200 --> 2:08:59.600\n but it would not be good to have a misbalance,\n\n2:08:59.600 --> 2:09:03.280\n at least from my core beliefs and the why I'm doing\n\n2:09:03.280 --> 2:09:05.760\n what I'm doing when I go to work and I research\n\n2:09:05.760 --> 2:09:07.120\n what I research.\n\n2:09:07.120 --> 2:09:09.520\n Well, this is how I know you're human\n\n2:09:09.520 --> 2:09:11.760\n and this is how you've passed the Turing test.\n\n2:09:12.720 --> 2:09:15.000\n And you are one of the special humans, Oriel.\n\n2:09:15.000 --> 2:09:17.120\n It's a huge honor that you would talk with me\n\n2:09:17.120 --> 2:09:19.920\n and I hope we get the chance to speak again,\n\n2:09:19.920 --> 2:09:23.040\n maybe once before the singularity, once after\n\n2:09:23.040 --> 2:09:25.440\n and see how our view of the world changes.\n\n2:09:25.440 --> 2:09:26.600\n Thank you again for talking today.\n\n2:09:26.600 --> 2:09:28.200\n Thank you for the amazing work you do.\n\n2:09:28.200 --> 2:09:31.320\n You're a shining example of a research\n\n2:09:31.320 --> 2:09:32.960\n and a human being in this community.\n\n2:09:32.960 --> 2:09:33.800\n Thanks a lot.\n\n2:09:33.800 --> 2:09:36.240\n Like yeah, looking forward to before the singularity\n\n2:09:36.240 --> 2:09:39.000\n certainly and maybe after.\n\n2:09:39.920 --> 2:09:41.480\n Thanks for listening to this conversation\n\n2:09:41.480 --> 2:09:43.120\n with Oriel Venialis.\n\n2:09:43.120 --> 2:09:45.520\n To support this podcast, please check out our sponsors\n\n2:09:45.520 --> 2:09:46.960\n in the description.\n\n2:09:46.960 --> 2:09:50.080\n And now let me leave you with some words from Alan Turing.\n\n2:09:51.160 --> 2:09:55.080\n Those who can imagine anything can create the impossible.\n\n2:09:55.080 --> 2:10:08.080\n Thank you for listening and hope to see you next time.\n\n"
}