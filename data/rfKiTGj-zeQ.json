{
  "title": "Nick Bostrom: Simulation and Superintelligence | Lex Fridman Podcast #83",
  "id": "rfKiTGj-zeQ",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:05.520\n The following is a conversation with Nick Bostrom, a philosopher at University of Oxford\n\n00:05.520 --> 00:08.840\n and the director of the Future of Humanity Institute.\n\n00:08.840 --> 00:15.320\n He has worked on fascinating and important ideas in existential risk, simulation hypothesis,\n\n00:15.320 --> 00:20.480\n human enhancement ethics, and the risks of superintelligent AI systems, including in\n\n00:20.480 --> 00:23.200\n his book, Superintelligence.\n\n00:23.200 --> 00:27.920\n I can see talking to Nick multiple times in this podcast, many hours each time, because\n\n00:27.920 --> 00:34.520\n he has done some incredible work in artificial intelligence, in technology, space, science,\n\n00:34.520 --> 00:38.920\n and really philosophy in general, but we have to start somewhere.\n\n00:38.920 --> 00:43.620\n This conversation was recorded before the outbreak of the coronavirus pandemic that\n\n00:43.620 --> 00:49.240\n both Nick and I, I'm sure, will have a lot to say about next time we speak, and perhaps\n\n00:49.240 --> 00:54.440\n that is for the best, because the deepest lessons can be learned only in retrospect\n\n00:54.440 --> 00:56.680\n when the storm has passed.\n\n00:56.680 --> 01:01.380\n I do recommend you read many of his papers on the topic of existential risk, including\n\n01:01.380 --> 01:07.600\n the technical report titled Global Catastrophic Risks Survey that he coauthored with Anders\n\n01:07.600 --> 01:08.600\n Sandberg.\n\n01:08.600 --> 01:14.000\n For everyone feeling the medical, psychological, and financial burden of this crisis, I'm\n\n01:14.000 --> 01:15.680\n sending love your way.\n\n01:15.680 --> 01:16.680\n Stay strong.\n\n01:16.680 --> 01:17.840\n We're in this together.\n\n01:17.840 --> 01:20.880\n We'll beat this thing.\n\n01:20.880 --> 01:22.920\n This is the Artificial Intelligence Podcast.\n\n01:22.920 --> 01:28.120\n If you enjoy it, subscribe on YouTube, review it with five stars on Apple Podcast, support\n\n01:28.120 --> 01:33.760\n it on Patreon, or simply connect with me on Twitter at Lex Friedman, spelled F R I D M\n\n01:33.760 --> 01:34.760\n A N.\n\n01:34.760 --> 01:39.120\n As usual, I'll do one or two minutes of ads now and never any ads in the middle that\n\n01:39.120 --> 01:41.080\n can break the flow of the conversation.\n\n01:41.080 --> 01:46.040\n I hope that works for you and doesn't hurt the listening experience.\n\n01:46.040 --> 01:50.080\n This show is presented by Cash App, the number one finance app in the App Store.\n\n01:50.080 --> 01:53.800\n When you get it, use code LEXPODCAST.\n\n01:53.800 --> 01:57.920\n Cash App lets you send money to friends, buy Bitcoin, and invest in the stock market with\n\n01:57.920 --> 01:59.480\n as little as one dollar.\n\n01:59.480 --> 02:05.200\n Since Cash App does fractional share trading, let me mention that the order execution algorithm\n\n02:05.200 --> 02:10.360\n that works behind the scenes to create the abstraction of fractional orders is an algorithmic\n\n02:10.360 --> 02:11.360\n marvel.\n\n02:11.360 --> 02:16.200\n So big props to the Cash App engineers for solving a hard problem that in the end provides\n\n02:16.200 --> 02:20.360\n an easy interface that takes a step up to the next layer of abstraction over the stock\n\n02:20.360 --> 02:26.960\n market, making trading more accessible for new investors and diversification much easier.\n\n02:26.960 --> 02:33.200\n So again, if you get Cash App from the App Store, Google Play, and use the code LEXPODCAST,\n\n02:33.200 --> 02:38.560\n you get $10, and Cash App will also donate $10 to FIRST, an organization that is helping\n\n02:38.560 --> 02:43.960\n to advance robotics and STEM education for young people around the world.\n\n02:43.960 --> 02:49.160\n And now, here's my conversation with Nick Bostrom.\n\n02:49.160 --> 02:54.200\n At the risk of asking the Beatles to play yesterday or the Rolling Stones to play Satisfaction,\n\n02:54.200 --> 02:56.520\n let me ask you the basics.\n\n02:56.520 --> 02:59.520\n What is the simulation hypothesis?\n\n02:59.520 --> 03:02.960\n That we are living in a computer simulation.\n\n03:02.960 --> 03:04.440\n What is a computer simulation?\n\n03:04.440 --> 03:06.760\n How are we supposed to even think about that?\n\n03:06.760 --> 03:15.360\n Well, so the hypothesis is meant to be understood in a literal sense, not that we can kind of\n\n03:15.360 --> 03:21.320\n metaphorically view the universe as an information processing physical system, but that there\n\n03:21.320 --> 03:28.980\n is some advanced civilization who built a lot of computers and that what we experience\n\n03:28.980 --> 03:34.960\n is an effect of what's going on inside one of those computers so that the world around\n\n03:34.960 --> 03:43.800\n us, our own brains, everything we see and perceive and think and feel would exist because\n\n03:43.800 --> 03:48.280\n this computer is running certain programs.\n\n03:48.280 --> 03:55.320\n So do you think of this computer as something similar to the computers of today, these deterministic\n\n03:55.320 --> 03:58.160\n sort of Turing machine type things?\n\n03:58.160 --> 04:01.480\n Is that what we're supposed to imagine or we're supposed to think of something more\n\n04:01.480 --> 04:07.080\n like a quantum mechanical system?\n\n04:07.080 --> 04:11.240\n Something much bigger, something much more complicated, something much more mysterious\n\n04:11.240 --> 04:12.840\n from our current perspective?\n\n04:12.840 --> 04:15.400\n The ones we have today would do fine, I mean, bigger, certainly.\n\n04:15.400 --> 04:18.720\n You'd need more memory and more processing power.\n\n04:18.720 --> 04:21.280\n I don't think anything else would be required.\n\n04:21.280 --> 04:26.580\n Now, it might well be that they do have additional, maybe they have quantum computers and other\n\n04:26.580 --> 04:31.320\n things that would give them even more of, it seems kind of plausible, but I don't think\n\n04:31.320 --> 04:38.880\n it's a necessary assumption in order to get to the conclusion that a technologically\n\n04:38.880 --> 04:44.520\n mature civilization would be able to create these kinds of computer simulations with conscious\n\n04:44.520 --> 04:46.640\n beings inside them.\n\n04:46.640 --> 04:52.840\n So do you think the simulation hypothesis is an idea that's most useful in philosophy,\n\n04:52.840 --> 05:02.480\n computer science, physics, sort of where do you see it having valuable kind of starting\n\n05:02.480 --> 05:05.280\n point in terms of a thought experiment of it?\n\n05:05.280 --> 05:06.280\n Is it useful?\n\n05:06.280 --> 05:14.480\n I guess it's more informative and interesting and maybe important, but it's not designed\n\n05:14.480 --> 05:16.560\n to be useful for something else.\n\n05:16.560 --> 05:18.480\n Okay, interesting, sure.\n\n05:18.480 --> 05:23.360\n But is it philosophically interesting or is there some kind of implications of computer\n\n05:23.360 --> 05:24.900\n science and physics?\n\n05:24.900 --> 05:29.460\n I think not so much for computer science or physics per se.\n\n05:29.460 --> 05:37.320\n Certainly it would be of interest in philosophy, I think also to say cosmology or physics in\n\n05:37.320 --> 05:43.860\n as much as you're interested in the fundamental building blocks of the world and the rules\n\n05:43.860 --> 05:46.040\n that govern it.\n\n05:46.040 --> 05:50.480\n If we are in a simulation, there is then the possibility that say physics at the level\n\n05:50.480 --> 05:57.640\n where the computer running the simulation could be different from the physics governing\n\n05:57.640 --> 05:59.780\n phenomena in the simulation.\n\n05:59.780 --> 06:06.280\n So I think it might be interesting from point of view of religion or just for kind of trying\n\n06:06.280 --> 06:09.720\n to figure out what the heck is going on.\n\n06:09.720 --> 06:14.680\n So we mentioned the simulation hypothesis so far.\n\n06:14.680 --> 06:19.800\n There is also the simulation argument, which I tend to make a distinction.\n\n06:19.800 --> 06:23.080\n So simulation hypothesis, we are living in a computer simulation.\n\n06:23.080 --> 06:27.880\n Simulation argument, this argument that tries to show that one of three propositions is\n\n06:27.880 --> 06:34.840\n true, one of which is the simulation hypothesis, but there are two alternatives in the original\n\n06:34.840 --> 06:36.840\n simulation argument, which we can get to.\n\n06:36.840 --> 06:37.840\n Yeah, let's go there.\n\n06:37.840 --> 06:43.320\n By the way, confusing terms because people will, I think, probably naturally think simulation\n\n06:43.320 --> 06:47.160\n argument equals simulation hypothesis, just terminology wise.\n\n06:47.160 --> 06:48.160\n But let's go there.\n\n06:48.160 --> 06:52.560\n So simulation hypothesis means that we are living in a simulations, the hypothesis that\n\n06:52.560 --> 06:58.840\n we're living in a simulation, simulation argument has these three complete possibilities that\n\n06:58.840 --> 07:00.480\n cover all possibilities.\n\n07:00.480 --> 07:01.480\n So what are they?\n\n07:01.480 --> 07:02.480\n Yeah.\n\n07:02.480 --> 07:03.480\n So it's like a disjunction.\n\n07:03.480 --> 07:08.440\n It says at least one of these three is true, although it doesn't on its own tell us which\n\n07:08.440 --> 07:10.760\n one.\n\n07:10.760 --> 07:17.560\n So the first one is that almost all civilizations that are current stage of technological development\n\n07:17.560 --> 07:23.820\n go extinct before they reach technological maturity.\n\n07:23.820 --> 07:34.520\n So there is some great filter that makes it so that basically none of the civilizations\n\n07:34.520 --> 07:41.800\n throughout maybe a vast cosmos will ever get to realize the full potential of technological\n\n07:41.800 --> 07:42.800\n development.\n\n07:42.800 --> 07:47.660\n And this could be, theoretically speaking, this could be because most civilizations kill\n\n07:47.660 --> 07:52.840\n themselves too eagerly or destroy themselves too eagerly, or it might be super difficult\n\n07:52.840 --> 07:55.200\n to build a simulation.\n\n07:55.200 --> 07:57.400\n So the span of time.\n\n07:57.400 --> 07:58.400\n Theoretically it could be both.\n\n07:58.400 --> 08:04.500\n Now I think it looks like we would technologically be able to get there in a time span that\n\n08:04.500 --> 08:13.080\n is short compared to, say, the lifetime of planets and other sort of astronomical processes.\n\n08:13.080 --> 08:16.520\n So your intuition is to build a simulation is not...\n\n08:16.520 --> 08:21.240\n Well, so this is interesting concept of technological maturity.\n\n08:21.240 --> 08:25.160\n It's kind of an interesting concept to have other purposes as well.\n\n08:25.160 --> 08:31.120\n We can see even based on our current limited understanding what some lower bound would\n\n08:31.120 --> 08:37.480\n be on the capabilities that you could realize by just developing technologies that we already\n\n08:37.480 --> 08:38.600\n see are possible.\n\n08:38.600 --> 08:46.680\n So for example, one of my research fellows here, Eric Drexler, back in the 80s, studied\n\n08:46.680 --> 08:48.560\n molecular manufacturing.\n\n08:48.560 --> 08:55.520\n That is you could analyze using theoretical tools and computer modeling the performance\n\n08:55.520 --> 09:01.160\n of various molecularly precise structures that we didn't then and still don't today\n\n09:01.160 --> 09:04.100\n have the ability to actually fabricate.\n\n09:04.100 --> 09:07.460\n But you could say that, well, if we could put these atoms together in this way, then\n\n09:07.460 --> 09:13.760\n the system would be stable and it would rotate at this speed and have all these computational\n\n09:13.760 --> 09:16.260\n characteristics.\n\n09:16.260 --> 09:22.740\n And he also outlined some pathways that would enable us to get to this kind of molecularly\n\n09:22.740 --> 09:25.000\n manufacturing in the fullness of time.\n\n09:25.000 --> 09:28.280\n And you could do other studies we've done.\n\n09:28.280 --> 09:33.440\n You could look at the speed at which, say, it would be possible to colonize the galaxy\n\n09:33.440 --> 09:36.120\n if you had mature technology.\n\n09:36.120 --> 09:38.360\n We have an upper limit, which is the speed of light.\n\n09:38.360 --> 09:42.360\n We have sort of a lower current limit, which is how fast current rockets go.\n\n09:42.360 --> 09:47.560\n We know we can go faster than that by just making them bigger and have more fuel and\n\n09:47.560 --> 09:48.560\n stuff.\n\n09:48.560 --> 09:56.120\n We can then start to describe the technological affordances that would exist once a civilization\n\n09:56.120 --> 10:01.000\n has had enough time to develop, at least those technologies we already know are possible.\n\n10:01.000 --> 10:05.800\n Then maybe they would discover other new physical phenomena as well that we haven't realized\n\n10:05.800 --> 10:08.120\n that would enable them to do even more.\n\n10:08.120 --> 10:11.480\n But at least there is this kind of basic set of capabilities.\n\n10:11.480 --> 10:18.560\n Can you just link on that, how do we jump from molecular manufacturing to deep space\n\n10:18.560 --> 10:23.200\n exploration to mature technology?\n\n10:23.200 --> 10:24.200\n What's the connection there?\n\n10:24.200 --> 10:31.340\n Well, so these would be two examples of technological capability sets that we can have a high degree\n\n10:31.340 --> 10:38.000\n of confidence are physically possible in our universe and that a civilization that was\n\n10:38.000 --> 10:42.720\n allowed to continue to develop its science and technology would eventually attain.\n\n10:42.720 --> 10:48.960\n You can intuit like, we can kind of see the set of breakthroughs that are likely to happen.\n\n10:48.960 --> 10:53.240\n So you can see like, what did you call it, the technological set?\n\n10:53.240 --> 10:58.080\n With computers, maybe it's easiest.\n\n10:58.080 --> 11:01.800\n One is we could just imagine bigger computers using exactly the same parts that we have.\n\n11:01.800 --> 11:04.480\n So you can kind of scale things that way, right?\n\n11:04.480 --> 11:07.760\n But you could also make processors a bit faster.\n\n11:07.760 --> 11:13.080\n If you had this molecular nanotechnology that Eric Drexler described, he characterized a\n\n11:13.080 --> 11:19.760\n kind of crude computer built with these parts that would perform at a million times the\n\n11:19.760 --> 11:25.600\n human brain while being significantly smaller, the size of a sugar cube.\n\n11:25.600 --> 11:30.160\n And he made no claim that that's the optimum computing structure, like for all you know,\n\n11:30.160 --> 11:33.660\n we could build faster computers that would be more efficient, but at least you could\n\n11:33.660 --> 11:37.200\n do that if you had the ability to do things that were atomically precise.\n\n11:37.200 --> 11:39.600\n I mean, so you can then combine these two.\n\n11:39.600 --> 11:45.600\n You could have this kind of nanomolecular ability to build things atom by atom and then\n\n11:45.600 --> 11:53.040\n say at this as a spatial scale that would be attainable through space colonizing technology.\n\n11:53.040 --> 11:58.200\n You could then start, for example, to characterize a lower bound on the amount of computing power\n\n11:58.200 --> 12:01.840\n that a technologically mature civilization would have.\n\n12:01.840 --> 12:07.680\n If it could grab resources, you know, planets and so forth, and then use this molecular\n\n12:07.680 --> 12:15.320\n nanotechnology to optimize them for computing, you'd get a very, very high lower bound on\n\n12:15.320 --> 12:17.000\n the amount of compute.\n\n12:17.000 --> 12:22.120\n So sorry, just to define some terms, so technologically mature civilization is one that took that\n\n12:22.120 --> 12:26.220\n piece of technology to its lower bound.\n\n12:26.220 --> 12:27.920\n What is a technologically mature civilization?\n\n12:27.920 --> 12:31.160\n So that means it's a stronger concept than we really need for the simulation hypothesis.\n\n12:31.160 --> 12:34.040\n I just think it's interesting in its own right.\n\n12:34.040 --> 12:38.880\n So it would be the idea that there is some stage of technological development where you've\n\n12:38.880 --> 12:45.320\n basically maxed out, that you developed all those general purpose, widely useful technologies\n\n12:45.320 --> 12:51.600\n that could be developed, or at least kind of come very close to the, you know, 99.9%\n\n12:51.600 --> 12:53.440\n there or something.\n\n12:53.440 --> 12:55.160\n So that's an independent question.\n\n12:55.160 --> 12:59.480\n You can think either that there is such a ceiling, or you might think it just goes,\n\n12:59.480 --> 13:03.200\n the technology tree just goes on forever.\n\n13:03.200 --> 13:04.600\n Where does your sense fall?\n\n13:04.600 --> 13:10.000\n I would guess that there is a maximum that you would start to asymptote towards.\n\n13:10.000 --> 13:13.900\n So new things won't keep springing up, new ceilings.\n\n13:13.900 --> 13:18.980\n In terms of basic technological capabilities, I think that, yeah, there is like a finite\n\n13:18.980 --> 13:23.840\n set of laws that can exist in this universe.\n\n13:23.840 --> 13:30.120\n Moreover, I mean, I wouldn't be that surprised if we actually reached close to that level\n\n13:30.120 --> 13:33.980\n fairly shortly after we have, say, machine superintelligence.\n\n13:33.980 --> 13:39.760\n So I don't think it would take millions of years for a human originating civilization\n\n13:39.760 --> 13:42.760\n to begin to do this.\n\n13:42.760 --> 13:46.680\n It's more likely to happen on historical timescales.\n\n13:46.680 --> 13:51.320\n But that's an independent speculation from the simulation argument.\n\n13:51.320 --> 13:55.080\n I mean, for the purpose of the simulation argument, it doesn't really matter whether\n\n13:55.080 --> 13:59.400\n it goes indefinitely far up or whether there is a ceiling, as long as we know we can at\n\n13:59.400 --> 14:01.880\n least get to a certain level.\n\n14:01.880 --> 14:06.720\n And it also doesn't matter whether that's going to happen in 100 years or 5,000 years\n\n14:06.720 --> 14:08.440\n or 50 million years.\n\n14:08.440 --> 14:11.600\n Like the timescales really don't make any difference for this.\n\n14:11.600 --> 14:13.360\n Can you look on that a little bit?\n\n14:13.360 --> 14:19.080\n Like there's a big difference between 100 years and 10 million years.\n\n14:19.080 --> 14:25.500\n So it doesn't really not matter because you just said it doesn't matter if we jump scales\n\n14:25.500 --> 14:28.680\n to beyond historical scales.\n\n14:28.680 --> 14:30.200\n So we described that.\n\n14:30.200 --> 14:40.840\n So for the simulation argument, sort of doesn't it matter that we if it takes 10 million years,\n\n14:40.840 --> 14:44.720\n it gives us a lot more opportunity to destroy civilization in the meantime?\n\n14:44.720 --> 14:49.840\n Yeah, well, so it would shift around the probabilities between these three alternatives.\n\n14:49.840 --> 14:54.840\n That is, if we are very, very far away from being able to create these simulations, if\n\n14:54.840 --> 14:58.800\n it's like, say, billions of years into the future, then it's more likely that we will\n\n14:58.800 --> 14:59.800\n fail ever to get there.\n\n14:59.800 --> 15:04.080\n There's more time for us to kind of go extinct along the way.\n\n15:04.080 --> 15:06.360\n And so this is similarly for other civilizations.\n\n15:06.360 --> 15:11.000\n So it is important to think about how hard it is to build a simulation.\n\n15:11.000 --> 15:14.580\n In terms of figuring out which of the disjuncts.\n\n15:14.580 --> 15:19.360\n But for the simulation argument itself, which is agnostic as to which of these three alternatives\n\n15:19.360 --> 15:20.360\n is true.\n\n15:20.360 --> 15:21.360\n Yeah.\n\n15:21.360 --> 15:22.360\n Okay.\n\n15:22.360 --> 15:26.720\n It's like you don't have to like the simulation argument would be true whether or not we thought\n\n15:26.720 --> 15:29.840\n this could be done in 500 years or it would take 500 million years.\n\n15:29.840 --> 15:30.840\n No, for sure.\n\n15:30.840 --> 15:31.840\n The simulation argument stands.\n\n15:31.840 --> 15:36.200\n I mean, I'm sure there might be some people who oppose it, but it doesn't matter.\n\n15:36.200 --> 15:39.480\n I mean, it's very nice those three cases cover it.\n\n15:39.480 --> 15:44.240\n But the fun part is at least not saying what the probabilities are, but kind of thinking\n\n15:44.240 --> 15:50.480\n about kind of intuiting reasoning about what's more likely, what are the kind of things that\n\n15:50.480 --> 15:54.400\n would make some of the arguments less and more so like.\n\n15:54.400 --> 15:56.520\n But let's actually, I don't think we went through them.\n\n15:56.520 --> 16:00.440\n So number one is we destroy ourselves before we ever create simulation.\n\n16:00.440 --> 16:01.440\n Right.\n\n16:01.440 --> 16:07.960\n So that's kind of sad, but we have to think not just what might destroy us.\n\n16:07.960 --> 16:14.180\n I mean, so there could be some whatever disaster, some meteor slamming the earth a few years\n\n16:14.180 --> 16:16.120\n from now that could destroy us.\n\n16:16.120 --> 16:17.120\n Right.\n\n16:17.120 --> 16:24.880\n But you'd have to postulate in order for this first disjunct to be true that almost all\n\n16:24.880 --> 16:32.200\n civilizations throughout the cosmos also failed to reach technological maturity.\n\n16:32.200 --> 16:37.680\n And the underlying assumption there is that there is likely a very large number of other\n\n16:37.680 --> 16:39.520\n intelligent civilizations.\n\n16:39.520 --> 16:45.280\n Well, if there are, yeah, then they would virtually all have to succumb in the same\n\n16:45.280 --> 16:46.280\n way.\n\n16:46.280 --> 16:50.120\n I mean, then that leads off another, I guess there are a lot of little digressions that\n\n16:50.120 --> 16:51.120\n are interesting.\n\n16:51.120 --> 16:52.120\n Definitely, let's go there.\n\n16:52.120 --> 16:53.120\n Let's go there.\n\n16:53.120 --> 16:54.120\n Keep dragging us back.\n\n16:54.120 --> 16:58.940\n Well, there are these, there is a set of basic questions that always come up in conversations\n\n16:58.940 --> 17:05.500\n with interesting people, like the Fermi paradox, like there's like, you could almost define\n\n17:05.500 --> 17:09.920\n whether a person is interesting, whether at some point the question of the Fermi paradox\n\n17:09.920 --> 17:16.920\n comes up, like, well, so for what it's worth, it looks to me that the universe is very big.\n\n17:16.920 --> 17:23.160\n I mean, in fact, according to the most popular current cosmological theories, infinitely\n\n17:23.160 --> 17:25.320\n big.\n\n17:25.320 --> 17:31.420\n And so then it would follow pretty trivially that it would contain a lot of other civilizations,\n\n17:31.420 --> 17:34.120\n in fact, infinitely many.\n\n17:34.120 --> 17:39.660\n If you have some local stochasticity and infinitely many, it's like, you know, infinitely many\n\n17:39.660 --> 17:43.320\n lumps of matter, one next to another, there's kind of random stuff in each one, then you're\n\n17:43.320 --> 17:51.160\n going to get all possible outcomes with probability one infinitely repeated.\n\n17:51.160 --> 17:54.640\n So then certainly there would be a lot of extraterrestrials out there.\n\n17:54.640 --> 18:02.660\n Even short of that, if the universe is very big, that might be a finite but large number.\n\n18:02.660 --> 18:09.920\n If we were literally the only one, yeah, then of course, if we went extinct, then all of\n\n18:09.920 --> 18:14.400\n civilizations at our current stage would have gone extinct before becoming technological\n\n18:14.400 --> 18:15.400\n material.\n\n18:15.400 --> 18:22.560\n So then it kind of becomes trivially true that a very high fraction of those went extinct.\n\n18:22.560 --> 18:25.480\n But if we think there are many, I mean, it's interesting, because there are certain things\n\n18:25.480 --> 18:35.840\n that possibly could kill us, like if you look at existential risks, and it might be a different,\n\n18:35.840 --> 18:40.280\n like the best answer to what would be most likely to kill us might be a different answer\n\n18:40.280 --> 18:46.920\n than the best answer to the question, if there is something that kills almost everyone, what\n\n18:46.920 --> 18:47.920\n would that be?\n\n18:47.920 --> 18:53.400\n Because that would have to be some risk factor that was kind of uniform overall possible\n\n18:53.400 --> 18:54.400\n civilization.\n\n18:54.400 --> 18:59.760\n So in this, for the sake of this argument, you have to think about not just us, but like\n\n18:59.760 --> 19:05.160\n every civilization dies out before they create the simulation or something very close to\n\n19:05.160 --> 19:06.160\n everybody.\n\n19:06.160 --> 19:07.160\n Okay.\n\n19:07.160 --> 19:14.080\n So what's number two in the number two is the convergence hypothesis that is that maybe\n\n19:14.080 --> 19:18.760\n like a lot of some of these civilizations do make it through to technological maturity,\n\n19:18.760 --> 19:26.400\n but out of those who do get there, they all lose interest in creating these simulations.\n\n19:26.400 --> 19:32.960\n So they just have the capability of doing it, but they choose not to.\n\n19:32.960 --> 19:40.260\n Not just a few of them decide not to, but out of a million, maybe not even a single\n\n19:40.260 --> 19:41.760\n one of them would do it.\n\n19:41.760 --> 19:48.880\n And I think when you say lose interest, that sounds like unlikely because it's like they\n\n19:48.880 --> 19:53.680\n get bored or whatever, but it could be so many possibilities within that.\n\n19:53.680 --> 20:02.380\n I mean, losing interest could be, it could be anything from it being exceptionally difficult\n\n20:02.380 --> 20:09.440\n to do to fundamentally changing the sort of the fabric of reality.\n\n20:09.440 --> 20:14.400\n If you do it is ethical concerns, all those kinds of things could be exceptionally strong\n\n20:14.400 --> 20:15.400\n pressures.\n\n20:15.400 --> 20:18.440\n Well, certainly, I mean, yeah, ethical concerns.\n\n20:18.440 --> 20:21.160\n I mean, not really too difficult to do.\n\n20:21.160 --> 20:26.000\n I mean, in a sense, that's the first assumption that you get to technological maturity where\n\n20:26.000 --> 20:32.600\n you would have the ability using only a tiny fraction of your resources to create many,\n\n20:32.600 --> 20:34.520\n many simulations.\n\n20:34.520 --> 20:39.760\n So it wouldn't be the case that they would need to spend half of their GDP forever in\n\n20:39.760 --> 20:43.480\n order to create one simulation and they had this like difficult debate about whether they\n\n20:43.480 --> 20:46.400\n should invest half of their GDP for this.\n\n20:46.400 --> 20:50.320\n It would more be like, well, if any little fraction of the civilization feels like doing\n\n20:50.320 --> 20:57.000\n this at any point during maybe their millions of years of existence, then that would be\n\n20:57.000 --> 21:00.560\n millions of simulations.\n\n21:00.560 --> 21:07.800\n But certainly, there could be many conceivable reasons for why there would be this convert,\n\n21:07.800 --> 21:13.020\n many possible reasons for not running ancestor simulations or other computer simulations,\n\n21:13.020 --> 21:15.160\n even if you could do so cheaply.\n\n21:15.160 --> 21:17.160\n By the way, what's an ancestor simulation?\n\n21:17.160 --> 21:24.880\n Well, that would be the type of computer simulation that would contain people like those we think\n\n21:24.880 --> 21:30.060\n have lived on our planet in the past and like ourselves in terms of the types of experiences\n\n21:30.060 --> 21:33.560\n they have and where those simulated people are conscious.\n\n21:33.560 --> 21:41.400\n So like not just simulated in the same sense that a non player character would be simulated\n\n21:41.400 --> 21:45.560\n in the current computer game where it's kind of has like an avatar body and then a very\n\n21:45.560 --> 21:49.880\n simple mechanism that moves it forward or backwards.\n\n21:49.880 --> 21:56.880\n But something where the simulated being has a brain, let's say that's simulated at a sufficient\n\n21:56.880 --> 22:03.400\n level of granularity that it would have the same subjective experiences as we have.\n\n22:03.400 --> 22:06.400\n So where does consciousness fit into this?\n\n22:06.400 --> 22:10.380\n Do you think simulation, I guess there are different ways to think about how this can\n\n22:10.380 --> 22:14.320\n be simulated, just like you're talking about now.\n\n22:14.320 --> 22:21.040\n Do we have to simulate each brain within the larger simulation?\n\n22:21.040 --> 22:26.840\n Is it enough to simulate just the brain, just the minds and not the simulation, not the\n\n22:26.840 --> 22:27.840\n universe itself?\n\n22:27.840 --> 22:29.880\n Like, is there a different ways to think about this?\n\n22:29.880 --> 22:38.280\n Yeah, I guess there is a kind of premise in the simulation argument rolled in from philosophy\n\n22:38.280 --> 22:45.200\n of mind that is that it would be possible to create a conscious mind in a computer.\n\n22:45.200 --> 22:51.680\n And that what determines whether some system is conscious or not is not like whether it's\n\n22:51.680 --> 22:56.880\n built from organic biological neurons, but maybe something like what the structure of\n\n22:56.880 --> 22:59.680\n the computation is that it implements.\n\n22:59.680 --> 23:05.860\n So we can discuss that if we want, but I think it would be more forward as far as my view\n\n23:05.860 --> 23:15.140\n that it would be sufficient, say, if you had a computation that was identical to the computation\n\n23:15.140 --> 23:17.520\n in the human brain down to the level of neurons.\n\n23:17.520 --> 23:21.040\n So if you had a simulation with 100 billion neurons connected in the same way as the human\n\n23:21.040 --> 23:27.520\n brain, and you then roll that forward with the same kind of synaptic weights and so forth,\n\n23:27.520 --> 23:33.320\n so you actually had the same behavior coming out of this as a human with that brain would\n\n23:33.320 --> 23:36.000\n have done, then I think that would be conscious.\n\n23:36.000 --> 23:43.200\n Now it's possible you could also generate consciousness without having that detailed\n\n23:43.200 --> 23:50.080\n assimilation, there I'm getting more uncertain exactly how much you could simplify or abstract\n\n23:50.080 --> 23:51.080\n away.\n\n23:51.080 --> 23:52.080\n Can you look on that?\n\n23:52.080 --> 23:53.080\n What do you mean?\n\n23:53.080 --> 23:56.920\n I missed where you're placing consciousness in the second.\n\n23:56.920 --> 24:01.800\n Well, so if you are a computationalist, do you think that what creates consciousness\n\n24:01.800 --> 24:04.880\n is the implementation of a computation?\n\n24:04.880 --> 24:07.760\n Some property, emergent property of the computation itself.\n\n24:07.760 --> 24:08.760\n Yeah.\n\n24:08.760 --> 24:09.760\n That's the idea.\n\n24:09.760 --> 24:10.760\n Yeah, you could say that.\n\n24:10.760 --> 24:16.960\n But then the question is, what's the class of computations such that when they are run,\n\n24:16.960 --> 24:18.160\n consciousness emerges?\n\n24:18.160 --> 24:24.040\n So if you just have something that adds one plus one plus one plus one, like a simple\n\n24:24.040 --> 24:28.760\n computation, you think maybe that's not going to have any consciousness.\n\n24:28.760 --> 24:36.160\n If on the other hand, the computation is one like our human brains are performing, where\n\n24:36.160 --> 24:43.440\n as part of the computation, there is a global workspace, a sophisticated attention mechanism,\n\n24:43.440 --> 24:50.280\n there is self representations of other cognitive processes and a whole lot of other things\n\n24:50.280 --> 24:52.920\n that possibly would be conscious.\n\n24:52.920 --> 24:56.680\n And in fact, if it's exactly like ours, I think definitely it would.\n\n24:56.680 --> 25:02.880\n But exactly how much less than the full computation that the human brain is performing would be\n\n25:02.880 --> 25:09.320\n required is a little bit, I think, of an open question.\n\n25:09.320 --> 25:17.640\n He asked another interesting question as well, which is, would it be sufficient to just have\n\n25:17.640 --> 25:24.840\n say the brain or would you need the environment in order to generate the same kind of experiences\n\n25:24.840 --> 25:26.600\n that we have?\n\n25:26.600 --> 25:29.200\n And there is a bunch of stuff we don't know.\n\n25:29.200 --> 25:35.440\n I mean, if you look at, say, current virtual reality environments, one thing that's clear\n\n25:35.440 --> 25:40.800\n is that we don't have to simulate all details of them all the time in order for, say, the\n\n25:40.800 --> 25:47.000\n human player to have the perception that there is a full reality and that you can have say\n\n25:47.000 --> 25:51.160\n procedurally generated where you might only render a scene when it's actually within the\n\n25:51.160 --> 25:55.440\n view of the player character.\n\n25:55.440 --> 26:06.440\n And so similarly, if this environment that we perceive is simulated, it might be that\n\n26:06.440 --> 26:10.520\n all of the parts that come into our view are rendered at any given time.\n\n26:10.520 --> 26:16.680\n And a lot of aspects that never come into view, say the details of this microphone I'm\n\n26:16.680 --> 26:23.160\n talking into, exactly what each atom is doing at any given point in time, might not be part\n\n26:23.160 --> 26:27.200\n of the simulation, only a more coarse grained representation.\n\n26:27.200 --> 26:31.560\n So that to me is actually from an engineering perspective, why the simulation hypothesis\n\n26:31.560 --> 26:39.960\n is really interesting to think about is how difficult is it to fake sort of in a virtual\n\n26:39.960 --> 26:45.000\n reality context, I don't know if fake is the right word, but to construct a reality that\n\n26:45.000 --> 26:52.080\n is sufficiently real to us to be immersive in the way that the physical world is.\n\n26:52.080 --> 26:59.400\n I think that's actually probably an answerable question of psychology, of computer science,\n\n26:59.400 --> 27:06.600\n of how, where's the line where it becomes so immersive that you don't want to leave\n\n27:06.600 --> 27:07.600\n that world?\n\n27:07.600 --> 27:13.080\n Yeah, or that you don't realize while you're in it that it is a virtual world.\n\n27:13.080 --> 27:17.760\n Yeah, those are two actually questions, yours is the more sort of the good question about\n\n27:17.760 --> 27:23.120\n the realism, but mine, from my perspective, what's interesting is it doesn't have to be\n\n27:23.120 --> 27:29.480\n real, but how can we construct a world that we wouldn't want to leave?\n\n27:29.480 --> 27:34.440\n Yeah, I mean, I think that might be too low a bar, I mean, if you think, say when people\n\n27:34.440 --> 27:38.600\n first had pong or something like that, I'm sure there were people who wanted to keep\n\n27:38.600 --> 27:44.720\n playing it for a long time because it was fun and they wanted to be in this little world.\n\n27:44.720 --> 27:48.680\n I'm not sure we would say it's immersive, I mean, I guess in some sense it is, but like\n\n27:48.680 --> 27:51.360\n an absorbing activity doesn't even have to be.\n\n27:51.360 --> 27:54.240\n But they left that world though, that's the thing.\n\n27:54.240 --> 27:59.000\n So like, I think that bar is deceivingly high.\n\n27:59.000 --> 28:05.600\n So they eventually left, so you can play pong or Starcraft or whatever more sophisticated\n\n28:05.600 --> 28:12.360\n games for hours, for months, you know, while the work has to be in a big addiction, but\n\n28:12.360 --> 28:13.880\n eventually they escaped that.\n\n28:13.880 --> 28:19.560\n So you mean when it's absorbing enough that you would spend your entire, you would choose\n\n28:19.560 --> 28:21.160\n to spend your entire life in there.\n\n28:21.160 --> 28:28.520\n And then thereby changing the concept of what reality is, because your reality becomes the\n\n28:28.520 --> 28:29.620\n game.\n\n28:29.620 --> 28:33.320\n Not because you're fooled, but because you've made that choice.\n\n28:33.320 --> 28:38.880\n Yeah, and it made, different people might have different preferences regarding that.\n\n28:38.880 --> 28:47.640\n Some might, even if you had any perfect virtual reality, might still prefer not to spend the\n\n28:47.640 --> 28:49.160\n rest of their lives there.\n\n28:49.160 --> 28:53.880\n I mean, in philosophy, there's this experience machine, thought experiment.\n\n28:53.880 --> 28:55.920\n Have you come across this?\n\n28:55.920 --> 29:03.960\n So Robert Nozick had this thought experiment where you imagine some crazy super duper neuroscientist\n\n29:03.960 --> 29:08.160\n of the future have created a machine that could give you any experience you want if\n\n29:08.160 --> 29:10.280\n you step in there.\n\n29:10.280 --> 29:15.840\n And for the rest of your life, you can kind of pre programmed it in different ways.\n\n29:15.840 --> 29:24.240\n So your fun dreams could come true, you could, whatever you dream, you want to be a great\n\n29:24.240 --> 29:29.320\n artist, a great lover, like have a wonderful life, all of these things.\n\n29:29.320 --> 29:36.400\n If you step into the experience machine will be your experiences, constantly happy.\n\n29:36.400 --> 29:39.120\n But you would kind of disconnect from the rest of reality and you would float there\n\n29:39.120 --> 29:41.760\n in a tank.\n\n29:41.760 --> 29:48.960\n And so Nozick thought that most people would choose not to enter the experience machine.\n\n29:48.960 --> 29:51.880\n I mean, many might want to go there for a holiday, but they wouldn't want to have to\n\n29:51.880 --> 29:54.560\n check out of existence permanently.\n\n29:54.560 --> 30:01.480\n And so he thought that was an argument against certain views of value according to what we\n\n30:01.480 --> 30:04.760\n value is a function of what we experience.\n\n30:04.760 --> 30:08.560\n Because in the experience machine, you could have any experience you want, and yet many\n\n30:08.560 --> 30:12.000\n people would think that would not be much value.\n\n30:12.000 --> 30:18.600\n So therefore, what we value depends on other things than what we experience.\n\n30:18.600 --> 30:21.920\n So okay, can you can you take that argument further?\n\n30:21.920 --> 30:25.120\n What about the fact that maybe what we value is the up and down of life?\n\n30:25.120 --> 30:29.080\n So you could have up and downs in the experience machine, right?\n\n30:29.080 --> 30:31.080\n But what can't you have in the experience machine?\n\n30:31.080 --> 30:35.480\n Well, I mean, that then becomes an interesting question to explore.\n\n30:35.480 --> 30:40.480\n But for example, real connection with other people, if the experience machine is a solo\n\n30:40.480 --> 30:44.920\n machine where it's only you, like that's something you wouldn't have there.\n\n30:44.920 --> 30:49.440\n You would have this subjective experience that would be like fake people.\n\n30:49.440 --> 30:53.840\n But when if you gave somebody flowers, there wouldn't be anybody there who actually got\n\n30:53.840 --> 30:54.840\n happy.\n\n30:54.840 --> 30:58.480\n It would just be a little simulation of somebody smiling.\n\n30:58.480 --> 31:01.480\n But the simulation would not be the kind of simulation I'm talking about in the simulation\n\n31:01.480 --> 31:06.840\n argument where the simulated creature is conscious, it would just be a kind of smiley face that\n\n31:06.840 --> 31:08.600\n would look perfectly real to you.\n\n31:08.600 --> 31:14.720\n So we're now drawing a distinction between appear to be perfectly real and actually being\n\n31:14.720 --> 31:15.720\n real.\n\n31:15.720 --> 31:16.720\n Yeah.\n\n31:16.720 --> 31:22.040\n Um, so that could be one thing, I mean, like a big impact on history, maybe is also something\n\n31:22.040 --> 31:25.640\n you won't have if you check into this experience machine.\n\n31:25.640 --> 31:29.880\n So some people might actually feel the life I want to have for me is one where I have\n\n31:29.880 --> 31:35.520\n a big positive impact on history unfolds.\n\n31:35.520 --> 31:43.560\n So you could kind of explore these different possible explanations for why it is you wouldn't\n\n31:43.560 --> 31:48.000\n want to go into the experience machine if that's, if that's what you feel.\n\n31:48.000 --> 31:53.320\n And one interesting observation regarding this Nozick thought experiment and the conclusions\n\n31:53.320 --> 31:58.760\n he wanted to draw from it is how much is a kind of a status quo effect.\n\n31:58.760 --> 32:04.800\n So a lot of people might not want to get this on current reality to plug into this dream\n\n32:04.800 --> 32:06.160\n machine.\n\n32:06.160 --> 32:13.240\n But if they instead were told, well, what you've experienced up to this point was a\n\n32:13.240 --> 32:20.640\n dream now, do you want to disconnect from this and enter the real world when you have\n\n32:20.640 --> 32:24.880\n no idea maybe what the real world is, or maybe you could say, well, you're actually a farmer\n\n32:24.880 --> 32:32.280\n in Peru, growing, you know, peanuts, and you could live for the rest of your life in this\n\n32:32.280 --> 32:40.480\n way, or would you want to continue your dream life as Alex Friedman going around the world\n\n32:40.480 --> 32:44.080\n making podcasts and doing research.\n\n32:44.080 --> 32:51.320\n So if the status quo was that they were actually in the experience machine, I think a lot of\n\n32:51.320 --> 32:55.440\n people might then prefer to live the life that they are familiar with rather than sort\n\n32:55.440 --> 32:57.120\n of bail out into.\n\n32:57.120 --> 33:02.600\n So that's interesting, the change itself, the leap, yeah, so it might not be so much\n\n33:02.600 --> 33:04.440\n the reality itself that we're after.\n\n33:04.440 --> 33:09.000\n But it's more that we are maybe involved in certain projects and relationships.\n\n33:09.000 --> 33:14.040\n And we have, you know, a self identity and these things that our values are kind of connected\n\n33:14.040 --> 33:15.880\n with carrying that forward.\n\n33:15.880 --> 33:22.760\n And then whether it's inside a tank or outside a tank in Peru, or whether inside a computer\n\n33:22.760 --> 33:29.120\n outside a computer, that's kind of less important to what we ultimately care about.\n\n33:29.120 --> 33:34.720\n Yeah, but still, so just to linger on it, it is interesting.\n\n33:34.720 --> 33:39.600\n I find maybe people are different, but I find myself quite willing to take the leap to the\n\n33:39.600 --> 33:46.800\n farmer in Peru, especially as the virtual reality system become more realistic.\n\n33:46.800 --> 33:50.760\n I find that possibility and I think more people would take that leap.\n\n33:50.760 --> 33:53.640\n But so in this thought experiment, just to make sure we are understanding, so in this\n\n33:53.640 --> 34:01.280\n case, the farmer in Peru would not be a virtual reality, that would be the real, your life,\n\n34:01.280 --> 34:04.400\n like before this whole experience machine started.\n\n34:04.400 --> 34:09.560\n Well, I kind of assumed from that description, you're being very specific, but that kind\n\n34:09.560 --> 34:15.320\n of idea just like washes away the concept of what's real.\n\n34:15.320 --> 34:23.320\n I'm still a little hesitant about your kind of distinction between real and illusion.\n\n34:23.320 --> 34:31.080\n Because when you can have an illusion that feels, I mean, that looks real, I don't know\n\n34:31.080 --> 34:35.320\n how you can definitively say something is real or not, like what's a good way to prove\n\n34:35.320 --> 34:37.600\n that something is real in that context?\n\n34:37.600 --> 34:41.040\n Well, so I guess in this case, it's more a stipulation.\n\n34:41.040 --> 34:47.400\n In one case, you're floating in a tank with these wires by the super duper neuroscientists\n\n34:47.400 --> 34:52.440\n plugging into your head, giving you like Friedman experiences.\n\n34:52.440 --> 34:57.120\n In the other, you're actually tilling the soil in Peru, growing peanuts, and then those\n\n34:57.120 --> 35:01.440\n peanuts are being eaten by other people all around the world who buy the exports.\n\n35:01.440 --> 35:08.600\n That's two different possible situations in the one and the same real world that you could\n\n35:08.600 --> 35:09.600\n choose to occupy.\n\n35:09.600 --> 35:15.400\n But just to be clear, when you're in a vat with wires and the neuroscientists, you can\n\n35:15.400 --> 35:19.000\n still go farming in Peru, right?\n\n35:19.000 --> 35:25.120\n No, well, if you wanted to, you could have the experience of farming in Peru, but there\n\n35:25.120 --> 35:28.760\n wouldn't actually be any peanuts grown.\n\n35:28.760 --> 35:36.560\n But what makes a peanut, so a peanut could be grown and you could feed things with that\n\n35:36.560 --> 35:41.600\n peanut and why can't all of that be done in a simulation?\n\n35:41.600 --> 35:45.760\n I hope, first of all, that they actually have peanut farms in Peru, I guess we'll get a\n\n35:45.760 --> 35:50.000\n lot of comments otherwise from Angrit.\n\n35:50.000 --> 35:54.160\n I was way up to the point when you started talking about Peru peanuts, that's when I\n\n35:54.160 --> 35:56.200\n realized you're relying out of these.\n\n35:56.200 --> 35:57.200\n In that climate.\n\n35:57.200 --> 36:05.120\n No, I mean, I think, I mean, in the simulation, I think there is a sense, the important sense\n\n36:05.120 --> 36:07.200\n in which it would all be real.\n\n36:07.200 --> 36:13.720\n Nevertheless, there is a distinction between inside the simulation and outside the simulation.\n\n36:13.720 --> 36:19.680\n Or in the case of Nozick's thought experiment, whether you're in the vat or outside the vat,\n\n36:19.680 --> 36:22.440\n and some of those differences may or may not be important.\n\n36:22.440 --> 36:25.520\n I mean, that comes down to your values and preferences.\n\n36:25.520 --> 36:32.840\n So if the, if the experience machine only gives you the experience of growing peanuts,\n\n36:32.840 --> 36:35.680\n but you're the only one in the experience machines.\n\n36:35.680 --> 36:40.400\n No, but there's other, you can, within the experience machine, others can plug in.\n\n36:40.400 --> 36:43.840\n Well, there are versions of the experience machine.\n\n36:43.840 --> 36:47.520\n So in fact, you might want to have, distinguish different thought experiments, different versions\n\n36:47.520 --> 36:48.520\n of it.\n\n36:48.520 --> 36:49.520\n I see.\n\n36:49.520 --> 36:51.840\n So in, like in the original thought experiment, maybe it's only you, right?\n\n36:51.840 --> 36:54.480\n And you think, I wouldn't want to go in there.\n\n36:54.480 --> 36:58.200\n Well, that tells you something interesting about what you value and what you care about.\n\n36:58.200 --> 37:02.000\n Then you could say, well, what if you add the fact that there would be other people\n\n37:02.000 --> 37:03.440\n in there and you would interact with them?\n\n37:03.440 --> 37:06.920\n Well, it starts to make it more attractive, right?\n\n37:06.920 --> 37:10.920\n Then you could add in, well, what if you could also have important longterm effects on human\n\n37:10.920 --> 37:14.840\n history and the world, and you could actually do something useful, even though you were\n\n37:14.840 --> 37:15.840\n in there.\n\n37:15.840 --> 37:17.760\n That makes it maybe even more attractive.\n\n37:17.760 --> 37:22.480\n Like you could actually have a life that had a purpose and consequences.\n\n37:22.480 --> 37:30.760\n And so as you sort of add more into it, it becomes more similar to the baseline reality\n\n37:30.760 --> 37:32.920\n that you were comparing it to.\n\n37:32.920 --> 37:37.840\n Yeah, but I just think inside the experience machine and without taking those steps you\n\n37:37.840 --> 37:45.720\n just mentioned, you still have an impact on longterm history of the creatures that live\n\n37:45.720 --> 37:53.360\n inside that, of the quote unquote fake creatures that live inside that experience machine.\n\n37:53.360 --> 37:59.800\n And that, like at a certain point, you know, if there's a person waiting for you inside\n\n37:59.800 --> 38:06.920\n that experience machine, maybe your newly found wife and she dies, she has fear, she\n\n38:06.920 --> 38:12.900\n has hopes, and she exists in that machine when you plug out, when you unplug yourself\n\n38:12.900 --> 38:16.080\n and plug back in, she's still there going on about her life.\n\n38:16.080 --> 38:20.640\n Well, in that case, yeah, she starts to have more of an independent existence.\n\n38:20.640 --> 38:21.640\n Independent existence.\n\n38:21.640 --> 38:26.680\n But it depends, I think, on how she's implemented in the experience machine.\n\n38:26.680 --> 38:32.480\n Take one limit case where all she is is a static picture on the wall, a photograph.\n\n38:32.480 --> 38:36.060\n So you think, well, I can look at her, right?\n\n38:36.060 --> 38:37.240\n But that's it.\n\n38:37.240 --> 38:38.240\n There's no...\n\n38:38.240 --> 38:41.960\n Then you think, well, it doesn't really matter much what happens to that, any more than a\n\n38:41.960 --> 38:45.080\n normal photograph if you tear it up, right?\n\n38:45.080 --> 38:49.300\n It means you can't see it anymore, but you haven't harmed the person whose picture you\n\n38:49.300 --> 38:52.300\n tore up.\n\n38:52.300 --> 38:58.120\n But if she's actually implemented, say, at a neural level of detail so that she's a fully\n\n38:58.120 --> 39:06.120\n realized digital mind with the same behavioral repertoire as you have, then very plausibly\n\n39:06.120 --> 39:09.240\n she would be a conscious person like you are.\n\n39:09.240 --> 39:14.220\n And then what you do in this experience machine would have real consequences for how this\n\n39:14.220 --> 39:17.680\n other mind felt.\n\n39:17.680 --> 39:21.100\n So you have to specify which of these experience machines you're talking about.\n\n39:21.100 --> 39:27.920\n I think it's not entirely obvious that it would be possible to have an experience machine\n\n39:27.920 --> 39:34.240\n that gave you a normal set of human experiences, which include experiences of interacting with\n\n39:34.240 --> 39:40.560\n other people, without that also generating consciousnesses corresponding to those other\n\n39:40.560 --> 39:41.560\n people.\n\n39:41.560 --> 39:47.480\n That is, if you create another entity that you perceive and interact with, that to you\n\n39:47.480 --> 39:49.320\n looks entirely realistic.\n\n39:49.320 --> 39:53.160\n Not just when you say hello, they say hello back, but you have a rich interaction, many\n\n39:53.160 --> 39:54.840\n days, deep conversations.\n\n39:54.840 --> 40:00.960\n It might be that the only possible way of implementing that would be one that also has\n\n40:00.960 --> 40:06.600\n a side effect, instantiated this other person in enough detail that you would have a second\n\n40:06.600 --> 40:07.680\n consciousness there.\n\n40:07.680 --> 40:11.800\n I think that's to some extent an open question.\n\n40:11.800 --> 40:15.040\n So you don't think it's possible to fake consciousness and fake intelligence?\n\n40:15.040 --> 40:16.040\n Well, it might be.\n\n40:16.040 --> 40:21.340\n I mean, I think you can certainly fake, if you have a very limited interaction with somebody,\n\n40:21.340 --> 40:24.320\n you could certainly fake that.\n\n40:24.320 --> 40:28.320\n If all you have to go on is somebody said hello to you, that's not enough for you to\n\n40:28.320 --> 40:34.880\n tell whether that was a real person there, or a prerecorded message, or a very superficial\n\n40:34.880 --> 40:39.280\n simulation that has no consciousness, because that's something easy to fake.\n\n40:39.280 --> 40:43.720\n We could already fake it, now you can record a voice recording.\n\n40:43.720 --> 40:49.160\n But if you have a richer set of interactions where you're allowed to ask open ended questions\n\n40:49.160 --> 40:54.920\n and probe from different angles, you couldn't give canned answer to all of the possible\n\n40:54.920 --> 41:00.280\n ways that you could probe it, then it starts to become more plausible that the only way\n\n41:00.280 --> 41:05.160\n to realize this thing in such a way that you would get the right answer from any which\n\n41:05.160 --> 41:10.040\n angle you probed it, would be a way of instantiating it, where you also instantiated a conscious\n\n41:10.040 --> 41:11.040\n mind.\n\n41:11.040 --> 41:13.960\n Yeah, I'm with you on the intelligence part, but is there something about me that says\n\n41:13.960 --> 41:15.960\n consciousness is easier to fake?\n\n41:15.960 --> 41:23.080\n Like I've recently gotten my hands on a lot of rubas, don't ask me why or how.\n\n41:23.080 --> 41:28.540\n And I've made them, there's just a nice robotic mobile platform for experiments.\n\n41:28.540 --> 41:34.560\n And I made them scream and or moan in pain, so on, just to see when they're responding\n\n41:34.560 --> 41:35.560\n to me.\n\n41:35.560 --> 41:39.240\n And it's just a sort of psychological experiment on myself.\n\n41:39.240 --> 41:43.120\n And I think they appear conscious to me pretty quickly.\n\n41:43.120 --> 41:46.720\n To me, at least my brain can be tricked quite easily.\n\n41:46.720 --> 41:53.760\n I said if I introspect, it's harder for me to be tricked that something is intelligent.\n\n41:53.760 --> 41:58.860\n So I just have this feeling that inside this experience machine, just saying that you're\n\n41:58.860 --> 42:05.000\n conscious and having certain qualities of the interaction, like being able to suffer,\n\n42:05.000 --> 42:12.040\n like being able to hurt, like being able to wander about the essence of your own existence,\n\n42:12.040 --> 42:18.040\n not actually, I mean, creating the illusion that you're wandering about it is enough to\n\n42:18.040 --> 42:23.120\n create the illusion of consciousness.\n\n42:23.120 --> 42:27.440\n And because of that, create a really immersive experience to where you feel like that is\n\n42:27.440 --> 42:28.440\n the real world.\n\n42:28.440 --> 42:33.260\n So you think there's a big gap between appearing conscious and being conscious?\n\n42:33.260 --> 42:36.080\n Or is it that you think it's very easy to be conscious?\n\n42:36.080 --> 42:38.120\n I'm not actually sure what it means to be conscious.\n\n42:38.120 --> 42:48.200\n All I'm saying is the illusion of consciousness is enough to create a social interaction that's\n\n42:48.200 --> 42:52.480\n as good as if the thing was conscious, meaning I'm making it about myself.\n\n42:52.480 --> 42:53.480\n Right.\n\n42:53.480 --> 42:54.480\n Yeah.\n\n42:54.480 --> 42:55.480\n I mean, I guess there are a few different things.\n\n42:55.480 --> 42:59.740\n One is how good the interaction is, which might, I mean, if you don't really care about\n\n42:59.740 --> 43:05.080\n like probing hard for whether the thing is conscious, maybe it would be a satisfactory\n\n43:05.080 --> 43:10.720\n interaction, whether or not you really thought it was conscious.\n\n43:10.720 --> 43:20.040\n Now, if you really care about it being conscious in like inside this experience machine, how\n\n43:20.040 --> 43:22.340\n easy would it be to fake it?\n\n43:22.340 --> 43:28.000\n And you say, it sounds fairly easy, but then the question is, would that also mean it's\n\n43:28.000 --> 43:30.600\n very easy to instantiate consciousness?\n\n43:30.600 --> 43:35.440\n Like it's much more widely spread in the world and we have thought it doesn't require a big\n\n43:35.440 --> 43:39.600\n human brain with a hundred billion neurons, all you need is some system that exhibits\n\n43:39.600 --> 43:43.300\n basic intentionality and can respond and you already have consciousness.\n\n43:43.300 --> 43:49.080\n Like in that case, I guess you still have a close coupling.\n\n43:49.080 --> 43:54.600\n I guess that case would be where they can come apart, where you could create the appearance\n\n43:54.600 --> 43:59.200\n of there being a conscious mind with actually not being another conscious mind.\n\n43:59.200 --> 44:03.320\n I'm somewhat agnostic exactly where these lines go.\n\n44:03.320 --> 44:12.280\n I think one observation that makes it plausible that you could have very realistic appearances\n\n44:12.280 --> 44:18.320\n relatively simply, which also is relevant for the simulation argument and in terms of\n\n44:18.320 --> 44:24.400\n thinking about how realistic would a virtual reality model have to be in order for the\n\n44:24.400 --> 44:27.960\n simulated creature not to notice that anything was awry.\n\n44:27.960 --> 44:33.960\n Well, just think of our own humble brains during the wee hours of the night when we\n\n44:33.960 --> 44:35.400\n are dreaming.\n\n44:35.400 --> 44:40.560\n Many times, well, dreams are very immersive, but often you also don't realize that you're\n\n44:40.560 --> 44:43.440\n in a dream.\n\n44:43.440 --> 44:51.320\n And that's produced by simple primitive three pound lumps of neural matter effortlessly.\n\n44:51.320 --> 44:57.160\n So if a simple brain like this can create the virtual reality that seems pretty real\n\n44:57.160 --> 45:03.120\n to us, then how much easier would it be for a super intelligent civilization with planetary\n\n45:03.120 --> 45:09.760\n sized computers optimized over the eons to create a realistic environment for you to\n\n45:09.760 --> 45:10.760\n interact with?\n\n45:10.760 --> 45:11.760\n Yeah.\n\n45:11.760 --> 45:17.720\n By the way, behind that intuition is that our brain is not that impressive relative\n\n45:17.720 --> 45:21.280\n to the possibilities of what technology could bring.\n\n45:21.280 --> 45:26.820\n It's also possible that the brain is the epitome, is the ceiling.\n\n45:26.820 --> 45:30.960\n How is that possible?\n\n45:30.960 --> 45:36.240\n Meaning like this is the smartest possible thing that the universe could create.\n\n45:36.240 --> 45:39.800\n So that seems unlikely to me.\n\n45:39.800 --> 45:40.800\n Yeah.\n\n45:40.800 --> 45:47.600\n I mean, for some of these reasons we alluded to earlier in terms of designs we already\n\n45:47.600 --> 45:54.960\n have for computers that would be faster by many orders of magnitude than the human brain.\n\n45:54.960 --> 45:55.960\n Yeah.\n\n45:55.960 --> 46:01.120\n We can see that the constraints, the cognitive constraints in themselves is what enables\n\n46:01.120 --> 46:02.440\n the intelligence.\n\n46:02.440 --> 46:09.420\n So the more powerful you make the computer, the less likely it is to become super intelligent.\n\n46:09.420 --> 46:12.120\n This is where I say dumb things to push back on that statement.\n\n46:12.120 --> 46:13.120\n Yeah.\n\n46:13.120 --> 46:14.120\n I'm not sure I thought that we might.\n\n46:14.120 --> 46:15.120\n No.\n\n46:15.120 --> 46:18.120\n I mean, so there are different dimensions of intelligence.\n\n46:18.120 --> 46:20.220\n A simple one is just speed.\n\n46:20.220 --> 46:25.360\n Like if you can solve the same challenge faster in some sense, you're like smarter.\n\n46:25.360 --> 46:31.560\n So there I think we have very strong evidence for thinking that you could have a computer\n\n46:31.560 --> 46:37.880\n in this universe that would be much faster than the human brain and therefore have speed\n\n46:37.880 --> 46:42.920\n super intelligence, like be completely superior, maybe a million times faster.\n\n46:42.920 --> 46:46.960\n Then maybe there are other ways in which you could be smarter as well, maybe more qualitative\n\n46:46.960 --> 46:48.680\n ways, right?\n\n46:48.680 --> 46:51.840\n And the concepts are a little bit less clear cut.\n\n46:51.840 --> 46:59.640\n So it's harder to make a very crisp, neat, firmly logical argument for why that could\n\n46:59.640 --> 47:03.240\n be qualitative super intelligence as opposed to just things that were faster.\n\n47:03.240 --> 47:08.040\n Although I still think it's very plausible and for various reasons that are less than\n\n47:08.040 --> 47:09.240\n watertight arguments.\n\n47:09.240 --> 47:14.680\n But when you can sort of, for example, if you look at animals and even within humans,\n\n47:14.680 --> 47:19.760\n like there seems to be like Einstein versus random person, like it's not just that Einstein\n\n47:19.760 --> 47:25.000\n was a little bit faster, but like how long would it take a normal person to invent general\n\n47:25.000 --> 47:30.080\n relativity is like, it's not 20% longer than it took Einstein or something like that.\n\n47:30.080 --> 47:32.920\n It's like, I don't know whether they would do it at all or it would take millions of\n\n47:32.920 --> 47:37.320\n years or some totally bizarre.\n\n47:37.320 --> 47:42.600\n But your intuition is that the compute size will get you go increasing the size of the\n\n47:42.600 --> 47:49.560\n computer and the speed of the computer might create some much more powerful levels of intelligence\n\n47:49.560 --> 47:53.560\n that would enable some of the things we've been talking about with like the simulation,\n\n47:53.560 --> 48:00.760\n being able to simulate an ultra realistic environment, ultra realistic perception of\n\n48:00.760 --> 48:01.760\n reality.\n\n48:01.760 --> 48:02.760\n Yeah.\n\n48:02.760 --> 48:05.720\n I mean, strictly speaking, it would not be necessary to have super intelligence in order\n\n48:05.720 --> 48:14.280\n to have say the technology to make these simulations, ancestor simulations or other kinds of simulations.\n\n48:14.280 --> 48:20.800\n As a matter of fact, I think if we are in a simulation, it would most likely be one\n\n48:20.800 --> 48:26.280\n built by a civilization that had super intelligence.\n\n48:26.280 --> 48:27.560\n It certainly would help a lot.\n\n48:27.560 --> 48:31.400\n I mean, you could build more efficient larger scale structures if you had super intelligence.\n\n48:31.400 --> 48:34.960\n I also think that if you had the technology to build these simulations, that's like a\n\n48:34.960 --> 48:35.960\n very advanced technology.\n\n48:35.960 --> 48:40.520\n It seems kind of easier to get the technology to super intelligence.\n\n48:40.520 --> 48:45.280\n I'd expect by the time they could make these fully realistic simulations of human history\n\n48:45.280 --> 48:49.160\n with human brains in there, like before that they got to that stage, they would have figured\n\n48:49.160 --> 48:55.520\n out how to create machine super intelligence or maybe biological enhancements of their\n\n48:55.520 --> 48:59.200\n own brains if there were biological creatures to start with.\n\n48:59.200 --> 49:04.240\n So we talked about the three parts of the simulation argument.\n\n49:04.240 --> 49:08.480\n One, we destroy ourselves before we ever create the simulation.\n\n49:08.480 --> 49:13.200\n Two, we somehow, everybody somehow loses interest in creating the simulation.\n\n49:13.200 --> 49:16.280\n Three, we're living in a simulation.\n\n49:16.280 --> 49:21.760\n So you've kind of, I don't know if your thinking has evolved on this point, but you kind of\n\n49:21.760 --> 49:28.320\n said that we know so little that these three cases might as well be equally probable.\n\n49:28.320 --> 49:31.720\n So probabilistically speaking, where do you stand on this?\n\n49:31.720 --> 49:41.280\n Yeah, I mean, I don't think equal necessarily would be the most supported probability assignment.\n\n49:41.280 --> 49:47.280\n So how would you, without assigning actual numbers, what's more or less likely in your\n\n49:47.280 --> 49:48.280\n view?\n\n49:48.280 --> 49:54.600\n Well, I mean, I've historically tended to punt on the question of like between these\n\n49:54.600 --> 49:55.600\n three.\n\n49:55.600 --> 50:01.640\n So maybe you ask me another way is which kind of things would make each of these more or\n\n50:01.640 --> 50:03.440\n less likely?\n\n50:03.440 --> 50:05.200\n What kind of intuition?\n\n50:05.200 --> 50:10.960\n Certainly in general terms, if you think anything that say increases or reduces the probability\n\n50:10.960 --> 50:17.040\n of one of these, we tend to slosh probability around on the other.\n\n50:17.040 --> 50:20.600\n So if one becomes less probable, like the other would have to, cause it's got to add\n\n50:20.600 --> 50:22.000\n up to one.\n\n50:22.000 --> 50:28.960\n So if we consider the first hypothesis, the first alternative that there's this filter\n\n50:28.960 --> 50:39.160\n that makes it so that virtually no civilization reaches technological maturity, in particular\n\n50:39.160 --> 50:42.440\n our own civilization, if that's true, then it's like very unlikely that we would reach\n\n50:42.440 --> 50:47.600\n technological maturity because if almost no civilization at our stage does it, then it's\n\n50:47.600 --> 50:49.120\n unlikely that we do it.\n\n50:49.120 --> 50:50.120\n So hence...\n\n50:50.120 --> 50:51.120\n Sorry, can you linger on that for a second?\n\n50:51.120 --> 50:59.000\n Well, so if it's the case that almost all civilizations at our current stage of technological\n\n50:59.000 --> 51:05.280\n development failed to reach maturity, that would give us very strong reason for thinking\n\n51:05.280 --> 51:07.480\n we will fail to reach technological maturity.\n\n51:07.480 --> 51:12.000\n Oh, and also sort of the flip side of that is the fact that we've reached it means that\n\n51:12.000 --> 51:13.680\n many other civilizations have reached this point.\n\n51:13.680 --> 51:14.680\n Yeah.\n\n51:14.680 --> 51:20.200\n So that means if we get closer and closer to actually reaching technological maturity,\n\n51:20.200 --> 51:26.200\n there's less and less distance left where we could go extinct before we are there, and\n\n51:26.200 --> 51:31.520\n therefore the probability that we will reach increases as we get closer, and that would\n\n51:31.520 --> 51:36.120\n make it less likely to be true that almost all civilizations at our current stage failed\n\n51:36.120 --> 51:37.120\n to get there.\n\n51:37.120 --> 51:38.880\n Like we would have this...\n\n51:38.880 --> 51:42.960\n The one case we had started ourselves would be very close to getting there, that would\n\n51:42.960 --> 51:46.440\n be strong evidence that it's not so hard to get to technological maturity.\n\n51:46.440 --> 51:52.800\n So to the extent that we feel we are moving nearer to technological maturity, that would\n\n51:52.800 --> 51:58.000\n tend to reduce the probability of the first alternative and increase the probability of\n\n51:58.000 --> 51:59.600\n the other two.\n\n51:59.600 --> 52:01.960\n It doesn't need to be a monotonic change.\n\n52:01.960 --> 52:07.440\n Like if every once in a while some new threat comes into view, some bad new thing you could\n\n52:07.440 --> 52:13.460\n do with some novel technology, for example, that could change our probabilities in the\n\n52:13.460 --> 52:15.160\n other direction.\n\n52:15.160 --> 52:20.840\n But that technology, again, you have to think about as that technology has to be able to\n\n52:20.840 --> 52:26.400\n equally in an even way affect every civilization out there.\n\n52:26.400 --> 52:28.120\n Yeah, pretty much.\n\n52:28.120 --> 52:30.760\n I mean, that's strictly speaking, it's not true.\n\n52:30.760 --> 52:36.800\n I mean, that could be two different existential risks and every civilization, you know, one\n\n52:36.800 --> 52:42.440\n or the other, like, but none of them kills more than 50%.\n\n52:42.440 --> 52:50.240\n But incidentally, so in some of my work, I mean, on machine superintelligence, like pointed\n\n52:50.240 --> 52:54.440\n to some existential risks related to sort of super intelligent AI and how we must make\n\n52:54.440 --> 52:59.820\n sure, you know, to handle that wisely and carefully.\n\n52:59.820 --> 53:09.880\n It's not the right kind of existential catastrophe to make the first alternative true though.\n\n53:09.880 --> 53:15.480\n Like it might be bad for us if the future lost a lot of value as a result of it being\n\n53:15.480 --> 53:21.160\n shaped by some process that optimized for some completely nonhuman value.\n\n53:21.160 --> 53:27.920\n But even if we got killed by machine superintelligence, that machine superintelligence might still\n\n53:27.920 --> 53:29.520\n attain technological maturity.\n\n53:29.520 --> 53:33.560\n Oh, I see, so you're not human exclusive.\n\n53:33.560 --> 53:38.360\n This could be any intelligent species that achieves, like it's all about the technological\n\n53:38.360 --> 53:39.360\n maturity.\n\n53:39.360 --> 53:43.040\n But the humans have to attain it.\n\n53:43.040 --> 53:44.040\n Right.\n\n53:44.040 --> 53:47.320\n So like superintelligence could replace us and that's just as well for the simulation\n\n53:47.320 --> 53:48.320\n argument.\n\n53:48.320 --> 53:49.320\n Yeah, yeah.\n\n53:49.320 --> 53:51.800\n I mean, it could interact with the second hypothesis by alternative.\n\n53:51.800 --> 53:57.120\n Like if the thing that replaced us was either more likely or less likely than we would be\n\n53:57.120 --> 54:02.840\n to have an interest in creating ancestor simulations, you know, that could affect probabilities.\n\n54:02.840 --> 54:09.840\n But yeah, to a first order, like if we all just die, then yeah, we won't produce any\n\n54:09.840 --> 54:11.920\n simulations because we are dead.\n\n54:11.920 --> 54:17.560\n But if we all die and get replaced by some other intelligent thing that then gets to\n\n54:17.560 --> 54:21.680\n technological maturity, the question remains, of course, if not that thing, then use some\n\n54:21.680 --> 54:25.280\n of its resources to do this stuff.\n\n54:25.280 --> 54:30.760\n So can you reason about this stuff, given how little we know about the universe?\n\n54:30.760 --> 54:36.760\n Is it reasonable to reason about these probabilities?\n\n54:36.760 --> 54:45.200\n So like how little, well, maybe you can disagree, but to me, it's not trivial to figure out\n\n54:45.200 --> 54:47.520\n how difficult it is to build a simulation.\n\n54:47.520 --> 54:49.640\n We kind of talked about it a little bit.\n\n54:49.640 --> 54:56.080\n We also don't know, like as we try to start building it, like start creating virtual worlds\n\n54:56.080 --> 54:59.640\n and so on, how that changes the fabric of society.\n\n54:59.640 --> 55:04.560\n Like there's all these things along the way that can fundamentally change just so many\n\n55:04.560 --> 55:09.480\n aspects of our society about our existence that we don't know anything about, like the\n\n55:09.480 --> 55:19.380\n kind of things we might discover when we understand to a greater degree the fundamental, the physics,\n\n55:19.380 --> 55:23.360\n like the theory, if we have a breakthrough, have a theory and everything, how that changes\n\n55:23.360 --> 55:27.600\n stuff, how that changes deep space exploration and so on.\n\n55:27.600 --> 55:33.040\n Like, is it still possible to reason about probabilities given how little we know?\n\n55:33.040 --> 55:41.960\n Yes, I think there will be a large residual of uncertainty that we'll just have to acknowledge.\n\n55:41.960 --> 55:47.880\n And I think that's true for most of these big picture questions that we might wonder\n\n55:47.880 --> 55:49.840\n about.\n\n55:49.840 --> 55:57.840\n It's just we are small, short lived, small brained, cognitively very limited humans with\n\n55:57.840 --> 55:59.520\n little evidence.\n\n55:59.520 --> 56:04.760\n And it's amazing we can figure out as much as we can really about the cosmos.\n\n56:04.760 --> 56:10.960\n But okay, so there's this cognitive trick that seems to happen when I look at the simulation\n\n56:10.960 --> 56:16.360\n argument, which for me, it seems like case one and two feel unlikely.\n\n56:16.360 --> 56:22.080\n I want to say feel unlikely as opposed to sort of like, it's not like I have too much\n\n56:22.080 --> 56:26.980\n scientific evidence to say that either one or two are not true.\n\n56:26.980 --> 56:32.400\n It just seems unlikely that every single civilization destroys itself.\n\n56:32.400 --> 56:37.160\n And it seems like feels unlikely that the civilizations lose interest.\n\n56:37.160 --> 56:44.660\n So naturally, without necessarily explicitly doing it, but the simulation argument basically\n\n56:44.660 --> 56:49.080\n says it's very likely we're living in a simulation.\n\n56:49.080 --> 56:51.800\n To me, my mind naturally goes there.\n\n56:51.800 --> 56:54.860\n I think the mind goes there for a lot of people.\n\n56:54.860 --> 56:57.400\n Is that the incorrect place for it to go?\n\n56:57.400 --> 56:59.160\n Well, not necessarily.\n\n56:59.160 --> 57:09.040\n I think the second alternative, which has to do with the motivations and interests of\n\n57:09.040 --> 57:15.160\n technological and material civilizations, I think there is much we don't understand about\n\n57:15.160 --> 57:16.160\n that.\n\n57:16.160 --> 57:18.440\n Can you talk about that a little bit?\n\n57:18.440 --> 57:19.440\n What do you think?\n\n57:19.440 --> 57:22.940\n I mean, this is a question that pops up when you when you build an AGI system or build\n\n57:22.940 --> 57:26.320\n a general intelligence.\n\n57:26.320 --> 57:27.880\n How does that change our motivations?\n\n57:27.880 --> 57:30.800\n Do you think it'll fundamentally transform our motivations?\n\n57:30.800 --> 57:39.280\n Well, it doesn't seem that implausible that once you take this leap to to technological\n\n57:39.280 --> 57:44.920\n maturity, I mean, I think like it involves creating machine super intelligence, possibly\n\n57:44.920 --> 57:50.840\n that would be sort of on the path for basically all civilizations, maybe before they are able\n\n57:50.840 --> 57:55.840\n to create large numbers of ancestry simulations, they would that that possibly could be one\n\n57:55.840 --> 58:03.240\n of these things that quite radically changes the orientation of what a civilization is,\n\n58:03.240 --> 58:06.400\n in fact, optimizing for.\n\n58:06.400 --> 58:08.760\n There are other things as well.\n\n58:08.760 --> 58:20.240\n So at the moment, we have not perfect control over our own being our own mental states,\n\n58:20.240 --> 58:25.240\n our own experiences are not under our direct control.\n\n58:25.240 --> 58:33.840\n So for example, if if you want to experience a pleasure and happiness, you might have to\n\n58:33.840 --> 58:39.720\n do a whole host of things in the external world to try to get into the stage into the\n\n58:39.720 --> 58:44.240\n mental state where you experience pleasure, like some people get some pleasure from eating\n\n58:44.240 --> 58:45.240\n great food.\n\n58:45.240 --> 58:49.960\n Well, they can just turn that on, they have to kind of actually go to a nice restaurant\n\n58:49.960 --> 58:51.400\n and then they have to make money.\n\n58:51.400 --> 58:58.120\n So there's like all this kind of activity that maybe arises from the fact that we are\n\n58:58.120 --> 59:02.120\n trying to ultimately produce mental states.\n\n59:02.120 --> 59:06.560\n But the only way to do that is by a whole host of complicated activities in the external\n\n59:06.560 --> 59:07.560\n world.\n\n59:07.560 --> 59:11.560\n Now, at some level of technological development, I think we'll become auto potent in the sense\n\n59:11.560 --> 59:18.920\n of gaining direct ability to choose our own internal configuration, and enough knowledge\n\n59:18.920 --> 59:22.840\n and insight to be able to actually do that in a meaningful way.\n\n59:22.840 --> 59:28.160\n So then it could turn out that there are a lot of instrumental goals that would drop\n\n59:28.160 --> 59:33.420\n out of the picture and be replaced by other instrumental goals, because we could now serve\n\n59:33.420 --> 59:37.200\n some of these final goals in more direct ways.\n\n59:37.200 --> 59:45.500\n And who knows how all of that shakes out after civilizations reflect on that and converge\n\n59:45.500 --> 59:49.880\n on different attractors and so on and so forth.\n\n59:49.880 --> 59:57.040\n And that could be new instrumental considerations that come into view as well, that we are just\n\n59:57.040 --> 1:00:04.240\n oblivious to, that would maybe have a strong shaping effect on actions, like very strong\n\n1:00:04.240 --> 1:00:08.160\n reasons to do something or not to do something, then we just don't realize they are there\n\n1:00:08.160 --> 1:00:11.160\n because we are so dumb, bumbling through the universe.\n\n1:00:11.160 --> 1:00:17.680\n But if almost inevitably en route to attaining the ability to create many ancestors simulations,\n\n1:00:17.680 --> 1:00:23.240\n you do have this cognitive enhancement, or advice from super intelligences or yourself,\n\n1:00:23.240 --> 1:00:27.120\n then maybe there's like this additional set of considerations coming into view and it's\n\n1:00:27.120 --> 1:00:32.000\n obvious that the thing that makes sense is to do X, whereas right now it seems you could\n\n1:00:32.000 --> 1:00:39.720\n X, Y or Z and different people will do different things and we are kind of random in that sense.\n\n1:00:39.720 --> 1:00:45.160\n Because at this time, with our limited technology, the impact of our decisions is minor.\n\n1:00:45.160 --> 1:00:48.440\n I mean, that's starting to change in some ways.\n\n1:00:48.440 --> 1:00:49.440\n But\u2026\n\n1:00:49.440 --> 1:00:53.840\n Well, I'm not sure how it follows that the impact of our decisions is minor.\n\n1:00:53.840 --> 1:00:55.600\n Well, it's starting to change.\n\n1:00:55.600 --> 1:00:58.760\n I mean, I suppose 100 years ago it was minor.\n\n1:00:58.760 --> 1:01:00.240\n It's starting to\u2026\n\n1:01:00.240 --> 1:01:03.520\n Well, it depends on how you view it.\n\n1:01:03.520 --> 1:01:08.640\n What people did 100 years ago still have effects on the world today.\n\n1:01:08.640 --> 1:01:11.920\n Oh, I see.\n\n1:01:11.920 --> 1:01:14.600\n As a civilization in the togetherness.\n\n1:01:14.600 --> 1:01:15.600\n Yeah.\n\n1:01:15.600 --> 1:01:21.760\n So it might be that the greatest impact of individuals is not at technological maturity\n\n1:01:21.760 --> 1:01:22.760\n or very far down.\n\n1:01:22.760 --> 1:01:28.440\n It might be earlier on when there are different tracks, civilization could go down.\n\n1:01:28.440 --> 1:01:33.280\n Maybe the population is smaller, things still haven't settled out.\n\n1:01:33.280 --> 1:01:41.720\n If you count indirect effects, those could be bigger than the direct effects that people\n\n1:01:41.720 --> 1:01:43.360\n have later on.\n\n1:01:43.360 --> 1:01:46.280\n So part three of the argument says that\u2026\n\n1:01:46.280 --> 1:01:53.520\n So that leads us to a place where eventually somebody creates a simulation.\n\n1:01:53.520 --> 1:01:55.520\n I think you had a conversation with Joe Rogan.\n\n1:01:55.520 --> 1:02:01.200\n I think there's some aspect here where you got stuck a little bit.\n\n1:02:01.200 --> 1:02:06.400\n How does that lead to we're likely living in a simulation?\n\n1:02:06.400 --> 1:02:12.920\n So this kind of probability argument, if somebody eventually creates a simulation, why does\n\n1:02:12.920 --> 1:02:15.980\n that mean that we're now in a simulation?\n\n1:02:15.980 --> 1:02:22.780\n What you get to if you accept alternative three first is there would be more simulated\n\n1:02:22.780 --> 1:02:26.440\n people with our kinds of experiences than non simulated ones.\n\n1:02:26.440 --> 1:02:34.120\n Like if you look at the world as a whole, by the end of time as it were, you just count\n\n1:02:34.120 --> 1:02:36.240\n it up.\n\n1:02:36.240 --> 1:02:39.600\n That would be more simulated ones than non simulated ones.\n\n1:02:39.600 --> 1:02:43.160\n Then there is an extra step to get from that.\n\n1:02:43.160 --> 1:02:48.320\n If you assume that, suppose for the sake of the argument, that that's true.\n\n1:02:48.320 --> 1:02:57.840\n How do you get from that to the statement we are probably in a simulation?\n\n1:02:57.840 --> 1:03:05.920\n So here you're introducing an indexical statement like it's that this person right now is in\n\n1:03:05.920 --> 1:03:06.920\n a simulation.\n\n1:03:06.920 --> 1:03:10.880\n There are all these other people that are in simulations and some that are not in the\n\n1:03:10.880 --> 1:03:13.520\n simulation.\n\n1:03:13.520 --> 1:03:19.200\n But what probability should you have that you yourself is one of the simulated ones\n\n1:03:19.200 --> 1:03:21.040\n in that setup?\n\n1:03:21.040 --> 1:03:28.440\n So I call it the bland principle of indifference, which is that in cases like this, when you\n\n1:03:28.440 --> 1:03:37.320\n have two sets of observers, one of which is much larger than the other and you can't from\n\n1:03:37.320 --> 1:03:46.320\n any internal evidence you have, tell which set you belong to, you should assign a probability\n\n1:03:46.320 --> 1:03:50.240\n that's proportional to the size of these sets.\n\n1:03:50.240 --> 1:03:55.240\n So that if there are 10 times more simulated people with your kinds of experiences, you\n\n1:03:55.240 --> 1:03:58.520\n would be 10 times more likely to be one of those.\n\n1:03:58.520 --> 1:04:00.600\n Is that as intuitive as it sounds?\n\n1:04:00.600 --> 1:04:06.000\n I mean, that seems kind of, if you don't have enough information, you should rationally\n\n1:04:06.000 --> 1:04:10.880\n just assign the same probability as the size of the set.\n\n1:04:10.880 --> 1:04:15.800\n It seems pretty plausible to me.\n\n1:04:15.800 --> 1:04:17.040\n Where are the holes in this?\n\n1:04:17.040 --> 1:04:23.800\n Is it at the very beginning, the assumption that everything stretches, you have infinite\n\n1:04:23.800 --> 1:04:24.800\n time essentially?\n\n1:04:24.800 --> 1:04:26.920\n You don't need infinite time.\n\n1:04:26.920 --> 1:04:29.840\n You just need, how long does the time take?\n\n1:04:29.840 --> 1:04:36.040\n However long it takes, I guess, for a universe to produce an intelligent civilization that\n\n1:04:36.040 --> 1:04:40.520\n attains the technology to run some ancestry simulations.\n\n1:04:40.520 --> 1:04:45.840\n When the first simulation is created, that stretch of time, just a little longer than\n\n1:04:45.840 --> 1:04:48.200\n they'll all start creating simulations.\n\n1:04:48.200 --> 1:04:52.200\n Well, I mean, there might be a difference.\n\n1:04:52.200 --> 1:04:57.720\n If you think of there being a lot of different planets and some subset of them have life\n\n1:04:57.720 --> 1:05:03.280\n and then some subset of those get to intelligent life and some of those maybe eventually start\n\n1:05:03.280 --> 1:05:07.760\n creating simulations, they might get started at quite different times.\n\n1:05:07.760 --> 1:05:13.960\n Maybe on some planet, it takes a billion years longer before you get monkeys or before you\n\n1:05:13.960 --> 1:05:19.720\n get even bacteria than on another planet.\n\n1:05:19.720 --> 1:05:25.000\n This might happen at different cosmological epochs.\n\n1:05:25.000 --> 1:05:28.920\n Is there a connection here to the doomsday argument and that sampling there?\n\n1:05:28.920 --> 1:05:36.880\n Yeah, there is a connection in that they both involve an application of anthropic reasoning\n\n1:05:36.880 --> 1:05:41.120\n that is reasoning about these kind of indexical propositions.\n\n1:05:41.120 --> 1:05:49.360\n But the assumption you need in the case of the simulation argument is much weaker than\n\n1:05:49.360 --> 1:05:53.760\n the assumption you need to make the doomsday argument go through.\n\n1:05:53.760 --> 1:05:58.640\n What is the doomsday argument and maybe you can speak to the anthropic reasoning in more\n\n1:05:58.640 --> 1:05:59.640\n general.\n\n1:05:59.640 --> 1:06:03.680\n Yeah, that's a big and interesting topic in its own right, anthropics, but the doomsday\n\n1:06:03.680 --> 1:06:11.120\n argument is this really first discovered by Brandon Carter, who was a theoretical physicist\n\n1:06:11.120 --> 1:06:15.920\n and then developed by philosopher John Leslie.\n\n1:06:15.920 --> 1:06:21.080\n I think it might have been discovered initially in the 70s or 80s and Leslie wrote this book,\n\n1:06:21.080 --> 1:06:23.280\n I think in 96.\n\n1:06:23.280 --> 1:06:27.600\n And there are some other versions as well by Richard Gott, who's a physicist, but let's\n\n1:06:27.600 --> 1:06:38.420\n focus on the Carter Leslie version where it's an argument that we have systematically underestimated\n\n1:06:38.420 --> 1:06:44.160\n the probability that humanity will go extinct soon.\n\n1:06:44.160 --> 1:06:49.040\n Now I should say most people probably think at the end of the day there is something wrong\n\n1:06:49.040 --> 1:06:52.260\n with this doomsday argument that it doesn't really hold.\n\n1:06:52.260 --> 1:06:56.280\n It's like there's something wrong with it, but it's proved hard to say exactly what is\n\n1:06:56.280 --> 1:07:00.600\n wrong with it and different people have different accounts.\n\n1:07:00.600 --> 1:07:06.480\n My own view is it seems inconclusive, but I can say what the argument is.\n\n1:07:06.480 --> 1:07:08.080\n Yeah, that would be good.\n\n1:07:08.080 --> 1:07:17.720\n So maybe it's easiest to explain via an analogy to sampling from urns.\n\n1:07:17.720 --> 1:07:27.000\n So imagine you have two urns in front of you and they have balls in them that have numbers.\n\n1:07:27.000 --> 1:07:30.020\n The two urns look the same, but inside one there are 10 balls.\n\n1:07:30.020 --> 1:07:33.520\n Ball number one, two, three, up to ball number 10.\n\n1:07:33.520 --> 1:07:41.720\n And then in the other urn you have a million balls numbered one to a million and somebody\n\n1:07:41.720 --> 1:07:48.280\n puts one of these urns in front of you and asks you to guess what's the chance it's the\n\n1:07:48.280 --> 1:07:53.280\n 10 ball urn and you say, well, 50, 50, I can't tell which urn it is.\n\n1:07:53.280 --> 1:07:58.360\n But then you're allowed to reach in and pick a ball at random from the urn and that's suppose\n\n1:07:58.360 --> 1:08:02.200\n you find that it's ball number seven.\n\n1:08:02.200 --> 1:08:05.680\n So that's strong evidence for the 10 ball hypothesis.\n\n1:08:05.680 --> 1:08:11.240\n It's a lot more likely that you would get such a low numbered ball if there are only\n\n1:08:11.240 --> 1:08:14.960\n 10 balls in the urn, like it's in fact 10% done, right?\n\n1:08:14.960 --> 1:08:19.680\n Then if there are a million balls, it would be very unlikely you would get number seven.\n\n1:08:19.680 --> 1:08:27.200\n So you perform a Bayesian update and if your prior was 50, 50 that it was the 10 ball urn,\n\n1:08:27.200 --> 1:08:31.640\n you become virtually certain after finding the random sample was seven that it's only\n\n1:08:31.640 --> 1:08:33.320\n has 10 balls in it.\n\n1:08:33.320 --> 1:08:37.480\n So in the case of the urns, this is uncontroversial, just elementary probability theory.\n\n1:08:37.480 --> 1:08:43.240\n The Doomsday Argument says that you should reason in a similar way with respect to different\n\n1:08:43.240 --> 1:08:49.680\n hypotheses about how many balls there will be in the urn of humanity as it were, how\n\n1:08:49.680 --> 1:08:54.360\n many humans there will ever have been by the time we go extinct.\n\n1:08:54.360 --> 1:09:00.400\n So to simplify, let's suppose we only consider two hypotheses, either maybe 200 billion humans\n\n1:09:00.400 --> 1:09:05.800\n in total or 200 trillion humans in total.\n\n1:09:05.800 --> 1:09:09.360\n You could fill in more hypotheses, but it doesn't change the principle here.\n\n1:09:09.360 --> 1:09:12.200\n So it's easiest to see if we just consider these two.\n\n1:09:12.200 --> 1:09:18.080\n So you start with some prior based on ordinary empirical ideas about threats to civilization\n\n1:09:18.080 --> 1:09:19.080\n and so forth.\n\n1:09:19.080 --> 1:09:23.640\n And maybe you say it's a 5% chance that we will go extinct by the time there will have\n\n1:09:23.640 --> 1:09:28.560\n been 200 billion only, you're kind of optimistic, let's say, you think probably we'll make it\n\n1:09:28.560 --> 1:09:31.840\n through, colonize the universe.\n\n1:09:31.840 --> 1:09:39.240\n But then, according to this Doomsday Argument, you should take off your own birth rank as\n\n1:09:39.240 --> 1:09:40.240\n a random sample.\n\n1:09:40.240 --> 1:09:47.720\n So your birth rank is your sequence in the position of all humans that have ever existed.\n\n1:09:47.720 --> 1:09:52.680\n It turns out you're about a human number of 100 billion, you know, give or take.\n\n1:09:52.680 --> 1:09:55.440\n That's like, roughly how many people have been born before you.\n\n1:09:55.440 --> 1:09:59.840\n That's fascinating, because I probably, we each have a number.\n\n1:09:59.840 --> 1:10:04.120\n We would each have a number in this, I mean, obviously, the exact number would depend on\n\n1:10:04.120 --> 1:10:09.080\n where you started counting, like which ancestors was human enough to count as human.\n\n1:10:09.080 --> 1:10:13.120\n But those are not really important, there are relatively few of them.\n\n1:10:13.120 --> 1:10:16.200\n So yeah, so you're roughly 100 billion.\n\n1:10:16.200 --> 1:10:20.960\n Now, if they're only going to be 200 billion in total, that's a perfectly unremarkable\n\n1:10:20.960 --> 1:10:21.960\n number.\n\n1:10:21.960 --> 1:10:22.960\n You're somewhere in the middle, right?\n\n1:10:22.960 --> 1:10:26.400\n It's a run of the mill human, completely unsurprising.\n\n1:10:26.400 --> 1:10:32.960\n Now, if they're going to be 200 trillion, you would be remarkably early, like what are\n\n1:10:32.960 --> 1:10:40.000\n the chances out of these 200 trillion human that you should be human number 100 billion?\n\n1:10:40.000 --> 1:10:45.040\n That seems it would have a much lower conditional probability.\n\n1:10:45.040 --> 1:10:50.820\n And so analogously to how in the urn case, you thought after finding this low numbered\n\n1:10:50.820 --> 1:10:54.880\n random sample, you update it in favor of the urn having few balls.\n\n1:10:54.880 --> 1:11:00.240\n Similarly, in this case, you should update in favor of the human species having a lower\n\n1:11:00.240 --> 1:11:04.480\n total number of members that is doomed soon.\n\n1:11:04.480 --> 1:11:05.960\n You said doomed soon?\n\n1:11:05.960 --> 1:11:11.640\n Well, that would be the hypothesis in this case that it will end 100 billion.\n\n1:11:11.640 --> 1:11:14.600\n I just like that term for that hypothesis.\n\n1:11:14.600 --> 1:11:20.160\n So what it kind of crucially relies on, the Doomsday Argument, is the idea that you should\n\n1:11:20.160 --> 1:11:27.480\n reason as if you were a random sample from the set of all humans that will have existed.\n\n1:11:27.480 --> 1:11:31.000\n If you have that assumption, then I think the rest kind of follows.\n\n1:11:31.000 --> 1:11:34.280\n The question then is, why should you make that assumption?\n\n1:11:34.280 --> 1:11:38.880\n In fact, you know you're 100 billion, so where do you get this prior?\n\n1:11:38.880 --> 1:11:45.280\n And then there is like a literature on that with different ways of supporting that assumption.\n\n1:11:45.280 --> 1:11:48.200\n That's just one example of anthropic reasoning, right?\n\n1:11:48.200 --> 1:11:53.800\n That seems to be kind of convenient when you think about humanity, when you think about\n\n1:11:53.800 --> 1:12:00.320\n sort of even like existential threats and so on, as it seems that quite naturally that\n\n1:12:00.320 --> 1:12:03.680\n you should assume that you're just an average case.\n\n1:12:03.680 --> 1:12:07.840\n Yeah, that you're kind of a typical randomly sample.\n\n1:12:07.840 --> 1:12:12.240\n Now, in the case of the Doomsday Argument, it seems to lead to what intuitively we think\n\n1:12:12.240 --> 1:12:16.480\n is the wrong conclusion, or at least many people have this reaction that there's got\n\n1:12:16.480 --> 1:12:19.320\n to be something fishy about this argument.\n\n1:12:19.320 --> 1:12:25.840\n Because from very, very weak premises, it gets this very striking implication that we\n\n1:12:25.840 --> 1:12:30.980\n have almost no chance of reaching size 200 trillion humans in the future.\n\n1:12:30.980 --> 1:12:35.480\n And how could we possibly get there just by reflecting on when we were born?\n\n1:12:35.480 --> 1:12:39.720\n It seems you would need sophisticated arguments about the impossibility of space colonization,\n\n1:12:39.720 --> 1:12:40.720\n blah, blah.\n\n1:12:40.720 --> 1:12:45.560\n So one might be tempted to reject this key assumption, I call it the self sampling assumption,\n\n1:12:45.560 --> 1:12:50.280\n the idea that you should reason as if you're a random sample from all observers or in your\n\n1:12:50.280 --> 1:12:52.600\n some reference class.\n\n1:12:52.600 --> 1:12:58.920\n However, it turns out that in other domains, it looks like we need something like this\n\n1:12:58.920 --> 1:13:04.840\n self sampling assumption to make sense of bona fide scientific inferences.\n\n1:13:04.840 --> 1:13:09.320\n In contemporary cosmology, for example, you have these multiverse theories.\n\n1:13:09.320 --> 1:13:14.880\n And according to a lot of those, all possible human observations are made.\n\n1:13:14.880 --> 1:13:18.880\n So if you have a sufficiently large universe, you will have a lot of people observing all\n\n1:13:18.880 --> 1:13:22.040\n kinds of different things.\n\n1:13:22.040 --> 1:13:29.880\n So if you have two competing theories, say about the value of some constant, it could\n\n1:13:29.880 --> 1:13:34.980\n be true according to both of these theories that there will be some observers observing\n\n1:13:34.980 --> 1:13:42.160\n the value that corresponds to the other theory, because there will be some observers that\n\n1:13:42.160 --> 1:13:47.600\n have hallucinations, so there's a local fluctuation or a statistically anomalous measurement,\n\n1:13:47.600 --> 1:13:49.460\n these things will happen.\n\n1:13:49.460 --> 1:13:53.320\n And if enough observers make enough different observations, there will be some that sort\n\n1:13:53.320 --> 1:13:55.960\n of by chance make these different ones.\n\n1:13:55.960 --> 1:14:04.280\n And so what we would want to say is, well, many more observers, a larger proportion of\n\n1:14:04.280 --> 1:14:08.560\n the observers will observe as it were the true value.\n\n1:14:08.560 --> 1:14:10.880\n And a few will observe the wrong value.\n\n1:14:10.880 --> 1:14:15.400\n If we think of ourselves as a random sample, we should expect with a probability to observe\n\n1:14:15.400 --> 1:14:20.180\n the true value and that will then allow us to conclude that the evidence we actually\n\n1:14:20.180 --> 1:14:24.640\n have is evidence for the theories we think are supported.\n\n1:14:24.640 --> 1:14:32.200\n It kind of then is a way of making sense of these inferences that clearly seem correct,\n\n1:14:32.200 --> 1:14:38.480\n that we can make various observations and infer what the temperature of the cosmic background\n\n1:14:38.480 --> 1:14:44.080\n is and the fine structure constant and all of this.\n\n1:14:44.080 --> 1:14:49.980\n But it seems that without rolling in some assumption similar to the self sampling assumption,\n\n1:14:49.980 --> 1:14:51.840\n this inference just doesn't go through.\n\n1:14:51.840 --> 1:14:53.180\n And there are other examples.\n\n1:14:53.180 --> 1:14:56.620\n So there are these scientific contexts where it looks like this kind of anthropic reasoning\n\n1:14:56.620 --> 1:14:59.120\n is needed and makes perfect sense.\n\n1:14:59.120 --> 1:15:02.960\n And yet, in the case of the Dupest argument, it has this weird consequence and people might\n\n1:15:02.960 --> 1:15:05.760\n think there's something wrong with it there.\n\n1:15:05.760 --> 1:15:14.200\n So there's then this project that would consist in trying to figure out what are the legitimate\n\n1:15:14.200 --> 1:15:20.400\n ways of reasoning about these indexical facts when observer selection effects are in play.\n\n1:15:20.400 --> 1:15:23.520\n In other words, developing a theory of anthropics.\n\n1:15:23.520 --> 1:15:29.280\n And there are different views of looking at that and it's a difficult methodological area.\n\n1:15:29.280 --> 1:15:37.920\n But to tie it back to the simulation argument, the key assumption there, this bland principle\n\n1:15:37.920 --> 1:15:43.460\n of indifference, is much weaker than the self sampling assumption.\n\n1:15:43.460 --> 1:15:48.220\n So if you think about, in the case of the Dupest argument, it says you should reason\n\n1:15:48.220 --> 1:15:52.100\n as if you are a random sample from all humans that will have lived, even though in fact\n\n1:15:52.100 --> 1:15:59.840\n you know that you are about number 100 billionth human and you're alive in the year 2020.\n\n1:15:59.840 --> 1:16:04.240\n Whereas in the case of the simulation argument, it says that, well, if you actually have no\n\n1:16:04.240 --> 1:16:10.240\n way of telling which one you are, then you should assign this kind of uniform probability.\n\n1:16:10.240 --> 1:16:14.640\n Yeah, yeah, your role as the observer in the simulation argument is different, it seems\n\n1:16:14.640 --> 1:16:15.640\n like.\n\n1:16:15.640 --> 1:16:16.640\n Like who's the observer?\n\n1:16:16.640 --> 1:16:19.640\n I mean, I keep assigning the individual consciousness.\n\n1:16:19.640 --> 1:16:26.480\n But a lot of observers in the context of the simulation argument, the relevant observers\n\n1:16:26.480 --> 1:16:33.440\n would be A, the people in original histories, and B, the people in simulations.\n\n1:16:33.440 --> 1:16:37.520\n So this would be the class of observers that we need, I mean, they're also maybe the simulators,\n\n1:16:37.520 --> 1:16:40.280\n but we can set those aside for this.\n\n1:16:40.280 --> 1:16:46.160\n So the question is, given that class of observers, a small set of original history observers\n\n1:16:46.160 --> 1:16:51.480\n and a large class of simulated observers, which one should you think is you?\n\n1:16:51.480 --> 1:16:54.200\n Where are you amongst this set of observers?\n\n1:16:54.200 --> 1:17:00.020\n I'm maybe having a little bit of trouble wrapping my head around the intricacies of what it\n\n1:17:00.020 --> 1:17:08.020\n means to be an observer in this, in the different instantiations of the anthropic reasoning\n\n1:17:08.020 --> 1:17:09.020\n cases that we mentioned.\n\n1:17:09.020 --> 1:17:10.020\n Yeah.\n\n1:17:10.020 --> 1:17:11.020\n I mean, does it have to be...\n\n1:17:11.020 --> 1:17:12.020\n It's not the observer.\n\n1:17:12.020 --> 1:17:16.560\n Yeah, I mean, it may be an easier way of putting it is just like, are you simulated, are you\n\n1:17:16.560 --> 1:17:21.120\n not simulated, given this assumption that these two groups of people exist?\n\n1:17:21.120 --> 1:17:22.120\n Yeah.\n\n1:17:22.120 --> 1:17:24.040\n In the simulation case, it seems pretty straightforward.\n\n1:17:24.040 --> 1:17:25.040\n Yeah.\n\n1:17:25.040 --> 1:17:32.560\n So the key point is the methodological assumption you need to make to get the simulation argument\n\n1:17:32.560 --> 1:17:38.940\n to where it wants to go is much weaker and less problematic than the methodological assumption\n\n1:17:38.940 --> 1:17:43.000\n you need to make to get the doomsday argument to its conclusion.\n\n1:17:43.000 --> 1:17:48.240\n Maybe the doomsday argument is sound or unsound, but you need to make a much stronger and more\n\n1:17:48.240 --> 1:17:52.140\n controversial assumption to make it go through.\n\n1:17:52.140 --> 1:17:58.820\n In the case of the simulation argument, I guess one maybe way intuition pumped to support\n\n1:17:58.820 --> 1:18:05.560\n this bland principle of indifference is to consider a sequence of different cases where\n\n1:18:05.560 --> 1:18:12.600\n the fraction of people who are simulated to non simulated approaches one.\n\n1:18:12.600 --> 1:18:22.580\n So in the limiting case where everybody is simulated, obviously you can deduce with certainty\n\n1:18:22.580 --> 1:18:24.920\n that you are simulated.\n\n1:18:24.920 --> 1:18:30.980\n If everybody with your experiences is simulated and you know you've got to be one of those,\n\n1:18:30.980 --> 1:18:36.200\n you don't need a probability at all, you just kind of logically conclude it, right?\n\n1:18:36.200 --> 1:18:48.640\n So then as we move from a case where say 90% of everybody is simulated, 99%, 99.9%, it\n\n1:18:48.640 --> 1:18:54.720\n should seem plausible that the probability you assign should sort of approach one certainty\n\n1:18:54.720 --> 1:19:02.400\n as the fraction approaches the case where everybody is in a simulation.\n\n1:19:02.400 --> 1:19:06.800\n You wouldn't expect that to be a discrete, well, if there's one non simulated person,\n\n1:19:06.800 --> 1:19:12.520\n then it's 50, 50, but if we move that, then it's 100%, like it should kind of, there are\n\n1:19:12.520 --> 1:19:18.300\n other arguments as well one can use to support this bland principle of indifference, but\n\n1:19:18.300 --> 1:19:19.300\n that might be enough to.\n\n1:19:19.300 --> 1:19:25.560\n But in general, when you start from time equals zero and go into the future, the fraction\n\n1:19:25.560 --> 1:19:30.600\n of simulated, if it's possible to create simulated worlds, the fraction of simulated worlds will\n\n1:19:30.600 --> 1:19:31.600\n go to one.\n\n1:19:31.600 --> 1:19:37.800\n Well, I mean, it won't go all the way to one.\n\n1:19:37.800 --> 1:19:43.680\n In reality, that would be some ratio, although maybe a technologically mature civilization\n\n1:19:43.680 --> 1:19:52.040\n could run a lot of simulations using a small portion of its resources, it probably wouldn't\n\n1:19:52.040 --> 1:19:53.160\n be able to run infinitely many.\n\n1:19:53.160 --> 1:19:59.280\n I mean, if we take say the observed, the physics in the observed universe, if we assume that\n\n1:19:59.280 --> 1:20:05.460\n that's also the physics at the level of the simulators, that would be limits to the amount\n\n1:20:05.460 --> 1:20:16.120\n of information processing that any one civilization could perform in its future trajectory.\n\n1:20:16.120 --> 1:20:20.000\n First of all, there's limited amount of matter you can get your hands off because with a\n\n1:20:20.000 --> 1:20:25.760\n positive cosmological constant, the universe is accelerating, there's like a finite sphere\n\n1:20:25.760 --> 1:20:28.800\n of stuff, even if you traveled with the speed of light that you could ever reach, you have\n\n1:20:28.800 --> 1:20:31.880\n a finite amount of stuff.\n\n1:20:31.880 --> 1:20:37.760\n And then if you think there is like a lower limit to the amount of loss you get when you\n\n1:20:37.760 --> 1:20:42.260\n perform an erasure of a computation, or if you think, for example, just matter gradually\n\n1:20:42.260 --> 1:20:49.200\n over cosmological timescales, decay, maybe protons decay, other things, and you radiate\n\n1:20:49.200 --> 1:20:55.040\n out gravitational waves, like there's all kinds of seemingly unavoidable losses that\n\n1:20:55.040 --> 1:20:56.040\n occur.\n\n1:20:56.040 --> 1:21:04.080\n Eventually, we'll have something like a heat death of the universe or a cold death or whatever,\n\n1:21:04.080 --> 1:21:05.080\n but yeah.\n\n1:21:05.080 --> 1:21:11.360\n So it's finite, but of course, we don't know which, if there's many ancestral simulations,\n\n1:21:11.360 --> 1:21:13.640\n we don't know which level we are.\n\n1:21:13.640 --> 1:21:18.640\n So there could be, couldn't there be like an arbitrary number of simulation that spawned\n\n1:21:18.640 --> 1:21:26.160\n ours, and those had more resources, in terms of physical universe to work with?\n\n1:21:26.160 --> 1:21:29.360\n Sorry, what do you mean that that could be?\n\n1:21:29.360 --> 1:21:40.280\n Sort of, okay, so if simulations spawn other simulations, it seems like each new spawn\n\n1:21:40.280 --> 1:21:44.200\n has fewer resources to work with.\n\n1:21:44.200 --> 1:21:50.240\n But we don't know at which step along the way we are at.\n\n1:21:50.240 --> 1:21:58.320\n Any one observer doesn't know whether we're in level 42, or 100, or one, or is that not\n\n1:21:58.320 --> 1:22:01.160\n matter for the resources?\n\n1:22:01.160 --> 1:22:08.800\n I mean, it's true that there would be uncertainty as to, you could have stacked simulations,\n\n1:22:08.800 --> 1:22:16.040\n and that could then be uncertainty as to which level we are at.\n\n1:22:16.040 --> 1:22:24.680\n As you remarked also, all the computations performed in a simulation within the simulation\n\n1:22:24.680 --> 1:22:28.640\n also have to be expanded at the level of the simulation.\n\n1:22:28.640 --> 1:22:32.320\n So the computer in basement reality where all these simulations with the simulations\n\n1:22:32.320 --> 1:22:37.320\n with the simulations are taking place, like that computer, ultimately, it's CPU or whatever\n\n1:22:37.320 --> 1:22:40.000\n it is, like that has to power this whole tower, right?\n\n1:22:40.000 --> 1:22:46.000\n So if there is a finite compute power in basement reality, that would impose a limit to how\n\n1:22:46.000 --> 1:22:48.440\n tall this tower can be.\n\n1:22:48.440 --> 1:22:53.920\n And if each level kind of imposes a large extra overhead, you might think maybe the\n\n1:22:53.920 --> 1:23:00.720\n tower would not be very tall, that most people would be low down in the tower.\n\n1:23:00.720 --> 1:23:03.120\n I love the term basement reality.\n\n1:23:03.120 --> 1:23:09.320\n Let me ask one of the popularizers, you said there's many through this, when you look at\n\n1:23:09.320 --> 1:23:14.640\n sort of the last few years of the simulation hypothesis, just like you said, it comes up\n\n1:23:14.640 --> 1:23:17.680\n every once in a while, some new community discovers it and so on.\n\n1:23:17.680 --> 1:23:22.800\n But I would say one of the biggest popularizers of this idea is Elon Musk.\n\n1:23:22.800 --> 1:23:27.880\n Do you have any kind of intuition about what Elon thinks about when he thinks about simulation?\n\n1:23:27.880 --> 1:23:30.000\n Why is this of such interest?\n\n1:23:30.000 --> 1:23:34.000\n Is it all the things we've talked about, or is there some special kind of intuition about\n\n1:23:34.000 --> 1:23:36.240\n simulation that he has?\n\n1:23:36.240 --> 1:23:40.000\n I mean, you might have a better, I think, I mean, why it's of interest, I think it's\n\n1:23:40.000 --> 1:23:45.200\n like seems pretty obvious why, to the extent that one thinks the argument is credible,\n\n1:23:45.200 --> 1:23:48.720\n why it would be of interest, it would, if it's correct, tell us something very important\n\n1:23:48.720 --> 1:23:53.280\n about the world in one way or the other, whichever of the three alternatives for a simulation\n\n1:23:53.280 --> 1:23:58.160\n that seems like arguably one of the most fundamental discoveries, right?\n\n1:23:58.160 --> 1:24:02.160\n Now, interestingly, in the case of someone like Elon, so there's like the standard arguments\n\n1:24:02.160 --> 1:24:06.720\n for why you might want to take the simulation hypothesis seriously, the simulation argument,\n\n1:24:06.720 --> 1:24:07.720\n right?\n\n1:24:07.720 --> 1:24:12.360\n In the case that if you are actually Elon Musk, let us say, there's a kind of an additional\n\n1:24:12.360 --> 1:24:17.280\n reason in that what are the chances you would be Elon Musk?\n\n1:24:17.280 --> 1:24:24.400\n It seems like maybe there would be more interest in simulating the lives of very unusual and\n\n1:24:24.400 --> 1:24:26.280\n remarkable people.\n\n1:24:26.280 --> 1:24:32.440\n So if you consider not just simulations where all of human history or the whole of human\n\n1:24:32.440 --> 1:24:37.880\n civilization are simulated, but also other kinds of simulations, which only include some\n\n1:24:37.880 --> 1:24:44.200\n subset of people, like in those simulations that only include a subset, it might be more\n\n1:24:44.200 --> 1:24:49.080\n likely that they would include subsets of people with unusually interesting or consequential\n\n1:24:49.080 --> 1:24:50.080\n lives.\n\n1:24:50.080 --> 1:24:54.320\n So if you're Elon Musk, it's more likely that you're an inspiration.\n\n1:24:54.320 --> 1:25:00.560\n Like if you're Donald Trump, or if you're Bill Gates, or you're like, some particularly\n\n1:25:00.560 --> 1:25:06.200\n like distinctive character, you might think that that, I mean, if you just think of yourself\n\n1:25:06.200 --> 1:25:11.400\n into the shoes, right, it's got to be like an extra reason to think that's kind of.\n\n1:25:11.400 --> 1:25:12.400\n So interesting.\n\n1:25:12.400 --> 1:25:19.200\n So on a scale of like farmer in Peru to Elon Musk, the more you get towards the Elon Musk,\n\n1:25:19.200 --> 1:25:20.200\n the higher the probability.\n\n1:25:20.200 --> 1:25:25.280\n You'd imagine that would be some extra boost from that.\n\n1:25:25.280 --> 1:25:26.280\n There's an extra boost.\n\n1:25:26.280 --> 1:25:32.520\n So he also asked the question of what he would ask an AGI saying, the question being, what's\n\n1:25:32.520 --> 1:25:34.680\n outside the simulation?\n\n1:25:34.680 --> 1:25:37.740\n Do you think about the answer to this question?\n\n1:25:37.740 --> 1:25:41.520\n If we are living in a simulation, what is outside the simulation?\n\n1:25:41.520 --> 1:25:44.520\n So the programmer of the simulation?\n\n1:25:44.520 --> 1:25:49.760\n Yeah, I mean, I think it connects to the question of what's inside the simulation in that.\n\n1:25:49.760 --> 1:25:56.920\n So if you had views about the creators of the simulation, it might help you make predictions\n\n1:25:56.920 --> 1:26:03.480\n about what kind of simulation it is, what might happen, what happens after the simulation,\n\n1:26:03.480 --> 1:26:06.640\n if there is some after, but also like the kind of setup.\n\n1:26:06.640 --> 1:26:12.000\n So these two questions would be quite closely intertwined.\n\n1:26:12.000 --> 1:26:17.920\n But do you think it would be very surprising to like, is the stuff inside the simulation,\n\n1:26:17.920 --> 1:26:21.520\n is it possible for it to be fundamentally different than the stuff outside?\n\n1:26:21.520 --> 1:26:22.520\n Yeah.\n\n1:26:22.520 --> 1:26:29.520\n Like, another way to put it, can the creatures inside the simulation be smart enough to even\n\n1:26:29.520 --> 1:26:34.640\n understand or have the cognitive capabilities or any kind of information processing capabilities\n\n1:26:34.640 --> 1:26:40.520\n enough to understand the mechanism that created them?\n\n1:26:40.520 --> 1:26:43.160\n They might understand some aspects of it.\n\n1:26:43.160 --> 1:26:50.120\n I mean, it's a level of, it's kind of, there are levels of explanation, like degrees to\n\n1:26:50.120 --> 1:26:51.120\n which you can understand.\n\n1:26:51.120 --> 1:26:53.840\n So does your dog understand what it is to be human?\n\n1:26:53.840 --> 1:26:58.120\n Well, it's got some idea, like humans are these physical objects that move around and\n\n1:26:58.120 --> 1:26:59.880\n do things.\n\n1:26:59.880 --> 1:27:05.720\n And a normal human would have a deeper understanding of what it is to be a human.\n\n1:27:05.720 --> 1:27:12.120\n And maybe some very experienced psychologist or great novelist might understand a little\n\n1:27:12.120 --> 1:27:14.280\n bit more about what it is to be human.\n\n1:27:14.280 --> 1:27:18.760\n And maybe superintelligence could see right through your soul.\n\n1:27:18.760 --> 1:27:27.080\n So similarly, I do think that we are quite limited in our ability to understand all of\n\n1:27:27.080 --> 1:27:31.880\n the relevant aspects of the larger context that we exist in.\n\n1:27:31.880 --> 1:27:33.400\n But there might be hope for some.\n\n1:27:33.400 --> 1:27:36.080\n I think we understand some aspects of it.\n\n1:27:36.080 --> 1:27:38.320\n But you know, how much good is that?\n\n1:27:38.320 --> 1:27:44.640\n If there's like one key aspect that changes the significance of all the other aspects.\n\n1:27:44.640 --> 1:27:51.800\n So we understand maybe seven out of 10 key insights that you need.\n\n1:27:51.800 --> 1:27:57.760\n But the answer actually, like varies completely depending on what like number eight, nine\n\n1:27:57.760 --> 1:28:00.200\n and 10 insight is.\n\n1:28:00.200 --> 1:28:07.040\n It's like whether you want to suppose that the big task were to guess whether a certain\n\n1:28:07.040 --> 1:28:12.280\n number was odd or even, like a 10 digit number.\n\n1:28:12.280 --> 1:28:16.440\n And if it's even, the best thing for you to do in life is to go north.\n\n1:28:16.440 --> 1:28:21.060\n And if it's odd, the best thing for you is to go south.\n\n1:28:21.060 --> 1:28:25.000\n Now we are in a situation where maybe through our science and philosophy, we figured out\n\n1:28:25.000 --> 1:28:26.680\n what the first seven digits are.\n\n1:28:26.680 --> 1:28:28.680\n So we have a lot of information, right?\n\n1:28:28.680 --> 1:28:31.040\n Most of it we figured out.\n\n1:28:31.040 --> 1:28:34.320\n But we are clueless about what the last three digits are.\n\n1:28:34.320 --> 1:28:38.720\n So we are still completely clueless about whether the number is odd or even and therefore\n\n1:28:38.720 --> 1:28:41.120\n whether we should go north or go south.\n\n1:28:41.120 --> 1:28:45.760\n I feel that's an analogy, but I feel we're somewhat in that predicament.\n\n1:28:45.760 --> 1:28:48.460\n We know a lot about the universe.\n\n1:28:48.460 --> 1:28:52.640\n We've come maybe more than half of the way there to kind of fully understanding it.\n\n1:28:52.640 --> 1:28:58.680\n But the parts we're missing are plausibly ones that could completely change the overall\n\n1:28:58.680 --> 1:29:04.800\n upshot of the thing and including change our overall view about what the scheme of priorities\n\n1:29:04.800 --> 1:29:07.680\n should be or which strategic direction would make sense to pursue.\n\n1:29:07.680 --> 1:29:08.680\n Yeah.\n\n1:29:08.680 --> 1:29:15.520\n I think your analogy of us being the dog trying to understand human beings is an entertaining\n\n1:29:15.520 --> 1:29:17.600\n one, and probably correct.\n\n1:29:17.600 --> 1:29:24.960\n The closer the understanding tends from the dog's viewpoint to us human psychologist viewpoint,\n\n1:29:24.960 --> 1:29:29.880\n the steps along the way there will have completely transformative ideas of what it means to be\n\n1:29:29.880 --> 1:29:30.880\n human.\n\n1:29:30.880 --> 1:29:33.920\n So the dog has a very shallow understanding.\n\n1:29:33.920 --> 1:29:39.800\n It's interesting to think that, to analogize that a dog's understanding of a human being\n\n1:29:39.800 --> 1:29:45.880\n is the same as our current understanding of the fundamental laws of physics in the universe.\n\n1:29:45.880 --> 1:29:47.880\n Oh man.\n\n1:29:47.880 --> 1:29:48.880\n Okay.\n\n1:29:48.880 --> 1:29:51.560\n We spent an hour and 40 minutes talking about the simulation.\n\n1:29:51.560 --> 1:29:53.120\n I like it.\n\n1:29:53.120 --> 1:29:54.360\n Let's talk about super intelligence.\n\n1:29:54.360 --> 1:29:57.120\n At least for a little bit.\n\n1:29:57.120 --> 1:29:58.720\n And let's start at the basics.\n\n1:29:58.720 --> 1:30:00.720\n What to you is intelligence?\n\n1:30:00.720 --> 1:30:01.720\n Yeah.\n\n1:30:01.720 --> 1:30:05.960\n I tend not to get too stuck with the definitional question.\n\n1:30:05.960 --> 1:30:11.400\n I mean, the common sense to understand, like the ability to solve complex problems, to\n\n1:30:11.400 --> 1:30:18.600\n learn from experience, to plan, to reason, some combination of things like that.\n\n1:30:18.600 --> 1:30:21.160\n Is consciousness mixed up into that or no?\n\n1:30:21.160 --> 1:30:23.200\n Is consciousness mixed up into that?\n\n1:30:23.200 --> 1:30:31.000\n Well, I think it could be fairly intelligent at least without being conscious probably.\n\n1:30:31.000 --> 1:30:33.920\n So then what is super intelligence?\n\n1:30:33.920 --> 1:30:40.120\n That would be like something that was much more, had much more general cognitive capacity\n\n1:30:40.120 --> 1:30:41.760\n than we humans have.\n\n1:30:41.760 --> 1:30:48.640\n So if we talk about general super intelligence, it would be much faster learner be able to\n\n1:30:48.640 --> 1:30:53.520\n reason much better, make plans that are more effective at achieving its goals, say in a\n\n1:30:53.520 --> 1:30:57.000\n wide range of complex challenging environments.\n\n1:30:57.000 --> 1:31:03.040\n In terms of as we turn our eye to the idea of sort of existential threats from super\n\n1:31:03.040 --> 1:31:08.920\n intelligence, do you think super intelligence has to exist in the physical world or can\n\n1:31:08.920 --> 1:31:10.880\n it be digital only?\n\n1:31:10.880 --> 1:31:17.920\n Sort of we think of our general intelligence as us humans, as an intelligence that's associated\n\n1:31:17.920 --> 1:31:22.040\n with the body, that's able to interact with the world, that's able to affect the world\n\n1:31:22.040 --> 1:31:23.360\n directly with physically.\n\n1:31:23.360 --> 1:31:26.200\n I mean, digital only is perfectly fine, I think.\n\n1:31:26.200 --> 1:31:31.320\n I mean, you could, it's physical in the sense that obviously the computers and the memories\n\n1:31:31.320 --> 1:31:32.320\n are physical.\n\n1:31:32.320 --> 1:31:34.960\n But it's capability to affect the world sort of.\n\n1:31:34.960 --> 1:31:42.160\n Could be very strong, even if it has a limited set of actuators, if it can type text on the\n\n1:31:42.160 --> 1:31:45.860\n screen or something like that, that would be, I think, ample.\n\n1:31:45.860 --> 1:31:52.960\n So in terms of the concerns of existential threat of AI, how can an AI system that's\n\n1:31:52.960 --> 1:32:00.800\n in the digital world have existential risk, sort of, and what are the attack vectors for\n\n1:32:00.800 --> 1:32:01.800\n a digital system?\n\n1:32:01.800 --> 1:32:07.800\n Well, I mean, I guess maybe to take one step back, so I should emphasize that I also think\n\n1:32:07.800 --> 1:32:13.440\n there's this huge positive potential from machine intelligence, including super intelligence.\n\n1:32:13.440 --> 1:32:20.920\n And I want to stress that because some of my writing has focused on what can go wrong.\n\n1:32:20.920 --> 1:32:27.020\n And when I wrote the book Superintelligence, at that point, I felt that there was a kind\n\n1:32:27.020 --> 1:32:34.300\n of neglect of what would happen if AI succeeds, and in particular, a need to get a more granular\n\n1:32:34.300 --> 1:32:38.680\n understanding of where the pitfalls are so we can avoid them.\n\n1:32:38.680 --> 1:32:45.560\n I think that since the book came out in 2014, there has been a much wider recognition of\n\n1:32:45.560 --> 1:32:46.560\n that.\n\n1:32:46.560 --> 1:32:51.620\n And a number of research groups are now actually working on developing, say, AI alignment techniques\n\n1:32:51.620 --> 1:32:52.620\n and so on and so forth.\n\n1:32:52.620 --> 1:33:01.740\n So yeah, I think now it's important to make sure we bring back onto the table the upside\n\n1:33:01.740 --> 1:33:02.740\n as well.\n\n1:33:02.740 --> 1:33:07.060\n And there's a little bit of a neglect now on the upside, which is, I mean, if you look\n\n1:33:07.060 --> 1:33:11.780\n at, I was talking to a friend, if you look at the amount of information that is available,\n\n1:33:11.780 --> 1:33:16.180\n or people talking and people being excited about the positive possibilities of general\n\n1:33:16.180 --> 1:33:23.860\n intelligence, that's not, it's far outnumbered by the negative possibilities in terms of\n\n1:33:23.860 --> 1:33:25.140\n our public discourse.\n\n1:33:25.140 --> 1:33:26.140\n Possibly, yeah.\n\n1:33:26.140 --> 1:33:28.280\n It's hard to measure.\n\n1:33:28.280 --> 1:33:33.560\n But what are, can you linger on that for a little bit, what are some, to you, possible\n\n1:33:33.560 --> 1:33:37.700\n big positive impacts of general intelligence?\n\n1:33:37.700 --> 1:33:38.700\n Super intelligence?\n\n1:33:38.700 --> 1:33:43.200\n Well, I mean, super intelligence, because I tend to also want to distinguish these two\n\n1:33:43.200 --> 1:33:48.900\n different contexts of thinking about AI and AI impacts, the kind of near term and long\n\n1:33:48.900 --> 1:33:54.680\n term, if you want, both of which I think are legitimate things to think about, and people\n\n1:33:54.680 --> 1:34:02.020\n should discuss both of them, but they are different and they often get mixed up.\n\n1:34:02.020 --> 1:34:06.680\n And then, then I get, you get confusion, like, I think you get simultaneously like maybe\n\n1:34:06.680 --> 1:34:10.220\n an overhyping of the near term and then under hyping of the long term.\n\n1:34:10.220 --> 1:34:15.260\n And so I think as long as we keep them apart, we can have like, two good conversations,\n\n1:34:15.260 --> 1:34:18.660\n but or we can mix them together and have one bad conversation.\n\n1:34:18.660 --> 1:34:22.820\n Can you clarify just the two things we were talking about, the near term and the long\n\n1:34:22.820 --> 1:34:23.820\n term?\n\n1:34:23.820 --> 1:34:24.820\n Yeah.\n\n1:34:24.820 --> 1:34:25.820\n And what are the distinctions?\n\n1:34:25.820 --> 1:34:28.100\n Well, it's a, it's a blurry distinction.\n\n1:34:28.100 --> 1:34:34.940\n But say the things I wrote about in this book, super intelligence, long term, things people\n\n1:34:34.940 --> 1:34:41.540\n are worrying about today with, I don't know, algorithmic discrimination, or even things,\n\n1:34:41.540 --> 1:34:47.100\n self driving cars and drones and stuff, more near term.\n\n1:34:47.100 --> 1:34:51.340\n And then of course, you could imagine some medium term where they kind of overlap and\n\n1:34:51.340 --> 1:34:55.180\n they one evolves into the other.\n\n1:34:55.180 --> 1:35:00.020\n But at any rate, I think both, yeah, the issues look kind of somewhat different depending\n\n1:35:00.020 --> 1:35:01.600\n on which of these contexts.\n\n1:35:01.600 --> 1:35:10.340\n So I think, I think it'd be nice if we can talk about the long term and think about a\n\n1:35:10.340 --> 1:35:17.820\n positive impact or a better world because of the existence of the long term super intelligence.\n\n1:35:17.820 --> 1:35:19.420\n Do you have views of such a world?\n\n1:35:19.420 --> 1:35:20.420\n Yeah.\n\n1:35:20.420 --> 1:35:24.780\n I mean, I guess it's a little hard to articulate because it seems obvious that the world has\n\n1:35:24.780 --> 1:35:29.700\n a lot of problems as it currently stands.\n\n1:35:29.700 --> 1:35:36.620\n And it's hard to think of any one of those, which it wouldn't be useful to have like a\n\n1:35:36.620 --> 1:35:40.720\n friendly aligned super intelligence working on.\n\n1:35:40.720 --> 1:35:48.300\n So from health to the economic system to be able to sort of improve the investment and\n\n1:35:48.300 --> 1:35:52.180\n trade and foreign policy decisions, all that kind of stuff.\n\n1:35:52.180 --> 1:35:56.300\n All that kind of stuff and a lot more.\n\n1:35:56.300 --> 1:35:57.780\n I mean, what's the killer app?\n\n1:35:57.780 --> 1:35:59.540\n Well, I don't think there is one.\n\n1:35:59.540 --> 1:36:05.900\n I think AI, especially artificial general intelligence is really the ultimate general\n\n1:36:05.900 --> 1:36:07.820\n purpose technology.\n\n1:36:07.820 --> 1:36:12.220\n So it's not that there is this one problem, this one area where it will have a big impact.\n\n1:36:12.220 --> 1:36:18.980\n But if and when it succeeds, it will really apply across the board in all fields where\n\n1:36:18.980 --> 1:36:23.820\n human creativity and intelligence and problem solving is useful, which is pretty much all\n\n1:36:23.820 --> 1:36:24.820\n fields.\n\n1:36:24.820 --> 1:36:25.820\n Right.\n\n1:36:25.820 --> 1:36:30.840\n The thing that it would do is give us a lot more control over nature.\n\n1:36:30.840 --> 1:36:37.220\n It wouldn't automatically solve the problems that arise from conflict between humans, fundamentally\n\n1:36:37.220 --> 1:36:38.220\n political problems.\n\n1:36:38.220 --> 1:36:42.180\n Some subset of those might go away if you just had more resources and cooler tech.\n\n1:36:42.180 --> 1:36:50.360\n But some subset would require coordination that is not automatically achieved just by\n\n1:36:50.360 --> 1:36:53.240\n having more technological capability.\n\n1:36:53.240 --> 1:36:59.500\n But anything that's not of that sort, I think you just get an enormous boost with this kind\n\n1:36:59.500 --> 1:37:02.940\n of cognitive technology once it goes all the way.\n\n1:37:02.940 --> 1:37:10.460\n Now, again, that doesn't mean I'm thinking, oh, people don't recognize what's possible\n\n1:37:10.460 --> 1:37:14.180\n with current technology and like sometimes things get overhyped.\n\n1:37:14.180 --> 1:37:16.940\n But I mean, those are perfectly consistent views to hold.\n\n1:37:16.940 --> 1:37:19.940\n The ultimate potential being enormous.\n\n1:37:19.940 --> 1:37:23.780\n And then it's a very different question of how far are we from that or what can we do\n\n1:37:23.780 --> 1:37:25.060\n with near term technology?\n\n1:37:25.060 --> 1:37:26.060\n Yeah.\n\n1:37:26.060 --> 1:37:29.220\n So what's your intuition about the idea of intelligence explosion?\n\n1:37:29.220 --> 1:37:34.920\n So there's this, you know, when you start to think about that leap from the near term\n\n1:37:34.920 --> 1:37:40.120\n to the long term, the natural inclination, like for me, sort of building machine learning\n\n1:37:40.120 --> 1:37:45.180\n systems today, it seems like it's a lot of work to get the general intelligence, but\n\n1:37:45.180 --> 1:37:49.380\n there's some intuition of exponential growth of exponential improvement of intelligence\n\n1:37:49.380 --> 1:37:50.380\n explosion.\n\n1:37:50.380 --> 1:38:00.860\n Can you maybe try to elucidate, try to talk about what's your intuition about the possibility\n\n1:38:00.860 --> 1:38:05.500\n of an intelligence explosion, that it won't be this gradual slow process, there might\n\n1:38:05.500 --> 1:38:07.900\n be a phase shift?\n\n1:38:07.900 --> 1:38:13.500\n Yeah, I think it's, we don't know how explosive it will be.\n\n1:38:13.500 --> 1:38:19.420\n I think for what it's worth, it seems fairly likely to me that at some point, there will\n\n1:38:19.420 --> 1:38:24.540\n be some intelligence explosion, like some period of time, where progress in AI becomes\n\n1:38:24.540 --> 1:38:32.220\n extremely rapid, roughly, roughly in the area where you might say it's kind of humanish\n\n1:38:32.220 --> 1:38:40.600\n equivalent in core cognitive faculties, that the concept of human equivalent starts to\n\n1:38:40.600 --> 1:38:43.260\n break down when you look too closely at it.\n\n1:38:43.260 --> 1:38:48.300\n And just how explosive does something have to be for it to be called an intelligence\n\n1:38:48.300 --> 1:38:49.300\n explosion?\n\n1:38:49.300 --> 1:38:54.460\n Like, does it have to be like overnight, literally, or a few years?\n\n1:38:54.460 --> 1:39:00.420\n But overall, I guess, if you plotted the opinions of different people in the world, I guess\n\n1:39:00.420 --> 1:39:06.100\n that would be somewhat more probability towards the intelligence explosion scenario than probably\n\n1:39:06.100 --> 1:39:09.620\n the average, you know, AI researcher, I guess.\n\n1:39:09.620 --> 1:39:14.620\n So and then the other part of the intelligence explosion, or just forget explosion, just\n\n1:39:14.620 --> 1:39:21.540\n progress is once you achieve that gray area of human level intelligence, is it obvious\n\n1:39:21.540 --> 1:39:26.780\n to you that we should be able to proceed beyond it to get to super intelligence?\n\n1:39:26.780 --> 1:39:33.580\n Yeah, that seems, I mean, as much as any of these things can be obvious, given we've never\n\n1:39:33.580 --> 1:39:39.380\n had one, people have different views, smart people have different views, it's like some\n\n1:39:39.380 --> 1:39:44.860\n degree of uncertainty that always remains for any big, futuristic, philosophical grand\n\n1:39:44.860 --> 1:39:49.660\n question that just we realize humans are fallible, especially about these things.\n\n1:39:49.660 --> 1:39:55.460\n But it does seem, as far as I'm judging things based on my own impressions, that it seems\n\n1:39:55.460 --> 1:40:04.340\n very unlikely that that would be a ceiling at or near human cognitive capacity.\n\n1:40:04.340 --> 1:40:10.260\n And that's such a, I don't know, that's such a special moment, it's both terrifying and\n\n1:40:10.260 --> 1:40:15.020\n exciting to create a system that's beyond our intelligence.\n\n1:40:15.020 --> 1:40:22.140\n So maybe you can step back and say, like, how does that possibility make you feel that\n\n1:40:22.140 --> 1:40:28.700\n we can create something, it feels like there's a line beyond which it steps, it'll be able\n\n1:40:28.700 --> 1:40:31.180\n to outsmart you.\n\n1:40:31.180 --> 1:40:35.060\n And therefore, it feels like a step where we lose control.\n\n1:40:35.060 --> 1:40:42.080\n Well, I don't think the latter follows that is you could imagine.\n\n1:40:42.080 --> 1:40:46.500\n And in fact, this is what a number of people are working towards making sure that we could\n\n1:40:46.500 --> 1:40:53.060\n ultimately project higher levels of problem solving ability while still making sure that\n\n1:40:53.060 --> 1:40:58.620\n they are aligned, like they are in the service of human values.\n\n1:40:58.620 --> 1:41:06.300\n I mean, so losing control, I think, is not a given that that would happen.\n\n1:41:06.300 --> 1:41:10.060\n Now you asked how it makes me feel, I mean, to some extent, I've lived with this for so\n\n1:41:10.060 --> 1:41:16.820\n long, since as long as I can remember, being an adult or even a teenager, it seemed to\n\n1:41:16.820 --> 1:41:19.780\n me obvious that at some point, AI will succeed.\n\n1:41:19.780 --> 1:41:27.020\n And so I actually misspoke, I didn't mean control, I meant, because the control problem\n\n1:41:27.020 --> 1:41:28.020\n is an interesting thing.\n\n1:41:28.020 --> 1:41:33.820\n And I think the hope is, at least we should be able to maintain control over systems that\n\n1:41:33.820 --> 1:41:35.500\n are smarter than us.\n\n1:41:35.500 --> 1:41:46.460\n But we do lose our specialness, it sort of will lose our place as the smartest, coolest\n\n1:41:46.460 --> 1:41:48.700\n thing on earth.\n\n1:41:48.700 --> 1:41:55.700\n And there's an ego involved with that, that humans aren't very good at dealing with.\n\n1:41:55.700 --> 1:41:59.860\n I mean, I value my intelligence as a human being.\n\n1:41:59.860 --> 1:42:04.740\n It seems like a big transformative step to realize there's something out there that's\n\n1:42:04.740 --> 1:42:05.740\n more intelligent.\n\n1:42:05.740 --> 1:42:09.580\n I mean, you don't see that as such a fundamentally...\n\n1:42:09.580 --> 1:42:14.300\n I think yes, a lot, I think it would be small, because I mean, I think there are already\n\n1:42:14.300 --> 1:42:18.980\n a lot of things out there that are, I mean, certainly, if you think the universe is big,\n\n1:42:18.980 --> 1:42:23.140\n there's going to be other civilizations that already have super intelligences, or that\n\n1:42:23.140 --> 1:42:29.240\n just naturally have brains the size of beach balls and are like, completely leaving us\n\n1:42:29.240 --> 1:42:30.900\n in the dust.\n\n1:42:30.900 --> 1:42:33.540\n And we haven't come face to face with them.\n\n1:42:33.540 --> 1:42:34.820\n We haven't come face to face.\n\n1:42:34.820 --> 1:42:41.900\n But I mean, that's an open question, what would happen in a kind of post human world?\n\n1:42:41.900 --> 1:42:49.300\n Like how much day to day would these super intelligences be involved in the lives of\n\n1:42:49.300 --> 1:42:50.300\n ordinary?\n\n1:42:50.300 --> 1:42:54.420\n I mean, you could imagine some scenario where it would be more like a background thing that\n\n1:42:54.420 --> 1:42:58.980\n would help protect against some things, but you wouldn't like that, they wouldn't be this\n\n1:42:58.980 --> 1:43:04.620\n intrusive kind of, like making you feel bad by like, making clever jokes on your expert,\n\n1:43:04.620 --> 1:43:09.180\n like there's like all sorts of things that maybe in the human context would feel awkward\n\n1:43:09.180 --> 1:43:10.180\n about that.\n\n1:43:10.180 --> 1:43:14.500\n You don't want to be the dumbest kid in your class, everybody picks it, like, a lot of\n\n1:43:14.500 --> 1:43:19.380\n those things, maybe you need to abstract away from, if you're thinking about this context\n\n1:43:19.380 --> 1:43:26.260\n where we have infrastructure that is in some sense, beyond any or all humans.\n\n1:43:26.260 --> 1:43:30.780\n I mean, it's a little bit like, say, the scientific community as a whole, if you think of that\n\n1:43:30.780 --> 1:43:33.420\n as a mind, it's a little bit of a metaphor.\n\n1:43:33.420 --> 1:43:39.500\n But I mean, obviously, it's got to be like, way more capacious than any individual.\n\n1:43:39.500 --> 1:43:44.620\n So in some sense, there is this mind like thing already out there that's just vastly\n\n1:43:44.620 --> 1:43:49.760\n more intelligent than any individual is.\n\n1:43:49.760 --> 1:43:55.380\n And we think, okay, that's, you just accept that as a fact.\n\n1:43:55.380 --> 1:43:59.260\n That's the basic fabric of our existence is there's super intelligent.\n\n1:43:59.260 --> 1:44:06.020\n You get used to a lot of, I mean, there's already Google and Twitter and Facebook, these\n\n1:44:06.020 --> 1:44:13.220\n recommender systems that are the basic fabric of our, I could see them becoming, I mean,\n\n1:44:13.220 --> 1:44:17.480\n do you think of the collective intelligence of these systems as already perhaps reaching\n\n1:44:17.480 --> 1:44:19.260\n super intelligence level?\n\n1:44:19.260 --> 1:44:26.260\n Well, I mean, so here it comes to the concept of intelligence and the scale and what human\n\n1:44:26.260 --> 1:44:29.700\n level means.\n\n1:44:29.700 --> 1:44:37.820\n The kind of vagueness and indeterminacy of those concepts starts to dominate how you\n\n1:44:37.820 --> 1:44:38.900\n would answer that question.\n\n1:44:38.900 --> 1:44:45.020\n So like, say the Google search engine has a very high capacity of a certain kind, like\n\n1:44:45.020 --> 1:44:54.940\n retrieving, remembering and retrieving information, particularly like text or images that are,\n\n1:44:54.940 --> 1:45:02.980\n you have a kind of string, a word string key, obviously superhuman at that, but a vast set\n\n1:45:02.980 --> 1:45:06.260\n of other things it can't even do at all.\n\n1:45:06.260 --> 1:45:12.780\n Not just not do well, but so you have these current AI systems that are superhuman in\n\n1:45:12.780 --> 1:45:19.300\n some limited domain and then like radically subhuman in all other domains.\n\n1:45:19.300 --> 1:45:23.900\n Same with a chess, like are just a simple computer that can multiply really large numbers,\n\n1:45:23.900 --> 1:45:24.900\n right?\n\n1:45:24.900 --> 1:45:28.060\n So it's going to have this like one spike of super intelligence and then a kind of a\n\n1:45:28.060 --> 1:45:32.300\n zero level of capability across all other cognitive fields.\n\n1:45:32.300 --> 1:45:37.340\n Yeah, I don't necessarily think the generalness, I mean, I'm not so attached with it, but I\n\n1:45:37.340 --> 1:45:44.340\n think it's sort of, it's a gray area and it's a feeling, but to me sort of alpha zero is\n\n1:45:44.340 --> 1:45:51.620\n somehow much more intelligent, much, much more intelligent than Deep Blue.\n\n1:45:51.620 --> 1:45:55.380\n And to say which domain, you could say, well, these are both just board games, they're both\n\n1:45:55.380 --> 1:45:59.380\n just able to play board games, who cares if they're going to do better or not, but there's\n\n1:45:59.380 --> 1:46:06.120\n something about the learning, the self play that makes it, crosses over into that land\n\n1:46:06.120 --> 1:46:09.460\n of intelligence that doesn't necessarily need to be general.\n\n1:46:09.460 --> 1:46:14.180\n In the same way, Google is much closer to Deep Blue currently in terms of its search\n\n1:46:14.180 --> 1:46:17.900\n engine than it is to sort of the alpha zero.\n\n1:46:17.900 --> 1:46:22.860\n And the moment it becomes, the moment these recommender systems really become more like\n\n1:46:22.860 --> 1:46:29.820\n alpha zero, but being able to learn a lot without the constraints of being heavily constrained\n\n1:46:29.820 --> 1:46:34.100\n by human interaction, that seems like a special moment in time.\n\n1:46:34.100 --> 1:46:43.140\n I mean, certainly learning ability seems to be an important facet of general intelligence,\n\n1:46:43.140 --> 1:46:48.200\n that you can take some new domain that you haven't seen before and you weren't specifically\n\n1:46:48.200 --> 1:46:52.340\n pre programmed for, and then figure out what's going on there and eventually become really\n\n1:46:52.340 --> 1:46:53.520\n good at it.\n\n1:46:53.520 --> 1:47:00.140\n So that's something alpha zero has much more of than Deep Blue had.\n\n1:47:00.140 --> 1:47:06.340\n And in fact, I mean, systems like alpha zero can learn not just Go, but other, in fact,\n\n1:47:06.340 --> 1:47:09.460\n probably beat Deep Blue in chess and so forth.\n\n1:47:09.460 --> 1:47:13.700\n So you do see this as general and it matches the intuition.\n\n1:47:13.700 --> 1:47:17.640\n We feel it's more intelligent and it also has more of this general purpose learning\n\n1:47:17.640 --> 1:47:20.100\n ability.\n\n1:47:20.100 --> 1:47:23.060\n And if we get systems that have even more general purpose learning ability, it might\n\n1:47:23.060 --> 1:47:28.100\n also trigger an even stronger intuition that they are actually starting to get smart.\n\n1:47:28.100 --> 1:47:33.980\n So if you were to pick a future, what do you think a utopia looks like with AGI systems?\n\n1:47:33.980 --> 1:47:40.540\n Sort of, is it the neural link brain computer interface world where we're kind of really\n\n1:47:40.540 --> 1:47:43.780\n closely interlinked with AI systems?\n\n1:47:43.780 --> 1:47:50.900\n Is it possibly where AGI systems replace us completely while maintaining the values and\n\n1:47:50.900 --> 1:47:53.500\n the consciousness?\n\n1:47:53.500 --> 1:47:57.460\n Is it something like it's a completely invisible fabric, like you mentioned, a society where\n\n1:47:57.460 --> 1:48:02.180\n just aids and a lot of stuff that we do like curing diseases and so on.\n\n1:48:02.180 --> 1:48:03.980\n What is utopia if you get to pick?\n\n1:48:03.980 --> 1:48:09.020\n Yeah, I mean, it is a good question and a deep and difficult one.\n\n1:48:09.020 --> 1:48:10.300\n I'm quite interested in it.\n\n1:48:10.300 --> 1:48:15.180\n I don't have all the answers yet, but I might never have.\n\n1:48:15.180 --> 1:48:19.660\n But I think there are some different observations one can make.\n\n1:48:19.660 --> 1:48:26.180\n One is if this scenario actually did come to pass, it would open up this vast space\n\n1:48:26.180 --> 1:48:30.500\n of possible modes of being.\n\n1:48:30.500 --> 1:48:36.260\n On one hand, material and resource constraints would just be like expanded dramatically.\n\n1:48:36.260 --> 1:48:41.900\n So there would be a lot of a big pie, let's say.\n\n1:48:41.900 --> 1:48:51.940\n Also it would enable us to do things, including to ourselves, it would just open up this much\n\n1:48:51.940 --> 1:48:59.100\n larger design space and option space than we have ever had access to in human history.\n\n1:48:59.100 --> 1:49:01.140\n I think two things follow from that.\n\n1:49:01.140 --> 1:49:08.420\n One is that we probably would need to make a fairly fundamental rethink of what ultimately\n\n1:49:08.420 --> 1:49:11.940\n we value, like think things through more from first principles.\n\n1:49:11.940 --> 1:49:15.140\n The context would be so different from the familiar that we could have just take what\n\n1:49:15.140 --> 1:49:21.260\n we've always been doing and then like, oh, well, we have this cleaning robot that cleans\n\n1:49:21.260 --> 1:49:24.780\n the dishes in the sink and a few other small things.\n\n1:49:24.780 --> 1:49:27.100\n I think we would have to go back to first principles.\n\n1:49:27.100 --> 1:49:31.560\n So even from the individual level, go back to the first principles of what is the meaning\n\n1:49:31.560 --> 1:49:35.540\n of life, what is happiness, what is fulfillment.\n\n1:49:35.540 --> 1:49:43.300\n And then also connected to this large space of resources is that it would be possible.\n\n1:49:43.300 --> 1:49:52.860\n And I think something we should aim for is to do well by the lights of more than one\n\n1:49:52.860 --> 1:49:55.260\n value system.\n\n1:49:55.260 --> 1:50:06.380\n That is, we wouldn't have to choose only one value criterion and say we're going to do\n\n1:50:06.380 --> 1:50:15.860\n something that scores really high on the metric of, say, hedonism, and then is like a zero\n\n1:50:15.860 --> 1:50:22.520\n by other criteria, like kind of wireheaded brain synovat, and it's like a lot of pleasure,\n\n1:50:22.520 --> 1:50:26.860\n that's good, but then like no beauty, no achievement like that.\n\n1:50:26.860 --> 1:50:32.740\n Or pick it up, I think to some significant, not unlimited sense, but the significant sense,\n\n1:50:32.740 --> 1:50:40.060\n it would be possible to do very well by many criteria, like maybe you could get like 98%\n\n1:50:40.060 --> 1:50:47.900\n of the best according to several criteria at the same time, given this great expansion\n\n1:50:47.900 --> 1:50:50.820\n of the option space.\n\n1:50:50.820 --> 1:50:57.400\n So have competing value systems, competing criteria, as a sort of forever, just like\n\n1:50:57.400 --> 1:51:02.780\n our Democrat versus Republican, there seems to be this always multiple parties that are\n\n1:51:02.780 --> 1:51:08.260\n useful for our progress in society, even though it might seem dysfunctional inside the moment,\n\n1:51:08.260 --> 1:51:14.820\n but having the multiple value system seems to be beneficial for, I guess, a balance of\n\n1:51:14.820 --> 1:51:15.820\n power.\n\n1:51:15.820 --> 1:51:21.740\n So that's, yeah, not exactly what I have in mind that it, well, although maybe in an indirect\n\n1:51:21.740 --> 1:51:30.460\n way it is, but that if you had the chance to do something that scored well on several\n\n1:51:30.460 --> 1:51:36.300\n different metrics, our first instinct should be to do that rather than immediately leap\n\n1:51:36.300 --> 1:51:40.940\n to the thing, which ones of these value systems are we going to screw over?\n\n1:51:40.940 --> 1:51:44.680\n Like our first, let's first try to do very well by all of them.\n\n1:51:44.680 --> 1:51:49.140\n Then it might be that you can't get 100% of all and you would have to then like have the\n\n1:51:49.140 --> 1:51:51.740\n hard conversation about which one will only get 97%.\n\n1:51:51.740 --> 1:51:52.740\n There you go.\n\n1:51:52.740 --> 1:51:57.540\n There's my cynicism that all of existence is always a trade off, but you say, maybe\n\n1:51:57.540 --> 1:51:58.900\n it's not such a bad trade off.\n\n1:51:58.900 --> 1:52:00.100\n Let's first at least try it.\n\n1:52:00.100 --> 1:52:06.140\n Well, this would be a distinctive context in which at least some of the constraints\n\n1:52:06.140 --> 1:52:07.140\n would be removed.\n\n1:52:07.140 --> 1:52:08.140\n I'll leave it at that.\n\n1:52:08.140 --> 1:52:10.560\n So there's probably still be trade offs in the end.\n\n1:52:10.560 --> 1:52:16.820\n It's just that we should first make sure we at least take advantage of this abundance.\n\n1:52:16.820 --> 1:52:21.580\n So in terms of thinking about this, like, yeah, one should think, I think in this kind\n\n1:52:21.580 --> 1:52:31.060\n of frame of mind of generosity and inclusiveness to different value systems and see how far\n\n1:52:31.060 --> 1:52:34.900\n one can get there at first.\n\n1:52:34.900 --> 1:52:41.860\n And I think one could do something that would be very good according to many different criteria.\n\n1:52:41.860 --> 1:52:50.300\n We kind of talked about AGI fundamentally transforming the value system of our existence,\n\n1:52:50.300 --> 1:52:52.620\n the meaning of life.\n\n1:52:52.620 --> 1:52:56.360\n But today, what do you think is the meaning of life?\n\n1:52:56.360 --> 1:52:59.620\n The silliest or perhaps the biggest question, what's the meaning of life?\n\n1:52:59.620 --> 1:53:03.060\n What's the meaning of existence?\n\n1:53:03.060 --> 1:53:07.980\n What gives your life fulfillment, purpose, happiness, meaning?\n\n1:53:07.980 --> 1:53:14.620\n Yeah, I think these are, I guess, a bunch of different but related questions in there\n\n1:53:14.620 --> 1:53:17.500\n that one can ask.\n\n1:53:17.500 --> 1:53:18.500\n Happiness meaning.\n\n1:53:18.500 --> 1:53:19.500\n Yeah.\n\n1:53:19.500 --> 1:53:22.900\n I mean, like you could imagine somebody getting a lot of happiness from something that they\n\n1:53:22.900 --> 1:53:27.060\n didn't think was meaningful.\n\n1:53:27.060 --> 1:53:31.620\n Like mindless, like watching reruns of some television series, waiting junk food, like\n\n1:53:31.620 --> 1:53:35.940\n maybe some people that gives pleasure, but they wouldn't think it had a lot of meaning.\n\n1:53:35.940 --> 1:53:39.740\n Whereas, conversely, something that might be quite loaded with meaning might not be\n\n1:53:39.740 --> 1:53:45.640\n very fun always, like some difficult achievement that really helps a lot of people, maybe requires\n\n1:53:45.640 --> 1:53:49.520\n self sacrifice and hard work.\n\n1:53:49.520 --> 1:53:57.400\n So these things can, I think, come apart, which is something to bear in mind also when\n\n1:53:57.400 --> 1:54:06.380\n if you're thinking about these utopia questions that you might, to actually start to do some\n\n1:54:06.380 --> 1:54:12.500\n constructive thinking about that, you might have to isolate and distinguish these different\n\n1:54:12.500 --> 1:54:16.360\n kinds of things that might be valuable in different ways.\n\n1:54:16.360 --> 1:54:20.060\n Make sure you can sort of clearly perceive each one of them and then you can think about\n\n1:54:20.060 --> 1:54:22.180\n how you can combine them.\n\n1:54:22.180 --> 1:54:27.540\n And just as you said, hopefully come up with a way to maximize all of them together.\n\n1:54:27.540 --> 1:54:33.300\n Yeah, or at least get, I mean, maximize or get like a very high score on a wide range\n\n1:54:33.300 --> 1:54:35.100\n of them, even if not literally all.\n\n1:54:35.100 --> 1:54:39.460\n You can always come up with values that are exactly opposed to one another, right?\n\n1:54:39.460 --> 1:54:45.340\n But I think for many values, they're kind of opposed with, if you place them within\n\n1:54:45.340 --> 1:54:51.860\n a certain dimensionality of your space, like there are shapes that are kind of, you can't\n\n1:54:51.860 --> 1:54:57.220\n untangle like in a given dimensionality, but if you start adding dimensions, then it might\n\n1:54:57.220 --> 1:55:02.140\n in many cases just be that they are easy to pull apart and you could.\n\n1:55:02.140 --> 1:55:07.300\n So we'll see how much space there is for that, but I think that there could be a lot in this\n\n1:55:07.300 --> 1:55:12.180\n context of radical abundance, if ever we get to that.\n\n1:55:12.180 --> 1:55:15.580\n I don't think there's a better way to end it, Nick.\n\n1:55:15.580 --> 1:55:20.980\n You've influenced a huge number of people to work on what could very well be the most\n\n1:55:20.980 --> 1:55:22.660\n important problems of our time.\n\n1:55:22.660 --> 1:55:23.660\n So it's a huge honor.\n\n1:55:23.660 --> 1:55:24.660\n Thank you so much for talking.\n\n1:55:24.660 --> 1:55:25.660\n Well, thank you for coming by, Lex.\n\n1:55:25.660 --> 1:55:26.660\n That was fun.\n\n1:55:26.660 --> 1:55:27.660\n Thank you.\n\n1:55:27.660 --> 1:55:31.940\n Thanks for listening to this conversation with Nick Bostrom, and thank you to our presenting\n\n1:55:31.940 --> 1:55:33.940\n sponsor, Cash App.\n\n1:55:33.940 --> 1:55:40.140\n Please consider supporting the podcast by downloading Cash App and using code LEXPodcast.\n\n1:55:40.140 --> 1:55:45.100\n If you enjoy this podcast, subscribe on YouTube, review it with five stars on Apple Podcast,\n\n1:55:45.100 --> 1:55:50.980\n subscribe on Patreon, or simply connect with me on Twitter at Lex Friedman.\n\n1:55:50.980 --> 1:55:55.420\n And now, let me leave you with some words from Nick Bostrom.\n\n1:55:55.420 --> 1:56:00.060\n Our approach to existential risks cannot be one of trial and error.\n\n1:56:00.060 --> 1:56:02.780\n There's no opportunity to learn from errors.\n\n1:56:02.780 --> 1:56:09.500\n The reactive approach, see what happens, limit damages, and learn from experience is unworkable.\n\n1:56:09.500 --> 1:56:13.140\n Rather, we must take a proactive approach.\n\n1:56:13.140 --> 1:56:17.900\n This requires foresight to anticipate new types of threats and a willingness to take\n\n1:56:17.900 --> 1:56:26.060\n decisive, preventative action and to bear the costs, moral and economic, of such actions.\n\n1:56:26.060 --> 1:56:43.340\n Thank you for listening, and hope to see you next time.\n\n"
}