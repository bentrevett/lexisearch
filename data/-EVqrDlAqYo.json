{
  "title": "Jeff Hawkins: Thousand Brains Theory of Intelligence | Lex Fridman Podcast #25",
  "id": "-EVqrDlAqYo",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:02.360\n The following is a conversation with Jeff Hawkins.\n\n00:02.360 --> 00:04.120\n He's the founder of the Redwood Center\n\n00:04.120 --> 00:08.980\n for Theoretical Neuroscience in 2002, and NuMenta in 2005.\n\n00:08.980 --> 00:11.920\n In his 2004 book, titled On Intelligence,\n\n00:11.920 --> 00:13.840\n and in the research before and after,\n\n00:13.840 --> 00:16.200\n he and his team have worked to reverse engineer\n\n00:16.200 --> 00:19.160\n the neural cortex, and propose artificial intelligence\n\n00:19.160 --> 00:21.360\n architectures, approaches, and ideas\n\n00:21.360 --> 00:23.640\n that are inspired by the human brain.\n\n00:23.640 --> 00:25.960\n These ideas include Hierarchical Tupperware Memory,\n\n00:25.960 --> 00:28.920\n HTM, from 2004, and new work,\n\n00:28.920 --> 00:30.720\n the Thousand Brains Theory of Intelligence\n\n00:30.720 --> 00:33.760\n from 2017, 18, and 19.\n\n00:33.760 --> 00:36.120\n Jeff's ideas have been an inspiration\n\n00:36.120 --> 00:38.200\n to many who have looked for progress\n\n00:38.200 --> 00:40.480\n beyond the current machine learning approaches,\n\n00:40.480 --> 00:42.720\n but they have also received criticism\n\n00:42.720 --> 00:44.680\n for lacking a body of empirical evidence\n\n00:44.680 --> 00:46.240\n supporting the models.\n\n00:46.240 --> 00:48.440\n This is always a challenge when seeking more\n\n00:48.440 --> 00:51.440\n than small incremental steps forward in AI.\n\n00:51.440 --> 00:54.120\n Jeff is a brilliant mind, and many of the ideas\n\n00:54.120 --> 00:56.500\n he has developed and aggregated from neuroscience\n\n00:56.500 --> 00:59.120\n are worth understanding and thinking about.\n\n00:59.120 --> 01:00.920\n There are limits to deep learning,\n\n01:00.920 --> 01:02.920\n as it is currently defined.\n\n01:02.920 --> 01:05.760\n Forward progress in AI is shrouded in mystery.\n\n01:05.760 --> 01:07.760\n My hope is that conversations like this\n\n01:07.760 --> 01:11.440\n can help provide an inspiring spark for new ideas.\n\n01:11.440 --> 01:14.020\n This is the Artificial Intelligence Podcast.\n\n01:14.020 --> 01:16.720\n If you enjoy it, subscribe on YouTube, iTunes,\n\n01:16.720 --> 01:18.640\n or simply connect with me on Twitter\n\n01:18.640 --> 01:21.520\n at Lex Friedman, spelled F R I D.\n\n01:21.520 --> 01:26.520\n And now, here's my conversation with Jeff Hawkins.\n\n01:26.780 --> 01:29.860\n Are you more interested in understanding the human brain\n\n01:29.860 --> 01:32.000\n or in creating artificial systems\n\n01:32.000 --> 01:34.640\n that have many of the same qualities\n\n01:34.640 --> 01:38.560\n but don't necessarily require that you actually understand\n\n01:38.560 --> 01:41.480\n the underpinning workings of our mind?\n\n01:41.480 --> 01:44.000\n So there's a clear answer to that question.\n\n01:44.000 --> 01:46.760\n My primary interest is understanding the human brain.\n\n01:46.760 --> 01:47.700\n No question about it.\n\n01:47.700 --> 01:52.700\n But I also firmly believe that we will not be able\n\n01:53.280 --> 01:55.040\n to create fully intelligent machines\n\n01:55.040 --> 01:57.280\n until we understand how the human brain works.\n\n01:57.280 --> 02:00.120\n So I don't see those as separate problems.\n\n02:00.120 --> 02:01.720\n I think there's limits to what can be done\n\n02:01.720 --> 02:03.540\n with machine intelligence if you don't understand\n\n02:03.540 --> 02:05.680\n the principles by which the brain works.\n\n02:05.680 --> 02:07.900\n And so I actually believe that studying the brain\n\n02:07.900 --> 02:11.960\n is actually the fastest way to get to machine intelligence.\n\n02:11.960 --> 02:14.640\n And within that, let me ask the impossible question,\n\n02:14.640 --> 02:17.160\n how do you, not define, but at least think\n\n02:17.160 --> 02:19.440\n about what it means to be intelligent?\n\n02:19.440 --> 02:22.280\n So I didn't try to answer that question first.\n\n02:22.280 --> 02:24.520\n We said, let's just talk about how the brain works\n\n02:24.520 --> 02:26.720\n and let's figure out how certain parts of the brain,\n\n02:26.720 --> 02:29.920\n mostly the neocortex, but some other parts too.\n\n02:29.920 --> 02:32.340\n The parts of the brain most associated with intelligence.\n\n02:32.340 --> 02:35.840\n And let's discover the principles by how they work.\n\n02:35.840 --> 02:39.360\n Because intelligence isn't just like some mechanism\n\n02:39.360 --> 02:40.680\n and it's not just some capabilities.\n\n02:40.680 --> 02:42.520\n It's like, okay, we don't even know\n\n02:42.520 --> 02:44.040\n where to begin on this stuff.\n\n02:44.040 --> 02:49.040\n And so now that we've made a lot of progress on this,\n\n02:49.320 --> 02:50.480\n after we've made a lot of progress\n\n02:50.480 --> 02:53.200\n on how the neocortex works, and we can talk about that,\n\n02:53.200 --> 02:55.840\n I now have a very good idea what's gonna be required\n\n02:55.840 --> 02:57.200\n to make intelligent machines.\n\n02:57.200 --> 02:59.600\n I can tell you today, some of the things\n\n02:59.600 --> 03:02.140\n are gonna be necessary, I believe,\n\n03:02.140 --> 03:03.480\n to create intelligent machines.\n\n03:03.480 --> 03:04.600\n Well, so we'll get there.\n\n03:04.600 --> 03:07.440\n We'll get to the neocortex and some of the theories\n\n03:07.440 --> 03:09.200\n of how the whole thing works.\n\n03:09.200 --> 03:11.760\n And you're saying, as we understand more and more\n\n03:11.760 --> 03:14.760\n about the neocortex, about our own human mind,\n\n03:14.760 --> 03:17.680\n we'll be able to start to more specifically define\n\n03:17.680 --> 03:18.680\n what it means to be intelligent.\n\n03:18.680 --> 03:21.840\n It's not useful to really talk about that until.\n\n03:21.840 --> 03:23.560\n I don't know if it's not useful.\n\n03:23.560 --> 03:26.160\n Look, there's a long history of AI, as you know.\n\n03:26.160 --> 03:28.900\n And there's been different approaches taken to it.\n\n03:28.900 --> 03:31.300\n And who knows, maybe they're all useful.\n\n03:32.240 --> 03:37.240\n So the good old fashioned AI, the expert systems,\n\n03:37.280 --> 03:38.920\n the current convolutional neural networks,\n\n03:38.920 --> 03:40.380\n they all have their utility.\n\n03:40.380 --> 03:43.780\n They all have a value in the world.\n\n03:43.780 --> 03:45.220\n But I would think almost everyone agree\n\n03:45.220 --> 03:46.620\n that none of them are really intelligent\n\n03:46.620 --> 03:49.860\n in a sort of a deep way that humans are.\n\n03:49.860 --> 03:53.620\n And so it's just the question of how do you get\n\n03:53.620 --> 03:56.420\n from where those systems were or are today\n\n03:56.420 --> 03:59.240\n to where a lot of people think we're gonna go.\n\n03:59.240 --> 04:02.340\n And there's a big, big gap there, a huge gap.\n\n04:02.340 --> 04:06.220\n And I think the quickest way of bridging that gap\n\n04:06.220 --> 04:08.820\n is to figure out how the brain does that.\n\n04:08.820 --> 04:10.100\n And then we can sit back and look and say,\n\n04:10.100 --> 04:12.980\n oh, which of these principles that the brain works on\n\n04:12.980 --> 04:15.140\n are necessary and which ones are not?\n\n04:15.140 --> 04:16.620\n Clearly, we don't have to build this in,\n\n04:16.620 --> 04:18.460\n and intelligent machines aren't gonna be built\n\n04:18.460 --> 04:22.720\n out of organic living cells.\n\n04:22.720 --> 04:24.700\n But there's a lot of stuff that goes on the brain\n\n04:24.700 --> 04:25.900\n that's gonna be necessary.\n\n04:25.900 --> 04:30.260\n So let me ask maybe, before we get into the fun details,\n\n04:30.260 --> 04:33.060\n let me ask maybe a depressing or a difficult question.\n\n04:33.060 --> 04:36.220\n Do you think it's possible that we will never\n\n04:36.220 --> 04:38.060\n be able to understand how our brain works,\n\n04:38.060 --> 04:41.820\n that maybe there's aspects to the human mind,\n\n04:41.820 --> 04:46.140\n like we ourselves cannot introspectively get to the core,\n\n04:46.140 --> 04:48.100\n that there's a wall you eventually hit?\n\n04:48.100 --> 04:50.220\n Yeah, I don't believe that's the case.\n\n04:52.020 --> 04:53.240\n I have never believed that's the case.\n\n04:53.240 --> 04:56.620\n There's not been a single thing humans have ever put\n\n04:56.620 --> 04:58.620\n their minds to that we've said, oh, we reached the wall,\n\n04:58.620 --> 04:59.700\n we can't go any further.\n\n04:59.700 --> 05:01.660\n It's just, people keep saying that.\n\n05:01.660 --> 05:03.380\n People used to believe that about life.\n\n05:03.380 --> 05:05.180\n Alain Vital, right, there's like,\n\n05:05.180 --> 05:06.380\n what's the difference between living matter\n\n05:06.380 --> 05:07.980\n and nonliving matter, something special\n\n05:07.980 --> 05:09.100\n that we never understand.\n\n05:09.100 --> 05:10.660\n We no longer think that.\n\n05:10.660 --> 05:14.220\n So there's no historical evidence that suggests this\n\n05:14.220 --> 05:16.340\n is the case, and I just never even consider\n\n05:16.340 --> 05:17.620\n that's a possibility.\n\n05:17.620 --> 05:21.860\n I would also say, today, we understand so much\n\n05:21.860 --> 05:22.820\n about the neocortex.\n\n05:22.820 --> 05:25.480\n We've made tremendous progress in the last few years\n\n05:25.480 --> 05:30.000\n that I no longer think of it as an open question.\n\n05:30.000 --> 05:32.100\n The answers are very clear to me.\n\n05:32.100 --> 05:34.800\n The pieces we don't know are clear to me,\n\n05:34.800 --> 05:36.740\n but the framework is all there, and it's like,\n\n05:36.740 --> 05:38.620\n oh, okay, we're gonna be able to do this.\n\n05:38.620 --> 05:41.100\n This is not a problem anymore, just takes time and effort,\n\n05:41.100 --> 05:44.060\n but there's no mystery, a big mystery anymore.\n\n05:44.060 --> 05:47.800\n So then let's get into it for people like myself\n\n05:47.800 --> 05:52.800\n who are not very well versed in the human brain,\n\n05:52.940 --> 05:53.840\n except my own.\n\n05:54.780 --> 05:57.300\n Can you describe to me, at the highest level,\n\n05:57.300 --> 05:59.140\n what are the different parts of the human brain,\n\n05:59.140 --> 06:02.060\n and then zooming in on the neocortex,\n\n06:02.060 --> 06:04.120\n the parts of the neocortex, and so on,\n\n06:04.120 --> 06:05.500\n a quick overview.\n\n06:05.500 --> 06:06.620\n Yeah, sure.\n\n06:06.620 --> 06:09.940\n The human brain, we can divide it roughly into two parts.\n\n06:10.780 --> 06:14.220\n There's the old parts, lots of pieces,\n\n06:14.220 --> 06:15.700\n and then there's the new part.\n\n06:15.700 --> 06:18.020\n The new part is the neocortex.\n\n06:18.020 --> 06:20.420\n It's new because it didn't exist before mammals.\n\n06:20.420 --> 06:22.180\n The only mammals have a neocortex,\n\n06:22.180 --> 06:24.780\n and in humans, in primates, it's very large.\n\n06:24.780 --> 06:26.900\n In the human brain, the neocortex occupies\n\n06:26.900 --> 06:30.660\n about 70 to 75% of the volume of the brain.\n\n06:30.660 --> 06:32.100\n It's huge.\n\n06:32.100 --> 06:34.860\n And the old parts of the brain are,\n\n06:34.860 --> 06:36.020\n there's lots of pieces there.\n\n06:36.020 --> 06:38.740\n There's the spinal cord, and there's the brain stem,\n\n06:38.740 --> 06:40.240\n and the cerebellum, and the different parts\n\n06:40.240 --> 06:42.020\n of the basal ganglia, and so on.\n\n06:42.020 --> 06:42.960\n In the old parts of the brain,\n\n06:42.960 --> 06:44.800\n you have the autonomic regulation,\n\n06:44.800 --> 06:46.280\n like breathing and heart rate.\n\n06:46.280 --> 06:49.460\n You have basic behaviors, so like walking and running\n\n06:49.460 --> 06:51.380\n are controlled by the old parts of the brain.\n\n06:51.380 --> 06:53.060\n All the emotional centers of the brain\n\n06:53.060 --> 06:53.940\n are in the old part of the brain,\n\n06:53.940 --> 06:55.380\n so when you feel anger or hungry, lust,\n\n06:55.380 --> 06:56.500\n or things like that, those are all\n\n06:56.500 --> 06:57.940\n in the old parts of the brain.\n\n06:57.940 --> 07:02.180\n And we associate with the neocortex\n\n07:02.180 --> 07:04.060\n all the things we think about as sort of\n\n07:04.060 --> 07:08.100\n high level perception and cognitive functions,\n\n07:08.100 --> 07:12.240\n anything from seeing and hearing and touching things\n\n07:12.240 --> 07:15.140\n to language to mathematics and engineering\n\n07:15.140 --> 07:16.940\n and science and so on.\n\n07:16.940 --> 07:19.760\n Those are all associated with the neocortex,\n\n07:19.760 --> 07:21.760\n and they're certainly correlated.\n\n07:21.760 --> 07:23.980\n Our abilities in those regards are correlated\n\n07:23.980 --> 07:25.820\n with the relative size of our neocortex\n\n07:25.820 --> 07:27.940\n compared to other mammals.\n\n07:27.940 --> 07:30.520\n So that's like the rough division,\n\n07:30.520 --> 07:32.740\n and you obviously can't understand\n\n07:32.740 --> 07:35.160\n the neocortex completely isolated,\n\n07:35.160 --> 07:37.020\n but you can understand a lot of it\n\n07:37.020 --> 07:40.340\n with just a few interfaces to the old parts of the brain,\n\n07:40.340 --> 07:44.980\n and so it gives you a system to study.\n\n07:44.980 --> 07:48.020\n The other remarkable thing about the neocortex,\n\n07:48.020 --> 07:49.880\n compared to the old parts of the brain,\n\n07:49.880 --> 07:52.900\n is the neocortex is extremely uniform.\n\n07:52.900 --> 07:55.860\n It's not visibly or anatomically,\n\n07:57.060 --> 07:59.460\n it's very, I always like to say\n\n07:59.460 --> 08:01.300\n it's like the size of a dinner napkin,\n\n08:01.300 --> 08:03.740\n about two and a half millimeters thick,\n\n08:03.740 --> 08:05.980\n and it looks remarkably the same everywhere.\n\n08:05.980 --> 08:07.900\n Everywhere you look in that two and a half millimeters\n\n08:07.900 --> 08:10.060\n is this detailed architecture,\n\n08:10.060 --> 08:11.580\n and it looks remarkably the same everywhere,\n\n08:11.580 --> 08:12.620\n and that's across species.\n\n08:12.620 --> 08:15.380\n A mouse versus a cat and a dog and a human.\n\n08:15.380 --> 08:17.060\n Where if you look at the old parts of the brain,\n\n08:17.060 --> 08:19.620\n there's lots of little pieces do specific things.\n\n08:19.620 --> 08:22.060\n So it's like the old parts of our brain evolved,\n\n08:22.060 --> 08:23.660\n like this is the part that controls heart rate,\n\n08:23.660 --> 08:24.860\n and this is the part that controls this,\n\n08:24.860 --> 08:25.780\n and this is this kind of thing,\n\n08:25.780 --> 08:27.180\n and that's this kind of thing,\n\n08:27.180 --> 08:30.100\n and these evolved for eons a long, long time,\n\n08:30.100 --> 08:31.580\n and they have their specific functions,\n\n08:31.580 --> 08:33.220\n and all of a sudden mammals come along,\n\n08:33.220 --> 08:35.180\n and they got this thing called the neocortex,\n\n08:35.180 --> 08:38.140\n and it got large by just replicating the same thing\n\n08:38.140 --> 08:39.420\n over and over and over again.\n\n08:39.420 --> 08:42.660\n This is like, wow, this is incredible.\n\n08:42.660 --> 08:45.380\n So all the evidence we have,\n\n08:46.260 --> 08:50.020\n and this is an idea that was first articulated\n\n08:50.020 --> 08:52.020\n in a very cogent and beautiful argument\n\n08:52.020 --> 08:55.660\n by a guy named Vernon Malcastle in 1978, I think it was,\n\n08:56.820 --> 09:01.580\n that the neocortex all works on the same principle.\n\n09:01.580 --> 09:05.260\n So language, hearing, touch, vision, engineering,\n\n09:05.260 --> 09:06.980\n all these things are basically underlying,\n\n09:06.980 --> 09:10.340\n are all built on the same computational substrate.\n\n09:10.340 --> 09:11.820\n They're really all the same problem.\n\n09:11.820 --> 09:14.860\n So the low level of the building blocks all look similar.\n\n09:14.860 --> 09:16.300\n Yeah, and they're not even that low level.\n\n09:16.300 --> 09:17.900\n We're not talking about like neurons.\n\n09:17.900 --> 09:19.940\n We're talking about this very complex circuit\n\n09:19.940 --> 09:21.420\n that exists throughout the neocortex.\n\n09:21.420 --> 09:23.500\n It's remarkably similar.\n\n09:23.500 --> 09:26.580\n It's like, yes, you see variations of it here and there,\n\n09:26.580 --> 09:29.620\n more of the cell, less and less, and so on.\n\n09:29.620 --> 09:32.700\n But what Malcastle argued was, he says,\n\n09:32.700 --> 09:35.580\n you know, if you take a section of neocortex,\n\n09:35.580 --> 09:38.580\n why is one a visual area and one is a auditory area?\n\n09:38.580 --> 09:41.180\n Or why is, and his answer was,\n\n09:41.180 --> 09:43.180\n it's because one is connected to eyes\n\n09:43.180 --> 09:45.380\n and one is connected to ears.\n\n09:45.380 --> 09:47.820\n Literally, you mean just it's most closest\n\n09:47.820 --> 09:49.020\n in terms of number of connections\n\n09:49.020 --> 09:50.900\n to the sensor. Literally, literally,\n\n09:50.900 --> 09:53.780\n if you took the optic nerve and attached it\n\n09:53.780 --> 09:55.300\n to a different part of the neocortex,\n\n09:55.300 --> 09:57.940\n that part would become a visual region.\n\n09:57.940 --> 10:00.380\n This actually, this experiment was actually done\n\n10:00.380 --> 10:04.980\n by Merkankasur in developing, I think it was lemurs,\n\n10:04.980 --> 10:06.700\n I can't remember what it was, some animal.\n\n10:06.700 --> 10:08.540\n And there's a lot of evidence to this.\n\n10:08.540 --> 10:09.940\n You know, if you take a blind person,\n\n10:09.940 --> 10:12.180\n a person who's born blind at birth,\n\n10:12.180 --> 10:15.420\n they're born with a visual neocortex.\n\n10:15.420 --> 10:18.260\n It doesn't, may not get any input from the eyes\n\n10:18.260 --> 10:21.260\n because of some congenital defect or something.\n\n10:21.260 --> 10:24.700\n And that region becomes, does something else.\n\n10:24.700 --> 10:27.020\n It picks up another task.\n\n10:27.020 --> 10:32.020\n So, and it's, so it's this very complex thing.\n\n10:32.300 --> 10:33.740\n It's not like, oh, they're all built on neurons.\n\n10:33.740 --> 10:36.460\n No, they're all built in this very complex circuit\n\n10:36.460 --> 10:40.300\n and somehow that circuit underlies everything.\n\n10:40.300 --> 10:43.580\n And so this is the, it's called\n\n10:43.580 --> 10:45.900\n the common cortical algorithm, if you will.\n\n10:45.900 --> 10:47.980\n Some scientists just find it hard to believe\n\n10:47.980 --> 10:50.060\n and they just, I can't believe that's true,\n\n10:50.060 --> 10:52.100\n but the evidence is overwhelming in this case.\n\n10:52.100 --> 10:54.340\n And so a large part of what it means\n\n10:54.340 --> 10:56.420\n to figure out how the brain creates intelligence\n\n10:56.420 --> 10:59.860\n and what is intelligence in the brain\n\n10:59.860 --> 11:02.020\n is to understand what that circuit does.\n\n11:02.020 --> 11:05.020\n If you can figure out what that circuit does,\n\n11:05.020 --> 11:06.940\n as amazing as it is, then you can,\n\n11:06.940 --> 11:08.620\n then you understand what all these\n\n11:08.620 --> 11:10.500\n other cognitive functions are.\n\n11:10.500 --> 11:13.300\n So if you were to sort of put neocortex\n\n11:13.300 --> 11:15.140\n outside of your book on intelligence,\n\n11:15.140 --> 11:18.020\n you look, if you wrote a giant tome, a textbook\n\n11:18.020 --> 11:21.980\n on the neocortex, and you look maybe\n\n11:21.980 --> 11:23.740\n a couple of centuries from now,\n\n11:23.740 --> 11:26.500\n how much of what we know now would still be accurate\n\n11:26.500 --> 11:27.660\n two centuries from now?\n\n11:27.660 --> 11:30.820\n So how close are we in terms of understanding?\n\n11:30.820 --> 11:32.980\n I have to speak from my own particular experience here.\n\n11:32.980 --> 11:35.860\n So I run a small research lab here.\n\n11:35.860 --> 11:38.020\n It's like any other research lab.\n\n11:38.020 --> 11:39.420\n I'm sort of the principal investigator.\n\n11:39.420 --> 11:40.260\n There's actually two of us\n\n11:40.260 --> 11:42.540\n and there's a bunch of other people.\n\n11:42.540 --> 11:43.820\n And this is what we do.\n\n11:43.820 --> 11:46.060\n We study the neocortex and we publish our results\n\n11:46.060 --> 11:46.900\n and so on.\n\n11:46.900 --> 11:48.460\n So about three years ago,\n\n11:49.820 --> 11:52.460\n we had a real breakthrough in this field.\n\n11:52.460 --> 11:53.300\n Just tremendous breakthrough.\n\n11:53.300 --> 11:56.500\n We've now published, I think, three papers on it.\n\n11:56.500 --> 12:00.180\n And so I have a pretty good understanding\n\n12:00.180 --> 12:02.300\n of all the pieces and what we're missing.\n\n12:02.300 --> 12:06.260\n I would say that almost all the empirical data\n\n12:06.260 --> 12:08.540\n we've collected about the brain, which is enormous.\n\n12:08.540 --> 12:10.340\n If you don't know the neuroscience literature,\n\n12:10.340 --> 12:13.980\n it's just incredibly big.\n\n12:13.980 --> 12:16.860\n And it's, for the most part, all correct.\n\n12:16.860 --> 12:21.660\n It's facts and experimental results and measurements\n\n12:21.660 --> 12:22.980\n and all kinds of stuff.\n\n12:22.980 --> 12:25.860\n But none of that has been really assimilated\n\n12:25.860 --> 12:27.900\n into a theoretical framework.\n\n12:27.900 --> 12:32.300\n It's data without, in the language of Thomas Kuhn,\n\n12:32.300 --> 12:35.340\n the historian, would be a sort of a pre paradigm science.\n\n12:35.340 --> 12:38.180\n Lots of data, but no way to fit it together.\n\n12:38.180 --> 12:39.540\n I think almost all of that's correct.\n\n12:39.540 --> 12:42.180\n There's just gonna be some mistakes in there.\n\n12:42.180 --> 12:43.300\n And for the most part,\n\n12:43.300 --> 12:45.940\n there aren't really good cogent theories about it,\n\n12:45.940 --> 12:47.300\n how to put it together.\n\n12:47.300 --> 12:50.060\n It's not like we have two or three competing good theories,\n\n12:50.060 --> 12:51.540\n which ones are right and which ones are wrong.\n\n12:51.540 --> 12:54.780\n It's like, nah, people are just scratching their heads.\n\n12:54.780 --> 12:55.620\n Some people have given up\n\n12:55.620 --> 12:57.620\n on trying to figure out what the whole thing does.\n\n12:57.620 --> 13:01.020\n In fact, there's very, very few labs that we do\n\n13:01.020 --> 13:03.340\n that focus really on theory\n\n13:03.340 --> 13:06.780\n and all this unassimilated data and trying to explain it.\n\n13:06.780 --> 13:08.940\n So it's not like we've got it wrong.\n\n13:08.940 --> 13:11.100\n It's just that we haven't got it at all.\n\n13:11.100 --> 13:15.020\n So it's really, I would say, pretty early days\n\n13:15.020 --> 13:19.060\n in terms of understanding the fundamental theory's forces\n\n13:19.060 --> 13:20.220\n of the way our mind works.\n\n13:20.220 --> 13:21.500\n I don't think so.\n\n13:21.500 --> 13:23.740\n I would have said that's true five years ago.\n\n13:25.340 --> 13:26.980\n So as I said,\n\n13:26.980 --> 13:29.300\n we had some really big breakthroughs on this recently\n\n13:29.300 --> 13:30.780\n and we started publishing papers on this.\n\n13:30.780 --> 13:34.180\n So we'll get to that.\n\n13:34.180 --> 13:35.940\n But so I don't think it's,\n\n13:35.940 --> 13:38.260\n I'm an optimist and from where I sit today,\n\n13:38.260 --> 13:39.420\n most people would disagree with this,\n\n13:39.420 --> 13:41.620\n but from where I sit today, from what I know,\n\n13:43.260 --> 13:44.940\n it's not super early days anymore.\n\n13:44.940 --> 13:46.860\n We are, the way these things go\n\n13:46.860 --> 13:48.180\n is it's not a linear path, right?\n\n13:48.180 --> 13:49.820\n You don't just start accumulating\n\n13:49.820 --> 13:50.820\n and get better and better and better.\n\n13:50.820 --> 13:52.900\n No, all this stuff you've collected,\n\n13:52.900 --> 13:53.780\n none of it makes sense.\n\n13:53.780 --> 13:55.580\n All these different things are just sort of around.\n\n13:55.580 --> 13:57.100\n And then you're gonna have some breaking points\n\n13:57.100 --> 13:59.420\n where all of a sudden, oh my God, now we got it right.\n\n13:59.420 --> 14:01.100\n That's how it goes in science.\n\n14:01.100 --> 14:04.460\n And I personally feel like we passed that little thing\n\n14:04.460 --> 14:06.300\n about a couple of years ago,\n\n14:06.300 --> 14:07.580\n all that big thing a couple of years ago.\n\n14:07.580 --> 14:09.620\n So we can talk about that.\n\n14:09.620 --> 14:11.020\n Time will tell if I'm right,\n\n14:11.020 --> 14:12.660\n but I feel very confident about it.\n\n14:12.660 --> 14:15.220\n That's why I'm willing to say it on tape like this.\n\n14:15.220 --> 14:18.060\n At least very optimistic.\n\n14:18.060 --> 14:20.220\n So let's, before those few years ago,\n\n14:20.220 --> 14:23.260\n let's take a step back to HTM,\n\n14:23.260 --> 14:26.020\n the hierarchical temporal memory theory,\n\n14:26.020 --> 14:27.580\n which you first proposed on intelligence\n\n14:27.580 --> 14:29.340\n and went through a few different generations.\n\n14:29.340 --> 14:31.300\n Can you describe what it is,\n\n14:31.300 --> 14:33.740\n how it evolved through the three generations\n\n14:33.740 --> 14:35.460\n since you first put it on paper?\n\n14:35.460 --> 14:39.340\n Yeah, so one of the things that neuroscientists\n\n14:39.340 --> 14:42.980\n just sort of missed for many, many years,\n\n14:42.980 --> 14:45.820\n and especially people who were thinking about theory,\n\n14:45.820 --> 14:47.780\n was the nature of time in the brain.\n\n14:49.100 --> 14:51.700\n Brains process information through time.\n\n14:51.700 --> 14:52.900\n The information coming into the brain\n\n14:52.900 --> 14:54.180\n is constantly changing.\n\n14:55.220 --> 14:57.620\n The patterns from my speech right now,\n\n14:57.620 --> 15:00.140\n if you were listening to it at normal speed,\n\n15:00.140 --> 15:01.500\n would be changing on your ears\n\n15:01.500 --> 15:04.100\n about every 10 milliseconds or so, you'd have a change.\n\n15:04.100 --> 15:06.740\n This constant flow, when you look at the world,\n\n15:06.740 --> 15:08.220\n your eyes are moving constantly,\n\n15:08.220 --> 15:09.700\n three to five times a second,\n\n15:09.700 --> 15:11.380\n and the input's completely changing.\n\n15:11.380 --> 15:13.500\n If I were to touch something like a coffee cup,\n\n15:13.500 --> 15:15.220\n as I move my fingers, the input changes.\n\n15:15.220 --> 15:19.500\n So this idea that the brain works on time changing patterns\n\n15:19.500 --> 15:22.340\n is almost completely, or was almost completely missing\n\n15:22.340 --> 15:23.620\n from a lot of the basic theories,\n\n15:23.620 --> 15:25.020\n like fears of vision and so on.\n\n15:25.020 --> 15:26.860\n It's like, oh no, we're gonna put this image\n\n15:26.860 --> 15:29.580\n in front of you and flash it and say, what is it?\n\n15:29.580 --> 15:32.180\n Convolutional neural networks work that way today, right?\n\n15:32.180 --> 15:34.220\n Classify this picture.\n\n15:34.220 --> 15:35.980\n But that's not what vision is like.\n\n15:35.980 --> 15:38.740\n Vision is this sort of crazy time based pattern\n\n15:38.740 --> 15:40.060\n that's going all over the place,\n\n15:40.060 --> 15:41.820\n and so is touch and so is hearing.\n\n15:41.820 --> 15:43.780\n So the first part of hierarchical temporal memory\n\n15:43.780 --> 15:45.060\n was the temporal part.\n\n15:45.060 --> 15:48.260\n It's to say, you won't understand the brain,\n\n15:48.260 --> 15:50.020\n nor will you understand intelligent machines\n\n15:50.020 --> 15:52.460\n unless you're dealing with time based patterns.\n\n15:52.460 --> 15:55.460\n The second thing was, the memory component of it was,\n\n15:55.460 --> 16:00.300\n is to say that we aren't just processing input,\n\n16:00.300 --> 16:02.820\n we learn a model of the world.\n\n16:02.820 --> 16:05.500\n And the memory stands for that model.\n\n16:05.500 --> 16:07.340\n The point of the brain, the part of the neocortex,\n\n16:07.340 --> 16:08.500\n it learns a model of the world.\n\n16:08.500 --> 16:11.580\n We have to store things, our experiences,\n\n16:11.580 --> 16:14.220\n in a form that leads to a model of the world.\n\n16:14.220 --> 16:15.700\n So we can move around the world,\n\n16:15.700 --> 16:17.380\n we can pick things up and do things and navigate\n\n16:17.380 --> 16:18.220\n and know how it's going on.\n\n16:18.220 --> 16:19.980\n So that's what the memory referred to.\n\n16:19.980 --> 16:22.100\n And many people just, they were thinking about\n\n16:22.100 --> 16:25.140\n like certain processes without memory at all.\n\n16:25.140 --> 16:26.740\n They're just like processing things.\n\n16:26.740 --> 16:29.020\n And then finally, the hierarchical component\n\n16:29.020 --> 16:32.260\n was a reflection to that the neocortex,\n\n16:32.260 --> 16:34.420\n although it's this uniform sheet of cells,\n\n16:35.820 --> 16:37.580\n different parts of it project to other parts,\n\n16:37.580 --> 16:39.340\n which project to other parts.\n\n16:39.340 --> 16:43.060\n And there is a sort of rough hierarchy in terms of that.\n\n16:43.060 --> 16:45.980\n So the hierarchical temporal memory is just saying,\n\n16:45.980 --> 16:47.700\n look, we should be thinking about the brain\n\n16:47.700 --> 16:52.020\n as time based, model memory based,\n\n16:52.020 --> 16:54.780\n and hierarchical processing.\n\n16:54.780 --> 16:58.180\n And that was a placeholder for a bunch of components\n\n16:58.180 --> 17:00.860\n that we would then plug into that.\n\n17:00.860 --> 17:02.620\n We still believe all those things I just said,\n\n17:02.620 --> 17:06.980\n but we now know so much more that I'm stopping to use\n\n17:06.980 --> 17:08.180\n the word hierarchical temporal memory yet\n\n17:08.180 --> 17:11.340\n because it's insufficient to capture the stuff we know.\n\n17:11.340 --> 17:13.660\n So again, it's not incorrect, but it's,\n\n17:13.660 --> 17:15.820\n I now know more and I would rather describe it\n\n17:15.820 --> 17:16.820\n more accurately.\n\n17:16.820 --> 17:20.340\n Yeah, so you're basically, we could think of HTM\n\n17:20.340 --> 17:24.780\n as emphasizing that there's three aspects of intelligence\n\n17:24.780 --> 17:25.900\n that are important to think about\n\n17:25.900 --> 17:28.900\n whatever the eventual theory it converges to.\n\n17:28.900 --> 17:32.460\n So in terms of time, how do you think of nature of time\n\n17:32.460 --> 17:33.860\n across different time scales?\n\n17:33.860 --> 17:36.820\n So you mentioned things changing,\n\n17:36.820 --> 17:39.140\n sensory inputs changing every 10, 20 minutes.\n\n17:39.140 --> 17:42.100\n What about every few minutes, every few months and years?\n\n17:42.100 --> 17:44.820\n Well, if you think about a neuroscience problem,\n\n17:44.820 --> 17:49.620\n the brain problem, neurons themselves can stay active\n\n17:49.620 --> 17:52.780\n for certain periods of time, parts of the brain\n\n17:52.780 --> 17:54.260\n where they stay active for minutes.\n\n17:54.260 --> 17:59.460\n You could hold a certain perception or an activity\n\n17:59.460 --> 18:01.580\n for a certain period of time,\n\n18:01.580 --> 18:04.820\n but most of them don't last that long.\n\n18:04.820 --> 18:07.180\n And so if you think about your thoughts\n\n18:07.180 --> 18:09.180\n are the activity of neurons,\n\n18:09.180 --> 18:10.580\n if you're gonna wanna involve something\n\n18:10.580 --> 18:11.980\n that happened a long time ago,\n\n18:11.980 --> 18:14.420\n even just this morning, for example,\n\n18:14.420 --> 18:16.420\n the neurons haven't been active throughout that time.\n\n18:16.420 --> 18:17.860\n So you have to store that.\n\n18:17.860 --> 18:20.860\n So if I ask you, what did you have for breakfast today?\n\n18:20.860 --> 18:23.660\n That is memory, that is you've built into your model\n\n18:23.660 --> 18:24.860\n the world now, you remember that.\n\n18:24.860 --> 18:27.780\n And that memory is in the synapses,\n\n18:27.780 --> 18:29.980\n is basically in the formation of synapses.\n\n18:29.980 --> 18:34.700\n And so you're sliding into what,\n\n18:34.700 --> 18:36.660\n you know, it's the different timescales.\n\n18:36.660 --> 18:39.060\n There's timescales of which we are like understanding\n\n18:39.060 --> 18:41.260\n my language and moving about and seeing things rapidly\n\n18:41.260 --> 18:42.540\n and over time, that's the timescales\n\n18:42.540 --> 18:44.220\n of activities of neurons.\n\n18:44.220 --> 18:46.140\n But if you wanna get in longer timescales,\n\n18:46.140 --> 18:47.100\n then it's more memory.\n\n18:47.100 --> 18:49.460\n And we have to invoke those memories to say,\n\n18:49.460 --> 18:51.740\n oh yes, well now I can remember what I had for breakfast\n\n18:51.740 --> 18:54.180\n because I stored that someplace.\n\n18:54.180 --> 18:58.140\n I may forget it tomorrow, but I'd store it for now.\n\n18:58.140 --> 19:01.620\n So does memory also need to have,\n\n19:02.820 --> 19:06.180\n so the hierarchical aspect of reality\n\n19:06.180 --> 19:08.780\n is not just about concepts, it's also about time?\n\n19:08.780 --> 19:10.260\n Do you think of it that way?\n\n19:10.260 --> 19:12.820\n Yeah, time is infused in everything.\n\n19:12.820 --> 19:15.540\n It's like you really can't separate it out.\n\n19:15.540 --> 19:18.700\n If I ask you, what is your, you know,\n\n19:18.700 --> 19:21.340\n how's the brain learn a model of this coffee cup here?\n\n19:21.340 --> 19:23.220\n I have a coffee cup and I'm at the coffee cup.\n\n19:23.220 --> 19:25.980\n I say, well, time is not an inherent property\n\n19:25.980 --> 19:28.540\n of the model I have of this cup,\n\n19:28.540 --> 19:31.460\n whether it's a visual model or a tactile model.\n\n19:31.460 --> 19:32.580\n I can sense it through time,\n\n19:32.580 --> 19:34.900\n but the model itself doesn't really have much time.\n\n19:34.900 --> 19:36.420\n If I asked you, if I said,\n\n19:36.420 --> 19:38.980\n well, what is the model of my cell phone?\n\n19:38.980 --> 19:40.740\n My brain has learned a model of the cell phone.\n\n19:40.740 --> 19:43.380\n So if you have a smartphone like this,\n\n19:43.380 --> 19:45.700\n and I said, well, this has time aspects to it.\n\n19:45.700 --> 19:48.040\n I have expectations when I turn it on,\n\n19:48.040 --> 19:50.460\n what's gonna happen, what or how long it's gonna take\n\n19:50.460 --> 19:52.860\n to do certain things, if I bring up an app,\n\n19:52.860 --> 19:54.540\n what sequences, and so I have,\n\n19:54.540 --> 19:57.260\n and it's like melodies in the world, you know?\n\n19:57.260 --> 19:58.540\n Melody has a sense of time.\n\n19:58.540 --> 20:01.220\n So many things in the world move and act,\n\n20:01.220 --> 20:03.740\n and there's a sense of time related to them.\n\n20:03.740 --> 20:08.260\n Some don't, but most things do actually.\n\n20:08.260 --> 20:12.100\n So it's sort of infused throughout the models of the world.\n\n20:12.100 --> 20:13.700\n You build a model of the world,\n\n20:13.700 --> 20:16.420\n you're learning the structure of the objects in the world,\n\n20:16.420 --> 20:18.980\n and you're also learning how those things change\n\n20:18.980 --> 20:20.780\n through time.\n\n20:20.780 --> 20:23.900\n Okay, so it really is just a fourth dimension\n\n20:23.900 --> 20:26.760\n that's infused deeply, and you have to make sure\n\n20:26.760 --> 20:30.080\n that your models of intelligence incorporate it.\n\n20:30.940 --> 20:34.840\n So, like you mentioned, the state of neuroscience\n\n20:34.840 --> 20:37.800\n is deeply empirical, a lot of data collection.\n\n20:37.800 --> 20:41.420\n It's, you know, that's where it is.\n\n20:41.420 --> 20:43.100\n You mentioned Thomas Kuhn, right?\n\n20:43.100 --> 20:44.580\n Yeah.\n\n20:44.580 --> 20:48.020\n And then you're proposing a theory of intelligence,\n\n20:48.020 --> 20:50.460\n and which is really the next step,\n\n20:50.460 --> 20:52.900\n the really important step to take,\n\n20:52.900 --> 20:57.900\n but why is HTM, or what we'll talk about soon,\n\n21:00.840 --> 21:03.700\n the right theory?\n\n21:03.700 --> 21:07.700\n So is it more in the, is it backed by intuition?\n\n21:07.700 --> 21:09.920\n Is it backed by evidence?\n\n21:09.920 --> 21:11.980\n Is it backed by a mixture of both?\n\n21:11.980 --> 21:15.580\n Is it kind of closer to where string theory is in physics,\n\n21:15.580 --> 21:18.460\n where there's mathematical components\n\n21:18.460 --> 21:21.060\n which show that, you know what,\n\n21:21.060 --> 21:24.740\n it seems that this, it fits together too well\n\n21:24.740 --> 21:28.100\n for it not to be true, which is where string theory is.\n\n21:28.100 --> 21:29.500\n Is that where you're kind of seeing?\n\n21:29.500 --> 21:30.740\n It's a mixture of all those things,\n\n21:30.740 --> 21:32.780\n although definitely where we are right now\n\n21:32.780 --> 21:34.620\n is definitely much more on the empirical side\n\n21:34.620 --> 21:36.180\n than, let's say, string theory.\n\n21:37.060 --> 21:39.280\n The way this goes about, we're theorists, right?\n\n21:39.280 --> 21:41.580\n So we look at all this data, and we're trying to come up\n\n21:41.580 --> 21:44.340\n with some sort of model that explains it, basically,\n\n21:44.340 --> 21:46.860\n and there's, unlike string theory,\n\n21:46.860 --> 21:50.220\n there's vast more amounts of empirical data here\n\n21:50.220 --> 21:53.340\n that I think than most physicists deal with.\n\n21:54.660 --> 21:57.540\n And so our challenge is to sort through that\n\n21:57.540 --> 22:02.020\n and figure out what kind of constructs would explain this.\n\n22:02.020 --> 22:04.940\n And when we have an idea,\n\n22:04.940 --> 22:06.400\n you come up with a theory of some sort,\n\n22:06.400 --> 22:08.740\n you have lots of ways of testing it.\n\n22:08.740 --> 22:13.740\n First of all, there are 100 years of assimilated,\n\n22:13.740 --> 22:16.620\n assimilated, unassimilated empirical data from neuroscience.\n\n22:16.620 --> 22:18.140\n So we go back and read papers,\n\n22:18.140 --> 22:20.680\n and we say, oh, did someone find this already?\n\n22:20.680 --> 22:23.280\n We can predict X, Y, and Z,\n\n22:23.280 --> 22:25.220\n and maybe no one's even talked about it\n\n22:25.220 --> 22:28.180\n since 1972 or something, but we go back and find that,\n\n22:28.180 --> 22:31.140\n and we say, oh, either it can support the theory\n\n22:31.140 --> 22:33.420\n or it can invalidate the theory.\n\n22:33.420 --> 22:34.880\n And we say, okay, we have to start over again.\n\n22:34.880 --> 22:38.140\n Oh, no, it's supportive, let's keep going with that one.\n\n22:38.140 --> 22:42.260\n So the way I kind of view it, when we do our work,\n\n22:42.260 --> 22:45.460\n we look at all this empirical data,\n\n22:45.460 --> 22:47.700\n and what I call it is a set of constraints.\n\n22:47.700 --> 22:48.700\n We're not interested in something\n\n22:48.700 --> 22:49.900\n that's biologically inspired.\n\n22:49.900 --> 22:52.140\n We're trying to figure out how the actual brain works.\n\n22:52.140 --> 22:53.660\n So every piece of empirical data\n\n22:53.660 --> 22:55.500\n is a constraint on a theory.\n\n22:55.500 --> 22:57.020\n In theory, if you have the correct theory,\n\n22:57.020 --> 22:59.420\n it needs to explain every pin, right?\n\n22:59.420 --> 23:03.140\n So we have this huge number of constraints on the problem,\n\n23:03.140 --> 23:05.960\n which initially makes it very, very difficult.\n\n23:05.960 --> 23:07.220\n If you don't have many constraints,\n\n23:07.220 --> 23:08.500\n you can make up stuff all the day.\n\n23:08.500 --> 23:10.200\n You can say, oh, here's an answer on how you can do this,\n\n23:10.200 --> 23:11.360\n you can do that, you can do this.\n\n23:11.360 --> 23:13.760\n But if you consider all biology as a set of constraints,\n\n23:13.760 --> 23:15.580\n all neuroscience as a set of constraints,\n\n23:15.580 --> 23:17.240\n and even if you're working in one little part\n\n23:17.240 --> 23:18.380\n of the neocortex, for example,\n\n23:18.380 --> 23:20.620\n there are hundreds and hundreds of constraints.\n\n23:20.620 --> 23:22.460\n These are empirical constraints\n\n23:22.460 --> 23:24.840\n that it's very, very difficult initially\n\n23:24.840 --> 23:27.260\n to come up with a theoretical framework for that.\n\n23:27.260 --> 23:30.100\n But when you do, and it solves all those constraints\n\n23:30.100 --> 23:32.980\n at once, you have a high confidence\n\n23:32.980 --> 23:35.660\n that you got something close to correct.\n\n23:35.660 --> 23:39.160\n It's just mathematically almost impossible not to be.\n\n23:39.160 --> 23:43.900\n So that's the curse and the advantage of what we have.\n\n23:43.900 --> 23:45.260\n The curse is we have to solve,\n\n23:45.260 --> 23:48.960\n we have to meet all these constraints, which is really hard.\n\n23:48.960 --> 23:50.900\n But when you do meet them,\n\n23:50.900 --> 23:53.220\n then you have a great confidence\n\n23:53.220 --> 23:54.940\n that you've discovered something.\n\n23:54.940 --> 23:58.040\n In addition, then we work with scientific labs.\n\n23:58.040 --> 24:00.000\n So we'll say, oh, there's something we can't find,\n\n24:00.000 --> 24:01.260\n we can predict something,\n\n24:01.260 --> 24:04.180\n but we can't find it anywhere in the literature.\n\n24:04.180 --> 24:06.900\n So we will then, we have people we've collaborated with,\n\n24:06.900 --> 24:09.220\n we'll say, sometimes they'll say, you know what?\n\n24:09.220 --> 24:11.740\n I have some collected data, which I didn't publish,\n\n24:11.740 --> 24:13.020\n but we can go back and look at it\n\n24:13.020 --> 24:14.780\n and see if we can find that,\n\n24:14.780 --> 24:17.020\n which is much easier than designing a new experiment.\n\n24:17.020 --> 24:20.340\n You know, neuroscience experiments take a long time, years.\n\n24:20.340 --> 24:23.300\n So, although some people are doing that now too.\n\n24:23.300 --> 24:26.840\n So, but between all of these things,\n\n24:27.740 --> 24:29.100\n I think it's a reasonable,\n\n24:30.020 --> 24:31.620\n actually a very, very good approach.\n\n24:31.620 --> 24:35.020\n We are blessed with the fact that we can test our theories\n\n24:35.020 --> 24:37.100\n out the yin yang here because there's so much\n\n24:37.100 --> 24:39.640\n unassimilar data and we can also falsify our theories\n\n24:39.640 --> 24:41.460\n very easily, which we do often.\n\n24:41.460 --> 24:44.380\n So it's kind of reminiscent to whenever that was\n\n24:44.380 --> 24:47.300\n with Copernicus, you know, when you figure out\n\n24:47.300 --> 24:51.140\n that the sun's at the center of the solar system\n\n24:51.140 --> 24:54.900\n as opposed to earth, the pieces just fall into place.\n\n24:54.900 --> 24:59.580\n Yeah, I think that's the general nature of aha moments\n\n24:59.580 --> 25:02.020\n is, and it's Copernicus, it could be,\n\n25:02.020 --> 25:05.220\n you could say the same thing about Darwin,\n\n25:05.220 --> 25:07.580\n you could say the same thing about, you know,\n\n25:07.580 --> 25:09.660\n about the double helix,\n\n25:09.660 --> 25:12.780\n that people have been working on a problem for so long\n\n25:12.780 --> 25:14.580\n and have all this data and they can't make sense of it,\n\n25:14.580 --> 25:15.820\n they can't make sense of it.\n\n25:15.820 --> 25:17.420\n But when the answer comes to you\n\n25:17.420 --> 25:19.380\n and everything falls into place,\n\n25:19.380 --> 25:21.660\n it's like, oh my gosh, that's it.\n\n25:21.660 --> 25:23.080\n That's got to be right.\n\n25:23.080 --> 25:28.080\n I asked both Jim Watson and Francis Crick about this.\n\n25:29.140 --> 25:31.700\n I asked them, you know, when you were working on\n\n25:31.700 --> 25:34.460\n trying to discover the structure of the double helix,\n\n25:35.760 --> 25:39.620\n and when you came up with the sort of the structure\n\n25:39.620 --> 25:44.020\n that ended up being correct, but it was sort of a guess,\n\n25:44.020 --> 25:45.700\n you know, it wasn't really verified yet.\n\n25:45.700 --> 25:48.460\n I said, did you know that it was right?\n\n25:48.460 --> 25:50.220\n And they both said, absolutely.\n\n25:50.220 --> 25:51.860\n So we absolutely knew it was right.\n\n25:51.860 --> 25:54.740\n And it doesn't matter if other people didn't believe it\n\n25:54.740 --> 25:55.660\n or not, we knew it was right.\n\n25:55.660 --> 25:56.700\n They'd get around to thinking it\n\n25:56.700 --> 25:59.060\n and agree with it eventually anyway.\n\n25:59.060 --> 26:01.300\n And that's the kind of thing you hear a lot with scientists\n\n26:01.300 --> 26:04.220\n who really are studying a difficult problem.\n\n26:04.220 --> 26:07.140\n And I feel that way too about our work.\n\n26:07.140 --> 26:10.700\n Have you talked to Crick or Watson about the problem\n\n26:10.700 --> 26:15.700\n you're trying to solve, the, of finding the DNA of the brain?\n\n26:15.940 --> 26:19.960\n Yeah, in fact, Francis Crick was very interested in this\n\n26:19.960 --> 26:21.540\n in the latter part of his life.\n\n26:21.540 --> 26:23.780\n And in fact, I got interested in brains\n\n26:23.780 --> 26:26.900\n by reading an essay he wrote in 1979\n\n26:26.900 --> 26:28.800\n called Thinking About the Brain.\n\n26:28.800 --> 26:32.620\n And that was when I decided I'm gonna leave my profession\n\n26:32.620 --> 26:35.580\n of computers and engineering and become a neuroscientist.\n\n26:35.580 --> 26:37.660\n Just reading that one essay from Francis Crick.\n\n26:37.660 --> 26:39.840\n I got to meet him later in life.\n\n26:41.640 --> 26:44.660\n I spoke at the Salk Institute and he was in the audience.\n\n26:44.660 --> 26:46.660\n And then I had a tea with him afterwards.\n\n26:48.820 --> 26:50.620\n He was interested in a different problem.\n\n26:50.620 --> 26:52.380\n He was focused on consciousness.\n\n26:53.380 --> 26:54.260\n The easy problem, right?\n\n26:54.260 --> 26:58.640\n Well, I think it's the red herring.\n\n26:58.640 --> 27:01.300\n And so we weren't really overlapping a lot there.\n\n27:02.260 --> 27:04.580\n Jim Watson, who's still alive,\n\n27:05.380 --> 27:07.420\n is also interested in this problem.\n\n27:07.420 --> 27:09.020\n And he was, when he was director\n\n27:09.020 --> 27:11.140\n of the Cold Spring Harbor Laboratories,\n\n27:12.420 --> 27:15.140\n he was really sort of behind moving in the direction\n\n27:15.140 --> 27:16.580\n of neuroscience there.\n\n27:16.580 --> 27:19.300\n And so he had a personal interest in this field.\n\n27:20.220 --> 27:22.280\n And I have met with him numerous times.\n\n27:23.620 --> 27:27.680\n And in fact, the last time was a little bit over a year ago,\n\n27:27.680 --> 27:30.340\n I gave a talk at Cold Spring Harbor Labs\n\n27:30.340 --> 27:34.620\n about the progress we were making in our work.\n\n27:34.620 --> 27:39.620\n And it was a lot of fun because he said,\n\n27:39.860 --> 27:41.100\n well, you wouldn't be coming here\n\n27:41.100 --> 27:42.380\n unless you had something important to say.\n\n27:42.380 --> 27:44.740\n So I'm gonna go attend your talk.\n\n27:44.740 --> 27:46.620\n So he sat in the very front row.\n\n27:46.620 --> 27:50.140\n Next to him was the director of the lab, Bruce Stillman.\n\n27:50.140 --> 27:52.540\n So these guys are in the front row of this auditorium.\n\n27:52.540 --> 27:54.620\n Nobody else in the auditorium wants to sit in the front row\n\n27:54.620 --> 27:56.980\n because there's Jim Watson and there's the director.\n\n27:56.980 --> 28:01.980\n And I gave a talk and then I had dinner with him afterwards.\n\n28:03.700 --> 28:07.060\n But there's a great picture of my colleague Subitai Amantak\n\n28:07.060 --> 28:09.860\n where I'm up there sort of like screaming the basics\n\n28:09.860 --> 28:11.700\n of this new framework we have.\n\n28:11.700 --> 28:13.780\n And Jim Watson's on the edge of his chair.\n\n28:13.780 --> 28:15.180\n He's literally on the edge of his chair,\n\n28:15.180 --> 28:17.820\n like intently staring up at the screen.\n\n28:17.820 --> 28:21.740\n And when he discovered the structure of DNA,\n\n28:21.740 --> 28:23.800\n the first public talk he gave\n\n28:23.800 --> 28:25.940\n was at Cold Spring Harbor Labs.\n\n28:25.940 --> 28:27.460\n And there's a picture, there's a famous picture\n\n28:27.460 --> 28:29.340\n of Jim Watson standing at the whiteboard\n\n28:29.340 --> 28:31.540\n with an overrated thing pointing at something,\n\n28:31.540 --> 28:33.180\n pointing at the double helix with his pointer.\n\n28:33.180 --> 28:34.980\n And it actually looks a lot like the picture of me.\n\n28:34.980 --> 28:36.100\n So there was a sort of funny,\n\n28:36.100 --> 28:37.460\n there's Arian talking about the brain\n\n28:37.460 --> 28:39.300\n and there's Jim Watson staring intently at it.\n\n28:39.300 --> 28:41.620\n And of course there with, whatever, 60 years earlier,\n\n28:41.620 --> 28:44.260\n he was standing pointing at the double helix.\n\n28:44.260 --> 28:47.260\n That's one of the great discoveries in all of,\n\n28:47.260 --> 28:49.740\n whatever, biology, science, all science and DNA.\n\n28:49.740 --> 28:54.540\n So it's funny that there's echoes of that in your presentation.\n\n28:54.540 --> 28:58.360\n Do you think, in terms of evolutionary timeline and history,\n\n28:58.360 --> 29:01.960\n the development of the neocortex was a big leap?\n\n29:01.960 --> 29:06.060\n Or is it just a small step?\n\n29:07.020 --> 29:09.780\n So like, if we ran the whole thing over again,\n\n29:09.780 --> 29:12.660\n from the birth of life on Earth,\n\n29:12.660 --> 29:15.260\n how likely would we develop the mechanism of the neocortex?\n\n29:15.260 --> 29:17.220\n Okay, well those are two separate questions.\n\n29:17.220 --> 29:18.660\n One is, was it a big leap?\n\n29:18.660 --> 29:21.380\n And one was how likely it is, okay?\n\n29:21.380 --> 29:22.880\n They're not necessarily related.\n\n29:22.880 --> 29:23.720\n Maybe correlated.\n\n29:23.720 --> 29:25.100\n Maybe correlated, maybe not.\n\n29:25.100 --> 29:26.100\n And we don't really have enough data\n\n29:26.100 --> 29:28.100\n to make a judgment about that.\n\n29:28.100 --> 29:29.980\n I would say definitely it was a big leap.\n\n29:29.980 --> 29:30.980\n And I can tell you why.\n\n29:30.980 --> 29:34.060\n I don't think it was just another incremental step.\n\n29:34.060 --> 29:35.900\n I don't get that at the moment.\n\n29:35.900 --> 29:38.420\n I don't really have any idea how likely it is.\n\n29:38.420 --> 29:39.860\n If we look at evolution,\n\n29:39.860 --> 29:42.540\n we have one data point, which is Earth, right?\n\n29:42.540 --> 29:45.220\n Life formed on Earth billions of years ago,\n\n29:45.220 --> 29:48.100\n whether it was introduced here or it created it here,\n\n29:48.100 --> 29:49.560\n or someone introduced it, we don't really know,\n\n29:49.560 --> 29:51.220\n but it was here early.\n\n29:51.220 --> 29:55.140\n It took a long, long time to get to multicellular life.\n\n29:55.140 --> 29:57.200\n And then for multicellular life,\n\n29:58.940 --> 30:02.300\n it took a long, long time to get the neocortex.\n\n30:02.300 --> 30:05.460\n And we've only had the neocortex for a few 100,000 years.\n\n30:05.460 --> 30:08.000\n So that's like nothing, okay?\n\n30:08.000 --> 30:09.600\n So is it likely?\n\n30:09.600 --> 30:10.740\n Well, it certainly isn't something\n\n30:10.740 --> 30:13.560\n that happened right away on Earth.\n\n30:13.560 --> 30:15.200\n And there were multiple steps to get there.\n\n30:15.200 --> 30:17.220\n So I would say it's probably not gonna be something\n\n30:17.220 --> 30:18.260\n that would happen instantaneously\n\n30:18.260 --> 30:20.620\n on other planets that might have life.\n\n30:20.620 --> 30:23.160\n It might take several billion years on average.\n\n30:23.160 --> 30:24.380\n Is it likely?\n\n30:24.380 --> 30:25.740\n I don't know, but you'd have to survive\n\n30:25.740 --> 30:27.900\n for several billion years to find out.\n\n30:27.900 --> 30:29.340\n Probably.\n\n30:29.340 --> 30:30.260\n Is it a big leap?\n\n30:30.260 --> 30:35.260\n Yeah, I think it is a qualitative difference\n\n30:35.500 --> 30:37.860\n in all other evolutionary steps.\n\n30:37.860 --> 30:39.820\n I can try to describe that if you'd like.\n\n30:39.820 --> 30:41.980\n Sure, in which way?\n\n30:41.980 --> 30:43.940\n Yeah, I can tell you how.\n\n30:43.940 --> 30:47.740\n Pretty much, let's start with a little preface.\n\n30:47.740 --> 30:50.500\n Many of the things that humans are able to do\n\n30:50.500 --> 30:55.500\n do not have obvious survival advantages precedent.\n\n30:58.620 --> 31:00.260\n We could create music, is that,\n\n31:00.260 --> 31:02.700\n is there a really survival advantage to that?\n\n31:02.700 --> 31:03.900\n Maybe, maybe not.\n\n31:03.900 --> 31:04.900\n What about mathematics?\n\n31:04.900 --> 31:07.020\n Is there a real survival advantage to mathematics?\n\n31:07.020 --> 31:09.340\n Well, you could stretch it.\n\n31:09.340 --> 31:11.540\n You can try to figure these things out, right?\n\n31:13.140 --> 31:14.800\n But most of evolutionary history,\n\n31:14.800 --> 31:18.700\n everything had immediate survival advantages to it.\n\n31:18.700 --> 31:22.020\n So, I'll tell you a story, which I like,\n\n31:22.020 --> 31:26.500\n may or may not be true, but the story goes as follows.\n\n31:29.140 --> 31:30.860\n Organisms have been evolving for,\n\n31:30.860 --> 31:33.740\n since the beginning of life here on Earth,\n\n31:33.740 --> 31:35.700\n and adding this sort of complexity onto that,\n\n31:35.700 --> 31:36.860\n and this sort of complexity onto that,\n\n31:36.860 --> 31:39.700\n and the brain itself is evolved this way.\n\n31:39.700 --> 31:42.420\n In fact, there's old parts, and older parts,\n\n31:42.420 --> 31:43.740\n and older, older parts of the brain\n\n31:43.740 --> 31:45.500\n that kind of just keeps calming on new things,\n\n31:45.500 --> 31:47.260\n and we keep adding capabilities.\n\n31:47.260 --> 31:48.700\n When we got to the neocortex,\n\n31:48.700 --> 31:52.500\n initially it had a very clear survival advantage\n\n31:52.500 --> 31:54.380\n in that it produced better vision,\n\n31:54.380 --> 31:55.700\n and better hearing, and better touch,\n\n31:55.700 --> 31:57.780\n and maybe, and so on.\n\n31:57.780 --> 32:01.140\n But what I think happens is that evolution discovered,\n\n32:01.140 --> 32:05.100\n it took a mechanism, and this is in our recent theories,\n\n32:05.100 --> 32:08.140\n but it took a mechanism evolved a long time ago\n\n32:08.140 --> 32:10.380\n for navigating in the world, for knowing where you are.\n\n32:10.380 --> 32:13.360\n These are the so called grid cells and place cells\n\n32:13.360 --> 32:15.160\n of an old part of the brain.\n\n32:15.160 --> 32:20.160\n And it took that mechanism for building maps of the world,\n\n32:20.900 --> 32:22.580\n and knowing where you are on those maps,\n\n32:22.580 --> 32:24.140\n and how to navigate those maps,\n\n32:24.140 --> 32:27.060\n and turns it into a sort of a slimmed down,\n\n32:27.060 --> 32:28.380\n idealized version of it.\n\n32:29.540 --> 32:31.600\n And that idealized version could now apply\n\n32:31.600 --> 32:32.820\n to building maps of other things.\n\n32:32.820 --> 32:35.100\n Maps of coffee cups, and maps of phones,\n\n32:35.100 --> 32:36.460\n maps of mathematics.\n\n32:36.460 --> 32:37.300\n Concepts almost.\n\n32:37.300 --> 32:40.260\n Concepts, yes, and not just almost, exactly.\n\n32:40.260 --> 32:44.140\n And so, and it just started replicating this stuff, right?\n\n32:44.140 --> 32:45.220\n You just think more, and more, and more.\n\n32:45.220 --> 32:48.780\n So we went from being sort of dedicated purpose\n\n32:48.780 --> 32:51.460\n neural hardware to solve certain problems\n\n32:51.460 --> 32:53.200\n that are important to survival,\n\n32:53.200 --> 32:55.820\n to a general purpose neural hardware\n\n32:55.820 --> 32:58.100\n that could be applied to all problems.\n\n32:58.100 --> 33:01.700\n And now it's escaped the orbit of survival.\n\n33:02.600 --> 33:04.460\n We are now able to apply it to things\n\n33:04.460 --> 33:06.760\n which we find enjoyment,\n\n33:08.700 --> 33:13.700\n but aren't really clearly survival characteristics.\n\n33:13.700 --> 33:16.740\n And that it seems to only have happened in humans,\n\n33:16.740 --> 33:18.260\n to the large extent.\n\n33:19.260 --> 33:20.980\n And so that's what's going on,\n\n33:20.980 --> 33:22.940\n where we sort of have,\n\n33:22.940 --> 33:26.360\n we've sort of escaped the gravity of evolutionary pressure,\n\n33:26.360 --> 33:28.620\n in some sense, in the neocortex.\n\n33:28.620 --> 33:31.540\n And it now does things which are not,\n\n33:31.540 --> 33:32.780\n that are really interesting,\n\n33:32.780 --> 33:34.340\n discovering models of the universe,\n\n33:34.340 --> 33:36.100\n which may not really help us.\n\n33:36.100 --> 33:37.100\n Does it matter?\n\n33:37.100 --> 33:38.600\n How does it help us surviving,\n\n33:38.600 --> 33:40.240\n knowing that there might be multiverses,\n\n33:40.240 --> 33:42.940\n or that there might be the age of the universe,\n\n33:42.940 --> 33:46.140\n or how do various stellar things occur?\n\n33:46.140 --> 33:47.820\n It doesn't really help us survive at all.\n\n33:47.820 --> 33:50.460\n But we enjoy it, and that's what happened.\n\n33:50.460 --> 33:53.300\n Or at least not in the obvious way, perhaps.\n\n33:53.300 --> 33:54.880\n It is required,\n\n33:56.200 --> 33:58.540\n if you look at the entire universe in an evolutionary way,\n\n33:58.540 --> 34:00.900\n it's required for us to do interplanetary travel,\n\n34:00.900 --> 34:03.140\n and therefore survive past our own sun.\n\n34:03.140 --> 34:04.500\n But you know, let's not get too.\n\n34:04.500 --> 34:07.220\n Yeah, but evolution works at one time frame,\n\n34:07.220 --> 34:11.340\n it's survival, if you think of survival of the phenotype,\n\n34:11.340 --> 34:13.180\n survival of the individual.\n\n34:13.180 --> 34:16.360\n What you're talking about there is spans well beyond that.\n\n34:16.360 --> 34:18.740\n So there's no genetic,\n\n34:18.740 --> 34:23.420\n I'm not transferring any genetic traits to my children\n\n34:23.420 --> 34:26.540\n that are gonna help them survive better on Mars.\n\n34:26.540 --> 34:28.260\n Totally different mechanism, that's right.\n\n34:28.260 --> 34:31.340\n So let's get into the new, as you've mentioned,\n\n34:31.340 --> 34:34.860\n this idea of the, I don't know if you have a nice name,\n\n34:34.860 --> 34:35.700\n thousand.\n\n34:35.700 --> 34:37.340\n We call it the thousand brain theory of intelligence.\n\n34:37.340 --> 34:38.180\n I like it.\n\n34:38.180 --> 34:43.180\n Can you talk about this idea of a spatial view of concepts\n\n34:43.620 --> 34:44.460\n and so on?\n\n34:44.460 --> 34:46.500\n Yeah, so can I just describe sort of the,\n\n34:46.500 --> 34:49.300\n there's an underlying core discovery,\n\n34:49.300 --> 34:51.140\n which then everything comes from that.\n\n34:51.140 --> 34:54.580\n That's a very simple, this is really what happened.\n\n34:55.660 --> 34:58.580\n We were deep into problems about understanding\n\n34:58.580 --> 35:00.540\n how we build models of stuff in the world\n\n35:00.540 --> 35:03.020\n and how we make predictions about things.\n\n35:03.020 --> 35:07.220\n And I was holding a coffee cup just like this in my hand.\n\n35:07.220 --> 35:10.540\n And my finger was touching the side, my index finger.\n\n35:10.540 --> 35:12.700\n And then I moved it to the top\n\n35:12.700 --> 35:15.460\n and I was gonna feel the rim at the top of the cup.\n\n35:15.460 --> 35:18.280\n And I asked myself a very simple question.\n\n35:18.280 --> 35:20.100\n I said, well, first of all, I say,\n\n35:20.100 --> 35:22.260\n I know that my brain predicts what it's gonna feel\n\n35:22.260 --> 35:23.300\n before it touches it.\n\n35:23.300 --> 35:26.040\n You can just think about it and imagine it.\n\n35:26.040 --> 35:27.660\n And so we know that the brain's making predictions\n\n35:27.660 --> 35:28.500\n all the time.\n\n35:28.500 --> 35:31.540\n So the question is, what does it take to predict that?\n\n35:31.540 --> 35:33.620\n And there's a very interesting answer.\n\n35:33.620 --> 35:35.400\n First of all, it says the brain has to know\n\n35:35.400 --> 35:36.500\n it's touching a coffee cup.\n\n35:36.500 --> 35:38.020\n It has to have a model of a coffee cup.\n\n35:38.020 --> 35:41.020\n It needs to know where the finger currently is\n\n35:41.020 --> 35:43.260\n on the cup relative to the cup.\n\n35:43.260 --> 35:44.420\n Because when I make a movement,\n\n35:44.420 --> 35:46.340\n it needs to know where it's going to be on the cup\n\n35:46.340 --> 35:50.380\n after the movement is completed relative to the cup.\n\n35:50.380 --> 35:51.900\n And then it can make a prediction\n\n35:51.900 --> 35:53.340\n about what it's gonna sense.\n\n35:53.340 --> 35:54.960\n So this told me that the neocortex,\n\n35:54.960 --> 35:56.380\n which is making this prediction,\n\n35:56.380 --> 35:59.420\n needs to know that it's sensing it's touching a cup.\n\n35:59.420 --> 36:01.420\n And it needs to know the location of my finger\n\n36:01.420 --> 36:04.380\n relative to that cup in a reference frame of the cup.\n\n36:04.380 --> 36:06.300\n It doesn't matter where the cup is relative to my body.\n\n36:06.300 --> 36:08.260\n It doesn't matter its orientation.\n\n36:08.260 --> 36:09.160\n None of that matters.\n\n36:09.160 --> 36:10.940\n It's where my finger is relative to the cup,\n\n36:10.940 --> 36:13.540\n which tells me then that the neocortex\n\n36:13.540 --> 36:17.340\n has a reference frame that's anchored to the cup.\n\n36:17.340 --> 36:19.280\n Because otherwise I wouldn't be able to say the location\n\n36:19.280 --> 36:21.500\n and I wouldn't be able to predict my new location.\n\n36:21.500 --> 36:24.120\n And then we quickly, very instantly can say,\n\n36:24.120 --> 36:26.240\n well, every part of my skin could touch this cup.\n\n36:26.240 --> 36:28.100\n And therefore every part of my skin is making predictions\n\n36:28.100 --> 36:30.940\n and every part of my skin must have a reference frame\n\n36:30.940 --> 36:33.520\n that it's using to make predictions.\n\n36:33.520 --> 36:38.520\n So the big idea is that throughout the neocortex,\n\n36:39.500 --> 36:44.500\n there are, everything is being stored\n\n36:44.940 --> 36:46.740\n and referenced in reference frames.\n\n36:46.740 --> 36:48.820\n You can think of them like XYZ reference frames,\n\n36:48.820 --> 36:50.380\n but they're not like that.\n\n36:50.380 --> 36:52.060\n We know a lot about the neural mechanisms for this,\n\n36:52.060 --> 36:54.860\n but the brain thinks in reference frames.\n\n36:54.860 --> 36:56.700\n And as an engineer, if you're an engineer,\n\n36:56.700 --> 36:57.740\n this is not surprising.\n\n36:57.740 --> 37:00.340\n You'd say, if I wanted to build a CAD model\n\n37:00.340 --> 37:02.120\n of the coffee cup, well, I would bring it up\n\n37:02.120 --> 37:04.100\n and some CAD software, and I would assign\n\n37:04.100 --> 37:05.460\n some reference frame and say this features\n\n37:05.460 --> 37:06.980\n at this locations and so on.\n\n37:06.980 --> 37:09.700\n But the fact that this, the idea that this is occurring\n\n37:09.700 --> 37:14.360\n throughout the neocortex everywhere, it was a novel idea.\n\n37:14.360 --> 37:19.080\n And then a zillion things fell into place after that,\n\n37:19.080 --> 37:19.940\n a zillion.\n\n37:19.940 --> 37:21.860\n So now we think about the neocortex\n\n37:21.860 --> 37:23.420\n as processing information quite differently\n\n37:23.420 --> 37:24.260\n than we used to do it.\n\n37:24.260 --> 37:25.540\n We used to think about the neocortex\n\n37:25.540 --> 37:28.700\n as processing sensory data and extracting features\n\n37:28.700 --> 37:30.860\n from that sensory data and then extracting features\n\n37:30.860 --> 37:33.580\n from the features, very much like a deep learning network\n\n37:33.580 --> 37:34.900\n does today.\n\n37:34.900 --> 37:36.620\n But that's not how the brain works at all.\n\n37:36.620 --> 37:39.300\n The brain works by assigning everything,\n\n37:39.300 --> 37:41.860\n every input, everything to reference frames.\n\n37:41.860 --> 37:44.380\n And there are thousands, hundreds of thousands\n\n37:44.380 --> 37:46.420\n of them active at once in your neocortex.\n\n37:47.660 --> 37:49.580\n It's a surprising thing to think about,\n\n37:49.580 --> 37:51.060\n but once you sort of internalize this,\n\n37:51.060 --> 37:54.380\n you understand that it explains almost every,\n\n37:54.380 --> 37:57.780\n almost all the mysteries we've had about this structure.\n\n37:57.780 --> 38:00.200\n So one of the consequences of that\n\n38:00.200 --> 38:02.620\n is that every small part of the neocortex,\n\n38:02.620 --> 38:06.340\n say a millimeter square, and there's 150,000 of those.\n\n38:06.340 --> 38:08.620\n So it's about 150,000 square millimeters.\n\n38:08.620 --> 38:11.380\n If you take every little square millimeter of the cortex,\n\n38:11.380 --> 38:13.260\n it's got some input coming into it\n\n38:13.260 --> 38:14.940\n and it's gonna have reference frames\n\n38:14.940 --> 38:16.800\n where it's assigned that input to.\n\n38:16.800 --> 38:19.320\n And each square millimeter can learn\n\n38:19.320 --> 38:20.980\n complete models of objects.\n\n38:20.980 --> 38:22.020\n So what do I mean by that?\n\n38:22.020 --> 38:23.300\n If I'm touching the coffee cup,\n\n38:23.300 --> 38:25.580\n well, if I just touch it in one place,\n\n38:25.580 --> 38:27.180\n I can't learn what this coffee cup is\n\n38:27.180 --> 38:28.980\n because I'm just feeling one part.\n\n38:28.980 --> 38:31.060\n But if I move it around the cup\n\n38:31.060 --> 38:32.540\n and touch it at different areas,\n\n38:32.540 --> 38:34.060\n I can build up a complete model of the cup\n\n38:34.060 --> 38:36.700\n because I'm now filling in that three dimensional map,\n\n38:36.700 --> 38:37.540\n which is the coffee cup.\n\n38:37.540 --> 38:38.660\n I can say, oh, what am I feeling\n\n38:38.660 --> 38:39.900\n at all these different locations?\n\n38:39.900 --> 38:43.020\n That's the basic idea, it's more complicated than that.\n\n38:43.020 --> 38:46.220\n But so through time, and we talked about time earlier,\n\n38:46.220 --> 38:48.180\n through time, even a single column,\n\n38:48.180 --> 38:50.300\n which is only looking at, or a single part of the cortex,\n\n38:50.300 --> 38:52.720\n which is only looking at a small part of the world,\n\n38:52.720 --> 38:55.060\n can build up a complete model of an object.\n\n38:55.060 --> 38:57.100\n And so if you think about the part of the brain,\n\n38:57.100 --> 38:59.100\n which is getting input from all my fingers,\n\n38:59.100 --> 39:01.700\n so they're spread across the top of your head here.\n\n39:01.700 --> 39:04.040\n This is the somatosensory cortex.\n\n39:04.040 --> 39:05.180\n There's columns associated\n\n39:05.180 --> 39:07.380\n with all the different areas of my skin.\n\n39:07.380 --> 39:10.100\n And what we believe is happening\n\n39:10.100 --> 39:12.900\n is that all of them are building models of this cup,\n\n39:12.900 --> 39:15.340\n every one of them, or things.\n\n39:15.340 --> 39:16.620\n They're not all building,\n\n39:16.620 --> 39:18.180\n not every column or every part of the cortex\n\n39:18.180 --> 39:19.500\n builds models of everything,\n\n39:19.500 --> 39:21.700\n but they're all building models of something.\n\n39:21.700 --> 39:26.700\n And so you have, so when I touch this cup with my hand,\n\n39:26.700 --> 39:28.980\n there are multiple models of the cup being invoked.\n\n39:28.980 --> 39:30.460\n If I look at it with my eyes,\n\n39:30.460 --> 39:32.540\n there are, again, many models of the cup being invoked,\n\n39:32.540 --> 39:34.300\n because each part of the visual system,\n\n39:34.300 --> 39:35.820\n the brain doesn't process an image.\n\n39:35.820 --> 39:38.740\n That's a misleading idea.\n\n39:38.740 --> 39:40.460\n It's just like your fingers touching the cup,\n\n39:40.460 --> 39:41.300\n so different parts of my retina\n\n39:41.300 --> 39:42.980\n are looking at different parts of the cup.\n\n39:42.980 --> 39:45.540\n And thousands and thousands of models of the cup\n\n39:45.540 --> 39:47.380\n are being invoked at once.\n\n39:47.380 --> 39:48.900\n And they're all voting with each other,\n\n39:48.900 --> 39:50.140\n trying to figure out what's going on.\n\n39:50.140 --> 39:51.740\n So that's why we call it the thousand brains theory\n\n39:51.740 --> 39:54.700\n of intelligence, because there isn't one model of a cup.\n\n39:54.700 --> 39:56.300\n There are thousands of models of this cup.\n\n39:56.300 --> 39:57.940\n There are thousands of models of your cellphone\n\n39:57.940 --> 40:00.860\n and about cameras and microphones and so on.\n\n40:00.860 --> 40:02.860\n It's a distributed modeling system,\n\n40:02.860 --> 40:03.700\n which is very different\n\n40:03.700 --> 40:04.860\n than the way people have thought about it.\n\n40:04.860 --> 40:07.340\n And so that's a really compelling and interesting idea.\n\n40:07.340 --> 40:08.700\n I have two first questions.\n\n40:08.700 --> 40:12.060\n So one, on the ensemble part of everything coming together,\n\n40:12.060 --> 40:13.620\n you have these thousand brains.\n\n40:14.860 --> 40:17.900\n How do you know which one has done the best job\n\n40:17.900 --> 40:18.740\n of forming the...\n\n40:18.740 --> 40:19.580\n Great question.\n\n40:19.580 --> 40:20.420\n Let me try to explain it.\n\n40:20.420 --> 40:23.500\n There's a problem that's known in neuroscience\n\n40:23.500 --> 40:25.220\n called the sensor fusion problem.\n\n40:25.220 --> 40:26.060\n Yes.\n\n40:26.060 --> 40:27.740\n And so the idea is there's something like,\n\n40:27.740 --> 40:29.140\n oh, the image comes from the eye.\n\n40:29.140 --> 40:30.620\n There's a picture on the retina\n\n40:30.620 --> 40:32.380\n and then it gets projected to the neocortex.\n\n40:32.380 --> 40:35.100\n Oh, by now it's all spread out all over the place\n\n40:35.100 --> 40:37.100\n and it's kind of squirrely and distorted\n\n40:37.100 --> 40:39.020\n and pieces are all over the...\n\n40:39.020 --> 40:40.900\n It doesn't look like a picture anymore.\n\n40:40.900 --> 40:43.660\n When does it all come back together again?\n\n40:43.660 --> 40:45.380\n Or you might say, well, yes,\n\n40:45.380 --> 40:48.620\n but I also have sounds or touches associated with the cup.\n\n40:48.620 --> 40:50.660\n So I'm seeing the cup and touching the cup.\n\n40:50.660 --> 40:52.620\n How do they get combined together again?\n\n40:52.620 --> 40:54.260\n So it's called the sensor fusion problem.\n\n40:54.260 --> 40:55.860\n As if all these disparate parts\n\n40:55.860 --> 40:59.020\n have to be brought together into one model someplace.\n\n40:59.020 --> 41:01.140\n That's the wrong idea.\n\n41:01.140 --> 41:03.500\n The right idea is that you've got all these guys voting.\n\n41:03.500 --> 41:05.420\n There's auditory models of the cup.\n\n41:05.420 --> 41:06.620\n There's visual models of the cup.\n\n41:06.620 --> 41:08.260\n There's tactile models of the cup.\n\n41:09.860 --> 41:10.700\n In the vision system,\n\n41:10.700 --> 41:12.580\n there might be ones that are more focused on black and white\n\n41:12.580 --> 41:13.620\n and ones focusing on color.\n\n41:13.620 --> 41:14.460\n It doesn't really matter.\n\n41:14.460 --> 41:17.020\n There's just thousands and thousands of models of this cup.\n\n41:17.020 --> 41:17.900\n And they vote.\n\n41:17.900 --> 41:20.620\n They don't actually come together in one spot.\n\n41:20.620 --> 41:21.900\n Just literally think of it this way.\n\n41:21.900 --> 41:24.100\n Imagine you have these columns\n\n41:24.100 --> 41:26.660\n that are like about the size of a little piece of spaghetti.\n\n41:26.660 --> 41:28.500\n Like a two and a half millimeters tall\n\n41:28.500 --> 41:30.020\n and about a millimeter in wide.\n\n41:30.020 --> 41:33.300\n They're not physical, but you could think of them that way.\n\n41:33.300 --> 41:35.300\n And each one's trying to guess what this thing is\n\n41:35.300 --> 41:36.140\n or touching.\n\n41:36.140 --> 41:38.060\n Now, they can do a pretty good job\n\n41:38.060 --> 41:40.060\n if they're allowed to move over time.\n\n41:40.060 --> 41:41.620\n So I can reach my hand into a black box\n\n41:41.620 --> 41:43.540\n and move my finger around an object.\n\n41:43.540 --> 41:45.540\n And if I touch enough spaces, I go, okay,\n\n41:45.540 --> 41:46.980\n now I know what it is.\n\n41:46.980 --> 41:48.300\n But often we don't do that.\n\n41:48.300 --> 41:49.940\n Often I can just reach and grab something with my hand\n\n41:49.940 --> 41:51.020\n all at once and I get it.\n\n41:51.020 --> 41:53.740\n Or if I had to look through the world through a straw,\n\n41:53.740 --> 41:55.860\n so I'm only invoking one little column,\n\n41:55.860 --> 41:56.700\n I can only see part of something\n\n41:56.700 --> 41:58.140\n because I have to move the straw around.\n\n41:58.140 --> 42:00.460\n But if I open my eyes, I see the whole thing at once.\n\n42:00.460 --> 42:01.460\n So what we think is going on\n\n42:01.460 --> 42:03.180\n is all these little pieces of spaghetti,\n\n42:03.180 --> 42:05.300\n if you will, all these little columns in the cortex,\n\n42:05.300 --> 42:08.580\n are all trying to guess what it is that they're sensing.\n\n42:08.580 --> 42:10.740\n They'll do a better guess if they have time\n\n42:10.740 --> 42:11.700\n and can move over time.\n\n42:11.700 --> 42:13.620\n So if I move my eyes, I move my fingers.\n\n42:13.620 --> 42:16.580\n But if they don't, they have a poor guess.\n\n42:16.580 --> 42:20.060\n It's a probabilistic guess of what they might be touching.\n\n42:20.060 --> 42:22.940\n Now, imagine they can post their probability\n\n42:22.940 --> 42:24.580\n at the top of a little piece of spaghetti.\n\n42:24.580 --> 42:25.420\n Each one of them says,\n\n42:25.420 --> 42:27.420\n I think, and it's not really a probability distribution.\n\n42:27.420 --> 42:29.460\n It's more like a set of possibilities.\n\n42:29.460 --> 42:31.980\n In the brain, it doesn't work as a probability distribution.\n\n42:31.980 --> 42:34.020\n It works as more like what we call a union.\n\n42:34.020 --> 42:35.860\n So you could say, and one column says,\n\n42:35.860 --> 42:37.540\n I think it could be a coffee cup,\n\n42:37.540 --> 42:39.940\n a soda can, or a water bottle.\n\n42:39.940 --> 42:40.900\n And another column says,\n\n42:40.900 --> 42:42.300\n I think it could be a coffee cup\n\n42:42.300 --> 42:46.460\n or a telephone or a camera or whatever, right?\n\n42:46.460 --> 42:49.940\n And all these guys are saying what they think it might be.\n\n42:49.940 --> 42:51.620\n And there's these long range connections\n\n42:51.620 --> 42:53.460\n in certain layers in the cortex.\n\n42:53.460 --> 42:56.660\n So there's in some layers in some cells types\n\n42:56.660 --> 43:00.060\n in each column, send the projections across the brain.\n\n43:00.060 --> 43:01.740\n And that's the voting occurs.\n\n43:01.740 --> 43:04.100\n And so there's a simple associative memory mechanism.\n\n43:04.100 --> 43:06.140\n We've described this in a recent paper\n\n43:06.140 --> 43:09.500\n and we've modeled this that says,\n\n43:09.500 --> 43:11.940\n they can all quickly settle on the only\n\n43:11.940 --> 43:14.900\n or the one best answer for all of them.\n\n43:14.900 --> 43:16.420\n If there is a single best answer,\n\n43:16.420 --> 43:18.940\n they all vote and say, yep, it's gotta be the coffee cup.\n\n43:18.940 --> 43:21.060\n And at that point, they all know it's a coffee cup.\n\n43:21.060 --> 43:23.380\n And at that point, everyone acts as if it's a coffee cup.\n\n43:23.380 --> 43:24.220\n They're like, yep, we know it's a coffee,\n\n43:24.220 --> 43:26.380\n even though I've only seen one little piece of this world,\n\n43:26.380 --> 43:27.700\n I know it's a coffee cup I'm touching\n\n43:27.700 --> 43:28.980\n or I'm seeing or whatever.\n\n43:28.980 --> 43:30.900\n And so you can think of all these columns\n\n43:30.900 --> 43:33.020\n are looking at different parts in different places,\n\n43:33.020 --> 43:35.220\n different sensory input, different locations,\n\n43:35.220 --> 43:36.180\n they're all different.\n\n43:36.180 --> 43:40.460\n But this layer that's doing the voting, it solidifies.\n\n43:40.460 --> 43:42.260\n It's just like it crystallizes and says,\n\n43:42.260 --> 43:44.140\n oh, we all know what we're doing.\n\n43:44.140 --> 43:46.460\n And so you don't bring these models together in one model,\n\n43:46.460 --> 43:49.140\n you just vote and there's a crystallization of the vote.\n\n43:49.140 --> 43:51.780\n Great, that's at least a compelling way\n\n43:51.780 --> 43:56.780\n to think about the way you form a model of the world.\n\n43:58.180 --> 44:00.420\n Now, you talk about a coffee cup.\n\n44:00.420 --> 44:03.220\n Do you see this, as far as I understand,\n\n44:03.220 --> 44:04.660\n you are proposing this as well,\n\n44:04.660 --> 44:06.900\n that this extends to much more than coffee cups?\n\n44:06.900 --> 44:07.740\n Yeah.\n\n44:07.740 --> 44:09.540\n It does.\n\n44:09.540 --> 44:10.780\n Or at least the physical world,\n\n44:10.780 --> 44:14.100\n it expands to the world of concepts.\n\n44:14.100 --> 44:15.020\n Yeah, it does.\n\n44:15.020 --> 44:18.220\n And well, first, the primary thing is evidence for that\n\n44:18.220 --> 44:20.700\n is that the regions of the neocortex\n\n44:20.700 --> 44:22.340\n that are associated with language\n\n44:22.340 --> 44:23.860\n or high level thought or mathematics\n\n44:23.860 --> 44:24.700\n or things like that,\n\n44:24.700 --> 44:26.180\n they look like the regions of the neocortex\n\n44:26.180 --> 44:28.300\n that process vision, hearing, and touch.\n\n44:28.300 --> 44:29.700\n They don't look any different.\n\n44:29.700 --> 44:31.580\n Or they look only marginally different.\n\n44:32.820 --> 44:36.420\n And so one would say, well, if Vernon Mountcastle,\n\n44:36.420 --> 44:38.860\n who proposed that all the parts of the neocortex\n\n44:38.860 --> 44:41.060\n do the same thing, if he's right,\n\n44:41.060 --> 44:42.820\n then the parts that are doing language\n\n44:42.820 --> 44:44.540\n or mathematics or physics\n\n44:44.540 --> 44:45.700\n are working on the same principle.\n\n44:45.700 --> 44:48.500\n They must be working on the principle of reference frames.\n\n44:48.500 --> 44:50.100\n So that's a little odd thought.\n\n44:51.820 --> 44:53.940\n But of course, we had no prior idea\n\n44:53.940 --> 44:55.020\n how these things happen.\n\n44:55.020 --> 44:56.420\n So let's go with that.\n\n44:57.340 --> 44:59.900\n And we, in our recent paper,\n\n44:59.900 --> 45:01.620\n we talked a little bit about that.\n\n45:01.620 --> 45:02.820\n I've been working on it more since.\n\n45:02.820 --> 45:05.380\n I have better ideas about it now.\n\n45:05.380 --> 45:06.980\n I'm sitting here very confident\n\n45:06.980 --> 45:08.020\n that that's what's happening.\n\n45:08.020 --> 45:09.260\n And I can give you some examples\n\n45:09.260 --> 45:11.220\n that help you think about that.\n\n45:11.220 --> 45:12.500\n It's not we understand it completely,\n\n45:12.500 --> 45:14.300\n but I understand it better than I've described it\n\n45:14.300 --> 45:15.660\n in any paper so far.\n\n45:15.660 --> 45:17.700\n So, but we did put that idea out there.\n\n45:17.700 --> 45:18.940\n It says, okay, this is,\n\n45:18.940 --> 45:22.620\n it's a good place to start, you know?\n\n45:22.620 --> 45:24.900\n And the evidence would suggest it's how it's happening.\n\n45:24.900 --> 45:26.660\n And then we can start tackling that problem\n\n45:26.660 --> 45:27.500\n one piece at a time.\n\n45:27.500 --> 45:29.060\n Like, what does it mean to do high level thought?\n\n45:29.060 --> 45:30.020\n What does it mean to do language?\n\n45:30.020 --> 45:34.220\n How would that fit into a reference frame framework?\n\n45:34.220 --> 45:35.980\n Yeah, so there's a,\n\n45:35.980 --> 45:37.580\n I don't know if you could tell me if there's a connection,\n\n45:37.580 --> 45:40.180\n but there's an app called Anki\n\n45:40.180 --> 45:42.420\n that helps you remember different concepts.\n\n45:42.420 --> 45:45.100\n And they talk about like a memory palace\n\n45:45.100 --> 45:47.780\n that helps you remember completely random concepts\n\n45:47.780 --> 45:51.380\n by trying to put them in a physical space in your mind\n\n45:51.380 --> 45:52.220\n and putting them next to each other.\n\n45:52.220 --> 45:53.580\n It's called the method of loci.\n\n45:53.580 --> 45:54.700\n Loci, yeah.\n\n45:54.700 --> 45:57.580\n For some reason, that seems to work really well.\n\n45:57.580 --> 45:59.420\n Now, that's a very narrow kind of application\n\n45:59.420 --> 46:00.580\n of just remembering some facts.\n\n46:00.580 --> 46:03.260\n But that's a very, very telling one.\n\n46:03.260 --> 46:04.100\n Yes, exactly.\n\n46:04.100 --> 46:06.740\n So this seems like you're describing a mechanism\n\n46:06.740 --> 46:09.620\n why this seems to work.\n\n46:09.620 --> 46:11.820\n So basically the way what we think is going on\n\n46:11.820 --> 46:15.060\n is all things you know, all concepts, all ideas,\n\n46:15.060 --> 46:20.060\n words, everything you know are stored in reference frames.\n\n46:20.460 --> 46:24.300\n And so if you want to remember something,\n\n46:24.300 --> 46:26.860\n you have to basically navigate through a reference frame\n\n46:26.860 --> 46:28.620\n the same way a rat navigates through a maze\n\n46:28.620 --> 46:31.420\n and the same way my finger navigates to this coffee cup.\n\n46:31.420 --> 46:33.500\n You are moving through some space.\n\n46:33.500 --> 46:35.900\n And so if you have a random list of things\n\n46:35.900 --> 46:37.460\n you were asked to remember,\n\n46:37.460 --> 46:39.300\n by assigning them to a reference frame,\n\n46:39.300 --> 46:42.100\n you've already know very well to see your house, right?\n\n46:42.100 --> 46:43.580\n And the idea of the method of loci is you can say,\n\n46:43.580 --> 46:45.820\n okay, in my lobby, I'm going to put this thing.\n\n46:45.820 --> 46:47.660\n And then the bedroom, I put this one.\n\n46:47.660 --> 46:48.940\n I go down the hall, I put this thing.\n\n46:48.940 --> 46:50.820\n And then you want to recall those facts\n\n46:50.820 --> 46:51.660\n or recall those things.\n\n46:51.660 --> 46:54.100\n You just walk mentally, you walk through your house.\n\n46:54.100 --> 46:56.540\n You're mentally moving through a reference frame\n\n46:56.540 --> 46:57.660\n that you already had.\n\n46:57.660 --> 46:59.260\n And that tells you,\n\n46:59.260 --> 47:00.580\n there's two things that are really important about that.\n\n47:00.580 --> 47:02.740\n It tells us the brain prefers to store things\n\n47:02.740 --> 47:03.940\n in reference frames.\n\n47:03.940 --> 47:06.820\n And that the method of recalling things\n\n47:06.820 --> 47:08.220\n or thinking, if you will,\n\n47:08.220 --> 47:11.500\n is to move mentally through those reference frames.\n\n47:11.500 --> 47:13.540\n You could move physically through some reference frames,\n\n47:13.540 --> 47:15.220\n like I could physically move through the reference frame\n\n47:15.220 --> 47:16.300\n of this coffee cup.\n\n47:16.300 --> 47:17.900\n I can also mentally move through the reference frame\n\n47:17.900 --> 47:19.980\n of the coffee cup, imagining me touching it.\n\n47:19.980 --> 47:22.420\n But I can also mentally move my house.\n\n47:22.420 --> 47:24.660\n And so now we can ask yourself,\n\n47:24.660 --> 47:26.740\n or are all concepts stored this way?\n\n47:26.740 --> 47:31.380\n There was some recent research using human subjects\n\n47:31.380 --> 47:33.540\n in fMRI, and I'm going to apologize for not knowing\n\n47:33.540 --> 47:35.540\n the name of the scientists who did this.\n\n47:36.660 --> 47:41.060\n But what they did is they put humans in this fMRI machine,\n\n47:41.060 --> 47:42.780\n which is one of these imaging machines.\n\n47:42.780 --> 47:46.460\n And they gave the humans tasks to think about birds.\n\n47:46.460 --> 47:47.780\n So they had different types of birds,\n\n47:47.780 --> 47:49.660\n and birds that look big and small,\n\n47:49.660 --> 47:52.220\n and long necks and long legs, things like that.\n\n47:52.220 --> 47:54.140\n And what they could tell from the fMRI\n\n47:55.260 --> 47:56.700\n was a very clever experiment.\n\n47:57.580 --> 48:00.780\n You get to tell when humans were thinking about the birds,\n\n48:00.780 --> 48:03.580\n that the birds, the knowledge of birds\n\n48:03.580 --> 48:05.500\n was arranged in a reference frame,\n\n48:05.500 --> 48:07.100\n similar to the ones that are used\n\n48:07.100 --> 48:08.980\n when you navigate in a room.\n\n48:08.980 --> 48:10.340\n That these are called grid cells,\n\n48:10.340 --> 48:12.820\n and there are grid cell like patterns of activity\n\n48:12.820 --> 48:15.380\n in the neocortex when they do this.\n\n48:15.380 --> 48:18.980\n So it's a very clever experiment.\n\n48:18.980 --> 48:20.180\n And what it basically says,\n\n48:20.180 --> 48:22.140\n that even when you're thinking about something abstract,\n\n48:22.140 --> 48:24.700\n and you're not really thinking about it as a reference frame,\n\n48:24.700 --> 48:26.980\n it tells us the brain is actually using a reference frame.\n\n48:26.980 --> 48:28.780\n And it's using the same neural mechanisms.\n\n48:28.780 --> 48:30.780\n These grid cells are the basic same neural mechanism\n\n48:30.780 --> 48:32.860\n that we propose that grid cells,\n\n48:32.860 --> 48:34.980\n which exist in the old part of the brain,\n\n48:34.980 --> 48:37.340\n the entorhinal cortex, that that mechanism\n\n48:37.340 --> 48:40.060\n is now similar mechanism is used throughout the neocortex.\n\n48:40.060 --> 48:43.180\n It's the same nature to preserve this interesting way\n\n48:43.180 --> 48:44.580\n of creating reference frames.\n\n48:44.580 --> 48:46.940\n And so now they have empirical evidence\n\n48:46.940 --> 48:49.500\n that when you think about concepts like birds,\n\n48:49.500 --> 48:51.220\n that you're using reference frames\n\n48:51.220 --> 48:53.180\n that are built on grid cells.\n\n48:53.180 --> 48:55.180\n So that's similar to the method of loci,\n\n48:55.180 --> 48:56.820\n but in this case, the birds are related.\n\n48:56.820 --> 48:58.620\n So they create their own reference frame,\n\n48:58.620 --> 49:01.100\n which is consistent with bird space.\n\n49:01.100 --> 49:03.540\n And when you think about something, you go through that.\n\n49:03.540 --> 49:04.820\n You can make the same example,\n\n49:04.820 --> 49:06.620\n let's take mathematics.\n\n49:06.620 --> 49:09.260\n Let's say you wanna prove a conjecture.\n\n49:09.260 --> 49:10.100\n What is a conjecture?\n\n49:10.100 --> 49:13.300\n A conjecture is a statement you believe to be true,\n\n49:13.300 --> 49:15.140\n but you haven't proven it.\n\n49:15.140 --> 49:16.540\n And so it might be an equation.\n\n49:16.540 --> 49:19.140\n I wanna show that this is equal to that.\n\n49:19.140 --> 49:21.180\n And you have some places you start with.\n\n49:21.180 --> 49:22.340\n You say, well, I know this is true,\n\n49:22.340 --> 49:23.420\n and I know this is true.\n\n49:23.420 --> 49:25.900\n And I think that maybe to get to the final proof,\n\n49:25.900 --> 49:28.700\n I need to go through some intermediate results.\n\n49:28.700 --> 49:33.140\n What I believe is happening is literally these equations\n\n49:33.140 --> 49:36.380\n or these points are assigned to a reference frame,\n\n49:36.380 --> 49:37.980\n a mathematical reference frame.\n\n49:37.980 --> 49:39.820\n And when you do mathematical operations,\n\n49:39.820 --> 49:41.660\n a simple one might be multiply or divide,\n\n49:41.660 --> 49:44.060\n but you might be a little plus transform or something else.\n\n49:44.060 --> 49:47.500\n That is like a movement in the reference frame of the math.\n\n49:47.500 --> 49:50.260\n And so you're literally trying to discover a path\n\n49:50.260 --> 49:52.660\n from one location to another location\n\n49:52.660 --> 49:56.140\n in a space of mathematics.\n\n49:56.140 --> 49:58.220\n And if you can get to these intermediate results,\n\n49:58.220 --> 50:00.420\n then you know your map is pretty good,\n\n50:00.420 --> 50:02.940\n and you know you're using the right operations.\n\n50:02.940 --> 50:05.940\n Much of what we think about is solving hard problems\n\n50:05.940 --> 50:08.820\n is designing the correct reference frame for that problem,\n\n50:08.820 --> 50:11.100\n figuring out how to organize the information\n\n50:11.100 --> 50:14.300\n and what behaviors I wanna use in that space\n\n50:14.300 --> 50:15.180\n to get me there.\n\n50:16.220 --> 50:19.260\n Yeah, so if you dig in an idea of this reference frame,\n\n50:19.260 --> 50:21.700\n whether it's the math, you start a set of axioms\n\n50:21.700 --> 50:25.140\n to try to get to proving the conjecture.\n\n50:25.140 --> 50:28.140\n Can you try to describe, maybe take a step back,\n\n50:28.140 --> 50:30.660\n how you think of the reference frame in that context?\n\n50:30.660 --> 50:35.660\n Is it the reference frame that the axioms are happy in?\n\n50:36.140 --> 50:38.780\n Is it the reference frame that might contain everything?\n\n50:38.780 --> 50:41.780\n Is it a changing thing as you?\n\n50:41.780 --> 50:43.140\n You have many, many reference frames.\n\n50:43.140 --> 50:44.580\n I mean, in fact, the way the theory,\n\n50:44.580 --> 50:46.140\n the thousand brain theory of intelligence says\n\n50:46.140 --> 50:47.380\n that every single thing in the world\n\n50:47.380 --> 50:48.300\n has its own reference frame.\n\n50:48.300 --> 50:50.860\n So every word has its own reference frames.\n\n50:50.860 --> 50:52.940\n And we can talk about this.\n\n50:52.940 --> 50:54.460\n The mathematics work out,\n\n50:54.460 --> 50:55.940\n this is no problem for neurons to do this.\n\n50:55.940 --> 50:58.740\n But how many reference frames does a coffee cup have?\n\n50:58.740 --> 51:00.140\n Well, it's on a table.\n\n51:00.140 --> 51:03.700\n Let's say you ask how many reference frames\n\n51:03.700 --> 51:06.020\n could a column in my finger\n\n51:06.020 --> 51:07.420\n that's touching the coffee cup have?\n\n51:07.420 --> 51:09.060\n Because there are many, many copy,\n\n51:09.060 --> 51:10.500\n there are many, many models of the coffee cup.\n\n51:10.500 --> 51:13.020\n So the coffee, there is no one model of a coffee cup.\n\n51:13.020 --> 51:14.220\n There are many models of a coffee cup.\n\n51:14.220 --> 51:15.220\n And you could say, well,\n\n51:15.220 --> 51:17.260\n how many different things can my finger learn?\n\n51:17.260 --> 51:19.540\n Is this the question you want to ask?\n\n51:19.540 --> 51:21.780\n Imagine I say every concept, every idea,\n\n51:21.780 --> 51:23.860\n everything you've ever know about that you can say,\n\n51:23.860 --> 51:27.260\n I know that thing has a reference frame\n\n51:27.260 --> 51:28.180\n associated with it.\n\n51:28.180 --> 51:30.180\n And what we do when we build composite objects,\n\n51:30.180 --> 51:32.460\n we assign reference frames\n\n51:32.460 --> 51:33.940\n to point another reference frame.\n\n51:33.940 --> 51:37.060\n So my coffee cup has multiple components to it.\n\n51:37.060 --> 51:40.660\n It's got a limb, it's got a cylinder, it's got a handle.\n\n51:40.660 --> 51:42.820\n And those things have their own reference frames\n\n51:42.820 --> 51:45.060\n and they're assigned to a master reference frame,\n\n51:45.060 --> 51:46.380\n which is called this cup.\n\n51:46.380 --> 51:48.180\n And now I have this Numenta logo on it.\n\n51:48.180 --> 51:50.420\n Well, that's something that exists elsewhere in the world.\n\n51:50.420 --> 51:51.260\n It's its own thing.\n\n51:51.260 --> 51:52.300\n So it has its own reference frame.\n\n51:52.300 --> 51:53.140\n So we now have to say,\n\n51:53.140 --> 51:56.740\n how can I assign the Numenta logo reference frame\n\n51:56.740 --> 51:59.180\n onto the cylinder or onto the coffee cup?\n\n51:59.180 --> 52:01.500\n So it's all, we talked about this in the paper\n\n52:01.500 --> 52:05.740\n that came out in December of this last year.\n\n52:06.860 --> 52:08.780\n The idea of how you can assign reference frames\n\n52:08.780 --> 52:10.540\n to reference frames, how neurons could do this.\n\n52:10.540 --> 52:12.620\n So, well, my question is,\n\n52:12.620 --> 52:14.740\n even though you mentioned reference frames a lot,\n\n52:14.740 --> 52:16.940\n I almost feel it's really useful to dig into\n\n52:16.940 --> 52:20.140\n how you think of what a reference frame is.\n\n52:20.140 --> 52:22.020\n I mean, it was already helpful for me to understand\n\n52:22.020 --> 52:23.700\n that you think of reference frames\n\n52:23.700 --> 52:26.340\n as something there is a lot of.\n\n52:26.340 --> 52:28.780\n Okay, so let's just say that we're gonna have\n\n52:28.780 --> 52:31.060\n some neurons in the brain, not many, actually,\n\n52:31.060 --> 52:32.740\n 10,000, 20,000 are gonna create\n\n52:32.740 --> 52:34.300\n a whole bunch of reference frames.\n\n52:34.300 --> 52:35.540\n What does it mean?\n\n52:35.540 --> 52:37.300\n What is a reference frame?\n\n52:37.300 --> 52:40.060\n First of all, these reference frames are different\n\n52:40.060 --> 52:42.220\n than the ones you might be used to.\n\n52:42.220 --> 52:43.420\n We know lots of reference frames.\n\n52:43.420 --> 52:46.060\n For example, we know the Cartesian coordinates, X, Y, Z,\n\n52:46.060 --> 52:47.580\n that's a type of reference frame.\n\n52:47.580 --> 52:50.260\n We know longitude and latitude,\n\n52:50.260 --> 52:52.780\n that's a different type of reference frame.\n\n52:52.780 --> 52:54.540\n If I look at a printed map,\n\n52:54.540 --> 52:58.460\n you might have columns A through M,\n\n52:58.460 --> 53:00.060\n and rows one through 20,\n\n53:00.060 --> 53:01.420\n that's a different type of reference frame.\n\n53:01.420 --> 53:04.660\n It's kind of a Cartesian coordinate reference frame.\n\n53:04.660 --> 53:06.580\n The interesting thing about the reference frames\n\n53:06.580 --> 53:08.580\n in the brain, and we know this because these\n\n53:08.580 --> 53:10.820\n have been established through neuroscience\n\n53:10.820 --> 53:12.260\n studying the entorhinal cortex.\n\n53:12.260 --> 53:13.580\n So I'm not speculating here, okay?\n\n53:13.580 --> 53:16.780\n This is known neuroscience in an old part of the brain.\n\n53:16.780 --> 53:18.860\n The way these cells create reference frames,\n\n53:18.860 --> 53:20.700\n they have no origin.\n\n53:20.700 --> 53:24.340\n So what it's more like, you have a point,\n\n53:24.340 --> 53:27.620\n a point in some space, and you,\n\n53:27.620 --> 53:29.060\n given a particular movement,\n\n53:29.060 --> 53:31.460\n you can then tell what the next point should be.\n\n53:32.340 --> 53:34.100\n And you can then tell what the next point would be,\n\n53:34.100 --> 53:35.460\n and so on.\n\n53:35.460 --> 53:38.700\n You can use this to calculate\n\n53:38.700 --> 53:40.340\n how to get from one point to another.\n\n53:40.340 --> 53:43.180\n So how do I get from my house to my home,\n\n53:43.180 --> 53:44.940\n or how do I get my finger from the side of my cup\n\n53:44.940 --> 53:46.740\n to the top of the cup?\n\n53:46.740 --> 53:50.540\n How do I get from the axioms to the conjecture?\n\n53:50.540 --> 53:54.420\n So it's a different type of reference frame,\n\n53:54.420 --> 53:57.380\n and I can, if you want, I can describe in more detail,\n\n53:57.380 --> 53:59.060\n I can paint a picture of how you might want\n\n53:59.060 --> 53:59.900\n to think about that.\n\n53:59.900 --> 54:00.980\n It's really helpful to think it's something\n\n54:00.980 --> 54:03.740\n you can move through, but is there,\n\n54:03.740 --> 54:08.700\n is it helpful to think of it as spatial in some sense,\n\n54:08.700 --> 54:09.540\n or is there something that's more?\n\n54:09.540 --> 54:11.140\n No, it's definitely spatial.\n\n54:11.140 --> 54:13.820\n It's spatial in a mathematical sense.\n\n54:13.820 --> 54:14.820\n How many dimensions?\n\n54:14.820 --> 54:16.260\n Can it be a crazy number of dimensions?\n\n54:16.260 --> 54:17.460\n Well, that's an interesting question.\n\n54:17.460 --> 54:20.260\n In the old part of the brain, the entorhinal cortex,\n\n54:20.260 --> 54:22.940\n they studied rats, and initially it looks like,\n\n54:22.940 --> 54:24.220\n oh, this is just two dimensional.\n\n54:24.220 --> 54:27.260\n It's like the rat is in some box in the maze or whatever,\n\n54:27.260 --> 54:28.820\n and they know where the rat is using\n\n54:28.820 --> 54:30.300\n these two dimensional reference frames\n\n54:30.300 --> 54:32.380\n to know where it is in the maze.\n\n54:32.380 --> 54:35.540\n We said, well, okay, but what about bats?\n\n54:35.540 --> 54:38.740\n That's a mammal, and they fly in three dimensional space.\n\n54:38.740 --> 54:39.580\n How do they do that?\n\n54:39.580 --> 54:41.700\n They seem to know where they are, right?\n\n54:41.700 --> 54:44.300\n So this is a current area of active research,\n\n54:44.300 --> 54:46.380\n and it seems like somehow the neurons\n\n54:46.380 --> 54:50.300\n in the entorhinal cortex can learn three dimensional space.\n\n54:50.300 --> 54:52.700\n We just, two members of our team,\n\n54:52.700 --> 54:55.940\n along with Elif Fett from MIT,\n\n54:55.940 --> 54:59.580\n just released a paper this literally last week.\n\n54:59.580 --> 55:03.620\n It's on bioRxiv, where they show that you can,\n\n55:03.620 --> 55:05.460\n if you, the way these things work,\n\n55:05.460 --> 55:06.700\n and I won't get, unless you want to,\n\n55:06.700 --> 55:08.100\n I won't get into the detail,\n\n55:08.100 --> 55:12.540\n but grid cells can represent any n dimensional space.\n\n55:12.540 --> 55:15.340\n It's not inherently limited.\n\n55:15.340 --> 55:16.620\n You can think of it this way.\n\n55:16.620 --> 55:18.620\n If you had two dimensional, the way it works\n\n55:18.620 --> 55:20.780\n is you had a bunch of two dimensional slices.\n\n55:20.780 --> 55:21.940\n That's the way these things work.\n\n55:21.940 --> 55:24.260\n There's a whole bunch of two dimensional models,\n\n55:24.260 --> 55:26.140\n and you can just, you can slice up\n\n55:26.140 --> 55:29.300\n any n dimensional space with two dimensional projections.\n\n55:29.300 --> 55:31.660\n So, and you could have one dimensional models.\n\n55:31.660 --> 55:34.420\n So there's nothing inherent about the mathematics\n\n55:34.420 --> 55:35.780\n about the way the neurons do this,\n\n55:35.780 --> 55:39.460\n which constrain the dimensionality of the space,\n\n55:39.460 --> 55:41.460\n which I think was important.\n\n55:41.460 --> 55:44.060\n So obviously I have a three dimensional map of this cup.\n\n55:44.060 --> 55:46.340\n Maybe it's even more than that, I don't know.\n\n55:46.340 --> 55:48.340\n But it's clearly a three dimensional map of the cup.\n\n55:48.340 --> 55:50.900\n I don't just have a projection of the cup.\n\n55:50.900 --> 55:52.020\n But when I think about birds,\n\n55:52.020 --> 55:53.180\n or when I think about mathematics,\n\n55:53.180 --> 55:55.260\n perhaps it's more than three dimensions.\n\n55:55.260 --> 55:56.260\n Who knows?\n\n55:56.260 --> 56:00.100\n So in terms of each individual column\n\n56:00.100 --> 56:04.020\n building up more and more information over time,\n\n56:04.020 --> 56:06.380\n do you think that mechanism is well understood?\n\n56:06.380 --> 56:09.860\n In your mind, you've proposed a lot of architectures there.\n\n56:09.860 --> 56:11.820\n Is that a key piece, or is it,\n\n56:11.820 --> 56:16.220\n is the big piece, the thousand brain theory of intelligence,\n\n56:16.220 --> 56:17.500\n the ensemble of it all?\n\n56:17.500 --> 56:18.460\n Well, I think they're both big.\n\n56:18.460 --> 56:20.940\n I mean, clearly the concept, as a theorist,\n\n56:20.940 --> 56:23.060\n the concept is most exciting, right?\n\n56:23.060 --> 56:23.900\n The high level concept.\n\n56:23.900 --> 56:24.740\n The high level concept.\n\n56:24.740 --> 56:26.140\n This is a totally new way of thinking\n\n56:26.140 --> 56:27.220\n about how the neocortex works.\n\n56:27.220 --> 56:28.660\n So that is appealing.\n\n56:28.660 --> 56:30.700\n It has all these ramifications.\n\n56:30.700 --> 56:33.780\n And with that, as a framework for how the brain works,\n\n56:33.780 --> 56:34.980\n you can make all kinds of predictions\n\n56:34.980 --> 56:36.220\n and solve all kinds of problems.\n\n56:36.220 --> 56:37.260\n Now we're trying to work through\n\n56:37.260 --> 56:38.460\n many of these details right now.\n\n56:38.460 --> 56:40.540\n Okay, how do the neurons actually do this?\n\n56:40.540 --> 56:42.500\n Well, it turns out, if you think about grid cells\n\n56:42.500 --> 56:44.740\n and place cells in the old parts of the brain,\n\n56:44.740 --> 56:45.980\n there's a lot that's known about them,\n\n56:45.980 --> 56:47.020\n but there's still some mysteries.\n\n56:47.020 --> 56:49.060\n There's a lot of debate about exactly the details,\n\n56:49.060 --> 56:50.740\n how these work and what are the signs.\n\n56:50.740 --> 56:52.860\n And we have that still, that same level of detail,\n\n56:52.860 --> 56:54.140\n that same level of concern.\n\n56:54.140 --> 56:56.820\n What we spend here most of our time doing\n\n56:56.820 --> 57:00.060\n is trying to make a very good list\n\n57:00.060 --> 57:02.660\n of the things we don't understand yet.\n\n57:02.660 --> 57:04.020\n That's the key part here.\n\n57:04.020 --> 57:05.260\n What are the constraints?\n\n57:05.260 --> 57:07.020\n It's not like, oh, this thing seems to work, we're done.\n\n57:07.020 --> 57:08.820\n No, it's like, okay, it kind of works,\n\n57:08.820 --> 57:10.700\n but these are other things we know it has to do\n\n57:10.700 --> 57:12.860\n and it's not doing those yet.\n\n57:12.860 --> 57:15.060\n I would say we're well on the way here.\n\n57:15.060 --> 57:17.100\n We're not done yet.\n\n57:17.100 --> 57:20.020\n There's a lot of trickiness to this system,\n\n57:20.020 --> 57:23.180\n but the basic principles about how different layers\n\n57:23.180 --> 57:26.500\n in the neocortex are doing much of this, we understand.\n\n57:27.340 --> 57:28.620\n But there's some fundamental parts\n\n57:28.620 --> 57:30.020\n that we don't understand as well.\n\n57:30.020 --> 57:34.100\n So what would you say is one of the harder open problems\n\n57:34.100 --> 57:37.220\n or one of the ones that have been bothering you,\n\n57:37.220 --> 57:38.460\n keeping you up at night the most?\n\n57:38.460 --> 57:40.620\n Oh, well, right now, this is a detailed thing\n\n57:40.620 --> 57:42.980\n that wouldn't apply to most people, okay?\n\n57:42.980 --> 57:43.820\n Sure.\n\n57:43.820 --> 57:44.660\n But you want me to answer that question?\n\n57:44.660 --> 57:46.180\n Yeah, please.\n\n57:46.180 --> 57:48.380\n We've talked about as if, oh,\n\n57:48.380 --> 57:50.660\n to predict what you're going to sense on this coffee cup,\n\n57:50.660 --> 57:52.300\n I need to know where my finger is gonna be\n\n57:52.300 --> 57:53.580\n on the coffee cup.\n\n57:53.580 --> 57:55.380\n That is true, but it's insufficient.\n\n57:56.340 --> 57:58.460\n Think about my finger touches the edge of the coffee cup.\n\n57:58.460 --> 58:01.660\n My finger can touch it at different orientations.\n\n58:01.660 --> 58:06.340\n I can rotate my finger around here and that doesn't change.\n\n58:06.340 --> 58:08.780\n I can make that prediction and somehow,\n\n58:08.780 --> 58:10.100\n so it's not just the location.\n\n58:10.100 --> 58:13.300\n There's an orientation component of this as well.\n\n58:13.300 --> 58:15.140\n This is known in the old parts of the brain too.\n\n58:15.140 --> 58:16.620\n There's things called head direction cells,\n\n58:16.620 --> 58:18.020\n which way the rat is facing.\n\n58:18.020 --> 58:20.460\n It's the same kind of basic idea.\n\n58:20.460 --> 58:23.620\n So if my finger were a rat, you know, in three dimensions,\n\n58:23.620 --> 58:25.740\n I have a three dimensional orientation\n\n58:25.740 --> 58:27.220\n and I have a three dimensional location.\n\n58:27.220 --> 58:28.620\n If I was a rat, I would have a,\n\n58:28.620 --> 58:30.620\n you might think of it as a two dimensional location,\n\n58:30.620 --> 58:31.460\n a two dimensional orientation,\n\n58:31.460 --> 58:32.540\n a one dimensional orientation,\n\n58:32.540 --> 58:35.100\n like just which way is it facing?\n\n58:35.100 --> 58:38.260\n So how the two components work together,\n\n58:38.260 --> 58:41.500\n how it is that I combine orientation,\n\n58:41.500 --> 58:43.100\n the orientation of my sensor,\n\n58:43.940 --> 58:48.940\n as well as the location is a tricky problem.\n\n58:49.660 --> 58:52.740\n And I think I've made progress on it.\n\n58:52.740 --> 58:55.140\n So at a bigger version of that,\n\n58:55.140 --> 58:58.460\n so perspective is super interesting, but super specific.\n\n58:58.460 --> 59:00.060\n Yeah, I warned you.\n\n59:00.060 --> 59:01.260\n No, no, no, that's really good,\n\n59:01.260 --> 59:03.740\n but there's a more general version of that.\n\n59:03.740 --> 59:06.940\n Do you think context matters,\n\n59:06.940 --> 59:10.700\n the fact that we're in a building in North America,\n\n59:10.700 --> 59:15.700\n that we, in the day and age where we have mugs?\n\n59:15.940 --> 59:19.180\n I mean, there's all this extra information\n\n59:19.180 --> 59:22.060\n that you bring to the table about everything else\n\n59:22.060 --> 59:24.700\n in the room that's outside of just the coffee cup.\n\n59:24.700 --> 59:27.340\n How does it get connected, do you think?\n\n59:27.340 --> 59:30.300\n Yeah, and that is another really interesting question.\n\n59:30.300 --> 59:32.140\n I'm gonna throw that under the rubric\n\n59:32.140 --> 59:35.100\n or the name of attentional problems.\n\n59:35.100 --> 59:36.180\n First of all, we have this model,\n\n59:36.180 --> 59:38.020\n I have many, many models.\n\n59:38.020 --> 59:40.140\n And also the question, does it matter?\n\n59:40.140 --> 59:42.620\n Well, it matters for certain things, of course it does.\n\n59:42.620 --> 59:44.980\n Maybe what we think of that as a coffee cup\n\n59:44.980 --> 59:45.900\n in another part of the world\n\n59:45.900 --> 59:47.660\n is viewed as something completely different.\n\n59:47.660 --> 59:50.420\n Or maybe our logo, which is very benign\n\n59:50.420 --> 59:51.340\n in this part of the world,\n\n59:51.340 --> 59:52.540\n it means something very different\n\n59:52.540 --> 59:53.780\n in another part of the world.\n\n59:53.780 --> 59:56.500\n So those things do matter.\n\n59:57.380 --> 1:00:00.380\n I think the way to think about it is the following,\n\n1:00:00.380 --> 1:00:01.740\n one way to think about it,\n\n1:00:01.740 --> 1:00:04.740\n is we have all these models of the world, okay?\n\n1:00:04.740 --> 1:00:06.140\n And we model everything.\n\n1:00:06.140 --> 1:00:08.860\n And as I said earlier, I kind of snuck it in there,\n\n1:00:08.860 --> 1:00:12.500\n our models are actually, we build composite structure.\n\n1:00:12.500 --> 1:00:15.260\n So every object is composed of other objects,\n\n1:00:15.260 --> 1:00:16.420\n which are composed of other objects,\n\n1:00:16.420 --> 1:00:18.700\n and they become members of other objects.\n\n1:00:18.700 --> 1:00:20.700\n So this room has chairs and a table and a room\n\n1:00:20.700 --> 1:00:21.620\n and walls and so on.\n\n1:00:21.620 --> 1:00:24.300\n Now we can just arrange these things in a certain way\n\n1:00:24.300 --> 1:00:26.580\n and go, oh, that's the nomenclature conference room.\n\n1:00:26.580 --> 1:00:31.260\n So, and what we do is when we go around the world\n\n1:00:31.260 --> 1:00:33.620\n and we experience the world,\n\n1:00:33.620 --> 1:00:35.740\n by walking into a room, for example,\n\n1:00:35.740 --> 1:00:36.780\n the first thing I do is I can say,\n\n1:00:36.780 --> 1:00:38.660\n oh, I'm in this room, do I recognize the room?\n\n1:00:38.660 --> 1:00:41.900\n Then I can say, oh, look, there's a table here.\n\n1:00:41.900 --> 1:00:43.460\n And by attending to the table,\n\n1:00:43.460 --> 1:00:45.620\n I'm then assigning this table in the context of the room.\n\n1:00:45.620 --> 1:00:48.100\n Then I can say, oh, on the table, there's a coffee cup.\n\n1:00:48.100 --> 1:00:49.740\n Oh, and on the table, there's a logo.\n\n1:00:49.740 --> 1:00:51.260\n And in the logo, there's the word Nementa.\n\n1:00:51.260 --> 1:00:53.420\n Oh, and look in the logo, there's the letter E.\n\n1:00:53.420 --> 1:00:55.740\n Oh, and look, it has an unusual serif.\n\n1:00:55.740 --> 1:00:59.660\n And it doesn't actually, but I pretended to serif.\n\n1:00:59.660 --> 1:01:03.860\n So the point is your attention is kind of drilling\n\n1:01:03.860 --> 1:01:06.500\n deep in and out of these nested structures.\n\n1:01:07.460 --> 1:01:09.340\n And I can pop back up and I can pop back down.\n\n1:01:09.340 --> 1:01:10.900\n I can pop back up and I can pop back down.\n\n1:01:10.900 --> 1:01:13.220\n So when I attend to the coffee cup,\n\n1:01:13.220 --> 1:01:15.660\n I haven't lost the context of everything else,\n\n1:01:15.660 --> 1:01:18.900\n but it's sort of, there's this sort of nested structure.\n\n1:01:18.900 --> 1:01:22.100\n So the attention filters the reference frame information\n\n1:01:22.980 --> 1:01:24.420\n for that particular period of time?\n\n1:01:24.420 --> 1:01:26.620\n Yes, it basically, moment to moment,\n\n1:01:26.620 --> 1:01:28.420\n you attend the sub components,\n\n1:01:28.420 --> 1:01:29.740\n and then you can attend the sub components\n\n1:01:29.740 --> 1:01:30.580\n to sub components.\n\n1:01:30.580 --> 1:01:31.420\n And you can move up and down.\n\n1:01:31.420 --> 1:01:32.340\n You can move up and down.\n\n1:01:32.340 --> 1:01:33.180\n We do that all the time.\n\n1:01:33.180 --> 1:01:35.580\n You're not even, now that I'm aware of it,\n\n1:01:35.580 --> 1:01:36.700\n I'm very conscious of it.\n\n1:01:36.700 --> 1:01:39.980\n But until, but most people don't even think about this.\n\n1:01:39.980 --> 1:01:41.700\n You just walk in a room and you don't say,\n\n1:01:41.700 --> 1:01:43.500\n oh, I looked at the chair and I looked at the board\n\n1:01:43.500 --> 1:01:44.620\n and looked at that word on the board\n\n1:01:44.620 --> 1:01:47.100\n and I looked over here, what's going on, right?\n\n1:01:47.100 --> 1:01:50.020\n So what percent of your day are you deeply aware of this?\n\n1:01:50.020 --> 1:01:52.860\n And what part can you actually relax and just be Jeff?\n\n1:01:52.860 --> 1:01:54.460\n Me personally, like my personal day?\n\n1:01:54.460 --> 1:01:55.540\n Yeah.\n\n1:01:55.540 --> 1:01:58.340\n Unfortunately, I'm afflicted with too much of the former.\n\n1:02:01.340 --> 1:02:02.820\n Well, unfortunately or unfortunately.\n\n1:02:02.820 --> 1:02:03.660\n Yeah.\n\n1:02:03.660 --> 1:02:04.580\n You don't think it's useful?\n\n1:02:04.580 --> 1:02:06.820\n Oh, it is useful, totally useful.\n\n1:02:06.820 --> 1:02:09.180\n I think about this stuff almost all the time.\n\n1:02:09.180 --> 1:02:12.540\n And one of my primary ways of thinking\n\n1:02:12.540 --> 1:02:13.860\n is when I'm in sleep at night,\n\n1:02:13.860 --> 1:02:15.860\n I always wake up in the middle of the night.\n\n1:02:15.860 --> 1:02:17.860\n And then I stay awake for at least an hour\n\n1:02:17.860 --> 1:02:20.700\n with my eyes shut in sort of a half sleep state\n\n1:02:20.700 --> 1:02:21.660\n thinking about these things.\n\n1:02:21.660 --> 1:02:23.700\n I come up with answers to problems very often\n\n1:02:23.700 --> 1:02:25.660\n in that sort of half sleeping state.\n\n1:02:25.660 --> 1:02:27.460\n I think about it on my bike ride, I think about it on walks.\n\n1:02:27.460 --> 1:02:28.780\n I'm just constantly thinking about this.\n\n1:02:28.780 --> 1:02:32.420\n I have to almost schedule time\n\n1:02:32.420 --> 1:02:34.100\n to not think about this stuff\n\n1:02:34.100 --> 1:02:37.820\n because it's very, it's mentally taxing.\n\n1:02:37.820 --> 1:02:39.780\n Are you, when you're thinking about this stuff,\n\n1:02:39.780 --> 1:02:41.220\n are you thinking introspectively,\n\n1:02:41.220 --> 1:02:43.700\n like almost taking a step outside of yourself\n\n1:02:43.700 --> 1:02:45.660\n and trying to figure out what is your mind doing right now?\n\n1:02:45.660 --> 1:02:48.020\n I do that all the time, but that's not all I do.\n\n1:02:49.060 --> 1:02:50.780\n I'm constantly observing myself.\n\n1:02:50.780 --> 1:02:53.060\n So as soon as I started thinking about grid cells,\n\n1:02:53.060 --> 1:02:55.260\n for example, and getting into that,\n\n1:02:55.260 --> 1:02:56.780\n I started saying, oh, well, grid cells\n\n1:02:56.780 --> 1:02:58.380\n can have my place of sense in the world.\n\n1:02:58.380 --> 1:02:59.660\n That's where you know where you are.\n\n1:02:59.660 --> 1:03:01.380\n And it's interesting, we always have a sense\n\n1:03:01.380 --> 1:03:03.020\n of where we are unless we're lost.\n\n1:03:03.020 --> 1:03:04.740\n And so I started at night when I got up\n\n1:03:04.740 --> 1:03:06.980\n to go to the bathroom, I would start trying to do it\n\n1:03:06.980 --> 1:03:08.500\n completely with my eyes closed all the time.\n\n1:03:08.500 --> 1:03:10.060\n And I would test my sense of grid cells.\n\n1:03:10.060 --> 1:03:13.700\n I would walk five feet and say, okay, I think I'm here.\n\n1:03:13.700 --> 1:03:14.540\n Am I really there?\n\n1:03:14.540 --> 1:03:15.460\n What's my error?\n\n1:03:15.460 --> 1:03:16.780\n And then I would calculate my error again\n\n1:03:16.780 --> 1:03:17.940\n and see how the errors could accumulate.\n\n1:03:17.940 --> 1:03:19.460\n So even something as simple as getting up\n\n1:03:19.460 --> 1:03:20.420\n in the middle of the night to go to the bathroom,\n\n1:03:20.420 --> 1:03:22.620\n I'm testing these theories out.\n\n1:03:22.620 --> 1:03:23.460\n It's kind of fun.\n\n1:03:23.460 --> 1:03:25.580\n I mean, the coffee cup is an example of that too.\n\n1:03:25.580 --> 1:03:30.380\n So I find that these sort of everyday introspections\n\n1:03:30.380 --> 1:03:31.940\n are actually quite helpful.\n\n1:03:32.820 --> 1:03:34.860\n It doesn't mean you can ignore the science.\n\n1:03:34.860 --> 1:03:37.060\n I mean, I spend hours every day\n\n1:03:37.060 --> 1:03:39.140\n reading ridiculously complex papers.\n\n1:03:40.180 --> 1:03:41.740\n That's not nearly as much fun,\n\n1:03:41.740 --> 1:03:44.580\n but you have to sort of build up those constraints\n\n1:03:44.580 --> 1:03:46.860\n and the knowledge about the field and who's doing what\n\n1:03:46.860 --> 1:03:48.860\n and what exactly they think is happening here.\n\n1:03:48.860 --> 1:03:50.060\n And then you can sit back and say,\n\n1:03:50.060 --> 1:03:52.500\n okay, let's try to piece this all together.\n\n1:03:53.380 --> 1:03:56.020\n Let's come up with some, I'm very,\n\n1:03:56.020 --> 1:03:58.460\n in this group here, people, they know they do,\n\n1:03:58.460 --> 1:03:59.300\n I do this all the time.\n\n1:03:59.300 --> 1:04:01.220\n I come in with these introspective ideas and say,\n\n1:04:01.220 --> 1:04:02.380\n well, have you ever thought about this?\n\n1:04:02.380 --> 1:04:04.700\n Now watch, well, let's all do this together.\n\n1:04:04.700 --> 1:04:05.940\n And it's helpful.\n\n1:04:05.940 --> 1:04:09.580\n It's not, as long as you don't,\n\n1:04:09.580 --> 1:04:12.340\n all you did was that, then you're just making up stuff.\n\n1:04:12.340 --> 1:04:14.780\n But if you're constraining it by the reality\n\n1:04:14.780 --> 1:04:17.820\n of the neuroscience, then it's really helpful.\n\n1:04:17.820 --> 1:04:20.180\n So let's talk a little bit about deep learning\n\n1:04:20.180 --> 1:04:25.180\n and the successes in the applied space of neural networks,\n\n1:04:26.860 --> 1:04:29.020\n ideas of training model on data\n\n1:04:29.020 --> 1:04:31.420\n and these simple computational units,\n\n1:04:31.420 --> 1:04:36.580\n artificial neurons that with backpropagation,\n\n1:04:36.580 --> 1:04:40.460\n statistical ways of being able to generalize\n\n1:04:40.460 --> 1:04:42.780\n from the training set onto data\n\n1:04:42.780 --> 1:04:44.300\n that's similar to that training set.\n\n1:04:44.300 --> 1:04:47.420\n So where do you think are the limitations\n\n1:04:47.420 --> 1:04:48.460\n of those approaches?\n\n1:04:48.460 --> 1:04:50.380\n What do you think are its strengths\n\n1:04:50.380 --> 1:04:52.180\n relative to your major efforts\n\n1:04:52.180 --> 1:04:56.020\n of constructing a theory of human intelligence?\n\n1:04:56.020 --> 1:04:57.820\n Well, I'm not an expert in this field.\n\n1:04:57.820 --> 1:04:59.140\n I'm somewhat knowledgeable.\n\n1:04:59.140 --> 1:04:59.980\n So, but I'm not.\n\n1:04:59.980 --> 1:05:01.620\n Some of it is in just your intuition.\n\n1:05:01.620 --> 1:05:02.460\n What are your?\n\n1:05:02.460 --> 1:05:03.860\n Well, I have a little bit more than intuition,\n\n1:05:03.860 --> 1:05:05.420\n but I just want to say like,\n\n1:05:05.420 --> 1:05:07.660\n you know, one of the things that you asked me,\n\n1:05:07.660 --> 1:05:09.220\n do I spend all my time thinking about neuroscience?\n\n1:05:09.220 --> 1:05:10.060\n I do.\n\n1:05:10.060 --> 1:05:11.340\n That's to the exclusion of thinking about things\n\n1:05:11.340 --> 1:05:13.660\n like convolutional neural networks.\n\n1:05:13.660 --> 1:05:15.260\n But I try to stay current.\n\n1:05:15.260 --> 1:05:17.860\n So look, I think it's great, the progress they've made.\n\n1:05:17.860 --> 1:05:18.780\n It's fantastic.\n\n1:05:18.780 --> 1:05:19.860\n And as I mentioned earlier,\n\n1:05:19.860 --> 1:05:21.860\n it's very highly useful for many things.\n\n1:05:22.940 --> 1:05:26.140\n The models that we have today are actually derived\n\n1:05:26.140 --> 1:05:28.220\n from a lot of neuroscience principles.\n\n1:05:28.220 --> 1:05:30.020\n There are distributed processing systems\n\n1:05:30.020 --> 1:05:31.260\n and distributed memory systems,\n\n1:05:31.260 --> 1:05:33.260\n and that's how the brain works.\n\n1:05:33.260 --> 1:05:35.900\n They use things that we might call them neurons,\n\n1:05:35.900 --> 1:05:37.020\n but they're really not neurons at all.\n\n1:05:37.020 --> 1:05:39.220\n So we can just, they're not really neurons.\n\n1:05:39.220 --> 1:05:41.220\n So they're distributed processing systems.\n\n1:05:41.220 --> 1:05:44.700\n And that nature of hierarchy,\n\n1:05:44.700 --> 1:05:47.140\n that came also from neuroscience.\n\n1:05:47.140 --> 1:05:48.220\n And so there's a lot of things,\n\n1:05:48.220 --> 1:05:49.780\n the learning rules, basically,\n\n1:05:49.780 --> 1:05:51.140\n not back prop, but other, you know,\n\n1:05:51.140 --> 1:05:52.540\n sort of heavy on top of that.\n\n1:05:52.540 --> 1:05:55.020\n I'd be curious to say they're not neurons at all.\n\n1:05:55.020 --> 1:05:56.180\n Can you describe in which way?\n\n1:05:56.180 --> 1:05:57.700\n I mean, some of it is obvious,\n\n1:05:57.700 --> 1:06:00.380\n but I'd be curious if you have specific ways\n\n1:06:00.380 --> 1:06:02.820\n in which you think are the biggest differences.\n\n1:06:02.820 --> 1:06:04.940\n Yeah, we had a paper in 2016 called\n\n1:06:04.940 --> 1:06:06.940\n Why Neurons Have Thousands of Synapses.\n\n1:06:06.940 --> 1:06:09.460\n And if you read that paper,\n\n1:06:09.460 --> 1:06:11.420\n you'll know what I'm talking about here.\n\n1:06:11.420 --> 1:06:14.460\n A real neuron in the brain is a complex thing.\n\n1:06:14.460 --> 1:06:17.180\n And let's just start with the synapses on it,\n\n1:06:17.180 --> 1:06:19.020\n which is a connection between neurons.\n\n1:06:19.020 --> 1:06:20.700\n Real neurons can have everywhere\n\n1:06:20.700 --> 1:06:23.500\n from five to 30,000 synapses on them.\n\n1:06:25.460 --> 1:06:27.220\n The ones near the cell body,\n\n1:06:27.220 --> 1:06:30.420\n the ones that are close to the soma of the cell body,\n\n1:06:30.420 --> 1:06:32.100\n those are like the ones that people model\n\n1:06:32.100 --> 1:06:33.740\n in artificial neurons.\n\n1:06:33.740 --> 1:06:35.060\n There is a few hundred of those.\n\n1:06:35.060 --> 1:06:37.100\n Maybe they can affect the cell.\n\n1:06:37.100 --> 1:06:39.700\n They can make the cell become active.\n\n1:06:39.700 --> 1:06:43.540\n 95% of the synapses can't do that.\n\n1:06:43.540 --> 1:06:44.580\n They're too far away.\n\n1:06:44.580 --> 1:06:45.980\n So if you activate one of those synapses,\n\n1:06:45.980 --> 1:06:47.860\n it just doesn't affect the cell body enough\n\n1:06:47.860 --> 1:06:48.860\n to make any difference.\n\n1:06:48.860 --> 1:06:50.100\n Any one of them individually.\n\n1:06:50.100 --> 1:06:50.940\n Any one of them individually,\n\n1:06:50.940 --> 1:06:52.540\n or even if you do a mass of them.\n\n1:06:54.060 --> 1:06:57.420\n What real neurons do is the following.\n\n1:06:57.420 --> 1:07:02.420\n If you activate or you get 10 to 20 of them\n\n1:07:03.500 --> 1:07:04.460\n active at the same time,\n\n1:07:04.460 --> 1:07:06.660\n meaning they're all receiving an input at the same time,\n\n1:07:06.660 --> 1:07:09.100\n and those 10 to 20 synapses or 40 synapses\n\n1:07:09.100 --> 1:07:11.340\n within a very short distance on the dendrite,\n\n1:07:11.340 --> 1:07:13.300\n like 40 microns, a very small area.\n\n1:07:13.300 --> 1:07:14.580\n So if you activate a bunch of these\n\n1:07:14.580 --> 1:07:17.580\n right next to each other at some distant place,\n\n1:07:17.580 --> 1:07:19.300\n what happens is it creates\n\n1:07:19.300 --> 1:07:21.300\n what's called the dendritic spike.\n\n1:07:21.300 --> 1:07:24.540\n And the dendritic spike travels through the dendrites\n\n1:07:24.540 --> 1:07:26.900\n and can reach the soma or the cell body.\n\n1:07:27.820 --> 1:07:31.260\n Now, when it gets there, it changes the voltage,\n\n1:07:31.260 --> 1:07:33.580\n which is sort of like gonna make the cell fire,\n\n1:07:33.580 --> 1:07:36.060\n but never enough to make the cell fire.\n\n1:07:36.060 --> 1:07:38.500\n It's sort of what we call, it says we depolarize the cell,\n\n1:07:38.500 --> 1:07:39.580\n you raise the voltage a little bit,\n\n1:07:39.580 --> 1:07:41.620\n but not enough to do anything.\n\n1:07:41.620 --> 1:07:42.580\n It's like, well, what good is that?\n\n1:07:42.580 --> 1:07:44.460\n And then it goes back down again.\n\n1:07:44.460 --> 1:07:47.780\n So we propose a theory,\n\n1:07:47.780 --> 1:07:50.500\n which I'm very confident in basics are,\n\n1:07:50.500 --> 1:07:52.780\n is that what's happening there is\n\n1:07:52.780 --> 1:07:55.860\n those 95% of the synapses are recognizing\n\n1:07:55.860 --> 1:07:58.460\n dozens to hundreds of unique patterns.\n\n1:07:58.460 --> 1:08:02.060\n They can write about 10, 20 synapses at a time,\n\n1:08:02.060 --> 1:08:04.460\n and they're acting like predictions.\n\n1:08:04.460 --> 1:08:07.620\n So the neuron actually is a predictive engine on its own.\n\n1:08:07.620 --> 1:08:09.700\n It can fire when it gets enough,\n\n1:08:09.700 --> 1:08:10.900\n what they call proximal input\n\n1:08:10.900 --> 1:08:11.980\n from those ones near the cell fire,\n\n1:08:11.980 --> 1:08:15.460\n but it can get ready to fire from dozens to hundreds\n\n1:08:15.460 --> 1:08:18.100\n of patterns that it recognizes from the other guys.\n\n1:08:18.100 --> 1:08:21.260\n And the advantage of this to the neuron\n\n1:08:21.260 --> 1:08:23.500\n is that when it actually does produce a spike\n\n1:08:23.500 --> 1:08:24.780\n in action potential,\n\n1:08:24.780 --> 1:08:27.700\n it does so slightly sooner than it would have otherwise.\n\n1:08:27.700 --> 1:08:29.740\n And so what could is slightly sooner?\n\n1:08:29.740 --> 1:08:31.820\n Well, the slightly sooner part is it,\n\n1:08:31.820 --> 1:08:34.940\n all the excitatory neurons in the brain\n\n1:08:34.940 --> 1:08:36.660\n are surrounded by these inhibitory neurons,\n\n1:08:36.660 --> 1:08:38.980\n and they're very fast, the inhibitory neurons,\n\n1:08:38.980 --> 1:08:40.420\n these basket cells.\n\n1:08:40.420 --> 1:08:42.580\n And if I get my spike out\n\n1:08:42.580 --> 1:08:44.220\n a little bit sooner than someone else,\n\n1:08:44.220 --> 1:08:47.020\n I inhibit all my neighbors around me, right?\n\n1:08:47.020 --> 1:08:49.740\n And what you end up with is a different representation.\n\n1:08:49.740 --> 1:08:52.060\n You end up with a reputation that matches your prediction.\n\n1:08:52.060 --> 1:08:53.780\n It's a sparser representation,\n\n1:08:53.780 --> 1:08:55.740\n meaning fewer neurons are active,\n\n1:08:55.740 --> 1:08:57.860\n but it's much more specific.\n\n1:08:57.860 --> 1:09:00.300\n And so we showed how networks of these neurons\n\n1:09:00.300 --> 1:09:04.180\n can do very sophisticated temporal prediction, basically.\n\n1:09:04.180 --> 1:09:07.020\n So this, summarize this,\n\n1:09:07.020 --> 1:09:10.980\n real neurons in the brain are time based prediction engines,\n\n1:09:10.980 --> 1:09:14.660\n and there's no concept of this at all\n\n1:09:14.660 --> 1:09:18.100\n in artificial, what we call point neurons.\n\n1:09:18.100 --> 1:09:20.060\n I don't think you can build a brain without them.\n\n1:09:20.060 --> 1:09:21.340\n I don't think you can build intelligence without them,\n\n1:09:21.340 --> 1:09:26.020\n because it's where a large part of the time comes from.\n\n1:09:26.020 --> 1:09:29.060\n These are predictive models, and the time is,\n\n1:09:29.060 --> 1:09:32.220\n there's a prior and a prediction and an action,\n\n1:09:32.220 --> 1:09:34.940\n and it's inherent through every neuron in the neocortex.\n\n1:09:34.940 --> 1:09:37.740\n So I would say that point neurons sort of model\n\n1:09:37.740 --> 1:09:40.620\n a piece of that, and not very well at that either.\n\n1:09:40.620 --> 1:09:45.620\n But like for example, synapses are very unreliable,\n\n1:09:46.060 --> 1:09:49.900\n and you cannot assign any precision to them.\n\n1:09:49.900 --> 1:09:52.460\n So even one digit of precision is not possible.\n\n1:09:52.460 --> 1:09:55.540\n So the way real neurons work is they don't add these,\n\n1:09:55.540 --> 1:09:57.420\n they don't change these weights accurately\n\n1:09:57.420 --> 1:09:59.340\n like artificial neural networks do.\n\n1:09:59.340 --> 1:10:01.020\n They basically form new synapses,\n\n1:10:01.020 --> 1:10:03.780\n and so what you're trying to always do is\n\n1:10:03.780 --> 1:10:06.540\n detect the presence of some 10 to 20\n\n1:10:06.540 --> 1:10:08.780\n active synapses at the same time,\n\n1:10:08.780 --> 1:10:11.300\n as opposed, and they're almost binary.\n\n1:10:11.300 --> 1:10:12.820\n It's like, because you can't really represent\n\n1:10:12.820 --> 1:10:14.620\n anything much finer than that.\n\n1:10:14.620 --> 1:10:16.220\n So these are the kind of,\n\n1:10:16.220 --> 1:10:18.060\n and I think that's actually another essential component,\n\n1:10:18.060 --> 1:10:20.940\n because the brain works on sparse patterns,\n\n1:10:20.940 --> 1:10:24.180\n and all that mechanism is based on sparse patterns,\n\n1:10:24.180 --> 1:10:26.620\n and I don't actually think you could build real brains\n\n1:10:26.620 --> 1:10:29.100\n or machine intelligence without\n\n1:10:29.100 --> 1:10:30.900\n incorporating some of those ideas.\n\n1:10:30.900 --> 1:10:32.660\n It's hard to even think about the complexity\n\n1:10:32.660 --> 1:10:34.420\n that emerges from the fact that\n\n1:10:34.420 --> 1:10:37.140\n the timing of the firing matters in the brain,\n\n1:10:37.140 --> 1:10:40.980\n the fact that you form new synapses,\n\n1:10:40.980 --> 1:10:44.020\n and I mean, everything you just mentioned\n\n1:10:44.020 --> 1:10:44.940\n in the past couple minutes.\n\n1:10:44.940 --> 1:10:46.540\n Trust me, if you spend time on it,\n\n1:10:46.540 --> 1:10:47.940\n you can get your mind around it.\n\n1:10:47.940 --> 1:10:49.860\n It's not like, it's no longer a mystery to me.\n\n1:10:49.860 --> 1:10:53.820\n No, but sorry, as a function, in a mathematical way,\n\n1:10:53.820 --> 1:10:56.940\n can you start getting an intuition about\n\n1:10:56.940 --> 1:10:58.540\n what gets it excited, what not,\n\n1:10:58.540 --> 1:10:59.380\n and what kind of representation?\n\n1:10:59.380 --> 1:11:01.060\n Yeah, it's not as easy as,\n\n1:11:02.580 --> 1:11:04.660\n there's many other types of neural networks\n\n1:11:04.660 --> 1:11:07.580\n that are more amenable to pure analysis,\n\n1:11:09.220 --> 1:11:10.780\n especially very simple networks.\n\n1:11:10.780 --> 1:11:12.580\n Oh, I have four neurons, and they're doing this.\n\n1:11:12.580 --> 1:11:14.500\n Can we describe to them mathematically\n\n1:11:14.500 --> 1:11:16.300\n what they're doing type of thing?\n\n1:11:16.300 --> 1:11:19.340\n Even the complexity of convolutional neural networks today,\n\n1:11:19.340 --> 1:11:20.300\n it's sort of a mystery.\n\n1:11:20.300 --> 1:11:22.500\n They can't really describe the whole system.\n\n1:11:22.500 --> 1:11:24.780\n And so it's different.\n\n1:11:24.780 --> 1:11:29.780\n My colleague Subitai Ahmad, he did a nice paper on this.\n\n1:11:31.500 --> 1:11:32.740\n You can get all this stuff on our website\n\n1:11:32.740 --> 1:11:34.100\n if you're interested,\n\n1:11:34.100 --> 1:11:36.180\n talking about sort of the mathematical properties\n\n1:11:36.180 --> 1:11:37.660\n of sparse representations.\n\n1:11:37.660 --> 1:11:40.620\n And so what we can do is we can show mathematically,\n\n1:11:40.620 --> 1:11:44.940\n for example, why 10 to 20 synapses to recognize a pattern\n\n1:11:44.940 --> 1:11:47.740\n is the correct number, is the right number you'd wanna use.\n\n1:11:47.740 --> 1:11:49.980\n And by the way, that matches biology.\n\n1:11:49.980 --> 1:11:53.900\n We can show mathematically some of these concepts\n\n1:11:53.900 --> 1:11:58.620\n about the show why the brain is so robust\n\n1:11:58.620 --> 1:12:01.020\n to noise and error and fallout and so on.\n\n1:12:01.020 --> 1:12:02.260\n We can show that mathematically\n\n1:12:02.260 --> 1:12:05.020\n as well as empirically in simulations.\n\n1:12:05.020 --> 1:12:07.860\n But the system can't be analyzed completely.\n\n1:12:07.860 --> 1:12:11.980\n Any complex system can't, and so that's out of the realm.\n\n1:12:11.980 --> 1:12:16.980\n But there is mathematical benefits and intuitions\n\n1:12:17.660 --> 1:12:19.460\n that can be derived from mathematics.\n\n1:12:19.460 --> 1:12:20.860\n And we try to do that as well.\n\n1:12:20.860 --> 1:12:23.300\n Most of our papers have a section about that.\n\n1:12:23.300 --> 1:12:25.900\n So I think it's refreshing and useful for me\n\n1:12:25.900 --> 1:12:29.060\n to be talking to you about deep neural networks,\n\n1:12:29.060 --> 1:12:30.900\n because your intuition basically says\n\n1:12:30.900 --> 1:12:34.540\n that we can't achieve anything like intelligence\n\n1:12:34.540 --> 1:12:35.940\n with artificial neural networks.\n\n1:12:35.940 --> 1:12:36.940\n Well, not in the current form.\n\n1:12:36.940 --> 1:12:37.780\n Not in the current form.\n\n1:12:37.780 --> 1:12:40.180\n I'm sure we can do it in the ultimate form, sure.\n\n1:12:40.180 --> 1:12:41.260\n So let me dig into it\n\n1:12:41.260 --> 1:12:43.300\n and see what your thoughts are there a little bit.\n\n1:12:43.300 --> 1:12:45.980\n So I'm not sure if you read this little blog post\n\n1:12:45.980 --> 1:12:49.460\n called Bitter Lesson by Rich Sutton recently.\n\n1:12:49.460 --> 1:12:51.660\n He's a reinforcement learning pioneer.\n\n1:12:51.660 --> 1:12:53.260\n I'm not sure if you're familiar with him.\n\n1:12:53.260 --> 1:12:56.780\n His basic idea is that all the stuff we've done in AI\n\n1:12:56.780 --> 1:13:00.660\n in the past 70 years, he's one of the old school guys.\n\n1:13:02.980 --> 1:13:06.860\n The biggest lesson learned is that all the tricky things\n\n1:13:06.860 --> 1:13:10.420\n we've done, they benefit in the short term,\n\n1:13:10.420 --> 1:13:12.100\n but in the long term, what wins out\n\n1:13:12.100 --> 1:13:16.700\n is a simple general method that just relies on Moore's law,\n\n1:13:16.700 --> 1:13:19.820\n on computation getting faster and faster.\n\n1:13:19.820 --> 1:13:21.260\n This is what he's saying.\n\n1:13:21.260 --> 1:13:23.220\n This is what has worked up to now.\n\n1:13:23.220 --> 1:13:25.380\n This is what has worked up to now.\n\n1:13:25.380 --> 1:13:29.060\n If you're trying to build a system,\n\n1:13:29.060 --> 1:13:30.060\n if we're talking about,\n\n1:13:30.060 --> 1:13:31.420\n he's not concerned about intelligence.\n\n1:13:31.420 --> 1:13:34.420\n He's concerned about a system that works\n\n1:13:34.420 --> 1:13:36.500\n in terms of making predictions\n\n1:13:36.500 --> 1:13:38.780\n on applied narrow AI problems, right?\n\n1:13:38.780 --> 1:13:40.620\n That's what this discussion is about.\n\n1:13:40.620 --> 1:13:44.220\n That you just try to go as general as possible\n\n1:13:44.220 --> 1:13:48.500\n and wait years or decades for the computation\n\n1:13:48.500 --> 1:13:50.220\n to make it actually.\n\n1:13:50.220 --> 1:13:51.700\n Is he saying that as a criticism\n\n1:13:51.700 --> 1:13:53.260\n or is he saying this is a prescription\n\n1:13:53.260 --> 1:13:54.340\n of what we ought to be doing?\n\n1:13:54.340 --> 1:13:55.860\n Well, it's very difficult.\n\n1:13:55.860 --> 1:13:57.980\n He's saying this is what has worked\n\n1:13:57.980 --> 1:14:00.340\n and yes, a prescription, but it's a difficult prescription\n\n1:14:00.340 --> 1:14:02.380\n because it says all the fun things\n\n1:14:02.380 --> 1:14:05.820\n you guys are trying to do, we are trying to do.\n\n1:14:05.820 --> 1:14:07.340\n He's part of the community.\n\n1:14:07.340 --> 1:14:10.780\n He's saying it's only going to be short term gains.\n\n1:14:10.780 --> 1:14:13.780\n So this all leads up to a question, I guess,\n\n1:14:13.780 --> 1:14:15.580\n on artificial neural networks\n\n1:14:15.580 --> 1:14:19.060\n and maybe our own biological neural networks\n\n1:14:19.060 --> 1:14:23.780\n is do you think if we just scale things up significantly,\n\n1:14:23.780 --> 1:14:27.180\n so take these dumb artificial neurons,\n\n1:14:27.180 --> 1:14:29.020\n the point neurons, I like that term.\n\n1:14:30.420 --> 1:14:33.260\n If we just have a lot more of them,\n\n1:14:33.260 --> 1:14:34.540\n do you think some of the elements\n\n1:14:34.540 --> 1:14:38.060\n that we see in the brain may start emerging?\n\n1:14:38.060 --> 1:14:39.540\n No, I don't think so.\n\n1:14:39.540 --> 1:14:43.420\n We can do bigger problems of the same type.\n\n1:14:43.420 --> 1:14:45.260\n I mean, it's been pointed out by many people\n\n1:14:45.260 --> 1:14:46.860\n that today's convolutional neural networks\n\n1:14:46.860 --> 1:14:47.860\n aren't really much different\n\n1:14:47.860 --> 1:14:50.580\n than the ones we had quite a while ago.\n\n1:14:50.580 --> 1:14:51.820\n They're bigger and train more\n\n1:14:51.820 --> 1:14:53.900\n and we have more labeled data and so on.\n\n1:14:56.300 --> 1:14:58.580\n But I don't think you can get to the kind of things\n\n1:14:58.580 --> 1:15:01.380\n I know the brain can do and that we think about\n\n1:15:01.380 --> 1:15:03.700\n as intelligence by just scaling it up.\n\n1:15:03.700 --> 1:15:06.580\n So that may be, it's a good description\n\n1:15:06.580 --> 1:15:07.660\n of what's happened in the past,\n\n1:15:07.660 --> 1:15:09.940\n what's happened recently with the reemergence\n\n1:15:09.940 --> 1:15:12.500\n of artificial neural networks.\n\n1:15:12.500 --> 1:15:14.380\n It may be a good prescription\n\n1:15:14.380 --> 1:15:16.540\n for what's gonna happen in the short term.\n\n1:15:17.580 --> 1:15:19.180\n But I don't think that's the path.\n\n1:15:19.180 --> 1:15:20.860\n I've said that earlier.\n\n1:15:20.860 --> 1:15:21.700\n There's an alternate path.\n\n1:15:21.700 --> 1:15:22.900\n I should mention to you, by the way,\n\n1:15:22.900 --> 1:15:25.900\n that we've made sufficient progress\n\n1:15:25.900 --> 1:15:28.900\n on the whole cortical theory in the last few years\n\n1:15:28.900 --> 1:15:33.900\n that last year we decided to start actively pursuing\n\n1:15:35.660 --> 1:15:39.140\n how do we get these ideas embedded into machine learning?\n\n1:15:40.100 --> 1:15:41.860\n Well, that's, again, being led by my colleague,\n\n1:15:41.860 --> 1:15:45.140\n Subed Tariman, and he's more of a machine learning guy.\n\n1:15:45.140 --> 1:15:46.740\n I'm more of a neuroscience guy.\n\n1:15:46.740 --> 1:15:51.180\n So this is now, I wouldn't say our focus,\n\n1:15:51.180 --> 1:15:54.140\n but it is now an equal focus here\n\n1:15:54.140 --> 1:15:58.220\n because we need to proselytize what we've learned\n\n1:15:58.220 --> 1:16:00.220\n and we need to show how it's beneficial\n\n1:16:01.460 --> 1:16:03.740\n to the machine learning layer.\n\n1:16:03.740 --> 1:16:05.580\n So we're putting, we have a plan in place right now.\n\n1:16:05.580 --> 1:16:07.700\n In fact, we just did our first paper on this.\n\n1:16:07.700 --> 1:16:09.700\n I can tell you about that.\n\n1:16:09.700 --> 1:16:11.380\n But one of the reasons I wanna talk to you\n\n1:16:11.380 --> 1:16:14.100\n is because I'm trying to get more people\n\n1:16:14.100 --> 1:16:15.980\n in the machine learning community to say,\n\n1:16:15.980 --> 1:16:17.140\n I need to learn about this stuff.\n\n1:16:17.140 --> 1:16:19.380\n And maybe we should just think about this a bit more\n\n1:16:19.380 --> 1:16:20.860\n about what we've learned about the brain\n\n1:16:20.860 --> 1:16:23.860\n and what are those team at Nimenta, what have they done?\n\n1:16:23.860 --> 1:16:25.220\n Is that useful for us?\n\n1:16:25.220 --> 1:16:28.500\n Yeah, so is there elements of all the cortical theory\n\n1:16:28.500 --> 1:16:29.820\n that things we've been talking about\n\n1:16:29.820 --> 1:16:31.900\n that may be useful in the short term?\n\n1:16:31.900 --> 1:16:33.420\n Yes, in the short term, yes.\n\n1:16:33.420 --> 1:16:34.780\n This is the, sorry to interrupt,\n\n1:16:34.780 --> 1:16:37.740\n but the open question is,\n\n1:16:37.740 --> 1:16:39.260\n it certainly feels from my perspective\n\n1:16:39.260 --> 1:16:41.060\n that in the long term,\n\n1:16:41.060 --> 1:16:42.820\n some of the ideas we've been talking about\n\n1:16:42.820 --> 1:16:44.260\n will be extremely useful.\n\n1:16:44.260 --> 1:16:46.020\n The question is whether in the short term.\n\n1:16:46.020 --> 1:16:48.340\n Well, this is always what I would call\n\n1:16:48.340 --> 1:16:50.620\n the entrepreneur's dilemma.\n\n1:16:50.620 --> 1:16:53.060\n So you have this long term vision,\n\n1:16:53.060 --> 1:16:55.300\n oh, we're gonna all be driving electric cars\n\n1:16:55.300 --> 1:16:56.780\n or we're all gonna have computers\n\n1:16:56.780 --> 1:16:59.020\n or we're all gonna, whatever.\n\n1:16:59.020 --> 1:17:01.860\n And you're at some point in time and you say,\n\n1:17:01.860 --> 1:17:02.980\n I can see that long term vision,\n\n1:17:02.980 --> 1:17:03.820\n I'm sure it's gonna happen.\n\n1:17:03.820 --> 1:17:05.780\n How do I get there without killing myself?\n\n1:17:05.780 --> 1:17:07.380\n Without going out of business, right?\n\n1:17:07.380 --> 1:17:08.740\n That's the challenge.\n\n1:17:08.740 --> 1:17:09.580\n That's the dilemma.\n\n1:17:09.580 --> 1:17:11.100\n That's the really difficult thing to do.\n\n1:17:11.100 --> 1:17:13.100\n So we're facing that right now.\n\n1:17:13.100 --> 1:17:14.660\n So ideally what you'd wanna do\n\n1:17:14.660 --> 1:17:16.100\n is find some steps along the way\n\n1:17:16.100 --> 1:17:17.420\n that you can get there incrementally.\n\n1:17:17.420 --> 1:17:19.180\n You don't have to like throw it all out\n\n1:17:19.180 --> 1:17:20.460\n and start over again.\n\n1:17:20.460 --> 1:17:22.340\n The first thing that we've done\n\n1:17:22.340 --> 1:17:25.380\n is we focus on the sparse representations.\n\n1:17:25.380 --> 1:17:28.420\n So just in case you don't know what that means\n\n1:17:28.420 --> 1:17:31.220\n or some of the listeners don't know what that means,\n\n1:17:31.220 --> 1:17:34.100\n in the brain, if I have like 10,000 neurons,\n\n1:17:34.100 --> 1:17:36.980\n what you would see is maybe 2% of them active at a time.\n\n1:17:36.980 --> 1:17:39.540\n You don't see 50%, you don't see 30%,\n\n1:17:39.540 --> 1:17:41.220\n you might see 2%.\n\n1:17:41.220 --> 1:17:42.660\n And it's always like that.\n\n1:17:42.660 --> 1:17:44.380\n For any set of sensory inputs?\n\n1:17:44.380 --> 1:17:45.340\n It doesn't matter if anything,\n\n1:17:45.340 --> 1:17:47.380\n doesn't matter any part of the brain.\n\n1:17:47.380 --> 1:17:51.100\n But which neurons differs?\n\n1:17:51.100 --> 1:17:52.620\n Which neurons are active?\n\n1:17:52.620 --> 1:17:55.380\n Yeah, so let's say I take 10,000 neurons\n\n1:17:55.380 --> 1:17:56.300\n that are representing something.\n\n1:17:56.300 --> 1:17:57.940\n They're sitting there in a little block together.\n\n1:17:57.940 --> 1:18:00.060\n It's a teeny little block of neurons, 10,000 neurons.\n\n1:18:00.060 --> 1:18:01.620\n And they're representing a location,\n\n1:18:01.620 --> 1:18:02.500\n they're representing a cup,\n\n1:18:02.500 --> 1:18:04.060\n they're representing the input from my sensors.\n\n1:18:04.060 --> 1:18:05.380\n I don't know, it doesn't matter.\n\n1:18:05.380 --> 1:18:07.020\n It's representing something.\n\n1:18:07.020 --> 1:18:09.140\n The way the representations occur,\n\n1:18:09.140 --> 1:18:10.620\n it's always a sparse representation.\n\n1:18:10.620 --> 1:18:11.860\n Meaning it's a population code.\n\n1:18:11.860 --> 1:18:14.980\n So which 200 cells are active tells me what's going on.\n\n1:18:14.980 --> 1:18:18.060\n It's not, individual cells aren't that important at all.\n\n1:18:18.060 --> 1:18:20.260\n It's the population code that matters.\n\n1:18:20.260 --> 1:18:23.140\n And when you have sparse population codes,\n\n1:18:23.140 --> 1:18:26.300\n then all kinds of beautiful properties come out of them.\n\n1:18:26.300 --> 1:18:28.100\n So the brain uses sparse population codes.\n\n1:18:28.100 --> 1:18:30.780\n We've written and described these benefits\n\n1:18:30.780 --> 1:18:32.420\n in some of our papers.\n\n1:18:32.420 --> 1:18:37.420\n So they give this tremendous robustness to the systems.\n\n1:18:37.660 --> 1:18:39.180\n Brains are incredibly robust.\n\n1:18:39.180 --> 1:18:41.140\n Neurons are dying all the time and spasming\n\n1:18:41.140 --> 1:18:43.940\n and synapses are falling apart all the time.\n\n1:18:43.940 --> 1:18:45.340\n And it keeps working.\n\n1:18:45.340 --> 1:18:50.340\n So what Sibutai and Louise, one of our other engineers here\n\n1:18:51.220 --> 1:18:55.740\n have done, have shown they're introducing sparseness\n\n1:18:55.740 --> 1:18:56.860\n into convolutional neural networks.\n\n1:18:56.860 --> 1:18:58.140\n Now other people are thinking along these lines,\n\n1:18:58.140 --> 1:19:00.980\n but we're going about it in a more principled way, I think.\n\n1:19:00.980 --> 1:19:04.100\n And we're showing that if you enforce sparseness\n\n1:19:04.100 --> 1:19:06.340\n throughout these convolutional neural networks\n\n1:19:07.340 --> 1:19:09.660\n in both the act, which sort of,\n\n1:19:09.660 --> 1:19:12.780\n which neurons are active and the connections between them,\n\n1:19:12.780 --> 1:19:15.660\n that you get some very desirable properties.\n\n1:19:15.660 --> 1:19:18.860\n So one of the current hot topics in deep learning right now\n\n1:19:18.860 --> 1:19:20.900\n are these adversarial examples.\n\n1:19:20.900 --> 1:19:23.500\n So, you know, you give me any deep learning network\n\n1:19:23.500 --> 1:19:26.060\n and I can give you a picture that looks perfect\n\n1:19:26.060 --> 1:19:27.100\n and you're going to call it, you know,\n\n1:19:27.100 --> 1:19:30.300\n you're going to say the monkey is, you know, an airplane.\n\n1:19:30.300 --> 1:19:32.540\n So that's a problem.\n\n1:19:32.540 --> 1:19:34.140\n And DARPA just announced some big thing.\n\n1:19:34.140 --> 1:19:36.580\n They're trying to, you know, have some contest for this.\n\n1:19:36.580 --> 1:19:40.180\n But if you enforce sparse representations here,\n\n1:19:40.180 --> 1:19:41.500\n many of these problems go away.\n\n1:19:41.500 --> 1:19:44.940\n They're much more robust and they're not easy to fool.\n\n1:19:44.940 --> 1:19:48.340\n So we've already shown some of those results,\n\n1:19:48.340 --> 1:19:51.140\n just literally in January or February,\n\n1:19:51.140 --> 1:19:52.780\n just like last month we did that.\n\n1:19:53.740 --> 1:19:57.340\n And you can, I think it's on bioRxiv right now,\n\n1:19:57.340 --> 1:19:59.540\n or on iRxiv, you can read about it.\n\n1:19:59.540 --> 1:20:03.100\n But, so that's like a baby step, okay?\n\n1:20:03.100 --> 1:20:04.340\n That's taking something from the brain.\n\n1:20:04.340 --> 1:20:05.620\n We know about sparseness.\n\n1:20:05.620 --> 1:20:06.500\n We know why it's important.\n\n1:20:06.500 --> 1:20:08.060\n We know what it gives the brain.\n\n1:20:08.060 --> 1:20:09.500\n So let's try to enforce that onto this.\n\n1:20:09.500 --> 1:20:12.420\n What's your intuition why sparsity leads to robustness?\n\n1:20:12.420 --> 1:20:15.060\n Because it feels like it would be less robust.\n\n1:20:15.060 --> 1:20:17.260\n Why would you feel the rest robust to you?\n\n1:20:17.260 --> 1:20:22.260\n So it just feels like if the fewer neurons are involved,\n\n1:20:24.380 --> 1:20:26.660\n the more fragile the representation.\n\n1:20:26.660 --> 1:20:28.260\n But I didn't say there was lots of few neurons.\n\n1:20:28.260 --> 1:20:29.860\n I said, let's say 200.\n\n1:20:29.860 --> 1:20:31.020\n That's a lot.\n\n1:20:31.020 --> 1:20:32.620\n There's still a lot, it's just.\n\n1:20:32.620 --> 1:20:35.260\n So here's an intuition for it.\n\n1:20:35.260 --> 1:20:39.860\n This is a bit technical, so for engineers,\n\n1:20:39.860 --> 1:20:41.260\n machine learning people, this will be easy,\n\n1:20:41.260 --> 1:20:42.980\n but all the listeners, maybe not.\n\n1:20:44.300 --> 1:20:45.740\n If you're trying to classify something,\n\n1:20:45.740 --> 1:20:48.380\n you're trying to divide some very high dimensional space\n\n1:20:48.380 --> 1:20:50.380\n into different pieces, A and B.\n\n1:20:50.380 --> 1:20:52.820\n And you're trying to create some point where you say,\n\n1:20:52.820 --> 1:20:54.780\n all these points in this high dimensional space are A,\n\n1:20:54.780 --> 1:20:57.580\n and all these points in this high dimensional space are B.\n\n1:20:57.580 --> 1:21:01.980\n And if you have points that are close to that line,\n\n1:21:01.980 --> 1:21:02.900\n it's not very robust.\n\n1:21:02.900 --> 1:21:04.940\n It works for all the points you know about,\n\n1:21:04.940 --> 1:21:07.100\n but it's not very robust,\n\n1:21:07.100 --> 1:21:08.260\n because you can just move a little bit\n\n1:21:08.260 --> 1:21:10.300\n and you've crossed over the line.\n\n1:21:10.300 --> 1:21:12.700\n When you have sparse representations,\n\n1:21:12.700 --> 1:21:16.060\n imagine I pick, I'm gonna pick 200 cells active\n\n1:21:16.060 --> 1:21:19.260\n out of 10,000, okay?\n\n1:21:19.260 --> 1:21:20.340\n So I have 200 cells active.\n\n1:21:20.340 --> 1:21:22.220\n Now let's say I pick randomly another,\n\n1:21:22.220 --> 1:21:24.420\n a different representation, 200.\n\n1:21:24.420 --> 1:21:26.740\n The overlap between those is gonna be very small,\n\n1:21:26.740 --> 1:21:28.060\n just a few.\n\n1:21:28.060 --> 1:21:32.740\n I can pick millions of samples randomly of 200 neurons,\n\n1:21:32.740 --> 1:21:36.980\n and not one of them will overlap more than just a few.\n\n1:21:36.980 --> 1:21:39.140\n So one way to think about it is,\n\n1:21:39.140 --> 1:21:41.460\n if I wanna fool one of these representations\n\n1:21:41.460 --> 1:21:43.460\n to look like one of those other representations,\n\n1:21:43.460 --> 1:21:45.660\n I can't move just one cell, or two cells,\n\n1:21:45.660 --> 1:21:46.780\n or three cells, or four cells.\n\n1:21:46.780 --> 1:21:49.140\n I have to move 100 cells.\n\n1:21:49.140 --> 1:21:52.700\n And that makes them robust.\n\n1:21:52.700 --> 1:21:56.180\n In terms of further, so you mentioned sparsity.\n\n1:21:56.180 --> 1:21:57.260\n What would be the next thing?\n\n1:21:57.260 --> 1:21:58.100\n Yeah.\n\n1:21:58.100 --> 1:22:00.460\n Okay, so we have, we picked one.\n\n1:22:00.460 --> 1:22:02.380\n We don't know if it's gonna work well yet.\n\n1:22:02.380 --> 1:22:04.540\n So again, we're trying to come up with incremental ways\n\n1:22:04.540 --> 1:22:07.860\n to moving from brain theory to add pieces\n\n1:22:07.860 --> 1:22:10.140\n to machine learning, current machine learning world,\n\n1:22:10.140 --> 1:22:12.260\n and one step at a time.\n\n1:22:12.260 --> 1:22:13.740\n So the next thing we're gonna try to do\n\n1:22:13.740 --> 1:22:15.820\n is sort of incorporate some of the ideas\n\n1:22:15.820 --> 1:22:19.100\n of the thousand brains theory,\n\n1:22:19.100 --> 1:22:22.580\n that you have many, many models that are voting.\n\n1:22:22.580 --> 1:22:23.700\n Now that idea is not new.\n\n1:22:23.700 --> 1:22:25.300\n There's a mixture of models that's been around\n\n1:22:25.300 --> 1:22:26.280\n for a long time.\n\n1:22:27.160 --> 1:22:29.740\n But the way the brain does it is a little different.\n\n1:22:29.740 --> 1:22:33.620\n And the way it votes is different.\n\n1:22:33.620 --> 1:22:36.220\n And the kind of way it represents uncertainty\n\n1:22:36.220 --> 1:22:37.180\n is different.\n\n1:22:37.180 --> 1:22:39.980\n So we're just starting this work,\n\n1:22:39.980 --> 1:22:42.280\n but we're gonna try to see if we can sort of incorporate\n\n1:22:42.280 --> 1:22:43.760\n some of the principles of voting,\n\n1:22:43.760 --> 1:22:45.940\n or principles of the thousand brain theory.\n\n1:22:45.940 --> 1:22:49.420\n Like lots of simple models that talk to each other\n\n1:22:49.420 --> 1:22:53.040\n in a certain way.\n\n1:22:53.940 --> 1:22:57.700\n And can we build more machines, systems that learn faster\n\n1:22:57.700 --> 1:23:02.700\n and also, well mostly are multimodal\n\n1:23:03.220 --> 1:23:07.500\n and robust to multimodal type of issues.\n\n1:23:07.500 --> 1:23:09.580\n So one of the challenges there\n\n1:23:09.580 --> 1:23:13.100\n is the machine learning computer vision community\n\n1:23:13.100 --> 1:23:15.600\n has certain sets of benchmarks,\n\n1:23:15.600 --> 1:23:18.180\n sets of tests based on which they compete.\n\n1:23:18.180 --> 1:23:22.060\n And I would argue, especially from your perspective,\n\n1:23:22.060 --> 1:23:24.660\n that those benchmarks aren't that useful\n\n1:23:24.660 --> 1:23:28.860\n for testing the aspects that the brain is good at,\n\n1:23:28.860 --> 1:23:29.940\n or intelligence.\n\n1:23:29.940 --> 1:23:31.300\n They're not really testing intelligence.\n\n1:23:31.300 --> 1:23:32.980\n They're very fine.\n\n1:23:32.980 --> 1:23:34.780\n And it's been extremely useful\n\n1:23:34.780 --> 1:23:37.420\n for developing specific mathematical models,\n\n1:23:37.420 --> 1:23:40.420\n but it's not useful in the long term\n\n1:23:40.420 --> 1:23:41.680\n for creating intelligence.\n\n1:23:41.680 --> 1:23:44.660\n So you think you also have a role in proposing\n\n1:23:44.660 --> 1:23:47.020\n better tests?\n\n1:23:47.020 --> 1:23:48.460\n Yeah, this is a very,\n\n1:23:48.460 --> 1:23:50.460\n you've identified a very serious problem.\n\n1:23:51.440 --> 1:23:53.340\n First of all, the tests that they have\n\n1:23:53.340 --> 1:23:54.580\n are the tests that they want.\n\n1:23:54.580 --> 1:23:55.860\n Not the tests of the other things\n\n1:23:55.860 --> 1:23:57.620\n that we're trying to do, right?\n\n1:23:58.740 --> 1:24:01.700\n You know, what are the, so on.\n\n1:24:01.700 --> 1:24:04.220\n The second thing is sometimes these,\n\n1:24:04.220 --> 1:24:06.620\n to be competitive in these tests,\n\n1:24:06.620 --> 1:24:09.940\n you have to have huge data sets and huge computing power.\n\n1:24:10.820 --> 1:24:13.420\n And so, you know, and we don't have that here.\n\n1:24:13.420 --> 1:24:15.500\n We don't have it as well as other big teams\n\n1:24:15.500 --> 1:24:17.600\n that big companies do.\n\n1:24:18.700 --> 1:24:20.900\n So there's numerous issues there.\n\n1:24:20.900 --> 1:24:22.420\n You know, we come out, you know,\n\n1:24:22.420 --> 1:24:24.260\n where our approach to this is all based on,\n\n1:24:24.260 --> 1:24:26.100\n in some sense, you might argue, elegance.\n\n1:24:26.100 --> 1:24:27.780\n We're coming at it from like a theoretical base\n\n1:24:27.780 --> 1:24:29.980\n that we think, oh my God, this is so clearly elegant.\n\n1:24:29.980 --> 1:24:30.820\n This is how brains work.\n\n1:24:30.820 --> 1:24:31.860\n This is what intelligence is.\n\n1:24:31.860 --> 1:24:33.940\n But the machine learning world has gotten in this phase\n\n1:24:33.940 --> 1:24:35.500\n where they think it doesn't matter.\n\n1:24:35.500 --> 1:24:36.600\n Doesn't matter what you think,\n\n1:24:36.600 --> 1:24:39.440\n as long as you do, you know, 0.1% better on this benchmark,\n\n1:24:39.440 --> 1:24:40.780\n that's what, that's all that matters.\n\n1:24:40.780 --> 1:24:42.740\n And that's a problem.\n\n1:24:43.860 --> 1:24:46.060\n You know, we have to figure out how to get around that.\n\n1:24:46.060 --> 1:24:47.300\n That's a challenge for us.\n\n1:24:47.300 --> 1:24:50.500\n That's one of the challenges that we have to deal with.\n\n1:24:50.500 --> 1:24:52.820\n So I agree, you've identified a big issue.\n\n1:24:52.820 --> 1:24:55.900\n It's difficult for those reasons.\n\n1:24:55.900 --> 1:24:59.580\n But you know, part of the reasons I'm talking to you here\n\n1:24:59.580 --> 1:25:01.620\n today is I hope I'm gonna get some machine learning people\n\n1:25:01.620 --> 1:25:03.260\n to say, I'm gonna read those papers.\n\n1:25:03.260 --> 1:25:04.500\n Those might be some interesting ideas.\n\n1:25:04.500 --> 1:25:08.460\n I'm tired of doing this 0.1% improvement stuff, you know?\n\n1:25:08.460 --> 1:25:10.340\n Well, that's why I'm here as well,\n\n1:25:10.340 --> 1:25:13.020\n because I think machine learning now as a community\n\n1:25:13.020 --> 1:25:18.020\n is at a place where the next step needs to be orthogonal\n\n1:25:18.500 --> 1:25:21.300\n to what has received success in the past.\n\n1:25:21.300 --> 1:25:23.100\n Well, you see other leaders saying this,\n\n1:25:23.100 --> 1:25:25.500\n machine learning leaders, you know,\n\n1:25:25.500 --> 1:25:27.940\n Jeff Hinton with his capsules idea.\n\n1:25:27.940 --> 1:25:29.300\n Many people have gotten up to say, you know,\n\n1:25:29.300 --> 1:25:32.100\n we're gonna hit road map, maybe we should look at the brain,\n\n1:25:32.100 --> 1:25:33.460\n you know, things like that.\n\n1:25:33.460 --> 1:25:38.100\n So hopefully that thinking will occur organically.\n\n1:25:38.100 --> 1:25:40.740\n And then we're in a nice position for people to come\n\n1:25:40.740 --> 1:25:41.740\n and look at our work and say,\n\n1:25:41.740 --> 1:25:43.180\n well, what can we learn from these guys?\n\n1:25:43.180 --> 1:25:47.500\n Yeah, MIT is launching a billion dollar computing college\n\n1:25:47.500 --> 1:25:49.220\n that's centered around this idea, so.\n\n1:25:49.220 --> 1:25:50.980\n Is it on this idea of what?\n\n1:25:50.980 --> 1:25:52.700\n Well, the idea that, you know,\n\n1:25:52.700 --> 1:25:54.980\n the humanities, psychology, and neuroscience\n\n1:25:54.980 --> 1:25:58.860\n have to work all together to get to build the S.\n\n1:25:58.860 --> 1:26:00.340\n Yeah, I mean, Stanford just did\n\n1:26:00.340 --> 1:26:02.500\n this Human Centered AI Center.\n\n1:26:02.500 --> 1:26:04.420\n I'm a little disappointed in these initiatives\n\n1:26:04.420 --> 1:26:08.340\n because, you know, they're focusing\n\n1:26:08.340 --> 1:26:09.940\n on sort of the human side of it,\n\n1:26:09.940 --> 1:26:12.140\n and it could very easily slip into\n\n1:26:12.140 --> 1:26:16.060\n how humans interact with intelligent machines,\n\n1:26:16.060 --> 1:26:17.620\n which is nothing wrong with that,\n\n1:26:17.620 --> 1:26:19.420\n but that's not, that is orthogonal\n\n1:26:19.420 --> 1:26:20.380\n to what we're trying to do.\n\n1:26:20.380 --> 1:26:21.340\n We're trying to say, like,\n\n1:26:21.340 --> 1:26:22.860\n what is the essence of intelligence?\n\n1:26:22.860 --> 1:26:23.700\n I don't care.\n\n1:26:23.700 --> 1:26:25.500\n In fact, I wanna build intelligent machines\n\n1:26:25.500 --> 1:26:28.620\n that aren't emotional, that don't smile at you,\n\n1:26:28.620 --> 1:26:31.820\n that, you know, that aren't trying to tuck you in at night.\n\n1:26:31.820 --> 1:26:34.020\n Yeah, there is that pattern that you,\n\n1:26:34.020 --> 1:26:36.500\n when you talk about understanding humans\n\n1:26:36.500 --> 1:26:38.380\n is important for understanding intelligence,\n\n1:26:38.380 --> 1:26:41.140\n that you start slipping into topics of ethics\n\n1:26:41.140 --> 1:26:43.700\n or, yeah, like you said,\n\n1:26:43.700 --> 1:26:45.700\n the interactive elements as opposed to,\n\n1:26:45.700 --> 1:26:47.380\n no, no, no, we have to zoom in on the brain,\n\n1:26:47.380 --> 1:26:51.460\n study what the human brain, the baby, the...\n\n1:26:51.460 --> 1:26:52.900\n Let's study what a brain does.\n\n1:26:52.900 --> 1:26:53.740\n Does.\n\n1:26:53.740 --> 1:26:54.780\n And then we can decide which parts of that\n\n1:26:54.780 --> 1:26:57.740\n we wanna recreate in some system,\n\n1:26:57.740 --> 1:26:59.900\n but until you have that theory about what the brain does,\n\n1:26:59.900 --> 1:27:01.300\n what's the point, you know, it's just,\n\n1:27:01.300 --> 1:27:02.740\n you're gonna be wasting time, I think.\n\n1:27:02.740 --> 1:27:04.060\n Right, just to break it down\n\n1:27:04.060 --> 1:27:05.620\n on the artificial neural network side,\n\n1:27:05.620 --> 1:27:06.740\n maybe you could speak to this\n\n1:27:06.740 --> 1:27:09.180\n on the biological neural network side,\n\n1:27:09.180 --> 1:27:11.940\n the process of learning versus the process of inference.\n\n1:27:13.300 --> 1:27:15.620\n Maybe you can explain to me,\n\n1:27:15.620 --> 1:27:18.460\n is there a difference between,\n\n1:27:18.460 --> 1:27:19.860\n you know, in artificial neural networks,\n\n1:27:19.860 --> 1:27:21.500\n there's a difference between the learning stage\n\n1:27:21.500 --> 1:27:22.940\n and the inference stage.\n\n1:27:22.940 --> 1:27:24.980\n Do you see the brain as something different?\n\n1:27:24.980 --> 1:27:29.020\n One of the big distinctions that people often say,\n\n1:27:29.020 --> 1:27:30.660\n I don't know how correct it is,\n\n1:27:30.660 --> 1:27:32.940\n is artificial neural networks need a lot of data.\n\n1:27:32.940 --> 1:27:34.820\n They're very inefficient learning.\n\n1:27:34.820 --> 1:27:37.340\n Do you see that as a correct distinction\n\n1:27:37.340 --> 1:27:40.300\n from the biology of the human brain,\n\n1:27:40.300 --> 1:27:41.980\n that the human brain is very efficient,\n\n1:27:41.980 --> 1:27:44.220\n or is that just something we deceive ourselves?\n\n1:27:44.220 --> 1:27:45.420\n No, it is efficient, obviously.\n\n1:27:45.420 --> 1:27:47.580\n We can learn new things almost instantly.\n\n1:27:47.580 --> 1:27:50.020\n And so what elements do you think are useful?\n\n1:27:50.020 --> 1:27:50.860\n Yeah, I can talk about that.\n\n1:27:50.860 --> 1:27:52.300\n You brought up two issues there.\n\n1:27:52.300 --> 1:27:54.820\n So remember I talked early about the constraints\n\n1:27:54.820 --> 1:27:57.260\n we always feel, well, one of those constraints\n\n1:27:57.260 --> 1:28:00.940\n is the fact that brains are continually learning.\n\n1:28:00.940 --> 1:28:03.780\n That's not something we said, oh, we can add that later.\n\n1:28:03.780 --> 1:28:05.780\n That's something that was upfront,\n\n1:28:05.780 --> 1:28:07.740\n had to be there from the start,\n\n1:28:08.900 --> 1:28:11.260\n made our problems harder.\n\n1:28:11.260 --> 1:28:14.420\n But we showed, going back to the 2016 paper\n\n1:28:14.420 --> 1:28:16.780\n on sequence memory, we showed how that happens,\n\n1:28:16.780 --> 1:28:19.940\n how the brains infer and learn at the same time.\n\n1:28:19.940 --> 1:28:21.740\n And our models do that.\n\n1:28:21.740 --> 1:28:24.060\n And they're not two separate phases,\n\n1:28:24.060 --> 1:28:26.340\n or two separate sets of time.\n\n1:28:26.340 --> 1:28:29.780\n I think that's a big, big problem in AI,\n\n1:28:29.780 --> 1:28:32.540\n at least for many applications, not for all.\n\n1:28:33.420 --> 1:28:34.380\n So I can talk about that.\n\n1:28:34.380 --> 1:28:37.180\n There are some, it gets detailed,\n\n1:28:37.180 --> 1:28:39.660\n there are some parts of the neocortex in the brain\n\n1:28:39.660 --> 1:28:41.740\n where actually what's going on,\n\n1:28:41.740 --> 1:28:46.740\n there's these cycles of activity in the brain.\n\n1:28:46.860 --> 1:28:49.260\n And there's very strong evidence\n\n1:28:49.260 --> 1:28:51.260\n that you're doing more of inference\n\n1:28:51.260 --> 1:28:52.300\n on one part of the phase,\n\n1:28:52.300 --> 1:28:54.100\n and more of learning on the other part of the phase.\n\n1:28:54.100 --> 1:28:55.500\n So the brain can actually sort of separate\n\n1:28:55.500 --> 1:28:56.660\n different populations of cells\n\n1:28:56.660 --> 1:28:58.340\n or going back and forth like this.\n\n1:28:58.340 --> 1:29:01.540\n But in general, I would say that's an important problem.\n\n1:29:01.540 --> 1:29:05.620\n We have all of our networks that we've come up with do both.\n\n1:29:05.620 --> 1:29:08.220\n And they're continuous learning networks.\n\n1:29:08.220 --> 1:29:10.980\n And you mentioned benchmarks earlier.\n\n1:29:10.980 --> 1:29:12.500\n Well, there are no benchmarks about that.\n\n1:29:12.500 --> 1:29:17.180\n So we have to, we get in our little soapbox,\n\n1:29:17.180 --> 1:29:19.220\n and hey, by the way, this is important,\n\n1:29:19.220 --> 1:29:20.580\n and here's a mechanism for doing that.\n\n1:29:20.580 --> 1:29:23.900\n But until you can prove it to someone\n\n1:29:23.900 --> 1:29:26.700\n in some commercial system or something, it's a little harder.\n\n1:29:26.700 --> 1:29:28.980\n So yeah, one of the things I had to linger on that\n\n1:29:28.980 --> 1:29:33.780\n is in some ways to learn the concept of a coffee cup,\n\n1:29:33.780 --> 1:29:35.900\n you only need this one coffee cup\n\n1:29:35.900 --> 1:29:37.980\n and maybe some time alone in a room with it.\n\n1:29:37.980 --> 1:29:39.940\n Well, the first thing is,\n\n1:29:39.940 --> 1:29:41.820\n imagine I reach my hand into a black box\n\n1:29:41.820 --> 1:29:43.700\n and I'm reaching, I'm trying to touch something.\n\n1:29:43.700 --> 1:29:46.220\n I don't know upfront if it's something I already know\n\n1:29:46.220 --> 1:29:47.860\n or if it's a new thing.\n\n1:29:47.860 --> 1:29:50.460\n And I have to, I'm doing both at the same time.\n\n1:29:50.460 --> 1:29:53.260\n I don't say, oh, let's see if it's a new thing.\n\n1:29:53.260 --> 1:29:54.740\n Oh, let's see if it's an old thing.\n\n1:29:54.740 --> 1:29:55.580\n I don't do that.\n\n1:29:55.580 --> 1:29:59.420\n As I go, my brain says, oh, it's new or it's not new.\n\n1:29:59.420 --> 1:30:02.300\n And if it's new, I start learning what it is.\n\n1:30:02.300 --> 1:30:04.820\n And by the way, it starts learning from the get go,\n\n1:30:04.820 --> 1:30:06.020\n even if it's gonna recognize it.\n\n1:30:06.020 --> 1:30:08.900\n So they're not separate problems.\n\n1:30:08.900 --> 1:30:10.060\n And so that's the thing there.\n\n1:30:10.060 --> 1:30:12.540\n The other thing you mentioned was the fast learning.\n\n1:30:13.540 --> 1:30:15.580\n So I was just talking about continuous learning,\n\n1:30:15.580 --> 1:30:16.660\n but there's also fast learning.\n\n1:30:16.660 --> 1:30:18.780\n Literally, I can show you this coffee cup\n\n1:30:18.780 --> 1:30:20.060\n and I say, here's a new coffee cup.\n\n1:30:20.060 --> 1:30:21.340\n It's got the logo on it.\n\n1:30:21.340 --> 1:30:23.860\n Take a look at it, done, you're done.\n\n1:30:23.860 --> 1:30:25.380\n You can predict what it's gonna look like,\n\n1:30:25.380 --> 1:30:27.460\n you know, in different positions.\n\n1:30:27.460 --> 1:30:29.540\n So I can talk about that too.\n\n1:30:29.540 --> 1:30:34.220\n In the brain, the way learning occurs,\n\n1:30:34.220 --> 1:30:35.700\n I mentioned this earlier, but I'll mention it again.\n\n1:30:35.700 --> 1:30:36.820\n The way learning occurs,\n\n1:30:36.820 --> 1:30:39.180\n imagine I am a section of a dendrite of a neuron,\n\n1:30:40.140 --> 1:30:43.740\n and I'm gonna learn something new.\n\n1:30:43.740 --> 1:30:44.580\n Doesn't matter what it is.\n\n1:30:44.580 --> 1:30:46.180\n I'm just gonna learn something new.\n\n1:30:46.180 --> 1:30:48.900\n I need to recognize a new pattern.\n\n1:30:48.900 --> 1:30:52.540\n So what I'm gonna do is I'm gonna form new synapses.\n\n1:30:52.540 --> 1:30:55.140\n New synapses, we're gonna rewire the brain\n\n1:30:55.140 --> 1:30:57.900\n onto that section of the dendrite.\n\n1:30:57.900 --> 1:31:01.020\n Once I've done that, everything else that neuron has learned\n\n1:31:01.020 --> 1:31:02.580\n is not affected by it.\n\n1:31:02.580 --> 1:31:04.340\n That's because it's isolated\n\n1:31:04.340 --> 1:31:06.380\n to that small section of the dendrite.\n\n1:31:06.380 --> 1:31:09.580\n They're not all being added together, like a point neuron.\n\n1:31:09.580 --> 1:31:11.740\n So if I learn something new on this segment here,\n\n1:31:11.740 --> 1:31:13.180\n it doesn't change any of the learning\n\n1:31:13.180 --> 1:31:14.860\n that occur anywhere else in that neuron.\n\n1:31:14.860 --> 1:31:18.420\n So I can add something without affecting previous learning.\n\n1:31:18.420 --> 1:31:19.780\n And I can do it quickly.\n\n1:31:20.940 --> 1:31:22.300\n Now let's talk, we can talk about the quickness,\n\n1:31:22.300 --> 1:31:24.020\n how it's done in real neurons.\n\n1:31:24.020 --> 1:31:26.740\n You might say, well, doesn't it take time to form synapses?\n\n1:31:26.740 --> 1:31:30.900\n Yes, it can take maybe an hour to form a new synapse.\n\n1:31:30.900 --> 1:31:32.500\n We can form memories quicker than that,\n\n1:31:32.500 --> 1:31:35.860\n and I can explain that how it happens too, if you want.\n\n1:31:35.860 --> 1:31:38.300\n But it's getting a bit neurosciencey.\n\n1:31:39.460 --> 1:31:41.380\n That's great, but is there an understanding\n\n1:31:41.380 --> 1:31:43.100\n of these mechanisms at every level?\n\n1:31:43.100 --> 1:31:43.940\n Yeah.\n\n1:31:43.940 --> 1:31:46.420\n So from the short term memories and the forming.\n\n1:31:48.620 --> 1:31:51.580\n So this idea of synaptogenesis, the growth of new synapses,\n\n1:31:51.580 --> 1:31:54.100\n that's well described, it's well understood.\n\n1:31:54.100 --> 1:31:55.820\n And that's an essential part of learning.\n\n1:31:55.820 --> 1:31:56.780\n That is learning.\n\n1:31:56.780 --> 1:31:58.180\n That is learning.\n\n1:31:58.180 --> 1:31:59.020\n Okay.\n\n1:32:01.980 --> 1:32:03.860\n Going back many, many years,\n\n1:32:03.860 --> 1:32:06.340\n people, you know, it was, what's his name,\n\n1:32:06.340 --> 1:32:09.580\n the psychologist who proposed, Hebb, Donald Hebb.\n\n1:32:09.580 --> 1:32:12.020\n He proposed that learning was the modification\n\n1:32:12.020 --> 1:32:15.460\n of the strength of a connection between two neurons.\n\n1:32:15.460 --> 1:32:18.180\n People interpreted that as the modification\n\n1:32:18.180 --> 1:32:19.660\n of the strength of a synapse.\n\n1:32:19.660 --> 1:32:20.980\n He didn't say that.\n\n1:32:20.980 --> 1:32:22.340\n He just said there's a modification\n\n1:32:22.340 --> 1:32:24.540\n between the effect of one neuron and another.\n\n1:32:24.540 --> 1:32:26.500\n So synaptogenesis is totally consistent\n\n1:32:26.500 --> 1:32:28.180\n with what Donald Hebb said.\n\n1:32:28.180 --> 1:32:29.860\n But anyway, there's these mechanisms,\n\n1:32:29.860 --> 1:32:30.860\n the growth of new synapses.\n\n1:32:30.860 --> 1:32:32.260\n You can go online, you can watch a video\n\n1:32:32.260 --> 1:32:33.900\n of a synapse growing in real time.\n\n1:32:33.900 --> 1:32:37.140\n It's literally, you can see this little thing going boop.\n\n1:32:37.140 --> 1:32:38.420\n It's pretty impressive.\n\n1:32:38.420 --> 1:32:39.740\n So those mechanisms are known.\n\n1:32:39.740 --> 1:32:42.340\n Now there's another thing that we've speculated\n\n1:32:42.340 --> 1:32:43.540\n and we've written about,\n\n1:32:43.540 --> 1:32:45.780\n which is consistent with known neuroscience,\n\n1:32:45.780 --> 1:32:48.340\n but it's less proven.\n\n1:32:48.340 --> 1:32:50.580\n And this is the idea, how do I form a memory\n\n1:32:50.580 --> 1:32:51.620\n really, really quickly?\n\n1:32:51.620 --> 1:32:52.820\n Like instantaneous.\n\n1:32:52.820 --> 1:32:54.580\n If it takes an hour to grow a synapse,\n\n1:32:54.580 --> 1:32:56.820\n like that's not instantaneous.\n\n1:32:56.820 --> 1:33:01.700\n So there are types of synapses called silent synapses.\n\n1:33:01.700 --> 1:33:04.060\n They look like a synapse, but they don't do anything.\n\n1:33:04.060 --> 1:33:04.900\n They're just sitting there.\n\n1:33:04.900 --> 1:33:07.900\n It's like if an action potential comes in,\n\n1:33:07.900 --> 1:33:10.140\n it doesn't release any neurotransmitter.\n\n1:33:10.140 --> 1:33:12.500\n Some parts of the brain have more of these than others.\n\n1:33:12.500 --> 1:33:14.020\n For example, the hippocampus has a lot of them,\n\n1:33:14.020 --> 1:33:17.020\n which is where we associate most short term memory with.\n\n1:33:18.540 --> 1:33:22.100\n So what we speculated, again, in that 2016 paper,\n\n1:33:22.100 --> 1:33:26.420\n we proposed that the way we form very quick memories,\n\n1:33:26.420 --> 1:33:28.940\n very short term memories, or quick memories,\n\n1:33:28.940 --> 1:33:33.860\n is that we convert silent synapses into active synapses.\n\n1:33:33.860 --> 1:33:36.060\n It's like saying a synapse has a zero weight\n\n1:33:36.060 --> 1:33:36.900\n and a one weight,\n\n1:33:37.860 --> 1:33:41.460\n but the longterm memory has to be formed by synaptogenesis.\n\n1:33:41.460 --> 1:33:43.300\n So you can remember something really quickly\n\n1:33:43.300 --> 1:33:46.220\n by just flipping a bunch of these guys from silent to active.\n\n1:33:46.220 --> 1:33:49.140\n It's not from 0.1 to 0.15.\n\n1:33:49.140 --> 1:33:50.700\n It's like, it doesn't do anything\n\n1:33:50.700 --> 1:33:52.260\n till it releases transmitter.\n\n1:33:52.260 --> 1:33:53.500\n And if I do that over a bunch of these,\n\n1:33:53.500 --> 1:33:55.820\n I've got a very quick short term memory.\n\n1:33:56.860 --> 1:33:58.500\n So I guess the lesson behind this\n\n1:33:58.500 --> 1:34:01.860\n is that most neural networks today are fully connected.\n\n1:34:01.860 --> 1:34:03.380\n Every neuron connects every other neuron\n\n1:34:03.380 --> 1:34:04.580\n from layer to layer.\n\n1:34:04.580 --> 1:34:06.060\n That's not correct in the brain.\n\n1:34:06.060 --> 1:34:06.980\n We don't want that.\n\n1:34:06.980 --> 1:34:08.340\n We actually don't want that.\n\n1:34:08.340 --> 1:34:09.260\n It's bad.\n\n1:34:09.260 --> 1:34:10.700\n You want a very sparse connectivity\n\n1:34:10.700 --> 1:34:14.500\n so that any neuron connects to some subset of the neurons\n\n1:34:14.500 --> 1:34:15.340\n in the other layer.\n\n1:34:15.340 --> 1:34:18.980\n And it does so on a dendrite by dendrite segment basis.\n\n1:34:18.980 --> 1:34:21.580\n So it's a very some parcelated out type of thing.\n\n1:34:21.580 --> 1:34:25.380\n And that then learning is not adjusting all these weights,\n\n1:34:25.380 --> 1:34:26.340\n but learning is just saying,\n\n1:34:26.340 --> 1:34:30.180\n okay, connect to these 10 cells here right now.\n\n1:34:30.180 --> 1:34:32.980\n In that process, you know, with artificial neural networks,\n\n1:34:32.980 --> 1:34:36.060\n it's a very simple process of backpropagation\n\n1:34:36.060 --> 1:34:37.180\n that adjusts the weights.\n\n1:34:37.180 --> 1:34:40.100\n The process of synaptogenesis.\n\n1:34:40.100 --> 1:34:40.940\n Synaptogenesis.\n\n1:34:40.940 --> 1:34:42.300\n Synaptogenesis.\n\n1:34:42.300 --> 1:34:43.140\n It's even easier.\n\n1:34:43.140 --> 1:34:43.980\n It's even easier.\n\n1:34:43.980 --> 1:34:44.820\n It's even easier.\n\n1:34:44.820 --> 1:34:47.260\n Backpropagation requires something\n\n1:34:47.260 --> 1:34:48.700\n that really can't happen in brains.\n\n1:34:48.700 --> 1:34:51.220\n This backpropagation of this error signal,\n\n1:34:51.220 --> 1:34:52.060\n that really can't happen.\n\n1:34:52.060 --> 1:34:53.500\n People are trying to make it happen in brains,\n\n1:34:53.500 --> 1:34:54.740\n but it doesn't happen in brains.\n\n1:34:54.740 --> 1:34:56.780\n This is pure Hebbian learning.\n\n1:34:56.780 --> 1:34:58.660\n Well, synaptogenesis is pure Hebbian learning.\n\n1:34:58.660 --> 1:35:00.140\n It's basically saying,\n\n1:35:00.140 --> 1:35:01.540\n there's a population of cells over here\n\n1:35:01.540 --> 1:35:03.020\n that are active right now.\n\n1:35:03.020 --> 1:35:04.340\n And there's a population of cells over here\n\n1:35:04.340 --> 1:35:05.380\n active right now.\n\n1:35:05.380 --> 1:35:07.980\n How do I form connections between those active cells?\n\n1:35:07.980 --> 1:35:11.100\n And it's literally saying this guy became active.\n\n1:35:11.100 --> 1:35:13.260\n These 100 neurons here became active\n\n1:35:13.260 --> 1:35:15.080\n before this neuron became active.\n\n1:35:15.080 --> 1:35:17.140\n So form connections to those ones.\n\n1:35:17.140 --> 1:35:17.960\n That's it.\n\n1:35:17.960 --> 1:35:19.940\n There's no propagation of error, nothing.\n\n1:35:19.940 --> 1:35:20.980\n All the networks we do,\n\n1:35:20.980 --> 1:35:25.700\n all the models we have work on almost completely on\n\n1:35:25.700 --> 1:35:26.540\n Hebbian learning,\n\n1:35:26.540 --> 1:35:30.260\n but on dendritic segments\n\n1:35:30.260 --> 1:35:33.060\n and multiple synapses at the same time.\n\n1:35:33.060 --> 1:35:34.540\n So now let's sort of turn the question\n\n1:35:34.540 --> 1:35:35.820\n that you already answered,\n\n1:35:35.820 --> 1:35:37.760\n and maybe you can answer it again.\n\n1:35:38.820 --> 1:35:41.260\n If you look at the history of artificial intelligence,\n\n1:35:41.260 --> 1:35:43.540\n where do you think we stand?\n\n1:35:43.540 --> 1:35:45.780\n How far are we from solving intelligence?\n\n1:35:45.780 --> 1:35:47.700\n You said you were very optimistic.\n\n1:35:47.700 --> 1:35:48.900\n Can you elaborate on that?\n\n1:35:48.900 --> 1:35:53.500\n Yeah, it's always the crazy question to ask\n\n1:35:53.500 --> 1:35:55.100\n because no one can predict the future.\n\n1:35:55.100 --> 1:35:55.940\n Absolutely.\n\n1:35:55.940 --> 1:35:58.180\n So I'll tell you a story.\n\n1:35:58.180 --> 1:36:01.400\n I used to run a different neuroscience institute\n\n1:36:01.400 --> 1:36:02.620\n called the Redwood Neuroscience Institute,\n\n1:36:02.620 --> 1:36:04.740\n and we would hold these symposiums\n\n1:36:04.740 --> 1:36:06.340\n and we'd get like 35 scientists\n\n1:36:06.340 --> 1:36:08.060\n from around the world to come together.\n\n1:36:08.060 --> 1:36:10.380\n And I used to ask them all the same question.\n\n1:36:10.380 --> 1:36:11.740\n I would say, well, how long do you think it'll be\n\n1:36:11.740 --> 1:36:14.540\n before we understand how the neocortex works?\n\n1:36:14.540 --> 1:36:15.540\n And everyone went around the room\n\n1:36:15.540 --> 1:36:16.560\n and they had introduced the name\n\n1:36:16.560 --> 1:36:18.240\n and they have to answer that question.\n\n1:36:18.240 --> 1:36:22.940\n So I got, the typical answer was 50 to 100 years.\n\n1:36:22.940 --> 1:36:24.780\n Some people would say 500 years.\n\n1:36:24.780 --> 1:36:25.860\n Some people said never.\n\n1:36:25.860 --> 1:36:27.820\n I said, why are you a neuroscientist?\n\n1:36:27.820 --> 1:36:30.500\n It's never gonna, it's a good pay.\n\n1:36:32.780 --> 1:36:34.380\n It's interesting.\n\n1:36:34.380 --> 1:36:36.300\n So, you know, but it doesn't work like that.\n\n1:36:36.300 --> 1:36:38.740\n As I mentioned earlier, these are not,\n\n1:36:38.740 --> 1:36:39.620\n these are step functions.\n\n1:36:39.620 --> 1:36:41.780\n Things happen and then bingo, they happen.\n\n1:36:41.780 --> 1:36:43.620\n You can't predict that.\n\n1:36:43.620 --> 1:36:45.620\n I feel I've already passed a step function.\n\n1:36:45.620 --> 1:36:49.100\n So if I can do my job correctly over the next five years,\n\n1:36:50.740 --> 1:36:53.540\n then, meaning I can proselytize these ideas.\n\n1:36:53.540 --> 1:36:56.140\n I can convince other people they're right.\n\n1:36:56.140 --> 1:36:58.740\n We can show that other people,\n\n1:36:58.740 --> 1:37:00.260\n machine learning people should pay attention\n\n1:37:00.260 --> 1:37:01.420\n to these ideas.\n\n1:37:01.420 --> 1:37:04.580\n Then we're definitely in an under 20 year timeframe.\n\n1:37:04.580 --> 1:37:07.780\n If I can do those things, if I'm not successful in that,\n\n1:37:07.780 --> 1:37:09.780\n and this is the last time anyone talks to me\n\n1:37:09.780 --> 1:37:12.180\n and no one reads our papers and you know,\n\n1:37:12.180 --> 1:37:13.980\n and I'm wrong or something like that,\n\n1:37:13.980 --> 1:37:15.940\n then I don't know.\n\n1:37:15.940 --> 1:37:17.820\n But it's not 50 years.\n\n1:37:21.820 --> 1:37:22.940\n Think about electric cars.\n\n1:37:22.940 --> 1:37:24.940\n How quickly are they gonna populate the world?\n\n1:37:24.940 --> 1:37:27.040\n It probably takes about a 20 year span.\n\n1:37:27.900 --> 1:37:28.820\n It'll be something like that.\n\n1:37:28.820 --> 1:37:31.740\n But I think if I can do what I said, we're starting it.\n\n1:37:31.740 --> 1:37:34.220\n And of course there could be other,\n\n1:37:34.220 --> 1:37:35.400\n you said step functions.\n\n1:37:35.400 --> 1:37:40.100\n It could be everybody gives up on your ideas for 20 years\n\n1:37:40.100 --> 1:37:42.180\n and then all of a sudden somebody picks it up again.\n\n1:37:42.180 --> 1:37:43.620\n Wait, that guy was onto something.\n\n1:37:43.620 --> 1:37:46.660\n Yeah, so that would be a failure on my part, right?\n\n1:37:47.540 --> 1:37:49.820\n Think about Charles Babbage.\n\n1:37:49.820 --> 1:37:52.220\n Charles Babbage, he's the guy who invented the computer\n\n1:37:52.220 --> 1:37:55.820\n back in the 18 something, 1800s.\n\n1:37:55.820 --> 1:37:59.460\n And everyone forgot about it until 100 years later.\n\n1:37:59.460 --> 1:38:00.900\n And say, hey, this guy figured this stuff out\n\n1:38:00.900 --> 1:38:02.380\n a long time ago.\n\n1:38:02.380 --> 1:38:03.940\n But he was ahead of his time.\n\n1:38:03.940 --> 1:38:06.460\n I don't think, as I said,\n\n1:38:06.460 --> 1:38:09.780\n I recognize this is part of any entrepreneur's challenge.\n\n1:38:09.780 --> 1:38:11.500\n I use entrepreneur broadly in this case.\n\n1:38:11.500 --> 1:38:12.980\n I'm not meaning like I'm building a business\n\n1:38:12.980 --> 1:38:13.820\n or trying to sell something.\n\n1:38:13.820 --> 1:38:15.900\n I mean, I'm trying to sell ideas.\n\n1:38:15.900 --> 1:38:19.380\n And this is the challenge as to how you get people\n\n1:38:19.380 --> 1:38:21.540\n to pay attention to you, how do you get them\n\n1:38:21.540 --> 1:38:24.700\n to give you positive or negative feedback,\n\n1:38:24.700 --> 1:38:25.960\n how do you get the people to act differently\n\n1:38:25.960 --> 1:38:27.220\n based on your ideas.\n\n1:38:27.220 --> 1:38:30.180\n So we'll see how well we do on that.\n\n1:38:30.180 --> 1:38:32.300\n So you know that there's a lot of hype\n\n1:38:32.300 --> 1:38:34.640\n behind artificial intelligence currently.\n\n1:38:34.640 --> 1:38:39.540\n Do you, as you look to spread the ideas\n\n1:38:39.540 --> 1:38:43.300\n that are of neocortical theory, the things you're working on,\n\n1:38:43.300 --> 1:38:45.100\n do you think there's some possibility\n\n1:38:45.100 --> 1:38:47.300\n we'll hit an AI winter once again?\n\n1:38:47.300 --> 1:38:48.940\n Yeah, it's certainly a possibility.\n\n1:38:48.940 --> 1:38:49.780\n No question about it.\n\n1:38:49.780 --> 1:38:50.600\n Is that something you worry about?\n\n1:38:50.600 --> 1:38:52.740\n Yeah, well, I guess, do I worry about it?\n\n1:38:54.340 --> 1:38:57.540\n I haven't decided yet if that's good or bad for my mission.\n\n1:38:57.540 --> 1:38:59.660\n That's true, that's very true.\n\n1:38:59.660 --> 1:39:02.940\n Because it's almost like you need the winter\n\n1:39:02.940 --> 1:39:04.300\n to refresh the palette.\n\n1:39:04.300 --> 1:39:07.860\n Yeah, it's like, I want, here's what you wanna have it is.\n\n1:39:07.860 --> 1:39:12.180\n You want, like to the extent that everyone is so thrilled\n\n1:39:12.180 --> 1:39:15.460\n about the current state of machine learning and AI\n\n1:39:15.460 --> 1:39:18.100\n and they don't imagine they need anything else,\n\n1:39:18.100 --> 1:39:19.740\n it makes my job harder.\n\n1:39:19.740 --> 1:39:22.580\n If everything crashed completely\n\n1:39:22.580 --> 1:39:24.260\n and every student left the field\n\n1:39:24.260 --> 1:39:26.200\n and there was no money for anybody to do anything\n\n1:39:26.200 --> 1:39:27.460\n and it became an embarrassment\n\n1:39:27.460 --> 1:39:29.020\n to talk about machine intelligence and AI,\n\n1:39:29.020 --> 1:39:30.740\n that wouldn't be good for us either.\n\n1:39:30.740 --> 1:39:33.400\n You want sort of the soft landing approach, right?\n\n1:39:33.400 --> 1:39:36.620\n You want enough people, the senior people in AI\n\n1:39:36.620 --> 1:39:37.860\n and machine learning to say, you know,\n\n1:39:37.860 --> 1:39:38.900\n we need other approaches.\n\n1:39:38.900 --> 1:39:40.460\n We really need other approaches.\n\n1:39:40.460 --> 1:39:42.020\n Damn, we need other approaches.\n\n1:39:42.020 --> 1:39:43.100\n Maybe we should look to the brain.\n\n1:39:43.100 --> 1:39:44.220\n Okay, let's look to the brain.\n\n1:39:44.220 --> 1:39:45.380\n Who's got some brain ideas?\n\n1:39:45.380 --> 1:39:47.900\n Okay, let's start a little project on the side here\n\n1:39:47.900 --> 1:39:49.700\n trying to do brain idea related stuff.\n\n1:39:49.700 --> 1:39:51.820\n That's the ideal outcome we would want.\n\n1:39:51.820 --> 1:39:53.980\n So I don't want a total winter\n\n1:39:53.980 --> 1:39:57.680\n and yet I don't want it to be sunny all the time either.\n\n1:39:57.680 --> 1:40:00.300\n So what do you think it takes to build a system\n\n1:40:00.300 --> 1:40:03.020\n with human level intelligence\n\n1:40:03.020 --> 1:40:06.820\n where once demonstrated you would be very impressed?\n\n1:40:06.820 --> 1:40:08.700\n So does it have to have a body?\n\n1:40:08.700 --> 1:40:12.780\n Does it have to have the C word we used before,\n\n1:40:12.780 --> 1:40:17.780\n consciousness as an entirety in a holistic sense?\n\n1:40:19.140 --> 1:40:20.500\n First of all, I don't think the goal\n\n1:40:20.500 --> 1:40:23.740\n is to create a machine that is human level intelligence.\n\n1:40:23.740 --> 1:40:24.980\n I think it's a false goal.\n\n1:40:24.980 --> 1:40:27.380\n Back to Turing, I think it was a false statement.\n\n1:40:27.380 --> 1:40:29.060\n We want to understand what intelligence is\n\n1:40:29.060 --> 1:40:30.780\n and then we can build intelligent machines\n\n1:40:30.780 --> 1:40:33.380\n of all different scales, all different capabilities.\n\n1:40:34.260 --> 1:40:35.300\n A dog is intelligent.\n\n1:40:35.300 --> 1:40:38.460\n I don't need, that'd be pretty good to have a dog.\n\n1:40:38.460 --> 1:40:39.580\n But what about something that doesn't look\n\n1:40:39.580 --> 1:40:41.660\n like an animal at all, in different spaces?\n\n1:40:41.660 --> 1:40:44.300\n So my thinking about this is that\n\n1:40:44.300 --> 1:40:46.060\n we want to define what intelligence is,\n\n1:40:46.060 --> 1:40:48.840\n agree upon what makes an intelligent system.\n\n1:40:48.840 --> 1:40:51.100\n We can then say, okay, we're now gonna build systems\n\n1:40:51.100 --> 1:40:54.340\n that work on those principles or some subset of them\n\n1:40:54.340 --> 1:40:57.380\n and we can apply them to all different types of problems.\n\n1:40:57.380 --> 1:41:00.860\n And the kind, the idea, it's not computing.\n\n1:41:00.860 --> 1:41:05.340\n We don't ask, if I take a little one chip computer,\n\n1:41:05.340 --> 1:41:06.660\n I don't say, well, that's not a computer\n\n1:41:06.660 --> 1:41:09.660\n because it's not as powerful as this big server over here.\n\n1:41:09.660 --> 1:41:11.260\n No, no, because we know that what the principles\n\n1:41:11.260 --> 1:41:12.940\n of computing are and I can apply those principles\n\n1:41:12.940 --> 1:41:14.860\n to a small problem or into a big problem.\n\n1:41:14.860 --> 1:41:16.520\n And same, intelligence needs to get there.\n\n1:41:16.520 --> 1:41:17.620\n We have to say, these are the principles.\n\n1:41:17.620 --> 1:41:19.020\n I can make a small one, a big one.\n\n1:41:19.020 --> 1:41:19.940\n I can make them distributed.\n\n1:41:19.940 --> 1:41:21.620\n I can put them on different sensors.\n\n1:41:21.620 --> 1:41:23.220\n They don't have to be human like at all.\n\n1:41:23.220 --> 1:41:24.740\n Now, you did bring up a very interesting question\n\n1:41:24.740 --> 1:41:25.620\n about embodiment.\n\n1:41:25.620 --> 1:41:27.500\n Does it have to have a body?\n\n1:41:27.500 --> 1:41:30.660\n It has to have some concept of movement.\n\n1:41:30.660 --> 1:41:33.260\n It has to be able to move through these reference frames\n\n1:41:33.260 --> 1:41:34.460\n I talked about earlier.\n\n1:41:34.460 --> 1:41:35.820\n Whether it's physically moving,\n\n1:41:35.820 --> 1:41:37.420\n like I need, if I'm gonna have an AI\n\n1:41:37.420 --> 1:41:38.780\n that understands coffee cups,\n\n1:41:38.780 --> 1:41:40.500\n it's gonna have to pick up the coffee cup\n\n1:41:40.500 --> 1:41:43.180\n and touch it and look at it with its eyes and hands\n\n1:41:43.180 --> 1:41:45.380\n or something equivalent to that.\n\n1:41:45.380 --> 1:41:48.100\n If I have a mathematical AI,\n\n1:41:48.100 --> 1:41:51.340\n maybe it needs to move through mathematical spaces.\n\n1:41:51.340 --> 1:41:55.240\n I could have a virtual AI that lives in the internet\n\n1:41:55.240 --> 1:41:58.980\n and its movements are traversing links\n\n1:41:58.980 --> 1:42:00.260\n and digging into files,\n\n1:42:00.260 --> 1:42:03.100\n but it's got a location that it's traveling\n\n1:42:03.100 --> 1:42:04.940\n through some space.\n\n1:42:04.940 --> 1:42:09.060\n You can't have an AI that just take some flash thing input.\n\n1:42:09.060 --> 1:42:10.620\n We call it flash inference.\n\n1:42:10.620 --> 1:42:12.860\n Here's a pattern, done.\n\n1:42:12.860 --> 1:42:15.740\n No, it's movement pattern, movement pattern,\n\n1:42:15.740 --> 1:42:19.020\n movement pattern, attention, digging, building structure,\n\n1:42:19.020 --> 1:42:20.420\n figuring out the model of the world.\n\n1:42:20.420 --> 1:42:22.740\n So some sort of embodiment,\n\n1:42:22.740 --> 1:42:25.780\n whether it's physical or not, has to be part of it.\n\n1:42:25.780 --> 1:42:28.020\n So self awareness and the way to be able to answer\n\n1:42:28.020 --> 1:42:28.860\n where am I?\n\n1:42:28.860 --> 1:42:29.680\n Well, you're bringing up self,\n\n1:42:29.680 --> 1:42:31.460\n that's a different topic, self awareness.\n\n1:42:31.460 --> 1:42:33.700\n No, the very narrow definition of self,\n\n1:42:33.700 --> 1:42:37.740\n meaning knowing a sense of self enough to know\n\n1:42:37.740 --> 1:42:39.980\n where am I in the space where it's actually.\n\n1:42:39.980 --> 1:42:43.500\n Yeah, basically the system needs to know its location\n\n1:42:43.500 --> 1:42:46.020\n or each component of the system needs to know\n\n1:42:46.020 --> 1:42:48.620\n where it is in the world at that point in time.\n\n1:42:48.620 --> 1:42:51.660\n So self awareness and consciousness.\n\n1:42:51.660 --> 1:42:55.620\n Do you think one, from the perspective of neuroscience\n\n1:42:55.620 --> 1:42:58.180\n and neurocortex, these are interesting topics,\n\n1:42:58.180 --> 1:42:59.780\n solvable topics.\n\n1:42:59.780 --> 1:43:02.180\n Do you have any ideas of why the heck it is\n\n1:43:02.180 --> 1:43:04.420\n that we have a subjective experience at all?\n\n1:43:04.420 --> 1:43:05.260\n Yeah, I have a lot of thoughts on that.\n\n1:43:05.260 --> 1:43:08.460\n And is it useful or is it just a side effect of us?\n\n1:43:08.460 --> 1:43:10.140\n It's interesting to think about.\n\n1:43:10.140 --> 1:43:13.460\n I don't think it's useful as a means to figure out\n\n1:43:13.460 --> 1:43:15.160\n how to build intelligent machines.\n\n1:43:16.360 --> 1:43:20.180\n It's something that systems do\n\n1:43:20.180 --> 1:43:22.780\n and we can talk about what it is that are like,\n\n1:43:22.780 --> 1:43:23.980\n well, if I build a system like this,\n\n1:43:23.980 --> 1:43:25.300\n then it would be self aware.\n\n1:43:25.300 --> 1:43:28.340\n Or if I build it like this, it wouldn't be self aware.\n\n1:43:28.340 --> 1:43:30.040\n So that's a choice I can have.\n\n1:43:30.040 --> 1:43:32.300\n It's not like, oh my God, it's self aware.\n\n1:43:32.300 --> 1:43:35.800\n I can't turn, I heard an interview recently\n\n1:43:35.800 --> 1:43:37.120\n with this philosopher from Yale,\n\n1:43:37.120 --> 1:43:39.020\n I can't remember his name, I apologize for that.\n\n1:43:39.020 --> 1:43:39.860\n But he was talking about,\n\n1:43:39.860 --> 1:43:41.420\n well, if these computers are self aware,\n\n1:43:41.420 --> 1:43:42.900\n then it would be a crime to unplug them.\n\n1:43:42.900 --> 1:43:45.060\n And I'm like, oh, come on, that's not,\n\n1:43:45.060 --> 1:43:47.260\n I unplug myself every night, I go to sleep.\n\n1:43:47.260 --> 1:43:48.260\n Is that a crime?\n\n1:43:48.260 --> 1:43:51.340\n I plug myself in again in the morning and there I am.\n\n1:43:51.340 --> 1:43:56.020\n So people get kind of bent out of shape about this.\n\n1:43:56.020 --> 1:43:59.500\n I have very definite, very detailed understanding\n\n1:43:59.500 --> 1:44:02.260\n or opinions about what it means to be conscious\n\n1:44:02.260 --> 1:44:04.580\n and what it means to be self aware.\n\n1:44:04.580 --> 1:44:06.780\n I don't think it's that interesting a problem.\n\n1:44:06.780 --> 1:44:08.740\n You've talked to Christoph Koch.\n\n1:44:08.740 --> 1:44:10.900\n He thinks that's the only problem.\n\n1:44:10.900 --> 1:44:12.380\n I didn't actually listen to your interview with him,\n\n1:44:12.380 --> 1:44:15.820\n but I know him and I know that's the thing he cares about.\n\n1:44:15.820 --> 1:44:18.260\n He also thinks intelligence and consciousness are disjoint.\n\n1:44:18.260 --> 1:44:21.020\n So I mean, it's not, you don't have to have one or the other.\n\n1:44:21.020 --> 1:44:21.860\n So he is.\n\n1:44:21.860 --> 1:44:22.740\n I disagree with that.\n\n1:44:22.740 --> 1:44:24.600\n I just totally disagree with that.\n\n1:44:24.600 --> 1:44:26.300\n So where's your thoughts and consciousness,\n\n1:44:26.300 --> 1:44:27.660\n where does it emerge from?\n\n1:44:27.660 --> 1:44:28.500\n Because it is.\n\n1:44:28.500 --> 1:44:30.860\n So then we have to break it down to the two parts, okay?\n\n1:44:30.860 --> 1:44:32.140\n Because consciousness isn't one thing.\n\n1:44:32.140 --> 1:44:33.660\n That's part of the problem with that term\n\n1:44:33.660 --> 1:44:35.460\n is it means different things to different people\n\n1:44:35.460 --> 1:44:37.600\n and there's different components of it.\n\n1:44:37.600 --> 1:44:40.820\n There is a concept of self awareness, okay?\n\n1:44:40.820 --> 1:44:43.100\n That can be very easily explained.\n\n1:44:43.100 --> 1:44:46.060\n You have a model of your own body.\n\n1:44:46.060 --> 1:44:48.140\n The neocortex models things in the world\n\n1:44:48.140 --> 1:44:50.500\n and it also models your own body.\n\n1:44:50.500 --> 1:44:53.340\n And then it has a memory.\n\n1:44:53.340 --> 1:44:55.860\n It can remember what you've done, okay?\n\n1:44:55.860 --> 1:44:57.540\n So it can remember what you did this morning,\n\n1:44:57.540 --> 1:44:59.640\n can remember what you had for breakfast and so on.\n\n1:44:59.640 --> 1:45:01.820\n And so I can say to you, okay, Lex,\n\n1:45:03.080 --> 1:45:06.900\n were you conscious this morning when you had your bagel?\n\n1:45:06.900 --> 1:45:08.820\n And you'd say, yes, I was conscious.\n\n1:45:08.820 --> 1:45:10.300\n Now what if I could take your brain\n\n1:45:10.300 --> 1:45:12.020\n and revert all the synapses back\n\n1:45:12.020 --> 1:45:14.180\n to the state they were this morning?\n\n1:45:14.180 --> 1:45:15.900\n And then I said to you, Lex,\n\n1:45:15.900 --> 1:45:17.220\n were you conscious when you ate the bagel?\n\n1:45:17.220 --> 1:45:18.540\n And you said, no, I wasn't conscious.\n\n1:45:18.540 --> 1:45:19.740\n I said, here's a video of eating the bagel.\n\n1:45:19.740 --> 1:45:21.460\n And you said, I wasn't there.\n\n1:45:22.420 --> 1:45:23.380\n That's not possible\n\n1:45:23.380 --> 1:45:25.660\n because I must've been unconscious at that time.\n\n1:45:25.660 --> 1:45:27.460\n So we can just make this one to one correlation\n\n1:45:27.460 --> 1:45:31.000\n between memory of your body's trajectory through the world\n\n1:45:31.000 --> 1:45:32.100\n over some period of time,\n\n1:45:32.100 --> 1:45:34.260\n a memory and the ability to recall that memory\n\n1:45:34.260 --> 1:45:35.900\n is what you would call conscious.\n\n1:45:35.900 --> 1:45:38.940\n I was conscious of that, it's a self awareness.\n\n1:45:38.940 --> 1:45:41.340\n And any system that can recall,\n\n1:45:41.340 --> 1:45:43.540\n memorize what it's done recently\n\n1:45:43.540 --> 1:45:46.340\n and bring that back and invoke it again\n\n1:45:46.340 --> 1:45:48.220\n would say, yeah, I'm aware.\n\n1:45:48.220 --> 1:45:49.380\n I remember what I did.\n\n1:45:49.380 --> 1:45:50.420\n All right, I got it.\n\n1:45:51.340 --> 1:45:52.420\n That's an easy one.\n\n1:45:52.420 --> 1:45:54.780\n Although some people think that's a hard one.\n\n1:45:54.780 --> 1:45:57.380\n The more challenging part of consciousness\n\n1:45:57.380 --> 1:45:59.060\n is this one that's sometimes used\n\n1:45:59.060 --> 1:46:01.300\n going by the word of qualia,\n\n1:46:01.300 --> 1:46:04.860\n which is, why does an object seem red?\n\n1:46:04.860 --> 1:46:06.860\n Or what is pain?\n\n1:46:06.860 --> 1:46:08.740\n And why does pain feel like something?\n\n1:46:08.740 --> 1:46:10.380\n Why do I feel redness?\n\n1:46:10.380 --> 1:46:12.660\n Or why do I feel painness?\n\n1:46:12.660 --> 1:46:13.500\n And then I could say, well,\n\n1:46:13.500 --> 1:46:15.620\n why does sight seems different than hearing?\n\n1:46:15.620 --> 1:46:16.460\n It's the same problem.\n\n1:46:16.460 --> 1:46:18.580\n It's really, these are all just neurons.\n\n1:46:18.580 --> 1:46:20.300\n And so how is it that,\n\n1:46:20.300 --> 1:46:24.140\n why does looking at you feel different than hearing you?\n\n1:46:24.140 --> 1:46:26.080\n It feels different, but there's just neurons in my head.\n\n1:46:26.080 --> 1:46:27.820\n They're all doing the same thing.\n\n1:46:27.820 --> 1:46:29.820\n So that's an interesting question.\n\n1:46:29.820 --> 1:46:31.540\n The best treatise I've read about this\n\n1:46:31.540 --> 1:46:33.580\n is by a guy named Oregon.\n\n1:46:33.580 --> 1:46:34.740\n He wrote a book called,\n\n1:46:35.740 --> 1:46:37.480\n Why Red Doesn't Sound Like a Bell.\n\n1:46:37.480 --> 1:46:42.040\n It's a little, it's not a trade book, easy to read,\n\n1:46:42.040 --> 1:46:46.040\n but it, and it's an interesting question.\n\n1:46:46.040 --> 1:46:47.880\n Take something like color.\n\n1:46:47.880 --> 1:46:49.360\n Color really doesn't exist in the world.\n\n1:46:49.360 --> 1:46:51.160\n It's not a property of the world.\n\n1:46:51.160 --> 1:46:54.240\n Property of the world that exists is light frequency.\n\n1:46:54.240 --> 1:46:55.640\n And that gets turned into,\n\n1:46:55.640 --> 1:46:57.960\n we have certain cells in the retina\n\n1:46:57.960 --> 1:46:59.320\n that respond to different frequencies\n\n1:46:59.320 --> 1:47:00.240\n different than others.\n\n1:47:00.240 --> 1:47:01.440\n And so when they enter the brain,\n\n1:47:01.440 --> 1:47:02.440\n you just have a bunch of axons\n\n1:47:02.440 --> 1:47:04.500\n that are firing at different rates.\n\n1:47:04.500 --> 1:47:06.680\n And from that, we perceive color.\n\n1:47:06.680 --> 1:47:07.960\n But there is no color in the brain.\n\n1:47:07.960 --> 1:47:10.840\n I mean, there's no color coming in on those synapses.\n\n1:47:10.840 --> 1:47:14.380\n It's just a correlation between some axons\n\n1:47:14.380 --> 1:47:16.400\n and some property of frequency.\n\n1:47:17.360 --> 1:47:18.880\n And that isn't even color itself.\n\n1:47:18.880 --> 1:47:20.140\n Frequency doesn't have a color.\n\n1:47:20.140 --> 1:47:22.940\n It's just what it is.\n\n1:47:22.940 --> 1:47:24.120\n So then the question is,\n\n1:47:24.120 --> 1:47:26.820\n well, why does it even appear to have a color at all?\n\n1:47:27.880 --> 1:47:29.080\n Just as you're describing it,\n\n1:47:29.080 --> 1:47:31.000\n there seems to be a connection to those ideas\n\n1:47:31.000 --> 1:47:32.560\n of reference frames.\n\n1:47:32.560 --> 1:47:37.040\n I mean, it just feels like consciousness\n\n1:47:37.040 --> 1:47:38.400\n having the subject,\n\n1:47:38.400 --> 1:47:42.600\n assigning the feeling of red to the actual color\n\n1:47:42.600 --> 1:47:47.600\n or to the wavelength is useful for intelligence.\n\n1:47:47.920 --> 1:47:49.600\n Yeah, I think that's a good way of putting it.\n\n1:47:49.600 --> 1:47:51.600\n It's useful as a predictive mechanism\n\n1:47:51.600 --> 1:47:53.840\n or useful as a generalization idea.\n\n1:47:53.840 --> 1:47:55.660\n It's a way of grouping things together to say,\n\n1:47:55.660 --> 1:47:57.560\n it's useful to have a model like this.\n\n1:47:57.560 --> 1:48:02.560\n So think about the well known syndrome\n\n1:48:02.640 --> 1:48:04.800\n that people who've lost a limb experience\n\n1:48:04.800 --> 1:48:06.960\n called phantom limbs.\n\n1:48:06.960 --> 1:48:11.960\n And what they claim is they can have their arm is removed,\n\n1:48:12.120 --> 1:48:13.280\n but they feel their arm.\n\n1:48:13.280 --> 1:48:15.960\n That not only feel it, they know it's there.\n\n1:48:15.960 --> 1:48:17.740\n It's there, I know it's there.\n\n1:48:17.740 --> 1:48:19.000\n They'll swear to you that it's there.\n\n1:48:19.000 --> 1:48:20.360\n And then they can feel pain in their arm\n\n1:48:20.360 --> 1:48:21.840\n and they'll feel pain in their finger.\n\n1:48:21.840 --> 1:48:25.280\n And if they move their non existent arm behind their back,\n\n1:48:25.280 --> 1:48:27.320\n then they feel the pain behind their back.\n\n1:48:27.320 --> 1:48:30.120\n So this whole idea that your arm exists\n\n1:48:30.120 --> 1:48:31.360\n is a model of your brain.\n\n1:48:31.360 --> 1:48:33.480\n It may or may not really exist.\n\n1:48:34.360 --> 1:48:38.520\n And just like, but it's useful to have a model of something\n\n1:48:38.520 --> 1:48:40.360\n that sort of correlates to things in the world.\n\n1:48:40.360 --> 1:48:41.960\n So you can make predictions about what would happen\n\n1:48:41.960 --> 1:48:43.520\n when those things occur.\n\n1:48:43.520 --> 1:48:44.640\n It's a little bit of a fuzzy,\n\n1:48:44.640 --> 1:48:46.480\n but I think you're getting quite towards the answer there.\n\n1:48:46.480 --> 1:48:51.280\n It's useful for the model to express things certain ways\n\n1:48:51.280 --> 1:48:53.640\n that we can then map them into these reference frames\n\n1:48:53.640 --> 1:48:55.780\n and make predictions about them.\n\n1:48:55.780 --> 1:48:57.680\n I need to spend more time on this topic.\n\n1:48:57.680 --> 1:48:58.880\n It doesn't bother me.\n\n1:48:58.880 --> 1:49:00.360\n Do you really need to spend more time?\n\n1:49:00.360 --> 1:49:01.840\n Yeah, I know.\n\n1:49:01.840 --> 1:49:04.720\n It does feel special that we have subjective experience,\n\n1:49:04.720 --> 1:49:07.320\n but I'm yet to know why.\n\n1:49:07.320 --> 1:49:09.040\n I'm just personally curious.\n\n1:49:09.040 --> 1:49:11.400\n It's not necessary for the work we're doing here.\n\n1:49:11.400 --> 1:49:13.080\n I don't think I need to solve that problem\n\n1:49:13.080 --> 1:49:15.560\n to build intelligent machines at all, not at all.\n\n1:49:15.560 --> 1:49:17.800\n But there is sort of the silly notion\n\n1:49:17.800 --> 1:49:19.480\n that you described briefly\n\n1:49:20.440 --> 1:49:23.280\n that doesn't seem so silly to us humans is,\n\n1:49:23.280 --> 1:49:27.080\n if you're successful building intelligent machines,\n\n1:49:27.080 --> 1:49:30.240\n it feels wrong to then turn them off.\n\n1:49:30.240 --> 1:49:33.240\n Because if you're able to build a lot of them,\n\n1:49:33.240 --> 1:49:38.240\n it feels wrong to then be able to turn off the...\n\n1:49:38.760 --> 1:49:39.600\n Well, why?\n\n1:49:39.600 --> 1:49:41.840\n Let's break that down a bit.\n\n1:49:41.840 --> 1:49:43.920\n As humans, why do we fear death?\n\n1:49:43.920 --> 1:49:45.760\n There's two reasons we fear death.\n\n1:49:47.060 --> 1:49:47.900\n Well, first of all, I'll say,\n\n1:49:47.900 --> 1:49:48.960\n when you're dead, it doesn't matter at all.\n\n1:49:48.960 --> 1:49:49.800\n Who cares?\n\n1:49:49.800 --> 1:49:50.640\n You're dead.\n\n1:49:50.640 --> 1:49:51.840\n So why do we fear death?\n\n1:49:51.840 --> 1:49:53.480\n We fear death for two reasons.\n\n1:49:53.480 --> 1:49:57.760\n One is because we are programmed genetically to fear death.\n\n1:49:57.760 --> 1:50:01.840\n That's a survival and pop beginning of the genes thing.\n\n1:50:02.940 --> 1:50:05.120\n And we also are programmed to feel sad\n\n1:50:05.120 --> 1:50:06.880\n when people we know die.\n\n1:50:06.880 --> 1:50:08.560\n We don't feel sad for someone we don't know dies.\n\n1:50:08.560 --> 1:50:09.600\n There's people dying right now,\n\n1:50:09.600 --> 1:50:10.420\n they're only just gonna say,\n\n1:50:10.420 --> 1:50:11.260\n I don't feel bad about them,\n\n1:50:11.260 --> 1:50:12.080\n because I don't know them.\n\n1:50:12.080 --> 1:50:13.420\n But if I knew them, I'd feel really bad.\n\n1:50:13.420 --> 1:50:16.840\n So again, these are old brain,\n\n1:50:16.840 --> 1:50:19.880\n genetically embedded things that we fear death.\n\n1:50:19.880 --> 1:50:24.280\n It's outside of those uncomfortable feelings.\n\n1:50:24.280 --> 1:50:25.840\n There's nothing else to worry about.\n\n1:50:25.840 --> 1:50:27.360\n Well, wait, hold on a second.\n\n1:50:27.360 --> 1:50:30.440\n Do you know the denial of death by Becker?\n\n1:50:30.440 --> 1:50:31.360\n No.\n\n1:50:31.360 --> 1:50:34.440\n There's a thought that death is,\n\n1:50:36.760 --> 1:50:41.280\n our whole conception of our world model\n\n1:50:41.280 --> 1:50:43.800\n kind of assumes immortality.\n\n1:50:43.800 --> 1:50:47.040\n And then death is this terror that underlies it all.\n\n1:50:47.040 --> 1:50:47.880\n So like...\n\n1:50:47.880 --> 1:50:50.400\n Some people's world model, not mine.\n\n1:50:50.400 --> 1:50:52.760\n But, okay, so what Becker would say\n\n1:50:52.760 --> 1:50:54.520\n is that you're just living in an illusion.\n\n1:50:54.520 --> 1:50:56.200\n You've constructed an illusion for yourself\n\n1:50:56.200 --> 1:50:59.000\n because it's such a terrible terror,\n\n1:50:59.000 --> 1:51:00.160\n the fact that this...\n\n1:51:00.160 --> 1:51:01.160\n What's the illusion?\n\n1:51:01.160 --> 1:51:02.640\n The illusion that death doesn't matter.\n\n1:51:02.640 --> 1:51:04.800\n You're still not coming to grips with...\n\n1:51:04.800 --> 1:51:05.620\n The illusion of what?\n\n1:51:05.620 --> 1:51:07.120\n That death is...\n\n1:51:07.120 --> 1:51:08.700\n Going to happen.\n\n1:51:08.700 --> 1:51:10.440\n Oh, like it's not gonna happen?\n\n1:51:10.440 --> 1:51:11.880\n You're actually operating.\n\n1:51:11.880 --> 1:51:14.280\n You haven't, even though you said you've accepted it,\n\n1:51:14.280 --> 1:51:16.120\n you haven't really accepted the notion that you're gonna die\n\n1:51:16.120 --> 1:51:16.960\n is what you say.\n\n1:51:16.960 --> 1:51:21.440\n So it sounds like you disagree with that notion.\n\n1:51:21.440 --> 1:51:22.400\n Yeah, yeah, totally.\n\n1:51:22.400 --> 1:51:27.400\n I literally, every night I go to bed, it's like dying.\n\n1:51:28.040 --> 1:51:28.880\n Like little deaths.\n\n1:51:28.880 --> 1:51:29.720\n It's little deaths.\n\n1:51:29.720 --> 1:51:32.960\n And if I didn't wake up, it wouldn't matter to me.\n\n1:51:32.960 --> 1:51:35.160\n Only if I knew that was gonna happen would it be bothersome.\n\n1:51:35.160 --> 1:51:37.600\n If I didn't know it was gonna happen, how would I know?\n\n1:51:37.600 --> 1:51:39.520\n Then I would worry about my wife.\n\n1:51:39.520 --> 1:51:43.040\n So imagine I was a loner and I lived in Alaska\n\n1:51:43.040 --> 1:51:45.420\n and I lived out there and there was no animals.\n\n1:51:45.420 --> 1:51:46.480\n Nobody knew I existed.\n\n1:51:46.480 --> 1:51:48.720\n I was just eating these roots all the time.\n\n1:51:48.720 --> 1:51:51.120\n And nobody knew I was there.\n\n1:51:51.120 --> 1:51:53.320\n And one day I didn't wake up.\n\n1:51:54.680 --> 1:51:57.040\n What pain in the world would there exist?\n\n1:51:57.040 --> 1:51:59.800\n Well, so most people that think about this problem\n\n1:51:59.800 --> 1:52:01.960\n would say that you're just deeply enlightened\n\n1:52:01.960 --> 1:52:04.120\n or are completely delusional.\n\n1:52:04.120 --> 1:52:05.920\n One of the two.\n\n1:52:05.920 --> 1:52:10.720\n But I would say that's a very enlightened way\n\n1:52:10.720 --> 1:52:11.720\n to see the world.\n\n1:52:13.120 --> 1:52:14.760\n That's the rational one as well.\n\n1:52:14.760 --> 1:52:15.760\n It's rational, that's right.\n\n1:52:15.760 --> 1:52:17.840\n But the fact is we don't,\n\n1:52:19.040 --> 1:52:22.360\n I mean, we really don't have an understanding\n\n1:52:22.360 --> 1:52:24.920\n of why the heck it is we're born and why we die\n\n1:52:24.920 --> 1:52:25.960\n and what happens after we die.\n\n1:52:25.960 --> 1:52:27.880\n Well, maybe there isn't a reason, maybe there is.\n\n1:52:27.880 --> 1:52:30.120\n So I'm interested in those big problems too, right?\n\n1:52:30.120 --> 1:52:32.560\n You interviewed Max Tegmark,\n\n1:52:32.560 --> 1:52:33.600\n and there's people like that, right?\n\n1:52:33.600 --> 1:52:35.240\n I'm interested in those big problems as well.\n\n1:52:35.240 --> 1:52:38.240\n And in fact, when I was young,\n\n1:52:38.240 --> 1:52:41.200\n I made a list of the biggest problems I could think of.\n\n1:52:41.200 --> 1:52:43.360\n First, why does anything exist?\n\n1:52:43.360 --> 1:52:46.600\n Second, why do we have the laws of physics that we have?\n\n1:52:46.600 --> 1:52:49.200\n Third, is life inevitable?\n\n1:52:49.200 --> 1:52:50.120\n And why is it here?\n\n1:52:50.120 --> 1:52:52.240\n Fourth, is intelligence inevitable?\n\n1:52:52.240 --> 1:52:53.080\n And why is it here?\n\n1:52:53.080 --> 1:52:55.000\n I stopped there because I figured\n\n1:52:55.000 --> 1:52:57.840\n if you can make a truly intelligent system,\n\n1:52:57.840 --> 1:52:59.240\n that will be the quickest way\n\n1:52:59.240 --> 1:53:01.040\n to answer the first three questions.\n\n1:53:01.040 --> 1:53:04.440\n I'm serious.\n\n1:53:04.440 --> 1:53:07.960\n And so I said, my mission, you asked me earlier,\n\n1:53:07.960 --> 1:53:09.440\n my first mission is to understand the brain,\n\n1:53:09.440 --> 1:53:10.760\n but I felt that is the shortest way\n\n1:53:10.760 --> 1:53:12.160\n to get to true machine intelligence.\n\n1:53:12.160 --> 1:53:13.680\n And I wanna get to true machine intelligence\n\n1:53:13.680 --> 1:53:15.920\n because even if it doesn't occur in my lifetime,\n\n1:53:15.920 --> 1:53:17.480\n other people will benefit from it\n\n1:53:17.480 --> 1:53:19.200\n because I think it'll occur in my lifetime,\n\n1:53:19.200 --> 1:53:21.040\n but 20 years, you never know.\n\n1:53:23.640 --> 1:53:26.080\n But that will be the quickest way for us to,\n\n1:53:26.080 --> 1:53:27.800\n we can make super mathematicians,\n\n1:53:27.800 --> 1:53:29.520\n we can make super space explorers,\n\n1:53:29.520 --> 1:53:34.240\n we can make super physicist brains that do these things\n\n1:53:34.240 --> 1:53:37.440\n and that can run experiments that we can't run.\n\n1:53:37.440 --> 1:53:40.360\n We don't have the abilities to manipulate things and so on,\n\n1:53:40.360 --> 1:53:42.800\n but we can build intelligent machines that do all those things\n\n1:53:42.800 --> 1:53:46.560\n with the ultimate goal of finding out the answers\n\n1:53:46.560 --> 1:53:47.680\n to the other questions.\n\n1:53:48.800 --> 1:53:51.480\n Let me ask you another depressing and difficult question,\n\n1:53:51.480 --> 1:53:56.480\n which is once we achieve that goal of creating,\n\n1:53:57.880 --> 1:54:01.200\n no, of understanding intelligence,\n\n1:54:01.200 --> 1:54:02.960\n do you think we would be happier,\n\n1:54:02.960 --> 1:54:04.760\n more fulfilled as a species?\n\n1:54:04.760 --> 1:54:05.720\n The understanding intelligence\n\n1:54:05.720 --> 1:54:07.920\n or understanding the answers to the big questions?\n\n1:54:07.920 --> 1:54:08.960\n Understanding intelligence.\n\n1:54:08.960 --> 1:54:11.800\n Oh, totally, totally.\n\n1:54:11.800 --> 1:54:13.960\n It would be far more fun place to live.\n\n1:54:13.960 --> 1:54:14.800\n You think so?\n\n1:54:14.800 --> 1:54:15.680\n Oh yeah, why not?\n\n1:54:15.680 --> 1:54:19.720\n I mean, just put aside this terminator nonsense\n\n1:54:19.720 --> 1:54:24.320\n and just think about, you can think about,\n\n1:54:24.320 --> 1:54:26.840\n we can talk about the risks of AI if you want.\n\n1:54:26.840 --> 1:54:28.240\n I'd love to, so let's talk about.\n\n1:54:28.240 --> 1:54:30.640\n But I think the world would be far better knowing things.\n\n1:54:30.640 --> 1:54:32.080\n We're always better than know things.\n\n1:54:32.080 --> 1:54:35.080\n Do you think it's better, is it a better place to live in\n\n1:54:35.080 --> 1:54:37.440\n that I know that our planet is one of many\n\n1:54:37.440 --> 1:54:39.240\n in the solar system and the solar system's one of many\n\n1:54:39.240 --> 1:54:40.080\n in the galaxy?\n\n1:54:40.080 --> 1:54:43.400\n I think it's a more, I dread, I sometimes think like,\n\n1:54:43.400 --> 1:54:45.360\n God, what would it be like to live 300 years ago?\n\n1:54:45.360 --> 1:54:47.440\n I'd be looking up at the sky, I can't understand anything.\n\n1:54:47.440 --> 1:54:49.240\n Oh my God, I'd be like going to bed every night going,\n\n1:54:49.240 --> 1:54:50.160\n what's going on here?\n\n1:54:50.160 --> 1:54:52.480\n Well, I mean, in some sense I agree with you,\n\n1:54:52.480 --> 1:54:54.800\n but I'm not exactly sure.\n\n1:54:54.800 --> 1:54:57.240\n So I'm also a scientist, so I share your views,\n\n1:54:57.240 --> 1:55:00.960\n but I'm not, we're like rolling down the hill together.\n\n1:55:02.640 --> 1:55:03.480\n What's down the hill?\n\n1:55:03.480 --> 1:55:05.280\n I feel like we're climbing a hill.\n\n1:55:05.280 --> 1:55:06.120\n Whatever.\n\n1:55:06.120 --> 1:55:07.640\n We're getting closer to enlightenment\n\n1:55:07.640 --> 1:55:10.200\n and you're going down the hill.\n\n1:55:10.200 --> 1:55:12.240\n We're climbing, we're getting pulled up a hill\n\n1:55:12.240 --> 1:55:13.840\n by our curiosity.\n\n1:55:13.840 --> 1:55:16.120\n Our curiosity is, we're pulling ourselves up the hill\n\n1:55:16.120 --> 1:55:16.960\n by our curiosity.\n\n1:55:16.960 --> 1:55:19.160\n Yeah, Sisyphus was doing the same thing with the rock.\n\n1:55:19.160 --> 1:55:20.880\n Yeah, yeah, yeah, yeah.\n\n1:55:20.880 --> 1:55:24.280\n But okay, our happiness aside, do you have concerns\n\n1:55:24.280 --> 1:55:29.040\n about, you talk about Sam Harris, Elon Musk,\n\n1:55:29.040 --> 1:55:31.880\n of existential threats of intelligent systems?\n\n1:55:31.880 --> 1:55:33.800\n No, I'm not worried about existential threats at all.\n\n1:55:33.800 --> 1:55:36.400\n There are some things we really do need to worry about.\n\n1:55:36.400 --> 1:55:38.440\n Even today's AI, we have things we have to worry about.\n\n1:55:38.440 --> 1:55:39.560\n We have to worry about privacy\n\n1:55:39.560 --> 1:55:42.800\n and about how it impacts false beliefs in the world.\n\n1:55:42.800 --> 1:55:47.000\n And we have real problems and things to worry about\n\n1:55:47.000 --> 1:55:48.280\n with today's AI.\n\n1:55:48.280 --> 1:55:51.480\n And that will continue as we create more intelligent systems.\n\n1:55:51.480 --> 1:55:53.080\n There's no question, the whole issue\n\n1:55:53.080 --> 1:55:57.080\n about making intelligent armaments and weapons\n\n1:55:57.080 --> 1:55:59.920\n is something that really we have to think about carefully.\n\n1:55:59.920 --> 1:56:01.880\n I don't think of those as existential threats.\n\n1:56:01.880 --> 1:56:04.320\n I think those are the kind of threats we always face\n\n1:56:04.320 --> 1:56:05.840\n and we'll have to face them here\n\n1:56:05.840 --> 1:56:08.560\n and we'll have to deal with them.\n\n1:56:10.400 --> 1:56:12.040\n We could talk about what people think\n\n1:56:12.040 --> 1:56:13.880\n are the existential threats,\n\n1:56:13.880 --> 1:56:16.200\n but when I hear people talking about them,\n\n1:56:16.200 --> 1:56:17.760\n they all sound hollow to me.\n\n1:56:17.760 --> 1:56:20.000\n They're based on ideas, they're based on people\n\n1:56:20.000 --> 1:56:22.160\n who really have no idea what intelligence is.\n\n1:56:22.160 --> 1:56:24.920\n And if they knew what intelligence was,\n\n1:56:24.920 --> 1:56:26.640\n they wouldn't say those things.\n\n1:56:26.640 --> 1:56:28.600\n So those are not experts in the field.\n\n1:56:28.600 --> 1:56:32.040\n Yeah, so there's two, right?\n\n1:56:32.040 --> 1:56:33.720\n So one is like super intelligence.\n\n1:56:33.720 --> 1:56:37.720\n So a system that becomes far, far superior\n\n1:56:37.720 --> 1:56:42.720\n in reasoning ability than us humans.\n\n1:56:43.160 --> 1:56:44.800\n How is that an existential threat?\n\n1:56:46.200 --> 1:56:49.120\n Then, so there's a lot of ways in which it could be.\n\n1:56:49.120 --> 1:56:54.040\n One way is us humans are actually irrational, inefficient\n\n1:56:54.040 --> 1:56:59.040\n and get in the way of, not happiness,\n\n1:57:00.520 --> 1:57:02.120\n but whatever the objective function is\n\n1:57:02.120 --> 1:57:04.320\n of maximizing that objective function.\n\n1:57:04.320 --> 1:57:05.160\n Super intelligent.\n\n1:57:05.160 --> 1:57:06.720\n The paperclip problem and things like that.\n\n1:57:06.720 --> 1:57:09.440\n So the paperclip problem but with the super intelligent.\n\n1:57:09.440 --> 1:57:10.480\n Yeah, yeah, yeah, yeah.\n\n1:57:10.480 --> 1:57:14.180\n So we already face this threat in some sense.\n\n1:57:15.680 --> 1:57:17.320\n They're called bacteria.\n\n1:57:17.320 --> 1:57:18.960\n These are organisms in the world\n\n1:57:18.960 --> 1:57:21.400\n that would like to turn everything into bacteria.\n\n1:57:21.400 --> 1:57:23.040\n And they're constantly morphing,\n\n1:57:23.040 --> 1:57:26.360\n they're constantly changing to evade our protections.\n\n1:57:26.360 --> 1:57:30.680\n And in the past, they have killed huge swaths\n\n1:57:30.680 --> 1:57:33.400\n of populations of humans on this planet.\n\n1:57:33.400 --> 1:57:34.560\n So if you wanna worry about something\n\n1:57:34.560 --> 1:57:38.360\n that's gonna multiply endlessly, we have it.\n\n1:57:38.360 --> 1:57:40.600\n And I'm far more worried in that regard.\n\n1:57:40.600 --> 1:57:43.280\n I'm far more worried that some scientists in the laboratory\n\n1:57:43.280 --> 1:57:45.440\n will create a super virus or a super bacteria\n\n1:57:45.440 --> 1:57:47.120\n that we cannot control.\n\n1:57:47.120 --> 1:57:49.640\n That is a more of an existential threat.\n\n1:57:49.640 --> 1:57:52.160\n Putting an intelligence thing on top of it\n\n1:57:52.160 --> 1:57:54.240\n actually seems to make it less existential to me.\n\n1:57:54.240 --> 1:57:56.640\n It's like, it limits its power.\n\n1:57:56.640 --> 1:57:57.720\n It limits where it can go.\n\n1:57:57.720 --> 1:57:59.820\n It limits the number of things it can do in many ways.\n\n1:57:59.820 --> 1:58:03.080\n A bacteria is something you can't even see.\n\n1:58:03.080 --> 1:58:04.240\n So that's only one of those problems.\n\n1:58:04.240 --> 1:58:05.080\n Yes, exactly.\n\n1:58:05.080 --> 1:58:09.600\n So the other one, just in your intuition about intelligence,\n\n1:58:09.600 --> 1:58:12.480\n when you think about intelligence of us humans,\n\n1:58:12.480 --> 1:58:14.880\n do you think of that as something,\n\n1:58:14.880 --> 1:58:16.960\n if you look at intelligence on a spectrum\n\n1:58:16.960 --> 1:58:18.900\n from zero to us humans,\n\n1:58:18.900 --> 1:58:22.080\n do you think you can scale that to something far,\n\n1:58:22.080 --> 1:58:24.760\n far superior to all the mechanisms we've been talking about?\n\n1:58:24.760 --> 1:58:28.360\n I wanna make another point here, Lex, before I get there.\n\n1:58:28.360 --> 1:58:30.920\n Intelligence is the neocortex.\n\n1:58:30.920 --> 1:58:32.400\n It is not the entire brain.\n\n1:58:34.080 --> 1:58:36.200\n The goal is not to make a human.\n\n1:58:36.200 --> 1:58:38.400\n The goal is not to make an emotional system.\n\n1:58:38.400 --> 1:58:39.560\n The goal is not to make a system\n\n1:58:39.560 --> 1:58:41.440\n that wants to have sex and reproduce.\n\n1:58:41.440 --> 1:58:42.880\n Why would I build that?\n\n1:58:42.880 --> 1:58:44.560\n If I wanna have a system that wants to reproduce\n\n1:58:44.560 --> 1:58:47.260\n and have sex, make bacteria, make computer viruses.\n\n1:58:47.260 --> 1:58:49.800\n Those are bad things, don't do that.\n\n1:58:49.800 --> 1:58:52.360\n Those are really bad, don't do those things.\n\n1:58:52.360 --> 1:58:53.560\n Regulate those.\n\n1:58:53.560 --> 1:58:56.120\n But if I just say I want an intelligent system,\n\n1:58:56.120 --> 1:58:58.560\n why does it have to have any of the human like emotions?\n\n1:58:58.560 --> 1:59:00.400\n Why does it even care if it lives?\n\n1:59:00.400 --> 1:59:02.560\n Why does it even care if it has food?\n\n1:59:02.560 --> 1:59:03.840\n It doesn't care about those things.\n\n1:59:03.840 --> 1:59:06.320\n It's just, you know, it's just in a trance\n\n1:59:06.320 --> 1:59:08.280\n thinking about mathematics or it's out there\n\n1:59:08.280 --> 1:59:12.280\n just trying to build the space for it on Mars.\n\n1:59:14.000 --> 1:59:15.320\n That's a choice we make.\n\n1:59:15.320 --> 1:59:17.160\n Don't make human like things,\n\n1:59:17.160 --> 1:59:18.480\n don't make replicating things,\n\n1:59:18.480 --> 1:59:19.840\n don't make things that have emotions,\n\n1:59:19.840 --> 1:59:21.000\n just stick to the neocortex.\n\n1:59:21.000 --> 1:59:23.120\n So that's a view actually that I share\n\n1:59:23.120 --> 1:59:25.400\n but not everybody shares in the sense that\n\n1:59:25.400 --> 1:59:29.840\n you have faith and optimism about us as engineers of systems,\n\n1:59:29.840 --> 1:59:34.840\n humans as builders of systems to not put in stupid, not.\n\n1:59:34.840 --> 1:59:37.640\n So this is why I mentioned the bacteria one.\n\n1:59:37.640 --> 1:59:40.760\n Because you might say, well, some person's gonna do that.\n\n1:59:40.760 --> 1:59:42.920\n Well, some person today could create a bacteria\n\n1:59:42.920 --> 1:59:46.920\n that's resistant to all the known antibacterial agents.\n\n1:59:46.920 --> 1:59:49.200\n So we already have that threat.\n\n1:59:49.200 --> 1:59:51.320\n We already know this is going on.\n\n1:59:51.320 --> 1:59:52.760\n It's not a new threat.\n\n1:59:52.760 --> 1:59:56.280\n So just accept that and then we have to deal with it, right?\n\n1:59:56.280 --> 1:59:59.600\n Yeah, so my point is nothing to do with intelligence.\n\n1:59:59.600 --> 2:00:01.920\n Intelligence is a separate component\n\n2:00:01.920 --> 2:00:03.560\n that you might apply to a system\n\n2:00:03.560 --> 2:00:06.040\n that wants to reproduce and do stupid things.\n\n2:00:06.040 --> 2:00:07.240\n Let's not do that.\n\n2:00:07.240 --> 2:00:08.400\n Yeah, in fact, it is a mystery\n\n2:00:08.400 --> 2:00:10.520\n why people haven't done that yet.\n\n2:00:10.520 --> 2:00:14.320\n My dad is a physicist, believes that the reason,\n\n2:00:14.320 --> 2:00:18.080\n he says, for example, nuclear weapons haven't proliferated\n\n2:00:18.080 --> 2:00:19.040\n amongst evil people.\n\n2:00:19.040 --> 2:00:21.680\n So one belief that I share is that\n\n2:00:21.680 --> 2:00:25.160\n there's not that many evil people in the world\n\n2:00:25.160 --> 2:00:30.160\n that would use, whether it's bacteria or nuclear weapons\n\n2:00:31.280 --> 2:00:35.000\n or maybe the future AI systems to do bad.\n\n2:00:35.000 --> 2:00:36.200\n So the fraction is small.\n\n2:00:36.200 --> 2:00:38.400\n And the second is that it's actually really hard,\n\n2:00:38.400 --> 2:00:42.480\n technically, so the intersection between evil\n\n2:00:42.480 --> 2:00:45.160\n and competent is small in terms of, and that's the.\n\n2:00:45.160 --> 2:00:47.000\n And by the way, to really annihilate humanity,\n\n2:00:47.000 --> 2:00:50.800\n you'd have to have sort of the nuclear winter phenomenon,\n\n2:00:50.800 --> 2:00:54.080\n which is not one person shooting or even 10 bombs.\n\n2:00:54.080 --> 2:00:56.440\n You'd have to have some automated system\n\n2:00:56.440 --> 2:00:58.520\n that detonates a million bombs\n\n2:00:58.520 --> 2:01:00.400\n or whatever many thousands we have.\n\n2:01:00.400 --> 2:01:03.080\n So extreme evil combined with extreme competence.\n\n2:01:03.080 --> 2:01:05.080\n And to start with building some stupid system\n\n2:01:05.080 --> 2:01:08.000\n that would automatically, Dr. Strangelove type of thing,\n\n2:01:08.000 --> 2:01:11.920\n you know, I mean, look, we could have\n\n2:01:11.920 --> 2:01:14.600\n some nuclear bomb go off in some major city in the world.\n\n2:01:14.600 --> 2:01:17.120\n I think that's actually quite likely, even in my lifetime.\n\n2:01:17.120 --> 2:01:18.480\n I don't think that's an unlikely thing.\n\n2:01:18.480 --> 2:01:19.600\n And it'd be a tragedy.\n\n2:01:20.600 --> 2:01:23.160\n But it won't be an existential threat.\n\n2:01:23.160 --> 2:01:26.560\n And it's the same as, you know, the virus of 1917,\n\n2:01:26.560 --> 2:01:28.960\n whatever it was, you know, the influenza.\n\n2:01:30.000 --> 2:01:33.880\n These bad things can happen and the plague and so on.\n\n2:01:33.880 --> 2:01:35.360\n We can't always prevent them.\n\n2:01:35.360 --> 2:01:37.040\n We always try, but we can't.\n\n2:01:37.040 --> 2:01:38.240\n But they're not existential threats\n\n2:01:38.240 --> 2:01:41.200\n until we combine all those crazy things together.\n\n2:01:41.200 --> 2:01:45.440\n So on the spectrum of intelligence from zero to human,\n\n2:01:45.440 --> 2:01:47.960\n do you have a sense of whether it's possible\n\n2:01:47.960 --> 2:01:51.560\n to create several orders of magnitude\n\n2:01:51.560 --> 2:01:54.680\n or at least double that of human intelligence?\n\n2:01:54.680 --> 2:01:55.920\n Talking about neuro context.\n\n2:01:55.920 --> 2:01:59.000\n I think it's the wrong thing to say double the intelligence.\n\n2:01:59.000 --> 2:02:01.600\n Break it down into different components.\n\n2:02:01.600 --> 2:02:03.640\n Can I make something that's a million times fast\n\n2:02:03.640 --> 2:02:04.520\n than a human brain?\n\n2:02:04.520 --> 2:02:06.280\n Yes, I can do that.\n\n2:02:06.280 --> 2:02:09.160\n Could I make something that is,\n\n2:02:09.160 --> 2:02:10.960\n has a lot more storage than the human brain?\n\n2:02:10.960 --> 2:02:11.880\n Yes, I could do that.\n\n2:02:11.880 --> 2:02:13.600\n More common, more copies of common.\n\n2:02:13.600 --> 2:02:14.720\n Can I make something that attaches\n\n2:02:14.720 --> 2:02:16.160\n to different sensors than human brain?\n\n2:02:16.160 --> 2:02:17.280\n Yes, I can do that.\n\n2:02:17.280 --> 2:02:19.280\n Could I make something that's distributed?\n\n2:02:19.280 --> 2:02:21.160\n So these people, yeah, we talked early\n\n2:02:21.160 --> 2:02:23.120\n about the departure of the neocortex voting.\n\n2:02:23.120 --> 2:02:24.240\n They don't have to be co located.\n\n2:02:24.240 --> 2:02:25.680\n Like, you know, they can be all around the place.\n\n2:02:25.680 --> 2:02:26.680\n I could do that too.\n\n2:02:29.080 --> 2:02:32.440\n Those are the levers I have, but is it more intelligent?\n\n2:02:32.440 --> 2:02:33.760\n Well, it depends what I train it on.\n\n2:02:33.760 --> 2:02:34.800\n What is it doing?\n\n2:02:34.800 --> 2:02:35.640\n If it's.\n\n2:02:35.640 --> 2:02:36.720\n Well, so here's the thing.\n\n2:02:36.720 --> 2:02:39.400\n So let's say larger neocortex\n\n2:02:39.400 --> 2:02:44.400\n and or whatever size that allows for higher\n\n2:02:44.720 --> 2:02:47.920\n and higher hierarchies to form,\n\n2:02:47.920 --> 2:02:50.160\n we're talking about reference frames and concepts.\n\n2:02:50.160 --> 2:02:51.920\n Could I have something that's a super physicist\n\n2:02:51.920 --> 2:02:52.920\n or a super mathematician?\n\n2:02:52.920 --> 2:02:53.760\n Yes.\n\n2:02:53.760 --> 2:02:56.680\n And the question is, once you have a super physicist,\n\n2:02:56.680 --> 2:02:58.760\n will they be able to understand something?\n\n2:03:00.440 --> 2:03:02.200\n Do you have a sense that it will be orders of math,\n\n2:03:02.200 --> 2:03:03.040\n like us compared to ants?\n\n2:03:03.040 --> 2:03:04.560\n Could we ever understand it?\n\n2:03:04.560 --> 2:03:06.080\n Yeah.\n\n2:03:06.080 --> 2:03:11.080\n Most people cannot understand general relativity.\n\n2:03:11.920 --> 2:03:13.280\n It's a really hard thing to get.\n\n2:03:13.280 --> 2:03:15.800\n I mean, yeah, you can paint it in a fuzzy picture,\n\n2:03:15.800 --> 2:03:17.560\n stretchy space, you know?\n\n2:03:17.560 --> 2:03:19.920\n But the field equations to do that\n\n2:03:19.920 --> 2:03:23.080\n and the deep intuitions are really, really hard.\n\n2:03:23.080 --> 2:03:25.960\n And I've tried, I'm unable to do it.\n\n2:03:25.960 --> 2:03:28.800\n Like it's easy to get special relativity,\n\n2:03:28.800 --> 2:03:30.960\n but general relativity, man, that's too much.\n\n2:03:32.360 --> 2:03:34.960\n And so we already live with this to some extent.\n\n2:03:34.960 --> 2:03:37.320\n The vast majority of people can't understand actually\n\n2:03:37.320 --> 2:03:40.280\n what the vast majority of other people actually know.\n\n2:03:40.280 --> 2:03:41.960\n We're just, either we don't have the effort to,\n\n2:03:41.960 --> 2:03:43.280\n or we can't, or we don't have time,\n\n2:03:43.280 --> 2:03:45.080\n or just not smart enough, whatever.\n\n2:03:46.920 --> 2:03:48.560\n But we have ways of communicating.\n\n2:03:48.560 --> 2:03:51.600\n Einstein has spoken in a way that I can understand.\n\n2:03:51.600 --> 2:03:54.600\n He's given me analogies that are useful.\n\n2:03:54.600 --> 2:03:56.880\n I can use those analogies from my own work\n\n2:03:56.880 --> 2:03:59.880\n and think about concepts that are similar.\n\n2:04:01.040 --> 2:04:02.200\n It's not stupid.\n\n2:04:02.200 --> 2:04:04.040\n It's not like he's existing some other plane\n\n2:04:04.040 --> 2:04:06.680\n and there's no connection with my plane in the world here.\n\n2:04:06.680 --> 2:04:07.840\n So that will occur.\n\n2:04:07.840 --> 2:04:09.280\n It already has occurred.\n\n2:04:09.280 --> 2:04:10.760\n That's what my point of this story is.\n\n2:04:10.760 --> 2:04:11.720\n It already has occurred.\n\n2:04:11.720 --> 2:04:13.160\n We live it every day.\n\n2:04:14.360 --> 2:04:17.040\n One could argue that when we create machine intelligence\n\n2:04:17.040 --> 2:04:18.720\n that think a million times faster than us\n\n2:04:18.720 --> 2:04:20.920\n that it'll be so far we can't make the connections.\n\n2:04:20.920 --> 2:04:22.560\n But you know, at the moment,\n\n2:04:23.480 --> 2:04:25.640\n everything that seems really, really hard\n\n2:04:25.640 --> 2:04:26.680\n to figure out in the world,\n\n2:04:26.680 --> 2:04:29.000\n when you actually figure it out, it's not that hard.\n\n2:04:29.000 --> 2:04:32.160\n You know, almost everyone can understand the multiverses.\n\n2:04:32.160 --> 2:04:34.040\n Almost everyone can understand quantum physics.\n\n2:04:34.040 --> 2:04:36.120\n Almost everyone can understand these basic things,\n\n2:04:36.120 --> 2:04:39.000\n even though hardly any people could figure those things out.\n\n2:04:39.000 --> 2:04:41.320\n Yeah, but really understand.\n\n2:04:41.320 --> 2:04:42.360\n But you don't need to really.\n\n2:04:42.360 --> 2:04:43.800\n Only a few people really understand.\n\n2:04:43.800 --> 2:04:47.880\n You need to only understand the projections,\n\n2:04:47.880 --> 2:04:50.120\n the sprinkles of the useful insights from that.\n\n2:04:50.120 --> 2:04:51.760\n That was my example of Einstein, right?\n\n2:04:51.760 --> 2:04:53.800\n His general theory of relativity is one thing\n\n2:04:53.800 --> 2:04:56.000\n that very, very, very few people can get.\n\n2:04:56.000 --> 2:04:58.240\n And what if we just said those other few people\n\n2:04:58.240 --> 2:05:00.600\n are also artificial intelligences?\n\n2:05:00.600 --> 2:05:01.440\n How bad is that?\n\n2:05:01.440 --> 2:05:02.720\n In some sense they are, right?\n\n2:05:02.720 --> 2:05:04.280\n Yeah, they say already.\n\n2:05:04.280 --> 2:05:06.280\n I mean, Einstein wasn't a really normal person.\n\n2:05:06.280 --> 2:05:07.560\n He had a lot of weird quirks.\n\n2:05:07.560 --> 2:05:09.440\n And so did the other people who worked with him.\n\n2:05:09.440 --> 2:05:11.360\n So, you know, maybe they already were sort of\n\n2:05:11.360 --> 2:05:14.200\n this astral plane of intelligence that,\n\n2:05:14.200 --> 2:05:15.240\n we live with it already.\n\n2:05:15.240 --> 2:05:17.000\n It's not a problem.\n\n2:05:17.000 --> 2:05:20.160\n It's still useful and, you know.\n\n2:05:20.160 --> 2:05:22.960\n So do you think we are the only intelligent life\n\n2:05:22.960 --> 2:05:24.880\n out there in the universe?\n\n2:05:24.880 --> 2:05:26.640\n I would say that intelligent life\n\n2:05:27.880 --> 2:05:29.760\n has and will exist elsewhere in the universe.\n\n2:05:29.760 --> 2:05:31.480\n I'll say that.\n\n2:05:31.480 --> 2:05:32.600\n There was a question about\n\n2:05:32.600 --> 2:05:34.080\n contemporaneous intelligence life,\n\n2:05:34.080 --> 2:05:35.600\n which is hard to even answer\n\n2:05:35.600 --> 2:05:39.480\n when we think about relativity and the nature of space time.\n\n2:05:39.480 --> 2:05:41.120\n Can't say what exactly is this time\n\n2:05:41.120 --> 2:05:43.160\n someplace else in the world.\n\n2:05:43.160 --> 2:05:44.600\n But I think it's, you know,\n\n2:05:44.600 --> 2:05:48.440\n I do worry a lot about the filter idea,\n\n2:05:48.440 --> 2:05:52.240\n which is that perhaps intelligent species\n\n2:05:52.240 --> 2:05:54.040\n don't last very long.\n\n2:05:54.040 --> 2:05:55.720\n And so we haven't been around very long.\n\n2:05:55.720 --> 2:05:57.200\n And as a technological species,\n\n2:05:57.200 --> 2:05:59.800\n we've been around for almost nothing, you know.\n\n2:05:59.800 --> 2:06:01.600\n What, 200 years, something like that.\n\n2:06:02.720 --> 2:06:04.160\n And we don't have any data,\n\n2:06:04.160 --> 2:06:06.040\n a good data point on whether it's likely\n\n2:06:06.040 --> 2:06:07.400\n that we'll survive or not.\n\n2:06:08.480 --> 2:06:10.960\n So do I think that there have been intelligent life\n\n2:06:10.960 --> 2:06:11.800\n elsewhere in the universe?\n\n2:06:11.800 --> 2:06:13.400\n Almost certainly, of course.\n\n2:06:13.400 --> 2:06:15.280\n In the past, in the future, yes.\n\n2:06:16.440 --> 2:06:17.880\n Does it survive for a long time?\n\n2:06:17.880 --> 2:06:18.840\n I don't know.\n\n2:06:18.840 --> 2:06:21.120\n This is another reason I'm excited about our work,\n\n2:06:21.120 --> 2:06:24.240\n is our work meaning the general world of AI.\n\n2:06:24.240 --> 2:06:27.440\n I think we can build intelligent machines\n\n2:06:28.640 --> 2:06:30.120\n that outlast us.\n\n2:06:32.040 --> 2:06:34.080\n You know, they don't have to be tied to Earth.\n\n2:06:34.080 --> 2:06:35.800\n They don't have to, you know,\n\n2:06:35.800 --> 2:06:38.200\n I'm not saying they're recreating, you know,\n\n2:06:38.200 --> 2:06:40.680\n aliens, I'm just saying,\n\n2:06:40.680 --> 2:06:41.920\n if I asked myself,\n\n2:06:41.920 --> 2:06:44.280\n and this might be a good point to end on here.\n\n2:06:44.280 --> 2:06:45.120\n If I asked myself, you know,\n\n2:06:45.120 --> 2:06:47.240\n what's special about our species?\n\n2:06:47.240 --> 2:06:49.040\n We're not particularly interesting physically.\n\n2:06:49.040 --> 2:06:51.480\n We don't fly, we're not good swimmers,\n\n2:06:51.480 --> 2:06:54.000\n we're not very fast, we're not very strong, you know.\n\n2:06:54.000 --> 2:06:55.480\n It's our brain, that's the only thing.\n\n2:06:55.480 --> 2:06:57.440\n And we are the only species on this planet\n\n2:06:57.440 --> 2:06:58.760\n that's built the model of the world\n\n2:06:58.760 --> 2:07:01.160\n that extends beyond what we can actually sense.\n\n2:07:01.160 --> 2:07:03.000\n We're the only people who know about\n\n2:07:03.000 --> 2:07:05.160\n the far side of the moon, and the other universes,\n\n2:07:05.160 --> 2:07:07.280\n and I mean, other galaxies, and other stars,\n\n2:07:07.280 --> 2:07:09.520\n and about what happens in the atom.\n\n2:07:09.520 --> 2:07:12.440\n There's no, that knowledge doesn't exist anywhere else.\n\n2:07:12.440 --> 2:07:13.800\n It's only in our heads.\n\n2:07:13.800 --> 2:07:15.000\n Cats don't do it, dogs don't do it,\n\n2:07:15.000 --> 2:07:16.360\n monkeys don't do it, it's just on.\n\n2:07:16.360 --> 2:07:18.320\n And that is what we've created that's unique.\n\n2:07:18.320 --> 2:07:20.360\n Not our genes, it's knowledge.\n\n2:07:20.360 --> 2:07:23.160\n And if I asked me, what is the legacy of humanity?\n\n2:07:23.160 --> 2:07:25.120\n What should our legacy be?\n\n2:07:25.120 --> 2:07:25.960\n It should be knowledge.\n\n2:07:25.960 --> 2:07:27.560\n We should preserve our knowledge\n\n2:07:27.560 --> 2:07:30.080\n in a way that it can exist beyond us.\n\n2:07:30.080 --> 2:07:32.040\n And I think the best way of doing that,\n\n2:07:32.040 --> 2:07:33.080\n in fact you have to do it,\n\n2:07:33.080 --> 2:07:34.880\n is it has to go along with intelligent machines\n\n2:07:34.880 --> 2:07:36.400\n that understand that knowledge.\n\n2:07:37.800 --> 2:07:41.920\n It's a very broad idea, but we should be thinking,\n\n2:07:41.920 --> 2:07:43.800\n I call it a state planning for humanity.\n\n2:07:43.800 --> 2:07:46.560\n We should be thinking about what we wanna leave behind\n\n2:07:46.560 --> 2:07:49.320\n when as a species we're no longer here.\n\n2:07:49.320 --> 2:07:51.080\n And that'll happen sometime.\n\n2:07:51.080 --> 2:07:52.480\n Sooner or later it's gonna happen.\n\n2:07:52.480 --> 2:07:56.080\n And understanding intelligence and creating intelligence\n\n2:07:56.080 --> 2:07:58.400\n gives us a better chance to prolong.\n\n2:07:58.400 --> 2:08:01.120\n It does give us a better chance to prolong life, yes.\n\n2:08:01.120 --> 2:08:03.200\n It gives us a chance to live on other planets.\n\n2:08:03.200 --> 2:08:06.080\n But even beyond that, I mean our solar system\n\n2:08:06.080 --> 2:08:08.680\n will disappear one day, just given enough time.\n\n2:08:08.680 --> 2:08:11.760\n So I don't know, I doubt we'll ever be able to travel\n\n2:08:12.880 --> 2:08:15.480\n to other things, but we could tell the stars,\n\n2:08:15.480 --> 2:08:17.800\n but we could send intelligent machines to do that.\n\n2:08:17.800 --> 2:08:22.800\n So you have an optimistic, a hopeful view of our knowledge\n\n2:08:23.160 --> 2:08:26.040\n of the echoes of human civilization\n\n2:08:26.040 --> 2:08:29.240\n living through the intelligent systems we create?\n\n2:08:29.240 --> 2:08:30.080\n Oh, totally.\n\n2:08:30.080 --> 2:08:31.400\n Well, I think the intelligent systems we create\n\n2:08:31.400 --> 2:08:35.000\n are in some sense the vessel for bringing them beyond Earth\n\n2:08:36.200 --> 2:08:38.800\n or making them last beyond humans themselves.\n\n2:08:39.840 --> 2:08:41.280\n How do you feel about that?\n\n2:08:41.280 --> 2:08:43.640\n That they won't be human, quote unquote?\n\n2:08:43.640 --> 2:08:45.080\n Who cares?\n\n2:08:45.080 --> 2:08:46.120\n Human, what is human?\n\n2:08:46.120 --> 2:08:48.640\n Our species are changing all the time.\n\n2:08:48.640 --> 2:08:51.680\n Human today is not the same as human just 50 years ago.\n\n2:08:52.600 --> 2:08:53.440\n What is human?\n\n2:08:53.440 --> 2:08:54.520\n Do we care about our genetics?\n\n2:08:54.520 --> 2:08:56.160\n Why is that important?\n\n2:08:56.160 --> 2:08:58.320\n As I point out, our genetics are no more interesting\n\n2:08:58.320 --> 2:08:59.440\n than a bacterium's genetics.\n\n2:08:59.440 --> 2:09:01.720\n It's no more interesting than a monkey's genetics.\n\n2:09:01.720 --> 2:09:04.560\n What we have, what's unique and what's valuable\n\n2:09:04.560 --> 2:09:07.400\n is our knowledge, what we've learned about the world.\n\n2:09:07.400 --> 2:09:09.640\n And that is the rare thing.\n\n2:09:09.640 --> 2:09:11.480\n That's the thing we wanna preserve.\n\n2:09:11.480 --> 2:09:13.640\n It's, who cares about our genes?\n\n2:09:13.640 --> 2:09:14.480\n That's not.\n\n2:09:14.480 --> 2:09:16.280\n It's the knowledge.\n\n2:09:16.280 --> 2:09:17.120\n It's the knowledge.\n\n2:09:17.120 --> 2:09:19.080\n That's a really good place to end.\n\n2:09:19.080 --> 2:09:20.120\n Thank you so much for talking to me.\n\n2:09:20.120 --> 2:09:45.120\n No, it was fun.\n\n"
}