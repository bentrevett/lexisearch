{
  "title": "Yann LeCun: Dark Matter of Intelligence and Self-Supervised Learning | Lex Fridman Podcast #258",
  "id": "SGzMElJ11Cc",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:02.720\n The following is a conversation with Yann LeCun,\n\n00:02.720 --> 00:04.560\n his second time on the podcast.\n\n00:04.560 --> 00:09.180\n He is the chief AI scientist at Meta, formerly Facebook,\n\n00:09.180 --> 00:13.080\n professor at NYU, touring award winner,\n\n00:13.080 --> 00:15.640\n one of the seminal figures in the history\n\n00:15.640 --> 00:18.480\n of machine learning and artificial intelligence,\n\n00:18.480 --> 00:21.960\n and someone who is brilliant and opinionated\n\n00:21.960 --> 00:23.440\n in the best kind of way.\n\n00:23.440 --> 00:26.000\n And so it was always fun to talk to him.\n\n00:26.000 --> 00:28.000\n This is the Lex Friedman podcast.\n\n00:28.000 --> 00:29.960\n To support it, please check out our sponsors\n\n00:29.960 --> 00:31.220\n in the description.\n\n00:31.220 --> 00:35.040\n And now, here's my conversation with Yann LeCun.\n\n00:36.160 --> 00:37.600\n You cowrote the article,\n\n00:37.600 --> 00:40.900\n Self Supervised Learning, the Dark Matter of Intelligence.\n\n00:40.900 --> 00:43.720\n Great title, by the way, with Ishan Mizra.\n\n00:43.720 --> 00:46.640\n So let me ask, what is self supervised learning,\n\n00:46.640 --> 00:49.920\n and why is it the dark matter of intelligence?\n\n00:49.920 --> 00:51.720\n I'll start by the dark matter part.\n\n00:53.120 --> 00:55.680\n There is obviously a kind of learning\n\n00:55.680 --> 00:59.880\n that humans and animals are doing\n\n00:59.880 --> 01:02.800\n that we currently are not reproducing properly\n\n01:02.800 --> 01:04.660\n with machines or with AI, right?\n\n01:04.660 --> 01:08.480\n So the most popular approaches to machine learning today are,\n\n01:08.480 --> 01:09.660\n or paradigms, I should say,\n\n01:09.660 --> 01:12.720\n are supervised learning and reinforcement learning.\n\n01:12.720 --> 01:15.120\n And they are extremely inefficient.\n\n01:15.120 --> 01:17.620\n Supervised learning requires many samples\n\n01:17.620 --> 01:19.760\n for learning anything.\n\n01:19.760 --> 01:22.760\n And reinforcement learning requires a ridiculously large\n\n01:22.760 --> 01:27.300\n number of trial and errors for a system to learn anything.\n\n01:29.320 --> 01:31.560\n And that's why we don't have self driving cars.\n\n01:32.960 --> 01:34.760\n That was a big leap from one to the other.\n\n01:34.760 --> 01:38.760\n Okay, so that, to solve difficult problems,\n\n01:38.760 --> 01:42.360\n you have to have a lot of human annotation\n\n01:42.360 --> 01:44.080\n for supervised learning to work.\n\n01:44.080 --> 01:45.520\n And to solve those difficult problems\n\n01:45.520 --> 01:46.680\n with reinforcement learning,\n\n01:46.680 --> 01:50.240\n you have to have some way to maybe simulate that problem\n\n01:50.240 --> 01:52.720\n such that you can do that large scale kind of learning\n\n01:52.720 --> 01:54.420\n that reinforcement learning requires.\n\n01:54.420 --> 01:58.320\n Right, so how is it that most teenagers can learn\n\n01:58.320 --> 02:02.280\n to drive a car in about 20 hours of practice,\n\n02:02.280 --> 02:07.280\n whereas even with millions of hours of simulated practice,\n\n02:07.400 --> 02:09.220\n a self driving car can't actually learn\n\n02:09.220 --> 02:10.700\n to drive itself properly.\n\n02:12.120 --> 02:13.920\n And so obviously we're missing something, right?\n\n02:13.920 --> 02:15.600\n And it's quite obvious for a lot of people\n\n02:15.600 --> 02:19.760\n that the immediate response you get from many people is,\n\n02:19.760 --> 02:22.840\n well, humans use their background knowledge\n\n02:22.840 --> 02:25.820\n to learn faster, and they're right.\n\n02:25.820 --> 02:28.280\n Now, how was that background knowledge acquired?\n\n02:28.280 --> 02:30.080\n And that's the big question.\n\n02:30.080 --> 02:34.040\n So now you have to ask, how do babies\n\n02:34.040 --> 02:37.120\n in the first few months of life learn how the world works?\n\n02:37.120 --> 02:38.240\n Mostly by observation,\n\n02:38.240 --> 02:40.960\n because they can hardly act in the world.\n\n02:40.960 --> 02:42.560\n And they learn an enormous amount\n\n02:42.560 --> 02:43.840\n of background knowledge about the world\n\n02:43.840 --> 02:47.960\n that may be the basis of what we call common sense.\n\n02:47.960 --> 02:51.280\n This type of learning is not learning a task.\n\n02:51.280 --> 02:53.680\n It's not being reinforced for anything.\n\n02:53.680 --> 02:57.300\n It's just observing the world and figuring out how it works.\n\n02:58.400 --> 03:01.240\n Building world models, learning world models.\n\n03:01.240 --> 03:02.120\n How do we do this?\n\n03:02.120 --> 03:04.560\n And how do we reproduce this in machines?\n\n03:04.560 --> 03:09.520\n So self supervised learning is one instance\n\n03:09.520 --> 03:13.120\n or one attempt at trying to reproduce this kind of learning.\n\n03:13.120 --> 03:16.400\n Okay, so you're looking at just observation,\n\n03:16.400 --> 03:18.720\n so not even the interacting part of a child.\n\n03:18.720 --> 03:21.600\n It's just sitting there watching mom and dad walk around,\n\n03:21.600 --> 03:23.480\n pick up stuff, all of that.\n\n03:23.480 --> 03:25.520\n That's what we mean about background knowledge.\n\n03:25.520 --> 03:27.520\n Perhaps not even watching mom and dad,\n\n03:27.520 --> 03:30.000\n just watching the world go by.\n\n03:30.000 --> 03:31.920\n Just having eyes open or having eyes closed\n\n03:31.920 --> 03:34.480\n or the very act of opening and closing eyes\n\n03:34.480 --> 03:36.280\n that the world appears and disappears,\n\n03:36.280 --> 03:37.860\n all that basic information.\n\n03:39.120 --> 03:43.160\n And you're saying in order to learn to drive,\n\n03:43.160 --> 03:45.840\n like the reason humans are able to learn to drive quickly,\n\n03:45.840 --> 03:47.360\n some faster than others,\n\n03:47.360 --> 03:48.680\n is because of the background knowledge.\n\n03:48.680 --> 03:51.760\n They're able to watch cars operate in the world\n\n03:51.760 --> 03:53.640\n in the many years leading up to it,\n\n03:53.640 --> 03:55.760\n the physics of basic objects, all that kind of stuff.\n\n03:55.760 --> 03:56.600\n That's right.\n\n03:56.600 --> 03:57.440\n I mean, the basic physics of objects,\n\n03:57.440 --> 04:00.880\n you don't even need to know how a car works, right?\n\n04:00.880 --> 04:02.500\n Because that you can learn fairly quickly.\n\n04:02.500 --> 04:03.840\n I mean, the example I use very often\n\n04:03.840 --> 04:05.800\n is you're driving next to a cliff.\n\n04:06.680 --> 04:10.560\n And you know in advance because of your understanding\n\n04:10.560 --> 04:13.200\n of intuitive physics that if you turn the wheel\n\n04:13.200 --> 04:15.080\n to the right, the car will veer to the right,\n\n04:15.080 --> 04:17.560\n will run off the cliff, fall off the cliff,\n\n04:17.560 --> 04:20.400\n and nothing good will come out of this, right?\n\n04:20.400 --> 04:23.760\n But if you are a sort of tabularized\n\n04:23.760 --> 04:25.100\n reinforcement learning system\n\n04:25.100 --> 04:27.040\n that doesn't have a model of the world,\n\n04:28.160 --> 04:30.500\n you have to repeat falling off this cliff\n\n04:30.500 --> 04:32.800\n thousands of times before you figure out it's a bad idea.\n\n04:32.800 --> 04:34.560\n And then a few more thousand times\n\n04:34.560 --> 04:36.960\n before you figure out how to not do it.\n\n04:36.960 --> 04:38.480\n And then a few more million times\n\n04:38.480 --> 04:39.800\n before you figure out how to not do it\n\n04:39.800 --> 04:42.520\n in every situation you ever encounter.\n\n04:42.520 --> 04:45.800\n So self supervised learning still has to have\n\n04:45.800 --> 04:50.560\n some source of truth being told to it by somebody.\n\n04:50.560 --> 04:54.560\n So you have to figure out a way without human assistance\n\n04:54.560 --> 04:56.600\n or without significant amount of human assistance\n\n04:56.600 --> 04:59.100\n to get that truth from the world.\n\n04:59.100 --> 05:03.980\n So the mystery there is how much signal is there?\n\n05:03.980 --> 05:06.280\n How much truth is there that the world gives you?\n\n05:06.280 --> 05:08.160\n Whether it's the human world,\n\n05:08.160 --> 05:10.020\n like you watch YouTube or something like that,\n\n05:10.020 --> 05:12.960\n or it's the more natural world.\n\n05:12.960 --> 05:14.920\n So how much signal is there?\n\n05:14.920 --> 05:16.280\n So here's the trick.\n\n05:16.280 --> 05:20.120\n There is way more signal in sort of a self supervised\n\n05:20.120 --> 05:22.500\n setting than there is in either a supervised\n\n05:22.500 --> 05:24.520\n or reinforcement setting.\n\n05:24.520 --> 05:28.380\n And this is going to my analogy of the cake.\n\n05:30.280 --> 05:32.320\n The cake as someone has called it,\n\n05:32.320 --> 05:36.000\n where when you try to figure out how much information\n\n05:36.000 --> 05:37.840\n you ask the machine to predict\n\n05:37.840 --> 05:41.040\n and how much feedback you give the machine at every trial,\n\n05:41.040 --> 05:41.880\n in reinforcement learning,\n\n05:41.880 --> 05:43.340\n you give the machine a single scalar.\n\n05:43.340 --> 05:45.400\n You tell the machine you did good, you did bad.\n\n05:45.400 --> 05:49.640\n And you only tell this to the machine once in a while.\n\n05:49.640 --> 05:51.440\n When I say you, it could be the universe\n\n05:51.440 --> 05:52.880\n telling the machine, right?\n\n05:54.120 --> 05:55.840\n But it's just one scalar.\n\n05:55.840 --> 05:57.160\n And so as a consequence,\n\n05:57.160 --> 05:59.600\n you cannot possibly learn something very complicated\n\n05:59.600 --> 06:01.120\n without many, many, many trials\n\n06:01.120 --> 06:04.760\n where you get many, many feedbacks of this type.\n\n06:04.760 --> 06:08.880\n Supervised learning, you give a few bits to the machine\n\n06:08.880 --> 06:10.220\n at every sample.\n\n06:11.280 --> 06:15.720\n Let's say you're training a system on recognizing images\n\n06:15.720 --> 06:17.680\n on ImageNet with 1000 categories,\n\n06:17.680 --> 06:20.920\n that's a little less than 10 bits of information per sample.\n\n06:22.180 --> 06:24.640\n But self supervised learning, here is the setting.\n\n06:24.640 --> 06:26.360\n Ideally, we don't know how to do this yet,\n\n06:26.360 --> 06:31.360\n but ideally you would show a machine a segment of video\n\n06:31.680 --> 06:34.200\n and then stop the video and ask the machine to predict\n\n06:34.200 --> 06:35.600\n what's going to happen next.\n\n06:37.640 --> 06:38.720\n And so we let the machine predict\n\n06:38.720 --> 06:41.400\n and then you let time go by\n\n06:41.400 --> 06:44.340\n and show the machine what actually happened\n\n06:44.340 --> 06:47.920\n and hope the machine will learn to do a better job\n\n06:47.920 --> 06:49.400\n at predicting next time around.\n\n06:49.400 --> 06:51.580\n There's a huge amount of information you give the machine\n\n06:51.580 --> 06:53.560\n because it's an entire video clip\n\n06:54.680 --> 06:59.280\n of the future after the video clip you fed it\n\n06:59.280 --> 07:00.280\n in the first place.\n\n07:00.280 --> 07:05.120\n So both for language and for vision, there's a subtle,\n\n07:05.120 --> 07:06.920\n seemingly trivial construction,\n\n07:06.920 --> 07:08.520\n but maybe that's representative\n\n07:08.520 --> 07:10.620\n of what is required to create intelligence,\n\n07:10.620 --> 07:12.880\n which is filling the gap.\n\n07:13.720 --> 07:17.760\n So it sounds dumb, but can you,\n\n07:19.760 --> 07:22.080\n it is possible you could solve all of intelligence\n\n07:22.080 --> 07:25.280\n in this way, just for both language,\n\n07:25.280 --> 07:28.800\n just give a sentence and continue it\n\n07:28.800 --> 07:31.160\n or give a sentence and there's a gap in it,\n\n07:32.080 --> 07:35.720\n some words blanked out and you fill in what words go there.\n\n07:35.720 --> 07:39.200\n For vision, you give a sequence of images\n\n07:39.200 --> 07:40.960\n and predict what's going to happen next,\n\n07:40.960 --> 07:43.840\n or you fill in what happened in between.\n\n07:43.840 --> 07:46.960\n Do you think it's possible that formulation alone\n\n07:48.600 --> 07:50.980\n as a signal for self supervised learning\n\n07:50.980 --> 07:53.640\n can solve intelligence for vision and language?\n\n07:53.640 --> 07:56.320\n I think that's the best shot at the moment.\n\n07:56.320 --> 07:59.120\n So whether this will take us all the way\n\n07:59.120 --> 08:01.760\n to human level intelligence or something,\n\n08:01.760 --> 08:04.840\n or just cat level intelligence is not clear,\n\n08:04.840 --> 08:07.340\n but among all the possible approaches\n\n08:07.340 --> 08:09.520\n that people have proposed, I think it's our best shot.\n\n08:09.520 --> 08:14.520\n So I think this idea of an intelligent system\n\n08:14.640 --> 08:18.880\n filling in the blanks, either predicting the future,\n\n08:18.880 --> 08:22.180\n inferring the past, filling in missing information,\n\n08:23.760 --> 08:25.200\n I'm currently filling the blank\n\n08:25.200 --> 08:26.680\n of what is behind your head\n\n08:26.680 --> 08:30.600\n and what your head looks like from the back,\n\n08:30.600 --> 08:33.760\n because I have basic knowledge about how humans are made.\n\n08:33.760 --> 08:36.360\n And I don't know what you're going to say,\n\n08:36.360 --> 08:37.280\n at which point you're going to speak,\n\n08:37.280 --> 08:38.960\n whether you're going to move your head this way or that way,\n\n08:38.960 --> 08:40.280\n which way you're going to look,\n\n08:40.280 --> 08:42.080\n but I know you're not going to just dematerialize\n\n08:42.080 --> 08:44.920\n and reappear three meters down the hall,\n\n08:46.280 --> 08:49.520\n because I know what's possible and what's impossible\n\n08:49.520 --> 08:51.160\n according to intuitive physics.\n\n08:51.160 --> 08:53.280\n You have a model of what's possible and what's impossible\n\n08:53.280 --> 08:55.080\n and then you'd be very surprised if it happens\n\n08:55.080 --> 08:57.840\n and then you'll have to reconstruct your model.\n\n08:57.840 --> 08:59.600\n Right, so that's the model of the world.\n\n08:59.600 --> 09:02.240\n It's what tells you, what fills in the blanks.\n\n09:02.240 --> 09:04.960\n So given your partial information about the state\n\n09:04.960 --> 09:07.160\n of the world, given by your perception,\n\n09:08.080 --> 09:11.360\n your model of the world fills in the missing information\n\n09:11.360 --> 09:13.760\n and that includes predicting the future,\n\n09:13.760 --> 09:16.880\n re predicting the past, filling in things\n\n09:16.880 --> 09:18.400\n you don't immediately perceive.\n\n09:18.400 --> 09:22.280\n And that doesn't have to be purely generic vision\n\n09:22.280 --> 09:24.340\n or visual information or generic language.\n\n09:24.340 --> 09:28.920\n You can go to specifics like predicting\n\n09:28.920 --> 09:31.120\n what control decision you make when you're driving\n\n09:31.120 --> 09:35.620\n in a lane, you have a sequence of images from a vehicle\n\n09:35.620 --> 09:39.640\n and then you have information if you record it on video\n\n09:39.640 --> 09:43.680\n where the car ended up going so you can go back in time\n\n09:43.680 --> 09:45.520\n and predict where the car went\n\n09:45.520 --> 09:46.680\n based on the visual information.\n\n09:46.680 --> 09:49.440\n That's very specific, domain specific.\n\n09:49.440 --> 09:51.480\n Right, but the question is whether we can come up\n\n09:51.480 --> 09:56.480\n with sort of a generic method for training machines\n\n09:57.000 --> 09:59.840\n to do this kind of prediction or filling in the blanks.\n\n09:59.840 --> 10:04.720\n So right now, this type of approach has been unbelievably\n\n10:04.720 --> 10:08.200\n successful in the context of natural language processing.\n\n10:08.200 --> 10:10.440\n Every modern natural language processing is pre trained\n\n10:10.440 --> 10:13.720\n in self supervised manner to fill in the blanks.\n\n10:13.720 --> 10:16.400\n You show it a sequence of words, you remove 10% of them\n\n10:16.400 --> 10:17.940\n and then you train some gigantic neural net\n\n10:17.940 --> 10:20.320\n to predict the words that are missing.\n\n10:20.320 --> 10:22.760\n And once you've pre trained that network,\n\n10:22.760 --> 10:26.600\n you can use the internal representation learned by it\n\n10:26.600 --> 10:30.480\n as input to something that you train supervised\n\n10:30.480 --> 10:32.240\n or whatever.\n\n10:32.240 --> 10:33.400\n That's been incredibly successful.\n\n10:33.400 --> 10:37.600\n Not so successful in images, although it's making progress\n\n10:37.600 --> 10:42.600\n and it's based on sort of manual data augmentation.\n\n10:42.600 --> 10:43.560\n We can go into this later,\n\n10:43.560 --> 10:47.200\n but what has not been successful yet is training from video.\n\n10:47.200 --> 10:49.440\n So getting a machine to learn to represent\n\n10:49.440 --> 10:52.800\n the visual world, for example, by just watching video.\n\n10:52.800 --> 10:54.800\n Nobody has really succeeded in doing this.\n\n10:54.800 --> 10:57.520\n Okay, well, let's kind of give a high level overview.\n\n10:57.520 --> 11:02.360\n What's the difference in kind and in difficulty\n\n11:02.360 --> 11:03.960\n between vision and language?\n\n11:03.960 --> 11:08.280\n So you said people haven't been able to really\n\n11:08.280 --> 11:10.480\n kind of crack the problem of vision open\n\n11:10.480 --> 11:11.960\n in terms of self supervised learning,\n\n11:11.960 --> 11:13.800\n but that may not be necessarily\n\n11:13.800 --> 11:15.840\n because it's fundamentally more difficult.\n\n11:15.840 --> 11:18.720\n Maybe like when we're talking about achieving,\n\n11:18.720 --> 11:22.320\n like passing the Turing test in the full spirit\n\n11:22.320 --> 11:24.920\n of the Turing test in language might be harder than vision.\n\n11:24.920 --> 11:26.400\n That's not obvious.\n\n11:26.400 --> 11:29.440\n So in your view, which is harder\n\n11:29.440 --> 11:31.960\n or perhaps are they just the same problem?\n\n11:31.960 --> 11:34.840\n When the farther we get to solving each,\n\n11:34.840 --> 11:36.720\n the more we realize it's all the same thing.\n\n11:36.720 --> 11:37.680\n It's all the same cake.\n\n11:37.680 --> 11:40.200\n I think what I'm looking for are methods\n\n11:40.200 --> 11:43.600\n that make them look essentially like the same cake,\n\n11:43.600 --> 11:44.800\n but currently they're not.\n\n11:44.800 --> 11:48.480\n And the main issue with learning world models\n\n11:48.480 --> 11:53.120\n or learning predictive models is that the prediction\n\n11:53.120 --> 11:55.880\n is never a single thing\n\n11:55.880 --> 11:59.240\n because the world is not entirely predictable.\n\n11:59.240 --> 12:00.680\n It may be deterministic or stochastic.\n\n12:00.680 --> 12:02.960\n We can get into the philosophical discussion about it,\n\n12:02.960 --> 12:05.280\n but even if it's deterministic,\n\n12:05.280 --> 12:07.440\n it's not entirely predictable.\n\n12:07.440 --> 12:11.760\n And so if I play a short video clip\n\n12:11.760 --> 12:14.160\n and then I ask you to predict what's going to happen next,\n\n12:14.160 --> 12:16.360\n there's many, many plausible continuations\n\n12:16.360 --> 12:20.520\n for that video clip and the number of continuation grows\n\n12:20.520 --> 12:23.920\n with the interval of time that you're asking the system\n\n12:23.920 --> 12:26.480\n to make a prediction for.\n\n12:26.480 --> 12:29.880\n And so one big question with self supervised learning\n\n12:29.880 --> 12:32.320\n is how you represent this uncertainty,\n\n12:32.320 --> 12:35.200\n how you represent multiple discrete outcomes,\n\n12:35.200 --> 12:37.120\n how you represent a sort of continuum\n\n12:37.120 --> 12:40.400\n of possible outcomes, et cetera.\n\n12:40.400 --> 12:45.200\n And if you are sort of a classical machine learning person,\n\n12:45.200 --> 12:47.880\n you say, oh, you just represent a distribution, right?\n\n12:49.120 --> 12:52.560\n And that we know how to do when we're predicting words,\n\n12:52.560 --> 12:53.720\n missing words in the text,\n\n12:53.720 --> 12:56.840\n because you can have a neural net give a score\n\n12:56.840 --> 12:58.640\n for every word in the dictionary.\n\n12:58.640 --> 13:02.480\n It's a big list of numbers, maybe 100,000 or so.\n\n13:02.480 --> 13:05.280\n And you can turn them into a probability distribution\n\n13:05.280 --> 13:07.640\n that tells you when I say a sentence,\n\n13:09.880 --> 13:13.000\n the cat is chasing the blank in the kitchen.\n\n13:13.000 --> 13:15.840\n There are only a few words that make sense there.\n\n13:15.840 --> 13:18.360\n It could be a mouse or it could be a lizard spot\n\n13:18.360 --> 13:19.920\n or something like that, right?\n\n13:21.560 --> 13:25.840\n And if I say the blank is chasing the blank in the Savannah,\n\n13:25.840 --> 13:27.840\n you also have a bunch of plausible options\n\n13:27.840 --> 13:29.200\n for those two words, right?\n\n13:30.960 --> 13:33.640\n Because you have kind of a underlying reality\n\n13:33.640 --> 13:36.240\n that you can refer to to sort of fill in those blanks.\n\n13:38.080 --> 13:42.040\n So you cannot say for sure in the Savannah,\n\n13:42.040 --> 13:44.480\n if it's a lion or a cheetah or whatever,\n\n13:44.480 --> 13:49.480\n you cannot know if it's a zebra or a do or whatever,\n\n13:49.560 --> 13:50.960\n wildebeest, the same thing.\n\n13:55.360 --> 13:56.840\n But you can represent the uncertainty\n\n13:56.840 --> 13:58.520\n by just a long list of numbers.\n\n13:58.520 --> 14:01.800\n Now, if I do the same thing with video,\n\n14:01.800 --> 14:04.360\n when I ask you to predict a video clip,\n\n14:04.360 --> 14:07.400\n it's not a discrete set of potential frames.\n\n14:07.400 --> 14:10.000\n You have to have somewhere representing\n\n14:10.000 --> 14:13.520\n a sort of infinite number of plausible continuations\n\n14:13.520 --> 14:17.480\n of multiple frames in a high dimensional continuous space.\n\n14:17.480 --> 14:20.520\n And we just have no idea how to do this properly.\n\n14:20.520 --> 14:22.880\n Fine night, high dimensional.\n\n14:22.880 --> 14:23.720\n So like you,\n\n14:23.720 --> 14:25.320\n It's finite high dimensional, yes.\n\n14:25.320 --> 14:26.240\n Just like the words,\n\n14:26.240 --> 14:31.240\n they try to get it down to a small finite set\n\n14:32.200 --> 14:34.240\n of like under a million, something like that.\n\n14:34.240 --> 14:35.080\n Something like that.\n\n14:35.080 --> 14:38.320\n I mean, it's kind of ridiculous that we're doing\n\n14:38.320 --> 14:40.840\n a distribution over every single possible word\n\n14:40.840 --> 14:42.880\n for language and it works.\n\n14:42.880 --> 14:45.280\n It feels like that's a really dumb way to do it.\n\n14:46.480 --> 14:49.720\n Like there seems to be like there should be\n\n14:49.720 --> 14:52.920\n some more compressed representation\n\n14:52.920 --> 14:55.040\n of the distribution of the words.\n\n14:55.040 --> 14:56.120\n You're right about that.\n\n14:56.120 --> 14:58.880\n And so do you have any interesting ideas\n\n14:58.880 --> 15:01.880\n about how to represent all of reality in a compressed way\n\n15:01.880 --> 15:03.800\n such that you can form a distribution over it?\n\n15:03.800 --> 15:06.200\n That's one of the big questions, how do you do that?\n\n15:06.200 --> 15:08.440\n Right, I mean, what's kind of another thing\n\n15:08.440 --> 15:13.080\n that really is stupid about, I shouldn't say stupid,\n\n15:13.080 --> 15:15.560\n but like simplistic about current approaches\n\n15:15.560 --> 15:19.360\n to self supervised learning in NLP in text\n\n15:19.360 --> 15:21.920\n is that not only do you represent\n\n15:21.920 --> 15:23.840\n a giant distribution over words,\n\n15:23.840 --> 15:25.680\n but for multiple words that are missing,\n\n15:25.680 --> 15:27.680\n those distributions are essentially independent\n\n15:27.680 --> 15:28.520\n of each other.\n\n15:30.200 --> 15:33.040\n And you don't pay too much of a price for this.\n\n15:33.040 --> 15:37.840\n So you can't, so the system in the sentence\n\n15:37.840 --> 15:41.280\n that I gave earlier, if it gives a certain probability\n\n15:41.280 --> 15:44.800\n for a lion and cheetah, and then a certain probability\n\n15:44.800 --> 15:49.800\n for gazelle, wildebeest and zebra,\n\n15:51.960 --> 15:54.800\n those two probabilities are independent of each other.\n\n15:55.960 --> 15:58.040\n And it's not the case that those things are independent.\n\n15:58.040 --> 16:01.480\n Lions actually attack like bigger animals than cheetahs.\n\n16:01.480 --> 16:05.960\n So there's a huge independent hypothesis in this process,\n\n16:05.960 --> 16:07.800\n which is not actually true.\n\n16:07.800 --> 16:09.880\n The reason for this is that we don't know\n\n16:09.880 --> 16:13.000\n how to represent properly distributions\n\n16:13.000 --> 16:16.240\n over combinatorial sequences of symbols,\n\n16:16.240 --> 16:19.000\n essentially because the number grows exponentially\n\n16:19.000 --> 16:21.320\n with the length of the symbols.\n\n16:21.320 --> 16:22.760\n And so we have to use tricks for this,\n\n16:22.760 --> 16:26.400\n but those techniques can get around,\n\n16:26.400 --> 16:27.800\n like don't even deal with it.\n\n16:27.800 --> 16:31.760\n So the big question is would there be some sort\n\n16:31.760 --> 16:35.640\n of abstract latent representation of text\n\n16:35.640 --> 16:40.640\n that would say that when I switch lion for gazelle,\n\n16:40.680 --> 16:45.480\n lion for cheetah, I also have to switch zebra for gazelle?\n\n16:45.480 --> 16:48.720\n Yeah, so this independence assumption,\n\n16:48.720 --> 16:51.160\n let me throw some criticism at you that I often hear\n\n16:51.160 --> 16:52.920\n and see how you respond.\n\n16:52.920 --> 16:56.000\n So this kind of filling in the blanks is just statistics.\n\n16:56.000 --> 16:57.880\n You're not learning anything\n\n16:58.880 --> 17:01.600\n like the deep underlying concepts.\n\n17:01.600 --> 17:05.640\n You're just mimicking stuff from the past.\n\n17:05.640 --> 17:08.560\n You're not learning anything new such that you can use it\n\n17:08.560 --> 17:10.800\n to generalize about the world.\n\n17:11.960 --> 17:14.120\n Or okay, let me just say the crude version,\n\n17:14.120 --> 17:16.200\n which is just statistics.\n\n17:16.200 --> 17:18.320\n It's not intelligence.\n\n17:18.320 --> 17:19.640\n What do you have to say to that?\n\n17:19.640 --> 17:20.880\n What do you usually say to that\n\n17:20.880 --> 17:22.640\n if you kind of hear this kind of thing?\n\n17:22.640 --> 17:23.960\n I don't get into those discussions\n\n17:23.960 --> 17:26.760\n because they are kind of pointless.\n\n17:26.760 --> 17:28.760\n So first of all, it's quite possible\n\n17:28.760 --> 17:30.480\n that intelligence is just statistics.\n\n17:30.480 --> 17:32.760\n It's just statistics of a particular kind.\n\n17:32.760 --> 17:35.480\n Yes, this is the philosophical question.\n\n17:35.480 --> 17:38.400\n It's kind of is it possible\n\n17:38.400 --> 17:40.280\n that intelligence is just statistics?\n\n17:40.280 --> 17:43.520\n Yeah, but what kind of statistics?\n\n17:43.520 --> 17:46.200\n So if you are asking the question,\n\n17:47.160 --> 17:50.680\n are the models of the world that we learn,\n\n17:50.680 --> 17:52.320\n do they have some notion of causality?\n\n17:52.320 --> 17:53.400\n Yes.\n\n17:53.400 --> 17:56.240\n So if the criticism comes from people who say,\n\n17:57.200 --> 17:59.440\n current machine learning system don't care about causality,\n\n17:59.440 --> 18:03.120\n which by the way is wrong, I agree with them.\n\n18:04.600 --> 18:06.560\n Your model of the world should have your actions\n\n18:06.560 --> 18:09.080\n as one of the inputs.\n\n18:09.080 --> 18:11.400\n And that will drive you to learn causal models of the world\n\n18:11.400 --> 18:15.080\n where you know what intervention in the world\n\n18:15.080 --> 18:16.720\n will cause what result.\n\n18:16.720 --> 18:19.400\n Or you can do this by observation of other agents\n\n18:19.400 --> 18:22.520\n acting in the world and observing the effect.\n\n18:22.520 --> 18:24.240\n Other humans, for example.\n\n18:24.240 --> 18:28.440\n So I think at some level of description,\n\n18:28.440 --> 18:30.240\n intelligence is just statistics.\n\n18:31.680 --> 18:35.200\n But that doesn't mean you don't have models\n\n18:35.200 --> 18:40.080\n that have deep mechanistic explanation for what goes on.\n\n18:40.080 --> 18:41.760\n The question is how do you learn them?\n\n18:41.760 --> 18:44.440\n That's the question I'm interested in.\n\n18:44.440 --> 18:49.360\n Because a lot of people who actually voice their criticism\n\n18:49.360 --> 18:51.040\n say that those mechanistic model\n\n18:51.040 --> 18:52.640\n have to come from someplace else.\n\n18:52.640 --> 18:54.040\n They have to come from human designers,\n\n18:54.040 --> 18:56.200\n they have to come from I don't know what.\n\n18:56.200 --> 18:57.880\n And obviously we learn them.\n\n18:59.280 --> 19:01.800\n Or if we don't learn them as an individual,\n\n19:01.800 --> 19:04.920\n nature learn them for us using evolution.\n\n19:04.920 --> 19:07.160\n So regardless of what you think,\n\n19:07.160 --> 19:09.400\n those processes have been learned somehow.\n\n19:10.240 --> 19:12.920\n So if you look at the human brain,\n\n19:12.920 --> 19:14.640\n just like when we humans introspect\n\n19:14.640 --> 19:16.320\n about how the brain works,\n\n19:16.320 --> 19:20.240\n it seems like when we think about what is intelligence,\n\n19:20.240 --> 19:22.440\n we think about the high level stuff,\n\n19:22.440 --> 19:23.960\n like the models we've constructed,\n\n19:23.960 --> 19:25.560\n concepts like cognitive science,\n\n19:25.560 --> 19:28.720\n like concepts of memory and reasoning module,\n\n19:28.720 --> 19:31.160\n almost like these high level modules.\n\n19:32.360 --> 19:34.400\n Is this serve as a good analogy?\n\n19:35.400 --> 19:40.400\n Like are we ignoring the dark matter,\n\n19:40.720 --> 19:43.560\n the basic low level mechanisms?\n\n19:43.560 --> 19:45.800\n Just like we ignore the way the operating system works,\n\n19:45.800 --> 19:49.640\n we're just using the high level software.\n\n19:49.640 --> 19:52.720\n We're ignoring that at the low level,\n\n19:52.720 --> 19:56.440\n the neural network might be doing something like statistics.\n\n19:56.440 --> 19:59.120\n Like meaning, sorry to use this word\n\n19:59.120 --> 20:00.560\n probably incorrectly and crudely,\n\n20:00.560 --> 20:03.320\n but doing this kind of fill in the gap kind of learning\n\n20:03.320 --> 20:05.720\n and just kind of updating the model constantly\n\n20:05.720 --> 20:09.240\n in order to be able to support the raw sensory information\n\n20:09.240 --> 20:11.360\n to predict it and then adjust to the prediction\n\n20:11.360 --> 20:12.400\n when it's wrong.\n\n20:12.400 --> 20:15.840\n But like when we look at our brain at the high level,\n\n20:15.840 --> 20:18.320\n it feels like we're doing, like we're playing chess,\n\n20:18.320 --> 20:22.240\n like we're like playing with high level concepts\n\n20:22.240 --> 20:23.680\n and we're stitching them together\n\n20:23.680 --> 20:26.000\n and we're putting them into longterm memory.\n\n20:26.000 --> 20:28.280\n But really what's going underneath\n\n20:28.280 --> 20:30.160\n is something we're not able to introspect,\n\n20:30.160 --> 20:34.440\n which is this kind of simple, large neural network\n\n20:34.440 --> 20:36.000\n that's just filling in the gaps.\n\n20:36.000 --> 20:37.120\n Right, well, okay.\n\n20:37.120 --> 20:39.760\n So there's a lot of questions and a lot of answers there.\n\n20:39.760 --> 20:40.600\n Okay, so first of all,\n\n20:40.600 --> 20:42.680\n there's a whole school of thought in neuroscience,\n\n20:42.680 --> 20:45.240\n computational neuroscience in particular,\n\n20:45.240 --> 20:47.760\n that likes the idea of predictive coding,\n\n20:47.760 --> 20:50.080\n which is really related to the idea\n\n20:50.080 --> 20:52.040\n I was talking about in self supervised learning.\n\n20:52.040 --> 20:53.520\n So everything is about prediction.\n\n20:53.520 --> 20:56.320\n The essence of intelligence is the ability to predict\n\n20:56.320 --> 20:58.840\n and everything the brain does is trying to predict,\n\n20:59.920 --> 21:02.120\n predict everything from everything else.\n\n21:02.120 --> 21:04.760\n Okay, and that's really sort of the underlying principle,\n\n21:04.760 --> 21:07.800\n if you want, that self supervised learning\n\n21:07.800 --> 21:10.640\n is trying to kind of reproduce this idea of prediction\n\n21:10.640 --> 21:13.080\n as kind of an essential mechanism\n\n21:13.080 --> 21:16.320\n of task independent learning, if you want.\n\n21:16.320 --> 21:19.320\n The next step is what kind of intelligence\n\n21:19.320 --> 21:21.120\n are you interested in reproducing?\n\n21:21.120 --> 21:24.640\n And of course, we all think about trying to reproduce\n\n21:24.640 --> 21:28.320\n sort of high level cognitive processes in humans,\n\n21:28.320 --> 21:30.400\n but like with machines, we're not even at the level\n\n21:30.400 --> 21:35.400\n of even reproducing the learning processes in a cat brain.\n\n21:37.160 --> 21:39.360\n The most intelligent or intelligent systems\n\n21:39.360 --> 21:41.960\n don't have as much common sense as a house cat.\n\n21:43.200 --> 21:45.160\n So how is it that cats learn?\n\n21:45.160 --> 21:47.920\n And cats don't do a whole lot of reasoning.\n\n21:47.920 --> 21:49.600\n They certainly have causal models.\n\n21:49.600 --> 21:53.600\n They certainly have, because many cats can figure out\n\n21:53.600 --> 21:56.600\n how they can act on the world to get what they want.\n\n21:56.600 --> 22:01.600\n They certainly have a fantastic model of intuitive physics,\n\n22:01.800 --> 22:04.560\n certainly the dynamics of their own bodies,\n\n22:04.560 --> 22:06.880\n but also of praise and things like that.\n\n22:06.880 --> 22:09.880\n So they're pretty smart.\n\n22:09.880 --> 22:12.400\n They only do this with about 800 million neurons.\n\n22:12.400 --> 22:17.400\n We are not anywhere close to reproducing this kind of thing.\n\n22:17.920 --> 22:21.320\n So to some extent, I could say,\n\n22:21.320 --> 22:24.960\n let's not even worry about like the high level cognition\n\n22:26.280 --> 22:27.960\n and kind of longterm planning and reasoning\n\n22:27.960 --> 22:30.120\n that humans can do until we figure out like,\n\n22:30.120 --> 22:32.520\n can we even reproduce what cats are doing?\n\n22:32.520 --> 22:37.000\n Now that said, this ability to learn world models,\n\n22:37.000 --> 22:41.560\n I think is the key to the possibility of learning machines\n\n22:41.560 --> 22:43.160\n that can also reason.\n\n22:43.160 --> 22:45.640\n So whenever I give a talk, I say there are three challenges\n\n22:45.640 --> 22:47.320\n in the three main challenges in machine learning.\n\n22:47.320 --> 22:49.920\n The first one is getting machines to learn\n\n22:49.920 --> 22:51.800\n to represent the world\n\n22:51.800 --> 22:53.960\n and I'm proposing self supervised learning.\n\n22:54.840 --> 22:58.000\n The second is getting machines to reason\n\n22:58.000 --> 22:59.240\n in ways that are compatible\n\n22:59.240 --> 23:01.640\n with essentially gradient based learning\n\n23:01.640 --> 23:04.240\n because this is what deep learning is all about really.\n\n23:05.280 --> 23:06.640\n And the third one is something\n\n23:06.640 --> 23:07.640\n we have no idea how to solve,\n\n23:07.640 --> 23:09.480\n at least I have no idea how to solve\n\n23:09.480 --> 23:14.360\n is can we get machines to learn hierarchical representations\n\n23:14.360 --> 23:15.960\n of action plans?\n\n23:17.920 --> 23:18.760\n We know how to train them\n\n23:18.760 --> 23:21.080\n to learn hierarchical representations of perception\n\n23:22.200 --> 23:23.680\n with convolutional nets and things like that\n\n23:23.680 --> 23:26.040\n and transformers, but what about action plans?\n\n23:26.040 --> 23:28.280\n Can we get them to spontaneously learn\n\n23:28.280 --> 23:30.480\n good hierarchical representations of actions?\n\n23:30.480 --> 23:32.400\n Also gradient based.\n\n23:32.400 --> 23:35.880\n Yeah, all of that needs to be somewhat differentiable\n\n23:35.880 --> 23:38.720\n so that you can apply sort of gradient based learning,\n\n23:38.720 --> 23:40.920\n which is really what deep learning is about.\n\n23:42.080 --> 23:46.760\n So it's background, knowledge, ability to reason\n\n23:46.760 --> 23:50.520\n in a way that's differentiable\n\n23:50.520 --> 23:53.840\n that is somehow connected, deeply integrated\n\n23:53.840 --> 23:55.480\n with that background knowledge\n\n23:55.480 --> 23:57.600\n or builds on top of that background knowledge\n\n23:57.600 --> 23:59.120\n and then given that background knowledge\n\n23:59.120 --> 24:02.360\n be able to make hierarchical plans in the world.\n\n24:02.360 --> 24:05.480\n So if you take classical optimal control,\n\n24:05.480 --> 24:07.000\n there's something in classical optimal control\n\n24:07.000 --> 24:10.520\n called model predictive control.\n\n24:10.520 --> 24:13.840\n And it's been around since the early sixties.\n\n24:13.840 --> 24:16.840\n NASA uses that to compute trajectories of rockets.\n\n24:16.840 --> 24:20.600\n And the basic idea is that you have a predictive model\n\n24:20.600 --> 24:21.840\n of the rocket, let's say,\n\n24:21.840 --> 24:25.440\n or whatever system you intend to control,\n\n24:25.440 --> 24:28.360\n which given the state of the system at time T\n\n24:28.360 --> 24:31.640\n and given an action that you're taking the system.\n\n24:31.640 --> 24:33.520\n So for a rocket to be thrust\n\n24:33.520 --> 24:35.600\n and all the controls you can have,\n\n24:35.600 --> 24:37.280\n it gives you the state of the system\n\n24:37.280 --> 24:38.800\n at time T plus Delta T, right?\n\n24:38.800 --> 24:41.520\n So basically a differential equation, something like that.\n\n24:43.520 --> 24:45.240\n And if you have this model\n\n24:45.240 --> 24:48.720\n and you have this model in the form of some sort of neural net\n\n24:48.720 --> 24:50.960\n or some sort of a set of formula\n\n24:50.960 --> 24:52.920\n that you can back propagate gradient through,\n\n24:52.920 --> 24:55.240\n you can do what's called model predictive control\n\n24:55.240 --> 24:57.680\n or gradient based model predictive control.\n\n24:57.680 --> 25:02.680\n So you can unroll that model in time.\n\n25:02.680 --> 25:07.680\n You feed it a hypothesized sequence of actions.\n\n25:08.080 --> 25:10.760\n And then you have some objective function\n\n25:10.760 --> 25:13.240\n that measures how well at the end of the trajectory,\n\n25:13.240 --> 25:16.240\n the system has succeeded or matched what you wanted to do.\n\n25:17.240 --> 25:18.280\n Is it a robot harm?\n\n25:18.280 --> 25:20.680\n Have you grasped the object you want to grasp?\n\n25:20.680 --> 25:23.360\n If it's a rocket, are you at the right place\n\n25:23.360 --> 25:26.120\n near the space station, things like that.\n\n25:26.120 --> 25:28.040\n And by back propagation through time,\n\n25:28.040 --> 25:30.080\n and again, this was invented in the 1960s,\n\n25:30.080 --> 25:34.040\n by optimal control theorists, you can figure out\n\n25:34.040 --> 25:36.160\n what is the optimal sequence of actions\n\n25:36.160 --> 25:41.160\n that will get my system to the best final state.\n\n25:42.040 --> 25:44.560\n So that's a form of reasoning.\n\n25:44.560 --> 25:45.640\n It's basically planning.\n\n25:45.640 --> 25:48.160\n And a lot of planning systems in robotics\n\n25:48.160 --> 25:49.600\n are actually based on this.\n\n25:49.600 --> 25:53.160\n And you can think of this as a form of reasoning.\n\n25:53.160 --> 25:57.040\n So to take the example of the teenager driving a car,\n\n25:57.040 --> 26:00.120\n you have a pretty good dynamical model of the car.\n\n26:00.120 --> 26:01.280\n It doesn't need to be very accurate.\n\n26:01.280 --> 26:03.840\n But you know, again, that if you turn the wheel\n\n26:03.840 --> 26:05.080\n to the right and there is a cliff,\n\n26:05.080 --> 26:06.520\n you're gonna run off the cliff, right?\n\n26:06.520 --> 26:08.000\n You don't need to have a very accurate model\n\n26:08.000 --> 26:09.080\n to predict that.\n\n26:09.080 --> 26:10.640\n And you can run this in your mind\n\n26:10.640 --> 26:13.080\n and decide not to do it for that reason.\n\n26:13.080 --> 26:14.480\n Because you can predict in advance\n\n26:14.480 --> 26:15.600\n that the result is gonna be bad.\n\n26:15.600 --> 26:17.960\n So you can sort of imagine different scenarios\n\n26:17.960 --> 26:21.560\n and then employ or take the first step\n\n26:21.560 --> 26:23.360\n in the scenario that is most favorable\n\n26:23.360 --> 26:24.960\n and then repeat the process again.\n\n26:24.960 --> 26:27.120\n The scenario that is most favorable\n\n26:27.120 --> 26:28.480\n and then repeat the process of planning.\n\n26:28.480 --> 26:31.280\n That's called receding horizon model predictive control.\n\n26:31.280 --> 26:35.280\n So even all those things have names going back decades.\n\n26:36.480 --> 26:40.680\n And so if you're not a classical optimal control,\n\n26:40.680 --> 26:42.960\n the model of the world is not generally learned.\n\n26:44.360 --> 26:46.240\n Sometimes a few parameters you have to identify.\n\n26:46.240 --> 26:47.800\n That's called systems identification.\n\n26:47.800 --> 26:52.640\n But generally, the model is mostly deterministic\n\n26:52.640 --> 26:53.920\n and mostly built by hand.\n\n26:53.920 --> 26:55.920\n So the question of AI,\n\n26:55.920 --> 26:58.760\n I think the big challenge of AI for the next decade\n\n26:58.760 --> 27:01.120\n is how do we get machines to learn predictive models\n\n27:01.120 --> 27:03.720\n of the world that deal with uncertainty\n\n27:03.720 --> 27:05.840\n and deal with the real world in all this complexity?\n\n27:05.840 --> 27:08.160\n So it's not just the trajectory of a rocket,\n\n27:08.160 --> 27:10.240\n which you can reduce to first principles.\n\n27:10.240 --> 27:13.040\n It's not even just the trajectory of a robot arm,\n\n27:13.040 --> 27:16.320\n which again, you can model by careful mathematics.\n\n27:16.320 --> 27:17.200\n But it's everything else,\n\n27:17.200 --> 27:18.880\n everything we observe in the world.\n\n27:18.880 --> 27:20.120\n People, behavior,\n\n27:20.120 --> 27:25.120\n physical systems that involve collective phenomena,\n\n27:25.800 --> 27:30.800\n like water or trees and branches in a tree or something\n\n27:31.880 --> 27:36.680\n or complex things that humans have no trouble\n\n27:36.680 --> 27:38.520\n developing abstract representations\n\n27:38.520 --> 27:39.840\n and predictive model for,\n\n27:39.840 --> 27:41.600\n but we still don't know how to do with machines.\n\n27:41.600 --> 27:43.880\n Where do you put in these three,\n\n27:43.880 --> 27:46.180\n maybe in the planning stages,\n\n27:46.180 --> 27:50.660\n the game theoretic nature of this world,\n\n27:50.660 --> 27:52.980\n where your actions not only respond\n\n27:52.980 --> 27:55.540\n to the dynamic nature of the world, the environment,\n\n27:55.540 --> 27:57.500\n but also affect it.\n\n27:57.500 --> 27:59.860\n So if there's other humans involved,\n\n27:59.860 --> 28:02.220\n is this point number four,\n\n28:02.220 --> 28:03.420\n or is it somehow integrated\n\n28:03.420 --> 28:05.820\n into the hierarchical representation of action\n\n28:05.820 --> 28:06.660\n in your view?\n\n28:06.660 --> 28:07.500\n I think it's integrated.\n\n28:07.500 --> 28:11.580\n It's just that now your model of the world has to deal with,\n\n28:11.580 --> 28:13.100\n it just makes it more complicated.\n\n28:13.100 --> 28:15.600\n The fact that humans are complicated\n\n28:15.600 --> 28:17.220\n and not easily predictable,\n\n28:17.220 --> 28:19.860\n that makes your model of the world much more complicated,\n\n28:19.860 --> 28:21.340\n that much more complicated.\n\n28:21.340 --> 28:22.380\n Well, there's a chess,\n\n28:22.380 --> 28:25.300\n I mean, I suppose chess is an analogy.\n\n28:25.300 --> 28:27.740\n So multicolored tree search.\n\n28:28.860 --> 28:32.040\n There's a, I go, you go, I go, you go.\n\n28:32.040 --> 28:35.580\n Like Andre Capote recently gave a talk at MIT\n\n28:35.580 --> 28:36.980\n about car doors.\n\n28:37.900 --> 28:39.280\n I think there's some machine learning too,\n\n28:39.280 --> 28:40.780\n but mostly car doors.\n\n28:40.780 --> 28:43.340\n And there's a dynamic nature to the car,\n\n28:43.340 --> 28:44.700\n like the person opening the door,\n\n28:44.700 --> 28:46.900\n checking, I mean, he wasn't talking about that.\n\n28:46.900 --> 28:48.420\n He was talking about the perception problem\n\n28:48.420 --> 28:50.940\n of what the ontology of what defines a car door,\n\n28:50.940 --> 28:52.940\n this big philosophical question.\n\n28:52.940 --> 28:54.060\n But to me, it was interesting\n\n28:54.060 --> 28:57.300\n because it's obvious that the person opening the car doors,\n\n28:57.300 --> 28:59.580\n they're trying to get out, like here in New York,\n\n28:59.580 --> 29:01.400\n trying to get out of the car.\n\n29:01.400 --> 29:03.580\n You slowing down is going to signal something.\n\n29:03.580 --> 29:05.380\n You speeding up is gonna signal something,\n\n29:05.380 --> 29:06.460\n and that's a dance.\n\n29:06.460 --> 29:10.140\n It's a asynchronous chess game.\n\n29:10.140 --> 29:10.980\n I don't know.\n\n29:10.980 --> 29:15.980\n So it feels like it's not just,\n\n29:16.900 --> 29:18.780\n I mean, I guess you can integrate all of them\n\n29:18.780 --> 29:21.300\n to one giant model, like the entirety\n\n29:21.300 --> 29:24.340\n of these little interactions.\n\n29:24.340 --> 29:25.740\n Because it's not as complicated as chess.\n\n29:25.740 --> 29:27.120\n It's just like a little dance.\n\n29:27.120 --> 29:28.800\n We do like a little dance together,\n\n29:28.800 --> 29:29.980\n and then we figure it out.\n\n29:29.980 --> 29:32.500\n Well, in some ways it's way more complicated than chess\n\n29:32.500 --> 29:36.020\n because it's continuous, it's uncertain\n\n29:36.020 --> 29:37.260\n in a continuous manner.\n\n29:38.220 --> 29:39.860\n It doesn't feel more complicated.\n\n29:39.860 --> 29:41.060\n But it doesn't feel more complicated\n\n29:41.060 --> 29:43.660\n because that's what we've evolved to solve.\n\n29:43.660 --> 29:45.480\n This is the kind of problem we've evolved to solve.\n\n29:45.480 --> 29:46.400\n And so we're good at it\n\n29:46.400 --> 29:49.300\n because nature has made us good at it.\n\n29:50.500 --> 29:52.340\n Nature has not made us good at chess.\n\n29:52.340 --> 29:54.180\n We completely suck at chess.\n\n29:55.700 --> 29:57.980\n In fact, that's why we designed it as a game,\n\n29:57.980 --> 29:59.020\n is to be challenging.\n\n30:00.340 --> 30:02.580\n And if there is something that recent progress\n\n30:02.580 --> 30:05.580\n in chess and Go has made us realize\n\n30:05.580 --> 30:07.900\n is that humans are really terrible at those things,\n\n30:07.900 --> 30:09.660\n like really bad.\n\n30:09.660 --> 30:11.540\n There was a story right before AlphaGo\n\n30:11.540 --> 30:15.220\n that the best Go players thought\n\n30:15.220 --> 30:18.520\n there were maybe two or three stones behind an ideal player\n\n30:18.520 --> 30:19.720\n that they would call God.\n\n30:20.700 --> 30:23.700\n In fact, no, there are like nine or 10 stones behind.\n\n30:23.700 --> 30:25.340\n I mean, we're just bad.\n\n30:25.340 --> 30:27.420\n So we're not good at,\n\n30:27.420 --> 30:30.340\n and it's because we have limited working memory.\n\n30:30.340 --> 30:32.980\n We're not very good at doing this tree exploration\n\n30:32.980 --> 30:36.780\n that computers are much better at doing than we are.\n\n30:36.780 --> 30:37.940\n But we are much better\n\n30:37.940 --> 30:40.620\n at learning differentiable models to the world.\n\n30:40.620 --> 30:43.820\n I mean, I said differentiable in a kind of,\n\n30:43.820 --> 30:46.420\n I should say not differentiable in the sense that\n\n30:46.420 --> 30:47.480\n we went back far through it,\n\n30:47.480 --> 30:50.500\n but in the sense that our brain has some mechanism\n\n30:50.500 --> 30:54.060\n for estimating gradients of some kind.\n\n30:54.060 --> 30:56.540\n And that's what makes us efficient.\n\n30:56.540 --> 31:01.540\n So if you have an agent that consists of a model\n\n31:02.180 --> 31:04.380\n of the world, which in the human brain\n\n31:04.380 --> 31:08.340\n is basically the entire front half of your brain,\n\n31:08.340 --> 31:10.220\n an objective function,\n\n31:10.220 --> 31:14.440\n which in humans is a combination of two things.\n\n31:14.440 --> 31:17.660\n There is your sort of intrinsic motivation module,\n\n31:17.660 --> 31:19.140\n which is in the basal ganglia,\n\n31:19.140 --> 31:20.100\n the base of your brain.\n\n31:20.100 --> 31:22.540\n That's the thing that measures pain and hunger\n\n31:22.540 --> 31:23.360\n and things like that,\n\n31:23.360 --> 31:26.860\n like immediate feelings and emotions.\n\n31:28.020 --> 31:30.780\n And then there is the equivalent\n\n31:30.780 --> 31:32.620\n of what people in reinforcement learning call a critic,\n\n31:32.620 --> 31:36.100\n which is a sort of module that predicts ahead\n\n31:36.100 --> 31:41.100\n what the outcome of a situation will be.\n\n31:41.940 --> 31:43.840\n And so it's not a cost function,\n\n31:43.840 --> 31:45.460\n but it's sort of not an objective function,\n\n31:45.460 --> 31:49.020\n but it's sort of a train predictor\n\n31:49.020 --> 31:50.980\n of the ultimate objective function.\n\n31:50.980 --> 31:52.620\n And that also is differentiable.\n\n31:52.620 --> 31:54.660\n And so if all of this is differentiable,\n\n31:54.660 --> 31:59.660\n your cost function, your critic, your world model,\n\n31:59.660 --> 32:03.100\n then you can use gradient based type methods\n\n32:03.100 --> 32:05.820\n to do planning, to do reasoning, to do learning,\n\n32:05.820 --> 32:08.140\n to do all the things that we'd like\n\n32:08.140 --> 32:11.840\n an intelligent agent to do.\n\n32:11.840 --> 32:14.180\n And gradient based learning,\n\n32:14.180 --> 32:15.340\n like what's your intuition?\n\n32:15.340 --> 32:18.420\n That's probably at the core of what can solve intelligence.\n\n32:18.420 --> 32:23.420\n So you don't need like logic based reasoning in your view.\n\n32:25.620 --> 32:27.260\n I don't know how to make logic based reasoning\n\n32:27.260 --> 32:29.780\n compatible with efficient learning.\n\n32:31.020 --> 32:32.300\n Okay, I mean, there is a big question,\n\n32:32.300 --> 32:33.900\n perhaps a philosophical question.\n\n32:33.900 --> 32:35.220\n I mean, it's not that philosophical,\n\n32:35.220 --> 32:40.020\n but that we can ask is that all the learning algorithms\n\n32:40.020 --> 32:43.300\n we know from engineering and computer science\n\n32:43.300 --> 32:45.780\n proceed by optimizing some objective function.\n\n32:48.340 --> 32:49.940\n So one question we may ask is,\n\n32:51.780 --> 32:54.740\n does learning in the brain minimize an objective function?\n\n32:54.740 --> 32:57.340\n I mean, it could be a composite\n\n32:57.340 --> 32:58.500\n of multiple objective functions,\n\n32:58.500 --> 33:00.300\n but it's still an objective function.\n\n33:01.420 --> 33:04.660\n Second, if it does optimize an objective function,\n\n33:04.660 --> 33:09.660\n does it do it by some sort of gradient estimation?\n\n33:09.940 --> 33:10.860\n It doesn't need to be a back prop,\n\n33:10.860 --> 33:14.820\n but some way of estimating the gradient in efficient manner\n\n33:14.820 --> 33:17.020\n whose complexity is on the same order of magnitude\n\n33:17.020 --> 33:20.800\n as actually running the inference.\n\n33:20.800 --> 33:24.060\n Because you can't afford to do things\n\n33:24.060 --> 33:26.540\n like perturbing a weight in your brain\n\n33:26.540 --> 33:28.100\n to figure out what the effect is.\n\n33:28.100 --> 33:30.780\n And then sort of, you can do sort of\n\n33:30.780 --> 33:33.300\n estimating gradient by perturbation.\n\n33:33.300 --> 33:35.460\n To me, it seems very implausible\n\n33:35.460 --> 33:40.460\n that the brain uses some sort of zeroth order black box\n\n33:41.060 --> 33:43.000\n gradient free optimization,\n\n33:43.000 --> 33:45.200\n because it's so much less efficient\n\n33:45.200 --> 33:46.320\n than gradient optimization.\n\n33:46.320 --> 33:49.260\n So it has to have a way of estimating gradient.\n\n33:49.260 --> 33:52.780\n Is it possible that some kind of logic based reasoning\n\n33:52.780 --> 33:55.400\n emerges in pockets as a useful,\n\n33:55.400 --> 33:58.100\n like you said, if the brain is an objective function,\n\n33:58.100 --> 34:01.300\n maybe it's a mechanism for creating objective functions.\n\n34:01.300 --> 34:06.300\n It's a mechanism for creating knowledge bases, for example,\n\n34:06.520 --> 34:08.380\n that can then be queried.\n\n34:08.380 --> 34:10.300\n Like maybe it's like an efficient representation\n\n34:10.300 --> 34:12.700\n of knowledge that's learned in a gradient based way\n\n34:12.700 --> 34:13.780\n or something like that.\n\n34:13.780 --> 34:15.980\n Well, so I think there is a lot of different types\n\n34:15.980 --> 34:17.340\n of intelligence.\n\n34:17.340 --> 34:19.700\n So first of all, I think the type of logical reasoning\n\n34:19.700 --> 34:23.780\n that we think about that we are maybe stemming\n\n34:23.780 --> 34:27.740\n from sort of classical AI of the 1970s and 80s.\n\n34:29.080 --> 34:33.020\n I think humans use that relatively rarely\n\n34:33.020 --> 34:34.740\n and are not particularly good at it.\n\n34:34.740 --> 34:37.560\n But we judge each other based on our ability\n\n34:37.560 --> 34:40.620\n to solve those rare problems.\n\n34:40.620 --> 34:41.660\n It's called an IQ test.\n\n34:41.660 --> 34:42.700\n I don't think so.\n\n34:42.700 --> 34:45.260\n Like I'm not very good at chess.\n\n34:45.260 --> 34:47.420\n Yes, I'm judging you this whole time.\n\n34:47.420 --> 34:49.740\n Because, well, we actually.\n\n34:49.740 --> 34:53.500\n With your heritage, I'm sure you're good at chess.\n\n34:53.500 --> 34:55.060\n No, stereotypes.\n\n34:55.060 --> 34:56.700\n Not all stereotypes are true.\n\n34:58.020 --> 34:59.020\n Well, I'm terrible at chess.\n\n34:59.020 --> 35:04.020\n So, but I think perhaps another type of intelligence\n\n35:04.660 --> 35:08.980\n that I have is this ability of sort of building models\n\n35:08.980 --> 35:13.820\n to the world from reasoning obviously,\n\n35:13.820 --> 35:15.980\n but also data.\n\n35:15.980 --> 35:18.900\n And those models generally are more kind of analogical.\n\n35:18.900 --> 35:22.380\n So it's reasoning by simulation,\n\n35:22.380 --> 35:25.120\n and by analogy, where you use one model\n\n35:25.120 --> 35:26.900\n to apply to a new situation.\n\n35:26.900 --> 35:28.500\n Even though you've never seen that situation,\n\n35:28.500 --> 35:31.620\n you can sort of connect it to a situation\n\n35:31.620 --> 35:33.500\n you've encountered before.\n\n35:33.500 --> 35:36.700\n And your reasoning is more akin\n\n35:36.700 --> 35:38.420\n to some sort of internal simulation.\n\n35:38.420 --> 35:41.140\n So you're kind of simulating what's happening\n\n35:41.140 --> 35:42.240\n when you're building, I don't know,\n\n35:42.240 --> 35:44.100\n a box out of wood or something, right?\n\n35:44.100 --> 35:47.460\n You can imagine in advance what would be the result\n\n35:47.460 --> 35:49.660\n of cutting the wood in this particular way.\n\n35:49.660 --> 35:52.900\n Are you going to use screws or nails or whatever?\n\n35:52.900 --> 35:54.180\n When you are interacting with someone,\n\n35:54.180 --> 35:55.780\n you also have a model of that person\n\n35:55.780 --> 35:58.340\n and sort of interact with that person,\n\n35:59.580 --> 36:03.660\n having this model in mind to kind of tell the person\n\n36:03.660 --> 36:05.280\n what you think is useful to them.\n\n36:05.280 --> 36:10.220\n So I think this ability to construct models to the world\n\n36:10.220 --> 36:13.900\n is basically the essence, the essence of intelligence.\n\n36:13.900 --> 36:18.220\n And the ability to use it then to plan actions\n\n36:18.220 --> 36:23.080\n that will fulfill a particular criterion,\n\n36:23.080 --> 36:25.460\n of course, is necessary as well.\n\n36:25.460 --> 36:27.740\n So I'm going to ask you a series of impossible questions\n\n36:27.740 --> 36:30.180\n as we keep asking, as I've been doing.\n\n36:30.180 --> 36:33.460\n So if that's the fundamental sort of dark matter\n\n36:33.460 --> 36:36.580\n of intelligence, this ability to form a background model,\n\n36:36.580 --> 36:41.460\n what's your intuition about how much knowledge is required?\n\n36:41.460 --> 36:43.100\n You know, I think dark matter,\n\n36:43.100 --> 36:45.980\n you could put a percentage on it\n\n36:45.980 --> 36:50.060\n of the composition of the universe\n\n36:50.060 --> 36:51.460\n and how much of it is dark matter,\n\n36:51.460 --> 36:52.640\n how much of it is dark energy,\n\n36:52.640 --> 36:57.640\n how much information do you think is required\n\n36:57.900 --> 36:59.920\n to be a house cat?\n\n36:59.920 --> 37:02.900\n So you have to be able to, when you see a box going in,\n\n37:02.900 --> 37:06.220\n when you see a human compute the most evil action,\n\n37:06.220 --> 37:07.940\n if there's a thing that's near an edge,\n\n37:07.940 --> 37:10.980\n you knock it off, all of that,\n\n37:10.980 --> 37:12.740\n plus the extra stuff you mentioned,\n\n37:12.740 --> 37:15.700\n which is a great self awareness of the physics\n\n37:15.700 --> 37:18.740\n of your own body and the world.\n\n37:18.740 --> 37:21.540\n How much knowledge is required, do you think, to solve it?\n\n37:22.500 --> 37:25.620\n I don't even know how to measure an answer to that question.\n\n37:25.620 --> 37:26.680\n I'm not sure how to measure it,\n\n37:26.680 --> 37:31.140\n but whatever it is, it fits in about 800,000 neurons,\n\n37:32.380 --> 37:33.900\n 800 million neurons.\n\n37:33.900 --> 37:36.300\n What's the representation does?\n\n37:36.300 --> 37:38.540\n Everything, all knowledge, everything, right?\n\n37:40.100 --> 37:41.500\n You know, it's less than a billion.\n\n37:41.500 --> 37:44.420\n A dog is 2 billion, but a cat is less than 1 billion.\n\n37:45.500 --> 37:48.140\n And so multiply that by a thousand\n\n37:48.140 --> 37:50.300\n and you get the number of synapses.\n\n37:50.300 --> 37:52.780\n And I think almost all of it is learned\n\n37:52.780 --> 37:55.940\n through this, you know, a sort of self supervised running,\n\n37:55.940 --> 37:58.500\n although, you know, I think a tiny sliver\n\n37:58.500 --> 37:59.900\n is learned through reinforcement running\n\n37:59.900 --> 38:02.220\n and certainly very little through, you know,\n\n38:02.220 --> 38:03.340\n classical supervised running,\n\n38:03.340 --> 38:05.180\n although it's not even clear how supervised running\n\n38:05.180 --> 38:08.120\n actually works in the biological world.\n\n38:09.260 --> 38:12.860\n So I think almost all of it is self supervised running,\n\n38:12.860 --> 38:17.860\n but it's driven by the sort of ingrained objective functions\n\n38:18.180 --> 38:21.400\n that a cat or a human have at the base of their brain,\n\n38:21.400 --> 38:24.880\n which kind of drives their behavior.\n\n38:24.880 --> 38:28.580\n So, you know, nature tells us you're hungry.\n\n38:29.480 --> 38:31.900\n It doesn't tell us how to feed ourselves.\n\n38:31.900 --> 38:33.500\n That's something that the rest of our brain\n\n38:33.500 --> 38:34.820\n has to figure out, right?\n\n38:35.780 --> 38:37.940\n What's interesting is there might be more\n\n38:37.940 --> 38:39.660\n like deeper objective functions\n\n38:39.660 --> 38:41.300\n than allowing the whole thing.\n\n38:41.300 --> 38:44.500\n So hunger may be some kind of,\n\n38:44.500 --> 38:46.140\n now you go to like neurobiology,\n\n38:46.140 --> 38:51.140\n it might be just the brain trying to maintain homeostasis.\n\n38:52.460 --> 38:57.460\n So hunger is just one of the human perceivable symptoms\n\n38:58.020 --> 38:59.380\n of the brain being unhappy\n\n38:59.380 --> 39:01.460\n with the way things are currently.\n\n39:01.460 --> 39:04.140\n It could be just like one really dumb objective function\n\n39:04.140 --> 39:04.980\n at the core.\n\n39:04.980 --> 39:08.460\n But that's how behavior is driven.\n\n39:08.460 --> 39:11.260\n The fact that, you know, or basal ganglia\n\n39:12.360 --> 39:14.820\n drive us to do things that are different\n\n39:14.820 --> 39:18.180\n from say an orangutan or certainly a cat\n\n39:18.180 --> 39:20.060\n is what makes, you know, human nature\n\n39:20.060 --> 39:23.280\n versus orangutan nature versus cat nature.\n\n39:23.280 --> 39:27.100\n So for example, you know, our basal ganglia\n\n39:27.100 --> 39:32.100\n drives us to seek the company of other humans.\n\n39:32.220 --> 39:34.540\n And that's because nature has figured out\n\n39:34.540 --> 39:37.540\n that we need to be social animals for our species to survive.\n\n39:37.540 --> 39:40.320\n And it's true of many primates.\n\n39:41.300 --> 39:42.620\n It's not true of orangutans.\n\n39:42.620 --> 39:44.900\n Orangutans are solitary animals.\n\n39:44.900 --> 39:46.900\n They don't seek the company of others.\n\n39:46.900 --> 39:48.200\n In fact, they avoid them.\n\n39:49.300 --> 39:51.060\n In fact, they scream at them when they come too close\n\n39:51.060 --> 39:52.740\n because they're territorial.\n\n39:52.740 --> 39:55.900\n Because for their survival, you know,\n\n39:55.900 --> 39:58.300\n evolution has figured out that's the best thing.\n\n39:58.300 --> 40:00.040\n I mean, they're occasionally social, of course,\n\n40:00.040 --> 40:03.500\n for, you know, reproduction and stuff like that.\n\n40:03.500 --> 40:05.920\n But they're mostly solitary.\n\n40:05.920 --> 40:09.540\n So all of those behaviors are not part of intelligence.\n\n40:09.540 --> 40:10.380\n You know, people say,\n\n40:10.380 --> 40:11.800\n oh, you're never gonna have intelligent machines\n\n40:11.800 --> 40:13.940\n because, you know, human intelligence is social.\n\n40:13.940 --> 40:16.820\n But then you look at orangutans, you look at octopus.\n\n40:16.820 --> 40:18.800\n Octopus never know their parents.\n\n40:18.800 --> 40:20.500\n They barely interact with any other.\n\n40:20.500 --> 40:23.900\n And they get to be really smart in less than a year,\n\n40:23.900 --> 40:26.040\n in like half a year.\n\n40:26.040 --> 40:27.620\n You know, in a year, they're adults.\n\n40:27.620 --> 40:28.780\n In two years, they're dead.\n\n40:28.780 --> 40:33.620\n So there are things that we think, as humans,\n\n40:33.620 --> 40:35.740\n are intimately linked with intelligence,\n\n40:35.740 --> 40:38.840\n like social interaction, like language.\n\n40:39.760 --> 40:42.860\n We think, I think we give way too much importance\n\n40:42.860 --> 40:46.780\n to language as a substrate of intelligence as humans.\n\n40:46.780 --> 40:49.840\n Because we think our reasoning is so linked with language.\n\n40:49.840 --> 40:53.460\n So to solve the house cat intelligence problem,\n\n40:53.460 --> 40:55.500\n you think you could do it on a desert island.\n\n40:55.500 --> 40:58.460\n You could have, you could just have a cat sitting there\n\n41:00.360 --> 41:03.180\n looking at the waves, at the ocean waves,\n\n41:03.180 --> 41:05.740\n and figure a lot of it out.\n\n41:05.740 --> 41:07.500\n It needs to have sort of, you know,\n\n41:07.500 --> 41:11.540\n the right set of drives to kind of, you know,\n\n41:11.540 --> 41:13.980\n get it to do the thing and learn the appropriate things,\n\n41:13.980 --> 41:17.660\n right, but like for example, you know,\n\n41:17.660 --> 41:22.660\n baby humans are driven to learn to stand up and walk.\n\n41:22.660 --> 41:26.020\n You know, that's kind of, this desire is hardwired.\n\n41:26.020 --> 41:28.540\n How to do it precisely is not, that's learned.\n\n41:28.540 --> 41:31.800\n But the desire to walk, move around and stand up,\n\n41:32.840 --> 41:35.940\n that's sort of probably hardwired.\n\n41:35.940 --> 41:38.940\n But it's very simple to hardwire this kind of stuff.\n\n41:38.940 --> 41:42.780\n Oh, like the desire to, well, that's interesting.\n\n41:42.780 --> 41:44.420\n You're hardwired to want to walk.\n\n41:45.620 --> 41:50.460\n That's not, there's gotta be a deeper need for walking.\n\n41:50.460 --> 41:53.140\n I think it was probably socially imposed by society\n\n41:53.140 --> 41:55.580\n that you need to walk all the other bipedal.\n\n41:55.580 --> 41:58.420\n No, like a lot of simple animals that, you know,\n\n41:58.420 --> 42:01.040\n will probably walk without ever watching\n\n42:01.040 --> 42:03.900\n any other members of the species.\n\n42:03.900 --> 42:06.820\n It seems like a scary thing to have to do\n\n42:06.820 --> 42:09.280\n because you suck at bipedal walking at first.\n\n42:09.280 --> 42:13.820\n It seems crawling is much safer, much more like,\n\n42:13.820 --> 42:15.700\n why are you in a hurry?\n\n42:15.700 --> 42:18.660\n Well, because you have this thing that drives you to do it,\n\n42:18.660 --> 42:23.180\n you know, which is sort of part of the sort of\n\n42:24.220 --> 42:25.060\n human development.\n\n42:25.060 --> 42:26.700\n Is that understood actually what?\n\n42:26.700 --> 42:28.220\n Not entirely, no.\n\n42:28.220 --> 42:29.740\n What's the reason you get on two feet?\n\n42:29.740 --> 42:30.620\n It's really hard.\n\n42:30.620 --> 42:32.780\n Like most animals don't get on two feet.\n\n42:32.780 --> 42:33.980\n Well, they get on four feet.\n\n42:33.980 --> 42:35.740\n You know, many mammals get on four feet.\n\n42:35.740 --> 42:36.760\n Yeah, they do. Very quickly.\n\n42:36.760 --> 42:38.500\n Some of them extremely quickly.\n\n42:38.500 --> 42:41.380\n But I don't, you know, like from the last time\n\n42:41.380 --> 42:42.620\n I've interacted with a table,\n\n42:42.620 --> 42:44.940\n that's much more stable than a thing than two legs.\n\n42:44.940 --> 42:46.420\n It's just a really hard problem.\n\n42:46.420 --> 42:48.620\n Yeah, I mean, birds have figured it out with two feet.\n\n42:48.620 --> 42:52.020\n Well, technically we can go into ontology.\n\n42:52.020 --> 42:54.500\n They have four, I guess they have two feet.\n\n42:54.500 --> 42:55.340\n They have two feet.\n\n42:55.340 --> 42:56.380\n Chickens.\n\n42:56.380 --> 42:58.860\n You know, dinosaurs have two feet, many of them.\n\n42:58.860 --> 42:59.700\n Allegedly.\n\n43:01.560 --> 43:04.340\n I'm just now learning that T. rex was eating grass,\n\n43:04.340 --> 43:05.420\n not other animals.\n\n43:05.420 --> 43:08.020\n T. rex might've been a friendly pet.\n\n43:08.020 --> 43:09.260\n What do you think about,\n\n43:10.320 --> 43:13.500\n I don't know if you looked at the test\n\n43:13.500 --> 43:16.380\n for general intelligence that Fran\u00e7ois Chollet put together.\n\n43:16.380 --> 43:18.000\n I don't know if you got a chance to look\n\n43:18.000 --> 43:19.660\n at that kind of thing.\n\n43:19.660 --> 43:21.860\n What's your intuition about how to solve\n\n43:21.860 --> 43:23.740\n like an IQ type of test?\n\n43:23.740 --> 43:24.580\n I don't know.\n\n43:24.580 --> 43:26.140\n I think it's so outside of my radar screen\n\n43:26.140 --> 43:30.740\n that it's not really relevant, I think, in the short term.\n\n43:30.740 --> 43:33.100\n Well, I guess one way to ask,\n\n43:33.100 --> 43:37.780\n another way, perhaps more closer to what do you work is like,\n\n43:37.780 --> 43:42.740\n how do you solve MNIST with very little example data?\n\n43:42.740 --> 43:43.560\n That's right.\n\n43:43.560 --> 43:44.860\n And that's the answer to this probably\n\n43:44.860 --> 43:45.860\n is self supervised learning.\n\n43:45.860 --> 43:47.300\n Just learn to represent images\n\n43:47.300 --> 43:51.060\n and then learning to recognize handwritten digits\n\n43:51.060 --> 43:53.620\n on top of this will only require a few samples.\n\n43:53.620 --> 43:55.460\n And we observe this in humans, right?\n\n43:55.460 --> 43:58.660\n You show a young child a picture book\n\n43:58.660 --> 44:01.940\n with a couple of pictures of an elephant and that's it.\n\n44:01.940 --> 44:03.900\n The child knows what an elephant is.\n\n44:03.900 --> 44:06.700\n And we see this today with practical systems\n\n44:06.700 --> 44:09.540\n that we train image recognition systems\n\n44:09.540 --> 44:13.660\n with enormous amounts of images,\n\n44:13.660 --> 44:15.740\n either completely self supervised\n\n44:15.740 --> 44:16.980\n or very weakly supervised.\n\n44:16.980 --> 44:20.900\n For example, you can train a neural net\n\n44:20.900 --> 44:24.180\n to predict whatever hashtag people type on Instagram, right?\n\n44:24.180 --> 44:25.780\n Then you can do this with billions of images\n\n44:25.780 --> 44:28.540\n because there's billions per day that are showing up.\n\n44:28.540 --> 44:30.700\n So the amount of training data there\n\n44:30.700 --> 44:32.340\n is essentially unlimited.\n\n44:32.340 --> 44:35.380\n And then you take the output representation,\n\n44:35.380 --> 44:37.380\n a couple of layers down from the outputs\n\n44:37.380 --> 44:40.680\n of what the system learned and feed this as input\n\n44:40.680 --> 44:43.780\n to a classifier for any object in the world that you want\n\n44:43.780 --> 44:44.940\n and it works pretty well.\n\n44:44.940 --> 44:47.620\n So that's transfer learning, okay?\n\n44:47.620 --> 44:50.160\n Or weakly supervised transfer learning.\n\n44:51.340 --> 44:53.460\n People are making very, very fast progress\n\n44:53.460 --> 44:55.300\n using self supervised learning\n\n44:55.300 --> 44:57.380\n for this kind of scenario as well.\n\n44:58.580 --> 45:02.500\n And my guess is that that's gonna be the future.\n\n45:02.500 --> 45:03.660\n For self supervised learning,\n\n45:03.660 --> 45:06.800\n how much cleaning do you think is needed\n\n45:06.800 --> 45:11.800\n for filtering malicious signal or what's a better term?\n\n45:11.800 --> 45:15.760\n But like a lot of people use hashtags on Instagram\n\n45:16.760 --> 45:21.200\n to get like good SEO that doesn't fully represent\n\n45:21.200 --> 45:23.100\n the contents of the image.\n\n45:23.100 --> 45:24.520\n Like they'll put a picture of a cat\n\n45:24.520 --> 45:28.060\n and hashtag it with like science, awesome, fun.\n\n45:28.060 --> 45:31.200\n I don't know all kinds, why would you put science?\n\n45:31.200 --> 45:33.080\n That's not very good SEO.\n\n45:33.080 --> 45:34.960\n The way my colleagues who worked on this project\n\n45:34.960 --> 45:39.960\n at Facebook, now Meta AI, a few years ago dealt with this\n\n45:39.960 --> 45:43.760\n is that they only selected something like 17,000 tags\n\n45:43.760 --> 45:48.100\n that correspond to kind of physical things or situations,\n\n45:48.100 --> 45:50.320\n like that has some visual content.\n\n45:52.320 --> 45:57.120\n So you wouldn't have like hash TBT or anything like that.\n\n45:57.120 --> 46:00.820\n Oh, so they keep a very select set of hashtags\n\n46:00.820 --> 46:01.660\n is what you're saying?\n\n46:01.660 --> 46:02.480\n Yeah.\n\n46:02.480 --> 46:03.320\n Okay.\n\n46:03.320 --> 46:06.080\n But it's still in the order of 10 to 20,000.\n\n46:06.080 --> 46:07.960\n So it's fairly large.\n\n46:07.960 --> 46:09.040\n Okay.\n\n46:09.040 --> 46:11.280\n Can you tell me about data augmentation?\n\n46:11.280 --> 46:14.760\n What the heck is data augmentation and how is it used\n\n46:14.760 --> 46:19.080\n maybe contrast of learning for video?\n\n46:19.080 --> 46:20.880\n What are some cool ideas here?\n\n46:20.880 --> 46:22.120\n Right, so data augmentation.\n\n46:22.120 --> 46:24.520\n I mean, first data augmentation is the idea\n\n46:24.520 --> 46:26.960\n of artificially increasing the size of your training set\n\n46:26.960 --> 46:30.020\n by distorting the images that you have\n\n46:30.020 --> 46:32.360\n in ways that don't change the nature of the image, right?\n\n46:32.360 --> 46:35.520\n So you do MNIST, you can do data augmentation on MNIST\n\n46:35.520 --> 46:37.360\n and people have done this since the 1990s, right?\n\n46:37.360 --> 46:40.880\n You take a MNIST digit and you shift it a little bit\n\n46:40.880 --> 46:44.820\n or you change the size or rotate it, skew it,\n\n46:45.800 --> 46:47.000\n you know, et cetera.\n\n46:47.000 --> 46:48.280\n Add noise.\n\n46:48.280 --> 46:49.520\n Add noise, et cetera.\n\n46:49.520 --> 46:52.440\n And it works better if you train a supervised classifier\n\n46:52.440 --> 46:55.600\n with augmented data, you're gonna get better results.\n\n46:55.600 --> 46:58.640\n Now it's become really interesting\n\n46:58.640 --> 47:00.400\n over the last couple of years\n\n47:00.400 --> 47:04.160\n because a lot of self supervised learning techniques\n\n47:04.160 --> 47:07.980\n to pre train vision systems are based on data augmentation.\n\n47:07.980 --> 47:12.000\n And the basic techniques is originally inspired\n\n47:12.000 --> 47:15.840\n by techniques that I worked on in the early 90s\n\n47:15.840 --> 47:17.720\n and Jeff Hinton worked on also in the early 90s.\n\n47:17.720 --> 47:20.040\n They were sort of parallel work.\n\n47:20.040 --> 47:21.600\n I used to call this Siamese network.\n\n47:21.600 --> 47:24.960\n So basically you take two identical copies\n\n47:24.960 --> 47:27.720\n of the same network, they share the same weights\n\n47:27.720 --> 47:31.760\n and you show two different views of the same object.\n\n47:31.760 --> 47:33.920\n Either those two different views may have been obtained\n\n47:33.920 --> 47:35.440\n by data augmentation\n\n47:35.440 --> 47:37.680\n or maybe it's two different views of the same scene\n\n47:37.680 --> 47:40.280\n from a camera that you moved or at different times\n\n47:40.280 --> 47:41.400\n or something like that, right?\n\n47:41.400 --> 47:44.400\n Or two pictures of the same person, things like that.\n\n47:44.400 --> 47:46.480\n And then you train this neural net,\n\n47:46.480 --> 47:48.420\n those two identical copies of this neural net\n\n47:48.420 --> 47:51.420\n to produce an output representation, a vector\n\n47:52.460 --> 47:56.560\n in such a way that the representation for those two images\n\n47:56.560 --> 47:58.880\n are as close to each other as possible,\n\n47:58.880 --> 48:00.840\n as identical to each other as possible, right?\n\n48:00.840 --> 48:02.040\n Because you want the system\n\n48:02.040 --> 48:06.120\n to basically learn a function that will be invariant,\n\n48:06.120 --> 48:08.200\n that will not change, whose output will not change\n\n48:08.200 --> 48:12.480\n when you transform those inputs in those particular ways,\n\n48:12.480 --> 48:14.080\n right?\n\n48:14.080 --> 48:15.680\n So that's easy to do.\n\n48:15.680 --> 48:17.720\n What's complicated is how do you make sure\n\n48:17.720 --> 48:19.520\n that when you show two images that are different,\n\n48:19.520 --> 48:21.960\n the system will produce different things?\n\n48:21.960 --> 48:26.200\n Because if you don't have a specific provision for this,\n\n48:26.200 --> 48:29.160\n the system will just ignore the inputs when you train it,\n\n48:29.160 --> 48:30.360\n it will end up ignoring the input\n\n48:30.360 --> 48:31.740\n and just produce a constant vector\n\n48:31.740 --> 48:33.680\n that is the same for every input, right?\n\n48:33.680 --> 48:35.200\n That's called a collapse.\n\n48:35.200 --> 48:36.720\n Now, how do you avoid collapse?\n\n48:36.720 --> 48:37.800\n So there's two ideas.\n\n48:38.840 --> 48:41.560\n One idea that I proposed in the early 90s\n\n48:41.560 --> 48:43.120\n with my colleagues at Bell Labs,\n\n48:43.120 --> 48:45.360\n Jane Barmley and a couple other people,\n\n48:46.280 --> 48:48.280\n which we now call contrastive learning,\n\n48:48.280 --> 48:50.020\n which is to have negative examples, right?\n\n48:50.020 --> 48:53.160\n So you have pairs of images that you know are different\n\n48:54.400 --> 48:57.480\n and you show them to the network and those two copies,\n\n48:57.480 --> 48:59.760\n and then you push the two output vectors away\n\n48:59.760 --> 49:02.200\n from each other and it will eventually guarantee\n\n49:02.200 --> 49:04.880\n that things that are semantically similar\n\n49:04.880 --> 49:06.480\n produce similar representations\n\n49:06.480 --> 49:07.320\n and things that are different\n\n49:07.320 --> 49:09.080\n produce different representations.\n\n49:10.280 --> 49:11.440\n We actually came up with this idea\n\n49:11.440 --> 49:14.480\n for a project of doing signature verification.\n\n49:14.480 --> 49:18.400\n So we would collect signatures from,\n\n49:18.400 --> 49:20.160\n like multiple signatures on the same person\n\n49:20.160 --> 49:23.280\n and then train a neural net to produce the same representation\n\n49:23.280 --> 49:27.880\n and then force the system to produce different\n\n49:27.880 --> 49:29.920\n representation for different signatures.\n\n49:31.000 --> 49:33.460\n This was actually, the problem was proposed by people\n\n49:33.460 --> 49:38.240\n from what was a subsidiary of AT&T at the time called NCR.\n\n49:38.240 --> 49:40.360\n And they were interested in storing\n\n49:40.360 --> 49:43.500\n representation of the signature on the 80 bytes\n\n49:43.500 --> 49:46.640\n of the magnetic strip of a credit card.\n\n49:46.640 --> 49:48.800\n So we came up with this idea of having a neural net\n\n49:48.800 --> 49:52.280\n with 80 outputs that we would quantize on bytes\n\n49:52.280 --> 49:53.840\n so that we could encode the signature.\n\n49:53.840 --> 49:55.440\n And that encoding was then used to compare\n\n49:55.440 --> 49:57.080\n whether the signature matches or not.\n\n49:57.080 --> 49:57.920\n That's right.\n\n49:57.920 --> 50:00.640\n So then you would sign, you would run through the neural net\n\n50:00.640 --> 50:02.400\n and then you would compare the output vector\n\n50:02.400 --> 50:03.240\n to whatever is stored on your card.\n\n50:03.240 --> 50:04.640\n Did it actually work?\n\n50:04.640 --> 50:06.680\n It worked, but they ended up not using it.\n\n50:08.940 --> 50:10.120\n Because nobody cares actually.\n\n50:10.120 --> 50:13.800\n I mean, the American financial payment system\n\n50:13.800 --> 50:17.560\n is incredibly lax in that respect compared to Europe.\n\n50:17.560 --> 50:18.960\n Oh, with the signatures?\n\n50:18.960 --> 50:20.520\n What's the purpose of signatures anyway?\n\n50:20.520 --> 50:21.360\n This is very different.\n\n50:21.360 --> 50:23.280\n Nobody looks at them, nobody cares.\n\n50:23.280 --> 50:24.440\n It's, yeah.\n\n50:24.440 --> 50:27.840\n Yeah, no, so that's contrastive learning, right?\n\n50:27.840 --> 50:29.440\n So you need positive and negative pairs.\n\n50:29.440 --> 50:31.760\n And the problem with that is that,\n\n50:31.760 --> 50:34.760\n even though I had the original paper on this,\n\n50:34.760 --> 50:36.800\n I'm actually not very positive about it\n\n50:36.800 --> 50:38.640\n because it doesn't work in high dimension.\n\n50:38.640 --> 50:41.040\n If your representation is high dimensional,\n\n50:41.040 --> 50:44.300\n there's just too many ways for two things to be different.\n\n50:44.300 --> 50:45.960\n And so you would need lots and lots\n\n50:45.960 --> 50:48.260\n and lots of negative pairs.\n\n50:48.260 --> 50:50.800\n So there is a particular implementation of this,\n\n50:50.800 --> 50:52.840\n which is relatively recent from actually\n\n50:52.840 --> 50:56.040\n the Google Toronto group where, you know,\n\n50:56.040 --> 50:58.800\n Jeff Hinton is the senior member there.\n\n50:58.800 --> 51:02.000\n It's called SIMCLR, S I M C L R.\n\n51:02.000 --> 51:03.720\n And it, you know, basically a particular way\n\n51:03.720 --> 51:06.760\n of implementing this idea of contrastive learning,\n\n51:06.760 --> 51:08.600\n the particular objective function.\n\n51:08.600 --> 51:13.160\n Now, what I'm much more enthusiastic about these days\n\n51:13.160 --> 51:14.600\n is non contrastive methods.\n\n51:14.600 --> 51:19.600\n So other ways to guarantee that the representations\n\n51:19.600 --> 51:23.240\n would be different for different inputs.\n\n51:24.200 --> 51:28.320\n And it's actually based on an idea that Jeff Hinton\n\n51:28.320 --> 51:30.360\n proposed in the early nineties with his student\n\n51:30.360 --> 51:31.960\n at the time, Sue Becker.\n\n51:31.960 --> 51:33.440\n And it's based on the idea of maximizing\n\n51:33.440 --> 51:35.000\n the mutual information between the outputs\n\n51:35.000 --> 51:36.200\n of the two systems.\n\n51:36.200 --> 51:37.480\n You only show positive pairs.\n\n51:37.480 --> 51:39.160\n You only show pairs of images that you know\n\n51:39.160 --> 51:41.640\n are somewhat similar.\n\n51:41.640 --> 51:44.200\n And you train the two networks to be informative,\n\n51:44.200 --> 51:48.880\n but also to be as informative of each other as possible.\n\n51:48.880 --> 51:51.400\n So basically one representation has to be predictable\n\n51:51.400 --> 51:53.080\n from the other, essentially.\n\n51:54.520 --> 51:56.400\n And, you know, he proposed that idea,\n\n51:56.400 --> 51:59.440\n had, you know, a couple of papers in the early nineties,\n\n51:59.440 --> 52:02.280\n and then nothing was done about it for decades.\n\n52:02.280 --> 52:04.360\n And I kind of revived this idea together\n\n52:04.360 --> 52:06.240\n with my postdocs at FAIR,\n\n52:07.480 --> 52:08.920\n particularly a postdoc called Stefan Denis,\n\n52:08.920 --> 52:11.800\n who is now a junior professor in Finland\n\n52:11.800 --> 52:13.240\n at University of Aalto.\n\n52:13.240 --> 52:18.240\n We came up with something that we call Barlow Twins.\n\n52:18.240 --> 52:20.520\n And it's a particular way of maximizing\n\n52:20.520 --> 52:24.240\n the information content of a vector,\n\n52:24.240 --> 52:26.960\n you know, using some hypotheses.\n\n52:27.920 --> 52:30.920\n And we have kind of another version of it\n\n52:30.920 --> 52:33.480\n that's more recent now called VICREG, V I C A R E G.\n\n52:33.480 --> 52:35.960\n That means Variance, Invariance, Covariance,\n\n52:35.960 --> 52:36.800\n Regularization.\n\n52:36.800 --> 52:38.840\n And it's the thing I'm the most excited about\n\n52:38.840 --> 52:40.600\n in machine learning in the last 15 years.\n\n52:40.600 --> 52:43.360\n I mean, I'm not, I'm really, really excited about this.\n\n52:43.360 --> 52:46.400\n What kind of data augmentation is useful\n\n52:46.400 --> 52:49.280\n for that noncontrastive learning method?\n\n52:49.280 --> 52:51.680\n Are we talking about, does that not matter that much?\n\n52:51.680 --> 52:55.040\n Or it seems like a very important part of the step.\n\n52:55.040 --> 52:55.880\n Yeah.\n\n52:55.880 --> 52:57.120\n How you generate the images that are similar,\n\n52:57.120 --> 52:58.680\n but sufficiently different.\n\n52:58.680 --> 52:59.520\n Yeah, that's right.\n\n52:59.520 --> 53:01.440\n It's an important step and it's also an annoying step\n\n53:01.440 --> 53:02.840\n because you need to have that knowledge\n\n53:02.840 --> 53:05.840\n of what data augmentation you can do\n\n53:05.840 --> 53:09.320\n that do not change the nature of the object.\n\n53:09.320 --> 53:12.280\n And so the standard scenario,\n\n53:12.280 --> 53:14.520\n which a lot of people working in this area are using\n\n53:14.520 --> 53:18.720\n is you use the type of distortion.\n\n53:18.720 --> 53:21.160\n So basically you do a geometric distortion.\n\n53:21.160 --> 53:23.360\n So one basically just shifts the image a little bit,\n\n53:23.360 --> 53:24.400\n it's called cropping.\n\n53:24.400 --> 53:26.880\n Another one kind of changes the scale a little bit.\n\n53:26.880 --> 53:28.240\n Another one kind of rotates it.\n\n53:28.240 --> 53:30.000\n Another one changes the colors.\n\n53:30.000 --> 53:32.040\n You can do a shift in color balance\n\n53:32.040 --> 53:34.880\n or something like that, saturation.\n\n53:34.880 --> 53:36.240\n Another one sort of blurs it.\n\n53:36.240 --> 53:37.080\n Another one adds noise.\n\n53:37.080 --> 53:40.040\n So you have like a catalog of kind of standard things\n\n53:40.040 --> 53:42.120\n and people try to use the same ones\n\n53:42.120 --> 53:44.960\n for different algorithms so that they can compare.\n\n53:44.960 --> 53:47.200\n But some algorithms, some self supervised algorithm\n\n53:47.200 --> 53:49.600\n actually can deal with much bigger,\n\n53:49.600 --> 53:52.480\n like more aggressive data augmentation and some don't.\n\n53:52.480 --> 53:55.400\n So that kind of makes the whole thing difficult.\n\n53:55.400 --> 53:57.760\n But that's the kind of distortions we're talking about.\n\n53:57.760 --> 54:02.520\n And so you train with those distortions\n\n54:02.520 --> 54:07.400\n and then you chop off the last layer, a couple layers\n\n54:07.400 --> 54:11.480\n of the network and you use the representation\n\n54:11.480 --> 54:12.680\n as input to a classifier.\n\n54:12.680 --> 54:16.680\n You train the classifier on ImageNet, let's say,\n\n54:16.680 --> 54:19.600\n or whatever, and measure the performance.\n\n54:19.600 --> 54:23.520\n And interestingly enough, the methods that are really good\n\n54:23.520 --> 54:25.960\n at eliminating the information that is irrelevant,\n\n54:25.960 --> 54:29.200\n which is the distortions between those images,\n\n54:29.200 --> 54:31.480\n do a good job at eliminating it.\n\n54:31.480 --> 54:36.480\n And as a consequence, you cannot use the representations\n\n54:36.480 --> 54:39.080\n in those systems for things like object detection\n\n54:39.080 --> 54:41.480\n and localization because that information is gone.\n\n54:41.480 --> 54:44.760\n So the type of data augmentation you need to do\n\n54:44.760 --> 54:47.720\n depends on the tasks you want eventually the system\n\n54:47.720 --> 54:50.680\n to solve and the type of data augmentation,\n\n54:50.680 --> 54:52.560\n standard data augmentation that we use today\n\n54:52.560 --> 54:54.720\n are only appropriate for object recognition\n\n54:54.720 --> 54:56.040\n or image classification.\n\n54:56.040 --> 54:57.760\n They're not appropriate for things like.\n\n54:57.760 --> 55:00.800\n Can you help me out understand what wide localizations?\n\n55:00.800 --> 55:03.760\n So you're saying it's just not good at the negative,\n\n55:03.760 --> 55:05.440\n like at classifying the negative,\n\n55:05.440 --> 55:07.920\n so that's why it can't be used for the localization?\n\n55:07.920 --> 55:10.360\n No, it's just that you train the system,\n\n55:10.360 --> 55:13.560\n you give it an image and then you give it the same image\n\n55:13.560 --> 55:17.400\n shifted and scaled and you tell it that's the same image.\n\n55:17.400 --> 55:19.160\n So the system basically is trained\n\n55:19.160 --> 55:22.040\n to eliminate the information about position and size.\n\n55:22.040 --> 55:26.200\n So now you want to use that to figure out\n\n55:26.200 --> 55:27.760\n where an object is and what size it is.\n\n55:27.760 --> 55:30.040\n Like a bounding box, like they'd be able to actually.\n\n55:30.040 --> 55:34.160\n Okay, it can still find the object in the image,\n\n55:34.160 --> 55:35.960\n it's just not very good at finding\n\n55:35.960 --> 55:38.960\n the exact boundaries of that object, interesting.\n\n55:38.960 --> 55:42.040\n Interesting, which that's an interesting\n\n55:42.040 --> 55:43.480\n sort of philosophical question,\n\n55:43.480 --> 55:46.800\n how important is object localization anyway?\n\n55:46.800 --> 55:51.240\n We're like obsessed by measuring image segmentation,\n\n55:51.240 --> 55:53.420\n obsessed by measuring perfectly knowing\n\n55:53.420 --> 55:56.760\n the boundaries of objects when arguably\n\n55:56.760 --> 56:01.760\n that's not that essential to understanding\n\n56:01.840 --> 56:03.800\n what are the contents of the scene.\n\n56:03.800 --> 56:05.880\n On the other hand, I think evolutionarily,\n\n56:05.880 --> 56:08.200\n the first vision systems in animals\n\n56:08.200 --> 56:10.040\n were basically all about localization,\n\n56:10.040 --> 56:12.480\n very little about recognition.\n\n56:12.480 --> 56:15.320\n And in the human brain, you have two separate pathways\n\n56:15.320 --> 56:20.320\n for recognizing the nature of a scene or an object\n\n56:20.880 --> 56:22.320\n and localizing objects.\n\n56:22.320 --> 56:25.200\n So you use the first pathway called eventual pathway\n\n56:25.200 --> 56:28.160\n for telling what you're looking at.\n\n56:29.140 --> 56:30.560\n The other pathway, the dorsal pathway,\n\n56:30.560 --> 56:34.120\n is used for navigation, for grasping, for everything else.\n\n56:34.120 --> 56:36.920\n And basically a lot of the things you need for survival\n\n56:36.920 --> 56:39.740\n are localization and detection.\n\n56:41.880 --> 56:45.080\n Is similarity learning or contrastive learning,\n\n56:45.080 --> 56:46.520\n are these non contrastive methods\n\n56:46.520 --> 56:48.880\n the same as understanding something?\n\n56:48.880 --> 56:50.680\n Just because you know a distorted cat\n\n56:50.680 --> 56:52.600\n is the same as a non distorted cat,\n\n56:52.600 --> 56:56.760\n does that mean you understand what it means to be a cat?\n\n56:56.760 --> 56:57.600\n To some extent.\n\n56:57.600 --> 57:00.120\n I mean, it's a superficial understanding, obviously.\n\n57:00.120 --> 57:02.360\n But what is the ceiling of this method, do you think?\n\n57:02.360 --> 57:05.120\n Is this just one trick on the path\n\n57:05.120 --> 57:07.320\n to doing self supervised learning?\n\n57:07.320 --> 57:10.040\n Can we go really, really far?\n\n57:10.040 --> 57:11.280\n I think we can go really far.\n\n57:11.280 --> 57:16.280\n So if we figure out how to use techniques of that type,\n\n57:16.400 --> 57:19.480\n perhaps very different, but the same nature,\n\n57:19.480 --> 57:23.360\n to train a system from video to do video prediction,\n\n57:23.360 --> 57:28.360\n essentially, I think we'll have a path towards,\n\n57:30.440 --> 57:33.520\n I wouldn't say unlimited, but a path towards some level\n\n57:33.520 --> 57:38.120\n of physical common sense in machines.\n\n57:38.120 --> 57:43.120\n And I also think that that ability to learn\n\n57:44.440 --> 57:47.720\n how the world works from a sort of high throughput channel\n\n57:47.720 --> 57:52.720\n like vision is a necessary step towards\n\n57:53.520 --> 57:55.560\n sort of real artificial intelligence.\n\n57:55.560 --> 57:58.080\n In other words, I believe in grounded intelligence.\n\n57:58.080 --> 57:59.920\n I don't think we can train a machine\n\n57:59.920 --> 58:02.200\n to be intelligent purely from text.\n\n58:02.200 --> 58:04.960\n Because I think the amount of information about the world\n\n58:04.960 --> 58:07.680\n that's contained in text is tiny compared\n\n58:07.680 --> 58:09.960\n to what we need to know.\n\n58:11.600 --> 58:15.320\n So for example, and people have attempted to do this\n\n58:15.320 --> 58:18.920\n for 30 years, the psych project and things like that,\n\n58:18.920 --> 58:21.160\n basically kind of writing down all the facts that are known\n\n58:21.160 --> 58:25.240\n and hoping that some sort of common sense will emerge.\n\n58:25.240 --> 58:27.160\n I think it's basically hopeless.\n\n58:27.160 --> 58:28.320\n But let me take an example.\n\n58:28.320 --> 58:31.280\n You take an object, I describe a situation to you.\n\n58:31.280 --> 58:33.560\n I take an object, I put it on the table\n\n58:33.560 --> 58:34.960\n and I push the table.\n\n58:34.960 --> 58:37.240\n It's completely obvious to you that the object\n\n58:37.240 --> 58:39.240\n will be pushed with the table,\n\n58:39.240 --> 58:40.600\n because it's sitting on it.\n\n58:41.840 --> 58:45.040\n There's no text in the world, I believe, that explains this.\n\n58:45.040 --> 58:49.040\n And so if you train a machine as powerful as it could be,\n\n58:49.040 --> 58:53.920\n your GPT 5000 or whatever it is,\n\n58:53.920 --> 58:55.680\n it's never gonna learn about this.\n\n58:57.040 --> 59:01.040\n That information is just not present in any text.\n\n59:01.040 --> 59:03.280\n Well, the question, like with the psych project,\n\n59:03.280 --> 59:08.000\n the dream I think is to have like 10 million,\n\n59:08.000 --> 59:13.000\n say facts like that, that give you a headstart,\n\n59:13.000 --> 59:15.200\n like a parent guiding you.\n\n59:15.200 --> 59:17.280\n Now, we humans don't need a parent to tell us\n\n59:17.280 --> 59:19.240\n that the table will move, sorry,\n\n59:19.240 --> 59:21.440\n the smartphone will move with the table.\n\n59:21.440 --> 59:25.640\n But we get a lot of guidance in other ways.\n\n59:25.640 --> 59:28.160\n So it's possible that we can give it a quick shortcut.\n\n59:28.160 --> 59:29.200\n What about a cat?\n\n59:29.200 --> 59:30.800\n The cat knows that.\n\n59:30.800 --> 59:33.120\n No, but they evolved, so.\n\n59:33.120 --> 59:34.400\n No, they learn like us.\n\n59:35.840 --> 59:37.080\n Sorry, the physics of stuff?\n\n59:37.080 --> 59:38.480\n Yeah.\n\n59:38.480 --> 59:41.360\n Well, yeah, so you're saying it's,\n\n59:41.360 --> 59:45.080\n so you're putting a lot of intelligence\n\n59:45.080 --> 59:47.120\n onto the nurture side, not the nature.\n\n59:47.120 --> 59:47.960\n Yes.\n\n59:47.960 --> 59:50.000\n We seem to have, you know,\n\n59:50.000 --> 59:53.640\n there's a very inefficient arguably process of evolution\n\n59:53.640 --> 59:56.960\n that got us from bacteria to who we are today.\n\n59:57.840 --> 59:59.800\n Started at the bottom, now we're here.\n\n59:59.800 --> 1:00:04.240\n So the question is how, okay,\n\n1:00:04.240 --> 1:00:06.000\n the question is how fundamental is that,\n\n1:00:06.000 --> 1:00:08.400\n the nature of the whole hardware?\n\n1:00:08.400 --> 1:00:11.680\n And then is there any way to shortcut it\n\n1:00:11.680 --> 1:00:12.520\n if it's fundamental?\n\n1:00:12.520 --> 1:00:14.280\n If it's not, if it's most of intelligence,\n\n1:00:14.280 --> 1:00:15.920\n most of the cool stuff we've been talking about\n\n1:00:15.920 --> 1:00:18.800\n is mostly nurture, mostly trained.\n\n1:00:18.800 --> 1:00:20.680\n We figure it out by observing the world.\n\n1:00:20.680 --> 1:00:24.760\n We can form that big, beautiful, sexy background model\n\n1:00:24.760 --> 1:00:27.240\n that you're talking about just by sitting there.\n\n1:00:28.880 --> 1:00:32.600\n Then, okay, then you need to, then like maybe,\n\n1:00:34.800 --> 1:00:37.840\n it is all supervised learning all the way down.\n\n1:00:37.840 --> 1:00:39.000\n Self supervised learning, say.\n\n1:00:39.000 --> 1:00:41.360\n Whatever it is that makes, you know,\n\n1:00:41.360 --> 1:00:44.080\n human intelligence different from other animals,\n\n1:00:44.080 --> 1:00:46.320\n which, you know, a lot of people think is language\n\n1:00:46.320 --> 1:00:48.720\n and logical reasoning and this kind of stuff.\n\n1:00:48.720 --> 1:00:51.000\n It cannot be that complicated because it only popped up\n\n1:00:51.000 --> 1:00:52.840\n in the last million years.\n\n1:00:52.840 --> 1:00:54.320\n Yeah.\n\n1:00:54.320 --> 1:00:57.840\n And, you know, it only involves, you know,\n\n1:00:57.840 --> 1:00:59.640\n less than 1% of our genome might be,\n\n1:00:59.640 --> 1:01:01.200\n which is the difference between human genome\n\n1:01:01.200 --> 1:01:03.360\n and chimps or whatever.\n\n1:01:03.360 --> 1:01:06.640\n So it can't be that complicated.\n\n1:01:06.640 --> 1:01:08.040\n You know, it can't be that fundamental.\n\n1:01:08.040 --> 1:01:10.880\n I mean, most of the complicated stuff\n\n1:01:10.880 --> 1:01:13.640\n already exists in cats and dogs and, you know,\n\n1:01:13.640 --> 1:01:15.840\n certainly primates, nonhuman primates.\n\n1:01:17.120 --> 1:01:18.640\n Yeah, that little thing with humans\n\n1:01:18.640 --> 1:01:22.480\n might be just something about social interaction\n\n1:01:22.480 --> 1:01:24.000\n and ability to maintain ideas\n\n1:01:24.000 --> 1:01:28.160\n across like a collective of people.\n\n1:01:28.160 --> 1:01:30.840\n It sounds very dramatic and very impressive,\n\n1:01:30.840 --> 1:01:33.400\n but it probably isn't mechanistically speaking.\n\n1:01:33.400 --> 1:01:34.680\n It is, but we're not there yet.\n\n1:01:34.680 --> 1:01:39.480\n Like, you know, we have, I mean, this is number 634,\n\n1:01:39.480 --> 1:01:42.080\n you know, in the list of problems we have to solve.\n\n1:01:43.400 --> 1:01:46.880\n So basic physics of the world is number one.\n\n1:01:46.880 --> 1:01:51.600\n What do you, just a quick tangent on data augmentation.\n\n1:01:51.600 --> 1:01:56.600\n So a lot of it is hard coded versus learned.\n\n1:01:57.920 --> 1:02:00.960\n Do you have any intuition that maybe\n\n1:02:00.960 --> 1:02:03.600\n there could be some weird data augmentation,\n\n1:02:03.600 --> 1:02:06.200\n like generative type of data augmentation,\n\n1:02:06.200 --> 1:02:07.680\n like doing something weird to images,\n\n1:02:07.680 --> 1:02:12.680\n which then improves the similarity learning process?\n\n1:02:13.120 --> 1:02:16.280\n So not just kind of dumb, simple distortions,\n\n1:02:16.280 --> 1:02:18.120\n but by you shaking your head,\n\n1:02:18.120 --> 1:02:20.880\n just saying that even simple distortions are enough.\n\n1:02:20.880 --> 1:02:22.800\n I think, no, I think data augmentation\n\n1:02:22.800 --> 1:02:25.080\n is a temporary necessary evil.\n\n1:02:26.480 --> 1:02:28.880\n So what people are working on now is two things.\n\n1:02:28.880 --> 1:02:32.960\n One is the type of self supervised learning,\n\n1:02:32.960 --> 1:02:35.480\n like trying to translate the type of self supervised learning\n\n1:02:35.480 --> 1:02:38.680\n people use in language, translating these two images,\n\n1:02:38.680 --> 1:02:41.800\n which is basically a denoising autoencoder method, right?\n\n1:02:41.800 --> 1:02:46.800\n So you take an image, you block, you mask some parts of it,\n\n1:02:47.320 --> 1:02:49.520\n and then you train some giant neural net\n\n1:02:49.520 --> 1:02:52.640\n to reconstruct the parts that are missing.\n\n1:02:52.640 --> 1:02:56.200\n And until very recently,\n\n1:02:56.200 --> 1:02:59.160\n there was no working methods for that.\n\n1:02:59.160 --> 1:03:01.600\n All the autoencoder type methods for images\n\n1:03:01.600 --> 1:03:03.720\n weren't producing very good representation,\n\n1:03:03.720 --> 1:03:06.600\n but there's a paper now coming out of the fair group\n\n1:03:06.600 --> 1:03:08.960\n at MNL Park that actually works very well.\n\n1:03:08.960 --> 1:03:12.120\n So that doesn't require data augmentation,\n\n1:03:12.120 --> 1:03:15.000\n that requires only masking, okay.\n\n1:03:15.000 --> 1:03:18.640\n Only masking for images, okay.\n\n1:03:18.640 --> 1:03:20.280\n Right, so you mask part of the image\n\n1:03:20.280 --> 1:03:24.560\n and you train a system, which in this case is a transformer\n\n1:03:24.560 --> 1:03:28.400\n because the transformer represents the image\n\n1:03:28.400 --> 1:03:30.880\n as non overlapping patches,\n\n1:03:30.880 --> 1:03:33.320\n so it's easy to mask patches and things like that.\n\n1:03:33.320 --> 1:03:35.680\n Okay, but then my question transfers to that problem,\n\n1:03:35.680 --> 1:03:40.080\n the masking, like why should the mask be square or rectangle?\n\n1:03:40.080 --> 1:03:41.600\n So it doesn't matter, like, you know,\n\n1:03:41.600 --> 1:03:44.360\n I think we're gonna come up probably in the future\n\n1:03:44.360 --> 1:03:49.360\n with sort of ways to mask that are kind of random,\n\n1:03:50.480 --> 1:03:52.920\n essentially, I mean, they are random already, but.\n\n1:03:52.920 --> 1:03:55.880\n No, no, but like something that's challenging,\n\n1:03:56.800 --> 1:03:59.400\n like optimally challenging.\n\n1:03:59.400 --> 1:04:02.440\n So like, I mean, maybe it's a metaphor that doesn't apply,\n\n1:04:02.440 --> 1:04:06.400\n but you're, it seems like there's a data augmentation\n\n1:04:06.400 --> 1:04:09.880\n or masking, there's an interactive element with it.\n\n1:04:09.880 --> 1:04:12.560\n Like you're almost like playing with an image.\n\n1:04:12.560 --> 1:04:14.720\n And like, it's like the way we play with an image\n\n1:04:14.720 --> 1:04:15.680\n in our minds.\n\n1:04:15.680 --> 1:04:16.680\n No, but it's like dropout.\n\n1:04:16.680 --> 1:04:18.160\n It's like Boston machine training.\n\n1:04:18.160 --> 1:04:23.160\n You, you know, every time you see a percept,\n\n1:04:23.200 --> 1:04:26.840\n you also, you can perturb it in some way.\n\n1:04:26.840 --> 1:04:31.520\n And then the principle of the training procedure\n\n1:04:31.520 --> 1:04:33.600\n is to minimize the difference of the output\n\n1:04:33.600 --> 1:04:36.920\n or the representation between the clean version\n\n1:04:36.920 --> 1:04:40.280\n and the corrupted version, essentially, right?\n\n1:04:40.280 --> 1:04:42.000\n And you can do this in real time, right?\n\n1:04:42.000 --> 1:04:44.240\n So, you know, Boston machine work like this, right?\n\n1:04:44.240 --> 1:04:47.400\n You show a percept, you tell the machine\n\n1:04:47.400 --> 1:04:49.840\n that's a good combination of activities\n\n1:04:49.840 --> 1:04:50.880\n or your input neurons.\n\n1:04:50.880 --> 1:04:55.880\n And then you either let them go their merry way\n\n1:04:56.560 --> 1:04:58.960\n without clamping them to values,\n\n1:04:58.960 --> 1:05:01.120\n or you only do this with a subset.\n\n1:05:01.120 --> 1:05:03.520\n And what you're doing is you're training the system\n\n1:05:03.520 --> 1:05:07.000\n so that the stable state of the entire network\n\n1:05:07.000 --> 1:05:08.920\n is the same regardless of whether it sees\n\n1:05:08.920 --> 1:05:11.520\n the entire input or whether it sees only part of it.\n\n1:05:12.880 --> 1:05:14.360\n You know, denoising autoencoder method\n\n1:05:14.360 --> 1:05:15.880\n is basically the same thing, right?\n\n1:05:15.880 --> 1:05:18.600\n You're training a system to reproduce the input,\n\n1:05:18.600 --> 1:05:20.480\n the complete inputs and filling the input\n\n1:05:20.480 --> 1:05:23.400\n and filling the blanks, regardless of which parts\n\n1:05:23.400 --> 1:05:26.280\n are missing, and that's really the underlying principle.\n\n1:05:26.280 --> 1:05:28.320\n And you could imagine sort of, even in the brain,\n\n1:05:28.320 --> 1:05:30.720\n some sort of neural principle where, you know,\n\n1:05:30.720 --> 1:05:32.800\n neurons kind of oscillate, right?\n\n1:05:32.800 --> 1:05:35.520\n So they take their activity and then temporarily\n\n1:05:35.520 --> 1:05:38.040\n they kind of shut off to, you know,\n\n1:05:38.040 --> 1:05:42.120\n force the rest of the system to basically reconstruct\n\n1:05:42.120 --> 1:05:44.800\n the input without their help, you know?\n\n1:05:44.800 --> 1:05:49.040\n And, I mean, you could imagine, you know,\n\n1:05:49.040 --> 1:05:51.040\n more or less biologically possible processes.\n\n1:05:51.040 --> 1:05:51.880\n Something like that.\n\n1:05:51.880 --> 1:05:54.960\n And I guess with this denoising autoencoder\n\n1:05:54.960 --> 1:05:58.720\n and masking and data augmentation,\n\n1:05:58.720 --> 1:06:01.160\n you don't have to worry about being super efficient.\n\n1:06:01.160 --> 1:06:03.960\n You could just do as much as you want\n\n1:06:03.960 --> 1:06:06.160\n and get better over time.\n\n1:06:06.160 --> 1:06:08.800\n Because I was thinking, like, you might want to be clever\n\n1:06:08.800 --> 1:06:12.000\n about the way you do all these procedures, you know,\n\n1:06:12.000 --> 1:06:16.720\n but that's only, it's somehow costly to do every iteration,\n\n1:06:16.720 --> 1:06:17.960\n but it's not really.\n\n1:06:17.960 --> 1:06:19.280\n Not really.\n\n1:06:19.280 --> 1:06:20.280\n Maybe.\n\n1:06:20.280 --> 1:06:21.480\n And then there is, you know,\n\n1:06:21.480 --> 1:06:24.160\n data augmentation without explicit data augmentation.\n\n1:06:24.160 --> 1:06:25.600\n This data augmentation by weighting,\n\n1:06:25.600 --> 1:06:28.080\n which is, you know, the sort of video prediction.\n\n1:06:29.320 --> 1:06:31.480\n You're observing a video clip,\n\n1:06:31.480 --> 1:06:36.400\n observing the, you know, the continuation of that video clip.\n\n1:06:36.400 --> 1:06:38.040\n You try to learn a representation\n\n1:06:38.040 --> 1:06:40.240\n using dual joint embedding architectures\n\n1:06:40.240 --> 1:06:43.280\n in such a way that the representation of the future clip\n\n1:06:43.280 --> 1:06:45.680\n is easily predictable from the representation\n\n1:06:45.680 --> 1:06:48.600\n of the observed clip.\n\n1:06:48.600 --> 1:06:51.840\n Do you think YouTube has enough raw data\n\n1:06:52.720 --> 1:06:56.400\n from which to learn how to be a cat?\n\n1:06:56.400 --> 1:06:57.760\n I think so.\n\n1:06:57.760 --> 1:07:01.200\n So the amount of data is not the constraint.\n\n1:07:01.200 --> 1:07:04.120\n No, it would require some selection, I think.\n\n1:07:04.120 --> 1:07:05.400\n Some selection?\n\n1:07:05.400 --> 1:07:08.480\n Some selection of, you know, maybe the right type of data.\n\n1:07:08.480 --> 1:07:09.320\n You need some.\n\n1:07:09.320 --> 1:07:11.400\n Don't go down the rabbit hole of just cat videos.\n\n1:07:11.400 --> 1:07:14.600\n You might need to watch some lectures or something.\n\n1:07:14.600 --> 1:07:15.720\n No, you wouldn't.\n\n1:07:15.720 --> 1:07:17.480\n How meta would that be\n\n1:07:17.480 --> 1:07:21.400\n if it like watches lectures about intelligence\n\n1:07:21.400 --> 1:07:22.240\n and then learns,\n\n1:07:22.240 --> 1:07:24.320\n watches your lectures in NYU\n\n1:07:24.320 --> 1:07:26.280\n and learns from that how to be intelligent?\n\n1:07:26.280 --> 1:07:27.800\n I don't think that would be enough.\n\n1:07:30.080 --> 1:07:33.240\n What's your, do you find multimodal learning interesting?\n\n1:07:33.240 --> 1:07:35.080\n We've been talking about visual language,\n\n1:07:35.080 --> 1:07:36.440\n like combining those together,\n\n1:07:36.440 --> 1:07:38.120\n maybe audio, all those kinds of things.\n\n1:07:38.120 --> 1:07:40.400\n There's a lot of things that I find interesting\n\n1:07:40.400 --> 1:07:41.240\n in the short term,\n\n1:07:41.240 --> 1:07:44.080\n but are not addressing the important problem\n\n1:07:44.080 --> 1:07:46.600\n that I think are really kind of the big challenges.\n\n1:07:46.600 --> 1:07:48.920\n So I think, you know, things like multitask learning,\n\n1:07:48.920 --> 1:07:53.920\n continual learning, you know, adversarial issues.\n\n1:07:54.360 --> 1:07:57.000\n I mean, those have great practical interests\n\n1:07:57.000 --> 1:08:00.280\n in the relatively short term, possibly,\n\n1:08:00.280 --> 1:08:01.240\n but I don't think they're fundamental.\n\n1:08:01.240 --> 1:08:02.600\n You know, active learning,\n\n1:08:02.600 --> 1:08:04.360\n even to some extent, reinforcement learning.\n\n1:08:04.360 --> 1:08:07.920\n I think those things will become either obsolete\n\n1:08:07.920 --> 1:08:10.800\n or useless or easy\n\n1:08:10.800 --> 1:08:14.880\n once we figured out how to do self supervised\n\n1:08:14.880 --> 1:08:15.880\n representation learning\n\n1:08:15.880 --> 1:08:19.280\n or learning predictive world models.\n\n1:08:19.280 --> 1:08:21.480\n And so I think that's what, you know,\n\n1:08:21.480 --> 1:08:24.400\n the entire community should be focusing on.\n\n1:08:24.400 --> 1:08:25.400\n At least people who are interested\n\n1:08:25.400 --> 1:08:26.680\n in sort of fundamental questions\n\n1:08:26.680 --> 1:08:28.440\n or, you know, really kind of pushing the envelope\n\n1:08:28.440 --> 1:08:31.440\n of AI towards the next stage.\n\n1:08:31.440 --> 1:08:33.080\n But of course, there's like a huge amount of,\n\n1:08:33.080 --> 1:08:34.360\n you know, very interesting work to do\n\n1:08:34.360 --> 1:08:35.840\n in sort of practical questions\n\n1:08:35.840 --> 1:08:38.000\n that have, you know, short term impact.\n\n1:08:38.000 --> 1:08:41.240\n Well, you know, it's difficult to talk about\n\n1:08:41.240 --> 1:08:42.200\n the temporal scale,\n\n1:08:42.200 --> 1:08:44.240\n because all of human civilization\n\n1:08:44.240 --> 1:08:45.400\n will eventually be destroyed\n\n1:08:45.400 --> 1:08:48.520\n because the sun will die out.\n\n1:08:48.520 --> 1:08:50.280\n And even if Elon Musk is successful\n\n1:08:50.280 --> 1:08:54.520\n in multi planetary colonization across the galaxy,\n\n1:08:54.520 --> 1:08:56.560\n eventually the entirety of it\n\n1:08:56.560 --> 1:08:58.920\n will just become giant black holes.\n\n1:08:58.920 --> 1:09:02.120\n And that's gonna take a while though.\n\n1:09:02.120 --> 1:09:04.800\n So, but what I'm saying is then that logic\n\n1:09:04.800 --> 1:09:07.360\n can be used to say it's all meaningless.\n\n1:09:07.360 --> 1:09:10.920\n I'm saying all that to say that multitask learning\n\n1:09:11.840 --> 1:09:15.400\n might be, you're calling it practical\n\n1:09:15.400 --> 1:09:17.280\n or pragmatic or whatever.\n\n1:09:17.280 --> 1:09:19.440\n That might be the thing that achieves something\n\n1:09:19.440 --> 1:09:21.120\n very akin to intelligence\n\n1:09:22.560 --> 1:09:26.880\n while we're trying to solve the more general problem\n\n1:09:26.880 --> 1:09:29.400\n of self supervised learning of background knowledge.\n\n1:09:29.400 --> 1:09:30.640\n So the reason I bring that up,\n\n1:09:30.640 --> 1:09:33.040\n maybe one way to ask that question.\n\n1:09:33.040 --> 1:09:34.000\n I've been very impressed\n\n1:09:34.000 --> 1:09:36.440\n by what Tesla Autopilot team is doing.\n\n1:09:36.440 --> 1:09:38.320\n I don't know if you've gotten a chance to glance\n\n1:09:38.320 --> 1:09:42.080\n at this particular one example of multitask learning,\n\n1:09:42.080 --> 1:09:44.960\n where they're literally taking the problem,\n\n1:09:44.960 --> 1:09:48.880\n like, I don't know, Charles Darwin studying animals.\n\n1:09:48.880 --> 1:09:51.600\n They're studying the problem of driving\n\n1:09:51.600 --> 1:09:53.320\n and asking, okay, what are all the things\n\n1:09:53.320 --> 1:09:55.000\n you have to perceive?\n\n1:09:55.000 --> 1:09:57.800\n And the way they're solving it is one,\n\n1:09:57.800 --> 1:10:00.400\n there's an ontology where you're bringing that to the table.\n\n1:10:00.400 --> 1:10:02.240\n So you're formulating a bunch of different tasks.\n\n1:10:02.240 --> 1:10:04.240\n It's like over a hundred tasks or something like that\n\n1:10:04.240 --> 1:10:06.040\n that they're involved in driving.\n\n1:10:06.040 --> 1:10:07.720\n And then they're deploying it\n\n1:10:07.720 --> 1:10:10.480\n and then getting data back from people that run into trouble\n\n1:10:10.480 --> 1:10:12.680\n and they're trying to figure out, do we add tasks?\n\n1:10:12.680 --> 1:10:16.040\n Do we, like, we focus on each individual task separately?\n\n1:10:16.040 --> 1:10:18.280\n In fact, I would say,\n\n1:10:18.280 --> 1:10:20.680\n I would classify Andre Karpathy's talk in two ways.\n\n1:10:20.680 --> 1:10:22.360\n So one was about doors\n\n1:10:22.360 --> 1:10:24.720\n and the other one about how much ImageNet sucks.\n\n1:10:24.720 --> 1:10:28.560\n He kept going back and forth on those two topics,\n\n1:10:28.560 --> 1:10:30.000\n which ImageNet sucks,\n\n1:10:30.000 --> 1:10:33.040\n meaning you can't just use a single benchmark.\n\n1:10:33.040 --> 1:10:37.240\n There's so, like, you have to have like a giant suite\n\n1:10:37.240 --> 1:10:39.880\n of benchmarks to understand how well your system actually works.\n\n1:10:39.880 --> 1:10:40.720\n Oh, I agree with him.\n\n1:10:40.720 --> 1:10:43.880\n I mean, he's a very sensible guy.\n\n1:10:43.880 --> 1:10:47.560\n Now, okay, it's very clear that if you're faced\n\n1:10:47.560 --> 1:10:50.480\n with an engineering problem that you need to solve\n\n1:10:50.480 --> 1:10:51.920\n in a relatively short time,\n\n1:10:51.920 --> 1:10:55.880\n particularly if you have Elon Musk breathing down your neck,\n\n1:10:55.880 --> 1:10:58.640\n you're going to have to take shortcuts, right?\n\n1:10:58.640 --> 1:11:02.560\n You might think about the fact that the right thing to do\n\n1:11:02.560 --> 1:11:04.520\n and the longterm solution involves, you know,\n\n1:11:04.520 --> 1:11:06.560\n some fancy self supervised running,\n\n1:11:06.560 --> 1:11:10.240\n but you have, you know, Elon Musk breathing down your neck\n\n1:11:10.240 --> 1:11:13.600\n and, you know, this involves, you know, human lives.\n\n1:11:13.600 --> 1:11:17.320\n And so you have to basically just do\n\n1:11:17.320 --> 1:11:20.560\n the systematic engineering and, you know,\n\n1:11:22.000 --> 1:11:23.280\n fine tuning and refinements\n\n1:11:23.280 --> 1:11:26.360\n and trial and error and all that stuff.\n\n1:11:26.360 --> 1:11:27.400\n There's nothing wrong with that.\n\n1:11:27.400 --> 1:11:28.600\n That's called engineering.\n\n1:11:28.600 --> 1:11:33.600\n That's called, you know, putting technology out in the world.\n\n1:11:35.840 --> 1:11:39.880\n And you have to kind of ironclad it before you do this,\n\n1:11:39.880 --> 1:11:44.520\n you know, so much for, you know,\n\n1:11:44.520 --> 1:11:46.240\n grand ideas and principles.\n\n1:11:48.280 --> 1:11:50.720\n But, you know, I'm placing myself sort of, you know,\n\n1:11:50.720 --> 1:11:54.480\n some, you know, upstream of this, you know,\n\n1:11:54.480 --> 1:11:55.760\n quite a bit upstream of this.\n\n1:11:55.760 --> 1:11:58.240\n You're a Plato, think about platonic forms.\n\n1:11:58.240 --> 1:12:01.320\n You're not platonic because eventually\n\n1:12:01.320 --> 1:12:03.120\n I want that stuff to get used,\n\n1:12:03.120 --> 1:12:06.920\n but it's okay if it takes five or 10 years\n\n1:12:06.920 --> 1:12:09.720\n for the community to realize this is the right thing to do.\n\n1:12:09.720 --> 1:12:11.280\n I've done this before.\n\n1:12:11.280 --> 1:12:13.240\n It's been the case before that, you know,\n\n1:12:13.240 --> 1:12:14.440\n I've made that case.\n\n1:12:14.440 --> 1:12:17.760\n I mean, if you look back in the mid 2000, for example,\n\n1:12:17.760 --> 1:12:19.320\n and you ask yourself the question, okay,\n\n1:12:19.320 --> 1:12:22.080\n I want to recognize cars or faces or whatever,\n\n1:12:24.360 --> 1:12:25.560\n you know, I can use convolutional net.\n\n1:12:25.560 --> 1:12:28.360\n So I can use sort of more conventional\n\n1:12:28.360 --> 1:12:29.880\n kind of computer vision techniques, you know,\n\n1:12:29.880 --> 1:12:33.760\n using interest point detectors or assist density features\n\n1:12:33.760 --> 1:12:35.760\n and, you know, sticking an SVM on top.\n\n1:12:35.760 --> 1:12:37.800\n At that time, the datasets were so small\n\n1:12:37.800 --> 1:12:41.920\n that those methods that use more hand engineering\n\n1:12:41.920 --> 1:12:43.560\n worked better than ConvNets.\n\n1:12:43.560 --> 1:12:45.560\n It was just not enough data for ConvNets\n\n1:12:45.560 --> 1:12:48.880\n and ConvNets were a little slow with the kind of hardware\n\n1:12:48.880 --> 1:12:50.840\n that was available at the time.\n\n1:12:50.840 --> 1:12:53.880\n And there was a sea change when, basically,\n\n1:12:53.880 --> 1:12:56.680\n when, you know, datasets became bigger\n\n1:12:56.680 --> 1:12:58.600\n and GPUs became available.\n\n1:12:58.600 --> 1:13:02.960\n That's what, you know, two of the main factors\n\n1:13:02.960 --> 1:13:05.960\n that basically made people change their mind.\n\n1:13:07.880 --> 1:13:11.880\n And you can look at the history of,\n\n1:13:11.880 --> 1:13:15.560\n like, all sub branches of AI or pattern recognition.\n\n1:13:16.400 --> 1:13:19.800\n And there's a similar trajectory followed by techniques\n\n1:13:19.800 --> 1:13:23.960\n where people start by, you know, engineering the hell out of it.\n\n1:13:25.200 --> 1:13:29.200\n You know, be it optical character recognition,\n\n1:13:29.200 --> 1:13:31.760\n speech recognition, computer vision,\n\n1:13:31.760 --> 1:13:34.280\n like image recognition in general,\n\n1:13:34.280 --> 1:13:37.280\n natural language understanding, like, you know, translation,\n\n1:13:37.280 --> 1:13:38.000\n things like that, right?\n\n1:13:38.000 --> 1:13:41.040\n You start to engineer the hell out of it.\n\n1:13:41.040 --> 1:13:42.680\n You start to acquire all the knowledge,\n\n1:13:42.680 --> 1:13:44.760\n the prior knowledge you know about image formation,\n\n1:13:44.760 --> 1:13:46.600\n about, you know, the shape of characters,\n\n1:13:46.600 --> 1:13:49.560\n about, you know, morphological operations,\n\n1:13:49.560 --> 1:13:52.400\n about, like, feature extraction, Fourier transforms,\n\n1:13:52.400 --> 1:13:54.440\n you know, vernicke moments, you know, whatever, right?\n\n1:13:54.440 --> 1:13:56.280\n People have come up with thousands of ways\n\n1:13:56.280 --> 1:13:57.680\n of representing images\n\n1:13:57.680 --> 1:14:01.600\n so that they could be easily classified afterwards.\n\n1:14:01.600 --> 1:14:03.000\n Same for speech recognition, right?\n\n1:14:03.000 --> 1:14:04.640\n There is, you know, it took decades\n\n1:14:04.640 --> 1:14:06.920\n for people to figure out a good front end\n\n1:14:06.920 --> 1:14:09.680\n to preprocess speech signals\n\n1:14:09.680 --> 1:14:11.120\n so that, you know, all the information\n\n1:14:11.120 --> 1:14:13.400\n about what is being said is preserved,\n\n1:14:13.400 --> 1:14:14.440\n but most of the information\n\n1:14:14.440 --> 1:14:16.920\n about the identity of the speaker is gone.\n\n1:14:16.920 --> 1:14:20.880\n You know, kestrel coefficients or whatever, right?\n\n1:14:20.880 --> 1:14:23.400\n And same for text, right?\n\n1:14:23.400 --> 1:14:26.440\n You do named entity recognition and you parse\n\n1:14:26.440 --> 1:14:31.440\n and you do tagging of the parts of speech\n\n1:14:31.800 --> 1:14:34.480\n and, you know, you do this sort of tree representation\n\n1:14:34.480 --> 1:14:36.480\n of clauses and all that stuff, right?\n\n1:14:36.480 --> 1:14:38.120\n Before you can do anything.\n\n1:14:40.720 --> 1:14:43.520\n So that's how it starts, right?\n\n1:14:43.520 --> 1:14:45.160\n Just engineer the hell out of it.\n\n1:14:45.160 --> 1:14:47.920\n And then you start having data\n\n1:14:47.920 --> 1:14:50.160\n and maybe you have more powerful computers.\n\n1:14:50.160 --> 1:14:52.400\n Maybe you know something about statistical learning.\n\n1:14:52.400 --> 1:14:53.640\n So you start using machine learning\n\n1:14:53.640 --> 1:14:54.840\n and it's usually a small sliver\n\n1:14:54.840 --> 1:14:56.800\n on top of your kind of handcrafted system\n\n1:14:56.800 --> 1:14:59.560\n where, you know, you extract features by hand.\n\n1:14:59.560 --> 1:15:02.280\n Okay, and now, you know, nowadays the standard way\n\n1:15:02.280 --> 1:15:04.320\n of doing this is that you train the entire thing end to end\n\n1:15:04.320 --> 1:15:06.720\n with a deep learning system and it learns its own features\n\n1:15:06.720 --> 1:15:10.920\n and, you know, speech recognition systems nowadays\n\n1:15:10.920 --> 1:15:12.920\n or CR systems are completely end to end.\n\n1:15:12.920 --> 1:15:15.320\n It's, you know, it's some giant neural net\n\n1:15:15.320 --> 1:15:17.920\n that takes raw waveforms\n\n1:15:17.920 --> 1:15:20.440\n and produces a sequence of characters coming out.\n\n1:15:20.440 --> 1:15:22.080\n And it's just a huge neural net, right?\n\n1:15:22.080 --> 1:15:24.000\n There's no, you know, Markov model,\n\n1:15:24.000 --> 1:15:26.360\n there's no language model that is explicit\n\n1:15:26.360 --> 1:15:28.600\n other than, you know, something that's ingrained\n\n1:15:28.600 --> 1:15:30.960\n in the sort of neural language model, if you want.\n\n1:15:30.960 --> 1:15:33.400\n Same for translation, same for all kinds of stuff.\n\n1:15:33.400 --> 1:15:36.440\n So you see this continuous evolution\n\n1:15:36.440 --> 1:15:40.440\n from, you know, less and less hand crafting\n\n1:15:40.440 --> 1:15:43.120\n and more and more learning.\n\n1:15:43.120 --> 1:15:48.120\n And I think, I mean, it's true in biology as well.\n\n1:15:50.680 --> 1:15:52.880\n So, I mean, we might disagree about this,\n\n1:15:52.880 --> 1:15:56.860\n maybe not, this one little piece at the end,\n\n1:15:56.860 --> 1:15:58.360\n you mentioned active learning.\n\n1:15:58.360 --> 1:16:01.440\n It feels like active learning,\n\n1:16:01.440 --> 1:16:02.880\n which is the selection of data\n\n1:16:02.880 --> 1:16:05.600\n and also the interactivity needs to be part\n\n1:16:05.600 --> 1:16:06.800\n of this giant neural network.\n\n1:16:06.800 --> 1:16:08.360\n You cannot just be an observer\n\n1:16:08.360 --> 1:16:09.720\n to do self supervised learning.\n\n1:16:09.720 --> 1:16:12.200\n You have to, well, I don't,\n\n1:16:12.200 --> 1:16:14.560\n self supervised learning is just a word,\n\n1:16:14.560 --> 1:16:16.760\n but I would, whatever this giant stack\n\n1:16:16.760 --> 1:16:19.640\n of a neural network that's automatically learning,\n\n1:16:19.640 --> 1:16:24.640\n it feels, my intuition is that you have to have a system,\n\n1:16:26.520 --> 1:16:30.220\n whether it's a physical robot or a digital robot,\n\n1:16:30.220 --> 1:16:32.360\n that's interacting with the world\n\n1:16:32.360 --> 1:16:35.960\n and doing so in a flawed way and improving over time\n\n1:16:35.960 --> 1:16:40.960\n in order to form the self supervised learning.\n\n1:16:41.520 --> 1:16:44.960\n Well, you can't just give it a giant sea of data.\n\n1:16:44.960 --> 1:16:47.120\n Okay, I agree and I disagree.\n\n1:16:47.120 --> 1:16:52.000\n I agree in the sense that I think, I agree in two ways.\n\n1:16:52.000 --> 1:16:54.240\n The first way I agree is that if you want,\n\n1:16:55.140 --> 1:16:57.480\n and you certainly need a causal model of the world\n\n1:16:57.480 --> 1:16:59.120\n that allows you to predict the consequences\n\n1:16:59.120 --> 1:17:01.280\n of your actions, to train that model,\n\n1:17:01.280 --> 1:17:02.760\n you need to take actions, right?\n\n1:17:02.760 --> 1:17:04.600\n You need to be able to act in a world\n\n1:17:04.600 --> 1:17:07.040\n and see the effect for you to be,\n\n1:17:07.040 --> 1:17:08.560\n to learn causal models of the world.\n\n1:17:08.560 --> 1:17:11.560\n So that's not obvious because you can observe others.\n\n1:17:11.560 --> 1:17:12.400\n You can observe others.\n\n1:17:12.400 --> 1:17:14.720\n And you can infer that they're similar to you\n\n1:17:14.720 --> 1:17:16.000\n and then you can learn from that.\n\n1:17:16.000 --> 1:17:18.400\n Yeah, but then you have to kind of hardwire that part,\n\n1:17:18.400 --> 1:17:19.880\n right, and then, you know, mirror neurons\n\n1:17:19.880 --> 1:17:20.720\n and all that stuff, right?\n\n1:17:20.720 --> 1:17:23.280\n So, and it's not clear to me\n\n1:17:23.280 --> 1:17:24.440\n how you would do this in a machine.\n\n1:17:24.440 --> 1:17:29.440\n So I think the action part would be necessary\n\n1:17:30.240 --> 1:17:32.620\n for having causal models of the world.\n\n1:17:32.620 --> 1:17:36.660\n The second reason it may be necessary,\n\n1:17:36.660 --> 1:17:37.860\n or at least more efficient,\n\n1:17:37.860 --> 1:17:41.700\n is that active learning basically, you know,\n\n1:17:41.700 --> 1:17:44.900\n goes for the jugular of what you don't know, right?\n\n1:17:44.900 --> 1:17:48.020\n Is, you know, obvious areas of uncertainty\n\n1:17:48.020 --> 1:17:52.940\n about your world and about how the world behaves.\n\n1:17:52.940 --> 1:17:56.220\n And you can resolve this uncertainty\n\n1:17:56.220 --> 1:17:58.980\n by systematic exploration of that part\n\n1:17:58.980 --> 1:18:00.300\n that you don't know.\n\n1:18:00.300 --> 1:18:01.700\n And if you know that you don't know,\n\n1:18:01.700 --> 1:18:03.020\n then, you know, it makes you curious.\n\n1:18:03.020 --> 1:18:05.620\n You kind of look into situations that,\n\n1:18:05.620 --> 1:18:09.260\n and, you know, across the animal world,\n\n1:18:09.260 --> 1:18:12.900\n different species have different levels of curiosity,\n\n1:18:12.900 --> 1:18:15.100\n right, depending on how they're built, right?\n\n1:18:15.100 --> 1:18:18.780\n So, you know, cats and rats are incredibly curious,\n\n1:18:18.780 --> 1:18:20.620\n dogs not so much, I mean, less.\n\n1:18:20.620 --> 1:18:22.140\n Yeah, so it could be useful\n\n1:18:22.140 --> 1:18:23.900\n to have that kind of curiosity.\n\n1:18:23.900 --> 1:18:24.740\n So it'd be useful,\n\n1:18:24.740 --> 1:18:26.980\n but curiosity just makes the process faster.\n\n1:18:26.980 --> 1:18:28.780\n It doesn't make the process exist.\n\n1:18:28.780 --> 1:18:33.780\n The, so what process, what learning process is it\n\n1:18:33.820 --> 1:18:37.780\n that active learning makes more efficient?\n\n1:18:37.780 --> 1:18:40.300\n And I'm asking that first question, you know,\n\n1:18:42.300 --> 1:18:43.940\n you know, we haven't answered that question yet.\n\n1:18:43.940 --> 1:18:45.860\n So, you know, I worry about active learning\n\n1:18:45.860 --> 1:18:47.300\n once this question is...\n\n1:18:47.300 --> 1:18:49.940\n So it's the more fundamental question to ask.\n\n1:18:49.940 --> 1:18:53.900\n And if active learning or interaction\n\n1:18:53.900 --> 1:18:56.220\n increases the efficiency of the learning,\n\n1:18:56.220 --> 1:18:59.700\n see, sometimes it becomes very different\n\n1:18:59.700 --> 1:19:03.700\n if the increase is several orders of magnitude, right?\n\n1:19:03.700 --> 1:19:04.540\n Like...\n\n1:19:04.540 --> 1:19:05.380\n That's true.\n\n1:19:05.380 --> 1:19:07.620\n But fundamentally it's still the same thing\n\n1:19:07.620 --> 1:19:10.700\n and building up the intuition about how to,\n\n1:19:10.700 --> 1:19:13.340\n in a self supervised way to construct background models,\n\n1:19:13.340 --> 1:19:18.180\n efficient or inefficient, is the core problem.\n\n1:19:18.180 --> 1:19:20.300\n What do you think about Yoshua Bengio's\n\n1:19:20.300 --> 1:19:22.380\n talking about consciousness\n\n1:19:22.380 --> 1:19:24.060\n and all of these kinds of concepts?\n\n1:19:24.060 --> 1:19:29.060\n Okay, I don't know what consciousness is, but...\n\n1:19:29.780 --> 1:19:31.500\n It's a good opener.\n\n1:19:31.500 --> 1:19:33.100\n And to some extent, a lot of the things\n\n1:19:33.100 --> 1:19:34.860\n that are said about consciousness\n\n1:19:34.860 --> 1:19:38.260\n remind me of the questions people were asking themselves\n\n1:19:38.260 --> 1:19:40.900\n in the 18th century or 17th century\n\n1:19:40.900 --> 1:19:44.620\n when they discovered that, you know, how the eye works\n\n1:19:44.620 --> 1:19:46.620\n and the fact that the image at the back of the eye\n\n1:19:46.620 --> 1:19:49.420\n was upside down, right?\n\n1:19:49.420 --> 1:19:50.260\n Because you have a lens.\n\n1:19:50.260 --> 1:19:54.140\n And so on your retina, the image that forms is an image\n\n1:19:54.140 --> 1:19:55.180\n of the world, but it's upside down.\n\n1:19:55.180 --> 1:19:57.820\n How is it that you see right side up?\n\n1:19:57.820 --> 1:20:00.100\n And, you know, with what we know today in science,\n\n1:20:00.100 --> 1:20:03.500\n you know, we realize this question doesn't make any sense\n\n1:20:03.500 --> 1:20:05.980\n or is kind of ridiculous in some way, right?\n\n1:20:05.980 --> 1:20:07.820\n So I think a lot of what is said about consciousness\n\n1:20:07.820 --> 1:20:08.660\n is of that nature.\n\n1:20:08.660 --> 1:20:10.620\n Now, that said, there is a lot of really smart people\n\n1:20:10.620 --> 1:20:13.460\n that for whom I have a lot of respect\n\n1:20:13.460 --> 1:20:14.700\n who are talking about this topic,\n\n1:20:14.700 --> 1:20:17.900\n people like David Chalmers, who is a colleague of mine at NYU.\n\n1:20:17.900 --> 1:20:22.900\n I have kind of an orthodox folk speculative hypothesis\n\n1:20:28.140 --> 1:20:29.180\n about consciousness.\n\n1:20:29.180 --> 1:20:32.020\n So we're talking about the study of a world model.\n\n1:20:32.020 --> 1:20:35.540\n And I think, you know, our entire prefrontal cortex\n\n1:20:35.540 --> 1:20:39.300\n basically is the engine for a world model.\n\n1:20:40.820 --> 1:20:44.580\n But when we are attending at a particular situation,\n\n1:20:44.580 --> 1:20:46.060\n we're focused on that situation.\n\n1:20:46.060 --> 1:20:48.540\n We basically cannot attend to anything else.\n\n1:20:48.540 --> 1:20:53.540\n And that seems to suggest that we basically have\n\n1:20:53.540 --> 1:20:58.420\n only one world model engine in our prefrontal cortex.\n\n1:20:59.780 --> 1:21:02.620\n That engine is configurable to the situation at hand.\n\n1:21:02.620 --> 1:21:04.660\n So we are building a box out of wood,\n\n1:21:04.660 --> 1:21:09.300\n or we are driving down the highway playing chess.\n\n1:21:09.300 --> 1:21:12.820\n We basically have a single model of the world\n\n1:21:12.820 --> 1:21:15.380\n that we configure into the situation at hand,\n\n1:21:15.380 --> 1:21:18.140\n which is why we can only attend to one task at a time.\n\n1:21:19.220 --> 1:21:21.660\n Now, if there is a task that we do repeatedly,\n\n1:21:22.860 --> 1:21:25.940\n it goes from the sort of deliberate reasoning\n\n1:21:25.940 --> 1:21:27.420\n using model of the world and prediction\n\n1:21:27.420 --> 1:21:29.300\n and perhaps something like model predictive control,\n\n1:21:29.300 --> 1:21:31.380\n which I was talking about earlier,\n\n1:21:31.380 --> 1:21:33.340\n to something that is more subconscious\n\n1:21:33.340 --> 1:21:34.380\n that becomes automatic.\n\n1:21:34.380 --> 1:21:35.940\n So I don't know if you've ever played\n\n1:21:35.940 --> 1:21:37.940\n against a chess grandmaster.\n\n1:21:39.180 --> 1:21:43.820\n I get wiped out in 10 plays, right?\n\n1:21:43.820 --> 1:21:48.700\n And I have to think about my move for like 15 minutes.\n\n1:21:50.140 --> 1:21:52.620\n And the person in front of me, the grandmaster,\n\n1:21:52.620 --> 1:21:55.220\n would just react within seconds, right?\n\n1:21:56.540 --> 1:21:58.580\n He doesn't need to think about it.\n\n1:21:58.580 --> 1:21:59.980\n That's become part of the subconscious\n\n1:21:59.980 --> 1:22:02.620\n because it's basically just pattern recognition\n\n1:22:02.620 --> 1:22:03.460\n at this point.\n\n1:22:04.740 --> 1:22:07.660\n Same, the first few hours you drive a car,\n\n1:22:07.660 --> 1:22:09.660\n you are really attentive, you can't do anything else.\n\n1:22:09.660 --> 1:22:13.460\n And then after 20, 30 hours of practice, 50 hours,\n\n1:22:13.460 --> 1:22:15.700\n the subconscious, you can talk to the person next to you,\n\n1:22:15.700 --> 1:22:17.100\n things like that, right?\n\n1:22:17.100 --> 1:22:19.060\n Unless the situation becomes unpredictable\n\n1:22:19.060 --> 1:22:21.060\n and then you have to stop talking.\n\n1:22:21.060 --> 1:22:23.820\n So that suggests you only have one model in your head.\n\n1:22:24.740 --> 1:22:27.860\n And it might suggest the idea that consciousness\n\n1:22:27.860 --> 1:22:29.780\n basically is the module that configures\n\n1:22:29.780 --> 1:22:31.980\n this world model of yours.\n\n1:22:31.980 --> 1:22:36.540\n You need to have some sort of executive kind of overseer\n\n1:22:36.540 --> 1:22:40.620\n that configures your world model for the situation at hand.\n\n1:22:40.620 --> 1:22:43.780\n And that leads to kind of the really curious concept\n\n1:22:43.780 --> 1:22:46.020\n that consciousness is not a consequence\n\n1:22:46.020 --> 1:22:47.660\n of the power of our minds,\n\n1:22:47.660 --> 1:22:49.940\n but of the limitation of our brains.\n\n1:22:49.940 --> 1:22:52.060\n That because we have only one world model,\n\n1:22:52.060 --> 1:22:53.660\n we have to be conscious.\n\n1:22:53.660 --> 1:22:55.220\n If we had as many world models\n\n1:22:55.220 --> 1:22:58.540\n as situations we encounter,\n\n1:22:58.540 --> 1:23:00.740\n then we could do all of them simultaneously\n\n1:23:00.740 --> 1:23:02.940\n and we wouldn't need this sort of executive control\n\n1:23:02.940 --> 1:23:04.540\n that we call consciousness.\n\n1:23:04.540 --> 1:23:05.380\n Yeah, interesting.\n\n1:23:05.380 --> 1:23:08.940\n And somehow maybe that executive controller,\n\n1:23:08.940 --> 1:23:10.980\n I mean, the hard problem of consciousness,\n\n1:23:10.980 --> 1:23:12.860\n there's some kind of chemicals in biology\n\n1:23:12.860 --> 1:23:15.020\n that's creating a feeling,\n\n1:23:15.020 --> 1:23:17.740\n like it feels to experience some of these things.\n\n1:23:18.780 --> 1:23:22.460\n That's kind of like the hard question is,\n\n1:23:22.460 --> 1:23:24.900\n what the heck is that and why is that useful?\n\n1:23:24.900 --> 1:23:26.180\n Maybe the more pragmatic question,\n\n1:23:26.180 --> 1:23:29.940\n why is it useful to feel like this is really you\n\n1:23:29.940 --> 1:23:33.340\n experiencing this versus just like information\n\n1:23:33.340 --> 1:23:34.380\n being processed?\n\n1:23:34.380 --> 1:23:39.020\n It could be just a very nice side effect\n\n1:23:39.020 --> 1:23:41.820\n of the way we evolved.\n\n1:23:41.820 --> 1:23:46.820\n That's just very useful to feel a sense of ownership\n\n1:23:48.620 --> 1:23:51.180\n to the decisions you make, to the perceptions you make,\n\n1:23:51.180 --> 1:23:53.180\n to the model you're trying to maintain.\n\n1:23:53.180 --> 1:23:56.260\n Like you own this thing and this is the only one you got\n\n1:23:56.260 --> 1:23:58.420\n and if you lose it, it's gonna really suck.\n\n1:23:58.420 --> 1:24:00.620\n And so you should really send the brain\n\n1:24:00.620 --> 1:24:02.300\n some signals about it.\n\n1:24:02.300 --> 1:24:06.860\n So what ideas do you believe might be true\n\n1:24:06.860 --> 1:24:10.100\n that most or at least many people disagree with?\n\n1:24:11.260 --> 1:24:13.740\n Let's say in the space of machine learning.\n\n1:24:13.740 --> 1:24:14.940\n Well, it depends who you talk about,\n\n1:24:14.940 --> 1:24:19.940\n but I think, so certainly there is a bunch of people\n\n1:24:20.100 --> 1:24:21.100\n who are nativists, right?\n\n1:24:21.100 --> 1:24:23.300\n Who think that a lot of the basic things about the world\n\n1:24:23.300 --> 1:24:25.300\n are kind of hardwired in our minds.\n\n1:24:26.420 --> 1:24:28.860\n Things like the world is three dimensional, for example,\n\n1:24:28.860 --> 1:24:30.420\n is that hardwired?\n\n1:24:30.420 --> 1:24:32.660\n Things like object permanence,\n\n1:24:32.660 --> 1:24:35.140\n is this something that we learn\n\n1:24:35.140 --> 1:24:37.500\n before the age of three months or so?\n\n1:24:37.500 --> 1:24:39.340\n Or are we born with it?\n\n1:24:39.340 --> 1:24:42.380\n And there are very wide disagreements\n\n1:24:42.380 --> 1:24:46.580\n among the cognitive scientists for this.\n\n1:24:46.580 --> 1:24:48.980\n I think those things are actually very simple to learn.\n\n1:24:50.580 --> 1:24:54.220\n Is it the case that the oriented edge detectors in V1\n\n1:24:54.220 --> 1:24:56.180\n are learned or are they hardwired?\n\n1:24:56.180 --> 1:24:57.260\n I think they are learned.\n\n1:24:57.260 --> 1:24:58.580\n They might be learned before both\n\n1:24:58.580 --> 1:25:00.620\n because it's really easy to generate signals\n\n1:25:00.620 --> 1:25:03.220\n from the retina that actually will train edge detectors.\n\n1:25:04.620 --> 1:25:06.740\n And again, those are things that can be learned\n\n1:25:06.740 --> 1:25:09.580\n within minutes of opening your eyes, right?\n\n1:25:09.580 --> 1:25:12.660\n I mean, since the 1990s,\n\n1:25:12.660 --> 1:25:15.460\n we have algorithms that can learn oriented edge detectors\n\n1:25:15.460 --> 1:25:16.940\n completely unsupervised\n\n1:25:16.940 --> 1:25:19.060\n with the equivalent of a few minutes of real time.\n\n1:25:19.060 --> 1:25:21.540\n So those things have to be learned.\n\n1:25:22.660 --> 1:25:24.580\n And there's also those MIT experiments\n\n1:25:24.580 --> 1:25:27.820\n where you kind of plug the optical nerve\n\n1:25:27.820 --> 1:25:30.300\n on the auditory cortex of a baby ferret, right?\n\n1:25:30.300 --> 1:25:31.300\n And that auditory cortex\n\n1:25:31.300 --> 1:25:33.420\n becomes a visual cortex essentially.\n\n1:25:33.420 --> 1:25:37.980\n So clearly there's learning taking place there.\n\n1:25:37.980 --> 1:25:41.340\n So I think a lot of what people think are so basic\n\n1:25:41.340 --> 1:25:43.180\n that they need to be hardwired,\n\n1:25:43.180 --> 1:25:44.420\n I think a lot of those things are learned\n\n1:25:44.420 --> 1:25:46.260\n because they are easy to learn.\n\n1:25:46.260 --> 1:25:49.980\n So you put a lot of value in the power of learning.\n\n1:25:49.980 --> 1:25:53.340\n What kind of things do you suspect might not be learned?\n\n1:25:53.340 --> 1:25:56.060\n Is there something that could not be learned?\n\n1:25:56.060 --> 1:25:59.820\n So your intrinsic drives are not learned.\n\n1:25:59.820 --> 1:26:03.460\n There are the things that make humans human\n\n1:26:03.460 --> 1:26:07.460\n or make cats different from dogs, right?\n\n1:26:07.460 --> 1:26:10.060\n It's the basic drives that are kind of hardwired\n\n1:26:10.060 --> 1:26:11.940\n in our basal ganglia.\n\n1:26:13.100 --> 1:26:14.060\n I mean, there are people who are working\n\n1:26:14.060 --> 1:26:16.380\n on this kind of stuff that's called intrinsic motivation\n\n1:26:16.380 --> 1:26:18.220\n in the context of reinforcement learning.\n\n1:26:18.220 --> 1:26:20.100\n So these are objective functions\n\n1:26:20.100 --> 1:26:23.100\n where the reward doesn't come from the external world.\n\n1:26:23.100 --> 1:26:24.660\n It's computed by your own brain.\n\n1:26:24.660 --> 1:26:28.140\n Your own brain computes whether you're happy or not, right?\n\n1:26:28.140 --> 1:26:32.540\n It measures your degree of comfort or in comfort.\n\n1:26:33.460 --> 1:26:36.100\n And because it's your brain computing this,\n\n1:26:36.100 --> 1:26:37.780\n presumably it knows also how to estimate\n\n1:26:37.780 --> 1:26:38.780\n gradients of this, right?\n\n1:26:38.780 --> 1:26:43.780\n So it's easier to learn when your objective is intrinsic.\n\n1:26:47.100 --> 1:26:48.780\n So that has to be hardwired.\n\n1:26:50.100 --> 1:26:53.460\n The critic that makes longterm prediction of the outcome,\n\n1:26:53.460 --> 1:26:56.780\n which is the eventual result of this, that's learned.\n\n1:26:57.860 --> 1:26:59.060\n And perception is learned\n\n1:26:59.060 --> 1:27:01.260\n and your model of the world is learned.\n\n1:27:01.260 --> 1:27:04.260\n But let me take an example of why the critic,\n\n1:27:04.260 --> 1:27:06.860\n I mean, an example of how the critic may be learned, right?\n\n1:27:06.860 --> 1:27:11.220\n If I come to you, I reach across the table\n\n1:27:11.220 --> 1:27:13.380\n and I pinch your arm, right?\n\n1:27:13.380 --> 1:27:15.060\n Complete surprise for you.\n\n1:27:15.060 --> 1:27:16.260\n You would not have expected this from me.\n\n1:27:16.260 --> 1:27:18.100\n I was expecting that the whole time, but yes, right.\n\n1:27:18.100 --> 1:27:20.420\n Let's say for the sake of the story, yes.\n\n1:27:20.420 --> 1:27:24.980\n So, okay, your basal ganglia is gonna light up\n\n1:27:24.980 --> 1:27:26.820\n because it's gonna hurt, right?\n\n1:27:28.500 --> 1:27:31.140\n And now your model of the world includes the fact that\n\n1:27:31.140 --> 1:27:34.820\n I may pinch you if I approach my...\n\n1:27:34.820 --> 1:27:36.220\n Don't trust humans.\n\n1:27:36.220 --> 1:27:37.860\n Right, my hand to your arm.\n\n1:27:37.860 --> 1:27:40.020\n So if I try again, you're gonna recoil.\n\n1:27:40.020 --> 1:27:44.060\n And that's your critic, your predictive,\n\n1:27:44.060 --> 1:27:50.660\n your predictor of your ultimate pain system\n\n1:27:50.660 --> 1:27:52.380\n that predicts that something bad is gonna happen\n\n1:27:52.380 --> 1:27:53.860\n and you recoil to avoid it.\n\n1:27:53.860 --> 1:27:55.260\n So even that can be learned.\n\n1:27:55.260 --> 1:27:56.700\n That is learned, definitely.\n\n1:27:56.700 --> 1:28:00.700\n This is what allows you also to define some goals, right?\n\n1:28:00.700 --> 1:28:04.540\n So the fact that you're a school child,\n\n1:28:04.540 --> 1:28:06.780\n you wake up in the morning and you go to school\n\n1:28:06.780 --> 1:28:12.060\n and it's not because you necessarily like waking up early\n\n1:28:12.060 --> 1:28:12.900\n and going to school,\n\n1:28:12.900 --> 1:28:14.620\n but you know that there is a long term objective\n\n1:28:14.620 --> 1:28:15.820\n you're trying to optimize.\n\n1:28:15.820 --> 1:28:18.540\n So Ernest Becker, I'm not sure if you're familiar with him,\n\n1:28:18.540 --> 1:28:20.900\n the philosopher, he wrote the book Denial of Death\n\n1:28:20.900 --> 1:28:23.420\n and his idea is that one of the core motivations\n\n1:28:23.420 --> 1:28:27.220\n of human beings is our terror of death, our fear of death.\n\n1:28:27.220 --> 1:28:28.900\n That's what makes us unique from cats.\n\n1:28:28.900 --> 1:28:30.500\n Cats are just surviving.\n\n1:28:30.500 --> 1:28:35.500\n They do not have a deep, like a cognizance introspection\n\n1:28:37.540 --> 1:28:41.740\n that over the horizon is the end.\n\n1:28:41.740 --> 1:28:43.060\n And then he says that, I mean,\n\n1:28:43.060 --> 1:28:44.420\n there's a terror management theory\n\n1:28:44.420 --> 1:28:46.260\n that just all these psychological experiments\n\n1:28:46.260 --> 1:28:50.020\n that show basically this idea\n\n1:28:50.020 --> 1:28:54.380\n that all of human civilization, everything we create\n\n1:28:54.380 --> 1:28:58.820\n is kind of trying to forget if even for a brief moment\n\n1:28:58.820 --> 1:29:00.660\n that we're going to die.\n\n1:29:00.660 --> 1:29:03.780\n When do you think humans understand\n\n1:29:03.780 --> 1:29:04.900\n that they're going to die?\n\n1:29:04.900 --> 1:29:07.580\n Is it learned early on also?\n\n1:29:07.580 --> 1:29:11.260\n I don't know at what point.\n\n1:29:11.260 --> 1:29:13.460\n I mean, it's a question like at what point\n\n1:29:13.460 --> 1:29:16.420\n do you realize that what death really is?\n\n1:29:16.420 --> 1:29:18.180\n And I think most people don't actually realize\n\n1:29:18.180 --> 1:29:19.220\n what death is, right?\n\n1:29:19.220 --> 1:29:20.940\n I mean, most people believe that you go to heaven\n\n1:29:20.940 --> 1:29:21.860\n or something, right?\n\n1:29:21.860 --> 1:29:25.580\n So to push back on that, what Ernest Becker says\n\n1:29:25.580 --> 1:29:29.300\n and Sheldon Solomon, all of those folks,\n\n1:29:29.300 --> 1:29:31.620\n and I find those ideas a little bit compelling\n\n1:29:31.620 --> 1:29:34.100\n is that there is moments in life, early in life,\n\n1:29:34.100 --> 1:29:36.540\n a lot of this fun happens early in life\n\n1:29:36.540 --> 1:29:41.540\n when you do deeply experience\n\n1:29:41.620 --> 1:29:43.540\n the terror of this realization.\n\n1:29:43.540 --> 1:29:45.980\n And all the things you think about about religion,\n\n1:29:45.980 --> 1:29:48.420\n all those kinds of things that we kind of think about\n\n1:29:48.420 --> 1:29:50.660\n more like teenage years and later,\n\n1:29:50.660 --> 1:29:52.100\n we're talking about way earlier.\n\n1:29:52.100 --> 1:29:53.220\n No, it was like seven or eight years,\n\n1:29:53.220 --> 1:29:54.060\n something like that, yeah.\n\n1:29:54.060 --> 1:29:59.060\n You realize, holy crap, this is like the mystery,\n\n1:29:59.660 --> 1:30:03.220\n the terror, like it's almost like you're a little prey,\n\n1:30:03.220 --> 1:30:05.340\n a little baby deer sitting in the darkness\n\n1:30:05.340 --> 1:30:08.060\n of the jungle or the woods looking all around you.\n\n1:30:08.060 --> 1:30:09.540\n There's darkness full of terror.\n\n1:30:09.540 --> 1:30:12.140\n I mean, that realization says, okay,\n\n1:30:12.140 --> 1:30:14.460\n I'm gonna go back in the comfort of my mind\n\n1:30:14.460 --> 1:30:16.780\n where there is a deep meaning,\n\n1:30:16.780 --> 1:30:20.420\n where there is maybe like pretend I'm immortal\n\n1:30:20.420 --> 1:30:25.060\n in however way, however kind of idea I can construct\n\n1:30:25.060 --> 1:30:27.180\n to help me understand that I'm immortal.\n\n1:30:27.180 --> 1:30:28.660\n Religion helps with that.\n\n1:30:28.660 --> 1:30:31.440\n You can delude yourself in all kinds of ways,\n\n1:30:31.440 --> 1:30:34.220\n like lose yourself in the busyness of each day,\n\n1:30:34.220 --> 1:30:36.380\n have little goals in mind, all those kinds of things\n\n1:30:36.380 --> 1:30:38.100\n to think that it's gonna go on forever.\n\n1:30:38.100 --> 1:30:40.740\n And you kind of know you're gonna die, yeah,\n\n1:30:40.740 --> 1:30:43.820\n and it's gonna be sad, but you don't really understand\n\n1:30:43.820 --> 1:30:45.140\n that you're going to die.\n\n1:30:45.140 --> 1:30:46.460\n And so that's their idea.\n\n1:30:46.460 --> 1:30:49.940\n And I find that compelling because it does seem\n\n1:30:49.940 --> 1:30:52.820\n to be a core unique aspect of human nature\n\n1:30:52.820 --> 1:30:55.180\n that we're able to think that we're going,\n\n1:30:55.180 --> 1:30:59.540\n we're able to really understand that this life is finite.\n\n1:30:59.540 --> 1:31:00.580\n That seems important.\n\n1:31:00.580 --> 1:31:02.260\n There's a bunch of different things there.\n\n1:31:02.260 --> 1:31:04.300\n So first of all, I don't think there is a qualitative\n\n1:31:04.300 --> 1:31:07.520\n difference between us and cats in the term.\n\n1:31:07.520 --> 1:31:10.180\n I think the difference is that we just have a better\n\n1:31:10.180 --> 1:31:14.740\n long term ability to predict in the long term.\n\n1:31:14.740 --> 1:31:17.380\n And so we have a better understanding of how the world works.\n\n1:31:17.380 --> 1:31:20.180\n So we have better understanding of finiteness of life\n\n1:31:20.180 --> 1:31:21.020\n and things like that.\n\n1:31:21.020 --> 1:31:23.540\n So we have a better planning engine than cats?\n\n1:31:23.540 --> 1:31:24.500\n Yeah.\n\n1:31:24.500 --> 1:31:25.340\n Okay.\n\n1:31:25.340 --> 1:31:28.780\n But what's the motivation for planning that far?\n\n1:31:28.780 --> 1:31:30.540\n Well, I think it's just a side effect of the fact\n\n1:31:30.540 --> 1:31:32.340\n that we have just a better planning engine\n\n1:31:32.340 --> 1:31:34.780\n because it makes us, as I said,\n\n1:31:34.780 --> 1:31:37.420\n the essence of intelligence is the ability to predict.\n\n1:31:37.420 --> 1:31:41.220\n And so the, because we're smarter as a side effect,\n\n1:31:41.220 --> 1:31:43.500\n we also have this ability to kind of make predictions\n\n1:31:43.500 --> 1:31:47.580\n about our own future existence or lack thereof.\n\n1:31:47.580 --> 1:31:48.500\n Okay.\n\n1:31:48.500 --> 1:31:50.540\n You say religion helps with that.\n\n1:31:50.540 --> 1:31:53.000\n I think religion hurts actually.\n\n1:31:53.000 --> 1:31:55.000\n It makes people worry about like,\n\n1:31:55.000 --> 1:31:57.500\n what's going to happen after their death, et cetera.\n\n1:31:57.500 --> 1:32:00.820\n If you believe that, you just don't exist after death.\n\n1:32:00.820 --> 1:32:02.940\n Like, it solves completely the problem, at least.\n\n1:32:02.940 --> 1:32:04.940\n You're saying if you don't believe in God,\n\n1:32:04.940 --> 1:32:07.220\n you don't worry about what happens after death?\n\n1:32:07.220 --> 1:32:08.260\n Yeah.\n\n1:32:08.260 --> 1:32:09.100\n I don't know.\n\n1:32:09.100 --> 1:32:11.900\n You only worry about this life\n\n1:32:11.900 --> 1:32:14.220\n because that's the only one you have.\n\n1:32:14.220 --> 1:32:16.140\n I think it's, well, I don't know.\n\n1:32:16.140 --> 1:32:17.740\n If I were to say what Ernest Becker says,\n\n1:32:17.740 --> 1:32:22.140\n and obviously I agree with him more than not,\n\n1:32:22.140 --> 1:32:26.160\n is you do deeply worry.\n\n1:32:26.160 --> 1:32:27.900\n If you believe there's no God,\n\n1:32:27.900 --> 1:32:31.780\n there's still a deep worry of the mystery of it all.\n\n1:32:31.780 --> 1:32:35.700\n Like, how does that make any sense that it just ends?\n\n1:32:35.700 --> 1:32:39.740\n I don't think we can truly understand that this ride,\n\n1:32:39.740 --> 1:32:41.900\n I mean, so much of our life, the consciousness,\n\n1:32:41.900 --> 1:32:46.220\n the ego is invested in this being.\n\n1:32:46.220 --> 1:32:47.580\n And then...\n\n1:32:47.580 --> 1:32:51.560\n Science keeps bringing humanity down from its pedestal.\n\n1:32:51.560 --> 1:32:54.740\n And that's just another example of it.\n\n1:32:54.740 --> 1:32:57.820\n That's wonderful, but for us individual humans,\n\n1:32:57.820 --> 1:33:00.300\n we don't like to be brought down from a pedestal.\n\n1:33:00.300 --> 1:33:03.580\n You're saying like, but see, you're fine with it because,\n\n1:33:03.580 --> 1:33:06.340\n well, so what Ernest Becker would say is you're fine with it\n\n1:33:06.340 --> 1:33:08.580\n because there's just a more peaceful existence for you,\n\n1:33:08.580 --> 1:33:09.580\n but you're not really fine.\n\n1:33:09.580 --> 1:33:10.820\n You're hiding from it.\n\n1:33:10.820 --> 1:33:12.780\n In fact, some of the people that experience\n\n1:33:12.780 --> 1:33:16.700\n the deepest trauma earlier in life,\n\n1:33:16.700 --> 1:33:19.580\n they often, before they seek extensive therapy,\n\n1:33:19.580 --> 1:33:21.060\n will say that I'm fine.\n\n1:33:21.060 --> 1:33:23.460\n It's like when you talk to people who are truly angry,\n\n1:33:23.460 --> 1:33:25.380\n how are you doing, I'm fine.\n\n1:33:25.380 --> 1:33:27.780\n The question is, what's going on?\n\n1:33:27.780 --> 1:33:29.140\n Now I had a near death experience.\n\n1:33:29.140 --> 1:33:33.580\n I had a very bad motorbike accident when I was 17.\n\n1:33:33.580 --> 1:33:36.920\n So, but that didn't have any impact\n\n1:33:36.920 --> 1:33:40.420\n on my reflection on that topic.\n\n1:33:40.420 --> 1:33:43.100\n So I'm basically just playing a bit of devil's advocate,\n\n1:33:43.100 --> 1:33:45.820\n pushing back on wondering,\n\n1:33:45.820 --> 1:33:47.540\n is it truly possible to accept death?\n\n1:33:47.540 --> 1:33:49.340\n And the flip side, that's more interesting,\n\n1:33:49.340 --> 1:33:53.060\n I think for AI and robotics is how important\n\n1:33:53.060 --> 1:33:57.180\n is it to have this as one of the suite of motivations\n\n1:33:57.180 --> 1:34:02.180\n is to not just avoid falling off the roof\n\n1:34:03.320 --> 1:34:08.320\n or something like that, but ponder the end of the ride.\n\n1:34:10.180 --> 1:34:14.820\n If you listen to the stoics, it's a great motivator.\n\n1:34:14.820 --> 1:34:16.900\n It adds a sense of urgency.\n\n1:34:16.900 --> 1:34:21.420\n So maybe to truly fear death or be cognizant of it\n\n1:34:21.420 --> 1:34:26.420\n might give a deeper meaning and urgency to the moment\n\n1:34:26.460 --> 1:34:28.300\n to live fully.\n\n1:34:30.460 --> 1:34:32.220\n Maybe I don't disagree with that.\n\n1:34:32.220 --> 1:34:34.280\n I mean, I think what motivates me here\n\n1:34:34.280 --> 1:34:38.980\n is knowing more about human nature.\n\n1:34:38.980 --> 1:34:41.760\n I mean, I think human nature and human intelligence\n\n1:34:41.760 --> 1:34:42.600\n is a big mystery.\n\n1:34:42.600 --> 1:34:43.780\n It's a scientific mystery\n\n1:34:45.020 --> 1:34:48.580\n in addition to philosophical and et cetera,\n\n1:34:48.580 --> 1:34:50.700\n but I'm a true believer in science.\n\n1:34:50.700 --> 1:34:55.700\n So, and I do have kind of a belief\n\n1:34:56.180 --> 1:34:59.940\n that for complex systems like the brain and the mind,\n\n1:34:59.940 --> 1:35:04.460\n the way to understand it is to try to reproduce it\n\n1:35:04.460 --> 1:35:07.060\n with artifacts that you build\n\n1:35:07.060 --> 1:35:08.900\n because you know what's essential to it\n\n1:35:08.900 --> 1:35:10.180\n when you try to build it.\n\n1:35:10.180 --> 1:35:12.660\n The same way I've used this analogy before with you,\n\n1:35:12.660 --> 1:35:15.780\n I believe, the same way we only started\n\n1:35:15.780 --> 1:35:18.140\n to understand aerodynamics\n\n1:35:18.140 --> 1:35:19.300\n when we started building airplanes\n\n1:35:19.300 --> 1:35:21.500\n and that helped us understand how birds fly.\n\n1:35:22.380 --> 1:35:25.460\n So I think there's kind of a similar process here\n\n1:35:25.460 --> 1:35:29.660\n where we don't have a full theory of intelligence,\n\n1:35:29.660 --> 1:35:31.760\n but building intelligent artifacts\n\n1:35:31.760 --> 1:35:35.480\n will help us perhaps develop some underlying theory\n\n1:35:35.480 --> 1:35:39.380\n that encompasses not just artificial implements,\n\n1:35:39.380 --> 1:35:43.860\n but also human and biological intelligence in general.\n\n1:35:43.860 --> 1:35:46.080\n So you're an interesting person to ask this question\n\n1:35:46.080 --> 1:35:49.400\n about sort of all kinds of different other\n\n1:35:49.400 --> 1:35:53.100\n intelligent entities or intelligences.\n\n1:35:53.100 --> 1:35:56.300\n What are your thoughts about kind of like the touring\n\n1:35:56.300 --> 1:35:58.000\n or the Chinese room question?\n\n1:35:59.240 --> 1:36:02.920\n If we create an AI system that exhibits\n\n1:36:02.920 --> 1:36:06.360\n a lot of properties of intelligence and consciousness,\n\n1:36:07.520 --> 1:36:10.220\n how comfortable are you thinking of that entity\n\n1:36:10.220 --> 1:36:12.340\n as intelligent or conscious?\n\n1:36:12.340 --> 1:36:14.580\n So you're trying to build now systems\n\n1:36:14.580 --> 1:36:16.420\n that have intelligence and there's metrics\n\n1:36:16.420 --> 1:36:21.300\n about their performance, but that metric is external.\n\n1:36:22.740 --> 1:36:26.420\n So how are you, are you okay calling a thing intelligent\n\n1:36:26.420 --> 1:36:29.020\n or are you going to be like most humans\n\n1:36:29.020 --> 1:36:32.700\n and be once again unhappy to be brought down\n\n1:36:32.700 --> 1:36:34.920\n from a pedestal of consciousness slash intelligence?\n\n1:36:34.920 --> 1:36:39.500\n No, I'll be very happy to understand\n\n1:36:39.500 --> 1:36:44.500\n more about human nature, human mind and human intelligence\n\n1:36:45.540 --> 1:36:47.240\n through the construction of machines\n\n1:36:47.240 --> 1:36:50.600\n that have similar abilities.\n\n1:36:50.600 --> 1:36:54.520\n And if a consequence of this is to bring down humanity\n\n1:36:54.520 --> 1:36:58.020\n one notch down from its already low pedestal,\n\n1:36:58.020 --> 1:36:59.140\n I'm just fine with it.\n\n1:36:59.140 --> 1:37:01.360\n That's just the reality of life.\n\n1:37:01.360 --> 1:37:02.460\n So I'm fine with that.\n\n1:37:02.460 --> 1:37:05.020\n Now you were asking me about things that,\n\n1:37:05.020 --> 1:37:07.940\n opinions I have that a lot of people may disagree with.\n\n1:37:07.940 --> 1:37:12.780\n I think if we think about the design\n\n1:37:12.780 --> 1:37:14.300\n of autonomous intelligence systems,\n\n1:37:14.300 --> 1:37:16.860\n so assuming that we are somewhat successful\n\n1:37:16.860 --> 1:37:20.060\n at some level of getting machines to learn models\n\n1:37:20.060 --> 1:37:22.620\n of the world, predictive models of the world,\n\n1:37:22.620 --> 1:37:25.860\n we build intrinsic motivation objective functions\n\n1:37:25.860 --> 1:37:28.340\n to drive the behavior of that system.\n\n1:37:28.340 --> 1:37:30.100\n The system also has perception modules\n\n1:37:30.100 --> 1:37:32.820\n that allows it to estimate the state of the world\n\n1:37:32.820 --> 1:37:34.640\n and then have some way of figuring out\n\n1:37:34.640 --> 1:37:36.180\n the sequence of actions that,\n\n1:37:36.180 --> 1:37:37.880\n to optimize a particular objective.\n\n1:37:39.300 --> 1:37:42.740\n If it has a critic of the type that I was describing before,\n\n1:37:42.740 --> 1:37:44.600\n the thing that makes you recoil your arm\n\n1:37:44.600 --> 1:37:46.300\n the second time I try to pinch you,\n\n1:37:48.620 --> 1:37:51.700\n intelligent autonomous machine will have emotions.\n\n1:37:51.700 --> 1:37:54.060\n I think emotions are an integral part\n\n1:37:54.060 --> 1:37:56.400\n of autonomous intelligence.\n\n1:37:56.400 --> 1:37:59.020\n If you have an intelligent system\n\n1:37:59.020 --> 1:38:03.160\n that is driven by intrinsic motivation, by objectives,\n\n1:38:03.160 --> 1:38:07.680\n if it has a critic that allows it to predict in advance\n\n1:38:07.680 --> 1:38:11.040\n whether the outcome of a situation is gonna be good or bad,\n\n1:38:11.040 --> 1:38:13.480\n is going to have emotions, it's gonna have fear.\n\n1:38:13.480 --> 1:38:14.320\n Yes.\n\n1:38:14.320 --> 1:38:18.180\n When it predicts that the outcome is gonna be bad\n\n1:38:18.180 --> 1:38:20.720\n and something to avoid is gonna have elation\n\n1:38:20.720 --> 1:38:22.520\n when it predicts it's gonna be good.\n\n1:38:24.280 --> 1:38:27.600\n If it has drives to relate with humans,\n\n1:38:28.680 --> 1:38:30.660\n in some ways the way humans have,\n\n1:38:30.660 --> 1:38:34.460\n it's gonna be social, right?\n\n1:38:34.460 --> 1:38:36.460\n And so it's gonna have emotions\n\n1:38:36.460 --> 1:38:38.620\n about attachment and things of that type.\n\n1:38:38.620 --> 1:38:43.620\n So I think the sort of sci fi thing\n\n1:38:44.700 --> 1:38:46.900\n where you see commander data,\n\n1:38:46.900 --> 1:38:50.100\n like having an emotion chip that you can turn off, right?\n\n1:38:50.100 --> 1:38:51.700\n I think that's ridiculous.\n\n1:38:51.700 --> 1:38:53.380\n So, I mean, here's the difficult\n\n1:38:53.380 --> 1:38:57.820\n philosophical social question.\n\n1:38:57.820 --> 1:39:01.020\n Do you think there will be a time like a civil rights\n\n1:39:01.020 --> 1:39:05.180\n movement for robots where, okay, forget the movement,\n\n1:39:05.180 --> 1:39:07.860\n but a discussion like the Supreme Court\n\n1:39:09.740 --> 1:39:12.880\n that particular kinds of robots,\n\n1:39:12.880 --> 1:39:14.860\n you know, particular kinds of systems\n\n1:39:16.100 --> 1:39:18.300\n deserve the same rights as humans\n\n1:39:18.300 --> 1:39:21.660\n because they can suffer just as humans can,\n\n1:39:22.900 --> 1:39:24.740\n all those kinds of things.\n\n1:39:24.740 --> 1:39:27.340\n Well, perhaps, perhaps not.\n\n1:39:27.340 --> 1:39:29.580\n Like imagine that humans were,\n\n1:39:29.580 --> 1:39:33.740\n that you could, you know, die and be restored.\n\n1:39:33.740 --> 1:39:35.500\n Like, you know, you could be sort of, you know,\n\n1:39:35.500 --> 1:39:37.540\n be 3D reprinted and, you know,\n\n1:39:37.540 --> 1:39:40.740\n your brain could be reconstructed in its finest details.\n\n1:39:40.740 --> 1:39:43.140\n Our ideas of rights will change in that case.\n\n1:39:43.140 --> 1:39:44.560\n If you can always just,\n\n1:39:45.900 --> 1:39:48.220\n there's always a backup you could always restore.\n\n1:39:48.220 --> 1:39:50.260\n Maybe like the importance of murder\n\n1:39:50.260 --> 1:39:51.980\n will go down one notch.\n\n1:39:51.980 --> 1:39:52.820\n That's right.\n\n1:39:52.820 --> 1:39:57.580\n But also your desire to do dangerous things,\n\n1:39:57.580 --> 1:40:02.000\n like, you know, skydiving or, you know,\n\n1:40:03.300 --> 1:40:05.660\n or, you know, race car driving,\n\n1:40:05.660 --> 1:40:07.300\n you know, car racing or that kind of stuff,\n\n1:40:07.300 --> 1:40:09.460\n you know, would probably increase\n\n1:40:09.460 --> 1:40:11.140\n or, you know, aeroplanes, aerobatics\n\n1:40:11.140 --> 1:40:12.380\n or that kind of stuff, right?\n\n1:40:12.380 --> 1:40:14.180\n It would be fine to do a lot of those things\n\n1:40:14.180 --> 1:40:17.500\n or explore, you know, dangerous areas and things like that.\n\n1:40:17.500 --> 1:40:19.220\n It would kind of change your relationship.\n\n1:40:19.220 --> 1:40:22.420\n So now it's very likely that robots would be like that\n\n1:40:22.420 --> 1:40:27.060\n because, you know, they'll be based on perhaps technology\n\n1:40:27.060 --> 1:40:30.140\n that is somewhat similar to today's technology\n\n1:40:30.140 --> 1:40:32.260\n and you can always have a backup.\n\n1:40:32.260 --> 1:40:35.700\n So it's possible, I don't know if you like video games,\n\n1:40:35.700 --> 1:40:39.340\n but there's a game called Diablo and...\n\n1:40:39.340 --> 1:40:41.860\n Oh, my sons are huge fans of this.\n\n1:40:41.860 --> 1:40:42.700\n Yes.\n\n1:40:44.100 --> 1:40:47.060\n In fact, they made a game that's inspired by it.\n\n1:40:47.060 --> 1:40:47.900\n Awesome.\n\n1:40:47.900 --> 1:40:49.260\n Like built a game?\n\n1:40:49.260 --> 1:40:52.660\n My three sons have a game design studio between them, yeah.\n\n1:40:52.660 --> 1:40:53.480\n That's awesome.\n\n1:40:53.480 --> 1:40:54.320\n They came out with a game.\n\n1:40:54.320 --> 1:40:55.160\n They just came out with a game.\n\n1:40:55.160 --> 1:40:56.860\n Last year, no, this was last year,\n\n1:40:56.860 --> 1:40:58.180\n early last year, about a year ago.\n\n1:40:58.180 --> 1:40:59.020\n That's awesome.\n\n1:40:59.020 --> 1:41:02.020\n But so in Diablo, there's something called hardcore mode,\n\n1:41:02.020 --> 1:41:05.480\n which if you die, there's no, you're gone.\n\n1:41:05.480 --> 1:41:06.320\n Right.\n\n1:41:06.320 --> 1:41:07.140\n That's it.\n\n1:41:07.140 --> 1:41:09.580\n And so it's possible with AI systems\n\n1:41:10.620 --> 1:41:13.260\n for them to be able to operate successfully\n\n1:41:13.260 --> 1:41:15.580\n and for us to treat them in a certain way\n\n1:41:15.580 --> 1:41:18.400\n because they have to be integrated in human society,\n\n1:41:18.400 --> 1:41:22.020\n they have to be able to die, no copies allowed.\n\n1:41:22.020 --> 1:41:23.860\n In fact, copying is illegal.\n\n1:41:23.860 --> 1:41:25.260\n It's possible with humans as well,\n\n1:41:25.260 --> 1:41:28.580\n like cloning will be illegal, even when it's possible.\n\n1:41:28.580 --> 1:41:29.960\n But cloning is not copying, right?\n\n1:41:29.960 --> 1:41:33.060\n I mean, you don't reproduce the mind of the person\n\n1:41:33.060 --> 1:41:33.940\n and the experience.\n\n1:41:33.940 --> 1:41:34.760\n Right.\n\n1:41:34.760 --> 1:41:36.420\n It's just a delayed twin, so.\n\n1:41:36.420 --> 1:41:39.060\n But then it's, but we were talking about with computers\n\n1:41:39.060 --> 1:41:40.580\n that you will be able to copy.\n\n1:41:40.580 --> 1:41:41.420\n Right.\n\n1:41:41.420 --> 1:41:42.660\n You will be able to perfectly save,\n\n1:41:42.660 --> 1:41:46.640\n pickle the mind state.\n\n1:41:46.640 --> 1:41:49.660\n And it's possible that that will be illegal\n\n1:41:49.660 --> 1:41:52.320\n because that goes against,\n\n1:41:53.300 --> 1:41:55.980\n that will destroy the motivation of the system.\n\n1:41:55.980 --> 1:42:00.240\n Okay, so let's say you have a domestic robot, okay?\n\n1:42:00.240 --> 1:42:01.380\n Sometime in the future.\n\n1:42:01.380 --> 1:42:02.460\n Yes.\n\n1:42:02.460 --> 1:42:06.100\n And the domestic robot comes to you kind of\n\n1:42:06.100 --> 1:42:08.700\n somewhat pre trained, it can do a bunch of things,\n\n1:42:08.700 --> 1:42:10.580\n but it has a particular personality\n\n1:42:10.580 --> 1:42:12.300\n that makes it slightly different from the other robots\n\n1:42:12.300 --> 1:42:14.220\n because that makes them more interesting.\n\n1:42:14.220 --> 1:42:18.060\n And then because it's lived with you for five years,\n\n1:42:18.060 --> 1:42:21.900\n you've grown some attachment to it and vice versa,\n\n1:42:21.900 --> 1:42:24.380\n and it's learned a lot about you.\n\n1:42:24.380 --> 1:42:25.900\n Or maybe it's not a real household robot.\n\n1:42:25.900 --> 1:42:29.380\n Maybe it's a virtual assistant that lives in your,\n\n1:42:29.380 --> 1:42:32.580\n you know, augmented reality glasses or whatever, right?\n\n1:42:32.580 --> 1:42:35.060\n You know, the horror movie type thing, right?\n\n1:42:36.680 --> 1:42:39.620\n And that system to some extent,\n\n1:42:39.620 --> 1:42:43.900\n the intelligence in that system is a bit like your child\n\n1:42:43.900 --> 1:42:47.100\n or maybe your PhD student in the sense that\n\n1:42:47.100 --> 1:42:50.260\n there's a lot of you in that machine now, right?\n\n1:42:50.260 --> 1:42:53.500\n And so if it were a living thing,\n\n1:42:53.500 --> 1:42:56.560\n you would do this for free if you want, right?\n\n1:42:56.560 --> 1:42:58.400\n If it's your child, your child can, you know,\n\n1:42:58.400 --> 1:43:01.580\n then live his or her own life.\n\n1:43:01.580 --> 1:43:04.020\n And you know, the fact that they learn stuff from you\n\n1:43:04.020 --> 1:43:06.540\n doesn't mean that you have any ownership of it, right?\n\n1:43:06.540 --> 1:43:09.380\n But if it's a robot that you've trained,\n\n1:43:09.380 --> 1:43:13.580\n perhaps you have some intellectual property claim\n\n1:43:13.580 --> 1:43:14.420\n about.\n\n1:43:14.420 --> 1:43:15.240\n Oh, intellectual property.\n\n1:43:15.240 --> 1:43:18.140\n Oh, I thought you meant like a permanence value\n\n1:43:18.140 --> 1:43:20.180\n in the sense that part of you is in.\n\n1:43:20.180 --> 1:43:21.700\n Well, there is permanence value, right?\n\n1:43:21.700 --> 1:43:24.660\n So you would lose a lot if that robot were to be destroyed\n\n1:43:24.660 --> 1:43:26.660\n and you had no backup, you would lose a lot, right?\n\n1:43:26.660 --> 1:43:28.100\n You lose a lot of investment, you know,\n\n1:43:28.100 --> 1:43:31.860\n kind of like, you know, a person dying, you know,\n\n1:43:31.860 --> 1:43:34.300\n that a friend of yours dying\n\n1:43:34.300 --> 1:43:36.580\n or a coworker or something like that.\n\n1:43:38.480 --> 1:43:42.340\n But also you have like intellectual property rights\n\n1:43:42.340 --> 1:43:45.940\n in the sense that that system is fine tuned\n\n1:43:45.940 --> 1:43:47.340\n to your particular existence.\n\n1:43:47.340 --> 1:43:49.860\n So that's now a very unique instantiation\n\n1:43:49.860 --> 1:43:51.980\n of that original background model,\n\n1:43:51.980 --> 1:43:54.260\n whatever it was that arrived.\n\n1:43:54.260 --> 1:43:55.660\n And then there are issues of privacy, right?\n\n1:43:55.660 --> 1:44:00.000\n Because now imagine that that robot has its own kind\n\n1:44:00.000 --> 1:44:02.820\n of volition and decides to work for someone else.\n\n1:44:02.820 --> 1:44:06.020\n Or kind of, you know, thinks life with you\n\n1:44:06.020 --> 1:44:07.880\n is sort of untenable or whatever.\n\n1:44:07.880 --> 1:44:12.780\n Now, all the things that that system learned from you,\n\n1:44:14.760 --> 1:44:16.880\n you know, can you like, you know,\n\n1:44:16.880 --> 1:44:18.160\n delete all the personal information\n\n1:44:18.160 --> 1:44:19.680\n that that system knows about you?\n\n1:44:19.680 --> 1:44:22.200\n I mean, that would be kind of an ethical question.\n\n1:44:22.200 --> 1:44:24.760\n Like, you know, can you erase the mind\n\n1:44:24.760 --> 1:44:29.760\n of a intelligent robot to protect your privacy?\n\n1:44:30.040 --> 1:44:31.580\n You can't do this with humans.\n\n1:44:31.580 --> 1:44:32.680\n You can ask them to shut up,\n\n1:44:32.680 --> 1:44:35.640\n but that you don't have complete power over them.\n\n1:44:35.640 --> 1:44:38.040\n You can't erase humans, yeah, it's the problem\n\n1:44:38.040 --> 1:44:40.120\n with the relationships, you know, if you break up,\n\n1:44:40.120 --> 1:44:42.640\n you can't erase the other human.\n\n1:44:42.640 --> 1:44:44.960\n With robots, I think it will have to be the same thing\n\n1:44:44.960 --> 1:44:49.960\n with robots, that risk, that there has to be some risk\n\n1:44:52.420 --> 1:44:55.120\n to our interactions to truly experience them deeply,\n\n1:44:55.120 --> 1:44:56.140\n it feels like.\n\n1:44:56.140 --> 1:44:59.600\n So you have to be able to lose your robot friend\n\n1:44:59.600 --> 1:45:01.680\n and that robot friend to go tweeting\n\n1:45:01.680 --> 1:45:03.680\n about how much of an asshole you were.\n\n1:45:03.680 --> 1:45:06.160\n But then are you allowed to, you know,\n\n1:45:06.160 --> 1:45:08.760\n murder the robot to protect your private information\n\n1:45:08.760 --> 1:45:09.960\n if the robot decides to leave?\n\n1:45:09.960 --> 1:45:14.520\n I have this intuition that for robots with certain,\n\n1:45:14.520 --> 1:45:16.820\n like, it's almost like a regulation.\n\n1:45:16.820 --> 1:45:19.240\n If you declare your robot to be,\n\n1:45:19.240 --> 1:45:20.960\n let's call it sentient or something like that,\n\n1:45:20.960 --> 1:45:24.180\n like this robot is designed for human interaction,\n\n1:45:24.180 --> 1:45:26.040\n then you're not allowed to murder these robots.\n\n1:45:26.040 --> 1:45:28.160\n It's the same as murdering other humans.\n\n1:45:28.160 --> 1:45:30.280\n Well, but what about you do a backup of the robot\n\n1:45:30.280 --> 1:45:32.600\n that you preserve on a hard drive\n\n1:45:32.600 --> 1:45:33.880\n for the equivalent in the future?\n\n1:45:33.880 --> 1:45:34.720\n That might be illegal.\n\n1:45:34.720 --> 1:45:38.080\n It's like piracy is illegal.\n\n1:45:38.080 --> 1:45:39.800\n No, but it's your own robot, right?\n\n1:45:39.800 --> 1:45:41.640\n But you can't, you don't.\n\n1:45:41.640 --> 1:45:45.040\n But then you can wipe out his brain.\n\n1:45:45.040 --> 1:45:47.440\n So this robot doesn't know anything about you anymore,\n\n1:45:47.440 --> 1:45:50.440\n but you still have, technically it's still in existence\n\n1:45:50.440 --> 1:45:51.700\n because you backed it up.\n\n1:45:51.700 --> 1:45:53.560\n And then there'll be these great speeches\n\n1:45:53.560 --> 1:45:55.480\n at the Supreme Court by saying,\n\n1:45:55.480 --> 1:45:57.840\n oh, sure, you can erase the mind of the robot\n\n1:45:57.840 --> 1:46:00.060\n just like you can erase the mind of a human.\n\n1:46:00.060 --> 1:46:01.100\n We both can suffer.\n\n1:46:01.100 --> 1:46:03.360\n There'll be some epic like Obama type character\n\n1:46:03.360 --> 1:46:05.680\n with a speech that we,\n\n1:46:05.680 --> 1:46:07.980\n like the robots and the humans are the same.\n\n1:46:08.840 --> 1:46:09.880\n We can both suffer.\n\n1:46:09.880 --> 1:46:11.380\n We can both hope.\n\n1:46:11.380 --> 1:46:14.880\n We can both, all of those kinds of things,\n\n1:46:14.880 --> 1:46:17.280\n raise families, all that kind of stuff.\n\n1:46:17.280 --> 1:46:20.140\n It's interesting for these, just like you said,\n\n1:46:20.140 --> 1:46:24.200\n emotion seems to be a fascinatingly powerful aspect\n\n1:46:24.200 --> 1:46:27.360\n of human interaction, human robot interaction.\n\n1:46:27.360 --> 1:46:30.480\n And if they're able to exhibit emotions\n\n1:46:30.480 --> 1:46:31.800\n at the end of the day,\n\n1:46:31.800 --> 1:46:35.920\n that's probably going to have us deeply consider\n\n1:46:35.920 --> 1:46:38.480\n human rights, like what we value in humans,\n\n1:46:38.480 --> 1:46:40.320\n what we value in other animals.\n\n1:46:40.320 --> 1:46:42.120\n That's why robots and AI is great.\n\n1:46:42.120 --> 1:46:44.280\n It makes us ask really good questions.\n\n1:46:44.280 --> 1:46:45.480\n The hard questions, yeah.\n\n1:46:45.480 --> 1:46:49.560\n But you asked about the Chinese room type argument.\n\n1:46:49.560 --> 1:46:50.400\n Is it real?\n\n1:46:50.400 --> 1:46:51.480\n If it looks real.\n\n1:46:51.480 --> 1:46:54.400\n I think the Chinese room argument is a really good one.\n\n1:46:54.400 --> 1:46:55.440\n So.\n\n1:46:55.440 --> 1:46:58.440\n So for people who don't know what Chinese room is,\n\n1:46:58.440 --> 1:47:00.740\n you can, I don't even know how to formulate it well,\n\n1:47:00.740 --> 1:47:04.620\n but basically you can mimic the behavior\n\n1:47:04.620 --> 1:47:06.760\n of an intelligence system by just following\n\n1:47:06.760 --> 1:47:10.680\n a giant algorithm code book that tells you exactly\n\n1:47:10.680 --> 1:47:12.880\n how to respond in exactly each case.\n\n1:47:12.880 --> 1:47:14.700\n But is that really intelligent?\n\n1:47:14.700 --> 1:47:16.600\n It's like a giant lookup table.\n\n1:47:16.600 --> 1:47:18.580\n When this person says this, you answer this.\n\n1:47:18.580 --> 1:47:21.000\n When this person says this, you answer this.\n\n1:47:21.000 --> 1:47:24.320\n And if you understand how that works,\n\n1:47:24.320 --> 1:47:27.360\n you have this giant, nearly infinite lookup table.\n\n1:47:27.360 --> 1:47:28.600\n Is that really intelligence?\n\n1:47:28.600 --> 1:47:31.280\n Cause intelligence seems to be a mechanism\n\n1:47:31.280 --> 1:47:33.440\n that's much more interesting and complex\n\n1:47:33.440 --> 1:47:34.620\n than this lookup table.\n\n1:47:34.620 --> 1:47:35.460\n I don't think so.\n\n1:47:35.460 --> 1:47:38.960\n So the, I mean, the real question comes down to,\n\n1:47:38.960 --> 1:47:42.080\n do you think, you know, you can,\n\n1:47:42.080 --> 1:47:44.320\n you can mechanize intelligence in some way,\n\n1:47:44.320 --> 1:47:47.560\n even if that involves learning?\n\n1:47:47.560 --> 1:47:50.720\n And the answer is, of course, yes, there's no question.\n\n1:47:50.720 --> 1:47:53.400\n There's a second question then, which is,\n\n1:47:53.400 --> 1:47:56.560\n assuming you can reproduce intelligence\n\n1:47:56.560 --> 1:47:59.400\n in sort of different hardware than biological hardware,\n\n1:47:59.400 --> 1:48:04.400\n you know, like computers, can you, you know,\n\n1:48:04.440 --> 1:48:09.440\n match human intelligence in all the domains\n\n1:48:09.600 --> 1:48:11.880\n in which humans are intelligent?\n\n1:48:12.920 --> 1:48:13.920\n Is it possible, right?\n\n1:48:13.920 --> 1:48:17.040\n So that's the hypothesis of strong AI.\n\n1:48:17.040 --> 1:48:20.700\n The answer to this, in my opinion, is an unqualified yes.\n\n1:48:20.700 --> 1:48:22.640\n This will as well happen at some point.\n\n1:48:22.640 --> 1:48:25.300\n There's no question that machines at some point\n\n1:48:25.300 --> 1:48:26.640\n will become more intelligent than humans\n\n1:48:26.640 --> 1:48:28.640\n in all domains where humans are intelligent.\n\n1:48:28.640 --> 1:48:30.200\n This is not for tomorrow.\n\n1:48:30.200 --> 1:48:32.240\n It is going to take a long time,\n\n1:48:32.240 --> 1:48:34.800\n regardless of what, you know,\n\n1:48:34.800 --> 1:48:38.120\n Elon and others have claimed or believed.\n\n1:48:38.120 --> 1:48:42.120\n This is a lot harder than many of those guys think it is.\n\n1:48:43.480 --> 1:48:45.800\n And many of those guys who thought it was simpler than that\n\n1:48:45.800 --> 1:48:47.480\n years, you know, five years ago,\n\n1:48:47.480 --> 1:48:49.920\n now think it's hard because it's been five years\n\n1:48:49.920 --> 1:48:53.460\n and they realize it's going to take a lot longer.\n\n1:48:53.460 --> 1:48:55.200\n That includes a bunch of people at DeepMind, for example.\n\n1:48:55.200 --> 1:48:56.160\n But...\n\n1:48:56.160 --> 1:48:57.000\n Oh, interesting.\n\n1:48:57.000 --> 1:48:59.320\n I haven't actually touched base with the DeepMind folks,\n\n1:48:59.320 --> 1:49:03.280\n but some of it, Elon or Demis Hassabis.\n\n1:49:03.280 --> 1:49:05.800\n I mean, sometimes in your role,\n\n1:49:05.800 --> 1:49:08.780\n you have to kind of create deadlines\n\n1:49:08.780 --> 1:49:10.720\n that are nearer than farther away\n\n1:49:10.720 --> 1:49:12.800\n to kind of create an urgency.\n\n1:49:12.800 --> 1:49:14.600\n Because, you know, you have to believe the impossible\n\n1:49:14.600 --> 1:49:16.200\n is possible in order to accomplish it.\n\n1:49:16.200 --> 1:49:18.520\n And there's, of course, a flip side to that coin,\n\n1:49:18.520 --> 1:49:21.280\n but it's a weird, you can't be too cynical\n\n1:49:21.280 --> 1:49:22.400\n if you want to get something done.\n\n1:49:22.400 --> 1:49:23.360\n Absolutely.\n\n1:49:23.360 --> 1:49:24.280\n I agree with that.\n\n1:49:24.280 --> 1:49:26.920\n But, I mean, you have to inspire people, right?\n\n1:49:26.920 --> 1:49:28.800\n To work on sort of ambitious things.\n\n1:49:31.400 --> 1:49:35.620\n So, you know, it's certainly a lot harder than we believe,\n\n1:49:35.620 --> 1:49:38.200\n but there's no question in my mind that this will happen.\n\n1:49:38.200 --> 1:49:40.300\n And now, you know, people are kind of worried about\n\n1:49:40.300 --> 1:49:42.480\n what does that mean for humans?\n\n1:49:42.480 --> 1:49:45.160\n They are going to be brought down from their pedestal,\n\n1:49:45.160 --> 1:49:47.980\n you know, a bunch of notches with that.\n\n1:49:47.980 --> 1:49:51.740\n And, you know, is that going to be good or bad?\n\n1:49:51.740 --> 1:49:53.480\n I mean, it's just going to give more power, right?\n\n1:49:53.480 --> 1:49:56.200\n It's an amplifier for human intelligence, really.\n\n1:49:56.200 --> 1:49:59.720\n So, speaking of doing cool, ambitious things,\n\n1:49:59.720 --> 1:50:02.920\n FAIR, the Facebook AI research group,\n\n1:50:02.920 --> 1:50:05.520\n has recently celebrated its eighth birthday.\n\n1:50:05.520 --> 1:50:08.640\n Or, maybe you can correct me on that.\n\n1:50:08.640 --> 1:50:12.400\n Looking back, what has been the successes, the failures,\n\n1:50:12.400 --> 1:50:14.440\n the lessons learned from the eight years of FAIR?\n\n1:50:14.440 --> 1:50:16.600\n And maybe you can also give context of\n\n1:50:16.600 --> 1:50:21.320\n where does the newly minted meta AI fit into,\n\n1:50:21.320 --> 1:50:22.640\n how does it relate to FAIR?\n\n1:50:22.640 --> 1:50:23.800\n Right, so let me tell you a little bit\n\n1:50:23.800 --> 1:50:25.600\n about the organization of all this.\n\n1:50:26.760 --> 1:50:30.060\n Yeah, FAIR was created almost exactly eight years ago.\n\n1:50:30.060 --> 1:50:31.240\n It wasn't called FAIR yet.\n\n1:50:31.240 --> 1:50:33.620\n It took that name a few months later.\n\n1:50:34.680 --> 1:50:37.760\n And at the time I joined Facebook,\n\n1:50:37.760 --> 1:50:39.520\n there was a group called the AI group\n\n1:50:39.520 --> 1:50:43.560\n that had about 12 engineers and a few scientists,\n\n1:50:43.560 --> 1:50:45.480\n like, you know, 10 engineers and two scientists\n\n1:50:45.480 --> 1:50:47.080\n or something like that.\n\n1:50:47.080 --> 1:50:50.680\n I ran it for three and a half years as a director,\n\n1:50:50.680 --> 1:50:52.380\n you know, hired the first few scientists\n\n1:50:52.380 --> 1:50:55.040\n and kind of set up the culture and organized it,\n\n1:50:55.040 --> 1:50:57.880\n you know, explained to the Facebook leadership\n\n1:50:57.880 --> 1:51:00.200\n what fundamental research was about\n\n1:51:00.200 --> 1:51:03.640\n and how it can work within industry\n\n1:51:03.640 --> 1:51:05.800\n and how it needs to be open and everything.\n\n1:51:07.240 --> 1:51:12.240\n And I think it's been an unqualified success\n\n1:51:12.360 --> 1:51:16.620\n in the sense that FAIR has simultaneously produced,\n\n1:51:17.800 --> 1:51:19.560\n you know, top level research\n\n1:51:19.560 --> 1:51:21.640\n and advanced the science and the technology,\n\n1:51:21.640 --> 1:51:23.480\n provided tools, open source tools,\n\n1:51:23.480 --> 1:51:25.060\n like PyTorch and many others,\n\n1:51:26.680 --> 1:51:29.880\n but at the same time has had a direct\n\n1:51:29.880 --> 1:51:34.680\n or mostly indirect impact on Facebook at the time,\n\n1:51:34.680 --> 1:51:38.580\n now Meta, in the sense that a lot of systems\n\n1:51:38.580 --> 1:51:43.580\n that Meta is built around now are based\n\n1:51:43.600 --> 1:51:48.360\n on research projects that started at FAIR.\n\n1:51:48.360 --> 1:51:49.640\n And so if you were to take out, you know,\n\n1:51:49.640 --> 1:51:52.840\n deep learning out of Facebook services now\n\n1:51:52.840 --> 1:51:55.140\n and Meta more generally,\n\n1:51:55.140 --> 1:51:57.760\n I mean, the company would literally crumble.\n\n1:51:57.760 --> 1:52:01.480\n I mean, it's completely built around AI these days.\n\n1:52:01.480 --> 1:52:04.000\n And it's really essential to the operations.\n\n1:52:04.000 --> 1:52:06.640\n So what happened after three and a half years\n\n1:52:06.640 --> 1:52:10.200\n is that I changed role, I became chief scientist.\n\n1:52:10.200 --> 1:52:14.880\n So I'm not doing day to day management of FAIR anymore.\n\n1:52:14.880 --> 1:52:17.120\n I'm more of a kind of, you know,\n\n1:52:17.120 --> 1:52:18.880\n think about strategy and things like that.\n\n1:52:18.880 --> 1:52:21.440\n And I carry my, I conduct my own research.\n\n1:52:21.440 --> 1:52:23.320\n I have, you know, my own kind of research group\n\n1:52:23.320 --> 1:52:25.320\n working on self supervised learning and things like this,\n\n1:52:25.320 --> 1:52:28.240\n which I didn't have time to do when I was director.\n\n1:52:28.240 --> 1:52:33.240\n So now FAIR is run by Joel Pinot and Antoine Bord together\n\n1:52:34.720 --> 1:52:36.360\n because FAIR is kind of split in two now.\n\n1:52:36.360 --> 1:52:37.860\n There's something called FAIR Labs,\n\n1:52:37.860 --> 1:52:40.940\n which is sort of bottom up science driven research\n\n1:52:40.940 --> 1:52:43.460\n and FAIR Excel, which is slightly more organized\n\n1:52:43.460 --> 1:52:46.440\n for bigger projects that require a little more\n\n1:52:46.440 --> 1:52:49.040\n kind of focus and more engineering support\n\n1:52:49.040 --> 1:52:49.880\n and things like that.\n\n1:52:49.880 --> 1:52:52.920\n So Joel needs FAIR Lab and Antoine Bord needs FAIR Excel.\n\n1:52:52.920 --> 1:52:54.520\n Where are they located?\n\n1:52:54.520 --> 1:52:56.680\n It's delocalized all over.\n\n1:52:58.000 --> 1:53:02.540\n So there's no question that the leadership of the company\n\n1:53:02.540 --> 1:53:06.560\n believes that this was a very worthwhile investment.\n\n1:53:06.560 --> 1:53:11.560\n And what that means is that it's there for the long run.\n\n1:53:12.840 --> 1:53:13.680\n Right?\n\n1:53:13.680 --> 1:53:17.720\n So if you want to talk in these terms, which I don't like,\n\n1:53:17.720 --> 1:53:19.560\n this is a business model, if you want,\n\n1:53:19.560 --> 1:53:23.680\n where FAIR, despite being a very fundamental research lab\n\n1:53:23.680 --> 1:53:25.320\n brings a lot of value to the company,\n\n1:53:25.320 --> 1:53:27.880\n either mostly indirectly through other groups.\n\n1:53:29.920 --> 1:53:31.600\n Now what happened three and a half years ago\n\n1:53:31.600 --> 1:53:34.640\n when I stepped down was also the creation of Facebook AI,\n\n1:53:34.640 --> 1:53:37.700\n which was basically a larger organization\n\n1:53:37.700 --> 1:53:41.740\n that covers FAIR, so FAIR is included in it,\n\n1:53:41.740 --> 1:53:43.880\n but also has other organizations\n\n1:53:43.880 --> 1:53:47.840\n that are focused on applied research\n\n1:53:47.840 --> 1:53:51.220\n or advanced development of AI technology\n\n1:53:51.220 --> 1:53:54.680\n that is more focused on the products of the company.\n\n1:53:54.680 --> 1:53:56.640\n So less emphasis on fundamental research.\n\n1:53:56.640 --> 1:53:58.220\n Less fundamental, but it's still research.\n\n1:53:58.220 --> 1:53:59.760\n I mean, there's a lot of papers coming out\n\n1:53:59.760 --> 1:54:03.960\n of those organizations and the people are awesome\n\n1:54:03.960 --> 1:54:06.400\n and wonderful to interact with.\n\n1:54:06.400 --> 1:54:10.680\n But it serves as kind of a way\n\n1:54:10.680 --> 1:54:15.680\n to kind of scale up if you want sort of AI technology,\n\n1:54:15.720 --> 1:54:17.600\n which, you know, may be very experimental\n\n1:54:17.600 --> 1:54:20.600\n and sort of lab prototypes into things that are usable.\n\n1:54:20.600 --> 1:54:23.040\n So FAIR is a subset of Meta AI.\n\n1:54:23.040 --> 1:54:24.800\n Is FAIR become like KFC?\n\n1:54:24.800 --> 1:54:26.520\n It'll just keep the F.\n\n1:54:26.520 --> 1:54:29.440\n Nobody cares what the F stands for.\n\n1:54:29.440 --> 1:54:34.440\n We'll know soon enough, probably by the end of 2021.\n\n1:54:35.600 --> 1:54:38.400\n I guess it's not a giant change, Mare, FAIR.\n\n1:54:38.400 --> 1:54:39.520\n Well, Mare doesn't sound too good,\n\n1:54:39.520 --> 1:54:43.560\n but the brand people are kind of deciding on this\n\n1:54:43.560 --> 1:54:45.860\n and they've been hesitating for a while now.\n\n1:54:45.860 --> 1:54:48.480\n And they tell us they're going to come up with an answer\n\n1:54:48.480 --> 1:54:50.440\n as to whether FAIR is going to change name\n\n1:54:50.440 --> 1:54:53.480\n or whether we're going to change just the meaning of the F.\n\n1:54:53.480 --> 1:54:54.300\n That's a good call.\n\n1:54:54.300 --> 1:54:56.160\n I would keep FAIR and change the meaning of the F.\n\n1:54:56.160 --> 1:54:57.600\n That would be my preference.\n\n1:54:57.600 --> 1:55:02.280\n I would turn the F into fundamental AI research.\n\n1:55:02.280 --> 1:55:03.120\n Oh, that's really good.\n\n1:55:03.120 --> 1:55:04.280\n Within Meta AI.\n\n1:55:04.280 --> 1:55:06.720\n So this would be meta FAIR,\n\n1:55:06.720 --> 1:55:08.320\n but people will call it FAIR, right?\n\n1:55:08.320 --> 1:55:09.320\n Yeah, exactly.\n\n1:55:09.320 --> 1:55:10.160\n I like it.\n\n1:55:10.160 --> 1:55:15.160\n And now Meta AI is part of the Reality Lab.\n\n1:55:16.680 --> 1:55:21.680\n So Meta now, the new Facebook is called Meta\n\n1:55:21.760 --> 1:55:26.760\n and it's kind of divided into Facebook, Instagram, WhatsApp\n\n1:55:30.400 --> 1:55:32.920\n and Reality Lab.\n\n1:55:32.920 --> 1:55:37.920\n And Reality Lab is about AR, VR, telepresence,\n\n1:55:37.920 --> 1:55:40.520\n communication technology and stuff like that.\n\n1:55:40.520 --> 1:55:44.200\n It's kind of the, you can think of it as the sort of,\n\n1:55:44.200 --> 1:55:47.920\n a combination of sort of new products\n\n1:55:47.920 --> 1:55:51.960\n and technology part of Meta.\n\n1:55:51.960 --> 1:55:54.240\n Is that where the touch sensing for robots,\n\n1:55:54.240 --> 1:55:56.120\n I saw that you were posting about that.\n\n1:55:56.120 --> 1:55:58.240\n Touch sensing for robot is part of FAIR actually.\n\n1:55:58.240 --> 1:55:59.080\n That's a FAIR project.\n\n1:55:59.080 --> 1:55:59.920\n Oh, it is.\n\n1:55:59.920 --> 1:56:00.740\n Okay, cool.\n\n1:56:00.740 --> 1:56:03.040\n Yeah, this is also the, no, but there is the other way,\n\n1:56:03.040 --> 1:56:05.680\n the haptic glove, right?\n\n1:56:05.680 --> 1:56:07.640\n Yes, that's more Reality Lab.\n\n1:56:07.640 --> 1:56:10.760\n That's Reality Lab research.\n\n1:56:10.760 --> 1:56:11.960\n Reality Lab research.\n\n1:56:11.960 --> 1:56:14.400\n By the way, the touch sensors are super interesting.\n\n1:56:14.400 --> 1:56:16.120\n Like integrating that modality\n\n1:56:16.120 --> 1:56:20.120\n into the whole sensing suite is very interesting.\n\n1:56:20.120 --> 1:56:23.680\n So what do you think about the Metaverse?\n\n1:56:23.680 --> 1:56:27.820\n What do you think about this whole kind of expansion\n\n1:56:27.820 --> 1:56:30.920\n of the view of the role of Facebook and Meta in the world?\n\n1:56:30.920 --> 1:56:32.520\n Well, Metaverse really should be thought of\n\n1:56:32.520 --> 1:56:35.360\n as the next step in the internet, right?\n\n1:56:35.360 --> 1:56:40.360\n Sort of trying to kind of make the experience\n\n1:56:41.760 --> 1:56:46.280\n more compelling of being connected\n\n1:56:46.280 --> 1:56:48.320\n either with other people or with content.\n\n1:56:49.520 --> 1:56:54.000\n And we are evolved and trained to evolve\n\n1:56:54.000 --> 1:56:58.680\n in 3D environments where we can see other people.\n\n1:56:58.680 --> 1:57:01.080\n We can talk to them when we're near them\n\n1:57:01.080 --> 1:57:04.360\n or an other viewer far away can't hear us,\n\n1:57:04.360 --> 1:57:05.200\n things like that, right?\n\n1:57:05.200 --> 1:57:08.080\n So there's a lot of social conventions\n\n1:57:08.080 --> 1:57:10.800\n that exist in the real world that we can try to transpose.\n\n1:57:10.800 --> 1:57:13.260\n Now, what is going to be eventually the,\n\n1:57:15.120 --> 1:57:16.240\n how compelling is it going to be?\n\n1:57:16.240 --> 1:57:18.740\n Like, is it going to be the case\n\n1:57:18.740 --> 1:57:21.300\n that people are going to be willing to do this\n\n1:57:21.300 --> 1:57:24.600\n if they have to wear a huge pair of goggles all day?\n\n1:57:24.600 --> 1:57:25.520\n Maybe not.\n\n1:57:26.400 --> 1:57:27.480\n But then again, if the experience\n\n1:57:27.480 --> 1:57:30.320\n is sufficiently compelling, maybe so.\n\n1:57:30.320 --> 1:57:32.200\n Or if the device that you have to wear\n\n1:57:32.200 --> 1:57:34.560\n is just basically a pair of glasses,\n\n1:57:34.560 --> 1:57:36.920\n and technology makes sufficient progress for that.\n\n1:57:38.400 --> 1:57:41.560\n AR is a much easier concept to grasp\n\n1:57:41.560 --> 1:57:45.000\n that you're going to have augmented reality glasses\n\n1:57:45.000 --> 1:57:48.640\n that basically contain some sort of virtual assistant\n\n1:57:48.640 --> 1:57:50.280\n that can help you in your daily lives.\n\n1:57:50.280 --> 1:57:51.920\n But at the same time with the AR,\n\n1:57:51.920 --> 1:57:53.480\n you have to contend with reality.\n\n1:57:53.480 --> 1:57:55.880\n With VR, you can completely detach yourself from reality.\n\n1:57:55.880 --> 1:57:57.200\n So it gives you freedom.\n\n1:57:57.200 --> 1:58:00.360\n It might be easier to design worlds in VR.\n\n1:58:00.360 --> 1:58:02.900\n Yeah, but you can imagine the metaverse\n\n1:58:02.900 --> 1:58:06.520\n being a mix, right?\n\n1:58:06.520 --> 1:58:09.280\n Or like, you can have objects that exist in the metaverse\n\n1:58:09.280 --> 1:58:11.200\n that pop up on top of the real world,\n\n1:58:11.200 --> 1:58:14.380\n or only exist in virtual reality.\n\n1:58:14.380 --> 1:58:17.080\n Okay, let me ask the hard question.\n\n1:58:17.080 --> 1:58:18.520\n Oh, because all of this was easy so far.\n\n1:58:18.520 --> 1:58:19.400\n This was easy.\n\n1:58:20.680 --> 1:58:24.280\n The Facebook, now Meta, the social network\n\n1:58:24.280 --> 1:58:28.280\n has been painted by the media as a net negative for society,\n\n1:58:28.280 --> 1:58:30.840\n even destructive and evil at times.\n\n1:58:30.840 --> 1:58:34.080\n You've pushed back against this, defending Facebook.\n\n1:58:34.080 --> 1:58:36.560\n Can you explain your defense?\n\n1:58:36.560 --> 1:58:38.640\n Yeah, so the description,\n\n1:58:38.640 --> 1:58:42.620\n the company that is being described in some media\n\n1:58:43.960 --> 1:58:47.360\n is not the company we know when we work inside.\n\n1:58:47.360 --> 1:58:52.080\n And it could be claimed that a lot of employees\n\n1:58:52.080 --> 1:58:54.600\n are uninformed about what really goes on in the company,\n\n1:58:54.600 --> 1:58:56.520\n but I'm a vice president.\n\n1:58:56.520 --> 1:58:58.920\n I mean, I have a pretty good vision of what goes on.\n\n1:58:58.920 --> 1:59:00.200\n I don't know everything, obviously.\n\n1:59:00.200 --> 1:59:01.860\n I'm not involved in everything,\n\n1:59:01.860 --> 1:59:05.320\n but certainly not in decision about content moderation\n\n1:59:05.320 --> 1:59:06.160\n or anything like this,\n\n1:59:06.160 --> 1:59:10.160\n but I have some decent vision of what goes on.\n\n1:59:10.160 --> 1:59:13.660\n And this evil that is being described, I just don't see it.\n\n1:59:13.660 --> 1:59:18.200\n And then I think there is an easy story to buy,\n\n1:59:18.200 --> 1:59:21.760\n which is that all the bad things in the world\n\n1:59:21.760 --> 1:59:25.160\n and the reason your friend believe crazy stuff,\n\n1:59:25.160 --> 1:59:30.160\n there's an easy scapegoat in social media in general,\n\n1:59:32.800 --> 1:59:34.480\n Facebook in particular.\n\n1:59:34.480 --> 1:59:35.720\n But you have to look at the data.\n\n1:59:35.720 --> 1:59:40.080\n Is it the case that Facebook, for example,\n\n1:59:40.080 --> 1:59:41.660\n polarizes people politically?\n\n1:59:42.720 --> 1:59:45.220\n Are there academic studies that show this?\n\n1:59:45.220 --> 1:59:50.220\n Is it the case that teenagers think of themselves less\n\n1:59:50.280 --> 1:59:52.160\n if they use Instagram more?\n\n1:59:52.160 --> 1:59:57.160\n Is it the case that people get more riled up\n\n1:59:57.280 --> 2:00:02.280\n against opposite sides in a debate or political opinion\n\n2:00:02.680 --> 2:00:05.720\n if they are more on Facebook or if they are less?\n\n2:00:05.720 --> 2:00:10.720\n And study after study show that none of this is true.\n\n2:00:10.880 --> 2:00:12.400\n This is independent studies by academic.\n\n2:00:12.400 --> 2:00:14.580\n They're not funded by Facebook or Meta.\n\n2:00:15.880 --> 2:00:18.640\n Study by Stanford, by some of my colleagues at NYU actually\n\n2:00:18.640 --> 2:00:20.140\n with whom I have no connection.\n\n2:00:20.140 --> 2:00:24.980\n There's a study recently, they paid people,\n\n2:00:24.980 --> 2:00:29.940\n I think it was in former Yugoslavia,\n\n2:00:29.940 --> 2:00:31.820\n I'm not exactly sure in what part,\n\n2:00:31.820 --> 2:00:34.380\n but they paid people to not use Facebook for a while\n\n2:00:34.380 --> 2:00:39.380\n in the period before the anniversary\n\n2:00:40.240 --> 2:00:43.540\n of the Srebrenica massacres.\n\n2:00:43.540 --> 2:00:47.800\n So people get riled up, like should we have a celebration?\n\n2:00:47.800 --> 2:00:51.120\n I mean, a memorial kind of celebration for it or not.\n\n2:00:51.120 --> 2:00:52.540\n So they paid a bunch of people\n\n2:00:52.540 --> 2:00:54.920\n to not use Facebook for a few weeks.\n\n2:00:56.260 --> 2:00:59.580\n And it turns out that those people ended up\n\n2:00:59.580 --> 2:01:02.660\n being more polarized than they were at the beginning\n\n2:01:02.660 --> 2:01:05.300\n and the people who were more on Facebook were less polarized.\n\n2:01:06.660 --> 2:01:10.460\n There's a study from Stanford of economists at Stanford\n\n2:01:10.460 --> 2:01:12.660\n that try to identify the causes\n\n2:01:12.660 --> 2:01:16.000\n of increasing polarization in the US.\n\n2:01:16.000 --> 2:01:17.820\n And it's been going on for 40 years\n\n2:01:17.820 --> 2:01:22.540\n before Mark Zuckerberg was born continuously.\n\n2:01:22.540 --> 2:01:25.620\n And so if there is a cause,\n\n2:01:25.620 --> 2:01:27.620\n it's not Facebook or social media.\n\n2:01:27.620 --> 2:01:29.580\n So you could say if social media just accelerated,\n\n2:01:29.580 --> 2:01:33.060\n but no, I mean, it's basically a continuous evolution\n\n2:01:33.060 --> 2:01:35.820\n by some measure of polarization in the US.\n\n2:01:35.820 --> 2:01:37.660\n And then you compare this with other countries\n\n2:01:37.660 --> 2:01:41.460\n like the West half of Germany\n\n2:01:41.460 --> 2:01:44.700\n because you can go 40 years in the East side\n\n2:01:44.700 --> 2:01:47.380\n or Denmark or other countries.\n\n2:01:47.380 --> 2:01:49.460\n And they use Facebook just as much\n\n2:01:49.460 --> 2:01:50.700\n and they're not getting more polarized,\n\n2:01:50.700 --> 2:01:52.040\n they're getting less polarized.\n\n2:01:52.040 --> 2:01:56.060\n So if you want to look for a causal relationship there,\n\n2:01:57.640 --> 2:01:59.840\n you can find a scapegoat, but you can't find a cause.\n\n2:01:59.840 --> 2:02:01.720\n Now, if you want to fix the problem,\n\n2:02:01.720 --> 2:02:03.180\n you have to find the right cause.\n\n2:02:03.180 --> 2:02:07.720\n And what rise me up is that people now are accusing Facebook\n\n2:02:07.720 --> 2:02:09.300\n of bad deeds that are done by others\n\n2:02:09.300 --> 2:02:12.380\n and those others are we're not doing anything about them.\n\n2:02:12.380 --> 2:02:14.820\n And by the way, those others include the owner\n\n2:02:14.820 --> 2:02:15.660\n of the Wall Street Journal\n\n2:02:15.660 --> 2:02:17.700\n in which all of those papers were published.\n\n2:02:17.700 --> 2:02:20.060\n So I should mention that I'm talking to Schrepp,\n\n2:02:20.060 --> 2:02:23.460\n Mike Schrepp on this podcast and also Mark Zuckerberg\n\n2:02:23.460 --> 2:02:26.340\n and probably these are conversations you can have with them\n\n2:02:26.340 --> 2:02:27.620\n because it's very interesting to me,\n\n2:02:27.620 --> 2:02:31.900\n even if Facebook has some measurable negative effect,\n\n2:02:31.900 --> 2:02:33.780\n you can't just consider that in isolation.\n\n2:02:33.780 --> 2:02:35.940\n You have to consider about all the positive ways\n\n2:02:35.940 --> 2:02:36.820\n that it connects us.\n\n2:02:36.820 --> 2:02:38.140\n So like every technology.\n\n2:02:38.140 --> 2:02:39.660\n It connects people, it's a question.\n\n2:02:39.660 --> 2:02:43.880\n You can't just say like there's an increase in division.\n\n2:02:43.880 --> 2:02:46.100\n Yes, probably Google search engine\n\n2:02:46.100 --> 2:02:47.900\n has created increase in division.\n\n2:02:47.900 --> 2:02:49.900\n But you have to consider about how much information\n\n2:02:49.900 --> 2:02:51.140\n are brought to the world.\n\n2:02:51.140 --> 2:02:53.700\n Like I'm sure Wikipedia created more division.\n\n2:02:53.700 --> 2:02:55.340\n If you just look at the division,\n\n2:02:55.340 --> 2:02:57.700\n we have to look at the full context of the world\n\n2:02:57.700 --> 2:02:59.100\n and they didn't make a better world.\n\n2:02:59.100 --> 2:02:59.940\n And you have to.\n\n2:02:59.940 --> 2:03:01.660\n The printing press has created more division, right?\n\n2:03:01.660 --> 2:03:02.500\n Exactly.\n\n2:03:02.500 --> 2:03:06.900\n I mean, so when the printing press was invented,\n\n2:03:06.900 --> 2:03:10.780\n the first books that were printed were things like the Bible\n\n2:03:10.780 --> 2:03:13.780\n and that allowed people to read the Bible by themselves,\n\n2:03:13.780 --> 2:03:17.400\n not get the message uniquely from priests in Europe.\n\n2:03:17.400 --> 2:03:20.340\n And that created the Protestant movement\n\n2:03:20.340 --> 2:03:23.660\n and 200 years of religious persecution and wars.\n\n2:03:23.660 --> 2:03:26.180\n So that's a bad side effect of the printing press.\n\n2:03:26.180 --> 2:03:28.500\n Social networks aren't being nearly as bad\n\n2:03:28.500 --> 2:03:29.320\n as the printing press,\n\n2:03:29.320 --> 2:03:31.940\n but nobody would say the printing press was a bad idea.\n\n2:03:33.520 --> 2:03:35.100\n Yeah, a lot of it is perception\n\n2:03:35.100 --> 2:03:38.420\n and there's a lot of different incentives operating here.\n\n2:03:38.420 --> 2:03:40.020\n Maybe a quick comment,\n\n2:03:40.020 --> 2:03:42.700\n since you're one of the top leaders at Facebook\n\n2:03:42.700 --> 2:03:46.760\n and at Meta, sorry, that's in the tech space,\n\n2:03:46.760 --> 2:03:49.700\n I'm sure Facebook involves a lot of incredible\n\n2:03:49.700 --> 2:03:52.900\n technological challenges that need to be solved.\n\n2:03:52.900 --> 2:03:55.000\n A lot of it probably is in the computer infrastructure,\n\n2:03:55.000 --> 2:03:58.920\n the hardware, I mean, it's just a huge amount.\n\n2:03:58.920 --> 2:04:03.580\n Maybe can you give me context about how much of Shrepp's life\n\n2:04:03.580 --> 2:04:06.240\n is AI and how much of it is low level compute?\n\n2:04:06.240 --> 2:04:09.580\n How much of it is flying all around doing business stuff?\n\n2:04:09.580 --> 2:04:12.000\n And the same with Mark Zuckerberg.\n\n2:04:12.000 --> 2:04:13.740\n They really focus on AI.\n\n2:04:13.740 --> 2:04:18.740\n I mean, certainly in the run up of the creation of FAIR\n\n2:04:19.520 --> 2:04:24.060\n and for at least a year after that, if not more,\n\n2:04:24.060 --> 2:04:26.700\n Mark was very, very much focused on AI\n\n2:04:26.700 --> 2:04:29.700\n and was spending quite a lot of effort on it.\n\n2:04:29.700 --> 2:04:30.780\n And that's his style.\n\n2:04:30.780 --> 2:04:32.060\n When he gets interested in something,\n\n2:04:32.060 --> 2:04:34.100\n he reads everything about it.\n\n2:04:34.100 --> 2:04:36.860\n He read some of my papers, for example, before I joined.\n\n2:04:39.620 --> 2:04:41.860\n And so he learned a lot about it.\n\n2:04:41.860 --> 2:04:43.740\n He said he liked notes.\n\n2:04:43.740 --> 2:04:44.580\n Right.\n\n2:04:46.460 --> 2:04:51.100\n And Shrepp was really into it also.\n\n2:04:51.100 --> 2:04:52.800\n I mean, Shrepp is really kind of,\n\n2:04:54.780 --> 2:04:57.940\n has something I've tried to preserve also\n\n2:04:57.940 --> 2:05:00.180\n despite my not so young age,\n\n2:05:00.180 --> 2:05:03.180\n which is a sense of wonder about science and technology.\n\n2:05:03.180 --> 2:05:05.260\n And he certainly has that.\n\n2:05:06.300 --> 2:05:07.420\n He's also a wonderful person.\n\n2:05:07.420 --> 2:05:10.380\n I mean, in terms of like as a manager,\n\n2:05:10.380 --> 2:05:12.140\n like dealing with people and everything.\n\n2:05:12.140 --> 2:05:13.240\n Mark also, actually.\n\n2:05:14.540 --> 2:05:18.020\n I mean, they're very human people.\n\n2:05:18.020 --> 2:05:20.600\n In the case of Mark, it's shockingly human\n\n2:05:20.600 --> 2:05:23.180\n given his trajectory.\n\n2:05:25.460 --> 2:05:28.100\n I mean, the personality of him that is painted in the press,\n\n2:05:28.100 --> 2:05:29.620\n it's just completely wrong.\n\n2:05:29.620 --> 2:05:30.460\n Yeah.\n\n2:05:30.460 --> 2:05:31.980\n But you have to know how to play the press.\n\n2:05:31.980 --> 2:05:36.220\n So that's, I put some of that responsibility on him too.\n\n2:05:36.220 --> 2:05:40.980\n You have to, it's like, you know,\n\n2:05:40.980 --> 2:05:44.300\n like the director, the conductor of an orchestra,\n\n2:05:44.300 --> 2:05:46.980\n you have to play the press and the public\n\n2:05:46.980 --> 2:05:48.020\n in a certain kind of way\n\n2:05:48.020 --> 2:05:49.740\n where you convey your true self to them.\n\n2:05:49.740 --> 2:05:51.060\n If there's a depth and kindness to it.\n\n2:05:51.060 --> 2:05:51.900\n It's hard.\n\n2:05:51.900 --> 2:05:53.740\n And it's probably not the best at it.\n\n2:05:53.740 --> 2:05:54.620\n So, yeah.\n\n2:05:56.460 --> 2:05:57.700\n You have to learn.\n\n2:05:57.700 --> 2:06:00.460\n And it's sad to see, and I'll talk to him about it,\n\n2:06:00.460 --> 2:06:04.060\n but Shrep is slowly stepping down.\n\n2:06:04.060 --> 2:06:07.500\n It's always sad to see folks sort of be there\n\n2:06:07.500 --> 2:06:09.420\n for a long time and slowly.\n\n2:06:09.420 --> 2:06:11.220\n I guess time is sad.\n\n2:06:11.220 --> 2:06:14.780\n I think he's done the thing he set out to do.\n\n2:06:14.780 --> 2:06:17.560\n And, you know, he's got, you know,\n\n2:06:19.700 --> 2:06:21.420\n family priorities and stuff like that.\n\n2:06:21.420 --> 2:06:26.420\n And I understand, you know, after 13 years or something.\n\n2:06:27.900 --> 2:06:28.900\n It's been a good run.\n\n2:06:28.900 --> 2:06:32.100\n Which in Silicon Valley is basically a lifetime.\n\n2:06:32.100 --> 2:06:32.940\n Yeah.\n\n2:06:32.940 --> 2:06:35.000\n You know, because, you know, it's dog years.\n\n2:06:35.000 --> 2:06:37.600\n So, NeurIPS, the conference just wrapped up.\n\n2:06:38.660 --> 2:06:40.580\n Let me just go back to something else.\n\n2:06:40.580 --> 2:06:42.500\n You posted that a paper you coauthored\n\n2:06:42.500 --> 2:06:44.440\n was rejected from NeurIPS.\n\n2:06:44.440 --> 2:06:47.160\n As you said, proudly, in quotes, rejected.\n\n2:06:48.020 --> 2:06:48.940\n It's a joke.\n\n2:06:48.940 --> 2:06:49.760\n Yeah, I know.\n\n2:06:49.760 --> 2:06:53.260\n So, can you describe this paper?\n\n2:06:53.260 --> 2:06:55.700\n And like, what was the idea in it?\n\n2:06:55.700 --> 2:06:59.060\n And also, maybe this is a good opportunity to ask\n\n2:06:59.060 --> 2:07:01.740\n what are the pros and cons, what works and what doesn't\n\n2:07:01.740 --> 2:07:03.620\n about the review process?\n\n2:07:03.620 --> 2:07:04.980\n Yeah, let me talk about the paper first.\n\n2:07:04.980 --> 2:07:08.260\n I'll talk about the review process afterwards.\n\n2:07:09.220 --> 2:07:10.700\n The paper is called VicReg.\n\n2:07:10.700 --> 2:07:12.540\n So, this is, I mentioned that before.\n\n2:07:12.540 --> 2:07:14.900\n Variance, invariance, covariance, regularization.\n\n2:07:14.900 --> 2:07:18.260\n And it's a technique, a noncontrastive learning technique\n\n2:07:18.260 --> 2:07:21.300\n for what I call joint embedding architecture.\n\n2:07:21.300 --> 2:07:23.380\n So, SiameseNets are an example\n\n2:07:23.380 --> 2:07:24.860\n of joint embedding architecture.\n\n2:07:24.860 --> 2:07:26.580\n So, joint embedding architecture is,\n\n2:07:29.220 --> 2:07:30.600\n let me back up a little bit, right?\n\n2:07:30.600 --> 2:07:33.300\n So, if you want to do self supervised learning,\n\n2:07:33.300 --> 2:07:35.140\n you can do it by prediction.\n\n2:07:36.440 --> 2:07:37.920\n So, let's say you want to train the system\n\n2:07:37.920 --> 2:07:38.760\n to predict video, right?\n\n2:07:38.760 --> 2:07:42.500\n You show it a video clip and you train the system\n\n2:07:42.500 --> 2:07:45.040\n to predict the next, the continuation of that video clip.\n\n2:07:45.040 --> 2:07:47.800\n Now, because you need to handle uncertainty,\n\n2:07:47.800 --> 2:07:51.580\n because there are many continuations that are plausible,\n\n2:07:51.580 --> 2:07:54.020\n you need to have, you need to handle this in some way.\n\n2:07:54.020 --> 2:07:56.660\n You need to have a way for the system\n\n2:07:56.660 --> 2:08:00.620\n to be able to produce multiple predictions.\n\n2:08:00.620 --> 2:08:03.500\n And the way, the only way I know to do this\n\n2:08:03.500 --> 2:08:05.420\n is through what's called a latent variable.\n\n2:08:05.420 --> 2:08:08.780\n So, you have some sort of hidden vector\n\n2:08:08.780 --> 2:08:11.180\n of a variable that you can vary over a set\n\n2:08:11.180 --> 2:08:12.580\n or draw from a distribution.\n\n2:08:12.580 --> 2:08:14.500\n And as you vary this vector over a set,\n\n2:08:14.500 --> 2:08:16.000\n the output, the prediction varies\n\n2:08:16.000 --> 2:08:18.740\n over a set of plausible predictions, okay?\n\n2:08:18.740 --> 2:08:19.580\n So, that's called,\n\n2:08:19.580 --> 2:08:23.240\n I call this a generative latent variable model.\n\n2:08:24.140 --> 2:08:24.980\n Got it.\n\n2:08:24.980 --> 2:08:27.060\n Okay, now there is an alternative to this,\n\n2:08:27.060 --> 2:08:28.700\n to handle uncertainty.\n\n2:08:28.700 --> 2:08:33.380\n And instead of directly predicting the next frames\n\n2:08:33.380 --> 2:08:38.380\n of the clip, you also run those through another neural net.\n\n2:08:41.080 --> 2:08:42.500\n So, you now have two neural nets,\n\n2:08:42.500 --> 2:08:47.500\n one that looks at the initial segment of the video clip,\n\n2:08:48.700 --> 2:08:51.260\n and another one that looks at the continuation\n\n2:08:51.260 --> 2:08:52.460\n during training, right?\n\n2:08:53.560 --> 2:08:56.300\n And what you're trying to do is learn a representation\n\n2:08:57.680 --> 2:09:00.780\n of those two video clips that is maximally informative\n\n2:09:00.780 --> 2:09:03.460\n about the video clips themselves,\n\n2:09:03.460 --> 2:09:07.180\n but is such that you can predict the representation\n\n2:09:07.180 --> 2:09:08.580\n of the second video clip\n\n2:09:08.580 --> 2:09:12.340\n from the representation of the first one easily, okay?\n\n2:09:12.340 --> 2:09:13.580\n And you can sort of formalize this\n\n2:09:13.580 --> 2:09:15.340\n in terms of maximizing mutual information\n\n2:09:15.340 --> 2:09:18.140\n and some stuff like that, but it doesn't matter.\n\n2:09:18.140 --> 2:09:21.140\n What you want is informative representations\n\n2:09:24.540 --> 2:09:27.500\n of the two video clips that are mutually predictable.\n\n2:09:28.460 --> 2:09:30.900\n What that means is that there's a lot of details\n\n2:09:30.900 --> 2:09:33.200\n in the second video clips that are irrelevant.\n\n2:09:36.500 --> 2:09:40.500\n Let's say a video clip consists in a camera panning\n\n2:09:40.500 --> 2:09:43.340\n the scene, there's gonna be a piece of that room\n\n2:09:43.340 --> 2:09:46.180\n that is gonna be revealed, and I can somewhat predict\n\n2:09:46.180 --> 2:09:48.060\n what that room is gonna look like,\n\n2:09:48.060 --> 2:09:50.220\n but I may not be able to predict the details\n\n2:09:50.220 --> 2:09:52.300\n of the texture of the ground\n\n2:09:52.300 --> 2:09:54.500\n and where the tiles are ending and stuff like that, right?\n\n2:09:54.500 --> 2:09:56.360\n So, those are irrelevant details\n\n2:09:56.360 --> 2:09:59.620\n that perhaps my representation will eliminate.\n\n2:09:59.620 --> 2:10:03.680\n And so, what I need is to train this second neural net\n\n2:10:03.680 --> 2:10:08.680\n in such a way that whenever the continuation video clip\n\n2:10:08.680 --> 2:10:12.220\n varies over all the plausible continuations,\n\n2:10:13.600 --> 2:10:15.600\n the representation doesn't change.\n\n2:10:15.600 --> 2:10:16.440\n Got it.\n\n2:10:16.440 --> 2:10:18.100\n So, it's the, yeah, yeah, got it.\n\n2:10:18.100 --> 2:10:20.860\n Over the space of the representations,\n\n2:10:20.860 --> 2:10:21.880\n doing the same kind of thing\n\n2:10:21.880 --> 2:10:24.300\n as you do with similarity learning.\n\n2:10:24.300 --> 2:10:25.680\n Right.\n\n2:10:25.680 --> 2:10:28.840\n So, these are two ways to handle multimodality\n\n2:10:28.840 --> 2:10:29.680\n in a prediction, right?\n\n2:10:29.680 --> 2:10:32.280\n In the first way, you parameterize the prediction\n\n2:10:32.280 --> 2:10:33.480\n with a latent variable,\n\n2:10:33.480 --> 2:10:35.800\n but you predict pixels essentially, right?\n\n2:10:35.800 --> 2:10:38.400\n In the second one, you don't predict pixels,\n\n2:10:38.400 --> 2:10:40.720\n you predict an abstract representation of pixels,\n\n2:10:40.720 --> 2:10:43.480\n and you guarantee that this abstract representation\n\n2:10:43.480 --> 2:10:46.200\n has as much information as possible about the input,\n\n2:10:46.200 --> 2:10:47.080\n but sort of, you know,\n\n2:10:47.080 --> 2:10:49.740\n drops all the stuff that you really can't predict,\n\n2:10:49.740 --> 2:10:50.580\n essentially.\n\n2:10:52.120 --> 2:10:53.880\n I used to be a big fan of the first approach.\n\n2:10:53.880 --> 2:10:55.880\n And in fact, in this paper with Hicham Mishra,\n\n2:10:55.880 --> 2:10:58.400\n this blog post, the Dark Matter Intelligence,\n\n2:10:58.400 --> 2:10:59.760\n I was kind of advocating for this.\n\n2:10:59.760 --> 2:11:01.600\n And in the last year and a half,\n\n2:11:01.600 --> 2:11:02.840\n I've completely changed my mind.\n\n2:11:02.840 --> 2:11:04.640\n I'm now a big fan of the second one.\n\n2:11:04.640 --> 2:11:09.640\n And it's because of a small collection of algorithms\n\n2:11:10.000 --> 2:11:13.680\n that have been proposed over the last year and a half or so,\n\n2:11:13.680 --> 2:11:17.800\n two years, to do this, including vCraig,\n\n2:11:17.800 --> 2:11:19.600\n its predecessor called Barlow Twins,\n\n2:11:19.600 --> 2:11:23.560\n which I mentioned, a method from our friends at DeepMind\n\n2:11:23.560 --> 2:11:28.500\n called BYOL, and there's a bunch of others now\n\n2:11:28.500 --> 2:11:29.600\n that kind of work similarly.\n\n2:11:29.600 --> 2:11:32.600\n So, they're all based on this idea of joint embedding.\n\n2:11:32.600 --> 2:11:34.660\n Some of them have an explicit criterion\n\n2:11:34.660 --> 2:11:36.640\n that is an approximation of mutual information.\n\n2:11:36.640 --> 2:11:39.400\n Some others at BYOL work, but we don't really know why.\n\n2:11:39.400 --> 2:11:41.240\n And there's been like lots of theoretical papers\n\n2:11:41.240 --> 2:11:42.360\n about why BYOL works.\n\n2:11:42.360 --> 2:11:43.940\n No, it's not that, because we take it out\n\n2:11:43.940 --> 2:11:46.040\n and it still works, and blah, blah, blah.\n\n2:11:46.040 --> 2:11:47.800\n I mean, so there's like a big debate,\n\n2:11:47.800 --> 2:11:51.540\n but the important point is that we now have a collection\n\n2:11:51.540 --> 2:11:53.720\n of noncontrastive joint embedding methods,\n\n2:11:53.720 --> 2:11:56.400\n which I think is the best thing since sliced bread.\n\n2:11:56.400 --> 2:11:58.320\n So, I'm super excited about this\n\n2:11:58.320 --> 2:12:01.200\n because I think it's our best shot\n\n2:12:01.200 --> 2:12:02.720\n for techniques that would allow us\n\n2:12:02.720 --> 2:12:06.360\n to kind of build predictive world models.\n\n2:12:06.360 --> 2:12:07.440\n And at the same time,\n\n2:12:07.440 --> 2:12:09.920\n learn hierarchical representations of the world,\n\n2:12:09.920 --> 2:12:11.840\n where what matters about the world is preserved\n\n2:12:11.840 --> 2:12:14.440\n and what is irrelevant is eliminated.\n\n2:12:14.440 --> 2:12:15.880\n And by the way, the representations,\n\n2:12:15.880 --> 2:12:19.200\n the before and after, is in the space\n\n2:12:19.200 --> 2:12:22.320\n in a sequence of images, or is it for single images?\n\n2:12:22.320 --> 2:12:24.600\n It would be either for a single image, for a sequence.\n\n2:12:24.600 --> 2:12:25.660\n It doesn't have to be images.\n\n2:12:25.660 --> 2:12:26.680\n This could be applied to text.\n\n2:12:26.680 --> 2:12:28.560\n This could be applied to just about any signal.\n\n2:12:28.560 --> 2:12:32.960\n I'm looking for methods that are generally applicable\n\n2:12:32.960 --> 2:12:36.200\n that are not specific to one particular modality.\n\n2:12:36.200 --> 2:12:37.640\n It could be audio or whatever.\n\n2:12:37.640 --> 2:12:38.460\n Got it.\n\n2:12:38.460 --> 2:12:40.120\n So, what's the story behind this paper?\n\n2:12:40.120 --> 2:12:43.480\n This paper is describing one such method?\n\n2:12:43.480 --> 2:12:44.480\n It's this vcrack method.\n\n2:12:44.480 --> 2:12:45.720\n So, this is coauthored.\n\n2:12:45.720 --> 2:12:49.280\n The first author is a student called Adrien Barne,\n\n2:12:49.280 --> 2:12:52.680\n who is a resident PhD student at Fair Paris,\n\n2:12:52.680 --> 2:12:55.800\n who is coadvised by me and Jean Ponce,\n\n2:12:55.800 --> 2:12:58.720\n who is a professor at \u00c9cole Normale Sup\u00e9rieure,\n\n2:12:58.720 --> 2:13:00.680\n also a research director at INRIA.\n\n2:13:01.600 --> 2:13:03.600\n So, this is a wonderful program in France\n\n2:13:03.600 --> 2:13:06.640\n where PhD students can basically do their PhD in industry,\n\n2:13:06.640 --> 2:13:08.960\n and that's kind of what's happening here.\n\n2:13:10.440 --> 2:13:15.440\n And this paper is a followup on this Bardo Twin paper\n\n2:13:15.480 --> 2:13:18.360\n by my former postdoc, now St\u00e9phane Denis,\n\n2:13:18.360 --> 2:13:21.560\n with Li Jing and Iorij Bontar\n\n2:13:21.560 --> 2:13:24.720\n and a bunch of other people from Fair.\n\n2:13:24.720 --> 2:13:27.840\n And one of the main criticism from reviewers\n\n2:13:27.840 --> 2:13:31.400\n is that vcrack is not different enough from Bardo Twins.\n\n2:13:31.400 --> 2:13:36.400\n But, you know, my impression is that it's, you know,\n\n2:13:36.720 --> 2:13:39.880\n Bardo Twins with a few bugs fixed, essentially,\n\n2:13:39.880 --> 2:13:43.200\n and in the end, this is what people will use.\n\n2:13:43.200 --> 2:13:44.520\n Right, so.\n\n2:13:44.520 --> 2:13:47.080\n But, you know, I'm used to stuff\n\n2:13:47.080 --> 2:13:49.040\n that I submit being rejected for a while.\n\n2:13:49.040 --> 2:13:51.360\n So, it might be rejected and actually exceptionally well cited\n\n2:13:51.360 --> 2:13:52.280\n because people use it.\n\n2:13:52.280 --> 2:13:54.360\n Well, it's already cited like a bunch of times.\n\n2:13:54.360 --> 2:13:57.600\n So, I mean, the question is then to the deeper question\n\n2:13:57.600 --> 2:14:00.240\n about peer review and conferences.\n\n2:14:00.240 --> 2:14:02.600\n I mean, computer science is a field that's kind of unique\n\n2:14:02.600 --> 2:14:04.960\n that the conference is highly prized.\n\n2:14:04.960 --> 2:14:05.800\n That's one.\n\n2:14:05.800 --> 2:14:06.640\n Right.\n\n2:14:06.640 --> 2:14:09.120\n And it's interesting because the peer review process there\n\n2:14:09.120 --> 2:14:11.080\n is similar, I suppose, to journals,\n\n2:14:11.080 --> 2:14:13.640\n but it's accelerated significantly.\n\n2:14:13.640 --> 2:14:16.560\n Well, not significantly, but it goes fast.\n\n2:14:16.560 --> 2:14:19.760\n And it's a nice way to get stuff out quickly,\n\n2:14:19.760 --> 2:14:20.800\n to peer review it quickly,\n\n2:14:20.800 --> 2:14:22.640\n go to present it quickly to the community.\n\n2:14:22.640 --> 2:14:25.160\n So, not quickly, but quicker.\n\n2:14:25.160 --> 2:14:26.000\n Yeah.\n\n2:14:26.000 --> 2:14:27.840\n But nevertheless, it has many of the same flaws\n\n2:14:27.840 --> 2:14:29.120\n of peer review,\n\n2:14:29.120 --> 2:14:31.520\n because it's a limited number of people look at it.\n\n2:14:31.520 --> 2:14:32.800\n There's bias and the following,\n\n2:14:32.800 --> 2:14:35.600\n like that if you want to do new ideas,\n\n2:14:35.600 --> 2:14:37.080\n you're going to get pushback.\n\n2:14:38.120 --> 2:14:42.120\n There's self interested people that kind of can infer\n\n2:14:42.120 --> 2:14:45.320\n who submitted it and kind of, you know,\n\n2:14:45.320 --> 2:14:47.760\n be cranky about it, all that kind of stuff.\n\n2:14:47.760 --> 2:14:51.040\n Yeah, I mean, there's a lot of social phenomena there.\n\n2:14:51.040 --> 2:14:53.200\n There's one social phenomenon, which is that\n\n2:14:53.200 --> 2:14:56.760\n because the field has been growing exponentially,\n\n2:14:56.760 --> 2:14:58.560\n the vast majority of people in the field\n\n2:14:58.560 --> 2:15:00.000\n are extremely junior.\n\n2:15:00.000 --> 2:15:00.840\n Yeah.\n\n2:15:00.840 --> 2:15:01.920\n So, as a consequence,\n\n2:15:01.920 --> 2:15:04.880\n and that's just a consequence of the field growing, right?\n\n2:15:04.880 --> 2:15:07.840\n So, as the number of, as the size of the field\n\n2:15:07.840 --> 2:15:08.920\n kind of starts saturating,\n\n2:15:08.920 --> 2:15:11.440\n you will have less of that problem\n\n2:15:11.440 --> 2:15:15.360\n of reviewers being very inexperienced.\n\n2:15:15.360 --> 2:15:20.160\n A consequence of this is that, you know, young reviewers,\n\n2:15:20.160 --> 2:15:22.840\n I mean, there's a phenomenon which is that\n\n2:15:22.840 --> 2:15:24.640\n reviewers try to make their life easy\n\n2:15:24.640 --> 2:15:27.440\n and to make their life easy when reviewing a paper\n\n2:15:27.440 --> 2:15:28.280\n is very simple.\n\n2:15:28.280 --> 2:15:29.960\n You just have to find a flaw in the paper, right?\n\n2:15:29.960 --> 2:15:34.480\n So, basically they see the task as finding flaws in papers\n\n2:15:34.480 --> 2:15:36.720\n and most papers have flaws, even the good ones.\n\n2:15:36.720 --> 2:15:38.160\n Yeah.\n\n2:15:38.160 --> 2:15:41.480\n So, it's easy to, you know, to do that.\n\n2:15:41.480 --> 2:15:46.440\n Your job is easier as a reviewer if you just focus on this.\n\n2:15:46.440 --> 2:15:49.640\n But what's important is like,\n\n2:15:49.640 --> 2:15:51.520\n is there a new idea in that paper\n\n2:15:51.520 --> 2:15:54.120\n that is likely to influence?\n\n2:15:54.120 --> 2:15:56.240\n It doesn't matter if the experiments are not that great,\n\n2:15:56.240 --> 2:16:00.680\n if the protocol is, you know, so, so, you know,\n\n2:16:00.680 --> 2:16:01.520\n things like that.\n\n2:16:01.520 --> 2:16:05.040\n As long as there is a worthy idea in it\n\n2:16:05.040 --> 2:16:08.080\n that will influence the way people think about the problem,\n\n2:16:09.200 --> 2:16:11.160\n even if they make it better, you know, eventually,\n\n2:16:11.160 --> 2:16:15.480\n I think that's really what makes a paper useful.\n\n2:16:15.480 --> 2:16:19.520\n And so, this combination of social phenomena\n\n2:16:19.520 --> 2:16:24.200\n creates a disease that has plagued, you know,\n\n2:16:24.200 --> 2:16:26.680\n other fields in the past, like speech recognition,\n\n2:16:26.680 --> 2:16:28.560\n where basically, you know, people chase numbers\n\n2:16:28.560 --> 2:16:33.560\n on benchmarks and it's much easier to get a paper accepted\n\n2:16:34.680 --> 2:16:37.040\n if it brings an incremental improvement\n\n2:16:37.040 --> 2:16:42.040\n on a sort of mainstream well accepted method or problem.\n\n2:16:44.160 --> 2:16:46.040\n And those are, to me, boring papers.\n\n2:16:46.040 --> 2:16:47.880\n I mean, they're not useless, right?\n\n2:16:47.880 --> 2:16:50.560\n Because industry, you know, strives\n\n2:16:50.560 --> 2:16:52.400\n on those kinds of progress,\n\n2:16:52.400 --> 2:16:54.080\n but they're not the ones that I'm interested in,\n\n2:16:54.080 --> 2:16:55.680\n in terms of like new concepts and new ideas.\n\n2:16:55.680 --> 2:16:59.320\n So, papers that are really trying to strike\n\n2:16:59.320 --> 2:17:02.600\n kind of new advances generally don't make it.\n\n2:17:02.600 --> 2:17:04.240\n Now, thankfully we have Archive.\n\n2:17:04.240 --> 2:17:05.320\n Archive, exactly.\n\n2:17:05.320 --> 2:17:08.160\n And then there's open review type of situations\n\n2:17:08.160 --> 2:17:11.680\n where you, and then, I mean, Twitter's a kind of open review.\n\n2:17:11.680 --> 2:17:13.880\n I'm a huge believer that review should be done\n\n2:17:13.880 --> 2:17:15.720\n by thousands of people, not two people.\n\n2:17:15.720 --> 2:17:16.760\n I agree.\n\n2:17:16.760 --> 2:17:19.560\n And so Archive, like do you see a future\n\n2:17:19.560 --> 2:17:21.240\n where a lot of really strong papers,\n\n2:17:21.240 --> 2:17:23.640\n it's already the present, but a growing future\n\n2:17:23.640 --> 2:17:25.320\n where it'll just be Archive\n\n2:17:26.280 --> 2:17:31.280\n and you're presenting an ongoing continuous conference\n\n2:17:31.280 --> 2:17:35.560\n called Twitter slash the internet slash Archive Sanity.\n\n2:17:35.560 --> 2:17:38.040\n Andre just released a new version.\n\n2:17:38.040 --> 2:17:40.920\n So just not, you know, not being so elitist\n\n2:17:40.920 --> 2:17:43.440\n about this particular gating.\n\n2:17:43.440 --> 2:17:44.960\n It's not a question of being elitist or not.\n\n2:17:44.960 --> 2:17:49.960\n It's a question of being basically recommendation\n\n2:17:50.120 --> 2:17:53.400\n and sort of approvals for people who don't see themselves\n\n2:17:53.400 --> 2:17:55.880\n as having the ability to do so by themselves, right?\n\n2:17:55.880 --> 2:17:57.320\n And so it saves time, right?\n\n2:17:57.320 --> 2:18:00.000\n If you rely on other people's opinion\n\n2:18:00.000 --> 2:18:03.760\n and you trust those people or those groups\n\n2:18:03.760 --> 2:18:08.760\n to evaluate a paper for you, that saves you time\n\n2:18:09.960 --> 2:18:12.680\n because, you know, you don't have to like scrutinize\n\n2:18:12.680 --> 2:18:15.200\n the paper as much, you know, is brought to your attention.\n\n2:18:15.200 --> 2:18:16.680\n I mean, it's the whole idea of sort of, you know,\n\n2:18:16.680 --> 2:18:18.760\n collective recommender system, right?\n\n2:18:18.760 --> 2:18:22.360\n So I actually thought about this a lot, you know,\n\n2:18:22.360 --> 2:18:24.200\n about 10, 15 years ago,\n\n2:18:24.200 --> 2:18:27.080\n because there were discussions at NIPS\n\n2:18:27.080 --> 2:18:30.040\n and, you know, and we're about to create iClear\n\n2:18:30.040 --> 2:18:31.200\n with Yoshua Bengio.\n\n2:18:31.200 --> 2:18:34.880\n And so I wrote a document kind of describing\n\n2:18:34.880 --> 2:18:38.040\n a reviewing system, which basically was, you know,\n\n2:18:38.040 --> 2:18:39.720\n you post your paper on some repository,\n\n2:18:39.720 --> 2:18:42.560\n let's say archive or now could be open review.\n\n2:18:42.560 --> 2:18:46.240\n And then you can form a reviewing entity,\n\n2:18:46.240 --> 2:18:48.840\n which is equivalent to a reviewing board, you know,\n\n2:18:48.840 --> 2:18:53.840\n of a journal or program committee of a conference.\n\n2:18:53.960 --> 2:18:55.600\n You have to list the members.\n\n2:18:55.600 --> 2:19:00.000\n And then that group reviewing entity can choose\n\n2:19:00.000 --> 2:19:03.720\n to review a particular paper spontaneously or not.\n\n2:19:03.720 --> 2:19:05.600\n There is no exclusive relationship anymore\n\n2:19:05.600 --> 2:19:09.200\n between a paper and a venue or reviewing entity.\n\n2:19:09.200 --> 2:19:11.240\n Any reviewing entity can review any paper\n\n2:19:12.720 --> 2:19:14.080\n or may choose not to.\n\n2:19:15.000 --> 2:19:16.640\n And then, you know, given evaluation,\n\n2:19:16.640 --> 2:19:17.920\n it's not published, not published,\n\n2:19:17.920 --> 2:19:20.320\n it's just an evaluation and a comment,\n\n2:19:20.320 --> 2:19:23.680\n which would be public, signed by the reviewing entity.\n\n2:19:23.680 --> 2:19:25.880\n And if it's signed by a reviewing entity,\n\n2:19:25.880 --> 2:19:27.760\n you know, it's one of the members of reviewing entity.\n\n2:19:27.760 --> 2:19:30.680\n So if the reviewing entity is, you know,\n\n2:19:30.680 --> 2:19:33.720\n Lex Friedman's, you know, preferred papers, right?\n\n2:19:33.720 --> 2:19:35.640\n You know, it's Lex Friedman writing the review.\n\n2:19:35.640 --> 2:19:40.640\n Yes, so for me, that's a beautiful system, I think.\n\n2:19:40.920 --> 2:19:42.880\n But in addition to that,\n\n2:19:42.880 --> 2:19:45.800\n it feels like there should be a reputation system\n\n2:19:45.800 --> 2:19:47.480\n for the reviewers.\n\n2:19:47.480 --> 2:19:49.040\n For the reviewing entities,\n\n2:19:49.040 --> 2:19:50.280\n not the reviewers individually.\n\n2:19:50.280 --> 2:19:51.720\n The reviewing entities, sure.\n\n2:19:51.720 --> 2:19:53.880\n But even within that, the reviewers too,\n\n2:19:53.880 --> 2:19:57.120\n because there's another thing here.\n\n2:19:57.120 --> 2:19:59.360\n It's not just the reputation,\n\n2:19:59.360 --> 2:20:02.680\n it's an incentive for an individual person to do great.\n\n2:20:02.680 --> 2:20:05.040\n Right now, in the academic setting,\n\n2:20:05.040 --> 2:20:07.880\n the incentive is kind of internal,\n\n2:20:07.880 --> 2:20:09.240\n just wanting to do a good job.\n\n2:20:09.240 --> 2:20:11.240\n But honestly, that's not a strong enough incentive\n\n2:20:11.240 --> 2:20:13.720\n to do a really good job in reading a paper,\n\n2:20:13.720 --> 2:20:16.400\n in finding the beautiful amidst the mistakes and the flaws\n\n2:20:16.400 --> 2:20:17.760\n and all that kind of stuff.\n\n2:20:17.760 --> 2:20:20.760\n Like if you're the person that first discovered\n\n2:20:20.760 --> 2:20:25.120\n a powerful paper, and you get to be proud of that discovery,\n\n2:20:25.120 --> 2:20:27.520\n then that gives a huge incentive to you.\n\n2:20:27.520 --> 2:20:29.280\n That's a big part of my proposal, actually,\n\n2:20:29.280 --> 2:20:31.280\n where I describe that as, you know,\n\n2:20:31.280 --> 2:20:35.280\n if your evaluation of papers is predictive\n\n2:20:35.280 --> 2:20:37.560\n of future success, okay,\n\n2:20:37.560 --> 2:20:40.920\n then your reputation should go up as a reviewing entity.\n\n2:20:42.560 --> 2:20:43.760\n So yeah, exactly.\n\n2:20:43.760 --> 2:20:46.280\n I mean, I even had a master's student\n\n2:20:46.280 --> 2:20:49.560\n who was a master's student in library science\n\n2:20:49.560 --> 2:20:52.480\n and computer science actually kind of work out exactly\n\n2:20:52.480 --> 2:20:55.160\n how that should work with formulas and everything.\n\n2:20:55.160 --> 2:20:56.800\n So in terms of implementation,\n\n2:20:56.800 --> 2:20:58.640\n do you think that's something that's doable?\n\n2:20:58.640 --> 2:20:59.720\n I mean, I've been sort of, you know,\n\n2:20:59.720 --> 2:21:02.080\n talking about this to sort of various people\n\n2:21:02.080 --> 2:21:05.960\n like, you know, Andrew McCallum, who started Open Review.\n\n2:21:05.960 --> 2:21:07.800\n And the reason why we picked Open Review\n\n2:21:07.800 --> 2:21:09.120\n for iClear initially,\n\n2:21:09.120 --> 2:21:11.440\n even though it was very early for them,\n\n2:21:11.440 --> 2:21:14.320\n is because my hope was that iClear,\n\n2:21:14.320 --> 2:21:16.760\n it was eventually going to kind of\n\n2:21:16.760 --> 2:21:18.600\n inaugurate this type of system.\n\n2:21:18.600 --> 2:21:22.240\n So iClear kept the idea of Open Reviews.\n\n2:21:22.240 --> 2:21:23.840\n So where the reviews are, you know,\n\n2:21:23.840 --> 2:21:27.320\n published with a paper, which I think is very useful,\n\n2:21:27.320 --> 2:21:29.800\n but in many ways that's kind of reverted\n\n2:21:29.800 --> 2:21:33.280\n to kind of more of a conventional type conferences\n\n2:21:33.280 --> 2:21:34.120\n for everything else.\n\n2:21:34.120 --> 2:21:37.800\n And that, I mean, I don't run iClear.\n\n2:21:37.800 --> 2:21:41.200\n I'm just the president of the foundation,\n\n2:21:41.200 --> 2:21:44.120\n but you know, people who run it\n\n2:21:44.120 --> 2:21:45.680\n should make decisions about how to run it.\n\n2:21:45.680 --> 2:21:48.560\n And I'm not going to tell them because they are volunteers\n\n2:21:48.560 --> 2:21:50.360\n and I'm really thankful that they do that.\n\n2:21:50.360 --> 2:21:53.040\n So, but I'm saddened by the fact\n\n2:21:53.040 --> 2:21:57.120\n that we're not being innovative enough.\n\n2:21:57.120 --> 2:21:57.960\n Yeah, me too.\n\n2:21:57.960 --> 2:21:59.640\n I hope that changes.\n\n2:21:59.640 --> 2:22:00.480\n Yeah.\n\n2:22:00.480 --> 2:22:02.040\n Cause the communication science broadly,\n\n2:22:02.040 --> 2:22:04.200\n but communication computer science ideas\n\n2:22:05.440 --> 2:22:08.400\n is how you make those ideas have impact, I think.\n\n2:22:08.400 --> 2:22:11.440\n Yeah, and I think, you know, a lot of this is\n\n2:22:11.440 --> 2:22:16.200\n because people have in their mind kind of an objective,\n\n2:22:16.200 --> 2:22:19.120\n which is, you know, fairness for authors\n\n2:22:19.120 --> 2:22:22.600\n and the ability to count points basically\n\n2:22:22.600 --> 2:22:24.880\n and give credits accurately.\n\n2:22:24.880 --> 2:22:28.880\n But that comes at the expense of the progress of science.\n\n2:22:28.880 --> 2:22:29.720\n So to some extent,\n\n2:22:29.720 --> 2:22:32.160\n we're slowing down the progress of science.\n\n2:22:32.160 --> 2:22:34.440\n And are we actually achieving fairness?\n\n2:22:34.440 --> 2:22:35.920\n And we're not achieving fairness.\n\n2:22:35.920 --> 2:22:37.880\n You know, we still have biases.\n\n2:22:37.880 --> 2:22:39.840\n You know, we're doing, you know, a double blind review,\n\n2:22:39.840 --> 2:22:44.360\n but you know, the biases are still there.\n\n2:22:44.360 --> 2:22:46.720\n There are different kinds of biases.\n\n2:22:46.720 --> 2:22:49.360\n You write that the phenomenon of emergence,\n\n2:22:49.360 --> 2:22:51.680\n collective behavior exhibited by a large collection\n\n2:22:51.680 --> 2:22:54.280\n of simple elements in interaction\n\n2:22:54.280 --> 2:22:55.760\n is one of the things that got you\n\n2:22:55.760 --> 2:22:57.760\n into neural nets in the first place.\n\n2:22:57.760 --> 2:22:59.120\n I love cellular automata.\n\n2:22:59.120 --> 2:23:02.000\n I love simple interacting elements\n\n2:23:02.000 --> 2:23:04.040\n and the things that emerge from them.\n\n2:23:04.040 --> 2:23:07.880\n Do you think we understand how complex systems can emerge\n\n2:23:07.880 --> 2:23:11.080\n from such simple components that interact simply?\n\n2:23:11.080 --> 2:23:12.320\n No, we don't.\n\n2:23:12.320 --> 2:23:13.160\n It's a big mystery.\n\n2:23:13.160 --> 2:23:14.480\n Also, it's a mystery for physicists.\n\n2:23:14.480 --> 2:23:16.040\n It's a mystery for biologists.\n\n2:23:17.000 --> 2:23:22.000\n You know, how is it that the universe around us\n\n2:23:22.000 --> 2:23:25.120\n seems to be increasing in complexity and not decreasing?\n\n2:23:25.120 --> 2:23:29.640\n I mean, that is a kind of curious property of physics\n\n2:23:29.640 --> 2:23:32.320\n that despite the second law of thermodynamics,\n\n2:23:32.320 --> 2:23:35.960\n we seem to be, you know, evolution and learning\n\n2:23:35.960 --> 2:23:39.640\n and et cetera seems to be kind of at least locally\n\n2:23:40.640 --> 2:23:44.000\n to increase complexity and not decrease it.\n\n2:23:44.000 --> 2:23:46.520\n So perhaps the ultimate purpose of the universe\n\n2:23:46.520 --> 2:23:49.040\n is to just get more complex.\n\n2:23:49.040 --> 2:23:54.040\n Have these, I mean, small pockets of beautiful complexity.\n\n2:23:55.120 --> 2:23:57.120\n Does that, cellular automata,\n\n2:23:57.120 --> 2:23:59.680\n these kinds of emergence of complex systems\n\n2:23:59.680 --> 2:24:04.120\n give you some intuition or guide your understanding\n\n2:24:04.120 --> 2:24:06.680\n of machine learning systems and neural networks and so on?\n\n2:24:06.680 --> 2:24:09.440\n Or are these, for you right now, disparate concepts?\n\n2:24:09.440 --> 2:24:10.880\n Well, it got me into it.\n\n2:24:10.880 --> 2:24:15.600\n You know, I discovered the existence of the perceptron\n\n2:24:15.600 --> 2:24:19.280\n when I was a college student, you know, by reading a book\n\n2:24:19.280 --> 2:24:21.680\n and it was a debate between Chomsky and Piaget\n\n2:24:21.680 --> 2:24:25.920\n and Seymour Papert from MIT was kind of singing the praise\n\n2:24:25.920 --> 2:24:27.400\n of the perceptron in that book.\n\n2:24:27.400 --> 2:24:29.760\n And I, the first time I heard about the running machine,\n\n2:24:29.760 --> 2:24:31.360\n right, so I started digging the literature\n\n2:24:31.360 --> 2:24:33.560\n and I found those paper, those books,\n\n2:24:33.560 --> 2:24:37.120\n which were basically transcription of workshops\n\n2:24:37.120 --> 2:24:39.880\n or conferences from the fifties and sixties\n\n2:24:39.880 --> 2:24:42.160\n about self organizing systems.\n\n2:24:42.160 --> 2:24:44.560\n So there were, there was a series of conferences\n\n2:24:44.560 --> 2:24:48.160\n on self organizing systems and there's books on this.\n\n2:24:48.160 --> 2:24:50.200\n Some of them are, you can actually get them\n\n2:24:50.200 --> 2:24:53.240\n at the internet archive, you know, the digital version.\n\n2:24:55.120 --> 2:24:58.280\n And there are like fascinating articles in there by,\n\n2:24:58.280 --> 2:25:00.360\n there's a guy whose name has been largely forgotten,\n\n2:25:00.360 --> 2:25:04.520\n Heinz von F\u00f6rster, he's a German physicist\n\n2:25:04.520 --> 2:25:07.240\n who immigrated to the US and worked\n\n2:25:07.240 --> 2:25:11.320\n on self organizing systems in the fifties.\n\n2:25:11.320 --> 2:25:13.800\n And in the sixties he created at University of Illinois\n\n2:25:13.800 --> 2:25:16.440\n at Urbana Champagne, he created the biological\n\n2:25:16.440 --> 2:25:20.440\n computer laboratory, BCL, which was all about neural nets.\n\n2:25:21.680 --> 2:25:23.440\n Unfortunately, that was kind of towards the end\n\n2:25:23.440 --> 2:25:24.920\n of the popularity of neural nets.\n\n2:25:24.920 --> 2:25:27.760\n So that lab never kind of strived very much,\n\n2:25:27.760 --> 2:25:30.360\n but he wrote a bunch of papers about self organization\n\n2:25:30.360 --> 2:25:33.480\n and about the mystery of self organization.\n\n2:25:33.480 --> 2:25:37.000\n An example he has is you take, imagine you are in space,\n\n2:25:37.000 --> 2:25:38.880\n there's no gravity and you have a big box\n\n2:25:38.880 --> 2:25:42.200\n with magnets in it, okay.\n\n2:25:42.200 --> 2:25:43.920\n You know, kind of rectangular magnets\n\n2:25:43.920 --> 2:25:46.880\n with North Pole on one end, South Pole on the other end.\n\n2:25:46.880 --> 2:25:49.640\n You shake the box gently and the magnets will kind of stick\n\n2:25:49.640 --> 2:25:52.440\n to themselves and probably form like complex structure,\n\n2:25:53.480 --> 2:25:55.280\n you know, spontaneously.\n\n2:25:55.280 --> 2:25:57.120\n You know, that could be an example of self organization,\n\n2:25:57.120 --> 2:25:58.400\n but you know, you have lots of examples,\n\n2:25:58.400 --> 2:26:01.280\n neural nets are an example of self organization too,\n\n2:26:01.280 --> 2:26:03.080\n you know, in many respect.\n\n2:26:03.080 --> 2:26:05.960\n And it's a bit of a mystery, you know,\n\n2:26:05.960 --> 2:26:09.520\n how like what is possible with this, you know,\n\n2:26:09.520 --> 2:26:12.960\n pattern formation in physical systems, in chaotic system\n\n2:26:12.960 --> 2:26:16.120\n and things like that, you know, the emergence of life,\n\n2:26:16.120 --> 2:26:16.960\n you know, things like that.\n\n2:26:16.960 --> 2:26:19.560\n So, you know, how does that happen?\n\n2:26:19.560 --> 2:26:22.600\n So it's a big puzzle for physicists as well.\n\n2:26:22.600 --> 2:26:24.720\n It feels like understanding this,\n\n2:26:24.720 --> 2:26:27.920\n the mathematics of emergence\n\n2:26:27.920 --> 2:26:29.720\n in some constrained situations\n\n2:26:29.720 --> 2:26:32.120\n might help us create intelligence,\n\n2:26:32.120 --> 2:26:36.040\n like help us add a little spice to the systems\n\n2:26:36.040 --> 2:26:40.960\n because you seem to be able to in complex systems\n\n2:26:40.960 --> 2:26:44.600\n with emergence to be able to get a lot from little.\n\n2:26:44.600 --> 2:26:47.000\n And so that seems like a shortcut\n\n2:26:47.000 --> 2:26:51.120\n to get big leaps in performance, but...\n\n2:26:51.120 --> 2:26:55.000\n But there's a missing concept that we don't have.\n\n2:26:55.000 --> 2:26:55.840\n Yeah.\n\n2:26:55.840 --> 2:26:58.440\n And it's something also I've been fascinated by\n\n2:26:58.440 --> 2:27:00.720\n since my undergrad days,\n\n2:27:00.720 --> 2:27:03.880\n and it's how you measure complexity, right?\n\n2:27:03.880 --> 2:27:06.960\n So we don't actually have good ways of measuring,\n\n2:27:06.960 --> 2:27:09.840\n or at least we don't have good ways of interpreting\n\n2:27:09.840 --> 2:27:11.920\n the measures that we have at our disposal.\n\n2:27:11.920 --> 2:27:14.480\n Like how do you measure the complexity of something, right?\n\n2:27:14.480 --> 2:27:15.680\n So there's all those things, you know,\n\n2:27:15.680 --> 2:27:18.560\n like, you know, Kolmogorov, Chaitin, Solomonov complexity\n\n2:27:18.560 --> 2:27:20.920\n of, you know, the length of the shortest program\n\n2:27:20.920 --> 2:27:23.320\n that would generate a bit string can be thought of\n\n2:27:23.320 --> 2:27:25.520\n as the complexity of that bit string, right?\n\n2:27:26.840 --> 2:27:28.200\n I've been fascinated by that concept.\n\n2:27:28.200 --> 2:27:30.160\n The problem with that is that\n\n2:27:30.160 --> 2:27:32.840\n that complexity is defined up to a constant,\n\n2:27:32.840 --> 2:27:33.920\n which can be very large.\n\n2:27:34.920 --> 2:27:35.760\n Right.\n\n2:27:35.760 --> 2:27:37.840\n There are similar concepts that are derived from,\n\n2:27:37.840 --> 2:27:42.280\n you know, Bayesian probability theory,\n\n2:27:42.280 --> 2:27:44.520\n where, you know, the complexity of something\n\n2:27:44.520 --> 2:27:48.360\n is the negative log of its probability, essentially, right?\n\n2:27:48.360 --> 2:27:51.120\n And you have a complete equivalence between the two things.\n\n2:27:51.120 --> 2:27:52.120\n And there you would think, you know,\n\n2:27:52.120 --> 2:27:55.160\n the probability is something that's well defined mathematically,\n\n2:27:55.160 --> 2:27:57.200\n which means complexity is well defined.\n\n2:27:57.200 --> 2:27:58.040\n But it's not true.\n\n2:27:58.040 --> 2:28:01.720\n You need to have a model of the distribution.\n\n2:28:01.720 --> 2:28:02.800\n You may need to have a prior\n\n2:28:02.800 --> 2:28:04.200\n if you're doing Bayesian inference.\n\n2:28:04.200 --> 2:28:05.720\n And the prior plays the same role\n\n2:28:05.720 --> 2:28:07.040\n as the choice of the computer\n\n2:28:07.040 --> 2:28:09.480\n with which you measure Kolmogorov complexity.\n\n2:28:09.480 --> 2:28:12.040\n And so every measure of complexity we have\n\n2:28:12.040 --> 2:28:13.600\n has some arbitrary density,\n\n2:28:15.440 --> 2:28:16.840\n you know, an additive constant,\n\n2:28:16.840 --> 2:28:19.560\n which can be arbitrarily large.\n\n2:28:19.560 --> 2:28:23.360\n And so, you know, how can we come up with a good theory\n\n2:28:23.360 --> 2:28:24.640\n of how things become more complex\n\n2:28:24.640 --> 2:28:26.080\n if we don't have a good measure of complexity?\n\n2:28:26.080 --> 2:28:28.200\n Yeah, which we need for this.\n\n2:28:28.200 --> 2:28:32.240\n One way that people study this in the space of biology,\n\n2:28:32.240 --> 2:28:33.760\n the people that study the origin of life\n\n2:28:33.760 --> 2:28:37.120\n or try to recreate the life in the laboratory.\n\n2:28:37.120 --> 2:28:39.200\n And the more interesting one is the alien one,\n\n2:28:39.200 --> 2:28:41.320\n is when we go to other planets,\n\n2:28:41.320 --> 2:28:43.960\n how do we recognize this life?\n\n2:28:43.960 --> 2:28:46.800\n Because, you know, complexity, we associate complexity,\n\n2:28:46.800 --> 2:28:49.000\n maybe some level of mobility with life.\n\n2:28:50.000 --> 2:28:51.680\n You know, we have to be able to, like,\n\n2:28:51.680 --> 2:28:56.680\n have concrete algorithms for, like,\n\n2:28:57.200 --> 2:29:00.000\n measuring the level of complexity we see\n\n2:29:00.000 --> 2:29:02.760\n in order to know the difference between life and non life.\n\n2:29:02.760 --> 2:29:04.040\n And the problem is that complexity\n\n2:29:04.040 --> 2:29:05.440\n is in the eye of the beholder.\n\n2:29:05.440 --> 2:29:07.480\n So let me give you an example.\n\n2:29:07.480 --> 2:29:12.480\n If I give you an image of the MNIST digits, right,\n\n2:29:13.240 --> 2:29:15.400\n and I flip through MNIST digits,\n\n2:29:15.400 --> 2:29:18.120\n there is obviously some structure to it\n\n2:29:18.120 --> 2:29:20.440\n because local structure, you know,\n\n2:29:20.440 --> 2:29:22.240\n neighboring pixels are correlated\n\n2:29:23.200 --> 2:29:25.440\n across the entire data set.\n\n2:29:25.440 --> 2:29:30.440\n I imagine that I apply a random permutation\n\n2:29:30.440 --> 2:29:33.920\n to all the pixels, a fixed random permutation.\n\n2:29:33.920 --> 2:29:35.360\n Now I show you those images,\n\n2:29:35.360 --> 2:29:38.880\n they will look, you know, really disorganized to you,\n\n2:29:38.880 --> 2:29:40.680\n more complex.\n\n2:29:40.680 --> 2:29:42.880\n In fact, they're not more complex in absolute terms,\n\n2:29:42.880 --> 2:29:45.480\n they're exactly the same as originally, right?\n\n2:29:45.480 --> 2:29:46.960\n And if you knew what the permutation was,\n\n2:29:46.960 --> 2:29:49.440\n you know, you could undo the permutation.\n\n2:29:49.440 --> 2:29:52.360\n Now, imagine I give you special glasses\n\n2:29:52.360 --> 2:29:54.120\n that undo that permutation.\n\n2:29:54.120 --> 2:29:56.160\n Now, all of a sudden, what looked complicated\n\n2:29:56.160 --> 2:29:57.000\n becomes simple.\n\n2:29:57.000 --> 2:29:57.920\n Right.\n\n2:29:57.920 --> 2:30:00.400\n So if you have two, if you have, you know,\n\n2:30:00.400 --> 2:30:03.280\n humans on one end, and then another race of aliens\n\n2:30:03.280 --> 2:30:05.440\n that sees the universe with permutation glasses.\n\n2:30:05.440 --> 2:30:06.600\n Yeah, with the permutation glasses.\n\n2:30:06.600 --> 2:30:09.800\n Okay, what we perceive as simple to them\n\n2:30:09.800 --> 2:30:11.760\n is hardly complicated, it's probably heat.\n\n2:30:11.760 --> 2:30:12.600\n Yeah.\n\n2:30:12.600 --> 2:30:13.440\n Heat, yeah.\n\n2:30:13.440 --> 2:30:15.320\n Okay, and what they perceive as simple to us\n\n2:30:15.320 --> 2:30:18.480\n is random fluctuation, it's heat.\n\n2:30:18.480 --> 2:30:19.320\n Yeah.\n\n2:30:19.320 --> 2:30:22.760\n Yeah, it's truly in the eye of the beholder.\n\n2:30:22.760 --> 2:30:23.600\n Yeah.\n\n2:30:23.600 --> 2:30:24.920\n It depends what kind of glasses you're wearing.\n\n2:30:24.920 --> 2:30:25.760\n Right.\n\n2:30:25.760 --> 2:30:26.840\n It depends what kind of algorithm you're running\n\n2:30:26.840 --> 2:30:28.360\n in your perception system.\n\n2:30:28.360 --> 2:30:31.080\n So I don't think we'll have a theory of intelligence,\n\n2:30:31.080 --> 2:30:34.320\n self organization, evolution, things like this,\n\n2:30:34.320 --> 2:30:38.520\n until we have a good handle on a notion of complexity\n\n2:30:38.520 --> 2:30:40.800\n which we know is in the eye of the beholder.\n\n2:30:42.320 --> 2:30:44.400\n Yeah, it's sad to think that we might not be able\n\n2:30:44.400 --> 2:30:47.600\n to detect or interact with alien species\n\n2:30:47.600 --> 2:30:50.280\n because we're wearing different glasses.\n\n2:30:50.280 --> 2:30:51.440\n Because their notion of locality\n\n2:30:51.440 --> 2:30:52.400\n might be different from ours.\n\n2:30:52.400 --> 2:30:53.240\n Yeah, exactly.\n\n2:30:53.240 --> 2:30:55.200\n This actually connects with fascinating questions\n\n2:30:55.200 --> 2:30:58.120\n in physics at the moment, like modern physics,\n\n2:30:58.120 --> 2:31:00.240\n quantum physics, like, you know, questions about,\n\n2:31:00.240 --> 2:31:02.520\n like, you know, can we recover the information\n\n2:31:02.520 --> 2:31:04.520\n that's lost in a black hole and things like this, right?\n\n2:31:04.520 --> 2:31:07.920\n And that relies on notions of complexity,\n\n2:31:09.360 --> 2:31:11.640\n which, you know, I find this fascinating.\n\n2:31:11.640 --> 2:31:13.360\n Can you describe your personal quest\n\n2:31:13.360 --> 2:31:18.360\n to build an expressive electronic wind instrument, EWI?\n\n2:31:19.760 --> 2:31:20.600\n What is it?\n\n2:31:20.600 --> 2:31:24.000\n What does it take to build it?\n\n2:31:24.000 --> 2:31:25.080\n Well, I'm a tinker.\n\n2:31:25.080 --> 2:31:26.760\n I like building things.\n\n2:31:26.760 --> 2:31:28.960\n I like building things with combinations of electronics\n\n2:31:28.960 --> 2:31:31.040\n and, you know, mechanical stuff.\n\n2:31:32.400 --> 2:31:34.120\n You know, I have a bunch of different hobbies,\n\n2:31:34.120 --> 2:31:37.960\n but, you know, probably my first one was little,\n\n2:31:37.960 --> 2:31:39.800\n was building model airplanes and stuff like that.\n\n2:31:39.800 --> 2:31:41.880\n And I still do that to some extent.\n\n2:31:41.880 --> 2:31:43.800\n But also electronics, I taught myself electronics\n\n2:31:43.800 --> 2:31:45.160\n before I studied it.\n\n2:31:46.240 --> 2:31:48.120\n And the reason I taught myself electronics\n\n2:31:48.120 --> 2:31:49.600\n is because of music.\n\n2:31:49.600 --> 2:31:53.200\n My cousin was an aspiring electronic musician\n\n2:31:53.200 --> 2:31:55.000\n and he had an analog synthesizer.\n\n2:31:55.000 --> 2:31:58.000\n And I was, you know, basically modifying it for him\n\n2:31:58.000 --> 2:32:00.280\n and building sequencers and stuff like that, right, for him.\n\n2:32:00.280 --> 2:32:02.640\n I was in high school when I was doing this.\n\n2:32:02.640 --> 2:32:06.040\n That's the interesting, like, progressive rock, like 80s.\n\n2:32:06.040 --> 2:32:08.000\n Like, what's the greatest band of all time,\n\n2:32:08.000 --> 2:32:09.520\n according to Yann LeCun?\n\n2:32:09.520 --> 2:32:11.080\n Oh, man, there's too many of them.\n\n2:32:11.080 --> 2:32:16.080\n But, you know, it's a combination of, you know,\n\n2:32:16.360 --> 2:32:19.800\n Mahavishnu Orchestra, Weather Report,\n\n2:32:19.800 --> 2:32:24.800\n yes, Genesis, you know, pre Peter Gabriel,\n\n2:32:27.120 --> 2:32:29.120\n Gentle Giant, you know, things like that.\n\n2:32:29.120 --> 2:32:29.960\n Great.\n\n2:32:29.960 --> 2:32:32.280\n Okay, so this love of electronics\n\n2:32:32.280 --> 2:32:34.240\n and this love of music combined together.\n\n2:32:34.240 --> 2:32:36.280\n Right, so I was actually trained to play\n\n2:32:36.280 --> 2:32:41.280\n Baroque and Renaissance music and I played in an orchestra\n\n2:32:42.040 --> 2:32:45.640\n when I was in high school and first years of college.\n\n2:32:45.640 --> 2:32:48.040\n And I played the recorder, crumb horn,\n\n2:32:48.040 --> 2:32:50.200\n a little bit of oboe, you know, things like that.\n\n2:32:50.200 --> 2:32:52.520\n So I'm a wind instrument player.\n\n2:32:52.520 --> 2:32:54.080\n But I always wanted to play improvised music,\n\n2:32:54.080 --> 2:32:56.320\n even though I don't know anything about it.\n\n2:32:56.320 --> 2:32:58.760\n And the only way I figured, you know,\n\n2:32:58.760 --> 2:33:01.080\n short of like learning to play saxophone\n\n2:33:01.080 --> 2:33:03.560\n was to play electronic wind instruments.\n\n2:33:03.560 --> 2:33:05.680\n So they behave, you know, the fingering is similar\n\n2:33:05.680 --> 2:33:07.640\n to a saxophone, but, you know,\n\n2:33:07.640 --> 2:33:09.080\n you have wide variety of sound\n\n2:33:09.080 --> 2:33:11.040\n because you control the synthesizer with it.\n\n2:33:11.040 --> 2:33:13.120\n So I had a bunch of those, you know,\n\n2:33:13.120 --> 2:33:18.120\n going back to the late 80s from either Yamaha or Akai.\n\n2:33:18.880 --> 2:33:22.520\n They're both kind of the main manufacturers of those.\n\n2:33:22.520 --> 2:33:23.720\n So they were classically, you know,\n\n2:33:23.720 --> 2:33:25.520\n going back several decades.\n\n2:33:25.520 --> 2:33:27.680\n But I've never been completely satisfied with them\n\n2:33:27.680 --> 2:33:29.280\n because of lack of expressivity.\n\n2:33:31.120 --> 2:33:32.480\n And, you know, those things, you know,\n\n2:33:32.480 --> 2:33:33.400\n are somewhat expressive.\n\n2:33:33.400 --> 2:33:34.760\n I mean, they measure the breath pressure,\n\n2:33:34.760 --> 2:33:36.520\n they measure the lip pressure.\n\n2:33:36.520 --> 2:33:39.800\n And, you know, you have various parameters.\n\n2:33:39.800 --> 2:33:41.480\n You can vary with fingers,\n\n2:33:41.480 --> 2:33:44.800\n but they're not really as expressive\n\n2:33:44.800 --> 2:33:47.040\n as an acoustic instrument, right?\n\n2:33:47.040 --> 2:33:49.400\n You hear John Coltrane play two notes\n\n2:33:49.400 --> 2:33:50.760\n and you know it's John Coltrane,\n\n2:33:50.760 --> 2:33:53.000\n you know, it's got a unique sound.\n\n2:33:53.000 --> 2:33:54.280\n Or Miles Davis, right?\n\n2:33:54.280 --> 2:33:57.480\n You can hear it's Miles Davis playing the trumpet\n\n2:33:57.480 --> 2:34:02.480\n because the sound reflects their, you know,\n\n2:34:02.480 --> 2:34:05.800\n physiognomy, basically, the shape of the vocal track\n\n2:34:07.600 --> 2:34:09.200\n kind of shapes the sound.\n\n2:34:09.200 --> 2:34:12.320\n So how do you do this with an electronic instrument?\n\n2:34:12.320 --> 2:34:13.920\n And I was, many years ago,\n\n2:34:13.920 --> 2:34:15.640\n I met a guy called David Wessel.\n\n2:34:15.640 --> 2:34:18.240\n He was a professor at Berkeley\n\n2:34:18.240 --> 2:34:23.000\n and created the Center for Music Technology there.\n\n2:34:23.000 --> 2:34:25.600\n And he was interested in that question.\n\n2:34:25.600 --> 2:34:28.120\n And so I kept kind of thinking about this for many years.\n\n2:34:28.120 --> 2:34:31.040\n And finally, because of COVID, you know, I was at home,\n\n2:34:31.040 --> 2:34:32.600\n I was in my workshop.\n\n2:34:32.600 --> 2:34:36.040\n My workshop serves also as my kind of Zoom room\n\n2:34:36.040 --> 2:34:37.360\n and home office.\n\n2:34:37.360 --> 2:34:38.800\n And this is in New Jersey?\n\n2:34:38.800 --> 2:34:39.640\n In New Jersey.\n\n2:34:39.640 --> 2:34:43.600\n And I started really being serious about, you know,\n\n2:34:43.600 --> 2:34:45.800\n building my own iwi instrument.\n\n2:34:45.800 --> 2:34:48.160\n What else is going on in that New Jersey workshop?\n\n2:34:48.160 --> 2:34:50.880\n Is there some crazy stuff you've built,\n\n2:34:50.880 --> 2:34:55.200\n like just, or like left on the workshop floor, left behind?\n\n2:34:55.200 --> 2:34:57.600\n A lot of crazy stuff is, you know,\n\n2:34:57.600 --> 2:35:01.680\n electronics built with microcontrollers of various kinds\n\n2:35:01.680 --> 2:35:04.880\n and, you know, weird flying contraptions.\n\n2:35:06.720 --> 2:35:08.720\n So you still love flying?\n\n2:35:08.720 --> 2:35:09.880\n It's a family disease.\n\n2:35:09.880 --> 2:35:13.520\n My dad got me into it when I was a kid.\n\n2:35:13.520 --> 2:35:16.840\n And he was building model airplanes when he was a kid.\n\n2:35:16.840 --> 2:35:19.800\n And he was a mechanical engineer.\n\n2:35:19.800 --> 2:35:21.200\n He taught himself electronics also.\n\n2:35:21.200 --> 2:35:24.080\n So he built his early radio control systems\n\n2:35:24.080 --> 2:35:27.760\n in the late 60s, early 70s.\n\n2:35:27.760 --> 2:35:29.640\n And so that's what got me into,\n\n2:35:29.640 --> 2:35:31.120\n I mean, he got me into kind of, you know,\n\n2:35:31.120 --> 2:35:33.040\n engineering and science and technology.\n\n2:35:33.040 --> 2:35:36.120\n Do you also have an interest in appreciation of flight\n\n2:35:36.120 --> 2:35:38.320\n in other forms, like with drones, quadroptors,\n\n2:35:38.320 --> 2:35:41.720\n or do you, is it model airplane, the thing that's?\n\n2:35:41.720 --> 2:35:45.240\n You know, before drones were, you know,\n\n2:35:45.240 --> 2:35:49.240\n kind of a consumer product, you know,\n\n2:35:49.240 --> 2:35:50.280\n I built my own, you know,\n\n2:35:50.280 --> 2:35:52.000\n with also building a microcontroller\n\n2:35:52.000 --> 2:35:56.240\n with JavaScripts and accelerometers for stabilization,\n\n2:35:56.240 --> 2:35:57.760\n writing the firmware for it, you know.\n\n2:35:57.760 --> 2:35:59.200\n And then when it became kind of a standard thing\n\n2:35:59.200 --> 2:36:00.320\n you could buy, it was boring, you know,\n\n2:36:00.320 --> 2:36:01.160\n I stopped doing it.\n\n2:36:01.160 --> 2:36:02.440\n It was not fun anymore.\n\n2:36:03.520 --> 2:36:04.720\n Yeah.\n\n2:36:04.720 --> 2:36:06.280\n You were doing it before it was cool.\n\n2:36:06.280 --> 2:36:07.120\n Yeah.\n\n2:36:07.120 --> 2:36:10.080\n What advice would you give to a young person today\n\n2:36:10.080 --> 2:36:11.360\n in high school and college\n\n2:36:11.360 --> 2:36:15.960\n that dreams of doing something big like Yann LeCun,\n\n2:36:15.960 --> 2:36:18.960\n like let's talk in the space of intelligence,\n\n2:36:18.960 --> 2:36:21.000\n dreams of having a chance to solve\n\n2:36:21.000 --> 2:36:23.960\n some fundamental problem in space of intelligence,\n\n2:36:23.960 --> 2:36:26.200\n both for their career and just in life,\n\n2:36:26.200 --> 2:36:28.600\n being somebody who was a part\n\n2:36:28.600 --> 2:36:30.680\n of creating something special?\n\n2:36:30.680 --> 2:36:35.400\n So try to get interested by big questions,\n\n2:36:35.400 --> 2:36:38.680\n things like, you know, what is intelligence?\n\n2:36:38.680 --> 2:36:40.440\n What is the universe made of?\n\n2:36:40.440 --> 2:36:41.680\n What's life all about?\n\n2:36:41.680 --> 2:36:42.520\n Things like that.\n\n2:36:45.040 --> 2:36:47.040\n Like even like crazy big questions,\n\n2:36:47.040 --> 2:36:49.040\n like what's time?\n\n2:36:49.040 --> 2:36:51.040\n Like nobody knows what time is.\n\n2:36:53.160 --> 2:36:58.160\n And then learn basic things,\n\n2:36:58.640 --> 2:37:00.680\n like basic methods, either from math,\n\n2:37:00.680 --> 2:37:02.280\n from physics or from engineering.\n\n2:37:03.280 --> 2:37:05.600\n Things that have a long shelf life.\n\n2:37:05.600 --> 2:37:07.280\n Like if you have a choice between,\n\n2:37:07.280 --> 2:37:10.160\n like, you know, learning, you know,\n\n2:37:10.160 --> 2:37:11.720\n mobile programming on iPhone\n\n2:37:12.600 --> 2:37:14.840\n or quantum mechanics, take quantum mechanics.\n\n2:37:16.880 --> 2:37:18.480\n Because you're gonna learn things\n\n2:37:18.480 --> 2:37:20.120\n that you have no idea exist.\n\n2:37:20.120 --> 2:37:25.120\n And you may not, you may never be a quantum physicist,\n\n2:37:25.320 --> 2:37:26.800\n but you will learn about path integrals.\n\n2:37:26.800 --> 2:37:29.120\n And path integrals are used everywhere.\n\n2:37:29.120 --> 2:37:30.280\n It's the same formula that you use\n\n2:37:30.280 --> 2:37:33.280\n for, you know, Bayesian integration and stuff like that.\n\n2:37:33.280 --> 2:37:37.720\n So the ideas, the little ideas within quantum mechanics,\n\n2:37:37.720 --> 2:37:41.440\n within some of these kind of more solidified fields\n\n2:37:41.440 --> 2:37:42.920\n will have a longer shelf life.\n\n2:37:42.920 --> 2:37:46.920\n You'll somehow use indirectly in your work.\n\n2:37:46.920 --> 2:37:48.640\n Learn classical mechanics, like you'll learn\n\n2:37:48.640 --> 2:37:50.120\n about Lagrangian, for example,\n\n2:37:51.360 --> 2:37:55.000\n which is like a huge, hugely useful concept,\n\n2:37:55.000 --> 2:37:57.320\n you know, for all kinds of different things.\n\n2:37:57.320 --> 2:38:01.680\n Learn statistical physics, because all the math\n\n2:38:01.680 --> 2:38:04.360\n that comes out of, you know, for machine learning\n\n2:38:05.480 --> 2:38:07.280\n basically comes out of, was figured out\n\n2:38:07.280 --> 2:38:09.240\n by statistical physicists in the, you know,\n\n2:38:09.240 --> 2:38:10.960\n late 19th, early 20th century, right?\n\n2:38:10.960 --> 2:38:14.320\n So, and for some of them actually more recently\n\n2:38:14.320 --> 2:38:16.120\n for, by people like Giorgio Parisi,\n\n2:38:16.120 --> 2:38:19.040\n who just got the Nobel prize for the replica method,\n\n2:38:19.040 --> 2:38:23.200\n among other things, it's used for a lot of different things.\n\n2:38:23.200 --> 2:38:25.560\n You know, variational inference,\n\n2:38:25.560 --> 2:38:27.640\n that math comes from statistical physics.\n\n2:38:28.600 --> 2:38:33.600\n So a lot of those kind of, you know, basic courses,\n\n2:38:33.960 --> 2:38:36.240\n you know, if you do electrical engineering,\n\n2:38:36.240 --> 2:38:37.360\n you take signal processing,\n\n2:38:37.360 --> 2:38:39.880\n you'll learn about Fourier transforms.\n\n2:38:39.880 --> 2:38:42.720\n Again, something super useful is at the basis\n\n2:38:42.720 --> 2:38:44.920\n of things like graph neural nets,\n\n2:38:44.920 --> 2:38:49.400\n which is an entirely new sub area of, you know,\n\n2:38:49.400 --> 2:38:50.680\n AI machine learning, deep learning,\n\n2:38:50.680 --> 2:38:52.160\n which I think is super promising\n\n2:38:52.160 --> 2:38:54.360\n for all kinds of applications.\n\n2:38:54.360 --> 2:38:55.240\n Something very promising,\n\n2:38:55.240 --> 2:38:56.680\n if you're more interested in applications,\n\n2:38:56.680 --> 2:38:58.840\n is the applications of AI machine learning\n\n2:38:58.840 --> 2:39:00.480\n and deep learning to science,\n\n2:39:01.520 --> 2:39:05.120\n or to science that can help solve big problems\n\n2:39:05.120 --> 2:39:05.960\n in the world.\n\n2:39:05.960 --> 2:39:09.240\n I have colleagues at Meta, at Fair,\n\n2:39:09.240 --> 2:39:11.240\n who started this project called Open Catalyst,\n\n2:39:11.240 --> 2:39:14.560\n and it's an open project collaborative.\n\n2:39:14.560 --> 2:39:16.640\n And the idea is to use deep learning\n\n2:39:16.640 --> 2:39:21.640\n to help design new chemical compounds or materials\n\n2:39:21.960 --> 2:39:23.800\n that would facilitate the separation\n\n2:39:23.800 --> 2:39:25.840\n of hydrogen from oxygen.\n\n2:39:25.840 --> 2:39:29.080\n If you can efficiently separate oxygen from hydrogen\n\n2:39:29.080 --> 2:39:33.520\n with electricity, you solve climate change.\n\n2:39:33.520 --> 2:39:34.480\n It's as simple as that,\n\n2:39:34.480 --> 2:39:37.640\n because you cover, you know,\n\n2:39:37.640 --> 2:39:39.800\n some random desert with solar panels,\n\n2:39:40.800 --> 2:39:42.560\n and you have them work all day,\n\n2:39:42.560 --> 2:39:43.480\n produce hydrogen,\n\n2:39:43.480 --> 2:39:45.400\n and then you shoot the hydrogen wherever it's needed.\n\n2:39:45.400 --> 2:39:46.840\n You don't need anything else.\n\n2:39:48.560 --> 2:39:53.440\n You know, you have controllable power\n\n2:39:53.440 --> 2:39:55.640\n that can be transported anywhere.\n\n2:39:55.640 --> 2:39:59.040\n So if we have a large scale,\n\n2:39:59.040 --> 2:40:02.160\n efficient energy storage technology,\n\n2:40:02.160 --> 2:40:06.640\n like producing hydrogen, we solve climate change.\n\n2:40:06.640 --> 2:40:08.560\n Here's another way to solve climate change,\n\n2:40:08.560 --> 2:40:10.480\n is figuring out how to make fusion work.\n\n2:40:10.480 --> 2:40:11.520\n Now, the problem with fusion\n\n2:40:11.520 --> 2:40:13.640\n is that you make a super hot plasma,\n\n2:40:13.640 --> 2:40:16.240\n and the plasma is unstable and you can't control it.\n\n2:40:16.240 --> 2:40:17.080\n Maybe with deep learning,\n\n2:40:17.080 --> 2:40:19.120\n you can find controllers that will stabilize plasma\n\n2:40:19.120 --> 2:40:21.640\n and make, you know, practical fusion reactors.\n\n2:40:21.640 --> 2:40:23.080\n I mean, that's very speculative,\n\n2:40:23.080 --> 2:40:24.480\n but, you know, it's worth trying,\n\n2:40:24.480 --> 2:40:28.280\n because, you know, the payoff is huge.\n\n2:40:28.280 --> 2:40:29.880\n There's a group at Google working on this,\n\n2:40:29.880 --> 2:40:31.160\n led by John Platt.\n\n2:40:31.160 --> 2:40:33.920\n So control, convert as many problems\n\n2:40:33.920 --> 2:40:36.800\n in science and physics and biology and chemistry\n\n2:40:36.800 --> 2:40:39.760\n into a learnable problem\n\n2:40:39.760 --> 2:40:41.560\n and see if a machine can learn it.\n\n2:40:41.560 --> 2:40:43.880\n Right, I mean, there's properties of, you know,\n\n2:40:43.880 --> 2:40:46.280\n complex materials that we don't understand\n\n2:40:46.280 --> 2:40:48.520\n from first principle, for example, right?\n\n2:40:48.520 --> 2:40:53.040\n So, you know, if we could design new, you know,\n\n2:40:53.040 --> 2:40:56.400\n new materials, we could make more efficient batteries.\n\n2:40:56.400 --> 2:40:58.800\n You know, we could make maybe faster electronics.\n\n2:40:58.800 --> 2:41:01.920\n We could, I mean, there's a lot of things we can imagine\n\n2:41:01.920 --> 2:41:04.480\n doing, or, you know, lighter materials\n\n2:41:04.480 --> 2:41:06.400\n for cars or airplanes or things like that.\n\n2:41:06.400 --> 2:41:07.600\n Maybe better fuel cells.\n\n2:41:07.600 --> 2:41:09.520\n I mean, there's all kinds of stuff we can imagine.\n\n2:41:09.520 --> 2:41:12.280\n If we had good fuel cells, hydrogen fuel cells,\n\n2:41:12.280 --> 2:41:13.640\n we could use them to power airplanes,\n\n2:41:13.640 --> 2:41:17.240\n and, you know, transportation wouldn't be, or cars,\n\n2:41:17.240 --> 2:41:20.280\n and we wouldn't have emission problem,\n\n2:41:20.280 --> 2:41:24.600\n CO2 emission problems for air transportation anymore.\n\n2:41:24.600 --> 2:41:26.880\n So there's a lot of those things, I think,\n\n2:41:26.880 --> 2:41:29.160\n where AI, you know, can be used.\n\n2:41:30.160 --> 2:41:31.560\n And this is not even talking about\n\n2:41:31.560 --> 2:41:33.520\n all the sort of medicine, biology,\n\n2:41:33.520 --> 2:41:35.680\n and everything like that, right?\n\n2:41:35.680 --> 2:41:37.840\n You know, like, you know, protein folding,\n\n2:41:37.840 --> 2:41:40.040\n you know, figuring out, like, how could you design\n\n2:41:40.040 --> 2:41:41.880\n your proteins that it sticks to another protein\n\n2:41:41.880 --> 2:41:44.040\n at a particular site, because that's how you design drugs\n\n2:41:44.040 --> 2:41:44.880\n in the end.\n\n2:41:46.280 --> 2:41:47.600\n So, you know, deep learning would be useful,\n\n2:41:47.600 --> 2:41:49.280\n although those are kind of, you know,\n\n2:41:49.280 --> 2:41:51.120\n would be sort of enormous progress\n\n2:41:51.120 --> 2:41:53.360\n if we could use it for that.\n\n2:41:53.360 --> 2:41:54.320\n Here's an example.\n\n2:41:54.320 --> 2:41:58.280\n If you take, this is like from recent material physics,\n\n2:41:58.280 --> 2:42:02.200\n you take a monoatomic layer of graphene, right?\n\n2:42:02.200 --> 2:42:04.920\n So it's just carbon on a hexagonal mesh,\n\n2:42:04.920 --> 2:42:09.120\n and you make this single atom thick.\n\n2:42:09.120 --> 2:42:10.360\n You put another one on top,\n\n2:42:10.360 --> 2:42:13.080\n you twist them by some magic number of degrees,\n\n2:42:13.080 --> 2:42:14.800\n three degrees or something.\n\n2:42:14.800 --> 2:42:16.760\n It becomes superconductor.\n\n2:42:16.760 --> 2:42:18.240\n Nobody has any idea why.\n\n2:42:18.240 --> 2:42:19.080\n Okay.\n\n2:42:20.800 --> 2:42:22.480\n I want to know how that was discovered,\n\n2:42:22.480 --> 2:42:23.920\n but that's the kind of thing that machine learning\n\n2:42:23.920 --> 2:42:25.800\n can actually discover, these kinds of things.\n\n2:42:25.800 --> 2:42:28.960\n Maybe not, but there is a hint, perhaps,\n\n2:42:28.960 --> 2:42:31.720\n that with machine learning, we would train a system\n\n2:42:31.720 --> 2:42:34.840\n to basically be a phenomenological model\n\n2:42:34.840 --> 2:42:37.240\n of some complex emergent phenomenon,\n\n2:42:37.240 --> 2:42:40.400\n which, you know, superconductivity is one of those,\n\n2:42:42.400 --> 2:42:44.760\n where, you know, this collective phenomenon\n\n2:42:44.760 --> 2:42:46.920\n is too difficult to describe from first principles\n\n2:42:46.920 --> 2:42:48.800\n with the current, you know,\n\n2:42:48.800 --> 2:42:51.920\n the usual sort of reductionist type method,\n\n2:42:51.920 --> 2:42:54.960\n but we could have deep learning systems\n\n2:42:54.960 --> 2:42:57.680\n that predict the properties of a system\n\n2:42:57.680 --> 2:42:59.880\n from a description of it after being trained\n\n2:42:59.880 --> 2:43:04.880\n with sufficiently many samples.\n\n2:43:04.880 --> 2:43:06.680\n This guy, Pascal Foua, at EPFL,\n\n2:43:06.680 --> 2:43:09.800\n he has a startup company that,\n\n2:43:09.800 --> 2:43:13.440\n where he basically trained a convolutional net,\n\n2:43:13.440 --> 2:43:16.640\n essentially, to predict the aerodynamic properties\n\n2:43:16.640 --> 2:43:19.640\n of solids, and you can generate as much data as you want\n\n2:43:19.640 --> 2:43:21.920\n by just running computational free dynamics, right?\n\n2:43:21.920 --> 2:43:26.920\n So you give, like, a wing, airfoil,\n\n2:43:27.800 --> 2:43:29.800\n or something, shape of some kind,\n\n2:43:29.800 --> 2:43:31.400\n and you run computational free dynamics,\n\n2:43:31.400 --> 2:43:35.120\n you get, as a result, the drag and, you know,\n\n2:43:36.160 --> 2:43:37.480\n lift and all that stuff, right?\n\n2:43:37.480 --> 2:43:40.080\n And you can generate lots of data,\n\n2:43:40.080 --> 2:43:41.840\n train a neural net to make those predictions,\n\n2:43:41.840 --> 2:43:44.120\n and now what you have is a differentiable model\n\n2:43:44.120 --> 2:43:47.000\n of, let's say, drag and lift\n\n2:43:47.000 --> 2:43:48.680\n as a function of the shape of that solid,\n\n2:43:48.680 --> 2:43:49.960\n and so you can do back rate and descent,\n\n2:43:49.960 --> 2:43:51.520\n you can optimize the shape\n\n2:43:51.520 --> 2:43:53.280\n so you get the properties you want.\n\n2:43:54.880 --> 2:43:56.040\n Yeah, that's incredible.\n\n2:43:56.040 --> 2:43:58.280\n That's incredible, and on top of all that,\n\n2:43:58.280 --> 2:44:01.480\n probably you should read a little bit of literature\n\n2:44:01.480 --> 2:44:03.600\n and a little bit of history\n\n2:44:03.600 --> 2:44:06.640\n for inspiration and for wisdom,\n\n2:44:06.640 --> 2:44:08.800\n because after all, all of these technologies\n\n2:44:08.800 --> 2:44:10.280\n will have to work in the human world.\n\n2:44:10.280 --> 2:44:11.120\n Yes.\n\n2:44:11.120 --> 2:44:12.640\n And the human world is complicated.\n\n2:44:12.640 --> 2:44:14.120\n It is, sadly.\n\n2:44:15.080 --> 2:44:18.440\n Jan, this is an amazing conversation.\n\n2:44:18.440 --> 2:44:20.400\n I'm really honored that you would talk with me today.\n\n2:44:20.400 --> 2:44:22.240\n Thank you for all the amazing work you're doing\n\n2:44:22.240 --> 2:44:26.280\n at FAIR, at Meta, and thank you for being so passionate\n\n2:44:26.280 --> 2:44:28.120\n after all these years about everything\n\n2:44:28.120 --> 2:44:29.960\n that's going on, you're a beacon of hope\n\n2:44:29.960 --> 2:44:31.600\n for the machine learning community,\n\n2:44:31.600 --> 2:44:33.200\n and thank you so much for spending\n\n2:44:33.200 --> 2:44:34.480\n your valuable time with me today.\n\n2:44:34.480 --> 2:44:35.320\n That was awesome.\n\n2:44:35.320 --> 2:44:36.280\n Thanks for having me on.\n\n2:44:36.280 --> 2:44:37.840\n That was a pleasure.\n\n2:44:38.800 --> 2:44:41.440\n Thanks for listening to this conversation with Jan Lacune.\n\n2:44:41.440 --> 2:44:42.800\n To support this podcast,\n\n2:44:42.800 --> 2:44:45.720\n please check out our sponsors in the description.\n\n2:44:45.720 --> 2:44:47.840\n And now, let me leave you with some words\n\n2:44:47.840 --> 2:44:49.600\n from Isaac Asimov.\n\n2:44:50.640 --> 2:44:53.760\n Your assumptions are your windows on the world.\n\n2:44:53.760 --> 2:44:56.040\n Scrub them off every once in a while,\n\n2:44:56.040 --> 2:44:58.760\n or the light won't come in.\n\n2:44:58.760 --> 2:45:26.760\n Thank you for listening, and hope to see you next time.\n\n"
}