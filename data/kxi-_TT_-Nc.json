{
  "title": "Sergey Levine: Robotics and Machine Learning | Lex Fridman Podcast #108",
  "id": "kxi-_TT_-Nc",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:05.360\n The following is a conversation with Sergei Levine, a professor at Berkeley and a world\n\n00:05.360 --> 00:10.860\n class researcher in deep learning, reinforcement learning, robotics, and computer vision, including\n\n00:10.860 --> 00:15.660\n the development of algorithms for end to end training of neural network policies that combine\n\n00:15.660 --> 00:21.160\n perception and control, scalable algorithms for inverse reinforcement learning, and, in\n\n00:21.160 --> 00:24.100\n general, deep RL algorithms.\n\n00:24.100 --> 00:25.340\n Quick summary of the ads.\n\n00:25.340 --> 00:28.740\n Two sponsors, Cash App and ExpressVPN.\n\n00:28.740 --> 00:34.100\n Please consider supporting the podcast by downloading Cash App and using code LexPodcast\n\n00:34.100 --> 00:38.920\n and signing up at expressvpn.com slash lexpod.\n\n00:38.920 --> 00:44.340\n Click the links, buy the stuff, it's the best way to support this podcast and, in general,\n\n00:44.340 --> 00:45.340\n the journey I'm on.\n\n00:45.340 --> 00:51.100\n If you enjoy this thing, subscribe on YouTube, review it with 5 stars on Apple Podcast, follow\n\n00:51.100 --> 00:57.740\n on Spotify, support it on Patreon, or connect with me on Twitter at lexfriedman.\n\n00:57.740 --> 01:01.540\n As usual, I'll do a few minutes of ads now and never any ads in the middle that can break\n\n01:01.540 --> 01:04.020\n the flow of the conversation.\n\n01:04.020 --> 01:08.460\n This show is presented by Cash App, the number one finance app in the App Store.\n\n01:08.460 --> 01:11.780\n When you get it, use code lexpodcast.\n\n01:11.780 --> 01:15.940\n Cash App lets you send money to friends, buy Bitcoin, and invest in the stock market with\n\n01:15.940 --> 01:18.380\n as little as one dollar.\n\n01:18.380 --> 01:23.460\n Since Cash App does fractional share trading, let me mention that the order execution algorithm\n\n01:23.460 --> 01:29.020\n that works behind the scenes to create the abstraction of fractional orders is an algorithmic\n\n01:29.020 --> 01:30.020\n marvel.\n\n01:30.020 --> 01:34.500\n So, big props to the Cash App engineers for taking a step up to the next layer of abstraction\n\n01:34.500 --> 01:40.100\n over the stock market, making trading more accessible for new investors and diversification\n\n01:40.100 --> 01:41.100\n much easier.\n\n01:41.100 --> 01:48.300\n So, again, if you get Cash App from the App Store or Google Play and use the code lexpodcast,\n\n01:48.300 --> 01:54.220\n you get $10, and Cash App will also donate $10 to FIRST, an organization that is helping\n\n01:54.220 --> 01:59.840\n to advance robotics and STEM education for young people around the world.\n\n01:59.840 --> 02:04.220\n This show is also sponsored by ExpressVPN.\n\n02:04.220 --> 02:11.680\n Get it at expressvpn.com slash lexpod to support this podcast and to get an extra three months\n\n02:11.680 --> 02:14.500\n free on a one year package.\n\n02:14.500 --> 02:17.380\n I've been using ExpressVPN for many years.\n\n02:17.380 --> 02:18.580\n I love it.\n\n02:18.580 --> 02:22.020\n I think ExpressVPN is the best VPN out there.\n\n02:22.020 --> 02:26.300\n They told me to say it, but it happens to be true in my humble opinion.\n\n02:26.300 --> 02:31.160\n It doesn't log your data, it's crazy fast, and it's easy to use literally just one big\n\n02:31.160 --> 02:32.580\n power on button.\n\n02:32.580 --> 02:37.700\n Again, it's probably obvious to you, but I should say it again, it's really important\n\n02:37.700 --> 02:40.140\n that they don't log your data.\n\n02:40.140 --> 02:45.180\n It works on Linux and every other operating system, but Linux, of course, is the best\n\n02:45.180 --> 02:46.620\n operating system.\n\n02:46.620 --> 02:50.780\n Shout out to my favorite flavor, Ubuntu Mate 2004.\n\n02:50.780 --> 02:56.620\n Once again, get it at expressvpn.com slash lexpod to support this podcast and to get\n\n02:56.620 --> 03:00.940\n an extra three months free on a one year package.\n\n03:00.940 --> 03:05.500\n And now, here's my conversation with Sergey Levine.\n\n03:05.500 --> 03:10.260\n What's the difference between a state of the art human, such as you and I, well, I don't\n\n03:10.260 --> 03:14.540\n know if we qualify as state of the art humans, but a state of the art human and a state of\n\n03:14.540 --> 03:16.500\n the art robot?\n\n03:16.500 --> 03:19.100\n That's a very interesting question.\n\n03:19.100 --> 03:26.860\n Robot capability is, it's kind of a, I think it's a very tricky thing to understand because\n\n03:26.860 --> 03:29.620\n there are some things that are difficult that we wouldn't think are difficult and some things\n\n03:29.620 --> 03:33.060\n that are easy that we wouldn't think are easy.\n\n03:33.060 --> 03:37.740\n And there's also a really big gap between capabilities of robots in terms of hardware\n\n03:37.740 --> 03:43.060\n and their physical capability and capabilities of robots in terms of what they can do autonomously.\n\n03:43.060 --> 03:47.460\n There is a little video that I think robotics researchers really like to show, especially\n\n03:47.460 --> 03:53.220\n robotics learning researchers like myself, from 2004 from Stanford, which demonstrates\n\n03:53.220 --> 03:58.340\n a prototype robot called the PR1, and the PR1 was a robot that was designed as a home\n\n03:58.340 --> 03:59.340\n assistance robot.\n\n03:59.340 --> 04:03.980\n And there's this beautiful video showing the PR1 tidying up a living room, putting away\n\n04:03.980 --> 04:10.380\n toys and at the end bringing a beer to the person sitting on the couch, which looks really\n\n04:10.380 --> 04:11.660\n amazing.\n\n04:11.660 --> 04:16.060\n And then the punchline is that this robot is entirely controlled by a person.\n\n04:16.060 --> 04:20.660\n So in some ways the gap between a state of the art human and state of the art robot,\n\n04:20.660 --> 04:23.980\n if the robot has a human brain, is actually not that large.\n\n04:23.980 --> 04:28.340\n Now obviously like human bodies are sophisticated and very robust and resilient in many ways,\n\n04:28.340 --> 04:32.620\n but on the whole, if we're willing to like spend a bit of money and do a bit of engineering,\n\n04:32.620 --> 04:35.880\n we can kind of close the hardware gap almost.\n\n04:35.880 --> 04:40.420\n But the intelligence gap, that one is very wide.\n\n04:40.420 --> 04:43.820\n And when you say hardware, you're referring to the physical, sort of the actuators, the\n\n04:43.820 --> 04:49.020\n actual body of the robot, as opposed to the hardware on which the cognition, the hardware\n\n04:49.020 --> 04:50.020\n of the nervous system.\n\n04:50.020 --> 04:51.020\n Yes, exactly.\n\n04:51.020 --> 04:54.660\n I'm referring to the body rather than the mind.\n\n04:54.660 --> 04:56.660\n So that means that the kind of the work is cut out for us.\n\n04:56.660 --> 05:00.500\n Like while we can still make the body better, we kind of know that the big bottleneck right\n\n05:00.500 --> 05:02.880\n now is really the mind.\n\n05:02.880 --> 05:03.980\n And how big is that gap?\n\n05:03.980 --> 05:11.300\n How big is the difference in your sense of ability to learn, ability to reason, ability\n\n05:11.300 --> 05:16.880\n to perceive the world between humans and our best robots?\n\n05:16.880 --> 05:23.720\n The gap is very large and the gap becomes larger the more unexpected events can happen\n\n05:23.720 --> 05:24.720\n in the world.\n\n05:24.720 --> 05:30.860\n So essentially the spectrum along which you can measure the size of that gap is the spectrum\n\n05:30.860 --> 05:32.220\n of how open the world is.\n\n05:32.220 --> 05:36.120\n If you control everything in the world very tightly, if you put the robot in like a factory\n\n05:36.120 --> 05:41.420\n and you tell it where everything is and you rigidly program its motion, then it can do\n\n05:41.420 --> 05:43.580\n things, you know, one might even say in a superhuman way.\n\n05:43.580 --> 05:47.280\n It can move faster, it's stronger, it can lift up a car and things like that.\n\n05:47.280 --> 05:51.300\n But as soon as anything starts to vary in the environment, now it'll trip up.\n\n05:51.300 --> 05:55.700\n And if many, many things vary like they would like in your kitchen, for example, then things\n\n05:55.700 --> 05:57.940\n are pretty much like wide open.\n\n05:57.940 --> 06:03.820\n Now, again, we're going to stick a bit on the philosophical questions, but how much\n\n06:03.820 --> 06:11.140\n on the human side of the cognitive abilities in your sense is nature versus nurture?\n\n06:11.140 --> 06:18.420\n So how much of it is a product of evolution and how much of it is something we'll learn\n\n06:18.420 --> 06:22.060\n from sort of scratch from the day we're born?\n\n06:22.060 --> 06:26.260\n I'm going to read into your question as asking about the implications of this for AI.\n\n06:26.260 --> 06:30.540\n Because I'm not a biologist, I can't really like speak authoritatively.\n\n06:30.540 --> 06:36.580\n So until we go on it, if it's so, if it's all about learning, then there's more hope\n\n06:36.580 --> 06:38.540\n for AI.\n\n06:38.540 --> 06:44.220\n So the way that I look at this is that, you know, well, first, of course, biology is very\n\n06:44.220 --> 06:45.300\n messy.\n\n06:45.300 --> 06:49.980\n And it's if you ask the question, how does a person do something or has a person's mind\n\n06:49.980 --> 06:54.220\n do something, you can come up with a bunch of hypotheses and oftentimes you can find\n\n06:54.220 --> 06:58.220\n support for many different, often conflicting hypotheses.\n\n06:58.220 --> 07:03.380\n One way that we can approach the question of what the implications of this for AI are\n\n07:03.380 --> 07:05.500\n is we can think about what's sufficient.\n\n07:05.500 --> 07:11.220\n So you know, maybe a person is from birth very, very good at some things like, for example,\n\n07:11.220 --> 07:12.220\n recognizing faces.\n\n07:12.220 --> 07:13.980\n There's a very strong evolutionary pressure to do that.\n\n07:13.980 --> 07:18.820\n If you can recognize your mother's face, then you're more likely to survive and therefore\n\n07:18.820 --> 07:20.560\n people are good at this.\n\n07:20.560 --> 07:23.940\n But we can also ask like, what's the minimum sufficient thing?\n\n07:23.940 --> 07:27.060\n And one of the ways that we can study the minimal sufficient thing is we could, for\n\n07:27.060 --> 07:29.380\n example, see what people do in unusual situations.\n\n07:29.380 --> 07:33.860\n If you present them with things that evolution couldn't have prepared them for, you know,\n\n07:33.860 --> 07:36.360\n our daily lives actually do this to us all the time.\n\n07:36.360 --> 07:41.500\n We didn't evolve to deal with, you know, automobiles and space flight and whatever.\n\n07:41.500 --> 07:45.460\n So there are all these situations that we can find ourselves in and we do very well\n\n07:45.460 --> 07:46.460\n there.\n\n07:46.460 --> 07:50.580\n Like I can give you a joystick to control a robotic arm, which you've never used before\n\n07:50.580 --> 07:52.940\n and you might be pretty bad for the first couple of seconds.\n\n07:52.940 --> 07:58.260\n But if I tell you like your life depends on using this robotic arm to like open this door,\n\n07:58.260 --> 07:59.660\n you'll probably manage it.\n\n07:59.660 --> 08:03.140\n Even though you've never seen this device before, you've never used the joystick control\n\n08:03.140 --> 08:04.820\n us and you'll kind of muddle through it.\n\n08:04.820 --> 08:08.580\n And that's not your evolved natural ability.\n\n08:08.580 --> 08:11.340\n That's your, your flexibility or your adaptability.\n\n08:11.340 --> 08:14.860\n And that's exactly where our current robotic systems really kind of fall flat.\n\n08:14.860 --> 08:22.500\n But I wonder how much general, almost what we think of as common sense, pre trained models\n\n08:22.500 --> 08:24.220\n underneath all of that.\n\n08:24.220 --> 08:32.100\n So that ability to adapt to a joystick is, requires you to have a kind of, you know,\n\n08:32.100 --> 08:33.100\n I'm human.\n\n08:33.100 --> 08:37.220\n So it's hard for me to introspect all the knowledge I have about the world, but it seems\n\n08:37.220 --> 08:42.180\n like there might be an iceberg underneath of the amount of knowledge we actually bring\n\n08:42.180 --> 08:43.260\n to the table.\n\n08:43.260 --> 08:45.260\n That's kind of the open question.\n\n08:45.260 --> 08:48.900\n There's absolutely an iceberg of knowledge that we bring to the table, but I think it's\n\n08:48.900 --> 08:54.060\n very likely that iceberg of knowledge is actually built up over our lifetimes.\n\n08:54.060 --> 08:58.700\n Because we have, you know, we have a lot of prior experience to draw on.\n\n08:58.700 --> 09:05.060\n And it kind of makes sense that the right way for us to, you know, to optimize our,\n\n09:05.060 --> 09:10.300\n our efficiency, our evolutionary fitness and so on is to utilize all of that experience\n\n09:10.300 --> 09:13.360\n to build up the best iceberg we can get.\n\n09:13.360 --> 09:16.620\n And that's actually one of the, you know, while that sounds an awful lot like what machine\n\n09:16.620 --> 09:20.240\n learning actually does, I think that for modern machine learning, it's actually a really big\n\n09:20.240 --> 09:25.320\n challenge to take this unstructured mass of experience and distill out something that\n\n09:25.320 --> 09:28.340\n looks like a common sense understanding of the world.\n\n09:28.340 --> 09:32.660\n And perhaps part of that isn't, it's not because something about machine learning itself is,\n\n09:32.660 --> 09:38.340\n is broken or hard, but because we've been a little too rigid in subscribing to a very\n\n09:38.340 --> 09:42.460\n supervised, very rigid notion of learning, you know, kind of the input output, X's go\n\n09:42.460 --> 09:43.980\n to Y's sort of model.\n\n09:43.980 --> 09:51.260\n And maybe what we really need to do is to view the world more as like a mass of experience\n\n09:51.260 --> 09:55.060\n that is not necessarily providing any rigid supervision, but sort of providing many, many\n\n09:55.060 --> 09:56.980\n instances of things that could be.\n\n09:56.980 --> 10:00.700\n And then you take that and you distill it into some sort of common sense understanding.\n\n10:00.700 --> 10:06.700\n I see what you're, you're painting an optimistic, beautiful picture, especially from the robotics\n\n10:06.700 --> 10:12.540\n perspective because that means we just need to invest and build better learning algorithms,\n\n10:12.540 --> 10:17.620\n figure out how we can get access to more and more data for those learning algorithms to\n\n10:17.620 --> 10:22.260\n extract signal from, and then accumulate that iceberg of knowledge.\n\n10:22.260 --> 10:23.740\n It's a beautiful picture.\n\n10:23.740 --> 10:25.100\n It's a hopeful one.\n\n10:25.100 --> 10:29.020\n I think it's potentially a little bit more than just that.\n\n10:29.020 --> 10:32.880\n And this is, this is where we perhaps reach the limits of our current understanding.\n\n10:32.880 --> 10:37.700\n But one thing that I think that the research community hasn't really resolved in a satisfactory\n\n10:37.700 --> 10:43.540\n way is how much it matters where that experience comes from, like, you know, do you just like\n\n10:43.540 --> 10:48.860\n download everything on the internet and cram it into essentially the 21st century analog\n\n10:48.860 --> 10:54.540\n of the giant language model and then see what happens or does it actually matter whether\n\n10:54.540 --> 10:59.380\n your machine physically experiences the world or in the sense that it actually attempts\n\n10:59.380 --> 11:03.860\n things, observes the outcome of its actions and kind of augments its experience that way.\n\n11:03.860 --> 11:09.500\n And it chooses which parts of the world it gets to interact with and observe and learn\n\n11:09.500 --> 11:10.500\n from.\n\n11:10.500 --> 11:11.500\n Right.\n\n11:11.500 --> 11:16.700\n It may be that the world is so complex that simply obtaining a large mass of sort of\n\n11:16.700 --> 11:21.140\n IID samples of the world is a very difficult way to go.\n\n11:21.140 --> 11:25.040\n But if you are actually interacting with the world and essentially performing this sort\n\n11:25.040 --> 11:30.060\n of hard negative mining by attempting what you think might work, observing the sometimes\n\n11:30.060 --> 11:35.620\n happy and sometimes sad outcomes of that and augmenting your understanding using that experience\n\n11:35.620 --> 11:40.140\n and you're just doing this continually for many years, maybe that sort of data in some\n\n11:40.140 --> 11:44.800\n sense is actually much more favorable to obtaining a common sense understanding.\n\n11:44.800 --> 11:49.700\n One reason we might think that this is true is that, you know, what we associate with\n\n11:49.700 --> 11:55.140\n common sense or lack of common sense is often characterized by the ability to reason about\n\n11:55.140 --> 12:01.000\n kind of counterfactual questions like, you know, if I were to hear this bottle of water\n\n12:01.000 --> 12:04.780\n sitting on the table, everything is fine if I were to knock it over, which I'm not going\n\n12:04.780 --> 12:05.780\n to do.\n\n12:05.780 --> 12:07.700\n But if I were to do that, what would happen?\n\n12:07.700 --> 12:10.360\n And I know that nothing good would happen from that.\n\n12:10.360 --> 12:14.100\n But if I have a bad understanding of the world, I might think that that's a good way for me\n\n12:14.100 --> 12:16.840\n to like, you know, gain more utility.\n\n12:16.840 --> 12:22.300\n If I actually go about my daily life doing the things that my current understanding of\n\n12:22.300 --> 12:28.760\n the world suggests will give me high utility, in some ways, I'll get exactly the right supervision\n\n12:28.760 --> 12:33.200\n to tell me not to do those bad things and to keep doing the good things.\n\n12:33.200 --> 12:39.220\n So there's a spectrum between IID, random walk through the space of data, and then there's\n\n12:39.220 --> 12:45.820\n and what we humans do, I don't even know if we do it optimal, but that might be beyond.\n\n12:45.820 --> 12:52.540\n So this open question that you raised, where do you think systems, intelligent systems\n\n12:52.540 --> 12:56.460\n that would be able to deal with this world fall?\n\n12:56.460 --> 13:02.120\n Can we do pretty well by reading all of Wikipedia, sort of randomly sampling it like language\n\n13:02.120 --> 13:03.900\n models do?\n\n13:03.900 --> 13:09.620\n Or do we have to be exceptionally selective and intelligent about which aspects of the\n\n13:09.620 --> 13:12.100\n world we interact with?\n\n13:12.100 --> 13:15.980\n So I think this is first an open scientific problem, and I don't have like a clear answer,\n\n13:15.980 --> 13:18.300\n but I can speculate a little bit.\n\n13:18.300 --> 13:23.580\n And what I would speculate is that you don't need to be super, super careful.\n\n13:23.580 --> 13:28.480\n I think it's less about like, being careful to avoid the useless stuff, and more about\n\n13:28.480 --> 13:31.620\n making sure that you hit on the really important stuff.\n\n13:31.620 --> 13:37.540\n So perhaps it's okay, if you spend part of your day, just, you know, guided by your curiosity,\n\n13:37.540 --> 13:42.140\n reading interesting regions of your state space, but it's important for you to, you\n\n13:42.140 --> 13:47.060\n know, every once in a while, make sure that you really try out the solutions that your\n\n13:47.060 --> 13:51.120\n current model of the world suggests might be effective, and observe whether those solutions\n\n13:51.120 --> 13:53.060\n are working as you expect or not.\n\n13:53.060 --> 13:59.740\n And perhaps some of that is really essential to have kind of a perpetual improvement loop.\n\n13:59.740 --> 14:03.540\n This perpetual improvement loop is really like, that's really the key, the key that's\n\n14:03.540 --> 14:07.860\n going to potentially distinguish the best current methods from the best methods of tomorrow\n\n14:07.860 --> 14:08.860\n in a sense.\n\n14:08.860 --> 14:15.820\n How important do you think is exploration or total out of the box thinking exploration\n\n14:15.820 --> 14:19.300\n in this space as you jump to totally different domains?\n\n14:19.300 --> 14:24.260\n So you kind of mentioned there's an optimization problem, you kind of kind of explore the specifics\n\n14:24.260 --> 14:27.820\n of a particular strategy, whatever the thing you're trying to solve.\n\n14:27.820 --> 14:33.040\n How important is it to explore totally outside of the strategies that have been working for\n\n14:33.040 --> 14:34.040\n you so far?\n\n14:34.040 --> 14:35.040\n What's your intuition there?\n\n14:35.040 --> 14:38.900\n Yeah, I think it's a very problem dependent kind of question.\n\n14:38.900 --> 14:45.580\n And I think that that's actually, you know, in some ways that question gets at one of\n\n14:45.580 --> 14:51.580\n the big differences between sort of the classic formulation of a reinforcement learning problem\n\n14:51.580 --> 14:57.480\n and some of the sort of more open ended reformulations of that problem that have been explored in\n\n14:57.480 --> 14:58.480\n recent years.\n\n14:58.480 --> 15:02.740\n So classically reinforcement learning is framed as a problem of maximizing utility, like any\n\n15:02.740 --> 15:08.940\n kind of rational AI agent, and then anything you do is in service to maximizing that utility.\n\n15:08.940 --> 15:15.220\n But a very interesting kind of way to look at, I'm not necessarily saying this is the\n\n15:15.220 --> 15:17.820\n best way to look at it, but an interesting alternative way to look at these problems\n\n15:17.820 --> 15:24.300\n is as something where you first get to explore the world, however you please, and then afterwards\n\n15:24.300 --> 15:26.700\n you will be tasked with doing something.\n\n15:26.700 --> 15:28.960\n And that might suggest a somewhat different solution.\n\n15:28.960 --> 15:31.860\n So if you don't know what you're going to be tasked with doing, and you just want to\n\n15:31.860 --> 15:35.980\n prepare yourself optimally for whatever your uncertain future holds, maybe then you will\n\n15:35.980 --> 15:41.820\n choose to attain some sort of coverage, build up sort of an arsenal of cognitive tools,\n\n15:41.820 --> 15:46.400\n if you will, such that later on when someone tells you, now your job is to fetch the coffee\n\n15:46.400 --> 15:49.180\n for me, you will be well prepared to undertake that task.\n\n15:49.180 --> 15:54.380\n And that you see that as the modern formulation of the reinforcement learning problem, as\n\n15:54.380 --> 16:00.460\n a kind of the more multitask, the general intelligence kind of formulation.\n\n16:00.460 --> 16:04.500\n I think that's one possible vision of where things might be headed.\n\n16:04.500 --> 16:08.220\n I don't think that's by any means the mainstream or standard way of doing things, and it's\n\n16:08.220 --> 16:09.940\n not like if I had to...\n\n16:09.940 --> 16:10.940\n But I like it.\n\n16:10.940 --> 16:11.940\n It's a beautiful vision.\n\n16:11.940 --> 16:14.220\n So maybe you actually take a step back.\n\n16:14.220 --> 16:16.700\n What is the goal of robotics?\n\n16:16.700 --> 16:18.940\n What's the general problem of robotics we're trying to solve?\n\n16:18.940 --> 16:21.260\n You actually kind of painted two pictures here.\n\n16:21.260 --> 16:23.340\n One of sort of the narrow, one of the general.\n\n16:23.340 --> 16:26.780\n What in your view is the big problem of robotics?\n\n16:26.780 --> 16:31.200\n And ridiculously philosophical high level questions.\n\n16:31.200 --> 16:34.620\n I think that maybe there are two ways I can answer this question.\n\n16:34.620 --> 16:41.100\n One is there's a very pragmatic problem, which is like what would make robots, what would\n\n16:41.100 --> 16:44.060\n sort of maximize the usefulness of robots?\n\n16:44.060 --> 16:53.620\n And there the answer might be something like a system where a system that can perform whatever\n\n16:53.620 --> 16:59.580\n task a human user sets for it, within the physical constraints, of course.\n\n16:59.580 --> 17:02.560\n If you tell it to teleport to another planet, it probably can't do that.\n\n17:02.560 --> 17:06.440\n But if you ask it to do something that's within its physical capability, then potentially\n\n17:06.440 --> 17:10.420\n with a little bit of additional training or a little bit of additional trial and error,\n\n17:10.420 --> 17:14.180\n it ought to be able to figure it out in much the same way as like a human teleoperator\n\n17:14.180 --> 17:16.760\n ought to figure out how to drive the robot to do that.\n\n17:16.760 --> 17:22.740\n That's kind of the very pragmatic view of what it would take to kind of solve the robotics\n\n17:22.740 --> 17:24.960\n problem, if you will.\n\n17:24.960 --> 17:29.480\n But I think that there is a second answer, and that answer is a lot closer to why I want\n\n17:29.480 --> 17:34.300\n to work on robotics, which is that I think it's less about what it would take to do a\n\n17:34.300 --> 17:39.160\n really good job in the world of robotics, but more the other way around, what robotics\n\n17:39.160 --> 17:44.840\n can bring to the table to help us understand artificial intelligence.\n\n17:44.840 --> 17:48.260\n So your dream fundamentally is to understand intelligence?\n\n17:48.260 --> 17:49.260\n Yes.\n\n17:49.260 --> 17:53.120\n And I think that's the dream for many people who actually work in this space.\n\n17:53.120 --> 17:58.640\n I think that there's something very pragmatic and very useful about studying robotics, but\n\n17:58.640 --> 18:02.920\n I do think that a lot of people that go into this field actually, you know, the things\n\n18:02.920 --> 18:09.400\n that they draw inspiration from are the potential for robots to like help us learn about intelligence\n\n18:09.400 --> 18:10.720\n and about ourselves.\n\n18:10.720 --> 18:18.280\n So that's fascinating that robotics is basically the space by which you can get closer to understanding\n\n18:18.280 --> 18:20.680\n the fundamentals of artificial intelligence.\n\n18:20.680 --> 18:25.440\n So what is it about robotics that's different from some of the other approaches?\n\n18:25.440 --> 18:30.020\n So if we look at some of the early breakthroughs in deep learning or in the computer vision\n\n18:30.020 --> 18:34.920\n space and the natural language processing, there's really nice clean benchmarks that\n\n18:34.920 --> 18:38.540\n a lot of people competed on and thereby came up with a lot of brilliant ideas.\n\n18:38.540 --> 18:43.760\n What's the fundamental difference to you between computer vision purely defined and ImageNet\n\n18:43.760 --> 18:46.640\n and kind of the bigger robotics problem?\n\n18:46.640 --> 18:48.480\n So there are a couple of things.\n\n18:48.480 --> 18:55.520\n One is that with robotics, you kind of have to take away many of the crutches.\n\n18:55.520 --> 19:01.760\n So you have to deal with both the particular problems of perception control and so on,\n\n19:01.760 --> 19:04.560\n but you also have to deal with the integration of those things.\n\n19:04.560 --> 19:08.800\n And you know, classically, we've always thought of the integration as kind of a separate problem.\n\n19:08.800 --> 19:12.800\n So a classic kind of modular engineering approach is that we solve the individual subproblems\n\n19:12.800 --> 19:16.080\n and then wire them together and then the whole thing works.\n\n19:16.080 --> 19:19.720\n And one of the things that we've been seeing over the last couple of decades is that, well,\n\n19:19.720 --> 19:24.200\n maybe studying the thing as a whole might lead to just like very different solutions\n\n19:24.200 --> 19:26.640\n than if we were to study the parts and wire them together.\n\n19:26.640 --> 19:32.320\n So the integrative nature of robotics research helps us see, you know, the different perspectives\n\n19:32.320 --> 19:34.240\n on the problem.\n\n19:34.240 --> 19:40.960\n Another part of the answer is that with robotics, it casts a certain paradox into very clever\n\n19:40.960 --> 19:41.960\n relief.\n\n19:41.960 --> 19:48.480\n This is sometimes referred to as Moravec's paradox, the idea that in artificial intelligence,\n\n19:48.480 --> 19:52.800\n things that are very hard for people can be very easy for machines and vice versa.\n\n19:52.800 --> 19:54.880\n Things that are very easy for people can be very hard for machines.\n\n19:54.880 --> 20:02.080\n So you know, integral and differential calculus is pretty difficult to learn for people.\n\n20:02.080 --> 20:06.080\n But if you program a computer, do it, it can derive derivatives and integrals for you all\n\n20:06.080 --> 20:08.400\n day long without any trouble.\n\n20:08.400 --> 20:13.320\n Whereas some things like, you know, drinking from a cup of water, very easy for a person\n\n20:13.320 --> 20:16.720\n to do, very hard for a robot to deal with.\n\n20:16.720 --> 20:21.680\n And sometimes when we see such blatant discrepancies, that gives us a really strong hint that we're\n\n20:21.680 --> 20:23.160\n missing something important.\n\n20:23.160 --> 20:28.000\n So if we really try to zero in on those discrepancies, we might find that little bit that we're missing.\n\n20:28.000 --> 20:32.320\n And it's not that we need to make machines better or worse at math and better at drinking\n\n20:32.320 --> 20:37.800\n water, but just that by studying those discrepancies, we might find some new insight.\n\n20:37.800 --> 20:41.680\n So that could be in any space, it doesn't have to be robotics.\n\n20:41.680 --> 20:48.560\n But you're saying, I mean, it's kind of interesting that robotics seems to have a lot of those\n\n20:48.560 --> 20:49.560\n discrepancies.\n\n20:49.560 --> 20:56.600\n So the Hans Marvak paradox is probably referring to the space of the physical interaction,\n\n20:56.600 --> 21:00.640\n like you said, object manipulation, walking, all the kind of stuff we do in the physical\n\n21:00.640 --> 21:01.640\n world.\n\n21:01.640 --> 21:13.280\n How do you make sense if you were to try to disentangle the Marvak paradox, like why is\n\n21:13.280 --> 21:17.800\n there such a gap in our intuition about it?\n\n21:17.800 --> 21:23.420\n Why do you think manipulating objects is so hard from everything you've learned from applying\n\n21:23.420 --> 21:25.480\n reinforcement learning in this space?\n\n21:25.480 --> 21:33.760\n Yeah, I think that one reason is maybe that for many of the other problems that we've\n\n21:33.760 --> 21:41.120\n studied in AI and computer science and so on, the notion of input output and supervision\n\n21:41.120 --> 21:42.380\n is much, much cleaner.\n\n21:42.380 --> 21:45.920\n So computer vision, for example, deals with very complex inputs.\n\n21:45.920 --> 21:52.080\n But it's comparatively a bit easier, at least up to some level of abstraction, to cast it\n\n21:52.080 --> 21:54.840\n as a very tightly supervised problem.\n\n21:54.840 --> 21:59.640\n It's comparatively much, much harder to cast robotic manipulation as a very tightly supervised\n\n21:59.640 --> 22:00.720\n problem.\n\n22:00.720 --> 22:03.440\n You can do it, it just doesn't seem to work all that well.\n\n22:03.440 --> 22:06.980\n So you could say that, well, maybe we get a labeled data set where we know exactly which\n\n22:06.980 --> 22:09.200\n motor commands to send, and then we train on that.\n\n22:09.200 --> 22:13.800\n But for various reasons, that's not actually such a great solution.\n\n22:13.800 --> 22:17.440\n And it also doesn't seem to be even remotely similar to how people and animals learn to\n\n22:17.440 --> 22:22.980\n do things, because we're not told by our parents, here's how you fire your muscles in order\n\n22:22.980 --> 22:24.280\n to walk.\n\n22:24.280 --> 22:29.080\n So we do get some guidance, but the really low level detailed stuff we figure out mostly\n\n22:29.080 --> 22:30.080\n on our own.\n\n22:30.080 --> 22:34.400\n And that's what you mean by tightly coupled, that every single little sub action gets a\n\n22:34.400 --> 22:37.560\n supervised signal of whether it's a good one or not.\n\n22:37.560 --> 22:38.560\n Right.\n\n22:38.560 --> 22:41.360\n So while in computer vision, you could sort of imagine up to a level of abstraction that\n\n22:41.360 --> 22:45.640\n maybe somebody told you this is a car and this is a cat and this is a dog, in motor\n\n22:45.640 --> 22:49.400\n control, it's very clear that that was not the case.\n\n22:49.400 --> 22:57.120\n If we look at sort of the sub spaces of robotics, that, again, as you said, robotics integrates\n\n22:57.120 --> 23:00.880\n all of them together, and we get to see how this beautiful mess interplays.\n\n23:00.880 --> 23:04.040\n But so there's nevertheless still perception.\n\n23:04.040 --> 23:09.880\n So it's the computer vision problem, broadly speaking, understanding the environment.\n\n23:09.880 --> 23:14.600\n And there's also maybe you can correct me on this kind of categorization of the space,\n\n23:14.600 --> 23:20.480\n and there's prediction in trying to anticipate what things are going to do into the future\n\n23:20.480 --> 23:24.440\n in order for you to be able to act in that world.\n\n23:24.440 --> 23:31.580\n And then there's also this game theoretic aspect of how your actions will change the\n\n23:31.580 --> 23:34.120\n behavior of others.\n\n23:34.120 --> 23:38.640\n In this kind of space, what, and this is bigger than reinforcement learning, this is just\n\n23:38.640 --> 23:42.840\n broadly looking at the problem of robotics, what's the hardest problem here?\n\n23:42.840 --> 23:52.280\n Or is there, or is what you said true that when you start to look at all of them together,\n\n23:52.280 --> 23:57.360\n that's a whole nother thing, like you can't even say which one individually is harder\n\n23:57.360 --> 24:01.400\n because all of them together, you should only be looking at them all together.\n\n24:01.400 --> 24:05.240\n I think when you look at them all together, some things actually become easier.\n\n24:05.240 --> 24:07.520\n And I think that's actually pretty important.\n\n24:07.520 --> 24:16.040\n So we had back in 2014, we had some work, basically our first work on end to end reinforcement\n\n24:16.040 --> 24:21.040\n learning for robotic manipulation skills from vision, which at the time was something that\n\n24:21.040 --> 24:25.520\n seemed a little inflammatory and controversial in the robotics world.\n\n24:25.520 --> 24:30.320\n But other than the inflammatory and controversial part of it, the point that we were actually\n\n24:30.320 --> 24:35.720\n trying to make in that work is that for the particular case of combining perception and\n\n24:35.720 --> 24:39.480\n control, you could actually do better if you treat them together than if you try to separate\n\n24:39.480 --> 24:40.480\n them.\n\n24:40.480 --> 24:43.240\n And the way that we tried to demonstrate this is we picked a fairly simple motor control\n\n24:43.240 --> 24:49.560\n task where a robot had to insert a little red trapezoid into a trapezoidal hole.\n\n24:49.560 --> 24:54.800\n And we had our separated solution, which involved first detecting the hole using a pose detector\n\n24:54.800 --> 24:57.720\n and then actuating the arm to put it in.\n\n24:57.720 --> 25:01.780\n And then our intent solution, which just mapped pixels to the torques.\n\n25:01.780 --> 25:05.960\n And one of the things we observed is that if you use the intent solution, essentially\n\n25:05.960 --> 25:08.400\n the pressure on the perception part of the model is actually lower.\n\n25:08.400 --> 25:11.320\n Like it doesn't have to figure out exactly where the thing is in 3D space.\n\n25:11.320 --> 25:15.500\n It just needs to figure out where it is, you know, distributing the errors in such a way\n\n25:15.500 --> 25:19.280\n that the horizontal difference matters more than the vertical difference because vertically\n\n25:19.280 --> 25:22.320\n it just pushes it down all the way until it can't go any further.\n\n25:22.320 --> 25:26.480\n And their perceptual errors are a lot less harmful, whereas perpendicular to the direction\n\n25:26.480 --> 25:29.060\n of motion, perceptual errors are much more harmful.\n\n25:29.060 --> 25:33.560\n So the point is that if you combine these two things, you can trade off errors between\n\n25:33.560 --> 25:38.120\n the components optimally to best accomplish the task.\n\n25:38.120 --> 25:41.680\n And the components can actually be weaker while still leading to better overall performance.\n\n25:41.680 --> 25:44.000\n It's a profound idea.\n\n25:44.000 --> 25:48.400\n I mean, in the space of pegs and things like that, it's quite simple.\n\n25:48.400 --> 25:55.080\n It almost is tempting to overlook, but that seems to be at least intuitively an idea that\n\n25:55.080 --> 26:01.280\n should generalize to basically all aspects of perception and control, that one strengthens\n\n26:01.280 --> 26:02.280\n the other.\n\n26:02.280 --> 26:03.280\n Yeah.\n\n26:03.280 --> 26:07.080\n And we, you know, people who have studied sort of perceptual heuristics in humans and\n\n26:07.080 --> 26:08.960\n animals find things like that all the time.\n\n26:08.960 --> 26:12.400\n So one very well known example of this is something called the gaze heuristic, which\n\n26:12.400 --> 26:17.280\n is a little trick that you can use to intercept a flying object.\n\n26:17.280 --> 26:21.960\n So if you want to catch a ball, for instance, you could try to localize it in 3D space,\n\n26:21.960 --> 26:25.040\n estimate its velocity, estimate the effect of wind resistance, solve a complex system\n\n26:25.040 --> 26:27.480\n of differential equations in your head.\n\n26:27.480 --> 26:33.280\n Or you can maintain a running speed so that the object stays in the same position as in\n\n26:33.280 --> 26:34.280\n your field of view.\n\n26:34.280 --> 26:35.760\n So if it dips a little bit, you speed up.\n\n26:35.760 --> 26:38.200\n If it rises a little bit, you slow down.\n\n26:38.200 --> 26:40.800\n And if you follow the simple rule, you'll actually arrive at exactly the place where\n\n26:40.800 --> 26:43.060\n the object lands and you'll catch it.\n\n26:43.060 --> 26:46.960\n And humans use it when they play baseball, human pilots use it when they fly airplanes\n\n26:46.960 --> 26:50.520\n to figure out if they're about to collide with somebody, frogs use this to catch insects\n\n26:50.520 --> 26:51.580\n and so on and so on.\n\n26:51.580 --> 26:53.640\n So this is something that actually happens in nature.\n\n26:53.640 --> 26:57.120\n And I'm sure this is just one instance of it that we were able to identify just because\n\n26:57.120 --> 27:00.440\n all the scientists were able to identify because it's so prevalent, but there are probably\n\n27:00.440 --> 27:01.440\n many others.\n\n27:01.440 --> 27:06.840\n Do you have a, just so we can zoom in as we talk about robotics, do you have a canonical\n\n27:06.840 --> 27:12.800\n problem, sort of a simple, clean, beautiful representative problem in robotics that you\n\n27:12.800 --> 27:16.000\n think about when you're thinking about some of these problems?\n\n27:16.000 --> 27:23.600\n We talked about robotic manipulation, to me that seems intuitively, at least the robotics\n\n27:23.600 --> 27:28.760\n community has converged towards that as a space that's the canonical problem.\n\n27:28.760 --> 27:33.240\n If you agree, then maybe do you zoom in in some particular aspect of that problem that\n\n27:33.240 --> 27:34.240\n you just like?\n\n27:34.240 --> 27:44.040\n Like if we solve that problem perfectly, it'll unlock a major step towards human level intelligence.\n\n27:44.040 --> 27:46.360\n I don't think I have like a really great answer to that.\n\n27:46.360 --> 27:53.040\n And I think partly the reason I don't have a great answer kind of has to do with the,\n\n27:53.040 --> 27:57.420\n it has to do with the fact that the difficulty is really in the flexibility and adaptability\n\n27:57.420 --> 28:01.160\n rather than in doing a particular thing really, really well.\n\n28:01.160 --> 28:06.680\n So it's hard to just say like, oh, if you can, I don't know, like shuffle a deck of\n\n28:06.680 --> 28:12.920\n cards as fast as like a Vegas casino dealer, then you'll be very proficient.\n\n28:12.920 --> 28:21.120\n It's really the ability to quickly figure out how to do some arbitrary new thing well\n\n28:21.120 --> 28:26.160\n enough to like, you know, to move on to the next arbitrary thing.\n\n28:26.160 --> 28:33.680\n But the source of newness and uncertainty, have you found problems in which it's easy\n\n28:33.680 --> 28:38.520\n to generate new newnessnesses?\n\n28:38.520 --> 28:40.120\n New types of newness.\n\n28:40.120 --> 28:41.120\n Yeah.\n\n28:41.120 --> 28:46.920\n So a few years ago, so if you had asked me this question around like 2016, maybe I would\n\n28:46.920 --> 28:51.840\n have probably said that robotic grasping is a really great example of that because it's\n\n28:51.840 --> 28:54.320\n a task with great real world utility.\n\n28:54.320 --> 28:57.320\n Like you will get a lot of money if you can do it well.\n\n28:57.320 --> 28:58.960\n What is robotic grasping?\n\n28:58.960 --> 29:02.400\n Picking up any object with a robotic hand.\n\n29:02.400 --> 29:03.400\n Exactly.\n\n29:03.400 --> 29:06.680\n So you will get a lot of money if you do it well, because lots of people want to run warehouses\n\n29:06.680 --> 29:13.360\n with robots and it's highly non trivial because very different objects will require very different\n\n29:13.360 --> 29:15.240\n grasping strategies.\n\n29:15.240 --> 29:19.740\n But actually since then, people have gotten really good at building systems to solve this\n\n29:19.740 --> 29:25.880\n problem to the point where I'm not actually sure how much more progress we can make with\n\n29:25.880 --> 29:29.560\n that as like the main guiding thing.\n\n29:29.560 --> 29:32.960\n But it's kind of interesting to see the kind of methods that have actually worked well\n\n29:32.960 --> 29:39.760\n in that space because robotic grasping classically used to be regarded very much as kind of almost\n\n29:39.760 --> 29:41.400\n like a geometry problem.\n\n29:41.400 --> 29:46.620\n So people who have studied the history of computer vision will find this very familiar\n\n29:46.620 --> 29:49.760\n that it's kind of in the same way that in the early days of computer vision, people\n\n29:49.760 --> 29:52.480\n thought of it very much as like an inverse graphics thing.\n\n29:52.480 --> 29:57.000\n In robotic grasping, people thought of it as an inverse physics problem essentially.\n\n29:57.000 --> 30:01.160\n You look at what's in front of you, figure out the shapes, then use your best estimate\n\n30:01.160 --> 30:05.960\n of the laws of physics to figure out where to put your fingers on, you pick up the thing.\n\n30:05.960 --> 30:10.360\n And it turns out that works really well for robotic grasping instantiated in many different\n\n30:10.360 --> 30:15.960\n recent works, including our own, but also ones from many other labs is to use learning\n\n30:15.960 --> 30:21.200\n methods with some combination of either exhaustive simulation or like actual real world trial\n\n30:21.200 --> 30:22.200\n and error.\n\n30:22.200 --> 30:24.360\n And it turns out that those things actually work really well and then you don't have to\n\n30:24.360 --> 30:29.160\n worry about solving geometry problems or physics problems.\n\n30:29.160 --> 30:35.040\n What are, just by the way, in the grasping, what are the difficulties that have been worked\n\n30:35.040 --> 30:36.040\n on?\n\n30:36.040 --> 30:41.080\n So one is like the materials of things, maybe occlusions on the perception side.\n\n30:41.080 --> 30:45.360\n Why is it such a difficult, why is picking stuff up such a difficult problem?\n\n30:45.360 --> 30:50.920\n Yeah, it's a difficult problem because the number of things that you might have to deal\n\n30:50.920 --> 30:54.940\n with or the variety of things that you have to deal with is extremely large.\n\n30:54.940 --> 30:59.680\n And oftentimes things that work for one class of objects won't work for other classes of\n\n30:59.680 --> 31:00.680\n objects.\n\n31:00.680 --> 31:05.400\n So if you, if you get really good at picking up boxes and now you have to pick up plastic\n\n31:05.400 --> 31:09.800\n bags, you know, you just need to employ a very different strategy.\n\n31:09.800 --> 31:15.440\n And there are many properties of objects that are more than just their geometry that has\n\n31:15.440 --> 31:19.580\n to do with, you know, the bits that are easier to pick up, the bits that are hard to pick\n\n31:19.580 --> 31:23.440\n up, the bits that are more flexible, the bits that will cause the thing to pivot and bend\n\n31:23.440 --> 31:28.000\n and drop out of your hand versus the bits that result in a nice secure grasp.\n\n31:28.000 --> 31:31.520\n Things that are flexible, things that if you pick them up the wrong way, they'll fall upside\n\n31:31.520 --> 31:33.840\n down and the contents will spill out.\n\n31:33.840 --> 31:38.820\n So there's all these little details that come up, but the task is still kind of can be characterized\n\n31:38.820 --> 31:39.820\n as one task.\n\n31:39.820 --> 31:43.800\n Like there's a very clear notion of you did it or you didn't do it.\n\n31:43.800 --> 31:50.880\n So in terms of spilling things, there creeps in this notion that starts to sound and feel\n\n31:50.880 --> 31:53.060\n like common sense reasoning.\n\n31:53.060 --> 32:01.720\n Do you think solving the general problem of robotics requires common sense reasoning,\n\n32:01.720 --> 32:09.440\n requires general intelligence, this kind of human level capability of, you know, like\n\n32:09.440 --> 32:14.320\n you said, be robust and deal with uncertainty, but also be able to sort of reason and assimilate\n\n32:14.320 --> 32:17.120\n different pieces of knowledge that you have?\n\n32:17.120 --> 32:18.120\n Yeah.\n\n32:18.120 --> 32:23.040\n What are your thoughts on the needs?\n\n32:23.040 --> 32:28.560\n Of common sense reasoning in the space of the general robotics problem?\n\n32:28.560 --> 32:32.520\n So I'm going to slightly dodge that question and say that I think maybe actually it's the\n\n32:32.520 --> 32:38.120\n other way around is that studying robotics can help us understand how to put common sense\n\n32:38.120 --> 32:40.600\n into our AI systems.\n\n32:40.600 --> 32:45.080\n One way to think about common sense is that, and why our current systems might lack common\n\n32:45.080 --> 32:51.640\n sense is that common sense is an emergent property of actually having to interact with\n\n32:51.640 --> 32:56.120\n a particular world, a particular universe, and get things done in that universe.\n\n32:56.120 --> 33:01.420\n So you might think that, for instance, like an image captioning system, maybe it looks\n\n33:01.420 --> 33:05.880\n at pictures of the world and it types out English sentences.\n\n33:05.880 --> 33:09.360\n So it kind of deals with our world.\n\n33:09.360 --> 33:12.860\n And then you can easily construct situations where image captioning systems do things that\n\n33:12.860 --> 33:16.460\n defy common sense, like give it a picture of a person wearing a fur coat and we'll say\n\n33:16.460 --> 33:18.560\n it's a teddy bear.\n\n33:18.560 --> 33:22.800\n But I think what's really happening in those settings is that the system doesn't actually\n\n33:22.800 --> 33:24.160\n live in our world.\n\n33:24.160 --> 33:28.480\n It lives in its own world that consists of pixels and English sentences and doesn't actually\n\n33:28.480 --> 33:33.280\n consist of having to put on a fur coat in the winter so you don't get cold.\n\n33:33.280 --> 33:39.860\n So perhaps the reason for the disconnect is that the systems that we have now simply inhabit\n\n33:39.860 --> 33:40.860\n a different universe.\n\n33:40.860 --> 33:45.120\n And if we build AI systems that are forced to deal with all of the messiness and complexity\n\n33:45.120 --> 33:50.520\n of our universe, maybe they will have to acquire common sense to essentially maximize their\n\n33:50.520 --> 33:51.680\n utility.\n\n33:51.680 --> 33:53.600\n Whereas the systems we're building now don't have to do that.\n\n33:53.600 --> 33:56.560\n They can take some shortcuts.\n\n33:56.560 --> 33:57.560\n That's fascinating.\n\n33:57.560 --> 34:02.400\n You've a couple of times already sort of reframed the role of robotics in this whole thing.\n\n34:02.400 --> 34:08.160\n And for some reason, I don't know if my way of thinking is common, but I thought like\n\n34:08.160 --> 34:13.240\n we need to understand and solve intelligence in order to solve robotics.\n\n34:13.240 --> 34:18.080\n And you're kind of framing it as, no, robotics is one of the best ways to just study artificial\n\n34:18.080 --> 34:24.940\n intelligence and build sort of like, robotics is like the right space in which you get to\n\n34:24.940 --> 34:33.880\n explore some of the fundamental learning mechanisms, fundamental sort of multimodal multitask aggregation\n\n34:33.880 --> 34:36.760\n of knowledge mechanisms that are required for general intelligence.\n\n34:36.760 --> 34:41.580\n It's really interesting way to think about it, but let me ask about learning.\n\n34:41.580 --> 34:47.000\n Can the general sort of robotics, the epitome of the robotics problem be solved purely through\n\n34:47.000 --> 34:55.860\n learning, perhaps end to end learning, sort of learning from scratch as opposed to injecting\n\n34:55.860 --> 35:00.120\n human expertise and rules and heuristics and so on?\n\n35:00.120 --> 35:04.680\n I think that in terms of the spirit of the question, I would say yes.\n\n35:04.680 --> 35:12.360\n I mean, I think that though in some ways it's maybe like an overly sharp dichotomy, I think\n\n35:12.360 --> 35:20.120\n that in some ways when we build algorithms, at some point a person does something, a person\n\n35:20.120 --> 35:26.460\n turned on the computer, a person implemented a TensorFlow.\n\n35:26.460 --> 35:29.840\n But yeah, I think that in terms of the point that you're getting at, I do think the answer\n\n35:29.840 --> 35:30.840\n is yes.\n\n35:30.840 --> 35:36.600\n I think that we can solve many problems that have previously required meticulous manual\n\n35:36.600 --> 35:40.120\n engineering through automated optimization techniques.\n\n35:40.120 --> 35:43.560\n And actually one thing I will say on this topic is I don't think this is actually a\n\n35:43.560 --> 35:45.200\n very radical or very new idea.\n\n35:45.200 --> 35:51.300\n I think people have been thinking about automated optimization techniques as a way to do control\n\n35:51.300 --> 35:53.680\n for a very, very long time.\n\n35:53.680 --> 35:58.040\n And in some ways what's changed is really more the name.\n\n35:58.040 --> 36:03.800\n So today we would say that, oh, my robot does machine learning, it does reinforcement learning.\n\n36:03.800 --> 36:08.520\n Maybe in the 1960s you'd say, oh, my robot is doing optimal control.\n\n36:08.520 --> 36:12.560\n And maybe the difference between typing out a system of differential equations and doing\n\n36:12.560 --> 36:17.040\n feedback linearization versus training a neural net, maybe it's not such a large difference.\n\n36:17.040 --> 36:21.840\n It's just pushing the optimization deeper and deeper into the thing.\n\n36:21.840 --> 36:28.360\n Well, it's interesting you think that way, but especially with deep learning that the\n\n36:28.360 --> 36:35.480\n accumulation of sort of experiences in data form to form deep representations starts to\n\n36:35.480 --> 36:38.880\n feel like knowledge as opposed to optimal control.\n\n36:38.880 --> 36:42.920\n So this feels like there's an accumulation of knowledge through the learning process.\n\n36:42.920 --> 36:43.920\n Yes.\n\n36:43.920 --> 36:44.920\n Yeah.\n\n36:44.920 --> 36:45.920\n So I think that is a good point.\n\n36:45.920 --> 36:49.720\n That one big difference between learning based systems and classic optimal control systems\n\n36:49.720 --> 36:53.840\n is that learning based systems in principle should get better and better the more they\n\n36:53.840 --> 36:54.840\n do something.\n\n36:54.840 --> 36:55.840\n Right.\n\n36:55.840 --> 36:58.160\n And I do think that that's actually a very, very powerful difference.\n\n36:58.160 --> 37:04.640\n So if we look back at the world of expert systems and symbolic AI and so on of using\n\n37:04.640 --> 37:11.640\n logic to accumulate expertise, human expertise, human encoded expertise, do you think that\n\n37:11.640 --> 37:13.680\n will have a role at some point?\n\n37:13.680 --> 37:20.620\n The deep learning, machine learning, reinforcement learning has shown incredible results and\n\n37:20.620 --> 37:26.620\n breakthroughs and just inspired thousands, maybe millions of researchers.\n\n37:26.620 --> 37:32.680\n But there's this less popular now, but it used to be popular idea of symbolic AI.\n\n37:32.680 --> 37:35.240\n Do you think that will have a role?\n\n37:35.240 --> 37:44.740\n I think in some ways the descendants of symbolic AI actually already have a role.\n\n37:44.740 --> 37:49.000\n So this is the highly biased history from my perspective.\n\n37:49.000 --> 37:53.920\n You say that, well, initially we thought that rational decision making involves logical\n\n37:53.920 --> 37:54.920\n manipulation.\n\n37:54.920 --> 37:59.940\n So you have some model of the world expressed in terms of logic.\n\n37:59.940 --> 38:04.760\n You have some query, like what action do I take in order for X to be true?\n\n38:04.760 --> 38:08.520\n And then you manipulate your logical symbolic representation to get an answer.\n\n38:08.520 --> 38:14.240\n What that turned into somewhere in the 1990s is, well, instead of building kind of predicates\n\n38:14.240 --> 38:20.800\n and statements that have true or false values, we'll build probabilistic systems where things\n\n38:20.800 --> 38:23.160\n have probabilities associated and probabilities of being true and false.\n\n38:23.160 --> 38:25.280\n And that turned into Bayes nets.\n\n38:25.280 --> 38:30.440\n And that provided sort of a boost to what were really still essentially logical inference\n\n38:30.440 --> 38:33.240\n systems, just probabilistic logical inference systems.\n\n38:33.240 --> 38:37.940\n And then people said, well, let's actually learn the individual probabilities inside\n\n38:37.940 --> 38:39.560\n these models.\n\n38:39.560 --> 38:43.240\n And then people said, well, let's not even specify the nodes in the models, let's just\n\n38:43.240 --> 38:45.500\n put a big neural net in there.\n\n38:45.500 --> 38:48.960\n But in many ways, I see these as actually kind of descendants from the same idea.\n\n38:48.960 --> 38:54.040\n It's essentially instantiating rational decision making by means of some inference process\n\n38:54.040 --> 38:57.840\n and learning by means of an optimization process.\n\n38:57.840 --> 39:00.320\n So in a sense, I would say, yes, that it has a place.\n\n39:00.320 --> 39:04.480\n And in many ways that place is, it already holds that place.\n\n39:04.480 --> 39:05.480\n It's already in there.\n\n39:05.480 --> 39:06.480\n Yeah.\n\n39:06.480 --> 39:07.480\n It's just quite different.\n\n39:07.480 --> 39:09.000\n It looks slightly different than it was before.\n\n39:09.000 --> 39:10.000\n Yeah.\n\n39:10.000 --> 39:13.200\n But there are some things that we can think about that make this a little bit more obvious.\n\n39:13.200 --> 39:17.760\n Like if I train a big neural net model to predict what will happen in response to my\n\n39:17.760 --> 39:22.880\n robot's actions, and then I run probabilistic inference, meaning I invert that model to\n\n39:22.880 --> 39:26.300\n figure out the actions that lead to some plausible outcome, like to me, that seems like a kind\n\n39:26.300 --> 39:27.520\n of logic.\n\n39:27.520 --> 39:32.000\n You have a model of the world that just happens to be expressed by a neural net, and you are\n\n39:32.000 --> 39:37.880\n doing some inference procedure, some sort of manipulation on that model to figure out\n\n39:37.880 --> 39:39.680\n the answer to a query that you have.\n\n39:39.680 --> 39:41.160\n It's the interpretability.\n\n39:41.160 --> 39:46.100\n It's the explainability, though, that seems to be lacking more so because the nice thing\n\n39:46.100 --> 39:52.200\n about sort of expert systems is you can follow the reasoning of the system that to us mere\n\n39:52.200 --> 39:56.320\n humans is somehow compelling.\n\n39:56.320 --> 40:04.020\n It's just I don't know what to make of this fact that there's a human desire for intelligence\n\n40:04.020 --> 40:12.680\n systems to be able to convey in a poetic way to us why it made the decisions it did, like\n\n40:12.680 --> 40:15.520\n tell a convincing story.\n\n40:15.520 --> 40:22.720\n And perhaps that's like a silly human thing, like we shouldn't expect that of intelligence\n\n40:22.720 --> 40:23.720\n systems.\n\n40:23.720 --> 40:27.800\n I'm super happy that there is intelligence systems out there.\n\n40:27.800 --> 40:33.640\n But if I were to sort of psychoanalyze the researchers at the time, I would say expert\n\n40:33.640 --> 40:40.120\n systems connected to that part, that desire of AI researchers for systems to be explainable.\n\n40:40.120 --> 40:48.000\n I mean, maybe on that topic, do you have a hope that sort of inferences of learning based\n\n40:48.000 --> 40:55.040\n systems will be as explainable as the dream was with expert systems, for example?\n\n40:55.040 --> 40:59.120\n I think it's a very complicated question because I think that in some ways the question of\n\n40:59.120 --> 41:07.440\n explainability is kind of very closely tied to the question of like performance, like,\n\n41:07.440 --> 41:11.520\n you know, why do you want your system to explain itself so that when it screws up, you can\n\n41:11.520 --> 41:14.960\n kind of figure out why it did it.\n\n41:14.960 --> 41:17.360\n But in some ways that's a much bigger problem, actually.\n\n41:17.360 --> 41:22.880\n Like your system might screw up and then it might screw up in how it explains itself.\n\n41:22.880 --> 41:26.640\n Or you might have some bug somewhere so that it's not actually doing what it was supposed\n\n41:26.640 --> 41:27.640\n to do.\n\n41:27.640 --> 41:32.360\n So, you know, maybe a good way to view that problem is really as a problem, as a bigger\n\n41:32.360 --> 41:38.640\n problem of verification and validation, of which explainability is sort of one component.\n\n41:38.640 --> 41:39.640\n I see.\n\n41:39.640 --> 41:41.200\n I just see it differently.\n\n41:41.200 --> 41:45.400\n I see explainability, you put it beautifully, I think you actually summarize the field of\n\n41:45.400 --> 41:46.400\n explainability.\n\n41:46.400 --> 41:52.880\n But to me, there's another aspect of explainability, which is like storytelling that has nothing\n\n41:52.880 --> 42:05.120\n to do with errors or with, like, it uses errors as elements of its story as opposed to a fundamental\n\n42:05.120 --> 42:08.240\n need to be explainable when errors occur.\n\n42:08.240 --> 42:12.520\n It's just that for other intelligent systems to be in our world, we seem to want to tell\n\n42:12.520 --> 42:14.800\n each other stories.\n\n42:14.800 --> 42:19.840\n And that's true in the political world, that's true in the academic world.\n\n42:19.840 --> 42:24.480\n And that, you know, neural networks are less capable of doing that, or perhaps they're\n\n42:24.480 --> 42:26.920\n equally capable of storytelling and storytelling.\n\n42:26.920 --> 42:30.360\n Maybe it doesn't matter what the fundamentals of the system are.\n\n42:30.360 --> 42:32.900\n You just need to be a good storyteller.\n\n42:32.900 --> 42:38.240\n Maybe one specific story I can tell you about in that space is actually about some work\n\n42:38.240 --> 42:43.360\n that was done by my former collaborator, who's now a professor at MIT named Jacob Andreas.\n\n42:43.360 --> 42:47.280\n Jacob actually works in natural language processing, but he had this idea to do a little bit of\n\n42:47.280 --> 42:53.360\n work in reinforcement learning on how natural language can basically structure the internals\n\n42:53.360 --> 42:55.880\n of policies trained with RL.\n\n42:55.880 --> 43:01.360\n And one of the things he did is he set up a model that attempts to perform some task\n\n43:01.360 --> 43:06.560\n that's defined by a reward function, but the model reads in a natural language instruction.\n\n43:06.560 --> 43:08.880\n So this is a pretty common thing to do in instruction following.\n\n43:08.880 --> 43:13.640\n So you tell it like, you know, go to the red house and then it's supposed to go to the red house.\n\n43:13.640 --> 43:18.300\n But then one of the things that Jacob did is he treated that sentence, not as a command\n\n43:18.300 --> 43:25.600\n from a person, but as a representation of the internal kind of a state of the mind of\n\n43:25.600 --> 43:26.680\n this policy, essentially.\n\n43:26.680 --> 43:30.320\n So that when it was faced with a new task, what it would do is it would basically try\n\n43:30.320 --> 43:34.760\n to think of possible language descriptions, attempt to do them and see if they led to\n\n43:34.760 --> 43:35.760\n the right outcome.\n\n43:35.760 --> 43:38.680\n So it would kind of think out loud, like, you know, I'm faced with this new task.\n\n43:38.680 --> 43:39.680\n What am I going to do?\n\n43:39.680 --> 43:40.680\n Let me go to the red house.\n\n43:40.680 --> 43:41.680\n Oh, that didn't work.\n\n43:41.680 --> 43:43.840\n Let me go to the blue room or something.\n\n43:43.840 --> 43:45.560\n Let me go to the green plant.\n\n43:45.560 --> 43:47.700\n And once it got some reward, it would say, oh, go to the green plant.\n\n43:47.700 --> 43:48.700\n That's what's working.\n\n43:48.700 --> 43:49.700\n I'm going to go to the green plant.\n\n43:49.700 --> 43:51.800\n And then you could look at the string that it came up with, and that was a description\n\n43:51.800 --> 43:54.480\n of how it thought it should solve the problem.\n\n43:54.480 --> 43:58.800\n So you could do, you could basically incorporate language as internal state and you can start\n\n43:58.800 --> 44:01.000\n getting some handle on these kinds of things.\n\n44:01.000 --> 44:05.400\n And then what I was kind of trying to get to is that also, if you add to the reward\n\n44:05.400 --> 44:10.160\n function, the convincingness of that story.\n\n44:10.160 --> 44:15.640\n So I have another reward signal of like people who review that story, how much they like\n\n44:15.640 --> 44:16.640\n it.\n\n44:16.640 --> 44:22.880\n So that, you know, initially that could be a hyperparameter sort of hard coded heuristic\n\n44:22.880 --> 44:30.420\n type of thing, but it's an interesting notion of the convincingness of the story becoming\n\n44:30.420 --> 44:34.160\n part of the reward function, the objective function of the explainability.\n\n44:34.160 --> 44:40.800\n That's in the world of sort of Twitter and fake news, that might be a scary notion that\n\n44:40.800 --> 44:45.640\n the nature of truth may not be as important as the convincingness of the, how convincing\n\n44:45.640 --> 44:49.880\n you are in telling the story around the facts.\n\n44:49.880 --> 44:55.480\n Well, let me ask the basic question.\n\n44:55.480 --> 44:58.700\n You're one of the world class researchers in reinforcement learning, deep reinforcement\n\n44:58.700 --> 45:01.920\n learning, certainly in the robotic space.\n\n45:01.920 --> 45:04.500\n What is reinforcement learning?\n\n45:04.500 --> 45:09.960\n I think that what reinforcement learning refers to today is really just the kind of the modern\n\n45:09.960 --> 45:13.100\n incarnation of learning based control.\n\n45:13.100 --> 45:16.420\n So classically reinforcement learning has a much more narrow definition, which is that\n\n45:16.420 --> 45:20.520\n it's literally learning from reinforcement, like the thing does something and then it\n\n45:20.520 --> 45:22.760\n gets a reward or punishment.\n\n45:22.760 --> 45:26.680\n But really I think the way the term is used today is it's used to refer more broadly to\n\n45:26.680 --> 45:28.280\n learning based control.\n\n45:28.280 --> 45:33.460\n So some kind of system that's supposed to be controlling something and it uses data\n\n45:33.460 --> 45:34.800\n to get better.\n\n45:34.800 --> 45:35.920\n And what does control mean?\n\n45:35.920 --> 45:38.520\n So this action is the fundamental element there.\n\n45:38.520 --> 45:41.140\n It means making rational decisions.\n\n45:41.140 --> 45:44.420\n And rational decisions are decisions that maximize a measure of utility.\n\n45:44.420 --> 45:48.360\n And sequentially, so you made decisions time and time and time again.\n\n45:48.360 --> 45:54.820\n Now like it's easier to see that kind of idea in the space of maybe games and the space\n\n45:54.820 --> 45:55.820\n of robotics.\n\n45:55.820 --> 45:58.880\n Do you see it bigger than that?\n\n45:58.880 --> 45:59.880\n Is it applicable?\n\n45:59.880 --> 46:04.280\n Like where are the limits of the applicability of reinforcement learning?\n\n46:04.280 --> 46:12.120\n Yeah, so rational decision making is essentially the encapsulation of the AI problem viewed\n\n46:12.120 --> 46:13.120\n through a particular lens.\n\n46:13.120 --> 46:18.560\n So any problem that we would want a machine to do, an intelligent machine, can likely\n\n46:18.560 --> 46:20.960\n be represented as a decision making problem.\n\n46:20.960 --> 46:26.760\n Learning images is a decision making problem, although not a sequential one typically.\n\n46:26.760 --> 46:30.680\n Controlling a chemical plant is a decision making problem.\n\n46:30.680 --> 46:34.640\n Deciding what videos to recommend on YouTube is a decision making problem.\n\n46:34.640 --> 46:39.800\n And one of the really appealing things about reinforcement learning is if it does encapsulate\n\n46:39.800 --> 46:43.760\n the range of all these decision making problems, perhaps working on reinforcement learning\n\n46:43.760 --> 46:50.480\n is one of the ways to reach a very broad swath of AI problems.\n\n46:50.480 --> 46:55.720\n What is the fundamental difference between reinforcement learning and maybe supervised\n\n46:55.720 --> 46:57.840\n machine learning?\n\n46:57.840 --> 47:02.840\n So reinforcement learning can be viewed as a generalization of supervised machine learning.\n\n47:02.840 --> 47:05.680\n You can certainly cast supervised learning as a reinforcement learning problem.\n\n47:05.680 --> 47:09.120\n You can just say your loss function is the negative of your reward.\n\n47:09.120 --> 47:10.120\n But you have stronger assumptions.\n\n47:10.120 --> 47:14.560\n You have the assumption that someone actually told you what the correct answer was, that\n\n47:14.560 --> 47:16.040\n your data was IID and so on.\n\n47:16.040 --> 47:20.400\n So you could view reinforcement learning as essentially relaxing some of those assumptions.\n\n47:20.400 --> 47:22.800\n Now that's not always a very productive way to look at it because if you actually have\n\n47:22.800 --> 47:26.760\n a supervised learning problem, you'll probably solve it much more effectively by using supervised\n\n47:26.760 --> 47:29.600\n learning methods because it's easier.\n\n47:29.600 --> 47:32.560\n But you can view reinforcement learning as a generalization of that.\n\n47:32.560 --> 47:33.560\n No, for sure.\n\n47:33.560 --> 47:36.040\n But they're fundamentally different.\n\n47:36.040 --> 47:37.320\n That's a mathematical statement.\n\n47:37.320 --> 47:38.960\n That's absolutely correct.\n\n47:38.960 --> 47:43.480\n But it seems that reinforcement learning, the kind of tools we bring to the table today\n\n47:43.480 --> 47:44.480\n of today.\n\n47:44.480 --> 47:49.080\n So maybe down the line, everything will be a reinforcement learning problem.\n\n47:49.080 --> 47:53.760\n Just like you said, image classification should be mapped to a reinforcement learning problem.\n\n47:53.760 --> 48:01.000\n But today, the tools and ideas, the way we think about them are different, sort of supervised\n\n48:01.000 --> 48:07.080\n learning has been used very effectively to solve basic narrow AI problems.\n\n48:07.080 --> 48:11.680\n Reinforcement learning kind of represents the dream of AI.\n\n48:11.680 --> 48:17.240\n It's very much so in the research space now in sort of captivating the imagination of\n\n48:17.240 --> 48:22.960\n people of what we can do with intelligent systems, but it hasn't yet had as wide of\n\n48:22.960 --> 48:25.520\n an impact as the supervised learning approaches.\n\n48:25.520 --> 48:32.520\n So my question comes from the more practical sense, like what do you see is the gap between\n\n48:32.520 --> 48:38.480\n the more general reinforcement learning and the very specific, yes, it's a question decision\n\n48:38.480 --> 48:43.200\n making with one step in the sequence of the supervised learning?\n\n48:43.200 --> 48:49.040\n So from a practical standpoint, I think that one thing that is potentially a little tough\n\n48:49.040 --> 48:53.000\n now, and this is I think something that we'll see, this is a gap that we might see closing\n\n48:53.000 --> 48:57.680\n over the next couple of years, is the ability of reinforcement learning algorithms to effectively\n\n48:57.680 --> 49:00.600\n utilize large amounts of prior data.\n\n49:00.600 --> 49:05.440\n So one of the reasons why it's a bit difficult today to use reinforcement learning for all\n\n49:05.440 --> 49:10.120\n the things that we might want to use it for is that in most of the settings where we want\n\n49:10.120 --> 49:15.200\n to do rational decision making, it's a little bit tough to just deploy some policy that\n\n49:15.200 --> 49:18.960\n does crazy stuff and learns purely through trial and error.\n\n49:18.960 --> 49:23.260\n It's much easier to collect a lot of data, a lot of logs of some other policy that you've\n\n49:23.260 --> 49:28.360\n got, and then maybe if you can get a good policy out of that, then you deploy it and\n\n49:28.360 --> 49:30.880\n let it kind of fine tune a little bit.\n\n49:30.880 --> 49:33.520\n But algorithmically, it's quite difficult to do that.\n\n49:33.520 --> 49:37.940\n So I think that once we figure out how to get reinforcement learning to bootstrap effectively\n\n49:37.940 --> 49:44.160\n from large data sets, then we'll see very, very rapid growth in applications of these\n\n49:44.160 --> 49:45.160\n technologies.\n\n49:45.160 --> 49:48.800\n So this is what's referred to as off policy reinforcement learning or offline RL or batch\n\n49:48.800 --> 49:50.080\n RL.\n\n49:50.080 --> 49:53.640\n And I think we're seeing a lot of research right now that's bringing us closer and closer\n\n49:53.640 --> 49:54.640\n to that.\n\n49:54.640 --> 49:57.160\n Can you maybe paint the picture of the different methods?\n\n49:57.160 --> 50:02.000\n So you said off policy, what's value based reinforcement learning?\n\n50:02.000 --> 50:03.000\n What's policy based?\n\n50:03.000 --> 50:04.000\n What's model based?\n\n50:04.000 --> 50:05.000\n What's off policy, on policy?\n\n50:05.000 --> 50:07.600\n What are the different categories of reinforcement learning?\n\n50:07.600 --> 50:08.600\n Okay.\n\n50:08.600 --> 50:14.360\n So one way we can think about reinforcement learning is that it's, in some very fundamental\n\n50:14.360 --> 50:20.200\n way, it's about learning models that can answer kind of what if questions.\n\n50:20.200 --> 50:24.360\n So what would happen if I take this action that I hadn't taken before?\n\n50:24.360 --> 50:26.840\n And you do that, of course, from experience, from data.\n\n50:26.840 --> 50:28.400\n And oftentimes you do it in a loop.\n\n50:28.400 --> 50:32.900\n So you build a model that answers these what if questions, use it to figure out the best\n\n50:32.900 --> 50:36.720\n action you can take, and then go and try taking that and see if the outcome agrees with what\n\n50:36.720 --> 50:38.880\n you predicted.\n\n50:38.880 --> 50:43.320\n So the different kinds of techniques basically refer to different ways of doing it.\n\n50:43.320 --> 50:48.840\n So model based methods answer a question of what state you would get, basically what would\n\n50:48.840 --> 50:50.960\n happen to the world if you were to take a certain action.\n\n50:50.960 --> 50:55.080\n Value based methods, they answer the question of what value you would get, meaning what\n\n50:55.080 --> 50:57.180\n utility you would get.\n\n50:57.180 --> 51:00.940\n But in a sense, they're not really all that different because they're both really just\n\n51:00.940 --> 51:03.360\n answering these what if questions.\n\n51:03.360 --> 51:07.240\n Now unfortunately for us, with current machine learning methods, answering what if questions\n\n51:07.240 --> 51:12.520\n can be really hard because they are really questions about things that didn't happen.\n\n51:12.520 --> 51:14.960\n If you wanted to answer what if questions about things that did happen, you wouldn't\n\n51:14.960 --> 51:15.960\n need a learn model.\n\n51:15.960 --> 51:19.080\n You would just like repeat the thing that worked before.\n\n51:19.080 --> 51:23.480\n And that's really a big part of why RL is a little bit tough.\n\n51:23.480 --> 51:28.960\n So if you have a purely on policy kind of online process, then you ask these what if\n\n51:28.960 --> 51:33.280\n questions, you make some mistakes, then you go and try doing those mistaken things.\n\n51:33.280 --> 51:36.640\n And then you observe kind of the counter examples that will teach you not to do those things\n\n51:36.640 --> 51:37.760\n again.\n\n51:37.760 --> 51:42.240\n If you have a bunch of off policy data and you just want to synthesize the best policy\n\n51:42.240 --> 51:46.760\n you can out of that data, then you really have to deal with the challenges of making\n\n51:46.760 --> 51:47.760\n these counterfactual.\n\n51:47.760 --> 51:50.520\n First of all, what's a policy?\n\n51:50.520 --> 51:59.200\n A policy is a model or some kind of function that maps from observations of the world to\n\n51:59.200 --> 52:00.200\n actions.\n\n52:00.200 --> 52:05.360\n So in reinforcement learning, we often refer to the current configuration of the world\n\n52:05.360 --> 52:06.360\n as the state.\n\n52:06.360 --> 52:10.000\n So we say the state kind of encompasses everything you need to fully define where the world is\n\n52:10.000 --> 52:11.560\n at the moment.\n\n52:11.560 --> 52:15.200\n And depending on how we formulate the problem, we might say you either get to see the state\n\n52:15.200 --> 52:19.840\n or you get to see an observation, which is some snapshot, some piece of the state.\n\n52:19.840 --> 52:25.880\n So policy just includes everything in it in order to be able to act in this world.\n\n52:25.880 --> 52:26.880\n Yes.\n\n52:26.880 --> 52:29.200\n And so what does off policy mean?\n\n52:29.200 --> 52:33.560\n Yeah, so the terms on policy and off policy refer to how you get your data.\n\n52:33.560 --> 52:37.480\n So if you get your data from somebody else who was doing some other stuff, maybe you\n\n52:37.480 --> 52:43.760\n get your data from some manually programmed system that was just running in the world\n\n52:43.760 --> 52:46.640\n before that's referred to as off policy data.\n\n52:46.640 --> 52:50.200\n But if you got the data by actually acting in the world based on what your current policy\n\n52:50.200 --> 52:53.420\n thinks is good, we call that on policy data.\n\n52:53.420 --> 52:58.120\n And obviously on policy data is more useful to you because if your current policy makes\n\n52:58.120 --> 53:01.860\n some bad decisions, you will actually see that those decisions are bad.\n\n53:01.860 --> 53:06.040\n Off policy data, however, might be much easier to obtain because maybe that's all the logged\n\n53:06.040 --> 53:08.680\n data that you have from before.\n\n53:08.680 --> 53:14.920\n So we talk about offline, talked about autonomous vehicles so you can envision off policy kind\n\n53:14.920 --> 53:19.880\n of approaches in robotic spaces where there's already a ton of robots out there, but they\n\n53:19.880 --> 53:26.360\n don't get the luxury of being able to explore based on a reinforcement learning framework.\n\n53:26.360 --> 53:32.040\n So how do we make, again, open question, but how do we make off policy methods work?\n\n53:32.040 --> 53:33.040\n Yeah.\n\n53:33.040 --> 53:37.140\n So this is something that has been kind of a big open problem for a while.\n\n53:37.140 --> 53:41.800\n And in the last few years, people have made a little bit of progress on that.\n\n53:41.800 --> 53:44.740\n You know, I can tell you about, and it's not by any means solved yet, but I can tell you\n\n53:44.740 --> 53:49.680\n some of the things that, for example, we've done to try to address some of the challenges.\n\n53:49.680 --> 53:53.640\n It turns out that one really big challenge with off policy reinforcement learning is\n\n53:53.640 --> 53:59.680\n that you can't really trust your models to give accurate predictions for any possible\n\n53:59.680 --> 54:00.680\n action.\n\n54:00.680 --> 54:05.880\n So if I've never tried to, if in my data set I never saw somebody steering the car off\n\n54:05.880 --> 54:11.240\n the road onto the sidewalk, my value function or my model is probably not going to predict\n\n54:11.240 --> 54:14.480\n the right thing if I ask what would happen if I were to steer the car off the road onto\n\n54:14.480 --> 54:15.680\n the sidewalk.\n\n54:15.680 --> 54:20.600\n So one of the important things you have to do to get off policy RL to work is you have\n\n54:20.600 --> 54:24.600\n to be able to figure out whether a given action will result in a trustworthy prediction or\n\n54:24.600 --> 54:25.600\n not.\n\n54:25.600 --> 54:31.240\n And you can use a kind of distribution estimation methods, kind of density estimation methods\n\n54:31.240 --> 54:32.240\n to try to figure that out.\n\n54:32.240 --> 54:35.920\n So you could figure out that, well, this action, my model is telling me that it's great, but\n\n54:35.920 --> 54:38.680\n it looks totally different from any action I've taken before, so my model is probably\n\n54:38.680 --> 54:39.680\n not correct.\n\n54:39.680 --> 54:45.200\n And you can incorporate regularization terms into your learning objective that will essentially\n\n54:45.200 --> 54:50.880\n tell you not to ask those questions that your model is unable to answer.\n\n54:50.880 --> 54:54.040\n What would lead to breakthroughs in this space, do you think?\n\n54:54.040 --> 54:55.480\n Like what's needed?\n\n54:55.480 --> 54:57.240\n Is this a data set question?\n\n54:57.240 --> 55:03.780\n Do we need to collect big benchmark data sets that allow us to explore the space?\n\n55:03.780 --> 55:08.560\n Is it a new kinds of methodologies?\n\n55:08.560 --> 55:09.960\n Like what's your sense?\n\n55:09.960 --> 55:14.160\n Or maybe coming together in a space of robotics and defining the right problem to be working\n\n55:14.160 --> 55:15.160\n on?\n\n55:15.160 --> 55:18.200\n I think for off policy reinforcement learning in particular, it's very much an algorithms\n\n55:18.200 --> 55:19.880\n question right now.\n\n55:19.880 --> 55:25.320\n And this is something that I think is great because an algorithms question is that that\n\n55:25.320 --> 55:29.800\n just takes some very smart people to get together and think about it really hard, whereas if\n\n55:29.800 --> 55:34.780\n it was like a data problem or a hardware problem, that would take some serious engineering.\n\n55:34.780 --> 55:38.340\n So that's why I'm pretty excited about that problem because I think that we're in a position\n\n55:38.340 --> 55:42.200\n where we can make some real progress on it just by coming up with the right algorithms.\n\n55:42.200 --> 55:47.900\n In terms of which algorithms they could be, the problems at their core are very related\n\n55:47.900 --> 55:51.640\n to problems in things like causal inference.\n\n55:51.640 --> 55:55.960\n Because what you're really dealing with is situations where you have a model, a statistical\n\n55:55.960 --> 56:00.620\n model, that's trying to make predictions about things that it hadn't seen before.\n\n56:00.620 --> 56:04.840\n And if it's a model that's generalizing properly, that'll make good predictions.\n\n56:04.840 --> 56:09.000\n If it's a model that picks up on spurious correlations, that will not generalize properly.\n\n56:09.000 --> 56:11.100\n And then you have an arsenal of tools you can use.\n\n56:11.100 --> 56:15.200\n You could, for example, figure out what are the regions where it's trustworthy, or on\n\n56:15.200 --> 56:18.760\n the other hand, you could try to make it generalize better somehow, or some combination of the\n\n56:18.760 --> 56:20.800\n two.\n\n56:20.800 --> 56:30.160\n Is there room for mixing where most of it, like 90, 95% is off policy, you already have\n\n56:30.160 --> 56:36.360\n the data set, and then you get to send the robot out to do a little exploration?\n\n56:36.360 --> 56:38.880\n What's that role of mixing them together?\n\n56:38.880 --> 56:39.880\n Yeah, absolutely.\n\n56:39.880 --> 56:45.320\n I think that this is something that you actually described very well at the beginning of our\n\n56:45.320 --> 56:47.480\n discussion when you talked about the iceberg.\n\n56:47.480 --> 56:48.480\n This is the iceberg.\n\n56:48.480 --> 56:51.720\n The 99% of your prior experience, that's your iceberg.\n\n56:51.720 --> 56:54.160\n You'd use that for off policy reinforcement learning.\n\n56:54.160 --> 56:59.240\n And then, of course, if you've never opened that particular kind of door with that particular\n\n56:59.240 --> 57:02.120\n lock before, then you have to go out and fiddle with it a little bit.\n\n57:02.120 --> 57:05.320\n And that's that additional 1% to help you figure out a new task.\n\n57:05.320 --> 57:08.200\n And I think that's actually a pretty good recipe going forward.\n\n57:08.200 --> 57:12.840\n Is this, to you, the most exciting space of reinforcement learning now?\n\n57:12.840 --> 57:18.240\n Or is there, what's, and maybe taking a step back, not just now, but what's, to you, is\n\n57:18.240 --> 57:23.240\n the most beautiful idea, apologize for the romanticized question, but the beautiful idea\n\n57:23.240 --> 57:27.280\n or concept in reinforcement learning?\n\n57:27.280 --> 57:32.640\n In general, I actually think that one of the things that is a very beautiful idea in reinforcement\n\n57:32.640 --> 57:41.800\n learning is just the idea that you can obtain a near optimal control or near optimal policy\n\n57:41.800 --> 57:45.640\n without actually having a complete model of the world.\n\n57:45.640 --> 57:53.080\n This is, you know, it's something that feels perhaps kind of obvious if you just hear the\n\n57:53.080 --> 57:55.880\n term reinforcement learning or you think about trial and error learning.\n\n57:55.880 --> 58:01.800\n But from a controls perspective, it's a very weird thing because classically, you know,\n\n58:01.800 --> 58:07.480\n we think about engineered systems and controlling engineered systems as the problem of writing\n\n58:07.480 --> 58:11.000\n down some equations and then figuring out given these equations, you know, basically\n\n58:11.000 --> 58:16.820\n solve for X, figure out the thing that maximizes its performance.\n\n58:16.820 --> 58:21.360\n And the theory of reinforcement learning actually gives us a mathematically principled framework\n\n58:21.360 --> 58:27.080\n to think, to reason about, you know, optimizing some quantity when you don't actually know\n\n58:27.080 --> 58:28.900\n the equations that govern that system.\n\n58:28.900 --> 58:35.040\n And I don't, to me, that's actually seems kind of, you know, very elegant, not something\n\n58:35.040 --> 58:40.160\n that sort of becomes immediately obvious, at least in the mathematical sense.\n\n58:40.160 --> 58:42.960\n Does it make sense to you that it works at all?\n\n58:42.960 --> 58:48.360\n Well, I think it makes sense when you take some time to think about it, but it is a little\n\n58:48.360 --> 58:49.360\n surprising.\n\n58:49.360 --> 58:56.720\n Well, then taking a step into the more deeper representations, which is also very surprising\n\n58:56.720 --> 59:04.840\n of sort of the richness of the state space, the space of environments that this kind of\n\n59:04.840 --> 59:10.480\n approach can operate in, can you maybe say what is deep reinforcement learning?\n\n59:10.480 --> 59:16.100\n Well, deep reinforcement learning simply refers to taking reinforcement learning algorithms\n\n59:16.100 --> 59:20.520\n and combining them with high capacity neural net representations.\n\n59:20.520 --> 59:24.140\n Which is, you know, kind of, it might at first seem like a pretty arbitrary thing, just take\n\n59:24.140 --> 59:26.560\n these two components and stick them together.\n\n59:26.560 --> 59:32.320\n But the reason that it's something that has become so important in recent years is that\n\n59:32.320 --> 59:38.160\n reinforcement learning, it kind of faces an exacerbated version of a problem that has\n\n59:38.160 --> 59:40.080\n faced many other machine learning techniques.\n\n59:40.080 --> 59:45.360\n So if we go back to like, you know, the early two thousands or the late nineties, we'll\n\n59:45.360 --> 59:50.780\n see a lot of research on machine learning methods that have some very appealing mathematical\n\n59:50.780 --> 59:56.220\n properties like they reduce the convex optimization problems, for instance, but they require very\n\n59:56.220 --> 59:57.220\n special inputs.\n\n59:57.220 --> 1:00:01.600\n They require a representation of the input that is clean in some way.\n\n1:00:01.600 --> 1:00:06.320\n Like for example, clean in the sense that the classes in your multi class classification\n\n1:00:06.320 --> 1:00:07.720\n problems separate linearly.\n\n1:00:07.720 --> 1:00:12.560\n So they have some kind of good representation and we call this a feature representation.\n\n1:00:12.560 --> 1:00:15.520\n And for a long time, people were very worried about features in the world of supervised\n\n1:00:15.520 --> 1:00:18.560\n learning because somebody had to actually build those features so you couldn't just\n\n1:00:18.560 --> 1:00:22.920\n take an image and plug it into your logistic regression or your SVM or something.\n\n1:00:22.920 --> 1:00:26.840\n How to take that image and process it using some handwritten code.\n\n1:00:26.840 --> 1:00:30.900\n And then neural nets came along and they could actually learn the features and suddenly we\n\n1:00:30.900 --> 1:00:35.360\n could apply learning directly to the raw inputs, which was great for images, but it was even\n\n1:00:35.360 --> 1:00:40.020\n more great for all the other fields where people hadn't come up with good features yet.\n\n1:00:40.020 --> 1:00:43.400\n And one of those fields actually reinforcement learning because in reinforcement learning,\n\n1:00:43.400 --> 1:00:46.840\n the notion of features, if you don't use neural nets and you have to design your own features\n\n1:00:46.840 --> 1:00:48.580\n is very, very opaque.\n\n1:00:48.580 --> 1:00:53.920\n Like it's very hard to imagine, let's say I'm playing chess or go.\n\n1:00:53.920 --> 1:00:58.760\n What is a feature with which I can represent the value function for go or even the optimal\n\n1:00:58.760 --> 1:00:59.760\n policy for go linearly?\n\n1:00:59.760 --> 1:01:03.100\n Like I don't even know how to start thinking about it.\n\n1:01:03.100 --> 1:01:06.040\n And people tried all sorts of things that would write down, you know, an expert chess\n\n1:01:06.040 --> 1:01:09.160\n player looks for whether the knight is in the middle of the board or not.\n\n1:01:09.160 --> 1:01:11.760\n So that's a feature is knight in middle of board.\n\n1:01:11.760 --> 1:01:15.960\n And they would write these like long lists of kind of arbitrary made up stuff.\n\n1:01:15.960 --> 1:01:17.680\n And that was really kind of getting us nowhere.\n\n1:01:17.680 --> 1:01:21.960\n And that's a little, chess is a little more accessible than the robotics problem.\n\n1:01:21.960 --> 1:01:22.960\n Absolutely.\n\n1:01:22.960 --> 1:01:23.960\n Right.\n\n1:01:23.960 --> 1:01:30.340\n There's at least experts in the different features for chess, but still like the neural\n\n1:01:30.340 --> 1:01:35.700\n network there, to me, that's, I mean, you put it eloquently and almost made it seem\n\n1:01:35.700 --> 1:01:41.000\n like a natural step to add neural networks, but the fact that neural networks are able\n\n1:01:41.000 --> 1:01:45.640\n to discover features in the control problem, it's very interesting.\n\n1:01:45.640 --> 1:01:46.640\n It's hopeful.\n\n1:01:46.640 --> 1:01:51.880\n I'm not sure what to think about it, but it feels hopeful that the control problem has\n\n1:01:51.880 --> 1:01:54.680\n features to be learned.\n\n1:01:54.680 --> 1:02:02.360\n Like I guess my question is, is it surprising to you how far the deep side of deep reinforcement\n\n1:02:02.360 --> 1:02:07.560\n learning was able to like what the space of problems has been able to tackle from, especially\n\n1:02:07.560 --> 1:02:17.600\n in games with alpha star and alpha zero and just the representation power there and in\n\n1:02:17.600 --> 1:02:23.120\n the robotics space and what is your sense of the limits of this representation power\n\n1:02:23.120 --> 1:02:26.120\n and the control context?\n\n1:02:26.120 --> 1:02:32.900\n I think that in regard to the limits that here, I think that one thing that makes it\n\n1:02:32.900 --> 1:02:39.380\n a little hard to fully answer this question is because in settings where we would like\n\n1:02:39.380 --> 1:02:44.040\n to push these things to the limit, we encounter other bottlenecks.\n\n1:02:44.040 --> 1:02:51.480\n So like the reason that I can't get my robot to learn how to like, I don't know, do the\n\n1:02:51.480 --> 1:02:56.220\n dishes in the kitchen, it's not because it's neural net is not big enough.\n\n1:02:56.220 --> 1:03:02.680\n It's because when you try to actually do trial and error learning, reinforcement learning,\n\n1:03:02.680 --> 1:03:07.840\n directly in the real world where you have the potential to gather these large, highly\n\n1:03:07.840 --> 1:03:11.720\n varied and complex data sets, you start running into other problems.\n\n1:03:11.720 --> 1:03:16.920\n Like one problem you run into very quickly, it'll first sound like a very pragmatic problem,\n\n1:03:16.920 --> 1:03:19.480\n but it actually turns out to be a pretty deep scientific problem.\n\n1:03:19.480 --> 1:03:22.320\n Take the robot, put it in your kitchen, have it try to learn to do the dishes with trial\n\n1:03:22.320 --> 1:03:23.320\n and error.\n\n1:03:23.320 --> 1:03:27.120\n It'll break all your dishes and then we'll have no more dishes to clean.\n\n1:03:27.120 --> 1:03:30.080\n Now you might think this is a very practical issue, but there's something to this, which\n\n1:03:30.080 --> 1:03:33.720\n is that if you have a person trying to do this, a person will have some degree of common\n\n1:03:33.720 --> 1:03:34.720\n sense.\n\n1:03:34.720 --> 1:03:37.360\n They'll break one dish, they'll be a little more careful with the next one, and if they\n\n1:03:37.360 --> 1:03:41.200\n break all of them, they're going to go and get more or something like that.\n\n1:03:41.200 --> 1:03:46.800\n So there's all sorts of scaffolding that comes very naturally to us for our learning process.\n\n1:03:46.800 --> 1:03:50.720\n Like if I have to learn something through trial and error, I have the common sense to\n\n1:03:50.720 --> 1:03:53.120\n know that I have to try multiple times.\n\n1:03:53.120 --> 1:03:57.440\n If I screw something up, I ask for help or I reset things or something like that.\n\n1:03:57.440 --> 1:04:02.100\n And all of that is kind of outside of the classic reinforcement learning problem formulation.\n\n1:04:02.100 --> 1:04:07.360\n There are other things that can also be categorized as kind of scaffolding, but are very important.\n\n1:04:07.360 --> 1:04:09.520\n Like for example, where do you get your reward function?\n\n1:04:09.520 --> 1:04:15.360\n If I want to learn how to pour a cup of water, well, how do I know if I've done it correctly?\n\n1:04:15.360 --> 1:04:18.840\n Now that probably requires an entire computer vision system to be built just to determine\n\n1:04:18.840 --> 1:04:21.220\n that, and that seems a little bit inelegant.\n\n1:04:21.220 --> 1:04:24.460\n So there are all sorts of things like this that start to come up when we think through\n\n1:04:24.460 --> 1:04:28.560\n what we really need to get reinforcement learning to happen at scale in the real world.\n\n1:04:28.560 --> 1:04:32.320\n And many of these things actually suggest a little bit of a shortcoming in the problem\n\n1:04:32.320 --> 1:04:36.240\n formulation and a few deeper questions that we have to resolve.\n\n1:04:36.240 --> 1:04:37.240\n That's really interesting.\n\n1:04:37.240 --> 1:04:45.440\n I talked to David Silver about AlphaZero, and it seems like there's no, again, we haven't\n\n1:04:45.440 --> 1:04:50.200\n hit the limit at all in the context where there's no broken dishes.\n\n1:04:50.200 --> 1:04:55.080\n So in the case of Go, you can, it's really about just scaling compute.\n\n1:04:55.080 --> 1:05:00.760\n So again, like the bottleneck is the amount of money you're willing to invest in compute\n\n1:05:00.760 --> 1:05:06.160\n and then maybe the different, the scaffolding around how difficult it is to scale compute\n\n1:05:06.160 --> 1:05:09.000\n maybe, but there, there's no limit.\n\n1:05:09.000 --> 1:05:12.640\n And it's interesting, now we'll move to the real world and there's the broken dishes,\n\n1:05:12.640 --> 1:05:17.080\n there's all the, and the reward function, like you mentioned, that's really nice.\n\n1:05:17.080 --> 1:05:19.920\n So what, how do we push forward there?\n\n1:05:19.920 --> 1:05:25.680\n Do you think there's, there's this kind of a sample efficiency question that people bring\n\n1:05:25.680 --> 1:05:30.740\n up of, you know, not having to break a hundred thousand dishes.\n\n1:05:30.740 --> 1:05:33.020\n Is this an algorithm question?\n\n1:05:33.020 --> 1:05:37.680\n Is this a data selection like question?\n\n1:05:37.680 --> 1:05:38.680\n What do you think?\n\n1:05:38.680 --> 1:05:41.320\n How do we, how do we not break too many dishes?\n\n1:05:41.320 --> 1:05:42.320\n Yeah.\n\n1:05:42.320 --> 1:05:51.360\n Well, one way we can think about that is that maybe we need to be better at, at reusing\n\n1:05:51.360 --> 1:05:54.080\n our data, building that, that iceberg.\n\n1:05:54.080 --> 1:06:02.560\n So perhaps, perhaps it's too much to hope that you can have a machine that's in isolation\n\n1:06:02.560 --> 1:06:07.280\n in the vacuum without anything else, can just master complex tasks in like in minutes the\n\n1:06:07.280 --> 1:06:10.840\n way that people do, but perhaps it also doesn't have to, perhaps what it really needs to do\n\n1:06:10.840 --> 1:06:16.240\n is have an existence, a lifetime where it does many things and the previous things that\n\n1:06:16.240 --> 1:06:20.400\n it has done, prepare it to do new things more efficiently.\n\n1:06:20.400 --> 1:06:24.260\n And you know, the study of these kinds of questions typically falls under categories\n\n1:06:24.260 --> 1:06:29.200\n like multitask learning or meta learning, but they all fundamentally deal with the same\n\n1:06:29.200 --> 1:06:35.640\n general theme, which is use experience for doing other things to learn to do new things\n\n1:06:35.640 --> 1:06:37.240\n efficiently and quickly.\n\n1:06:37.240 --> 1:06:41.880\n So what do you think about if we just look at the one particular case study of a Tesla\n\n1:06:41.880 --> 1:06:48.520\n autopilot that has quickly approaching towards a million vehicles on the road where some\n\n1:06:48.520 --> 1:06:54.440\n percentage of the time, 30, 40% of the time is driven using the computer vision, multitask\n\n1:06:54.440 --> 1:06:57.960\n hydranet, right?\n\n1:06:57.960 --> 1:07:03.040\n And then the other percent, that's what they call it, hydranet.\n\n1:07:03.040 --> 1:07:06.360\n The other percent is human controlled.\n\n1:07:06.360 --> 1:07:09.920\n In the human side, how can we use that data?\n\n1:07:09.920 --> 1:07:12.920\n What's your sense?\n\n1:07:12.920 --> 1:07:13.920\n What's the signal?\n\n1:07:13.920 --> 1:07:17.900\n Do you have ideas in this autonomous vehicle space when people can lose their lives?\n\n1:07:17.900 --> 1:07:21.560\n You know, it's a safety critical environment.\n\n1:07:21.560 --> 1:07:23.960\n So how do we use that data?\n\n1:07:23.960 --> 1:07:33.000\n So I think that actually the kind of problems that come up when we want systems that are\n\n1:07:33.000 --> 1:07:37.040\n reliable and that can kind of understand the limits of their capabilities, they're actually\n\n1:07:37.040 --> 1:07:40.680\n very similar to the kind of problems that come up when we're doing off policy reinforcement\n\n1:07:40.680 --> 1:07:41.680\n learning.\n\n1:07:41.680 --> 1:07:46.120\n So as I mentioned before, in off policy reinforcement learning, the big problem is you need to know\n\n1:07:46.120 --> 1:07:50.880\n when you can trust the predictions of your model, because if you're trying to evaluate\n\n1:07:50.880 --> 1:07:54.240\n some pattern of behavior for which your model doesn't give you an accurate prediction, then\n\n1:07:54.240 --> 1:07:57.360\n you shouldn't use that to modify your policy.\n\n1:07:57.360 --> 1:08:00.200\n It's actually very similar to the problem that we're faced when we actually then deploy\n\n1:08:00.200 --> 1:08:05.120\n that thing and we want to decide whether we trust it in the moment or not.\n\n1:08:05.120 --> 1:08:08.360\n So perhaps we just need to do a better job of figuring out that part, and that's a very\n\n1:08:08.360 --> 1:08:11.460\n deep research question, of course, but it's also a question that a lot of people are working\n\n1:08:11.460 --> 1:08:12.460\n on.\n\n1:08:12.460 --> 1:08:15.920\n So I'm pretty optimistic that we can make some progress on that over the next few years.\n\n1:08:15.920 --> 1:08:20.400\n What's the role of simulation in reinforcement learning, deep reinforcement learning, reinforcement\n\n1:08:20.400 --> 1:08:21.400\n learning?\n\n1:08:21.400 --> 1:08:23.000\n Like how essential is it?\n\n1:08:23.000 --> 1:08:28.160\n It's been essential for the breakthroughs so far for some interesting breakthroughs.\n\n1:08:28.160 --> 1:08:31.440\n Do you think it's a crutch that we rely on?\n\n1:08:31.440 --> 1:08:37.360\n I mean, again, this connects to our off policy discussion, but do you think we can ever get\n\n1:08:37.360 --> 1:08:40.160\n rid of simulation or do you think simulation will actually take over?\n\n1:08:40.160 --> 1:08:46.000\n We'll create more and more realistic simulations that will allow us to solve actual real world\n\n1:08:46.000 --> 1:08:49.960\n problems, like transfer the models we learn in simulation to real world problems.\n\n1:08:49.960 --> 1:08:54.360\n I think that simulation is a very pragmatic tool that we can use to get a lot of useful\n\n1:08:54.360 --> 1:09:00.000\n stuff to work right now, but I think that in the long run, we will need to build machines\n\n1:09:00.000 --> 1:09:03.400\n that can learn from real data because that's the only way that we'll get them to improve\n\n1:09:03.400 --> 1:09:08.680\n perpetually because if we can't have our machines learn from real data, if they have to rely\n\n1:09:08.680 --> 1:09:11.680\n on simulated data, eventually the simulator becomes the bottleneck.\n\n1:09:11.680 --> 1:09:13.560\n In fact, this is a general thing.\n\n1:09:13.560 --> 1:09:19.120\n If your machine has any bottleneck that is built by humans and that doesn't improve from\n\n1:09:19.120 --> 1:09:23.400\n data, it will eventually be the thing that holds it back.\n\n1:09:23.400 --> 1:09:25.900\n And if you're entirely reliant on your simulator, that'll be the bottleneck.\n\n1:09:25.900 --> 1:09:30.520\n If you're entirely reliant on a manually designed controller, that's going to be the bottleneck.\n\n1:09:30.520 --> 1:09:32.160\n So simulation is very useful.\n\n1:09:32.160 --> 1:09:39.840\n It's very pragmatic, but it's not a substitute for being able to utilize real experience.\n\n1:09:39.840 --> 1:09:44.600\n And by the way, this is something that I think is quite relevant now, especially in the context\n\n1:09:44.600 --> 1:09:48.840\n of some of the things we've discussed, because some of these kind of scaffolding issues that\n\n1:09:48.840 --> 1:09:52.000\n I mentioned, things like the broken dishes and the unknown reward function, like these\n\n1:09:52.000 --> 1:09:57.700\n are not problems that you would ever stumble on when working in a purely simulated kind\n\n1:09:57.700 --> 1:10:01.720\n of environment, but they become very apparent when we try to actually run these things in\n\n1:10:01.720 --> 1:10:02.720\n the real world.\n\n1:10:02.720 --> 1:10:07.080\n To throw a brief wrench into our discussion, let me ask, do you think we're living in a\n\n1:10:07.080 --> 1:10:08.080\n simulation?\n\n1:10:08.080 --> 1:10:09.080\n Oh, I have no idea.\n\n1:10:09.080 --> 1:10:15.960\n Do you think that's a useful thing to even think about, about the fundamental physics\n\n1:10:15.960 --> 1:10:18.880\n nature of reality?\n\n1:10:18.880 --> 1:10:24.520\n Or another perspective, the reason I think the simulation hypothesis is interesting is\n\n1:10:24.520 --> 1:10:33.080\n to think about how difficult is it to create sort of a virtual reality game type situation\n\n1:10:33.080 --> 1:10:38.760\n that will be sufficiently convincing to us humans or sufficiently enjoyable that we wouldn't\n\n1:10:38.760 --> 1:10:39.760\n want to leave.\n\n1:10:39.760 --> 1:10:43.560\n I mean, that's actually a practical engineering challenge.\n\n1:10:43.560 --> 1:10:47.820\n And I personally really enjoy virtual reality, but it's quite far away.\n\n1:10:47.820 --> 1:10:52.520\n I kind of think about what would it take for me to want to spend more time in virtual reality\n\n1:10:52.520 --> 1:10:55.320\n versus the real world.\n\n1:10:55.320 --> 1:11:03.920\n And that's a sort of a nice clean question because at that point, if I want to live in\n\n1:11:03.920 --> 1:11:08.040\n a virtual reality, that means we're just a few years away where a majority of the population\n\n1:11:08.040 --> 1:11:09.040\n lives in a virtual reality.\n\n1:11:09.040 --> 1:11:11.480\n And that's how we create the simulation, right?\n\n1:11:11.480 --> 1:11:19.860\n You don't need to actually simulate the quantum gravity and just every aspect of the universe.\n\n1:11:19.860 --> 1:11:24.800\n And that's an interesting question for reinforcement learning too, is if we want to make sufficiently\n\n1:11:24.800 --> 1:11:32.520\n realistic simulations that may blend the difference between sort of the real world and the simulation,\n\n1:11:32.520 --> 1:11:37.640\n thereby just some of the things we've been talking about, kind of the problems go away\n\n1:11:37.640 --> 1:11:40.840\n if we can create actually interesting, rich simulations.\n\n1:11:40.840 --> 1:11:41.840\n It's an interesting question.\n\n1:11:41.840 --> 1:11:46.320\n And it actually, I think your question casts your previous question in a very interesting\n\n1:11:46.320 --> 1:11:53.560\n light, because in some ways asking whether we can, well, the more kind of practical version\n\n1:11:53.560 --> 1:11:57.600\n is like, you know, can we build simulators that are good enough to train essentially\n\n1:11:57.600 --> 1:12:02.200\n AI systems that will work in the world?\n\n1:12:02.200 --> 1:12:06.440\n And it's kind of interesting to think about this, about what this implies, if true, it\n\n1:12:06.440 --> 1:12:11.260\n kind of implies that it's easier to create the universe than it is to create a brain.\n\n1:12:11.260 --> 1:12:14.520\n And that seems like, put this way, it seems kind of weird.\n\n1:12:14.520 --> 1:12:21.120\n The aspect of the simulation most interesting to me is the simulation of other humans.\n\n1:12:21.120 --> 1:12:27.980\n That seems to be a complexity that makes the robotics problem harder.\n\n1:12:27.980 --> 1:12:32.040\n Now I don't know if every robotics person agrees with that notion.\n\n1:12:32.040 --> 1:12:38.040\n Just as a quick aside, what are your thoughts about when the human enters the picture of\n\n1:12:38.040 --> 1:12:39.960\n the robotics problem?\n\n1:12:39.960 --> 1:12:44.560\n How does that change the reinforcement learning problem, the learning problem in general?\n\n1:12:44.560 --> 1:12:48.720\n Yeah, I think that's a, it's a kind of a complex question.\n\n1:12:48.720 --> 1:12:56.680\n And I guess my hope for a while had been that if we build these robotic learning systems\n\n1:12:56.680 --> 1:13:03.280\n that are multitask, that utilize lots of prior data and that learn from their own experience,\n\n1:13:03.280 --> 1:13:07.480\n the bit where they have to interact with people will be perhaps handled in much the same way\n\n1:13:07.480 --> 1:13:08.840\n as all the other bits.\n\n1:13:08.840 --> 1:13:12.440\n So if they have prior experience of interacting with people and they can learn from their\n\n1:13:12.440 --> 1:13:16.640\n own experience of interacting with people for this new task, maybe that'll be enough.\n\n1:13:16.640 --> 1:13:20.700\n Now, of course, if it's not enough, there are many other things we can do and there's\n\n1:13:20.700 --> 1:13:22.880\n quite a bit of research in that area.\n\n1:13:22.880 --> 1:13:29.400\n But I think it's worth a shot to see whether the multi agent interaction, the ability to\n\n1:13:29.400 --> 1:13:35.220\n understand that other beings in the world have their own goals and tensions and thoughts\n\n1:13:35.220 --> 1:13:41.580\n and so on, whether that kind of understanding can emerge automatically from simply learning\n\n1:13:41.580 --> 1:13:44.160\n to do things with and maximize utility.\n\n1:13:44.160 --> 1:13:46.940\n That information arises from the data.\n\n1:13:46.940 --> 1:13:53.400\n You've said something about gravity, that you don't need to explicitly inject anything\n\n1:13:53.400 --> 1:13:54.400\n into the system.\n\n1:13:54.400 --> 1:13:55.840\n They can be learned from the data.\n\n1:13:55.840 --> 1:13:59.740\n And gravity is an example of something that could be learned from data, so like the physics\n\n1:13:59.740 --> 1:14:05.300\n of the world.\n\n1:14:05.300 --> 1:14:08.520\n What are the limits of what we can learn from data?\n\n1:14:08.520 --> 1:14:10.460\n Do you really think we can?\n\n1:14:10.460 --> 1:14:15.600\n So a very simple, clean way to ask that is, do you really think we can learn gravity from\n\n1:14:15.600 --> 1:14:19.920\n just data, the idea, the laws of gravity?\n\n1:14:19.920 --> 1:14:25.720\n So something that I think is a common kind of pitfall when thinking about prior knowledge\n\n1:14:25.720 --> 1:14:33.360\n and learning is to assume that just because we know something, then that it's better to\n\n1:14:33.360 --> 1:14:36.880\n tell the machine about that rather than have it figured out on its own.\n\n1:14:36.880 --> 1:14:44.060\n In many cases, things that are important that affect many of the events that the machine\n\n1:14:44.060 --> 1:14:48.360\n will experience are actually pretty easy to learn.\n\n1:14:48.360 --> 1:14:54.320\n If every time you drop something, it falls down, yeah, you might get the Newton's version,\n\n1:14:54.320 --> 1:14:58.680\n not Einstein's version, but it'll be pretty good and it will probably be sufficient for\n\n1:14:58.680 --> 1:15:03.320\n you to act rationally in the world because you see the phenomenon all the time.\n\n1:15:03.320 --> 1:15:07.640\n So things that are readily apparent from the data, we might not need to specify those by\n\n1:15:07.640 --> 1:15:08.640\n hand.\n\n1:15:08.640 --> 1:15:10.320\n It might actually be easier to let the machine figure them out.\n\n1:15:10.320 --> 1:15:17.400\n It just feels like that there might be a space of many local minima in terms of theories\n\n1:15:17.400 --> 1:15:25.760\n of this world that we would discover and get stuck on, that Newtonian mechanics is not necessarily\n\n1:15:25.760 --> 1:15:27.320\n easy to come by.\n\n1:15:27.320 --> 1:15:28.320\n Yeah.\n\n1:15:28.320 --> 1:15:33.040\n And in fact, in some fields of science, for example, human civilization is itself full\n\n1:15:33.040 --> 1:15:34.040\n of these local optima.\n\n1:15:34.040 --> 1:15:40.520\n So for example, if you think about how people tried to figure out biology and medicine for\n\n1:15:40.520 --> 1:15:45.800\n the longest time, the kind of rules, the kind of principles that serve us very well in our\n\n1:15:45.800 --> 1:15:50.160\n day to day lives actually serve us very poorly in understanding medicine and biology.\n\n1:15:50.160 --> 1:15:55.320\n We had kind of very superstitious and weird ideas about how the body worked until the\n\n1:15:55.320 --> 1:15:58.020\n advent of the modern scientific method.\n\n1:15:58.020 --> 1:16:02.080\n So that does seem to be a failing of this approach, but it's also a failing of human\n\n1:16:02.080 --> 1:16:04.380\n intelligence arguably.\n\n1:16:04.380 --> 1:16:09.680\n Maybe a small aside, but some, you know, the idea of self play is fascinating in reinforcement\n\n1:16:09.680 --> 1:16:14.840\n learning sort of these competitive, creating a competitive context in which agents can\n\n1:16:14.840 --> 1:16:20.340\n play against each other in a, sort of at the same skill level and thereby increasing each\n\n1:16:20.340 --> 1:16:21.340\n other skill level.\n\n1:16:21.340 --> 1:16:26.320\n It seems to be this kind of self improving mechanism is exceptionally powerful in the\n\n1:16:26.320 --> 1:16:29.020\n context where it could be applied.\n\n1:16:29.020 --> 1:16:34.920\n First of all, is that beautiful to you that this mechanism work as well as it does?\n\n1:16:34.920 --> 1:16:41.880\n And also can we generalize to other contexts like in the robotic space or anything that's\n\n1:16:41.880 --> 1:16:43.840\n applicable to the real world?\n\n1:16:43.840 --> 1:16:51.560\n I think that it's a very interesting idea, but I suspect that the bottleneck to actually\n\n1:16:51.560 --> 1:16:56.240\n generalizing it to the robotic setting is actually going to be the same as the bottleneck\n\n1:16:56.240 --> 1:17:01.200\n for everything else that we need to be able to build machines that can get better and\n\n1:17:01.200 --> 1:17:04.760\n better through natural interaction with the world.\n\n1:17:04.760 --> 1:17:08.400\n And once we can do that, then they can go out and play with, they can play with each\n\n1:17:08.400 --> 1:17:13.040\n other, they can play with people, they can play with the natural environment.\n\n1:17:13.040 --> 1:17:16.040\n But before we get there, we've got all these other problems we've got, we have to get out\n\n1:17:16.040 --> 1:17:17.040\n of the way.\n\n1:17:17.040 --> 1:17:18.040\n So there's no shortcut around that.\n\n1:17:18.040 --> 1:17:21.160\n You have to interact with a natural environment that.\n\n1:17:21.160 --> 1:17:24.660\n Well because in a, in a self play setting, you still need a mediating mechanism.\n\n1:17:24.660 --> 1:17:30.080\n So the, the reason that, you know, self play works for a board game is because the rules\n\n1:17:30.080 --> 1:17:33.780\n of that board game mediate the interaction between the agents.\n\n1:17:33.780 --> 1:17:37.760\n So the kind of intelligent behavior that will emerge depends very heavily on the nature\n\n1:17:37.760 --> 1:17:39.920\n of that mediating mechanism.\n\n1:17:39.920 --> 1:17:44.360\n So on the side of reward functions, that's coming up with good reward functions seems\n\n1:17:44.360 --> 1:17:50.760\n to be the thing that we associate with general intelligence, like human beings seem to value\n\n1:17:50.760 --> 1:17:57.000\n the idea of developing our own reward functions of, you know, at arriving at meaning and so\n\n1:17:57.000 --> 1:17:58.440\n on.\n\n1:17:58.440 --> 1:18:02.840\n And yet for reinforcement learning, we often kind of specify that's the given.\n\n1:18:02.840 --> 1:18:08.360\n What's your sense of how we develop reward, you know, good reward functions?\n\n1:18:08.360 --> 1:18:12.160\n Yeah, I think that's a very complicated and very deep question.\n\n1:18:12.160 --> 1:18:16.520\n And you're completely right that classically in reinforcement learning, this question,\n\n1:18:16.520 --> 1:18:21.420\n I guess, kind of been treated as an on issue that you sort of treat the reward as this\n\n1:18:21.420 --> 1:18:27.360\n external thing that comes from some other bit of your biology and you kind of don't\n\n1:18:27.360 --> 1:18:28.520\n worry about it.\n\n1:18:28.520 --> 1:18:32.520\n And I do think that that's actually, you know, a little bit of a mistake that we should worry\n\n1:18:32.520 --> 1:18:33.520\n about it.\n\n1:18:33.520 --> 1:18:34.920\n And we can approach it in a few different ways.\n\n1:18:34.920 --> 1:18:39.040\n We can approach it, for instance, by thinking of rewards as a communication medium.\n\n1:18:39.040 --> 1:18:43.400\n We can say, well, how does a person communicate to a robot what its objective is?\n\n1:18:43.400 --> 1:18:47.720\n You can approach it also as a sort of more of an intrinsic motivation medium.\n\n1:18:47.720 --> 1:18:55.200\n You could say, can we write down kind of a general objective that leads to good capability?\n\n1:18:55.200 --> 1:18:58.000\n Like for example, can you write down some objectives such that even in the absence of\n\n1:18:58.000 --> 1:19:02.680\n any other task, if you maximize that objective, you'll sort of learn useful things.\n\n1:19:02.680 --> 1:19:07.040\n This is something that has sometimes been called unsupervised reinforcement learning,\n\n1:19:07.040 --> 1:19:11.600\n which I think is a really fascinating area of research, especially today.\n\n1:19:11.600 --> 1:19:13.040\n We've done a bit of work on that recently.\n\n1:19:13.040 --> 1:19:19.920\n One of the things we've studied is whether we can have some notion of unsupervised reinforcement\n\n1:19:19.920 --> 1:19:25.160\n learning by means of, you know, information theoretic quantities, like for instance, minimizing\n\n1:19:25.160 --> 1:19:26.660\n a Bayesian measure of surprise.\n\n1:19:26.660 --> 1:19:30.160\n This is an idea that was, you know, pioneered actually in the computational neuroscience\n\n1:19:30.160 --> 1:19:32.900\n community by folks like Carl Friston.\n\n1:19:32.900 --> 1:19:35.980\n And we've done some work recently that shows that you can actually learn pretty interesting\n\n1:19:35.980 --> 1:19:41.920\n skills by essentially behaving in a way that allows you to make accurate predictions about\n\n1:19:41.920 --> 1:19:42.920\n the world.\n\n1:19:42.920 --> 1:19:48.840\n Like do the things that will lead to you getting the right answer for prediction.\n\n1:19:48.840 --> 1:19:52.960\n But you can, you know, by doing this, you can sort of discover stable niches in the\n\n1:19:52.960 --> 1:19:53.960\n world.\n\n1:19:53.960 --> 1:19:57.940\n You can discover that if you're playing Tetris, then correctly, you know, clearing the rows\n\n1:19:57.940 --> 1:20:01.840\n will let you play Tetris for longer and keep the board nice and clean, which sort of satisfies\n\n1:20:01.840 --> 1:20:04.180\n some desire for order in the world.\n\n1:20:04.180 --> 1:20:07.400\n And as a result, get some degree of leverage over your domain.\n\n1:20:07.400 --> 1:20:08.800\n So we're exploring that pretty actively.\n\n1:20:08.800 --> 1:20:15.960\n Is there a role for a human notion of curiosity in itself being the reward, sort of discovering\n\n1:20:15.960 --> 1:20:19.880\n new things about the world?\n\n1:20:19.880 --> 1:20:26.000\n So one of the things that I'm pretty interested in is actually whether discovering new things\n\n1:20:26.000 --> 1:20:30.760\n can actually be an emergent property of some other objective that quantifies capability.\n\n1:20:30.760 --> 1:20:36.440\n So new things for the sake of new things maybe is not, maybe might not by itself be the right\n\n1:20:36.440 --> 1:20:42.280\n answer, but perhaps we can figure out an objective for which discovering new things is actually\n\n1:20:42.280 --> 1:20:44.480\n the natural consequence.\n\n1:20:44.480 --> 1:20:47.400\n That's something we're working on right now, but I don't have a clear answer for you there\n\n1:20:47.400 --> 1:20:49.640\n yet that's still a work in progress.\n\n1:20:49.640 --> 1:20:57.640\n You mean just that it's a curious observation to see sort of creative patterns of curiosity\n\n1:20:57.640 --> 1:21:00.980\n on the way to optimize for a particular task?\n\n1:21:00.980 --> 1:21:05.520\n On the way to optimize for a particular measure of capability.\n\n1:21:05.520 --> 1:21:15.040\n Is there ways to understand or anticipate unexpected unintended consequences of particular\n\n1:21:15.040 --> 1:21:22.280\n reward functions, sort of anticipate the kind of strategies that might be developed and\n\n1:21:22.280 --> 1:21:27.120\n try to avoid highly detrimental strategies?\n\n1:21:27.120 --> 1:21:30.260\n So classically, this is something that has been pretty hard in reinforcement learning\n\n1:21:30.260 --> 1:21:35.380\n because it's difficult for a designer to have good intuition about, you know, what a learning\n\n1:21:35.380 --> 1:21:38.960\n algorithm will come up with when they give it some objective.\n\n1:21:38.960 --> 1:21:40.340\n There are ways to mitigate that.\n\n1:21:40.340 --> 1:21:45.240\n One way to mitigate it is to actually define an objective that says like, don't do weird\n\n1:21:45.240 --> 1:21:46.240\n stuff.\n\n1:21:46.240 --> 1:21:47.240\n You can actually quantify it.\n\n1:21:47.240 --> 1:21:52.340\n You can say just like, don't enter situations that have low probability under the distribution\n\n1:21:52.340 --> 1:21:54.720\n of states you've seen before.\n\n1:21:54.720 --> 1:21:57.840\n It turns out that that's actually one very good way to do off policy reinforcement learning\n\n1:21:57.840 --> 1:21:59.560\n actually.\n\n1:21:59.560 --> 1:22:02.500\n So we can do some things like that.\n\n1:22:02.500 --> 1:22:08.360\n If we slowly venture in speaking about reward functions into greater and greater levels\n\n1:22:08.360 --> 1:22:16.280\n of intelligence, there's, I mean, Stuart Russell thinks about this, the alignment of AI systems\n\n1:22:16.280 --> 1:22:18.160\n with us humans.\n\n1:22:18.160 --> 1:22:23.040\n So how do we ensure that AGI systems align with us humans?\n\n1:22:23.040 --> 1:22:32.320\n It's kind of a reward function question of specifying the behavior of AI systems such\n\n1:22:32.320 --> 1:22:39.640\n that their success aligns with this, with the broader intended success interest of human\n\n1:22:39.640 --> 1:22:40.640\n beings.\n\n1:22:40.640 --> 1:22:41.640\n Do you have thoughts on this?\n\n1:22:41.640 --> 1:22:45.840\n Do you have kind of concerns of where reinforcement learning fits into this, or are you really\n\n1:22:45.840 --> 1:22:50.840\n focused on the current moment of us being quite far away and trying to solve the robotics\n\n1:22:50.840 --> 1:22:51.840\n problem?\n\n1:22:51.840 --> 1:22:56.780\n I don't have a great answer to this, but, you know, and I do think that this is a problem\n\n1:22:56.780 --> 1:22:59.520\n that's important to figure out.\n\n1:22:59.520 --> 1:23:04.520\n For my part, I'm actually a bit more concerned about the other side of the, of this equation\n\n1:23:04.520 --> 1:23:11.920\n that, you know, maybe rather than unintended consequences for objectives that are specified\n\n1:23:11.920 --> 1:23:15.980\n too well, I'm actually more worried right now about unintended consequences for objectives\n\n1:23:15.980 --> 1:23:21.480\n that are not optimized well enough, which might become a very pressing problem when\n\n1:23:21.480 --> 1:23:26.520\n we, for instance, try to use these techniques for safety critical systems like cars and\n\n1:23:26.520 --> 1:23:28.520\n aircraft and so on.\n\n1:23:28.520 --> 1:23:32.360\n I think at some point we'll face the issue of objectives being optimized too well, but\n\n1:23:32.360 --> 1:23:36.240\n right now I think we're, we're more likely to face the issue of them not being optimized\n\n1:23:36.240 --> 1:23:37.240\n well enough.\n\n1:23:37.240 --> 1:23:41.360\n But you don't think unintended consequences can arise even when you're far from optimality,\n\n1:23:41.360 --> 1:23:43.200\n sort of like on the path to it?\n\n1:23:43.200 --> 1:23:46.960\n Oh no, I think unintended consequences can absolutely arise.\n\n1:23:46.960 --> 1:23:52.000\n It's just, I think right now the bottleneck for improving reliability, safety and things\n\n1:23:52.000 --> 1:23:57.400\n like that is more with systems that like need to work better, that need to optimize their\n\n1:23:57.400 --> 1:23:58.400\n objectives better.\n\n1:23:58.400 --> 1:24:05.360\n Do you have thoughts, concerns about existential threats of human level intelligence that have,\n\n1:24:05.360 --> 1:24:11.700\n if we put on our hat of looking in 10, 20, 100, 500 years from now, do you have concerns\n\n1:24:11.700 --> 1:24:15.720\n about existential threats of AI systems?\n\n1:24:15.720 --> 1:24:19.400\n I think there are absolutely existential threats for AI systems, just like there are for any\n\n1:24:19.400 --> 1:24:22.480\n powerful technology.\n\n1:24:22.480 --> 1:24:28.240\n But I think that the, these kinds of problems can take many forms and, and some of those\n\n1:24:28.240 --> 1:24:34.200\n forms will come down to, you know, people with nefarious intent.\n\n1:24:34.200 --> 1:24:38.960\n Some of them will come down to AI systems that have some fatal flaws.\n\n1:24:38.960 --> 1:24:42.380\n And some of them will, will of course come down to AI systems that are too capable in\n\n1:24:42.380 --> 1:24:44.740\n some way.\n\n1:24:44.740 --> 1:24:50.320\n But among this set of potential concerns, I would actually be much more concerned about\n\n1:24:50.320 --> 1:24:55.040\n the first two right now, and principally the one with nefarious humans, because, you know,\n\n1:24:55.040 --> 1:24:57.160\n just through all of human history, actually it's the nefarious humans that have been the\n\n1:24:57.160 --> 1:25:01.680\n problem, not the nefarious machines, than I am about the others.\n\n1:25:01.680 --> 1:25:07.080\n And I think that right now the best that I can do to make sure things go well is to build\n\n1:25:07.080 --> 1:25:13.820\n the best technology I can and also hopefully promote responsible use of that technology.\n\n1:25:13.820 --> 1:25:19.000\n Do you think RL Systems has something to teach us humans?\n\n1:25:19.000 --> 1:25:21.080\n You said nefarious humans getting us in trouble.\n\n1:25:21.080 --> 1:25:26.960\n I mean, machine learning systems have in some ways have revealed to us the ethical flaws\n\n1:25:26.960 --> 1:25:27.960\n in our data.\n\n1:25:27.960 --> 1:25:32.680\n In that same kind of way, can reinforcement learning teach us about ourselves?\n\n1:25:32.680 --> 1:25:34.480\n Has it taught something?\n\n1:25:34.480 --> 1:25:40.600\n What have you learned about yourself from trying to build robots and reinforcement learning\n\n1:25:40.600 --> 1:25:42.920\n systems?\n\n1:25:42.920 --> 1:25:49.960\n I'm not sure what I've learned about myself, but maybe part of the answer to your question\n\n1:25:49.960 --> 1:25:55.180\n might become a little bit more apparent once we see more widespread deployment of reinforcement\n\n1:25:55.180 --> 1:26:02.720\n learning for decision making support in domains like healthcare, education, social media,\n\n1:26:02.720 --> 1:26:03.720\n etc.\n\n1:26:03.720 --> 1:26:06.720\n And I think we will see some interesting stuff emerge there.\n\n1:26:06.720 --> 1:26:12.800\n We will see, for instance, what kind of behaviors these systems come up with in situations where\n\n1:26:12.800 --> 1:26:17.840\n there is interaction with humans and where they have a possibility of influencing human\n\n1:26:17.840 --> 1:26:18.840\n behavior.\n\n1:26:18.840 --> 1:26:22.360\n I think we're not quite there yet, but maybe in the next few years we'll see some interesting\n\n1:26:22.360 --> 1:26:23.800\n stuff come out in that area.\n\n1:26:23.800 --> 1:26:28.880\n I hope outside the research space, because the exciting space where this could be observed\n\n1:26:28.880 --> 1:26:35.200\n is sort of large companies that deal with large data, and I hope there's some transparency.\n\n1:26:35.200 --> 1:26:40.400\n One of the things that's unclear when I look at social networks and just online is why\n\n1:26:40.400 --> 1:26:45.200\n an algorithm did something or whether even an algorithm was involved.\n\n1:26:45.200 --> 1:26:52.080\n And that'd be interesting from a research perspective, just to observe the results of\n\n1:26:52.080 --> 1:26:58.320\n algorithms, to open up that data, or to at least be sufficiently transparent about the\n\n1:26:58.320 --> 1:27:02.280\n behavior of these AI systems in the real world.\n\n1:27:02.280 --> 1:27:03.280\n What's your sense?\n\n1:27:03.280 --> 1:27:08.380\n I don't know if you looked at the blog post, Bitter Lesson, by Rich Sutton, where it looks\n\n1:27:08.380 --> 1:27:16.520\n at sort of the big lesson of researching AI and reinforcement learning is that simple\n\n1:27:16.520 --> 1:27:21.480\n methods, general methods that leverage computation seem to work well.\n\n1:27:21.480 --> 1:27:26.280\n So basically don't try to do any kind of fancy algorithms, just wait for computation to get\n\n1:27:26.280 --> 1:27:28.480\n fast.\n\n1:27:28.480 --> 1:27:31.160\n Do you share this kind of intuition?\n\n1:27:31.160 --> 1:27:34.200\n I think the high level idea makes a lot of sense.\n\n1:27:34.200 --> 1:27:37.480\n I'm not sure that my takeaway would be that we don't need to work on algorithms.\n\n1:27:37.480 --> 1:27:43.800\n I think that my takeaway would be that we should work on general algorithms.\n\n1:27:43.800 --> 1:27:52.360\n And actually, I think that this idea of needing to better automate the acquisition of experience\n\n1:27:52.360 --> 1:27:58.780\n in the real world actually follows pretty naturally from Rich Sutton's conclusion.\n\n1:27:58.780 --> 1:28:06.600\n So if the claim is that automated general methods plus data leads to good results, then\n\n1:28:06.600 --> 1:28:09.760\n it makes sense that we should build general methods and we should build the kind of methods\n\n1:28:09.760 --> 1:28:14.440\n that we can deploy and get them to go out there and collect their experience autonomously.\n\n1:28:14.440 --> 1:28:19.200\n I think that one place where I think that the current state of things falls a little\n\n1:28:19.200 --> 1:28:23.560\n bit short of that is actually the going out there and collecting the data autonomously,\n\n1:28:23.560 --> 1:28:27.440\n which is easy to do in a simulated board game, but very hard to do in the real world.\n\n1:28:27.440 --> 1:28:31.840\n Yeah, it keeps coming back to this one problem, right?\n\n1:28:31.840 --> 1:28:35.800\n Your mind is focused there now in this real world.\n\n1:28:35.800 --> 1:28:43.840\n It just seems scary, the step of collecting the data, and it seems unclear to me how we\n\n1:28:43.840 --> 1:28:44.840\n can do it effectively.\n\n1:28:44.840 --> 1:28:49.360\n Well, you know, seven billion people in the world, each of them had to do that at some\n\n1:28:49.360 --> 1:28:51.040\n point in their lives.\n\n1:28:51.040 --> 1:28:54.860\n And we should leverage that experience that they've all done.\n\n1:28:54.860 --> 1:28:58.440\n We should be able to try to collect that kind of data.\n\n1:28:58.440 --> 1:29:02.760\n Okay, big questions.\n\n1:29:02.760 --> 1:29:10.480\n Maybe stepping back through your life, what book or books, technical or fiction or philosophical,\n\n1:29:10.480 --> 1:29:15.840\n had a big impact on the way you saw the world, on the way you thought about in the world,\n\n1:29:15.840 --> 1:29:19.480\n your life in general?\n\n1:29:19.480 --> 1:29:24.160\n And maybe what books, if it's different, would you recommend people consider reading on their\n\n1:29:24.160 --> 1:29:26.320\n own intellectual journey?\n\n1:29:26.320 --> 1:29:30.280\n It could be within reinforcement learning, but it could be very much bigger.\n\n1:29:30.280 --> 1:29:39.360\n I don't know if this is like a scientifically, like, particularly meaningful answer.\n\n1:29:39.360 --> 1:29:45.800\n But like, the honest answer is that I actually found a lot of the work by Isaac Asimov to\n\n1:29:45.800 --> 1:29:47.720\n be very inspiring when I was younger.\n\n1:29:47.720 --> 1:29:50.840\n I don't know if that has anything to do with AI necessarily.\n\n1:29:50.840 --> 1:29:53.380\n You don't think it had a ripple effect in your life?\n\n1:29:53.380 --> 1:29:56.200\n Maybe it did.\n\n1:29:56.200 --> 1:30:06.800\n But yeah, I think that a vision of a future where, well, first of all, artificial, I might\n\n1:30:06.800 --> 1:30:10.880\n say artificial intelligence system, artificial robotic systems have, you know, kind of a\n\n1:30:10.880 --> 1:30:18.560\n big place, a big role in society, and where we try to imagine the sort of the limiting\n\n1:30:18.560 --> 1:30:25.640\n case of technological advancement and how that might play out in our future history.\n\n1:30:25.640 --> 1:30:30.720\n But yeah, I think that that was in some way influential.\n\n1:30:30.720 --> 1:30:33.720\n I don't really know how.\n\n1:30:33.720 --> 1:30:34.720\n I would recommend it.\n\n1:30:34.720 --> 1:30:37.040\n I mean, if nothing else, you'd be well entertained.\n\n1:30:37.040 --> 1:30:41.840\n When did you first yourself like fall in love with the idea of artificial intelligence,\n\n1:30:41.840 --> 1:30:45.080\n get captivated by this field?\n\n1:30:45.080 --> 1:30:52.280\n So my honest answer here is actually that I only really started to think about it as\n\n1:30:52.280 --> 1:30:56.200\n something that I might want to do actually in graduate school pretty late.\n\n1:30:56.200 --> 1:31:02.400\n And a big part of that was that until, you know, somewhere around 2009, 2010, it just\n\n1:31:02.400 --> 1:31:06.920\n wasn't really high on my priority list because I didn't think that it was something where\n\n1:31:06.920 --> 1:31:11.560\n we're going to see very substantial advances in my lifetime.\n\n1:31:11.560 --> 1:31:18.120\n And you know, maybe in terms of my career, the time when I really decided I wanted to\n\n1:31:18.120 --> 1:31:23.480\n work on this was when I actually took a seminar course that was taught by Professor Andrew\n\n1:31:23.480 --> 1:31:24.480\n Ng.\n\n1:31:24.480 --> 1:31:29.320\n And, you know, at that point, I, of course, had like a decent understanding of the technical\n\n1:31:29.320 --> 1:31:30.320\n things involved.\n\n1:31:30.320 --> 1:31:33.640\n But one of the things that really resonated with me was when he said in the opening lecture\n\n1:31:33.640 --> 1:31:37.140\n something to the effect of like, well, he used to have graduate students come to him\n\n1:31:37.140 --> 1:31:40.920\n and talk about how they want to work on AI, and he would kind of chuckle and give them\n\n1:31:40.920 --> 1:31:42.600\n some math problem to deal with.\n\n1:31:42.600 --> 1:31:45.940\n But now he's actually thinking that this is an area where we might see like substantial\n\n1:31:45.940 --> 1:31:47.840\n advances in our lifetime.\n\n1:31:47.840 --> 1:31:52.280\n And that kind of got me thinking because, you know, in some abstract sense, yeah, like\n\n1:31:52.280 --> 1:31:56.940\n you can kind of imagine that, but in a very real sense, when someone who had been working\n\n1:31:56.940 --> 1:32:02.520\n on that kind of stuff their whole career suddenly says that, yeah, like that had some effect\n\n1:32:02.520 --> 1:32:03.520\n on me.\n\n1:32:03.520 --> 1:32:08.040\n Yeah, this might be a special moment in the history of the field.\n\n1:32:08.040 --> 1:32:14.060\n That this is where we might see some interesting breakthroughs.\n\n1:32:14.060 --> 1:32:19.120\n So in the space of advice, somebody who's interested in getting started in machine learning\n\n1:32:19.120 --> 1:32:23.720\n or reinforcement learning, what advice would you give to maybe an undergraduate student\n\n1:32:23.720 --> 1:32:30.520\n or maybe even younger, how, what are the first steps to take and further on what are the\n\n1:32:30.520 --> 1:32:32.800\n steps to take on that journey?\n\n1:32:32.800 --> 1:32:43.160\n So something that I think is important to do is to not be afraid to like spend time\n\n1:32:43.160 --> 1:32:46.280\n imagining the kind of outcome that you might like to see.\n\n1:32:46.280 --> 1:32:51.480\n So you know, one outcome might be a successful career, a large paycheck or something, or\n\n1:32:51.480 --> 1:32:54.920\n state of the art results on some benchmark, but hopefully that's not the thing that's\n\n1:32:54.920 --> 1:32:57.760\n like the main driving force for somebody.\n\n1:32:57.760 --> 1:33:04.360\n But I think that if someone who is a student considering a career in AI like takes a little\n\n1:33:04.360 --> 1:33:07.420\n while, sits down and thinks like, what do I really want to see?\n\n1:33:07.420 --> 1:33:09.120\n What I want to see a machine do?\n\n1:33:09.120 --> 1:33:10.320\n What do I want to see a robot do?\n\n1:33:10.320 --> 1:33:11.320\n What do I want to do?\n\n1:33:11.320 --> 1:33:15.200\n What do I want to see a natural language system, which is like, imagine, you know, imagine\n\n1:33:15.200 --> 1:33:19.040\n it almost like a commercial for a future product or something or like, like something that\n\n1:33:19.040 --> 1:33:23.520\n you'd like to see in the world and then actually sit down and think about the steps that are\n\n1:33:23.520 --> 1:33:25.160\n necessary to get there.\n\n1:33:25.160 --> 1:33:29.000\n And hopefully that thing is not a better number on image net classification.\n\n1:33:29.000 --> 1:33:32.000\n It's like, it's probably like an actual thing that we can't do today that would be really\n\n1:33:32.000 --> 1:33:33.000\n awesome.\n\n1:33:33.000 --> 1:33:38.280\n Whether it's a robot Butler or a, you know, a really awesome healthcare decision making\n\n1:33:38.280 --> 1:33:41.760\n support system, whatever it is that you find inspiring.\n\n1:33:41.760 --> 1:33:45.240\n And I think that thinking about that and then backtracking from there and imagining the\n\n1:33:45.240 --> 1:33:48.240\n steps needed to get there will actually lead to much better research.\n\n1:33:48.240 --> 1:33:50.480\n It'll lead to rethinking the assumptions.\n\n1:33:50.480 --> 1:33:55.880\n It'll lead to working on the bottlenecks that other people aren't working on.\n\n1:33:55.880 --> 1:34:01.080\n And then naturally to turn to you, we've talked about reward functions and you just give an\n\n1:34:01.080 --> 1:34:05.440\n advice on looking forward, how you'd like to see, what kind of change you would like\n\n1:34:05.440 --> 1:34:06.920\n to make in the world.\n\n1:34:06.920 --> 1:34:11.560\n What do you think, ridiculous, big question, what do you think is the meaning of life?\n\n1:34:11.560 --> 1:34:13.480\n What is the meaning of your life?\n\n1:34:13.480 --> 1:34:20.540\n What gives you fulfillment, purpose, happiness and meaning?\n\n1:34:20.540 --> 1:34:24.600\n That's a very big question.\n\n1:34:24.600 --> 1:34:27.640\n What's the reward function under which you are operating?\n\n1:34:27.640 --> 1:34:28.640\n Yeah.\n\n1:34:28.640 --> 1:34:33.600\n I think one thing that does give, you know, if not meaning, at least satisfaction is some\n\n1:34:33.600 --> 1:34:37.400\n degree of confidence that I'm working on a problem that really matters.\n\n1:34:37.400 --> 1:34:42.960\n I feel like it's less important to me to like actually solve a problem, but it's quite nice\n\n1:34:42.960 --> 1:34:49.400\n to take things to spend my time on that I believe really matter.\n\n1:34:49.400 --> 1:34:53.080\n And I try pretty hard to look for that.\n\n1:34:53.080 --> 1:34:59.160\n I don't know if it's easy to answer this, but if you're successful, what does that look\n\n1:34:59.160 --> 1:35:00.160\n like?\n\n1:35:00.160 --> 1:35:01.880\n What's the big dream?\n\n1:35:01.880 --> 1:35:09.840\n Now, of course, success is built on top of success and you keep going forever, but what\n\n1:35:09.840 --> 1:35:10.840\n is the dream?\n\n1:35:10.840 --> 1:35:11.840\n Yeah.\n\n1:35:11.840 --> 1:35:18.040\n So one very concrete thing or maybe as concrete as it's going to get here is to see machines\n\n1:35:18.040 --> 1:35:23.420\n that actually get better and better the longer they exist in the world.\n\n1:35:23.420 --> 1:35:26.820\n And that kind of seems like on the surface, one might even think that that's something\n\n1:35:26.820 --> 1:35:28.840\n that we have today, but I think we really don't.\n\n1:35:28.840 --> 1:35:38.480\n I think that there is an ending complexity in the universe and to date, all of the machines\n\n1:35:38.480 --> 1:35:44.200\n that we've been able to build don't sort of improve up to the limit of that complexity.\n\n1:35:44.200 --> 1:35:45.660\n They hit a wall somewhere.\n\n1:35:45.660 --> 1:35:50.260\n Maybe they hit a wall because they're in a simulator that has, that is only a very limited,\n\n1:35:50.260 --> 1:35:54.320\n very pale imitation of the real world, or they hit a wall because they rely on a label\n\n1:35:54.320 --> 1:36:00.400\n data set, but they never hit the wall of like running out of stuff to see.\n\n1:36:00.400 --> 1:36:04.920\n So I'd like to build a machine that can go as far as possible.\n\n1:36:04.920 --> 1:36:08.160\n Runs up against the ceiling of the complexity of the universe.\n\n1:36:08.160 --> 1:36:09.160\n Yes.\n\n1:36:09.160 --> 1:36:12.000\n Well, I don't think there's a better way to end it, Sergey.\n\n1:36:12.000 --> 1:36:13.000\n Thank you so much.\n\n1:36:13.000 --> 1:36:14.000\n It's a huge honor.\n\n1:36:14.000 --> 1:36:20.280\n I can't wait to see the amazing work that you have to publish and in education space\n\n1:36:20.280 --> 1:36:21.820\n in terms of reinforcement learning.\n\n1:36:21.820 --> 1:36:23.000\n Thank you for inspiring the world.\n\n1:36:23.000 --> 1:36:24.720\n Thank you for the great research you do.\n\n1:36:24.720 --> 1:36:25.720\n Thank you.\n\n1:36:25.720 --> 1:36:31.000\n Thanks for listening to this conversation with Sergey Levine and thank you to our sponsors,\n\n1:36:31.000 --> 1:36:33.560\n Cash App and ExpressVPN.\n\n1:36:33.560 --> 1:36:40.360\n Please consider supporting this podcast by downloading Cash App and using code LexPodcast\n\n1:36:40.360 --> 1:36:44.840\n and signing up at expressvpn.com slash LexPod.\n\n1:36:44.840 --> 1:36:50.900\n Click all the links, buy all the stuff, it's the best way to support this podcast and the\n\n1:36:50.900 --> 1:36:51.900\n journey I'm on.\n\n1:36:51.900 --> 1:36:57.440\n If you enjoy this thing, subscribe on YouTube, review it with five stars on Apple Podcast,\n\n1:36:57.440 --> 1:37:02.900\n support it on Patreon, or connect with me on Twitter at Lex Friedman, spelled somehow\n\n1:37:02.900 --> 1:37:08.920\n if you can figure out how without using the letter E, just F R I D M A N.\n\n1:37:08.920 --> 1:37:14.120\n And now let me leave you with some words from Salvador Dali.\n\n1:37:14.120 --> 1:37:18.820\n Intelligence without ambition is a bird without wings.\n\n1:37:18.820 --> 1:37:22.000\n Thank you for listening and hope to see you next time.\n\n"
}