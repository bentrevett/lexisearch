{
  "title": "Max Tegmark: AI and Physics | Lex Fridman Podcast #155",
  "id": "RL4j4KPwNGM",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:02.840\n The following is a conversation with Max Tegmark,\n\n00:02.840 --> 00:04.760\n his second time on the podcast.\n\n00:04.760 --> 00:07.120\n In fact, the previous conversation\n\n00:07.120 --> 00:10.960\n was episode number one of this very podcast.\n\n00:10.960 --> 00:14.800\n He is a physicist and artificial intelligence researcher\n\n00:14.800 --> 00:18.840\n at MIT, cofounder of the Future of Life Institute,\n\n00:18.840 --> 00:21.360\n and author of Life 3.0,\n\n00:21.360 --> 00:24.560\n Being Human in the Age of Artificial Intelligence.\n\n00:24.560 --> 00:27.120\n He's also the head of a bunch of other huge,\n\n00:27.120 --> 00:29.240\n fascinating projects and has written\n\n00:29.240 --> 00:30.560\n a lot of different things\n\n00:30.560 --> 00:32.080\n that you should definitely check out.\n\n00:32.080 --> 00:34.480\n He has been one of the key humans\n\n00:34.480 --> 00:37.400\n who has been outspoken about longterm existential risks\n\n00:37.400 --> 00:40.500\n of AI and also its exciting possibilities\n\n00:40.500 --> 00:42.900\n and solutions to real world problems.\n\n00:42.900 --> 00:46.440\n Most recently at the intersection of AI and physics,\n\n00:46.440 --> 00:50.000\n and also in reengineering the algorithms\n\n00:50.000 --> 00:53.200\n that divide us by controlling the information we see\n\n00:53.200 --> 00:56.160\n and thereby creating bubbles and all other kinds\n\n00:56.160 --> 00:59.640\n of complex social phenomena that we see today.\n\n00:59.640 --> 01:01.440\n In general, he's one of the most passionate\n\n01:01.440 --> 01:04.340\n and brilliant people I have the fortune of knowing.\n\n01:04.340 --> 01:06.180\n I hope to talk to him many more times\n\n01:06.180 --> 01:08.280\n on this podcast in the future.\n\n01:08.280 --> 01:10.000\n Quick mention of our sponsors,\n\n01:10.000 --> 01:12.160\n The Jordan Harbinger Show,\n\n01:12.160 --> 01:14.360\n Four Sigmatic Mushroom Coffee,\n\n01:14.360 --> 01:18.480\n BetterHelp Online Therapy, and ExpressVPN.\n\n01:18.480 --> 01:23.560\n So the choices, wisdom, caffeine, sanity, or privacy.\n\n01:23.560 --> 01:25.880\n Choose wisely, my friends, and if you wish,\n\n01:25.880 --> 01:28.360\n click the sponsor links below to get a discount\n\n01:28.360 --> 01:30.560\n and to support this podcast.\n\n01:30.560 --> 01:33.900\n As a side note, let me say that much of the researchers\n\n01:33.900 --> 01:35.400\n in the machine learning\n\n01:35.400 --> 01:37.760\n and artificial intelligence communities\n\n01:37.760 --> 01:40.400\n do not spend much time thinking deeply\n\n01:40.400 --> 01:42.720\n about existential risks of AI.\n\n01:42.720 --> 01:46.160\n Because our current algorithms are seen as useful but dumb,\n\n01:46.160 --> 01:49.240\n it's difficult to imagine how they may become destructive\n\n01:49.240 --> 01:51.240\n to the fabric of human civilization\n\n01:51.240 --> 01:53.040\n in the foreseeable future.\n\n01:53.040 --> 01:56.120\n I understand this mindset, but it's very troublesome.\n\n01:56.120 --> 02:00.480\n To me, this is both a dangerous and uninspiring perspective,\n\n02:00.480 --> 02:03.980\n reminiscent of a lobster sitting in a pot of lukewarm water\n\n02:03.980 --> 02:06.160\n that a minute ago was cold.\n\n02:06.160 --> 02:08.640\n I feel a kinship with this lobster.\n\n02:08.640 --> 02:10.560\n I believe that already the algorithms\n\n02:10.560 --> 02:12.960\n that drive our interaction on social media\n\n02:12.960 --> 02:14.980\n have an intelligence and power\n\n02:14.980 --> 02:17.360\n that far outstrip the intelligence and power\n\n02:17.360 --> 02:19.220\n of any one human being.\n\n02:19.220 --> 02:21.640\n Now really is the time to think about this,\n\n02:21.640 --> 02:24.140\n to define the trajectory of the interplay\n\n02:24.140 --> 02:26.940\n of technology and human beings in our society.\n\n02:26.940 --> 02:29.680\n I think that the future of human civilization\n\n02:29.680 --> 02:32.820\n very well may be at stake over this very question\n\n02:32.820 --> 02:36.240\n of the role of artificial intelligence in our society.\n\n02:36.240 --> 02:38.160\n If you enjoy this thing, subscribe on YouTube,\n\n02:38.160 --> 02:40.960\n review it on Apple Podcasts, follow on Spotify,\n\n02:40.960 --> 02:43.840\n support on Patreon, or connect with me on Twitter\n\n02:43.840 --> 02:45.260\n at Lex Friedman.\n\n02:45.260 --> 02:48.840\n And now, here's my conversation with Max Tegmark.\n\n02:49.880 --> 02:51.440\n So people might not know this,\n\n02:51.440 --> 02:55.280\n but you were actually episode number one of this podcast\n\n02:55.280 --> 02:59.280\n just a couple of years ago, and now we're back.\n\n02:59.280 --> 03:02.280\n And it so happens that a lot of exciting things happened\n\n03:02.280 --> 03:05.600\n in both physics and artificial intelligence,\n\n03:05.600 --> 03:08.480\n both fields that you're super passionate about.\n\n03:08.480 --> 03:11.640\n Can we try to catch up to some of the exciting things\n\n03:11.640 --> 03:14.080\n happening in artificial intelligence,\n\n03:14.080 --> 03:17.340\n especially in the context of the way it's cracking,\n\n03:17.340 --> 03:20.020\n open the different problems of the sciences?\n\n03:20.020 --> 03:24.520\n Yeah, I'd love to, especially now as we start 2021 here,\n\n03:24.520 --> 03:26.280\n it's a really fun time to think about\n\n03:26.280 --> 03:29.560\n what were the biggest breakthroughs in AI,\n\n03:29.560 --> 03:31.800\n not the ones necessarily that media wrote about,\n\n03:31.800 --> 03:35.200\n but that really matter, and what does that mean\n\n03:35.200 --> 03:37.440\n for our ability to do better science?\n\n03:37.440 --> 03:39.920\n What does it mean for our ability\n\n03:39.920 --> 03:43.160\n to help people around the world?\n\n03:43.160 --> 03:46.440\n And what does it mean for new problems\n\n03:46.440 --> 03:48.440\n that they could cause if we're not smart enough\n\n03:48.440 --> 03:51.880\n to avoid them, so what do we learn basically from this?\n\n03:51.880 --> 03:52.720\n Yes, absolutely.\n\n03:52.720 --> 03:54.960\n So one of the amazing things you're a part of\n\n03:54.960 --> 03:57.680\n is the AI Institute for Artificial Intelligence\n\n03:57.680 --> 04:00.160\n and Fundamental Interactions.\n\n04:00.160 --> 04:02.280\n What's up with this institute?\n\n04:02.280 --> 04:03.680\n What are you working on?\n\n04:03.680 --> 04:05.000\n What are you thinking about?\n\n04:05.000 --> 04:09.080\n The idea is something I'm very on fire with,\n\n04:09.080 --> 04:11.920\n which is basically AI meets physics.\n\n04:11.920 --> 04:15.360\n And it's been almost five years now\n\n04:15.360 --> 04:18.360\n since I shifted my own MIT research\n\n04:18.360 --> 04:20.680\n from physics to machine learning.\n\n04:20.680 --> 04:22.880\n And in the beginning, I noticed that a lot of my colleagues,\n\n04:22.880 --> 04:24.280\n even though they were polite about it,\n\n04:24.280 --> 04:27.760\n were like kind of, what is Max doing?\n\n04:27.760 --> 04:29.040\n What is this weird stuff?\n\n04:29.040 --> 04:30.880\n He's lost his mind.\n\n04:30.880 --> 04:35.080\n But then gradually, I, together with some colleagues,\n\n04:35.080 --> 04:40.080\n were able to persuade more and more of the other professors\n\n04:40.080 --> 04:42.520\n in our physics department to get interested in this.\n\n04:42.520 --> 04:46.320\n And now we've got this amazing NSF Center,\n\n04:46.320 --> 04:50.000\n so 20 million bucks for the next five years, MIT,\n\n04:50.000 --> 04:53.200\n and a bunch of neighboring universities here also.\n\n04:53.200 --> 04:55.280\n And I noticed now those colleagues\n\n04:55.280 --> 04:57.040\n who were looking at me funny have stopped\n\n04:57.040 --> 05:00.320\n asking what the point is of this,\n\n05:00.320 --> 05:02.400\n because it's becoming more clear.\n\n05:02.400 --> 05:05.560\n And I really believe that, of course,\n\n05:05.560 --> 05:09.440\n AI can help physics a lot to do better physics.\n\n05:09.440 --> 05:13.160\n But physics can also help AI a lot,\n\n05:13.160 --> 05:16.440\n both by building better hardware.\n\n05:16.440 --> 05:18.840\n My colleague, Marin Soljacic, for example,\n\n05:18.840 --> 05:23.000\n is working on an optical chip for much faster machine\n\n05:23.000 --> 05:25.360\n learning, where the computation is done\n\n05:25.360 --> 05:30.240\n not by moving electrons around, but by moving photons around,\n\n05:30.240 --> 05:34.240\n dramatically less energy use, faster, better.\n\n05:34.240 --> 05:37.560\n We can also help AI a lot, I think,\n\n05:37.560 --> 05:42.840\n by having a different set of tools\n\n05:42.840 --> 05:46.440\n and a different, maybe more audacious attitude.\n\n05:46.440 --> 05:51.560\n AI has, to a significant extent, been an engineering discipline\n\n05:51.560 --> 05:54.240\n where you're just trying to make things that work\n\n05:54.240 --> 05:56.560\n and being more interested in maybe selling them\n\n05:56.560 --> 06:00.280\n than in figuring out exactly how they work\n\n06:00.280 --> 06:03.680\n and proving theorems about that they will always work.\n\n06:03.680 --> 06:05.240\n Contrast that with physics.\n\n06:05.240 --> 06:08.920\n When Elon Musk sends a rocket to the International Space\n\n06:08.920 --> 06:12.080\n Station, they didn't just train with machine learning.\n\n06:12.080 --> 06:14.080\n Oh, let's fire it a little bit more to the left,\n\n06:14.080 --> 06:14.920\n a bit more to the right.\n\n06:14.920 --> 06:15.800\n Oh, that also missed.\n\n06:15.800 --> 06:16.920\n Let's try here.\n\n06:16.920 --> 06:23.400\n No, we figured out Newton's laws of gravitation and other things\n\n06:23.400 --> 06:26.800\n and got a really deep fundamental understanding.\n\n06:26.800 --> 06:30.840\n And that's what gives us such confidence in rockets.\n\n06:30.840 --> 06:36.040\n And my vision is that in the future,\n\n06:36.040 --> 06:38.840\n all machine learning systems that actually have impact\n\n06:38.840 --> 06:40.960\n on people's lives will be understood\n\n06:40.960 --> 06:43.120\n at a really, really deep level.\n\n06:43.120 --> 06:46.960\n So we trust them, not because some sales rep told us to,\n\n06:46.960 --> 06:50.360\n but because they've earned our trust.\n\n06:50.360 --> 06:51.800\n And really safety critical things\n\n06:51.800 --> 06:55.600\n even prove that they will always do what we expect them to do.\n\n06:55.600 --> 06:57.040\n That's very much the physics mindset.\n\n06:57.040 --> 07:00.160\n So it's interesting, if you look at big breakthroughs\n\n07:00.160 --> 07:03.680\n that have happened in machine learning this year,\n\n07:03.680 --> 07:08.200\n from dancing robots, it's pretty fantastic.\n\n07:08.200 --> 07:10.280\n Not just because it's cool, but if you just\n\n07:10.280 --> 07:12.880\n think about not that many years ago,\n\n07:12.880 --> 07:16.680\n this YouTube video at this DARPA challenge with the MIT robot\n\n07:16.680 --> 07:20.680\n comes out of the car and face plants.\n\n07:20.680 --> 07:23.840\n How far we've come in just a few years.\n\n07:23.840 --> 07:30.360\n Similarly, Alpha Fold 2, crushing the protein folding\n\n07:30.360 --> 07:31.160\n problem.\n\n07:31.160 --> 07:33.080\n We can talk more about implications\n\n07:33.080 --> 07:34.400\n for medical research and stuff.\n\n07:34.400 --> 07:39.240\n But hey, that's huge progress.\n\n07:39.240 --> 07:44.120\n You can look at the GPT3 that can spout off\n\n07:44.120 --> 07:48.840\n English text, which sometimes really, really blows you away.\n\n07:48.840 --> 07:52.920\n You can look at DeepMind's MuZero,\n\n07:52.920 --> 07:57.920\n which doesn't just kick our butt in Go and Chess and Shogi,\n\n07:57.920 --> 07:59.760\n but also in all these Atari games.\n\n07:59.760 --> 08:02.960\n And you don't even have to teach it the rules now.\n\n08:02.960 --> 08:06.920\n What all of those have in common is, besides being powerful,\n\n08:06.920 --> 08:10.520\n is we don't fully understand how they work.\n\n08:10.520 --> 08:13.160\n And that's fine if it's just some dancing robots.\n\n08:13.160 --> 08:16.440\n And the worst thing that can happen is they face plant.\n\n08:16.440 --> 08:19.120\n Or if they're playing Go, and the worst thing that can happen\n\n08:19.120 --> 08:22.240\n is that they make a bad move and lose the game.\n\n08:22.240 --> 08:25.400\n It's less fine if that's what's controlling\n\n08:25.400 --> 08:29.120\n your self driving car or your nuclear power plant.\n\n08:29.120 --> 08:33.600\n And we've seen already that even though Hollywood\n\n08:33.600 --> 08:35.040\n had all these movies where they try\n\n08:35.040 --> 08:37.000\n to make us worry about the wrong things,\n\n08:37.000 --> 08:41.080\n like machines turning evil, the actual bad things that\n\n08:41.080 --> 08:43.480\n have happened with automation have not\n\n08:43.480 --> 08:45.440\n been machines turning evil.\n\n08:45.440 --> 08:48.840\n They've been caused by overtrust in things\n\n08:48.840 --> 08:51.440\n we didn't understand as well as we thought we did.\n\n08:51.440 --> 08:54.320\n Even very simple automated systems\n\n08:54.320 --> 09:00.440\n like what Boeing put into the 737 MAX killed a lot of people.\n\n09:00.440 --> 09:02.960\n Was it that that little simple system was evil?\n\n09:02.960 --> 09:03.920\n Of course not.\n\n09:03.920 --> 09:07.400\n But we didn't understand it as well as we should have.\n\n09:07.400 --> 09:10.640\n And we trusted without understanding.\n\n09:10.640 --> 09:11.440\n Exactly.\n\n09:11.440 --> 09:12.440\n That's the overtrust.\n\n09:12.440 --> 09:15.720\n We didn't even understand that we didn't understand.\n\n09:15.720 --> 09:19.880\n The humility is really at the core of being a scientist.\n\n09:19.880 --> 09:21.880\n I think step one, if you want to be a scientist,\n\n09:21.880 --> 09:25.000\n is don't ever fool yourself into thinking you understand things\n\n09:25.000 --> 09:27.080\n when you actually don't.\n\n09:27.080 --> 09:29.480\n That's probably good advice for humans in general.\n\n09:29.480 --> 09:31.320\n I think humility in general can do us good.\n\n09:31.320 --> 09:33.240\n But in science, it's so spectacular.\n\n09:33.240 --> 09:35.880\n Why did we have the wrong theory of gravity\n\n09:35.880 --> 09:40.520\n ever from Aristotle onward until Galileo's time?\n\n09:40.520 --> 09:43.680\n Why would we believe something so dumb as that if I throw\n\n09:43.680 --> 09:47.280\n this water bottle, it's going to go up with constant speed\n\n09:47.280 --> 09:49.680\n until it realizes that its natural motion is down?\n\n09:49.680 --> 09:51.040\n It changes its mind.\n\n09:51.040 --> 09:55.320\n Because people just kind of assumed Aristotle was right.\n\n09:55.320 --> 09:56.120\n He's an authority.\n\n09:56.120 --> 09:57.720\n We understand that.\n\n09:57.720 --> 09:59.920\n Why did we believe things like that the sun is\n\n09:59.920 --> 10:01.880\n going around the Earth?\n\n10:01.880 --> 10:04.000\n Why did we believe that time flows\n\n10:04.000 --> 10:06.440\n at the same rate for everyone until Einstein?\n\n10:06.440 --> 10:08.560\n Same exact mistake over and over again.\n\n10:08.560 --> 10:12.320\n We just weren't humble enough to acknowledge that we actually\n\n10:12.320 --> 10:13.920\n didn't know for sure.\n\n10:13.920 --> 10:15.720\n We assumed we knew.\n\n10:15.720 --> 10:17.760\n So we didn't discover the truth because we\n\n10:17.760 --> 10:20.560\n assumed there was nothing there to be discovered, right?\n\n10:20.560 --> 10:24.400\n There was something to be discovered about the 737 Max.\n\n10:24.400 --> 10:26.480\n And if you had been a bit more suspicious\n\n10:26.480 --> 10:28.680\n and tested it better, we would have found it.\n\n10:28.680 --> 10:30.600\n And it's the same thing with most harm\n\n10:30.600 --> 10:33.760\n that's been done by automation so far, I would say.\n\n10:33.760 --> 10:35.720\n So I don't know if you heard here of a company called\n\n10:35.720 --> 10:38.000\n Knight Capital?\n\n10:38.000 --> 10:38.760\n So good.\n\n10:38.760 --> 10:42.080\n That means you didn't invest in them earlier.\n\n10:42.080 --> 10:45.560\n They deployed this automated trading system,\n\n10:45.560 --> 10:47.000\n all nice and shiny.\n\n10:47.000 --> 10:49.480\n They didn't understand it as well as they thought.\n\n10:49.480 --> 10:51.320\n And it went about losing $10 million\n\n10:51.320 --> 10:55.520\n per minute for 44 minutes straight\n\n10:55.520 --> 10:59.520\n until someone presumably was like, oh, no, shut this up.\n\n10:59.520 --> 11:00.480\n Was it evil?\n\n11:00.480 --> 11:01.040\n No.\n\n11:01.040 --> 11:04.400\n It was, again, misplaced trust, something they didn't fully\n\n11:04.400 --> 11:05.240\n understand, right?\n\n11:05.240 --> 11:09.040\n And there have been so many, even when people\n\n11:09.040 --> 11:12.640\n have been killed by robots, which is quite rare still,\n\n11:12.640 --> 11:15.680\n but in factory accidents, it's in every single case\n\n11:15.680 --> 11:19.080\n been not malice, just that the robot didn't understand\n\n11:19.080 --> 11:24.400\n that a human is different from an auto part or whatever.\n\n11:24.400 --> 11:28.000\n So this is why I think there's so much opportunity\n\n11:28.000 --> 11:32.040\n for a physics approach, where you just aim for a higher\n\n11:32.040 --> 11:33.600\n level of understanding.\n\n11:33.600 --> 11:36.200\n And if you look at all these systems\n\n11:36.200 --> 11:40.680\n that we talked about from reinforcement learning\n\n11:40.680 --> 11:44.240\n systems and dancing robots to all these neural networks\n\n11:44.240 --> 11:49.600\n that power GPT3 and go playing software and stuff,\n\n11:49.600 --> 11:53.480\n they're all basically black boxes,\n\n11:53.480 --> 11:55.920\n not so different from if you teach a human something,\n\n11:55.920 --> 11:58.120\n you have no idea how their brain works, right?\n\n11:58.120 --> 11:59.960\n Except the human brain, at least,\n\n11:59.960 --> 12:03.800\n has been error corrected during many, many centuries\n\n12:03.800 --> 12:06.560\n of evolution in a way that some of these systems have not,\n\n12:06.560 --> 12:07.560\n right?\n\n12:07.560 --> 12:10.640\n And my MIT research is entirely focused\n\n12:10.640 --> 12:14.440\n on demystifying this black box, intelligible intelligence\n\n12:14.440 --> 12:15.960\n is my slogan.\n\n12:15.960 --> 12:18.440\n That's a good line, intelligible intelligence.\n\n12:18.440 --> 12:20.360\n Yeah, that we shouldn't settle for something\n\n12:20.360 --> 12:22.160\n that seems intelligent, but it should\n\n12:22.160 --> 12:24.280\n be intelligible so that we actually trust it\n\n12:24.280 --> 12:26.640\n because we understand it, right?\n\n12:26.640 --> 12:28.880\n Like, again, Elon trusts his rockets\n\n12:28.880 --> 12:31.600\n because he understands Newton's laws and thrust\n\n12:31.600 --> 12:33.800\n and how everything works.\n\n12:33.800 --> 12:36.880\n And can I tell you why I'm optimistic about this?\n\n12:36.880 --> 12:37.520\n Yes.\n\n12:37.520 --> 12:41.280\n I think we've made a bit of a mistake\n\n12:41.280 --> 12:44.880\n where some people still think that somehow we're never going\n\n12:44.880 --> 12:47.320\n to understand neural networks.\n\n12:47.320 --> 12:49.520\n We're just going to have to learn to live with this.\n\n12:49.520 --> 12:52.240\n It's this very powerful black box.\n\n12:52.240 --> 12:55.840\n Basically, for those who haven't spent time\n\n12:55.840 --> 12:59.000\n building their own, it's super simple what happens inside.\n\n12:59.000 --> 13:01.280\n You send in a long list of numbers,\n\n13:01.280 --> 13:04.880\n and then you do a bunch of operations on them,\n\n13:04.880 --> 13:06.880\n multiply by matrices, et cetera, et cetera,\n\n13:06.880 --> 13:09.840\n and some other numbers come out that's output of it.\n\n13:09.840 --> 13:13.520\n And then there are a bunch of knobs you can tune.\n\n13:13.520 --> 13:16.680\n And when you change them, it affects the computation,\n\n13:16.680 --> 13:18.080\n the input output relation.\n\n13:18.080 --> 13:19.560\n And then you just give the computer\n\n13:19.560 --> 13:22.680\n some definition of good, and it keeps optimizing these knobs\n\n13:22.680 --> 13:24.760\n until it performs as good as possible.\n\n13:24.760 --> 13:27.160\n And often, you go like, wow, that's really good.\n\n13:27.160 --> 13:29.480\n This robot can dance, or this machine\n\n13:29.480 --> 13:31.960\n is beating me at chess now.\n\n13:31.960 --> 13:33.400\n And in the end, you have something\n\n13:33.400 --> 13:35.240\n which, even though you can look inside it,\n\n13:35.240 --> 13:38.680\n you have very little idea of how it works.\n\n13:38.680 --> 13:42.040\n You can print out tables of all the millions of parameters\n\n13:42.040 --> 13:43.240\n in there.\n\n13:43.240 --> 13:45.000\n Is it crystal clear now how it's working?\n\n13:45.000 --> 13:46.840\n No, of course not.\n\n13:46.840 --> 13:49.080\n Many of my colleagues seem willing to settle for that.\n\n13:49.080 --> 13:51.560\n And I'm like, no, that's like the halfway point.\n\n13:54.360 --> 13:57.560\n Some have even gone as far as sort of guessing\n\n13:57.560 --> 14:00.800\n that the mistrutability of this is\n\n14:00.800 --> 14:02.760\n where some of the power comes from,\n\n14:02.760 --> 14:05.120\n and some sort of mysticism.\n\n14:05.120 --> 14:06.840\n I think that's total nonsense.\n\n14:06.840 --> 14:10.240\n I think the real power of neural networks\n\n14:10.240 --> 14:15.040\n comes not from inscrutability, but from differentiability.\n\n14:15.040 --> 14:17.640\n And what I mean by that is simply\n\n14:17.640 --> 14:23.880\n that the output changes only smoothly if you tweak your knobs.\n\n14:23.880 --> 14:26.640\n And then you can use all these powerful methods\n\n14:26.640 --> 14:28.320\n we have for optimization in science.\n\n14:28.320 --> 14:30.160\n We can just tweak them a little bit and see,\n\n14:30.160 --> 14:31.680\n did that get better or worse?\n\n14:31.680 --> 14:33.920\n That's the fundamental idea of machine learning,\n\n14:33.920 --> 14:36.080\n that the machine itself can keep optimizing\n\n14:36.080 --> 14:37.240\n until it gets better.\n\n14:37.240 --> 14:41.920\n Suppose you wrote this algorithm instead in Python\n\n14:41.920 --> 14:43.720\n or some other programming language,\n\n14:43.720 --> 14:46.280\n and then what the knobs did was they just changed\n\n14:46.280 --> 14:49.920\n random letters in your code.\n\n14:49.920 --> 14:51.440\n Now it would just epically fail.\n\n14:51.440 --> 14:53.560\n You change one thing, and instead of saying print,\n\n14:53.560 --> 14:56.840\n it says, synth, syntax error.\n\n14:56.840 --> 14:58.720\n You don't even know, was that for the better\n\n14:58.720 --> 14:59.920\n or for the worse, right?\n\n14:59.920 --> 15:02.720\n This, to me, is what I believe is\n\n15:02.720 --> 15:05.240\n the fundamental power of neural networks.\n\n15:05.240 --> 15:06.640\n And just to clarify, the changing\n\n15:06.640 --> 15:08.400\n of the different letters in a program\n\n15:08.400 --> 15:10.600\n would not be a differentiable process.\n\n15:10.600 --> 15:13.760\n It would make it an invalid program, typically.\n\n15:13.760 --> 15:16.800\n And then you wouldn't even know if you changed more letters\n\n15:16.800 --> 15:18.560\n if it would make it work again, right?\n\n15:18.560 --> 15:23.360\n So that's the magic of neural networks, the inscrutability.\n\n15:23.360 --> 15:26.600\n The differentiability, that every setting of the parameters\n\n15:26.600 --> 15:29.040\n is a program, and you can tell is it better or worse, right?\n\n15:29.040 --> 15:31.040\n And so.\n\n15:31.040 --> 15:33.680\n So you don't like the poetry of the mystery of neural networks\n\n15:33.680 --> 15:35.120\n as the source of its power?\n\n15:35.120 --> 15:37.880\n I generally like poetry, but.\n\n15:37.880 --> 15:39.200\n Not in this case.\n\n15:39.200 --> 15:40.440\n It's so misleading.\n\n15:40.440 --> 15:42.880\n And above all, it shortchanges us.\n\n15:42.880 --> 15:46.440\n It makes us underestimate the good things\n\n15:46.440 --> 15:47.880\n we can accomplish.\n\n15:47.880 --> 15:49.400\n So what we've been doing in my group\n\n15:49.400 --> 15:53.000\n is basically step one, train the mysterious neural network\n\n15:53.000 --> 15:54.920\n to do something well.\n\n15:54.920 --> 15:59.560\n And then step two, do some additional AI techniques\n\n15:59.560 --> 16:03.280\n to see if we can now transform this black box into something\n\n16:03.280 --> 16:07.120\n equally intelligent that you can actually understand.\n\n16:07.120 --> 16:09.800\n So for example, I'll give you one example, this AI Feynman\n\n16:09.800 --> 16:11.560\n project that we just published, right?\n\n16:11.560 --> 16:18.080\n So we took the 100 most famous or complicated equations\n\n16:18.080 --> 16:20.880\n from one of my favorite physics textbooks,\n\n16:20.880 --> 16:22.560\n in fact, the one that got me into physics\n\n16:22.560 --> 16:25.760\n in the first place, the Feynman lectures on physics.\n\n16:25.760 --> 16:28.520\n And so you have a formula.\n\n16:28.520 --> 16:31.680\n Maybe it has what goes into the formula\n\n16:31.680 --> 16:35.960\n as six different variables, and then what comes out as one.\n\n16:35.960 --> 16:38.000\n So then you can make a giant Excel spreadsheet\n\n16:38.000 --> 16:39.600\n with seven columns.\n\n16:39.600 --> 16:41.680\n You put in just random numbers for the six columns\n\n16:41.680 --> 16:43.880\n for those six input variables, and then you\n\n16:43.880 --> 16:46.880\n calculate with a formula the seventh column, the output.\n\n16:46.880 --> 16:50.440\n So maybe it's like the force equals in the last column\n\n16:50.440 --> 16:51.720\n some function of the other.\n\n16:51.720 --> 16:53.840\n And now the task is, OK, if I don't tell you\n\n16:53.840 --> 16:57.320\n what the formula was, can you figure that out\n\n16:57.320 --> 17:00.080\n from looking at my spreadsheet I gave you?\n\n17:00.080 --> 17:04.400\n This problem is called symbolic regression.\n\n17:04.400 --> 17:05.800\n If I tell you that the formula is\n\n17:05.800 --> 17:08.160\n what we call a linear formula, so it's just\n\n17:08.160 --> 17:14.760\n that the output is sum of all the things, input, the times,\n\n17:14.760 --> 17:17.440\n some constants, that's the famous easy problem\n\n17:17.440 --> 17:18.680\n we can solve.\n\n17:18.680 --> 17:21.360\n We do it all the time in science and engineering.\n\n17:21.360 --> 17:24.480\n But the general one, if it's more complicated functions\n\n17:24.480 --> 17:27.920\n with logarithms or cosines or other math,\n\n17:27.920 --> 17:30.560\n it's a very, very hard one and probably impossible\n\n17:30.560 --> 17:34.560\n to do fast in general, just because the number of formulas\n\n17:34.560 --> 17:37.360\n with n symbols just grows exponentially,\n\n17:37.360 --> 17:38.760\n just like the number of passwords\n\n17:38.760 --> 17:43.320\n you can make grow dramatically with length.\n\n17:43.320 --> 17:46.160\n But we had this idea that if you first\n\n17:46.160 --> 17:48.480\n have a neural network that can actually approximate\n\n17:48.480 --> 17:49.880\n the formula, you just trained it,\n\n17:49.880 --> 17:51.960\n even if you don't understand how it works,\n\n17:51.960 --> 17:56.560\n that can be the first step towards actually understanding\n\n17:56.560 --> 17:58.280\n how it works.\n\n17:58.280 --> 18:00.600\n So that's what we do first.\n\n18:00.600 --> 18:03.240\n And then we study that neural network now\n\n18:03.240 --> 18:04.880\n and put in all sorts of other data\n\n18:04.880 --> 18:06.720\n that wasn't in the original training data\n\n18:06.720 --> 18:09.400\n and use that to discover simplifying\n\n18:09.400 --> 18:11.460\n properties of the formula.\n\n18:11.460 --> 18:13.160\n And that lets us break it apart, often\n\n18:13.160 --> 18:15.560\n into many simpler pieces in a kind of divide\n\n18:15.560 --> 18:17.480\n and conquer approach.\n\n18:17.480 --> 18:20.120\n So we were able to solve all of those 100 formulas,\n\n18:20.120 --> 18:22.160\n discover them automatically, plus a whole bunch\n\n18:22.160 --> 18:22.720\n of other ones.\n\n18:22.720 --> 18:26.320\n And it's actually kind of humbling\n\n18:26.320 --> 18:29.480\n to see that this code, which anyone who wants now\n\n18:29.480 --> 18:33.200\n is listening to this, can type pip install AI Feynman\n\n18:33.200 --> 18:34.560\n on the computer and run it.\n\n18:34.560 --> 18:38.360\n It can actually do what Johannes Kepler spent four years doing\n\n18:38.360 --> 18:40.800\n when he stared at Mars data until he was like,\n\n18:40.800 --> 18:44.520\n finally, Eureka, this is an ellipse.\n\n18:44.520 --> 18:46.960\n This will do it automatically for you in one hour.\n\n18:46.960 --> 18:51.600\n Or Max Planck, he was looking at how much radiation comes out\n\n18:51.600 --> 18:54.160\n from different wavelengths from a hot object\n\n18:54.160 --> 18:57.400\n and discovered the famous blackbody formula.\n\n18:57.400 --> 19:00.400\n This discovers it automatically.\n\n19:00.400 --> 19:05.120\n I'm actually excited about seeing\n\n19:05.120 --> 19:08.640\n if we can discover not just old formulas again,\n\n19:08.640 --> 19:12.000\n but new formulas that no one has seen before.\n\n19:12.000 --> 19:14.680\n I do like this process of using kind of a neural network\n\n19:14.680 --> 19:18.440\n to find some basic insights and then dissecting\n\n19:18.440 --> 19:21.680\n the neural network to then gain the final.\n\n19:21.680 --> 19:30.680\n So in that way, you've forcing the explainability issue,\n\n19:30.680 --> 19:34.880\n really trying to analyze the neural network for the things\n\n19:34.880 --> 19:38.360\n it knows in order to come up with the final beautiful,\n\n19:38.360 --> 19:42.240\n simple theory underlying the initial system\n\n19:42.240 --> 19:43.080\n that you were looking at.\n\n19:43.080 --> 19:44.280\n I love that.\n\n19:44.280 --> 19:47.440\n And the reason I'm so optimistic that it\n\n19:47.440 --> 19:49.040\n can be generalized to so much more\n\n19:49.040 --> 19:53.480\n is because that's exactly what we do as human scientists.\n\n19:53.480 --> 19:55.680\n Think of Galileo, whom we mentioned, right?\n\n19:55.680 --> 19:58.760\n I bet when he was a little kid, if his dad threw him an apple,\n\n19:58.760 --> 20:01.080\n he would catch it.\n\n20:01.080 --> 20:01.560\n Why?\n\n20:01.560 --> 20:04.480\n Because he had a neural network in his brain\n\n20:04.480 --> 20:07.960\n that he had trained to predict the parabolic orbit of apples\n\n20:07.960 --> 20:09.960\n that are thrown under gravity.\n\n20:09.960 --> 20:12.000\n If you throw a tennis ball to a dog,\n\n20:12.000 --> 20:15.360\n it also has this same ability of deep learning\n\n20:15.360 --> 20:18.160\n to figure out how the ball is going to move and catch it.\n\n20:18.160 --> 20:21.960\n But Galileo went one step further when he got older.\n\n20:21.960 --> 20:26.040\n He went back and was like, wait a minute.\n\n20:26.040 --> 20:27.880\n I can write down a formula for this.\n\n20:27.880 --> 20:31.560\n Y equals x squared, a parabola.\n\n20:31.560 --> 20:36.520\n And he helped revolutionize physics as we know it, right?\n\n20:36.520 --> 20:38.200\n So there was a basic neural network\n\n20:38.200 --> 20:43.360\n in there from childhood that captured the experiences\n\n20:43.360 --> 20:46.440\n of observing different kinds of trajectories.\n\n20:46.440 --> 20:48.240\n And then he was able to go back in\n\n20:48.240 --> 20:51.000\n with another extra little neural network\n\n20:51.000 --> 20:53.480\n and analyze all those experiences and be like,\n\n20:53.480 --> 20:54.600\n wait a minute.\n\n20:54.600 --> 20:56.240\n There's a deeper rule here.\n\n20:56.240 --> 20:56.960\n Exactly.\n\n20:56.960 --> 21:00.720\n He was able to distill out in symbolic form\n\n21:00.720 --> 21:03.960\n what that complicated black box neural network was doing.\n\n21:03.960 --> 21:07.320\n Not only did the formula he got ultimately\n\n21:07.320 --> 21:09.840\n become more accurate, and similarly, this\n\n21:09.840 --> 21:12.000\n is how Newton got Newton's laws, which\n\n21:12.000 --> 21:15.600\n is why Elon can send rockets to the space station now, right?\n\n21:15.600 --> 21:19.480\n So it's not only more accurate, but it's also simpler,\n\n21:19.480 --> 21:20.120\n much simpler.\n\n21:20.120 --> 21:22.320\n And it's so simple that we can actually describe it\n\n21:22.320 --> 21:26.080\n to our friends and each other, right?\n\n21:26.080 --> 21:28.800\n We've talked about it just in the context of physics now.\n\n21:28.800 --> 21:31.560\n But hey, isn't this what we're doing when we're\n\n21:31.560 --> 21:33.360\n talking to each other also?\n\n21:33.360 --> 21:35.440\n We go around with our neural networks,\n\n21:35.440 --> 21:38.760\n just like dogs and cats and chipmunks and Blue Jays.\n\n21:38.760 --> 21:41.920\n And we experience things in the world.\n\n21:41.920 --> 21:43.840\n But then we humans do this additional step\n\n21:43.840 --> 21:46.720\n on top of that, where we then distill out\n\n21:46.720 --> 21:50.280\n certain high level knowledge that we've extracted from this\n\n21:50.280 --> 21:52.240\n in a way that we can communicate it\n\n21:52.240 --> 21:56.600\n to each other in a symbolic form in English in this case, right?\n\n21:56.600 --> 21:59.960\n So if we can do it and we believe\n\n21:59.960 --> 22:02.880\n that we are information processing entities,\n\n22:02.880 --> 22:04.960\n then we should be able to make machine learning that\n\n22:04.960 --> 22:07.160\n does it also.\n\n22:07.160 --> 22:10.200\n Well, do you think the entire thing could be learning?\n\n22:10.200 --> 22:14.160\n Because this dissection process, like for AI Feynman,\n\n22:14.160 --> 22:19.240\n the secondary stage feels like something like reasoning.\n\n22:19.240 --> 22:23.400\n And the initial step feels more like the more basic kind\n\n22:23.400 --> 22:25.280\n of differentiable learning.\n\n22:25.280 --> 22:27.680\n Do you think the whole thing could be differentiable\n\n22:27.680 --> 22:28.720\n learning?\n\n22:28.720 --> 22:31.120\n Do you think the whole thing could be basically neural\n\n22:31.120 --> 22:32.320\n networks on top of each other?\n\n22:32.320 --> 22:33.800\n It's like turtles all the way down.\n\n22:33.800 --> 22:35.920\n Could it be neural networks all the way down?\n\n22:35.920 --> 22:37.920\n I mean, that's a really interesting question.\n\n22:37.920 --> 22:41.040\n We know that in your case, it is neural networks all the way\n\n22:41.040 --> 22:42.960\n down because that's all you have in your skull\n\n22:42.960 --> 22:45.880\n is a bunch of neurons doing their thing, right?\n\n22:45.880 --> 22:50.320\n But if you ask the question more generally,\n\n22:50.320 --> 22:54.120\n what algorithms are being used in your brain,\n\n22:54.120 --> 22:56.160\n I think it's super interesting to compare.\n\n22:56.160 --> 22:58.760\n I think we've gone a little bit backwards historically\n\n22:58.760 --> 23:02.800\n because we humans first discovered good old fashioned\n\n23:02.800 --> 23:06.880\n AI, the logic based AI that we often call GoFi\n\n23:06.880 --> 23:09.080\n for good old fashioned AI.\n\n23:09.080 --> 23:12.600\n And then more recently, we did machine learning\n\n23:12.600 --> 23:14.160\n because it required bigger computers.\n\n23:14.160 --> 23:15.960\n So we had to discover it later.\n\n23:15.960 --> 23:19.160\n So we think of machine learning with neural networks\n\n23:19.160 --> 23:21.840\n as the modern thing and the logic based AI\n\n23:21.840 --> 23:24.280\n as the old fashioned thing.\n\n23:24.280 --> 23:27.800\n But if you look at evolution on Earth,\n\n23:27.800 --> 23:29.800\n it's actually been the other way around.\n\n23:29.800 --> 23:34.120\n I would say that, for example, an eagle\n\n23:34.120 --> 23:38.680\n has a better vision system than I have using.\n\n23:38.680 --> 23:42.360\n And dogs are just as good at casting tennis balls as I am.\n\n23:42.360 --> 23:45.920\n All this stuff which is done by training in neural network\n\n23:45.920 --> 23:49.920\n and not interpreting it in words is\n\n23:49.920 --> 23:51.880\n something so many of our animal friends can do,\n\n23:51.880 --> 23:53.680\n at least as well as us, right?\n\n23:53.680 --> 23:56.560\n What is it that we humans can do that the chipmunks\n\n23:56.560 --> 23:58.880\n and the eagles cannot?\n\n23:58.880 --> 24:01.600\n It's more to do with this logic based stuff, right,\n\n24:01.600 --> 24:04.840\n where we can extract out information\n\n24:04.840 --> 24:10.240\n in symbols, in language, and now even with equations\n\n24:10.240 --> 24:12.160\n if you're a scientist, right?\n\n24:12.160 --> 24:13.920\n So basically what happened was first we\n\n24:13.920 --> 24:16.880\n built these computers that could multiply numbers real fast\n\n24:16.880 --> 24:18.080\n and manipulate symbols.\n\n24:18.080 --> 24:20.520\n And we felt they were pretty dumb.\n\n24:20.520 --> 24:22.800\n And then we made neural networks that\n\n24:22.800 --> 24:25.280\n can see as well as a cat can and do\n\n24:25.280 --> 24:30.040\n a lot of this inscrutable black box neural networks.\n\n24:30.040 --> 24:33.000\n What we humans can do also is put the two together\n\n24:33.000 --> 24:34.040\n in a useful way.\n\n24:34.040 --> 24:36.120\n Yes, in our own brain.\n\n24:36.120 --> 24:37.360\n Yes, in our own brain.\n\n24:37.360 --> 24:40.920\n So if we ever want to get artificial general intelligence\n\n24:40.920 --> 24:45.160\n that can do all jobs as well as humans can, right,\n\n24:45.160 --> 24:47.040\n then that's what's going to be required\n\n24:47.040 --> 24:53.120\n to be able to combine the neural networks with symbolic,\n\n24:53.120 --> 24:55.200\n combine the old AI with the new AI in a good way.\n\n24:55.200 --> 24:57.200\n We do it in our brains.\n\n24:57.200 --> 24:59.760\n And there seems to be basically two strategies\n\n24:59.760 --> 25:01.000\n I see in industry now.\n\n25:01.000 --> 25:03.600\n One scares the heebie jeebies out of me,\n\n25:03.600 --> 25:05.840\n and the other one I find much more encouraging.\n\n25:05.840 --> 25:07.080\n OK, which one?\n\n25:07.080 --> 25:08.320\n Can we break them apart?\n\n25:08.320 --> 25:09.600\n Which of the two?\n\n25:09.600 --> 25:11.640\n The one that scares the heebie jeebies out of me\n\n25:11.640 --> 25:12.880\n is this attitude that we're just going\n\n25:12.880 --> 25:14.720\n to make ever bigger systems that we still\n\n25:14.720 --> 25:19.280\n don't understand until they can be as smart as humans.\n\n25:19.280 --> 25:22.200\n What could possibly go wrong?\n\n25:22.200 --> 25:24.200\n I think it's just such a reckless thing to do.\n\n25:24.200 --> 25:27.000\n And unfortunately, if we actually\n\n25:27.000 --> 25:30.120\n succeed as a species to build artificial general intelligence,\n\n25:30.120 --> 25:31.840\n then we still have no clue how it works.\n\n25:31.840 --> 25:35.440\n I think at least 50% chance we're\n\n25:35.440 --> 25:37.040\n going to be extinct before too long.\n\n25:37.040 --> 25:40.480\n It's just going to be an utter epic own goal.\n\n25:40.480 --> 25:46.600\n So it's that 44 minute losing money problem or the paper clip\n\n25:46.600 --> 25:49.480\n problem where we don't understand how it works,\n\n25:49.480 --> 25:51.280\n and it just in a matter of seconds\n\n25:51.280 --> 25:52.760\n runs away in some kind of direction\n\n25:52.760 --> 25:54.440\n that's going to be very problematic.\n\n25:54.440 --> 25:57.640\n Even long before, you have to worry about the machines\n\n25:57.640 --> 26:01.400\n themselves somehow deciding to do things.\n\n26:01.400 --> 26:06.840\n And to us, we have to worry about people using machines\n\n26:06.840 --> 26:09.840\n that are short of AGI and power to do bad things.\n\n26:09.840 --> 26:13.080\n I mean, just take a moment.\n\n26:13.080 --> 26:18.040\n And if anyone is not worried particularly about advanced AI,\n\n26:18.040 --> 26:20.800\n just take 10 seconds and just think\n\n26:20.800 --> 26:23.720\n about your least favorite leader on the planet right now.\n\n26:23.720 --> 26:25.120\n Don't tell me who it is.\n\n26:25.120 --> 26:26.760\n I want to keep this apolitical.\n\n26:26.760 --> 26:28.840\n But just see the face in front of you,\n\n26:28.840 --> 26:30.480\n that person, for 10 seconds.\n\n26:30.480 --> 26:35.280\n Now imagine that that person has this incredibly powerful AI\n\n26:35.280 --> 26:37.120\n under their control and can use it\n\n26:37.120 --> 26:38.760\n to impose their will on the whole planet.\n\n26:38.760 --> 26:39.960\n How does that make you feel?\n\n26:42.840 --> 26:44.280\n Yeah.\n\n26:44.280 --> 26:49.480\n So can we break that apart just briefly?\n\n26:49.480 --> 26:51.720\n For the 50% chance that we'll run\n\n26:51.720 --> 26:53.880\n to trouble with this approach, do you\n\n26:53.880 --> 26:58.040\n see the bigger worry in that leader or humans\n\n26:58.040 --> 27:00.600\n using the system to do damage?\n\n27:00.600 --> 27:05.360\n Or are you more worried, and I think I'm in this camp,\n\n27:05.360 --> 27:09.800\n more worried about accidental, unintentional destruction\n\n27:09.800 --> 27:10.840\n of everything?\n\n27:10.840 --> 27:14.960\n So humans trying to do good, and in a way\n\n27:14.960 --> 27:17.400\n where everyone agrees it's kind of good,\n\n27:17.400 --> 27:20.040\n it's just they're trying to do good without understanding.\n\n27:20.040 --> 27:22.480\n Because I think every evil leader in history\n\n27:22.480 --> 27:24.560\n thought they're, to some degree, thought\n\n27:24.560 --> 27:25.600\n they're trying to do good.\n\n27:25.600 --> 27:25.880\n Oh, yeah.\n\n27:25.880 --> 27:28.080\n I'm sure Hitler thought he was doing good.\n\n27:28.080 --> 27:29.480\n Yeah.\n\n27:29.480 --> 27:31.120\n I've been reading a lot about Stalin.\n\n27:31.120 --> 27:34.240\n I'm sure Stalin is from, he legitimately\n\n27:34.240 --> 27:36.560\n thought that communism was good for the world,\n\n27:36.560 --> 27:37.760\n and that he was doing good.\n\n27:37.760 --> 27:39.960\n I think Mao Zedong thought what he was doing with the Great\n\n27:39.960 --> 27:41.200\n Leap Forward was good too.\n\n27:41.200 --> 27:42.880\n Yeah.\n\n27:42.880 --> 27:45.560\n I'm actually concerned about both of those.\n\n27:45.560 --> 27:48.440\n Before, I promised to answer this in detail,\n\n27:48.440 --> 27:50.320\n but before we do that, let me finish\n\n27:50.320 --> 27:51.240\n answering the first question.\n\n27:51.240 --> 27:53.520\n Because I told you that there were two different routes we\n\n27:53.520 --> 27:55.400\n could get to artificial general intelligence,\n\n27:55.400 --> 27:57.240\n and one scares the hell out of me,\n\n27:57.240 --> 27:59.320\n which is this one where we build something,\n\n27:59.320 --> 28:02.040\n we just say bigger neural networks, ever more hardware,\n\n28:02.040 --> 28:03.760\n and just train the heck out of more data,\n\n28:03.760 --> 28:07.240\n and poof, now it's very powerful.\n\n28:07.240 --> 28:11.800\n That, I think, is the most unsafe and reckless approach.\n\n28:11.800 --> 28:16.480\n The alternative to that is the intelligible intelligence\n\n28:16.480 --> 28:22.840\n approach instead, where we say neural networks is just\n\n28:22.840 --> 28:27.000\n a tool for the first step to get the intuition,\n\n28:27.000 --> 28:29.120\n but then we're going to spend also\n\n28:29.120 --> 28:33.280\n serious resources on other AI techniques\n\n28:33.280 --> 28:35.960\n for demystifying this black box and figuring out\n\n28:35.960 --> 28:38.680\n what it's actually doing so we can convert it\n\n28:38.680 --> 28:41.040\n into something that's equally intelligent,\n\n28:41.040 --> 28:44.040\n but that we actually understand what it's doing.\n\n28:44.040 --> 28:45.960\n Maybe we can even prove theorems about it,\n\n28:45.960 --> 28:50.120\n that this car here will never be hacked when it's driving,\n\n28:50.120 --> 28:53.800\n because here is the proof.\n\n28:53.800 --> 28:55.160\n There is a whole science of this.\n\n28:55.160 --> 28:57.040\n It doesn't work for neural networks\n\n28:57.040 --> 28:58.800\n that are big black boxes, but it works well\n\n28:58.800 --> 29:02.880\n and works with certain other kinds of codes, right?\n\n29:02.880 --> 29:05.160\n That approach, I think, is much more promising.\n\n29:05.160 --> 29:07.160\n That's exactly why I'm working on it, frankly,\n\n29:07.160 --> 29:09.400\n not just because I think it's cool for science,\n\n29:09.400 --> 29:14.080\n but because I think the more we understand these systems,\n\n29:14.080 --> 29:16.160\n the better the chances that we can\n\n29:16.160 --> 29:18.400\n make them do the things that are good for us\n\n29:18.400 --> 29:21.520\n that are actually intended, not unintended.\n\n29:21.520 --> 29:24.280\n So you think it's possible to prove things\n\n29:24.280 --> 29:27.360\n about something as complicated as a neural network?\n\n29:27.360 --> 29:28.440\n That's the hope?\n\n29:28.440 --> 29:30.840\n Well, ideally, there's no reason it\n\n29:30.840 --> 29:34.320\n has to be a neural network in the end either, right?\n\n29:34.320 --> 29:36.480\n We discovered Newton's laws of gravity\n\n29:36.480 --> 29:40.040\n with neural network in Newton's head.\n\n29:40.040 --> 29:44.080\n But that's not the way it's programmed into the navigation\n\n29:44.080 --> 29:46.600\n system of Elon Musk's rocket anymore.\n\n29:46.600 --> 29:49.200\n It's written in C++, or I don't know\n\n29:49.200 --> 29:51.360\n what language he uses exactly.\n\n29:51.360 --> 29:53.400\n And then there are software tools called symbolic\n\n29:53.400 --> 29:54.640\n verification.\n\n29:54.640 --> 29:59.080\n DARPA and the US military has done a lot of really great\n\n29:59.080 --> 30:01.160\n research on this, because they really\n\n30:01.160 --> 30:03.760\n want to understand that when they build weapon systems,\n\n30:03.760 --> 30:07.480\n they don't just go fire at random or malfunction, right?\n\n30:07.480 --> 30:10.720\n And there is even a whole operating system\n\n30:10.720 --> 30:12.920\n called Cell 3 that's been developed by a DARPA grant,\n\n30:12.920 --> 30:16.160\n where you can actually mathematically prove\n\n30:16.160 --> 30:18.800\n that this thing can never be hacked.\n\n30:18.800 --> 30:20.360\n Wow.\n\n30:20.360 --> 30:22.280\n One day, I hope that will be something\n\n30:22.280 --> 30:25.120\n you can say about the OS that's running on our laptops too.\n\n30:25.120 --> 30:27.040\n As you know, we're not there.\n\n30:27.040 --> 30:30.080\n But I think we should be ambitious, frankly.\n\n30:30.080 --> 30:34.120\n And if we can use machine learning\n\n30:34.120 --> 30:36.320\n to help do the proofs and so on as well,\n\n30:36.320 --> 30:40.040\n then it's much easier to verify that a proof is correct\n\n30:40.040 --> 30:42.960\n than to come up with a proof in the first place.\n\n30:42.960 --> 30:45.000\n That's really the core idea here.\n\n30:45.000 --> 30:47.480\n If someone comes on your podcast and says\n\n30:47.480 --> 30:49.760\n they proved the Riemann hypothesis\n\n30:49.760 --> 30:55.480\n or some sensational new theorem, it's\n\n30:55.480 --> 30:58.640\n much easier for someone else, take some smart grad,\n\n30:58.640 --> 31:01.000\n math grad students to check, oh, there's an error here\n\n31:01.000 --> 31:04.000\n on equation five, or this really checks out,\n\n31:04.000 --> 31:07.080\n than it was to discover the proof.\n\n31:07.080 --> 31:09.000\n Yeah, although some of those proofs are pretty complicated.\n\n31:09.000 --> 31:11.080\n But yes, it's still nevertheless much easier\n\n31:11.080 --> 31:12.880\n to verify the proof.\n\n31:12.880 --> 31:14.680\n I love the optimism.\n\n31:14.680 --> 31:17.480\n We kind of, even with the security of systems,\n\n31:17.480 --> 31:21.760\n there's a kind of cynicism that pervades people\n\n31:21.760 --> 31:24.920\n who think about this, which is like, oh, it's hopeless.\n\n31:24.920 --> 31:27.080\n I mean, in the same sense, exactly like you're saying\n\n31:27.080 --> 31:29.000\n when you own networks, oh, it's hopeless to understand\n\n31:29.000 --> 31:30.440\n what's happening.\n\n31:30.440 --> 31:32.560\n With security, people are just like, well,\n\n31:32.560 --> 31:35.040\n it's always going, there's always going to be\n\n31:36.240 --> 31:40.800\n attack vectors, like ways to attack the system.\n\n31:40.800 --> 31:42.200\n But you're right, we're just very new\n\n31:42.200 --> 31:44.080\n with these computational systems.\n\n31:44.080 --> 31:46.400\n We're new with these intelligent systems.\n\n31:46.400 --> 31:49.560\n And it's not out of the realm of possibly,\n\n31:49.560 --> 31:51.840\n just like people that understand the movement\n\n31:51.840 --> 31:54.600\n of the stars and the planets and so on.\n\n31:54.600 --> 31:58.320\n It's entirely possible that within, hopefully soon,\n\n31:58.320 --> 32:00.360\n but it could be within 100 years,\n\n32:00.360 --> 32:03.600\n we start to have an obvious laws of gravity\n\n32:03.600 --> 32:08.600\n about intelligence and God forbid about consciousness too.\n\n32:09.280 --> 32:10.960\n That one is...\n\n32:10.960 --> 32:12.320\n Agreed.\n\n32:12.320 --> 32:15.240\n I think, of course, if you're selling computers\n\n32:15.240 --> 32:16.720\n that get hacked a lot, that's in your interest\n\n32:16.720 --> 32:18.640\n as a company that people think it's impossible\n\n32:18.640 --> 32:20.640\n to make it safe, but he's going to get the idea\n\n32:20.640 --> 32:21.480\n of suing you.\n\n32:21.480 --> 32:24.840\n I want to really inject optimism here.\n\n32:24.840 --> 32:29.480\n It's absolutely possible to do much better\n\n32:29.480 --> 32:30.320\n than we're doing now.\n\n32:30.320 --> 32:34.840\n And your laptop does so much stuff.\n\n32:34.840 --> 32:37.960\n You don't need the music player to be super safe\n\n32:37.960 --> 32:42.120\n in your future self driving car, right?\n\n32:42.120 --> 32:43.840\n If someone hacks it and starts playing music\n\n32:43.840 --> 32:47.880\n you don't like, the world won't end.\n\n32:47.880 --> 32:49.560\n But what you can do is you can break out\n\n32:49.560 --> 32:53.080\n and say that your drive computer that controls your safety\n\n32:53.080 --> 32:55.920\n must be completely physically decoupled entirely\n\n32:55.920 --> 32:57.600\n from the entertainment system.\n\n32:57.600 --> 33:01.080\n And it must physically be such that it can't take on\n\n33:01.080 --> 33:03.040\n over the air updates while you're driving.\n\n33:03.040 --> 33:08.040\n And it can have ultimately some operating system on it\n\n33:09.920 --> 33:12.280\n which is symbolically verified and proven\n\n33:13.320 --> 33:17.760\n that it's always going to do what it's supposed to do, right?\n\n33:17.760 --> 33:19.960\n We can basically have, and companies should take\n\n33:19.960 --> 33:20.800\n that attitude too.\n\n33:20.800 --> 33:22.440\n They should look at everything they do and say\n\n33:22.440 --> 33:25.840\n what are the few systems in our company\n\n33:25.840 --> 33:27.400\n that threaten the whole life of the company\n\n33:27.400 --> 33:31.800\n if they get hacked and have the highest standards for them.\n\n33:31.800 --> 33:34.560\n And then they can save money by going for the el cheapo\n\n33:34.560 --> 33:36.920\n poorly understood stuff for the rest.\n\n33:36.920 --> 33:38.920\n This is very feasible, I think.\n\n33:38.920 --> 33:41.720\n And coming back to the bigger question\n\n33:41.720 --> 33:45.000\n that you worried about that there'll be unintentional\n\n33:45.000 --> 33:47.720\n failures, I think there are two quite separate risks here.\n\n33:47.720 --> 33:48.560\n Right?\n\n33:48.560 --> 33:49.600\n We talked a lot about one of them\n\n33:49.600 --> 33:52.640\n which is that the goals are noble of the human.\n\n33:52.640 --> 33:56.920\n The human says, I want this airplane to not crash\n\n33:56.920 --> 33:58.640\n because this is not Muhammad Atta\n\n33:58.640 --> 34:00.480\n now flying the airplane, right?\n\n34:00.480 --> 34:03.240\n And now there's this technical challenge\n\n34:03.240 --> 34:05.960\n of making sure that the autopilot is actually\n\n34:05.960 --> 34:08.320\n gonna behave as the pilot wants.\n\n34:11.000 --> 34:13.360\n If you set that aside, there's also the separate question.\n\n34:13.360 --> 34:17.440\n How do you make sure that the goals of the pilot\n\n34:17.440 --> 34:19.680\n are actually aligned with the goals of the passenger?\n\n34:19.680 --> 34:22.480\n How do you make sure very much more broadly\n\n34:22.480 --> 34:24.640\n that if we can all agree as a species\n\n34:24.640 --> 34:26.200\n that we would like things to kind of go well\n\n34:26.200 --> 34:30.320\n for humanity as a whole, that the goals are aligned here.\n\n34:30.320 --> 34:31.560\n The alignment problem.\n\n34:31.560 --> 34:36.000\n And yeah, there's been a lot of progress\n\n34:36.000 --> 34:39.880\n in the sense that there's suddenly huge amounts\n\n34:39.880 --> 34:42.040\n of research going on on it about it.\n\n34:42.040 --> 34:43.400\n I'm very grateful to Elon Musk\n\n34:43.400 --> 34:44.960\n for giving us that money five years ago\n\n34:44.960 --> 34:46.680\n so we could launch the first research program\n\n34:46.680 --> 34:49.480\n on technical AI safety and alignment.\n\n34:49.480 --> 34:51.280\n There's a lot of stuff happening.\n\n34:51.280 --> 34:54.920\n But I think we need to do more than just make sure\n\n34:54.920 --> 34:57.280\n little machines do always what their owners do.\n\n34:58.200 --> 35:00.240\n That wouldn't have prevented September 11th\n\n35:00.240 --> 35:03.040\n if Muhammad Atta said, okay, autopilot,\n\n35:03.040 --> 35:05.600\n please fly into World Trade Center.\n\n35:06.720 --> 35:07.680\n And it's like, okay.\n\n35:08.960 --> 35:11.840\n That even happened in a different situation.\n\n35:11.840 --> 35:15.680\n There was this depressed pilot named Andreas Lubitz, right?\n\n35:15.680 --> 35:17.640\n Who told his German wings passenger jet\n\n35:17.640 --> 35:19.040\n to fly into the Alps.\n\n35:19.040 --> 35:21.640\n He just told the computer to change the altitude\n\n35:21.640 --> 35:23.280\n to a hundred meters or something like that.\n\n35:23.280 --> 35:25.360\n And you know what the computer said?\n\n35:25.360 --> 35:26.600\n Okay.\n\n35:26.600 --> 35:29.560\n And it had the freaking topographical map of the Alps\n\n35:29.560 --> 35:31.440\n in there, it had GPS, everything.\n\n35:31.440 --> 35:33.120\n No one had bothered teaching it\n\n35:33.120 --> 35:35.600\n even the basic kindergarten ethics of like,\n\n35:35.600 --> 35:39.600\n no, we never want airplanes to fly into mountains\n\n35:39.600 --> 35:41.040\n under any circumstances.\n\n35:41.040 --> 35:46.040\n And so we have to think beyond just the technical issues\n\n35:48.520 --> 35:51.120\n and think about how do we align in general incentives\n\n35:51.120 --> 35:53.760\n on this planet for the greater good?\n\n35:53.760 --> 35:55.520\n So starting with simple stuff like that,\n\n35:55.520 --> 35:58.160\n every airplane that has a computer in it\n\n35:58.160 --> 36:00.840\n should be taught whatever kindergarten ethics\n\n36:00.840 --> 36:02.280\n that's smart enough to understand.\n\n36:02.280 --> 36:05.040\n Like, no, don't fly into fixed objects\n\n36:05.040 --> 36:07.280\n if the pilot tells you to do so.\n\n36:07.280 --> 36:10.000\n Then go on autopilot mode.\n\n36:10.000 --> 36:13.480\n Send an email to the cops and land at the latest airport,\n\n36:13.480 --> 36:14.840\n nearest airport, you know.\n\n36:14.840 --> 36:18.240\n Any car with a forward facing camera\n\n36:18.240 --> 36:20.720\n should just be programmed by the manufacturer\n\n36:20.720 --> 36:23.560\n so that it will never accelerate into a human ever.\n\n36:24.760 --> 36:28.760\n That would avoid things like the NIS attack\n\n36:28.760 --> 36:31.120\n and many horrible terrorist vehicle attacks\n\n36:31.120 --> 36:33.720\n where they deliberately did that, right?\n\n36:33.720 --> 36:35.160\n This was not some sort of thing,\n\n36:35.160 --> 36:38.400\n oh, you know, US and China, different views on,\n\n36:38.400 --> 36:41.880\n no, there was not a single car manufacturer\n\n36:41.880 --> 36:44.080\n in the world, right, who wanted the cars to do this.\n\n36:44.080 --> 36:45.920\n They just hadn't thought to do the alignment.\n\n36:45.920 --> 36:48.520\n And if you look at more broadly problems\n\n36:48.520 --> 36:49.880\n that happen on this planet,\n\n36:51.280 --> 36:53.840\n the vast majority have to do a poor alignment.\n\n36:53.840 --> 36:57.160\n I mean, think about, let's go back really big\n\n36:57.160 --> 36:59.080\n because I know you're so good at that.\n\n36:59.080 --> 36:59.920\n Let's go big, yeah.\n\n36:59.920 --> 37:03.840\n Yeah, so long ago in evolution, we had these genes.\n\n37:03.840 --> 37:06.400\n And they wanted to make copies of themselves.\n\n37:06.400 --> 37:07.640\n That's really all they cared about.\n\n37:07.640 --> 37:12.640\n So some genes said, hey, I'm gonna build a brain\n\n37:13.160 --> 37:15.880\n on this body I'm in so that I can get better\n\n37:15.880 --> 37:17.280\n at making copies of myself.\n\n37:17.280 --> 37:20.240\n And then they decided for their benefit\n\n37:20.240 --> 37:23.320\n to get copied more, to align your brain's incentives\n\n37:23.320 --> 37:24.560\n with their incentives.\n\n37:24.560 --> 37:28.200\n So it didn't want you to starve to death.\n\n37:29.080 --> 37:31.520\n So it gave you an incentive to eat\n\n37:31.520 --> 37:35.080\n and it wanted you to make copies of the genes.\n\n37:35.080 --> 37:37.680\n So it gave you incentive to fall in love\n\n37:37.680 --> 37:40.960\n and do all sorts of naughty things\n\n37:40.960 --> 37:44.120\n to make copies of itself, right?\n\n37:44.120 --> 37:47.720\n So that was successful value alignment done on the genes.\n\n37:47.720 --> 37:50.440\n They created something more intelligent than themselves,\n\n37:50.440 --> 37:52.920\n but they made sure to try to align the values.\n\n37:52.920 --> 37:55.800\n But then something went a little bit wrong\n\n37:55.800 --> 37:58.400\n against the idea of what the genes wanted\n\n37:58.400 --> 38:00.360\n because a lot of humans discovered,\n\n38:00.360 --> 38:03.280\n hey, you know, yeah, we really like this business\n\n38:03.280 --> 38:06.640\n about sex that the genes have made us enjoy,\n\n38:06.640 --> 38:09.440\n but we don't wanna have babies right now.\n\n38:09.440 --> 38:13.800\n So we're gonna hack the genes and use birth control.\n\n38:13.800 --> 38:18.640\n And I really feel like drinking a Coca Cola right now,\n\n38:18.640 --> 38:20.080\n but I don't wanna get a potbelly,\n\n38:20.080 --> 38:21.960\n so I'm gonna drink Diet Coke.\n\n38:21.960 --> 38:24.600\n We have all these things we've figured out\n\n38:24.600 --> 38:26.400\n because we're smarter than the genes,\n\n38:26.400 --> 38:29.040\n how we can actually subvert their intentions.\n\n38:29.040 --> 38:33.440\n So it's not surprising that we humans now,\n\n38:33.440 --> 38:34.800\n when we are in the role of these genes,\n\n38:34.800 --> 38:37.640\n creating other nonhuman entities with a lot of power,\n\n38:37.640 --> 38:39.400\n have to face the same exact challenge.\n\n38:39.400 --> 38:41.720\n How do we make other powerful entities\n\n38:41.720 --> 38:45.280\n have incentives that are aligned with ours?\n\n38:45.280 --> 38:47.000\n And so they won't hack them.\n\n38:47.000 --> 38:48.720\n Corporations, for example, right?\n\n38:48.720 --> 38:51.280\n We humans decided to create corporations\n\n38:51.280 --> 38:53.440\n because it can benefit us greatly.\n\n38:53.440 --> 38:55.120\n Now all of a sudden there's a supermarket.\n\n38:55.120 --> 38:56.240\n I can go buy food there.\n\n38:56.240 --> 38:57.200\n I don't have to hunt.\n\n38:57.200 --> 39:02.200\n Awesome, and then to make sure that this corporation\n\n39:02.880 --> 39:05.960\n would do things that were good for us and not bad for us,\n\n39:05.960 --> 39:08.280\n we created institutions to keep them in check.\n\n39:08.280 --> 39:12.520\n Like if the local supermarket sells poisonous food,\n\n39:12.520 --> 39:17.520\n then the owners of the supermarket\n\n39:17.920 --> 39:22.160\n have to spend some years reflecting behind bars, right?\n\n39:22.160 --> 39:25.720\n So we created incentives to align them.\n\n39:25.720 --> 39:27.480\n But of course, just like we were able to see\n\n39:27.480 --> 39:30.640\n through this thing and you develop birth control,\n\n39:30.640 --> 39:31.840\n if you're a powerful corporation,\n\n39:31.840 --> 39:35.080\n you also have an incentive to try to hack the institutions\n\n39:35.080 --> 39:36.320\n that are supposed to govern you.\n\n39:36.320 --> 39:38.160\n Because you ultimately, as a corporation,\n\n39:38.160 --> 39:40.920\n have an incentive to maximize your profit.\n\n39:40.920 --> 39:42.080\n Just like you have an incentive\n\n39:42.080 --> 39:44.160\n to maximize the enjoyment your brain has,\n\n39:44.160 --> 39:46.000\n not for your genes.\n\n39:46.000 --> 39:50.480\n So if they can figure out a way of bribing regulators,\n\n39:50.480 --> 39:52.400\n then they're gonna do that.\n\n39:52.400 --> 39:54.440\n In the US, we kind of caught onto that\n\n39:54.440 --> 39:57.200\n and made laws against corruption and bribery.\n\n39:58.560 --> 40:03.560\n Then in the late 1800s, Teddy Roosevelt realized that,\n\n40:03.760 --> 40:05.360\n no, we were still being kind of hacked\n\n40:05.360 --> 40:07.280\n because the Massachusetts Railroad companies\n\n40:07.280 --> 40:10.120\n had like a bigger budget than the state of Massachusetts\n\n40:10.120 --> 40:13.600\n and they were doing a lot of very corrupt stuff.\n\n40:13.600 --> 40:15.480\n So he did the whole trust busting thing\n\n40:15.480 --> 40:18.440\n to try to align these other nonhuman entities,\n\n40:18.440 --> 40:19.440\n the companies, again,\n\n40:19.440 --> 40:23.040\n more with the incentives of Americans as a whole.\n\n40:23.040 --> 40:24.080\n It's not surprising, though,\n\n40:24.080 --> 40:26.160\n that this is a battle you have to keep fighting.\n\n40:26.160 --> 40:30.560\n Now we have even larger companies than we ever had before.\n\n40:30.560 --> 40:33.440\n And of course, they're gonna try to, again,\n\n40:34.320 --> 40:37.800\n subvert the institutions.\n\n40:37.800 --> 40:41.040\n Not because, I think people make a mistake\n\n40:41.040 --> 40:42.040\n of getting all too,\n\n40:44.280 --> 40:46.960\n thinking about things in terms of good and evil.\n\n40:46.960 --> 40:50.360\n Like arguing about whether corporations are good or evil,\n\n40:50.360 --> 40:53.080\n or whether robots are good or evil.\n\n40:53.080 --> 40:57.040\n A robot isn't good or evil, it's a tool.\n\n40:57.040 --> 40:58.400\n And you can use it for great things\n\n40:58.400 --> 41:01.080\n like robotic surgery or for bad things.\n\n41:01.080 --> 41:04.120\n And a corporation also is a tool, of course.\n\n41:04.120 --> 41:06.480\n And if you have good incentives to the corporation,\n\n41:06.480 --> 41:07.520\n it'll do great things,\n\n41:07.520 --> 41:10.000\n like start a hospital or a grocery store.\n\n41:10.000 --> 41:11.760\n If you have any bad incentives,\n\n41:12.680 --> 41:15.800\n then it's gonna start maybe marketing addictive drugs\n\n41:15.800 --> 41:18.600\n to people and you'll have an opioid epidemic, right?\n\n41:18.600 --> 41:19.880\n It's all about,\n\n41:21.440 --> 41:23.480\n we should not make the mistake of getting into\n\n41:23.480 --> 41:25.640\n some sort of fairytale, good, evil thing\n\n41:25.640 --> 41:27.920\n about corporations or robots.\n\n41:27.920 --> 41:30.800\n We should focus on putting the right incentives in place.\n\n41:30.800 --> 41:33.320\n My optimistic vision is that if we can do that,\n\n41:34.280 --> 41:35.840\n then we can really get good things.\n\n41:35.840 --> 41:38.000\n We're not doing so great with that right now,\n\n41:38.000 --> 41:39.240\n either on AI, I think,\n\n41:39.240 --> 41:42.680\n or on other intelligent nonhuman entities,\n\n41:42.680 --> 41:43.920\n like big companies, right?\n\n41:43.920 --> 41:47.440\n We just have a new second generation of AI\n\n41:47.440 --> 41:51.160\n and a secretary of defense who's gonna start up now\n\n41:51.160 --> 41:53.640\n in the Biden administration,\n\n41:53.640 --> 41:58.120\n who was an active member of the board of Raytheon,\n\n41:58.120 --> 41:59.240\n for example.\n\n41:59.240 --> 42:03.360\n So, I have nothing against Raytheon.\n\n42:04.720 --> 42:05.680\n I'm not a pacifist,\n\n42:05.680 --> 42:08.560\n but there's an obvious conflict of interest\n\n42:08.560 --> 42:12.360\n if someone is in the job where they decide\n\n42:12.360 --> 42:14.240\n who they're gonna contract with.\n\n42:14.240 --> 42:16.680\n And I think somehow we have,\n\n42:16.680 --> 42:19.520\n maybe we need another Teddy Roosevelt to come along again\n\n42:19.520 --> 42:20.640\n and say, hey, you know,\n\n42:20.640 --> 42:23.480\n we want what's good for all Americans,\n\n42:23.480 --> 42:26.600\n and we need to go do some serious realigning again\n\n42:26.600 --> 42:29.840\n of the incentives that we're giving to these big companies.\n\n42:30.760 --> 42:33.880\n And then we're gonna be better off.\n\n42:33.880 --> 42:35.800\n It seems that naturally with human beings,\n\n42:35.800 --> 42:37.720\n just like you beautifully described the history\n\n42:37.720 --> 42:38.880\n of this whole thing,\n\n42:38.880 --> 42:40.760\n of it all started with the genes\n\n42:40.760 --> 42:42.680\n and they're probably pretty upset\n\n42:42.680 --> 42:45.600\n by all the unintended consequences that happened since.\n\n42:45.600 --> 42:48.680\n But it seems that it kind of works out,\n\n42:48.680 --> 42:51.120\n like it's in this collective intelligence\n\n42:51.120 --> 42:53.480\n that emerges at the different levels.\n\n42:53.480 --> 42:56.920\n It seems to find sometimes last minute\n\n42:56.920 --> 43:00.920\n a way to realign the values or keep the values aligned.\n\n43:00.920 --> 43:03.800\n It's almost, it finds a way,\n\n43:03.800 --> 43:07.560\n like different leaders, different humans pop up\n\n43:07.560 --> 43:10.680\n all over the place that reset the system.\n\n43:10.680 --> 43:15.240\n Do you want, I mean, do you have an explanation why that is?\n\n43:15.240 --> 43:17.240\n Or is that just survivor bias?\n\n43:17.240 --> 43:19.600\n And also is that different,\n\n43:19.600 --> 43:23.120\n somehow fundamentally different than with AI systems\n\n43:23.120 --> 43:26.440\n where you're no longer dealing with something\n\n43:26.440 --> 43:30.200\n that was a direct, maybe companies are the same,\n\n43:30.200 --> 43:33.360\n a direct byproduct of the evolutionary process?\n\n43:33.360 --> 43:36.200\n I think there is one thing which has changed.\n\n43:36.200 --> 43:40.280\n That's why I'm not all optimistic.\n\n43:40.280 --> 43:42.280\n That's why I think there's about a 50% chance\n\n43:42.280 --> 43:46.120\n if we take the dumb route with artificial intelligence\n\n43:46.120 --> 43:50.240\n that humanity will be extinct in this century.\n\n43:51.680 --> 43:53.320\n First, just the big picture.\n\n43:53.320 --> 43:56.720\n Yeah, companies need to have the right incentives.\n\n43:57.880 --> 43:59.000\n Even governments, right?\n\n43:59.000 --> 44:00.920\n We used to have governments,\n\n44:02.120 --> 44:04.200\n usually there were just some king,\n\n44:04.200 --> 44:07.160\n who was the king because his dad was the king.\n\n44:07.160 --> 44:10.600\n And then there were some benefits\n\n44:10.600 --> 44:15.280\n of having this powerful kingdom or empire of any sort\n\n44:15.280 --> 44:17.960\n because then it could prevent a lot of local squabbles.\n\n44:17.960 --> 44:19.360\n So at least everybody in that region\n\n44:19.360 --> 44:20.800\n would stop warring against each other.\n\n44:20.800 --> 44:24.200\n And their incentives of different cities in the kingdom\n\n44:24.200 --> 44:25.160\n became more aligned, right?\n\n44:25.160 --> 44:27.200\n That was the whole selling point.\n\n44:27.200 --> 44:31.520\n Harare, Noel Harare has a beautiful piece\n\n44:31.520 --> 44:35.320\n on how empires were collaboration enablers.\n\n44:35.320 --> 44:36.760\n And then we also, Harare says,\n\n44:36.760 --> 44:38.280\n invented money for that reason\n\n44:38.280 --> 44:40.640\n so we could have better alignment\n\n44:40.640 --> 44:44.160\n and we could do trade even with people we didn't know.\n\n44:44.160 --> 44:45.840\n So this sort of stuff has been playing out\n\n44:45.840 --> 44:47.880\n since time immemorial, right?\n\n44:47.880 --> 44:51.520\n What's changed is that it happens on ever larger scales,\n\n44:51.520 --> 44:52.360\n right?\n\n44:52.360 --> 44:53.480\n The technology keeps getting better\n\n44:53.480 --> 44:54.760\n because science gets better.\n\n44:54.760 --> 44:57.600\n So now we can communicate over larger distances,\n\n44:57.600 --> 44:59.840\n transport things fast over larger distances.\n\n44:59.840 --> 45:02.960\n And so the entities get ever bigger,\n\n45:02.960 --> 45:05.480\n but our planet is not getting bigger anymore.\n\n45:05.480 --> 45:08.120\n So in the past, you could have one experiment\n\n45:08.120 --> 45:11.040\n that just totally screwed up like Easter Island,\n\n45:11.920 --> 45:15.160\n where they actually managed to have such poor alignment\n\n45:15.160 --> 45:17.600\n that when they went extinct, people there,\n\n45:17.600 --> 45:21.520\n there was no one else to come back and replace them, right?\n\n45:21.520 --> 45:24.000\n If Elon Musk doesn't get us to Mars\n\n45:24.000 --> 45:27.680\n and then we go extinct on a global scale,\n\n45:27.680 --> 45:28.920\n then we're not coming back.\n\n45:28.920 --> 45:31.480\n That's the fundamental difference.\n\n45:31.480 --> 45:35.800\n And that's a mistake we don't make for that reason.\n\n45:35.800 --> 45:39.800\n In the past, of course, history is full of fiascos, right?\n\n45:39.800 --> 45:42.160\n But it was never the whole planet.\n\n45:42.160 --> 45:45.960\n And then, okay, now there's this nice uninhabited land here.\n\n45:45.960 --> 45:49.400\n Some other people could move in and organize things better.\n\n45:49.400 --> 45:50.720\n This is different.\n\n45:50.720 --> 45:52.680\n The second thing, which is also different\n\n45:52.680 --> 45:57.680\n is that technology gives us so much more empowerment, right?\n\n45:58.200 --> 46:00.520\n Both to do good things and also to screw up.\n\n46:00.520 --> 46:02.920\n In the stone age, even if you had someone\n\n46:02.920 --> 46:04.760\n whose goals were really poorly aligned,\n\n46:04.760 --> 46:06.680\n like maybe he was really pissed off\n\n46:06.680 --> 46:08.760\n because his stone age girlfriend dumped him\n\n46:08.760 --> 46:09.920\n and he just wanted to,\n\n46:09.920 --> 46:12.640\n if he wanted to kill as many people as he could,\n\n46:12.640 --> 46:15.160\n how many could he really take out with a rock and a stick\n\n46:15.160 --> 46:17.200\n before he was overpowered, right?\n\n46:17.200 --> 46:18.920\n Just handful, right?\n\n46:18.920 --> 46:23.760\n Now, with today's technology,\n\n46:23.760 --> 46:25.640\n if we have an accidental nuclear war\n\n46:25.640 --> 46:27.880\n between Russia and the US,\n\n46:27.880 --> 46:31.080\n which we almost have about a dozen times,\n\n46:31.080 --> 46:32.280\n and then we have a nuclear winter,\n\n46:32.280 --> 46:34.760\n it could take out seven billion people\n\n46:34.760 --> 46:37.280\n or six billion people, we don't know.\n\n46:37.280 --> 46:40.440\n So the scale of the damage is bigger that we can do.\n\n46:40.440 --> 46:45.440\n And there's obviously no law of physics\n\n46:45.520 --> 46:48.080\n that says that technology will never get powerful enough\n\n46:48.080 --> 46:51.720\n that we could wipe out our species entirely.\n\n46:51.720 --> 46:53.640\n That would just be fantasy to think\n\n46:53.640 --> 46:55.080\n that science is somehow doomed\n\n46:55.080 --> 46:57.240\n to not get more powerful than that, right?\n\n46:57.240 --> 47:00.280\n And it's not at all unfeasible in our lifetime\n\n47:00.280 --> 47:03.120\n that someone could design a designer pandemic\n\n47:03.120 --> 47:04.640\n which spreads as easily as COVID,\n\n47:04.640 --> 47:06.880\n but just basically kills everybody.\n\n47:06.880 --> 47:08.480\n We already had smallpox.\n\n47:08.480 --> 47:10.680\n It killed one third of everybody who got it.\n\n47:13.000 --> 47:15.320\n What do you think of the, here's an intuition,\n\n47:15.320 --> 47:16.840\n maybe it's completely naive\n\n47:16.840 --> 47:18.960\n and this optimistic intuition I have,\n\n47:18.960 --> 47:22.880\n which it seems, and maybe it's a biased experience\n\n47:22.880 --> 47:25.920\n that I have, but it seems like the most brilliant people\n\n47:25.920 --> 47:30.920\n I've met in my life all are really like\n\n47:31.600 --> 47:33.680\n fundamentally good human beings.\n\n47:33.680 --> 47:37.440\n And not like naive good, like they really wanna do good\n\n47:37.440 --> 47:39.880\n for the world in a way that, well, maybe is aligned\n\n47:39.880 --> 47:41.800\n to my sense of what good means.\n\n47:41.800 --> 47:45.800\n And so I have a sense that the people\n\n47:47.840 --> 47:51.000\n that will be defining the very cutting edge of technology,\n\n47:51.000 --> 47:53.960\n there'll be much more of the ones that are doing good\n\n47:53.960 --> 47:55.840\n versus the ones that are doing evil.\n\n47:55.840 --> 48:00.160\n So the race, I'm optimistic on the,\n\n48:00.160 --> 48:03.080\n us always like last minute coming up with a solution.\n\n48:03.080 --> 48:06.480\n So if there's an engineered pandemic\n\n48:06.480 --> 48:09.280\n that has the capability to destroy\n\n48:09.280 --> 48:11.640\n most of the human civilization,\n\n48:11.640 --> 48:15.880\n it feels like to me either leading up to that before\n\n48:15.880 --> 48:19.240\n or as it's going on, there will be,\n\n48:19.240 --> 48:22.520\n we're able to rally the collective genius\n\n48:22.520 --> 48:23.800\n of the human species.\n\n48:23.800 --> 48:26.160\n I can tell by your smile that you're\n\n48:26.160 --> 48:30.080\n at least some percentage doubtful,\n\n48:30.080 --> 48:35.000\n but could that be a fundamental law of human nature?\n\n48:35.000 --> 48:40.000\n That evolution only creates, like karma is beneficial,\n\n48:40.880 --> 48:44.280\n good is beneficial, and therefore we'll be all right.\n\n48:44.280 --> 48:46.960\n I hope you're right.\n\n48:46.960 --> 48:48.720\n I would really love it if you're right,\n\n48:48.720 --> 48:51.000\n if there's some sort of law of nature that says\n\n48:51.000 --> 48:53.080\n that we always get lucky in the last second\n\n48:53.080 --> 48:58.080\n with karma, but I prefer not playing it so close\n\n49:01.160 --> 49:03.040\n and gambling on that.\n\n49:03.040 --> 49:06.480\n And I think, in fact, I think it can be dangerous\n\n49:06.480 --> 49:08.120\n to have too strong faith in that\n\n49:08.120 --> 49:10.800\n because it makes us complacent.\n\n49:10.800 --> 49:12.520\n Like if someone tells you, you never have to worry\n\n49:12.520 --> 49:13.760\n about your house burning down,\n\n49:13.760 --> 49:15.360\n then you're not gonna put in a smoke detector\n\n49:15.360 --> 49:17.000\n because why would you need to?\n\n49:17.000 --> 49:19.040\n Even if it's sometimes very simple precautions,\n\n49:19.040 --> 49:20.000\n we don't take them.\n\n49:20.000 --> 49:22.360\n If you're like, oh, the government is gonna take care\n\n49:22.360 --> 49:24.760\n of everything for us, I can always trust my politicians.\n\n49:24.760 --> 49:27.520\n I can always, we advocate our own responsibility.\n\n49:27.520 --> 49:29.080\n I think it's a healthier attitude to say,\n\n49:29.080 --> 49:30.840\n yeah, maybe things will work out.\n\n49:30.840 --> 49:33.560\n Maybe I'm actually gonna have to myself step up\n\n49:33.560 --> 49:35.080\n and take responsibility.\n\n49:37.160 --> 49:38.360\n And the stakes are so huge.\n\n49:38.360 --> 49:41.840\n I mean, if we do this right, we can develop\n\n49:41.840 --> 49:43.640\n all this ever more powerful technology\n\n49:43.640 --> 49:46.360\n and cure all diseases and create a future\n\n49:46.360 --> 49:48.040\n where humanity is healthy and wealthy\n\n49:48.040 --> 49:50.080\n for not just the next election cycle,\n\n49:50.080 --> 49:52.960\n but like billions of years throughout our universe.\n\n49:52.960 --> 49:54.760\n That's really worth working hard for\n\n49:54.760 --> 49:58.000\n and not just sitting and hoping\n\n49:58.000 --> 49:59.520\n for some sort of fairytale karma.\n\n49:59.520 --> 50:01.600\n Well, I just mean, so you're absolutely right.\n\n50:01.600 --> 50:03.080\n From the perspective of the individual,\n\n50:03.080 --> 50:05.600\n like for me, the primary thing should be\n\n50:05.600 --> 50:09.720\n to take responsibility and to build the solutions\n\n50:09.720 --> 50:11.320\n that your skillset allows.\n\n50:11.320 --> 50:12.720\n Yeah, which is a lot.\n\n50:12.720 --> 50:14.560\n I think we underestimate often very much\n\n50:14.560 --> 50:16.360\n how much good we can do.\n\n50:16.360 --> 50:19.520\n If you or anyone listening to this\n\n50:19.520 --> 50:23.000\n is completely confident that our government\n\n50:23.000 --> 50:25.720\n would do a perfect job on handling any future crisis\n\n50:25.720 --> 50:29.920\n with engineered pandemics or future AI,\n\n50:29.920 --> 50:34.920\n I actually reflect a bit on what actually happened in 2020.\n\n50:36.360 --> 50:39.680\n Do you feel that the government by and large\n\n50:39.680 --> 50:42.680\n around the world has handled this flawlessly?\n\n50:42.680 --> 50:45.160\n That's a really sad and disappointing reality\n\n50:45.160 --> 50:48.720\n that hopefully is a wake up call for everybody.\n\n50:48.720 --> 50:52.280\n For the scientists, for the engineers,\n\n50:52.280 --> 50:54.240\n for the researchers in AI especially,\n\n50:54.240 --> 50:59.240\n it was disappointing to see how inefficient we were\n\n51:01.000 --> 51:04.120\n at collecting the right amount of data\n\n51:04.120 --> 51:07.080\n in a privacy preserving way and spreading that data\n\n51:07.080 --> 51:09.200\n and utilizing that data to make decisions,\n\n51:09.200 --> 51:10.440\n all that kind of stuff.\n\n51:10.440 --> 51:13.360\n Yeah, I think when something bad happens to me,\n\n51:13.360 --> 51:17.280\n I made myself a promise many years ago\n\n51:17.280 --> 51:21.760\n that I would not be a whiner.\n\n51:21.760 --> 51:23.680\n So when something bad happens to me,\n\n51:23.680 --> 51:27.280\n of course it's a process of disappointment,\n\n51:27.280 --> 51:30.520\n but then I try to focus on what did I learn from this\n\n51:30.520 --> 51:32.600\n that can make me a better person in the future.\n\n51:32.600 --> 51:35.720\n And there's usually something to be learned when I fail.\n\n51:35.720 --> 51:38.200\n And I think we should all ask ourselves,\n\n51:38.200 --> 51:41.480\n what can we learn from the pandemic\n\n51:41.480 --> 51:43.400\n about how we can do better in the future?\n\n51:43.400 --> 51:46.360\n And you mentioned there a really good lesson.\n\n51:46.360 --> 51:49.440\n We were not as resilient as we thought we were\n\n51:50.480 --> 51:53.960\n and we were not as prepared maybe as we wish we were.\n\n51:53.960 --> 51:57.280\n You can even see very stark contrast around the planet.\n\n51:57.280 --> 52:01.760\n South Korea, they have over 50 million people.\n\n52:01.760 --> 52:03.520\n Do you know how many deaths they have from COVID\n\n52:03.520 --> 52:04.600\n last time I checked?\n\n52:05.600 --> 52:06.440\n No.\n\n52:06.440 --> 52:07.280\n It's about 500.\n\n52:08.880 --> 52:10.280\n Why is that?\n\n52:10.280 --> 52:15.280\n Well, the short answer is that they had prepared.\n\n52:16.760 --> 52:19.200\n They were incredibly quick,\n\n52:19.200 --> 52:21.520\n incredibly quick to get on it\n\n52:21.520 --> 52:25.520\n with very rapid testing and contact tracing and so on,\n\n52:25.520 --> 52:28.080\n which is why they never had more cases\n\n52:28.080 --> 52:30.040\n than they could contract trace effectively, right?\n\n52:30.040 --> 52:32.040\n They never even had to have the kind of big lockdowns\n\n52:32.040 --> 52:33.720\n we had in the West.\n\n52:33.720 --> 52:36.560\n But the deeper answer to,\n\n52:36.560 --> 52:39.080\n it's not just the Koreans are just somehow better people.\n\n52:39.080 --> 52:40.800\n The reason I think they were better prepared\n\n52:40.800 --> 52:45.320\n was because they had already had a pretty bad hit\n\n52:45.320 --> 52:47.560\n from the SARS pandemic,\n\n52:47.560 --> 52:49.920\n or which never became a pandemic,\n\n52:49.920 --> 52:52.120\n something like 17 years ago, I think.\n\n52:52.120 --> 52:53.400\n So it was kind of fresh memory\n\n52:53.400 --> 52:56.000\n that we need to be prepared for pandemics.\n\n52:56.000 --> 52:57.000\n So they were, right?\n\n52:59.080 --> 53:01.240\n So maybe this is a lesson here\n\n53:01.240 --> 53:03.280\n for all of us to draw from COVID\n\n53:03.280 --> 53:06.360\n that rather than just wait for the next pandemic\n\n53:06.360 --> 53:09.840\n or the next problem with AI getting out of control\n\n53:09.840 --> 53:11.320\n or anything else,\n\n53:11.320 --> 53:14.720\n maybe we should just actually set aside\n\n53:14.720 --> 53:16.800\n a tiny fraction of our GDP\n\n53:17.680 --> 53:19.320\n to have people very systematically\n\n53:19.320 --> 53:20.680\n do some horizon scanning and say,\n\n53:20.680 --> 53:23.320\n okay, what are the things that could go wrong?\n\n53:23.320 --> 53:24.600\n And let's duke it out and see\n\n53:24.600 --> 53:25.800\n which are the more likely ones\n\n53:25.800 --> 53:28.760\n and which are the ones that are actually actionable\n\n53:28.760 --> 53:29.800\n and then be prepared.\n\n53:29.800 --> 53:34.800\n So one of the observations as one little ant slash human\n\n53:36.560 --> 53:38.560\n that I am of disappointment\n\n53:38.560 --> 53:43.560\n is the political division over information\n\n53:44.040 --> 53:47.440\n that has been observed, that I observed this year,\n\n53:47.440 --> 53:50.880\n that it seemed the discussion was less about\n\n53:54.040 --> 53:57.600\n sort of what happened and understanding\n\n53:57.600 --> 54:00.680\n what happened deeply and more about\n\n54:00.680 --> 54:04.080\n there's different truths out there.\n\n54:04.080 --> 54:05.400\n And it's like an argument,\n\n54:05.400 --> 54:07.640\n my truth is better than your truth.\n\n54:07.640 --> 54:10.840\n And it's like red versus blue or different.\n\n54:10.840 --> 54:13.280\n It was like this ridiculous discourse\n\n54:13.280 --> 54:16.520\n that doesn't seem to get at any kind of notion of the truth.\n\n54:16.520 --> 54:19.000\n It's not like some kind of scientific process.\n\n54:19.000 --> 54:21.000\n Even science got politicized in ways\n\n54:21.000 --> 54:24.360\n that's very heartbreaking to me.\n\n54:24.360 --> 54:28.680\n You have an exciting project on the AI front\n\n54:28.680 --> 54:32.560\n of trying to rethink one of the,\n\n54:32.560 --> 54:34.240\n you mentioned corporations.\n\n54:34.240 --> 54:37.360\n There's one of the other collective intelligence systems\n\n54:37.360 --> 54:40.480\n that have emerged through all of this is social networks.\n\n54:40.480 --> 54:43.600\n And just the spread, the internet is the spread\n\n54:43.600 --> 54:46.400\n of information on the internet,\n\n54:46.400 --> 54:48.320\n our ability to share that information.\n\n54:48.320 --> 54:50.640\n There's all different kinds of news sources and so on.\n\n54:50.640 --> 54:53.200\n And so you said like that's from first principles,\n\n54:53.200 --> 54:57.320\n let's rethink how we think about the news,\n\n54:57.320 --> 54:59.080\n how we think about information.\n\n54:59.080 --> 55:02.480\n Can you talk about this amazing effort\n\n55:02.480 --> 55:03.640\n that you're undertaking?\n\n55:03.640 --> 55:04.560\n Oh, I'd love to.\n\n55:04.560 --> 55:06.400\n This has been my big COVID project\n\n55:06.400 --> 55:10.720\n and nights and weekends on ever since the lockdown.\n\n55:11.920 --> 55:13.080\n To segue into this actually,\n\n55:13.080 --> 55:14.520\n let me come back to what you said earlier\n\n55:14.520 --> 55:17.040\n that you had this hope that in your experience,\n\n55:17.040 --> 55:18.800\n people who you felt were very talented\n\n55:18.800 --> 55:21.240\n were often idealistic and wanted to do good.\n\n55:21.240 --> 55:25.160\n Frankly, I feel the same about all people by and large,\n\n55:25.160 --> 55:26.120\n there are always exceptions,\n\n55:26.120 --> 55:28.480\n but I think the vast majority of everybody,\n\n55:28.480 --> 55:30.320\n regardless of education and whatnot,\n\n55:30.320 --> 55:33.280\n really are fundamentally good, right?\n\n55:33.280 --> 55:36.920\n So how can it be that people still do so much nasty stuff?\n\n55:37.920 --> 55:40.040\n I think it has everything to do with this,\n\n55:40.040 --> 55:41.920\n with the information that we're given.\n\n55:41.920 --> 55:42.760\n Yes.\n\n55:42.760 --> 55:46.240\n If you go into Sweden 500 years ago\n\n55:46.240 --> 55:47.360\n and you start telling all the farmers\n\n55:47.360 --> 55:49.160\n that those Danes in Denmark,\n\n55:49.160 --> 55:52.840\n they're so terrible people, and we have to invade them\n\n55:52.840 --> 55:55.320\n because they've done all these terrible things\n\n55:55.320 --> 55:56.840\n that you can't fact check yourself.\n\n55:56.840 --> 55:59.720\n A lot of people, Swedes did that, right?\n\n55:59.720 --> 56:04.720\n And we're seeing so much of this today in the world,\n\n56:06.680 --> 56:11.680\n both geopolitically, where we are told that China is bad\n\n56:11.760 --> 56:13.960\n and Russia is bad and Venezuela is bad,\n\n56:13.960 --> 56:16.000\n and people in those countries are often told\n\n56:16.000 --> 56:17.320\n that we are bad.\n\n56:17.320 --> 56:21.840\n And we also see it at a micro level where people are told\n\n56:21.840 --> 56:24.640\n that, oh, those who voted for the other party are bad people.\n\n56:24.640 --> 56:26.480\n It's not just an intellectual disagreement,\n\n56:26.480 --> 56:31.480\n but they're bad people and we're getting ever more divided.\n\n56:32.880 --> 56:37.880\n So how do you reconcile this with this intrinsic goodness\n\n56:39.000 --> 56:39.840\n in people?\n\n56:39.840 --> 56:41.640\n I think it's pretty obvious that it has, again,\n\n56:41.640 --> 56:46.280\n to do with the information that we're fed and given, right?\n\n56:46.280 --> 56:49.800\n We evolved to live in small groups\n\n56:49.800 --> 56:52.080\n where you might know 30 people in total, right?\n\n56:52.080 --> 56:55.440\n So you then had a system that was quite good\n\n56:55.440 --> 56:57.760\n for assessing who you could trust and who you could not.\n\n56:57.760 --> 57:02.760\n And if someone told you that Joe there is a jerk,\n\n57:02.840 --> 57:05.000\n but you had interacted with him yourself\n\n57:05.000 --> 57:06.400\n and seen him in action,\n\n57:06.400 --> 57:08.320\n and you would quickly realize maybe\n\n57:08.320 --> 57:11.680\n that that's actually not quite accurate, right?\n\n57:11.680 --> 57:13.520\n But now that the most people on the planet\n\n57:13.520 --> 57:15.280\n are people we've never met,\n\n57:15.280 --> 57:17.200\n it's very important that we have a way\n\n57:17.200 --> 57:19.400\n of trusting the information we're given.\n\n57:19.400 --> 57:23.160\n And so, okay, so where does the news project come in?\n\n57:23.160 --> 57:26.560\n Well, throughout history, you can go read Machiavelli,\n\n57:26.560 --> 57:28.680\n from the 1400s, and you'll see how already then\n\n57:28.680 --> 57:30.040\n they were busy manipulating people\n\n57:30.040 --> 57:31.640\n with propaganda and stuff.\n\n57:31.640 --> 57:35.720\n Propaganda is not new at all.\n\n57:35.720 --> 57:37.720\n And the incentives to manipulate people\n\n57:37.720 --> 57:40.040\n is just not new at all.\n\n57:40.040 --> 57:41.240\n What is it that's new?\n\n57:41.240 --> 57:44.680\n What's new is machine learning meets propaganda.\n\n57:44.680 --> 57:45.760\n That's what's new.\n\n57:45.760 --> 57:47.880\n That's why this has gotten so much worse.\n\n57:47.880 --> 57:50.680\n Some people like to blame certain individuals,\n\n57:50.680 --> 57:53.120\n like in my liberal university bubble,\n\n57:53.120 --> 57:55.960\n many people blame Donald Trump and say it was his fault.\n\n57:56.920 --> 57:58.080\n I see it differently.\n\n57:59.120 --> 58:03.840\n I think Donald Trump just had this extreme skill\n\n58:03.840 --> 58:07.560\n at playing this game in the machine learning algorithm age.\n\n58:07.560 --> 58:09.920\n A game he couldn't have played 10 years ago.\n\n58:09.920 --> 58:10.920\n So what's changed?\n\n58:10.920 --> 58:13.200\n What's changed is, well, Facebook and Google\n\n58:13.200 --> 58:16.640\n and other companies, and I'm not badmouthing them,\n\n58:16.640 --> 58:18.800\n I have a lot of friends who work for these companies,\n\n58:18.800 --> 58:22.720\n good people, they deployed machine learning algorithms\n\n58:22.720 --> 58:24.280\n just to increase their profit a little bit,\n\n58:24.280 --> 58:28.520\n to just maximize the time people spent watching ads.\n\n58:28.520 --> 58:30.520\n And they had totally underestimated\n\n58:30.520 --> 58:32.360\n how effective they were gonna be.\n\n58:32.360 --> 58:37.560\n This was, again, the black box, non intelligible intelligence.\n\n58:37.560 --> 58:39.360\n They just noticed, oh, we're getting more ad revenue.\n\n58:39.360 --> 58:40.200\n Great.\n\n58:40.200 --> 58:42.400\n It took a long time until they even realized why and how\n\n58:42.400 --> 58:45.760\n and how damaging this was for society.\n\n58:45.760 --> 58:47.960\n Because of course, what the machine learning figured out\n\n58:47.960 --> 58:52.080\n was that the by far most effective way of gluing you\n\n58:52.080 --> 58:55.040\n to your little rectangle was to show you things\n\n58:55.040 --> 58:59.800\n that triggered strong emotions, anger, et cetera, resentment,\n\n58:59.800 --> 59:04.720\n and if it was true or not, it didn't really matter.\n\n59:04.720 --> 59:07.520\n It was also easier to find stories that weren't true.\n\n59:07.520 --> 59:09.320\n If you weren't limited, that's just the limitation,\n\n59:09.320 --> 59:10.600\n is to show people.\n\n59:10.600 --> 59:12.360\n That's a very limiting fact.\n\n59:12.360 --> 59:16.960\n And before long, we got these amazing filter bubbles\n\n59:16.960 --> 59:18.960\n on a scale we had never seen before.\n\n59:18.960 --> 59:24.600\n A couple of days to the fact that also the online news media\n\n59:24.600 --> 59:27.560\n were so effective that they killed a lot of people\n\n59:27.560 --> 59:30.200\n that were so effective that they killed a lot of print\n\n59:30.200 --> 59:30.800\n journalism.\n\n59:30.800 --> 59:34.120\n There's less than half as many journalists\n\n59:34.120 --> 59:39.640\n now in America, I believe, as there was a generation ago.\n\n59:39.640 --> 59:42.800\n You just couldn't compete with the online advertising.\n\n59:42.800 --> 59:47.240\n So all of a sudden, most people are not\n\n59:47.240 --> 59:48.680\n getting even reading newspapers.\n\n59:48.680 --> 59:51.320\n They get their news from social media.\n\n59:51.320 --> 59:55.000\n And most people only get news in their little bubble.\n\n59:55.000 --> 59:58.400\n So along comes now some people like Donald Trump,\n\n59:58.400 --> 1:00:01.560\n who figured out, among the first successful politicians,\n\n1:00:01.560 --> 1:00:04.080\n to figure out how to really play this new game\n\n1:00:04.080 --> 1:00:05.960\n and become very, very influential.\n\n1:00:05.960 --> 1:00:09.600\n But I think Donald Trump was as simple.\n\n1:00:09.600 --> 1:00:11.120\n He took advantage of it.\n\n1:00:11.120 --> 1:00:14.520\n He didn't create the fundamental conditions\n\n1:00:14.520 --> 1:00:19.040\n were created by machine learning taking over the news media.\n\n1:00:19.040 --> 1:00:22.920\n So this is what motivated my little COVID project here.\n\n1:00:22.920 --> 1:00:27.120\n So I said before, machine learning and tech in general\n\n1:00:27.120 --> 1:00:29.040\n is not evil, but it's also not good.\n\n1:00:29.040 --> 1:00:31.400\n It's just a tool that you can use\n\n1:00:31.400 --> 1:00:32.680\n for good things or bad things.\n\n1:00:32.680 --> 1:00:36.000\n And as it happens, machine learning and news\n\n1:00:36.000 --> 1:00:39.680\n was mainly used by the big players, big tech,\n\n1:00:39.680 --> 1:00:43.240\n to manipulate people and to watch as many ads as possible,\n\n1:00:43.240 --> 1:00:45.720\n which had this unintended consequence of really screwing\n\n1:00:45.720 --> 1:00:50.440\n up our democracy and fragmenting it into filter bubbles.\n\n1:00:50.440 --> 1:00:53.200\n So I thought, well, machine learning algorithms\n\n1:00:53.200 --> 1:00:54.400\n are basically free.\n\n1:00:54.400 --> 1:00:56.200\n They can run on your smartphone for free also\n\n1:00:56.200 --> 1:00:57.840\n if someone gives them away to you, right?\n\n1:00:57.840 --> 1:01:00.480\n There's no reason why they only have to help the big guy\n\n1:01:01.840 --> 1:01:02.960\n to manipulate the little guy.\n\n1:01:02.960 --> 1:01:05.280\n They can just as well help the little guy\n\n1:01:05.280 --> 1:01:07.880\n to see through all the manipulation attempts\n\n1:01:07.880 --> 1:01:08.720\n from the big guy.\n\n1:01:08.720 --> 1:01:10.600\n So this project is called,\n\n1:01:10.600 --> 1:01:12.800\n you can go to improvethenews.org.\n\n1:01:12.800 --> 1:01:16.600\n The first thing we've built is this little news aggregator.\n\n1:01:16.600 --> 1:01:17.880\n Looks a bit like Google News,\n\n1:01:17.880 --> 1:01:20.200\n except it has these sliders on it to help you break out\n\n1:01:20.200 --> 1:01:21.760\n of your filter bubble.\n\n1:01:21.760 --> 1:01:24.440\n So if you're reading, you can click, click\n\n1:01:24.440 --> 1:01:25.960\n and go to your favorite topic.\n\n1:01:27.080 --> 1:01:31.120\n And then if you just slide the left, right slider\n\n1:01:31.120 --> 1:01:32.560\n away all the way over to the left.\n\n1:01:32.560 --> 1:01:33.720\n There's two sliders, right?\n\n1:01:33.720 --> 1:01:36.280\n Yeah, there's the one, the most obvious one\n\n1:01:36.280 --> 1:01:38.800\n is the one that has left, right labeled on it.\n\n1:01:38.800 --> 1:01:40.560\n You go to the left, you get one set of articles,\n\n1:01:40.560 --> 1:01:43.360\n you go to the right, you see a very different truth\n\n1:01:43.360 --> 1:01:44.200\n appearing.\n\n1:01:44.200 --> 1:01:47.640\n Oh, that's literally left and right on the political spectrum.\n\n1:01:47.640 --> 1:01:48.480\n On the political spectrum.\n\n1:01:48.480 --> 1:01:51.560\n So if you're reading about immigration, for example,\n\n1:01:52.720 --> 1:01:55.560\n it's very, very noticeable.\n\n1:01:55.560 --> 1:01:57.080\n And I think step one always,\n\n1:01:57.080 --> 1:02:00.960\n if you wanna not get manipulated is just to be able\n\n1:02:00.960 --> 1:02:02.960\n to recognize the techniques people use.\n\n1:02:02.960 --> 1:02:05.880\n So it's very helpful to just see how they spin things\n\n1:02:05.880 --> 1:02:06.760\n on the two sides.\n\n1:02:08.160 --> 1:02:11.240\n I think many people are under the misconception\n\n1:02:11.240 --> 1:02:14.080\n that the main problem is fake news.\n\n1:02:14.080 --> 1:02:14.920\n It's not.\n\n1:02:14.920 --> 1:02:17.520\n I had an amazing team of MIT students\n\n1:02:17.520 --> 1:02:20.360\n where we did an academic project to use machine learning\n\n1:02:20.360 --> 1:02:23.080\n to detect the main kinds of bias over the summer.\n\n1:02:23.080 --> 1:02:25.640\n And yes, of course, sometimes there's fake news\n\n1:02:25.640 --> 1:02:29.120\n where someone just claims something that's false, right?\n\n1:02:30.000 --> 1:02:32.920\n Like, oh, Hillary Clinton just got divorced or something.\n\n1:02:32.920 --> 1:02:36.160\n But what we see much more of is actually just omissions.\n\n1:02:37.800 --> 1:02:41.920\n If you go to, there's some stories which just won't be\n\n1:02:41.920 --> 1:02:45.520\n mentioned by the left or the right, because it doesn't suit\n\n1:02:45.520 --> 1:02:46.360\n their agenda.\n\n1:02:46.360 --> 1:02:49.680\n And then they'll mention other ones very, very, very much.\n\n1:02:49.680 --> 1:02:54.680\n So for example, we've had a number of stories\n\n1:02:54.680 --> 1:02:58.640\n about the Trump family's financial dealings.\n\n1:02:59.600 --> 1:03:01.560\n And then there's been a bunch of stories\n\n1:03:01.560 --> 1:03:05.280\n about the Biden family's, Hunter Biden's financial dealings.\n\n1:03:05.280 --> 1:03:07.520\n Surprise, surprise, they don't get equal coverage\n\n1:03:07.520 --> 1:03:08.920\n on the left and the right.\n\n1:03:08.920 --> 1:03:13.320\n One side loves to cover the Biden, Hunter Biden's stuff,\n\n1:03:13.320 --> 1:03:15.360\n and one side loves to cover the Trump.\n\n1:03:15.360 --> 1:03:17.320\n You can never guess which is which, right?\n\n1:03:17.320 --> 1:03:21.560\n But the great news is if you're a normal American citizen\n\n1:03:21.560 --> 1:03:24.080\n and you dislike corruption in all its forms,\n\n1:03:24.960 --> 1:03:28.560\n then slide, slide, you can just look at both sides\n\n1:03:28.560 --> 1:03:32.520\n and you'll see all those political corruption stories.\n\n1:03:32.520 --> 1:03:37.520\n It's really liberating to just take in the both sides,\n\n1:03:37.520 --> 1:03:39.440\n the spin on both sides.\n\n1:03:39.440 --> 1:03:42.720\n It somehow unlocks your mind to think on your own,\n\n1:03:42.720 --> 1:03:47.040\n to realize that, I don't know, it's the same thing\n\n1:03:47.040 --> 1:03:49.840\n that was useful, right, in the Soviet Union times\n\n1:03:49.840 --> 1:03:54.360\n for when everybody was much more aware\n\n1:03:54.360 --> 1:03:57.200\n that they're surrounded by propaganda, right?\n\n1:03:57.200 --> 1:04:00.600\n That is so interesting what you're saying, actually.\n\n1:04:00.600 --> 1:04:04.000\n So Noam Chomsky, used to be our MIT colleague,\n\n1:04:04.000 --> 1:04:07.640\n once said that propaganda is to democracy\n\n1:04:07.640 --> 1:04:11.960\n what violence is to totalitarianism.\n\n1:04:11.960 --> 1:04:15.080\n And what he means by that is if you have\n\n1:04:15.080 --> 1:04:16.680\n a really totalitarian government,\n\n1:04:16.680 --> 1:04:18.040\n you don't need propaganda.\n\n1:04:19.680 --> 1:04:22.880\n People will do what you want them to do anyway,\n\n1:04:22.880 --> 1:04:24.320\n but out of fear, right?\n\n1:04:24.320 --> 1:04:28.080\n But otherwise, you need propaganda.\n\n1:04:28.080 --> 1:04:29.880\n So I would say actually that the propaganda\n\n1:04:29.880 --> 1:04:32.560\n is much higher quality in democracies,\n\n1:04:32.560 --> 1:04:34.240\n much more believable.\n\n1:04:34.240 --> 1:04:36.960\n And it's really, it's really striking.\n\n1:04:36.960 --> 1:04:39.520\n When I talk to colleagues, science colleagues\n\n1:04:39.520 --> 1:04:41.160\n like from Russia and China and so on,\n\n1:04:42.200 --> 1:04:45.120\n I notice they are actually much more aware\n\n1:04:45.120 --> 1:04:47.200\n of the propaganda in their own media\n\n1:04:47.200 --> 1:04:48.840\n than many of my American colleagues are\n\n1:04:48.840 --> 1:04:51.120\n about the propaganda in Western media.\n\n1:04:51.120 --> 1:04:51.960\n That's brilliant.\n\n1:04:51.960 --> 1:04:53.880\n That means the propaganda in the Western media\n\n1:04:53.880 --> 1:04:54.720\n is just better.\n\n1:04:54.720 --> 1:04:55.560\n Yes.\n\n1:04:55.560 --> 1:04:56.400\n That's so brilliant.\n\n1:04:56.400 --> 1:04:58.200\n Everything's better in the West, even the propaganda.\n\n1:04:58.200 --> 1:05:03.200\n But once you realize that,\n\n1:05:07.360 --> 1:05:09.280\n you realize there's also something very optimistic there\n\n1:05:09.280 --> 1:05:10.480\n that you can do about it, right?\n\n1:05:10.480 --> 1:05:12.920\n Because first of all, omissions,\n\n1:05:14.040 --> 1:05:16.920\n as long as there's no outright censorship,\n\n1:05:16.920 --> 1:05:18.480\n you can just look at both sides\n\n1:05:19.840 --> 1:05:22.760\n and pretty quickly piece together\n\n1:05:22.760 --> 1:05:26.120\n a much more accurate idea of what's actually going on, right?\n\n1:05:26.120 --> 1:05:28.040\n And develop a natural skepticism too.\n\n1:05:28.040 --> 1:05:28.880\n Yeah.\n\n1:05:28.880 --> 1:05:31.600\n Just an analytical scientific mind\n\n1:05:31.600 --> 1:05:32.880\n about the way you're taking the information.\n\n1:05:32.880 --> 1:05:33.720\n Yeah.\n\n1:05:33.720 --> 1:05:35.480\n And I think, I have to say,\n\n1:05:35.480 --> 1:05:38.480\n sometimes I feel that some of us in the academic bubble\n\n1:05:38.480 --> 1:05:41.440\n are too arrogant about this and somehow think,\n\n1:05:41.440 --> 1:05:44.560\n oh, it's just people who aren't as educated\n\n1:05:44.560 --> 1:05:45.800\n as the dots are pulled.\n\n1:05:45.800 --> 1:05:48.240\n When we are often just as gullible also,\n\n1:05:48.240 --> 1:05:52.080\n we read only our media and don't see through things.\n\n1:05:52.080 --> 1:05:53.960\n Anyone who looks at both sides like this\n\n1:05:53.960 --> 1:05:56.320\n and compares a little will immediately start noticing\n\n1:05:56.320 --> 1:05:58.080\n the shenanigans being pulled.\n\n1:05:58.080 --> 1:06:01.840\n And I think what I tried to do with this app\n\n1:06:01.840 --> 1:06:05.760\n is that the big tech has to some extent\n\n1:06:05.760 --> 1:06:08.960\n tried to blame the individual for being manipulated,\n\n1:06:08.960 --> 1:06:12.320\n much like big tobacco tried to blame the individuals\n\n1:06:12.320 --> 1:06:13.680\n entirely for smoking.\n\n1:06:13.680 --> 1:06:16.880\n And then later on, our government stepped up and say,\n\n1:06:16.880 --> 1:06:19.560\n actually, you can't just blame little kids\n\n1:06:19.560 --> 1:06:20.400\n for starting to smoke.\n\n1:06:20.400 --> 1:06:22.400\n We have to have more responsible advertising\n\n1:06:22.400 --> 1:06:23.480\n and this and that.\n\n1:06:23.480 --> 1:06:24.600\n I think it's a bit the same here.\n\n1:06:24.600 --> 1:06:27.600\n It's very convenient for a big tech to blame.\n\n1:06:27.600 --> 1:06:29.960\n So it's just people who are so dumb and get fooled.\n\n1:06:32.160 --> 1:06:34.160\n The blame usually comes in saying,\n\n1:06:34.160 --> 1:06:36.000\n oh, it's just human psychology.\n\n1:06:36.000 --> 1:06:38.360\n People just wanna hear what they already believe.\n\n1:06:38.360 --> 1:06:43.160\n But professor David Rand at MIT actually partly debunked that\n\n1:06:43.160 --> 1:06:45.280\n with a really nice study showing that people\n\n1:06:45.280 --> 1:06:47.640\n tend to be interested in hearing things\n\n1:06:47.640 --> 1:06:49.880\n that go against what they believe,\n\n1:06:49.880 --> 1:06:52.680\n if it's presented in a respectful way.\n\n1:06:52.680 --> 1:06:57.560\n Suppose, for example, that you have a company\n\n1:06:57.560 --> 1:06:59.120\n and you're just about to launch this project\n\n1:06:59.120 --> 1:07:00.280\n and you're convinced it's gonna work.\n\n1:07:00.280 --> 1:07:02.280\n And someone says, you know, Lex,\n\n1:07:03.520 --> 1:07:05.640\n I hate to tell you this, but this is gonna fail.\n\n1:07:05.640 --> 1:07:06.640\n And here's why.\n\n1:07:06.640 --> 1:07:08.920\n Would you be like, shut up, I don't wanna hear it.\n\n1:07:08.920 --> 1:07:10.640\n La, la, la, la, la, la, la, la, la.\n\n1:07:10.640 --> 1:07:11.480\n Would you?\n\n1:07:11.480 --> 1:07:13.000\n You would be interested, right?\n\n1:07:13.000 --> 1:07:15.480\n And also if you're on an airplane,\n\n1:07:16.360 --> 1:07:19.000\n back in the pre COVID times,\n\n1:07:19.000 --> 1:07:20.240\n and the guy next to you\n\n1:07:20.240 --> 1:07:23.160\n is clearly from the opposite side of the political spectrum,\n\n1:07:24.160 --> 1:07:26.720\n but is very respectful and polite to you.\n\n1:07:26.720 --> 1:07:28.840\n Wouldn't you be kind of interested to hear a bit about\n\n1:07:28.840 --> 1:07:31.960\n how he or she thinks about things?\n\n1:07:31.960 --> 1:07:32.800\n Of course.\n\n1:07:32.800 --> 1:07:35.360\n But it's not so easy to find out\n\n1:07:35.360 --> 1:07:36.760\n respectful disagreement now,\n\n1:07:36.760 --> 1:07:40.440\n because like, for example, if you are a Democrat\n\n1:07:40.440 --> 1:07:41.920\n and you're like, oh, I wanna see something\n\n1:07:41.920 --> 1:07:42.760\n on the other side,\n\n1:07:42.760 --> 1:07:45.080\n so you just go Breitbart.com.\n\n1:07:45.080 --> 1:07:46.960\n And then after the first 10 seconds,\n\n1:07:46.960 --> 1:07:49.400\n you feel deeply insulted by something.\n\n1:07:49.400 --> 1:07:52.480\n And they, it's not gonna work.\n\n1:07:52.480 --> 1:07:54.480\n Or if you take someone who votes Republican\n\n1:07:55.640 --> 1:07:57.400\n and they go to something on the left,\n\n1:07:57.400 --> 1:08:00.120\n then they just get very offended very quickly\n\n1:08:00.120 --> 1:08:02.200\n by them having put a deliberately ugly picture\n\n1:08:02.200 --> 1:08:04.320\n of Donald Trump on the front page or something.\n\n1:08:04.320 --> 1:08:05.640\n It doesn't really work.\n\n1:08:05.640 --> 1:08:09.800\n So this news aggregator also has this nuance slider,\n\n1:08:09.800 --> 1:08:11.440\n which you can pull to the right\n\n1:08:11.440 --> 1:08:13.960\n and then sort of make it easier to get exposed\n\n1:08:13.960 --> 1:08:16.120\n to actually more sort of academic style\n\n1:08:16.120 --> 1:08:17.200\n or more respectful,\n\n1:08:17.200 --> 1:08:19.480\n portrayals of different views.\n\n1:08:19.480 --> 1:08:22.080\n And finally, the one kind of bias\n\n1:08:22.080 --> 1:08:25.440\n I think people are mostly aware of is the left, right,\n\n1:08:25.440 --> 1:08:26.280\n because it's so obvious,\n\n1:08:26.280 --> 1:08:30.600\n because both left and right are very powerful here, right?\n\n1:08:30.600 --> 1:08:33.920\n Both of them have well funded TV stations and newspapers,\n\n1:08:33.920 --> 1:08:35.520\n and it's kind of hard to miss.\n\n1:08:35.520 --> 1:08:39.000\n But there's another one, the establishment slider,\n\n1:08:39.000 --> 1:08:41.320\n which is also really fun.\n\n1:08:41.320 --> 1:08:42.840\n I love to play with it.\n\n1:08:42.840 --> 1:08:44.360\n And that's more about corruption.\n\n1:08:44.360 --> 1:08:45.200\n Yeah, yeah.\n\n1:08:45.200 --> 1:08:47.600\n I love that one. Yes.\n\n1:08:47.600 --> 1:08:51.360\n Because if you have a society\n\n1:08:53.240 --> 1:08:57.320\n where almost all the powerful entities\n\n1:08:57.320 --> 1:08:59.480\n want you to believe a certain thing,\n\n1:08:59.480 --> 1:09:01.840\n that's what you're gonna read in both the big media,\n\n1:09:01.840 --> 1:09:04.640\n mainstream media on the left and on the right, of course.\n\n1:09:04.640 --> 1:09:08.200\n And the powerful companies can push back very hard,\n\n1:09:08.200 --> 1:09:10.160\n like tobacco companies push back very hard\n\n1:09:10.160 --> 1:09:12.160\n back in the day when some newspapers\n\n1:09:12.160 --> 1:09:15.400\n started writing articles about tobacco being dangerous,\n\n1:09:15.400 --> 1:09:17.000\n so that it was hard to get a lot of coverage\n\n1:09:17.000 --> 1:09:18.480\n about it initially.\n\n1:09:18.480 --> 1:09:20.880\n And also if you look geopolitically, right,\n\n1:09:20.880 --> 1:09:23.120\n of course, in any country, when you read their media,\n\n1:09:23.120 --> 1:09:24.880\n you're mainly gonna be reading a lot of articles\n\n1:09:24.880 --> 1:09:27.360\n about how our country is the good guy\n\n1:09:27.360 --> 1:09:30.400\n and the other countries are the bad guys, right?\n\n1:09:30.400 --> 1:09:33.360\n So if you wanna have a really more nuanced understanding,\n\n1:09:33.360 --> 1:09:37.040\n like the Germans used to be told that the British\n\n1:09:37.040 --> 1:09:38.840\n used to be told that the French were the bad guys\n\n1:09:38.840 --> 1:09:39.880\n and the French used to be told\n\n1:09:39.880 --> 1:09:41.880\n that the British were the bad guys.\n\n1:09:41.880 --> 1:09:45.680\n Now they visit each other's countries a lot\n\n1:09:45.680 --> 1:09:47.360\n and have a much more nuanced understanding.\n\n1:09:47.360 --> 1:09:48.840\n I don't think there's gonna be any more wars\n\n1:09:48.840 --> 1:09:50.120\n between France and Germany.\n\n1:09:50.120 --> 1:09:51.840\n But on the geopolitical scale,\n\n1:09:53.000 --> 1:09:54.520\n it's just as much as ever, you know,\n\n1:09:54.520 --> 1:09:57.600\n big Cold War, now US, China, and so on.\n\n1:09:57.600 --> 1:10:01.200\n And if you wanna get a more nuanced understanding\n\n1:10:01.200 --> 1:10:03.520\n of what's happening geopolitically,\n\n1:10:03.520 --> 1:10:05.960\n then it's really fun to look at this establishment slider\n\n1:10:05.960 --> 1:10:09.360\n because it turns out there are tons of little newspapers,\n\n1:10:09.360 --> 1:10:11.360\n both on the left and on the right,\n\n1:10:11.360 --> 1:10:14.480\n who sometimes challenge establishment and say,\n\n1:10:14.480 --> 1:10:17.800\n you know, maybe we shouldn't actually invade Iraq right now.\n\n1:10:17.800 --> 1:10:20.400\n Maybe this weapons of mass destruction thing is BS.\n\n1:10:20.400 --> 1:10:23.680\n If you look at the journalism research afterwards,\n\n1:10:23.680 --> 1:10:25.360\n you can actually see that quite clearly.\n\n1:10:25.360 --> 1:10:29.200\n Both CNN and Fox were very pro.\n\n1:10:29.200 --> 1:10:30.640\n Let's get rid of Saddam.\n\n1:10:30.640 --> 1:10:32.560\n There are weapons of mass destruction.\n\n1:10:32.560 --> 1:10:34.680\n Then there were a lot of smaller newspapers.\n\n1:10:34.680 --> 1:10:36.200\n They were like, wait a minute,\n\n1:10:36.200 --> 1:10:40.240\n this evidence seems a bit sketchy and maybe we...\n\n1:10:40.240 --> 1:10:42.240\n But of course they were so hard to find.\n\n1:10:42.240 --> 1:10:44.560\n Most people didn't even know they existed, right?\n\n1:10:44.560 --> 1:10:47.400\n Yet it would have been better for American national security\n\n1:10:47.400 --> 1:10:50.160\n if those voices had also come up.\n\n1:10:50.160 --> 1:10:52.560\n I think it harmed America's national security actually\n\n1:10:52.560 --> 1:10:53.800\n that we invaded Iraq.\n\n1:10:53.800 --> 1:10:55.560\n And arguably there's a lot more interest\n\n1:10:55.560 --> 1:11:00.480\n in that kind of thinking too, from those small sources.\n\n1:11:00.480 --> 1:11:02.600\n So like when you say big,\n\n1:11:02.600 --> 1:11:07.600\n it's more about kind of the reach of the broadcast,\n\n1:11:07.600 --> 1:11:12.040\n but it's not big in terms of the interest.\n\n1:11:12.040 --> 1:11:14.120\n I think there's a lot of interest\n\n1:11:14.120 --> 1:11:16.200\n in that kind of anti establishment\n\n1:11:16.200 --> 1:11:18.840\n or like skepticism towards, you know,\n\n1:11:18.840 --> 1:11:20.360\n out of the box thinking.\n\n1:11:20.360 --> 1:11:22.000\n There's a lot of interest in that kind of thing.\n\n1:11:22.000 --> 1:11:26.920\n Do you see this news project or something like it\n\n1:11:26.920 --> 1:11:30.600\n being basically taken over the world\n\n1:11:30.600 --> 1:11:32.920\n as the main way we consume information?\n\n1:11:32.920 --> 1:11:35.120\n Like how do we get there?\n\n1:11:35.120 --> 1:11:37.320\n Like how do we, you know?\n\n1:11:37.320 --> 1:11:39.000\n So, okay, the idea is brilliant.\n\n1:11:39.000 --> 1:11:44.000\n It's a, you're calling it your little project in 2020,\n\n1:11:44.000 --> 1:11:48.480\n but how does that become the new way we consume information?\n\n1:11:48.480 --> 1:11:51.000\n I hope, first of all, just to plant a little seed there\n\n1:11:51.000 --> 1:11:55.920\n because normally the big barrier of doing anything in media\n\n1:11:55.920 --> 1:11:59.280\n is you need a ton of money, but this costs no money at all.\n\n1:11:59.280 --> 1:12:00.640\n I've just been paying myself.\n\n1:12:00.640 --> 1:12:03.080\n You pay a tiny amount of money each month to Amazon\n\n1:12:03.080 --> 1:12:04.640\n to run the thing in their cloud.\n\n1:12:04.640 --> 1:12:06.920\n We're not, there never will never be any ads.\n\n1:12:06.920 --> 1:12:09.360\n The point is not to make any money off of it.\n\n1:12:09.360 --> 1:12:11.640\n And we just train machine learning algorithms\n\n1:12:11.640 --> 1:12:13.160\n to classify the articles and stuff.\n\n1:12:13.160 --> 1:12:14.840\n So it just kind of runs by itself.\n\n1:12:14.840 --> 1:12:17.760\n So if it actually gets good enough at some point\n\n1:12:17.760 --> 1:12:20.720\n that it starts catching on, it could scale.\n\n1:12:20.720 --> 1:12:23.120\n And if other people carbon copy it\n\n1:12:23.120 --> 1:12:24.960\n and make other versions that are better,\n\n1:12:24.960 --> 1:12:28.200\n that's the more the merrier.\n\n1:12:28.200 --> 1:12:32.920\n I think there's a real opportunity for machine learning\n\n1:12:32.920 --> 1:12:39.880\n to empower the individual against the powerful players.\n\n1:12:39.880 --> 1:12:41.600\n As I said in the beginning here, it's\n\n1:12:41.600 --> 1:12:43.280\n been mostly the other way around so far,\n\n1:12:43.280 --> 1:12:46.960\n that the big players have the AI and then they tell people,\n\n1:12:46.960 --> 1:12:49.600\n this is the truth, this is how it is.\n\n1:12:49.600 --> 1:12:52.200\n But it can just as well go the other way around.\n\n1:12:52.200 --> 1:12:54.320\n And when the internet was born, actually, a lot of people\n\n1:12:54.320 --> 1:12:56.280\n had this hope that maybe this will be\n\n1:12:56.280 --> 1:12:58.120\n a great thing for democracy, make it easier\n\n1:12:58.120 --> 1:12:59.480\n to find out about things.\n\n1:12:59.480 --> 1:13:02.320\n And maybe machine learning and things like this\n\n1:13:02.320 --> 1:13:03.720\n can actually help again.\n\n1:13:03.720 --> 1:13:07.080\n And I have to say, I think it's more important than ever now\n\n1:13:07.080 --> 1:13:12.160\n because this is very linked also to the whole future of life\n\n1:13:12.160 --> 1:13:13.920\n as we discussed earlier.\n\n1:13:13.920 --> 1:13:17.280\n We're getting this ever more powerful tech.\n\n1:13:17.280 --> 1:13:19.040\n Frank, it's pretty clear if you look\n\n1:13:19.040 --> 1:13:21.920\n on the one or two generation, three generation timescale\n\n1:13:21.920 --> 1:13:24.880\n that there are only two ways this can end geopolitically.\n\n1:13:24.880 --> 1:13:27.640\n Either it ends great for all humanity\n\n1:13:27.640 --> 1:13:31.640\n or it ends terribly for all of us.\n\n1:13:31.640 --> 1:13:33.680\n There's really no in between.\n\n1:13:33.680 --> 1:13:37.560\n And we're so stuck in that because technology\n\n1:13:37.560 --> 1:13:39.080\n knows no borders.\n\n1:13:39.080 --> 1:13:42.200\n And you can't have people fighting\n\n1:13:42.200 --> 1:13:44.480\n when the weapons just keep getting ever more\n\n1:13:44.480 --> 1:13:47.040\n powerful indefinitely.\n\n1:13:47.040 --> 1:13:50.280\n Eventually, the luck runs out.\n\n1:13:50.280 --> 1:13:55.480\n And right now we have, I love America,\n\n1:13:55.480 --> 1:13:59.840\n but the fact of the matter is what's good for America\n\n1:13:59.840 --> 1:14:02.000\n is not opposite in the long term to what's\n\n1:14:02.000 --> 1:14:04.600\n good for other countries.\n\n1:14:04.600 --> 1:14:07.400\n It would be if this was some sort of zero sum game\n\n1:14:07.400 --> 1:14:10.960\n like it was thousands of years ago when the only way one\n\n1:14:10.960 --> 1:14:13.440\n country could get more resources was\n\n1:14:13.440 --> 1:14:14.960\n to take land from other countries\n\n1:14:14.960 --> 1:14:17.640\n because that was basically the resource.\n\n1:14:17.640 --> 1:14:18.920\n Look at the map of Europe.\n\n1:14:18.920 --> 1:14:21.400\n Some countries kept getting bigger and smaller,\n\n1:14:21.400 --> 1:14:23.280\n endless wars.\n\n1:14:23.280 --> 1:14:26.400\n But then since 1945, there hasn't been any war\n\n1:14:26.400 --> 1:14:27.160\n in Western Europe.\n\n1:14:27.160 --> 1:14:29.920\n And they all got way richer because of tech.\n\n1:14:29.920 --> 1:14:34.760\n So the optimistic outcome is that the big winner\n\n1:14:34.760 --> 1:14:38.200\n in this century is going to be America and China and Russia\n\n1:14:38.200 --> 1:14:40.200\n and everybody else because technology just makes\n\n1:14:40.200 --> 1:14:41.760\n us all healthier and wealthier.\n\n1:14:41.760 --> 1:14:44.680\n And we just find some way of keeping the peace\n\n1:14:44.680 --> 1:14:46.640\n on this planet.\n\n1:14:46.640 --> 1:14:48.760\n But I think, unfortunately, there\n\n1:14:48.760 --> 1:14:50.440\n are some pretty powerful forces right now\n\n1:14:50.440 --> 1:14:52.560\n that are pushing in exactly the opposite direction\n\n1:14:52.560 --> 1:14:55.920\n and trying to demonize other countries, which just makes\n\n1:14:55.920 --> 1:14:58.360\n it more likely that this ever more powerful tech we're\n\n1:14:58.360 --> 1:15:02.200\n building is going to be used in disastrous ways.\n\n1:15:02.200 --> 1:15:04.400\n Yeah, for aggression versus cooperation,\n\n1:15:04.400 --> 1:15:05.200\n that kind of thing.\n\n1:15:05.200 --> 1:15:09.560\n Yeah, even look at just military AI now.\n\n1:15:09.560 --> 1:15:12.160\n It was so awesome to see these dancing robots.\n\n1:15:12.160 --> 1:15:14.000\n I loved it.\n\n1:15:14.000 --> 1:15:17.080\n But one of the biggest growth areas in robotics\n\n1:15:17.080 --> 1:15:19.480\n now is, of course, autonomous weapons.\n\n1:15:19.480 --> 1:15:23.200\n And 2020 was like the best marketing year\n\n1:15:23.200 --> 1:15:24.400\n ever for autonomous weapons.\n\n1:15:24.400 --> 1:15:27.520\n Because in both Libya, it's a civil war,\n\n1:15:27.520 --> 1:15:34.440\n and in Nagorno Karabakh, they made the decisive difference.\n\n1:15:34.440 --> 1:15:36.280\n And everybody else is watching this.\n\n1:15:36.280 --> 1:15:38.920\n Oh, yeah, we want to build autonomous weapons, too.\n\n1:15:38.920 --> 1:15:45.080\n In Libya, you had, on one hand, our ally,\n\n1:15:45.080 --> 1:15:47.080\n the United Arab Emirates that were flying\n\n1:15:47.080 --> 1:15:50.640\n their autonomous weapons that they bought from China,\n\n1:15:50.640 --> 1:15:51.880\n bombing Libyans.\n\n1:15:51.880 --> 1:15:54.280\n And on the other side, you had our other ally, Turkey,\n\n1:15:54.280 --> 1:15:57.200\n flying their drones.\n\n1:15:57.200 --> 1:16:00.480\n And they had no skin in the game,\n\n1:16:00.480 --> 1:16:01.680\n any of these other countries.\n\n1:16:01.680 --> 1:16:04.160\n And of course, it was the Libyans who really got screwed.\n\n1:16:04.160 --> 1:16:09.280\n In Nagorno Karabakh, you had actually, again,\n\n1:16:09.280 --> 1:16:12.400\n Turkey is sending drones built by this company that\n\n1:16:12.400 --> 1:16:17.080\n was actually founded by a guy who went to MIT AeroAstro.\n\n1:16:17.080 --> 1:16:17.800\n Do you know that?\n\n1:16:17.800 --> 1:16:18.280\n No.\n\n1:16:18.280 --> 1:16:18.960\n Bacratyar.\n\n1:16:18.960 --> 1:16:19.520\n Yeah.\n\n1:16:19.520 --> 1:16:21.480\n So MIT has a direct responsibility\n\n1:16:21.480 --> 1:16:22.680\n for ultimately this.\n\n1:16:22.680 --> 1:16:25.680\n And a lot of civilians were killed there.\n\n1:16:25.680 --> 1:16:29.640\n So because it was militarily so effective,\n\n1:16:29.640 --> 1:16:31.240\n now suddenly there's a huge push.\n\n1:16:31.240 --> 1:16:35.680\n Oh, yeah, yeah, let's go build ever more autonomy\n\n1:16:35.680 --> 1:16:39.440\n into these weapons, and it's going to be great.\n\n1:16:39.440 --> 1:16:44.640\n And I think, actually, people who\n\n1:16:44.640 --> 1:16:47.760\n are obsessed about some sort of future Terminator scenario\n\n1:16:47.760 --> 1:16:51.640\n right now should start focusing on the fact\n\n1:16:51.640 --> 1:16:54.000\n that we have two much more urgent threats happening\n\n1:16:54.000 --> 1:16:54.960\n from machine learning.\n\n1:16:54.960 --> 1:16:57.880\n One of them is the whole destruction of democracy\n\n1:16:57.880 --> 1:17:01.600\n that we've talked about now, where\n\n1:17:01.600 --> 1:17:03.560\n our flow of information is being manipulated\n\n1:17:03.560 --> 1:17:04.400\n by machine learning.\n\n1:17:04.400 --> 1:17:06.960\n And the other one is that right now,\n\n1:17:06.960 --> 1:17:10.440\n this is the year when the big arms race and out of control\n\n1:17:10.440 --> 1:17:12.800\n arms race in at least Thomas Weapons is going to start,\n\n1:17:12.800 --> 1:17:14.640\n or it's going to stop.\n\n1:17:14.640 --> 1:17:18.480\n So you have a sense that there is like 2020\n\n1:17:18.480 --> 1:17:24.280\n was an instrumental catalyst for the autonomous weapons race.\n\n1:17:24.280 --> 1:17:26.560\n Yeah, because it was the first year when they proved\n\n1:17:26.560 --> 1:17:28.360\n decisive in the battlefield.\n\n1:17:28.360 --> 1:17:31.400\n And these ones are still not fully autonomous, mostly.\n\n1:17:31.400 --> 1:17:32.640\n They're remote controlled, right?\n\n1:17:32.640 --> 1:17:38.720\n But we could very quickly make things\n\n1:17:38.720 --> 1:17:43.280\n about the size and cost of a smartphone, which you just put\n\n1:17:43.280 --> 1:17:45.160\n in the GPS coordinates or the face of the one\n\n1:17:45.160 --> 1:17:47.000\n you want to kill, a skin color or whatever,\n\n1:17:47.000 --> 1:17:48.480\n and it flies away and does it.\n\n1:17:48.480 --> 1:17:53.920\n And the real good reason why the US and all\n\n1:17:53.920 --> 1:17:57.040\n the other superpowers should put the kibosh on this\n\n1:17:57.040 --> 1:18:01.680\n is the same reason we decided to put the kibosh on bioweapons.\n\n1:18:01.680 --> 1:18:05.000\n So we gave the Future of Life Award\n\n1:18:05.000 --> 1:18:07.200\n that we can talk more about later to Matthew Messelson\n\n1:18:07.200 --> 1:18:08.680\n from Harvard before for convincing\n\n1:18:08.680 --> 1:18:10.320\n Nixon to ban bioweapons.\n\n1:18:10.320 --> 1:18:13.600\n And I asked him, how did you do it?\n\n1:18:13.600 --> 1:18:16.560\n And he was like, well, I just said, look,\n\n1:18:16.560 --> 1:18:20.520\n we don't want there to be a $500 weapon of mass destruction\n\n1:18:20.520 --> 1:18:26.560\n that all our enemies can afford, even nonstate actors.\n\n1:18:26.560 --> 1:18:32.120\n And Nixon was like, good point.\n\n1:18:32.120 --> 1:18:34.520\n It's in America's interest that the powerful weapons are all\n\n1:18:34.520 --> 1:18:37.600\n really expensive, so only we can afford them,\n\n1:18:37.600 --> 1:18:41.080\n or maybe some more stable adversaries, right?\n\n1:18:41.080 --> 1:18:42.960\n Nuclear weapons are like that.\n\n1:18:42.960 --> 1:18:44.920\n But bioweapons were not like that.\n\n1:18:44.920 --> 1:18:46.400\n That's why we banned them.\n\n1:18:46.400 --> 1:18:48.400\n And that's why you never hear about them now.\n\n1:18:48.400 --> 1:18:50.280\n That's why we love biology.\n\n1:18:50.280 --> 1:18:55.440\n So you have a sense that it's possible for the big power\n\n1:18:55.440 --> 1:18:58.480\n houses in terms of the big nations in the world\n\n1:18:58.480 --> 1:19:02.360\n to agree that autonomous weapons is not a race we want to be on,\n\n1:19:02.360 --> 1:19:03.680\n that it doesn't end well.\n\n1:19:03.680 --> 1:19:05.320\n Yeah, because we know it's just going\n\n1:19:05.320 --> 1:19:06.560\n to end in mass proliferation.\n\n1:19:06.560 --> 1:19:08.560\n And every terrorist everywhere is\n\n1:19:08.560 --> 1:19:10.280\n going to have these super cheap weapons\n\n1:19:10.280 --> 1:19:13.440\n that they will use against us.\n\n1:19:13.440 --> 1:19:15.960\n And our politicians have to constantly worry\n\n1:19:15.960 --> 1:19:18.240\n about being assassinated every time they go outdoors\n\n1:19:18.240 --> 1:19:21.040\n by some anonymous little mini drone.\n\n1:19:21.040 --> 1:19:21.840\n We don't want that.\n\n1:19:21.840 --> 1:19:25.920\n And even if the US and China and everyone else\n\n1:19:25.920 --> 1:19:27.840\n could just agree that you can only\n\n1:19:27.840 --> 1:19:31.560\n build these weapons if they cost at least $10 million,\n\n1:19:31.560 --> 1:19:34.760\n that would be a huge win for the superpowers\n\n1:19:34.760 --> 1:19:38.800\n and, frankly, for everybody.\n\n1:19:38.800 --> 1:19:41.000\n And people often push back and say, well, it's\n\n1:19:41.000 --> 1:19:43.200\n so hard to prevent cheating.\n\n1:19:43.200 --> 1:19:45.800\n But hey, you could say the same about bioweapons.\n\n1:19:45.800 --> 1:19:49.360\n Take any of your MIT colleagues in biology.\n\n1:19:49.360 --> 1:19:52.000\n Of course, they could build some nasty bioweapon\n\n1:19:52.000 --> 1:19:53.560\n if they really wanted to.\n\n1:19:53.560 --> 1:19:55.280\n But first of all, they don't want to\n\n1:19:55.280 --> 1:19:57.640\n because they think it's disgusting because of the stigma.\n\n1:19:57.640 --> 1:20:02.120\n And second, even if there's some sort of nutcase and want to,\n\n1:20:02.120 --> 1:20:04.160\n it's very likely that some of their grad students\n\n1:20:04.160 --> 1:20:06.560\n or someone would rat them out because everyone else thinks\n\n1:20:06.560 --> 1:20:08.000\n it's so disgusting.\n\n1:20:08.000 --> 1:20:11.480\n And in fact, we now know there was even a fair bit of cheating\n\n1:20:11.480 --> 1:20:13.480\n on the bioweapons ban.\n\n1:20:13.480 --> 1:20:17.520\n But no countries used them because it was so stigmatized\n\n1:20:17.520 --> 1:20:22.400\n that it just wasn't worth revealing that they had cheated.\n\n1:20:22.400 --> 1:20:24.840\n You talk about drones, but you kind of\n\n1:20:24.840 --> 1:20:28.960\n think that drones is a remote operation.\n\n1:20:28.960 --> 1:20:30.680\n Which they are, mostly, still.\n\n1:20:30.680 --> 1:20:34.600\n But you're not taking the next intellectual step\n\n1:20:34.600 --> 1:20:36.320\n of where does this go.\n\n1:20:36.320 --> 1:20:38.760\n You're kind of saying the problem with drones\n\n1:20:38.760 --> 1:20:42.400\n is that you're removing yourself from direct violence.\n\n1:20:42.400 --> 1:20:44.920\n Therefore, you're not able to sort of maintain\n\n1:20:44.920 --> 1:20:46.720\n the common humanity required to make\n\n1:20:46.720 --> 1:20:48.720\n the proper decisions strategically.\n\n1:20:48.720 --> 1:20:51.360\n But that's the criticism as opposed to like,\n\n1:20:51.360 --> 1:20:55.520\n if this is automated, and just exactly as you said,\n\n1:20:55.520 --> 1:20:58.640\n if you automate it and there's a race,\n\n1:20:58.640 --> 1:21:01.280\n then the technology's gonna get better and better and better\n\n1:21:01.280 --> 1:21:03.720\n which means getting cheaper and cheaper and cheaper.\n\n1:21:03.720 --> 1:21:06.080\n And unlike, perhaps, nuclear weapons\n\n1:21:06.080 --> 1:21:10.240\n which is connected to resources in a way,\n\n1:21:10.240 --> 1:21:13.760\n like it's hard to engineer, yeah.\n\n1:21:13.760 --> 1:21:17.600\n It feels like there's too much overlap\n\n1:21:17.600 --> 1:21:20.400\n between the tech industry and autonomous weapons\n\n1:21:20.400 --> 1:21:24.400\n to where you could have smartphone type of cheapness.\n\n1:21:24.400 --> 1:21:29.280\n If you look at drones, for $1,000,\n\n1:21:29.280 --> 1:21:30.800\n you can have an incredible system\n\n1:21:30.800 --> 1:21:34.600\n that's able to maintain flight autonomously for you\n\n1:21:34.600 --> 1:21:36.240\n and take pictures and stuff.\n\n1:21:36.240 --> 1:21:39.440\n You could see that going into the autonomous weapons space\n\n1:21:39.440 --> 1:21:43.240\n that's, but why is that not thought about\n\n1:21:43.240 --> 1:21:45.640\n or discussed enough in the public, do you think?\n\n1:21:45.640 --> 1:21:48.960\n You see those dancing Boston Dynamics robots\n\n1:21:48.960 --> 1:21:50.760\n and everybody has this kind of,\n\n1:21:52.600 --> 1:21:55.360\n as if this is like a far future.\n\n1:21:55.360 --> 1:21:58.640\n They have this fear like, oh, this'll be Terminator\n\n1:21:58.640 --> 1:22:03.080\n in like some, I don't know, unspecified 20, 30, 40 years.\n\n1:22:03.080 --> 1:22:05.640\n And they don't think about, well, this is like\n\n1:22:05.640 --> 1:22:09.120\n some much less dramatic version of that\n\n1:22:09.120 --> 1:22:11.160\n is actually happening now.\n\n1:22:11.160 --> 1:22:14.840\n It's not gonna be legged, it's not gonna be dancing,\n\n1:22:14.840 --> 1:22:17.160\n but it already has the capability\n\n1:22:17.160 --> 1:22:20.240\n to use artificial intelligence to kill humans.\n\n1:22:20.240 --> 1:22:22.880\n Yeah, the Boston Dynamics legged robots,\n\n1:22:22.880 --> 1:22:24.960\n I think the reason we imagine them holding guns\n\n1:22:24.960 --> 1:22:28.440\n is just because you've all seen Arnold Schwarzenegger, right?\n\n1:22:28.440 --> 1:22:30.600\n That's our reference point.\n\n1:22:30.600 --> 1:22:32.680\n That's pretty useless.\n\n1:22:32.680 --> 1:22:35.360\n That's not gonna be the main military use of them.\n\n1:22:35.360 --> 1:22:38.720\n They might be useful in law enforcement in the future\n\n1:22:38.720 --> 1:22:40.280\n and then there's a whole debate about,\n\n1:22:40.280 --> 1:22:42.640\n do you want robots showing up at your house with guns\n\n1:22:42.640 --> 1:22:45.440\n telling you who'll be perfectly obedient\n\n1:22:45.440 --> 1:22:47.560\n to whatever dictator controls them?\n\n1:22:47.560 --> 1:22:49.240\n But let's leave that aside for a moment\n\n1:22:49.240 --> 1:22:51.320\n and look at what's actually relevant now.\n\n1:22:51.320 --> 1:22:54.760\n So there's a spectrum of things you can do\n\n1:22:54.760 --> 1:22:55.760\n with AI in the military.\n\n1:22:55.760 --> 1:22:57.560\n And again, to put my card on the table,\n\n1:22:57.560 --> 1:23:01.240\n I'm not the pacifist, I think we should have good defense.\n\n1:23:03.480 --> 1:23:08.480\n So for example, a predator drone is basically\n\n1:23:08.480 --> 1:23:11.720\n a fancy little remote controlled airplane, right?\n\n1:23:11.720 --> 1:23:16.040\n There's a human piloting it and the decision ultimately\n\n1:23:16.040 --> 1:23:17.280\n about whether to kill somebody with it\n\n1:23:17.280 --> 1:23:19.400\n is made by a human still.\n\n1:23:19.400 --> 1:23:23.880\n And this is a line I think we should never cross.\n\n1:23:23.880 --> 1:23:25.880\n There's a current DOD policy.\n\n1:23:25.880 --> 1:23:27.920\n Again, you have to have a human in the loop.\n\n1:23:27.920 --> 1:23:30.680\n I think algorithms should never make life\n\n1:23:30.680 --> 1:23:33.120\n or death decisions, they should be left to humans.\n\n1:23:34.120 --> 1:23:37.720\n Now, why might we cross that line?\n\n1:23:37.720 --> 1:23:40.520\n Well, first of all, these are expensive, right?\n\n1:23:40.520 --> 1:23:45.520\n So for example, when Azerbaijan had all these drones\n\n1:23:46.560 --> 1:23:48.280\n and Armenia didn't have any, they start trying\n\n1:23:48.280 --> 1:23:51.760\n to jerry rig little cheap things, fly around.\n\n1:23:51.760 --> 1:23:54.040\n But then of course, the Armenians would jam them\n\n1:23:54.040 --> 1:23:55.600\n or the Azeris would jam them.\n\n1:23:55.600 --> 1:23:58.320\n And remote control things can be jammed,\n\n1:23:58.320 --> 1:24:00.040\n that makes them inferior.\n\n1:24:00.040 --> 1:24:02.960\n Also, there's a bit of a time delay between,\n\n1:24:02.960 --> 1:24:05.400\n if we're piloting something from far away,\n\n1:24:05.400 --> 1:24:08.640\n speed of light, and the human has a reaction time as well,\n\n1:24:08.640 --> 1:24:11.560\n it would be nice to eliminate that jamming possibility\n\n1:24:11.560 --> 1:24:14.320\n in the time that they by having it fully autonomous.\n\n1:24:14.320 --> 1:24:17.080\n But now you might be, so then if you do,\n\n1:24:17.080 --> 1:24:19.400\n but now you might be crossing that exact line.\n\n1:24:19.400 --> 1:24:22.360\n You might program it to just, oh yeah, the air drone,\n\n1:24:22.360 --> 1:24:25.280\n go hover over this country for a while\n\n1:24:25.280 --> 1:24:28.760\n and whenever you find someone who is a bad guy,\n\n1:24:28.760 --> 1:24:30.080\n kill them.\n\n1:24:30.960 --> 1:24:33.480\n Now the machine is making these sort of decisions\n\n1:24:33.480 --> 1:24:36.120\n and some people who defend this still say,\n\n1:24:36.120 --> 1:24:39.960\n well, that's morally fine because we are the good guys\n\n1:24:39.960 --> 1:24:43.000\n and we will tell it the definition of bad guy\n\n1:24:43.000 --> 1:24:44.320\n that we think is moral.\n\n1:24:45.640 --> 1:24:48.720\n But now it would be very naive to think\n\n1:24:48.720 --> 1:24:51.480\n that if ISIS buys that same drone,\n\n1:24:51.480 --> 1:24:54.040\n that they're gonna use our definition of bad guy.\n\n1:24:54.040 --> 1:24:55.840\n Maybe for them, bad guy is someone wearing\n\n1:24:55.840 --> 1:25:00.840\n a US army uniform or maybe there will be some,\n\n1:25:00.840 --> 1:25:04.680\n weird ethnic group who decides that someone\n\n1:25:04.680 --> 1:25:07.160\n of another ethnic group, they are the bad guys, right?\n\n1:25:07.160 --> 1:25:11.320\n The thing is human soldiers with all our faults,\n\n1:25:11.320 --> 1:25:14.080\n we still have some basic wiring in us.\n\n1:25:14.080 --> 1:25:18.040\n Like, no, it's not okay to kill kids and civilians.\n\n1:25:20.040 --> 1:25:21.760\n And Thomas Weprin has none of that.\n\n1:25:21.760 --> 1:25:23.600\n It's just gonna do whatever is programmed.\n\n1:25:23.600 --> 1:25:27.720\n It's like the perfect Adolf Eichmann on steroids.\n\n1:25:27.720 --> 1:25:30.840\n Like they told him, Adolf Eichmann, you know,\n\n1:25:30.840 --> 1:25:32.240\n he wanted to do this and this and this\n\n1:25:32.240 --> 1:25:33.680\n to make the Holocaust more efficient.\n\n1:25:33.680 --> 1:25:37.840\n And he was like, yeah, and off he went and did it, right?\n\n1:25:37.840 --> 1:25:41.120\n Do we really wanna make machines that are like that,\n\n1:25:41.120 --> 1:25:44.240\n like completely amoral and we'll take the user's definition\n\n1:25:44.240 --> 1:25:45.720\n of who is the bad guy?\n\n1:25:45.720 --> 1:25:47.920\n And do we then wanna make them so cheap\n\n1:25:47.920 --> 1:25:49.640\n that all our adversaries can have them?\n\n1:25:49.640 --> 1:25:52.720\n Like what could possibly go wrong?\n\n1:25:52.720 --> 1:25:56.720\n That's I think the big ordeal of the whole thing.\n\n1:25:56.720 --> 1:26:00.200\n I think the big argument for why we wanna,\n\n1:26:00.200 --> 1:26:03.520\n this year really put the kibosh on this.\n\n1:26:03.520 --> 1:26:06.360\n And I think you can tell there's a lot\n\n1:26:06.360 --> 1:26:10.120\n of very active debate even going on within the US military\n\n1:26:10.120 --> 1:26:13.080\n and undoubtedly in other militaries around the world also\n\n1:26:13.080 --> 1:26:14.200\n about whether we should have some sort\n\n1:26:14.200 --> 1:26:16.760\n of international agreement to at least require\n\n1:26:16.760 --> 1:26:20.640\n that these weapons have to be above a certain size\n\n1:26:20.640 --> 1:26:27.320\n and cost, you know, so that things just don't totally spiral\n\n1:26:27.320 --> 1:26:29.800\n out of control.\n\n1:26:29.800 --> 1:26:31.600\n And finally, just for your question,\n\n1:26:31.600 --> 1:26:33.560\n but is it possible to stop it?\n\n1:26:33.560 --> 1:26:37.000\n Because some people tell me, oh, just give up, you know.\n\n1:26:37.000 --> 1:26:41.560\n But again, so Matthew Messelsen again from Harvard, right,\n\n1:26:41.560 --> 1:26:46.640\n who the bioweapons hero, he had exactly this criticism\n\n1:26:46.640 --> 1:26:47.760\n also with bioweapons.\n\n1:26:47.760 --> 1:26:49.920\n People were like, how can you check for sure\n\n1:26:49.920 --> 1:26:52.960\n that the Russians aren't cheating?\n\n1:26:52.960 --> 1:26:58.560\n And he told me this, I think really ingenious insight.\n\n1:26:58.560 --> 1:27:01.200\n He said, you know, Max, some people\n\n1:27:01.200 --> 1:27:03.640\n think you have to have inspections and things\n\n1:27:03.640 --> 1:27:06.760\n and you have to make sure that you can catch the cheaters\n\n1:27:06.760 --> 1:27:08.960\n with 100% chance.\n\n1:27:08.960 --> 1:27:10.800\n You don't need 100%, he said.\n\n1:27:10.800 --> 1:27:14.080\n 1% is usually enough.\n\n1:27:14.080 --> 1:27:19.240\n Because if it's another big state,\n\n1:27:19.240 --> 1:27:23.480\n suppose China and the US have signed the treaty drawing\n\n1:27:23.480 --> 1:27:26.200\n a certain line and saying, yeah, these kind of drones are OK,\n\n1:27:26.200 --> 1:27:28.800\n but these fully autonomous ones are not.\n\n1:27:28.800 --> 1:27:34.400\n Now suppose you are China and you have cheated and secretly\n\n1:27:34.400 --> 1:27:36.000\n developed some clandestine little thing\n\n1:27:36.000 --> 1:27:37.560\n or you're thinking about doing it.\n\n1:27:37.560 --> 1:27:39.200\n What's your calculation that you do?\n\n1:27:39.200 --> 1:27:41.880\n Well, you're like, OK, what's the probability\n\n1:27:41.880 --> 1:27:44.920\n that we're going to get caught?\n\n1:27:44.920 --> 1:27:49.120\n If the probability is 100%, of course, we're not going to do it.\n\n1:27:49.120 --> 1:27:52.720\n But if the probability is 5% that we're going to get caught,\n\n1:27:52.720 --> 1:27:55.560\n then it's going to be like a huge embarrassment for us.\n\n1:27:55.560 --> 1:28:00.120\n And we still have our nuclear weapons anyway,\n\n1:28:00.120 --> 1:28:05.160\n so it doesn't really make an enormous difference in terms\n\n1:28:05.160 --> 1:28:07.520\n of deterring the US.\n\n1:28:07.520 --> 1:28:11.640\n And that feeds the stigma that you kind of established,\n\n1:28:11.640 --> 1:28:14.720\n like this fabric, this universal stigma over the thing.\n\n1:28:14.720 --> 1:28:15.520\n Exactly.\n\n1:28:15.520 --> 1:28:18.080\n It's very reasonable for them to say, well, we probably\n\n1:28:18.080 --> 1:28:18.800\n get away with it.\n\n1:28:18.800 --> 1:28:21.320\n If we don't, then the US will know we cheated,\n\n1:28:21.320 --> 1:28:23.720\n and then they're going to go full tilt with their program\n\n1:28:23.720 --> 1:28:25.000\n and say, look, the Chinese are cheaters,\n\n1:28:25.000 --> 1:28:27.080\n and now we have all these weapons against us,\n\n1:28:27.080 --> 1:28:27.920\n and that's bad.\n\n1:28:27.920 --> 1:28:32.160\n So the stigma alone is very, very powerful.\n\n1:28:32.160 --> 1:28:34.520\n And again, look what happened with bioweapons.\n\n1:28:34.520 --> 1:28:36.880\n It's been 50 years now.\n\n1:28:36.880 --> 1:28:40.120\n When was the last time you read about a bioterrorism attack?\n\n1:28:40.120 --> 1:28:42.680\n The only deaths I really know about with bioweapons\n\n1:28:42.680 --> 1:28:45.200\n that have happened when we Americans managed\n\n1:28:45.200 --> 1:28:47.200\n to kill some of our own with anthrax,\n\n1:28:47.200 --> 1:28:49.760\n or the idiot who sent them to Tom Daschle and others\n\n1:28:49.760 --> 1:28:50.880\n in letters, right?\n\n1:28:50.880 --> 1:28:55.960\n And similarly in Sverdlovsk in the Soviet Union,\n\n1:28:55.960 --> 1:28:57.960\n they had some anthrax in some lab there.\n\n1:28:57.960 --> 1:29:00.000\n Maybe they were cheating or who knows,\n\n1:29:00.000 --> 1:29:02.520\n and it leaked out and killed a bunch of Russians.\n\n1:29:02.520 --> 1:29:04.480\n I'd say that's a pretty good success, right?\n\n1:29:04.480 --> 1:29:08.360\n 50 years, just two own goals by the superpowers,\n\n1:29:08.360 --> 1:29:09.560\n and then nothing.\n\n1:29:09.560 --> 1:29:12.120\n And that's why whenever I ask anyone\n\n1:29:12.120 --> 1:29:15.160\n what they think about biology, they think it's great.\n\n1:29:15.160 --> 1:29:18.080\n They associate it with new cures, new diseases,\n\n1:29:18.080 --> 1:29:19.720\n maybe a good vaccine.\n\n1:29:19.720 --> 1:29:22.680\n This is how I want to think about AI in the future.\n\n1:29:22.680 --> 1:29:24.840\n And I want others to think about AI too,\n\n1:29:24.840 --> 1:29:27.840\n as a source of all these great solutions to our problems,\n\n1:29:27.840 --> 1:29:31.920\n not as, oh, AI, oh yeah, that's the reason\n\n1:29:31.920 --> 1:29:34.600\n I feel scared going outside these days.\n\n1:29:34.600 --> 1:29:37.920\n Yeah, it's kind of brilliant that bioweapons\n\n1:29:37.920 --> 1:29:40.760\n and nuclear weapons, we've figured out,\n\n1:29:40.760 --> 1:29:43.320\n I mean, of course there's still a huge source of danger,\n\n1:29:43.320 --> 1:29:47.760\n but we figured out some way of creating rules\n\n1:29:47.760 --> 1:29:51.440\n and social stigma over these weapons\n\n1:29:51.440 --> 1:29:54.600\n that then creates a stability to our,\n\n1:29:54.600 --> 1:29:57.640\n whatever that game theoretic stability that occurs.\n\n1:29:57.640 --> 1:29:59.200\n And we don't have that with AI,\n\n1:29:59.200 --> 1:30:03.760\n and you're kind of screaming from the top of the mountain\n\n1:30:03.760 --> 1:30:05.520\n about this, that we need to find that\n\n1:30:05.520 --> 1:30:10.520\n because it's very possible with the future of life,\n\n1:30:10.520 --> 1:30:15.000\n as you point out, Institute Awards pointed out\n\n1:30:15.000 --> 1:30:17.920\n that with nuclear weapons,\n\n1:30:17.920 --> 1:30:21.040\n we could have destroyed ourselves quite a few times.\n\n1:30:21.040 --> 1:30:26.040\n And it's a learning experience that is very costly.\n\n1:30:28.520 --> 1:30:30.960\n We gave this Future Life Award,\n\n1:30:30.960 --> 1:30:34.640\n we gave it the first time to this guy, Vasily Arkhipov.\n\n1:30:34.640 --> 1:30:37.480\n He was on, most people haven't even heard of him.\n\n1:30:37.480 --> 1:30:38.640\n Yeah, can you say who he is?\n\n1:30:38.640 --> 1:30:43.640\n Vasily Arkhipov, he has, in my opinion,\n\n1:30:44.080 --> 1:30:47.480\n made the greatest positive contribution to humanity\n\n1:30:47.480 --> 1:30:50.200\n of any human in modern history.\n\n1:30:50.200 --> 1:30:51.880\n And maybe it sounds like hyperbole here,\n\n1:30:51.880 --> 1:30:53.320\n like I'm just over the top,\n\n1:30:53.320 --> 1:30:56.080\n but let me tell you the story and I think maybe you'll agree.\n\n1:30:56.080 --> 1:30:58.200\n So during the Cuban Missile Crisis,\n\n1:31:00.000 --> 1:31:01.800\n we Americans first didn't know\n\n1:31:01.800 --> 1:31:04.240\n that the Russians had sent four submarines,\n\n1:31:05.160 --> 1:31:06.720\n but we caught two of them.\n\n1:31:06.720 --> 1:31:09.160\n And we didn't know that,\n\n1:31:09.160 --> 1:31:11.040\n so we dropped practice depth charges\n\n1:31:11.040 --> 1:31:12.360\n on the one that he was on,\n\n1:31:12.360 --> 1:31:13.840\n try to force it to the surface.\n\n1:31:15.440 --> 1:31:17.680\n But we didn't know that this nuclear submarine\n\n1:31:17.680 --> 1:31:20.560\n actually was a nuclear submarine with a nuclear torpedo.\n\n1:31:20.560 --> 1:31:22.640\n We also didn't know that they had authorization\n\n1:31:22.640 --> 1:31:25.120\n to launch it without clearance from Moscow.\n\n1:31:25.120 --> 1:31:26.040\n And we also didn't know\n\n1:31:26.040 --> 1:31:28.240\n that they were running out of electricity.\n\n1:31:28.240 --> 1:31:29.880\n Their batteries were almost dead.\n\n1:31:29.880 --> 1:31:31.840\n They were running out of oxygen.\n\n1:31:31.840 --> 1:31:34.280\n Sailors were fainting left and right.\n\n1:31:34.280 --> 1:31:39.120\n The temperature was about 110, 120 Fahrenheit on board.\n\n1:31:39.120 --> 1:31:40.920\n It was really hellish conditions,\n\n1:31:40.920 --> 1:31:43.240\n really just a kind of doomsday.\n\n1:31:43.240 --> 1:31:44.520\n And at that point,\n\n1:31:44.520 --> 1:31:46.280\n these giant explosions start happening\n\n1:31:46.280 --> 1:31:48.160\n from the Americans dropping these.\n\n1:31:48.160 --> 1:31:50.680\n The captain thought World War III had begun.\n\n1:31:50.680 --> 1:31:53.720\n They decided they were gonna launch the nuclear torpedo.\n\n1:31:53.720 --> 1:31:55.360\n And one of them shouted,\n\n1:31:55.360 --> 1:31:56.200\n we're all gonna die,\n\n1:31:56.200 --> 1:31:58.920\n but we're not gonna disgrace our Navy.\n\n1:31:58.920 --> 1:32:00.120\n We don't know what would have happened\n\n1:32:00.120 --> 1:32:03.400\n if there had been a giant mushroom cloud all of a sudden\n\n1:32:03.400 --> 1:32:04.760\n against the Americans.\n\n1:32:04.760 --> 1:32:07.360\n But since everybody had their hands on the triggers,\n\n1:32:09.200 --> 1:32:10.800\n you don't have to be too creative to think\n\n1:32:10.800 --> 1:32:13.080\n that it could have led to an all out nuclear war,\n\n1:32:13.080 --> 1:32:15.680\n in which case we wouldn't be having this conversation now.\n\n1:32:15.680 --> 1:32:17.600\n What actually took place was\n\n1:32:17.600 --> 1:32:21.040\n they needed three people to approve this.\n\n1:32:21.040 --> 1:32:22.200\n The captain had said yes.\n\n1:32:22.200 --> 1:32:24.120\n There was the Communist Party political officer.\n\n1:32:24.120 --> 1:32:26.000\n He also said, yes, let's do it.\n\n1:32:26.000 --> 1:32:29.040\n And the third man was this guy, Vasily Arkhipov,\n\n1:32:29.040 --> 1:32:29.880\n who said, no.\n\n1:32:29.880 --> 1:32:32.720\n For some reason, he was just more chill than the others\n\n1:32:32.720 --> 1:32:34.240\n and he was the right man at the right time.\n\n1:32:34.240 --> 1:32:38.120\n I don't want us as a species rely on the right person\n\n1:32:38.120 --> 1:32:40.720\n being there at the right time, you know.\n\n1:32:40.720 --> 1:32:42.920\n We tracked down his family\n\n1:32:42.920 --> 1:32:46.320\n living in relative poverty outside Moscow.\n\n1:32:47.320 --> 1:32:48.800\n When he flew his daughter,\n\n1:32:48.800 --> 1:32:52.720\n he had passed away and flew them to London.\n\n1:32:52.720 --> 1:32:54.000\n They had never been to the West even.\n\n1:32:54.000 --> 1:32:57.160\n It was incredibly moving to get to honor them for this.\n\n1:32:57.160 --> 1:32:59.320\n The next year we gave them a medal.\n\n1:32:59.320 --> 1:33:01.800\n The next year we gave this Future Life Award\n\n1:33:01.800 --> 1:33:04.160\n to Stanislav Petrov.\n\n1:33:04.160 --> 1:33:05.000\n Have you heard of him?\n\n1:33:05.000 --> 1:33:05.840\n Yes.\n\n1:33:05.840 --> 1:33:10.000\n So he was in charge of the Soviet early warning station,\n\n1:33:10.000 --> 1:33:12.880\n which was built with Soviet technology\n\n1:33:12.880 --> 1:33:14.760\n and honestly not that reliable.\n\n1:33:14.760 --> 1:33:17.320\n It said that there were five US missiles coming in.\n\n1:33:18.280 --> 1:33:21.440\n Again, if they had launched at that point,\n\n1:33:21.440 --> 1:33:23.440\n we probably wouldn't be having this conversation.\n\n1:33:23.440 --> 1:33:29.440\n He decided based on just mainly gut instinct\n\n1:33:29.440 --> 1:33:32.640\n to just not escalate this.\n\n1:33:32.640 --> 1:33:35.200\n And I'm very glad he wasn't replaced by an AI\n\n1:33:35.200 --> 1:33:37.600\n that was just automatically following orders.\n\n1:33:37.600 --> 1:33:39.840\n And then we gave the third one to Matthew Messelson.\n\n1:33:39.840 --> 1:33:44.360\n Last year, we gave this award to these guys\n\n1:33:44.360 --> 1:33:46.760\n who actually use technology for good,\n\n1:33:46.760 --> 1:33:50.120\n not avoiding something bad, but for something good.\n\n1:33:50.120 --> 1:33:52.120\n The guys who eliminated this disease,\n\n1:33:52.120 --> 1:33:55.440\n it was way worse than COVID that had killed\n\n1:33:55.440 --> 1:33:58.520\n half a billion people in its final century.\n\n1:33:58.520 --> 1:33:59.440\n Smallpox, right?\n\n1:33:59.440 --> 1:34:01.200\n So you mentioned it earlier.\n\n1:34:01.200 --> 1:34:05.320\n COVID on average kills less than 1% of people who get it.\n\n1:34:05.320 --> 1:34:08.240\n Smallpox, about 30%.\n\n1:34:08.240 --> 1:34:14.160\n And they just ultimately, Viktor Zhdanov and Bill Foege,\n\n1:34:14.160 --> 1:34:17.560\n most of my colleagues have never heard of either of them,\n\n1:34:17.560 --> 1:34:22.080\n one American, one Russian, they did this amazing effort\n\n1:34:22.080 --> 1:34:25.200\n not only was Zhdanov able to get the US and the Soviet Union\n\n1:34:25.200 --> 1:34:27.920\n to team up against smallpox during the Cold War,\n\n1:34:27.920 --> 1:34:30.320\n but Bill Foege came up with this ingenious strategy\n\n1:34:30.320 --> 1:34:32.840\n for making it actually go all the way\n\n1:34:32.840 --> 1:34:36.560\n to defeat the disease without funding\n\n1:34:36.560 --> 1:34:37.600\n for vaccinating everyone.\n\n1:34:37.600 --> 1:34:40.040\n And as a result, we haven't had any,\n\n1:34:40.040 --> 1:34:42.680\n we went from 15 million deaths the year\n\n1:34:42.680 --> 1:34:44.280\n I was born in smallpox.\n\n1:34:44.280 --> 1:34:45.640\n So what do we have in COVID now?\n\n1:34:45.640 --> 1:34:47.240\n A little bit short of 2 million, right?\n\n1:34:47.240 --> 1:34:48.120\n Yes.\n\n1:34:48.120 --> 1:34:52.040\n To zero deaths, of course, this year and forever.\n\n1:34:52.040 --> 1:34:53.960\n There have been 200 million people,\n\n1:34:53.960 --> 1:34:57.200\n we estimate, who would have died since then by smallpox\n\n1:34:57.200 --> 1:34:58.080\n had it not been for this.\n\n1:34:58.080 --> 1:35:02.160\n So isn't science awesome when you use it for good?\n\n1:35:02.160 --> 1:35:04.280\n The reason we wanna celebrate these sort of people\n\n1:35:04.280 --> 1:35:05.680\n is to remind them of this.\n\n1:35:05.680 --> 1:35:10.160\n Science is so awesome when you use it for good.\n\n1:35:10.160 --> 1:35:13.520\n And those awards actually, the variety there,\n\n1:35:13.520 --> 1:35:14.920\n it's a very interesting picture.\n\n1:35:14.920 --> 1:35:19.360\n So the first two are looking at,\n\n1:35:19.360 --> 1:35:22.680\n it's kind of exciting to think that these average humans\n\n1:35:22.680 --> 1:35:26.200\n in some sense, they're products of billions\n\n1:35:26.200 --> 1:35:30.200\n of other humans that came before them, evolution,\n\n1:35:30.200 --> 1:35:33.360\n and some little, you said gut,\n\n1:35:33.360 --> 1:35:35.320\n but there's something in there\n\n1:35:35.320 --> 1:35:40.320\n that stopped the annihilation of the human race.\n\n1:35:41.080 --> 1:35:43.040\n And that's a magical thing,\n\n1:35:43.040 --> 1:35:45.240\n but that's like this deeply human thing.\n\n1:35:45.240 --> 1:35:47.400\n And then there's the other aspect\n\n1:35:47.400 --> 1:35:49.800\n where that's also very human,\n\n1:35:49.800 --> 1:35:51.440\n which is to build solution\n\n1:35:51.440 --> 1:35:55.240\n to the existential crises that we're facing,\n\n1:35:55.240 --> 1:35:57.520\n like to build it, to take the responsibility\n\n1:35:57.520 --> 1:36:00.600\n and to come up with different technologies and so on.\n\n1:36:00.600 --> 1:36:04.080\n And both of those are deeply human,\n\n1:36:04.080 --> 1:36:07.400\n the gut and the mind, whatever that is that creates.\n\n1:36:07.400 --> 1:36:08.640\n The best is when they work together.\n\n1:36:08.640 --> 1:36:11.400\n Arkhipov, I wish I could have met him, of course,\n\n1:36:11.400 --> 1:36:13.200\n but he had passed away.\n\n1:36:13.200 --> 1:36:16.720\n He was really a fantastic military officer,\n\n1:36:16.720 --> 1:36:18.680\n combining all the best traits\n\n1:36:18.680 --> 1:36:21.000\n that we in America admire in our military.\n\n1:36:21.000 --> 1:36:23.160\n Because first of all, he was very loyal, of course.\n\n1:36:23.160 --> 1:36:26.280\n He never even told anyone about this during his whole life,\n\n1:36:26.280 --> 1:36:28.440\n even though you think he had some bragging rights, right?\n\n1:36:28.440 --> 1:36:30.000\n But he just was like, this is just business,\n\n1:36:30.000 --> 1:36:31.560\n just doing my job.\n\n1:36:31.560 --> 1:36:34.320\n It only came out later after his death.\n\n1:36:34.320 --> 1:36:37.120\n And second, the reason he did the right thing\n\n1:36:37.120 --> 1:36:39.240\n was not because he was some sort of liberal\n\n1:36:39.240 --> 1:36:43.960\n or some sort of, not because he was just,\n\n1:36:43.960 --> 1:36:47.360\n oh, peace and love.\n\n1:36:47.360 --> 1:36:49.800\n It was partly because he had been the captain\n\n1:36:49.800 --> 1:36:53.080\n on another submarine that had a nuclear reactor meltdown.\n\n1:36:53.080 --> 1:36:58.000\n And it was his heroism that helped contain this.\n\n1:36:58.000 --> 1:36:59.760\n That's why he died of cancer later also.\n\n1:36:59.760 --> 1:37:01.480\n But he had seen many of his crew members die.\n\n1:37:01.480 --> 1:37:04.160\n And I think for him, that gave him this gut feeling\n\n1:37:04.160 --> 1:37:06.200\n that if there's a nuclear war\n\n1:37:06.200 --> 1:37:08.400\n between the US and the Soviet Union,\n\n1:37:08.400 --> 1:37:11.080\n the whole world is gonna go through\n\n1:37:11.080 --> 1:37:13.760\n what I saw my dear crew members suffer through.\n\n1:37:13.760 --> 1:37:15.840\n It wasn't just an abstract thing for him.\n\n1:37:15.840 --> 1:37:17.680\n I think it was real.\n\n1:37:17.680 --> 1:37:20.640\n And second though, not just the gut, the mind, right?\n\n1:37:20.640 --> 1:37:23.960\n He was, for some reason, very levelheaded personality\n\n1:37:23.960 --> 1:37:25.080\n and very smart guy,\n\n1:37:25.960 --> 1:37:29.240\n which is exactly what we want our best fighter pilots\n\n1:37:29.240 --> 1:37:30.120\n to be also, right?\n\n1:37:30.120 --> 1:37:32.880\n I never forget Neil Armstrong when he's landing on the moon\n\n1:37:32.880 --> 1:37:34.560\n and almost running out of gas.\n\n1:37:34.560 --> 1:37:37.440\n And he doesn't even change when they say 30 seconds,\n\n1:37:37.440 --> 1:37:39.680\n he doesn't even change the tone of voice, just keeps going.\n\n1:37:39.680 --> 1:37:41.840\n Arkhipov, I think was just like that.\n\n1:37:41.840 --> 1:37:43.480\n So when the explosions start going off\n\n1:37:43.480 --> 1:37:45.520\n and his captain is screaming and we should nuke them\n\n1:37:45.520 --> 1:37:47.400\n and all, he's like,\n\n1:37:50.960 --> 1:37:54.280\n I don't think the Americans are trying to sink us.\n\n1:37:54.280 --> 1:37:56.480\n I think they're trying to send us a message.\n\n1:37:58.080 --> 1:37:59.200\n That's pretty bad ass.\n\n1:37:59.200 --> 1:38:00.040\n Yes.\n\n1:38:00.040 --> 1:38:02.720\n Coolness, because he said, if they wanted to sink us,\n\n1:38:03.680 --> 1:38:06.920\n and he said, listen, listen, it's alternating\n\n1:38:06.920 --> 1:38:10.160\n one loud explosion on the left, one on the right,\n\n1:38:10.160 --> 1:38:12.120\n one on the left, one on the right.\n\n1:38:12.120 --> 1:38:14.320\n He was the only one who noticed this pattern.\n\n1:38:15.840 --> 1:38:17.880\n And he's like, I think this is,\n\n1:38:17.880 --> 1:38:19.400\n I'm trying to send us a signal\n\n1:38:20.640 --> 1:38:22.840\n that they want it to surface\n\n1:38:22.840 --> 1:38:24.360\n and they're not gonna sink us.\n\n1:38:25.800 --> 1:38:27.400\n And somehow,\n\n1:38:29.320 --> 1:38:32.160\n this is how he then managed it ultimately\n\n1:38:32.160 --> 1:38:34.640\n with his combination of gut\n\n1:38:34.640 --> 1:38:37.960\n and also just cool analytical thinking,\n\n1:38:37.960 --> 1:38:40.120\n was able to deescalate the whole thing.\n\n1:38:40.120 --> 1:38:44.240\n And yeah, so this is some of the best in humanity.\n\n1:38:44.240 --> 1:38:45.880\n I guess coming back to what we talked about earlier,\n\n1:38:45.880 --> 1:38:47.400\n it's the combination of the neural network,\n\n1:38:47.400 --> 1:38:50.960\n the instinctive, with, I'm getting teary up here,\n\n1:38:50.960 --> 1:38:53.240\n getting emotional, but he was just,\n\n1:38:53.240 --> 1:38:54.800\n he is one of my superheroes,\n\n1:38:56.120 --> 1:39:00.440\n having both the heart and the mind combined.\n\n1:39:00.440 --> 1:39:03.760\n And especially in that time, there's something about the,\n\n1:39:03.760 --> 1:39:05.440\n I mean, this is a very, in America,\n\n1:39:05.440 --> 1:39:06.880\n people are used to this kind of idea\n\n1:39:06.880 --> 1:39:11.160\n of being the individual of like on your own thinking.\n\n1:39:12.040 --> 1:39:15.480\n I think under, in the Soviet Union under communism,\n\n1:39:15.480 --> 1:39:17.600\n it's actually much harder to do that.\n\n1:39:17.600 --> 1:39:19.960\n Oh yeah, he didn't even, he even got,\n\n1:39:19.960 --> 1:39:21.840\n he didn't get any accolades either\n\n1:39:21.840 --> 1:39:24.240\n when he came back for this, right?\n\n1:39:24.240 --> 1:39:25.880\n They just wanted to hush the whole thing up.\n\n1:39:25.880 --> 1:39:28.000\n Yeah, there's echoes of that with Chernobyl,\n\n1:39:28.000 --> 1:39:29.160\n there's all kinds of,\n\n1:39:30.920 --> 1:39:34.400\n that's one, that's a really hopeful thing\n\n1:39:34.400 --> 1:39:37.520\n that amidst big centralized powers,\n\n1:39:37.520 --> 1:39:39.920\n whether it's companies or states,\n\n1:39:39.920 --> 1:39:42.480\n there's still the power of the individual\n\n1:39:42.480 --> 1:39:43.880\n to think on their own, to act.\n\n1:39:43.880 --> 1:39:46.880\n But I think we need to think of people like this,\n\n1:39:46.880 --> 1:39:50.160\n not as a panacea we can always count on,\n\n1:39:50.160 --> 1:39:54.080\n but rather as a wake up call.\n\n1:39:55.720 --> 1:39:58.560\n So because of them, because of Arkhipov,\n\n1:39:58.560 --> 1:40:01.320\n we are alive to learn from this lesson,\n\n1:40:01.320 --> 1:40:03.120\n to learn from the fact that we shouldn't keep playing\n\n1:40:03.120 --> 1:40:04.840\n Russian roulette and almost have a nuclear war\n\n1:40:04.840 --> 1:40:06.600\n by mistake now and then,\n\n1:40:06.600 --> 1:40:09.600\n because relying on luck is not a good longterm strategy.\n\n1:40:09.600 --> 1:40:11.360\n If you keep playing Russian roulette over and over again,\n\n1:40:11.360 --> 1:40:13.560\n the probability of surviving just drops exponentially\n\n1:40:13.560 --> 1:40:14.400\n with time.\n\n1:40:14.400 --> 1:40:15.240\n Yeah.\n\n1:40:15.240 --> 1:40:16.680\n And if you have some probability\n\n1:40:16.680 --> 1:40:18.640\n of having an accidental nuke war every year,\n\n1:40:18.640 --> 1:40:21.200\n the probability of not having one also drops exponentially.\n\n1:40:21.200 --> 1:40:22.840\n I think we can do better than that.\n\n1:40:22.840 --> 1:40:26.000\n So I think the message is very clear,\n\n1:40:26.000 --> 1:40:27.840\n once in a while shit happens,\n\n1:40:27.840 --> 1:40:31.320\n and there's a lot of very concrete things we can do\n\n1:40:31.320 --> 1:40:34.920\n to reduce the risk of things like that happening\n\n1:40:34.920 --> 1:40:36.520\n in the first place.\n\n1:40:36.520 --> 1:40:39.600\n On the AI front, if we just link on that for a second.\n\n1:40:39.600 --> 1:40:40.960\n Yeah.\n\n1:40:40.960 --> 1:40:44.120\n So you're friends with, you often talk with Elon Musk\n\n1:40:44.120 --> 1:40:46.680\n throughout history, you've did a lot\n\n1:40:46.680 --> 1:40:48.680\n of interesting things together.\n\n1:40:48.680 --> 1:40:52.280\n He has a set of fears about the future\n\n1:40:52.280 --> 1:40:54.960\n of artificial intelligence, AGI.\n\n1:40:55.840 --> 1:40:59.720\n Do you have a sense, we've already talked about\n\n1:40:59.720 --> 1:41:01.560\n the things we should be worried about with AI,\n\n1:41:01.560 --> 1:41:04.040\n do you have a sense of the shape of his fears\n\n1:41:04.040 --> 1:41:06.880\n in particular about AI,\n\n1:41:06.880 --> 1:41:10.160\n of which subset of what we've talked about,\n\n1:41:10.160 --> 1:41:14.480\n whether it's creating, it's that direction\n\n1:41:14.480 --> 1:41:17.520\n of creating sort of these giant competition systems\n\n1:41:17.520 --> 1:41:19.160\n that are not explainable,\n\n1:41:19.160 --> 1:41:21.800\n they're not intelligible intelligence,\n\n1:41:21.800 --> 1:41:26.720\n or is it the...\n\n1:41:26.720 --> 1:41:28.840\n And then like as a branch of that,\n\n1:41:28.840 --> 1:41:31.840\n is it the manipulation by big corporations of that\n\n1:41:31.840 --> 1:41:35.400\n or individual evil people to use that for destruction\n\n1:41:35.400 --> 1:41:37.480\n or the unintentional consequences?\n\n1:41:37.480 --> 1:41:40.280\n Do you have a sense of where his thinking is on this?\n\n1:41:40.280 --> 1:41:42.440\n From my many conversations with Elon,\n\n1:41:42.440 --> 1:41:47.400\n yeah, I certainly have a model of how he thinks.\n\n1:41:47.400 --> 1:41:49.880\n It's actually very much like the way I think also,\n\n1:41:49.880 --> 1:41:51.080\n I'll elaborate on it a bit.\n\n1:41:51.080 --> 1:41:54.680\n I just wanna push back on when you said evil people,\n\n1:41:54.680 --> 1:41:58.520\n I don't think it's a very helpful concept.\n\n1:41:58.520 --> 1:42:02.320\n Evil people, sometimes people do very, very bad things,\n\n1:42:02.320 --> 1:42:05.440\n but they usually do it because they think it's a good thing\n\n1:42:05.440 --> 1:42:07.760\n because somehow other people had told them\n\n1:42:07.760 --> 1:42:08.640\n that that was a good thing\n\n1:42:08.640 --> 1:42:13.380\n or given them incorrect information or whatever, right?\n\n1:42:15.440 --> 1:42:18.400\n I believe in the fundamental goodness of humanity\n\n1:42:18.400 --> 1:42:21.680\n that if we educate people well\n\n1:42:21.680 --> 1:42:24.240\n and they find out how things really are,\n\n1:42:24.240 --> 1:42:27.240\n people generally wanna do good and be good.\n\n1:42:27.240 --> 1:42:30.360\n Hence the value alignment,\n\n1:42:30.360 --> 1:42:33.660\n as opposed to it's about information, about knowledge,\n\n1:42:33.660 --> 1:42:35.320\n and then once we have that,\n\n1:42:35.320 --> 1:42:39.960\n we'll likely be able to do good\n\n1:42:39.960 --> 1:42:41.600\n in the way that's aligned with everybody else\n\n1:42:41.600 --> 1:42:42.440\n who thinks differently.\n\n1:42:42.440 --> 1:42:44.000\n Yeah, and it's not just the individual people\n\n1:42:44.000 --> 1:42:44.960\n we have to align.\n\n1:42:44.960 --> 1:42:49.600\n So we don't just want people to be educated\n\n1:42:49.600 --> 1:42:51.200\n to know the way things actually are\n\n1:42:51.200 --> 1:42:53.200\n and to treat each other well,\n\n1:42:53.200 --> 1:42:56.280\n but we also need to align other nonhuman entities.\n\n1:42:56.280 --> 1:42:58.560\n We talked about corporations, there has to be institutions\n\n1:42:58.560 --> 1:42:59.960\n so that what they do is actually good\n\n1:42:59.960 --> 1:43:00.880\n for the country they're in\n\n1:43:00.880 --> 1:43:03.480\n and we should align, make sure that what countries do\n\n1:43:03.480 --> 1:43:07.780\n is actually good for the species as a whole, et cetera.\n\n1:43:07.780 --> 1:43:08.680\n Coming back to Elon,\n\n1:43:08.680 --> 1:43:13.600\n yeah, my understanding of how Elon sees this\n\n1:43:13.600 --> 1:43:15.240\n is really quite similar to my own,\n\n1:43:15.240 --> 1:43:18.200\n which is one of the reasons I like him so much\n\n1:43:18.200 --> 1:43:19.320\n and enjoy talking with him so much.\n\n1:43:19.320 --> 1:43:22.960\n I feel he's quite different from most people\n\n1:43:22.960 --> 1:43:27.720\n in that he thinks much more than most people\n\n1:43:27.720 --> 1:43:29.840\n about the really big picture,\n\n1:43:29.840 --> 1:43:32.540\n not just what's gonna happen in the next election cycle,\n\n1:43:32.540 --> 1:43:36.020\n but in millennia, millions and billions of years from now.\n\n1:43:36.840 --> 1:43:39.280\n And when you look in this more cosmic perspective,\n\n1:43:39.280 --> 1:43:43.080\n it's so obvious that we are gazing out into this universe\n\n1:43:43.080 --> 1:43:46.280\n that as far as we can tell is mostly dead\n\n1:43:46.280 --> 1:43:49.800\n with life being almost imperceptibly tiny perturbation,\n\n1:43:49.800 --> 1:43:52.640\n and he sees this enormous opportunity\n\n1:43:52.640 --> 1:43:54.280\n for our universe to come alive,\n\n1:43:54.280 --> 1:43:56.480\n first to become an interplanetary species.\n\n1:43:56.480 --> 1:44:01.480\n Mars is obviously just first stop on this cosmic journey.\n\n1:44:02.120 --> 1:44:05.020\n And precisely because he thinks more long term,\n\n1:44:06.760 --> 1:44:09.560\n it's much more clear to him than to most people\n\n1:44:09.560 --> 1:44:11.340\n that what we do with this Russian roulette thing\n\n1:44:11.340 --> 1:44:15.320\n we keep playing with our nukes is a really poor strategy,\n\n1:44:15.320 --> 1:44:16.720\n really reckless strategy.\n\n1:44:16.720 --> 1:44:18.620\n And also that we're just building\n\n1:44:18.620 --> 1:44:21.640\n these ever more powerful AI systems that we don't understand\n\n1:44:21.640 --> 1:44:23.840\n is also just a really reckless strategy.\n\n1:44:23.840 --> 1:44:26.640\n I feel Elon is very much a humanist\n\n1:44:26.640 --> 1:44:30.880\n in the sense that he wants an awesome future for humanity.\n\n1:44:30.880 --> 1:44:35.880\n He wants it to be us that control the machines\n\n1:44:35.960 --> 1:44:38.280\n rather than the machines that control us.\n\n1:44:39.400 --> 1:44:42.080\n And why shouldn't we insist on that?\n\n1:44:42.080 --> 1:44:44.560\n We're building them after all, right?\n\n1:44:44.560 --> 1:44:46.520\n Why should we build things that just make us\n\n1:44:46.520 --> 1:44:48.440\n into some little cog in the machinery\n\n1:44:48.440 --> 1:44:50.240\n that has no further say in the matter, right?\n\n1:44:50.240 --> 1:44:54.560\n That's not my idea of an inspiring future either.\n\n1:44:54.560 --> 1:44:57.880\n Yeah, if you think on the cosmic scale\n\n1:44:57.880 --> 1:44:59.820\n in terms of both time and space,\n\n1:45:00.720 --> 1:45:02.600\n so much is put into perspective.\n\n1:45:02.600 --> 1:45:04.220\n Yeah.\n\n1:45:04.220 --> 1:45:06.440\n Whenever I have a bad day, that's what I think about.\n\n1:45:06.440 --> 1:45:09.200\n It immediately makes me feel better.\n\n1:45:09.200 --> 1:45:13.520\n It makes me sad that for us individual humans,\n\n1:45:13.520 --> 1:45:16.400\n at least for now, the ride ends too quickly.\n\n1:45:16.400 --> 1:45:20.080\n That we don't get to experience the cosmic scale.\n\n1:45:20.080 --> 1:45:22.280\n Yeah, I mean, I think of our universe sometimes\n\n1:45:22.280 --> 1:45:25.200\n as an organism that has only begun to wake up a tiny bit,\n\n1:45:26.080 --> 1:45:30.120\n just like the very first little glimmers of consciousness\n\n1:45:30.120 --> 1:45:32.120\n you have in the morning when you start coming around.\n\n1:45:32.120 --> 1:45:33.160\n Before the coffee.\n\n1:45:33.160 --> 1:45:35.880\n Before the coffee, even before you get out of bed,\n\n1:45:35.880 --> 1:45:37.280\n before you even open your eyes.\n\n1:45:37.280 --> 1:45:40.320\n You start to wake up a little bit.\n\n1:45:40.320 --> 1:45:41.320\n There's something here.\n\n1:45:43.440 --> 1:45:47.120\n That's very much how I think of where we are.\n\n1:45:47.120 --> 1:45:48.600\n All those galaxies out there,\n\n1:45:48.600 --> 1:45:51.160\n I think they're really beautiful,\n\n1:45:51.160 --> 1:45:52.840\n but why are they beautiful?\n\n1:45:52.840 --> 1:45:55.040\n They're beautiful because conscious entities\n\n1:45:55.040 --> 1:45:57.000\n are actually observing them,\n\n1:45:57.000 --> 1:45:59.080\n experiencing them through our telescopes.\n\n1:46:01.720 --> 1:46:05.880\n I define consciousness as subjective experience,\n\n1:46:05.880 --> 1:46:09.420\n whether it be colors or emotions or sounds.\n\n1:46:09.420 --> 1:46:12.340\n So beauty is an experience.\n\n1:46:12.340 --> 1:46:13.800\n Meaning is an experience.\n\n1:46:13.800 --> 1:46:15.880\n Purpose is an experience.\n\n1:46:15.880 --> 1:46:18.000\n If there was no conscious experience,\n\n1:46:18.000 --> 1:46:20.320\n observing these galaxies, they wouldn't be beautiful.\n\n1:46:20.320 --> 1:46:24.960\n If we do something dumb with advanced AI in the future here\n\n1:46:24.960 --> 1:46:29.360\n and Earth originating, life goes extinct.\n\n1:46:29.360 --> 1:46:30.480\n And that was it for this.\n\n1:46:30.480 --> 1:46:33.560\n If there is nothing else with telescopes in our universe,\n\n1:46:33.560 --> 1:46:36.600\n then it's kind of game over for beauty\n\n1:46:36.600 --> 1:46:38.120\n and meaning and purpose in our whole universe.\n\n1:46:38.120 --> 1:46:39.880\n And I think that would be just such\n\n1:46:39.880 --> 1:46:41.800\n an opportunity lost, frankly.\n\n1:46:41.800 --> 1:46:46.080\n And I think when Elon points this out,\n\n1:46:46.080 --> 1:46:49.640\n he gets very unfairly maligned in the media\n\n1:46:49.640 --> 1:46:52.440\n for all the dumb media bias reasons we talked about.\n\n1:46:52.440 --> 1:46:55.680\n They want to print precisely the things about Elon\n\n1:46:55.680 --> 1:46:58.800\n out of context that are really click baity.\n\n1:46:58.800 --> 1:47:00.440\n He has gotten so much flack\n\n1:47:00.440 --> 1:47:03.420\n for this summoning the demon statement.\n\n1:47:04.720 --> 1:47:07.680\n I happen to know exactly the context\n\n1:47:07.680 --> 1:47:09.720\n because I was in the front row when he gave that talk.\n\n1:47:09.720 --> 1:47:11.280\n It was at MIT, you'll be pleased to know,\n\n1:47:11.280 --> 1:47:13.880\n it was the AeroAstro anniversary.\n\n1:47:13.880 --> 1:47:16.800\n They had Buzz Aldrin there from the moon landing,\n\n1:47:16.800 --> 1:47:19.000\n a whole house, a Kresge auditorium\n\n1:47:19.000 --> 1:47:20.840\n packed with MIT students.\n\n1:47:20.840 --> 1:47:23.920\n And he had this amazing Q&A, it might've gone for an hour.\n\n1:47:23.920 --> 1:47:27.160\n And they talked about rockets and Mars and everything.\n\n1:47:27.160 --> 1:47:29.600\n At the very end, this one student\n\n1:47:29.600 --> 1:47:33.200\n who has actually hit my class asked him, what about AI?\n\n1:47:33.200 --> 1:47:35.240\n Elon makes this one comment\n\n1:47:35.240 --> 1:47:39.440\n and they take this out of context, print it, goes viral.\n\n1:47:39.440 --> 1:47:40.600\n What is it like with AI,\n\n1:47:40.600 --> 1:47:42.920\n we're summoning the demons, something like that.\n\n1:47:42.920 --> 1:47:47.480\n And try to cast him as some sort of doom and gloom dude.\n\n1:47:47.480 --> 1:47:51.960\n You know Elon, he's not the doom and gloom dude.\n\n1:47:51.960 --> 1:47:54.000\n He is such a positive visionary.\n\n1:47:54.000 --> 1:47:55.680\n And the whole reason he warns about this\n\n1:47:55.680 --> 1:47:57.720\n is because he realizes more than most\n\n1:47:57.720 --> 1:47:59.880\n what the opportunity cost is of screwing up.\n\n1:47:59.880 --> 1:48:02.360\n That there is so much awesomeness in the future\n\n1:48:02.360 --> 1:48:05.480\n that we can and our descendants can enjoy\n\n1:48:05.480 --> 1:48:07.760\n if we don't screw up, right?\n\n1:48:07.760 --> 1:48:10.320\n I get so pissed off when people try to cast him\n\n1:48:10.320 --> 1:48:15.320\n as some sort of technophobic Luddite.\n\n1:48:15.320 --> 1:48:18.480\n And at this point, it's kind of ludicrous\n\n1:48:18.480 --> 1:48:21.640\n when I hear people say that people who worry about\n\n1:48:21.640 --> 1:48:24.560\n artificial general intelligence are Luddites\n\n1:48:24.560 --> 1:48:27.000\n because of course, if you look more closely,\n\n1:48:27.000 --> 1:48:32.000\n you have some of the most outspoken people making warnings\n\n1:48:32.920 --> 1:48:35.640\n are people like Professor Stuart Russell from Berkeley\n\n1:48:35.640 --> 1:48:38.360\n who's written the bestselling AI textbook, you know.\n\n1:48:38.360 --> 1:48:43.360\n So claiming that he's a Luddite who doesn't understand AI\n\n1:48:43.360 --> 1:48:46.520\n is the joke is really on the people who said it.\n\n1:48:46.520 --> 1:48:48.200\n But I think more broadly,\n\n1:48:48.200 --> 1:48:50.800\n this message is really not sunk in at all.\n\n1:48:50.800 --> 1:48:52.640\n What it is that people worry about,\n\n1:48:52.640 --> 1:48:56.680\n they think that Elon and Stuart Russell and others\n\n1:48:56.680 --> 1:49:01.680\n are worried about the dancing robots picking up an AR 15\n\n1:49:02.280 --> 1:49:04.360\n and going on a rampage, right?\n\n1:49:04.360 --> 1:49:08.440\n They think they're worried about robots turning evil.\n\n1:49:08.440 --> 1:49:10.360\n They're not, I'm not.\n\n1:49:10.360 --> 1:49:15.360\n The risk is not malice, it's competence.\n\n1:49:15.880 --> 1:49:17.560\n The risk is just that we build some systems\n\n1:49:17.560 --> 1:49:18.760\n that are incredibly competent,\n\n1:49:18.760 --> 1:49:20.040\n which means they're always gonna get\n\n1:49:20.040 --> 1:49:22.000\n their goals accomplished,\n\n1:49:22.000 --> 1:49:24.080\n even if they clash with our goals.\n\n1:49:24.080 --> 1:49:25.040\n That's the risk.\n\n1:49:25.920 --> 1:49:30.920\n Why did we humans drive the West African black rhino extinct?\n\n1:49:30.920 --> 1:49:34.840\n Is it because we're malicious, evil rhinoceros haters?\n\n1:49:34.840 --> 1:49:38.000\n No, it's just because our goals didn't align\n\n1:49:38.000 --> 1:49:39.240\n with the goals of those rhinos\n\n1:49:39.240 --> 1:49:41.440\n and tough luck for the rhinos, you know.\n\n1:49:42.360 --> 1:49:46.720\n So the point is just we don't wanna put ourselves\n\n1:49:46.720 --> 1:49:48.120\n in the position of those rhinos\n\n1:49:48.120 --> 1:49:51.240\n creating something more powerful than us\n\n1:49:51.240 --> 1:49:53.880\n if we haven't first figured out how to align the goals.\n\n1:49:53.880 --> 1:49:54.920\n And I am optimistic.\n\n1:49:54.920 --> 1:49:56.880\n I think we could do it if we worked really hard on it,\n\n1:49:56.880 --> 1:49:59.200\n because I spent a lot of time\n\n1:49:59.200 --> 1:50:01.800\n around intelligent entities that were more intelligent\n\n1:50:01.800 --> 1:50:04.040\n than me, my mom and my dad.\n\n1:50:05.960 --> 1:50:07.560\n And I was little and that was fine\n\n1:50:07.560 --> 1:50:09.160\n because their goals were actually aligned\n\n1:50:09.160 --> 1:50:10.200\n with mine quite well.\n\n1:50:11.280 --> 1:50:15.440\n But we've seen today many examples of where the goals\n\n1:50:15.440 --> 1:50:17.200\n of our powerful systems are not so aligned.\n\n1:50:17.200 --> 1:50:22.200\n So those click through optimization algorithms\n\n1:50:22.960 --> 1:50:24.560\n that are polarized social media, right?\n\n1:50:24.560 --> 1:50:26.160\n They were actually pretty poorly aligned\n\n1:50:26.160 --> 1:50:28.760\n with what was good for democracy, it turned out.\n\n1:50:28.760 --> 1:50:31.520\n And again, almost all problems we've had\n\n1:50:31.520 --> 1:50:33.640\n in the machine learning again came so far,\n\n1:50:33.640 --> 1:50:35.520\n not from malice, but from poor alignment.\n\n1:50:35.520 --> 1:50:38.240\n And that's exactly why that's why we should be concerned\n\n1:50:38.240 --> 1:50:39.320\n about it in the future.\n\n1:50:39.320 --> 1:50:43.240\n Do you think it's possible that with systems\n\n1:50:43.240 --> 1:50:47.320\n like Neuralink and brain computer interfaces,\n\n1:50:47.320 --> 1:50:49.280\n you know, again, thinking of the cosmic scale,\n\n1:50:49.280 --> 1:50:52.600\n Elon's talked about this, but others have as well\n\n1:50:52.600 --> 1:50:57.240\n throughout history of figuring out how the exact mechanism\n\n1:50:57.240 --> 1:51:00.000\n of how to achieve that kind of alignment.\n\n1:51:00.000 --> 1:51:03.160\n So one of them is having a symbiosis with AI,\n\n1:51:03.160 --> 1:51:05.560\n which is like coming up with clever ways\n\n1:51:05.560 --> 1:51:10.360\n where we're like stuck together in this weird relationship,\n\n1:51:10.360 --> 1:51:14.200\n whether it's biological or in some kind of other way.\n\n1:51:14.200 --> 1:51:17.240\n Do you think that's a possibility\n\n1:51:17.240 --> 1:51:19.200\n of having that kind of symbiosis?\n\n1:51:19.200 --> 1:51:20.960\n Or do we wanna instead kind of focus\n\n1:51:20.960 --> 1:51:27.960\n on this distinct entities of us humans talking\n\n1:51:28.200 --> 1:51:31.720\n to these intelligible, self doubting AIs,\n\n1:51:31.720 --> 1:51:33.600\n maybe like Stuart Russell thinks about it,\n\n1:51:33.600 --> 1:51:37.640\n like we're self doubting and full of uncertainty\n\n1:51:37.640 --> 1:51:39.760\n and our AI systems are full of uncertainty.\n\n1:51:39.760 --> 1:51:41.520\n We communicate back and forth\n\n1:51:41.520 --> 1:51:43.760\n and in that way achieve symbiosis.\n\n1:51:44.680 --> 1:51:46.200\n I honestly don't know.\n\n1:51:46.200 --> 1:51:48.600\n I would say that because we don't know for sure\n\n1:51:48.600 --> 1:51:52.200\n what if any of our, which of any of our ideas will work.\n\n1:51:52.200 --> 1:51:55.200\n But we do know that if we don't,\n\n1:51:55.200 --> 1:51:56.880\n I'm pretty convinced that if we don't get any\n\n1:51:56.880 --> 1:51:59.840\n of these things to work and just barge ahead,\n\n1:51:59.840 --> 1:52:01.440\n then our species is, you know,\n\n1:52:01.440 --> 1:52:03.720\n probably gonna go extinct this century.\n\n1:52:03.720 --> 1:52:04.600\n I think it's...\n\n1:52:04.600 --> 1:52:06.320\n This century, you think like,\n\n1:52:06.320 --> 1:52:09.720\n you think we're facing this crisis\n\n1:52:09.720 --> 1:52:11.320\n is a 21st century crisis.\n\n1:52:11.320 --> 1:52:13.520\n Like this century will be remembered.\n\n1:52:13.520 --> 1:52:18.520\n But on a hard drive and a hard drive somewhere\n\n1:52:18.720 --> 1:52:22.280\n or maybe by future generations is like,\n\n1:52:22.280 --> 1:52:26.240\n like there'll be future Future of Life Institute awards\n\n1:52:26.240 --> 1:52:30.640\n for people that have done something about AI.\n\n1:52:30.640 --> 1:52:31.880\n It could also end even worse,\n\n1:52:31.880 --> 1:52:33.720\n whether we're not superseded\n\n1:52:33.720 --> 1:52:35.280\n by leaving any AI behind either.\n\n1:52:35.280 --> 1:52:37.040\n We just totally wipe out, you know,\n\n1:52:37.040 --> 1:52:38.480\n like on Easter Island.\n\n1:52:38.480 --> 1:52:39.880\n Our century is long.\n\n1:52:39.880 --> 1:52:44.280\n You know, there are still 79 years left of it, right?\n\n1:52:44.280 --> 1:52:47.680\n Think about how far we've come just in the last 30 years.\n\n1:52:47.680 --> 1:52:52.680\n So we can talk more about what might go wrong,\n\n1:52:53.080 --> 1:52:54.600\n but you asked me this really good question\n\n1:52:54.600 --> 1:52:55.800\n about what's the best strategy.\n\n1:52:55.800 --> 1:52:59.800\n Is it Neuralink or Russell's approach or whatever?\n\n1:52:59.800 --> 1:53:04.800\n I think, you know, when we did the Manhattan project,\n\n1:53:05.480 --> 1:53:08.480\n we didn't know if any of our four ideas\n\n1:53:08.480 --> 1:53:11.760\n for enriching uranium and getting out the uranium 235\n\n1:53:11.760 --> 1:53:12.880\n were gonna work.\n\n1:53:12.880 --> 1:53:14.800\n But we felt this was really important\n\n1:53:14.800 --> 1:53:16.680\n to get it before Hitler did.\n\n1:53:16.680 --> 1:53:17.520\n So, you know what we did?\n\n1:53:17.520 --> 1:53:19.520\n We tried all four of them.\n\n1:53:19.520 --> 1:53:21.960\n Here, I think it's analogous\n\n1:53:21.960 --> 1:53:24.360\n where there's the greatest threat\n\n1:53:24.360 --> 1:53:25.920\n that's ever faced our species.\n\n1:53:25.920 --> 1:53:29.240\n And of course, US national security by implication.\n\n1:53:29.240 --> 1:53:31.480\n We don't know if we don't have any method\n\n1:53:31.480 --> 1:53:34.680\n that's guaranteed to work, but we have a lot of ideas.\n\n1:53:34.680 --> 1:53:35.960\n So we should invest pretty heavily\n\n1:53:35.960 --> 1:53:38.040\n in pursuing all of them with an open mind\n\n1:53:38.040 --> 1:53:40.560\n and hope that one of them at least works.\n\n1:53:40.560 --> 1:53:45.360\n These are, the good news is the century is long,\n\n1:53:45.360 --> 1:53:47.880\n and it might take decades\n\n1:53:47.880 --> 1:53:50.160\n until we have artificial general intelligence.\n\n1:53:50.160 --> 1:53:52.760\n So we have some time hopefully,\n\n1:53:52.760 --> 1:53:55.240\n but it takes a long time to solve\n\n1:53:55.240 --> 1:53:57.120\n these very, very difficult problems.\n\n1:53:57.120 --> 1:53:58.080\n It's gonna actually be the,\n\n1:53:58.080 --> 1:53:59.160\n it's the most difficult problem\n\n1:53:59.160 --> 1:54:01.320\n we were ever trying to solve as a species.\n\n1:54:01.320 --> 1:54:03.400\n So we have to start now.\n\n1:54:03.400 --> 1:54:05.840\n So we don't have, rather than begin thinking about it\n\n1:54:05.840 --> 1:54:08.720\n the night before some people who've had too much Red Bull\n\n1:54:08.720 --> 1:54:09.560\n switch it on.\n\n1:54:09.560 --> 1:54:11.840\n And we have to, coming back to your question,\n\n1:54:11.840 --> 1:54:14.240\n we have to pursue all of these different avenues and see.\n\n1:54:14.240 --> 1:54:16.800\n If you were my investment advisor\n\n1:54:16.800 --> 1:54:19.920\n and I was trying to invest in the future,\n\n1:54:19.920 --> 1:54:22.120\n how do you think the human species\n\n1:54:23.040 --> 1:54:27.560\n is most likely to destroy itself in the century?\n\n1:54:29.440 --> 1:54:32.120\n Yeah, so if the crises,\n\n1:54:32.120 --> 1:54:34.680\n many of the crises we're facing are really before us\n\n1:54:34.680 --> 1:54:37.160\n within the next hundred years,\n\n1:54:37.160 --> 1:54:41.200\n how do we make explicit,\n\n1:54:42.320 --> 1:54:46.640\n make known the unknowns and solve those problems\n\n1:54:46.640 --> 1:54:48.200\n to avoid the biggest,\n\n1:54:49.560 --> 1:54:51.920\n starting with the biggest existential crisis?\n\n1:54:51.920 --> 1:54:53.160\n So as your investment advisor,\n\n1:54:53.160 --> 1:54:55.680\n how are you planning to make money on us\n\n1:54:55.680 --> 1:54:56.640\n destroying ourselves?\n\n1:54:56.640 --> 1:54:57.480\n I have to ask.\n\n1:54:57.480 --> 1:54:58.320\n I don't know.\n\n1:54:58.320 --> 1:55:00.040\n It might be the Russian origins.\n\n1:55:01.080 --> 1:55:02.840\n Somehow it's involved.\n\n1:55:02.840 --> 1:55:04.760\n At the micro level of detailed strategies,\n\n1:55:04.760 --> 1:55:06.680\n of course, these are unsolved problems.\n\n1:55:08.640 --> 1:55:09.680\n For AI alignment,\n\n1:55:09.680 --> 1:55:12.240\n we can break it into three sub problems\n\n1:55:12.240 --> 1:55:13.480\n that are all unsolved.\n\n1:55:13.480 --> 1:55:16.720\n I think you want first to make machines\n\n1:55:16.720 --> 1:55:18.400\n understand our goals,\n\n1:55:18.400 --> 1:55:23.400\n then adopt our goals and then retain our goals.\n\n1:55:23.600 --> 1:55:26.160\n So to hit on all three real quickly.\n\n1:55:27.400 --> 1:55:31.080\n The problem when Andreas Lubitz told his autopilot\n\n1:55:31.080 --> 1:55:34.320\n to fly into the Alps was that the computer\n\n1:55:34.320 --> 1:55:39.040\n didn't even understand anything about his goals.\n\n1:55:39.040 --> 1:55:40.520\n It was too dumb.\n\n1:55:40.520 --> 1:55:42.720\n It could have understood actually,\n\n1:55:42.720 --> 1:55:45.280\n but you would have had to put some effort in\n\n1:55:45.280 --> 1:55:48.880\n as a systems designer to don't fly into mountains.\n\n1:55:48.880 --> 1:55:49.920\n So that's the first challenge.\n\n1:55:49.920 --> 1:55:54.480\n How do you program into computers human values,\n\n1:55:54.480 --> 1:55:55.320\n human goals?\n\n1:55:56.240 --> 1:55:58.280\n We can start rather than saying,\n\n1:55:58.280 --> 1:55:59.120\n oh, it's so hard.\n\n1:55:59.120 --> 1:56:01.400\n We should start with the simple stuff, as I said,\n\n1:56:02.400 --> 1:56:04.120\n self driving cars, airplanes,\n\n1:56:04.120 --> 1:56:07.240\n just put in all the goals that we all agree on already,\n\n1:56:07.240 --> 1:56:10.560\n and then have a habit of whenever machines get smarter\n\n1:56:10.560 --> 1:56:14.280\n so they can understand one level higher goals,\n\n1:56:15.480 --> 1:56:16.960\n put them into.\n\n1:56:16.960 --> 1:56:20.840\n The second challenge is getting them to adopt the goals.\n\n1:56:20.840 --> 1:56:22.320\n It's easy for situations like that\n\n1:56:22.320 --> 1:56:23.280\n where you just program it in,\n\n1:56:23.280 --> 1:56:26.040\n but when you have self learning systems like children,\n\n1:56:26.040 --> 1:56:29.320\n you know, any parent knows\n\n1:56:29.320 --> 1:56:33.440\n that there was a difference between getting our kids\n\n1:56:33.440 --> 1:56:34.840\n to understand what we want them to do\n\n1:56:34.840 --> 1:56:37.600\n and to actually adopt our goals, right?\n\n1:56:37.600 --> 1:56:40.040\n With humans, with children, fortunately,\n\n1:56:40.040 --> 1:56:44.000\n they go through this phase.\n\n1:56:44.000 --> 1:56:45.480\n First, they're too dumb to understand\n\n1:56:45.480 --> 1:56:46.800\n what we want our goals are.\n\n1:56:46.800 --> 1:56:50.360\n And then they have this period of some years\n\n1:56:50.360 --> 1:56:52.080\n when they're both smart enough to understand them\n\n1:56:52.080 --> 1:56:53.520\n and malleable enough that we have a chance\n\n1:56:53.520 --> 1:56:55.400\n to raise them well.\n\n1:56:55.400 --> 1:56:59.160\n And then they become teenagers kind of too late.\n\n1:56:59.160 --> 1:57:01.360\n But we have this window with machines,\n\n1:57:01.360 --> 1:57:04.120\n the challenges, the intelligence might grow so fast\n\n1:57:04.120 --> 1:57:05.880\n that that window is pretty short.\n\n1:57:06.800 --> 1:57:08.480\n So that's a research problem.\n\n1:57:08.480 --> 1:57:11.320\n The third one is how do you make sure they keep the goals\n\n1:57:11.320 --> 1:57:13.680\n if they keep learning more and getting smarter?\n\n1:57:14.520 --> 1:57:17.360\n Many sci fi movies are about how you have something\n\n1:57:17.360 --> 1:57:18.520\n in which initially was aligned,\n\n1:57:18.520 --> 1:57:20.320\n but then things kind of go off keel.\n\n1:57:20.320 --> 1:57:24.680\n And, you know, my kids were very, very excited\n\n1:57:24.680 --> 1:57:27.360\n about their Legos when they were little.\n\n1:57:27.360 --> 1:57:29.800\n Now they're just gathering dust in the basement.\n\n1:57:29.800 --> 1:57:32.560\n If we create machines that are really on board\n\n1:57:32.560 --> 1:57:34.320\n with the goal of taking care of humanity,\n\n1:57:34.320 --> 1:57:36.080\n we don't want them to get as bored with us\n\n1:57:36.080 --> 1:57:39.480\n as my kids got with Legos.\n\n1:57:39.480 --> 1:57:41.920\n So this is another research challenge.\n\n1:57:41.920 --> 1:57:43.400\n How can you make some sort of recursively\n\n1:57:43.400 --> 1:57:47.440\n self improving system retain certain basic goals?\n\n1:57:47.440 --> 1:57:50.880\n That said, a lot of adult people still play with Legos.\n\n1:57:50.880 --> 1:57:52.720\n So maybe we succeeded with the Legos.\n\n1:57:52.720 --> 1:57:55.320\n Maybe, I like your optimism.\n\n1:57:55.320 --> 1:57:56.160\n But above all.\n\n1:57:56.160 --> 1:57:59.120\n So not all AI systems have to maintain the goals, right?\n\n1:57:59.120 --> 1:58:00.200\n Just some fraction.\n\n1:58:00.200 --> 1:58:04.920\n Yeah, so there's a lot of talented AI researchers now\n\n1:58:04.920 --> 1:58:07.280\n who have heard of this and want to work on it.\n\n1:58:07.280 --> 1:58:08.880\n Not so much funding for it yet.\n\n1:58:10.960 --> 1:58:13.880\n Of the billions that go into building AI more powerful,\n\n1:58:14.800 --> 1:58:16.240\n it's only a minuscule fraction\n\n1:58:16.240 --> 1:58:18.280\n so far going into this safety research.\n\n1:58:18.280 --> 1:58:20.880\n My attitude is generally we should not try to slow down\n\n1:58:20.880 --> 1:58:22.840\n the technology, but we should greatly accelerate\n\n1:58:22.840 --> 1:58:25.880\n the investment in this sort of safety research.\n\n1:58:25.880 --> 1:58:29.320\n And also, this was very embarrassing last year,\n\n1:58:29.320 --> 1:58:31.840\n but the NSF decided to give out\n\n1:58:31.840 --> 1:58:33.680\n six of these big institutes.\n\n1:58:33.680 --> 1:58:37.040\n We got one of them for AI and science, you asked me about.\n\n1:58:37.040 --> 1:58:39.640\n Another one was supposed to be for AI safety research.\n\n1:58:40.720 --> 1:58:43.520\n And they gave it to people studying oceans\n\n1:58:43.520 --> 1:58:44.760\n and climate and stuff.\n\n1:58:46.920 --> 1:58:49.320\n So I'm all for studying oceans and climates,\n\n1:58:49.320 --> 1:58:51.120\n but we need to actually have some money\n\n1:58:51.120 --> 1:58:53.360\n that actually goes into AI safety research also\n\n1:58:53.360 --> 1:58:55.400\n and doesn't just get grabbed by whatever.\n\n1:58:56.400 --> 1:58:57.960\n That's a fantastic investment.\n\n1:58:57.960 --> 1:59:00.480\n And then at the higher level, you asked this question,\n\n1:59:00.480 --> 1:59:02.680\n okay, what can we do?\n\n1:59:02.680 --> 1:59:04.000\n What are the biggest risks?\n\n1:59:05.240 --> 1:59:08.760\n I think we cannot just consider this\n\n1:59:08.760 --> 1:59:11.000\n to be only a technical problem.\n\n1:59:11.000 --> 1:59:13.640\n Again, because if you solve only the technical problem,\n\n1:59:13.640 --> 1:59:14.680\n can I play with your robot?\n\n1:59:14.680 --> 1:59:15.520\n Yes, please.\n\n1:59:15.520 --> 1:59:20.520\n If we can get our machines to just blindly obey\n\n1:59:20.560 --> 1:59:21.880\n the orders we give them,\n\n1:59:22.760 --> 1:59:25.280\n so we can always trust that it will do what we want.\n\n1:59:26.160 --> 1:59:28.440\n That might be great for the owner of the robot.\n\n1:59:28.440 --> 1:59:31.400\n That might not be so great for the rest of humanity\n\n1:59:31.400 --> 1:59:34.080\n if that person is that least favorite world leader\n\n1:59:34.080 --> 1:59:35.680\n or whatever you imagine, right?\n\n1:59:36.600 --> 1:59:39.200\n So we have to also take a look at the,\n\n1:59:39.200 --> 1:59:41.960\n apply alignment, not just to machines,\n\n1:59:41.960 --> 1:59:44.560\n but to all the other powerful structures.\n\n1:59:44.560 --> 1:59:45.720\n That's why it's so important\n\n1:59:45.720 --> 1:59:47.040\n to strengthen our democracy again,\n\n1:59:47.040 --> 1:59:48.520\n as I said, to have institutions,\n\n1:59:48.520 --> 1:59:51.440\n make sure that the playing field is not rigged\n\n1:59:51.440 --> 1:59:54.800\n so that corporations are given the right incentives\n\n1:59:54.800 --> 1:59:57.240\n to do the things that both make profit\n\n1:59:57.240 --> 1:59:58.880\n and are good for people,\n\n1:59:58.880 --> 2:00:00.920\n to make sure that countries have incentives\n\n2:00:00.920 --> 2:00:03.320\n to do things that are both good for their people\n\n2:00:03.320 --> 2:00:06.840\n and don't screw up the rest of the world.\n\n2:00:06.840 --> 2:00:10.280\n And this is not just something for AI nerds to geek out on.\n\n2:00:10.280 --> 2:00:13.080\n This is an interesting challenge for political scientists,\n\n2:00:13.080 --> 2:00:16.800\n economists, and so many other thinkers.\n\n2:00:16.800 --> 2:00:18.680\n So one of the magical things\n\n2:00:18.680 --> 2:00:23.680\n that perhaps makes this earth quite unique\n\n2:00:25.240 --> 2:00:28.840\n is that it's home to conscious beings.\n\n2:00:28.840 --> 2:00:30.400\n So you mentioned consciousness.\n\n2:00:31.640 --> 2:00:35.000\n Perhaps as a small aside,\n\n2:00:35.000 --> 2:00:36.720\n because we didn't really get specific\n\n2:00:36.720 --> 2:00:39.440\n to how we might do the alignment.\n\n2:00:39.440 --> 2:00:40.280\n Like you said,\n\n2:00:40.280 --> 2:00:41.840\n is there just a really important research problem,\n\n2:00:41.840 --> 2:00:44.720\n but do you think engineering consciousness\n\n2:00:44.720 --> 2:00:49.720\n into AI systems is a possibility,\n\n2:00:49.880 --> 2:00:53.040\n is something that we might one day do,\n\n2:00:53.040 --> 2:00:56.800\n or is there something fundamental to consciousness\n\n2:00:56.800 --> 2:00:59.880\n that is, is there something about consciousness\n\n2:00:59.880 --> 2:01:02.360\n that is fundamental to humans and humans only?\n\n2:01:03.400 --> 2:01:04.640\n I think it's possible.\n\n2:01:04.640 --> 2:01:08.320\n I think both consciousness and intelligence\n\n2:01:08.320 --> 2:01:10.760\n are information processing.\n\n2:01:10.760 --> 2:01:13.480\n Certain types of information processing.\n\n2:01:13.480 --> 2:01:15.160\n And that fundamentally,\n\n2:01:15.160 --> 2:01:17.320\n it doesn't matter whether the information is processed\n\n2:01:17.320 --> 2:01:21.280\n by carbon atoms in the neurons and brains\n\n2:01:21.280 --> 2:01:25.920\n or by silicon atoms and so on in our technology.\n\n2:01:27.280 --> 2:01:28.280\n Some people disagree.\n\n2:01:28.280 --> 2:01:30.240\n This is what I think as a physicist.\n\n2:01:32.960 --> 2:01:34.960\n That consciousness is the same kind of,\n\n2:01:34.960 --> 2:01:37.720\n you said consciousness is information processing.\n\n2:01:37.720 --> 2:01:42.720\n So meaning, I think you had a quote of something like\n\n2:01:43.000 --> 2:01:47.760\n it's information knowing itself, that kind of thing.\n\n2:01:47.760 --> 2:01:49.280\n I think consciousness is, yeah,\n\n2:01:49.280 --> 2:01:51.960\n is the way information feels when it's being processed.\n\n2:01:51.960 --> 2:01:53.520\n One's being put in complex ways.\n\n2:01:53.520 --> 2:01:56.120\n We don't know exactly what those complex ways are.\n\n2:01:56.120 --> 2:01:59.240\n It's clear that most of the information processing\n\n2:01:59.240 --> 2:02:01.720\n in our brains does not create an experience.\n\n2:02:01.720 --> 2:02:03.600\n We're not even aware of it, right?\n\n2:02:03.600 --> 2:02:05.520\n Like for example,\n\n2:02:05.520 --> 2:02:07.880\n you're not aware of your heartbeat regulation right now,\n\n2:02:07.880 --> 2:02:10.600\n even though it's clearly being done by your body, right?\n\n2:02:10.600 --> 2:02:12.120\n It's just kind of doing its own thing.\n\n2:02:12.120 --> 2:02:13.680\n When you go jogging,\n\n2:02:13.680 --> 2:02:15.280\n there's a lot of complicated stuff\n\n2:02:15.280 --> 2:02:18.720\n about how you put your foot down and we know it's hard.\n\n2:02:18.720 --> 2:02:20.560\n That's why robots used to fall over so much,\n\n2:02:20.560 --> 2:02:22.720\n but you're mostly unaware about it.\n\n2:02:22.720 --> 2:02:25.760\n Your brain, your CEO consciousness module\n\n2:02:25.760 --> 2:02:26.600\n just sends an email,\n\n2:02:26.600 --> 2:02:29.160\n hey, I'm gonna keep jogging along this path.\n\n2:02:29.160 --> 2:02:31.560\n The rest is on autopilot, right?\n\n2:02:31.560 --> 2:02:33.200\n So most of it is not conscious,\n\n2:02:33.200 --> 2:02:36.640\n but somehow there is some of the information processing,\n\n2:02:36.640 --> 2:02:41.640\n which is we don't know what exactly.\n\n2:02:41.680 --> 2:02:44.120\n I think this is a science problem\n\n2:02:44.120 --> 2:02:47.680\n that I hope one day we'll have some equation for\n\n2:02:47.680 --> 2:02:49.080\n or something so we can be able to build\n\n2:02:49.080 --> 2:02:51.040\n a consciousness detector and say, yeah,\n\n2:02:51.040 --> 2:02:53.920\n here there is some consciousness, here there's not.\n\n2:02:53.920 --> 2:02:56.640\n Oh, don't boil that lobster because it's feeling pain\n\n2:02:56.640 --> 2:02:59.880\n or it's okay because it's not feeling pain.\n\n2:02:59.880 --> 2:03:03.440\n Right now we treat this as sort of just metaphysics,\n\n2:03:03.440 --> 2:03:06.920\n but it would be very useful in emergency rooms\n\n2:03:06.920 --> 2:03:09.760\n to know if a patient has locked in syndrome\n\n2:03:09.760 --> 2:03:14.560\n and is conscious or if they are actually just out.\n\n2:03:14.560 --> 2:03:17.720\n And in the future, if you build a very, very intelligent\n\n2:03:17.720 --> 2:03:20.120\n helper robot to take care of you,\n\n2:03:20.120 --> 2:03:21.480\n I think you'd like to know\n\n2:03:21.480 --> 2:03:24.120\n if you should feel guilty about shutting it down\n\n2:03:24.120 --> 2:03:27.080\n or if it's just like a zombie going through the motions\n\n2:03:27.080 --> 2:03:29.720\n like a fancy tape recorder, right?\n\n2:03:29.720 --> 2:03:32.800\n And once we can make progress\n\n2:03:32.800 --> 2:03:34.040\n on the science of consciousness\n\n2:03:34.040 --> 2:03:38.320\n and figure out what is conscious and what isn't,\n\n2:03:38.320 --> 2:03:43.320\n then assuming we want to create positive experiences\n\n2:03:45.960 --> 2:03:48.880\n and not suffering, we'll probably choose to build\n\n2:03:48.880 --> 2:03:51.760\n some machines that are deliberately unconscious\n\n2:03:51.760 --> 2:03:56.760\n that do incredibly boring, repetitive jobs\n\n2:03:56.760 --> 2:03:59.680\n in an iron mine somewhere or whatever.\n\n2:03:59.680 --> 2:04:03.120\n And maybe we'll choose to create helper robots\n\n2:04:03.120 --> 2:04:05.360\n for the elderly that are conscious\n\n2:04:05.360 --> 2:04:07.080\n so that people don't just feel creeped out\n\n2:04:07.080 --> 2:04:09.280\n that the robot is just faking it\n\n2:04:10.160 --> 2:04:12.160\n when it acts like it's sad or happy.\n\n2:04:12.160 --> 2:04:13.440\n Like you said, elderly,\n\n2:04:13.440 --> 2:04:16.920\n I think everybody gets pretty deeply lonely in this world.\n\n2:04:16.920 --> 2:04:19.640\n And so there's a place I think for everybody\n\n2:04:19.640 --> 2:04:21.640\n to have a connection with conscious beings,\n\n2:04:21.640 --> 2:04:24.400\n whether they're human or otherwise.\n\n2:04:24.400 --> 2:04:26.920\n But I know for sure that I would,\n\n2:04:26.920 --> 2:04:29.960\n if I had a robot, if I was gonna develop any kind\n\n2:04:29.960 --> 2:04:32.760\n of personal emotional connection with it,\n\n2:04:32.760 --> 2:04:33.840\n I would be very creeped out\n\n2:04:33.840 --> 2:04:35.280\n if I knew it in an intellectual level\n\n2:04:35.280 --> 2:04:36.840\n that the whole thing was just a fraud.\n\n2:04:36.840 --> 2:04:41.840\n Now today you can buy a little talking doll for a kid\n\n2:04:43.000 --> 2:04:46.120\n which will say things and the little child will often think\n\n2:04:46.120 --> 2:04:47.840\n that this is actually conscious\n\n2:04:47.840 --> 2:04:50.440\n and even real secrets to it that then go on the internet\n\n2:04:50.440 --> 2:04:52.520\n and with lots of the creepy repercussions.\n\n2:04:52.520 --> 2:04:57.520\n I would not wanna be just hacked and tricked like this.\n\n2:04:58.040 --> 2:05:01.560\n If I was gonna be developing real emotional connections\n\n2:05:01.560 --> 2:05:04.200\n with the robot, I would wanna know\n\n2:05:04.200 --> 2:05:05.440\n that this is actually real.\n\n2:05:05.440 --> 2:05:08.080\n It's acting conscious, acting happy\n\n2:05:08.080 --> 2:05:09.880\n because it actually feels it.\n\n2:05:09.880 --> 2:05:11.400\n And I think this is not sci fi.\n\n2:05:11.400 --> 2:05:15.560\n I think it's possible to measure, to come up with tools.\n\n2:05:15.560 --> 2:05:17.560\n After we understand the science of consciousness,\n\n2:05:17.560 --> 2:05:19.760\n you're saying we'll be able to come up with tools\n\n2:05:19.760 --> 2:05:21.400\n that can measure consciousness\n\n2:05:21.400 --> 2:05:25.120\n and definitively say like this thing is experiencing\n\n2:05:25.120 --> 2:05:27.360\n the things it says it's experiencing.\n\n2:05:27.360 --> 2:05:28.320\n Kind of by definition.\n\n2:05:28.320 --> 2:05:31.560\n If it is a physical phenomenon, information processing\n\n2:05:31.560 --> 2:05:34.040\n and we know that some information processing is conscious\n\n2:05:34.040 --> 2:05:36.000\n and some isn't, well, then there is something there\n\n2:05:36.000 --> 2:05:38.040\n to be discovered with the methods of science.\n\n2:05:38.040 --> 2:05:41.120\n Giulio Tononi has stuck his neck out the farthest\n\n2:05:41.120 --> 2:05:43.640\n and written down some equations for a theory.\n\n2:05:43.640 --> 2:05:45.680\n Maybe that's right, maybe it's wrong.\n\n2:05:45.680 --> 2:05:46.920\n We certainly don't know.\n\n2:05:46.920 --> 2:05:50.760\n But I applaud that kind of efforts to sort of take this,\n\n2:05:50.760 --> 2:05:53.960\n say this is not just something that philosophers\n\n2:05:53.960 --> 2:05:56.320\n can have beer and muse about,\n\n2:05:56.320 --> 2:05:58.720\n but something we can measure and study.\n\n2:05:58.720 --> 2:06:00.560\n And coming, bringing that back to us,\n\n2:06:00.560 --> 2:06:03.000\n I think what we would probably choose to do, as I said,\n\n2:06:03.000 --> 2:06:04.600\n is if we cannot figure this out,\n\n2:06:05.680 --> 2:06:09.000\n choose to make, to be quite mindful\n\n2:06:09.000 --> 2:06:11.280\n about what sort of consciousness, if any,\n\n2:06:11.280 --> 2:06:13.720\n we put in different machines that we have.\n\n2:06:16.080 --> 2:06:19.000\n And certainly, we wouldn't wanna make,\n\n2:06:19.000 --> 2:06:21.760\n we should not be making much machines that suffer\n\n2:06:21.760 --> 2:06:23.640\n without us even knowing it, right?\n\n2:06:23.640 --> 2:06:28.320\n And if at any point someone decides to upload themselves\n\n2:06:28.320 --> 2:06:30.120\n like Ray Kurzweil wants to do,\n\n2:06:30.120 --> 2:06:31.440\n I don't know if you've had him on your show.\n\n2:06:31.440 --> 2:06:33.040\n We agree, but then COVID happens,\n\n2:06:33.040 --> 2:06:34.680\n so we're waiting it out a little bit.\n\n2:06:34.680 --> 2:06:38.520\n Suppose he uploads himself into this robo Ray\n\n2:06:38.520 --> 2:06:42.200\n and it talks like him and acts like him and laughs like him.\n\n2:06:42.200 --> 2:06:44.840\n And before he powers off his biological body,\n\n2:06:46.480 --> 2:06:47.760\n he would probably be pretty disturbed\n\n2:06:47.760 --> 2:06:49.600\n if he realized that there's no one home.\n\n2:06:49.600 --> 2:06:52.760\n This robot is not having any subjective experience, right?\n\n2:06:53.760 --> 2:06:58.760\n If humanity gets replaced by machine descendants,\n\n2:06:59.840 --> 2:07:02.320\n which do all these cool things and build spaceships\n\n2:07:02.320 --> 2:07:05.640\n and go to intergalactic rock concerts,\n\n2:07:05.640 --> 2:07:10.000\n and it turns out that they are all unconscious,\n\n2:07:10.000 --> 2:07:11.440\n just going through the motions,\n\n2:07:11.440 --> 2:07:16.160\n wouldn't that be like the ultimate zombie apocalypse, right?\n\n2:07:16.160 --> 2:07:18.040\n Just a play for empty benches?\n\n2:07:18.040 --> 2:07:21.200\n Yeah, I have a sense that there's some kind of,\n\n2:07:21.200 --> 2:07:22.800\n once we understand consciousness better,\n\n2:07:22.800 --> 2:07:25.640\n we'll understand that there's some kind of continuum\n\n2:07:25.640 --> 2:07:28.000\n and it would be a greater appreciation.\n\n2:07:28.000 --> 2:07:30.440\n And we'll probably understand, just like you said,\n\n2:07:30.440 --> 2:07:32.400\n it'd be unfortunate if it's a trick.\n\n2:07:32.400 --> 2:07:33.920\n We'll probably definitely understand\n\n2:07:33.920 --> 2:07:37.760\n that love is indeed a trick that we'll play on each other,\n\n2:07:37.760 --> 2:07:40.960\n that we humans are, we convince ourselves we're conscious,\n\n2:07:40.960 --> 2:07:45.240\n but we're really, us and trees and dolphins\n\n2:07:45.240 --> 2:07:46.600\n are all the same kind of consciousness.\n\n2:07:46.600 --> 2:07:48.160\n Can I try to cheer you up a little bit\n\n2:07:48.160 --> 2:07:50.280\n with a philosophical thought here about the love part?\n\n2:07:50.280 --> 2:07:51.360\n Yes, let's do it.\n\n2:07:51.360 --> 2:07:53.920\n You know, you might say,\n\n2:07:53.920 --> 2:07:56.960\n okay, yeah, love is just a collaboration enabler.\n\n2:07:58.120 --> 2:08:01.800\n And then maybe you can go and get depressed about that.\n\n2:08:01.800 --> 2:08:04.640\n But I think that would be the wrong conclusion, actually.\n\n2:08:04.640 --> 2:08:08.640\n You know, I know that the only reason I enjoy food\n\n2:08:08.640 --> 2:08:11.000\n is because my genes hacked me\n\n2:08:11.000 --> 2:08:13.720\n and they don't want me to starve to death.\n\n2:08:13.720 --> 2:08:17.280\n Not because they care about me consciously\n\n2:08:17.280 --> 2:08:21.080\n enjoying succulent delights of pistachio ice cream,\n\n2:08:21.080 --> 2:08:23.360\n but they just want me to make copies of them.\n\n2:08:23.360 --> 2:08:24.520\n The whole thing, so in a sense,\n\n2:08:24.520 --> 2:08:28.960\n the whole enjoyment of food is also a scam like this.\n\n2:08:28.960 --> 2:08:31.280\n But does that mean I shouldn't take pleasure\n\n2:08:31.280 --> 2:08:32.560\n in this pistachio ice cream?\n\n2:08:32.560 --> 2:08:34.040\n I love pistachio ice cream.\n\n2:08:34.040 --> 2:08:38.200\n And I can tell you, I know this is an experimental fact.\n\n2:08:38.200 --> 2:08:41.600\n I enjoy pistachio ice cream every bit as much,\n\n2:08:41.600 --> 2:08:45.560\n even though I scientifically know exactly why,\n\n2:08:45.560 --> 2:08:46.880\n what kind of scam this was.\n\n2:08:46.880 --> 2:08:48.640\n Your genes really appreciate\n\n2:08:48.640 --> 2:08:50.440\n that you like the pistachio ice cream.\n\n2:08:50.440 --> 2:08:53.080\n Well, but I, my mind appreciates it too, you know?\n\n2:08:53.080 --> 2:08:55.800\n And I have a conscious experience right now.\n\n2:08:55.800 --> 2:08:58.640\n Ultimately, all of my brain is also just something\n\n2:08:58.640 --> 2:09:00.440\n the genes built to copy themselves.\n\n2:09:00.440 --> 2:09:01.600\n But so what?\n\n2:09:01.600 --> 2:09:03.200\n You know, I'm grateful that,\n\n2:09:03.200 --> 2:09:04.960\n yeah, thanks genes for doing this,\n\n2:09:04.960 --> 2:09:07.600\n but you know, now it's my brain that's in charge here\n\n2:09:07.600 --> 2:09:09.520\n and I'm gonna enjoy my conscious experience,\n\n2:09:09.520 --> 2:09:10.360\n thank you very much.\n\n2:09:10.360 --> 2:09:12.480\n And not just the pistachio ice cream,\n\n2:09:12.480 --> 2:09:15.440\n but also the love I feel for my amazing wife\n\n2:09:15.440 --> 2:09:19.280\n and all the other delights of being conscious.\n\n2:09:19.280 --> 2:09:22.240\n I don't, actually Richard Feynman,\n\n2:09:22.240 --> 2:09:25.080\n I think said this so well.\n\n2:09:25.080 --> 2:09:28.000\n He is also the guy, you know, really got me into physics.\n\n2:09:29.680 --> 2:09:31.240\n Some art friend said that,\n\n2:09:31.240 --> 2:09:34.520\n oh, science kind of just is the party pooper.\n\n2:09:34.520 --> 2:09:36.240\n It's kind of ruins the fun, right?\n\n2:09:36.240 --> 2:09:39.680\n When like you have a beautiful flowers as the artist\n\n2:09:39.680 --> 2:09:41.600\n and then the scientist is gonna deconstruct that\n\n2:09:41.600 --> 2:09:44.160\n into just a blob of quarks and electrons.\n\n2:09:44.160 --> 2:09:47.480\n And Feynman pushed back on that in such a beautiful way,\n\n2:09:47.480 --> 2:09:49.920\n which I think also can be used to push back\n\n2:09:49.920 --> 2:09:53.440\n and make you not feel guilty about falling in love.\n\n2:09:53.440 --> 2:09:55.000\n So here's what Feynman basically said.\n\n2:09:55.000 --> 2:09:56.920\n He said to his friend, you know,\n\n2:09:56.920 --> 2:09:59.080\n yeah, I can also as a scientist see\n\n2:09:59.080 --> 2:10:00.960\n that this is a beautiful flower, thank you very much.\n\n2:10:00.960 --> 2:10:03.280\n Maybe I can't draw as good a painting as you\n\n2:10:03.280 --> 2:10:04.560\n because I'm not as talented an artist,\n\n2:10:04.560 --> 2:10:06.800\n but yeah, I can really see the beauty in it.\n\n2:10:06.800 --> 2:10:09.360\n And it just, it also looks beautiful to me.\n\n2:10:09.360 --> 2:10:12.200\n But in addition to that, Feynman said, as a scientist,\n\n2:10:12.200 --> 2:10:16.960\n I see even more beauty that the artist did not see, right?\n\n2:10:16.960 --> 2:10:21.120\n Suppose this is a flower on a blossoming apple tree.\n\n2:10:21.120 --> 2:10:23.840\n You could say this tree has more beauty in it\n\n2:10:23.840 --> 2:10:26.400\n than just the colors and the fragrance.\n\n2:10:26.400 --> 2:10:29.040\n This tree is made of air, Feynman wrote.\n\n2:10:29.040 --> 2:10:31.240\n This is one of my favorite Feynman quotes ever.\n\n2:10:31.240 --> 2:10:33.760\n And it took the carbon out of the air\n\n2:10:33.760 --> 2:10:36.160\n and bound it in using the flaming heat of the sun,\n\n2:10:36.160 --> 2:10:38.600\n you know, to turn the air into a tree.\n\n2:10:38.600 --> 2:10:42.760\n And when you burn logs in your fireplace,\n\n2:10:42.760 --> 2:10:45.120\n it's really beautiful to think that this is being reversed.\n\n2:10:45.120 --> 2:10:48.600\n Now the tree is going, the wood is going back into air.\n\n2:10:48.600 --> 2:10:52.520\n And in this flaming, beautiful dance of the fire\n\n2:10:52.520 --> 2:10:56.000\n that the artist can see is the flaming light of the sun\n\n2:10:56.000 --> 2:10:59.120\n that was bound in to turn the air into tree.\n\n2:10:59.120 --> 2:11:01.480\n And then the ashes is the little residue\n\n2:11:01.480 --> 2:11:02.560\n that didn't come from the air\n\n2:11:02.560 --> 2:11:04.280\n that the tree sucked out of the ground, you know.\n\n2:11:04.280 --> 2:11:06.160\n Feynman said, these are beautiful things.\n\n2:11:06.160 --> 2:11:10.040\n And science just adds, it doesn't subtract.\n\n2:11:10.040 --> 2:11:12.760\n And I feel exactly that way about love\n\n2:11:12.760 --> 2:11:14.800\n and about pistachio ice cream also.\n\n2:11:16.000 --> 2:11:18.680\n I can understand that there is even more nuance\n\n2:11:18.680 --> 2:11:20.480\n to the whole thing, right?\n\n2:11:20.480 --> 2:11:22.480\n At this very visceral level,\n\n2:11:22.480 --> 2:11:24.560\n you can fall in love just as much as someone\n\n2:11:24.560 --> 2:11:26.400\n who knows nothing about neuroscience.\n\n2:11:27.680 --> 2:11:31.840\n But you can also appreciate this even greater beauty in it.\n\n2:11:31.840 --> 2:11:35.600\n Just like, isn't it remarkable that it came about\n\n2:11:35.600 --> 2:11:38.560\n from this completely lifeless universe,\n\n2:11:38.560 --> 2:11:43.080\n just a bunch of hot blob of plasma expanding.\n\n2:11:43.080 --> 2:11:46.160\n And then over the eons, you know, gradually,\n\n2:11:46.160 --> 2:11:48.440\n first the strong nuclear force decided\n\n2:11:48.440 --> 2:11:50.920\n to combine quarks together into nuclei.\n\n2:11:50.920 --> 2:11:53.040\n And then the electric force bound in electrons\n\n2:11:53.040 --> 2:11:53.880\n and made atoms.\n\n2:11:53.880 --> 2:11:55.240\n And then they clustered from gravity\n\n2:11:55.240 --> 2:11:57.720\n and you got planets and stars and this and that.\n\n2:11:57.720 --> 2:12:00.040\n And then natural selection came along\n\n2:12:00.040 --> 2:12:01.800\n and the genes had their little thing.\n\n2:12:01.800 --> 2:12:04.640\n And you started getting what went from seeming\n\n2:12:04.640 --> 2:12:06.240\n like a completely pointless universe\n\n2:12:06.240 --> 2:12:08.040\n that we're just trying to increase entropy\n\n2:12:08.040 --> 2:12:10.160\n and approach heat death into something\n\n2:12:10.160 --> 2:12:11.720\n that looked more goal oriented.\n\n2:12:11.720 --> 2:12:13.280\n Isn't that kind of beautiful?\n\n2:12:13.280 --> 2:12:15.760\n And then this goal orientedness through evolution\n\n2:12:15.760 --> 2:12:18.720\n got ever more sophisticated where you got ever more.\n\n2:12:18.720 --> 2:12:20.120\n And then you started getting this thing,\n\n2:12:20.120 --> 2:12:25.120\n which is kind of like DeepMind's mu zero and steroids,\n\n2:12:25.280 --> 2:12:29.400\n the ultimate self play is not what DeepMind's AI\n\n2:12:29.400 --> 2:12:32.080\n does against itself to get better at go.\n\n2:12:32.080 --> 2:12:34.440\n It's what all these little quark blobs did\n\n2:12:34.440 --> 2:12:38.920\n against each other in the game of survival of the fittest.\n\n2:12:38.920 --> 2:12:42.000\n Now, when you had really dumb bacteria\n\n2:12:42.000 --> 2:12:44.040\n living in a simple environment,\n\n2:12:44.040 --> 2:12:46.440\n there wasn't much incentive to get intelligent,\n\n2:12:46.440 --> 2:12:50.880\n but then the life made environment more complex.\n\n2:12:50.880 --> 2:12:53.520\n And then there was more incentive to get even smarter.\n\n2:12:53.520 --> 2:12:56.600\n And that gave the other organisms more of incentive\n\n2:12:56.600 --> 2:12:57.520\n to also get smarter.\n\n2:12:57.520 --> 2:12:59.880\n And then here we are now,\n\n2:12:59.880 --> 2:13:04.880\n just like mu zero learned to become world master at go\n\n2:13:05.040 --> 2:13:07.200\n and chess from playing against itself\n\n2:13:07.200 --> 2:13:08.560\n by just playing against itself.\n\n2:13:08.560 --> 2:13:10.680\n All the quirks here on our planet,\n\n2:13:10.680 --> 2:13:15.000\n the electrons have created giraffes and elephants\n\n2:13:15.000 --> 2:13:17.640\n and humans and love.\n\n2:13:17.640 --> 2:13:20.280\n I just find that really beautiful.\n\n2:13:20.280 --> 2:13:24.200\n And to me, that just adds to the enjoyment of love.\n\n2:13:24.200 --> 2:13:25.640\n It doesn't subtract anything.\n\n2:13:25.640 --> 2:13:27.320\n Do you feel a little more careful now?\n\n2:13:27.320 --> 2:13:30.640\n I feel way better, that was incredible.\n\n2:13:30.640 --> 2:13:33.920\n So this self play of quirks,\n\n2:13:33.920 --> 2:13:36.320\n taking back to the beginning of our conversation\n\n2:13:36.320 --> 2:13:39.520\n a little bit, there's so many exciting possibilities\n\n2:13:39.520 --> 2:13:42.040\n about artificial intelligence understanding\n\n2:13:42.040 --> 2:13:44.240\n the basic laws of physics.\n\n2:13:44.240 --> 2:13:47.400\n Do you think AI will help us unlock?\n\n2:13:47.400 --> 2:13:49.240\n There's been quite a bit of excitement\n\n2:13:49.240 --> 2:13:50.440\n throughout the history of physics\n\n2:13:50.440 --> 2:13:55.440\n of coming up with more and more general simple laws\n\n2:13:55.440 --> 2:13:58.400\n that explain the nature of our reality.\n\n2:13:58.400 --> 2:14:01.120\n And then the ultimate of that would be a theory\n\n2:14:01.120 --> 2:14:03.680\n of everything that combines everything together.\n\n2:14:03.680 --> 2:14:07.440\n Do you think it's possible that one, we humans,\n\n2:14:07.440 --> 2:14:12.440\n but perhaps AI systems will figure out a theory of physics\n\n2:14:13.640 --> 2:14:16.200\n that unifies all the laws of physics?\n\n2:14:17.120 --> 2:14:19.920\n Yeah, I think it's absolutely possible.\n\n2:14:19.920 --> 2:14:21.360\n I think it's very clear\n\n2:14:21.360 --> 2:14:24.960\n that we're gonna see a great boost to science.\n\n2:14:24.960 --> 2:14:26.720\n We're already seeing a boost actually\n\n2:14:26.720 --> 2:14:28.760\n from machine learning helping science.\n\n2:14:28.760 --> 2:14:30.280\n Alpha fold was an example,\n\n2:14:30.280 --> 2:14:33.360\n the decades old protein folding problem.\n\n2:14:34.440 --> 2:14:38.160\n So, and gradually, yeah, unless we go extinct\n\n2:14:38.160 --> 2:14:39.720\n by doing something dumb like we discussed,\n\n2:14:39.720 --> 2:14:44.040\n I think it's very likely\n\n2:14:44.040 --> 2:14:48.040\n that our understanding of physics will become so good\n\n2:14:48.040 --> 2:14:53.040\n that our technology will no longer be limited\n\n2:14:53.040 --> 2:14:55.200\n by human intelligence,\n\n2:14:55.200 --> 2:14:57.400\n but instead be limited by the laws of physics.\n\n2:14:58.240 --> 2:15:00.120\n So our tech today is limited\n\n2:15:00.120 --> 2:15:02.120\n by what we've been able to invent, right?\n\n2:15:02.120 --> 2:15:04.920\n I think as AI progresses,\n\n2:15:04.920 --> 2:15:07.200\n it'll just be limited by the speed of light\n\n2:15:07.200 --> 2:15:09.240\n and other physical limits,\n\n2:15:09.240 --> 2:15:13.960\n which would mean it's gonna be just dramatically beyond\n\n2:15:13.960 --> 2:15:15.280\n where we are now.\n\n2:15:15.280 --> 2:15:18.560\n Do you think it's a fundamentally mathematical pursuit\n\n2:15:18.560 --> 2:15:22.120\n of trying to understand like the laws\n\n2:15:22.120 --> 2:15:25.760\n of our universe from a mathematical perspective?\n\n2:15:25.760 --> 2:15:28.000\n So almost like if it's AI,\n\n2:15:28.000 --> 2:15:31.640\n it's exploring the space of like theorems\n\n2:15:31.640 --> 2:15:33.480\n and those kinds of things,\n\n2:15:33.480 --> 2:15:38.480\n or is there some other more computational ideas,\n\n2:15:39.760 --> 2:15:41.280\n more sort of empirical ideas?\n\n2:15:41.280 --> 2:15:43.120\n They're both, I would say.\n\n2:15:43.120 --> 2:15:45.920\n It's really interesting to look out at the landscape\n\n2:15:45.920 --> 2:15:48.000\n of everything we call science today.\n\n2:15:48.000 --> 2:15:50.200\n So here you come now with this big new hammer.\n\n2:15:50.200 --> 2:15:51.480\n It says machine learning on it\n\n2:15:51.480 --> 2:15:53.360\n and that's, you know, where are there some nails\n\n2:15:53.360 --> 2:15:56.600\n that you can help with here that you can hammer?\n\n2:15:56.600 --> 2:16:00.120\n Ultimately, if machine learning gets the point\n\n2:16:00.120 --> 2:16:02.800\n that it can do everything better than us,\n\n2:16:02.800 --> 2:16:06.000\n it will be able to help across the whole space of science.\n\n2:16:06.000 --> 2:16:08.120\n But maybe we can anchor it by starting a little bit\n\n2:16:08.120 --> 2:16:11.640\n right now near term and see how we kind of move forward.\n\n2:16:11.640 --> 2:16:14.840\n So like right now, first of all,\n\n2:16:14.840 --> 2:16:17.360\n you have a lot of big data science, right?\n\n2:16:17.360 --> 2:16:19.360\n Where, for example, with telescopes,\n\n2:16:19.360 --> 2:16:24.120\n we are able to collect way more data every hour\n\n2:16:24.120 --> 2:16:26.720\n than a grad student can just pour over\n\n2:16:26.720 --> 2:16:28.760\n like in the old times, right?\n\n2:16:28.760 --> 2:16:31.040\n And machine learning is already being used very effectively,\n\n2:16:31.040 --> 2:16:34.680\n even at MIT, to find planets around other stars,\n\n2:16:34.680 --> 2:16:36.560\n to detect exciting new signatures\n\n2:16:36.560 --> 2:16:38.760\n of new particle physics in the sky,\n\n2:16:38.760 --> 2:16:42.960\n to detect the ripples in the fabric of space time\n\n2:16:42.960 --> 2:16:44.640\n that we call gravitational waves\n\n2:16:44.640 --> 2:16:46.520\n caused by enormous black holes\n\n2:16:46.520 --> 2:16:48.120\n crashing into each other halfway\n\n2:16:48.120 --> 2:16:49.920\n across the observable universe.\n\n2:16:49.920 --> 2:16:52.680\n Machine learning is running and ticking right now,\n\n2:16:52.680 --> 2:16:53.800\n doing all these things,\n\n2:16:53.800 --> 2:16:57.560\n and it's really helping all these experimental fields.\n\n2:16:58.440 --> 2:17:01.880\n There is a separate front of physics,\n\n2:17:01.880 --> 2:17:03.240\n computational physics,\n\n2:17:03.240 --> 2:17:05.680\n which is getting an enormous boost also.\n\n2:17:05.680 --> 2:17:09.520\n So we had to do all our computations by hand, right?\n\n2:17:09.520 --> 2:17:11.240\n People would have these giant books\n\n2:17:11.240 --> 2:17:12.800\n with tables of logarithms,\n\n2:17:12.800 --> 2:17:16.720\n and oh my God, it pains me to even think\n\n2:17:16.720 --> 2:17:19.880\n how long time it would have taken to do simple stuff.\n\n2:17:19.880 --> 2:17:23.560\n Then we started to get little calculators and computers\n\n2:17:23.560 --> 2:17:26.520\n that could do some basic math for us.\n\n2:17:26.520 --> 2:17:28.840\n Now, what we're starting to see is\n\n2:17:31.160 --> 2:17:35.600\n kind of a shift from GOFI, computational physics,\n\n2:17:35.600 --> 2:17:40.000\n to neural network, computational physics.\n\n2:17:40.000 --> 2:17:44.520\n What I mean by that is most computational physics\n\n2:17:44.520 --> 2:17:48.480\n would be done by humans programming in\n\n2:17:48.480 --> 2:17:50.200\n the intelligence of how to do the computation\n\n2:17:50.200 --> 2:17:51.160\n into the computer.\n\n2:17:52.440 --> 2:17:55.400\n Just as when Garry Kasparov got his posterior kicked\n\n2:17:55.400 --> 2:17:56.920\n by IBM's Deep Blue in chess,\n\n2:17:56.920 --> 2:17:59.880\n humans had programmed in exactly how to play chess.\n\n2:17:59.880 --> 2:18:01.160\n Intelligence came from the humans.\n\n2:18:01.160 --> 2:18:02.400\n It wasn't learned, right?\n\n2:18:03.840 --> 2:18:08.480\n Mu zero can be not only Kasparov in chess,\n\n2:18:08.480 --> 2:18:09.880\n but also Stockfish,\n\n2:18:09.880 --> 2:18:12.560\n which is the best sort of GOFI chess program.\n\n2:18:12.560 --> 2:18:16.560\n By learning, and we're seeing more of that now,\n\n2:18:16.560 --> 2:18:18.320\n that shift beginning to happen in physics.\n\n2:18:18.320 --> 2:18:20.520\n So let me give you an example.\n\n2:18:20.520 --> 2:18:24.120\n So lattice QCD is an area of physics\n\n2:18:24.120 --> 2:18:27.320\n whose goal is basically to take the periodic table\n\n2:18:27.320 --> 2:18:30.120\n and just compute the whole thing from first principles.\n\n2:18:31.120 --> 2:18:33.920\n This is not the search for theory of everything.\n\n2:18:33.920 --> 2:18:36.360\n We already know the theory\n\n2:18:36.360 --> 2:18:39.720\n that's supposed to produce as output the periodic table,\n\n2:18:39.720 --> 2:18:42.720\n which atoms are stable, how heavy they are,\n\n2:18:42.720 --> 2:18:44.840\n all that good stuff, their spectral lines.\n\n2:18:45.840 --> 2:18:48.120\n It's a theory, lattice QCD,\n\n2:18:48.120 --> 2:18:50.000\n you can put it on your tshirt.\n\n2:18:50.000 --> 2:18:51.160\n Our colleague Frank Wilczek\n\n2:18:51.160 --> 2:18:53.120\n got the Nobel Prize for working on it.\n\n2:18:54.520 --> 2:18:56.600\n But the math is just too hard for us to solve.\n\n2:18:56.600 --> 2:18:58.640\n We have not been able to start with these equations\n\n2:18:58.640 --> 2:19:01.440\n and solve them to the extent that we can predict, oh yeah.\n\n2:19:01.440 --> 2:19:03.360\n And then there is carbon,\n\n2:19:03.360 --> 2:19:07.000\n and this is what the spectrum of the carbon atom looks like.\n\n2:19:07.000 --> 2:19:09.960\n But awesome people are building\n\n2:19:09.960 --> 2:19:12.040\n these supercomputer simulations\n\n2:19:12.040 --> 2:19:14.960\n where you just put in these equations\n\n2:19:14.960 --> 2:19:19.960\n and you make a big cubic lattice of space,\n\n2:19:20.680 --> 2:19:22.080\n or actually it's a very small lattice\n\n2:19:22.080 --> 2:19:25.640\n because you're going down to the subatomic scale,\n\n2:19:25.640 --> 2:19:26.880\n and you try to solve it.\n\n2:19:26.880 --> 2:19:28.960\n But it's just so computationally expensive\n\n2:19:28.960 --> 2:19:31.840\n that we still haven't been able to calculate things\n\n2:19:31.840 --> 2:19:34.960\n as accurately as we measure them in many cases.\n\n2:19:34.960 --> 2:19:37.520\n And now machine learning is really revolutionizing this.\n\n2:19:37.520 --> 2:19:40.040\n So my colleague Fiala Shanahan at MIT, for example,\n\n2:19:40.040 --> 2:19:43.280\n she's been using this really cool\n\n2:19:43.280 --> 2:19:47.560\n machine learning technique called normalizing flows,\n\n2:19:47.560 --> 2:19:49.800\n where she's realized she can actually speed up\n\n2:19:49.800 --> 2:19:52.160\n the calculation dramatically\n\n2:19:52.160 --> 2:19:54.620\n by having the AI learn how to do things faster.\n\n2:19:55.680 --> 2:19:57.280\n Another area like this\n\n2:19:57.280 --> 2:20:02.280\n where we suck up an enormous amount of supercomputer time\n\n2:20:02.280 --> 2:20:05.480\n to do physics is black hole collisions.\n\n2:20:05.480 --> 2:20:06.880\n So now that we've done the sexy stuff\n\n2:20:06.880 --> 2:20:09.960\n of detecting a bunch of this with LIGO and other experiments,\n\n2:20:09.960 --> 2:20:13.360\n we want to be able to know what we're seeing.\n\n2:20:13.360 --> 2:20:16.480\n And so it's a very simple conceptual problem.\n\n2:20:16.480 --> 2:20:18.000\n It's the two body problem.\n\n2:20:19.000 --> 2:20:23.000\n Newton solved it for classical gravity hundreds of years ago,\n\n2:20:23.000 --> 2:20:26.080\n but the two body problem is still not fully solved.\n\n2:20:26.080 --> 2:20:26.920\n For black holes.\n\n2:20:26.920 --> 2:20:29.000\n Black holes, yes, and Einstein's gravity\n\n2:20:29.000 --> 2:20:31.120\n because they won't just orbit in space,\n\n2:20:31.120 --> 2:20:33.560\n they won't just orbit each other forever anymore,\n\n2:20:33.560 --> 2:20:36.080\n two things, they give off gravitational waves\n\n2:20:36.080 --> 2:20:37.800\n and make sure they crash into each other.\n\n2:20:37.800 --> 2:20:40.320\n And the game, what you want to do is you want to figure out,\n\n2:20:40.320 --> 2:20:43.480\n okay, what kind of wave comes out\n\n2:20:43.480 --> 2:20:46.320\n as a function of the masses of the two black holes,\n\n2:20:46.320 --> 2:20:48.120\n as a function of how they're spinning,\n\n2:20:48.120 --> 2:20:50.720\n relative to each other, et cetera.\n\n2:20:50.720 --> 2:20:52.040\n And that is so hard.\n\n2:20:52.040 --> 2:20:54.200\n It can take months of supercomputer time\n\n2:20:54.200 --> 2:20:56.200\n and massive numbers of cores to do it.\n\n2:20:56.200 --> 2:21:00.000\n Now, wouldn't it be great if you can use machine learning\n\n2:21:01.240 --> 2:21:03.280\n to greatly speed that up, right?\n\n2:21:04.760 --> 2:21:09.360\n Now you can use the expensive old GoFi calculation\n\n2:21:09.360 --> 2:21:11.920\n as the truth, and then see if machine learning\n\n2:21:11.920 --> 2:21:13.600\n can figure out a smarter, faster way\n\n2:21:13.600 --> 2:21:15.000\n of getting the right answer.\n\n2:21:16.320 --> 2:21:20.000\n Yet another area, like computational physics.\n\n2:21:20.000 --> 2:21:22.280\n These are probably the big three\n\n2:21:22.280 --> 2:21:24.240\n that suck up the most computer time.\n\n2:21:24.240 --> 2:21:27.160\n Lattice QCD, black hole collisions,\n\n2:21:27.160 --> 2:21:29.560\n and cosmological simulations,\n\n2:21:29.560 --> 2:21:32.280\n where you take not a subatomic thing\n\n2:21:32.280 --> 2:21:34.400\n and try to figure out the mass of the proton,\n\n2:21:34.400 --> 2:21:37.680\n but you take something enormous\n\n2:21:37.680 --> 2:21:41.320\n and try to look at how all the galaxies get formed in there.\n\n2:21:41.320 --> 2:21:44.720\n There again, there are a lot of very cool ideas right now\n\n2:21:44.720 --> 2:21:46.080\n about how you can use machine learning\n\n2:21:46.080 --> 2:21:48.000\n to do this sort of stuff better.\n\n2:21:49.760 --> 2:21:51.560\n The difference between this and the big data\n\n2:21:51.560 --> 2:21:54.560\n is you kind of make the data yourself, right?\n\n2:21:54.560 --> 2:21:58.440\n So, and then finally,\n\n2:21:58.440 --> 2:22:00.200\n we're looking over the physics landscape\n\n2:22:00.200 --> 2:22:02.120\n and seeing what can we hammer with machine learning, right?\n\n2:22:02.120 --> 2:22:05.520\n So we talked about experimental data, big data,\n\n2:22:05.520 --> 2:22:07.880\n discovering cool stuff that we humans\n\n2:22:07.880 --> 2:22:09.520\n then look more closely at.\n\n2:22:09.520 --> 2:22:13.440\n Then we talked about taking the expensive computations\n\n2:22:13.440 --> 2:22:15.040\n we're doing now and figuring out\n\n2:22:15.040 --> 2:22:18.560\n how to do them much faster and better with AI.\n\n2:22:18.560 --> 2:22:21.920\n And finally, let's go really theoretical.\n\n2:22:21.920 --> 2:22:24.000\n So things like discovering equations,\n\n2:22:25.000 --> 2:22:27.560\n having deep fundamental insights,\n\n2:22:30.240 --> 2:22:33.040\n this is something closest to what I've been doing\n\n2:22:33.040 --> 2:22:33.880\n in my group.\n\n2:22:33.880 --> 2:22:35.920\n We talked earlier about the whole AI Feynman project,\n\n2:22:35.920 --> 2:22:37.920\n where if you just have some data,\n\n2:22:37.920 --> 2:22:39.840\n how do you automatically discover equations\n\n2:22:39.840 --> 2:22:42.160\n that seem to describe this well,\n\n2:22:42.160 --> 2:22:44.120\n that you can then go back as a human\n\n2:22:44.120 --> 2:22:46.640\n and then work with and test and explore.\n\n2:22:46.640 --> 2:22:50.320\n And you asked a really good question also\n\n2:22:50.320 --> 2:22:54.000\n about if this is sort of a search problem in some sense.\n\n2:22:54.000 --> 2:22:56.880\n That's very deep actually what you said, because it is.\n\n2:22:56.880 --> 2:23:00.320\n Suppose I ask you to prove some mathematical theorem.\n\n2:23:01.680 --> 2:23:02.960\n What is a proof in math?\n\n2:23:02.960 --> 2:23:05.360\n It's just a long string of steps, logical steps\n\n2:23:05.360 --> 2:23:06.960\n that you can write out with symbols.\n\n2:23:07.920 --> 2:23:10.240\n And once you find it, it's very easy to write a program\n\n2:23:10.240 --> 2:23:12.960\n to check whether it's a valid proof or not.\n\n2:23:14.640 --> 2:23:16.080\n So why is it so hard to prove it?\n\n2:23:16.080 --> 2:23:19.040\n Well, because there are ridiculously many possible\n\n2:23:19.040 --> 2:23:21.600\n candidate proofs you could write down, right?\n\n2:23:21.600 --> 2:23:25.440\n If the proof contains 10,000 symbols,\n\n2:23:25.440 --> 2:23:27.760\n even if there were only 10 options\n\n2:23:27.760 --> 2:23:29.120\n for what each symbol could be,\n\n2:23:29.120 --> 2:23:33.440\n that's 10 to the power of 1,000 possible proofs,\n\n2:23:33.440 --> 2:23:36.080\n which is way more than there are atoms in our universe.\n\n2:23:36.080 --> 2:23:38.400\n So you could say it's trivial to prove these things.\n\n2:23:38.400 --> 2:23:41.200\n You just write a computer, generate all strings,\n\n2:23:41.200 --> 2:23:43.680\n and then check, is this a valid proof?\n\n2:23:43.680 --> 2:23:44.520\n No.\n\n2:23:44.520 --> 2:23:45.360\n Is this a valid proof?\n\n2:23:45.360 --> 2:23:46.400\n Is this a valid proof?\n\n2:23:46.400 --> 2:23:47.720\n No.\n\n2:23:47.720 --> 2:23:51.040\n And then you just keep doing this forever.\n\n2:23:51.960 --> 2:23:53.160\n But there are a lot of,\n\n2:23:53.160 --> 2:23:55.120\n but it is fundamentally a search problem.\n\n2:23:55.120 --> 2:23:57.000\n You just want to search the space of all those,\n\n2:23:57.000 --> 2:24:02.000\n all strings of symbols to find one that is the proof, right?\n\n2:24:03.880 --> 2:24:08.800\n And there's a whole area of machine learning called search.\n\n2:24:08.800 --> 2:24:10.600\n How do you search through some giant space\n\n2:24:10.600 --> 2:24:12.400\n to find the needle in the haystack?\n\n2:24:12.400 --> 2:24:14.800\n And it's easier in cases\n\n2:24:14.800 --> 2:24:17.160\n where there's a clear measure of good,\n\n2:24:17.160 --> 2:24:18.800\n like you're not just right or wrong,\n\n2:24:18.800 --> 2:24:20.640\n but this is better and this is worse,\n\n2:24:20.640 --> 2:24:21.800\n so you can maybe get some hints\n\n2:24:21.800 --> 2:24:23.800\n as to which direction to go in.\n\n2:24:23.800 --> 2:24:27.000\n That's why we talked about neural networks work so well.\n\n2:24:28.400 --> 2:24:30.680\n I mean, that's such a human thing\n\n2:24:30.680 --> 2:24:32.280\n of that moment of genius\n\n2:24:32.280 --> 2:24:37.280\n of figuring out the intuition of good, essentially.\n\n2:24:37.360 --> 2:24:38.680\n I mean, we thought that that was...\n\n2:24:38.680 --> 2:24:40.120\n Or is it?\n\n2:24:40.120 --> 2:24:41.320\n Maybe it's not, right?\n\n2:24:41.320 --> 2:24:42.720\n We thought that about chess, right?\n\n2:24:42.720 --> 2:24:46.880\n That the ability to see like 10, 15,\n\n2:24:46.880 --> 2:24:50.680\n sometimes 20 steps ahead was not a calculation\n\n2:24:50.680 --> 2:24:51.760\n that humans were performing.\n\n2:24:51.760 --> 2:24:53.720\n It was some kind of weird intuition\n\n2:24:53.720 --> 2:24:57.280\n about different patterns, about board positions,\n\n2:24:57.280 --> 2:24:59.440\n about the relative positions,\n\n2:24:59.440 --> 2:25:01.640\n somehow stitching stuff together.\n\n2:25:01.640 --> 2:25:03.920\n And a lot of it is just like intuition,\n\n2:25:03.920 --> 2:25:05.960\n but then you have like alpha,\n\n2:25:05.960 --> 2:25:10.400\n I guess zero be the first one that did the self play.\n\n2:25:10.400 --> 2:25:12.160\n It just came up with this.\n\n2:25:12.160 --> 2:25:14.560\n It was able to learn through self play mechanism,\n\n2:25:14.560 --> 2:25:16.040\n this kind of intuition.\n\n2:25:16.040 --> 2:25:16.880\n Exactly.\n\n2:25:16.880 --> 2:25:19.960\n But just like you said, it's so fascinating to think,\n\n2:25:19.960 --> 2:25:24.640\n well, they're in the space of totally new ideas.\n\n2:25:24.640 --> 2:25:28.960\n Can that be done in developing theorems?\n\n2:25:28.960 --> 2:25:30.800\n We know it can be done by neural networks\n\n2:25:30.800 --> 2:25:32.280\n because we did it with the neural networks\n\n2:25:32.280 --> 2:25:36.280\n in the craniums of the great mathematicians of humanity.\n\n2:25:36.280 --> 2:25:38.640\n And I'm so glad you brought up alpha zero\n\n2:25:38.640 --> 2:25:39.960\n because that's the counter example.\n\n2:25:39.960 --> 2:25:41.840\n It turned out we were flattering ourselves\n\n2:25:41.840 --> 2:25:45.360\n when we said intuition is something different.\n\n2:25:45.360 --> 2:25:46.520\n Only humans can do it.\n\n2:25:46.520 --> 2:25:48.120\n It's not information processing.\n\n2:25:50.880 --> 2:25:52.320\n It used to be that way.\n\n2:25:53.720 --> 2:25:56.200\n Again, it's really instructive, I think,\n\n2:25:56.200 --> 2:25:58.480\n to compare the chess computer Deep Blue\n\n2:25:58.480 --> 2:26:02.040\n that beat Kasparov with alpha zero\n\n2:26:02.040 --> 2:26:04.280\n that beat Lisa Dahl at Go.\n\n2:26:04.280 --> 2:26:08.640\n Because for Deep Blue, there was no intuition.\n\n2:26:08.640 --> 2:26:12.000\n There was some, humans had programmed in some intuition.\n\n2:26:12.000 --> 2:26:13.600\n After humans had played a lot of games,\n\n2:26:13.600 --> 2:26:16.520\n they told the computer, count the pawn as one point,\n\n2:26:16.520 --> 2:26:19.920\n the bishop is three points, rook is five points,\n\n2:26:19.920 --> 2:26:21.120\n and so on, you add it all up,\n\n2:26:21.120 --> 2:26:23.400\n and then you add some extra points for past pawns\n\n2:26:23.400 --> 2:26:27.320\n and subtract if the opponent has it and blah, blah, blah.\n\n2:26:28.280 --> 2:26:31.560\n And then what Deep Blue did was just search.\n\n2:26:32.520 --> 2:26:34.960\n Just very brute force and tried many, many moves ahead,\n\n2:26:34.960 --> 2:26:37.400\n all these combinations and a prune tree search.\n\n2:26:37.400 --> 2:26:41.120\n And it could think much faster than Kasparov, and it won.\n\n2:26:42.680 --> 2:26:45.440\n And that, I think, inflated our egos\n\n2:26:45.440 --> 2:26:46.560\n in a way it shouldn't have,\n\n2:26:46.560 --> 2:26:48.760\n because people started to say, yeah, yeah,\n\n2:26:48.760 --> 2:26:51.360\n it's just brute force search, but it has no intuition.\n\n2:26:52.280 --> 2:26:57.280\n Alpha zero really popped our bubble there,\n\n2:26:57.760 --> 2:27:00.880\n because what alpha zero does,\n\n2:27:00.880 --> 2:27:03.880\n yes, it does also do some of that tree search,\n\n2:27:03.880 --> 2:27:06.560\n but it also has this intuition module,\n\n2:27:06.560 --> 2:27:09.560\n which in geek speak is called a value function,\n\n2:27:09.560 --> 2:27:11.120\n where it just looks at the board\n\n2:27:11.120 --> 2:27:13.960\n and comes up with a number for how good is that position.\n\n2:27:14.960 --> 2:27:17.960\n The difference was no human told it\n\n2:27:17.960 --> 2:27:21.120\n how good the position is, it just learned it.\n\n2:27:22.480 --> 2:27:26.840\n And mu zero is the coolest or scariest of all,\n\n2:27:26.840 --> 2:27:28.320\n depending on your mood,\n\n2:27:28.320 --> 2:27:32.080\n because the same basic AI system\n\n2:27:33.040 --> 2:27:35.320\n will learn what the good board position is,\n\n2:27:35.320 --> 2:27:38.640\n regardless of whether it's chess or Go or Shogi\n\n2:27:38.640 --> 2:27:42.920\n or Pacman or Lady Pacman or Breakout or Space Invaders\n\n2:27:42.920 --> 2:27:45.000\n or any number, a bunch of other games.\n\n2:27:45.000 --> 2:27:45.840\n You don't tell it anything,\n\n2:27:45.840 --> 2:27:49.760\n and it gets this intuition after a while for what's good.\n\n2:27:49.760 --> 2:27:52.760\n So this is very hopeful for science, I think,\n\n2:27:52.760 --> 2:27:55.240\n because if it can get intuition\n\n2:27:55.240 --> 2:27:57.280\n for what's a good position there,\n\n2:27:57.280 --> 2:27:58.880\n maybe it can also get intuition\n\n2:27:58.880 --> 2:28:00.640\n for what are some good directions to go\n\n2:28:00.640 --> 2:28:03.040\n if you're trying to prove something.\n\n2:28:03.040 --> 2:28:06.400\n I often, one of the most fun things in my science career\n\n2:28:06.400 --> 2:28:08.600\n is when I've been able to prove some theorem about something\n\n2:28:08.600 --> 2:28:12.160\n and it's very heavily intuition guided, of course.\n\n2:28:12.160 --> 2:28:14.080\n I don't sit and try all random strings.\n\n2:28:14.080 --> 2:28:16.280\n I have a hunch that, you know,\n\n2:28:16.280 --> 2:28:18.840\n this reminds me a little bit of about this other proof\n\n2:28:18.840 --> 2:28:19.920\n I've seen for this thing.\n\n2:28:19.920 --> 2:28:22.520\n So maybe I first, what if I try this?\n\n2:28:22.520 --> 2:28:24.720\n Nah, that didn't work out.\n\n2:28:24.720 --> 2:28:25.840\n But this reminds me actually,\n\n2:28:25.840 --> 2:28:28.560\n the way this failed reminds me of that.\n\n2:28:28.560 --> 2:28:33.560\n So combining the intuition with all these brute force\n\n2:28:33.880 --> 2:28:38.520\n capabilities, I think it's gonna be able to help physics too.\n\n2:28:38.520 --> 2:28:42.880\n Do you think there'll be a day when an AI system\n\n2:28:42.880 --> 2:28:46.400\n being the primary contributor, let's say 90% plus,\n\n2:28:46.400 --> 2:28:48.200\n wins the Nobel Prize in physics?\n\n2:28:50.400 --> 2:28:51.960\n Obviously they'll give it to the humans\n\n2:28:51.960 --> 2:28:54.800\n because we humans don't like to give prizes to machines.\n\n2:28:54.800 --> 2:28:57.560\n It'll give it to the humans behind the system.\n\n2:28:57.560 --> 2:28:59.920\n You could argue that AI has already been involved\n\n2:28:59.920 --> 2:29:01.560\n in some Nobel Prizes, probably,\n\n2:29:01.560 --> 2:29:03.560\n maybe something with black holes and stuff like that.\n\n2:29:03.560 --> 2:29:07.160\n Yeah, we don't like giving prizes to other life forms.\n\n2:29:07.160 --> 2:29:09.720\n If someone wins a horse racing contest,\n\n2:29:09.720 --> 2:29:11.360\n they don't give the prize to the horse either.\n\n2:29:11.360 --> 2:29:12.200\n That's true.\n\n2:29:13.400 --> 2:29:16.000\n But do you think that we might be able to see\n\n2:29:16.000 --> 2:29:19.200\n something like that in our lifetimes when AI,\n\n2:29:19.200 --> 2:29:21.840\n so like the first system I would say\n\n2:29:21.840 --> 2:29:25.360\n that makes us think about a Nobel Prize seriously\n\n2:29:25.360 --> 2:29:28.760\n is like Alpha Fold is making us think about\n\n2:29:28.760 --> 2:29:31.960\n in medicine, physiology, a Nobel Prize,\n\n2:29:31.960 --> 2:29:34.080\n perhaps discoveries that are direct result\n\n2:29:34.080 --> 2:29:36.640\n of something that's discovered by Alpha Fold.\n\n2:29:36.640 --> 2:29:39.560\n Do you think in physics we might be able\n\n2:29:39.560 --> 2:29:41.520\n to see that in our lifetimes?\n\n2:29:41.520 --> 2:29:43.520\n I think what's probably gonna happen\n\n2:29:43.520 --> 2:29:46.880\n is more of a blurring of the distinctions.\n\n2:29:46.880 --> 2:29:51.880\n So today if somebody uses a computer\n\n2:29:53.000 --> 2:29:54.920\n to do a computation that gives them the Nobel Prize,\n\n2:29:54.920 --> 2:29:57.160\n nobody's gonna dream of giving the prize to the computer.\n\n2:29:57.160 --> 2:29:59.000\n They're gonna be like, that was just a tool.\n\n2:29:59.000 --> 2:30:02.120\n I think for these things also,\n\n2:30:02.120 --> 2:30:04.000\n people are just gonna for a long time\n\n2:30:04.000 --> 2:30:06.120\n view the computer as a tool.\n\n2:30:06.120 --> 2:30:11.120\n But what's gonna change is the ubiquity of machine learning.\n\n2:30:11.120 --> 2:30:16.120\n I think at some point in my lifetime,\n\n2:30:17.120 --> 2:30:21.400\n finding a human physicist who knows nothing\n\n2:30:21.400 --> 2:30:23.800\n about machine learning is gonna be almost as hard\n\n2:30:23.800 --> 2:30:25.960\n as it is today finding a human physicist\n\n2:30:25.960 --> 2:30:29.160\n who doesn't says, oh, I don't know anything about computers\n\n2:30:29.160 --> 2:30:30.880\n or I don't use math.\n\n2:30:30.880 --> 2:30:32.880\n That would just be a ridiculous concept.\n\n2:30:34.000 --> 2:30:38.240\n You see, but the thing is there is a magic moment though,\n\n2:30:38.240 --> 2:30:42.320\n like with Alpha Zero, when the system surprises us\n\n2:30:42.320 --> 2:30:45.640\n in a way where the best people in the world\n\n2:30:46.680 --> 2:30:48.960\n truly learn something from the system\n\n2:30:48.960 --> 2:30:52.480\n in a way where you feel like it's another entity.\n\n2:30:52.480 --> 2:30:54.920\n Like the way people, the way Magnus Carlsen,\n\n2:30:54.920 --> 2:30:58.080\n the way certain people are looking at the work of Alpha Zero,\n\n2:30:58.080 --> 2:31:02.960\n it's like, it truly is no longer a tool\n\n2:31:02.960 --> 2:31:06.680\n in the sense that it doesn't feel like a tool.\n\n2:31:06.680 --> 2:31:08.960\n It feels like some other entity.\n\n2:31:08.960 --> 2:31:13.320\n So there's a magic difference like where you're like,\n\n2:31:13.320 --> 2:31:17.320\n if an AI system is able to come up with an insight\n\n2:31:17.320 --> 2:31:22.320\n that surprises everybody in some like major way\n\n2:31:23.760 --> 2:31:25.960\n that's a phase shift in our understanding\n\n2:31:25.960 --> 2:31:27.760\n of some particular science\n\n2:31:27.760 --> 2:31:30.040\n or some particular aspect of physics,\n\n2:31:30.040 --> 2:31:32.680\n I feel like that is no longer a tool.\n\n2:31:32.680 --> 2:31:34.840\n And then you can start to say\n\n2:31:35.800 --> 2:31:38.720\n that like it perhaps deserves the prize.\n\n2:31:38.720 --> 2:31:40.640\n So for sure, the more important\n\n2:31:40.640 --> 2:31:43.120\n and the more fundamental transformation\n\n2:31:43.120 --> 2:31:46.640\n of the 21st century science is exactly what you're saying,\n\n2:31:46.640 --> 2:31:50.680\n which is probably everybody will be doing machine learning.\n\n2:31:50.680 --> 2:31:51.560\n It's to some degree.\n\n2:31:51.560 --> 2:31:54.760\n Like if you want to be successful\n\n2:31:54.760 --> 2:31:57.560\n at unlocking the mysteries of science,\n\n2:31:57.560 --> 2:31:58.800\n you should be doing machine learning.\n\n2:31:58.800 --> 2:32:01.440\n But it's just exciting to think about like,\n\n2:32:01.440 --> 2:32:03.080\n whether there'll be one that comes along\n\n2:32:03.080 --> 2:32:08.000\n that's super surprising and they'll make us question\n\n2:32:08.000 --> 2:32:10.320\n like who the real inventors are in this world.\n\n2:32:10.320 --> 2:32:11.640\n Yeah.\n\n2:32:11.640 --> 2:32:14.240\n Yeah, I think the question of,\n\n2:32:14.240 --> 2:32:15.880\n isn't if it's gonna happen, but when?\n\n2:32:15.880 --> 2:32:17.960\n And, but it's important.\n\n2:32:17.960 --> 2:32:20.840\n Honestly, in my mind, the time when that happens\n\n2:32:20.840 --> 2:32:23.360\n is also more or less the same time\n\n2:32:23.360 --> 2:32:25.560\n when we get artificial general intelligence.\n\n2:32:25.560 --> 2:32:28.160\n And then we have a lot bigger things to worry about\n\n2:32:28.160 --> 2:32:31.000\n than whether we should get the Nobel prize or not, right?\n\n2:32:31.000 --> 2:32:31.840\n Yeah.\n\n2:32:31.840 --> 2:32:35.000\n Because when you have machines\n\n2:32:35.000 --> 2:32:39.360\n that can outperform our best scientists at science,\n\n2:32:39.360 --> 2:32:41.040\n they can probably outperform us\n\n2:32:41.040 --> 2:32:44.440\n at a lot of other stuff as well,\n\n2:32:44.440 --> 2:32:46.440\n which can at a minimum make them\n\n2:32:46.440 --> 2:32:49.440\n incredibly powerful agents in the world.\n\n2:32:49.440 --> 2:32:53.160\n And I think it's a mistake to think\n\n2:32:53.160 --> 2:32:57.040\n we only have to start worrying about loss of control\n\n2:32:57.040 --> 2:32:59.720\n when machines get to AGI across the board,\n\n2:32:59.720 --> 2:33:02.160\n where they can do everything, all our jobs.\n\n2:33:02.160 --> 2:33:07.160\n Long before that, they'll be hugely influential.\n\n2:33:07.880 --> 2:33:12.560\n We talked at length about how the hacking of our minds\n\n2:33:12.560 --> 2:33:17.560\n with algorithms trying to get us glued to our screens,\n\n2:33:18.440 --> 2:33:22.320\n right, has already had a big impact on society.\n\n2:33:22.320 --> 2:33:24.080\n That was an incredibly dumb algorithm\n\n2:33:24.080 --> 2:33:25.840\n in the grand scheme of things, right?\n\n2:33:25.840 --> 2:33:27.840\n The supervised machine learning,\n\n2:33:27.840 --> 2:33:29.520\n yet that had huge impact.\n\n2:33:29.520 --> 2:33:32.080\n So I just don't want us to be lulled\n\n2:33:32.080 --> 2:33:33.280\n into false sense of security\n\n2:33:33.280 --> 2:33:35.560\n and think there won't be any societal impact\n\n2:33:35.560 --> 2:33:37.040\n until things reach human level,\n\n2:33:37.040 --> 2:33:38.280\n because it's happening already.\n\n2:33:38.280 --> 2:33:40.560\n And I was just thinking the other week,\n\n2:33:40.560 --> 2:33:44.880\n when I see some scaremonger going,\n\n2:33:44.880 --> 2:33:47.080\n oh, the robots are coming,\n\n2:33:47.080 --> 2:33:50.280\n the implication is always that they're coming to kill us.\n\n2:33:50.280 --> 2:33:51.120\n Yeah.\n\n2:33:51.120 --> 2:33:52.360\n And maybe you should have worried about that\n\n2:33:52.360 --> 2:33:54.720\n if you were in Nagorno Karabakh\n\n2:33:54.720 --> 2:33:55.720\n during the recent war there.\n\n2:33:55.720 --> 2:34:00.720\n But more seriously, the robots are coming right now,\n\n2:34:01.440 --> 2:34:03.160\n but they're mainly not coming to kill us.\n\n2:34:03.160 --> 2:34:04.400\n They're coming to hack us.\n\n2:34:06.000 --> 2:34:08.200\n They're coming to hack our minds,\n\n2:34:08.200 --> 2:34:11.280\n into buying things that maybe we didn't need,\n\n2:34:11.280 --> 2:34:13.200\n to vote for people who may not have\n\n2:34:13.200 --> 2:34:15.360\n our best interest in mind.\n\n2:34:15.360 --> 2:34:17.600\n And it's kind of humbling, I think,\n\n2:34:17.600 --> 2:34:20.120\n actually, as a human being to admit\n\n2:34:20.120 --> 2:34:22.360\n that it turns out that our minds are actually\n\n2:34:22.360 --> 2:34:24.760\n much more hackable than we thought.\n\n2:34:24.760 --> 2:34:27.040\n And the ultimate insult is that we are actually\n\n2:34:27.040 --> 2:34:30.400\n getting hacked by the machine learning algorithms\n\n2:34:30.400 --> 2:34:31.560\n that are, in some objective sense,\n\n2:34:31.560 --> 2:34:33.960\n much dumber than us, you know?\n\n2:34:33.960 --> 2:34:35.720\n But maybe we shouldn't be so surprised\n\n2:34:35.720 --> 2:34:40.520\n because, you know, how do you feel about cute puppies?\n\n2:34:40.520 --> 2:34:41.600\n Love them.\n\n2:34:41.600 --> 2:34:43.640\n So, you know, you would probably argue\n\n2:34:43.640 --> 2:34:46.120\n that in some across the board measure,\n\n2:34:46.120 --> 2:34:47.680\n you're more intelligent than they are,\n\n2:34:47.680 --> 2:34:50.760\n but boy, are cute puppies good at hacking us, right?\n\n2:34:50.760 --> 2:34:51.600\n Yeah.\n\n2:34:51.600 --> 2:34:53.720\n They move into our house, persuade us to feed them\n\n2:34:53.720 --> 2:34:54.560\n and do all these things.\n\n2:34:54.560 --> 2:34:56.600\n And what do they ever do but for us?\n\n2:34:56.600 --> 2:34:57.440\n Yeah.\n\n2:34:57.440 --> 2:35:00.520\n Other than being cute and making us feel good, right?\n\n2:35:00.520 --> 2:35:03.080\n So if puppies can hack us,\n\n2:35:03.080 --> 2:35:04.920\n maybe we shouldn't be so surprised\n\n2:35:04.920 --> 2:35:09.040\n if pretty dumb machine learning algorithms can hack us too.\n\n2:35:09.040 --> 2:35:11.680\n Not to speak of cats, which is another level.\n\n2:35:11.680 --> 2:35:13.400\n And I think we should,\n\n2:35:13.400 --> 2:35:15.640\n to counter your previous point about there,\n\n2:35:15.640 --> 2:35:18.040\n let us not think about evil creatures in this world.\n\n2:35:18.040 --> 2:35:20.480\n We can all agree that cats are as close\n\n2:35:20.480 --> 2:35:22.960\n to objective evil as we can get.\n\n2:35:22.960 --> 2:35:24.400\n But that's just me saying that.\n\n2:35:24.400 --> 2:35:25.480\n Okay, so you have.\n\n2:35:25.480 --> 2:35:27.320\n Have you seen the cartoon?\n\n2:35:27.320 --> 2:35:29.840\n I think it's maybe the onion\n\n2:35:31.760 --> 2:35:33.720\n with this incredibly cute kitten.\n\n2:35:33.720 --> 2:35:36.840\n And it just says, it's underneath something\n\n2:35:36.840 --> 2:35:38.920\n that thinks about murder all day.\n\n2:35:38.920 --> 2:35:41.560\n Exactly.\n\n2:35:41.560 --> 2:35:43.080\n That's accurate.\n\n2:35:43.080 --> 2:35:45.200\n You've mentioned offline that there might be a link\n\n2:35:45.200 --> 2:35:47.960\n between post biological AGI and SETI.\n\n2:35:47.960 --> 2:35:51.280\n So last time we talked,\n\n2:35:52.520 --> 2:35:54.920\n you've talked about this intuition\n\n2:35:54.920 --> 2:35:59.280\n that we humans might be quite unique\n\n2:35:59.280 --> 2:36:02.360\n in our galactic neighborhood.\n\n2:36:02.360 --> 2:36:03.680\n Perhaps our galaxy,\n\n2:36:03.680 --> 2:36:06.360\n perhaps the entirety of the observable universe\n\n2:36:06.360 --> 2:36:10.840\n who might be the only intelligent civilization here,\n\n2:36:10.840 --> 2:36:15.840\n which is, and you argue pretty well for that thought.\n\n2:36:17.720 --> 2:36:21.240\n So I have a few little questions around this.\n\n2:36:21.240 --> 2:36:24.680\n One, the scientific question,\n\n2:36:24.680 --> 2:36:29.240\n in which way would you be,\n\n2:36:29.240 --> 2:36:33.120\n if you were wrong in that intuition,\n\n2:36:33.960 --> 2:36:36.680\n in which way do you think you would be surprised?\n\n2:36:36.680 --> 2:36:38.520\n Like why were you wrong?\n\n2:36:38.520 --> 2:36:41.600\n We find out that you ended up being wrong.\n\n2:36:41.600 --> 2:36:43.880\n Like in which dimension?\n\n2:36:43.880 --> 2:36:48.400\n So like, is it because we can't see them?\n\n2:36:48.400 --> 2:36:51.320\n Is it because the nature of their intelligence\n\n2:36:51.320 --> 2:36:54.760\n or the nature of their life is totally different\n\n2:36:54.760 --> 2:36:56.760\n than we can possibly imagine?\n\n2:36:56.760 --> 2:37:00.680\n Is it because the,\n\n2:37:00.680 --> 2:37:02.640\n I mean, something about the great filters\n\n2:37:02.640 --> 2:37:04.440\n and surviving them,\n\n2:37:04.440 --> 2:37:08.760\n or maybe because we're being protected from signals,\n\n2:37:08.760 --> 2:37:13.760\n all those explanations for why we haven't heard\n\n2:37:15.120 --> 2:37:20.120\n a big, loud, like red light that says we're here.\n\n2:37:21.680 --> 2:37:23.520\n So there are actually two separate things there\n\n2:37:23.520 --> 2:37:24.720\n that I could be wrong about,\n\n2:37:24.720 --> 2:37:26.840\n two separate claims that I made, right?\n\n2:37:28.920 --> 2:37:32.240\n One of them is, I made the claim,\n\n2:37:32.240 --> 2:37:35.520\n I think most civilizations,\n\n2:37:36.960 --> 2:37:41.800\n when you're going from simple bacteria like things\n\n2:37:41.800 --> 2:37:46.800\n to space colonizing civilizations,\n\n2:37:47.840 --> 2:37:50.840\n they spend only a very, very tiny fraction\n\n2:37:50.840 --> 2:37:55.160\n of their life being where we are.\n\n2:37:55.160 --> 2:37:57.280\n That I could be wrong about.\n\n2:37:57.280 --> 2:37:58.760\n The other one I could be wrong about\n\n2:37:58.760 --> 2:38:01.520\n is the quite different statement that I think that actually\n\n2:38:01.520 --> 2:38:04.680\n I'm guessing that we are the only civilization\n\n2:38:04.680 --> 2:38:06.120\n in our observable universe\n\n2:38:06.120 --> 2:38:08.240\n from which light has reached us so far\n\n2:38:08.240 --> 2:38:12.320\n that's actually gotten far enough to invent telescopes.\n\n2:38:12.320 --> 2:38:13.960\n So let's talk about maybe both of them in turn\n\n2:38:13.960 --> 2:38:15.000\n because they really are different.\n\n2:38:15.000 --> 2:38:19.880\n The first one, if you look at the N equals one,\n\n2:38:19.880 --> 2:38:22.080\n the data point we have on this planet, right?\n\n2:38:22.080 --> 2:38:25.880\n So we spent four and a half billion years\n\n2:38:25.880 --> 2:38:28.240\n fluxing around on this planet with life, right?\n\n2:38:28.240 --> 2:38:32.080\n We got, and most of it was pretty lame stuff\n\n2:38:32.080 --> 2:38:33.640\n from an intelligence perspective,\n\n2:38:33.640 --> 2:38:38.160\n you know, it was bacteria and then the dinosaurs spent,\n\n2:38:39.200 --> 2:38:41.280\n then the things gradually accelerated, right?\n\n2:38:41.280 --> 2:38:43.600\n Then the dinosaurs spent over a hundred million years\n\n2:38:43.600 --> 2:38:46.960\n stomping around here without even inventing smartphones.\n\n2:38:46.960 --> 2:38:50.240\n And then very recently, you know,\n\n2:38:50.240 --> 2:38:52.120\n it's only, we've only spent 400 years\n\n2:38:52.120 --> 2:38:55.320\n going from Newton to us, right?\n\n2:38:55.320 --> 2:38:56.480\n In terms of technology.\n\n2:38:56.480 --> 2:39:00.160\n And look what we've done even, you know,\n\n2:39:00.160 --> 2:39:02.600\n when I was a little kid, there was no internet even.\n\n2:39:02.600 --> 2:39:05.880\n So it's, I think it's pretty likely for,\n\n2:39:05.880 --> 2:39:08.160\n in this case of this planet, right?\n\n2:39:08.160 --> 2:39:12.200\n That we're either gonna really get our act together\n\n2:39:12.200 --> 2:39:15.080\n and start spreading life into space, the century,\n\n2:39:15.080 --> 2:39:16.440\n and doing all sorts of great things,\n\n2:39:16.440 --> 2:39:18.880\n or we're gonna wipe out.\n\n2:39:18.880 --> 2:39:20.080\n It's a little hard.\n\n2:39:20.080 --> 2:39:23.480\n If I, I could be wrong in the sense that maybe\n\n2:39:23.480 --> 2:39:25.800\n what happened on this earth is very atypical.\n\n2:39:25.800 --> 2:39:28.520\n And for some reason, what's more common on other planets\n\n2:39:28.520 --> 2:39:31.440\n is that they spend an enormously long time\n\n2:39:31.440 --> 2:39:33.720\n futzing around with the ham radio and things,\n\n2:39:33.720 --> 2:39:36.200\n but they just never really take it to the next level\n\n2:39:36.200 --> 2:39:38.400\n for reasons I don't, I haven't understood.\n\n2:39:38.400 --> 2:39:40.200\n I'm humble and open to that.\n\n2:39:40.200 --> 2:39:42.880\n But I would bet at least 10 to one\n\n2:39:42.880 --> 2:39:45.160\n that our situation is more typical\n\n2:39:45.160 --> 2:39:46.760\n because the whole thing with Moore's law\n\n2:39:46.760 --> 2:39:48.160\n and accelerating technology,\n\n2:39:48.160 --> 2:39:50.040\n it's pretty obvious why it's happening.\n\n2:39:51.200 --> 2:39:52.880\n Everything that grows exponentially,\n\n2:39:52.880 --> 2:39:54.080\n we call it an explosion,\n\n2:39:54.080 --> 2:39:56.640\n whether it's a population explosion or a nuclear explosion,\n\n2:39:56.640 --> 2:39:58.000\n it's always caused by the same thing.\n\n2:39:58.000 --> 2:40:01.480\n It's that the next step triggers a step after that.\n\n2:40:01.480 --> 2:40:04.320\n So I, we, tomorrow's technology,\n\n2:40:04.320 --> 2:40:06.760\n today's technology enables tomorrow's technology\n\n2:40:06.760 --> 2:40:09.080\n and that enables the next level.\n\n2:40:09.080 --> 2:40:13.800\n And as I think, because the technology is always better,\n\n2:40:13.800 --> 2:40:16.160\n of course, the steps can come faster and faster.\n\n2:40:17.200 --> 2:40:19.160\n On the other question that I might be wrong about,\n\n2:40:19.160 --> 2:40:22.320\n that's the much more controversial one, I think.\n\n2:40:22.320 --> 2:40:24.920\n But before we close out on this thing about,\n\n2:40:24.920 --> 2:40:27.080\n if, the first one, if it's true\n\n2:40:27.080 --> 2:40:30.520\n that most civilizations spend only a very short amount\n\n2:40:30.520 --> 2:40:32.880\n of their total time in the stage, say,\n\n2:40:32.880 --> 2:40:35.440\n between inventing\n\n2:40:37.320 --> 2:40:40.760\n telescopes or mastering electricity\n\n2:40:40.760 --> 2:40:43.880\n and leaving there and doing space travel,\n\n2:40:43.880 --> 2:40:46.200\n if that's actually generally true,\n\n2:40:46.200 --> 2:40:49.000\n then that should apply also elsewhere out there.\n\n2:40:49.000 --> 2:40:51.040\n So we should be very, very,\n\n2:40:51.040 --> 2:40:52.920\n we should be very, very surprised\n\n2:40:52.920 --> 2:40:55.480\n if we find some random civilization\n\n2:40:55.480 --> 2:40:56.920\n and we happen to catch them exactly\n\n2:40:56.920 --> 2:40:58.800\n in that very, very short stage.\n\n2:40:58.800 --> 2:40:59.640\n It's much more likely\n\n2:40:59.640 --> 2:41:02.960\n that we find a planet full of bacteria.\n\n2:41:02.960 --> 2:41:05.560\n Or that we find some civilization\n\n2:41:05.560 --> 2:41:07.480\n that's already post biological\n\n2:41:07.480 --> 2:41:11.880\n and has done some really cool galactic construction projects\n\n2:41:11.880 --> 2:41:13.360\n in their galaxy.\n\n2:41:13.360 --> 2:41:15.200\n Would we be able to recognize them, do you think?\n\n2:41:15.200 --> 2:41:17.480\n Is it possible that we just can't,\n\n2:41:17.480 --> 2:41:20.040\n I mean, this post biological world,\n\n2:41:21.120 --> 2:41:23.520\n could it be just existing in some other dimension?\n\n2:41:23.520 --> 2:41:26.280\n It could be just all a virtual reality game\n\n2:41:26.280 --> 2:41:28.480\n for them or something, I don't know,\n\n2:41:28.480 --> 2:41:30.560\n that it changes completely\n\n2:41:30.560 --> 2:41:32.880\n where we won't be able to detect.\n\n2:41:32.880 --> 2:41:35.280\n We have to be honestly very humble about this.\n\n2:41:35.280 --> 2:41:39.000\n I think I said earlier the number one principle\n\n2:41:39.000 --> 2:41:40.840\n of being a scientist is you have to be humble\n\n2:41:40.840 --> 2:41:42.960\n and willing to acknowledge that everything we think,\n\n2:41:42.960 --> 2:41:45.040\n guess might be totally wrong.\n\n2:41:45.040 --> 2:41:46.960\n Of course, you could imagine some civilization\n\n2:41:46.960 --> 2:41:48.640\n where they all decide to become Buddhists\n\n2:41:48.640 --> 2:41:49.880\n and very inward looking\n\n2:41:49.880 --> 2:41:52.360\n and just move into their little virtual reality\n\n2:41:52.360 --> 2:41:55.120\n and not disturb the flora and fauna around them\n\n2:41:55.120 --> 2:41:58.120\n and we might not notice them.\n\n2:41:58.120 --> 2:41:59.960\n But this is a numbers game, right?\n\n2:41:59.960 --> 2:42:02.280\n If you have millions of civilizations out there\n\n2:42:02.280 --> 2:42:03.680\n or billions of them,\n\n2:42:03.680 --> 2:42:08.080\n all it takes is one with a more ambitious mentality\n\n2:42:08.080 --> 2:42:10.280\n that decides, hey, we are gonna go out\n\n2:42:10.280 --> 2:42:15.280\n and settle a bunch of other solar systems\n\n2:42:15.520 --> 2:42:17.560\n and maybe galaxies.\n\n2:42:17.560 --> 2:42:18.440\n And then it doesn't matter\n\n2:42:18.440 --> 2:42:19.640\n if they're a bunch of quiet Buddhists,\n\n2:42:19.640 --> 2:42:23.040\n we're still gonna notice that expansionist one, right?\n\n2:42:23.040 --> 2:42:26.560\n And it seems like quite the stretch to assume that,\n\n2:42:26.560 --> 2:42:28.120\n now we know even in our own galaxy\n\n2:42:28.120 --> 2:42:33.120\n that there are probably a billion or more planets\n\n2:42:33.120 --> 2:42:35.280\n that are pretty Earth like.\n\n2:42:35.280 --> 2:42:37.680\n And many of them are formed over a billion years\n\n2:42:37.680 --> 2:42:40.640\n before ours, so had a big head start.\n\n2:42:40.640 --> 2:42:43.600\n So if you actually assume also\n\n2:42:43.600 --> 2:42:46.120\n that life happens kind of automatically\n\n2:42:46.120 --> 2:42:47.360\n on an Earth like planet,\n\n2:42:48.440 --> 2:42:52.080\n I think it's quite the stretch to then go and say,\n\n2:42:52.080 --> 2:42:55.280\n okay, so there are another billion civilizations out there\n\n2:42:55.280 --> 2:42:56.840\n that also have our level of tech\n\n2:42:56.840 --> 2:42:59.280\n and they all decided to become Buddhists\n\n2:42:59.280 --> 2:43:02.880\n and not a single one decided to go Hitler on the galaxy\n\n2:43:02.880 --> 2:43:05.280\n and say, we need to go out and colonize\n\n2:43:05.280 --> 2:43:08.840\n or not a single one decided for more benevolent reasons\n\n2:43:08.840 --> 2:43:10.520\n to go out and get more resources.\n\n2:43:11.480 --> 2:43:13.840\n That seems like a bit of a stretch, frankly.\n\n2:43:13.840 --> 2:43:16.560\n And this leads into the second thing\n\n2:43:16.560 --> 2:43:18.560\n you challenged me that I might be wrong about,\n\n2:43:18.560 --> 2:43:22.320\n how rare or common is life, you know?\n\n2:43:22.320 --> 2:43:25.120\n So Francis Drake, when he wrote down the Drake equation,\n\n2:43:25.120 --> 2:43:27.560\n multiplied together a huge number of factors\n\n2:43:27.560 --> 2:43:29.320\n and then we don't know any of them.\n\n2:43:29.320 --> 2:43:31.480\n So we know even less about what you get\n\n2:43:31.480 --> 2:43:33.680\n when you multiply together the whole product.\n\n2:43:35.120 --> 2:43:37.200\n Since then, a lot of those factors\n\n2:43:37.200 --> 2:43:38.880\n have become much better known.\n\n2:43:38.880 --> 2:43:40.840\n One of his big uncertainties was\n\n2:43:40.840 --> 2:43:44.360\n how common is it that a solar system even has a planet?\n\n2:43:44.360 --> 2:43:46.280\n Well, now we know it very common.\n\n2:43:46.280 --> 2:43:48.320\n Earth like planets, we know we have better.\n\n2:43:48.320 --> 2:43:50.440\n There are a dime a dozen, there are many, many of them,\n\n2:43:50.440 --> 2:43:52.080\n even in our galaxy.\n\n2:43:52.080 --> 2:43:55.000\n At the same time, you know, we have thanks to,\n\n2:43:55.000 --> 2:43:58.840\n I'm a big supporter of the SETI project and its cousins\n\n2:43:58.840 --> 2:44:00.520\n and I think we should keep doing this\n\n2:44:00.520 --> 2:44:02.400\n and we've learned a lot.\n\n2:44:02.400 --> 2:44:03.800\n We've learned that so far,\n\n2:44:03.800 --> 2:44:08.040\n all we have is still unconvincing hints, nothing more, right?\n\n2:44:08.040 --> 2:44:10.320\n And there are certainly many scenarios\n\n2:44:10.320 --> 2:44:13.080\n where it would be dead obvious.\n\n2:44:13.080 --> 2:44:15.920\n If there were a hundred million\n\n2:44:15.920 --> 2:44:19.000\n other human like civilizations in our galaxy,\n\n2:44:19.000 --> 2:44:21.600\n it would not be that hard to notice some of them\n\n2:44:21.600 --> 2:44:23.440\n with today's technology and we haven't, right?\n\n2:44:23.440 --> 2:44:27.720\n So what we can say is, well, okay,\n\n2:44:27.720 --> 2:44:30.560\n we can rule out that there is a human level of civilization\n\n2:44:30.560 --> 2:44:34.120\n on the moon and in fact, the many nearby solar systems\n\n2:44:34.120 --> 2:44:37.600\n where we cannot rule out, of course,\n\n2:44:37.600 --> 2:44:41.560\n that there is something like Earth sitting in a galaxy\n\n2:44:41.560 --> 2:44:43.120\n five billion light years away.\n\n2:44:45.120 --> 2:44:46.400\n But we've ruled out a lot\n\n2:44:46.400 --> 2:44:48.480\n and that's already kind of shocking\n\n2:44:48.480 --> 2:44:50.320\n given that there are all these planets there, you know?\n\n2:44:50.320 --> 2:44:51.440\n So like, where are they?\n\n2:44:51.440 --> 2:44:52.280\n Where are they all?\n\n2:44:52.280 --> 2:44:54.880\n That's the classic Fermi paradox.\n\n2:44:54.880 --> 2:44:59.240\n And so my argument, which might very well be wrong,\n\n2:44:59.240 --> 2:45:01.400\n it's very simple really, it just goes like this.\n\n2:45:01.400 --> 2:45:03.000\n Okay, we have no clue about this.\n\n2:45:05.240 --> 2:45:07.800\n It could be the probability of getting life\n\n2:45:07.800 --> 2:45:11.240\n on a random planet, it could be 10 to the minus one\n\n2:45:11.240 --> 2:45:14.680\n a priori or 10 to the minus five, 10, 10 to the minus 20,\n\n2:45:14.680 --> 2:45:17.400\n 10 to the minus 30, 10 to the minus 40.\n\n2:45:17.400 --> 2:45:20.240\n Basically every order of magnitude is about equally likely.\n\n2:45:21.400 --> 2:45:24.120\n When then do the math and ask the question,\n\n2:45:24.120 --> 2:45:27.400\n how close is our nearest neighbor?\n\n2:45:27.400 --> 2:45:30.520\n It's again, equally likely that it's 10 to the 10 meters away,\n\n2:45:30.520 --> 2:45:33.440\n 10 to 20 meters away, 10 to the 30 meters away.\n\n2:45:33.440 --> 2:45:35.640\n We have some nerdy ways of talking about this\n\n2:45:35.640 --> 2:45:38.080\n with Bayesian statistics and a uniform log prior,\n\n2:45:38.080 --> 2:45:39.360\n but that's irrelevant.\n\n2:45:39.360 --> 2:45:42.040\n This is the simple basic argument.\n\n2:45:42.040 --> 2:45:43.320\n And now comes the data.\n\n2:45:43.320 --> 2:45:46.320\n So we can say, okay, there are all these orders\n\n2:45:46.320 --> 2:45:49.280\n of magnitude, 10 to the 26 meters away,\n\n2:45:49.280 --> 2:45:51.960\n there's the edge of our observable universe.\n\n2:45:51.960 --> 2:45:54.840\n If it's farther than that, light hasn't even reached us yet.\n\n2:45:54.840 --> 2:45:58.040\n If it's less than 10 to the 16 meters away,\n\n2:45:58.040 --> 2:46:02.320\n well, it's within Earth's,\n\n2:46:02.320 --> 2:46:03.800\n it's no farther away than the sun.\n\n2:46:03.800 --> 2:46:05.240\n We can definitely rule that out.\n\n2:46:07.200 --> 2:46:08.520\n So I think about it like this,\n\n2:46:08.520 --> 2:46:10.840\n a priori before we looked at the telescopes,\n\n2:46:11.840 --> 2:46:14.320\n it could be 10 to the 10 meters, 10 to the 20,\n\n2:46:14.320 --> 2:46:16.520\n 10 to the 30, 10 to the 40, 10 to the 50, 10 to blah, blah, blah.\n\n2:46:16.520 --> 2:46:18.040\n Equally likely anywhere here.\n\n2:46:18.040 --> 2:46:21.760\n And now we've ruled out like this chunk.\n\n2:46:21.760 --> 2:46:26.760\n And here is the edge of our observable universe already.\n\n2:46:27.880 --> 2:46:30.560\n So I'm certainly not saying I don't think\n\n2:46:30.560 --> 2:46:32.480\n there's any life elsewhere in space.\n\n2:46:32.480 --> 2:46:33.680\n If space is infinite,\n\n2:46:33.680 --> 2:46:35.640\n then you're basically a hundred percent guaranteed\n\n2:46:35.640 --> 2:46:39.200\n that there is, but the probability that there is life,\n\n2:46:41.200 --> 2:46:42.280\n that the nearest neighbor,\n\n2:46:42.280 --> 2:46:43.760\n it happens to be in this little region\n\n2:46:43.760 --> 2:46:47.120\n between where we would have seen it already\n\n2:46:47.120 --> 2:46:48.680\n and where we will never see it.\n\n2:46:48.680 --> 2:46:51.920\n There's actually significantly less than one, I think.\n\n2:46:51.920 --> 2:46:54.280\n And I think there's a moral lesson from this,\n\n2:46:54.280 --> 2:46:55.840\n which is really important,\n\n2:46:55.840 --> 2:47:00.120\n which is to be good stewards of this planet\n\n2:47:00.120 --> 2:47:01.440\n and this shot we've had.\n\n2:47:01.440 --> 2:47:03.640\n It can be very dangerous to say,\n\n2:47:03.640 --> 2:47:07.640\n oh, it's fine if we nuke our planet or ruin the climate\n\n2:47:07.640 --> 2:47:10.280\n or mess it up with unaligned AI,\n\n2:47:10.280 --> 2:47:15.160\n because I know there is this nice Star Trek fleet out there.\n\n2:47:15.160 --> 2:47:18.040\n They're gonna swoop in and take over where we failed.\n\n2:47:18.040 --> 2:47:19.840\n Just like it wasn't the big deal\n\n2:47:19.840 --> 2:47:23.040\n that the Easter Island losers wiped themselves out.\n\n2:47:23.040 --> 2:47:25.200\n That's a dangerous way of lulling yourself\n\n2:47:25.200 --> 2:47:26.600\n into false sense of security.\n\n2:47:27.760 --> 2:47:32.000\n If it's actually the case that it might be up to us\n\n2:47:32.000 --> 2:47:35.000\n and only us, the whole future of intelligent life\n\n2:47:35.000 --> 2:47:36.360\n in our observable universe,\n\n2:47:37.680 --> 2:47:42.440\n then I think it really puts a lot of responsibility\n\n2:47:42.440 --> 2:47:43.280\n on our shoulders.\n\n2:47:43.280 --> 2:47:45.320\n It's inspiring, it's a little bit terrifying,\n\n2:47:45.320 --> 2:47:46.480\n but it's also inspiring.\n\n2:47:46.480 --> 2:47:48.600\n But it's empowering, I think, most of all,\n\n2:47:48.600 --> 2:47:50.240\n because the biggest problem today is,\n\n2:47:50.240 --> 2:47:51.960\n I see this even when I teach,\n\n2:47:53.120 --> 2:47:56.360\n so many people feel that it doesn't matter what they do\n\n2:47:56.360 --> 2:47:58.760\n or we do, we feel disempowered.\n\n2:47:58.760 --> 2:48:00.120\n Oh, it makes no difference.\n\n2:48:02.560 --> 2:48:05.080\n This is about as far from that as you can come.\n\n2:48:05.080 --> 2:48:06.880\n But we realize that what we do\n\n2:48:07.760 --> 2:48:12.200\n on our little spinning ball here in our lifetime\n\n2:48:12.200 --> 2:48:15.440\n could make the difference for the entire future of life\n\n2:48:15.440 --> 2:48:17.080\n in our universe.\n\n2:48:17.080 --> 2:48:18.720\n How empowering is that?\n\n2:48:18.720 --> 2:48:20.280\n Yeah, survival of consciousness.\n\n2:48:20.280 --> 2:48:25.280\n I mean, a very similar kind of empowering aspect\n\n2:48:25.840 --> 2:48:27.680\n of the Drake equation is,\n\n2:48:27.680 --> 2:48:31.120\n say there is a huge number of intelligent civilizations\n\n2:48:31.120 --> 2:48:32.920\n that spring up everywhere,\n\n2:48:32.920 --> 2:48:34.760\n but because of the Drake equation,\n\n2:48:34.760 --> 2:48:38.000\n which is the lifetime of a civilization,\n\n2:48:38.000 --> 2:48:39.880\n maybe many of them hit a wall.\n\n2:48:39.880 --> 2:48:43.360\n And just like you said, it's clear that that,\n\n2:48:43.360 --> 2:48:45.920\n for us, the great filter,\n\n2:48:45.920 --> 2:48:49.040\n the one possible great filter seems to be coming\n\n2:48:49.040 --> 2:48:51.240\n in the next 100 years.\n\n2:48:51.240 --> 2:48:53.720\n So it's also empowering to say,\n\n2:48:53.720 --> 2:48:58.720\n okay, well, we have a chance to not,\n\n2:48:58.720 --> 2:49:00.120\n I mean, the way great filters work,\n\n2:49:00.120 --> 2:49:02.080\n they just get most of them.\n\n2:49:02.080 --> 2:49:02.920\n Exactly.\n\n2:49:02.920 --> 2:49:06.120\n Nick Bostrom has articulated this really beautifully too.\n\n2:49:06.120 --> 2:49:09.480\n Every time yet another search for life on Mars\n\n2:49:09.480 --> 2:49:11.120\n comes back negative or something,\n\n2:49:11.120 --> 2:49:14.760\n I'm like, yes, yes.\n\n2:49:14.760 --> 2:49:17.840\n Our odds for us surviving is the best.\n\n2:49:17.840 --> 2:49:20.960\n You already made the argument in broad brush there, right?\n\n2:49:20.960 --> 2:49:22.560\n But just to unpack it, right?\n\n2:49:22.560 --> 2:49:24.520\n The point is we already know\n\n2:49:26.880 --> 2:49:28.640\n there is a crap ton of planets out there\n\n2:49:28.640 --> 2:49:29.640\n that are Earth like,\n\n2:49:29.640 --> 2:49:33.160\n and we also know that most of them do not seem\n\n2:49:33.160 --> 2:49:35.080\n to have anything like our kind of life on them.\n\n2:49:35.080 --> 2:49:37.240\n So what went wrong?\n\n2:49:37.240 --> 2:49:39.520\n There's clearly one step along the evolutionary,\n\n2:49:39.520 --> 2:49:42.360\n at least one filter or roadblock\n\n2:49:42.360 --> 2:49:45.600\n in going from no life to spacefaring life.\n\n2:49:45.600 --> 2:49:48.160\n And where is it?\n\n2:49:48.160 --> 2:49:50.720\n Is it in front of us or is it behind us, right?\n\n2:49:51.640 --> 2:49:54.080\n If there's no filter behind us,\n\n2:49:54.080 --> 2:49:59.080\n and we keep finding all sorts of little mice on Mars\n\n2:50:00.520 --> 2:50:01.880\n or whatever, right?\n\n2:50:01.880 --> 2:50:03.120\n That's actually very depressing\n\n2:50:03.120 --> 2:50:04.440\n because that makes it much more likely\n\n2:50:04.440 --> 2:50:06.280\n that the filter is in front of us.\n\n2:50:06.280 --> 2:50:08.080\n And that what actually is going on\n\n2:50:08.080 --> 2:50:11.080\n is like the ultimate dark joke\n\n2:50:11.080 --> 2:50:13.800\n that whenever a civilization\n\n2:50:13.800 --> 2:50:15.640\n invents sufficiently powerful tech,\n\n2:50:15.640 --> 2:50:17.240\n it's just, you just set your clock.\n\n2:50:17.240 --> 2:50:19.160\n And then after a little while it goes poof\n\n2:50:19.160 --> 2:50:21.840\n for one reason or other and wipes itself out.\n\n2:50:21.840 --> 2:50:24.240\n Now wouldn't that be like utterly depressing\n\n2:50:24.240 --> 2:50:26.120\n if we're actually doomed?\n\n2:50:26.120 --> 2:50:29.720\n Whereas if it turns out that there is a really,\n\n2:50:29.720 --> 2:50:31.720\n there is a great filter early on\n\n2:50:31.720 --> 2:50:33.960\n that for whatever reason seems to be really hard\n\n2:50:33.960 --> 2:50:38.960\n to get to the stage of sexually reproducing organisms\n\n2:50:39.160 --> 2:50:43.320\n or even the first ribosome or whatever, right?\n\n2:50:43.320 --> 2:50:47.160\n Or maybe you have lots of planets with dinosaurs and cows,\n\n2:50:47.160 --> 2:50:48.880\n but for some reason they tend to get stuck there\n\n2:50:48.880 --> 2:50:50.840\n and never invent smartphones.\n\n2:50:50.840 --> 2:50:55.200\n All of those are huge boosts for our own odds\n\n2:50:55.200 --> 2:50:58.840\n because been there done that, you know?\n\n2:50:58.840 --> 2:51:01.720\n It doesn't matter how hard or unlikely it was\n\n2:51:01.720 --> 2:51:03.800\n that we got past that roadblock\n\n2:51:03.800 --> 2:51:05.120\n because we already did.\n\n2:51:05.120 --> 2:51:07.520\n And then that makes it likely\n\n2:51:07.520 --> 2:51:11.440\n that the future is in our own hands, we're not doomed.\n\n2:51:11.440 --> 2:51:14.800\n So that's why I think the fact\n\n2:51:14.800 --> 2:51:18.280\n that life is rare in the universe,\n\n2:51:18.280 --> 2:51:21.440\n it's not just something that there is some evidence for,\n\n2:51:21.440 --> 2:51:24.560\n but also something we should actually hope for.\n\n2:51:26.680 --> 2:51:29.920\n So that's the end, the mortality,\n\n2:51:29.920 --> 2:51:31.520\n the death of human civilization\n\n2:51:31.520 --> 2:51:33.120\n that we've been discussing in life,\n\n2:51:33.120 --> 2:51:36.680\n maybe prospering beyond any kind of great filter.\n\n2:51:36.680 --> 2:51:39.440\n Do you think about your own death?\n\n2:51:39.440 --> 2:51:44.440\n Does it make you sad that you may not witness some of the,\n\n2:51:45.760 --> 2:51:47.440\n you know, you lead a research group\n\n2:51:47.440 --> 2:51:49.040\n on working some of the biggest questions\n\n2:51:49.040 --> 2:51:51.080\n in the universe actually,\n\n2:51:51.080 --> 2:51:53.720\n both on the physics and the AI side?\n\n2:51:53.720 --> 2:51:55.560\n Does it make you sad that you may not be able\n\n2:51:55.560 --> 2:51:58.840\n to see some of these exciting things come to fruition\n\n2:51:58.840 --> 2:52:00.640\n that we've been talking about?\n\n2:52:00.640 --> 2:52:03.920\n Of course, of course it sucks, the fact that I'm gonna die.\n\n2:52:04.840 --> 2:52:07.200\n I remember once when I was much younger,\n\n2:52:07.200 --> 2:52:10.800\n my dad made this remark that life is fundamentally tragic.\n\n2:52:10.800 --> 2:52:13.080\n And I'm like, what are you talking about, daddy?\n\n2:52:13.080 --> 2:52:15.640\n And then many years later, I felt,\n\n2:52:15.640 --> 2:52:17.320\n now I feel I totally understand what he means.\n\n2:52:17.320 --> 2:52:19.040\n You know, we grow up, we're little kids\n\n2:52:19.040 --> 2:52:21.920\n and everything is infinite and it's so cool.\n\n2:52:21.920 --> 2:52:24.720\n And then suddenly we find out that actually, you know,\n\n2:52:25.800 --> 2:52:26.840\n you got to serve only,\n\n2:52:26.840 --> 2:52:30.280\n this is the, you're gonna get game over at some point.\n\n2:52:30.280 --> 2:52:35.280\n So of course it's something that's sad.\n\n2:52:36.400 --> 2:52:37.240\n Are you afraid?\n\n2:52:42.640 --> 2:52:46.000\n No, not in the sense that I think anything terrible\n\n2:52:46.000 --> 2:52:48.240\n is gonna happen after I die or anything like that.\n\n2:52:48.240 --> 2:52:50.960\n No, I think it's really gonna be a game over,\n\n2:52:50.960 --> 2:52:55.960\n but it's more that it makes me very acutely aware\n\n2:52:56.280 --> 2:52:57.920\n of what a wonderful gift this is\n\n2:52:57.920 --> 2:53:00.200\n that I get to be alive right now.\n\n2:53:00.200 --> 2:53:04.680\n And is a steady reminder to just live life to the fullest\n\n2:53:04.680 --> 2:53:08.000\n and really enjoy it because it is finite, you know.\n\n2:53:08.000 --> 2:53:11.240\n And I think actually, and we know we all get\n\n2:53:11.240 --> 2:53:14.280\n the regular reminders when someone near and dear to us dies\n\n2:53:14.280 --> 2:53:19.280\n that one day it's gonna be our turn.\n\n2:53:19.560 --> 2:53:21.480\n It adds this kind of focus.\n\n2:53:21.480 --> 2:53:23.680\n I wonder what it would feel like actually\n\n2:53:23.680 --> 2:53:26.960\n to be an immortal being if they might even enjoy\n\n2:53:26.960 --> 2:53:29.440\n some of the wonderful things of life a little bit less\n\n2:53:29.440 --> 2:53:33.400\n just because there isn't that.\n\n2:53:33.400 --> 2:53:34.320\n Finiteness?\n\n2:53:34.320 --> 2:53:35.160\n Yeah.\n\n2:53:35.160 --> 2:53:38.040\n Do you think that could be a feature, not a bug,\n\n2:53:38.040 --> 2:53:42.040\n the fact that we beings are finite?\n\n2:53:42.040 --> 2:53:44.320\n Maybe there's lessons for engineering\n\n2:53:44.320 --> 2:53:46.940\n in artificial intelligence systems as well\n\n2:53:46.940 --> 2:53:48.400\n that are conscious.\n\n2:53:48.400 --> 2:53:53.400\n Like do you think it makes, is it possible\n\n2:53:53.920 --> 2:53:56.960\n that the reason the pistachio ice cream is delicious\n\n2:53:56.960 --> 2:53:59.920\n is the fact that you're going to die one day\n\n2:53:59.920 --> 2:54:03.720\n and you will not have all the pistachio ice cream\n\n2:54:03.720 --> 2:54:06.200\n that you could eat because of that fact?\n\n2:54:06.200 --> 2:54:07.560\n Well, let me say two things.\n\n2:54:07.560 --> 2:54:09.660\n First of all, it's actually quite profound\n\n2:54:09.660 --> 2:54:10.500\n what you're saying.\n\n2:54:10.500 --> 2:54:12.300\n I do think I appreciate the pistachio ice cream\n\n2:54:12.300 --> 2:54:14.400\n a lot more knowing that I will,\n\n2:54:14.400 --> 2:54:17.760\n there's only a finite number of times I get to enjoy that.\n\n2:54:17.760 --> 2:54:19.900\n And I can only remember a finite number of times\n\n2:54:19.900 --> 2:54:20.740\n in the past.\n\n2:54:21.720 --> 2:54:25.120\n And moreover, my life is not so long\n\n2:54:25.120 --> 2:54:26.800\n that it just starts to feel like things are repeating\n\n2:54:26.800 --> 2:54:28.120\n themselves in general.\n\n2:54:28.120 --> 2:54:30.520\n It's so new and fresh.\n\n2:54:30.520 --> 2:54:35.520\n I also think though that death is a little bit overrated\n\n2:54:36.400 --> 2:54:41.400\n in the sense that it comes from a sort of outdated view\n\n2:54:42.020 --> 2:54:45.640\n of physics and what life actually is.\n\n2:54:45.640 --> 2:54:49.120\n Because if you ask, okay, what is it that's gonna die\n\n2:54:49.120 --> 2:54:52.040\n exactly, what am I really?\n\n2:54:52.040 --> 2:54:56.000\n When I say I feel sad about the idea of myself dying,\n\n2:54:56.000 --> 2:54:59.180\n am I really sad that this skin cell here is gonna die?\n\n2:54:59.180 --> 2:55:01.600\n Of course not, because it's gonna die next week anyway\n\n2:55:01.600 --> 2:55:04.020\n and I'll grow a new one, right?\n\n2:55:04.020 --> 2:55:08.440\n And it's not any of my cells that I'm associating really\n\n2:55:08.440 --> 2:55:11.000\n with who I really am.\n\n2:55:11.000 --> 2:55:14.060\n Nor is it any of my atoms or quarks or electrons.\n\n2:55:15.640 --> 2:55:19.380\n In fact, basically all of my atoms get replaced\n\n2:55:19.380 --> 2:55:20.520\n on a regular basis, right?\n\n2:55:20.520 --> 2:55:22.880\n So what is it that's really me\n\n2:55:22.880 --> 2:55:24.320\n from a more modern physics perspective?\n\n2:55:24.320 --> 2:55:28.800\n It's the information in processing me.\n\n2:55:28.800 --> 2:55:31.520\n That's where my memory, that's my memories,\n\n2:55:31.520 --> 2:55:36.520\n that's my values, my dreams, my passion, my love.\n\n2:55:40.560 --> 2:55:43.580\n That's what's really fundamentally me.\n\n2:55:43.580 --> 2:55:48.580\n And frankly, not all of that will die when my body dies.\n\n2:55:48.580 --> 2:55:53.580\n Like Richard Feynman, for example, his body died of cancer,\n\n2:55:55.100 --> 2:55:59.720\n but many of his ideas that he felt made him very him\n\n2:55:59.720 --> 2:56:01.400\n actually live on.\n\n2:56:01.400 --> 2:56:04.100\n This is my own little personal tribute to Richard Feynman.\n\n2:56:04.100 --> 2:56:07.500\n I try to keep a little bit of him alive in myself.\n\n2:56:07.500 --> 2:56:09.620\n I've even quoted him today, right?\n\n2:56:09.620 --> 2:56:11.740\n Yeah, he almost came alive for a brief moment\n\n2:56:11.740 --> 2:56:13.320\n in this conversation, yeah.\n\n2:56:13.320 --> 2:56:17.500\n Yeah, and this honestly gives me some solace.\n\n2:56:17.500 --> 2:56:20.780\n When I work as a teacher, I feel,\n\n2:56:20.780 --> 2:56:25.780\n if I can actually share a bit about myself\n\n2:56:25.820 --> 2:56:30.740\n that my students feel worthy enough to copy and adopt\n\n2:56:30.740 --> 2:56:33.140\n as some part of things that they know\n\n2:56:33.140 --> 2:56:36.140\n or they believe or aspire to,\n\n2:56:36.140 --> 2:56:39.540\n now I live on also a little bit in them, right?\n\n2:56:39.540 --> 2:56:44.540\n And so being a teacher is a little bit\n\n2:56:44.540 --> 2:56:49.540\n of what I, that's something also that contributes\n\n2:56:49.740 --> 2:56:53.740\n to making me a little teeny bit less mortal, right?\n\n2:56:53.740 --> 2:56:56.740\n Because I'm not, at least not all gonna die all at once,\n\n2:56:56.740 --> 2:56:57.580\n right?\n\n2:56:57.580 --> 2:56:59.820\n And I find that a beautiful tribute to people\n\n2:56:59.820 --> 2:57:01.020\n we do not respect.\n\n2:57:01.020 --> 2:57:05.740\n If we can remember them and carry in us\n\n2:57:05.740 --> 2:57:10.260\n the things that we felt was the most awesome about them,\n\n2:57:10.260 --> 2:57:11.620\n right, then they live on.\n\n2:57:11.620 --> 2:57:13.580\n And I'm getting a bit emotional here,\n\n2:57:13.580 --> 2:57:16.140\n but it's a very beautiful idea you bring up there.\n\n2:57:16.140 --> 2:57:19.620\n I think we should stop this old fashioned materialism\n\n2:57:19.620 --> 2:57:24.620\n and just equate who we are with our quirks and electrons.\n\n2:57:25.220 --> 2:57:27.820\n There's no scientific basis for that really.\n\n2:57:27.820 --> 2:57:31.180\n And it's also very uninspiring.\n\n2:57:33.180 --> 2:57:36.980\n Now, if you look a little bit towards the future, right?\n\n2:57:36.980 --> 2:57:40.740\n One thing which really sucks about humans dying is that even\n\n2:57:40.740 --> 2:57:43.300\n though some of their teachings and memories and stories\n\n2:57:43.300 --> 2:57:47.540\n and ethics and so on will be copied by those around them,\n\n2:57:47.540 --> 2:57:50.260\n hopefully, a lot of it can't be copied\n\n2:57:50.260 --> 2:57:51.980\n and just dies with them, with their brain.\n\n2:57:51.980 --> 2:57:53.140\n And that really sucks.\n\n2:57:53.140 --> 2:57:56.860\n That's the fundamental reason why we find it so tragic\n\n2:57:56.860 --> 2:57:59.660\n when someone goes from having all this information there\n\n2:57:59.660 --> 2:58:03.460\n to the more just gone, ruined, right?\n\n2:58:03.460 --> 2:58:07.460\n With more post biological intelligence,\n\n2:58:07.460 --> 2:58:09.940\n that's going to shift a lot, right?\n\n2:58:10.940 --> 2:58:13.980\n The only reason it's so hard to make a backup of your brain\n\n2:58:13.980 --> 2:58:15.380\n in its entirety is exactly\n\n2:58:15.380 --> 2:58:17.580\n because it wasn't built for that, right?\n\n2:58:17.580 --> 2:58:21.540\n If you have a future machine intelligence,\n\n2:58:21.540 --> 2:58:24.300\n there's no reason for why it has to die at all.\n\n2:58:24.300 --> 2:58:28.300\n If you want to copy it, whatever it is,\n\n2:58:28.300 --> 2:58:30.780\n into some other machine intelligence,\n\n2:58:30.780 --> 2:58:35.780\n whatever it is, into some other quark blob, right?\n\n2:58:36.660 --> 2:58:39.540\n You can copy not just some of it, but all of it, right?\n\n2:58:39.540 --> 2:58:42.180\n And so in that sense,\n\n2:58:45.020 --> 2:58:48.300\n you can get immortality because all the information\n\n2:58:48.300 --> 2:58:51.180\n can be copied out of any individual entity.\n\n2:58:51.940 --> 2:58:54.220\n And it's not just mortality that will change\n\n2:58:54.220 --> 2:58:56.900\n if we get to more post biological life.\n\n2:58:56.900 --> 2:59:01.900\n It's also with that, very much the whole individualism\n\n2:59:03.180 --> 2:59:04.020\n we have now, right?\n\n2:59:04.020 --> 2:59:05.740\n The reason that we make such a big difference\n\n2:59:05.740 --> 2:59:09.100\n between me and you is exactly because\n\n2:59:09.100 --> 2:59:10.940\n we're a little bit limited in how much we can copy.\n\n2:59:10.940 --> 2:59:13.300\n Like I would just love to go like this\n\n2:59:13.300 --> 2:59:17.780\n and copy your Russian skills, Russian speaking skills.\n\n2:59:17.780 --> 2:59:18.820\n Wouldn't it be awesome?\n\n2:59:18.820 --> 2:59:21.980\n But I can't, I have to actually work for years\n\n2:59:21.980 --> 2:59:23.900\n if I want to get better on it.\n\n2:59:23.900 --> 2:59:27.940\n But if we were robots.\n\n2:59:27.940 --> 2:59:31.820\n Just copy and paste freely, then that loses completely.\n\n2:59:31.820 --> 2:59:35.140\n It washes away the sense of what immortality is.\n\n2:59:35.140 --> 2:59:37.460\n And also individuality a little bit, right?\n\n2:59:37.460 --> 2:59:39.300\n We would start feeling much more,\n\n2:59:40.620 --> 2:59:43.540\n maybe we would feel much more collaborative with each other\n\n2:59:43.540 --> 2:59:45.620\n if we can just, hey, you know, I'll give you my Russian,\n\n2:59:45.620 --> 2:59:46.540\n you can give me your Russian\n\n2:59:46.540 --> 2:59:47.940\n and I'll give you whatever,\n\n2:59:47.940 --> 2:59:50.220\n and suddenly you can speak Swedish.\n\n2:59:50.220 --> 2:59:52.060\n Maybe that's less a bad trade for you,\n\n2:59:52.060 --> 2:59:54.620\n but whatever else you want from my brain, right?\n\n2:59:54.620 --> 2:59:58.060\n And there've been a lot of sci fi stories\n\n2:59:58.060 --> 2:59:59.540\n about hive minds and so on,\n\n2:59:59.540 --> 3:00:02.140\n where people, where experiences\n\n3:00:02.140 --> 3:00:03.620\n can be more broadly shared.\n\n3:00:05.500 --> 3:00:08.540\n And I think one, we don't,\n\n3:00:08.540 --> 3:00:12.140\n I don't pretend to know what it would feel like\n\n3:00:12.140 --> 3:00:16.940\n to be a super intelligent machine,\n\n3:00:16.940 --> 3:00:20.420\n but I'm quite confident that however it feels\n\n3:00:20.420 --> 3:00:22.420\n about mortality and individuality\n\n3:00:22.420 --> 3:00:24.940\n will be very, very different from how it is for us.\n\n3:00:26.660 --> 3:00:30.500\n Well, for us, mortality and finiteness\n\n3:00:30.500 --> 3:00:34.100\n seems to be pretty important at this particular moment.\n\n3:00:34.100 --> 3:00:37.460\n And so all good things must come to an end.\n\n3:00:37.460 --> 3:00:39.100\n Just like this conversation, Max.\n\n3:00:39.100 --> 3:00:40.660\n I saw that coming.\n\n3:00:40.660 --> 3:00:44.660\n Sorry, this is the world's worst translation.\n\n3:00:44.660 --> 3:00:45.820\n I could talk to you forever.\n\n3:00:45.820 --> 3:00:49.100\n It's such a huge honor that you've spent time with me.\n\n3:00:49.100 --> 3:00:50.140\n The honor is mine.\n\n3:00:50.140 --> 3:00:53.380\n Thank you so much for getting me essentially\n\n3:00:53.380 --> 3:00:55.980\n to start this podcast by doing the first conversation,\n\n3:00:55.980 --> 3:00:58.500\n making me realize falling in love\n\n3:00:58.500 --> 3:01:01.140\n with conversation in itself.\n\n3:01:01.140 --> 3:01:03.220\n And thank you so much for inspiring\n\n3:01:03.220 --> 3:01:05.380\n so many people in the world with your books,\n\n3:01:05.380 --> 3:01:07.740\n with your research, with your talking,\n\n3:01:07.740 --> 3:01:12.740\n and with the other, like this ripple effect of friends,\n\n3:01:12.780 --> 3:01:15.460\n including Elon and everybody else that you inspire.\n\n3:01:15.460 --> 3:01:18.140\n So thank you so much for talking today.\n\n3:01:18.140 --> 3:01:21.540\n Thank you, I feel so fortunate\n\n3:01:21.540 --> 3:01:23.620\n that you're doing this podcast\n\n3:01:23.620 --> 3:01:27.780\n and getting so many interesting voices out there\n\n3:01:27.780 --> 3:01:30.940\n into the ether and not just the five second sound bites,\n\n3:01:30.940 --> 3:01:33.060\n but so many of the interviews I've watched you do.\n\n3:01:33.060 --> 3:01:36.140\n You really let people go in into depth\n\n3:01:36.140 --> 3:01:38.660\n in a way which we sorely need in this day and age.\n\n3:01:38.660 --> 3:01:41.740\n That I got to be number one, I feel super honored.\n\n3:01:41.740 --> 3:01:43.500\n Yeah, you started it.\n\n3:01:43.500 --> 3:01:44.660\n Thank you so much, Max.\n\n3:01:45.620 --> 3:01:47.180\n Thanks for listening to this conversation\n\n3:01:47.180 --> 3:01:50.260\n with Max Tegmark, and thank you to our sponsors,\n\n3:01:50.260 --> 3:01:54.860\n the Jordan Harbinger Show, For Sigmatic Mushroom Coffee,\n\n3:01:54.860 --> 3:01:58.940\n BetterHelp Online Therapy, and ExpressVPN.\n\n3:01:58.940 --> 3:02:03.940\n So the choice is wisdom, caffeine, sanity, or privacy.\n\n3:02:04.420 --> 3:02:05.820\n Choose wisely, my friends.\n\n3:02:05.820 --> 3:02:08.740\n And if you wish, click the sponsor links below\n\n3:02:08.740 --> 3:02:11.860\n to get a discount and to support this podcast.\n\n3:02:11.860 --> 3:02:15.100\n And now let me leave you with some words from Max Tegmark.\n\n3:02:15.100 --> 3:02:18.860\n If consciousness is the way that information feels\n\n3:02:18.860 --> 3:02:21.380\n when it's processed in certain ways,\n\n3:02:21.380 --> 3:02:24.220\n then it must be substrate independent.\n\n3:02:24.220 --> 3:02:26.660\n It's only the structure of information processing\n\n3:02:26.660 --> 3:02:29.100\n that matters, not the structure of the matter\n\n3:02:29.100 --> 3:02:31.900\n doing the information processing.\n\n3:02:31.900 --> 3:02:46.900\n Thank you for listening, and hope to see you next time.\n\n"
}