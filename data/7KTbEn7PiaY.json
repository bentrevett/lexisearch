{
  "title": "Kate Darling: Social Robotics | Lex Fridman Podcast #98",
  "id": "7KTbEn7PiaY",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:04.480\n The following is a conversation with Kate Darling, a researcher at MIT,\n\n00:04.480 --> 00:10.240\n interested in social robotics, robot ethics, and generally how technology intersects with society.\n\n00:11.040 --> 00:15.680\n She explores the emotional connection between human beings and lifelike machines,\n\n00:15.680 --> 00:20.480\n which for me is one of the most exciting topics in all of artificial intelligence.\n\n00:21.360 --> 00:26.240\n As she writes in her bio, she is a caretaker of several domestic robots,\n\n00:26.240 --> 00:33.600\n including her plio dinosaur robots named Yochai, Peter, and Mr. Spaghetti.\n\n00:33.600 --> 00:37.200\n She is one of the funniest and brightest minds I've ever had the fortune to talk to.\n\n00:37.840 --> 00:42.240\n This conversation was recorded recently, but before the outbreak of the pandemic.\n\n00:42.240 --> 00:46.000\n For everyone feeling the burden of this crisis, I'm sending love your way.\n\n00:46.720 --> 00:51.280\n This is the Artificial Intelligence Podcast. If you enjoy it, subscribe on YouTube,\n\n00:51.280 --> 00:56.800\n review it with five stars on Apple Podcast, support it on Patreon, or simply connect with me on Twitter\n\n00:56.800 --> 01:03.440\n at Lex Friedman, spelled F R I D M A N. As usual, I'll do a few minutes of ads now and never any\n\n01:03.440 --> 01:08.000\n ads in the middle that can break the flow of the conversation. I hope that works for you and\n\n01:08.000 --> 01:13.120\n doesn't hurt the listening experience. Quick summary of the ads. Two sponsors,\n\n01:13.760 --> 01:19.040\n Masterclass and ExpressVPN. Please consider supporting the podcast by signing up to\n\n01:19.040 --> 01:27.120\n Masterclass at masterclass.com slash Lex and getting ExpressVPN at expressvpn.com slash Lex\n\n01:27.120 --> 01:35.440\n Pod. This show is sponsored by Masterclass. Sign up at masterclass.com slash Lex to get a discount\n\n01:35.440 --> 01:40.720\n and to support this podcast. When I first heard about Masterclass, I thought it was too good to\n\n01:40.720 --> 01:47.680\n be true. For $180 a year, you get an all access pass to watch courses from, to list some of my\n\n01:47.680 --> 01:53.520\n favorites. Chris Hadfield on space exploration, Neil deGrasse Tyson on scientific thinking and\n\n01:53.520 --> 01:59.520\n communication, Will Wright, creator of SimCity and Sims, love those games, on game design,\n\n02:00.240 --> 02:06.800\n Carlos Santana on guitar, Garry Kasparov on chess, Daniel Nagrano on poker, and many more.\n\n02:07.680 --> 02:12.720\n Chris Hadfield explaining how rockets work and the experience of being launched into space alone\n\n02:12.720 --> 02:18.960\n is worth the money. By the way, you can watch it on basically any device. Once again,\n\n02:18.960 --> 02:24.160\n sign up on masterclass.com slash Lex to get a discount and to support this podcast.\n\n02:25.040 --> 02:33.120\n This show is sponsored by ExpressVPN. Get it at expressvpn.com slash Lex Pod to get a discount\n\n02:33.120 --> 02:39.600\n and to support this podcast. I've been using ExpressVPN for many years. I love it. It's easy\n\n02:39.600 --> 02:45.840\n to use, press the big power on button, and your privacy is protected. And, if you like, you can\n\n02:45.840 --> 02:50.960\n make it look like your location is anywhere else in the world. I might be in Boston now, but I can\n\n02:50.960 --> 02:56.240\n make it look like I'm in New York, London, Paris, or anywhere else. This has a large number of\n\n02:56.240 --> 03:01.520\n obvious benefits. Certainly, it allows you to access international versions of streaming websites\n\n03:01.520 --> 03:08.640\n like the Japanese Netflix or the UK Hulu. ExpressVPN works on any device you can imagine. I\n\n03:08.640 --> 03:17.120\n use it on Linux. Shout out to Ubuntu, 2004, Windows, Android, but it's available everywhere else too.\n\n03:17.760 --> 03:25.040\n Once again, get it at expressvpn.com slash Lex Pod to get a discount and to support this podcast.\n\n03:26.240 --> 03:29.920\n And now, here's my conversation with Kate Darling.\n\n03:31.040 --> 03:35.920\n You co taught robot ethics at Harvard. What are some ethical issues that arise\n\n03:35.920 --> 03:38.320\n in the world with robots?\n\n03:39.840 --> 03:44.400\n Yeah, that was a reading group that I did when I, like, at the very beginning,\n\n03:44.400 --> 03:48.800\n first became interested in this topic. So, I think if I taught that class today,\n\n03:48.800 --> 03:54.800\n it would look very, very different. Robot ethics, it sounds very science fictiony,\n\n03:54.800 --> 04:01.840\n especially did back then, but I think that some of the issues that people in robot ethics are\n\n04:01.840 --> 04:06.880\n concerned with are just around the ethical use of robotic technology in general. So, for example,\n\n04:06.880 --> 04:11.760\n responsibility for harm, automated weapon systems, things like privacy and data security,\n\n04:11.760 --> 04:19.200\n things like, you know, automation and labor markets. And then personally, I'm really\n\n04:19.200 --> 04:23.760\n interested in some of the social issues that come out of our social relationships with robots.\n\n04:23.760 --> 04:25.920\n One on one relationship with robots.\n\n04:25.920 --> 04:26.640\n Yeah.\n\n04:26.640 --> 04:30.160\n I think most of the stuff we have to talk about is like one on one social stuff. That's what I\n\n04:30.160 --> 04:35.200\n love. I think that's what you're, you love as well and are expert in. But a societal level,\n\n04:35.200 --> 04:39.280\n there's like, there's a presidential candidate now, Andrew Yang running,\n\n04:41.360 --> 04:48.640\n concerned about automation and robots and AI in general taking away jobs. He has a proposal of UBI,\n\n04:48.640 --> 04:55.440\n universal basic income of everybody gets 1000 bucks as a way to sort of save you if you lose\n\n04:55.440 --> 05:02.960\n your job from automation to allow you time to discover what it is that you would like to or\n\n05:02.960 --> 05:03.760\n even love to do.\n\n05:04.560 --> 05:12.160\n Yes. So I lived in Switzerland for 20 years and universal basic income has been more of a topic\n\n05:12.160 --> 05:19.360\n there separate from the whole robots and jobs issue. So it's so interesting to me to see kind\n\n05:19.360 --> 05:25.520\n of these Silicon Valley people latch onto this concept that came from a very kind of\n\n05:26.560 --> 05:37.120\n left wing socialist, kind of a different place in Europe. But on the automation labor markets\n\n05:37.120 --> 05:44.720\n topic, I think that it's very, so sometimes in those conversations, I think people overestimate\n\n05:44.720 --> 05:51.280\n where robotic technology is right now. And we also have this fallacy of constantly comparing robots\n\n05:51.280 --> 05:57.680\n to humans and thinking of this as a one to one replacement of jobs. So even like Bill Gates a few\n\n05:57.680 --> 06:03.920\n years ago said something about, maybe we should have a system that taxes robots for taking people's\n\n06:03.920 --> 06:10.480\n jobs. And it just, I mean, I'm sure that was taken out of context, he's a really smart guy,\n\n06:10.480 --> 06:15.920\n but that sounds to me like kind of viewing it as a one to one replacement versus viewing this\n\n06:15.920 --> 06:21.520\n technology as kind of a supplemental tool that of course is going to shake up a lot of stuff.\n\n06:21.520 --> 06:27.440\n It's going to change the job landscape, but I don't see, you know, robots taking all the\n\n06:27.440 --> 06:30.080\n jobs in the next 20 years. That's just not how it's going to work.\n\n06:30.800 --> 06:36.240\n Right. So maybe drifting into the land of more personal relationships with robots and\n\n06:36.240 --> 06:43.280\n interaction and so on. I got to warn you, I go, I may ask some silly philosophical questions.\n\n06:43.280 --> 06:43.920\n I apologize.\n\n06:43.920 --> 06:45.040\n Oh, please do.\n\n06:45.040 --> 06:52.560\n Okay. Do you think humans will abuse robots in their interactions? So you've had a lot of,\n\n06:52.560 --> 07:00.640\n and we'll talk about it sort of anthropomorphization and this intricate dance,\n\n07:00.640 --> 07:06.320\n emotional dance between human and robot, but there seems to be also a darker side where people, when\n\n07:06.320 --> 07:13.520\n they treat the other as servants, especially, they can be a little bit abusive or a lot abusive.\n\n07:13.520 --> 07:15.760\n Do you think about that? Do you worry about that?\n\n07:16.400 --> 07:22.960\n Yeah, I do think about that. So, I mean, one of my main interests is the fact that people\n\n07:22.960 --> 07:28.000\n subconsciously treat robots like living things. And even though they know that they're interacting\n\n07:28.000 --> 07:35.200\n with a machine and what it means in that context to behave violently. I don't know if you could say\n\n07:35.200 --> 07:42.000\n abuse because you're not actually abusing the inner mind of the robot. The robot doesn't have\n\n07:42.000 --> 07:42.640\n any feelings.\n\n07:42.640 --> 07:43.360\n As far as you know.\n\n07:44.000 --> 07:50.400\n Well, yeah. It also depends on how we define feelings and consciousness. But I think that's\n\n07:50.400 --> 07:54.080\n another area where people kind of overestimate where we currently are with the technology.\n\n07:54.080 --> 07:54.320\n Right.\n\n07:54.320 --> 08:00.320\n The robots are not even as smart as insects right now. And so I'm not worried about abuse\n\n08:00.320 --> 08:05.840\n in that sense. But it is interesting to think about what does people's behavior towards these\n\n08:05.840 --> 08:13.680\n things mean for our own behavior? Is it desensitizing the people to be verbally abusive\n\n08:13.680 --> 08:16.720\n to a robot or even physically abusive? And we don't know.\n\n08:17.360 --> 08:22.400\n Right. It's a similar connection from like if you play violent video games, what connection does\n\n08:22.400 --> 08:30.320\n that have to desensitization to violence? I haven't read literature on that. I wonder about that.\n\n08:32.080 --> 08:37.520\n Because everything I've heard, people don't seem to any longer be so worried about violent video\n\n08:37.520 --> 08:38.080\n games.\n\n08:38.080 --> 08:46.720\n Correct. The research on it is, it's a difficult thing to research. So it's sort of inconclusive,\n\n08:46.720 --> 08:53.680\n but we seem to have gotten the sense, at least as a society, that people can compartmentalize. When\n\n08:53.680 --> 08:58.320\n it's something on a screen and you're shooting a bunch of characters or running over people with\n\n08:58.320 --> 09:04.160\n your car, that doesn't necessarily translate to you doing that in real life. We do, however,\n\n09:04.160 --> 09:08.400\n have some concerns about children playing violent video games. And so we do restrict it there.\n\n09:09.680 --> 09:14.400\n I'm not sure that's based on any real evidence either, but it's just the way that we've kind of\n\n09:14.400 --> 09:19.040\n decided we want to be a little more cautious there. And the reason I think robots are a little\n\n09:19.040 --> 09:23.280\n bit different is because there is a lot of research showing that we respond differently\n\n09:23.280 --> 09:29.280\n to something in our physical space than something on a screen. We will treat it much more viscerally,\n\n09:29.280 --> 09:37.360\n much more like a physical actor. And so it's totally possible that this is not a problem.\n\n09:38.160 --> 09:43.280\n And it's the same thing as violence in video games. Maybe restrict it with kids to be safe,\n\n09:43.280 --> 09:48.560\n but adults can do what they want. But we just need to ask the question again because we don't\n\n09:48.560 --> 09:54.400\n have any evidence at all yet. Maybe there's an intermediate place too. I did my research\n\n09:55.840 --> 09:59.760\n on Twitter. By research, I mean scrolling through your Twitter feed.\n\n10:00.800 --> 10:03.920\n You mentioned that you were going at some point to an animal law conference.\n\n10:04.560 --> 10:07.840\n So I have to ask, do you think there's something that we can learn\n\n10:07.840 --> 10:12.320\n from animal rights that guides our thinking about robots?\n\n10:12.320 --> 10:17.120\n Oh, I think there is so much to learn from that. I'm actually writing a book on it right now. That's\n\n10:17.120 --> 10:22.400\n why I'm going to this conference. So I'm writing a book that looks at the history of animal\n\n10:22.400 --> 10:26.640\n domestication and how we've used animals for work, for weaponry, for companionship.\n\n10:27.280 --> 10:33.920\n And one of the things the book tries to do is move away from this fallacy that I talked about\n\n10:33.920 --> 10:39.680\n of comparing robots and humans because I don't think that's the right analogy. But I do think\n\n10:39.680 --> 10:43.920\n that on a social level, even on a social level, there's so much that we can learn from looking\n\n10:43.920 --> 10:49.360\n at that history because throughout history, we've treated most animals like tools, like products.\n\n10:49.360 --> 10:53.200\n And then some of them we've treated differently and we're starting to see people treat robots in\n\n10:53.200 --> 10:57.920\n really similar ways. So I think it's a really helpful predictor to how we're going to interact\n\n10:57.920 --> 11:04.400\n with the robots. Do you think we'll look back at this time like 100 years from now and see\n\n11:05.440 --> 11:13.360\n what we do to animals as like similar to the way we view like the Holocaust in World War II?\n\n11:13.360 --> 11:22.480\n That's a great question. I mean, I hope so. I am not convinced that we will. But I often wonder,\n\n11:22.480 --> 11:28.480\n you know, what are my grandkids going to view as, you know, abhorrent that my generation did\n\n11:28.480 --> 11:33.600\n that they would never do? And I'm like, well, what's the big deal? You know, it's a fun question\n\n11:33.600 --> 11:41.200\n to ask yourself. It always seems that there's atrocities that we discover later. So the things\n\n11:41.200 --> 11:49.600\n that at the time people didn't see as, you know, you look at everything from slavery to any kinds\n\n11:49.600 --> 11:56.480\n of abuse throughout history to the kind of insane wars that were happening to the way war was carried\n\n11:56.480 --> 12:05.360\n out and rape and the kind of violence that was happening during war that we now, you know,\n\n12:05.360 --> 12:12.880\n we see as atrocities, but at the time perhaps didn't as much. And so now I have this intuition\n\n12:12.880 --> 12:20.080\n that I have this worry, maybe you're going to probably criticize me, but I do anthropomorphize\n\n12:20.800 --> 12:29.280\n robots. I don't see a fundamental philosophical difference between a robot and a human being\n\n12:31.600 --> 12:39.200\n in terms of once the capabilities are matched. So the fact that we're really far away doesn't,\n\n12:39.200 --> 12:43.600\n in terms of capabilities and then that from natural language processing, understanding\n\n12:43.600 --> 12:48.800\n and generation to just reasoning and all that stuff. I think once you solve it, I see though,\n\n12:48.800 --> 12:53.920\n this is a very gray area and I don't feel comfortable with the kind of abuse that people\n\n12:53.920 --> 13:01.120\n throw at robots. Subtle, but I can see it becoming, I can see basically a civil rights movement for\n\n13:01.120 --> 13:07.040\n robots in the future. Do you think, let me put it in the form of a question, do you think robots\n\n13:07.040 --> 13:13.520\n should have some kinds of rights? Well, it's interesting because I came at this originally\n\n13:13.520 --> 13:19.040\n from your perspective. I was like, you know what, there's no fundamental difference between\n\n13:19.040 --> 13:24.800\n technology and like human consciousness. Like we, we can probably recreate anything. We just don't\n\n13:24.800 --> 13:32.640\n know how yet. And so there's no reason not to give machines the same rights that we have once,\n\n13:32.640 --> 13:38.080\n like you say, they're kind of on an equivalent level. But I realized that that is kind of a\n\n13:38.080 --> 13:41.600\n far future question. I still think we should talk about it because I think it's really interesting.\n\n13:41.600 --> 13:47.680\n But I realized that it's actually, we might need to ask the robot rights question even sooner than\n\n13:47.680 --> 13:56.160\n that while the machines are still, quote unquote, really dumb and not on our level because of the\n\n13:56.160 --> 14:00.560\n way that we perceive them. And I think one of the lessons we learned from looking at the history of\n\n14:00.560 --> 14:05.360\n animal rights and one of the reasons we may not get to a place in a hundred years where we view\n\n14:05.360 --> 14:11.440\n it as wrong to, you know, eat or otherwise, you know, use animals for our own purposes is because\n\n14:11.440 --> 14:17.920\n historically we've always protected those things that we relate to the most. So one example is\n\n14:17.920 --> 14:26.640\n whales. No one gave a shit about the whales. Am I allowed to swear? Yeah, no one gave a shit about\n\n14:26.640 --> 14:31.200\n freedom. Yeah, no one gave a shit about the whales until someone recorded them singing. And suddenly\n\n14:31.200 --> 14:35.840\n people were like, oh, this is a beautiful creature and now we need to save the whales. And that\n\n14:35.840 --> 14:45.360\n started the whole Save the Whales movement in the 70s. So as much as I, and I think a lot of people\n\n14:45.360 --> 14:52.400\n want to believe that we care about consistent biological criteria, that's not historically\n\n14:52.400 --> 15:00.880\n how we formed our alliances. Yeah, so what, why do we, why do we believe that all humans are created\n\n15:00.880 --> 15:07.120\n equal? Killing of a human being, no matter who the human being is, that's what I meant by equality,\n\n15:07.120 --> 15:14.480\n is bad. And then, because I'm connecting that to robots and I'm wondering whether mortality,\n\n15:14.480 --> 15:21.200\n so the killing act is what makes something, that's the fundamental first right. So I am currently\n\n15:21.200 --> 15:29.280\n allowed to take a shotgun and shoot a Roomba. I think, I'm not sure, but I'm pretty sure it's not\n\n15:29.280 --> 15:36.640\n considered murder, right. Or even shutting them off. So that's, that's where the line appears to\n\n15:36.640 --> 15:44.080\n be, right? Is this mortality a critical thing here? I think here again, like the animal analogy is\n\n15:44.080 --> 15:49.440\n really useful because you're also allowed to shoot your dog, but people won't be happy about it.\n\n15:49.440 --> 15:56.960\n So we give, we do give animals certain protections from like, you're not allowed to torture your dog\n\n15:56.960 --> 16:04.160\n and set it on fire, at least in most states and countries, but you're still allowed to treat it\n\n16:04.160 --> 16:11.920\n like a piece of property in a lot of other ways. And so we draw these arbitrary lines all the time.\n\n16:11.920 --> 16:20.320\n And, you know, there's a lot of philosophical thought on why viewing humans as something unique\n\n16:22.320 --> 16:31.040\n is not, is just speciesism and not, you know, based on any criteria that would actually justify\n\n16:31.040 --> 16:38.640\n making a difference between us and other species. Do you think in general people, most people are\n\n16:38.640 --> 16:49.040\n good? Do you think, or do you think there's evil and good in all of us? Is that's revealed through\n\n16:49.040 --> 16:55.760\n our circumstances and through our interactions? I like to view myself as a person who like believes\n\n16:55.760 --> 17:03.600\n that there's no absolute evil and good and that everything is, you know, gray. But I do think it's\n\n17:03.600 --> 17:08.080\n an interesting question. Like when I see people being violent towards robotic objects, you said\n\n17:08.080 --> 17:15.600\n that bothers you because the robots might someday, you know, be smart. And is that why?\n\n17:15.600 --> 17:21.280\n Well, it bothers me because it reveals, so I personally believe, because I've studied way too,\n\n17:21.280 --> 17:26.640\n so I'm Jewish. I studied the Holocaust and World War II exceptionally well. I personally believe\n\n17:26.640 --> 17:35.440\n that most of us have evil in us. That what bothers me is the abuse of robots reveals the evil in\n\n17:35.440 --> 17:44.320\n human beings. And it's, I think it doesn't just bother me. It's, I think it's an opportunity for\n\n17:44.320 --> 17:53.920\n roboticists to make, help people find the better sides, the angels of their nature, right? That\n\n17:53.920 --> 17:59.600\n abuse isn't just a fun side thing. That's a, you revealing a dark part that you shouldn't,\n\n17:59.600 --> 18:07.360\n that should be hidden deep inside. Yeah. I mean, you laugh, but some of our research does indicate\n\n18:07.360 --> 18:12.400\n that maybe people's behavior towards robots reveals something about their tendencies for\n\n18:12.400 --> 18:16.720\n empathy generally, even using very simple robots that we have today that like clearly don't feel\n\n18:16.720 --> 18:27.360\n anything. So, you know, Westworld is maybe, you know, not so far off and it's like, you know,\n\n18:27.360 --> 18:32.080\n depicting the bad characters as willing to go around and shoot and rape the robots and the good\n\n18:32.080 --> 18:37.520\n characters is not wanting to do that. Even without assuming that the robots have consciousness.\n\n18:37.520 --> 18:42.080\n So there's a opportunity, it's interesting, there's opportunity to almost practice empathy.\n\n18:42.080 --> 18:46.960\n The, on robots is an opportunity to practice empathy.\n\n18:47.840 --> 18:54.320\n I agree with you. Some people would say, why are we practicing empathy on robots instead of,\n\n18:54.320 --> 18:59.360\n you know, on our fellow humans or on animals that are actually alive and experienced the world?\n\n18:59.920 --> 19:03.840\n And I don't agree with them because I don't think empathy is a zero sum game. And I do\n\n19:03.840 --> 19:09.200\n think that it's a muscle that you can train and that we should be doing that. But some people\n\n19:09.200 --> 19:20.400\n disagree. So the interesting thing, you've heard, you know, raising kids sort of asking them or\n\n19:20.400 --> 19:28.000\n telling them to be nice to the smart speakers, to Alexa and so on, saying please and so on during\n\n19:28.000 --> 19:34.080\n the requests. I don't know if, I'm a huge fan of that idea because yeah, that's towards the idea of\n\n19:34.080 --> 19:39.120\n practicing empathy. I feel like politeness, I'm always polite to all the, all the systems that we\n\n19:39.120 --> 19:44.480\n build, especially anything that's speech interaction based. Like when we talk to the car, I'll always\n\n19:44.480 --> 19:51.280\n have a pretty good detector for please to, I feel like there should be a room for encouraging empathy\n\n19:51.280 --> 19:56.400\n in those interactions. Yeah. Okay. So I agree with you. So I'm going to play devil's advocate. Sure.\n\n19:58.400 --> 20:02.320\n So what is the, what is the devil's advocate argument there? The devil's advocate argument\n\n20:02.320 --> 20:08.560\n is that if you are the type of person who has abusive tendencies or needs to get some sort of\n\n20:08.560 --> 20:14.640\n like behavior like that out, needs an outlet for it, that it's great to have a robot that you can\n\n20:14.640 --> 20:19.760\n scream at so that you're not screaming at a person. And we just don't know whether that's true,\n\n20:19.760 --> 20:23.520\n whether it's an outlet for people or whether it just kind of, as my friend once said,\n\n20:23.520 --> 20:26.880\n trains their cruelty muscles and makes them more cruel in other situations.\n\n20:26.880 --> 20:36.320\n Oh boy. Yeah. And that expands to other topics, which I, I don't know, you know, there's a,\n\n20:36.320 --> 20:42.960\n there's a topic of sex, which is weird one that I tend to avoid it from robotics perspective.\n\n20:42.960 --> 20:50.080\n And most of the general public doesn't, they talk about sex robots and so on. Is that an area you've\n\n20:50.080 --> 20:57.920\n touched at all research wise? Like the way, cause that's what people imagine sort of any kind of\n\n20:57.920 --> 21:04.160\n interaction between human and robot that shows any kind of compassion. They immediately think\n\n21:04.160 --> 21:10.640\n from a product perspective in the near term is sort of expansion of what pornography is and all\n\n21:10.640 --> 21:16.000\n that kind of stuff. Yeah. Do researchers touch this? Well that's kind of you to like characterize\n\n21:16.000 --> 21:20.880\n it as though there's thinking rationally about product. I feel like sex robots are just such a\n\n21:20.880 --> 21:27.760\n like titillating news hook for people that they become like the story. And it's really hard to\n\n21:27.760 --> 21:32.480\n not get fatigued by it when you're in the space because you tell someone you do human robot\n\n21:32.480 --> 21:37.040\n interaction. Of course, the first thing they want to talk about is sex robots. Yeah, it happens a\n\n21:37.040 --> 21:42.320\n lot. And it's, it's unfortunate that I'm so fatigued by it because I do think that there\n\n21:42.320 --> 21:48.080\n are some interesting questions that become salient when you talk about, you know, sex with robots.\n\n21:48.880 --> 21:54.240\n See what I think would happen when people get sex robots, like if it's some guys, okay, guys get\n\n21:54.240 --> 22:03.360\n female sex robots. What I think there's an opportunity for is an actual, like, like they'll\n\n22:03.360 --> 22:09.440\n actually interact. What I'm trying to say, they won't outside of the sex would be the most\n\n22:09.440 --> 22:15.120\n fulfilling part. Like the interaction, it's like the folks who there's movies and this, right,\n\n22:15.120 --> 22:21.280\n who pray, pay a prostitute and then end up just talking to her the whole time. So I feel like\n\n22:21.280 --> 22:27.360\n there's an opportunity. It's like most guys and people in general joke about this, the sex act,\n\n22:27.360 --> 22:32.400\n but really people are just lonely inside and they're looking for connection. Many of them.\n\n22:32.400 --> 22:40.880\n And it'd be unfortunate if that connection is established through the sex industry. I feel like\n\n22:40.880 --> 22:46.480\n it should go into the front door of like, people are lonely and they want a connection.\n\n22:46.480 --> 22:52.480\n Well, I also feel like we should kind of de, you know, de stigmatize the sex industry because,\n\n22:54.000 --> 22:59.440\n you know, even prostitution, like there are prostitutes that specialize in disabled people\n\n22:59.440 --> 23:07.920\n who don't have the same kind of opportunities to explore their sexuality. So it's, I feel like we\n\n23:07.920 --> 23:13.200\n should like de stigmatize all of that generally. But yeah, that connection and that loneliness is\n\n23:13.200 --> 23:19.360\n an interesting topic that you bring up because while people are constantly worried about robots\n\n23:19.360 --> 23:23.840\n replacing humans and oh, if people get sex robots and the sex is really good, then they won't want\n\n23:23.840 --> 23:29.680\n their, you know, partner or whatever. But we rarely talk about robots actually filling a hole where\n\n23:29.680 --> 23:36.080\n there's nothing and what benefit that can provide to people. Yeah, I think that's an exciting,\n\n23:37.120 --> 23:43.120\n there's a whole giant, there's a giant hole that's unfillable by humans. It's asking too much of\n\n23:43.120 --> 23:47.280\n your, of people, your friends and people you're in a relationship with in your family to fill that\n\n23:47.280 --> 23:54.640\n hole. There's, because, you know, it's exploring the full, like, you know, exploring the full\n\n23:54.640 --> 24:02.560\n complexity and richness of who you are. Like who are you really? Like people, your family doesn't\n\n24:02.560 --> 24:06.800\n have enough patience to really sit there and listen to who are you really. And I feel like\n\n24:06.800 --> 24:11.760\n there's an opportunity to really make that connection with robots. I just feel like we're\n\n24:11.760 --> 24:18.720\n complex as humans and we're capable of lots of different types of relationships. So whether that's,\n\n24:18.720 --> 24:23.360\n you know, with family members, with friends, with our pets, or with robots, I feel like\n\n24:23.360 --> 24:27.520\n there's space for all of that and all of that can provide value in a different way.\n\n24:29.040 --> 24:35.520\n Yeah, absolutely. So I'm jumping around. Currently most of my work is in autonomous vehicles.\n\n24:35.520 --> 24:44.400\n So the most popular topic among the general public is the trolley problem. So most, most,\n\n24:45.760 --> 24:52.720\n most roboticists kind of hate this question, but what do you think of this thought experiment?\n\n24:52.720 --> 24:56.000\n What do you think we can learn from it outside of the silliness of\n\n24:56.000 --> 25:00.320\n the actual application of it to the autonomous vehicle? I think it's still an interesting\n\n25:00.320 --> 25:06.240\n ethical question. And that in itself, just like much of the interaction with robots\n\n25:06.880 --> 25:10.960\n has something to teach us. But from your perspective, do you think there's anything there?\n\n25:10.960 --> 25:14.320\n Well, I think you're right that it does have something to teach us because,\n\n25:14.320 --> 25:19.840\n but I think what people are forgetting in all of these conversations is the origins of the trolley\n\n25:19.840 --> 25:25.600\n problem and what it was meant to show us, which is that there is no right answer. And that sometimes\n\n25:25.600 --> 25:34.240\n our moral intuition that comes to us instinctively is not actually what we should follow if we care\n\n25:34.240 --> 25:40.800\n about creating systematic rules that apply to everyone. So I think that as a philosophical\n\n25:40.800 --> 25:46.960\n concept, it could teach us at least that, but that's not how people are using it right now.\n\n25:48.160 --> 25:54.000\n These are friends of mine and I love them dearly and their project adds a lot of value. But if\n\n25:54.000 --> 25:59.680\n we're viewing the moral machine project as what we can learn from the trolley problems, the moral\n\n25:59.680 --> 26:04.720\n machine is, I'm sure you're familiar, it's this website that you can go to and it gives you\n\n26:04.720 --> 26:10.640\n different scenarios like, oh, you're in a car, you can decide to run over these two people or\n\n26:10.640 --> 26:15.280\n this child. What do you choose? Do you choose the homeless person? Do you choose the person who's\n\n26:15.280 --> 26:21.520\n jaywalking? And so it pits these like moral choices against each other and then tries to\n\n26:21.520 --> 26:29.040\n crowdsource the quote unquote correct answer, which is really interesting and I think valuable data,\n\n26:29.040 --> 26:34.160\n but I don't think that's what we should base our rules in autonomous vehicles on because\n\n26:34.160 --> 26:39.840\n it is exactly what the trolley problem is trying to show, which is your first instinct might not\n\n26:39.840 --> 26:45.680\n be the correct one if you look at rules that then have to apply to everyone and everything.\n\n26:45.680 --> 26:50.800\n So how do we encode these ethical choices in interaction with robots? For example,\n\n26:50.800 --> 26:56.720\n autonomous vehicles, there is a serious ethical question of do I protect myself?\n\n26:58.960 --> 27:05.280\n Does my life have higher priority than the life of another human being? Because that changes\n\n27:05.280 --> 27:11.040\n certain control decisions that you make. So if your life matters more than other human beings,\n\n27:11.600 --> 27:16.960\n then you'd be more likely to swerve out of your current lane. So currently automated emergency\n\n27:16.960 --> 27:24.320\n braking systems that just brake, they don't ever swerve. So swerving into oncoming traffic or\n\n27:25.520 --> 27:31.840\n no, just in a different lane can cause significant harm to others, but it's possible that it causes\n\n27:31.840 --> 27:39.280\n less harm to you. So that's a difficult ethical question. Do you have a hope that\n\n27:41.680 --> 27:46.480\n like the trolley problem is not supposed to have a right answer, right? Do you hope that\n\n27:46.480 --> 27:50.960\n when we have robots at the table, we'll be able to discover the right answer for some of these\n\n27:50.960 --> 27:58.480\n questions? Well, what's happening right now, I think, is this question that we're facing of\n\n27:58.480 --> 28:03.600\n what ethical rules should we be programming into the machines is revealing to us that\n\n28:03.600 --> 28:11.280\n our ethical rules are much less programmable than we probably thought before. And so that's a really\n\n28:11.280 --> 28:19.360\n valuable insight, I think, that these issues are very complicated and that in a lot of these cases,\n\n28:19.360 --> 28:25.200\n it's you can't really make that call, like not even as a legislator. And so what's going to\n\n28:25.200 --> 28:31.440\n happen in reality, I think, is that car manufacturers are just going to try and avoid\n\n28:31.440 --> 28:36.000\n the problem and avoid liability in any way possible. Or like they're going to always protect\n\n28:36.000 --> 28:40.320\n the driver because who's going to buy a car if it's programmed to kill someone?\n\n28:40.320 --> 28:41.520\n Yeah.\n\n28:41.520 --> 28:47.040\n Kill you instead of someone else. So that's what's going to happen in reality.\n\n28:47.040 --> 28:51.680\n But what did you mean by like once we have robots at the table, like do you mean when they can help\n\n28:51.680 --> 28:54.720\n us figure out what to do?\n\n28:54.720 --> 29:01.920\n No, I mean when robots are part of the ethical decisions. So no, no, no, not they help us. Well.\n\n29:04.880 --> 29:08.560\n Oh, you mean when it's like, should I run over a robot or a person?\n\n29:08.560 --> 29:15.760\n Right. That kind of thing. So what, no, no, no. So when you, it's exactly what you said, which is\n\n29:15.760 --> 29:22.640\n when you have to encode the ethics into an algorithm, you start to try to really understand\n\n29:22.640 --> 29:27.200\n what are the fundamentals of the decision making process you make to make certain decisions.\n\n29:28.000 --> 29:34.960\n Should you, like capital punishment, should you take a person's life or not to punish them for\n\n29:34.960 --> 29:41.280\n a certain crime? Sort of, you can use, you can develop an algorithm to make that decision, right?\n\n29:42.480 --> 29:49.680\n And the hope is that the act of making that algorithm, however you make it, so there's a few\n\n29:49.680 --> 29:58.400\n approaches, will help us actually get to the core of what is right and what is wrong under our current\n\n29:59.600 --> 30:00.720\n societal standards.\n\n30:00.720 --> 30:05.600\n But isn't that what's happening right now? And we're realizing that we don't have a consensus on\n\n30:05.600 --> 30:06.560\n what's right and wrong.\n\n30:06.560 --> 30:08.240\n You mean in politics in general?\n\n30:08.240 --> 30:12.880\n Well, like when we're thinking about these trolley problems and autonomous vehicles and how to\n\n30:12.880 --> 30:22.320\n program ethics into machines and how to, you know, make AI algorithms fair and equitable, we're\n\n30:22.320 --> 30:28.080\n realizing that this is so complicated and it's complicated in part because there doesn't seem\n\n30:28.080 --> 30:30.640\n to be a one right answer in any of these cases.\n\n30:30.640 --> 30:35.680\n Do you have a hope for, like one of the ideas of the moral machine is that crowdsourcing can help\n\n30:35.680 --> 30:41.040\n us converge towards, like democracy can help us converge towards the right answer.\n\n30:42.080 --> 30:43.920\n Do you have a hope for crowdsourcing?\n\n30:43.920 --> 30:49.520\n Well, yes and no. So I think that in general, you know, I have a legal background and\n\n30:49.520 --> 30:55.440\n policymaking is often about trying to suss out, you know, what rules does this particular society\n\n30:55.440 --> 31:00.000\n agree on and then trying to codify that. So the law makes these choices all the time and then\n\n31:00.000 --> 31:06.080\n tries to adapt according to changing culture. But in the case of the moral machine project,\n\n31:06.080 --> 31:12.240\n I don't think that people's choices on that website necessarily reflect what laws they would\n\n31:12.240 --> 31:18.480\n want in place. I think you would have to ask them a series of different questions in order to get\n\n31:18.480 --> 31:20.720\n at what their consensus is.\n\n31:20.720 --> 31:25.680\n I agree, but that has to do more with the artificial nature of, I mean, they're showing\n\n31:25.680 --> 31:32.800\n some cute icons on a screen. That's almost, so if you, for example, we do a lot of work in virtual\n\n31:32.800 --> 31:38.720\n reality. And so if you put those same people into virtual reality where they have to make that\n\n31:38.720 --> 31:42.720\n decision, their decision would be very different, I think.\n\n31:42.720 --> 31:47.840\n I agree with that. That's one aspect. And the other aspect is it's a different question to ask\n\n31:47.840 --> 31:55.360\n someone, would you run over the homeless person or the doctor in this scene? Or do you want cars to\n\n31:55.360 --> 31:57.120\n always run over the homeless people?\n\n31:57.120 --> 32:04.320\n I think, yeah. So let's talk about anthropomorphism. To me, anthropomorphism, if I can\n\n32:04.320 --> 32:09.760\n pronounce it correctly, is one of the most fascinating phenomena from like both the\n\n32:09.760 --> 32:14.480\n engineering perspective and the psychology perspective, machine learning perspective,\n\n32:14.480 --> 32:23.280\n and robotics in general. Can you step back and define anthropomorphism, how you see it in\n\n32:23.280 --> 32:25.360\n general terms in your work?\n\n32:25.360 --> 32:32.160\n Sure. So anthropomorphism is this tendency that we have to project human like traits and\n\n32:32.160 --> 32:38.800\n behaviors and qualities onto nonhumans. And we often see it with animals, like we'll project\n\n32:38.800 --> 32:43.760\n emotions on animals that may or may not actually be there. We often see that we're trying to\n\n32:43.760 --> 32:49.120\n interpret things according to our own behavior when we get it wrong. But we do it with more\n\n32:49.120 --> 32:53.680\n than just animals. We do it with objects, you know, teddy bears. We see, you know, faces in\n\n32:53.680 --> 32:59.200\n the headlights of cars. And we do it with robots very, very extremely.\n\n32:59.200 --> 33:05.200\n You think that can be engineered? Can that be used to enrich an interaction between an AI\n\n33:05.200 --> 33:07.120\n system and the human?\n\n33:07.120 --> 33:08.480\n Oh, yeah, for sure.\n\n33:08.480 --> 33:17.600\n And do you see it being used that way often? Like, I don't, I haven't seen, whether it's\n\n33:17.600 --> 33:26.560\n Alexa or any of the smart speaker systems, often trying to optimize for the anthropomorphization.\n\n33:26.560 --> 33:27.920\n You said you haven't seen?\n\n33:27.920 --> 33:32.400\n I haven't seen. They keep moving away from that. I think they're afraid of that.\n\n33:32.400 --> 33:38.080\n They actually, so I only recently found out, but did you know that Amazon has like a whole\n\n33:38.080 --> 33:44.480\n team of people who are just there to work on Alexa's personality?\n\n33:44.480 --> 33:50.480\n So I know that depends on what you mean by personality. I didn't know that exact thing.\n\n33:50.480 --> 33:59.920\n But I do know that how the voice is perceived is worked on a lot, whether if it's a pleasant\n\n33:59.920 --> 34:04.080\n feeling about the voice, but that has to do more with the texture of the sound and the\n\n34:04.080 --> 34:08.640\n audio and so on. But personality is more like...\n\n34:08.640 --> 34:13.120\n It's like, what's her favorite beer when you ask her? And the personality team is different\n\n34:13.120 --> 34:17.520\n for every country too. Like there's a different personality for German Alexa than there is\n\n34:17.520 --> 34:26.800\n for American Alexa. That said, I think it's very difficult to, you know, use the, really,\n\n34:26.800 --> 34:34.000\n really harness the anthropomorphism with these voice assistants because the voice interface\n\n34:34.000 --> 34:40.000\n is still very primitive. And I think that in order to get people to really suspend their\n\n34:40.000 --> 34:47.520\n disbelief and treat a robot like it's alive, less is sometimes more. You want them to project\n\n34:47.520 --> 34:51.040\n onto the robot and you want the robot to not disappoint their expectations for how it's\n\n34:51.040 --> 34:57.920\n going to answer or behave in order for them to have this kind of illusion. And with Alexa,\n\n34:57.920 --> 35:03.280\n I don't think we're there yet, or Siri, that they're just not good at that. But if you\n\n35:03.280 --> 35:08.720\n look at some of the more animal like robots, like the baby seal that they use with the\n\n35:08.720 --> 35:12.960\n dementia patients, it's a much more simple design. It doesn't try to talk to you. It\n\n35:12.960 --> 35:17.760\n can't disappoint you in that way. It just makes little movements and sounds and people\n\n35:17.760 --> 35:22.720\n stroke it and it responds to their touch. And that is like a very effective way to harness\n\n35:23.280 --> 35:27.520\n people's tendency to kind of treat the robot like a living thing.\n\n35:28.880 --> 35:35.520\n Yeah. So you bring up some interesting ideas in your paper chapter, I guess,\n\n35:35.520 --> 35:40.400\n Anthropomorphic Framing Human Robot Interaction that I read the last time we scheduled this.\n\n35:40.400 --> 35:42.160\n Oh my God, that was a long time ago.\n\n35:42.160 --> 35:48.160\n Yeah. What are some good and bad cases of anthropomorphism in your perspective?\n\n35:49.280 --> 35:52.000\n Like when is the good ones and bad?\n\n35:52.000 --> 35:56.400\n Well, I should start by saying that, you know, while design can really enhance the\n\n35:56.400 --> 36:01.360\n anthropomorphism, it doesn't take a lot to get people to treat a robot like it's alive. Like\n\n36:01.360 --> 36:07.360\n people will, over 85% of Roombas have a name, which I'm, I don't know the numbers for your\n\n36:07.360 --> 36:12.160\n regular type of vacuum cleaner, but they're not that high, right? So people will feel bad for the\n\n36:12.160 --> 36:15.840\n Roomba when it gets stuck, they'll send it in for repair and want to get the same one back. And\n\n36:15.840 --> 36:23.280\n that's, that one is not even designed to like make you do that. So I think that some of the cases\n\n36:23.280 --> 36:28.560\n where it's maybe a little bit concerning that anthropomorphism is happening is when you have\n\n36:28.560 --> 36:32.000\n something that's supposed to function like a tool and people are using it in the wrong way.\n\n36:32.000 --> 36:44.160\n And one of the concerns is military robots where, so gosh, 2000, like early 2000s, which is a long\n\n36:44.160 --> 36:51.840\n time ago, iRobot, the Roomba company made this robot called the Pacbot that was deployed in Iraq\n\n36:51.840 --> 36:59.040\n and Afghanistan with the bomb disposal units that were there. And the soldiers became very emotionally\n\n36:59.040 --> 37:08.800\n attached to the robots. And that's fine until a soldier risks his life to save a robot, which\n\n37:08.800 --> 37:12.560\n you really don't want. But they were treating them like pets. Like they would name them,\n\n37:12.560 --> 37:17.280\n they would give them funerals with gun salutes, they would get really upset and traumatized when\n\n37:17.280 --> 37:23.760\n the robot got broken. So in situations where you want a robot to be a tool, in particular,\n\n37:23.760 --> 37:26.960\n when it's supposed to like do a dangerous job that you don't want a person doing,\n\n37:26.960 --> 37:32.960\n it can be hard when people get emotionally attached to it. That's maybe something that\n\n37:32.960 --> 37:39.040\n you would want to discourage. Another case for concern is maybe when companies try to\n\n37:39.840 --> 37:45.520\n leverage the emotional attachment to exploit people. So if it's something that's not in the\n\n37:45.520 --> 37:51.200\n consumer's interest, trying to like sell them products or services or exploit an emotional\n\n37:51.200 --> 37:57.200\n connection to keep them paying for a cloud service for a social robot or something like that might be,\n\n37:57.200 --> 37:59.680\n I think that's a little bit concerning as well.\n\n37:59.680 --> 38:04.720\n Yeah, the emotional manipulation, which probably happens behind the scenes now with some like\n\n38:04.720 --> 38:10.720\n social networks and so on, but making it more explicit. What's your favorite robot?\n\n38:12.000 --> 38:13.280\n Fictional or real?\n\n38:13.280 --> 38:23.360\n No, real. Real robot, which you have felt a connection with or not like, not anthropomorphic\n\n38:23.360 --> 38:31.040\n connection, but I mean like you sit back and say, damn, this is an impressive system.\n\n38:32.080 --> 38:38.960\n Wow. So two different robots. So the, the PLEO baby dinosaur robot that is no longer sold that\n\n38:38.960 --> 38:45.440\n came out in 2007, that one I was very impressed with. It was, but, but from an anthropomorphic\n\n38:45.440 --> 38:50.080\n perspective, I was impressed with how much I bonded with it, how much I like wanted to believe\n\n38:50.080 --> 38:51.760\n that it had this inner life.\n\n38:51.760 --> 38:58.160\n Can you describe PLEO, can you describe what it is? How big is it? What can it actually do?\n\n38:58.160 --> 39:06.400\n Yeah. PLEO is about the size of a small cat. It had a lot of like motors that gave it this kind\n\n39:06.400 --> 39:11.440\n of lifelike movement. It had things like touch sensors and an infrared camera. So it had all\n\n39:11.440 --> 39:18.800\n these like cool little technical features, even though it was a toy. And the thing that really\n\n39:18.800 --> 39:24.160\n struck me about it was that it, it could mimic pain and distress really well. So if you held\n\n39:24.160 --> 39:28.240\n it up by the tail, it had a tilt sensor that, you know, told it what direction it was facing\n\n39:28.240 --> 39:34.080\n and it would start to squirm and cry out. If you hit it too hard, it would start to cry.\n\n39:34.080 --> 39:37.120\n So it was very impressive in design.\n\n39:38.240 --> 39:43.040\n And what's the second robot that you were, you said there might've been two that you liked.\n\n39:43.680 --> 39:49.200\n Yeah. So the Boston Dynamics robots are just impressive feats of engineering.\n\n39:49.760 --> 39:51.280\n Have you met them in person?\n\n39:51.280 --> 39:55.280\n Yeah. I recently got a chance to go visit and I, you know, I was always one of those people who\n\n39:55.280 --> 39:59.920\n watched the videos and was like, this is super cool, but also it's a product video. Like,\n\n39:59.920 --> 40:02.800\n I don't know how many times that they had to shoot this to get it right.\n\n40:02.800 --> 40:03.360\n Yeah.\n\n40:03.360 --> 40:09.280\n But visiting them, I, you know, I'm pretty sure that I was very impressed. Let's put it that way.\n\n40:10.000 --> 40:14.880\n Yeah. And in terms of the control, I think that was a transformational moment for me\n\n40:15.520 --> 40:17.840\n when I met Spot Mini in person.\n\n40:17.840 --> 40:18.640\n Yeah.\n\n40:18.640 --> 40:25.360\n Because, okay, maybe this is a psychology experiment, but I anthropomorphized the,\n\n40:26.160 --> 40:30.880\n the crap out of it. So I immediately, it was like my best friend, right?\n\n40:30.880 --> 40:35.760\n I think it's really hard for anyone to watch Spot move and not feel like it has agency.\n\n40:35.760 --> 40:44.160\n Yeah. This movement, especially the arm on Spot Mini really obviously looks like a head.\n\n40:44.160 --> 40:44.400\n Yeah.\n\n40:44.400 --> 40:51.440\n That they say, no, wouldn't mean it that way, but it obviously, it looks exactly like that.\n\n40:51.440 --> 40:57.120\n And so it's almost impossible to not think of it as a, almost like the baby dinosaur,\n\n40:57.120 --> 41:02.000\n but slightly larger. And this movement of the, of course, the intelligence is,\n\n41:02.560 --> 41:07.840\n their whole idea is that it's not supposed to be intelligent. It's a platform on which you build\n\n41:08.480 --> 41:13.520\n higher intelligence. It's actually really, really dumb. It's just a basic movement platform.\n\n41:13.520 --> 41:19.920\n Yeah. But even dumb robots can, like, we can immediately respond to them in this visceral way.\n\n41:19.920 --> 41:26.640\n What are your thoughts about Sophia the robot? This kind of mix of some basic natural language\n\n41:26.640 --> 41:31.040\n processing and basically an art experiment.\n\n41:31.040 --> 41:35.920\n Yeah. An art experiment is a good way to characterize it. I'm much less impressed\n\n41:35.920 --> 41:37.840\n with Sophia than I am with Boston Dynamics.\n\n41:37.840 --> 41:40.160\n She said she likes you. She said she admires you.\n\n41:40.720 --> 41:43.440\n Yeah. She followed me on Twitter at some point. Yeah.\n\n41:44.160 --> 41:45.680\n She tweets about how much she likes you.\n\n41:45.680 --> 41:48.320\n So what does that mean? I have to be nice or?\n\n41:48.320 --> 41:55.040\n No, I don't know. I was emotionally manipulating you. No. How do you think of\n\n41:55.040 --> 42:00.560\n that? I think of the whole thing that happened with Sophia is quite a large number of people\n\n42:01.360 --> 42:06.640\n kind of immediately had a connection and thought that maybe we're far more advanced with robotics\n\n42:06.640 --> 42:11.840\n than we are or actually didn't even think much. I was surprised how little people cared\n\n42:13.680 --> 42:18.320\n that they kind of assumed that, well, of course AI can do this.\n\n42:19.200 --> 42:19.440\n Yeah.\n\n42:19.440 --> 42:24.960\n And then if they assume that, I felt they should be more impressed.\n\n42:26.960 --> 42:33.200\n Well, people really overestimate where we are. And so when something, I don't even think Sophia\n\n42:33.200 --> 42:37.680\n was very impressive or is very impressive. I think she's kind of a puppet, to be honest. But\n\n42:38.400 --> 42:43.120\n yeah, I think people are a little bit influenced by science fiction and pop culture to\n\n42:43.120 --> 42:45.200\n think that we should be further along than we are.\n\n42:45.200 --> 42:48.400\n So what's your favorite robots in movies and fiction?\n\n42:48.400 --> 42:49.120\n WALLI.\n\n42:49.680 --> 42:58.400\n WALLI. What do you like about WALLI? The humor, the cuteness, the perception control systems\n\n42:58.400 --> 43:02.960\n operating on WALLI that makes it all work? Just in general?\n\n43:02.960 --> 43:10.880\n The design of WALLI the robot, I think that animators figured out, starting in the 1940s,\n\n43:10.880 --> 43:19.040\n how to create characters that don't look real, but look like something that's even better than real,\n\n43:19.040 --> 43:23.120\n that we really respond to and think is really cute. They figured out how to make them move\n\n43:23.120 --> 43:27.600\n and look in the right way. And WALLI is just such a great example of that.\n\n43:27.600 --> 43:33.440\n You think eyes, big eyes or big something that's kind of eyeish. So it's always playing on some\n\n43:35.040 --> 43:36.960\n aspect of the human face, right?\n\n43:36.960 --> 43:44.080\n Often. Yeah. So big eyes. Well, I think one of the first animations to really play with this was\n\n43:44.080 --> 43:48.720\n Bambi. And they weren't originally going to do that. They were originally trying to make the\n\n43:48.720 --> 43:53.280\n deer look as lifelike as possible. They brought deer into the studio and had a little zoo there\n\n43:53.280 --> 43:56.880\n so that the animators could work with them. And then at some point they were like,\n\n43:57.520 --> 44:02.640\n if we make really big eyes and a small nose and big cheeks, kind of more like a baby face,\n\n44:02.640 --> 44:10.800\n then people like it even better than if it looks real. Do you think the future of things like\n\n44:10.800 --> 44:17.520\n Alexa in the home has possibility to take advantage of that, to build on that, to create\n\n44:18.960 --> 44:25.680\n these systems that are better than real, that create a close human connection? I can pretty\n\n44:25.680 --> 44:32.080\n much guarantee you without having any knowledge that those companies are going to make these\n\n44:32.080 --> 44:37.440\n things. And companies are working on that design behind the scenes. I'm pretty sure.\n\n44:37.440 --> 44:38.960\n I totally disagree with you.\n\n44:38.960 --> 44:39.440\n Really?\n\n44:39.440 --> 44:43.200\n So that's what I'm interested in. I'd like to build such a company. I know\n\n44:43.200 --> 44:47.920\n a lot of those folks and they're afraid of that because how do you make money off of it?\n\n44:49.120 --> 44:54.560\n Well, but even just making Alexa look a little bit more interesting than just a cylinder\n\n44:54.560 --> 44:55.680\n would do so much.\n\n44:55.680 --> 45:02.240\n It's an interesting thought, but I don't think people are from Amazon perspective are looking\n\n45:02.240 --> 45:08.320\n for that kind of connection. They want you to be addicted to the services provided by Alexa,\n\n45:08.320 --> 45:17.440\n not to the device. So the device itself, it's felt that you can lose a lot because if you create a\n\n45:17.440 --> 45:26.800\n connection and then it creates more opportunity for frustration for negative stuff than it does\n\n45:26.800 --> 45:29.920\n for positive stuff is I think the way they think about it.\n\n45:29.920 --> 45:35.600\n That's interesting. Like I agree that it's very difficult to get right and you have to get it\n\n45:35.600 --> 45:38.800\n exactly right. Otherwise you wind up with Microsoft's Clippy.\n\n45:40.000 --> 45:42.800\n Okay, easy now. What's your problem with Clippy?\n\n45:43.360 --> 45:45.040\n You like Clippy? Is Clippy your friend?\n\n45:45.040 --> 45:51.680\n Yeah, I like Clippy. I was just, I just talked to, we just had this argument and they said\n\n45:51.680 --> 45:57.520\n Microsoft's CTO and they said, he said he's not bringing Clippy back. They're not bringing\n\n45:57.520 --> 46:05.600\n Clippy back and that's very disappointing. I think it was Clippy was the greatest assistance\n\n46:05.600 --> 46:10.800\n we've ever built. It was a horrible attempt, of course, but it's the best we've ever done\n\n46:10.800 --> 46:17.760\n because it was a real attempt to have like a actual personality. I mean, it was obviously\n\n46:17.760 --> 46:25.040\n technology was way not there at the time of being able to be a recommender system for assisting you\n\n46:25.040 --> 46:30.480\n in anything and typing in Word or any kind of other application, but still it was an attempt\n\n46:30.480 --> 46:34.080\n of personality that was legitimate, which I thought was brave.\n\n46:34.880 --> 46:39.840\n Yes, yes. Okay. You know, you've convinced me I'll be slightly less hard on Clippy.\n\n46:39.840 --> 46:43.680\n And I know I have like an army of people behind me who also miss Clippy.\n\n46:43.680 --> 46:47.200\n Really? I want to meet these people. Who are these people?\n\n46:47.200 --> 46:53.680\n It's the people who like to hate stuff when it's there and miss it when it's gone.\n\n46:55.280 --> 46:56.240\n So everyone.\n\n46:56.240 --> 47:04.880\n It's everyone. Exactly. All right. So Enki and Jibo, the two companies,\n\n47:04.880 --> 47:10.080\n the two amazing companies, the social robotics companies that have recently been closed down.\n\n47:10.080 --> 47:10.580\n Yes.\n\n47:12.160 --> 47:17.280\n Why do you think it's so hard to create a personal robotics company? So making a business\n\n47:17.840 --> 47:23.840\n out of essentially something that people would anthropomorphize, have a deep connection with.\n\n47:23.840 --> 47:28.880\n Why is it so hard to make it work? Is the business case not there or what is it?\n\n47:28.880 --> 47:35.600\n I think it's a number of different things. I don't think it's going to be this way forever.\n\n47:35.600 --> 47:43.360\n I think at this current point in time, it takes so much work to build something that only barely\n\n47:43.360 --> 47:49.680\n meets people's minimal expectations because of science fiction and pop culture giving people\n\n47:49.680 --> 47:53.920\n this idea that we should be further than we already are. Like when people think about a robot\n\n47:53.920 --> 47:59.040\n assistant in the home, they think about Rosie from the Jetsons or something like that. And\n\n48:00.000 --> 48:06.240\n Enki and Jibo did such a beautiful job with the design and getting that interaction just right.\n\n48:06.240 --> 48:11.440\n But I think people just wanted more. They wanted more functionality. I think you're also right that\n\n48:11.440 --> 48:17.280\n the business case isn't really there because there hasn't been a killer application that's\n\n48:17.280 --> 48:23.440\n useful enough to get people to adopt the technology in great numbers. I think what we did see from the\n\n48:23.440 --> 48:31.040\n people who did get Jibo is a lot of them became very emotionally attached to it. But that's not,\n\n48:31.040 --> 48:35.040\n I mean, it's kind of like the Palm Pilot back in the day. Most people are like, why do I need this?\n\n48:35.040 --> 48:40.160\n Why would I? They don't see how they would benefit from it until they have it or some\n\n48:40.160 --> 48:45.760\n other company comes in and makes it a little better. Yeah. Like how far away are we, do you\n\n48:45.760 --> 48:50.320\n think? How hard is this problem? It's a good question. And I think it has a lot to do with\n\n48:50.320 --> 48:56.160\n people's expectations and those keep shifting depending on what science fiction that is popular.\n\n48:56.160 --> 49:01.840\n But also it's two things. It's people's expectation and people's need for an emotional\n\n49:01.840 --> 49:09.360\n connection. Yeah. And I believe the need is pretty high. Yes. But I don't think we're aware of it.\n\n49:10.080 --> 49:16.960\n That's right. There's like, I really think this is like the life as we know it. So we've just kind\n\n49:16.960 --> 49:24.640\n of gotten used to it of really, I hate to be dark because I have close friends, but we've gotten\n\n49:24.640 --> 49:32.720\n used to really never being close to anyone. Right. And we're deeply, I believe, okay, this is\n\n49:32.720 --> 49:37.680\n hypothesis. I think we're deeply lonely, all of us, even those in deep fulfilling relationships.\n\n49:37.680 --> 49:43.120\n In fact, what makes those relationship fulfilling, I think is that they at least tap into that deep\n\n49:43.120 --> 49:49.040\n loneliness a little bit. But I feel like there's more opportunity to explore that, that doesn't\n\n49:49.040 --> 49:54.160\n inter, doesn't interfere with the human relationships you have. It expands more on the,\n\n49:55.280 --> 50:01.760\n that, yeah, the rich deep unexplored complexity that's all of us, weird apes. Okay.\n\n50:02.560 --> 50:05.440\n I think you're right. Do you think it's possible to fall in love with a robot?\n\n50:05.440 --> 50:13.360\n Oh yeah, totally. Do you think it's possible to have a longterm committed monogamous relationship\n\n50:13.360 --> 50:18.480\n with a robot? Well, yeah, there are lots of different types of longterm committed monogamous\n\n50:18.480 --> 50:25.440\n relationships. I think monogamous implies like, you're not going to see other humans sexually or\n\n50:26.400 --> 50:32.320\n like you basically on Facebook have to say, I'm in a relationship with this person, this robot.\n\n50:32.320 --> 50:37.760\n I just don't like, again, I think this is comparing robots to humans when I would rather\n\n50:37.760 --> 50:45.360\n compare them to pets. Like you get a robot, it fulfills this loneliness that you have\n\n50:46.640 --> 50:52.400\n in maybe not the same way as a pet, maybe in a different way that is even supplemental in a\n\n50:52.400 --> 50:58.640\n different way. But I'm not saying that people won't like do this, be like, oh, I want to marry\n\n50:58.640 --> 51:05.840\n my robot or I want to have like a sexual relation, monogamous relationship with my robot. But I don't\n\n51:05.840 --> 51:11.520\n think that that's the main use case for them. But you think that there's still a gap between\n\n51:11.520 --> 51:24.480\n human and pet. So between a husband and pet, there's a different relationship. It's engineering.\n\n51:24.480 --> 51:30.160\n So that's a gap that can be closed through. I think it could be closed someday, but why\n\n51:30.160 --> 51:34.880\n would we close that? Like, I think it's so boring to think about recreating things that we already\n\n51:34.880 --> 51:43.040\n have when we could create something that's different. I know you're thinking about the\n\n51:43.040 --> 51:50.080\n people who like don't have a husband and like, what could we give them? Yeah. But I guess what\n\n51:50.080 --> 52:01.280\n I'm getting at is maybe not. So like the movie Her. Yeah. Right. So a better husband. Well,\n\n52:01.280 --> 52:07.360\n maybe better in some ways. Like it's, I do think that robots are going to continue to be a different\n\n52:07.360 --> 52:13.360\n type of relationship, even if we get them like very human looking or when, you know, the voice\n\n52:13.360 --> 52:18.320\n interactions we have with them feel very like natural and human like, I think there's still\n\n52:18.320 --> 52:22.480\n going to be differences. And there were in that movie too, like towards the end, it kind of goes\n\n52:22.480 --> 52:30.000\n off the rails. But it's just a movie. So your intuition is that, because you kind of said\n\n52:30.000 --> 52:39.120\n two things, right? So one is why would you want to basically replicate the husband? Yeah. Right.\n\n52:39.120 --> 52:46.160\n And the other is kind of implying that it's kind of hard to do. So like anytime you try,\n\n52:46.160 --> 52:51.920\n you might build something very impressive, but it'll be different. I guess my question is about\n\n52:51.920 --> 53:01.200\n human nature. It's like, how hard is it to satisfy that role of the husband? So we're moving any of\n\n53:01.200 --> 53:08.240\n the sexual stuff aside is the, it's more like the mystery, the tension, the dance of relationships\n\n53:08.240 --> 53:16.720\n you think with robots, that's difficult to build. What's your intuition? I think that, well, it also\n\n53:16.720 --> 53:22.960\n depends on are we talking about robots now in 50 years in like indefinite amount of time. I'm\n\n53:22.960 --> 53:29.920\n thinking like five or 10 years. Five or 10 years. I think that robots at best will be like, it's\n\n53:29.920 --> 53:33.920\n more similar to the relationship we have with our pets than relationship that we have with other\n\n53:33.920 --> 53:41.520\n people. I got it. So what do you think it takes to build a system that exhibits greater and greater\n\n53:41.520 --> 53:47.440\n levels of intelligence? Like it impresses us with this intelligence. Arumba, so you talk about\n\n53:47.440 --> 53:52.960\n anthropomorphization that doesn't, I think intelligence is not required. In fact, intelligence\n\n53:52.960 --> 54:00.640\n probably gets in the way sometimes, like you mentioned. But what do you think it takes to\n\n54:00.640 --> 54:06.800\n create a system where we sense that it has a human level intelligence? So something that,\n\n54:07.360 --> 54:11.920\n probably something conversational, human level intelligence. How hard do you think that problem\n\n54:11.920 --> 54:18.320\n is? It'd be interesting to sort of hear your perspective, not just purely, so I talk to a lot\n\n54:18.320 --> 54:24.640\n of people, how hard is the conversational agents? How hard is it to pass the torrent test? But my\n\n54:24.640 --> 54:33.440\n sense is it's easier than just solving, it's easier than solving the pure natural language\n\n54:33.440 --> 54:41.760\n processing problem. Because I feel like you can cheat. Yeah. So how hard is it to pass the torrent\n\n54:41.760 --> 54:47.120\n test in your view? Well, I think again, it's all about expectation management. If you set up\n\n54:47.120 --> 54:52.160\n people's expectations to think that they're communicating with, what was it, a 13 year old\n\n54:52.160 --> 54:56.160\n boy from the Ukraine? Yeah, that's right. Then they're not going to expect perfect English,\n\n54:56.160 --> 55:00.640\n they're not going to expect perfect, you know, understanding of concepts or even like being on\n\n55:00.640 --> 55:07.520\n the same wavelength in terms of like conversation flow. So it's much easier to pass in that case.\n\n55:08.560 --> 55:14.960\n Do you think, you kind of alluded this too with audio, do you think it needs to have a body?\n\n55:14.960 --> 55:21.440\n I think that we definitely have, so we treat physical things with more social agency,\n\n55:21.440 --> 55:25.040\n because we're very physical creatures. I think a body can be useful.\n\n55:29.840 --> 55:32.640\n Does it get in the way? Is there a negative aspects like...\n\n55:33.600 --> 55:38.320\n Yeah, there can be. So if you're trying to create a body that's too similar to something that people\n\n55:38.320 --> 55:44.320\n are familiar with, like I have this robot cat at home that has robots. I have a robot cat at home\n\n55:44.320 --> 55:50.960\n that has roommates. And it's very disturbing to watch because I'm constantly assuming that it's\n\n55:50.960 --> 55:56.000\n going to move like a real cat and it doesn't because it's like a $100 piece of technology.\n\n55:57.040 --> 56:04.800\n So it's very like disappointing and it's very hard to treat it like it's alive. So you can get a lot\n\n56:04.800 --> 56:09.680\n wrong with the body too, but you can also use tricks, same as, you know, the expectation\n\n56:09.680 --> 56:13.360\n management of the 13 year old boy from the Ukraine. If you pick an animal that people\n\n56:13.360 --> 56:17.680\n aren't intimately familiar with, like the baby dinosaur, like the baby seal that people have\n\n56:17.680 --> 56:22.400\n never actually held in their arms, you can get away with much more because they don't have these\n\n56:22.400 --> 56:27.280\n preformed expectations. Yeah, I remember you thinking of a Ted talk or something that clicked\n\n56:27.280 --> 56:34.400\n for me that nobody actually knows what a dinosaur looks like. So you can actually get away with a\n\n56:34.400 --> 56:44.320\n lot more. That was great. So what do you think about consciousness and mortality\n\n56:46.400 --> 56:55.760\n being displayed in a robot? So not actually having consciousness, but having these kind\n\n56:55.760 --> 57:01.600\n of human elements that are much more than just the interaction, much more than just,\n\n57:01.600 --> 57:07.440\n like you mentioned with a dinosaur moving kind of in an interesting ways, but really being worried\n\n57:07.440 --> 57:16.080\n about its own death and really acting as if it's aware and self aware and identity. Have you seen\n\n57:16.080 --> 57:23.680\n that done in robotics? What do you think about doing that? Is that a powerful good thing?\n\n57:24.560 --> 57:29.600\n Well, I think it can be a design tool that you can use for different purposes. So I can't say\n\n57:29.600 --> 57:35.440\n whether it's inherently good or bad, but I do think it can be a powerful tool. The fact that the\n\n57:36.480 --> 57:46.720\n pleo mimics distress when you quote unquote hurt it is a really powerful tool to get people to\n\n57:46.720 --> 57:52.560\n engage with it in a certain way. I had a research partner that I did some of the empathy work with\n\n57:52.560 --> 57:57.760\n named Palash Nandi and he had built a robot for himself that had like a lifespan and that would\n\n57:57.760 --> 58:02.800\n stop working after a certain amount of time just because he was interested in whether he himself\n\n58:02.800 --> 58:10.320\n would treat it differently. And we know from Tamagotchis, those little games that we used to\n\n58:10.320 --> 58:17.600\n have that were extremely primitive, that people respond to this idea of mortality and you can get\n\n58:17.600 --> 58:21.920\n people to do a lot with little design tricks like that. Now, whether it's a good thing depends on\n\n58:21.920 --> 58:27.760\n what you're trying to get them to do. Have a deeper relationship, have a deeper connection,\n\n58:27.760 --> 58:34.800\n sign a relationship. If it's for their own benefit, that sounds great. Okay. You could do that for a\n\n58:34.800 --> 58:39.920\n lot of other reasons. I see. So what kind of stuff are you worried about? So is it mostly about\n\n58:39.920 --> 58:44.880\n manipulation of your emotions for like advertisement and so on, things like that? Yeah, or data\n\n58:44.880 --> 58:51.280\n collection or, I mean, you could think of governments misusing this to extract information\n\n58:51.280 --> 58:57.200\n from people. It's, you know, just like any other technological tool, it just raises a lot of\n\n58:57.200 --> 59:02.880\n questions. If you look at Facebook, if you look at Twitter and social networks, there's a lot\n\n59:02.880 --> 59:10.480\n of concern of data collection now. What's from the legal perspective or in general,\n\n59:12.240 --> 59:19.760\n how do we prevent the violation of sort of these companies crossing a line? It's a great area,\n\n59:19.760 --> 59:24.480\n but crossing a line, they shouldn't in terms of manipulating, like we're talking about and\n\n59:24.480 --> 59:31.360\n manipulating our emotion, manipulating our behavior, using tactics that are not so savory.\n\n59:32.080 --> 59:38.960\n Yeah. It's really difficult because we are starting to create technology that relies on\n\n59:38.960 --> 59:44.000\n data collection to provide functionality. And there's not a lot of incentive,\n\n59:44.000 --> 59:49.600\n even on the consumer side, to curb that because the other problem is that the harms aren't\n\n59:49.600 --> 59:55.040\n tangible. They're not really apparent to a lot of people because they kind of trickle down on a\n\n59:55.040 --> 1:00:02.240\n societal level. And then suddenly we're living in like 1984, which, you know, sounds extreme,\n\n1:00:02.240 --> 1:00:11.280\n but that book was very prescient and I'm not worried about, you know, these systems. I have,\n\n1:00:11.280 --> 1:00:19.520\n you know, Amazon's Echo at home and tell Alexa all sorts of stuff. And it helps me because,\n\n1:00:19.520 --> 1:00:25.200\n you know, Alexa knows what brand of diaper we use. And so I can just easily order it again.\n\n1:00:25.200 --> 1:00:30.880\n So I don't have any incentive to ask a lawmaker to curb that. But when I think about that data\n\n1:00:30.880 --> 1:00:39.200\n then being used against low income people to target them for scammy loans or education programs,\n\n1:00:39.200 --> 1:00:45.120\n that's then a societal effect that I think is very severe and, you know,\n\n1:00:45.120 --> 1:00:47.280\n legislators should be thinking about.\n\n1:00:47.280 --> 1:00:53.920\n But yeah, the gray area is the removing ourselves from consideration of like,\n\n1:00:55.360 --> 1:00:58.880\n of explicitly defining objectives and more saying,\n\n1:00:58.880 --> 1:01:02.720\n well, we want to maximize engagement in our social network.\n\n1:01:03.680 --> 1:01:04.240\n Yeah.\n\n1:01:04.240 --> 1:01:11.040\n And then just, because you're not actually doing a bad thing. It makes sense. You want people to\n\n1:01:11.840 --> 1:01:15.840\n keep a conversation going, to have more conversations, to keep coming back\n\n1:01:16.480 --> 1:01:21.040\n again and again, to have conversations. And whatever happens after that,\n\n1:01:21.920 --> 1:01:28.320\n you're kind of not exactly directly responsible. You're only indirectly responsible. So I think\n\n1:01:28.320 --> 1:01:35.040\n it's a really hard problem. Are you optimistic about us ever being able to solve it?\n\n1:01:37.280 --> 1:01:42.480\n You mean the problem of capitalism? It's like, because the problem is that the companies\n\n1:01:43.120 --> 1:01:47.680\n are acting in the company's interests and not in people's interests. And when those interests are\n\n1:01:47.680 --> 1:01:53.840\n aligned, that's great. But the completely free market doesn't seem to work because of this\n\n1:01:53.840 --> 1:01:55.120\n information asymmetry.\n\n1:01:55.120 --> 1:02:01.120\n But it's hard to know how to, so say you were trying to do the right thing. I guess what I'm\n\n1:02:01.120 --> 1:02:07.600\n trying to say is it's not obvious for these companies what the good thing for society is to\n\n1:02:07.600 --> 1:02:14.880\n do. Like, I don't think they sit there with, I don't know, with a glass of wine and a cat,\n\n1:02:14.880 --> 1:02:21.120\n like petting a cat, evil cat. And there's two decisions and one of them is good for society.\n\n1:02:21.120 --> 1:02:26.960\n One is good for the profit and they choose the profit. I think they actually, there's a lot of\n\n1:02:26.960 --> 1:02:35.440\n money to be made by doing the right thing for society. Because Google, Facebook have so much cash\n\n1:02:36.480 --> 1:02:40.880\n that they actually, especially Facebook, would significantly benefit from making decisions that\n\n1:02:40.880 --> 1:02:46.800\n are good for society. It's good for their brand. But I don't know if they know what's good for\n\n1:02:46.800 --> 1:02:56.800\n society. I don't think we know what's good for society in terms of how we manage the\n\n1:02:56.800 --> 1:03:06.640\n conversation on Twitter or how we design, we're talking about robots. Like, should we\n\n1:03:06.640 --> 1:03:10.960\n emotionally manipulate you into having a deep connection with Alexa or not?\n\n1:03:10.960 --> 1:03:17.600\n Yeah. Yeah. Do you have optimism that we'll be able to solve some of these questions?\n\n1:03:17.600 --> 1:03:22.400\n Well, I'm going to say something that's controversial, like in my circles,\n\n1:03:22.400 --> 1:03:28.480\n which is that I don't think that companies who are reaching out to ethicists and trying to create\n\n1:03:28.480 --> 1:03:32.240\n interdisciplinary ethics boards, I don't think that that's totally just trying to whitewash\n\n1:03:32.240 --> 1:03:36.960\n the problem and so that they look like they've done something. I think that a lot of companies\n\n1:03:36.960 --> 1:03:42.960\n actually do, like you say, care about what the right answer is. They don't know what that is,\n\n1:03:42.960 --> 1:03:47.120\n and they're trying to find people to help them find them. Not in every case, but I think\n\n1:03:48.160 --> 1:03:52.320\n it's much too easy to just vilify the companies as, like you say, sitting there with their cat\n\n1:03:52.320 --> 1:03:59.600\n going, her, her, her, $1 million. That's not what happens. A lot of people are well meaning even\n\n1:03:59.600 --> 1:04:09.280\n within companies. I think that what we do absolutely need is more interdisciplinarity,\n\n1:04:09.840 --> 1:04:17.360\n both within companies, but also within the policymaking space because we've hurtled into\n\n1:04:17.360 --> 1:04:23.760\n the world where technological progress is much faster, it seems much faster than it was, and\n\n1:04:23.760 --> 1:04:28.480\n things are getting very complex. And you need people who understand the technology, but also\n\n1:04:28.480 --> 1:04:33.440\n people who understand what the societal implications are, and people who are thinking\n\n1:04:33.440 --> 1:04:39.280\n about this in a more systematic way to be talking to each other. There's no other solution, I think.\n\n1:04:39.920 --> 1:04:45.440\n You've also done work on intellectual property, so if you look at the algorithms that these\n\n1:04:45.440 --> 1:04:49.440\n companies are using, like YouTube, Twitter, Facebook, so on, I mean that's kind of,\n\n1:04:51.200 --> 1:04:58.400\n those are mostly secretive. The recommender systems behind these algorithms. Do you think\n\n1:04:58.400 --> 1:05:04.320\n about an IP and the transparency of algorithms like this? Like what is the responsibility of\n\n1:05:04.320 --> 1:05:11.440\n these companies to open source the algorithms or at least reveal to the public how these\n\n1:05:11.440 --> 1:05:16.000\n algorithms work? So I personally don't work on that. There are a lot of people who do though,\n\n1:05:16.000 --> 1:05:19.760\n and there are a lot of people calling for transparency. In fact, Europe's even trying\n\n1:05:19.760 --> 1:05:26.800\n to legislate transparency, maybe they even have at this point, where like if an algorithmic system\n\n1:05:26.800 --> 1:05:31.440\n makes some sort of decision that affects someone's life, that you need to be able to see how that\n\n1:05:31.440 --> 1:05:41.280\n decision was made. It's a tricky balance because obviously companies need to have some sort of\n\n1:05:41.280 --> 1:05:46.800\n competitive advantage and you can't take all of that away or you stifle innovation. But yeah,\n\n1:05:46.800 --> 1:05:51.680\n for some of the ways that these systems are already being used, I think it is pretty important that\n\n1:05:51.680 --> 1:05:56.960\n people understand how they work. What are your thoughts in general on intellectual property in\n\n1:05:56.960 --> 1:06:04.720\n this weird age of software, AI, robotics? Oh, that it's broken. I mean, the system is just broken. So\n\n1:06:04.720 --> 1:06:11.840\n can you describe, I actually, I don't even know what intellectual property is in the space of\n\n1:06:11.840 --> 1:06:20.240\n software, what it means to, I mean, so I believe I have a patent on a piece of software from my PhD.\n\n1:06:20.240 --> 1:06:26.880\n You believe? You don't know? No, we went through a whole process. Yeah, I do. You get the spam\n\n1:06:26.880 --> 1:06:36.320\n emails like, we'll frame your patent for you. Yeah, it's much like a thesis. But that's useless,\n\n1:06:36.320 --> 1:06:43.040\n right? Or not? Where does IP stand in this age? What's the right way to do it? What's the right\n\n1:06:43.040 --> 1:06:51.600\n way to protect and own ideas when it's just code and this mishmash of something that feels much\n\n1:06:51.600 --> 1:06:58.160\n softer than a piece of machinery? Yeah. I mean, it's hard because there are different types of\n\n1:06:58.160 --> 1:07:03.280\n intellectual property and they're kind of these blunt instruments. It's like patent law is like\n\n1:07:03.280 --> 1:07:07.200\n a wrench. It works really well for an industry like the pharmaceutical industry. But when you\n\n1:07:07.200 --> 1:07:12.080\n try and apply it to something else, it's like, I don't know, I'll just hit this thing with a wrench\n\n1:07:12.080 --> 1:07:21.600\n and hope it works. So software, you have a couple of different options. Any code that's written down\n\n1:07:21.600 --> 1:07:27.840\n in some tangible form is automatically copyrighted. So you have that protection, but that doesn't do\n\n1:07:27.840 --> 1:07:35.440\n much because if someone takes the basic idea that the code is executing and just does it in a\n\n1:07:35.440 --> 1:07:40.400\n slightly different way, they can get around the copyright. So that's not a lot of protection.\n\n1:07:40.400 --> 1:07:47.200\n Then you can patent software, but that's kind of, I mean, getting a patent costs,\n\n1:07:47.200 --> 1:07:51.280\n I don't know if you remember what yours cost or like, was it through an institution?\n\n1:07:51.280 --> 1:07:56.640\n Yeah, it was through a university. It was insane. There were so many lawyers, so many meetings.\n\n1:07:57.520 --> 1:08:02.160\n It made me feel like it must've been hundreds of thousands of dollars. It must've been something\n\n1:08:02.160 --> 1:08:07.760\n crazy. Oh yeah. It's insane the cost of getting a patent. And so this idea of protecting the\n\n1:08:07.760 --> 1:08:12.560\n inventor in their own garage who came up with a great idea is kind of, that's the thing of the\n\n1:08:12.560 --> 1:08:18.960\n past. It's all just companies trying to protect things and it costs a lot of money. And then\n\n1:08:18.960 --> 1:08:25.120\n with code, it's oftentimes by the time the patent is issued, which can take like five years,\n\n1:08:25.120 --> 1:08:31.520\n probably your code is obsolete at that point. So it's a very, again, a very blunt instrument that\n\n1:08:31.520 --> 1:08:37.440\n doesn't work well for that industry. And so at this point we should really have something better,\n\n1:08:37.440 --> 1:08:41.840\n but we don't. Do you like open source? Yeah. Is open source good for society?\n\n1:08:41.840 --> 1:08:48.720\n You think all of us should open source code? Well, so at the Media Lab at MIT, we have an\n\n1:08:48.720 --> 1:08:54.160\n open source default because what we've noticed is that people will come in, they'll write some code\n\n1:08:54.160 --> 1:08:58.640\n and they'll be like, how do I protect this? And we're like, that's not your problem right now.\n\n1:08:58.640 --> 1:09:02.160\n Your problem isn't that someone's going to steal your project. Your problem is getting people to\n\n1:09:02.160 --> 1:09:07.040\n use it at all. There's so much stuff out there. We don't even know if you're going to get traction\n\n1:09:07.040 --> 1:09:12.640\n for your work. And so open sourcing can sometimes help, you know, get people's work out there,\n\n1:09:12.640 --> 1:09:17.360\n but ensure that they get attribution for it, for the work that they've done. So like,\n\n1:09:17.360 --> 1:09:22.560\n I'm a fan of it in a lot of contexts. Obviously it's not like a one size fits all solution.\n\n1:09:23.680 --> 1:09:32.560\n So what I gleaned from your Twitter is, you're a mom. I saw a quote, a reference to baby bot.\n\n1:09:32.560 --> 1:09:41.520\n What have you learned about robotics and AI from raising a human baby bot?\n\n1:09:42.640 --> 1:09:48.560\n Well, I think that my child has made it more apparent to me that the systems we're currently\n\n1:09:48.560 --> 1:09:53.280\n creating aren't like human intelligence. Like there's not a lot to compare there.\n\n1:09:54.480 --> 1:09:59.920\n It's just, he has learned and developed in such a different way than a lot of the AI systems\n\n1:09:59.920 --> 1:10:07.360\n we're creating that that's not really interesting to me to compare. But what is interesting to me\n\n1:10:07.360 --> 1:10:13.520\n is how these systems are going to shape the world that he grows up in. And so I'm like even more\n\n1:10:13.520 --> 1:10:18.960\n concerned about kind of the societal effects of developing systems that, you know, rely on\n\n1:10:19.680 --> 1:10:26.720\n massive amounts of data collection, for example. So is he going to be allowed to use like Facebook or\n\n1:10:26.720 --> 1:10:33.360\n Facebook? Facebook is over. Kids don't use that anymore. Snapchat. What do they use? Instagram?\n\n1:10:33.360 --> 1:10:38.080\n Snapchat's over too. I don't know. I just heard that TikTok is over, which I've never even seen.\n\n1:10:38.080 --> 1:10:44.560\n So I don't know. No. We're old. We don't know. I need to, I'm going to start gaming and streaming\n\n1:10:44.560 --> 1:10:52.960\n my, my gameplay. So what do you see as the future of personal robotics, social robotics, interaction\n\n1:10:52.960 --> 1:10:58.320\n with other robots? Like what are you excited about if you were to sort of philosophize about what\n\n1:10:58.320 --> 1:11:05.040\n might happen in the next five, 10 years that would be cool to see? Oh, I really hope that we get kind\n\n1:11:05.040 --> 1:11:12.160\n of a home robot that makes it, that's a social robot and not just Alexa. Like it's, you know,\n\n1:11:12.160 --> 1:11:19.520\n I really love the Anki products. I thought Jibo was, had some really great aspects. So I'm hoping\n\n1:11:19.520 --> 1:11:26.800\n that a company cracks that. Me too. So Kate, it was a wonderful talking to you today. Likewise.\n\n1:11:26.800 --> 1:11:32.080\n Thank you so much. It was fun. Thanks for listening to this conversation with Kate Darling.\n\n1:11:32.080 --> 1:11:37.520\n And thank you to our sponsors, ExpressVPN and Masterclass. Please consider supporting the\n\n1:11:37.520 --> 1:11:45.200\n podcast by signing up to Masterclass at masterclass.com slash Lex and getting ExpressVPN at\n\n1:11:45.200 --> 1:11:52.160\n expressvpn.com slash LexPod. If you enjoy this podcast, subscribe on YouTube, review it with\n\n1:11:52.160 --> 1:11:57.200\n five stars on Apple podcast, support it on Patreon, or simply connect with me on Twitter\n\n1:11:57.200 --> 1:12:04.720\n at Lex Friedman. And now let me leave you with some tweets from Kate Darling. First tweet is\n\n1:12:05.440 --> 1:12:11.920\n the pandemic has fundamentally changed who I am. I now drink the leftover milk in the bottom of\n\n1:12:11.920 --> 1:12:19.600\n the cereal bowl. Second tweet is I came on here to complain that I had a really bad day and saw that\n\n1:12:19.600 --> 1:12:26.320\n a bunch of you are hurting too. Love to everyone. Thank you for listening. I hope to see you next\n\n1:12:26.320 --> 1:12:42.320\n time.\n\n"
}