{
  "title": "Fran\u00e7ois Chollet: Keras, Deep Learning, and the Progress of AI | Lex Fridman Podcast #38",
  "id": "Bo8MY4JpiXE",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:03.720\n The following is a conversation with Francois Chollet.\n\n00:03.720 --> 00:05.760\n He's the creator of Keras,\n\n00:05.760 --> 00:08.080\n which is an open source deep learning library\n\n00:08.080 --> 00:11.480\n that is designed to enable fast, user friendly experimentation\n\n00:11.480 --> 00:13.600\n with deep neural networks.\n\n00:13.600 --> 00:16.680\n It serves as an interface to several deep learning libraries,\n\n00:16.680 --> 00:19.040\n most popular of which is TensorFlow,\n\n00:19.040 --> 00:22.600\n and it was integrated into the TensorFlow main code base\n\n00:22.600 --> 00:24.080\n a while ago.\n\n00:24.080 --> 00:27.000\n Meaning, if you want to create, train,\n\n00:27.000 --> 00:28.640\n and use neural networks,\n\n00:28.640 --> 00:31.040\n probably the easiest and most popular option\n\n00:31.040 --> 00:33.840\n is to use Keras inside TensorFlow.\n\n00:34.840 --> 00:37.240\n Aside from creating an exceptionally useful\n\n00:37.240 --> 00:38.680\n and popular library,\n\n00:38.680 --> 00:41.920\n Francois is also a world class AI researcher\n\n00:41.920 --> 00:43.680\n and software engineer at Google.\n\n00:44.560 --> 00:46.960\n And he's definitely an outspoken,\n\n00:46.960 --> 00:50.560\n if not controversial personality in the AI world,\n\n00:50.560 --> 00:52.920\n especially in the realm of ideas\n\n00:52.920 --> 00:55.920\n around the future of artificial intelligence.\n\n00:55.920 --> 00:58.600\n This is the Artificial Intelligence Podcast.\n\n00:58.600 --> 01:01.000\n If you enjoy it, subscribe on YouTube,\n\n01:01.000 --> 01:02.760\n give it five stars on iTunes,\n\n01:02.760 --> 01:04.160\n support it on Patreon,\n\n01:04.160 --> 01:06.120\n or simply connect with me on Twitter\n\n01:06.120 --> 01:09.960\n at Lex Friedman, spelled F R I D M A N.\n\n01:09.960 --> 01:13.840\n And now, here's my conversation with Francois Chollet.\n\n01:14.880 --> 01:17.320\n You're known for not sugarcoating your opinions\n\n01:17.320 --> 01:19.160\n and speaking your mind about ideas in AI,\n\n01:19.160 --> 01:21.160\n especially on Twitter.\n\n01:21.160 --> 01:22.760\n It's one of my favorite Twitter accounts.\n\n01:22.760 --> 01:26.320\n So what's one of the more controversial ideas\n\n01:26.320 --> 01:29.360\n you've expressed online and gotten some heat for?\n\n01:30.440 --> 01:31.360\n How do you pick?\n\n01:33.080 --> 01:33.920\n How do I pick?\n\n01:33.920 --> 01:36.880\n Yeah, no, I think if you go through the trouble\n\n01:36.880 --> 01:39.640\n of maintaining a Twitter account,\n\n01:39.640 --> 01:41.840\n you might as well speak your mind, you know?\n\n01:41.840 --> 01:44.600\n Otherwise, what's even the point of having a Twitter account?\n\n01:44.600 --> 01:45.480\n It's like having a nice car\n\n01:45.480 --> 01:47.560\n and just leaving it in the garage.\n\n01:48.600 --> 01:50.840\n Yeah, so what's one thing for which I got\n\n01:50.840 --> 01:53.600\n a lot of pushback?\n\n01:53.600 --> 01:56.640\n Perhaps, you know, that time I wrote something\n\n01:56.640 --> 02:00.920\n about the idea of intelligence explosion,\n\n02:00.920 --> 02:04.520\n and I was questioning the idea\n\n02:04.520 --> 02:06.840\n and the reasoning behind this idea.\n\n02:06.840 --> 02:09.640\n And I got a lot of pushback on that.\n\n02:09.640 --> 02:11.840\n I got a lot of flak for it.\n\n02:11.840 --> 02:13.600\n So yeah, so intelligence explosion,\n\n02:13.600 --> 02:14.960\n I'm sure you're familiar with the idea,\n\n02:14.960 --> 02:18.800\n but it's the idea that if you were to build\n\n02:18.800 --> 02:22.920\n general AI problem solving algorithms,\n\n02:22.920 --> 02:26.000\n well, the problem of building such an AI,\n\n02:27.480 --> 02:30.520\n that itself is a problem that could be solved by your AI,\n\n02:30.520 --> 02:31.880\n and maybe it could be solved better\n\n02:31.880 --> 02:33.760\n than what humans can do.\n\n02:33.760 --> 02:36.840\n So your AI could start tweaking its own algorithm,\n\n02:36.840 --> 02:39.520\n could start making a better version of itself,\n\n02:39.520 --> 02:43.240\n and so on iteratively in a recursive fashion.\n\n02:43.240 --> 02:47.320\n And so you would end up with an AI\n\n02:47.320 --> 02:50.080\n with exponentially increasing intelligence.\n\n02:50.080 --> 02:50.920\n That's right.\n\n02:50.920 --> 02:55.880\n And I was basically questioning this idea,\n\n02:55.880 --> 02:59.040\n first of all, because the notion of intelligence explosion\n\n02:59.040 --> 03:02.200\n uses an implicit definition of intelligence\n\n03:02.200 --> 03:05.360\n that doesn't sound quite right to me.\n\n03:05.360 --> 03:10.360\n It considers intelligence as a property of a brain\n\n03:11.200 --> 03:13.680\n that you can consider in isolation,\n\n03:13.680 --> 03:16.640\n like the height of a building, for instance.\n\n03:16.640 --> 03:19.040\n But that's not really what intelligence is.\n\n03:19.040 --> 03:22.200\n Intelligence emerges from the interaction\n\n03:22.200 --> 03:25.240\n between a brain, a body,\n\n03:25.240 --> 03:28.320\n like embodied intelligence, and an environment.\n\n03:28.320 --> 03:30.720\n And if you're missing one of these pieces,\n\n03:30.720 --> 03:33.800\n then you cannot really define intelligence anymore.\n\n03:33.800 --> 03:36.800\n So just tweaking a brain to make it smaller and smaller\n\n03:36.800 --> 03:39.120\n doesn't actually make any sense to me.\n\n03:39.120 --> 03:39.960\n So first of all,\n\n03:39.960 --> 03:43.000\n you're crushing the dreams of many people, right?\n\n03:43.000 --> 03:46.000\n So there's a, let's look at like Sam Harris.\n\n03:46.000 --> 03:48.680\n Actually, a lot of physicists, Max Tegmark,\n\n03:48.680 --> 03:52.120\n people who think the universe\n\n03:52.120 --> 03:54.640\n is an information processing system,\n\n03:54.640 --> 03:57.680\n our brain is kind of an information processing system.\n\n03:57.680 --> 03:59.400\n So what's the theoretical limit?\n\n03:59.400 --> 04:03.160\n Like, it doesn't make sense that there should be some,\n\n04:04.800 --> 04:07.520\n it seems naive to think that our own brain\n\n04:07.520 --> 04:10.000\n is somehow the limit of the capabilities\n\n04:10.000 --> 04:11.600\n of this information system.\n\n04:11.600 --> 04:13.600\n I'm playing devil's advocate here.\n\n04:13.600 --> 04:15.600\n This information processing system.\n\n04:15.600 --> 04:17.760\n And then if you just scale it,\n\n04:17.760 --> 04:19.360\n if you're able to build something\n\n04:19.360 --> 04:20.920\n that's on par with the brain,\n\n04:20.920 --> 04:24.040\n you just, the process that builds it just continues\n\n04:24.040 --> 04:26.400\n and it'll improve exponentially.\n\n04:26.400 --> 04:30.160\n So that's the logic that's used actually\n\n04:30.160 --> 04:32.560\n by almost everybody\n\n04:32.560 --> 04:36.920\n that is worried about super human intelligence.\n\n04:36.920 --> 04:39.120\n So you're trying to make,\n\n04:39.120 --> 04:40.960\n so most people who are skeptical of that\n\n04:40.960 --> 04:43.000\n are kind of like, this doesn't,\n\n04:43.000 --> 04:46.520\n their thought process, this doesn't feel right.\n\n04:46.520 --> 04:47.680\n Like that's for me as well.\n\n04:47.680 --> 04:49.760\n So I'm more like, it doesn't,\n\n04:51.440 --> 04:52.800\n the whole thing is shrouded in mystery\n\n04:52.800 --> 04:55.840\n where you can't really say anything concrete,\n\n04:55.840 --> 04:57.880\n but you could say this doesn't feel right.\n\n04:57.880 --> 05:00.640\n This doesn't feel like that's how the brain works.\n\n05:00.640 --> 05:02.400\n And you're trying to with your blog posts\n\n05:02.400 --> 05:05.680\n and now making it a little more explicit.\n\n05:05.680 --> 05:10.680\n So one idea is that the brain isn't exist alone.\n\n05:10.680 --> 05:13.200\n It exists within the environment.\n\n05:13.200 --> 05:15.680\n So you can't exponentially,\n\n05:15.680 --> 05:18.000\n you would have to somehow exponentially improve\n\n05:18.000 --> 05:20.920\n the environment and the brain together almost.\n\n05:20.920 --> 05:25.920\n Yeah, in order to create something that's much smarter\n\n05:25.960 --> 05:27.840\n in some kind of,\n\n05:27.840 --> 05:29.960\n of course we don't have a definition of intelligence.\n\n05:29.960 --> 05:31.280\n That's correct, that's correct.\n\n05:31.280 --> 05:34.280\n I don't think, you should look at very smart people today,\n\n05:34.280 --> 05:37.280\n even humans, not even talking about AIs.\n\n05:37.280 --> 05:38.640\n I don't think their brain\n\n05:38.640 --> 05:41.960\n and the performance of their brain is the bottleneck\n\n05:41.960 --> 05:45.200\n to their expressed intelligence, to their achievements.\n\n05:46.600 --> 05:49.960\n You cannot just tweak one part of this system,\n\n05:49.960 --> 05:52.840\n like of this brain, body, environment system\n\n05:52.840 --> 05:55.960\n and expect that capabilities like what emerges\n\n05:55.960 --> 06:00.280\n out of this system to just explode exponentially.\n\n06:00.280 --> 06:04.200\n Because anytime you improve one part of a system\n\n06:04.200 --> 06:06.760\n with many interdependencies like this,\n\n06:06.760 --> 06:09.520\n there's a new bottleneck that arises, right?\n\n06:09.520 --> 06:12.280\n And I don't think even today for very smart people,\n\n06:12.280 --> 06:15.000\n their brain is not the bottleneck\n\n06:15.000 --> 06:17.560\n to the sort of problems they can solve, right?\n\n06:17.560 --> 06:19.800\n In fact, many very smart people today,\n\n06:20.760 --> 06:22.520\n you know, they are not actually solving\n\n06:22.520 --> 06:24.800\n any big scientific problems, they're not Einstein.\n\n06:24.800 --> 06:28.280\n They're like Einstein, but you know, the patent clerk days.\n\n06:29.800 --> 06:31.920\n Like Einstein became Einstein\n\n06:31.920 --> 06:36.080\n because this was a meeting of a genius\n\n06:36.080 --> 06:39.480\n with a big problem at the right time, right?\n\n06:39.480 --> 06:42.480\n But maybe this meeting could have never happened\n\n06:42.480 --> 06:44.960\n and then Einstein would have just been a patent clerk, right?\n\n06:44.960 --> 06:48.400\n And in fact, many people today are probably like\n\n06:49.760 --> 06:52.240\n genius level smart, but you wouldn't know\n\n06:52.240 --> 06:54.800\n because they're not really expressing any of that.\n\n06:54.800 --> 06:55.640\n Wow, that's brilliant.\n\n06:55.640 --> 06:58.520\n So we can think of the world, Earth,\n\n06:58.520 --> 07:02.720\n but also the universe as just as a space of problems.\n\n07:02.720 --> 07:05.160\n So all these problems and tasks are roaming it\n\n07:05.160 --> 07:06.880\n of various difficulty.\n\n07:06.880 --> 07:10.120\n And there's agents, creatures like ourselves\n\n07:10.120 --> 07:13.360\n and animals and so on that are also roaming it.\n\n07:13.360 --> 07:16.480\n And then you get coupled with a problem\n\n07:16.480 --> 07:17.640\n and then you solve it.\n\n07:17.640 --> 07:19.880\n But without that coupling,\n\n07:19.880 --> 07:22.560\n you can't demonstrate your quote unquote intelligence.\n\n07:22.560 --> 07:24.480\n Exactly, intelligence is the meeting\n\n07:24.480 --> 07:27.480\n of great problem solving capabilities\n\n07:27.480 --> 07:28.760\n with a great problem.\n\n07:28.760 --> 07:30.560\n And if you don't have the problem,\n\n07:30.560 --> 07:32.280\n you don't really express any intelligence.\n\n07:32.280 --> 07:34.760\n All you're left with is potential intelligence,\n\n07:34.760 --> 07:36.240\n like the performance of your brain\n\n07:36.240 --> 07:38.680\n or how high your IQ is,\n\n07:38.680 --> 07:42.080\n which in itself is just a number, right?\n\n07:42.080 --> 07:46.520\n So you mentioned problem solving capacity.\n\n07:46.520 --> 07:47.360\n Yeah.\n\n07:47.360 --> 07:51.800\n What do you think of as problem solving capacity?\n\n07:51.800 --> 07:55.160\n Can you try to define intelligence?\n\n07:56.640 --> 08:00.000\n Like what does it mean to be more or less intelligent?\n\n08:00.000 --> 08:03.000\n Is it completely coupled to a particular problem\n\n08:03.000 --> 08:05.720\n or is there something a little bit more universal?\n\n08:05.720 --> 08:07.440\n Yeah, I do believe all intelligence\n\n08:07.440 --> 08:09.080\n is specialized intelligence.\n\n08:09.080 --> 08:12.200\n Even human intelligence has some degree of generality.\n\n08:12.200 --> 08:15.320\n Well, all intelligent systems have some degree of generality\n\n08:15.320 --> 08:19.400\n but they're always specialized in one category of problems.\n\n08:19.400 --> 08:21.880\n So the human intelligence is specialized\n\n08:21.880 --> 08:23.560\n in the human experience.\n\n08:23.560 --> 08:25.560\n And that shows at various levels,\n\n08:25.560 --> 08:30.200\n that shows in some prior knowledge that's innate\n\n08:30.200 --> 08:32.040\n that we have at birth.\n\n08:32.040 --> 08:35.360\n Knowledge about things like agents,\n\n08:35.360 --> 08:38.080\n goal driven behavior, visual priors\n\n08:38.080 --> 08:43.080\n about what makes an object, priors about time and so on.\n\n08:43.520 --> 08:45.360\n That shows also in the way we learn.\n\n08:45.360 --> 08:47.160\n For instance, it's very, very easy for us\n\n08:47.160 --> 08:48.600\n to pick up language.\n\n08:49.560 --> 08:52.080\n It's very, very easy for us to learn certain things\n\n08:52.080 --> 08:54.920\n because we are basically hard coded to learn them.\n\n08:54.920 --> 08:58.280\n And we are specialized in solving certain kinds of problem\n\n08:58.280 --> 08:59.720\n and we are quite useless\n\n08:59.720 --> 09:01.440\n when it comes to other kinds of problems.\n\n09:01.440 --> 09:06.160\n For instance, we are not really designed\n\n09:06.160 --> 09:08.800\n to handle very long term problems.\n\n09:08.800 --> 09:12.880\n We have no capability of seeing the very long term.\n\n09:12.880 --> 09:16.880\n We don't have very much working memory.\n\n09:18.000 --> 09:20.080\n So how do you think about long term?\n\n09:20.080 --> 09:21.360\n Do you think long term planning,\n\n09:21.360 --> 09:24.880\n are we talking about scale of years, millennia?\n\n09:24.880 --> 09:26.400\n What do you mean by long term?\n\n09:26.400 --> 09:28.120\n We're not very good.\n\n09:28.120 --> 09:29.760\n Well, human intelligence is specialized\n\n09:29.760 --> 09:30.720\n in the human experience.\n\n09:30.720 --> 09:32.800\n And human experience is very short.\n\n09:32.800 --> 09:34.240\n One lifetime is short.\n\n09:34.240 --> 09:35.880\n Even within one lifetime,\n\n09:35.880 --> 09:40.000\n we have a very hard time envisioning things\n\n09:40.000 --> 09:41.360\n on a scale of years.\n\n09:41.360 --> 09:43.240\n It's very difficult to project yourself\n\n09:43.240 --> 09:46.960\n at a scale of five years, at a scale of 10 years and so on.\n\n09:46.960 --> 09:50.000\n We can solve only fairly narrowly scoped problems.\n\n09:50.000 --> 09:52.320\n So when it comes to solving bigger problems,\n\n09:52.320 --> 09:53.760\n larger scale problems,\n\n09:53.760 --> 09:56.360\n we are not actually doing it on an individual level.\n\n09:56.360 --> 09:59.280\n So it's not actually our brain doing it.\n\n09:59.280 --> 10:03.040\n We have this thing called civilization, right?\n\n10:03.040 --> 10:06.600\n Which is itself a sort of problem solving system,\n\n10:06.600 --> 10:10.000\n a sort of artificially intelligent system, right?\n\n10:10.000 --> 10:12.120\n And it's not running on one brain,\n\n10:12.120 --> 10:14.080\n it's running on a network of brains.\n\n10:14.080 --> 10:15.640\n In fact, it's running on much more\n\n10:15.640 --> 10:16.760\n than a network of brains.\n\n10:16.760 --> 10:20.080\n It's running on a lot of infrastructure,\n\n10:20.080 --> 10:23.040\n like books and computers and the internet\n\n10:23.040 --> 10:25.800\n and human institutions and so on.\n\n10:25.800 --> 10:30.240\n And that is capable of handling problems\n\n10:30.240 --> 10:33.760\n on a much greater scale than any individual human.\n\n10:33.760 --> 10:37.600\n If you look at computer science, for instance,\n\n10:37.600 --> 10:39.840\n that's an institution that solves problems\n\n10:39.840 --> 10:42.560\n and it is superhuman, right?\n\n10:42.560 --> 10:44.200\n It operates on a greater scale.\n\n10:44.200 --> 10:46.880\n It can solve much bigger problems\n\n10:46.880 --> 10:49.080\n than an individual human could.\n\n10:49.080 --> 10:52.160\n And science itself, science as a system, as an institution,\n\n10:52.160 --> 10:57.120\n is a kind of artificially intelligent problem solving\n\n10:57.120 --> 10:59.360\n algorithm that is superhuman.\n\n10:59.360 --> 11:02.800\n Yeah, it's, at least computer science\n\n11:02.800 --> 11:07.720\n is like a theorem prover at a scale of thousands,\n\n11:07.720 --> 11:10.400\n maybe hundreds of thousands of human beings.\n\n11:10.400 --> 11:14.680\n At that scale, what do you think is an intelligent agent?\n\n11:14.680 --> 11:18.280\n So there's us humans at the individual level,\n\n11:18.280 --> 11:22.400\n there is millions, maybe billions of bacteria in our skin.\n\n11:23.880 --> 11:26.400\n There is, that's at the smaller scale.\n\n11:26.400 --> 11:29.160\n You can even go to the particle level\n\n11:29.160 --> 11:31.000\n as systems that behave,\n\n11:31.840 --> 11:34.360\n you can say intelligently in some ways.\n\n11:35.440 --> 11:37.840\n And then you can look at the earth as a single organism,\n\n11:37.840 --> 11:39.200\n you can look at our galaxy\n\n11:39.200 --> 11:42.160\n and even the universe as a single organism.\n\n11:42.160 --> 11:44.680\n Do you think, how do you think about scale\n\n11:44.680 --> 11:46.280\n in defining intelligent systems?\n\n11:46.280 --> 11:50.440\n And we're here at Google, there is millions of devices\n\n11:50.440 --> 11:53.360\n doing computation just in a distributed way.\n\n11:53.360 --> 11:55.880\n How do you think about intelligence versus scale?\n\n11:55.880 --> 11:59.400\n You can always characterize anything as a system.\n\n12:00.640 --> 12:03.600\n I think people who talk about things\n\n12:03.600 --> 12:05.320\n like intelligence explosion,\n\n12:05.320 --> 12:08.760\n tend to focus on one agent is basically one brain,\n\n12:08.760 --> 12:10.960\n like one brain considered in isolation,\n\n12:10.960 --> 12:13.200\n like a brain, a jaw that's controlling a body\n\n12:13.200 --> 12:16.280\n in a very like top to bottom kind of fashion.\n\n12:16.280 --> 12:19.480\n And that body is pursuing goals into an environment.\n\n12:19.480 --> 12:20.720\n So it's a very hierarchical view.\n\n12:20.720 --> 12:22.880\n You have the brain at the top of the pyramid,\n\n12:22.880 --> 12:25.960\n then you have the body just plainly receiving orders.\n\n12:25.960 --> 12:27.640\n And then the body is manipulating objects\n\n12:27.640 --> 12:28.920\n in the environment and so on.\n\n12:28.920 --> 12:32.920\n So everything is subordinate to this one thing,\n\n12:32.920 --> 12:34.720\n this epicenter, which is the brain.\n\n12:34.720 --> 12:37.120\n But in real life, intelligent agents\n\n12:37.120 --> 12:39.240\n don't really work like this, right?\n\n12:39.240 --> 12:40.920\n There is no strong delimitation\n\n12:40.920 --> 12:43.400\n between the brain and the body to start with.\n\n12:43.400 --> 12:45.000\n You have to look not just at the brain,\n\n12:45.000 --> 12:46.560\n but at the nervous system.\n\n12:46.560 --> 12:48.840\n But then the nervous system and the body\n\n12:48.840 --> 12:50.760\n are naturally two separate entities.\n\n12:50.760 --> 12:53.960\n So you have to look at an entire animal as one agent.\n\n12:53.960 --> 12:57.000\n But then you start realizing as you observe an animal\n\n12:57.000 --> 13:00.200\n over any length of time,\n\n13:00.200 --> 13:03.160\n that a lot of the intelligence of an animal\n\n13:03.160 --> 13:04.600\n is actually externalized.\n\n13:04.600 --> 13:06.240\n That's especially true for humans.\n\n13:06.240 --> 13:08.880\n A lot of our intelligence is externalized.\n\n13:08.880 --> 13:10.360\n When you write down some notes,\n\n13:10.360 --> 13:11.960\n that is externalized intelligence.\n\n13:11.960 --> 13:14.000\n When you write a computer program,\n\n13:14.000 --> 13:16.000\n you are externalizing cognition.\n\n13:16.000 --> 13:19.720\n So it's externalizing books, it's externalized in computers,\n\n13:19.720 --> 13:21.520\n the internet, in other humans.\n\n13:23.080 --> 13:25.400\n It's externalizing language and so on.\n\n13:25.400 --> 13:30.400\n So there is no hard delimitation\n\n13:30.480 --> 13:32.640\n of what makes an intelligent agent.\n\n13:32.640 --> 13:33.880\n It's all about context.\n\n13:34.960 --> 13:38.800\n Okay, but AlphaGo is better at Go\n\n13:38.800 --> 13:40.200\n than the best human player.\n\n13:42.520 --> 13:45.000\n There's levels of skill here.\n\n13:45.000 --> 13:48.600\n So do you think there's such a ability,\n\n13:48.600 --> 13:52.800\n such a concept as intelligence explosion\n\n13:52.800 --> 13:54.760\n in a specific task?\n\n13:54.760 --> 13:57.360\n And then, well, yeah.\n\n13:57.360 --> 14:00.120\n Do you think it's possible to have a category of tasks\n\n14:00.120 --> 14:02.080\n on which you do have something\n\n14:02.080 --> 14:05.040\n like an exponential growth of ability\n\n14:05.040 --> 14:07.440\n to solve that particular problem?\n\n14:07.440 --> 14:10.320\n I think if you consider a specific vertical,\n\n14:10.320 --> 14:13.720\n it's probably possible to some extent.\n\n14:15.320 --> 14:18.320\n I also don't think we have to speculate about it\n\n14:18.320 --> 14:22.280\n because we have real world examples\n\n14:22.280 --> 14:26.920\n of recursively self improving intelligent systems, right?\n\n14:26.920 --> 14:30.920\n So for instance, science is a problem solving system,\n\n14:30.920 --> 14:32.600\n a knowledge generation system,\n\n14:32.600 --> 14:36.240\n like a system that experiences the world in some sense\n\n14:36.240 --> 14:40.160\n and then gradually understands it and can act on it.\n\n14:40.160 --> 14:42.120\n And that system is superhuman\n\n14:42.120 --> 14:45.600\n and it is clearly recursively self improving\n\n14:45.600 --> 14:47.560\n because science feeds into technology.\n\n14:47.560 --> 14:50.200\n Technology can be used to build better tools,\n\n14:50.200 --> 14:52.880\n better computers, better instrumentation and so on,\n\n14:52.880 --> 14:56.720\n which in turn can make science faster, right?\n\n14:56.720 --> 15:00.560\n So science is probably the closest thing we have today\n\n15:00.560 --> 15:04.760\n to a recursively self improving superhuman AI.\n\n15:04.760 --> 15:08.520\n And you can just observe is science,\n\n15:08.520 --> 15:10.320\n is scientific progress to the exploding,\n\n15:10.320 --> 15:12.800\n which itself is an interesting question.\n\n15:12.800 --> 15:15.560\n You can use that as a basis to try to understand\n\n15:15.560 --> 15:17.920\n what will happen with a superhuman AI\n\n15:17.920 --> 15:21.000\n that has a science like behavior.\n\n15:21.000 --> 15:23.320\n Let me linger on it a little bit more.\n\n15:23.320 --> 15:27.600\n What is your intuition why an intelligence explosion\n\n15:27.600 --> 15:28.560\n is not possible?\n\n15:28.560 --> 15:30.920\n Like taking the scientific,\n\n15:30.920 --> 15:33.240\n all the semi scientific revolutions,\n\n15:33.240 --> 15:38.080\n why can't we slightly accelerate that process?\n\n15:38.080 --> 15:41.200\n So you can absolutely accelerate\n\n15:41.200 --> 15:43.120\n any problem solving process.\n\n15:43.120 --> 15:46.720\n So a recursively self improvement\n\n15:46.720 --> 15:48.640\n is absolutely a real thing.\n\n15:48.640 --> 15:51.880\n But what happens with a recursively self improving system\n\n15:51.880 --> 15:53.680\n is typically not explosion\n\n15:53.680 --> 15:56.520\n because no system exists in isolation.\n\n15:56.520 --> 15:58.640\n And so tweaking one part of the system\n\n15:58.640 --> 16:00.880\n means that suddenly another part of the system\n\n16:00.880 --> 16:02.200\n becomes a bottleneck.\n\n16:02.200 --> 16:03.800\n And if you look at science, for instance,\n\n16:03.800 --> 16:06.800\n which is clearly a recursively self improving,\n\n16:06.800 --> 16:09.040\n clearly a problem solving system,\n\n16:09.040 --> 16:12.000\n scientific progress is not actually exploding.\n\n16:12.000 --> 16:13.520\n If you look at science,\n\n16:13.520 --> 16:16.480\n what you see is the picture of a system\n\n16:16.480 --> 16:19.240\n that is consuming an exponentially increasing\n\n16:19.240 --> 16:20.520\n amount of resources,\n\n16:20.520 --> 16:23.960\n but it's having a linear output\n\n16:23.960 --> 16:26.000\n in terms of scientific progress.\n\n16:26.000 --> 16:28.960\n And maybe that will seem like a very strong claim.\n\n16:28.960 --> 16:31.160\n Many people are actually saying that,\n\n16:31.160 --> 16:34.560\n scientific progress is exponential,\n\n16:34.560 --> 16:36.120\n but when they're claiming this,\n\n16:36.120 --> 16:38.400\n they're actually looking at indicators\n\n16:38.400 --> 16:43.080\n of resource consumption by science.\n\n16:43.080 --> 16:46.680\n For instance, the number of papers being published,\n\n16:47.560 --> 16:49.960\n the number of patents being filed and so on,\n\n16:49.960 --> 16:53.600\n which are just completely correlated\n\n16:53.600 --> 16:58.480\n with how many people are working on science today.\n\n16:58.480 --> 17:00.640\n So it's actually an indicator of resource consumption,\n\n17:00.640 --> 17:03.200\n but what you should look at is the output,\n\n17:03.200 --> 17:06.680\n is progress in terms of the knowledge\n\n17:06.680 --> 17:08.040\n that science generates,\n\n17:08.040 --> 17:10.640\n in terms of the scope and significance\n\n17:10.640 --> 17:12.520\n of the problems that we solve.\n\n17:12.520 --> 17:16.720\n And some people have actually been trying to measure that.\n\n17:16.720 --> 17:20.160\n Like Michael Nielsen, for instance,\n\n17:20.160 --> 17:21.920\n he had a very nice paper,\n\n17:21.920 --> 17:23.720\n I think that was last year about it.\n\n17:25.200 --> 17:28.360\n So his approach to measure scientific progress\n\n17:28.360 --> 17:33.360\n was to look at the timeline of scientific discoveries\n\n17:33.720 --> 17:37.160\n over the past, you know, 100, 150 years.\n\n17:37.160 --> 17:41.360\n And for each major discovery,\n\n17:41.360 --> 17:44.360\n ask a panel of experts to rate\n\n17:44.360 --> 17:46.760\n the significance of the discovery.\n\n17:46.760 --> 17:49.600\n And if the output of science as an institution\n\n17:49.600 --> 17:50.440\n were exponential,\n\n17:50.440 --> 17:55.440\n you would expect the temporal density of significance\n\n17:56.600 --> 17:58.160\n to go up exponentially.\n\n17:58.160 --> 18:00.960\n Maybe because there's a faster rate of discoveries,\n\n18:00.960 --> 18:02.960\n maybe because the discoveries are, you know,\n\n18:02.960 --> 18:04.920\n increasingly more important.\n\n18:04.920 --> 18:06.800\n And what actually happens\n\n18:06.800 --> 18:10.040\n if you plot this temporal density of significance\n\n18:10.040 --> 18:11.320\n measured in this way,\n\n18:11.320 --> 18:14.520\n is that you see very much a flat graph.\n\n18:14.520 --> 18:16.600\n You see a flat graph across all disciplines,\n\n18:16.600 --> 18:19.720\n across physics, biology, medicine, and so on.\n\n18:19.720 --> 18:22.480\n And it actually makes a lot of sense\n\n18:22.480 --> 18:23.320\n if you think about it,\n\n18:23.320 --> 18:26.000\n because think about the progress of physics\n\n18:26.000 --> 18:28.000\n 110 years ago, right?\n\n18:28.000 --> 18:30.080\n It was a time of crazy change.\n\n18:30.080 --> 18:31.960\n Think about the progress of technology,\n\n18:31.960 --> 18:34.360\n you know, 170 years ago,\n\n18:34.360 --> 18:35.400\n when we started having, you know,\n\n18:35.400 --> 18:37.560\n replacing horses with cars,\n\n18:37.560 --> 18:40.000\n when we started having electricity and so on.\n\n18:40.000 --> 18:41.520\n It was a time of incredible change.\n\n18:41.520 --> 18:44.600\n And today is also a time of very, very fast change,\n\n18:44.600 --> 18:48.040\n but it would be an unfair characterization\n\n18:48.040 --> 18:50.560\n to say that today technology and science\n\n18:50.560 --> 18:52.920\n are moving way faster than they did 50 years ago\n\n18:52.920 --> 18:54.360\n or 100 years ago.\n\n18:54.360 --> 18:59.360\n And if you do try to rigorously plot\n\n18:59.520 --> 19:04.520\n the temporal density of the significance,\n\n19:04.880 --> 19:07.320\n yeah, of significance, sorry,\n\n19:07.320 --> 19:09.720\n you do see very flat curves.\n\n19:09.720 --> 19:12.040\n And you can check out the paper\n\n19:12.040 --> 19:16.000\n that Michael Nielsen had about this idea.\n\n19:16.000 --> 19:20.000\n And so the way I interpret it is,\n\n19:20.000 --> 19:24.160\n as you make progress in a given field,\n\n19:24.160 --> 19:26.120\n or in a given subfield of science,\n\n19:26.120 --> 19:28.680\n it becomes exponentially more difficult\n\n19:28.680 --> 19:30.440\n to make further progress.\n\n19:30.440 --> 19:35.000\n Like the very first person to work on information theory.\n\n19:35.000 --> 19:36.440\n If you enter a new field,\n\n19:36.440 --> 19:37.920\n and it's still the very early years,\n\n19:37.920 --> 19:41.160\n there's a lot of low hanging fruit you can pick.\n\n19:41.160 --> 19:42.000\n That's right, yeah.\n\n19:42.000 --> 19:43.960\n But the next generation of researchers\n\n19:43.960 --> 19:48.160\n is gonna have to dig much harder, actually,\n\n19:48.160 --> 19:50.640\n to make smaller discoveries,\n\n19:50.640 --> 19:52.640\n probably larger number of smaller discoveries,\n\n19:52.640 --> 19:54.640\n and to achieve the same amount of impact,\n\n19:54.640 --> 19:57.480\n you're gonna need a much greater head count.\n\n19:57.480 --> 20:00.040\n And that's exactly the picture you're seeing with science,\n\n20:00.040 --> 20:03.760\n that the number of scientists and engineers\n\n20:03.760 --> 20:06.520\n is in fact increasing exponentially.\n\n20:06.520 --> 20:08.400\n The amount of computational resources\n\n20:08.400 --> 20:10.040\n that are available to science\n\n20:10.040 --> 20:11.880\n is increasing exponentially and so on.\n\n20:11.880 --> 20:15.560\n So the resource consumption of science is exponential,\n\n20:15.560 --> 20:18.200\n but the output in terms of progress,\n\n20:18.200 --> 20:21.000\n in terms of significance, is linear.\n\n20:21.000 --> 20:23.120\n And the reason why is because,\n\n20:23.120 --> 20:26.000\n and even though science is regressively self improving,\n\n20:26.000 --> 20:28.440\n meaning that scientific progress\n\n20:28.440 --> 20:30.240\n turns into technological progress,\n\n20:30.240 --> 20:32.960\n which in turn helps science.\n\n20:32.960 --> 20:35.280\n If you look at computers, for instance,\n\n20:35.280 --> 20:38.480\n our products of science and computers\n\n20:38.480 --> 20:41.560\n are tremendously useful in speeding up science.\n\n20:41.560 --> 20:43.840\n The internet, same thing, the internet is a technology\n\n20:43.840 --> 20:47.480\n that's made possible by very recent scientific advances.\n\n20:47.480 --> 20:52.400\n And itself, because it enables scientists to network,\n\n20:52.400 --> 20:55.520\n to communicate, to exchange papers and ideas much faster,\n\n20:55.520 --> 20:57.440\n it is a way to speed up scientific progress.\n\n20:57.440 --> 20:58.440\n So even though you're looking\n\n20:58.440 --> 21:01.440\n at a regressively self improving system,\n\n21:01.440 --> 21:04.080\n it is consuming exponentially more resources\n\n21:04.080 --> 21:09.080\n to produce the same amount of problem solving, very much.\n\n21:09.200 --> 21:11.080\n So that's a fascinating way to paint it,\n\n21:11.080 --> 21:14.960\n and certainly that holds for the deep learning community.\n\n21:14.960 --> 21:18.120\n If you look at the temporal, what did you call it,\n\n21:18.120 --> 21:21.240\n the temporal density of significant ideas,\n\n21:21.240 --> 21:23.920\n if you look at in deep learning,\n\n21:24.840 --> 21:26.960\n I think, I'd have to think about that,\n\n21:26.960 --> 21:29.040\n but if you really look at significant ideas\n\n21:29.040 --> 21:32.400\n in deep learning, they might even be decreasing.\n\n21:32.400 --> 21:37.400\n So I do believe the per paper significance is decreasing,\n\n21:39.600 --> 21:41.240\n but the amount of papers\n\n21:41.240 --> 21:43.440\n is still today exponentially increasing.\n\n21:43.440 --> 21:45.880\n So I think if you look at an aggregate,\n\n21:45.880 --> 21:48.840\n my guess is that you would see a linear progress.\n\n21:48.840 --> 21:53.840\n If you were to sum the significance of all papers,\n\n21:56.120 --> 21:58.640\n you would see roughly in your progress.\n\n21:58.640 --> 22:03.640\n And in my opinion, it is not a coincidence\n\n22:03.880 --> 22:05.800\n that you're seeing linear progress in science\n\n22:05.800 --> 22:07.720\n despite exponential resource consumption.\n\n22:07.720 --> 22:10.280\n I think the resource consumption\n\n22:10.280 --> 22:15.280\n is dynamically adjusting itself to maintain linear progress\n\n22:15.880 --> 22:18.560\n because we as a community expect linear progress,\n\n22:18.560 --> 22:21.240\n meaning that if we start investing less\n\n22:21.240 --> 22:23.600\n and seeing less progress, it means that suddenly\n\n22:23.600 --> 22:26.800\n there are some lower hanging fruits that become available\n\n22:26.800 --> 22:31.280\n and someone's gonna step up and pick them, right?\n\n22:31.280 --> 22:36.280\n So it's very much like a market for discoveries and ideas.\n\n22:36.920 --> 22:38.720\n But there's another fundamental part\n\n22:38.720 --> 22:41.800\n which you're highlighting, which as a hypothesis\n\n22:41.800 --> 22:45.160\n as science or like the space of ideas,\n\n22:45.160 --> 22:48.160\n any one path you travel down,\n\n22:48.160 --> 22:51.080\n it gets exponentially more difficult\n\n22:51.080 --> 22:54.720\n to get a new way to develop new ideas.\n\n22:54.720 --> 22:57.640\n And your sense is that's gonna hold\n\n22:57.640 --> 23:01.520\n across our mysterious universe.\n\n23:01.520 --> 23:03.360\n Yes, well, exponential progress\n\n23:03.360 --> 23:05.480\n triggers exponential friction.\n\n23:05.480 --> 23:07.440\n So that if you tweak one part of the system,\n\n23:07.440 --> 23:10.680\n suddenly some other part becomes a bottleneck, right?\n\n23:10.680 --> 23:14.880\n For instance, let's say you develop some device\n\n23:14.880 --> 23:17.160\n that measures its own acceleration\n\n23:17.160 --> 23:18.720\n and then it has some engine\n\n23:18.720 --> 23:20.800\n and it outputs even more acceleration\n\n23:20.800 --> 23:22.360\n in proportion of its own acceleration\n\n23:22.360 --> 23:23.320\n and you drop it somewhere,\n\n23:23.320 --> 23:25.240\n it's not gonna reach infinite speed\n\n23:25.240 --> 23:27.880\n because it exists in a certain context.\n\n23:29.080 --> 23:31.000\n So the air around it is gonna generate friction\n\n23:31.000 --> 23:34.320\n and it's gonna block it at some top speed.\n\n23:34.320 --> 23:37.480\n And even if you were to consider the broader context\n\n23:37.480 --> 23:39.840\n and lift the bottleneck there,\n\n23:39.840 --> 23:42.240\n like the bottleneck of friction,\n\n23:43.120 --> 23:45.120\n then some other part of the system\n\n23:45.120 --> 23:48.120\n would start stepping in and creating exponential friction,\n\n23:48.120 --> 23:49.920\n maybe the speed of flight or whatever.\n\n23:49.920 --> 23:51.920\n And this definitely holds true\n\n23:51.920 --> 23:54.960\n when you look at the problem solving algorithm\n\n23:54.960 --> 23:58.160\n that is being run by science as an institution,\n\n23:58.160 --> 23:59.720\n science as a system.\n\n23:59.720 --> 24:01.720\n As you make more and more progress,\n\n24:01.720 --> 24:05.800\n despite having this recursive self improvement component,\n\n24:06.760 --> 24:09.840\n you are encountering exponential friction.\n\n24:09.840 --> 24:13.480\n The more researchers you have working on different ideas,\n\n24:13.480 --> 24:14.880\n the more overhead you have\n\n24:14.880 --> 24:18.040\n in terms of communication across researchers.\n\n24:18.040 --> 24:22.920\n If you look at, you were mentioning quantum mechanics, right?\n\n24:22.920 --> 24:26.880\n Well, if you want to start making significant discoveries\n\n24:26.880 --> 24:29.680\n today, significant progress in quantum mechanics,\n\n24:29.680 --> 24:33.000\n there is an amount of knowledge you have to ingest,\n\n24:33.000 --> 24:34.080\n which is huge.\n\n24:34.080 --> 24:36.520\n So there's a very large overhead\n\n24:36.520 --> 24:39.240\n to even start to contribute.\n\n24:39.240 --> 24:40.680\n There's a large amount of overhead\n\n24:40.680 --> 24:44.040\n to synchronize across researchers and so on.\n\n24:44.040 --> 24:47.440\n And of course, the significant practical experiments\n\n24:48.600 --> 24:52.160\n are going to require exponentially expensive equipment\n\n24:52.160 --> 24:56.480\n because the easier ones have already been run, right?\n\n24:56.480 --> 25:00.480\n So in your senses, there's no way escaping,\n\n25:00.480 --> 25:04.480\n there's no way of escaping this kind of friction\n\n25:04.480 --> 25:08.600\n with artificial intelligence systems.\n\n25:08.600 --> 25:11.520\n Yeah, no, I think science is a very good way\n\n25:11.520 --> 25:14.280\n to model what would happen with a superhuman\n\n25:14.280 --> 25:16.440\n recursive research improving AI.\n\n25:16.440 --> 25:18.240\n That's your sense, I mean, the...\n\n25:18.240 --> 25:19.680\n That's my intuition.\n\n25:19.680 --> 25:23.400\n It's not like a mathematical proof of anything.\n\n25:23.400 --> 25:24.400\n That's not my point.\n\n25:24.400 --> 25:26.600\n Like, I'm not trying to prove anything.\n\n25:26.600 --> 25:27.920\n I'm just trying to make an argument\n\n25:27.920 --> 25:31.160\n to question the narrative of intelligence explosion,\n\n25:31.160 --> 25:32.880\n which is quite a dominant narrative.\n\n25:32.880 --> 25:35.840\n And you do get a lot of pushback if you go against it.\n\n25:35.840 --> 25:39.320\n Because, so for many people, right,\n\n25:39.320 --> 25:42.200\n AI is not just a subfield of computer science.\n\n25:42.200 --> 25:44.120\n It's more like a belief system.\n\n25:44.120 --> 25:48.640\n Like this belief that the world is headed towards an event,\n\n25:48.640 --> 25:55.040\n the singularity, past which, you know, AI will become...\n\n25:55.040 --> 25:57.080\n will go exponential very much,\n\n25:57.080 --> 25:58.600\n and the world will be transformed,\n\n25:58.600 --> 26:00.840\n and humans will become obsolete.\n\n26:00.840 --> 26:03.880\n And if you go against this narrative,\n\n26:03.880 --> 26:06.920\n because it is not really a scientific argument,\n\n26:06.920 --> 26:08.880\n but more of a belief system,\n\n26:08.880 --> 26:11.240\n it is part of the identity of many people.\n\n26:11.240 --> 26:12.600\n If you go against this narrative,\n\n26:12.600 --> 26:14.400\n it's like you're attacking the identity\n\n26:14.400 --> 26:15.560\n of people who believe in it.\n\n26:15.560 --> 26:17.640\n It's almost like saying God doesn't exist,\n\n26:17.640 --> 26:19.000\n or something.\n\n26:19.000 --> 26:21.880\n So you do get a lot of pushback\n\n26:21.880 --> 26:24.040\n if you try to question these ideas.\n\n26:24.040 --> 26:26.520\n First of all, I believe most people,\n\n26:26.520 --> 26:29.240\n they might not be as eloquent or explicit as you're being,\n\n26:29.240 --> 26:30.920\n but most people in computer science\n\n26:30.920 --> 26:33.000\n are most people who actually have built\n\n26:33.000 --> 26:36.360\n anything that you could call AI, quote, unquote,\n\n26:36.360 --> 26:38.080\n would agree with you.\n\n26:38.080 --> 26:40.560\n They might not be describing in the same kind of way.\n\n26:40.560 --> 26:43.960\n It's more, so the pushback you're getting\n\n26:43.960 --> 26:48.080\n is from people who get attached to the narrative\n\n26:48.080 --> 26:51.000\n from, not from a place of science,\n\n26:51.000 --> 26:53.400\n but from a place of imagination.\n\n26:53.400 --> 26:54.760\n That's correct, that's correct.\n\n26:54.760 --> 26:56.920\n So why do you think that's so appealing?\n\n26:56.920 --> 27:01.920\n Because the usual dreams that people have\n\n27:02.120 --> 27:03.960\n when you create a superintelligence system\n\n27:03.960 --> 27:05.120\n past the singularity,\n\n27:05.120 --> 27:08.600\n that what people imagine is somehow always destructive.\n\n27:09.440 --> 27:12.240\n Do you have, if you were put on your psychology hat,\n\n27:12.240 --> 27:17.240\n what's, why is it so appealing to imagine\n\n27:17.400 --> 27:20.760\n the ways that all of human civilization will be destroyed?\n\n27:20.760 --> 27:22.080\n I think it's a good story.\n\n27:22.080 --> 27:23.120\n You know, it's a good story.\n\n27:23.120 --> 27:28.120\n And very interestingly, it mirrors a religious stories,\n\n27:28.160 --> 27:30.560\n right, religious mythology.\n\n27:30.560 --> 27:34.360\n If you look at the mythology of most civilizations,\n\n27:34.360 --> 27:38.280\n it's about the world being headed towards some final events\n\n27:38.280 --> 27:40.480\n in which the world will be destroyed\n\n27:40.480 --> 27:42.800\n and some new world order will arise\n\n27:42.800 --> 27:44.920\n that will be mostly spiritual,\n\n27:44.920 --> 27:49.400\n like the apocalypse followed by a paradise probably, right?\n\n27:49.400 --> 27:52.600\n It's a very appealing story on a fundamental level.\n\n27:52.600 --> 27:54.560\n And we all need stories.\n\n27:54.560 --> 27:58.160\n We all need stories to structure the way we see the world,\n\n27:58.160 --> 27:59.960\n especially at timescales\n\n27:59.960 --> 28:04.520\n that are beyond our ability to make predictions, right?\n\n28:04.520 --> 28:08.840\n So on a more serious non exponential explosion,\n\n28:08.840 --> 28:13.840\n question, do you think there will be a time\n\n28:15.000 --> 28:19.800\n when we'll create something like human level intelligence\n\n28:19.800 --> 28:23.800\n or intelligent systems that will make you sit back\n\n28:23.800 --> 28:28.520\n and be just surprised at damn how smart this thing is?\n\n28:28.520 --> 28:30.160\n That doesn't require exponential growth\n\n28:30.160 --> 28:32.120\n or an exponential improvement,\n\n28:32.120 --> 28:35.600\n but what's your sense of the timeline and so on\n\n28:35.600 --> 28:40.600\n that you'll be really surprised at certain capabilities?\n\n28:41.080 --> 28:42.560\n And we'll talk about limitations and deep learning.\n\n28:42.560 --> 28:44.480\n So do you think in your lifetime,\n\n28:44.480 --> 28:46.600\n you'll be really damn surprised?\n\n28:46.600 --> 28:51.440\n Around 2013, 2014, I was many times surprised\n\n28:51.440 --> 28:53.960\n by the capabilities of deep learning actually.\n\n28:53.960 --> 28:55.920\n That was before we had assessed exactly\n\n28:55.920 --> 28:57.880\n what deep learning could do and could not do.\n\n28:57.880 --> 29:00.600\n And it felt like a time of immense potential.\n\n29:00.600 --> 29:03.080\n And then we started narrowing it down,\n\n29:03.080 --> 29:04.360\n but I was very surprised.\n\n29:04.360 --> 29:07.120\n I would say it has already happened.\n\n29:07.120 --> 29:10.800\n Was there a moment, there must've been a day in there\n\n29:10.800 --> 29:14.360\n where your surprise was almost bordering\n\n29:14.360 --> 29:19.360\n on the belief of the narrative that we just discussed.\n\n29:19.440 --> 29:20.800\n Was there a moment,\n\n29:20.800 --> 29:22.400\n because you've written quite eloquently\n\n29:22.400 --> 29:23.960\n about the limits of deep learning,\n\n29:23.960 --> 29:25.760\n was there a moment that you thought\n\n29:25.760 --> 29:27.720\n that maybe deep learning is limitless?\n\n29:30.000 --> 29:32.400\n No, I don't think I've ever believed this.\n\n29:32.400 --> 29:35.560\n What was really shocking is that it worked.\n\n29:35.560 --> 29:37.640\n It worked at all, yeah.\n\n29:37.640 --> 29:40.520\n But there's a big jump between being able\n\n29:40.520 --> 29:43.400\n to do really good computer vision\n\n29:43.400 --> 29:44.920\n and human level intelligence.\n\n29:44.920 --> 29:49.520\n So I don't think at any point I wasn't under the impression\n\n29:49.520 --> 29:51.280\n that the results we got in computer vision\n\n29:51.280 --> 29:54.080\n meant that we were very close to human level intelligence.\n\n29:54.080 --> 29:56.040\n I don't think we're very close to human level intelligence.\n\n29:56.040 --> 29:58.520\n I do believe that there's no reason\n\n29:58.520 --> 30:01.760\n why we won't achieve it at some point.\n\n30:01.760 --> 30:06.400\n I also believe that it's the problem\n\n30:06.400 --> 30:08.560\n with talking about human level intelligence\n\n30:08.560 --> 30:11.240\n that implicitly you're considering\n\n30:11.240 --> 30:14.360\n like an axis of intelligence with different levels,\n\n30:14.360 --> 30:16.720\n but that's not really how intelligence works.\n\n30:16.720 --> 30:19.480\n Intelligence is very multi dimensional.\n\n30:19.480 --> 30:22.480\n And so there's the question of capabilities,\n\n30:22.480 --> 30:25.560\n but there's also the question of being human like,\n\n30:25.560 --> 30:27.040\n and it's two very different things.\n\n30:27.040 --> 30:28.280\n Like you can build potentially\n\n30:28.280 --> 30:30.640\n very advanced intelligent agents\n\n30:30.640 --> 30:32.640\n that are not human like at all.\n\n30:32.640 --> 30:35.240\n And you can also build very human like agents.\n\n30:35.240 --> 30:37.840\n And these are two very different things, right?\n\n30:37.840 --> 30:38.760\n Right.\n\n30:38.760 --> 30:42.240\n Let's go from the philosophical to the practical.\n\n30:42.240 --> 30:44.240\n Can you give me a history of Keras\n\n30:44.240 --> 30:46.440\n and all the major deep learning frameworks\n\n30:46.440 --> 30:48.480\n that you kind of remember in relation to Keras\n\n30:48.480 --> 30:52.040\n and in general, TensorFlow, Theano, the old days.\n\n30:52.040 --> 30:55.400\n Can you give a brief overview Wikipedia style history\n\n30:55.400 --> 30:59.120\n and your role in it before we return to AGI discussions?\n\n30:59.120 --> 31:00.720\n Yeah, that's a broad topic.\n\n31:00.720 --> 31:04.040\n So I started working on Keras.\n\n31:04.920 --> 31:06.240\n It was the name Keras at the time.\n\n31:06.240 --> 31:08.320\n I actually picked the name like\n\n31:08.320 --> 31:10.200\n just the day I was going to release it.\n\n31:10.200 --> 31:14.800\n So I started working on it in February, 2015.\n\n31:14.800 --> 31:17.240\n And so at the time there weren't too many people\n\n31:17.240 --> 31:20.320\n working on deep learning, maybe like fewer than 10,000.\n\n31:20.320 --> 31:22.840\n The software tooling was not really developed.\n\n31:25.320 --> 31:28.800\n So the main deep learning library was Cafe,\n\n31:28.800 --> 31:30.840\n which was mostly C++.\n\n31:30.840 --> 31:32.760\n Why do you say Cafe was the main one?\n\n31:32.760 --> 31:36.000\n Cafe was vastly more popular than Theano\n\n31:36.000 --> 31:38.920\n in late 2014, early 2015.\n\n31:38.920 --> 31:42.400\n Cafe was the one library that everyone was using\n\n31:42.400 --> 31:43.400\n for computer vision.\n\n31:43.400 --> 31:46.120\n And computer vision was the most popular problem\n\n31:46.120 --> 31:46.960\n in deep learning at the time.\n\n31:46.960 --> 31:47.800\n Absolutely.\n\n31:47.800 --> 31:50.440\n Like ConvNets was like the subfield of deep learning\n\n31:50.440 --> 31:53.160\n that everyone was working on.\n\n31:53.160 --> 31:57.680\n So myself, so in late 2014,\n\n31:57.680 --> 32:00.600\n I was actually interested in RNNs,\n\n32:00.600 --> 32:01.760\n in recurrent neural networks,\n\n32:01.760 --> 32:05.800\n which was a very niche topic at the time, right?\n\n32:05.800 --> 32:08.640\n It really took off around 2016.\n\n32:08.640 --> 32:11.080\n And so I was looking for good tools.\n\n32:11.080 --> 32:14.800\n I had used Torch 7, I had used Theano,\n\n32:14.800 --> 32:17.640\n used Theano a lot in Kaggle competitions.\n\n32:19.320 --> 32:20.840\n I had used Cafe.\n\n32:20.840 --> 32:25.840\n And there was no like good solution for RNNs at the time.\n\n32:25.840 --> 32:28.640\n Like there was no reusable open source implementation\n\n32:28.640 --> 32:30.000\n of an LSTM, for instance.\n\n32:30.000 --> 32:32.920\n So I decided to build my own.\n\n32:32.920 --> 32:35.440\n And at first, the pitch for that was,\n\n32:35.440 --> 32:39.960\n it was gonna be mostly around LSTM recurrent neural networks.\n\n32:39.960 --> 32:41.360\n It was gonna be in Python.\n\n32:42.280 --> 32:44.280\n An important decision at the time\n\n32:44.280 --> 32:45.440\n that was kind of not obvious\n\n32:45.440 --> 32:50.360\n is that the models would be defined via Python code,\n\n32:50.360 --> 32:54.400\n which was kind of like going against the mainstream\n\n32:54.400 --> 32:58.000\n at the time because Cafe, Pylon 2, and so on,\n\n32:58.000 --> 33:00.600\n like all the big libraries were actually going\n\n33:00.600 --> 33:03.520\n with the approach of setting configuration files\n\n33:03.520 --> 33:05.560\n in YAML to define models.\n\n33:05.560 --> 33:08.840\n So some libraries were using code to define models,\n\n33:08.840 --> 33:12.280\n like Torch 7, obviously, but that was not Python.\n\n33:12.280 --> 33:16.680\n Lasagne was like a Theano based very early library\n\n33:16.680 --> 33:18.640\n that was, I think, developed, I don't remember exactly,\n\n33:18.640 --> 33:20.240\n probably late 2014.\n\n33:20.240 --> 33:21.200\n It's Python as well.\n\n33:21.200 --> 33:22.040\n It's Python as well.\n\n33:22.040 --> 33:24.320\n It was like on top of Theano.\n\n33:24.320 --> 33:28.320\n And so I started working on something\n\n33:29.480 --> 33:32.520\n and the value proposition at the time was that\n\n33:32.520 --> 33:36.240\n not only what I think was the first\n\n33:36.240 --> 33:38.800\n reusable open source implementation of LSTM,\n\n33:40.400 --> 33:44.440\n you could combine RNNs and covenants\n\n33:44.440 --> 33:45.440\n with the same library,\n\n33:45.440 --> 33:46.920\n which is not really possible before,\n\n33:46.920 --> 33:49.080\n like Cafe was only doing covenants.\n\n33:50.440 --> 33:52.560\n And it was kind of easy to use\n\n33:52.560 --> 33:54.440\n because, so before I was using Theano,\n\n33:54.440 --> 33:55.680\n I was actually using scikitlin\n\n33:55.680 --> 33:58.320\n and I loved scikitlin for its usability.\n\n33:58.320 --> 34:01.560\n So I drew a lot of inspiration from scikitlin\n\n34:01.560 --> 34:02.400\n when I made Keras.\n\n34:02.400 --> 34:05.600\n It's almost like scikitlin for neural networks.\n\n34:05.600 --> 34:06.680\n The fit function.\n\n34:06.680 --> 34:07.960\n Exactly, the fit function,\n\n34:07.960 --> 34:10.800\n like reducing a complex string loop\n\n34:10.800 --> 34:12.880\n to a single function call, right?\n\n34:12.880 --> 34:14.880\n And of course, some people will say,\n\n34:14.880 --> 34:16.320\n this is hiding a lot of details,\n\n34:16.320 --> 34:18.680\n but that's exactly the point, right?\n\n34:18.680 --> 34:20.280\n The magic is the point.\n\n34:20.280 --> 34:22.680\n So it's magical, but in a good way.\n\n34:22.680 --> 34:24.960\n It's magical in the sense that it's delightful.\n\n34:24.960 --> 34:26.160\n Yeah, yeah.\n\n34:26.160 --> 34:27.640\n I'm actually quite surprised.\n\n34:27.640 --> 34:29.600\n I didn't know that it was born out of desire\n\n34:29.600 --> 34:32.480\n to implement RNNs and LSTMs.\n\n34:32.480 --> 34:33.320\n It was.\n\n34:33.320 --> 34:34.160\n That's fascinating.\n\n34:34.160 --> 34:36.040\n So you were actually one of the first people\n\n34:36.040 --> 34:37.960\n to really try to attempt\n\n34:37.960 --> 34:41.000\n to get the major architectures together.\n\n34:41.000 --> 34:42.760\n And it's also interesting.\n\n34:42.760 --> 34:45.160\n You made me realize that that was a design decision at all\n\n34:45.160 --> 34:47.360\n is defining the model and code.\n\n34:47.360 --> 34:49.920\n Just, I'm putting myself in your shoes,\n\n34:49.920 --> 34:53.200\n whether the YAML, especially if cafe was the most popular.\n\n34:53.200 --> 34:54.720\n It was the most popular by far.\n\n34:54.720 --> 34:58.480\n If I was, if I were, yeah, I don't,\n\n34:58.480 --> 34:59.560\n I didn't like the YAML thing,\n\n34:59.560 --> 35:02.840\n but it makes more sense that you will put\n\n35:02.840 --> 35:05.720\n in a configuration file, the definition of a model.\n\n35:05.720 --> 35:07.200\n That's an interesting gutsy move\n\n35:07.200 --> 35:10.040\n to stick with defining it in code.\n\n35:10.040 --> 35:11.600\n Just if you look back.\n\n35:11.600 --> 35:13.480\n Other libraries were doing it as well,\n\n35:13.480 --> 35:16.320\n but it was definitely the more niche option.\n\n35:16.320 --> 35:17.160\n Yeah.\n\n35:17.160 --> 35:18.360\n Okay, Keras and then.\n\n35:18.360 --> 35:21.520\n So I released Keras in March, 2015,\n\n35:21.520 --> 35:24.160\n and it got users pretty much from the start.\n\n35:24.160 --> 35:25.800\n So the deep learning community was very, very small\n\n35:25.800 --> 35:27.240\n at the time.\n\n35:27.240 --> 35:30.600\n Lots of people were starting to be interested in LSTM.\n\n35:30.600 --> 35:32.440\n So it was gonna release it at the right time\n\n35:32.440 --> 35:35.560\n because it was offering an easy to use LSTM implementation.\n\n35:35.560 --> 35:37.680\n Exactly at the time where lots of people started\n\n35:37.680 --> 35:42.280\n to be intrigued by the capabilities of RNN, RNNs for NLP.\n\n35:42.280 --> 35:43.920\n So it grew from there.\n\n35:43.920 --> 35:48.920\n Then I joined Google about six months later,\n\n35:51.480 --> 35:54.920\n and that was actually completely unrelated to Keras.\n\n35:54.920 --> 35:57.080\n So I actually joined a research team\n\n35:57.080 --> 35:59.520\n working on image classification,\n\n35:59.520 --> 36:00.680\n mostly like computer vision.\n\n36:00.680 --> 36:02.320\n So I was doing computer vision research\n\n36:02.320 --> 36:03.640\n at Google initially.\n\n36:03.640 --> 36:05.520\n And immediately when I joined Google,\n\n36:05.520 --> 36:10.520\n I was exposed to the early internal version of TensorFlow.\n\n36:10.520 --> 36:13.920\n And the way it appeared to me at the time,\n\n36:13.920 --> 36:15.720\n and it was definitely the way it was at the time\n\n36:15.720 --> 36:20.760\n is that this was an improved version of Theano.\n\n36:20.760 --> 36:24.720\n So I immediately knew I had to port Keras\n\n36:24.720 --> 36:26.800\n to this new TensorFlow thing.\n\n36:26.800 --> 36:29.800\n And I was actually very busy as a noobler,\n\n36:29.800 --> 36:30.720\n as a new Googler.\n\n36:31.600 --> 36:34.520\n So I had not time to work on that.\n\n36:34.520 --> 36:38.680\n But then in November, I think it was November, 2015,\n\n36:38.680 --> 36:41.240\n TensorFlow got released.\n\n36:41.240 --> 36:44.560\n And it was kind of like my wake up call\n\n36:44.560 --> 36:47.320\n that, hey, I had to actually go and make it happen.\n\n36:47.320 --> 36:52.200\n So in December, I ported Keras to run on top of TensorFlow,\n\n36:52.200 --> 36:53.320\n but it was not exactly a port.\n\n36:53.320 --> 36:55.280\n It was more like a refactoring\n\n36:55.280 --> 36:57.920\n where I was abstracting away\n\n36:57.920 --> 37:00.480\n all the backend functionality into one module\n\n37:00.480 --> 37:02.320\n so that the same code base\n\n37:02.320 --> 37:05.080\n could run on top of multiple backends.\n\n37:05.080 --> 37:07.440\n So on top of TensorFlow or Theano.\n\n37:07.440 --> 37:09.760\n And for the next year,\n\n37:09.760 --> 37:14.760\n Theano stayed as the default option.\n\n37:15.400 --> 37:20.400\n It was easier to use, somewhat less buggy.\n\n37:20.640 --> 37:23.360\n It was much faster, especially when it came to audience.\n\n37:23.360 --> 37:26.360\n But eventually, TensorFlow overtook it.\n\n37:27.480 --> 37:30.200\n And TensorFlow, the early TensorFlow,\n\n37:30.200 --> 37:33.960\n has similar architectural decisions as Theano, right?\n\n37:33.960 --> 37:37.440\n So it was a natural transition.\n\n37:37.440 --> 37:38.320\n Yeah, absolutely.\n\n37:38.320 --> 37:42.960\n So what, I mean, that still Keras is a side,\n\n37:42.960 --> 37:45.280\n almost fun project, right?\n\n37:45.280 --> 37:49.040\n Yeah, so it was not my job assignment.\n\n37:49.040 --> 37:50.360\n It was not.\n\n37:50.360 --> 37:52.240\n I was doing it on the side.\n\n37:52.240 --> 37:55.840\n And even though it grew to have a lot of users\n\n37:55.840 --> 37:59.600\n for a deep learning library at the time, like Stroud 2016,\n\n37:59.600 --> 38:02.480\n but I wasn't doing it as my main job.\n\n38:02.480 --> 38:04.760\n So things started changing in,\n\n38:04.760 --> 38:09.760\n I think it must have been maybe October, 2016.\n\n38:10.200 --> 38:11.320\n So one year later.\n\n38:12.360 --> 38:15.240\n So Rajat, who was the lead on TensorFlow,\n\n38:15.240 --> 38:19.240\n basically showed up one day in our building\n\n38:19.240 --> 38:20.080\n where I was doing like,\n\n38:20.080 --> 38:21.640\n so I was doing research and things like,\n\n38:21.640 --> 38:24.640\n so I did a lot of computer vision research,\n\n38:24.640 --> 38:27.560\n also collaborations with Christian Zighetti\n\n38:27.560 --> 38:29.640\n and deep learning for theorem proving.\n\n38:29.640 --> 38:32.920\n It was a really interesting research topic.\n\n38:34.520 --> 38:37.640\n And so Rajat was saying,\n\n38:37.640 --> 38:41.040\n hey, we saw Keras, we like it.\n\n38:41.040 --> 38:42.440\n We saw that you're at Google.\n\n38:42.440 --> 38:45.280\n Why don't you come over for like a quarter\n\n38:45.280 --> 38:47.280\n and work with us?\n\n38:47.280 --> 38:49.240\n And I was like, yeah, that sounds like a great opportunity.\n\n38:49.240 --> 38:50.400\n Let's do it.\n\n38:50.400 --> 38:55.400\n And so I started working on integrating the Keras API\n\n38:55.720 --> 38:57.320\n into TensorFlow more tightly.\n\n38:57.320 --> 39:02.320\n So what followed up is a sort of like temporary\n\n39:02.640 --> 39:05.480\n TensorFlow only version of Keras\n\n39:05.480 --> 39:09.320\n that was in TensorFlow.com Trib for a while.\n\n39:09.320 --> 39:12.200\n And finally moved to TensorFlow Core.\n\n39:12.200 --> 39:15.360\n And I've never actually gotten back\n\n39:15.360 --> 39:17.600\n to my old team doing research.\n\n39:17.600 --> 39:22.320\n Well, it's kind of funny that somebody like you\n\n39:22.320 --> 39:27.320\n who dreams of, or at least sees the power of AI systems\n\n39:28.960 --> 39:31.680\n that reason and theorem proving we'll talk about\n\n39:31.680 --> 39:36.520\n has also created a system that makes the most basic\n\n39:36.520 --> 39:40.400\n kind of Lego building that is deep learning\n\n39:40.400 --> 39:42.640\n super accessible, super easy.\n\n39:42.640 --> 39:43.800\n So beautifully so.\n\n39:43.800 --> 39:47.720\n It's a funny irony that you're both,\n\n39:47.720 --> 39:49.120\n you're responsible for both things,\n\n39:49.120 --> 39:54.000\n but so TensorFlow 2.0 is kind of, there's a sprint.\n\n39:54.000 --> 39:55.080\n I don't know how long it'll take,\n\n39:55.080 --> 39:56.960\n but there's a sprint towards the finish.\n\n39:56.960 --> 40:01.040\n What do you look, what are you working on these days?\n\n40:01.040 --> 40:02.160\n What are you excited about?\n\n40:02.160 --> 40:04.280\n What are you excited about in 2.0?\n\n40:04.280 --> 40:05.760\n I mean, eager execution.\n\n40:05.760 --> 40:08.440\n There's so many things that just make it a lot easier\n\n40:08.440 --> 40:09.760\n to work.\n\n40:09.760 --> 40:13.640\n What are you excited about and what's also really hard?\n\n40:13.640 --> 40:15.800\n What are the problems you have to kind of solve?\n\n40:15.800 --> 40:19.080\n So I've spent the past year and a half working on\n\n40:19.080 --> 40:22.920\n TensorFlow 2.0 and it's been a long journey.\n\n40:22.920 --> 40:25.080\n I'm actually extremely excited about it.\n\n40:25.080 --> 40:26.440\n I think it's a great product.\n\n40:26.440 --> 40:29.360\n It's a delightful product compared to TensorFlow 1.0.\n\n40:29.360 --> 40:31.440\n We've made huge progress.\n\n40:32.640 --> 40:37.400\n So on the Keras side, what I'm really excited about is that,\n\n40:37.400 --> 40:42.400\n so previously Keras has been this very easy to use\n\n40:42.400 --> 40:45.840\n high level interface to do deep learning.\n\n40:45.840 --> 40:47.280\n But if you wanted to,\n\n40:50.520 --> 40:53.040\n if you wanted a lot of flexibility,\n\n40:53.040 --> 40:57.520\n the Keras framework was probably not the optimal way\n\n40:57.520 --> 40:59.760\n to do things compared to just writing everything\n\n40:59.760 --> 41:00.600\n from scratch.\n\n41:01.800 --> 41:04.680\n So in some way, the framework was getting in the way.\n\n41:04.680 --> 41:07.960\n And in TensorFlow 2.0, you don't have this at all, actually.\n\n41:07.960 --> 41:11.040\n You have the usability of the high level interface,\n\n41:11.040 --> 41:14.480\n but you have the flexibility of this lower level interface.\n\n41:14.480 --> 41:16.800\n And you have this spectrum of workflows\n\n41:16.800 --> 41:21.560\n where you can get more or less usability\n\n41:21.560 --> 41:26.560\n and flexibility trade offs depending on your needs, right?\n\n41:26.640 --> 41:29.680\n You can write everything from scratch\n\n41:29.680 --> 41:32.320\n and you get a lot of help doing so\n\n41:32.320 --> 41:36.400\n by subclassing models and writing some train loops\n\n41:36.400 --> 41:38.200\n using ego execution.\n\n41:38.200 --> 41:40.160\n It's very flexible, it's very easy to debug,\n\n41:40.160 --> 41:41.400\n it's very powerful.\n\n41:42.280 --> 41:45.000\n But all of this integrates seamlessly\n\n41:45.000 --> 41:49.440\n with higher level features up to the classic Keras workflows,\n\n41:49.440 --> 41:51.560\n which are very scikit learn like\n\n41:51.560 --> 41:56.040\n and are ideal for a data scientist,\n\n41:56.040 --> 41:58.240\n machine learning engineer type of profile.\n\n41:58.240 --> 42:00.840\n So now you can have the same framework\n\n42:00.840 --> 42:02.880\n offering the same set of APIs\n\n42:02.880 --> 42:05.000\n that enable a spectrum of workflows\n\n42:05.000 --> 42:08.560\n that are more or less low level, more or less high level\n\n42:08.560 --> 42:13.520\n that are suitable for profiles ranging from researchers\n\n42:13.520 --> 42:15.560\n to data scientists and everything in between.\n\n42:15.560 --> 42:16.960\n Yeah, so that's super exciting.\n\n42:16.960 --> 42:18.400\n I mean, it's not just that,\n\n42:18.400 --> 42:21.680\n it's connected to all kinds of tooling.\n\n42:21.680 --> 42:24.520\n You can go on mobile, you can go with TensorFlow Lite,\n\n42:24.520 --> 42:27.240\n you can go in the cloud or serving and so on.\n\n42:27.240 --> 42:28.960\n It all is connected together.\n\n42:28.960 --> 42:31.880\n Now some of the best software written ever\n\n42:31.880 --> 42:36.880\n is often done by one person, sometimes two.\n\n42:36.880 --> 42:40.800\n So with a Google, you're now seeing sort of Keras\n\n42:40.800 --> 42:42.840\n having to be integrated in TensorFlow,\n\n42:42.840 --> 42:46.800\n I'm sure has a ton of engineers working on.\n\n42:46.800 --> 42:51.040\n And there's, I'm sure a lot of tricky design decisions\n\n42:51.040 --> 42:52.200\n to be made.\n\n42:52.200 --> 42:54.440\n How does that process usually happen\n\n42:54.440 --> 42:56.800\n from at least your perspective?\n\n42:56.800 --> 42:59.800\n What are the debates like?\n\n43:00.720 --> 43:04.200\n Is there a lot of thinking,\n\n43:04.200 --> 43:06.880\n considering different options and so on?\n\n43:06.880 --> 43:08.160\n Yes.\n\n43:08.160 --> 43:12.640\n So a lot of the time I spend at Google\n\n43:12.640 --> 43:17.280\n is actually discussing design discussions, right?\n\n43:17.280 --> 43:20.480\n Writing design docs, participating in design review meetings\n\n43:20.480 --> 43:22.080\n and so on.\n\n43:22.080 --> 43:25.240\n This is as important as actually writing a code.\n\n43:25.240 --> 43:26.080\n Right.\n\n43:26.080 --> 43:28.120\n So there's a lot of thought, there's a lot of thought\n\n43:28.120 --> 43:32.280\n and a lot of care that is taken\n\n43:32.280 --> 43:34.160\n in coming up with these decisions\n\n43:34.160 --> 43:37.160\n and taking into account all of our users\n\n43:37.160 --> 43:40.680\n because TensorFlow has this extremely diverse user base,\n\n43:40.680 --> 43:41.520\n right?\n\n43:41.520 --> 43:43.120\n It's not like just one user segment\n\n43:43.120 --> 43:45.480\n where everyone has the same needs.\n\n43:45.480 --> 43:47.640\n We have small scale production users,\n\n43:47.640 --> 43:49.520\n large scale production users.\n\n43:49.520 --> 43:52.800\n We have startups, we have researchers,\n\n43:53.720 --> 43:55.080\n you know, it's all over the place.\n\n43:55.080 --> 43:57.560\n And we have to cater to all of their needs.\n\n43:57.560 --> 44:00.040\n If I just look at the standard debates\n\n44:00.040 --> 44:04.000\n of C++ or Python, there's some heated debates.\n\n44:04.000 --> 44:06.000\n Do you have those at Google?\n\n44:06.000 --> 44:08.080\n I mean, they're not heated in terms of emotionally,\n\n44:08.080 --> 44:10.800\n but there's probably multiple ways to do it, right?\n\n44:10.800 --> 44:14.040\n So how do you arrive through those design meetings\n\n44:14.040 --> 44:15.440\n at the best way to do it?\n\n44:15.440 --> 44:19.280\n Especially in deep learning where the field is evolving\n\n44:19.280 --> 44:20.880\n as you're doing it.\n\n44:21.880 --> 44:23.600\n Is there some magic to it?\n\n44:23.600 --> 44:26.240\n Is there some magic to the process?\n\n44:26.240 --> 44:28.280\n I don't know if there's magic to the process,\n\n44:28.280 --> 44:30.640\n but there definitely is a process.\n\n44:30.640 --> 44:33.760\n So making design decisions\n\n44:33.760 --> 44:36.080\n is about satisfying a set of constraints,\n\n44:36.080 --> 44:39.920\n but also trying to do so in the simplest way possible,\n\n44:39.920 --> 44:42.240\n because this is what can be maintained,\n\n44:42.240 --> 44:44.920\n this is what can be expanded in the future.\n\n44:44.920 --> 44:49.120\n So you don't want to naively satisfy the constraints\n\n44:49.120 --> 44:51.880\n by just, you know, for each capability you need available,\n\n44:51.880 --> 44:53.960\n you're gonna come up with one argument in your API\n\n44:53.960 --> 44:54.800\n and so on.\n\n44:54.800 --> 44:59.800\n You want to design APIs that are modular and hierarchical\n\n45:00.640 --> 45:04.080\n so that they have an API surface\n\n45:04.080 --> 45:07.040\n that is as small as possible, right?\n\n45:07.040 --> 45:11.640\n And you want this modular hierarchical architecture\n\n45:11.640 --> 45:14.560\n to reflect the way that domain experts\n\n45:14.560 --> 45:16.400\n think about the problem.\n\n45:16.400 --> 45:17.880\n Because as a domain expert,\n\n45:17.880 --> 45:19.840\n when you are reading about a new API,\n\n45:19.840 --> 45:24.760\n you're reading a tutorial or some docs pages,\n\n45:24.760 --> 45:28.200\n you already have a way that you're thinking about the problem.\n\n45:28.200 --> 45:32.320\n You already have like certain concepts in mind\n\n45:32.320 --> 45:35.680\n and you're thinking about how they relate together.\n\n45:35.680 --> 45:37.200\n And when you're reading docs,\n\n45:37.200 --> 45:40.280\n you're trying to build as quickly as possible\n\n45:40.280 --> 45:45.280\n a mapping between the concepts featured in your API\n\n45:45.280 --> 45:46.800\n and the concepts in your mind.\n\n45:46.800 --> 45:48.880\n So you're trying to map your mental model\n\n45:48.880 --> 45:53.600\n as a domain expert to the way things work in the API.\n\n45:53.600 --> 45:57.040\n So you need an API and an underlying implementation\n\n45:57.040 --> 46:00.120\n that are reflecting the way people think about these things.\n\n46:00.120 --> 46:02.880\n So in minimizing the time it takes to do the mapping.\n\n46:02.880 --> 46:04.680\n Yes, minimizing the time,\n\n46:04.680 --> 46:06.560\n the cognitive load there is\n\n46:06.560 --> 46:10.920\n in ingesting this new knowledge about your API.\n\n46:10.920 --> 46:13.160\n An API should not be self referential\n\n46:13.160 --> 46:15.520\n or referring to implementation details.\n\n46:15.520 --> 46:19.160\n It should only be referring to domain specific concepts\n\n46:19.160 --> 46:21.360\n that people already understand.\n\n46:23.240 --> 46:24.480\n Brilliant.\n\n46:24.480 --> 46:27.560\n So what's the future of Keras and TensorFlow look like?\n\n46:27.560 --> 46:29.640\n What does TensorFlow 3.0 look like?\n\n46:30.600 --> 46:33.720\n So that's kind of too far in the future for me to answer,\n\n46:33.720 --> 46:37.800\n especially since I'm not even the one making these decisions.\n\n46:37.800 --> 46:39.080\n Okay.\n\n46:39.080 --> 46:41.240\n But so from my perspective,\n\n46:41.240 --> 46:43.200\n which is just one perspective\n\n46:43.200 --> 46:46.040\n among many different perspectives on the TensorFlow team,\n\n46:47.200 --> 46:52.200\n I'm really excited by developing even higher level APIs,\n\n46:52.360 --> 46:53.560\n higher level than Keras.\n\n46:53.560 --> 46:56.480\n I'm really excited by hyperparameter tuning,\n\n46:56.480 --> 46:59.240\n by automated machine learning, AutoML.\n\n47:01.120 --> 47:03.200\n I think the future is not just, you know,\n\n47:03.200 --> 47:07.600\n defining a model like you were assembling Lego blocks\n\n47:07.600 --> 47:09.200\n and then collect fit on it.\n\n47:09.200 --> 47:13.680\n It's more like an automagical model\n\n47:13.680 --> 47:16.080\n that would just look at your data\n\n47:16.080 --> 47:19.040\n and optimize the objective you're after, right?\n\n47:19.040 --> 47:23.040\n So that's what I'm looking into.\n\n47:23.040 --> 47:26.480\n Yeah, so you put the baby into a room with the problem\n\n47:26.480 --> 47:28.760\n and come back a few hours later\n\n47:28.760 --> 47:30.960\n with a fully solved problem.\n\n47:30.960 --> 47:33.560\n Exactly, it's not like a box of Legos.\n\n47:33.560 --> 47:35.920\n It's more like the combination of a kid\n\n47:35.920 --> 47:38.800\n that's really good at Legos and a box of Legos.\n\n47:38.800 --> 47:41.520\n It's just building the thing on its own.\n\n47:41.520 --> 47:42.680\n Very nice.\n\n47:42.680 --> 47:44.160\n So that's an exciting future.\n\n47:44.160 --> 47:46.080\n I think there's a huge amount of applications\n\n47:46.080 --> 47:48.560\n and revolutions to be had\n\n47:49.920 --> 47:52.640\n under the constraints of the discussion we previously had.\n\n47:52.640 --> 47:57.480\n But what do you think of the current limits of deep learning?\n\n47:57.480 --> 48:02.480\n If we look specifically at these function approximators\n\n48:03.840 --> 48:06.160\n that tries to generalize from data.\n\n48:06.160 --> 48:10.160\n You've talked about local versus extreme generalization.\n\n48:11.120 --> 48:13.280\n You mentioned that neural networks don't generalize well\n\n48:13.280 --> 48:14.560\n and humans do.\n\n48:14.560 --> 48:15.760\n So there's this gap.\n\n48:17.640 --> 48:20.840\n And you've also mentioned that extreme generalization\n\n48:20.840 --> 48:23.960\n requires something like reasoning to fill those gaps.\n\n48:23.960 --> 48:27.560\n So how can we start trying to build systems like that?\n\n48:27.560 --> 48:30.600\n Right, yeah, so this is by design, right?\n\n48:30.600 --> 48:37.080\n Deep learning models are like huge parametric models,\n\n48:37.080 --> 48:39.280\n differentiable, so continuous,\n\n48:39.280 --> 48:42.680\n that go from an input space to an output space.\n\n48:42.680 --> 48:44.120\n And they're trained with gradient descent.\n\n48:44.120 --> 48:47.160\n So they're trained pretty much point by point.\n\n48:47.160 --> 48:50.520\n They are learning a continuous geometric morphing\n\n48:50.520 --> 48:55.320\n from an input vector space to an output vector space.\n\n48:55.320 --> 48:58.960\n And because this is done point by point,\n\n48:58.960 --> 49:02.200\n a deep neural network can only make sense\n\n49:02.200 --> 49:05.880\n of points in experience space that are very close\n\n49:05.880 --> 49:08.520\n to things that it has already seen in string data.\n\n49:08.520 --> 49:12.520\n At best, it can do interpolation across points.\n\n49:13.840 --> 49:17.360\n But that means in order to train your network,\n\n49:17.360 --> 49:21.680\n you need a dense sampling of the input cross output space,\n\n49:22.880 --> 49:25.240\n almost a point by point sampling,\n\n49:25.240 --> 49:27.160\n which can be very expensive if you're dealing\n\n49:27.160 --> 49:29.320\n with complex real world problems,\n\n49:29.320 --> 49:33.240\n like autonomous driving, for instance, or robotics.\n\n49:33.240 --> 49:36.000\n It's doable if you're looking at the subset\n\n49:36.000 --> 49:37.120\n of the visual space.\n\n49:37.120 --> 49:38.800\n But even then, it's still fairly expensive.\n\n49:38.800 --> 49:40.920\n You still need millions of examples.\n\n49:40.920 --> 49:44.240\n And it's only going to be able to make sense of things\n\n49:44.240 --> 49:46.880\n that are very close to what it has seen before.\n\n49:46.880 --> 49:49.160\n And in contrast to that, well, of course,\n\n49:49.160 --> 49:50.160\n you have human intelligence.\n\n49:50.160 --> 49:53.240\n But even if you're not looking at human intelligence,\n\n49:53.240 --> 49:56.800\n you can look at very simple rules, algorithms.\n\n49:56.800 --> 49:58.080\n If you have a symbolic rule,\n\n49:58.080 --> 50:03.080\n it can actually apply to a very, very large set of inputs\n\n50:03.120 --> 50:04.880\n because it is abstract.\n\n50:04.880 --> 50:09.560\n It is not obtained by doing a point by point mapping.\n\n50:10.720 --> 50:14.000\n For instance, if you try to learn a sorting algorithm\n\n50:14.000 --> 50:15.520\n using a deep neural network,\n\n50:15.520 --> 50:18.520\n well, you're very much limited to learning point by point\n\n50:20.080 --> 50:24.360\n what the sorted representation of this specific list is like.\n\n50:24.360 --> 50:29.360\n But instead, you could have a very, very simple\n\n50:29.400 --> 50:31.920\n sorting algorithm written in a few lines.\n\n50:31.920 --> 50:34.520\n Maybe it's just two nested loops.\n\n50:35.560 --> 50:40.560\n And it can process any list at all because it is abstract,\n\n50:41.040 --> 50:42.240\n because it is a set of rules.\n\n50:42.240 --> 50:45.160\n So deep learning is really like point by point\n\n50:45.160 --> 50:48.640\n geometric morphings, train with good and decent.\n\n50:48.640 --> 50:53.640\n And meanwhile, abstract rules can generalize much better.\n\n50:53.640 --> 50:56.160\n And I think the future is we need to combine the two.\n\n50:56.160 --> 50:59.160\n So how do we, do you think, combine the two?\n\n50:59.160 --> 51:03.040\n How do we combine good point by point functions\n\n51:03.040 --> 51:08.040\n with programs, which is what the symbolic AI type systems?\n\n51:08.920 --> 51:11.600\n At which levels the combination happen?\n\n51:11.600 --> 51:14.680\n I mean, obviously we're jumping into the realm\n\n51:14.680 --> 51:16.880\n of where there's no good answers.\n\n51:16.880 --> 51:20.280\n It's just kind of ideas and intuitions and so on.\n\n51:20.280 --> 51:23.080\n Well, if you look at the really successful AI systems\n\n51:23.080 --> 51:26.320\n today, I think they are already hybrid systems\n\n51:26.320 --> 51:29.520\n that are combining symbolic AI with deep learning.\n\n51:29.520 --> 51:32.520\n For instance, successful robotics systems\n\n51:32.520 --> 51:36.400\n are already mostly model based, rule based,\n\n51:37.400 --> 51:39.400\n things like planning algorithms and so on.\n\n51:39.400 --> 51:42.200\n At the same time, they're using deep learning\n\n51:42.200 --> 51:43.840\n as perception modules.\n\n51:43.840 --> 51:46.000\n Sometimes they're using deep learning as a way\n\n51:46.000 --> 51:50.920\n to inject fuzzy intuition into a rule based process.\n\n51:50.920 --> 51:54.560\n If you look at the system like in a self driving car,\n\n51:54.560 --> 51:57.240\n it's not just one big end to end neural network.\n\n51:57.240 --> 51:59.000\n You know, that wouldn't work at all.\n\n51:59.000 --> 52:00.760\n Precisely because in order to train that,\n\n52:00.760 --> 52:05.160\n you would need a dense sampling of experience base\n\n52:05.160 --> 52:06.200\n when it comes to driving,\n\n52:06.200 --> 52:08.880\n which is completely unrealistic, obviously.\n\n52:08.880 --> 52:12.440\n Instead, the self driving car is mostly\n\n52:13.920 --> 52:18.360\n symbolic, you know, it's software, it's programmed by hand.\n\n52:18.360 --> 52:21.640\n So it's mostly based on explicit models.\n\n52:21.640 --> 52:25.840\n In this case, mostly 3D models of the environment\n\n52:25.840 --> 52:29.520\n around the car, but it's interfacing with the real world\n\n52:29.520 --> 52:31.440\n using deep learning modules, right?\n\n52:31.440 --> 52:33.440\n So the deep learning there serves as a way\n\n52:33.440 --> 52:36.080\n to convert the raw sensory information\n\n52:36.080 --> 52:38.320\n to something usable by symbolic systems.\n\n52:39.760 --> 52:42.400\n Okay, well, let's linger on that a little more.\n\n52:42.400 --> 52:45.440\n So dense sampling from input to output.\n\n52:45.440 --> 52:48.240\n You said it's obviously very difficult.\n\n52:48.240 --> 52:50.120\n Is it possible?\n\n52:50.120 --> 52:51.800\n In the case of self driving, you mean?\n\n52:51.800 --> 52:53.040\n Let's say self driving, right?\n\n52:53.040 --> 52:55.760\n Self driving for many people,\n\n52:57.560 --> 52:59.520\n let's not even talk about self driving,\n\n52:59.520 --> 53:03.880\n let's talk about steering, so staying inside the lane.\n\n53:05.040 --> 53:07.080\n Lane following, yeah, it's definitely a problem\n\n53:07.080 --> 53:08.880\n you can solve with an end to end deep learning model,\n\n53:08.880 --> 53:10.600\n but that's like one small subset.\n\n53:10.600 --> 53:11.440\n Hold on a second.\n\n53:11.440 --> 53:12.760\n Yeah, I don't know why you're jumping\n\n53:12.760 --> 53:14.480\n from the extreme so easily,\n\n53:14.480 --> 53:16.280\n because I disagree with you on that.\n\n53:16.280 --> 53:21.000\n I think, well, it's not obvious to me\n\n53:21.000 --> 53:23.400\n that you can solve lane following.\n\n53:23.400 --> 53:25.840\n No, it's not obvious, I think it's doable.\n\n53:25.840 --> 53:30.840\n I think in general, there is no hard limitations\n\n53:31.200 --> 53:33.680\n to what you can learn with a deep neural network,\n\n53:33.680 --> 53:38.680\n as long as the search space is rich enough,\n\n53:40.320 --> 53:42.240\n is flexible enough, and as long as you have\n\n53:42.240 --> 53:45.360\n this dense sampling of the input cross output space.\n\n53:45.360 --> 53:47.720\n The problem is that this dense sampling\n\n53:47.720 --> 53:51.120\n could mean anything from 10,000 examples\n\n53:51.120 --> 53:52.840\n to like trillions and trillions.\n\n53:52.840 --> 53:54.360\n So that's my question.\n\n53:54.360 --> 53:56.200\n So what's your intuition?\n\n53:56.200 --> 53:58.720\n And if you could just give it a chance\n\n53:58.720 --> 54:01.880\n and think what kind of problems can be solved\n\n54:01.880 --> 54:04.240\n by getting a huge amounts of data\n\n54:04.240 --> 54:08.000\n and thereby creating a dense mapping.\n\n54:08.000 --> 54:12.480\n So let's think about natural language dialogue,\n\n54:12.480 --> 54:14.000\n the Turing test.\n\n54:14.000 --> 54:17.000\n Do you think the Turing test can be solved\n\n54:17.000 --> 54:21.120\n with a neural network alone?\n\n54:21.120 --> 54:24.440\n Well, the Turing test is all about tricking people\n\n54:24.440 --> 54:26.880\n into believing they're talking to a human.\n\n54:26.880 --> 54:29.040\n And I don't think that's actually very difficult\n\n54:29.040 --> 54:34.040\n because it's more about exploiting human perception\n\n54:35.600 --> 54:37.520\n and not so much about intelligence.\n\n54:37.520 --> 54:39.680\n There's a big difference between mimicking\n\n54:39.680 --> 54:42.080\n intelligent behavior and actual intelligent behavior.\n\n54:42.080 --> 54:45.360\n So, okay, let's look at maybe the Alexa prize and so on.\n\n54:45.360 --> 54:47.480\n The different formulations of the natural language\n\n54:47.480 --> 54:50.520\n conversation that are less about mimicking\n\n54:50.520 --> 54:52.800\n and more about maintaining a fun conversation\n\n54:52.800 --> 54:54.720\n that lasts for 20 minutes.\n\n54:54.720 --> 54:56.200\n That's a little less about mimicking\n\n54:56.200 --> 54:59.080\n and that's more about, I mean, it's still mimicking,\n\n54:59.080 --> 55:01.440\n but it's more about being able to carry forward\n\n55:01.440 --> 55:03.640\n a conversation with all the tangents that happen\n\n55:03.640 --> 55:05.080\n in dialogue and so on.\n\n55:05.080 --> 55:08.320\n Do you think that problem is learnable\n\n55:08.320 --> 55:13.320\n with a neural network that does the point to point mapping?\n\n55:14.520 --> 55:16.280\n So I think it would be very, very challenging\n\n55:16.280 --> 55:17.800\n to do this with deep learning.\n\n55:17.800 --> 55:21.480\n I don't think it's out of the question either.\n\n55:21.480 --> 55:23.240\n I wouldn't rule it out.\n\n55:23.240 --> 55:25.400\n The space of problems that can be solved\n\n55:25.400 --> 55:26.920\n with a large neural network.\n\n55:26.920 --> 55:30.080\n What's your sense about the space of those problems?\n\n55:30.080 --> 55:32.560\n So useful problems for us.\n\n55:32.560 --> 55:34.800\n In theory, it's infinite, right?\n\n55:34.800 --> 55:36.200\n You can solve any problem.\n\n55:36.200 --> 55:39.800\n In practice, well, deep learning is a great fit\n\n55:39.800 --> 55:41.800\n for perception problems.\n\n55:41.800 --> 55:46.800\n In general, any problem which is naturally amenable\n\n55:47.640 --> 55:52.200\n to explicit handcrafted rules or rules that you can generate\n\n55:52.200 --> 55:54.960\n by exhaustive search over some program space.\n\n55:56.080 --> 55:59.320\n So perception, artificial intuition,\n\n55:59.320 --> 56:03.240\n as long as you have a sufficient training dataset.\n\n56:03.240 --> 56:05.360\n And that's the question, I mean, perception,\n\n56:05.360 --> 56:08.400\n there's interpretation and understanding of the scene,\n\n56:08.400 --> 56:10.280\n which seems to be outside the reach\n\n56:10.280 --> 56:12.960\n of current perception systems.\n\n56:12.960 --> 56:15.920\n So do you think larger networks will be able\n\n56:15.920 --> 56:18.280\n to start to understand the physics\n\n56:18.280 --> 56:21.080\n and the physics of the scene,\n\n56:21.080 --> 56:23.400\n the three dimensional structure and relationships\n\n56:23.400 --> 56:25.560\n of objects in the scene and so on?\n\n56:25.560 --> 56:28.320\n Or really that's where symbolic AI has to step in?\n\n56:28.320 --> 56:34.480\n Well, it's always possible to solve these problems\n\n56:34.480 --> 56:36.800\n with deep learning.\n\n56:36.800 --> 56:38.560\n It's just extremely inefficient.\n\n56:38.560 --> 56:42.000\n A model would be an explicit rule based abstract model\n\n56:42.000 --> 56:45.240\n would be a far better, more compressed\n\n56:45.240 --> 56:46.840\n representation of physics.\n\n56:46.840 --> 56:49.080\n Then learning just this mapping between\n\n56:49.080 --> 56:50.960\n in this situation, this thing happens.\n\n56:50.960 --> 56:52.720\n If you change the situation slightly,\n\n56:52.720 --> 56:54.760\n then this other thing happens and so on.\n\n56:54.760 --> 56:57.440\n Do you think it's possible to automatically generate\n\n56:57.440 --> 57:02.200\n the programs that would require that kind of reasoning?\n\n57:02.200 --> 57:05.360\n Or does it have to, so the way the expert systems fail,\n\n57:05.360 --> 57:07.120\n there's so many facts about the world\n\n57:07.120 --> 57:08.960\n had to be hand coded in.\n\n57:08.960 --> 57:14.600\n Do you think it's possible to learn those logical statements\n\n57:14.600 --> 57:18.200\n that are true about the world and their relationships?\n\n57:18.200 --> 57:20.360\n Do you think, I mean, that's kind of what theorem proving\n\n57:20.360 --> 57:22.680\n at a basic level is trying to do, right?\n\n57:22.680 --> 57:26.160\n Yeah, except it's much harder to formulate statements\n\n57:26.160 --> 57:28.480\n about the world compared to formulating\n\n57:28.480 --> 57:30.320\n mathematical statements.\n\n57:30.320 --> 57:34.200\n Statements about the world tend to be subjective.\n\n57:34.200 --> 57:39.600\n So can you learn rule based models?\n\n57:39.600 --> 57:40.920\n Yes, definitely.\n\n57:40.920 --> 57:43.640\n That's the field of program synthesis.\n\n57:43.640 --> 57:48.040\n However, today we just don't really know how to do it.\n\n57:48.040 --> 57:52.400\n So it's very much a grass search or tree search problem.\n\n57:52.400 --> 57:56.800\n And so we are limited to the sort of tree session grass\n\n57:56.800 --> 57:58.560\n search algorithms that we have today.\n\n57:58.560 --> 58:02.760\n Personally, I think genetic algorithms are very promising.\n\n58:02.760 --> 58:04.360\n So almost like genetic programming.\n\n58:04.360 --> 58:05.560\n Genetic programming, exactly.\n\n58:05.560 --> 58:08.840\n Can you discuss the field of program synthesis?\n\n58:08.840 --> 58:14.560\n Like how many people are working and thinking about it?\n\n58:14.560 --> 58:17.960\n Where we are in the history of program synthesis\n\n58:17.960 --> 58:20.720\n and what are your hopes for it?\n\n58:20.720 --> 58:24.600\n Well, if it were deep learning, this is like the 90s.\n\n58:24.600 --> 58:29.120\n So meaning that we already have existing solutions.\n\n58:29.120 --> 58:34.280\n We are starting to have some basic understanding\n\n58:34.280 --> 58:35.480\n of what this is about.\n\n58:35.480 --> 58:38.000\n But it's still a field that is in its infancy.\n\n58:38.000 --> 58:40.440\n There are very few people working on it.\n\n58:40.440 --> 58:44.480\n There are very few real world applications.\n\n58:44.480 --> 58:47.640\n So the one real world application I'm aware of\n\n58:47.640 --> 58:51.680\n is Flash Fill in Excel.\n\n58:51.680 --> 58:55.080\n It's a way to automatically learn very simple programs\n\n58:55.080 --> 58:58.200\n to format cells in an Excel spreadsheet\n\n58:58.200 --> 59:00.240\n from a few examples.\n\n59:00.240 --> 59:02.800\n For instance, learning a way to format a date, things like that.\n\n59:02.800 --> 59:03.680\n Oh, that's fascinating.\n\n59:03.680 --> 59:04.560\n Yeah.\n\n59:04.560 --> 59:06.280\n You know, OK, that's a fascinating topic.\n\n59:06.280 --> 59:10.480\n I always wonder when I provide a few samples to Excel,\n\n59:10.480 --> 59:12.600\n what it's able to figure out.\n\n59:12.600 --> 59:15.960\n Like just giving it a few dates, what\n\n59:15.960 --> 59:18.480\n are you able to figure out from the pattern I just gave you?\n\n59:18.480 --> 59:19.760\n That's a fascinating question.\n\n59:19.760 --> 59:23.320\n And it's fascinating whether that's learnable patterns.\n\n59:23.320 --> 59:25.520\n And you're saying they're working on that.\n\n59:25.520 --> 59:28.200\n How big is the toolbox currently?\n\n59:28.200 --> 59:29.520\n Are we completely in the dark?\n\n59:29.520 --> 59:30.440\n So if you said the 90s.\n\n59:30.440 --> 59:31.720\n In terms of program synthesis?\n\n59:31.720 --> 59:32.360\n No.\n\n59:32.360 --> 59:37.720\n So I would say, so maybe 90s is even too optimistic.\n\n59:37.720 --> 59:41.080\n Because by the 90s, we already understood back prop.\n\n59:41.080 --> 59:43.960\n We already understood the engine of deep learning,\n\n59:43.960 --> 59:47.280\n even though we couldn't really see its potential quite.\n\n59:47.280 --> 59:48.520\n Today, I don't think we have found\n\n59:48.520 --> 59:50.400\n the engine of program synthesis.\n\n59:50.400 --> 59:52.880\n So we're in the winter before back prop.\n\n59:52.880 --> 59:54.160\n Yeah.\n\n59:54.160 --> 59:55.720\n In a way, yes.\n\n59:55.720 --> 1:00:00.120\n So I do believe program synthesis and general discrete search\n\n1:00:00.120 --> 1:00:02.760\n over rule based models is going to be\n\n1:00:02.760 --> 1:00:06.640\n a cornerstone of AI research in the next century.\n\n1:00:06.640 --> 1:00:10.200\n And that doesn't mean we are going to drop deep learning.\n\n1:00:10.200 --> 1:00:11.880\n Deep learning is immensely useful.\n\n1:00:11.880 --> 1:00:17.200\n Like, being able to learn is a very flexible, adaptable,\n\n1:00:17.200 --> 1:00:18.120\n parametric model.\n\n1:00:18.120 --> 1:00:20.720\n So it's got to understand that's actually immensely useful.\n\n1:00:20.720 --> 1:00:23.040\n All it's doing is pattern cognition.\n\n1:00:23.040 --> 1:00:25.640\n But being good at pattern cognition, given lots of data,\n\n1:00:25.640 --> 1:00:27.920\n is just extremely powerful.\n\n1:00:27.920 --> 1:00:30.320\n So we are still going to be working on deep learning.\n\n1:00:30.320 --> 1:00:31.840\n We are going to be working on program synthesis.\n\n1:00:31.840 --> 1:00:34.680\n We are going to be combining the two in increasingly automated\n\n1:00:34.680 --> 1:00:36.400\n ways.\n\n1:00:36.400 --> 1:00:38.520\n So let's talk a little bit about data.\n\n1:00:38.520 --> 1:00:44.600\n You've tweeted, about 10,000 deep learning papers\n\n1:00:44.600 --> 1:00:47.080\n have been written about hard coding priors\n\n1:00:47.080 --> 1:00:49.600\n about a specific task in a neural network architecture\n\n1:00:49.600 --> 1:00:52.440\n works better than a lack of a prior.\n\n1:00:52.440 --> 1:00:55.120\n Basically, summarizing all these efforts,\n\n1:00:55.120 --> 1:00:56.920\n they put a name to an architecture.\n\n1:00:56.920 --> 1:00:59.280\n But really, what they're doing is hard coding some priors\n\n1:00:59.280 --> 1:01:01.560\n that improve the performance of the system.\n\n1:01:01.560 --> 1:01:06.880\n But which gets straight to the point is probably true.\n\n1:01:06.880 --> 1:01:09.800\n So you say that you can always buy performance by,\n\n1:01:09.800 --> 1:01:12.920\n in quotes, performance by either training on more data,\n\n1:01:12.920 --> 1:01:15.480\n better data, or by injecting task information\n\n1:01:15.480 --> 1:01:18.400\n to the architecture of the preprocessing.\n\n1:01:18.400 --> 1:01:21.280\n However, this isn't informative about the generalization power\n\n1:01:21.280 --> 1:01:23.080\n the techniques use, the fundamental ability\n\n1:01:23.080 --> 1:01:24.200\n to generalize.\n\n1:01:24.200 --> 1:01:26.800\n Do you think we can go far by coming up\n\n1:01:26.800 --> 1:01:29.920\n with better methods for this kind of cheating,\n\n1:01:29.920 --> 1:01:33.520\n for better methods of large scale annotation of data?\n\n1:01:33.520 --> 1:01:34.960\n So building better priors.\n\n1:01:34.960 --> 1:01:37.280\n If you automate it, it's not cheating anymore.\n\n1:01:37.280 --> 1:01:38.360\n Right.\n\n1:01:38.360 --> 1:01:41.600\n I'm joking about the cheating, but large scale.\n\n1:01:41.600 --> 1:01:46.560\n So basically, I'm asking about something\n\n1:01:46.560 --> 1:01:48.280\n that hasn't, from my perspective,\n\n1:01:48.280 --> 1:01:53.360\n been researched too much is exponential improvement\n\n1:01:53.360 --> 1:01:55.960\n in annotation of data.\n\n1:01:55.960 --> 1:01:58.120\n Do you often think about?\n\n1:01:58.120 --> 1:02:00.840\n I think it's actually been researched quite a bit.\n\n1:02:00.840 --> 1:02:02.720\n You just don't see publications about it.\n\n1:02:02.720 --> 1:02:05.840\n Because people who publish papers\n\n1:02:05.840 --> 1:02:07.920\n are going to publish about known benchmarks.\n\n1:02:07.920 --> 1:02:09.800\n Sometimes they're going to read a new benchmark.\n\n1:02:09.800 --> 1:02:12.200\n People who actually have real world large scale\n\n1:02:12.200 --> 1:02:13.880\n depending on problems, they're going\n\n1:02:13.880 --> 1:02:16.960\n to spend a lot of resources into data annotation\n\n1:02:16.960 --> 1:02:18.400\n and good data annotation pipelines,\n\n1:02:18.400 --> 1:02:19.640\n but you don't see any papers about it.\n\n1:02:19.640 --> 1:02:20.400\n That's interesting.\n\n1:02:20.400 --> 1:02:22.720\n So do you think, certainly resources,\n\n1:02:22.720 --> 1:02:24.840\n but do you think there's innovation happening?\n\n1:02:24.840 --> 1:02:25.880\n Oh, yeah.\n\n1:02:25.880 --> 1:02:28.880\n To clarify the point in the tweet.\n\n1:02:28.880 --> 1:02:31.160\n So machine learning in general is\n\n1:02:31.160 --> 1:02:33.840\n the science of generalization.\n\n1:02:33.840 --> 1:02:37.800\n You want to generate knowledge that\n\n1:02:37.800 --> 1:02:40.440\n can be reused across different data sets,\n\n1:02:40.440 --> 1:02:42.000\n across different tasks.\n\n1:02:42.000 --> 1:02:45.280\n And if instead you're looking at one data set\n\n1:02:45.280 --> 1:02:50.000\n and then you are hard coding knowledge about this task\n\n1:02:50.000 --> 1:02:54.040\n into your architecture, this is no more useful\n\n1:02:54.040 --> 1:02:56.760\n than training a network and then saying, oh, I\n\n1:02:56.760 --> 1:03:01.920\n found these weight values perform well.\n\n1:03:01.920 --> 1:03:05.680\n So David Ha, I don't know if you know David,\n\n1:03:05.680 --> 1:03:08.760\n he had a paper the other day about weight\n\n1:03:08.760 --> 1:03:10.400\n agnostic neural networks.\n\n1:03:10.400 --> 1:03:12.120\n And this is a very interesting paper\n\n1:03:12.120 --> 1:03:14.400\n because it really illustrates the fact\n\n1:03:14.400 --> 1:03:17.400\n that an architecture, even without weights,\n\n1:03:17.400 --> 1:03:21.360\n an architecture is knowledge about a task.\n\n1:03:21.360 --> 1:03:23.640\n It encodes knowledge.\n\n1:03:23.640 --> 1:03:25.840\n And when it comes to architectures\n\n1:03:25.840 --> 1:03:30.440\n that are uncrafted by researchers, in some cases,\n\n1:03:30.440 --> 1:03:34.160\n it is very, very clear that all they are doing\n\n1:03:34.160 --> 1:03:38.880\n is artificially reencoding the template that\n\n1:03:38.880 --> 1:03:44.400\n corresponds to the proper way to solve the task encoding\n\n1:03:44.400 --> 1:03:45.200\n a given data set.\n\n1:03:45.200 --> 1:03:48.120\n For instance, I know if you looked\n\n1:03:48.120 --> 1:03:52.280\n at the baby data set, which is about natural language\n\n1:03:52.280 --> 1:03:55.520\n question answering, it is generated by an algorithm.\n\n1:03:55.520 --> 1:03:57.680\n So this is a question answer pairs\n\n1:03:57.680 --> 1:03:59.280\n that are generated by an algorithm.\n\n1:03:59.280 --> 1:04:01.520\n The algorithm is solving a certain template.\n\n1:04:01.520 --> 1:04:04.400\n Turns out, if you craft a network that\n\n1:04:04.400 --> 1:04:06.360\n literally encodes this template, you\n\n1:04:06.360 --> 1:04:09.640\n can solve this data set with nearly 100% accuracy.\n\n1:04:09.640 --> 1:04:11.160\n But that doesn't actually tell you\n\n1:04:11.160 --> 1:04:14.640\n anything about how to solve question answering\n\n1:04:14.640 --> 1:04:17.680\n in general, which is the point.\n\n1:04:17.680 --> 1:04:19.400\n The question is just to linger on it,\n\n1:04:19.400 --> 1:04:21.560\n whether it's from the data side or from the size\n\n1:04:21.560 --> 1:04:23.280\n of the network.\n\n1:04:23.280 --> 1:04:25.920\n I don't know if you've read the blog post by Rich Sutton,\n\n1:04:25.920 --> 1:04:28.400\n The Bitter Lesson, where he says,\n\n1:04:28.400 --> 1:04:31.480\n the biggest lesson that we can read from 70 years of AI\n\n1:04:31.480 --> 1:04:34.720\n research is that general methods that leverage computation\n\n1:04:34.720 --> 1:04:37.160\n are ultimately the most effective.\n\n1:04:37.160 --> 1:04:39.720\n So as opposed to figuring out methods\n\n1:04:39.720 --> 1:04:41.840\n that can generalize effectively, do you\n\n1:04:41.840 --> 1:04:47.720\n think we can get pretty far by just having something\n\n1:04:47.720 --> 1:04:51.520\n that leverages computation and the improvement of computation?\n\n1:04:51.520 --> 1:04:54.960\n Yeah, so I think Rich is making a very good point, which\n\n1:04:54.960 --> 1:04:57.560\n is that a lot of these papers, which are actually\n\n1:04:57.560 --> 1:05:02.800\n all about manually hardcoding prior knowledge about a task\n\n1:05:02.800 --> 1:05:04.720\n into some system, it doesn't have\n\n1:05:04.720 --> 1:05:08.600\n to be deep learning architecture, but into some system.\n\n1:05:08.600 --> 1:05:11.920\n These papers are not actually making any impact.\n\n1:05:11.920 --> 1:05:14.800\n Instead, what's making really long term impact\n\n1:05:14.800 --> 1:05:18.520\n is very simple, very general systems\n\n1:05:18.520 --> 1:05:21.280\n that are really agnostic to all these tricks.\n\n1:05:21.280 --> 1:05:23.320\n Because these tricks do not generalize.\n\n1:05:23.320 --> 1:05:27.480\n And of course, the one general and simple thing\n\n1:05:27.480 --> 1:05:33.160\n that you should focus on is that which leverages computation.\n\n1:05:33.160 --> 1:05:36.200\n Because computation, the availability\n\n1:05:36.200 --> 1:05:39.400\n of large scale computation has been increasing exponentially\n\n1:05:39.400 --> 1:05:40.560\n following Moore's law.\n\n1:05:40.560 --> 1:05:44.080\n So if your algorithm is all about exploiting this,\n\n1:05:44.080 --> 1:05:47.440\n then your algorithm is suddenly exponentially improving.\n\n1:05:47.440 --> 1:05:52.400\n So I think Rich is definitely right.\n\n1:05:52.400 --> 1:05:57.120\n However, he's right about the past 70 years.\n\n1:05:57.120 --> 1:05:59.440\n He's like assessing the past 70 years.\n\n1:05:59.440 --> 1:06:02.360\n I am not sure that this assessment will still\n\n1:06:02.360 --> 1:06:04.880\n hold true for the next 70 years.\n\n1:06:04.880 --> 1:06:07.160\n It might to some extent.\n\n1:06:07.160 --> 1:06:08.560\n I suspect it will not.\n\n1:06:08.560 --> 1:06:11.560\n Because the truth of his assessment\n\n1:06:11.560 --> 1:06:16.800\n is a function of the context in which this research took place.\n\n1:06:16.800 --> 1:06:18.600\n And the context is changing.\n\n1:06:18.600 --> 1:06:21.440\n Moore's law might not be applicable anymore,\n\n1:06:21.440 --> 1:06:23.760\n for instance, in the future.\n\n1:06:23.760 --> 1:06:31.200\n And I do believe that when you tweak one aspect of a system,\n\n1:06:31.200 --> 1:06:32.920\n when you exploit one aspect of a system,\n\n1:06:32.920 --> 1:06:36.480\n some other aspect starts becoming the bottleneck.\n\n1:06:36.480 --> 1:06:38.800\n Let's say you have unlimited computation.\n\n1:06:38.800 --> 1:06:41.440\n Well, then data is the bottleneck.\n\n1:06:41.440 --> 1:06:43.560\n And I think we are already starting\n\n1:06:43.560 --> 1:06:45.720\n to be in a regime where our systems are\n\n1:06:45.720 --> 1:06:48.120\n so large in scale and so data ingrained\n\n1:06:48.120 --> 1:06:50.360\n that data today and the quality of data\n\n1:06:50.360 --> 1:06:53.040\n and the scale of data is the bottleneck.\n\n1:06:53.040 --> 1:06:58.160\n And in this environment, the bitter lesson from Rich\n\n1:06:58.160 --> 1:07:00.800\n is not going to be true anymore.\n\n1:07:00.800 --> 1:07:03.960\n So I think we are going to move from a focus\n\n1:07:03.960 --> 1:07:09.840\n on a computation scale to focus on data efficiency.\n\n1:07:09.840 --> 1:07:10.720\n Data efficiency.\n\n1:07:10.720 --> 1:07:13.120\n So that's getting to the question of symbolic AI.\n\n1:07:13.120 --> 1:07:16.280\n But to linger on the deep learning approaches,\n\n1:07:16.280 --> 1:07:19.240\n do you have hope for either unsupervised learning\n\n1:07:19.240 --> 1:07:23.280\n or reinforcement learning, which are\n\n1:07:23.280 --> 1:07:28.120\n ways of being more data efficient in terms\n\n1:07:28.120 --> 1:07:31.560\n of the amount of data they need that required human annotation?\n\n1:07:31.560 --> 1:07:34.280\n So unsupervised learning and reinforcement learning\n\n1:07:34.280 --> 1:07:36.640\n are frameworks for learning, but they are not\n\n1:07:36.640 --> 1:07:39.000\n like any specific technique.\n\n1:07:39.000 --> 1:07:41.200\n So usually when people say reinforcement learning,\n\n1:07:41.200 --> 1:07:43.320\n what they really mean is deep reinforcement learning,\n\n1:07:43.320 --> 1:07:47.440\n which is like one approach which is actually very questionable.\n\n1:07:47.440 --> 1:07:50.920\n The question I was asking was unsupervised learning\n\n1:07:50.920 --> 1:07:54.680\n with deep neural networks and deep reinforcement learning.\n\n1:07:54.680 --> 1:07:56.840\n Well, these are not really data efficient\n\n1:07:56.840 --> 1:08:00.520\n because you're still leveraging these huge parametric models\n\n1:08:00.520 --> 1:08:03.720\n point by point with gradient descent.\n\n1:08:03.720 --> 1:08:08.000\n It is more efficient in terms of the number of annotations,\n\n1:08:08.000 --> 1:08:09.520\n the density of annotations you need.\n\n1:08:09.520 --> 1:08:13.840\n So the idea being to learn the latent space around which\n\n1:08:13.840 --> 1:08:17.960\n the data is organized and then map the sparse annotations\n\n1:08:17.960 --> 1:08:18.760\n into it.\n\n1:08:18.760 --> 1:08:23.560\n And sure, I mean, that's clearly a very good idea.\n\n1:08:23.560 --> 1:08:26.080\n It's not really a topic I would be working on,\n\n1:08:26.080 --> 1:08:28.040\n but it's clearly a good idea.\n\n1:08:28.040 --> 1:08:31.760\n So it would get us to solve some problems that?\n\n1:08:31.760 --> 1:08:34.880\n It will get us to incremental improvements\n\n1:08:34.880 --> 1:08:38.240\n in labeled data efficiency.\n\n1:08:38.240 --> 1:08:43.520\n Do you have concerns about short term or long term threats\n\n1:08:43.520 --> 1:08:47.800\n from AI, from artificial intelligence?\n\n1:08:47.800 --> 1:08:50.640\n Yes, definitely to some extent.\n\n1:08:50.640 --> 1:08:52.800\n And what's the shape of those concerns?\n\n1:08:52.800 --> 1:08:56.880\n This is actually something I've briefly written about.\n\n1:08:56.880 --> 1:09:02.680\n But the capabilities of deep learning technology\n\n1:09:02.680 --> 1:09:05.200\n can be used in many ways that are\n\n1:09:05.200 --> 1:09:09.760\n concerning from mass surveillance with things\n\n1:09:09.760 --> 1:09:11.880\n like facial recognition.\n\n1:09:11.880 --> 1:09:15.440\n In general, tracking lots of data about everyone\n\n1:09:15.440 --> 1:09:18.920\n and then being able to making sense of this data\n\n1:09:18.920 --> 1:09:22.240\n to do identification, to do prediction.\n\n1:09:22.240 --> 1:09:23.160\n That's concerning.\n\n1:09:23.160 --> 1:09:26.560\n That's something that's being very aggressively pursued\n\n1:09:26.560 --> 1:09:31.440\n by totalitarian states like China.\n\n1:09:31.440 --> 1:09:34.000\n One thing I am very much concerned about\n\n1:09:34.000 --> 1:09:40.640\n is that our lives are increasingly online,\n\n1:09:40.640 --> 1:09:43.280\n are increasingly digital, made of information,\n\n1:09:43.280 --> 1:09:48.080\n made of information consumption and information production,\n\n1:09:48.080 --> 1:09:51.800\n our digital footprint, I would say.\n\n1:09:51.800 --> 1:09:56.280\n And if you absorb all of this data\n\n1:09:56.280 --> 1:10:01.440\n and you are in control of where you consume information,\n\n1:10:01.440 --> 1:10:06.960\n social networks and so on, recommendation engines,\n\n1:10:06.960 --> 1:10:10.200\n then you can build a sort of reinforcement\n\n1:10:10.200 --> 1:10:13.760\n loop for human behavior.\n\n1:10:13.760 --> 1:10:18.360\n You can observe the state of your mind at time t.\n\n1:10:18.360 --> 1:10:21.080\n You can predict how you would react\n\n1:10:21.080 --> 1:10:23.800\n to different pieces of content, how\n\n1:10:23.800 --> 1:10:27.000\n to get you to move your mind in a certain direction.\n\n1:10:27.000 --> 1:10:33.160\n And then you can feed you the specific piece of content\n\n1:10:33.160 --> 1:10:35.680\n that would move you in a specific direction.\n\n1:10:35.680 --> 1:10:41.800\n And you can do this at scale in terms\n\n1:10:41.800 --> 1:10:44.960\n of doing it continuously in real time.\n\n1:10:44.960 --> 1:10:46.440\n You can also do it at scale in terms\n\n1:10:46.440 --> 1:10:50.480\n of scaling this to many, many people, to entire populations.\n\n1:10:50.480 --> 1:10:53.840\n So potentially, artificial intelligence,\n\n1:10:53.840 --> 1:10:57.440\n even in its current state, if you combine it\n\n1:10:57.440 --> 1:11:01.760\n with the internet, with the fact that all of our lives\n\n1:11:01.760 --> 1:11:05.120\n are moving to digital devices and digital information\n\n1:11:05.120 --> 1:11:08.720\n consumption and creation, what you get\n\n1:11:08.720 --> 1:11:14.480\n is the possibility to achieve mass manipulation of behavior\n\n1:11:14.480 --> 1:11:16.840\n and mass psychological control.\n\n1:11:16.840 --> 1:11:18.520\n And this is a very real possibility.\n\n1:11:18.520 --> 1:11:22.080\n Yeah, so you're talking about any kind of recommender system.\n\n1:11:22.080 --> 1:11:26.160\n Let's look at the YouTube algorithm, Facebook,\n\n1:11:26.160 --> 1:11:29.720\n anything that recommends content you should watch next.\n\n1:11:29.720 --> 1:11:32.960\n And it's fascinating to think that there's\n\n1:11:32.960 --> 1:11:41.120\n some aspects of human behavior that you can say a problem of,\n\n1:11:41.120 --> 1:11:45.400\n is this person hold Republican beliefs or Democratic beliefs?\n\n1:11:45.400 --> 1:11:50.240\n And this is a trivial, that's an objective function.\n\n1:11:50.240 --> 1:11:52.600\n And you can optimize, and you can measure,\n\n1:11:52.600 --> 1:11:54.360\n and you can turn everybody into a Republican\n\n1:11:54.360 --> 1:11:56.080\n or everybody into a Democrat.\n\n1:11:56.080 --> 1:11:57.840\n I do believe it's true.\n\n1:11:57.840 --> 1:12:03.680\n So the human mind is very, if you look at the human mind\n\n1:12:03.680 --> 1:12:05.320\n as a kind of computer program, it\n\n1:12:05.320 --> 1:12:07.560\n has a very large exploit surface.\n\n1:12:07.560 --> 1:12:09.360\n It has many, many vulnerabilities.\n\n1:12:09.360 --> 1:12:10.840\n Exploit surfaces, yeah.\n\n1:12:10.840 --> 1:12:13.520\n Ways you can control it.\n\n1:12:13.520 --> 1:12:16.680\n For instance, when it comes to your political beliefs,\n\n1:12:16.680 --> 1:12:19.400\n this is very much tied to your identity.\n\n1:12:19.400 --> 1:12:23.040\n So for instance, if I'm in control of your news feed\n\n1:12:23.040 --> 1:12:26.000\n on your favorite social media platforms,\n\n1:12:26.000 --> 1:12:29.360\n this is actually where you're getting your news from.\n\n1:12:29.360 --> 1:12:32.960\n And of course, I can choose to only show you\n\n1:12:32.960 --> 1:12:37.120\n news that will make you see the world in a specific way.\n\n1:12:37.120 --> 1:12:41.920\n But I can also create incentives for you\n\n1:12:41.920 --> 1:12:44.720\n to post about some political beliefs.\n\n1:12:44.720 --> 1:12:47.960\n And then when I get you to express a statement,\n\n1:12:47.960 --> 1:12:51.840\n if it's a statement that me as the controller,\n\n1:12:51.840 --> 1:12:53.800\n I want to reinforce.\n\n1:12:53.800 --> 1:12:55.560\n I can just show it to people who will agree,\n\n1:12:55.560 --> 1:12:56.880\n and they will like it.\n\n1:12:56.880 --> 1:12:59.280\n And that will reinforce the statement in your mind.\n\n1:12:59.280 --> 1:13:02.760\n If this is a statement I want you to,\n\n1:13:02.760 --> 1:13:05.320\n this is a belief I want you to abandon,\n\n1:13:05.320 --> 1:13:09.600\n I can, on the other hand, show it to opponents.\n\n1:13:09.600 --> 1:13:10.640\n We'll attack you.\n\n1:13:10.640 --> 1:13:12.840\n And because they attack you, at the very least,\n\n1:13:12.840 --> 1:13:16.840\n next time you will think twice about posting it.\n\n1:13:16.840 --> 1:13:20.280\n But maybe you will even start believing this\n\n1:13:20.280 --> 1:13:22.840\n because you got pushback.\n\n1:13:22.840 --> 1:13:28.440\n So there are many ways in which social media platforms\n\n1:13:28.440 --> 1:13:30.520\n can potentially control your opinions.\n\n1:13:30.520 --> 1:13:35.040\n And today, so all of these things\n\n1:13:35.040 --> 1:13:38.240\n are already being controlled by AI algorithms.\n\n1:13:38.240 --> 1:13:41.880\n These algorithms do not have any explicit political goal\n\n1:13:41.880 --> 1:13:42.880\n today.\n\n1:13:42.880 --> 1:13:48.680\n Well, potentially they could, like if some totalitarian\n\n1:13:48.680 --> 1:13:52.720\n government takes over social media platforms\n\n1:13:52.720 --> 1:13:55.360\n and decides that now we are going to use this not just\n\n1:13:55.360 --> 1:13:58.040\n for mass surveillance, but also for mass opinion control\n\n1:13:58.040 --> 1:13:59.360\n and behavior control.\n\n1:13:59.360 --> 1:14:01.840\n Very bad things could happen.\n\n1:14:01.840 --> 1:14:06.480\n But what's really fascinating and actually quite concerning\n\n1:14:06.480 --> 1:14:11.280\n is that even without an explicit intent to manipulate,\n\n1:14:11.280 --> 1:14:14.760\n you're already seeing very dangerous dynamics\n\n1:14:14.760 --> 1:14:18.160\n in terms of how these content recommendation\n\n1:14:18.160 --> 1:14:19.800\n algorithms behave.\n\n1:14:19.800 --> 1:14:24.920\n Because right now, the goal, the objective function\n\n1:14:24.920 --> 1:14:28.640\n of these algorithms is to maximize engagement,\n\n1:14:28.640 --> 1:14:32.520\n which seems fairly innocuous at first.\n\n1:14:32.520 --> 1:14:36.480\n However, it is not because content\n\n1:14:36.480 --> 1:14:42.000\n that will maximally engage people, get people to react\n\n1:14:42.000 --> 1:14:44.720\n in an emotional way, get people to click on something.\n\n1:14:44.720 --> 1:14:52.200\n It is very often content that is not\n\n1:14:52.200 --> 1:14:54.400\n healthy to the public discourse.\n\n1:14:54.400 --> 1:14:58.200\n For instance, fake news are far more\n\n1:14:58.200 --> 1:15:01.320\n likely to get you to click on them than real news\n\n1:15:01.320 --> 1:15:06.960\n simply because they are not constrained to reality.\n\n1:15:06.960 --> 1:15:11.360\n So they can be as outrageous, as surprising,\n\n1:15:11.360 --> 1:15:15.880\n as good stories as you want because they're artificial.\n\n1:15:15.880 --> 1:15:18.880\n To me, that's an exciting world because so much good\n\n1:15:18.880 --> 1:15:19.560\n can come.\n\n1:15:19.560 --> 1:15:24.520\n So there's an opportunity to educate people.\n\n1:15:24.520 --> 1:15:31.200\n You can balance people's worldview with other ideas.\n\n1:15:31.200 --> 1:15:33.800\n So there's so many objective functions.\n\n1:15:33.800 --> 1:15:35.840\n The space of objective functions that\n\n1:15:35.840 --> 1:15:40.720\n create better civilizations is large, arguably infinite.\n\n1:15:40.720 --> 1:15:43.720\n But there's also a large space that\n\n1:15:43.720 --> 1:15:51.480\n creates division and destruction, civil war,\n\n1:15:51.480 --> 1:15:53.160\n a lot of bad stuff.\n\n1:15:53.160 --> 1:15:56.920\n And the worry is, naturally, probably that space\n\n1:15:56.920 --> 1:15:59.160\n is bigger, first of all.\n\n1:15:59.160 --> 1:16:04.920\n And if we don't explicitly think about what kind of effects\n\n1:16:04.920 --> 1:16:08.320\n are going to be observed from different objective functions,\n\n1:16:08.320 --> 1:16:10.160\n then we're going to get into trouble.\n\n1:16:10.160 --> 1:16:14.480\n But the question is, how do we get into rooms\n\n1:16:14.480 --> 1:16:18.560\n and have discussions, so inside Google, inside Facebook,\n\n1:16:18.560 --> 1:16:21.840\n inside Twitter, and think about, OK,\n\n1:16:21.840 --> 1:16:24.840\n how can we drive up engagement and, at the same time,\n\n1:16:24.840 --> 1:16:28.200\n create a good society?\n\n1:16:28.200 --> 1:16:29.560\n Is it even possible to have that kind\n\n1:16:29.560 --> 1:16:31.720\n of philosophical discussion?\n\n1:16:31.720 --> 1:16:33.080\n I think you can definitely try.\n\n1:16:33.080 --> 1:16:37.280\n So from my perspective, I would feel rather uncomfortable\n\n1:16:37.280 --> 1:16:41.560\n with companies that are uncomfortable with these new\n\n1:16:41.560 --> 1:16:47.120\n student algorithms, with them making explicit decisions\n\n1:16:47.120 --> 1:16:50.440\n to manipulate people's opinions or behaviors,\n\n1:16:50.440 --> 1:16:53.480\n even if the intent is good, because that's\n\n1:16:53.480 --> 1:16:55.200\n a very totalitarian mindset.\n\n1:16:55.200 --> 1:16:57.440\n So instead, what I would like to see\n\n1:16:57.440 --> 1:16:58.880\n is probably never going to happen,\n\n1:16:58.880 --> 1:17:00.360\n because it's not super realistic,\n\n1:17:00.360 --> 1:17:02.520\n but that's actually something I really care about.\n\n1:17:02.520 --> 1:17:06.280\n I would like all these algorithms\n\n1:17:06.280 --> 1:17:10.560\n to present configuration settings to their users,\n\n1:17:10.560 --> 1:17:14.600\n so that the users can actually make the decision about how\n\n1:17:14.600 --> 1:17:19.000\n they want to be impacted by these information\n\n1:17:19.000 --> 1:17:21.960\n recommendation, content recommendation algorithms.\n\n1:17:21.960 --> 1:17:24.240\n For instance, as a user of something\n\n1:17:24.240 --> 1:17:26.520\n like YouTube or Twitter, maybe I want\n\n1:17:26.520 --> 1:17:30.280\n to maximize learning about a specific topic.\n\n1:17:30.280 --> 1:17:36.800\n So I want the algorithm to feed my curiosity,\n\n1:17:36.800 --> 1:17:38.760\n which is in itself a very interesting problem.\n\n1:17:38.760 --> 1:17:41.200\n So instead of maximizing my engagement,\n\n1:17:41.200 --> 1:17:44.600\n it will maximize how fast and how much I'm learning.\n\n1:17:44.600 --> 1:17:47.360\n And it will also take into account the accuracy,\n\n1:17:47.360 --> 1:17:50.680\n hopefully, of the information I'm learning.\n\n1:17:50.680 --> 1:17:55.680\n So yeah, the user should be able to determine exactly\n\n1:17:55.680 --> 1:17:58.560\n how these algorithms are affecting their lives.\n\n1:17:58.560 --> 1:18:03.520\n I don't want actually any entity making decisions\n\n1:18:03.520 --> 1:18:09.480\n about in which direction they're going to try to manipulate me.\n\n1:18:09.480 --> 1:18:11.680\n I want technology.\n\n1:18:11.680 --> 1:18:14.280\n So AI, these algorithms are increasingly\n\n1:18:14.280 --> 1:18:18.560\n going to be our interface to a world that is increasingly\n\n1:18:18.560 --> 1:18:19.960\n made of information.\n\n1:18:19.960 --> 1:18:25.840\n And I want everyone to be in control of this interface,\n\n1:18:25.840 --> 1:18:29.160\n to interface with the world on their own terms.\n\n1:18:29.160 --> 1:18:32.840\n So if someone wants these algorithms\n\n1:18:32.840 --> 1:18:37.640\n to serve their own personal growth goals,\n\n1:18:37.640 --> 1:18:40.640\n they should be able to configure these algorithms\n\n1:18:40.640 --> 1:18:41.800\n in such a way.\n\n1:18:41.800 --> 1:18:46.680\n Yeah, but so I know it's painful to have explicit decisions.\n\n1:18:46.680 --> 1:18:51.080\n But there is underlying explicit decisions,\n\n1:18:51.080 --> 1:18:53.360\n which is some of the most beautiful fundamental\n\n1:18:53.360 --> 1:18:57.400\n philosophy that we have before us,\n\n1:18:57.400 --> 1:19:01.120\n which is personal growth.\n\n1:19:01.120 --> 1:19:05.680\n If I want to watch videos from which I can learn,\n\n1:19:05.680 --> 1:19:08.080\n what does that mean?\n\n1:19:08.080 --> 1:19:11.800\n So if I have a checkbox that wants to emphasize learning,\n\n1:19:11.800 --> 1:19:15.480\n there's still an algorithm with explicit decisions in it\n\n1:19:15.480 --> 1:19:17.800\n that would promote learning.\n\n1:19:17.800 --> 1:19:19.200\n What does that mean for me?\n\n1:19:19.200 --> 1:19:22.800\n For example, I've watched a documentary on flat Earth\n\n1:19:22.800 --> 1:19:23.640\n theory, I guess.\n\n1:19:27.280 --> 1:19:28.240\n I learned a lot.\n\n1:19:28.240 --> 1:19:29.800\n I'm really glad I watched it.\n\n1:19:29.800 --> 1:19:32.560\n It was a friend recommended it to me.\n\n1:19:32.560 --> 1:19:35.800\n Because I don't have such an allergic reaction to crazy\n\n1:19:35.800 --> 1:19:37.640\n people, as my fellow colleagues do.\n\n1:19:37.640 --> 1:19:40.360\n But it was very eye opening.\n\n1:19:40.360 --> 1:19:42.120\n And for others, it might not be.\n\n1:19:42.120 --> 1:19:45.560\n From others, they might just get turned off from that, same\n\n1:19:45.560 --> 1:19:47.160\n with Republican and Democrat.\n\n1:19:47.160 --> 1:19:50.200\n And it's a non trivial problem.\n\n1:19:50.200 --> 1:19:52.880\n And first of all, if it's done well,\n\n1:19:52.880 --> 1:19:56.560\n I don't think it's something that wouldn't happen,\n\n1:19:56.560 --> 1:19:59.280\n that YouTube wouldn't be promoting,\n\n1:19:59.280 --> 1:20:00.200\n or Twitter wouldn't be.\n\n1:20:00.200 --> 1:20:02.280\n It's just a really difficult problem,\n\n1:20:02.280 --> 1:20:05.520\n how to give people control.\n\n1:20:05.520 --> 1:20:08.960\n Well, it's mostly an interface design problem.\n\n1:20:08.960 --> 1:20:11.080\n The way I see it, you want to create technology\n\n1:20:11.080 --> 1:20:16.400\n that's like a mentor, or a coach, or an assistant,\n\n1:20:16.400 --> 1:20:20.520\n so that it's not your boss.\n\n1:20:20.520 --> 1:20:22.560\n You are in control of it.\n\n1:20:22.560 --> 1:20:25.760\n You are telling it what to do for you.\n\n1:20:25.760 --> 1:20:27.840\n And if you feel like it's manipulating you,\n\n1:20:27.840 --> 1:20:31.760\n it's not actually doing what you want.\n\n1:20:31.760 --> 1:20:34.920\n You should be able to switch to a different algorithm.\n\n1:20:34.920 --> 1:20:36.440\n So that's fine tune control.\n\n1:20:36.440 --> 1:20:38.840\n You kind of learn that you're trusting\n\n1:20:38.840 --> 1:20:40.080\n the human collaboration.\n\n1:20:40.080 --> 1:20:41.920\n I mean, that's how I see autonomous vehicles too,\n\n1:20:41.920 --> 1:20:44.480\n is giving as much information as possible,\n\n1:20:44.480 --> 1:20:47.240\n and you learn that dance yourself.\n\n1:20:47.240 --> 1:20:50.280\n Yeah, Adobe, I don't know if you use Adobe product\n\n1:20:50.280 --> 1:20:52.280\n for like Photoshop.\n\n1:20:52.280 --> 1:20:55.040\n They're trying to see if they can inject YouTube\n\n1:20:55.040 --> 1:20:57.120\n into their interface, but basically allow you\n\n1:20:57.120 --> 1:20:59.840\n to show you all these videos,\n\n1:20:59.840 --> 1:21:03.320\n that everybody's confused about what to do with features.\n\n1:21:03.320 --> 1:21:07.120\n So basically teach people by linking to,\n\n1:21:07.120 --> 1:21:10.280\n in that way, it's an assistant that uses videos\n\n1:21:10.280 --> 1:21:13.440\n as a basic element of information.\n\n1:21:13.440 --> 1:21:18.240\n Okay, so what practically should people do\n\n1:21:18.240 --> 1:21:24.000\n to try to fight against abuses of these algorithms,\n\n1:21:24.000 --> 1:21:27.400\n or algorithms that manipulate us?\n\n1:21:27.400 --> 1:21:29.280\n Honestly, it's a very, very difficult problem,\n\n1:21:29.280 --> 1:21:32.800\n because to start with, there is very little public awareness\n\n1:21:32.800 --> 1:21:35.040\n of these issues.\n\n1:21:35.040 --> 1:21:38.520\n Very few people would think there's anything wrong\n\n1:21:38.520 --> 1:21:39.720\n with the unused algorithm,\n\n1:21:39.720 --> 1:21:42.040\n even though there is actually something wrong already,\n\n1:21:42.040 --> 1:21:44.480\n which is that it's trying to maximize engagement\n\n1:21:44.480 --> 1:21:49.880\n most of the time, which has very negative side effects.\n\n1:21:49.880 --> 1:21:56.160\n So ideally, so the very first thing is to stop\n\n1:21:56.160 --> 1:21:59.560\n trying to purely maximize engagement,\n\n1:21:59.560 --> 1:22:06.560\n try to propagate content based on popularity, right?\n\n1:22:06.560 --> 1:22:11.040\n Instead, take into account the goals\n\n1:22:11.040 --> 1:22:13.560\n and the profiles of each user.\n\n1:22:13.560 --> 1:22:16.920\n So you will be, one example is, for instance,\n\n1:22:16.920 --> 1:22:20.800\n when I look at topic recommendations on Twitter,\n\n1:22:20.800 --> 1:22:24.480\n it's like, you know, they have this news tab\n\n1:22:24.480 --> 1:22:25.480\n with switch recommendations.\n\n1:22:25.480 --> 1:22:28.480\n It's always the worst coverage,\n\n1:22:28.480 --> 1:22:30.360\n because it's content that appeals\n\n1:22:30.360 --> 1:22:34.080\n to the smallest common denominator\n\n1:22:34.080 --> 1:22:37.080\n to all Twitter users, because they're trying to optimize.\n\n1:22:37.080 --> 1:22:39.040\n They're purely trying to optimize popularity.\n\n1:22:39.040 --> 1:22:41.320\n They're purely trying to optimize engagement.\n\n1:22:41.320 --> 1:22:42.960\n But that's not what I want.\n\n1:22:42.960 --> 1:22:46.080\n So they should put me in control of some setting\n\n1:22:46.080 --> 1:22:50.360\n so that I define what's the objective function\n\n1:22:50.360 --> 1:22:52.200\n that Twitter is going to be following\n\n1:22:52.200 --> 1:22:54.120\n to show me this content.\n\n1:22:54.120 --> 1:22:57.360\n And honestly, so this is all about interface design.\n\n1:22:57.360 --> 1:22:59.440\n And we are not, it's not realistic\n\n1:22:59.440 --> 1:23:01.760\n to give users control of a bunch of knobs\n\n1:23:01.760 --> 1:23:03.400\n that define algorithm.\n\n1:23:03.400 --> 1:23:06.760\n Instead, we should purely put them in charge\n\n1:23:06.760 --> 1:23:09.400\n of defining the objective function.\n\n1:23:09.400 --> 1:23:13.240\n Like, let the user tell us what they want to achieve,\n\n1:23:13.240 --> 1:23:15.280\n how they want this algorithm to impact their lives.\n\n1:23:15.280 --> 1:23:16.680\n So do you think it is that,\n\n1:23:16.680 --> 1:23:19.360\n or do they provide individual article by article\n\n1:23:19.360 --> 1:23:21.600\n reward structure where you give a signal,\n\n1:23:21.600 --> 1:23:24.720\n I'm glad I saw this, or I'm glad I didn't?\n\n1:23:24.720 --> 1:23:28.480\n So like a Spotify type feedback mechanism,\n\n1:23:28.480 --> 1:23:30.680\n it works to some extent.\n\n1:23:30.680 --> 1:23:32.000\n I'm kind of skeptical about it\n\n1:23:32.000 --> 1:23:34.880\n because the only way the algorithm,\n\n1:23:34.880 --> 1:23:39.120\n the algorithm will attempt to relate your choices\n\n1:23:39.120 --> 1:23:41.040\n with the choices of everyone else,\n\n1:23:41.040 --> 1:23:45.000\n which might, you know, if you have an average profile\n\n1:23:45.000 --> 1:23:47.880\n that works fine, I'm sure Spotify accommodations work fine\n\n1:23:47.880 --> 1:23:49.560\n if you just like mainstream stuff.\n\n1:23:49.560 --> 1:23:53.960\n If you don't, it can be, it's not optimal at all actually.\n\n1:23:53.960 --> 1:23:56.040\n It'll be in an efficient search\n\n1:23:56.040 --> 1:24:00.800\n for the part of the Spotify world that represents you.\n\n1:24:00.800 --> 1:24:02.960\n So it's a tough problem,\n\n1:24:02.960 --> 1:24:07.960\n but do note that even a feedback system\n\n1:24:07.960 --> 1:24:10.880\n like what Spotify has does not give me control\n\n1:24:10.880 --> 1:24:15.000\n over what the algorithm is trying to optimize for.\n\n1:24:16.320 --> 1:24:19.360\n Well, public awareness, which is what we're doing now,\n\n1:24:19.360 --> 1:24:21.360\n is a good place to start.\n\n1:24:21.360 --> 1:24:25.960\n Do you have concerns about longterm existential threats\n\n1:24:25.960 --> 1:24:27.360\n of artificial intelligence?\n\n1:24:28.280 --> 1:24:31.040\n Well, as I was saying,\n\n1:24:31.040 --> 1:24:33.360\n our world is increasingly made of information.\n\n1:24:33.360 --> 1:24:36.240\n AI algorithms are increasingly going to be our interface\n\n1:24:36.240 --> 1:24:37.880\n to this world of information,\n\n1:24:37.880 --> 1:24:41.480\n and somebody will be in control of these algorithms.\n\n1:24:41.480 --> 1:24:45.920\n And that puts us in any kind of a bad situation, right?\n\n1:24:45.920 --> 1:24:46.880\n It has risks.\n\n1:24:46.880 --> 1:24:50.840\n It has risks coming from potentially large companies\n\n1:24:50.840 --> 1:24:53.760\n wanting to optimize their own goals,\n\n1:24:53.760 --> 1:24:55.960\n maybe profit, maybe something else.\n\n1:24:55.960 --> 1:25:00.720\n Also from governments who might want to use these algorithms\n\n1:25:00.720 --> 1:25:03.520\n as a means of control of the population.\n\n1:25:03.520 --> 1:25:05.000\n Do you think there's existential threat\n\n1:25:05.000 --> 1:25:06.320\n that could arise from that?\n\n1:25:06.320 --> 1:25:09.120\n So existential threat.\n\n1:25:09.120 --> 1:25:13.240\n So maybe you're referring to the singularity narrative\n\n1:25:13.240 --> 1:25:15.560\n where robots just take over.\n\n1:25:15.560 --> 1:25:18.320\n Well, I don't, I'm not terminating robots,\n\n1:25:18.320 --> 1:25:21.000\n and I don't believe it has to be a singularity.\n\n1:25:21.000 --> 1:25:24.800\n We're just talking to, just like you said,\n\n1:25:24.800 --> 1:25:27.920\n the algorithm controlling masses of populations.\n\n1:25:28.920 --> 1:25:31.120\n The existential threat being,\n\n1:25:32.640 --> 1:25:36.760\n hurt ourselves much like a nuclear war would hurt ourselves.\n\n1:25:36.760 --> 1:25:37.600\n That kind of thing.\n\n1:25:37.600 --> 1:25:39.480\n I don't think that requires a singularity.\n\n1:25:39.480 --> 1:25:42.560\n That requires a loss of control over AI algorithm.\n\n1:25:42.560 --> 1:25:43.560\n Yes.\n\n1:25:43.560 --> 1:25:47.000\n So I do agree there are concerning trends.\n\n1:25:47.000 --> 1:25:52.000\n Honestly, I wouldn't want to make any longterm predictions.\n\n1:25:52.960 --> 1:25:56.000\n I don't think today we really have the capability\n\n1:25:56.000 --> 1:25:58.560\n to see what the dangers of AI\n\n1:25:58.560 --> 1:26:01.360\n are going to be in 50 years, in 100 years.\n\n1:26:01.360 --> 1:26:04.800\n I do see that we are already faced\n\n1:26:04.800 --> 1:26:08.840\n with concrete and present dangers\n\n1:26:08.840 --> 1:26:11.560\n surrounding the negative side effects\n\n1:26:11.560 --> 1:26:14.960\n of content recombination systems, of newsfeed algorithms\n\n1:26:14.960 --> 1:26:17.640\n concerning algorithmic bias as well.\n\n1:26:18.640 --> 1:26:21.200\n So we are delegating more and more\n\n1:26:22.240 --> 1:26:25.080\n decision processes to algorithms.\n\n1:26:25.080 --> 1:26:26.760\n Some of these algorithms are uncrafted,\n\n1:26:26.760 --> 1:26:29.360\n some are learned from data,\n\n1:26:29.360 --> 1:26:31.920\n but we are delegating control.\n\n1:26:32.920 --> 1:26:36.280\n Sometimes it's a good thing, sometimes not so much.\n\n1:26:36.280 --> 1:26:39.480\n And there is in general very little supervision\n\n1:26:39.480 --> 1:26:41.000\n of this process, right?\n\n1:26:41.000 --> 1:26:45.400\n So we are still in this period of very fast change,\n\n1:26:45.400 --> 1:26:50.400\n even chaos, where society is restructuring itself,\n\n1:26:50.920 --> 1:26:53.160\n turning into an information society,\n\n1:26:53.160 --> 1:26:54.520\n which itself is turning into\n\n1:26:54.520 --> 1:26:58.360\n an increasingly automated information passing society.\n\n1:26:58.360 --> 1:27:02.520\n And well, yeah, I think the best we can do today\n\n1:27:02.520 --> 1:27:06.040\n is try to raise awareness around some of these issues.\n\n1:27:06.040 --> 1:27:07.680\n And I think we're actually making good progress.\n\n1:27:07.680 --> 1:27:11.720\n If you look at algorithmic bias, for instance,\n\n1:27:12.760 --> 1:27:14.760\n three years ago, even two years ago,\n\n1:27:14.760 --> 1:27:17.040\n very, very few people were talking about it.\n\n1:27:17.040 --> 1:27:20.320\n And now all the big companies are talking about it.\n\n1:27:20.320 --> 1:27:22.360\n They are often not in a very serious way,\n\n1:27:22.360 --> 1:27:24.560\n but at least it is part of the public discourse.\n\n1:27:24.560 --> 1:27:27.080\n You see people in Congress talking about it.\n\n1:27:27.080 --> 1:27:31.960\n And it all started from raising awareness.\n\n1:27:31.960 --> 1:27:32.800\n Right.\n\n1:27:32.800 --> 1:27:36.080\n So in terms of alignment problem,\n\n1:27:36.080 --> 1:27:39.400\n trying to teach as we allow algorithms,\n\n1:27:39.400 --> 1:27:41.520\n just even recommender systems on Twitter,\n\n1:27:43.640 --> 1:27:47.080\n encoding human values and morals,\n\n1:27:48.280 --> 1:27:50.200\n decisions that touch on ethics,\n\n1:27:50.200 --> 1:27:52.600\n how hard do you think that problem is?\n\n1:27:52.600 --> 1:27:57.240\n How do we have lost functions in neural networks\n\n1:27:57.240 --> 1:27:58.640\n that have some component,\n\n1:27:58.640 --> 1:28:01.080\n some fuzzy components of human morals?\n\n1:28:01.080 --> 1:28:06.080\n Well, I think this is really all about objective function engineering,\n\n1:28:06.080 --> 1:28:10.520\n which is probably going to be increasingly a topic of concern in the future.\n\n1:28:10.520 --> 1:28:14.640\n Like for now, we're just using very naive loss functions\n\n1:28:14.640 --> 1:28:17.760\n because the hard part is not actually what you're trying to minimize.\n\n1:28:17.760 --> 1:28:19.040\n It's everything else.\n\n1:28:19.040 --> 1:28:22.840\n But as the everything else is going to be increasingly automated,\n\n1:28:22.840 --> 1:28:27.040\n we're going to be focusing our human attention\n\n1:28:27.040 --> 1:28:30.240\n on increasingly high level components,\n\n1:28:30.240 --> 1:28:32.680\n like what's actually driving the whole learning system,\n\n1:28:32.680 --> 1:28:33.960\n like the objective function.\n\n1:28:33.960 --> 1:28:36.920\n So loss function engineering is going to be,\n\n1:28:36.920 --> 1:28:40.640\n loss function engineer is probably going to be a job title in the future.\n\n1:28:40.640 --> 1:28:44.520\n And then the tooling you're creating with Keras essentially\n\n1:28:44.520 --> 1:28:47.040\n takes care of all the details underneath.\n\n1:28:47.040 --> 1:28:52.720\n And basically the human expert is needed for exactly that.\n\n1:28:52.720 --> 1:28:53.920\n That's the idea.\n\n1:28:53.920 --> 1:28:57.640\n Keras is the interface between the data you're collecting\n\n1:28:57.640 --> 1:28:59.080\n and the business goals.\n\n1:28:59.080 --> 1:29:03.480\n And your job as an engineer is going to be to express your business goals\n\n1:29:03.480 --> 1:29:06.720\n and your understanding of your business or your product,\n\n1:29:06.720 --> 1:29:11.840\n your system as a kind of loss function or a kind of set of constraints.\n\n1:29:11.840 --> 1:29:19.480\n Does the possibility of creating an AGI system excite you or scare you or bore you?\n\n1:29:19.480 --> 1:29:22.080\n So intelligence can never really be general.\n\n1:29:22.080 --> 1:29:26.400\n You know, at best it can have some degree of generality like human intelligence.\n\n1:29:26.400 --> 1:29:30.640\n It also always has some specialization in the same way that human intelligence\n\n1:29:30.640 --> 1:29:33.440\n is specialized in a certain category of problems,\n\n1:29:33.440 --> 1:29:35.440\n is specialized in the human experience.\n\n1:29:35.440 --> 1:29:37.280\n And when people talk about AGI,\n\n1:29:37.280 --> 1:29:42.520\n I'm never quite sure if they're talking about very, very smart AI,\n\n1:29:42.520 --> 1:29:45.080\n so smart that it's even smarter than humans,\n\n1:29:45.080 --> 1:29:48.000\n or they're talking about human like intelligence,\n\n1:29:48.000 --> 1:29:49.680\n because these are different things.\n\n1:29:49.680 --> 1:29:54.760\n Let's say, presumably I'm oppressing you today with my humanness.\n\n1:29:54.760 --> 1:29:59.240\n So imagine that I was in fact a robot.\n\n1:29:59.240 --> 1:30:01.920\n So what does that mean?\n\n1:30:01.920 --> 1:30:04.920\n That I'm impressing you with natural language processing.\n\n1:30:04.920 --> 1:30:07.840\n Maybe if you weren't able to see me, maybe this is a phone call.\n\n1:30:07.840 --> 1:30:10.000\n So that kind of system.\n\n1:30:10.000 --> 1:30:11.120\n Companion.\n\n1:30:11.120 --> 1:30:15.040\n So that's very much about building human like AI.\n\n1:30:15.040 --> 1:30:18.200\n And you're asking me, you know, is this an exciting perspective?\n\n1:30:18.200 --> 1:30:19.440\n Yes.\n\n1:30:19.440 --> 1:30:21.760\n I think so, yes.\n\n1:30:21.760 --> 1:30:28.000\n Not so much because of what artificial human like intelligence could do,\n\n1:30:28.000 --> 1:30:30.880\n but, you know, from an intellectual perspective,\n\n1:30:30.880 --> 1:30:34.120\n I think if you could build truly human like intelligence,\n\n1:30:34.120 --> 1:30:37.240\n that means you could actually understand human intelligence,\n\n1:30:37.240 --> 1:30:39.880\n which is fascinating, right?\n\n1:30:39.880 --> 1:30:42.680\n Human like intelligence is going to require emotions.\n\n1:30:42.680 --> 1:30:44.400\n It's going to require consciousness,\n\n1:30:44.400 --> 1:30:49.720\n which is not things that would normally be required by an intelligent system.\n\n1:30:49.720 --> 1:30:53.160\n If you look at, you know, we were mentioning earlier like science\n\n1:30:53.160 --> 1:30:59.600\n as a superhuman problem solving agent or system,\n\n1:30:59.600 --> 1:31:02.120\n it does not have consciousness, it doesn't have emotions.\n\n1:31:02.120 --> 1:31:04.320\n In general, so emotions,\n\n1:31:04.320 --> 1:31:07.640\n I see consciousness as being on the same spectrum as emotions.\n\n1:31:07.640 --> 1:31:12.280\n It is a component of the subjective experience\n\n1:31:12.280 --> 1:31:18.800\n that is meant very much to guide behavior generation, right?\n\n1:31:18.800 --> 1:31:20.800\n It's meant to guide your behavior.\n\n1:31:20.800 --> 1:31:24.520\n In general, human intelligence and animal intelligence\n\n1:31:24.520 --> 1:31:29.280\n has evolved for the purpose of behavior generation, right?\n\n1:31:29.280 --> 1:31:30.680\n Including in a social context.\n\n1:31:30.680 --> 1:31:32.480\n So that's why we actually need emotions.\n\n1:31:32.480 --> 1:31:34.920\n That's why we need consciousness.\n\n1:31:34.920 --> 1:31:38.360\n An artificial intelligence system developed in a different context\n\n1:31:38.360 --> 1:31:42.800\n may well never need them, may well never be conscious like science.\n\n1:31:42.800 --> 1:31:47.960\n Well, on that point, I would argue it's possible to imagine\n\n1:31:47.960 --> 1:31:51.480\n that there's echoes of consciousness in science\n\n1:31:51.480 --> 1:31:55.480\n when viewed as an organism, that science is consciousness.\n\n1:31:55.480 --> 1:31:59.160\n So, I mean, how would you go about testing this hypothesis?\n\n1:31:59.160 --> 1:32:07.000\n How do you probe the subjective experience of an abstract system like science?\n\n1:32:07.000 --> 1:32:10.400\n Well, the point of probing any subjective experience is impossible\n\n1:32:10.400 --> 1:32:13.200\n because I'm not science, I'm Lex.\n\n1:32:13.200 --> 1:32:20.520\n So I can't probe another entity, it's no more than bacteria on my skin.\n\n1:32:20.520 --> 1:32:24.160\n You're Lex, I can ask you questions about your subjective experience\n\n1:32:24.160 --> 1:32:28.440\n and you can answer me, and that's how I know you're conscious.\n\n1:32:28.440 --> 1:32:31.840\n Yes, but that's because we speak the same language.\n\n1:32:31.840 --> 1:32:35.520\n You perhaps, we have to speak the language of science in order to ask it.\n\n1:32:35.520 --> 1:32:40.320\n Honestly, I don't think consciousness, just like emotions of pain and pleasure,\n\n1:32:40.320 --> 1:32:44.160\n is not something that inevitably arises\n\n1:32:44.160 --> 1:32:47.920\n from any sort of sufficiently intelligent information processing.\n\n1:32:47.920 --> 1:32:53.920\n It is a feature of the mind, and if you've not implemented it explicitly, it is not there.\n\n1:32:53.920 --> 1:32:58.960\n So you think it's an emergent feature of a particular architecture.\n\n1:32:58.960 --> 1:33:00.320\n So do you think...\n\n1:33:00.320 --> 1:33:02.000\n It's a feature in the same sense.\n\n1:33:02.000 --> 1:33:08.240\n So, again, the subjective experience is all about guiding behavior.\n\n1:33:08.240 --> 1:33:15.120\n If the problems you're trying to solve don't really involve an embodied agent,\n\n1:33:15.120 --> 1:33:19.520\n maybe in a social context, generating behavior and pursuing goals like this.\n\n1:33:19.520 --> 1:33:22.160\n And if you look at science, that's not really what's happening.\n\n1:33:22.160 --> 1:33:27.920\n Even though it is, it is a form of artificial AI, artificial intelligence,\n\n1:33:27.920 --> 1:33:31.920\n in the sense that it is solving problems, it is accumulating knowledge,\n\n1:33:31.920 --> 1:33:35.040\n accumulating solutions and so on.\n\n1:33:35.040 --> 1:33:39.440\n So if you're not explicitly implementing a subjective experience,\n\n1:33:39.440 --> 1:33:44.000\n implementing certain emotions and implementing consciousness,\n\n1:33:44.000 --> 1:33:47.360\n it's not going to just spontaneously emerge.\n\n1:33:47.360 --> 1:33:48.080\n Yeah.\n\n1:33:48.080 --> 1:33:53.200\n But so for a system like, human like intelligence system that has consciousness,\n\n1:33:53.200 --> 1:33:55.840\n do you think it needs to have a body?\n\n1:33:55.840 --> 1:33:56.720\n Yes, definitely.\n\n1:33:56.720 --> 1:33:59.600\n I mean, it doesn't have to be a physical body, right?\n\n1:33:59.600 --> 1:34:03.440\n And there's not that much difference between a realistic simulation in the real world.\n\n1:34:03.440 --> 1:34:06.400\n So there has to be something you have to preserve kind of thing.\n\n1:34:06.400 --> 1:34:11.840\n Yes, but human like intelligence can only arise in a human like context.\n\n1:34:11.840 --> 1:34:16.800\n Intelligence needs other humans in order for you to demonstrate\n\n1:34:16.800 --> 1:34:19.040\n that you have human like intelligence, essentially.\n\n1:34:19.040 --> 1:34:19.540\n Yes.\n\n1:34:20.320 --> 1:34:28.080\n So what kind of tests and demonstration would be sufficient for you\n\n1:34:28.080 --> 1:34:30.960\n to demonstrate human like intelligence?\n\n1:34:30.960 --> 1:34:31.360\n Yeah.\n\n1:34:31.360 --> 1:34:35.600\n Just out of curiosity, you've talked about in terms of theorem proving\n\n1:34:35.600 --> 1:34:38.000\n and program synthesis, I think you've written about\n\n1:34:38.000 --> 1:34:40.480\n that there's no good benchmarks for this.\n\n1:34:40.480 --> 1:34:40.720\n Yeah.\n\n1:34:40.720 --> 1:34:42.000\n That's one of the problems.\n\n1:34:42.000 --> 1:34:46.320\n So let's talk program synthesis.\n\n1:34:46.320 --> 1:34:47.760\n So what do you imagine is a good...\n\n1:34:48.800 --> 1:34:51.360\n I think it's related questions for human like intelligence\n\n1:34:51.360 --> 1:34:52.560\n and for program synthesis.\n\n1:34:53.360 --> 1:34:56.080\n What's a good benchmark for either or both?\n\n1:34:56.080 --> 1:34:56.480\n Right.\n\n1:34:56.480 --> 1:34:59.200\n So I mean, you're actually asking two questions,\n\n1:34:59.200 --> 1:35:02.480\n which is one is about quantifying intelligence\n\n1:35:02.480 --> 1:35:06.880\n and comparing the intelligence of an artificial system\n\n1:35:06.880 --> 1:35:08.480\n to the intelligence for human.\n\n1:35:08.480 --> 1:35:13.440\n And the other is about the degree to which this intelligence is human like.\n\n1:35:13.440 --> 1:35:15.120\n It's actually two different questions.\n\n1:35:16.560 --> 1:35:18.960\n So you mentioned earlier the Turing test.\n\n1:35:19.680 --> 1:35:23.200\n Well, I actually don't like the Turing test because it's very lazy.\n\n1:35:23.200 --> 1:35:28.720\n It's all about completely bypassing the problem of defining and measuring intelligence\n\n1:35:28.720 --> 1:35:34.160\n and instead delegating to a human judge or a panel of human judges.\n\n1:35:34.160 --> 1:35:37.120\n So it's a total copout, right?\n\n1:35:38.160 --> 1:35:43.200\n If you want to measure how human like an agent is,\n\n1:35:43.760 --> 1:35:46.640\n I think you have to make it interact with other humans.\n\n1:35:47.600 --> 1:35:53.760\n Maybe it's not necessarily a good idea to have these other humans be the judges.\n\n1:35:53.760 --> 1:35:59.280\n Maybe you should just observe behavior and compare it to what a human would actually have done.\n\n1:36:00.560 --> 1:36:05.120\n When it comes to measuring how smart, how clever an agent is\n\n1:36:05.120 --> 1:36:11.120\n and comparing that to the degree of human intelligence.\n\n1:36:11.120 --> 1:36:12.960\n So we're already talking about two things, right?\n\n1:36:13.520 --> 1:36:20.320\n The degree, kind of like the magnitude of an intelligence and its direction, right?\n\n1:36:20.320 --> 1:36:23.280\n Like the norm of a vector and its direction.\n\n1:36:23.280 --> 1:36:32.000\n And the direction is like human likeness and the magnitude, the norm is intelligence.\n\n1:36:32.720 --> 1:36:34.080\n You could call it intelligence, right?\n\n1:36:34.080 --> 1:36:41.040\n So the direction, your sense, the space of directions that are human like is very narrow.\n\n1:36:41.040 --> 1:36:41.200\n Yeah.\n\n1:36:42.240 --> 1:36:48.880\n So the way you would measure the magnitude of intelligence in a system\n\n1:36:48.880 --> 1:36:54.640\n in a way that also enables you to compare it to that of a human.\n\n1:36:54.640 --> 1:36:59.200\n Well, if you look at different benchmarks for intelligence today,\n\n1:36:59.200 --> 1:37:04.160\n they're all too focused on skill at a given task.\n\n1:37:04.160 --> 1:37:08.720\n Like skill at playing chess, skill at playing Go, skill at playing Dota.\n\n1:37:10.720 --> 1:37:15.600\n And I think that's not the right way to go about it because you can always\n\n1:37:15.600 --> 1:37:18.240\n beat a human at one specific task.\n\n1:37:19.200 --> 1:37:23.920\n The reason why our skill at playing Go or juggling or anything is impressive\n\n1:37:23.920 --> 1:37:28.400\n is because we are expressing this skill within a certain set of constraints.\n\n1:37:28.400 --> 1:37:32.320\n If you remove the constraints, the constraints that we have one lifetime,\n\n1:37:32.320 --> 1:37:36.080\n that we have this body and so on, if you remove the context,\n\n1:37:36.080 --> 1:37:40.480\n if you have unlimited string data, if you can have access to, you know,\n\n1:37:40.480 --> 1:37:44.640\n for instance, if you look at juggling, if you have no restriction on the hardware,\n\n1:37:44.640 --> 1:37:48.400\n then achieving arbitrary levels of skill is not very interesting\n\n1:37:48.400 --> 1:37:52.400\n and says nothing about the amount of intelligence you've achieved.\n\n1:37:52.400 --> 1:37:57.440\n So if you want to measure intelligence, you need to rigorously define what\n\n1:37:57.440 --> 1:38:02.960\n intelligence is, which in itself, you know, it's a very challenging problem.\n\n1:38:02.960 --> 1:38:04.320\n And do you think that's possible?\n\n1:38:04.320 --> 1:38:06.000\n To define intelligence? Yes, absolutely.\n\n1:38:06.000 --> 1:38:09.760\n I mean, you can provide, many people have provided, you know, some definition.\n\n1:38:10.560 --> 1:38:12.000\n I have my own definition.\n\n1:38:12.000 --> 1:38:13.440\n Where does your definition begin?\n\n1:38:13.440 --> 1:38:16.240\n Where does your definition begin if it doesn't end?\n\n1:38:16.240 --> 1:38:21.680\n Well, I think intelligence is essentially the efficiency\n\n1:38:22.320 --> 1:38:29.760\n with which you turn experience into generalizable programs.\n\n1:38:29.760 --> 1:38:32.800\n So what that means is it's the efficiency with which\n\n1:38:32.800 --> 1:38:36.720\n you turn a sampling of experience space into\n\n1:38:36.720 --> 1:38:46.000\n the ability to process a larger chunk of experience space.\n\n1:38:46.000 --> 1:38:52.560\n So measuring skill can be one proxy across many different tasks,\n\n1:38:52.560 --> 1:38:54.480\n can be one proxy for measuring intelligence.\n\n1:38:54.480 --> 1:38:58.720\n But if you want to only measure skill, you should control for two things.\n\n1:38:58.720 --> 1:39:04.960\n You should control for the amount of experience that your system has\n\n1:39:04.960 --> 1:39:08.080\n and the priors that your system has.\n\n1:39:08.080 --> 1:39:13.120\n But if you look at two agents and you give them the same priors\n\n1:39:13.120 --> 1:39:16.160\n and you give them the same amount of experience,\n\n1:39:16.160 --> 1:39:21.360\n there is one of the agents that is going to learn programs,\n\n1:39:21.360 --> 1:39:25.440\n representations, something, a model that will perform well\n\n1:39:25.440 --> 1:39:28.720\n on the larger chunk of experience space than the other.\n\n1:39:28.720 --> 1:39:30.960\n And that is the smaller agent.\n\n1:39:30.960 --> 1:39:36.960\n Yeah. So if you fix the experience, which generate better programs,\n\n1:39:37.680 --> 1:39:39.520\n better meaning more generalizable.\n\n1:39:39.520 --> 1:39:40.560\n That's really interesting.\n\n1:39:40.560 --> 1:39:42.400\n That's a very nice, clean definition of...\n\n1:39:42.400 --> 1:39:47.280\n Oh, by the way, in this definition, it is already very obvious\n\n1:39:47.280 --> 1:39:49.440\n that intelligence has to be specialized\n\n1:39:49.440 --> 1:39:51.680\n because you're talking about experience space\n\n1:39:51.680 --> 1:39:54.080\n and you're talking about segments of experience space.\n\n1:39:54.080 --> 1:39:57.200\n You're talking about priors and you're talking about experience.\n\n1:39:57.200 --> 1:40:02.480\n All of these things define the context in which intelligence emerges.\n\n1:40:04.480 --> 1:40:08.640\n And you can never look at the totality of experience space, right?\n\n1:40:09.760 --> 1:40:12.160\n So intelligence has to be specialized.\n\n1:40:12.160 --> 1:40:14.960\n But it can be sufficiently large, the experience space,\n\n1:40:14.960 --> 1:40:16.080\n even though it's specialized.\n\n1:40:16.080 --> 1:40:19.120\n There's a certain point when the experience space is large enough\n\n1:40:19.120 --> 1:40:21.440\n to where it might as well be general.\n\n1:40:22.000 --> 1:40:23.920\n It feels general. It looks general.\n\n1:40:23.920 --> 1:40:25.680\n Sure. I mean, it's very relative.\n\n1:40:25.680 --> 1:40:29.360\n Like, for instance, many people would say human intelligence is general.\n\n1:40:29.360 --> 1:40:31.200\n In fact, it is quite specialized.\n\n1:40:32.800 --> 1:40:37.120\n We can definitely build systems that start from the same innate priors\n\n1:40:37.120 --> 1:40:39.120\n as what humans have at birth.\n\n1:40:39.120 --> 1:40:42.320\n Because we already understand fairly well\n\n1:40:42.320 --> 1:40:44.480\n what sort of priors we have as humans.\n\n1:40:44.480 --> 1:40:46.080\n Like many people have worked on this problem.\n\n1:40:46.800 --> 1:40:51.040\n Most notably, Elisabeth Spelke from Harvard.\n\n1:40:51.040 --> 1:40:52.240\n I don't know if you know her.\n\n1:40:52.240 --> 1:40:56.000\n She's worked a lot on what she calls core knowledge.\n\n1:40:56.000 --> 1:41:00.640\n And it is very much about trying to determine and describe\n\n1:41:00.640 --> 1:41:02.320\n what priors we are born with.\n\n1:41:02.320 --> 1:41:04.720\n Like language skills and so on, all that kind of stuff.\n\n1:41:04.720 --> 1:41:05.220\n Exactly.\n\n1:41:06.880 --> 1:41:11.440\n So we have some pretty good understanding of what priors we are born with.\n\n1:41:11.440 --> 1:41:12.560\n So we could...\n\n1:41:13.760 --> 1:41:17.760\n So I've actually been working on a benchmark for the past couple years,\n\n1:41:17.760 --> 1:41:18.640\n you know, on and off.\n\n1:41:18.640 --> 1:41:20.480\n I hope to be able to release it at some point.\n\n1:41:20.480 --> 1:41:21.760\n That's exciting.\n\n1:41:21.760 --> 1:41:25.680\n The idea is to measure the intelligence of systems\n\n1:41:26.800 --> 1:41:28.640\n by countering for priors,\n\n1:41:28.640 --> 1:41:30.480\n countering for amount of experience,\n\n1:41:30.480 --> 1:41:34.800\n and by assuming the same priors as what humans are born with.\n\n1:41:34.800 --> 1:41:39.520\n So that you can actually compare these scores to human intelligence.\n\n1:41:39.520 --> 1:41:43.280\n You can actually have humans pass the same test in a way that's fair.\n\n1:41:43.280 --> 1:41:52.320\n Yeah. And so importantly, such a benchmark should be such that any amount\n\n1:41:52.960 --> 1:41:55.920\n of practicing does not increase your score.\n\n1:41:56.480 --> 1:42:00.560\n So try to picture a game where no matter how much you play this game,\n\n1:42:01.600 --> 1:42:05.040\n that does not change your skill at the game.\n\n1:42:05.040 --> 1:42:05.920\n Can you picture that?\n\n1:42:05.920 --> 1:42:11.040\n As a person who deeply appreciates practice, I cannot actually.\n\n1:42:11.040 --> 1:42:16.560\n There's actually a very simple trick.\n\n1:42:16.560 --> 1:42:19.440\n So in order to come up with a task,\n\n1:42:19.440 --> 1:42:21.760\n so the only thing you can measure is skill at the task.\n\n1:42:21.760 --> 1:42:22.320\n Yes.\n\n1:42:22.320 --> 1:42:24.800\n All tasks are going to involve priors.\n\n1:42:24.800 --> 1:42:25.600\n Yes.\n\n1:42:25.600 --> 1:42:29.920\n The trick is to know what they are and to describe that.\n\n1:42:29.920 --> 1:42:33.760\n And then you make sure that this is the same set of priors as what humans start with.\n\n1:42:33.760 --> 1:42:38.560\n So you create a task that assumes these priors, that exactly documents these priors,\n\n1:42:38.560 --> 1:42:42.240\n so that the priors are made explicit and there are no other priors involved.\n\n1:42:42.240 --> 1:42:48.960\n And then you generate a certain number of samples in experience space for this task, right?\n\n1:42:49.840 --> 1:42:56.320\n And this, for one task, assuming that the task is new for the agent passing it,\n\n1:42:56.320 --> 1:43:04.320\n that's one test of this definition of intelligence that we set up.\n\n1:43:04.320 --> 1:43:06.880\n And now you can scale that to many different tasks,\n\n1:43:06.880 --> 1:43:10.480\n that each task should be new to the agent passing it, right?\n\n1:43:11.360 --> 1:43:14.480\n And also it should be human interpretable and understandable\n\n1:43:14.480 --> 1:43:16.880\n so that you can actually have a human pass the same test.\n\n1:43:16.880 --> 1:43:19.760\n And then you can compare the score of your machine and the score of your human.\n\n1:43:19.760 --> 1:43:20.720\n Which could be a lot of stuff.\n\n1:43:20.720 --> 1:43:23.040\n You could even start a task like MNIST.\n\n1:43:23.040 --> 1:43:28.800\n Just as long as you start with the same set of priors.\n\n1:43:28.800 --> 1:43:34.080\n So the problem with MNIST, humans are already trying to recognize digits, right?\n\n1:43:35.600 --> 1:43:40.960\n But let's say we're considering objects that are not digits,\n\n1:43:42.400 --> 1:43:43.920\n some completely arbitrary patterns.\n\n1:43:44.480 --> 1:43:48.880\n Well, humans already come with visual priors about how to process that.\n\n1:43:48.880 --> 1:43:54.080\n So in order to make the game fair, you would have to isolate these priors\n\n1:43:54.080 --> 1:43:57.280\n and describe them and then express them as computational rules.\n\n1:43:57.280 --> 1:44:01.680\n Having worked a lot with vision science people, that's exceptionally difficult.\n\n1:44:01.680 --> 1:44:03.120\n A lot of progress has been made.\n\n1:44:03.120 --> 1:44:08.080\n There's been a lot of good tests and basically reducing all of human vision into some good priors.\n\n1:44:08.640 --> 1:44:10.960\n We're still probably far away from that perfectly,\n\n1:44:10.960 --> 1:44:14.640\n but as a start for a benchmark, that's an exciting possibility.\n\n1:44:14.640 --> 1:44:24.240\n Yeah, so Elisabeth Spelke actually lists objectness as one of the core knowledge priors.\n\n1:44:24.800 --> 1:44:25.920\n Objectness, cool.\n\n1:44:25.920 --> 1:44:26.880\n Objectness, yeah.\n\n1:44:27.440 --> 1:44:31.520\n So we have priors about objectness, like about the visual space, about time,\n\n1:44:31.520 --> 1:44:34.240\n about agents, about goal oriented behavior.\n\n1:44:35.280 --> 1:44:39.280\n We have many different priors, but what's interesting is that,\n\n1:44:39.280 --> 1:44:43.920\n sure, we have this pretty diverse and rich set of priors,\n\n1:44:43.920 --> 1:44:46.880\n but it's also not that diverse, right?\n\n1:44:46.880 --> 1:44:50.800\n We are not born into this world with a ton of knowledge about the world,\n\n1:44:50.800 --> 1:44:57.840\n with only a small set of core knowledge.\n\n1:44:58.640 --> 1:45:05.040\n Yeah, sorry, do you have a sense of how it feels to us humans that that set is not that large?\n\n1:45:05.040 --> 1:45:09.600\n But just even the nature of time that we kind of integrate pretty effectively\n\n1:45:09.600 --> 1:45:11.600\n through all of our perception, all of our reasoning,\n\n1:45:12.640 --> 1:45:17.680\n maybe how, you know, do you have a sense of how easy it is to encode those priors?\n\n1:45:17.680 --> 1:45:24.560\n Maybe it requires building a universe and then the human brain in order to encode those priors.\n\n1:45:25.440 --> 1:45:28.640\n Or do you have a hope that it can be listed like an axiomatic?\n\n1:45:28.640 --> 1:45:29.280\n I don't think so.\n\n1:45:29.280 --> 1:45:33.040\n So you have to keep in mind that any knowledge about the world that we are\n\n1:45:33.040 --> 1:45:41.120\n born with is something that has to have been encoded into our DNA by evolution at some point.\n\n1:45:41.120 --> 1:45:41.440\n Right.\n\n1:45:41.440 --> 1:45:45.440\n And DNA is a very, very low bandwidth medium.\n\n1:45:46.000 --> 1:45:51.200\n Like it's extremely long and expensive to encode anything into DNA because first of all,\n\n1:45:52.560 --> 1:45:57.440\n you need some sort of evolutionary pressure to guide this writing process.\n\n1:45:57.440 --> 1:46:03.440\n And then, you know, the higher level of information you're trying to write, the longer it's going to take.\n\n1:46:04.480 --> 1:46:13.520\n And the thing in the environment that you're trying to encode knowledge about has to be stable\n\n1:46:13.520 --> 1:46:15.280\n over this duration.\n\n1:46:15.280 --> 1:46:20.960\n So you can only encode into DNA things that constitute an evolutionary advantage.\n\n1:46:20.960 --> 1:46:25.280\n So this is actually a very small subset of all possible knowledge about the world.\n\n1:46:25.280 --> 1:46:32.080\n You can only encode things that are stable, that are true, over very, very long periods of time,\n\n1:46:32.080 --> 1:46:33.680\n typically millions of years.\n\n1:46:33.680 --> 1:46:38.720\n For instance, we might have some visual prior about the shape of snakes, right?\n\n1:46:38.720 --> 1:46:43.920\n But what makes a face, what's the difference between a face and an art face?\n\n1:46:44.560 --> 1:46:48.080\n But consider this interesting question.\n\n1:46:48.080 --> 1:46:56.640\n Do we have any innate sense of the visual difference between a male face and a female face?\n\n1:46:56.640 --> 1:46:57.600\n What do you think?\n\n1:46:58.640 --> 1:46:59.840\n For a human, I mean.\n\n1:46:59.840 --> 1:47:04.000\n I would have to look back into evolutionary history when the genders emerged.\n\n1:47:04.000 --> 1:47:06.240\n But yeah, most...\n\n1:47:06.240 --> 1:47:09.840\n I mean, the faces of humans are quite different from the faces of great apes.\n\n1:47:10.640 --> 1:47:11.600\n Great apes, right?\n\n1:47:12.880 --> 1:47:13.600\n Yeah.\n\n1:47:13.600 --> 1:47:14.800\n That's interesting.\n\n1:47:14.800 --> 1:47:22.800\n Yeah, you couldn't tell the face of a female chimpanzee from the face of a male chimpanzee,\n\n1:47:22.800 --> 1:47:23.440\n probably.\n\n1:47:23.440 --> 1:47:26.160\n Yeah, and I don't think most humans have all that ability.\n\n1:47:26.160 --> 1:47:33.280\n So we do have innate knowledge of what makes a face, but it's actually impossible for us to\n\n1:47:33.280 --> 1:47:40.320\n have any DNA encoded knowledge of the difference between a female human face and a male human face\n\n1:47:40.320 --> 1:47:50.560\n because that knowledge, that information came up into the world actually very recently.\n\n1:47:50.560 --> 1:47:56.400\n If you look at the slowness of the process of encoding knowledge into DNA.\n\n1:47:56.400 --> 1:47:57.360\n Yeah, so that's interesting.\n\n1:47:57.360 --> 1:48:02.080\n That's a really powerful argument that DNA is a low bandwidth and it takes a long time to encode.\n\n1:48:02.800 --> 1:48:05.200\n That naturally creates a very efficient encoding.\n\n1:48:05.200 --> 1:48:12.800\n But one important consequence of this is that, so yes, we are born into this world with a bunch of\n\n1:48:12.800 --> 1:48:17.600\n knowledge, sometimes high level knowledge about the world, like the shape, the rough shape of a\n\n1:48:17.600 --> 1:48:19.520\n snake, of the rough shape of a face.\n\n1:48:20.480 --> 1:48:26.960\n But importantly, because this knowledge takes so long to write, almost all of this innate\n\n1:48:26.960 --> 1:48:32.080\n knowledge is shared with our cousins, with great apes, right?\n\n1:48:32.080 --> 1:48:35.600\n So it is not actually this innate knowledge that makes us special.\n\n1:48:36.320 --> 1:48:42.000\n But to throw it right back at you from the earlier on in our discussion, it's that encoding\n\n1:48:42.960 --> 1:48:48.320\n might also include the entirety of the environment of Earth.\n\n1:48:49.360 --> 1:48:49.920\n To some extent.\n\n1:48:49.920 --> 1:48:56.480\n So it can include things that are important to survival and production, so for which there is\n\n1:48:56.480 --> 1:49:02.880\n some evolutionary pressure, and things that are stable, constant over very, very, very long time\n\n1:49:02.880 --> 1:49:03.380\n periods.\n\n1:49:04.160 --> 1:49:06.320\n And honestly, it's not that much information.\n\n1:49:06.320 --> 1:49:14.400\n There's also, besides the bandwidths constraint and the constraints of the writing process,\n\n1:49:14.400 --> 1:49:21.440\n there's also memory constraints, like DNA, the part of DNA that deals with the human brain,\n\n1:49:21.440 --> 1:49:22.640\n it's actually fairly small.\n\n1:49:22.640 --> 1:49:25.520\n It's like, you know, on the order of megabytes, right?\n\n1:49:25.520 --> 1:49:29.600\n There's not that much high level knowledge about the world you can encode.\n\n1:49:31.600 --> 1:49:38.880\n That's quite brilliant and hopeful for a benchmark that you're referring to of encoding\n\n1:49:38.880 --> 1:49:39.360\n priors.\n\n1:49:39.360 --> 1:49:43.120\n I actually look forward to, I'm skeptical whether you can do it in the next couple of\n\n1:49:43.120 --> 1:49:44.320\n years, but hopefully.\n\n1:49:45.040 --> 1:49:45.760\n I've been working.\n\n1:49:45.760 --> 1:49:49.920\n So honestly, it's a very simple benchmark, and it's not like a big breakthrough or anything.\n\n1:49:49.920 --> 1:49:53.200\n It's more like a fun side project, right?\n\n1:49:53.200 --> 1:49:55.680\n But these fun, so is ImageNet.\n\n1:49:56.480 --> 1:50:04.080\n These fun side projects could launch entire groups of efforts towards creating reasoning\n\n1:50:04.080 --> 1:50:04.960\n systems and so on.\n\n1:50:04.960 --> 1:50:05.440\n And I think...\n\n1:50:05.440 --> 1:50:06.160\n Yeah, that's the goal.\n\n1:50:06.160 --> 1:50:12.080\n It's trying to measure strong generalization, to measure the strength of abstraction in\n\n1:50:12.080 --> 1:50:16.960\n our minds, well, in our minds and in artificial intelligence agencies.\n\n1:50:16.960 --> 1:50:24.800\n And if there's anything true about this science organism is its individual cells love competition.\n\n1:50:24.800 --> 1:50:26.800\n So and benchmarks encourage competition.\n\n1:50:26.800 --> 1:50:29.520\n So that's an exciting possibility.\n\n1:50:29.520 --> 1:50:32.640\n If you, do you think an AI winter is coming?\n\n1:50:33.520 --> 1:50:34.640\n And how do we prevent it?\n\n1:50:35.440 --> 1:50:36.080\n Not really.\n\n1:50:36.080 --> 1:50:42.160\n So an AI winter is something that would occur when there's a big mismatch between how we\n\n1:50:42.160 --> 1:50:47.280\n are selling the capabilities of AI and the actual capabilities of AI.\n\n1:50:47.280 --> 1:50:50.560\n And today, some deep learning is creating a lot of value.\n\n1:50:50.560 --> 1:50:56.240\n And it will keep creating a lot of value in the sense that these models are applicable\n\n1:50:56.240 --> 1:51:00.000\n to a very wide range of problems that are relevant today.\n\n1:51:00.000 --> 1:51:05.120\n And we are only just getting started with applying these algorithms to every problem\n\n1:51:05.120 --> 1:51:06.320\n they could be solving.\n\n1:51:06.320 --> 1:51:10.160\n So deep learning will keep creating a lot of value for the time being.\n\n1:51:10.160 --> 1:51:15.920\n What's concerning, however, is that there's a lot of hype around deep learning and around\n\n1:51:15.920 --> 1:51:16.240\n AI.\n\n1:51:16.240 --> 1:51:22.000\n There are lots of people are overselling the capabilities of these systems, not just\n\n1:51:22.000 --> 1:51:27.760\n the capabilities, but also overselling the fact that they might be more or less, you\n\n1:51:27.760 --> 1:51:36.640\n know, brain like, like given the kind of a mystical aspect, these technologies and also\n\n1:51:36.640 --> 1:51:43.840\n overselling the pace of progress, which, you know, it might look fast in the sense that\n\n1:51:43.840 --> 1:51:46.480\n we have this exponentially increasing number of papers.\n\n1:51:47.760 --> 1:51:52.960\n But again, that's just a simple consequence of the fact that we have ever more people\n\n1:51:52.960 --> 1:51:53.840\n coming into the field.\n\n1:51:54.400 --> 1:51:57.440\n It doesn't mean the progress is actually exponentially fast.\n\n1:51:58.640 --> 1:52:02.720\n Let's say you're trying to raise money for your startup or your research lab.\n\n1:52:02.720 --> 1:52:09.120\n You might want to tell, you know, a grandiose story to investors about how deep learning\n\n1:52:09.120 --> 1:52:14.240\n is just like the brain and how it can solve all these incredible problems like self driving\n\n1:52:14.240 --> 1:52:15.760\n and robotics and so on.\n\n1:52:15.760 --> 1:52:19.440\n And maybe you can tell them that the field is progressing so fast and we are going to\n\n1:52:19.440 --> 1:52:23.040\n have AGI within 15 years or even 10 years.\n\n1:52:23.040 --> 1:52:25.920\n And none of this is true.\n\n1:52:25.920 --> 1:52:32.800\n And every time you're like saying these things and an investor or, you know, a decision maker\n\n1:52:32.800 --> 1:52:41.680\n believes them, well, this is like the equivalent of taking on credit card debt, but for trust,\n\n1:52:41.680 --> 1:52:42.480\n right?\n\n1:52:42.480 --> 1:52:50.160\n And maybe this will, you know, this will be what enables you to raise a lot of money,\n\n1:52:50.160 --> 1:52:54.320\n but ultimately you are creating damage, you are damaging the field.\n\n1:52:54.320 --> 1:53:00.160\n So that's the concern is that that debt, that's what happens with the other AI winters is\n\n1:53:00.160 --> 1:53:04.160\n the concern is you actually tweeted about this with autonomous vehicles, right?\n\n1:53:04.160 --> 1:53:08.960\n There's almost every single company now have promised that they will have full autonomous\n\n1:53:08.960 --> 1:53:11.760\n vehicles by 2021, 2022.\n\n1:53:11.760 --> 1:53:18.080\n That's a good example of the consequences of over hyping the capabilities of AI and\n\n1:53:18.080 --> 1:53:19.280\n the pace of progress.\n\n1:53:19.280 --> 1:53:25.200\n So because I work especially a lot recently in this area, I have a deep concern of what\n\n1:53:25.200 --> 1:53:30.400\n happens when all of these companies after I've invested billions have a meeting and\n\n1:53:30.400 --> 1:53:33.600\n say, how much do we actually, first of all, do we have an autonomous vehicle?\n\n1:53:33.600 --> 1:53:35.280\n The answer will definitely be no.\n\n1:53:35.840 --> 1:53:40.560\n And second will be, wait a minute, we've invested one, two, three, four billion dollars\n\n1:53:40.560 --> 1:53:43.120\n into this and we made no profit.\n\n1:53:43.120 --> 1:53:49.200\n And the reaction to that may be going very hard in other directions that might impact\n\n1:53:49.200 --> 1:53:50.400\n even other industries.\n\n1:53:50.400 --> 1:53:55.520\n And that's what we call an AI winter is when there is backlash where no one believes any\n\n1:53:55.520 --> 1:53:59.360\n of these promises anymore because they've turned that to be big lies the first time\n\n1:53:59.360 --> 1:54:00.240\n around.\n\n1:54:00.240 --> 1:54:06.000\n And this will definitely happen to some extent for autonomous vehicles because the public\n\n1:54:06.000 --> 1:54:13.360\n and decision makers have been convinced that around 2015, they've been convinced by these\n\n1:54:13.360 --> 1:54:19.600\n people who are trying to raise money for their startups and so on, that L5 driving was coming\n\n1:54:19.600 --> 1:54:22.880\n in maybe 2016, maybe 2017, maybe 2018.\n\n1:54:22.880 --> 1:54:26.080\n Now we're in 2019, we're still waiting for it.\n\n1:54:27.600 --> 1:54:32.800\n And so I don't believe we are going to have a full on AI winter because we have these\n\n1:54:32.800 --> 1:54:36.640\n technologies that are producing a tremendous amount of real value.\n\n1:54:37.680 --> 1:54:39.920\n But there is also too much hype.\n\n1:54:39.920 --> 1:54:43.520\n So there will be some backlash, especially there will be backlash.\n\n1:54:44.960 --> 1:54:53.040\n So some startups are trying to sell the dream of AGI and the fact that AGI is going to create\n\n1:54:53.040 --> 1:54:53.760\n infinite value.\n\n1:54:53.760 --> 1:54:55.680\n Like AGI is like a free lunch.\n\n1:54:55.680 --> 1:55:02.800\n Like if you can develop an AI system that passes a certain threshold of IQ or something,\n\n1:55:02.800 --> 1:55:04.400\n then suddenly you have infinite value.\n\n1:55:04.400 --> 1:55:14.160\n And well, there are actually lots of investors buying into this idea and they will wait maybe\n\n1:55:14.160 --> 1:55:17.760\n 10, 15 years and nothing will happen.\n\n1:55:17.760 --> 1:55:22.560\n And the next time around, well, maybe there will be a new generation of investors.\n\n1:55:22.560 --> 1:55:23.360\n No one will care.\n\n1:55:24.800 --> 1:55:27.280\n Human memory is fairly short after all.\n\n1:55:27.280 --> 1:55:34.320\n I don't know about you, but because I've spoken about AGI sometimes poetically, I get a lot\n\n1:55:34.320 --> 1:55:42.000\n of emails from people giving me, they're usually like a large manifestos of they've, they say\n\n1:55:42.000 --> 1:55:47.200\n to me that they have created an AGI system or they know how to do it.\n\n1:55:47.200 --> 1:55:48.880\n And there's a long write up of how to do it.\n\n1:55:48.880 --> 1:55:50.560\n I get a lot of these emails, yeah.\n\n1:55:50.560 --> 1:55:57.760\n They're a little bit feel like it's generated by an AI system actually, but there's usually\n\n1:55:57.760 --> 1:56:06.640\n no diagram, you have a transformer generating crank papers about AGI.\n\n1:56:06.640 --> 1:56:12.160\n So the question is about, because you've been such a good, you have a good radar for crank\n\n1:56:12.160 --> 1:56:16.720\n papers, how do we know they're not onto something?\n\n1:56:16.720 --> 1:56:24.240\n How do I, so when you start to talk about AGI or anything like the reasoning benchmarks\n\n1:56:24.240 --> 1:56:28.160\n and so on, so something that doesn't have a benchmark, it's really difficult to know.\n\n1:56:29.120 --> 1:56:34.560\n I mean, I talked to Jeff Hawkins, who's really looking at neuroscience approaches to how,\n\n1:56:35.200 --> 1:56:41.520\n and there's some, there's echoes of really interesting ideas in at least Jeff's case,\n\n1:56:41.520 --> 1:56:42.320\n which he's showing.\n\n1:56:43.280 --> 1:56:45.040\n How do you usually think about this?\n\n1:56:46.640 --> 1:56:52.880\n Like preventing yourself from being too narrow minded and elitist about deep learning, it\n\n1:56:52.880 --> 1:56:56.720\n has to work on these particular benchmarks, otherwise it's trash.\n\n1:56:56.720 --> 1:57:05.280\n Well, you know, the thing is, intelligence does not exist in the abstract.\n\n1:57:05.280 --> 1:57:07.200\n Intelligence has to be applied.\n\n1:57:07.200 --> 1:57:11.040\n So if you don't have a benchmark, if you have an improvement in some benchmark, maybe it's\n\n1:57:11.040 --> 1:57:12.400\n a new benchmark, right?\n\n1:57:12.400 --> 1:57:16.640\n Maybe it's not something we've been looking at before, but you do need a problem that\n\n1:57:16.640 --> 1:57:17.360\n you're trying to solve.\n\n1:57:17.360 --> 1:57:20.000\n You're not going to come up with a solution without a problem.\n\n1:57:20.000 --> 1:57:25.520\n So you, general intelligence, I mean, you've clearly highlighted generalization.\n\n1:57:26.320 --> 1:57:31.200\n If you want to claim that you have an intelligence system, it should come with a benchmark.\n\n1:57:31.200 --> 1:57:35.760\n It should, yes, it should display capabilities of some kind.\n\n1:57:35.760 --> 1:57:41.840\n It should show that it can create some form of value, even if it's a very artificial form\n\n1:57:41.840 --> 1:57:42.800\n of value.\n\n1:57:42.800 --> 1:57:48.800\n And that's also the reason why you don't actually need to care about telling which papers have\n\n1:57:48.800 --> 1:57:52.000\n actually some hidden potential and which do not.\n\n1:57:53.120 --> 1:57:59.200\n Because if there is a new technique that's actually creating value, this is going to\n\n1:57:59.200 --> 1:58:02.480\n be brought to light very quickly because it's actually making a difference.\n\n1:58:02.480 --> 1:58:08.160\n So it's the difference between something that is ineffectual and something that is actually\n\n1:58:08.160 --> 1:58:08.800\n useful.\n\n1:58:08.800 --> 1:58:14.080\n And ultimately usefulness is our guide, not just in this field, but if you look at science\n\n1:58:14.080 --> 1:58:18.720\n in general, maybe there are many, many people over the years that have had some really interesting\n\n1:58:19.440 --> 1:58:22.800\n theories of everything, but they were just completely useless.\n\n1:58:22.800 --> 1:58:27.280\n And you don't actually need to tell the interesting theories from the useless theories.\n\n1:58:28.000 --> 1:58:34.080\n All you need is to see, is this actually having an effect on something else?\n\n1:58:34.080 --> 1:58:35.360\n Is this actually useful?\n\n1:58:35.360 --> 1:58:36.800\n Is this making an impact or not?\n\n1:58:37.600 --> 1:58:38.640\n That's beautifully put.\n\n1:58:38.640 --> 1:58:43.680\n I mean, the same applies to quantum mechanics, to string theory, to the holographic principle.\n\n1:58:43.680 --> 1:58:45.280\n We are doing deep learning because it works.\n\n1:58:46.960 --> 1:58:52.720\n Before it started working, people considered people working on neural networks as cranks\n\n1:58:52.720 --> 1:58:53.120\n very much.\n\n1:58:54.560 --> 1:58:56.320\n No one was working on this anymore.\n\n1:58:56.320 --> 1:58:59.120\n And now it's working, which is what makes it valuable.\n\n1:58:59.120 --> 1:59:00.320\n It's not about being right.\n\n1:59:01.120 --> 1:59:02.560\n It's about being effective.\n\n1:59:02.560 --> 1:59:08.080\n And nevertheless, the individual entities of this scientific mechanism, just like Yoshua\n\n1:59:08.080 --> 1:59:12.480\n Banjo or Jan Lekun, they, while being called cranks, stuck with it.\n\n1:59:12.480 --> 1:59:12.880\n Right?\n\n1:59:12.880 --> 1:59:13.280\n Yeah.\n\n1:59:13.280 --> 1:59:17.840\n And so us individual agents, even if everyone's laughing at us, just stick with it.\n\n1:59:18.880 --> 1:59:21.840\n If you believe you have something, you should stick with it and see it through.\n\n1:59:23.520 --> 1:59:25.920\n That's a beautiful inspirational message to end on.\n\n1:59:25.920 --> 1:59:27.600\n Francois, thank you so much for talking today.\n\n1:59:27.600 --> 1:59:28.640\n That was amazing.\n\n1:59:28.640 --> 1:59:44.000\n Thank you.\n\n"
}