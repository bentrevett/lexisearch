{
  "title": "David Silver: AlphaGo, AlphaZero, and Deep Reinforcement Learning | Lex Fridman Podcast #86",
  "id": "uPUEq8d73JI",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:02.560\n The following is a conversation with David Silver,\n\n00:02.560 --> 00:05.000\n who leads the Reinforcement Learning Research Group\n\n00:05.000 --> 00:07.840\n at DeepMind, and was the lead researcher\n\n00:07.840 --> 00:12.080\n on AlphaGo, AlphaZero, and co led the AlphaStar\n\n00:12.080 --> 00:14.760\n and MuZero efforts, and a lot of important work\n\n00:14.760 --> 00:17.160\n in reinforcement learning in general.\n\n00:17.160 --> 00:20.840\n I believe AlphaZero is one of the most important\n\n00:20.840 --> 00:24.160\n accomplishments in the history of artificial intelligence.\n\n00:24.160 --> 00:27.760\n And David is one of the key humans who brought AlphaZero\n\n00:27.760 --> 00:30.560\n to life together with a lot of other great researchers\n\n00:30.560 --> 00:31.880\n at DeepMind.\n\n00:31.880 --> 00:35.160\n He's humble, kind, and brilliant.\n\n00:35.160 --> 00:39.040\n We were both jet lagged, but didn't care and made it happen.\n\n00:39.040 --> 00:43.280\n It was a pleasure and truly an honor to talk with David.\n\n00:43.280 --> 00:45.720\n This conversation was recorded before the outbreak\n\n00:45.720 --> 00:46.960\n of the pandemic.\n\n00:46.960 --> 00:49.520\n For everyone feeling the medical, psychological,\n\n00:49.520 --> 00:51.600\n and financial burden of this crisis,\n\n00:51.600 --> 00:53.360\n I'm sending love your way.\n\n00:53.360 --> 00:57.680\n Stay strong, we're in this together, we'll beat this thing.\n\n00:57.680 --> 01:00.360\n This is the Artificial Intelligence Podcast.\n\n01:00.360 --> 01:02.480\n If you enjoy it, subscribe on YouTube,\n\n01:02.480 --> 01:04.760\n review it with five stars on Apple Podcast,\n\n01:04.760 --> 01:07.960\n support on Patreon, or simply connect with me on Twitter\n\n01:07.960 --> 01:12.040\n at Lex Friedman, spelled F R I D M A N.\n\n01:12.040 --> 01:14.520\n As usual, I'll do a few minutes of ads now\n\n01:14.520 --> 01:16.080\n and never any ads in the middle\n\n01:16.080 --> 01:18.360\n that can break the flow of the conversation.\n\n01:18.360 --> 01:19.680\n I hope that works for you\n\n01:19.680 --> 01:22.560\n and doesn't hurt the listening experience.\n\n01:22.560 --> 01:23.920\n Quick summary of the ads.\n\n01:23.920 --> 01:27.360\n Two sponsors, Masterclass and Cash App.\n\n01:27.360 --> 01:29.040\n Please consider supporting the podcast\n\n01:29.040 --> 01:34.000\n by signing up to Masterclass and masterclass.com slash Lex\n\n01:34.000 --> 01:38.760\n and downloading Cash App and using code LexPodcast.\n\n01:38.760 --> 01:41.120\n This show is presented by Cash App,\n\n01:41.120 --> 01:43.480\n the number one finance app in the app store.\n\n01:43.480 --> 01:46.960\n When you get it, use code LexPodcast.\n\n01:46.960 --> 01:50.040\n Cash App lets you send money to friends, buy Bitcoin,\n\n01:50.040 --> 01:53.800\n and invest in the stock market with as little as $1.\n\n01:53.800 --> 01:56.040\n Since Cash App allows you to buy Bitcoin,\n\n01:56.040 --> 01:57.840\n let me mention that cryptocurrency\n\n01:57.840 --> 02:01.400\n in the context of the history of money is fascinating.\n\n02:01.400 --> 02:05.320\n I recommend Ascent of Money as a great book on this history.\n\n02:05.320 --> 02:10.040\n Debits and credits on Ledger started around 30,000 years ago.\n\n02:10.040 --> 02:12.840\n The US dollar created over 200 years ago,\n\n02:12.840 --> 02:15.840\n and Bitcoin, the first decentralized cryptocurrency,\n\n02:15.840 --> 02:18.600\n released just over 10 years ago.\n\n02:18.600 --> 02:21.880\n So given that history, cryptocurrency is still very much\n\n02:21.880 --> 02:23.880\n in its early days of development,\n\n02:23.880 --> 02:26.480\n but it's still aiming to and just might\n\n02:26.480 --> 02:29.040\n redefine the nature of money.\n\n02:29.040 --> 02:32.360\n So again, if you get Cash App from the app store or Google Play\n\n02:32.360 --> 02:35.880\n and use the code LexPodcast, you get $10,\n\n02:35.880 --> 02:38.640\n and Cash App will also donate $10 to FIRST,\n\n02:38.640 --> 02:41.080\n an organization that is helping to advance robotics\n\n02:41.080 --> 02:44.840\n and STEM education for young people around the world.\n\n02:44.840 --> 02:46.960\n This show is sponsored by Masterclass.\n\n02:46.960 --> 02:49.480\n Sign up at masterclass.com slash Lex\n\n02:49.480 --> 02:52.000\n to get a discount and to support this podcast.\n\n02:52.000 --> 02:53.560\n In fact, for a limited time now,\n\n02:53.560 --> 02:56.600\n if you sign up for an all access pass for a year,\n\n02:56.600 --> 02:59.480\n you get to get another all access pass\n\n02:59.480 --> 03:01.200\n to share with a friend.\n\n03:01.200 --> 03:02.600\n Buy one, get one free.\n\n03:02.600 --> 03:04.280\n When I first heard about Masterclass,\n\n03:04.280 --> 03:06.240\n I thought it was too good to be true.\n\n03:06.240 --> 03:09.680\n For $180 a year, you get an all access pass\n\n03:09.680 --> 03:12.920\n to watch courses from to list some of my favorites.\n\n03:12.920 --> 03:15.120\n Chris Hadfield on space exploration,\n\n03:15.120 --> 03:18.080\n Neil deGrasse Tyson on scientific thinking communication,\n\n03:18.080 --> 03:22.760\n Will Wright, the creator of SimCity and Sims on game design,\n\n03:22.760 --> 03:24.640\n Jane Goodall on conservation,\n\n03:24.640 --> 03:26.560\n Carlos Santana on guitar.\n\n03:26.560 --> 03:29.040\n His song Europa could be the most beautiful\n\n03:29.040 --> 03:30.960\n guitar song ever written.\n\n03:30.960 --> 03:34.240\n Gary Kasparov on chess, Daniel Negrano on poker,\n\n03:34.240 --> 03:35.640\n and many, many more.\n\n03:35.640 --> 03:37.840\n Chris Hadfield explaining how rockets work\n\n03:37.840 --> 03:40.400\n and the experience of being launched into space alone\n\n03:40.400 --> 03:41.640\n is worth the money.\n\n03:41.640 --> 03:44.680\n For me, the key is to not be overwhelmed\n\n03:44.680 --> 03:46.200\n by the abundance of choice.\n\n03:46.200 --> 03:48.040\n Pick three courses you want to complete,\n\n03:48.040 --> 03:50.080\n watch each of them all the way through.\n\n03:50.080 --> 03:51.880\n It's not that long, but it's an experience\n\n03:51.880 --> 03:55.240\n that will stick with you for a long time, I promise.\n\n03:55.240 --> 03:56.760\n It's easily worth the money.\n\n03:56.760 --> 03:59.160\n You can watch it on basically any device.\n\n03:59.160 --> 04:02.280\n Once again, sign up on masterclass.com slash Lex\n\n04:02.280 --> 04:04.720\n to get a discount and to support this podcast.\n\n04:05.600 --> 04:08.780\n And now, here's my conversation with David Silver.\n\n04:09.720 --> 04:12.160\n What was the first program you've ever written?\n\n04:12.160 --> 04:13.920\n And what programming language?\n\n04:13.920 --> 04:14.840\n Do you remember?\n\n04:14.840 --> 04:16.120\n I remember very clearly, yeah.\n\n04:16.120 --> 04:22.000\n My parents brought home this BBC Model B microcomputer.\n\n04:22.000 --> 04:24.160\n It was just this fascinating thing to me.\n\n04:24.160 --> 04:27.720\n I was about seven years old and couldn't resist\n\n04:27.720 --> 04:29.960\n just playing around with it.\n\n04:29.960 --> 04:35.400\n So I think first program ever was writing my name out\n\n04:35.400 --> 04:39.560\n in different colors and getting it to loop and repeat that.\n\n04:39.560 --> 04:41.600\n And there was something magical about that,\n\n04:41.600 --> 04:43.320\n which just led to more and more.\n\n04:43.320 --> 04:46.280\n How did you think about computers back then?\n\n04:46.280 --> 04:49.640\n Like the magical aspect of it, that you can write a program\n\n04:49.640 --> 04:52.840\n and there's this thing that you just gave birth to\n\n04:52.840 --> 04:56.240\n that's able to create sort of visual elements\n\n04:56.240 --> 04:57.640\n and live in its own.\n\n04:57.640 --> 04:59.960\n Or did you not think of it in those romantic notions?\n\n04:59.960 --> 05:02.440\n Was it more like, oh, that's cool.\n\n05:02.440 --> 05:05.240\n I can solve some puzzles.\n\n05:05.240 --> 05:06.880\n It was always more than solving puzzles.\n\n05:06.880 --> 05:08.600\n It was something where, you know,\n\n05:08.600 --> 05:13.400\n there was this limitless possibilities.\n\n05:13.400 --> 05:14.720\n Once you have a computer in front of you,\n\n05:14.720 --> 05:16.400\n you can do anything with it.\n\n05:16.400 --> 05:18.000\n I used to play with Lego with the same feeling.\n\n05:18.000 --> 05:20.000\n You can make anything you want out of Lego,\n\n05:20.000 --> 05:21.840\n but even more so with a computer, you know,\n\n05:21.840 --> 05:24.480\n you're not constrained by the amount of kit you've got.\n\n05:24.480 --> 05:26.960\n And so I was fascinated by it and started pulling out\n\n05:26.960 --> 05:29.560\n the user guide and the advanced user guide\n\n05:29.560 --> 05:30.680\n and then learning.\n\n05:30.680 --> 05:34.600\n So I started in basic and then later 6502.\n\n05:34.600 --> 05:38.360\n My father also became interested in this machine\n\n05:38.360 --> 05:40.240\n and gave up his career to go back to school\n\n05:40.240 --> 05:42.960\n and study for a master's degree\n\n05:42.960 --> 05:46.040\n in artificial intelligence, funnily enough,\n\n05:46.040 --> 05:48.560\n at Essex University when I was seven.\n\n05:48.560 --> 05:52.000\n So I was exposed to those things at an early age.\n\n05:52.000 --> 05:54.840\n He showed me how to program in prologue\n\n05:54.840 --> 05:57.600\n and do things like querying your family tree.\n\n05:57.600 --> 05:59.760\n And those are some of my earliest memories\n\n05:59.760 --> 06:04.040\n of trying to figure things out on a computer.\n\n06:04.040 --> 06:07.120\n Those are the early steps in computer science programming,\n\n06:07.120 --> 06:09.320\n but when did you first fall in love\n\n06:09.320 --> 06:12.040\n with artificial intelligence or with the ideas,\n\n06:12.040 --> 06:13.280\n the dreams of AI?\n\n06:14.840 --> 06:19.000\n I think it was really when I went to study at university.\n\n06:19.000 --> 06:20.880\n So I was an undergrad at Cambridge\n\n06:20.880 --> 06:23.800\n and studying computer science.\n\n06:23.800 --> 06:27.560\n And I really started to question,\n\n06:27.560 --> 06:29.480\n you know, what really are the goals?\n\n06:29.480 --> 06:30.320\n What's the goal?\n\n06:30.320 --> 06:32.760\n Where do we want to go with computer science?\n\n06:32.760 --> 06:37.360\n And it seemed to me that the only step\n\n06:37.360 --> 06:40.880\n of major significance to take was to try\n\n06:40.880 --> 06:44.200\n and recreate something akin to human intelligence.\n\n06:44.200 --> 06:47.480\n If we could do that, that would be a major leap forward.\n\n06:47.480 --> 06:50.960\n And that idea, I certainly wasn't the first to have it,\n\n06:50.960 --> 06:53.480\n but it, you know, nestled within me somewhere\n\n06:53.480 --> 06:55.480\n and became like a bug.\n\n06:55.480 --> 06:58.880\n You know, I really wanted to crack that problem.\n\n06:58.880 --> 07:00.760\n So you thought it was, like you had a notion\n\n07:00.760 --> 07:03.000\n that this is something that human beings can do,\n\n07:03.000 --> 07:07.280\n that it is possible to create an intelligent machine.\n\n07:07.280 --> 07:10.480\n Well, I mean, unless you believe in something metaphysical,\n\n07:11.360 --> 07:13.400\n then what are our brains doing?\n\n07:13.400 --> 07:17.240\n Well, at some level they're information processing systems,\n\n07:17.240 --> 07:22.240\n which are able to take whatever information is in there,\n\n07:22.440 --> 07:24.800\n transform it through some form of program\n\n07:24.800 --> 07:26.120\n and produce some kind of output,\n\n07:26.120 --> 07:29.360\n which enables that human being to do all the amazing things\n\n07:29.360 --> 07:31.800\n that they can do in this incredible world.\n\n07:31.800 --> 07:35.480\n So then do you remember the first time\n\n07:35.480 --> 07:37.960\n you've written a program that,\n\n07:37.960 --> 07:40.080\n because you also had an interest in games.\n\n07:40.080 --> 07:41.960\n Do you remember the first time you were in a program\n\n07:41.960 --> 07:43.780\n that beat you in a game?\n\n07:45.680 --> 07:47.360\n That more beat you at anything?\n\n07:47.360 --> 07:52.360\n Sort of achieved super David Silver level performance?\n\n07:54.280 --> 07:56.440\n So I used to work in the games industry.\n\n07:56.440 --> 08:01.280\n So for five years I programmed games for my first job.\n\n08:01.280 --> 08:03.080\n So it was an amazing opportunity\n\n08:03.080 --> 08:05.800\n to get involved in a startup company.\n\n08:05.800 --> 08:10.800\n And so I was involved in building AI at that time.\n\n08:12.080 --> 08:17.080\n And so for sure there was a sense of building handcrafted,\n\n08:18.200 --> 08:20.280\n what people used to call AI in the games industry,\n\n08:20.280 --> 08:23.120\n which I think is not really what we might think of as AI\n\n08:23.120 --> 08:24.000\n in its fullest sense,\n\n08:24.000 --> 08:29.000\n but something which is able to take actions\n\n08:29.280 --> 08:31.440\n and in a way which makes things interesting\n\n08:31.440 --> 08:33.800\n and challenging for the human player.\n\n08:35.000 --> 08:38.360\n And at that time I was able to build\n\n08:38.360 --> 08:39.400\n these handcrafted agents,\n\n08:39.400 --> 08:41.360\n which in certain limited cases could do things\n\n08:41.360 --> 08:45.360\n which were able to do better than me,\n\n08:45.360 --> 08:47.920\n but mostly in these kind of Twitch like scenarios\n\n08:47.920 --> 08:50.000\n where they were able to do things faster\n\n08:50.000 --> 08:51.680\n or because they had some pattern\n\n08:51.680 --> 08:55.400\n which was able to exploit repeatedly.\n\n08:55.400 --> 08:58.520\n I think if we're talking about real AI,\n\n08:58.520 --> 09:00.800\n the first experience for me came after that\n\n09:00.800 --> 09:05.600\n when I realized that this path I was on\n\n09:05.600 --> 09:06.840\n wasn't taking me towards,\n\n09:06.840 --> 09:10.200\n it wasn't dealing with that bug which I still had inside me\n\n09:10.200 --> 09:14.240\n to really understand intelligence and try and solve it.\n\n09:14.240 --> 09:15.760\n That everything people were doing in games\n\n09:15.760 --> 09:19.920\n was short term fixes rather than long term vision.\n\n09:19.920 --> 09:22.760\n And so I went back to study for my PhD,\n\n09:22.760 --> 09:26.320\n which was funny enough trying to apply reinforcement learning\n\n09:26.320 --> 09:27.880\n to the game of Go.\n\n09:27.880 --> 09:31.360\n And I built my first Go program using reinforcement learning,\n\n09:31.360 --> 09:35.000\n a system which would by trial and error play against itself\n\n09:35.000 --> 09:40.000\n and was able to learn which patterns were actually helpful\n\n09:40.000 --> 09:42.240\n to predict whether it was gonna win or lose the game\n\n09:42.240 --> 09:44.520\n and then choose the moves that led\n\n09:44.520 --> 09:45.640\n to the combination of patterns\n\n09:45.640 --> 09:47.760\n that would mean that you're more likely to win.\n\n09:47.760 --> 09:50.360\n And that system, that system beat me.\n\n09:50.360 --> 09:53.400\n And how did that make you feel?\n\n09:53.400 --> 09:54.240\n Made me feel good.\n\n09:54.240 --> 09:57.000\n I mean, was there sort of the, yeah,\n\n09:57.000 --> 09:59.560\n it's a mix of a sort of excitement\n\n09:59.560 --> 10:02.480\n and was there a tinge of sort of like,\n\n10:02.480 --> 10:04.440\n almost like a fearful awe?\n\n10:04.440 --> 10:08.240\n You know, it's like in space, 2001 Space Odyssey\n\n10:08.240 --> 10:12.680\n kind of realizing that you've created something that,\n\n10:12.680 --> 10:17.680\n you know, that's achieved human level intelligence\n\n10:19.160 --> 10:21.160\n in this one particular little task.\n\n10:21.160 --> 10:23.400\n And in that case, I suppose neural networks\n\n10:23.400 --> 10:24.320\n weren't involved.\n\n10:24.320 --> 10:26.840\n There were no neural networks in those days.\n\n10:26.840 --> 10:29.280\n This was pre deep learning revolution.\n\n10:30.560 --> 10:33.000\n But it was a principled self learning system\n\n10:33.000 --> 10:36.120\n based on a lot of the principles which people\n\n10:36.120 --> 10:38.940\n are still using in deep reinforcement learning.\n\n10:40.200 --> 10:41.200\n How did I feel?\n\n10:41.200 --> 10:46.200\n I think I found it immensely satisfying\n\n10:46.600 --> 10:49.600\n that a system which was able to learn\n\n10:49.600 --> 10:51.320\n from first principles for itself\n\n10:51.320 --> 10:52.400\n was able to reach the point\n\n10:52.400 --> 10:54.640\n that it was understanding this domain\n\n10:56.240 --> 11:00.040\n better than I could and able to outwit me.\n\n11:00.040 --> 11:01.560\n I don't think it was a sense of awe.\n\n11:01.560 --> 11:04.560\n It was a sense that satisfaction,\n\n11:04.560 --> 11:08.640\n that something I felt should work had worked.\n\n11:08.640 --> 11:11.840\n So to me, AlphaGo, and I don't know how else to put it,\n\n11:11.840 --> 11:14.560\n but to me, AlphaGo and AlphaGo Zero,\n\n11:14.560 --> 11:18.520\n mastering the game of Go is again, to me,\n\n11:18.520 --> 11:20.400\n the most profound and inspiring moment\n\n11:20.400 --> 11:23.440\n in the history of artificial intelligence.\n\n11:23.440 --> 11:26.560\n So you're one of the key people behind this achievement\n\n11:26.560 --> 11:27.580\n and I'm Russian.\n\n11:27.580 --> 11:31.840\n So I really felt the first sort of seminal achievement\n\n11:31.840 --> 11:34.800\n when Deep Blue beat Garry Kasparov in 1987.\n\n11:34.800 --> 11:39.800\n So as far as I know, the AI community at that point\n\n11:40.680 --> 11:43.960\n largely saw the game of Go as unbeatable in AI\n\n11:43.960 --> 11:46.160\n using the sort of the state of the art\n\n11:46.160 --> 11:48.760\n brute force methods, search methods.\n\n11:48.760 --> 11:51.480\n Even if you consider, at least the way I saw it,\n\n11:51.480 --> 11:55.920\n even if you consider arbitrary exponential scaling\n\n11:55.920 --> 11:59.160\n of compute, Go would still not be solvable,\n\n11:59.160 --> 12:01.380\n hence why it was thought to be impossible.\n\n12:01.380 --> 12:06.380\n So given that the game of Go was impossible to master,\n\n12:07.660 --> 12:09.460\n what was the dream for you?\n\n12:09.460 --> 12:11.420\n You just mentioned your PhD thesis\n\n12:11.420 --> 12:14.020\n of building the system that plays Go.\n\n12:14.020 --> 12:16.060\n What was the dream for you that you could actually\n\n12:16.060 --> 12:20.100\n build a computer program that achieves world class,\n\n12:20.100 --> 12:21.860\n not necessarily beats the world champion,\n\n12:21.860 --> 12:24.900\n but achieves that kind of level of playing Go?\n\n12:24.900 --> 12:27.260\n First of all, thank you, that's very kind words.\n\n12:27.260 --> 12:31.380\n And funnily enough, I just came from a panel\n\n12:31.380 --> 12:34.500\n where I was actually in a conversation\n\n12:34.500 --> 12:36.060\n with Garry Kasparov and Murray Campbell,\n\n12:36.060 --> 12:38.060\n who was the author of Deep Blue.\n\n12:38.980 --> 12:43.260\n And it was their first meeting together since the match.\n\n12:43.260 --> 12:44.500\n So that just occurred yesterday.\n\n12:44.500 --> 12:47.300\n So I'm literally fresh from that experience.\n\n12:47.300 --> 12:50.760\n So these are amazing moments when they happen,\n\n12:50.760 --> 12:52.280\n but where did it all start?\n\n12:52.280 --> 12:55.020\n Well, for me, it started when I became fascinated\n\n12:55.020 --> 12:56.100\n in the game of Go.\n\n12:56.100 --> 12:59.180\n So Go for me, I've grown up playing games.\n\n12:59.180 --> 13:01.820\n I've always had a fascination in board games.\n\n13:01.820 --> 13:04.860\n I played chess as a kid, I played Scrabble as a kid.\n\n13:06.060 --> 13:08.940\n When I was at university, I discovered the game of Go.\n\n13:08.940 --> 13:11.180\n And to me, it just blew all of those other games\n\n13:11.180 --> 13:12.020\n out of the water.\n\n13:12.020 --> 13:15.580\n It was just so deep and profound in its complexity\n\n13:15.580 --> 13:17.700\n with endless levels to it.\n\n13:17.700 --> 13:22.700\n What I discovered was that I could devote\n\n13:22.700 --> 13:25.940\n endless hours to this game.\n\n13:25.940 --> 13:28.180\n And I knew in my heart of hearts\n\n13:28.180 --> 13:30.340\n that no matter how many hours I would devote to it,\n\n13:30.340 --> 13:34.300\n I would never become a grandmaster,\n\n13:34.300 --> 13:35.980\n or there was another path.\n\n13:35.980 --> 13:38.180\n And the other path was to try and understand\n\n13:38.180 --> 13:40.340\n how you could get some other intelligence\n\n13:40.340 --> 13:43.500\n to play this game better than I would be able to.\n\n13:43.500 --> 13:46.780\n And so even in those days, I had this idea that,\n\n13:46.780 --> 13:49.340\n what if, what if it was possible to build a program\n\n13:49.340 --> 13:51.100\n that could crack this?\n\n13:51.100 --> 13:53.260\n And as I started to explore the domain,\n\n13:53.260 --> 13:57.500\n I discovered that this was really the domain\n\n13:57.500 --> 14:01.300\n where people felt deeply that if progress\n\n14:01.300 --> 14:02.140\n could be made in Go,\n\n14:02.140 --> 14:06.340\n it would really mean a giant leap forward for AI.\n\n14:06.340 --> 14:10.980\n It was the challenge where all other approaches had failed.\n\n14:10.980 --> 14:13.460\n This is coming out of the era you mentioned,\n\n14:13.460 --> 14:15.980\n which was in some sense, the golden era\n\n14:15.980 --> 14:19.940\n for the classical methods of AI, like heuristic search.\n\n14:19.940 --> 14:23.340\n In the 90s, they all fell one after another,\n\n14:23.340 --> 14:26.580\n not just chess with deep blue, but checkers,\n\n14:26.580 --> 14:28.900\n backgammon, Othello.\n\n14:28.900 --> 14:33.340\n There were numerous cases where systems\n\n14:33.340 --> 14:35.940\n built on top of heuristic search methods\n\n14:35.940 --> 14:37.980\n with these high performance systems\n\n14:37.980 --> 14:40.380\n had been able to defeat the human world champion\n\n14:40.380 --> 14:41.980\n in each of those domains.\n\n14:41.980 --> 14:43.900\n And yet in that same time period,\n\n14:44.900 --> 14:47.420\n there was a million dollar prize available\n\n14:47.420 --> 14:50.700\n for the game of Go, for the first system\n\n14:50.700 --> 14:52.700\n to be a human professional player.\n\n14:52.700 --> 14:54.700\n And at the end of that time period,\n\n14:54.700 --> 14:57.140\n in year 2000 when the prize expired,\n\n14:57.140 --> 15:00.060\n the strongest Go program in the world\n\n15:00.060 --> 15:02.700\n was defeated by a nine year old child\n\n15:02.700 --> 15:05.820\n when that nine year old child was giving nine free moves\n\n15:05.820 --> 15:07.500\n to the computer at the start of the game\n\n15:07.500 --> 15:08.780\n to try and even things up.\n\n15:09.820 --> 15:13.900\n And computer Go expert beat that same strongest program\n\n15:13.900 --> 15:18.140\n with 29 handicapped stones, 29 free moves.\n\n15:18.140 --> 15:20.420\n So that's what the state of affairs was\n\n15:20.420 --> 15:22.500\n when I became interested in this problem\n\n15:23.380 --> 15:28.380\n in around 2003 when I started working on computer Go.\n\n15:29.500 --> 15:33.180\n There was nothing, there was very, very little\n\n15:33.180 --> 15:36.700\n in the way of progress towards meaningful performance,\n\n15:36.700 --> 15:39.180\n again, anything approaching human level.\n\n15:39.180 --> 15:42.900\n And so people, it wasn't through lack of effort,\n\n15:42.900 --> 15:44.980\n people had tried many, many things.\n\n15:44.980 --> 15:46.700\n And so there was a strong sense\n\n15:46.700 --> 15:49.900\n that something different would be required for Go\n\n15:49.900 --> 15:52.220\n than had been needed for all of these other domains\n\n15:52.220 --> 15:54.220\n where AI had been successful.\n\n15:54.220 --> 15:56.380\n And maybe the single clearest example\n\n15:56.380 --> 15:58.700\n is that Go, unlike those other domains,\n\n15:59.820 --> 16:02.460\n had this kind of intuitive property\n\n16:02.460 --> 16:04.740\n that a Go player would look at a position\n\n16:04.740 --> 16:09.580\n and say, hey, here's this mess of black and white stones.\n\n16:09.580 --> 16:12.740\n But from this mess, oh, I can predict\n\n16:12.740 --> 16:15.860\n that this part of the board has become my territory,\n\n16:15.860 --> 16:17.900\n this part of the board has become your territory,\n\n16:17.900 --> 16:20.260\n and I've got this overall sense that I'm gonna win\n\n16:20.260 --> 16:22.380\n and that this is about the right move to play.\n\n16:22.380 --> 16:24.780\n And that intuitive sense of judgment,\n\n16:24.780 --> 16:28.220\n of being able to evaluate what's going on in a position,\n\n16:28.220 --> 16:31.820\n it was pivotal to humans being able to play this game\n\n16:31.820 --> 16:33.340\n and something that people had no idea\n\n16:33.340 --> 16:35.060\n how to put into computers.\n\n16:35.060 --> 16:37.780\n So this question of how to evaluate a position,\n\n16:37.780 --> 16:40.140\n how to come up with these intuitive judgments\n\n16:40.140 --> 16:43.700\n was the key reason why Go was so hard\n\n16:44.980 --> 16:47.900\n in addition to its enormous search space,\n\n16:47.900 --> 16:49.740\n and the reason why methods\n\n16:49.740 --> 16:53.220\n which had succeeded so well elsewhere failed in Go.\n\n16:53.220 --> 16:57.980\n And so people really felt deep down that in order to crack Go\n\n16:57.980 --> 17:00.420\n we would need to get something akin to human intuition.\n\n17:00.420 --> 17:02.700\n And if we got something akin to human intuition,\n\n17:02.700 --> 17:06.860\n we'd be able to solve many, many more problems in AI.\n\n17:06.860 --> 17:09.260\n So for me, that was the moment where it's like,\n\n17:09.260 --> 17:11.980\n okay, this is not just about playing the game of Go,\n\n17:11.980 --> 17:13.620\n this is about something profound.\n\n17:13.620 --> 17:15.020\n And it was back to that bug\n\n17:15.020 --> 17:17.740\n which had been itching me all those years.\n\n17:17.740 --> 17:19.660\n This is the opportunity to do something meaningful\n\n17:19.660 --> 17:23.780\n and transformative, and I guess a dream was born.\n\n17:23.780 --> 17:25.340\n That's a really interesting way to put it.\n\n17:25.340 --> 17:29.140\n So almost this realization that you need to find,\n\n17:29.140 --> 17:31.540\n formulate Go as a kind of a prediction problem\n\n17:31.540 --> 17:34.820\n versus a search problem was the intuition.\n\n17:34.820 --> 17:37.380\n I mean, maybe that's the wrong crude term,\n\n17:37.380 --> 17:42.380\n but to give it the ability to kind of intuit things\n\n17:44.020 --> 17:47.060\n about positional structure of the board.\n\n17:47.060 --> 17:51.340\n Now, okay, but what about the learning part of it?\n\n17:51.340 --> 17:54.940\n Did you have a sense that you have to,\n\n17:54.940 --> 17:57.580\n that learning has to be part of the system?\n\n17:57.580 --> 18:01.060\n Again, something that hasn't really as far as I think,\n\n18:01.060 --> 18:05.220\n except with TD Gammon in the 90s with RL a little bit,\n\n18:05.220 --> 18:07.500\n hasn't been part of those state of the art game playing\n\n18:07.500 --> 18:08.580\n systems.\n\n18:08.580 --> 18:12.820\n So I strongly felt that learning would be necessary.\n\n18:12.820 --> 18:16.020\n And that's why my PhD topic back then was trying\n\n18:16.020 --> 18:19.060\n to apply reinforcement learning to the game of Go\n\n18:20.100 --> 18:21.820\n and not just learning of any type,\n\n18:21.820 --> 18:26.180\n but I felt that the only way to really have a system\n\n18:26.180 --> 18:29.220\n to progress beyond human levels of performance\n\n18:29.220 --> 18:31.060\n wouldn't just be to mimic how humans do it,\n\n18:31.060 --> 18:33.140\n but to understand for themselves.\n\n18:33.140 --> 18:36.580\n And how else can a machine hope to understand\n\n18:36.580 --> 18:39.020\n what's going on except through learning?\n\n18:39.020 --> 18:40.420\n If you're not learning, what else are you doing?\n\n18:40.420 --> 18:42.540\n Well, you're putting all the knowledge into the system.\n\n18:42.540 --> 18:47.540\n And that just feels like something which decades of AI\n\n18:47.860 --> 18:50.580\n have told us is maybe not a dead end,\n\n18:50.580 --> 18:53.380\n but certainly has a ceiling to the capabilities.\n\n18:53.380 --> 18:55.420\n It's known as the knowledge acquisition bottleneck,\n\n18:55.420 --> 18:58.500\n that the more you try to put into something,\n\n18:58.500 --> 19:00.380\n the more brittle the system becomes.\n\n19:00.380 --> 19:02.780\n And so you just have to have learning.\n\n19:02.780 --> 19:03.620\n You have to have learning.\n\n19:03.620 --> 19:06.900\n That's the only way you're going to be able to get a system\n\n19:06.900 --> 19:10.380\n which has sufficient knowledge in it,\n\n19:10.380 --> 19:11.900\n millions and millions of pieces of knowledge,\n\n19:11.900 --> 19:14.220\n billions, trillions of a form\n\n19:14.220 --> 19:15.580\n that it can actually apply for itself\n\n19:15.580 --> 19:18.000\n and understand how those billions and trillions\n\n19:18.000 --> 19:20.940\n of pieces of knowledge can be leveraged in a way\n\n19:20.940 --> 19:22.780\n which will actually lead it towards its goal\n\n19:22.780 --> 19:26.420\n without conflict or other issues.\n\n19:27.500 --> 19:30.620\n Yeah, I mean, if I put myself back in that time,\n\n19:30.620 --> 19:33.180\n I just wouldn't think like that.\n\n19:33.180 --> 19:34.860\n Without a good demonstration of RL,\n\n19:34.860 --> 19:37.740\n I would think more in the symbolic AI,\n\n19:37.740 --> 19:42.740\n like not learning, but sort of a simulation\n\n19:42.780 --> 19:46.940\n of knowledge base, like a growing knowledge base,\n\n19:46.940 --> 19:50.060\n but it would still be sort of pattern based,\n\n19:50.060 --> 19:52.800\n like basically have little rules\n\n19:52.800 --> 19:54.660\n that you kind of assemble together\n\n19:54.660 --> 19:56.660\n into a large knowledge base.\n\n19:56.660 --> 19:59.820\n Well, in a sense, that was the state of the art back then.\n\n19:59.820 --> 20:01.140\n So if you look at the Go programs,\n\n20:01.140 --> 20:04.440\n which had been competing for this prize I mentioned,\n\n20:05.320 --> 20:09.860\n they were an assembly of different specialized systems,\n\n20:09.860 --> 20:11.900\n some of which used huge amounts of human knowledge\n\n20:11.900 --> 20:14.860\n to describe how you should play the opening,\n\n20:14.860 --> 20:16.740\n how you should, all the different patterns\n\n20:16.740 --> 20:19.880\n that were required to play well in the game of Go,\n\n20:21.460 --> 20:24.620\n end game theory, combinatorial game theory,\n\n20:24.620 --> 20:28.620\n and combined with more principled search based methods,\n\n20:28.620 --> 20:31.280\n which were trying to solve for particular sub parts\n\n20:31.280 --> 20:34.100\n of the game, like life and death,\n\n20:34.100 --> 20:36.840\n connecting groups together,\n\n20:36.840 --> 20:38.100\n all these amazing sub problems\n\n20:38.100 --> 20:40.420\n that just emerge in the game of Go,\n\n20:40.420 --> 20:43.280\n there were different pieces all put together\n\n20:43.280 --> 20:45.240\n into this like collage,\n\n20:45.240 --> 20:48.180\n which together would try and play against a human.\n\n20:49.120 --> 20:54.120\n And although not all of the pieces were handcrafted,\n\n20:54.620 --> 20:56.780\n the overall effect was nevertheless still brittle,\n\n20:56.780 --> 21:00.220\n and it was hard to make all these pieces work well together.\n\n21:00.220 --> 21:02.660\n And so really, what I was pressing for\n\n21:02.660 --> 21:05.600\n and the main innovation of the approach I took\n\n21:05.600 --> 21:08.440\n was to go back to first principles and say,\n\n21:08.440 --> 21:10.380\n well, let's back off that\n\n21:10.380 --> 21:12.860\n and try and find a principled approach\n\n21:12.860 --> 21:14.880\n where the system can learn for itself,\n\n21:16.900 --> 21:19.300\n just from the outcome, like learn for itself.\n\n21:19.300 --> 21:22.660\n If you try something, did that help or did it not help?\n\n21:22.660 --> 21:26.380\n And only through that procedure can you arrive at knowledge,\n\n21:26.380 --> 21:27.940\n which is verified.\n\n21:27.940 --> 21:29.760\n The system has to verify it for itself,\n\n21:29.760 --> 21:31.620\n not relying on any other third party\n\n21:31.620 --> 21:33.540\n to say this is right or this is wrong.\n\n21:33.540 --> 21:38.180\n And so that principle was already very important\n\n21:38.180 --> 21:39.820\n in those days, but unfortunately,\n\n21:39.820 --> 21:43.260\n we were missing some important pieces back then.\n\n21:43.260 --> 21:46.580\n So before we dive into maybe\n\n21:46.580 --> 21:49.140\n discussing the beauty of reinforcement learning,\n\n21:49.140 --> 21:52.660\n let's take a step back, we kind of skipped it a bit,\n\n21:52.660 --> 21:55.940\n but the rules of the game of Go,\n\n21:55.940 --> 22:00.940\n what the elements of it perhaps contrasting to chess\n\n22:02.100 --> 22:07.100\n that sort of you really enjoyed as a human being,\n\n22:07.100 --> 22:09.620\n and also that make it really difficult\n\n22:09.620 --> 22:13.100\n as a AI machine learning problem.\n\n22:13.100 --> 22:16.740\n So the game of Go has remarkably simple rules.\n\n22:16.740 --> 22:19.180\n In fact, so simple that people have speculated\n\n22:19.180 --> 22:22.220\n that if we were to meet alien life at some point,\n\n22:22.220 --> 22:23.820\n that we wouldn't be able to communicate with them,\n\n22:23.820 --> 22:26.140\n but we would be able to play Go with them.\n\n22:26.140 --> 22:28.980\n Probably have discovered the same rule set.\n\n22:28.980 --> 22:32.260\n So the game is played on a 19 by 19 grid,\n\n22:32.260 --> 22:34.140\n and you play on the intersections of the grid\n\n22:34.140 --> 22:35.580\n and the players take turns.\n\n22:35.580 --> 22:37.580\n And the aim of the game is very simple.\n\n22:37.580 --> 22:40.820\n It's to surround as much territory as you can,\n\n22:40.820 --> 22:43.600\n as many of these intersections with your stones\n\n22:43.600 --> 22:46.180\n and to surround more than your opponent does.\n\n22:46.180 --> 22:48.800\n And the only nuance to the game is that\n\n22:48.800 --> 22:50.500\n if you fully surround your opponent's piece,\n\n22:50.500 --> 22:52.420\n then you get to capture it and remove it from the board\n\n22:52.420 --> 22:54.460\n and it counts as your own territory.\n\n22:54.460 --> 22:58.320\n Now from those very simple rules, immense complexity arises.\n\n22:58.320 --> 22:59.820\n There's kind of profound strategies\n\n22:59.820 --> 23:02.020\n in how to surround territory,\n\n23:02.020 --> 23:04.680\n how to kind of trade off between\n\n23:04.680 --> 23:07.140\n making solid territory yourself now\n\n23:07.140 --> 23:09.260\n compared to building up influence\n\n23:09.260 --> 23:11.300\n that will help you acquire territory later in the game,\n\n23:11.300 --> 23:12.580\n how to connect groups together,\n\n23:12.580 --> 23:14.420\n how to keep your own groups alive,\n\n23:16.620 --> 23:19.940\n which patterns of stones are most useful\n\n23:19.940 --> 23:21.500\n compared to others.\n\n23:21.500 --> 23:23.920\n There's just immense knowledge.\n\n23:23.920 --> 23:27.180\n And human Go players have played this game for,\n\n23:27.180 --> 23:29.260\n it was discovered thousands of years ago,\n\n23:29.260 --> 23:30.860\n and human Go players have built up\n\n23:30.860 --> 23:33.760\n this immense knowledge base over the years.\n\n23:33.760 --> 23:36.300\n It's studied very deeply and played by\n\n23:36.300 --> 23:38.780\n something like 50 million players across the world,\n\n23:38.780 --> 23:41.220\n mostly in China, Japan, and Korea,\n\n23:41.220 --> 23:43.700\n where it's an important part of the culture,\n\n23:43.700 --> 23:45.900\n so much so that it's considered one of the\n\n23:45.900 --> 23:49.860\n four ancient arts that was required by Chinese scholars.\n\n23:49.860 --> 23:51.680\n So there's a deep history there.\n\n23:51.680 --> 23:53.100\n But there's interesting qualities.\n\n23:53.100 --> 23:55.620\n So if I sort of compare to chess,\n\n23:55.620 --> 23:59.380\n chess is in the same way as it is in Chinese culture for Go,\n\n23:59.380 --> 24:01.860\n and chess in Russia is also considered\n\n24:01.860 --> 24:03.980\n one of the sacred arts.\n\n24:03.980 --> 24:06.460\n So if we contrast sort of Go with chess,\n\n24:06.460 --> 24:08.440\n there's interesting qualities about Go.\n\n24:09.300 --> 24:10.840\n Maybe you can correct me if I'm wrong,\n\n24:10.840 --> 24:15.700\n but the evaluation of a particular static board\n\n24:15.700 --> 24:18.780\n is not as reliable.\n\n24:18.780 --> 24:21.820\n Like you can't, in chess you can kind of assign points\n\n24:21.820 --> 24:23.860\n to the different units,\n\n24:23.860 --> 24:26.620\n and it's kind of a pretty good measure\n\n24:26.620 --> 24:27.980\n of who's winning, who's losing.\n\n24:27.980 --> 24:29.800\n It's not so clear.\n\n24:29.800 --> 24:31.300\n Yeah, so in the game of Go,\n\n24:31.300 --> 24:33.420\n you find yourself in a situation where\n\n24:33.420 --> 24:36.020\n both players have played the same number of stones.\n\n24:36.020 --> 24:38.380\n Actually, captures at a strong level of play\n\n24:38.380 --> 24:40.260\n happen very rarely, which means that\n\n24:40.260 --> 24:41.180\n at any moment in the game,\n\n24:41.180 --> 24:43.700\n you've got the same number of white stones and black stones.\n\n24:43.700 --> 24:45.180\n And the only thing which differentiates\n\n24:45.180 --> 24:48.180\n how well you're doing is this intuitive sense\n\n24:48.180 --> 24:50.740\n of where are the territories ultimately\n\n24:50.740 --> 24:52.180\n going to form on this board?\n\n24:52.180 --> 24:55.660\n And if you look at the complexity of a real Go position,\n\n24:57.260 --> 25:00.560\n it's mind boggling that kind of question\n\n25:00.560 --> 25:02.660\n of what will happen in 300 moves from now\n\n25:02.660 --> 25:05.420\n when you see just a scattering of 20 white\n\n25:05.420 --> 25:06.920\n and black stones intermingled.\n\n25:07.860 --> 25:12.780\n And so that challenge is the reason\n\n25:12.780 --> 25:15.540\n why position evaluation is so hard in Go\n\n25:15.540 --> 25:17.420\n compared to other games.\n\n25:17.420 --> 25:19.300\n In addition to that, it has an enormous search space.\n\n25:19.300 --> 25:23.380\n So there's around 10 to the 170 positions\n\n25:23.380 --> 25:24.380\n in the game of Go.\n\n25:24.380 --> 25:26.220\n That's an astronomical number.\n\n25:26.220 --> 25:28.540\n And that search space is so great\n\n25:28.540 --> 25:30.500\n that traditional heuristic search methods\n\n25:30.500 --> 25:32.500\n that were so successful in things like Deep Blue\n\n25:32.500 --> 25:36.060\n and chess programs just kind of fall over in Go.\n\n25:36.060 --> 25:39.440\n So at which point did reinforcement learning\n\n25:39.440 --> 25:43.980\n enter your life, your research life, your way of thinking?\n\n25:43.980 --> 25:45.460\n We just talked about learning,\n\n25:45.460 --> 25:47.780\n but reinforcement learning is a very particular\n\n25:47.780 --> 25:49.660\n kind of learning.\n\n25:49.660 --> 25:53.060\n One that's both philosophically sort of profound,\n\n25:53.060 --> 25:55.860\n but also one that's pretty difficult to get to work\n\n25:55.860 --> 25:58.500\n as if we look back in the early days.\n\n25:58.500 --> 26:00.300\n So when did that enter your life\n\n26:00.300 --> 26:02.300\n and how did that work progress?\n\n26:02.300 --> 26:06.300\n So I had just finished working in the games industry\n\n26:06.300 --> 26:07.660\n at this startup company.\n\n26:07.660 --> 26:12.660\n And I took a year out to discover for myself\n\n26:13.080 --> 26:14.780\n exactly which path I wanted to take.\n\n26:14.780 --> 26:17.140\n I knew I wanted to study intelligence,\n\n26:17.140 --> 26:19.220\n but I wasn't sure what that meant at that stage.\n\n26:19.220 --> 26:21.420\n I really didn't feel I had the tools\n\n26:21.420 --> 26:23.980\n to decide on exactly which path I wanted to follow.\n\n26:24.860 --> 26:27.180\n So during that year, I read a lot.\n\n26:27.180 --> 26:31.460\n And one of the things I read was Saturn and Barto,\n\n26:31.460 --> 26:33.340\n the sort of seminal textbook\n\n26:33.340 --> 26:35.900\n on an introduction to reinforcement learning.\n\n26:35.900 --> 26:39.100\n And when I read that textbook,\n\n26:39.100 --> 26:43.500\n I just had this resonating feeling\n\n26:43.500 --> 26:46.900\n that this is what I understood intelligence to be.\n\n26:47.820 --> 26:51.420\n And this was the path that I felt would be necessary\n\n26:51.420 --> 26:54.900\n to go down to make progress in AI.\n\n26:55.780 --> 27:00.300\n So I got in touch with Rich Saturn\n\n27:00.300 --> 27:02.740\n and asked him if he would be interested\n\n27:02.740 --> 27:07.740\n in supervising me on a PhD thesis in computer go.\n\n27:07.780 --> 27:11.940\n And he basically said\n\n27:11.940 --> 27:15.740\n that if he's still alive, he'd be happy to.\n\n27:15.740 --> 27:19.460\n But unfortunately, he'd been struggling\n\n27:19.460 --> 27:21.780\n with very serious cancer for some years.\n\n27:21.780 --> 27:23.980\n And he really wasn't confident at that stage\n\n27:23.980 --> 27:26.340\n that he'd even be around to see the end event.\n\n27:26.340 --> 27:28.660\n But fortunately, that part of the story\n\n27:28.660 --> 27:29.860\n worked out very happily.\n\n27:29.860 --> 27:32.780\n And I found myself out there in Alberta.\n\n27:32.780 --> 27:34.820\n They've got a great games group out there\n\n27:34.820 --> 27:38.700\n with a history of fantastic work in board games as well,\n\n27:38.700 --> 27:40.860\n as Rich Saturn, the father of RL.\n\n27:40.860 --> 27:43.580\n So it was the natural place for me to go in some sense\n\n27:43.580 --> 27:45.900\n to study this question.\n\n27:45.900 --> 27:48.420\n And the more I looked into it,\n\n27:48.420 --> 27:53.420\n the more strongly I felt that this\n\n27:53.500 --> 27:56.260\n wasn't just the path to progress in computer go.\n\n27:56.260 --> 27:59.340\n But really, this was the thing I'd been looking for.\n\n27:59.340 --> 28:04.340\n This was really an opportunity\n\n28:04.900 --> 28:08.420\n to frame what intelligence means.\n\n28:08.420 --> 28:12.260\n Like what are the goals of AI in a clear,\n\n28:12.260 --> 28:14.220\n single clear problem definition,\n\n28:14.220 --> 28:15.620\n such that if we're able to solve\n\n28:15.620 --> 28:17.500\n that clear single problem definition,\n\n28:18.780 --> 28:21.180\n in some sense, we've cracked the problem of AI.\n\n28:21.180 --> 28:24.860\n So to you, reinforcement learning ideas,\n\n28:24.860 --> 28:26.220\n at least sort of echoes of it,\n\n28:26.220 --> 28:29.420\n would be at the core of intelligence.\n\n28:29.420 --> 28:31.340\n It is at the core of intelligence.\n\n28:31.340 --> 28:34.900\n And if we ever create a human level intelligence system,\n\n28:34.900 --> 28:37.460\n it would be at the core of that kind of system.\n\n28:37.460 --> 28:39.580\n Let me say it this way, that I think it's helpful\n\n28:39.580 --> 28:42.340\n to separate out the problem from the solution.\n\n28:42.340 --> 28:45.980\n So I see the problem of intelligence,\n\n28:45.980 --> 28:48.460\n I would say it can be formalized\n\n28:48.460 --> 28:50.700\n as the reinforcement learning problem,\n\n28:50.700 --> 28:52.820\n and that that formalization is enough\n\n28:52.820 --> 28:56.180\n to capture most, if not all of the things\n\n28:56.180 --> 28:58.460\n that we mean by intelligence,\n\n28:58.460 --> 29:01.060\n that they can all be brought within this framework\n\n29:01.060 --> 29:03.500\n and gives us a way to access them in a meaningful way\n\n29:03.500 --> 29:08.500\n that allows us as scientists to understand intelligence\n\n29:08.620 --> 29:11.700\n and us as computer scientists to build them.\n\n29:12.820 --> 29:16.260\n And so in that sense, I feel that it gives us a path,\n\n29:16.260 --> 29:20.300\n maybe not the only path, but a path towards AI.\n\n29:20.300 --> 29:24.940\n And so do I think that any system in the future\n\n29:24.940 --> 29:29.700\n that's solved AI would have to have RL within it?\n\n29:29.700 --> 29:30.700\n Well, I think if you ask that,\n\n29:30.700 --> 29:33.420\n you're asking about the solution methods.\n\n29:33.420 --> 29:35.500\n I would say that if we have such a thing,\n\n29:35.500 --> 29:37.860\n it would be a solution to the RL problem.\n\n29:37.860 --> 29:41.180\n Now, what particular methods have been used to get there?\n\n29:41.180 --> 29:42.300\n Well, we should keep an open mind\n\n29:42.300 --> 29:45.660\n about the best approaches to actually solve any problem.\n\n29:45.660 --> 29:49.420\n And the things we have right now for reinforcement learning,\n\n29:49.420 --> 29:53.500\n maybe I believe they've got a lot of legs,\n\n29:53.500 --> 29:54.860\n but maybe we're missing some things.\n\n29:54.860 --> 29:56.460\n Maybe there's gonna be better ideas.\n\n29:56.460 --> 29:59.060\n I think we should keep, let's remain modest\n\n29:59.060 --> 30:02.380\n and we're at the early days of this field\n\n30:02.380 --> 30:04.980\n and there are many amazing discoveries ahead of us.\n\n30:04.980 --> 30:06.300\n For sure, the specifics,\n\n30:06.300 --> 30:09.580\n especially of the different kinds of RL approaches currently,\n\n30:09.580 --> 30:11.260\n there could be other things that fall\n\n30:11.260 --> 30:13.420\n into the very large umbrella of RL.\n\n30:13.420 --> 30:16.700\n But if it's okay, can we take a step back\n\n30:16.700 --> 30:18.940\n and kind of ask the basic question\n\n30:18.940 --> 30:22.540\n of what is to you reinforcement learning?\n\n30:22.540 --> 30:25.500\n So reinforcement learning is the study\n\n30:25.500 --> 30:30.500\n and the science and the problem of intelligence\n\n30:31.340 --> 30:35.460\n in the form of an agent that interacts with an environment.\n\n30:35.460 --> 30:36.660\n So the problem you're trying to solve\n\n30:36.660 --> 30:38.100\n is represented by some environment,\n\n30:38.100 --> 30:40.700\n like the world in which that agent is situated.\n\n30:40.700 --> 30:42.500\n And the goal of RL is clear\n\n30:42.500 --> 30:44.700\n that the agent gets to take actions.\n\n30:45.580 --> 30:47.580\n Those actions have some effect on the environment\n\n30:47.580 --> 30:49.180\n and the environment gives back an observation\n\n30:49.180 --> 30:51.860\n to the agent saying, this is what you see or sense.\n\n30:52.820 --> 30:54.780\n And one special thing which it gives back\n\n30:54.780 --> 30:56.300\n is called the reward signal,\n\n30:56.300 --> 30:58.100\n how well it's doing in the environment.\n\n30:58.100 --> 30:59.900\n And the reinforcement learning problem\n\n30:59.900 --> 31:04.380\n is to simply take actions over time\n\n31:04.380 --> 31:06.220\n so as to maximize that reward signal.\n\n31:07.260 --> 31:10.100\n So a couple of basic questions.\n\n31:11.060 --> 31:13.860\n What types of RL approaches are there?\n\n31:13.860 --> 31:17.820\n So I don't know if there's a nice brief inwards way\n\n31:17.820 --> 31:21.500\n to paint the picture of sort of value based,\n\n31:21.500 --> 31:25.820\n model based, policy based reinforcement learning.\n\n31:25.820 --> 31:27.860\n Yeah, so now if we think about,\n\n31:27.860 --> 31:31.940\n okay, so there's this ambitious problem definition of RL.\n\n31:31.940 --> 31:33.380\n It's really, it's truly ambitious.\n\n31:33.380 --> 31:34.860\n It's trying to capture and encircle\n\n31:34.860 --> 31:36.980\n all of the things in which an agent interacts\n\n31:36.980 --> 31:38.460\n with an environment and say, well,\n\n31:38.460 --> 31:39.820\n how can we formalize and understand\n\n31:39.820 --> 31:41.980\n what it means to crack that?\n\n31:41.980 --> 31:43.820\n Now let's think about the solution method.\n\n31:43.820 --> 31:46.460\n Well, how do you solve a really hard problem like that?\n\n31:46.460 --> 31:48.060\n Well, one approach you can take\n\n31:48.060 --> 31:51.700\n is to decompose that very hard problem\n\n31:51.700 --> 31:55.380\n into pieces that work together to solve that hard problem.\n\n31:55.380 --> 31:58.020\n And so you can kind of look at the decomposition\n\n31:58.020 --> 32:00.660\n that's inside the agent's head, if you like,\n\n32:00.660 --> 32:03.740\n and ask, well, what form does that decomposition take?\n\n32:03.740 --> 32:06.140\n And some of the most common pieces that people use\n\n32:06.140 --> 32:07.300\n when they're kind of putting\n\n32:07.300 --> 32:09.540\n the solution method together,\n\n32:09.540 --> 32:11.660\n some of the most common pieces that people use\n\n32:11.660 --> 32:14.820\n are whether or not that solution has a value function.\n\n32:14.820 --> 32:16.740\n That means, is it trying to predict,\n\n32:16.740 --> 32:18.540\n explicitly trying to predict how much reward\n\n32:18.540 --> 32:20.060\n it will get in the future?\n\n32:20.060 --> 32:22.740\n Does it have a representation of a policy?\n\n32:22.740 --> 32:25.700\n That means something which is deciding how to pick actions.\n\n32:25.700 --> 32:28.980\n Is that decision making process explicitly represented?\n\n32:28.980 --> 32:31.980\n And is there a model in the system?\n\n32:31.980 --> 32:34.380\n Is there something which is explicitly trying to predict\n\n32:34.380 --> 32:36.540\n what will happen in the environment?\n\n32:36.540 --> 32:40.500\n And so those three pieces are, to me,\n\n32:40.500 --> 32:42.340\n some of the most common building blocks.\n\n32:42.340 --> 32:47.020\n And I understand the different choices in RL\n\n32:47.020 --> 32:49.860\n as choices of whether or not to use those building blocks\n\n32:49.860 --> 32:52.580\n when you're trying to decompose the solution.\n\n32:52.580 --> 32:54.260\n Should I have a value function represented?\n\n32:54.260 --> 32:56.700\n Should I have a policy represented?\n\n32:56.700 --> 32:58.420\n Should I have a model represented?\n\n32:58.420 --> 33:00.180\n And there are combinations of those pieces\n\n33:00.180 --> 33:01.700\n and, of course, other things that you could\n\n33:01.700 --> 33:03.140\n add into the picture as well.\n\n33:03.140 --> 33:04.980\n But those three fundamental choices\n\n33:04.980 --> 33:06.900\n give rise to some of the branches of RL\n\n33:06.900 --> 33:08.580\n with which we're very familiar.\n\n33:08.580 --> 33:10.860\n And so those, as you mentioned,\n\n33:10.860 --> 33:14.300\n there is a choice of what's specified\n\n33:14.300 --> 33:17.180\n or modeled explicitly.\n\n33:17.180 --> 33:20.460\n And the idea is that all of these\n\n33:20.460 --> 33:23.420\n are somehow implicitly learned within the system.\n\n33:23.420 --> 33:28.420\n So it's almost a choice of how you approach a problem.\n\n33:28.500 --> 33:30.260\n Do you see those as fundamental differences\n\n33:30.260 --> 33:35.260\n or are these almost like small specifics,\n\n33:35.420 --> 33:37.500\n like the details of how you solve a problem\n\n33:37.500 --> 33:40.900\n but they're not fundamentally different from each other?\n\n33:40.900 --> 33:45.900\n I think the fundamental idea is maybe at the higher level.\n\n33:45.940 --> 33:48.660\n The fundamental idea is the first step\n\n33:48.660 --> 33:50.860\n of the decomposition is really to say,\n\n33:50.860 --> 33:55.060\n well, how are we really gonna solve any kind of problem\n\n33:55.060 --> 33:57.380\n where you're trying to figure out how to take actions\n\n33:57.380 --> 33:59.780\n and just from this stream of observations,\n\n33:59.780 --> 34:02.140\n you've got some agent situated in its sensory motor stream\n\n34:02.140 --> 34:04.300\n and getting all these observations in,\n\n34:04.300 --> 34:06.140\n getting to take these actions, and what should it do?\n\n34:06.140 --> 34:07.420\n How can you even broach that problem?\n\n34:07.420 --> 34:10.780\n You know, maybe the complexity of the world is so great\n\n34:10.780 --> 34:13.220\n that you can't even imagine how to build a system\n\n34:13.220 --> 34:15.700\n that would understand how to deal with that.\n\n34:15.700 --> 34:18.540\n And so the first step of this decomposition is to say,\n\n34:18.540 --> 34:19.540\n well, you have to learn.\n\n34:19.540 --> 34:22.020\n The system has to learn for itself.\n\n34:22.020 --> 34:24.420\n And so note that the reinforcement learning problem\n\n34:24.420 --> 34:27.060\n doesn't actually stipulate that you have to learn.\n\n34:27.060 --> 34:29.340\n Like you could maximize your rewards without learning.\n\n34:29.340 --> 34:32.380\n It would just, wouldn't do a very good job of it.\n\n34:32.380 --> 34:34.420\n So learning is required\n\n34:34.420 --> 34:36.900\n because it's the only way to achieve good performance\n\n34:36.900 --> 34:40.500\n in any sufficiently large and complex environment.\n\n34:40.500 --> 34:42.260\n So that's the first step.\n\n34:42.260 --> 34:43.740\n And so that step gives commonality\n\n34:43.740 --> 34:45.340\n to all of the other pieces,\n\n34:45.340 --> 34:48.780\n because now you might ask, well, what should you be learning?\n\n34:48.780 --> 34:49.900\n What does learning even mean?\n\n34:49.900 --> 34:52.260\n You know, in this sense, you know, learning might mean,\n\n34:52.260 --> 34:55.740\n well, you're trying to update the parameters\n\n34:55.740 --> 34:59.060\n of some system, which is then the thing\n\n34:59.060 --> 35:00.860\n that actually picks the actions.\n\n35:00.860 --> 35:03.460\n And those parameters could be representing anything.\n\n35:03.460 --> 35:06.820\n They could be parameterizing a value function or a model\n\n35:06.820 --> 35:08.540\n or a policy.\n\n35:08.540 --> 35:10.860\n And so in that sense, there's a lot of commonality\n\n35:10.860 --> 35:12.380\n in that whatever is being represented there\n\n35:12.380 --> 35:13.580\n is the thing which is being learned,\n\n35:13.580 --> 35:15.740\n and it's being learned with the ultimate goal\n\n35:15.740 --> 35:17.500\n of maximizing rewards.\n\n35:17.500 --> 35:20.300\n But the way in which you decompose the problem\n\n35:20.300 --> 35:23.140\n is really what gives the semantics to the whole system.\n\n35:23.140 --> 35:27.300\n Like, are you trying to learn something to predict well,\n\n35:27.300 --> 35:28.580\n like a value function or a model?\n\n35:28.580 --> 35:31.700\n Are you learning something to perform well, like a policy?\n\n35:31.700 --> 35:34.020\n And the form of that objective\n\n35:34.020 --> 35:36.300\n is kind of giving the semantics to the system.\n\n35:36.300 --> 35:39.260\n And so it really is, at the next level down,\n\n35:39.260 --> 35:40.300\n a fundamental choice,\n\n35:40.300 --> 35:42.860\n and we have to make those fundamental choices\n\n35:42.860 --> 35:46.180\n as system designers or enable our algorithms\n\n35:46.180 --> 35:49.340\n to be able to learn how to make those choices for themselves.\n\n35:49.340 --> 35:52.020\n So then the next step you mentioned,\n\n35:52.020 --> 35:56.020\n the very first thing you have to deal with is,\n\n35:56.020 --> 36:00.060\n can you even take in this huge stream of observations\n\n36:00.060 --> 36:01.540\n and do anything with it?\n\n36:01.540 --> 36:05.060\n So the natural next basic question is,\n\n36:05.060 --> 36:08.140\n what is deep reinforcement learning?\n\n36:08.140 --> 36:11.540\n And what is this idea of using neural networks\n\n36:11.540 --> 36:14.580\n to deal with this huge incoming stream?\n\n36:14.580 --> 36:18.220\n So amongst all the approaches for reinforcement learning,\n\n36:18.220 --> 36:19.420\n deep reinforcement learning\n\n36:19.420 --> 36:23.180\n is one family of solution methods\n\n36:23.180 --> 36:28.180\n that tries to utilize powerful representations\n\n36:29.700 --> 36:31.620\n that are offered by neural networks\n\n36:31.620 --> 36:35.740\n to represent any of these different components\n\n36:35.740 --> 36:37.980\n of the solution, of the agent,\n\n36:37.980 --> 36:39.660\n like whether it's the value function\n\n36:39.660 --> 36:41.820\n or the model or the policy.\n\n36:41.820 --> 36:43.460\n The idea of deep learning is to say,\n\n36:43.460 --> 36:46.700\n well, here's a powerful toolkit that's so powerful\n\n36:46.700 --> 36:48.180\n that it's universal in the sense\n\n36:48.180 --> 36:50.140\n that it can represent any function\n\n36:50.140 --> 36:52.020\n and it can learn any function.\n\n36:52.020 --> 36:55.020\n And so if we can leverage that universality,\n\n36:55.020 --> 36:57.940\n that means that whatever we need to represent\n\n36:57.940 --> 37:00.260\n for our policy or for our value function or for a model,\n\n37:00.260 --> 37:01.940\n deep learning can do it.\n\n37:01.940 --> 37:04.860\n So that deep learning is one approach\n\n37:04.860 --> 37:06.620\n that offers us a toolkit\n\n37:06.620 --> 37:09.460\n that has no ceiling to its performance,\n\n37:09.460 --> 37:12.500\n that as we start to put more resources into the system,\n\n37:12.500 --> 37:17.180\n more memory and more computation and more data,\n\n37:17.180 --> 37:20.140\n more experience, more interactions with the environment,\n\n37:20.140 --> 37:22.220\n that these are systems that can just get better\n\n37:22.220 --> 37:24.420\n and better and better at doing whatever the job is\n\n37:24.420 --> 37:25.340\n they've asked them to do,\n\n37:25.340 --> 37:27.740\n whatever we've asked that function to represent,\n\n37:27.740 --> 37:31.140\n it can learn a function that does a better and better job\n\n37:31.140 --> 37:33.340\n of representing that knowledge,\n\n37:33.340 --> 37:35.500\n whether that knowledge be estimating\n\n37:35.500 --> 37:36.660\n how well you're gonna do in the world,\n\n37:36.660 --> 37:37.700\n the value function,\n\n37:37.700 --> 37:40.660\n whether it's gonna be choosing what to do in the world,\n\n37:40.660 --> 37:41.500\n the policy,\n\n37:41.500 --> 37:43.860\n or whether it's understanding the world itself,\n\n37:43.860 --> 37:45.780\n what's gonna happen next, the model.\n\n37:45.780 --> 37:49.100\n Nevertheless, the fact that neural networks\n\n37:49.100 --> 37:53.780\n are able to learn incredibly complex representations\n\n37:53.780 --> 37:55.780\n that allow you to do the policy, the model\n\n37:55.780 --> 38:00.780\n or the value function is, at least to my mind,\n\n38:00.780 --> 38:02.980\n exceptionally beautiful and surprising.\n\n38:02.980 --> 38:07.980\n Like, was it surprising to you?\n\n38:07.980 --> 38:10.660\n Can you still believe it works as well as it does?\n\n38:10.660 --> 38:13.980\n Do you have good intuition about why it works at all\n\n38:13.980 --> 38:15.820\n and works as well as it does?\n\n38:18.500 --> 38:22.140\n I think, let me take two parts to that question.\n\n38:22.140 --> 38:26.740\n I think it's not surprising to me\n\n38:26.740 --> 38:30.180\n that the idea of reinforcement learning works\n\n38:30.180 --> 38:34.420\n because in some sense, I think it's the,\n\n38:34.420 --> 38:36.860\n I feel it's the only thing which can ultimately.\n\n38:36.860 --> 38:39.460\n And so I feel we have to address it\n\n38:39.460 --> 38:41.940\n and there must be success as possible\n\n38:41.940 --> 38:44.140\n because we have examples of intelligence.\n\n38:44.140 --> 38:47.020\n And it must at some level be able to,\n\n38:47.020 --> 38:49.500\n possible to acquire experience\n\n38:49.500 --> 38:51.740\n and use that experience to do better\n\n38:51.740 --> 38:55.260\n in a way which is meaningful to environments\n\n38:55.260 --> 38:57.180\n of the complexity that humans can deal with.\n\n38:57.180 --> 38:58.020\n It must be.\n\n38:58.980 --> 39:00.540\n Am I surprised that our current systems\n\n39:00.540 --> 39:01.940\n can do as well as they can do?\n\n39:03.540 --> 39:05.460\n I think one of the big surprises for me\n\n39:05.460 --> 39:06.940\n and a lot of the community\n\n39:09.060 --> 39:13.660\n is really the fact that deep learning\n\n39:13.660 --> 39:18.660\n can continue to perform so well\n\n39:18.660 --> 39:21.980\n despite the fact that these neural networks\n\n39:21.980 --> 39:23.180\n that they're representing\n\n39:23.180 --> 39:27.340\n have these incredibly nonlinear kind of bumpy surfaces\n\n39:27.340 --> 39:30.540\n which to our kind of low dimensional intuitions\n\n39:30.540 --> 39:33.300\n make it feel like surely you're just gonna get stuck\n\n39:33.300 --> 39:34.540\n and learning will get stuck\n\n39:34.540 --> 39:37.940\n because you won't be able to make any further progress.\n\n39:37.940 --> 39:42.580\n And yet the big surprise is that learning continues\n\n39:42.580 --> 39:45.860\n and these what appear to be local optima\n\n39:45.860 --> 39:48.020\n turn out not to be because in high dimensions\n\n39:48.020 --> 39:49.780\n when we make really big neural nets,\n\n39:49.780 --> 39:51.580\n there's always a way out\n\n39:51.580 --> 39:52.980\n and there's a way to go even lower\n\n39:52.980 --> 39:55.900\n and then you're still not in a local optima\n\n39:55.900 --> 39:57.180\n because there's some other pathway\n\n39:57.180 --> 39:59.380\n that will take you out and take you lower still.\n\n39:59.380 --> 40:00.580\n And so no matter where you are,\n\n40:00.580 --> 40:04.580\n learning can proceed and do better and better and better\n\n40:04.580 --> 40:06.380\n without bound.\n\n40:06.380 --> 40:09.900\n And so that is a surprising\n\n40:09.900 --> 40:13.220\n and beautiful property of neural nets\n\n40:13.220 --> 40:16.860\n which I find elegant and beautiful\n\n40:16.860 --> 40:20.460\n and somewhat shocking that it turns out to be the case.\n\n40:20.460 --> 40:22.540\n As you said, which I really like\n\n40:22.540 --> 40:27.540\n to our low dimensional intuitions, that's surprising.\n\n40:27.940 --> 40:31.980\n Yeah, we're very tuned to working\n\n40:31.980 --> 40:33.900\n within a three dimensional environment.\n\n40:33.900 --> 40:36.300\n And so to start to visualize\n\n40:36.300 --> 40:41.300\n what a billion dimensional neural network surface\n\n40:41.300 --> 40:42.740\n that you're trying to optimize over,\n\n40:42.740 --> 40:45.620\n what that even looks like is very hard for us.\n\n40:45.620 --> 40:47.940\n And so I think that really,\n\n40:47.940 --> 40:50.380\n if you try to account for the,\n\n40:52.780 --> 40:54.260\n essentially the AI winter\n\n40:54.260 --> 40:56.780\n where people gave up on neural networks,\n\n40:56.780 --> 41:00.300\n I think it's really down to that lack of ability\n\n41:00.300 --> 41:03.260\n to generalize from low dimensions to high dimensions\n\n41:03.260 --> 41:05.780\n because back then we were in the low dimensional case.\n\n41:05.780 --> 41:07.180\n People could only build neural nets\n\n41:07.180 --> 41:11.460\n with 50 nodes in them or something.\n\n41:11.460 --> 41:14.180\n And to imagine that it might be possible\n\n41:14.180 --> 41:15.980\n to build a billion dimensional neural net\n\n41:15.980 --> 41:17.500\n and it might have a completely different,\n\n41:17.500 --> 41:21.340\n qualitatively different property was very hard to anticipate.\n\n41:21.340 --> 41:24.580\n And I think even now we're starting to build the theory\n\n41:24.580 --> 41:26.420\n to support that.\n\n41:26.420 --> 41:28.260\n And it's incomplete at the moment,\n\n41:28.260 --> 41:30.900\n but all of the theory seems to be pointing in the direction\n\n41:30.900 --> 41:34.820\n that indeed this is an approach which truly is universal\n\n41:34.820 --> 41:37.220\n both in its representational capacity, which was known,\n\n41:37.220 --> 41:40.860\n but also in its learning ability, which is surprising.\n\n41:40.860 --> 41:44.780\n And it makes one wonder what else we're missing\n\n41:44.780 --> 41:47.620\n due to our low dimensional intuitions\n\n41:47.620 --> 41:51.700\n that will seem obvious once it's discovered.\n\n41:51.700 --> 41:56.700\n I often wonder, when we one day do have AIs\n\n41:57.580 --> 42:00.980\n which are superhuman in their abilities\n\n42:00.980 --> 42:02.940\n to understand the world,\n\n42:05.380 --> 42:07.540\n what will they think of the algorithms\n\n42:07.540 --> 42:08.940\n that we developed back now?\n\n42:08.940 --> 42:11.540\n Will it be looking back at these days\n\n42:11.540 --> 42:16.540\n and thinking that, will we look back and feel\n\n42:17.100 --> 42:19.580\n that these algorithms were naive first steps\n\n42:19.580 --> 42:21.500\n or will they still be the fundamental ideas\n\n42:21.500 --> 42:24.940\n which are used even in 100,000, 10,000 years?\n\n42:26.180 --> 42:27.500\n It's hard to know.\n\n42:27.500 --> 42:30.300\n They'll watch back to this conversation\n\n42:30.300 --> 42:34.820\n and with a smile, maybe a little bit of a laugh.\n\n42:34.820 --> 42:39.820\n I mean, my sense is, I think just like when we used\n\n42:40.140 --> 42:45.140\n to think that the sun revolved around the earth,\n\n42:45.860 --> 42:49.540\n they'll see our systems of today, reinforcement learning\n\n42:49.540 --> 42:54.460\n as too complicated, that the answer was simple all along.\n\n42:54.460 --> 42:58.180\n There's something, just like you said in the game of Go,\n\n42:58.180 --> 43:01.700\n I mean, I love the systems of like cellular automata,\n\n43:01.700 --> 43:05.020\n that there's simple rules from which incredible complexity\n\n43:05.020 --> 43:08.180\n emerges, so it feels like there might be\n\n43:08.180 --> 43:10.540\n some really simple approaches,\n\n43:10.540 --> 43:12.660\n just like Rich Sutton says, right?\n\n43:12.660 --> 43:17.660\n These simple methods with compute over time\n\n43:17.700 --> 43:20.700\n seem to prove to be the most effective.\n\n43:20.700 --> 43:21.900\n I 100% agree.\n\n43:21.900 --> 43:26.900\n I think that if we try to anticipate\n\n43:27.780 --> 43:30.660\n what will generalize well into the future,\n\n43:30.660 --> 43:32.900\n I think it's likely to be the case\n\n43:32.900 --> 43:35.540\n that it's the simple, clear ideas\n\n43:35.540 --> 43:36.780\n which will have the longest legs\n\n43:36.780 --> 43:39.340\n and which will carry us furthest into the future.\n\n43:39.340 --> 43:40.860\n Nevertheless, we're in a situation\n\n43:40.860 --> 43:43.260\n where we need to make things work today,\n\n43:43.260 --> 43:44.940\n and sometimes that requires putting together\n\n43:44.940 --> 43:47.420\n more complex systems where we don't have\n\n43:47.420 --> 43:49.580\n the full answers yet as to what\n\n43:49.580 --> 43:51.580\n those minimal ingredients might be.\n\n43:51.580 --> 43:55.060\n So speaking of which, if we could take a step back to Go,\n\n43:55.060 --> 44:00.060\n what was MoGo and what was the key idea behind the system?\n\n44:00.780 --> 44:04.420\n So back during my PhD on Computer Go,\n\n44:04.420 --> 44:08.900\n around about that time, there was a major new development\n\n44:08.900 --> 44:12.780\n which actually happened in the context of Computer Go,\n\n44:12.780 --> 44:16.660\n and it was really a revolution in the way\n\n44:16.660 --> 44:18.700\n that heuristic search was done,\n\n44:18.700 --> 44:21.820\n and the idea was essentially that\n\n44:21.820 --> 44:26.300\n a position could be evaluated or a state in general\n\n44:26.300 --> 44:30.620\n could be evaluated not by humans saying\n\n44:30.620 --> 44:33.500\n whether that position is good or not,\n\n44:33.500 --> 44:35.100\n or even humans providing rules\n\n44:35.100 --> 44:37.220\n as to how you might evaluate it,\n\n44:37.220 --> 44:40.860\n but instead by allowing the system\n\n44:40.860 --> 44:45.820\n to randomly play out the game until the end multiple times\n\n44:45.820 --> 44:48.100\n and taking the average of those outcomes\n\n44:48.100 --> 44:50.620\n as the prediction of what will happen.\n\n44:50.620 --> 44:53.020\n So for example, if you're in the game of Go,\n\n44:53.020 --> 44:55.380\n the intuition is that you take a position\n\n44:55.380 --> 44:58.100\n and you get the system to kind of play random moves\n\n44:58.100 --> 45:00.100\n against itself all the way to the end of the game\n\n45:00.100 --> 45:01.740\n and you see who wins.\n\n45:01.740 --> 45:03.220\n And if black ends up winning\n\n45:03.220 --> 45:05.140\n more of those random games than white,\n\n45:05.140 --> 45:07.420\n well, you say, hey, this is a position that favors white.\n\n45:07.420 --> 45:09.580\n And if white ends up winning more of those random games\n\n45:09.580 --> 45:12.220\n than black, then it favors white.\n\n45:13.620 --> 45:18.140\n So that idea was known as Monte Carlo search,\n\n45:18.140 --> 45:21.140\n and a particular form of Monte Carlo search\n\n45:21.140 --> 45:24.140\n that became very effective and was developed in computer Go\n\n45:24.140 --> 45:26.620\n first by Remy Coulomb in 2006,\n\n45:26.620 --> 45:29.140\n and then taken further by others\n\n45:29.140 --> 45:31.860\n was something called Monte Carlo tree search,\n\n45:31.860 --> 45:34.020\n which basically takes that same idea\n\n45:34.020 --> 45:39.020\n and uses that insight to evaluate every node of a search tree\n\n45:39.020 --> 45:42.140\n is evaluated by the average of the random play outs\n\n45:42.140 --> 45:44.260\n from that node onwards.\n\n45:44.260 --> 45:46.820\n And this idea, when you think about it,\n\n45:46.820 --> 45:49.220\n and this idea was very powerful\n\n45:49.220 --> 45:51.620\n and suddenly led to huge leaps forward\n\n45:51.620 --> 45:54.180\n in the strength of computer Go playing programs.\n\n45:55.180 --> 45:58.500\n And among those, the strongest of the Go playing programs\n\n45:58.500 --> 46:00.700\n in those days was a program called MoGo,\n\n46:00.700 --> 46:03.860\n which was the first program to actually reach\n\n46:03.860 --> 46:07.660\n human master level on small boards, nine by nine boards.\n\n46:07.660 --> 46:11.860\n And so this was a program by someone called Sylvain Gelli,\n\n46:11.860 --> 46:13.140\n who's a good colleague of mine,\n\n46:13.140 --> 46:16.780\n but I worked with him a little bit in those days,\n\n46:16.780 --> 46:18.420\n part of my PhD thesis.\n\n46:18.420 --> 46:23.420\n And MoGo was a first step towards the latest successes\n\n46:23.500 --> 46:25.460\n we saw in computer Go,\n\n46:25.460 --> 46:28.020\n but it was still missing a key ingredient.\n\n46:28.020 --> 46:33.020\n MoGo was evaluating purely by random rollouts against itself.\n\n46:33.860 --> 46:36.380\n And in a way, it's truly remarkable\n\n46:36.380 --> 46:39.500\n that random play should give you anything at all.\n\n46:39.500 --> 46:42.580\n Why in this perfectly deterministic game\n\n46:42.580 --> 46:46.860\n that's very precise and involves these very exact sequences,\n\n46:46.860 --> 46:51.860\n why is it that randomization is helpful?\n\n46:52.100 --> 46:54.100\n And so the intuition is that randomization\n\n46:54.100 --> 46:59.060\n captures something about the nature of the search tree,\n\n46:59.060 --> 47:01.820\n from a position that you're understanding\n\n47:01.820 --> 47:04.580\n the nature of the search tree from that node onwards\n\n47:04.580 --> 47:06.980\n by using randomization.\n\n47:06.980 --> 47:09.220\n And this was a very powerful idea.\n\n47:09.220 --> 47:12.580\n And I've seen this in other spaces,\n\n47:12.580 --> 47:14.660\n talked to Richard Karp and so on,\n\n47:14.660 --> 47:17.340\n randomized algorithms somehow magically\n\n47:17.340 --> 47:19.740\n are able to do exceptionally well\n\n47:19.740 --> 47:23.540\n and simplifying the problem somehow.\n\n47:23.540 --> 47:25.660\n Makes you wonder about the fundamental nature\n\n47:25.660 --> 47:27.620\n of randomness in our universe.\n\n47:27.620 --> 47:29.500\n It seems to be a useful thing.\n\n47:29.500 --> 47:32.100\n But so from that moment,\n\n47:32.100 --> 47:33.980\n can you maybe tell the origin story\n\n47:33.980 --> 47:36.100\n and the journey of AlphaGo?\n\n47:36.100 --> 47:39.460\n Yeah, so programs based on Monte Carlo tree search\n\n47:39.460 --> 47:41.580\n were a first revolution\n\n47:41.580 --> 47:44.740\n in the sense that they led to suddenly programs\n\n47:44.740 --> 47:47.900\n that could play the game to any reasonable level,\n\n47:47.900 --> 47:50.100\n but they plateaued.\n\n47:50.100 --> 47:51.900\n It seemed that no matter how much effort\n\n47:51.900 --> 47:53.180\n people put into these techniques,\n\n47:53.180 --> 47:54.820\n they couldn't exceed the level\n\n47:54.820 --> 47:58.060\n of amateur Dan level Go players.\n\n47:58.060 --> 47:59.580\n So strong players,\n\n47:59.580 --> 48:02.580\n but not anywhere near the level of professionals,\n\n48:02.580 --> 48:04.460\n nevermind the world champion.\n\n48:04.460 --> 48:08.380\n And so that brings us to the birth of AlphaGo,\n\n48:08.380 --> 48:12.300\n which happened in the context of a startup company\n\n48:12.300 --> 48:14.540\n known as DeepMind.\n\n48:14.540 --> 48:15.460\n I heard of them.\n\n48:15.460 --> 48:19.020\n Where a project was born.\n\n48:19.020 --> 48:23.700\n And the project was really a scientific investigation\n\n48:23.700 --> 48:27.900\n where myself and Adger Huang\n\n48:27.900 --> 48:30.660\n and an intern, Chris Madison,\n\n48:30.660 --> 48:33.220\n were exploring a scientific question.\n\n48:33.220 --> 48:35.540\n And that scientific question was really,\n\n48:37.300 --> 48:39.620\n is there another fundamentally different approach\n\n48:39.620 --> 48:42.140\n to this key question of Go,\n\n48:42.140 --> 48:45.740\n the key challenge of how can you build that intuition\n\n48:45.740 --> 48:47.580\n and how can you just have a system\n\n48:47.580 --> 48:48.940\n that could look at a position\n\n48:48.940 --> 48:51.260\n and understand what move to play\n\n48:51.260 --> 48:53.340\n or how well you're doing in that position,\n\n48:53.340 --> 48:54.820\n who's gonna win?\n\n48:54.820 --> 48:59.140\n And so the deep learning revolution had just begun.\n\n48:59.140 --> 49:03.460\n That systems like ImageNet had suddenly been won\n\n49:03.460 --> 49:06.540\n by deep learning techniques back in 2012.\n\n49:06.540 --> 49:08.620\n And following that, it was natural to ask,\n\n49:08.620 --> 49:12.460\n well, if deep learning is able to scale up so effectively\n\n49:12.460 --> 49:16.660\n with images to understand them enough to classify them,\n\n49:16.660 --> 49:17.500\n well, why not go?\n\n49:17.500 --> 49:22.500\n Why not take the black and white stones of the Go board\n\n49:22.700 --> 49:25.340\n and build a system which can understand for itself\n\n49:25.340 --> 49:27.540\n what that means in terms of what move to pick\n\n49:27.540 --> 49:30.100\n or who's gonna win the game, black or white?\n\n49:31.140 --> 49:32.540\n And so that was our scientific question\n\n49:32.540 --> 49:35.660\n which we were probing and trying to understand.\n\n49:35.660 --> 49:37.860\n And as we started to look at it,\n\n49:37.860 --> 49:40.860\n we discovered that we could build a system.\n\n49:40.860 --> 49:43.620\n So in fact, our very first paper on AlphaGo\n\n49:43.620 --> 49:46.140\n was actually a pure deep learning system\n\n49:47.020 --> 49:49.460\n which was trying to answer this question.\n\n49:49.460 --> 49:52.420\n And we showed that actually a pure deep learning system\n\n49:52.420 --> 49:54.860\n with no search at all was actually able\n\n49:54.860 --> 49:58.260\n to reach human band level, master level\n\n49:58.260 --> 50:01.740\n at the full game of Go, 19 by 19 boards.\n\n50:01.740 --> 50:04.020\n And so without any search at all,\n\n50:04.020 --> 50:06.060\n suddenly we had systems which were playing\n\n50:06.060 --> 50:10.100\n at the level of the best Monte Carlo tree search systems,\n\n50:10.100 --> 50:11.780\n the ones with randomized rollouts.\n\n50:11.780 --> 50:13.100\n So first of all, sorry to interrupt,\n\n50:13.100 --> 50:16.620\n but that's kind of a groundbreaking notion.\n\n50:16.620 --> 50:20.700\n That's like basically a definitive step away\n\n50:20.700 --> 50:22.700\n from a couple of decades\n\n50:22.700 --> 50:26.300\n of essentially search dominating AI.\n\n50:26.300 --> 50:28.940\n So how did that make you feel?\n\n50:28.940 --> 50:33.020\n Was it surprising from a scientific perspective in general,\n\n50:33.020 --> 50:33.980\n how to make you feel?\n\n50:33.980 --> 50:37.340\n I found this to be profoundly surprising.\n\n50:37.340 --> 50:41.780\n In fact, it was so surprising that we had a bet back then.\n\n50:41.780 --> 50:44.980\n And like many good projects, bets are quite motivating.\n\n50:44.980 --> 50:47.900\n And the bet was whether it was possible\n\n50:47.900 --> 50:52.140\n for a system based purely on deep learning,\n\n50:52.140 --> 50:55.900\n with no search at all to beat a down level human player.\n\n50:55.900 --> 51:00.100\n And so we had someone who joined our team\n\n51:00.100 --> 51:01.100\n who was a down level player.\n\n51:01.100 --> 51:06.100\n He came in and we had this first match against him and...\n\n51:06.660 --> 51:09.420\n Which side of the bed were you on, by the way?\n\n51:09.420 --> 51:11.740\n The losing or the winning side?\n\n51:11.740 --> 51:14.660\n I tend to be an optimist with the power\n\n51:14.660 --> 51:18.420\n of deep learning and reinforcement learning.\n\n51:18.420 --> 51:21.140\n So the system won,\n\n51:21.140 --> 51:24.260\n and we were able to beat this human down level player.\n\n51:24.260 --> 51:26.420\n And for me, that was the moment where it was like,\n\n51:26.420 --> 51:29.460\n okay, something special is afoot here.\n\n51:29.460 --> 51:32.620\n We have a system which without search\n\n51:32.620 --> 51:36.180\n is able to already just look at this position\n\n51:36.180 --> 51:39.580\n and understand things as well as a strong human player.\n\n51:39.580 --> 51:41.500\n And from that point onwards,\n\n51:41.500 --> 51:46.500\n I really felt that reaching the top levels of human play,\n\n51:49.060 --> 51:50.820\n professional level, world champion level,\n\n51:50.820 --> 51:53.260\n I felt it was actually an inevitability.\n\n51:56.620 --> 51:59.700\n And if it was an inevitable outcome,\n\n51:59.700 --> 52:03.020\n I was rather keen that it would be us that achieved it.\n\n52:03.020 --> 52:05.420\n So we scaled up.\n\n52:05.420 --> 52:06.820\n This was something where,\n\n52:06.820 --> 52:09.380\n so I had lots of conversations back then\n\n52:09.380 --> 52:14.380\n with Demis Sassabis, the head of DeepMind,\n\n52:14.660 --> 52:16.100\n who was extremely excited.\n\n52:16.100 --> 52:21.100\n And we made the decision to scale up the project,\n\n52:21.140 --> 52:23.380\n brought more people on board.\n\n52:23.380 --> 52:28.380\n And so AlphaGo became something where we had a clear goal,\n\n52:30.060 --> 52:33.700\n which was to try and crack this outstanding challenge of AI\n\n52:33.700 --> 52:37.300\n to see if we could beat the world's best players.\n\n52:37.300 --> 52:42.300\n And this led within the space of not so many months\n\n52:42.460 --> 52:45.780\n to playing against the European champion Fan Hui\n\n52:45.780 --> 52:48.940\n in a match which became memorable in history\n\n52:48.940 --> 52:50.660\n as the first time a Go program\n\n52:50.660 --> 52:53.940\n had ever beaten a professional player.\n\n52:53.940 --> 52:56.220\n And at that time we had to make a judgment\n\n52:56.220 --> 52:59.700\n as to when and whether we should go\n\n52:59.700 --> 53:01.780\n and challenge the world champion.\n\n53:01.780 --> 53:04.140\n And this was a difficult decision to make.\n\n53:04.140 --> 53:08.460\n Again, we were basing our predictions on our own progress\n\n53:08.460 --> 53:11.300\n and had to estimate based on the rapidity\n\n53:11.300 --> 53:15.340\n of our own progress when we thought we would exceed\n\n53:15.340 --> 53:17.620\n the level of the human world champion.\n\n53:17.620 --> 53:20.420\n And we tried to make an estimate and set up a match\n\n53:20.420 --> 53:25.420\n and that became the AlphaGo versus Lee Sedol match in 2016.\n\n53:27.100 --> 53:29.900\n And we should say, spoiler alert,\n\n53:29.900 --> 53:33.740\n that AlphaGo was able to defeat Lee Sedol.\n\n53:33.740 --> 53:34.980\n That's right, yeah.\n\n53:34.980 --> 53:39.980\n So maybe we could take even a broader view.\n\n53:39.980 --> 53:44.980\n AlphaGo involves both learning from expert games\n\n53:45.900 --> 53:50.900\n and as far as I remember, a self play component\n\n53:51.220 --> 53:54.260\n to where it learns by playing against itself.\n\n53:54.260 --> 53:57.580\n But in your sense, what was the role of learning\n\n53:57.580 --> 53:59.060\n from expert games there?\n\n53:59.060 --> 54:01.380\n And in terms of your self evaluation,\n\n54:01.380 --> 54:04.140\n whether you can take on the world champion,\n\n54:04.140 --> 54:06.980\n what was the thing that you're trying to do more of?\n\n54:06.980 --> 54:09.420\n Sort of train more on expert games\n\n54:09.420 --> 54:12.620\n or was there's now another,\n\n54:12.620 --> 54:15.620\n I'm asking so many poorly phrased questions,\n\n54:15.620 --> 54:19.580\n but did you have a hope or dream that self play\n\n54:19.580 --> 54:23.420\n would be the key component at that moment yet?\n\n54:24.460 --> 54:26.420\n So in the early days of AlphaGo,\n\n54:26.420 --> 54:29.780\n we used human data to explore the science\n\n54:29.780 --> 54:31.380\n of what deep learning can achieve.\n\n54:31.380 --> 54:34.620\n And so when we had our first paper that showed\n\n54:34.620 --> 54:37.820\n that it was possible to predict the winner of the game,\n\n54:37.820 --> 54:39.700\n that it was possible to suggest moves,\n\n54:39.700 --> 54:41.260\n that was done using human data.\n\n54:41.260 --> 54:42.380\n A solely human data.\n\n54:42.380 --> 54:45.100\n Yeah, and so the reason that we did it that way\n\n54:45.100 --> 54:47.620\n was at that time we were exploring separately\n\n54:47.620 --> 54:48.940\n the deep learning aspect\n\n54:48.940 --> 54:51.100\n from the reinforcement learning aspect.\n\n54:51.100 --> 54:53.420\n That was the part which was new and unknown\n\n54:53.420 --> 54:57.300\n to me at that time was how far could that be stretched?\n\n54:58.260 --> 55:00.540\n Once we had that, it then became natural\n\n55:00.540 --> 55:03.060\n to try and use that same representation\n\n55:03.060 --> 55:04.940\n and see if we could learn for ourselves\n\n55:04.940 --> 55:06.580\n using that same representation.\n\n55:06.580 --> 55:08.340\n And so right from the beginning,\n\n55:08.340 --> 55:11.940\n actually our goal had been to build a system\n\n55:11.940 --> 55:13.060\n using self play.\n\n55:14.220 --> 55:16.860\n And to us, the human data right from the beginning\n\n55:16.860 --> 55:20.860\n was an expedient step to help us for pragmatic reasons\n\n55:20.860 --> 55:24.540\n to go faster towards the goals of the project\n\n55:24.540 --> 55:27.540\n than we might be able to starting solely from self play.\n\n55:27.540 --> 55:29.820\n And so in those days, we were very aware\n\n55:29.820 --> 55:32.780\n that we were choosing to use human data\n\n55:32.780 --> 55:37.380\n and that might not be the longterm holy grail of AI,\n\n55:37.380 --> 55:40.860\n but that it was something which was extremely useful to us.\n\n55:40.860 --> 55:42.260\n It helped us to understand the system.\n\n55:42.260 --> 55:44.380\n It helped us to build deep learning representations\n\n55:44.380 --> 55:48.420\n which were clear and simple and easy to use.\n\n55:48.420 --> 55:51.980\n And so really I would say it served a purpose\n\n55:51.980 --> 55:53.300\n not just as part of the algorithm,\n\n55:53.300 --> 55:56.180\n but something which I continue to use in our research today,\n\n55:56.180 --> 56:00.100\n which is trying to break down a very hard challenge\n\n56:00.100 --> 56:02.500\n into pieces which are easier to understand for us\n\n56:02.500 --> 56:04.180\n as researchers and develop.\n\n56:04.180 --> 56:07.740\n So if you use a component based on human data,\n\n56:07.740 --> 56:10.340\n it can help you to understand the system\n\n56:10.340 --> 56:11.340\n such that then you can build\n\n56:11.340 --> 56:14.180\n the more principled version later that does it for itself.\n\n56:15.220 --> 56:19.660\n So as I said, the AlphaGo victory,\n\n56:19.660 --> 56:23.740\n and I don't think I'm being sort of romanticizing this notion.\n\n56:23.740 --> 56:25.140\n I think it's one of the greatest moments\n\n56:25.140 --> 56:26.980\n in the history of AI.\n\n56:26.980 --> 56:29.900\n So were you cognizant of this magnitude\n\n56:29.900 --> 56:32.300\n of the accomplishment at the time?\n\n56:32.300 --> 56:35.900\n I mean, are you cognizant of it even now?\n\n56:35.900 --> 56:38.580\n Because to me, I feel like it's something that would,\n\n56:38.580 --> 56:41.300\n we mentioned what the AGI systems of the future\n\n56:41.300 --> 56:42.500\n will look back.\n\n56:42.500 --> 56:46.100\n I think they'll look back at the AlphaGo victory\n\n56:46.100 --> 56:49.140\n as like, holy crap, they figured it out.\n\n56:49.140 --> 56:51.700\n This is where it started.\n\n56:51.700 --> 56:52.740\n Well, thank you again.\n\n56:52.740 --> 56:56.220\n I mean, it's funny because I guess I've been working on,\n\n56:56.220 --> 56:58.100\n I've been working on ComputerGo for a long time.\n\n56:58.100 --> 57:00.300\n So I'd been working at the time of the AlphaGo match\n\n57:00.300 --> 57:03.020\n on ComputerGo for more than a decade.\n\n57:03.020 --> 57:06.060\n And throughout that decade, I'd had this dream\n\n57:06.060 --> 57:08.780\n of what would it be like to, what would it be like really\n\n57:08.780 --> 57:12.220\n to actually be able to build a system\n\n57:12.220 --> 57:14.300\n that could play against the world champion.\n\n57:14.300 --> 57:17.500\n And I imagined that that would be an interesting moment\n\n57:17.500 --> 57:20.300\n that maybe some people might care about that\n\n57:20.300 --> 57:23.220\n and that this might be a nice achievement.\n\n57:24.140 --> 57:27.500\n But I think when I arrived in Seoul\n\n57:27.500 --> 57:31.540\n and discovered the legions of journalists\n\n57:31.540 --> 57:34.220\n that were following us around and the 100 million people\n\n57:34.220 --> 57:37.620\n that were watching the match online live,\n\n57:37.620 --> 57:40.140\n I realized that I'd been off in my estimation\n\n57:40.140 --> 57:41.900\n of how significant this moment was\n\n57:41.900 --> 57:43.980\n by several orders of magnitude.\n\n57:43.980 --> 57:48.980\n And so there was definitely an adjustment process\n\n57:48.980 --> 57:53.140\n to realize that this was something\n\n57:53.140 --> 57:55.620\n which the world really cared about\n\n57:55.620 --> 57:57.980\n and which was a watershed moment.\n\n57:57.980 --> 58:00.380\n And I think there was that moment of realization.\n\n58:01.380 --> 58:02.540\n But it's also a little bit scary\n\n58:02.540 --> 58:05.580\n because if you go into something thinking\n\n58:05.580 --> 58:08.420\n it's gonna be maybe of interest\n\n58:08.420 --> 58:10.860\n and then discover that 100 million people are watching,\n\n58:10.860 --> 58:12.220\n it suddenly makes you worry about\n\n58:12.220 --> 58:13.660\n whether some of the decisions you'd made\n\n58:13.660 --> 58:16.140\n were really the best ones or the wisest,\n\n58:16.140 --> 58:18.260\n or were going to lead to the best outcome.\n\n58:18.260 --> 58:20.580\n And we knew for sure that there were still imperfections\n\n58:20.580 --> 58:22.700\n in AlphaGo, which were gonna be exposed\n\n58:22.700 --> 58:24.420\n to the whole world watching.\n\n58:24.420 --> 58:28.180\n And so, yeah, it was I think a great experience\n\n58:28.180 --> 58:32.220\n and I feel privileged to have been part of it,\n\n58:32.220 --> 58:34.740\n privileged to have led that amazing team.\n\n58:35.980 --> 58:38.860\n I feel privileged to have been in a moment of history\n\n58:38.860 --> 58:43.700\n like you say, but also lucky that in a sense\n\n58:43.700 --> 58:46.420\n I was insulated from the knowledge of,\n\n58:46.420 --> 58:48.860\n I think it would have been harder to focus on the research\n\n58:48.860 --> 58:52.500\n if the full kind of reality of what was gonna come to pass\n\n58:52.500 --> 58:55.340\n had been known to me and the team.\n\n58:55.340 --> 58:57.620\n I think it was, we were in our bubble\n\n58:57.620 --> 58:58.740\n and we were working on research\n\n58:58.740 --> 59:01.580\n and we were trying to answer the scientific questions\n\n59:01.580 --> 59:04.540\n and then bam, the public sees it.\n\n59:04.540 --> 59:07.500\n And I think it was better that way in retrospect.\n\n59:07.500 --> 59:10.180\n Were you confident that, I guess,\n\n59:10.180 --> 59:12.700\n what were the chances that you could get the win?\n\n59:13.580 --> 59:18.580\n So just like you said, I'm a little bit more familiar\n\n59:19.060 --> 59:20.300\n with another accomplishment\n\n59:20.300 --> 59:22.380\n that we may not even get a chance to talk to.\n\n59:22.380 --> 59:24.500\n I talked to Oriel Venialis about Alpha Star\n\n59:24.500 --> 59:26.260\n which is another incredible accomplishment,\n\n59:26.260 --> 59:31.140\n but here with Alpha Star and beating the StarCraft,\n\n59:31.140 --> 59:34.460\n there was already a track record with AlphaGo.\n\n59:34.460 --> 59:36.260\n This is the really first time\n\n59:36.260 --> 59:38.500\n you get to see reinforcement learning\n\n59:39.900 --> 59:41.700\n face the best human in the world.\n\n59:41.700 --> 59:45.000\n So what was your confidence like, what was the odds?\n\n59:45.000 --> 59:46.860\n Well, we actually. Was there a bet?\n\n59:47.860 --> 59:49.100\n Funnily enough, there was.\n\n59:49.100 --> 59:52.100\n So just before the match,\n\n59:52.100 --> 59:54.300\n we weren't betting on anything concrete,\n\n59:54.300 --> 59:56.520\n but we all held out a hand.\n\n59:56.520 --> 59:57.980\n Everyone in the team held out a hand\n\n59:57.980 --> 59:59.620\n at the beginning of the match.\n\n59:59.620 --> 1:00:01.500\n And the number of fingers that they had out on their hand\n\n1:00:01.500 --> 1:00:03.420\n was supposed to represent how many games\n\n1:00:03.420 --> 1:00:06.300\n they thought we would win against Lee Sedol.\n\n1:00:06.300 --> 1:00:09.660\n And there was an amazing spread in the team's predictions.\n\n1:00:10.540 --> 1:00:12.620\n But I have to say, I predicted four, one.\n\n1:00:15.060 --> 1:00:18.580\n And the reason was based purely on data.\n\n1:00:18.580 --> 1:00:20.620\n So I'm a scientist first and foremost.\n\n1:00:20.620 --> 1:00:23.140\n And one of the things which we had established\n\n1:00:23.140 --> 1:00:27.260\n was that AlphaGo in around one in five games\n\n1:00:27.260 --> 1:00:29.540\n would develop something which we called a delusion,\n\n1:00:29.540 --> 1:00:31.980\n which was a kind of in a hole in its knowledge\n\n1:00:31.980 --> 1:00:34.840\n where it wasn't able to fully understand\n\n1:00:34.840 --> 1:00:36.100\n everything about the position.\n\n1:00:36.100 --> 1:00:38.080\n And that hole in its knowledge would persist\n\n1:00:38.080 --> 1:00:40.720\n for tens of moves throughout the game.\n\n1:00:41.700 --> 1:00:42.720\n And we knew two things.\n\n1:00:42.720 --> 1:00:44.480\n We knew that if there were no delusions,\n\n1:00:44.480 --> 1:00:46.620\n that AlphaGo seemed to be playing at a level\n\n1:00:46.620 --> 1:00:49.420\n that was far beyond any human capabilities.\n\n1:00:49.420 --> 1:00:52.020\n But we also knew that if there were delusions,\n\n1:00:52.020 --> 1:00:53.780\n the opposite was true.\n\n1:00:53.780 --> 1:00:58.300\n And in fact, that's what came to pass.\n\n1:00:58.300 --> 1:01:00.180\n We saw all of those outcomes.\n\n1:01:00.180 --> 1:01:02.900\n And Lee Sedol in one of the games\n\n1:01:02.900 --> 1:01:04.580\n played a really beautiful sequence\n\n1:01:04.580 --> 1:01:08.180\n that AlphaGo just hadn't predicted.\n\n1:01:08.180 --> 1:01:11.800\n And after that, it led it into this situation\n\n1:01:11.800 --> 1:01:14.980\n where it was unable to really understand the position fully\n\n1:01:14.980 --> 1:01:17.900\n and found itself in one of these delusions.\n\n1:01:17.900 --> 1:01:20.780\n So indeed, yeah, 4.1 was the outcome.\n\n1:01:20.780 --> 1:01:23.220\n So yeah, and can you maybe speak to it a little bit more?\n\n1:01:23.220 --> 1:01:25.620\n What were the five games?\n\n1:01:25.620 --> 1:01:26.460\n What happened?\n\n1:01:26.460 --> 1:01:29.900\n Is there interesting things that come to memory\n\n1:01:29.900 --> 1:01:33.600\n in terms of the play of the human or the machine?\n\n1:01:33.600 --> 1:01:37.220\n So I remember all of these games vividly, of course.\n\n1:01:37.220 --> 1:01:39.320\n Moments like these don't come too often\n\n1:01:39.320 --> 1:01:42.460\n in the lifetime of a scientist.\n\n1:01:42.460 --> 1:01:49.900\n And the first game was magical because it was the first time\n\n1:01:49.900 --> 1:01:53.700\n that a computer program had defeated a world\n\n1:01:53.700 --> 1:01:57.020\n champion in this grand challenge of Go.\n\n1:01:57.020 --> 1:02:04.580\n And there was a moment where AlphaGo invaded Lee Sedol's\n\n1:02:04.580 --> 1:02:07.900\n territory towards the end of the game.\n\n1:02:07.900 --> 1:02:09.920\n And that's quite an audacious thing to do.\n\n1:02:09.920 --> 1:02:11.260\n It's like saying, hey, you thought\n\n1:02:11.260 --> 1:02:12.580\n this was going to be your territory in the game,\n\n1:02:12.580 --> 1:02:14.920\n but I'm going to stick a stone right in the middle of it\n\n1:02:14.920 --> 1:02:17.980\n and prove to you that I can break it up.\n\n1:02:17.980 --> 1:02:20.260\n And Lee Sedol's face just dropped.\n\n1:02:20.260 --> 1:02:26.140\n He wasn't expecting a computer to do something that audacious.\n\n1:02:26.140 --> 1:02:30.820\n The second game became famous for a move known as move 37.\n\n1:02:30.820 --> 1:02:36.540\n This was a move that was played by AlphaGo that broke\n\n1:02:36.540 --> 1:02:39.340\n all of the conventions of Go, that the Go players were\n\n1:02:39.340 --> 1:02:40.260\n so shocked by this.\n\n1:02:40.260 --> 1:02:45.300\n They thought that maybe the operator had made a mistake.\n\n1:02:45.300 --> 1:02:48.180\n They thought that there was something crazy going on.\n\n1:02:48.180 --> 1:02:50.580\n And it just broke every rule that Go players\n\n1:02:50.580 --> 1:02:52.580\n are taught from a very young age.\n\n1:02:52.580 --> 1:02:55.300\n They're just taught this kind of move called a shoulder hit.\n\n1:02:55.300 --> 1:02:58.820\n You can only play it on the third line or the fourth line,\n\n1:02:58.820 --> 1:03:00.700\n and AlphaGo played it on the fifth line.\n\n1:03:00.700 --> 1:03:03.500\n And it turned out to be a brilliant move\n\n1:03:03.500 --> 1:03:06.100\n and made this beautiful pattern in the middle of the board that\n\n1:03:06.100 --> 1:03:08.500\n ended up winning the game.\n\n1:03:08.500 --> 1:03:12.300\n And so this really was a clear instance\n\n1:03:12.300 --> 1:03:16.020\n where we could say computers exhibited creativity,\n\n1:03:16.020 --> 1:03:18.620\n that this was really a move that was something\n\n1:03:18.620 --> 1:03:22.620\n humans hadn't known about, hadn't anticipated.\n\n1:03:22.620 --> 1:03:24.860\n And computers discovered this idea.\n\n1:03:24.860 --> 1:03:27.460\n They were the ones to say, actually, here's\n\n1:03:27.460 --> 1:03:30.700\n a new idea, something new, not in the domains\n\n1:03:30.700 --> 1:03:33.460\n of human knowledge of the game.\n\n1:03:33.460 --> 1:03:38.260\n And now the humans think this is a reasonable thing to do.\n\n1:03:38.260 --> 1:03:41.580\n And it's part of Go knowledge now.\n\n1:03:41.580 --> 1:03:44.300\n The third game, something special\n\n1:03:44.300 --> 1:03:46.860\n happens when you play against a human world champion, which,\n\n1:03:46.860 --> 1:03:48.860\n again, I hadn't anticipated before going there,\n\n1:03:48.860 --> 1:03:53.300\n which is these players are amazing.\n\n1:03:53.300 --> 1:03:56.460\n Lee Sedol was a true champion, 18 time world champion,\n\n1:03:56.460 --> 1:04:01.020\n and had this amazing ability to probe AlphaGo\n\n1:04:01.020 --> 1:04:03.500\n for weaknesses of any kind.\n\n1:04:03.500 --> 1:04:06.200\n And in the third game, he was losing,\n\n1:04:06.200 --> 1:04:09.740\n and we felt we were sailing comfortably to victory.\n\n1:04:09.740 --> 1:04:14.620\n But he managed to, from nothing, stir up this fight\n\n1:04:14.620 --> 1:04:17.060\n and build what's called a double co,\n\n1:04:17.060 --> 1:04:20.500\n these kind of repetitive positions.\n\n1:04:20.500 --> 1:04:24.180\n And he knew that historically, no computer Go program had ever\n\n1:04:24.180 --> 1:04:26.780\n been able to deal correctly with double co positions.\n\n1:04:26.780 --> 1:04:29.800\n And he managed to summon one out of nothing.\n\n1:04:29.800 --> 1:04:33.220\n And so for us, this was a real challenge.\n\n1:04:33.220 --> 1:04:35.340\n Would AlphaGo be able to deal with this,\n\n1:04:35.340 --> 1:04:38.660\n or would it just kind of crumble in the face of this situation?\n\n1:04:38.660 --> 1:04:41.460\n And fortunately, it dealt with it perfectly.\n\n1:04:41.460 --> 1:04:46.180\n The fourth game was amazing in that Lee Sedol\n\n1:04:46.180 --> 1:04:48.380\n appeared to be losing this game.\n\n1:04:48.380 --> 1:04:49.900\n AlphaGo thought it was winning.\n\n1:04:49.900 --> 1:04:52.000\n And then Lee Sedol did something,\n\n1:04:52.000 --> 1:04:55.020\n which I think only a true world champion can do,\n\n1:04:55.020 --> 1:04:57.980\n which is he found a brilliant sequence\n\n1:04:57.980 --> 1:04:59.860\n in the middle of the game, a brilliant sequence\n\n1:04:59.860 --> 1:05:05.220\n that led him to really just transform the position.\n\n1:05:05.220 --> 1:05:10.780\n He kind of found just a piece of genius, really.\n\n1:05:10.780 --> 1:05:15.660\n And after that, AlphaGo, its evaluation just tumbled.\n\n1:05:15.660 --> 1:05:17.220\n It thought it was winning this game.\n\n1:05:17.220 --> 1:05:20.540\n And all of a sudden, it tumbled and said, oh, now\n\n1:05:20.540 --> 1:05:21.460\n I've got no chance.\n\n1:05:21.460 --> 1:05:24.420\n And it started to behave rather oddly at that point.\n\n1:05:24.420 --> 1:05:27.540\n In the final game, for some reason, we as a team\n\n1:05:27.540 --> 1:05:30.960\n were convinced, having seen AlphaGo in the previous game,\n\n1:05:30.960 --> 1:05:31.980\n suffer from delusions.\n\n1:05:31.980 --> 1:05:34.220\n We as a team were convinced that it\n\n1:05:34.220 --> 1:05:35.940\n was suffering from another delusion.\n\n1:05:35.940 --> 1:05:38.340\n We were convinced that it was misevaluating the position\n\n1:05:38.340 --> 1:05:41.260\n and that something was going terribly wrong.\n\n1:05:41.260 --> 1:05:43.740\n And it was only in the last few moves of the game\n\n1:05:43.740 --> 1:05:46.780\n that we realized that actually, although it\n\n1:05:46.780 --> 1:05:49.460\n had been predicting it was going to win all the way through,\n\n1:05:49.460 --> 1:05:51.380\n it really was.\n\n1:05:51.380 --> 1:05:54.220\n And so somehow, it just taught us yet again\n\n1:05:54.220 --> 1:05:56.180\n that you have to have faith in your systems.\n\n1:05:56.180 --> 1:05:58.700\n When they exceed your own level of ability\n\n1:05:58.700 --> 1:06:01.340\n and your own judgment, you have to trust in them\n\n1:06:01.340 --> 1:06:06.300\n to know better than you, the designer, once you've\n\n1:06:06.300 --> 1:06:10.580\n bestowed in them the ability to judge better than you can,\n\n1:06:10.580 --> 1:06:13.020\n then trust the system to do so.\n\n1:06:13.020 --> 1:06:18.900\n So just like in the case of Deep Blue beating Gary Kasparov,\n\n1:06:18.900 --> 1:06:23.120\n so Gary was, I think, the first time he's ever lost, actually,\n\n1:06:23.120 --> 1:06:24.460\n to anybody.\n\n1:06:24.460 --> 1:06:27.740\n And I mean, there's a similar situation with Lee Sedol.\n\n1:06:27.740 --> 1:06:36.580\n It's a tragic loss for humans, but a beautiful one,\n\n1:06:36.580 --> 1:06:40.780\n I think, that's kind of, from the tragedy,\n\n1:06:40.780 --> 1:06:45.020\n sort of emerges over time, emerges\n\n1:06:45.020 --> 1:06:47.300\n a kind of inspiring story.\n\n1:06:47.300 --> 1:06:52.180\n But Lee Sedol recently announced his retirement.\n\n1:06:52.180 --> 1:06:56.020\n I don't know if we can look too deeply into it,\n\n1:06:56.020 --> 1:06:59.540\n but he did say that even if I become number one,\n\n1:06:59.540 --> 1:07:02.620\n there's an entity that cannot be defeated.\n\n1:07:02.620 --> 1:07:05.460\n So what do you think about these words?\n\n1:07:05.460 --> 1:07:08.020\n What do you think about his retirement from the game ago?\n\n1:07:08.020 --> 1:07:09.660\n Well, let me take you back, first of all,\n\n1:07:09.660 --> 1:07:12.420\n to the first part of your comment about Gary Kasparov,\n\n1:07:12.420 --> 1:07:15.700\n because actually, at the panel yesterday,\n\n1:07:15.700 --> 1:07:19.780\n he specifically said that when he first lost to Deep Blue,\n\n1:07:19.780 --> 1:07:22.340\n he viewed it as a failure.\n\n1:07:22.340 --> 1:07:24.940\n He viewed that this had been a failure of his.\n\n1:07:24.940 --> 1:07:27.220\n But later on in his career, he said\n\n1:07:27.220 --> 1:07:30.420\n he'd come to realize that actually, it was a success.\n\n1:07:30.420 --> 1:07:33.380\n It was a success for everyone, because this marked\n\n1:07:33.380 --> 1:07:37.180\n transformational moment for AI.\n\n1:07:37.180 --> 1:07:39.120\n And so even for Gary Kasparov, he\n\n1:07:39.120 --> 1:07:42.500\n came to realize that that moment was pivotal\n\n1:07:42.500 --> 1:07:45.420\n and actually meant something much more\n\n1:07:45.420 --> 1:07:49.960\n than his personal loss in that moment.\n\n1:07:49.960 --> 1:07:53.840\n Lee Sedol, I think, was much more cognizant of that,\n\n1:07:53.840 --> 1:07:54.860\n even at the time.\n\n1:07:54.860 --> 1:07:57.940\n And so in his closing remarks to the match,\n\n1:07:57.940 --> 1:08:01.580\n he really felt very strongly that what\n\n1:08:01.580 --> 1:08:02.940\n had happened in the AlphaGo match\n\n1:08:02.940 --> 1:08:06.580\n was not only meaningful for AI, but for humans as well.\n\n1:08:06.580 --> 1:08:09.940\n And he felt as a Go player that it had opened his horizons\n\n1:08:09.940 --> 1:08:12.700\n and meant that he could start exploring new things.\n\n1:08:12.700 --> 1:08:14.460\n It brought his joy back for the game of Go,\n\n1:08:14.460 --> 1:08:18.620\n because it had broken all of the conventions and barriers\n\n1:08:18.620 --> 1:08:23.700\n and meant that suddenly, anything was possible again.\n\n1:08:23.700 --> 1:08:26.060\n So I was sad to hear that he'd retired,\n\n1:08:26.060 --> 1:08:31.180\n but he's been a great world champion over many, many years.\n\n1:08:31.180 --> 1:08:36.180\n And I think he'll be remembered for that ever more.\n\n1:08:36.180 --> 1:08:39.340\n He'll be remembered as the last person to beat AlphaGo.\n\n1:08:39.340 --> 1:08:43.100\n I mean, after that, we increased the power of the system.\n\n1:08:43.100 --> 1:08:49.580\n And the next version of AlphaGo beats the other strong human\n\n1:08:49.580 --> 1:08:52.260\n player 60 games to nil.\n\n1:08:52.260 --> 1:08:55.580\n So what a great moment for him and something\n\n1:08:55.580 --> 1:08:58.020\n to be remembered for.\n\n1:08:58.020 --> 1:09:02.380\n It's interesting that you spent time at AAAI on a panel\n\n1:09:02.380 --> 1:09:05.220\n with Garry Kasparov.\n\n1:09:05.220 --> 1:09:07.460\n What, I mean, it's almost, I'm just\n\n1:09:07.460 --> 1:09:12.020\n curious to learn the conversations you've\n\n1:09:12.020 --> 1:09:15.260\n had with Garry, because he's also now,\n\n1:09:15.260 --> 1:09:17.420\n he's written a book about artificial intelligence.\n\n1:09:17.420 --> 1:09:18.900\n He's thinking about AI.\n\n1:09:18.900 --> 1:09:21.140\n He has kind of a view of it.\n\n1:09:21.140 --> 1:09:23.820\n And he talks about AlphaGo a lot.\n\n1:09:23.820 --> 1:09:26.940\n What's your sense?\n\n1:09:26.940 --> 1:09:28.620\n Arguably, I'm not just being Russian,\n\n1:09:28.620 --> 1:09:31.100\n but I think Garry is the greatest chess player\n\n1:09:31.100 --> 1:09:34.700\n of all time, probably one of the greatest game\n\n1:09:34.700 --> 1:09:36.540\n players of all time.\n\n1:09:36.540 --> 1:09:41.700\n And you sort of at the center of creating\n\n1:09:41.700 --> 1:09:45.300\n a system that beats one of the greatest players of all time.\n\n1:09:45.300 --> 1:09:46.740\n So what is that conversation like?\n\n1:09:46.740 --> 1:09:50.420\n Is there anything, any interesting digs, any bets,\n\n1:09:50.420 --> 1:09:53.660\n any funny things, any profound things?\n\n1:09:53.660 --> 1:09:58.220\n So Garry Kasparov has an incredible respect\n\n1:09:58.220 --> 1:10:01.140\n for what we did with AlphaGo.\n\n1:10:01.140 --> 1:10:07.540\n And it's an amazing tribute coming from him of all people\n\n1:10:07.540 --> 1:10:11.780\n that he really appreciates and respects what we've done.\n\n1:10:11.780 --> 1:10:14.580\n And I think he feels that the progress which has happened\n\n1:10:14.580 --> 1:10:19.100\n in computer chess, which later after AlphaGo,\n\n1:10:19.100 --> 1:10:23.060\n we built the AlphaZero system, which\n\n1:10:23.060 --> 1:10:26.700\n defeated the world's strongest chess programs.\n\n1:10:26.700 --> 1:10:29.860\n And to Garry Kasparov, that moment in computer chess\n\n1:10:29.860 --> 1:10:32.980\n was more profound than Deep Blue.\n\n1:10:32.980 --> 1:10:35.660\n And the reason he believes it mattered more\n\n1:10:35.660 --> 1:10:37.620\n was because it was done with learning\n\n1:10:37.620 --> 1:10:39.940\n and a system which was able to discover for itself\n\n1:10:39.940 --> 1:10:42.620\n new principles, new ideas, which were\n\n1:10:42.620 --> 1:10:47.740\n able to play the game in a way which he hadn't always\n\n1:10:47.740 --> 1:10:50.180\n known about or anyone.\n\n1:10:50.180 --> 1:10:53.180\n And in fact, one of the things I discovered at this panel\n\n1:10:53.180 --> 1:10:56.500\n was that the current world champion, Magnus Carlsen,\n\n1:10:56.500 --> 1:11:00.460\n apparently recently commented on his improvement\n\n1:11:00.460 --> 1:11:01.820\n in performance.\n\n1:11:01.820 --> 1:11:03.860\n And he attributed it to AlphaZero,\n\n1:11:03.860 --> 1:11:05.860\n that he's been studying the games of AlphaZero.\n\n1:11:05.860 --> 1:11:08.700\n And he's changed his style to play more like AlphaZero.\n\n1:11:08.700 --> 1:11:13.820\n And it's led to him actually increasing his rating\n\n1:11:13.820 --> 1:11:15.100\n to a new peak.\n\n1:11:15.100 --> 1:11:18.420\n Yeah, I guess to me, just like to Garry,\n\n1:11:18.420 --> 1:11:21.340\n the inspiring thing is that, and just like you said,\n\n1:11:21.340 --> 1:11:25.140\n with reinforcement learning, reinforcement learning\n\n1:11:25.140 --> 1:11:26.940\n and deep learning, machine learning\n\n1:11:26.940 --> 1:11:29.540\n feels like what intelligence is.\n\n1:11:29.540 --> 1:11:35.900\n And you could attribute it to a bitter viewpoint\n\n1:11:35.900 --> 1:11:39.500\n from Garry's perspective, from us humans perspective,\n\n1:11:39.500 --> 1:11:43.740\n saying that pure search that IBM Deep Blue was doing\n\n1:11:43.740 --> 1:11:47.780\n is not really intelligence, but somehow it didn't feel like it.\n\n1:11:47.780 --> 1:11:49.100\n And so that's the magical.\n\n1:11:49.100 --> 1:11:50.900\n I'm not sure what it is about learning that\n\n1:11:50.900 --> 1:11:54.620\n feels like intelligence, but it does.\n\n1:11:54.620 --> 1:11:58.220\n So I think we should not demean the achievements of what\n\n1:11:58.220 --> 1:12:00.060\n was done in previous eras of AI.\n\n1:12:00.060 --> 1:12:04.140\n I think that Deep Blue was an amazing achievement in itself.\n\n1:12:04.140 --> 1:12:07.900\n And that heuristic search of the kind that was used by Deep\n\n1:12:07.900 --> 1:12:11.420\n Blue had some powerful ideas that were in there,\n\n1:12:11.420 --> 1:12:13.220\n but it also missed some things.\n\n1:12:13.220 --> 1:12:16.860\n So the fact that the evaluation function, the way\n\n1:12:16.860 --> 1:12:18.620\n that the chess position was understood,\n\n1:12:18.620 --> 1:12:22.540\n was created by humans and not by the machine\n\n1:12:22.540 --> 1:12:26.740\n is a limitation, which means that there's\n\n1:12:26.740 --> 1:12:28.900\n a ceiling on how well it can do.\n\n1:12:28.900 --> 1:12:30.900\n But maybe more importantly, it means\n\n1:12:30.900 --> 1:12:33.740\n that the same idea cannot be applied in other domains\n\n1:12:33.740 --> 1:12:38.500\n where we don't have access to the human grandmasters\n\n1:12:38.500 --> 1:12:41.140\n and that ability to encode exactly their knowledge\n\n1:12:41.140 --> 1:12:43.060\n into an evaluation function.\n\n1:12:43.060 --> 1:12:45.060\n And the reality is that the story of AI\n\n1:12:45.060 --> 1:12:48.580\n is that most domains turn out to be of the second type\n\n1:12:48.580 --> 1:12:52.020\n where knowledge is messy, it's hard to extract from experts,\n\n1:12:52.020 --> 1:12:53.940\n or it isn't even available.\n\n1:12:53.940 --> 1:12:59.860\n And so we need to solve problems in a different way.\n\n1:12:59.860 --> 1:13:02.740\n And I think AlphaGo is a step towards solving things\n\n1:13:02.740 --> 1:13:07.780\n in a way which puts learning as a first class citizen\n\n1:13:07.780 --> 1:13:11.420\n and says systems need to understand for themselves\n\n1:13:11.420 --> 1:13:19.300\n how to understand the world, how to judge the value of any action\n\n1:13:19.300 --> 1:13:20.780\n that they might take within that world\n\n1:13:20.780 --> 1:13:22.780\n and any state they might find themselves in.\n\n1:13:22.780 --> 1:13:29.060\n And in order to do that, we make progress towards AI.\n\n1:13:29.060 --> 1:13:32.980\n Yeah, so one of the nice things about taking a learning\n\n1:13:32.980 --> 1:13:36.540\n approach to the game of Go or game playing\n\n1:13:36.540 --> 1:13:39.380\n is that the things you learn, the things you figure out,\n\n1:13:39.380 --> 1:13:42.540\n are actually going to be applicable to other problems\n\n1:13:42.540 --> 1:13:44.100\n that are real world problems.\n\n1:13:44.100 --> 1:13:47.060\n That's ultimately, I mean, there's\n\n1:13:47.060 --> 1:13:49.100\n two really interesting things about AlphaGo.\n\n1:13:49.100 --> 1:13:52.420\n One is the science of it, just the science of learning,\n\n1:13:52.420 --> 1:13:54.540\n the science of intelligence.\n\n1:13:54.540 --> 1:13:56.980\n And then the other is while you're actually\n\n1:13:56.980 --> 1:13:59.900\n learning to figuring out how to build systems that\n\n1:13:59.900 --> 1:14:04.140\n would be potentially applicable in other applications,\n\n1:14:04.140 --> 1:14:06.580\n medical, autonomous vehicles, robotics,\n\n1:14:06.580 --> 1:14:10.580\n I mean, it's just open the door to all kinds of applications.\n\n1:14:10.580 --> 1:14:16.340\n So the next incredible step, really the profound step\n\n1:14:16.340 --> 1:14:18.220\n is probably AlphaGo Zero.\n\n1:14:18.220 --> 1:14:20.500\n I mean, it's arguable.\n\n1:14:20.500 --> 1:14:22.420\n I kind of see them all as the same place.\n\n1:14:22.420 --> 1:14:24.300\n But really, and perhaps you were already\n\n1:14:24.300 --> 1:14:26.740\n thinking that AlphaGo Zero is the natural.\n\n1:14:26.740 --> 1:14:29.180\n It was always going to be the next step.\n\n1:14:29.180 --> 1:14:33.340\n But it's removing the reliance on human expert games\n\n1:14:33.340 --> 1:14:35.340\n for pre training, as you mentioned.\n\n1:14:35.340 --> 1:14:38.260\n So how big of an intellectual leap\n\n1:14:38.260 --> 1:14:43.420\n was this that self play could achieve superhuman level\n\n1:14:43.420 --> 1:14:45.580\n performance in its own?\n\n1:14:45.580 --> 1:14:48.580\n And maybe could you also say, what is self play?\n\n1:14:48.580 --> 1:14:51.580\n Kind of mention it a few times.\n\n1:14:51.580 --> 1:14:55.180\n So let me start with self play.\n\n1:14:55.180 --> 1:14:58.300\n So the idea of self play is something\n\n1:14:58.300 --> 1:15:01.940\n which is really about systems learning for themselves,\n\n1:15:01.940 --> 1:15:05.660\n but in the situation where there's more than one agent.\n\n1:15:05.660 --> 1:15:08.300\n And so if you're in a game, and the game\n\n1:15:08.300 --> 1:15:11.100\n is played between two players, then self play\n\n1:15:11.100 --> 1:15:15.140\n is really about understanding that game just\n\n1:15:15.140 --> 1:15:17.540\n by playing games against yourself\n\n1:15:17.540 --> 1:15:19.940\n rather than against any actual real opponent.\n\n1:15:19.940 --> 1:15:23.860\n And so it's a way to kind of discover strategies\n\n1:15:23.860 --> 1:15:27.900\n without having to actually need to go out and play\n\n1:15:27.900 --> 1:15:32.620\n against any particular human player, for example.\n\n1:15:36.020 --> 1:15:38.940\n The main idea of Alpha Zero was really\n\n1:15:38.940 --> 1:15:45.300\n to try and step back from any of the knowledge\n\n1:15:45.300 --> 1:15:47.820\n that we put into the system and ask the question,\n\n1:15:47.820 --> 1:15:52.980\n is it possible to come up with a single elegant principle\n\n1:15:52.980 --> 1:15:57.380\n by which a system can learn for itself all of the knowledge\n\n1:15:57.380 --> 1:16:00.780\n which it requires to play a game such as Go?\n\n1:16:00.780 --> 1:16:03.220\n Importantly, by taking knowledge out,\n\n1:16:03.220 --> 1:16:08.860\n you not only make the system less brittle in the sense\n\n1:16:08.860 --> 1:16:10.620\n that perhaps the knowledge you were putting in\n\n1:16:10.620 --> 1:16:13.860\n was just getting in the way and maybe stopping the system\n\n1:16:13.860 --> 1:16:17.820\n learning for itself, but also you make it more general.\n\n1:16:17.820 --> 1:16:20.260\n The more knowledge you put in, the harder\n\n1:16:20.260 --> 1:16:23.460\n it is for a system to actually be placed,\n\n1:16:23.460 --> 1:16:26.700\n taken out of the system in which it's kind of been designed,\n\n1:16:26.700 --> 1:16:29.340\n and placed in some other system that maybe would need\n\n1:16:29.340 --> 1:16:31.420\n a completely different knowledge base to understand\n\n1:16:31.420 --> 1:16:32.860\n and perform well.\n\n1:16:32.860 --> 1:16:36.900\n And so the real goal here is to strip out all of the knowledge\n\n1:16:36.900 --> 1:16:39.580\n that we put in to the point that we can just plug it\n\n1:16:39.580 --> 1:16:41.700\n into something totally different.\n\n1:16:41.700 --> 1:16:45.260\n And that, to me, is really the promise of AI\n\n1:16:45.260 --> 1:16:47.700\n is that we can have systems such as that which,\n\n1:16:47.700 --> 1:16:51.540\n no matter what the goal is, no matter what goal\n\n1:16:51.540 --> 1:16:53.980\n we set to the system, we can come up\n\n1:16:53.980 --> 1:16:57.580\n with an algorithm which can be placed into that world,\n\n1:16:57.580 --> 1:16:59.940\n into that environment, and can succeed\n\n1:16:59.940 --> 1:17:01.780\n in achieving that goal.\n\n1:17:01.780 --> 1:17:06.620\n And then that, to me, is almost the essence of intelligence\n\n1:17:06.620 --> 1:17:07.980\n if we can achieve that.\n\n1:17:07.980 --> 1:17:11.340\n And so AlphaZero is a step towards that.\n\n1:17:11.340 --> 1:17:15.300\n And it's a step that was taken in the context of two player\n\n1:17:15.300 --> 1:17:18.820\n perfect information games like Go and chess.\n\n1:17:18.820 --> 1:17:21.460\n We also applied it to Japanese chess.\n\n1:17:21.460 --> 1:17:23.660\n So just to clarify, the first step\n\n1:17:23.660 --> 1:17:25.540\n was AlphaGo Zero.\n\n1:17:25.540 --> 1:17:29.860\n The first step was to try and take all of the knowledge out\n\n1:17:29.860 --> 1:17:32.580\n of AlphaGo in such a way that it could\n\n1:17:32.580 --> 1:17:39.620\n play in a fully self discovered way, purely from self play.\n\n1:17:39.620 --> 1:17:41.300\n And to me, the motivation for that\n\n1:17:41.300 --> 1:17:44.980\n was always that we could then plug it into other domains.\n\n1:17:44.980 --> 1:17:48.060\n But we saved that until later.\n\n1:17:48.060 --> 1:17:52.860\n Well, in fact, I mean, just for fun,\n\n1:17:52.860 --> 1:17:54.300\n I could tell you exactly the moment\n\n1:17:54.300 --> 1:17:57.460\n where the idea for AlphaZero occurred to me.\n\n1:17:57.460 --> 1:18:00.380\n Because I think there's maybe a lesson there for researchers\n\n1:18:00.380 --> 1:18:03.180\n who are too deeply embedded in their research\n\n1:18:03.180 --> 1:18:08.140\n and working 24 sevens to try and come up with the next idea,\n\n1:18:08.140 --> 1:18:13.660\n which is it actually occurred to me on honeymoon.\n\n1:18:13.660 --> 1:18:17.140\n And I was at my most fully relaxed state,\n\n1:18:17.140 --> 1:18:22.900\n really enjoying myself, and just bing,\n\n1:18:22.900 --> 1:18:29.860\n the algorithm for AlphaZero just appeared in its full form.\n\n1:18:29.860 --> 1:18:33.180\n And this was actually before we played against Lisa Dahl.\n\n1:18:33.180 --> 1:18:35.780\n But we just didn't.\n\n1:18:35.780 --> 1:18:39.140\n I think we were so busy trying to make sure\n\n1:18:39.140 --> 1:18:43.460\n we could beat the world champion that it was only later\n\n1:18:43.460 --> 1:18:47.420\n that we had the opportunity to step back and start\n\n1:18:47.420 --> 1:18:51.060\n examining that sort of deeper scientific question of whether\n\n1:18:51.060 --> 1:18:52.340\n this could really work.\n\n1:18:52.340 --> 1:18:56.260\n So nevertheless, so self play is probably\n\n1:18:56.260 --> 1:19:03.340\n one of the most profound ideas that represents, to me at least,\n\n1:19:03.340 --> 1:19:05.500\n artificial intelligence.\n\n1:19:05.500 --> 1:19:09.780\n But the fact that you could use that kind of mechanism\n\n1:19:09.780 --> 1:19:13.020\n to, again, beat world class players,\n\n1:19:13.020 --> 1:19:14.860\n that's very surprising.\n\n1:19:14.860 --> 1:19:19.180\n So to me, it feels like you have to train\n\n1:19:19.180 --> 1:19:21.300\n in a large number of expert games.\n\n1:19:21.300 --> 1:19:22.740\n So was it surprising to you?\n\n1:19:22.740 --> 1:19:23.660\n What was the intuition?\n\n1:19:23.660 --> 1:19:26.540\n Can you sort of think, not necessarily at that time,\n\n1:19:26.540 --> 1:19:27.980\n even now, what's your intuition?\n\n1:19:27.980 --> 1:19:30.060\n Why this thing works so well?\n\n1:19:30.060 --> 1:19:31.900\n Why it's able to learn from scratch?\n\n1:19:31.900 --> 1:19:34.580\n Well, let me first say why we tried it.\n\n1:19:34.580 --> 1:19:36.500\n So we tried it both because I feel\n\n1:19:36.500 --> 1:19:38.540\n that it was the deeper scientific question\n\n1:19:38.540 --> 1:19:42.140\n to be asking to make progress towards AI,\n\n1:19:42.140 --> 1:19:44.980\n and also because, in general, in my research,\n\n1:19:44.980 --> 1:19:49.060\n I don't like to do research on questions for which we already\n\n1:19:49.060 --> 1:19:51.060\n know the likely outcome.\n\n1:19:51.060 --> 1:19:53.380\n I don't see much value in running an experiment where\n\n1:19:53.380 --> 1:19:57.700\n you're 95% confident that you will succeed.\n\n1:19:57.700 --> 1:20:02.260\n And so we could have tried maybe to take AlphaGo and do\n\n1:20:02.260 --> 1:20:05.060\n something which we knew for sure it would succeed on.\n\n1:20:05.060 --> 1:20:07.620\n But much more interesting to me was to try it on the things\n\n1:20:07.620 --> 1:20:09.460\n which we weren't sure about.\n\n1:20:09.460 --> 1:20:12.980\n And one of the big questions on our minds\n\n1:20:12.980 --> 1:20:16.220\n back then was, could you really do this with self play alone?\n\n1:20:16.220 --> 1:20:17.660\n How far could that go?\n\n1:20:17.660 --> 1:20:19.540\n Would it be as strong?\n\n1:20:19.540 --> 1:20:22.340\n And honestly, we weren't sure.\n\n1:20:22.340 --> 1:20:25.380\n It was 50, 50, I think.\n\n1:20:25.380 --> 1:20:27.340\n If you'd asked me, I wasn't confident\n\n1:20:27.340 --> 1:20:30.660\n that it could reach the same level as these systems,\n\n1:20:30.660 --> 1:20:33.860\n but it felt like the right question to ask.\n\n1:20:33.860 --> 1:20:36.780\n And even if it had not achieved the same level,\n\n1:20:36.780 --> 1:20:41.620\n I felt that that was an important direction\n\n1:20:41.620 --> 1:20:42.900\n to be studying.\n\n1:20:42.900 --> 1:20:48.300\n And so then, lo and behold, it actually\n\n1:20:48.300 --> 1:20:52.380\n ended up outperforming the previous version of AlphaGo\n\n1:20:52.380 --> 1:20:55.940\n and indeed was able to beat it by 100 games to zero.\n\n1:20:55.940 --> 1:20:59.780\n So what's the intuition as to why?\n\n1:20:59.780 --> 1:21:02.380\n I think the intuition to me is clear,\n\n1:21:02.380 --> 1:21:09.420\n that whenever you have errors in a system, as we did in AlphaGo,\n\n1:21:09.420 --> 1:21:11.700\n AlphaGo suffered from these delusions.\n\n1:21:11.700 --> 1:21:13.300\n Occasionally, it would misunderstand\n\n1:21:13.300 --> 1:21:15.940\n what was going on in a position and miss evaluate it.\n\n1:21:15.940 --> 1:21:19.700\n How can you remove all of these errors?\n\n1:21:19.700 --> 1:21:21.820\n Errors arise from many sources.\n\n1:21:21.820 --> 1:21:25.300\n For us, they were arising both starting from the human data,\n\n1:21:25.300 --> 1:21:27.740\n but also from the nature of the search\n\n1:21:27.740 --> 1:21:29.780\n and the nature of the algorithm itself.\n\n1:21:29.780 --> 1:21:33.180\n But the only way to address them in any complex system\n\n1:21:33.180 --> 1:21:36.180\n is to give the system the ability\n\n1:21:36.180 --> 1:21:37.940\n to correct its own errors.\n\n1:21:37.940 --> 1:21:39.500\n It must be able to correct them.\n\n1:21:39.500 --> 1:21:41.420\n It must be able to learn for itself\n\n1:21:41.420 --> 1:21:44.660\n when it's doing something wrong and correct for it.\n\n1:21:44.660 --> 1:21:47.820\n And so it seemed to me that the way to correct delusions\n\n1:21:47.820 --> 1:21:51.340\n was indeed to have more iterations of reinforcement\n\n1:21:51.340 --> 1:21:53.540\n learning, that no matter where you start,\n\n1:21:53.540 --> 1:21:55.740\n you should be able to correct those errors\n\n1:21:55.740 --> 1:21:58.380\n until it gets to play that out and understand,\n\n1:21:58.380 --> 1:22:01.420\n oh, well, I thought that I was going to win in this situation,\n\n1:22:01.420 --> 1:22:03.220\n but then I ended up losing.\n\n1:22:03.220 --> 1:22:05.420\n That suggests that I was miss evaluating something.\n\n1:22:05.420 --> 1:22:07.620\n There's a hole in my knowledge, and now the system\n\n1:22:07.620 --> 1:22:11.580\n can correct for itself and understand how to do better.\n\n1:22:11.580 --> 1:22:14.300\n Now, if you take that same idea and trace it back\n\n1:22:14.300 --> 1:22:16.540\n all the way to the beginning, it should\n\n1:22:16.540 --> 1:22:19.180\n be able to take you from no knowledge,\n\n1:22:19.180 --> 1:22:21.740\n from completely random starting point,\n\n1:22:21.740 --> 1:22:24.740\n all the way to the highest levels of knowledge\n\n1:22:24.740 --> 1:22:27.100\n that you can achieve in a domain.\n\n1:22:27.100 --> 1:22:30.620\n And the principle is the same, that if you bestow a system\n\n1:22:30.620 --> 1:22:33.540\n with the ability to correct its own errors,\n\n1:22:33.540 --> 1:22:36.180\n then it can take you from random to something slightly\n\n1:22:36.180 --> 1:22:39.540\n better than random because it sees the stupid things\n\n1:22:39.540 --> 1:22:41.580\n that the random is doing, and it can correct them.\n\n1:22:41.580 --> 1:22:43.940\n And then it can take you from that slightly better system\n\n1:22:43.940 --> 1:22:45.900\n and understand, well, what's that doing wrong?\n\n1:22:45.900 --> 1:22:49.300\n And it takes you on to the next level and the next level.\n\n1:22:49.300 --> 1:22:52.980\n And this progress can go on indefinitely.\n\n1:22:52.980 --> 1:22:55.300\n And indeed, what would have happened\n\n1:22:55.300 --> 1:22:58.420\n if we'd carried on training AlphaGo Zero for longer?\n\n1:22:59.420 --> 1:23:03.340\n We saw no sign of it slowing down its improvements,\n\n1:23:03.340 --> 1:23:06.660\n or at least it was certainly carrying on to improve.\n\n1:23:06.660 --> 1:23:11.060\n And presumably, if you had the computational resources,\n\n1:23:11.060 --> 1:23:14.500\n this could lead to better and better systems\n\n1:23:14.500 --> 1:23:15.740\n that discover more and more.\n\n1:23:15.740 --> 1:23:18.940\n So your intuition is fundamentally\n\n1:23:18.940 --> 1:23:21.620\n there's not a ceiling to this process.\n\n1:23:21.620 --> 1:23:24.660\n One of the surprising things, just like you said,\n\n1:23:24.660 --> 1:23:27.340\n is the process of patching errors.\n\n1:23:27.340 --> 1:23:31.060\n It intuitively makes sense that this is,\n\n1:23:31.060 --> 1:23:33.580\n that reinforcement learning should be part of that process.\n\n1:23:33.580 --> 1:23:36.060\n But what is surprising is in the process\n\n1:23:36.060 --> 1:23:39.260\n of patching your own lack of knowledge,\n\n1:23:39.260 --> 1:23:41.980\n you don't open up other patches.\n\n1:23:41.980 --> 1:23:46.660\n You keep sort of, like there's a monotonic decrease\n\n1:23:46.660 --> 1:23:48.500\n of your weaknesses.\n\n1:23:48.500 --> 1:23:50.140\n Well, let me back this up.\n\n1:23:50.140 --> 1:23:53.780\n I think science always should make falsifiable hypotheses.\n\n1:23:53.780 --> 1:23:57.060\n So let me back up this claim with a falsifiable hypothesis,\n\n1:23:57.060 --> 1:23:59.780\n which is that if someone was to, in the future,\n\n1:23:59.780 --> 1:24:02.380\n take Alpha Zero as an algorithm\n\n1:24:02.380 --> 1:24:07.380\n and run it on with greater computational resources\n\n1:24:07.460 --> 1:24:09.500\n that we had available today,\n\n1:24:10.580 --> 1:24:12.860\n then I would predict that they would be able\n\n1:24:12.860 --> 1:24:15.380\n to beat the previous system 100 games to zero.\n\n1:24:15.380 --> 1:24:17.260\n And that if they were then to do the same thing\n\n1:24:17.260 --> 1:24:19.260\n a couple of years later,\n\n1:24:19.260 --> 1:24:22.100\n that that would beat that previous system 100 games to zero,\n\n1:24:22.100 --> 1:24:25.180\n and that that process would continue indefinitely\n\n1:24:25.180 --> 1:24:27.580\n throughout at least my human lifetime.\n\n1:24:27.580 --> 1:24:31.020\n Presumably the game of Go would set the ceiling.\n\n1:24:31.020 --> 1:24:31.860\n I mean.\n\n1:24:31.860 --> 1:24:33.220\n The game of Go would set the ceiling,\n\n1:24:33.220 --> 1:24:35.980\n but the game of Go has 10 to the 170 states in it.\n\n1:24:35.980 --> 1:24:40.420\n So the ceiling is unreachable by any computational device\n\n1:24:40.420 --> 1:24:44.540\n that can be built out of the 10 to the 80 atoms\n\n1:24:44.540 --> 1:24:45.380\n in the universe.\n\n1:24:46.620 --> 1:24:47.900\n You asked a really good question,\n\n1:24:47.900 --> 1:24:51.180\n which is, do you not open up other errors\n\n1:24:51.180 --> 1:24:53.660\n when you correct your previous ones?\n\n1:24:53.660 --> 1:24:56.180\n And the answer is yes, you do.\n\n1:24:56.180 --> 1:24:58.660\n And so it's a remarkable fact\n\n1:24:58.660 --> 1:25:02.260\n about this class of two player game\n\n1:25:02.260 --> 1:25:05.220\n and also true of single agent games\n\n1:25:05.220 --> 1:25:10.220\n that essentially progress will always lead you to,\n\n1:25:11.780 --> 1:25:15.100\n if you have sufficient representational resource,\n\n1:25:15.100 --> 1:25:16.620\n like imagine you had,\n\n1:25:16.620 --> 1:25:20.180\n could represent every state in a big table of the game,\n\n1:25:20.180 --> 1:25:24.060\n then we know for sure that a progress of self improvement\n\n1:25:24.060 --> 1:25:27.140\n will lead all the way in the single agent case\n\n1:25:27.140 --> 1:25:29.100\n to the optimal possible behavior,\n\n1:25:29.100 --> 1:25:31.820\n and in the two player case to the minimax optimal behavior.\n\n1:25:31.820 --> 1:25:34.420\n And that is the best way that I can play\n\n1:25:35.300 --> 1:25:38.020\n knowing that you're playing perfectly against me.\n\n1:25:38.020 --> 1:25:39.780\n And so for those cases,\n\n1:25:39.780 --> 1:25:44.700\n we know that even if you do open up some new error,\n\n1:25:44.700 --> 1:25:46.940\n that in some sense you've made progress.\n\n1:25:46.940 --> 1:25:50.460\n You're progressing towards the best that can be done.\n\n1:25:50.460 --> 1:25:55.220\n So AlphaGo was initially trained on expert games\n\n1:25:55.220 --> 1:25:56.460\n with some self play.\n\n1:25:56.460 --> 1:26:00.220\n AlphaGo Zero removed the need to be trained on expert games.\n\n1:26:00.220 --> 1:26:03.980\n And then another incredible step for me,\n\n1:26:03.980 --> 1:26:05.740\n because I just love chess,\n\n1:26:05.740 --> 1:26:09.500\n is to generalize that further to be in AlphaZero\n\n1:26:09.500 --> 1:26:12.220\n to be able to play the game of Go,\n\n1:26:12.220 --> 1:26:14.620\n beating AlphaGo Zero and AlphaGo,\n\n1:26:14.620 --> 1:26:18.140\n and then also being able to play the game of chess\n\n1:26:18.140 --> 1:26:19.140\n and others.\n\n1:26:19.140 --> 1:26:20.980\n So what was that step like?\n\n1:26:20.980 --> 1:26:23.580\n What's the interesting aspects there\n\n1:26:23.580 --> 1:26:26.660\n that required to make that happen?\n\n1:26:26.660 --> 1:26:29.940\n I think the remarkable observation,\n\n1:26:29.940 --> 1:26:31.980\n which we saw with AlphaZero,\n\n1:26:31.980 --> 1:26:35.740\n was that actually without modifying the algorithm at all,\n\n1:26:35.740 --> 1:26:37.500\n it was able to play and crack\n\n1:26:37.500 --> 1:26:41.300\n some of AI's greatest previous challenges.\n\n1:26:41.300 --> 1:26:44.780\n In particular, we dropped it into the game of chess.\n\n1:26:44.780 --> 1:26:47.180\n And unlike the previous systems like Deep Blue,\n\n1:26:47.180 --> 1:26:50.420\n which had been worked on for years and years,\n\n1:26:50.420 --> 1:26:52.660\n and we were able to beat\n\n1:26:52.660 --> 1:26:57.300\n the world's strongest computer chess program convincingly\n\n1:26:57.300 --> 1:27:00.940\n using a system that was fully discovered\n\n1:27:00.940 --> 1:27:04.940\n from scratch with its own principles.\n\n1:27:04.940 --> 1:27:08.180\n And in fact, one of the nice things that we found\n\n1:27:08.180 --> 1:27:11.540\n was that in fact, we also achieved the same result\n\n1:27:11.540 --> 1:27:13.500\n in Japanese chess, a variant of chess\n\n1:27:13.500 --> 1:27:15.180\n where you get to capture pieces\n\n1:27:15.180 --> 1:27:17.660\n and then place them back down on your own side\n\n1:27:17.660 --> 1:27:18.980\n as an extra piece.\n\n1:27:18.980 --> 1:27:21.860\n So a much more complicated variant of chess.\n\n1:27:21.860 --> 1:27:24.780\n And we also beat the world's strongest programs\n\n1:27:24.780 --> 1:27:28.020\n and reached superhuman performance in that game too.\n\n1:27:28.020 --> 1:27:32.100\n And it was the very first time that we'd ever run the system\n\n1:27:32.100 --> 1:27:34.460\n on that particular game,\n\n1:27:34.460 --> 1:27:35.860\n was the version that we published\n\n1:27:35.860 --> 1:27:38.700\n in the paper on AlphaZero.\n\n1:27:38.700 --> 1:27:41.700\n It just worked out of the box, literally, no touching it.\n\n1:27:41.700 --> 1:27:42.860\n We didn't have to do anything.\n\n1:27:42.860 --> 1:27:45.260\n And there it was, superhuman performance,\n\n1:27:45.260 --> 1:27:47.860\n no tweaking, no twiddling.\n\n1:27:47.860 --> 1:27:49.540\n And so I think there's something beautiful\n\n1:27:49.540 --> 1:27:52.980\n about that principle that you can take an algorithm\n\n1:27:52.980 --> 1:27:57.700\n and without twiddling anything, it just works.\n\n1:27:57.700 --> 1:28:02.740\n Now, to go beyond AlphaZero, what's required?\n\n1:28:02.740 --> 1:28:05.460\n AlphaZero is just a step.\n\n1:28:05.460 --> 1:28:06.940\n And there's a long way to go beyond that\n\n1:28:06.940 --> 1:28:09.940\n to really crack the deep problems of AI.\n\n1:28:10.980 --> 1:28:13.500\n But one of the important steps is to acknowledge\n\n1:28:13.500 --> 1:28:16.260\n that the world is a really messy place.\n\n1:28:16.260 --> 1:28:18.500\n It's this rich, complex, beautiful,\n\n1:28:18.500 --> 1:28:21.980\n but messy environment that we live in.\n\n1:28:21.980 --> 1:28:23.460\n And no one gives us the rules.\n\n1:28:23.460 --> 1:28:26.140\n Like no one knows the rules of the world.\n\n1:28:26.140 --> 1:28:28.500\n At least maybe we understand that it operates\n\n1:28:28.500 --> 1:28:31.180\n according to Newtonian or quantum mechanics\n\n1:28:31.180 --> 1:28:34.020\n at the micro level or according to relativity\n\n1:28:34.020 --> 1:28:35.140\n at the macro level.\n\n1:28:35.140 --> 1:28:38.420\n But that's not a model that's useful for us as people\n\n1:28:38.420 --> 1:28:40.220\n to operate in it.\n\n1:28:40.220 --> 1:28:43.780\n Somehow the agent needs to understand the world for itself\n\n1:28:43.780 --> 1:28:46.300\n in a way where no one tells it the rules of the game.\n\n1:28:46.300 --> 1:28:49.940\n And yet it can still figure out what to do in that world,\n\n1:28:50.860 --> 1:28:53.580\n deal with this stream of observations coming in,\n\n1:28:53.580 --> 1:28:55.300\n rich sensory input coming in,\n\n1:28:55.300 --> 1:28:58.300\n actions going out in a way that allows it to reason\n\n1:28:58.300 --> 1:29:01.460\n in the way that AlphaGo or AlphaZero can reason\n\n1:29:01.460 --> 1:29:03.660\n in the way that these go and chess playing programs\n\n1:29:03.660 --> 1:29:04.820\n can reason.\n\n1:29:04.820 --> 1:29:07.780\n But in a way that allows it to take actions\n\n1:29:07.780 --> 1:29:10.380\n in that messy world to achieve its goals.\n\n1:29:11.500 --> 1:29:15.260\n And so this led us to the most recent step\n\n1:29:15.260 --> 1:29:17.460\n in the story of AlphaGo,\n\n1:29:17.460 --> 1:29:19.500\n which was a system called MuZero.\n\n1:29:19.500 --> 1:29:23.380\n And MuZero is a system which learns for itself\n\n1:29:23.380 --> 1:29:25.420\n even when the rules are not given to it.\n\n1:29:25.420 --> 1:29:28.180\n It actually can be dropped into a system\n\n1:29:28.180 --> 1:29:29.700\n with messy perceptual inputs.\n\n1:29:29.700 --> 1:29:33.860\n We actually tried it in some Atari games,\n\n1:29:33.860 --> 1:29:36.540\n the canonical domains of Atari\n\n1:29:36.540 --> 1:29:38.540\n that have been used for reinforcement learning.\n\n1:29:38.540 --> 1:29:42.900\n And this system learned to build a model\n\n1:29:42.900 --> 1:29:46.940\n of these Atari games that was sufficiently rich\n\n1:29:46.940 --> 1:29:51.380\n and useful enough for it to be able to plan successfully.\n\n1:29:51.380 --> 1:29:53.500\n And in fact, that system not only went on\n\n1:29:53.500 --> 1:29:56.660\n to beat the state of the art in Atari,\n\n1:29:56.660 --> 1:29:59.300\n but the same system without modification\n\n1:29:59.300 --> 1:30:02.980\n was able to reach the same level of superhuman performance\n\n1:30:02.980 --> 1:30:06.900\n in go, chess, and shogi that we'd seen in AlphaZero,\n\n1:30:06.900 --> 1:30:08.700\n showing that even without the rules,\n\n1:30:08.700 --> 1:30:11.100\n the system can learn for itself just by trial and error,\n\n1:30:11.100 --> 1:30:13.100\n just by playing this game of go.\n\n1:30:13.100 --> 1:30:15.020\n And no one tells you what the rules are,\n\n1:30:15.020 --> 1:30:18.740\n but you just get to the end and someone says win or loss.\n\n1:30:19.580 --> 1:30:22.020\n You play this game of chess and someone says win or loss,\n\n1:30:22.020 --> 1:30:25.540\n or you play a game of breakout in Atari\n\n1:30:25.540 --> 1:30:28.020\n and someone just tells you your score at the end.\n\n1:30:28.020 --> 1:30:30.580\n And the system for itself figures out\n\n1:30:30.580 --> 1:30:31.900\n essentially the rules of the system,\n\n1:30:31.900 --> 1:30:35.180\n the dynamics of the world, how the world works.\n\n1:30:35.180 --> 1:30:39.580\n And not in any explicit way, but just implicitly,\n\n1:30:39.580 --> 1:30:41.820\n enough understanding for it to be able to plan\n\n1:30:41.820 --> 1:30:45.460\n in that system in order to achieve its goals.\n\n1:30:45.460 --> 1:30:48.020\n And that's the fundamental process\n\n1:30:48.020 --> 1:30:49.660\n that you have to go through when you're facing\n\n1:30:49.660 --> 1:30:51.500\n in any uncertain kind of environment\n\n1:30:51.500 --> 1:30:53.180\n that you would in the real world,\n\n1:30:53.180 --> 1:30:55.060\n is figuring out the sort of the rules,\n\n1:30:55.060 --> 1:30:56.540\n the basic rules of the game.\n\n1:30:56.540 --> 1:30:57.380\n That's right.\n\n1:30:57.380 --> 1:31:00.620\n So that allows it to be applicable\n\n1:31:00.620 --> 1:31:05.620\n to basically any domain that could be digitized\n\n1:31:05.860 --> 1:31:10.020\n in the way that it needs to in order to be consumable,\n\n1:31:10.020 --> 1:31:12.140\n sort of in order for the reinforcement learning framework\n\n1:31:12.140 --> 1:31:13.700\n to be able to sense the environment,\n\n1:31:13.700 --> 1:31:15.540\n to be able to act in the environment and so on.\n\n1:31:15.540 --> 1:31:16.980\n The full reinforcement learning problem\n\n1:31:16.980 --> 1:31:21.300\n needs to deal with worlds that are unknown and complex\n\n1:31:21.300 --> 1:31:23.700\n and the agent needs to learn for itself\n\n1:31:23.700 --> 1:31:24.820\n how to deal with that.\n\n1:31:24.820 --> 1:31:29.460\n And so MuZero is a further step in that direction.\n\n1:31:29.460 --> 1:31:32.180\n One of the things that inspired the general public\n\n1:31:32.180 --> 1:31:34.540\n and just in conversations I have like with my parents\n\n1:31:34.540 --> 1:31:38.300\n or something with my mom that just loves what was done\n\n1:31:38.300 --> 1:31:40.340\n is kind of at least the notion\n\n1:31:40.340 --> 1:31:42.140\n that there was some display of creativity,\n\n1:31:42.140 --> 1:31:45.860\n some new strategies, new behaviors that were created.\n\n1:31:45.860 --> 1:31:48.900\n That again has echoes of intelligence.\n\n1:31:48.900 --> 1:31:50.780\n So is there something that stands out?\n\n1:31:50.780 --> 1:31:52.940\n Do you see it the same way that there's creativity\n\n1:31:52.940 --> 1:31:57.220\n and there's some behaviors, patterns that you saw\n\n1:31:57.220 --> 1:32:00.740\n that AlphaZero was able to display that are truly creative?\n\n1:32:01.820 --> 1:32:06.660\n So let me start by saying that I think we should ask\n\n1:32:06.660 --> 1:32:08.260\n what creativity really means.\n\n1:32:08.260 --> 1:32:13.260\n So to me, creativity means discovering something\n\n1:32:13.820 --> 1:32:16.860\n which wasn't known before, something unexpected,\n\n1:32:16.860 --> 1:32:19.700\n something outside of our norms.\n\n1:32:19.700 --> 1:32:24.700\n And so in that sense, the process of reinforcement learning\n\n1:32:24.700 --> 1:32:28.580\n or the self play approach that was used by AlphaZero\n\n1:32:29.460 --> 1:32:31.780\n is the essence of creativity.\n\n1:32:31.780 --> 1:32:34.180\n It's really saying at every stage,\n\n1:32:34.180 --> 1:32:36.500\n you're playing according to your current norms\n\n1:32:36.500 --> 1:32:39.980\n and you try something and if it works out,\n\n1:32:39.980 --> 1:32:42.980\n you say, hey, here's something great,\n\n1:32:42.980 --> 1:32:44.580\n I'm gonna start using that.\n\n1:32:44.580 --> 1:32:47.180\n And then that process, it's like a micro discovery\n\n1:32:47.180 --> 1:32:49.580\n that happens millions and millions of times\n\n1:32:49.580 --> 1:32:51.580\n over the course of the algorithm's life\n\n1:32:51.580 --> 1:32:54.180\n where it just discovers some new idea,\n\n1:32:54.180 --> 1:32:56.500\n oh, this pattern, this pattern's working really well for me,\n\n1:32:56.500 --> 1:32:58.300\n I'm gonna start using that.\n\n1:32:58.300 --> 1:33:00.420\n And now, oh, here's this other thing I can do,\n\n1:33:00.420 --> 1:33:03.740\n I can start to connect these stones together in this way\n\n1:33:03.740 --> 1:33:08.660\n or I can start to sacrifice stones or give up on pieces\n\n1:33:08.660 --> 1:33:12.060\n or play shoulder hits on the fifth line or whatever it is.\n\n1:33:12.060 --> 1:33:13.940\n The system's discovering things like this for itself\n\n1:33:13.940 --> 1:33:16.740\n continually, repeatedly, all the time.\n\n1:33:16.740 --> 1:33:19.580\n And so it should come as no surprise to us then\n\n1:33:19.580 --> 1:33:21.740\n when if you leave these systems going,\n\n1:33:21.740 --> 1:33:25.740\n that they discover things that are not known to humans,\n\n1:33:25.740 --> 1:33:30.580\n that to the human norms are considered creative.\n\n1:33:30.580 --> 1:33:32.900\n And we've seen this several times.\n\n1:33:32.900 --> 1:33:35.700\n In fact, in AlphaGo Zero,\n\n1:33:35.700 --> 1:33:39.220\n we saw this beautiful timeline of discovery\n\n1:33:39.220 --> 1:33:44.020\n where what we saw was that there are these opening patterns\n\n1:33:44.020 --> 1:33:45.500\n that humans play called joseki,\n\n1:33:45.500 --> 1:33:47.820\n these are like the patterns that humans learn\n\n1:33:47.820 --> 1:33:49.660\n to play in the corners and they've been developed\n\n1:33:49.660 --> 1:33:51.940\n and refined over literally thousands of years\n\n1:33:51.940 --> 1:33:53.220\n in the game of Go.\n\n1:33:53.220 --> 1:33:56.340\n And what we saw was in the course of the training,\n\n1:33:57.220 --> 1:34:00.100\n AlphaGo Zero, over the course of the 40 days\n\n1:34:00.100 --> 1:34:01.900\n that we trained this system,\n\n1:34:01.900 --> 1:34:05.620\n it starts to discover exactly these patterns\n\n1:34:05.620 --> 1:34:06.980\n that human players play.\n\n1:34:06.980 --> 1:34:10.180\n And over time, we found that all of the joseki\n\n1:34:10.180 --> 1:34:13.180\n that humans played were discovered by the system\n\n1:34:13.180 --> 1:34:15.620\n through this process of self play\n\n1:34:15.620 --> 1:34:19.660\n and this sort of essential notion of creativity.\n\n1:34:19.660 --> 1:34:22.500\n But what was really interesting was that over time,\n\n1:34:22.500 --> 1:34:24.900\n it then starts to discard some of these\n\n1:34:24.900 --> 1:34:28.220\n in favor of its own joseki that humans didn't know about.\n\n1:34:28.220 --> 1:34:29.540\n And it starts to say, oh, well,\n\n1:34:29.540 --> 1:34:33.020\n you thought that the Knights move pincer joseki\n\n1:34:33.020 --> 1:34:34.100\n was a great idea,\n\n1:34:35.060 --> 1:34:37.060\n but here's something different you can do there\n\n1:34:37.060 --> 1:34:38.740\n which makes some new variation\n\n1:34:38.740 --> 1:34:40.380\n that humans didn't know about.\n\n1:34:40.380 --> 1:34:42.420\n And actually now the human Go players\n\n1:34:42.420 --> 1:34:44.660\n study the joseki that AlphaGo played\n\n1:34:44.660 --> 1:34:46.580\n and they become the new norms\n\n1:34:46.580 --> 1:34:51.260\n that are used in today's top level Go competitions.\n\n1:34:51.260 --> 1:34:52.540\n That never gets old.\n\n1:34:52.540 --> 1:34:54.740\n Even just the first to me,\n\n1:34:54.740 --> 1:34:58.300\n maybe just makes me feel good as a human being\n\n1:34:58.300 --> 1:35:01.900\n that a self play mechanism that knows nothing about us humans\n\n1:35:01.900 --> 1:35:04.540\n discovers patterns that we humans do.\n\n1:35:04.540 --> 1:35:06.340\n That's just like an affirmation\n\n1:35:06.340 --> 1:35:08.420\n that we're doing okay as humans.\n\n1:35:08.420 --> 1:35:09.260\n Yeah.\n\n1:35:09.260 --> 1:35:12.540\n We've, in this domain and other domains,\n\n1:35:12.540 --> 1:35:14.820\n we figured out it's like the Churchill quote\n\n1:35:14.820 --> 1:35:15.780\n about democracy.\n\n1:35:15.780 --> 1:35:18.380\n It's the, you know, it sucks,\n\n1:35:18.380 --> 1:35:20.260\n but it's the best one we've tried.\n\n1:35:20.260 --> 1:35:24.460\n So in general, taking a step outside of Go\n\n1:35:24.460 --> 1:35:27.180\n and you've like a million accomplishment\n\n1:35:27.180 --> 1:35:29.540\n that I have no time to talk about\n\n1:35:29.540 --> 1:35:32.860\n with AlphaStar and so on and the current work.\n\n1:35:32.860 --> 1:35:36.660\n But in general, this self play mechanism\n\n1:35:36.660 --> 1:35:38.180\n that you've inspired the world with\n\n1:35:38.180 --> 1:35:40.620\n by beating the world champion Go player.\n\n1:35:40.620 --> 1:35:43.820\n Do you see that as,\n\n1:35:43.820 --> 1:35:47.180\n do you see it being applied in other domains?\n\n1:35:47.180 --> 1:35:50.620\n Do you have sort of dreams and hopes\n\n1:35:50.620 --> 1:35:53.780\n that it's applied in both the simulated environments\n\n1:35:53.780 --> 1:35:56.380\n and the constrained environments of games?\n\n1:35:56.380 --> 1:35:59.020\n Constrained, I mean, AlphaStar really demonstrates\n\n1:35:59.020 --> 1:36:00.500\n that you can remove a lot of the constraints,\n\n1:36:00.500 --> 1:36:04.100\n but nevertheless, it's in a digital simulated environment.\n\n1:36:04.100 --> 1:36:07.220\n Do you have a hope, a dream that it starts being applied\n\n1:36:07.220 --> 1:36:09.100\n in the robotics environment?\n\n1:36:09.100 --> 1:36:12.940\n And maybe even in domains that are safety critical\n\n1:36:12.940 --> 1:36:15.180\n and so on and have, you know,\n\n1:36:15.180 --> 1:36:16.580\n have a real impact in the real world,\n\n1:36:16.580 --> 1:36:18.260\n like autonomous vehicles, for example,\n\n1:36:18.260 --> 1:36:21.140\n which seems like a very far out dream at this point.\n\n1:36:21.140 --> 1:36:25.540\n So I absolutely do hope and imagine\n\n1:36:25.540 --> 1:36:27.980\n that we will get to the point where ideas\n\n1:36:27.980 --> 1:36:31.140\n just like these are used in all kinds of different domains.\n\n1:36:31.140 --> 1:36:32.700\n In fact, one of the most satisfying things\n\n1:36:32.700 --> 1:36:35.340\n as a researcher is when you start to see other people\n\n1:36:35.340 --> 1:36:39.100\n use your algorithms in unexpected ways.\n\n1:36:39.100 --> 1:36:41.060\n So in the last couple of years, there have been,\n\n1:36:41.060 --> 1:36:43.180\n you know, a couple of nature papers\n\n1:36:43.180 --> 1:36:47.140\n where different teams, unbeknownst to us,\n\n1:36:47.140 --> 1:36:51.980\n took AlphaZero and applied exactly those same algorithms\n\n1:36:51.980 --> 1:36:57.580\n and ideas to real world problems of huge meaning to society.\n\n1:36:57.580 --> 1:37:00.980\n So one of them was the problem of chemical synthesis,\n\n1:37:00.980 --> 1:37:02.940\n and they were able to beat the state of the art\n\n1:37:02.940 --> 1:37:08.700\n in finding pathways of how to actually synthesize chemicals,\n\n1:37:08.700 --> 1:37:11.980\n retrochemical synthesis.\n\n1:37:11.980 --> 1:37:14.060\n And the second paper actually just came out\n\n1:37:14.060 --> 1:37:16.620\n a couple of weeks ago in Nature,\n\n1:37:16.620 --> 1:37:19.500\n showed that in quantum computation,\n\n1:37:19.500 --> 1:37:22.740\n you know, one of the big questions is how to understand\n\n1:37:22.740 --> 1:37:27.660\n the nature of the function in quantum computation\n\n1:37:27.660 --> 1:37:30.340\n and a system based on AlphaZero beat the state of the art\n\n1:37:30.340 --> 1:37:32.380\n by quite some distance there again.\n\n1:37:32.380 --> 1:37:34.060\n So these are just examples.\n\n1:37:34.060 --> 1:37:36.300\n And I think, you know, the lesson,\n\n1:37:36.300 --> 1:37:38.500\n which we've seen elsewhere in machine learning\n\n1:37:38.500 --> 1:37:42.620\n time and time again, is that if you make something general,\n\n1:37:42.620 --> 1:37:44.140\n it will be used in all kinds of ways.\n\n1:37:44.140 --> 1:37:47.340\n You know, you provide a really powerful tool to society,\n\n1:37:47.340 --> 1:37:51.700\n and those tools can be used in amazing ways.\n\n1:37:51.700 --> 1:37:53.580\n And so I think we're just at the beginning,\n\n1:37:53.580 --> 1:37:58.900\n and for sure, I hope that we see all kinds of outcomes.\n\n1:37:58.900 --> 1:38:03.340\n So the other side of the question of reinforcement\n\n1:38:03.340 --> 1:38:05.540\n learning framework is, you know,\n\n1:38:05.540 --> 1:38:07.620\n you usually want to specify a reward function\n\n1:38:07.620 --> 1:38:11.180\n and an objective function.\n\n1:38:11.180 --> 1:38:13.780\n What do you think about sort of ideas of intrinsic rewards\n\n1:38:13.780 --> 1:38:19.260\n of when we're not really sure about, you know,\n\n1:38:19.260 --> 1:38:23.660\n if we take, you know, human beings as existence proof\n\n1:38:23.660 --> 1:38:25.820\n that we don't seem to be operating\n\n1:38:25.820 --> 1:38:27.820\n according to a single reward,\n\n1:38:27.820 --> 1:38:32.100\n do you think that there's interesting ideas\n\n1:38:32.100 --> 1:38:35.540\n for when you don't know how to truly specify the reward,\n\n1:38:35.540 --> 1:38:38.140\n you know, that there's some flexibility\n\n1:38:38.140 --> 1:38:40.620\n for discovering it intrinsically or so on\n\n1:38:40.620 --> 1:38:42.700\n in the context of reinforcement learning?\n\n1:38:42.700 --> 1:38:45.020\n So I think, you know, when we think about intelligence,\n\n1:38:45.020 --> 1:38:46.740\n it's really important to be clear\n\n1:38:46.740 --> 1:38:48.380\n about the problem of intelligence.\n\n1:38:48.380 --> 1:38:51.180\n And I think it's clearest to understand that problem\n\n1:38:51.180 --> 1:38:52.660\n in terms of some ultimate goal\n\n1:38:52.660 --> 1:38:55.340\n that we want the system to try and solve for.\n\n1:38:55.340 --> 1:38:57.900\n And after all, if we don't understand the ultimate purpose\n\n1:38:57.900 --> 1:39:00.860\n of the system, do we really even have\n\n1:39:00.860 --> 1:39:04.340\n a clearly defined problem that we're solving at all?\n\n1:39:04.340 --> 1:39:10.380\n Now, within that, as with your example for humans,\n\n1:39:10.380 --> 1:39:13.980\n the system may choose to create its own motivations\n\n1:39:13.980 --> 1:39:16.340\n and subgoals that help the system\n\n1:39:16.340 --> 1:39:19.060\n to achieve its ultimate goal.\n\n1:39:19.060 --> 1:39:22.380\n And that may indeed be a hugely important mechanism\n\n1:39:22.380 --> 1:39:23.820\n to achieve those ultimate goals,\n\n1:39:23.820 --> 1:39:25.500\n but there is still some ultimate goal\n\n1:39:25.500 --> 1:39:27.060\n I think the system needs to be measurable\n\n1:39:27.060 --> 1:39:29.660\n and evaluated against.\n\n1:39:29.660 --> 1:39:31.380\n And even for humans, I mean, humans,\n\n1:39:31.380 --> 1:39:32.420\n we're incredibly flexible.\n\n1:39:32.420 --> 1:39:35.180\n We feel that we can, you know, any goal that we're given,\n\n1:39:35.180 --> 1:39:40.220\n we feel we can master to some degree.\n\n1:39:40.220 --> 1:39:41.860\n But if we think of those goals, really, you know,\n\n1:39:41.860 --> 1:39:44.860\n like the goal of being able to pick up an object\n\n1:39:44.860 --> 1:39:47.180\n or the goal of being able to communicate\n\n1:39:47.180 --> 1:39:50.980\n or influence people to do things in a particular way\n\n1:39:50.980 --> 1:39:56.940\n or whatever those goals are, really, they're subgoals,\n\n1:39:56.940 --> 1:39:58.580\n really, that we set ourselves.\n\n1:39:58.580 --> 1:40:00.900\n You know, we choose to pick up the object.\n\n1:40:00.900 --> 1:40:02.100\n We choose to communicate.\n\n1:40:02.100 --> 1:40:05.340\n We choose to influence someone else.\n\n1:40:05.340 --> 1:40:07.660\n And we choose those because we think it will lead us\n\n1:40:07.660 --> 1:40:10.460\n to something later on.\n\n1:40:10.460 --> 1:40:15.100\n We think that's helpful to us to achieve some ultimate goal.\n\n1:40:15.100 --> 1:40:18.260\n Now, I don't want to speculate whether or not humans\n\n1:40:18.260 --> 1:40:20.900\n as a system necessarily have a singular overall goal\n\n1:40:20.900 --> 1:40:23.540\n of survival or whatever it is.\n\n1:40:23.540 --> 1:40:25.660\n But I think the principle for understanding\n\n1:40:25.660 --> 1:40:28.140\n and implementing intelligence is, has to be,\n\n1:40:28.140 --> 1:40:30.100\n that if we're trying to understand intelligence\n\n1:40:30.100 --> 1:40:31.420\n or implement our own,\n\n1:40:31.420 --> 1:40:33.180\n there has to be a well defined problem.\n\n1:40:33.180 --> 1:40:37.500\n Otherwise, if it's not, I think it's like an admission\n\n1:40:37.500 --> 1:40:41.500\n of defeat, that for there to be hope for understanding\n\n1:40:41.500 --> 1:40:44.060\n or implementing intelligence, we have to know what we're doing.\n\n1:40:44.060 --> 1:40:46.420\n We have to know what we're asking the system to do.\n\n1:40:46.420 --> 1:40:48.860\n Otherwise, if you don't have a clearly defined purpose,\n\n1:40:48.860 --> 1:40:51.620\n you're not going to get a clearly defined answer.\n\n1:40:51.620 --> 1:40:56.420\n The ridiculous big question that has to naturally follow,\n\n1:40:56.420 --> 1:41:00.820\n because I have to pin you down on this thing,\n\n1:41:00.820 --> 1:41:03.340\n that nevertheless, one of the big silly\n\n1:41:03.340 --> 1:41:08.060\n or big real questions before humans is the meaning of life,\n\n1:41:08.060 --> 1:41:11.180\n is us trying to figure out our own reward function.\n\n1:41:11.180 --> 1:41:13.300\n And you just kind of mentioned that if you want to build\n\n1:41:13.300 --> 1:41:16.260\n intelligent systems and you know what you're doing,\n\n1:41:16.260 --> 1:41:18.380\n you should be at least cognizant to some degree\n\n1:41:18.380 --> 1:41:20.300\n of what the reward function is.\n\n1:41:20.300 --> 1:41:23.700\n So the natural question is what do you think\n\n1:41:23.700 --> 1:41:26.260\n is the reward function of human life,\n\n1:41:26.260 --> 1:41:29.260\n the meaning of life for us humans,\n\n1:41:29.260 --> 1:41:30.740\n the meaning of our existence?\n\n1:41:32.980 --> 1:41:36.620\n I think I'd be speculating beyond my own expertise,\n\n1:41:36.620 --> 1:41:38.460\n but just for fun, let me do that.\n\n1:41:38.460 --> 1:41:39.420\n Yes, please.\n\n1:41:39.420 --> 1:41:41.180\n And say, I think that there are many levels\n\n1:41:41.180 --> 1:41:43.020\n at which you can understand a system\n\n1:41:43.020 --> 1:41:46.420\n and you can understand something as optimizing\n\n1:41:46.420 --> 1:41:48.900\n for a goal at many levels.\n\n1:41:48.900 --> 1:41:52.540\n And so you can understand the,\n\n1:41:52.540 --> 1:41:54.500\n let's start with the universe.\n\n1:41:54.500 --> 1:41:55.780\n Does the universe have a purpose?\n\n1:41:55.780 --> 1:41:58.100\n Well, it feels like it's just at one level\n\n1:41:58.100 --> 1:42:02.340\n just following certain mechanical laws of physics\n\n1:42:02.340 --> 1:42:04.620\n and that that's led to the development of the universe.\n\n1:42:04.620 --> 1:42:08.500\n But at another level, you can view it as actually,\n\n1:42:08.500 --> 1:42:10.300\n there's the second law of thermodynamics that says\n\n1:42:10.300 --> 1:42:13.340\n that this is increasing in entropy over time forever.\n\n1:42:13.340 --> 1:42:15.380\n And now there's a view that's been developed\n\n1:42:15.380 --> 1:42:17.900\n by certain people at MIT that this,\n\n1:42:17.900 --> 1:42:20.660\n you can think of this as almost like a goal of the universe,\n\n1:42:20.660 --> 1:42:24.900\n that the purpose of the universe is to maximize entropy.\n\n1:42:24.900 --> 1:42:26.060\n So there are multiple levels\n\n1:42:26.060 --> 1:42:28.820\n at which you can understand a system.\n\n1:42:28.820 --> 1:42:30.660\n The next level down, you might say,\n\n1:42:30.660 --> 1:42:34.060\n well, if the goal is to maximize entropy,\n\n1:42:34.060 --> 1:42:39.060\n well, how can that be done by a particular system?\n\n1:42:40.020 --> 1:42:42.780\n And maybe evolution is something that the universe\n\n1:42:42.780 --> 1:42:45.900\n discovered in order to kind of dissipate energy\n\n1:42:45.900 --> 1:42:48.060\n as efficiently as possible.\n\n1:42:48.060 --> 1:42:49.940\n And by the way, I'm borrowing from Max Tegmark\n\n1:42:49.940 --> 1:42:53.900\n for some of these metaphors, the physicist.\n\n1:42:53.900 --> 1:42:55.460\n But if you can think of evolution\n\n1:42:55.460 --> 1:42:58.500\n as a mechanism for dispersing energy,\n\n1:42:59.380 --> 1:43:04.180\n then evolution, you might say, then becomes a goal,\n\n1:43:04.180 --> 1:43:06.620\n which is if evolution disperses energy\n\n1:43:06.620 --> 1:43:09.340\n by reproducing as efficiently as possible,\n\n1:43:09.340 --> 1:43:10.580\n what's evolution then?\n\n1:43:10.580 --> 1:43:13.700\n Well, it's now got its own goal within that,\n\n1:43:13.700 --> 1:43:18.700\n which is to actually reproduce as effectively as possible.\n\n1:43:19.300 --> 1:43:21.340\n And now how does reproduction,\n\n1:43:22.260 --> 1:43:25.020\n how is that made as effective as possible?\n\n1:43:25.020 --> 1:43:27.580\n Well, you need entities within that\n\n1:43:27.580 --> 1:43:29.900\n that can survive and reproduce as effectively as possible.\n\n1:43:29.900 --> 1:43:31.620\n And so it's natural that in order to achieve\n\n1:43:31.620 --> 1:43:33.860\n that high level goal, those individual organisms\n\n1:43:33.860 --> 1:43:37.700\n discover brains, intelligences,\n\n1:43:37.700 --> 1:43:42.700\n which enable them to support the goals of evolution.\n\n1:43:43.220 --> 1:43:45.340\n And those brains, what do they do?\n\n1:43:45.340 --> 1:43:47.820\n Well, perhaps the early brains,\n\n1:43:47.820 --> 1:43:51.980\n maybe they were controlling things at some direct level.\n\n1:43:51.980 --> 1:43:54.220\n Maybe they were the equivalent of preprogrammed systems,\n\n1:43:54.220 --> 1:43:57.540\n which were directly controlling what was going on\n\n1:43:57.540 --> 1:43:59.940\n and setting certain things in order\n\n1:43:59.940 --> 1:44:03.060\n to achieve these particular goals.\n\n1:44:03.060 --> 1:44:05.940\n But that led to another level of discovery,\n\n1:44:05.940 --> 1:44:07.260\n which was learning systems.\n\n1:44:07.260 --> 1:44:08.100\n There are parts of the brain\n\n1:44:08.100 --> 1:44:10.140\n which are able to learn for themselves\n\n1:44:10.140 --> 1:44:13.460\n and learn how to program themselves to achieve any goal.\n\n1:44:13.460 --> 1:44:16.580\n And presumably there are parts of the brain\n\n1:44:16.580 --> 1:44:20.340\n where goals are set to parts of that system\n\n1:44:20.340 --> 1:44:23.020\n and provides this very flexible notion of intelligence\n\n1:44:23.020 --> 1:44:25.020\n that we as humans presumably have,\n\n1:44:25.020 --> 1:44:26.820\n which is the ability to kind of,\n\n1:44:26.820 --> 1:44:30.020\n the reason we feel that we can achieve any goal.\n\n1:44:30.020 --> 1:44:32.980\n So it's a very long winded answer to say that,\n\n1:44:32.980 --> 1:44:34.700\n I think there are many perspectives\n\n1:44:34.700 --> 1:44:37.580\n and many levels at which intelligence can be understood.\n\n1:44:38.620 --> 1:44:40.460\n And at each of those levels,\n\n1:44:40.460 --> 1:44:42.220\n you can take multiple perspectives.\n\n1:44:42.220 --> 1:44:43.940\n You can view the system as something\n\n1:44:43.940 --> 1:44:45.420\n which is optimizing for a goal,\n\n1:44:45.420 --> 1:44:47.820\n which is understanding it at a level\n\n1:44:47.820 --> 1:44:49.500\n by which we can maybe implement it\n\n1:44:49.500 --> 1:44:53.340\n and understand it as AI researchers or computer scientists,\n\n1:44:53.340 --> 1:44:54.780\n or you can understand it at the level\n\n1:44:54.780 --> 1:44:56.420\n of the mechanistic thing which is going on\n\n1:44:56.420 --> 1:44:58.780\n that there are these atoms bouncing around in the brain\n\n1:44:58.780 --> 1:45:01.380\n and they lead to the outcome of that system\n\n1:45:01.380 --> 1:45:02.940\n is not in contradiction with the fact\n\n1:45:02.940 --> 1:45:07.100\n that it's also a decision making system\n\n1:45:07.100 --> 1:45:10.140\n that's optimizing for some goal and purpose.\n\n1:45:10.140 --> 1:45:14.380\n I've never heard the description of the meaning of life\n\n1:45:14.380 --> 1:45:16.860\n structured so beautifully in layers,\n\n1:45:16.860 --> 1:45:19.860\n but you did miss one layer, which is the next step,\n\n1:45:19.860 --> 1:45:21.740\n which you're responsible for,\n\n1:45:21.740 --> 1:45:26.740\n which is creating the artificial intelligence layer\n\n1:45:27.420 --> 1:45:28.260\n on top of that.\n\n1:45:28.260 --> 1:45:31.740\n And I can't wait to see, well, I may not be around,\n\n1:45:31.740 --> 1:45:36.740\n but I can't wait to see what the next layer beyond that be.\n\n1:45:36.860 --> 1:45:39.260\n Well, let's just take that argument\n\n1:45:39.260 --> 1:45:41.300\n and pursue it to its natural conclusion.\n\n1:45:41.300 --> 1:45:45.980\n So the next level indeed is for how can our learning brain\n\n1:45:46.860 --> 1:45:49.180\n achieve its goals most effectively?\n\n1:45:49.180 --> 1:45:53.700\n Well, maybe it does so by us as learning beings\n\n1:45:56.180 --> 1:46:00.180\n building a system which is able to solve for those goals\n\n1:46:00.180 --> 1:46:02.180\n more effectively than we can.\n\n1:46:02.180 --> 1:46:05.140\n And so when we build a system to play the game of Go,\n\n1:46:05.140 --> 1:46:06.940\n when I said that I wanted to build a system\n\n1:46:06.940 --> 1:46:08.740\n that can play Go better than I can,\n\n1:46:08.740 --> 1:46:12.180\n I've enabled myself to achieve that goal of playing Go\n\n1:46:12.180 --> 1:46:14.500\n better than I could by directly playing it\n\n1:46:14.500 --> 1:46:15.820\n and learning it myself.\n\n1:46:15.820 --> 1:46:18.740\n And so now a new layer has been created,\n\n1:46:18.740 --> 1:46:21.260\n which is systems which are able to achieve goals\n\n1:46:21.260 --> 1:46:22.620\n for themselves.\n\n1:46:22.620 --> 1:46:25.060\n And ultimately there may be layers beyond that\n\n1:46:25.060 --> 1:46:28.500\n where they set sub goals to parts of their own system\n\n1:46:28.500 --> 1:46:32.980\n in order to achieve those and so forth.\n\n1:46:32.980 --> 1:46:36.700\n So the story of intelligence, I think,\n\n1:46:36.700 --> 1:46:39.980\n is a multi layered one and a multi perspective one.\n\n1:46:39.980 --> 1:46:41.980\n We live in an incredible universe.\n\n1:46:41.980 --> 1:46:43.980\n David, thank you so much, first of all,\n\n1:46:43.980 --> 1:46:47.900\n for dreaming of using learning to solve Go\n\n1:46:47.900 --> 1:46:50.100\n and building intelligent systems\n\n1:46:50.100 --> 1:46:52.260\n and for actually making it happen\n\n1:46:52.260 --> 1:46:56.100\n and for inspiring millions of people in the process.\n\n1:46:56.100 --> 1:46:57.060\n It's truly an honor.\n\n1:46:57.060 --> 1:46:58.500\n Thank you so much for talking today.\n\n1:46:58.500 --> 1:46:59.940\n Okay, thank you.\n\n1:46:59.940 --> 1:47:01.300\n Thanks for listening to this conversation\n\n1:47:01.300 --> 1:47:04.060\n with David Silver and thank you to our sponsors,\n\n1:47:04.060 --> 1:47:05.980\n Masterclass and Cash App.\n\n1:47:05.980 --> 1:47:07.740\n Please consider supporting the podcast\n\n1:47:07.740 --> 1:47:12.100\n by signing up to Masterclass at masterclass.com slash Lex\n\n1:47:12.100 --> 1:47:15.740\n and downloading Cash App and using code LexPodcast.\n\n1:47:15.740 --> 1:47:18.020\n If you enjoy this podcast, subscribe on YouTube,\n\n1:47:18.020 --> 1:47:20.260\n review it with five stars on Apple Podcast,\n\n1:47:20.260 --> 1:47:21.420\n support it on Patreon,\n\n1:47:21.420 --> 1:47:25.260\n or simply connect with me on Twitter at LexFriedman.\n\n1:47:25.260 --> 1:47:28.700\n And now let me leave you with some words from David Silver.\n\n1:47:28.700 --> 1:47:31.300\n My personal belief is that we've seen something\n\n1:47:31.300 --> 1:47:34.460\n of a turning point where we're starting to understand\n\n1:47:34.460 --> 1:47:38.180\n that many abilities like intuition and creativity\n\n1:47:38.180 --> 1:47:40.820\n that we've previously thought were in the domain only\n\n1:47:40.820 --> 1:47:43.340\n of the human mind are actually accessible\n\n1:47:43.340 --> 1:47:45.500\n to machine intelligence as well.\n\n1:47:45.500 --> 1:47:48.220\n And I think that's a really exciting moment in history.\n\n1:47:48.220 --> 1:48:00.700\n Thank you for listening and hope to see you next time.\n\n"
}