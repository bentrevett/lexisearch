{
  "title": "Jeff Hawkins: The Thousand Brains Theory of Intelligence | Lex Fridman Podcast #208",
  "id": "Z1KwkpTUbkg",
  "transcript": "WEBVTT\n\n00:00.000 --> 00:05.440\n The following is a conversation with Jeff Hawkins, a neuroscientist seeking to understand\n\n00:05.440 --> 00:09.360\n the structure, function, and origin of intelligence in the human brain.\n\n00:10.080 --> 00:16.720\n He previously wrote a seminal book on the subject titled On Intelligence, and recently a new book\n\n00:16.720 --> 00:22.560\n called A Thousand Brains, which presents a new theory of intelligence that Richard Dawkins,\n\n00:22.560 --> 00:28.400\n for example, has been raving about, calling the book quote brilliant and exhilarating.\n\n00:28.400 --> 00:33.600\n I can't read those two words and not think of him saying it in his British accent.\n\n00:34.160 --> 00:41.280\n Quick mention of our sponsors, Codecademy, Biooptimizers, ExpressVPN, Asleep, and Blinkist.\n\n00:41.280 --> 00:44.080\n Check them out in the description to support this podcast.\n\n00:44.560 --> 00:49.360\n As a side note, let me say that one small but powerful idea that Jeff Hawkins mentions\n\n00:49.360 --> 00:54.960\n in his new book is that if human civilization were to destroy itself, all of knowledge,\n\n00:54.960 --> 01:00.880\n all our creations will go with us. He proposes that we should think about how to save that\n\n01:00.880 --> 01:07.040\n knowledge in a way that long outlives us, whether that's on Earth, in orbit around Earth,\n\n01:07.040 --> 01:13.040\n or in deep space, and then to send messages that advertise this backup of human knowledge\n\n01:13.040 --> 01:19.600\n to other intelligent alien civilizations. The main message of this advertisement is not that\n\n01:19.600 --> 01:28.240\n we are here, but that we were once here. This little difference somehow was deeply humbling\n\n01:28.240 --> 01:34.960\n to me, that we may, with some nonzero likelihood, destroy ourselves, and that an alien civilization\n\n01:34.960 --> 01:40.240\n thousands or millions of years from now may come across this knowledge store, and they\n\n01:40.240 --> 01:45.360\n would only with some low probability even notice it, not to mention be able to interpret it.\n\n01:45.360 --> 01:49.840\n And the deeper question here for me is what information in all of human knowledge is even\n\n01:49.840 --> 01:55.600\n essential? Does Wikipedia capture it or not at all? This thought experiment forces me\n\n01:55.600 --> 02:00.400\n to wonder what are the things we've accomplished and are hoping to still accomplish that will\n\n02:00.400 --> 02:08.560\n outlive us? Is it things like complex buildings, bridges, cars, rockets? Is it ideas like science,\n\n02:08.560 --> 02:15.440\n physics, and mathematics? Is it music and art? Is it computers, computational systems,\n\n02:15.440 --> 02:20.800\n or even artificial intelligence systems? I personally can't imagine that aliens wouldn't\n\n02:20.800 --> 02:27.120\n already have all of these things, in fact much more and much better. To me, the only\n\n02:27.120 --> 02:32.560\n unique thing we may have is consciousness itself, and the actual subjective experience\n\n02:32.560 --> 02:39.200\n and the actual subjective experience of suffering, of happiness, of hatred, of love. If we can\n\n02:39.200 --> 02:44.000\n record these experiences in the highest resolution directly from the human brain, such that aliens\n\n02:44.000 --> 02:49.760\n will be able to replay them, that is what we should store and send as a message. Not\n\n02:49.760 --> 02:56.640\n Wikipedia, but the extremes of conscious experiences, the most important of which, of course, is\n\n02:56.640 --> 03:02.880\n love. This is the Lex Friedman podcast, and here is my conversation with Jeff Hawkins.\n\n03:04.080 --> 03:09.760\n We previously talked over two years ago. Do you think there's still neurons in your brain\n\n03:09.760 --> 03:15.600\n that remember that conversation, that remember me and got excited? Like there's a Lex neuron\n\n03:15.600 --> 03:19.920\n in your brain that just like finally has a purpose? I do remember our conversation. I\n\n03:19.920 --> 03:26.480\n have some memories of it, and I formed additional memories of you in the meantime. I wouldn't\n\n03:26.480 --> 03:31.360\n say there's a neuron or neurons in my brain that know you. There are synapses in my brain\n\n03:31.360 --> 03:36.800\n that have formed that reflect my knowledge of you and the model I have of you in the\n\n03:36.800 --> 03:41.520\n world. Whether the exact same synapses were formed two years ago, it's hard to say because\n\n03:41.520 --> 03:46.480\n these things come and go all the time. One of the things to know about brains is that\n\n03:46.480 --> 03:50.400\n when you think of things, you often erase the memory and rewrite it again. Yes, but I have\n\n03:50.400 --> 03:55.360\n a memory of you, and that's instantiated in synapses. There's a simpler way to think about\n\n03:55.360 --> 04:02.400\n it. We have a model of the world in your head, and that model is continually being updated.\n\n04:02.400 --> 04:06.640\n I updated this morning. You offered me this water. You said it was from the refrigerator.\n\n04:07.200 --> 04:12.960\n I remember these things. The model includes where we live, the places we know, the words,\n\n04:12.960 --> 04:16.960\n the objects in the world. It's a monstrous model, and it's constantly being updated.\n\n04:17.600 --> 04:23.360\n People are just part of that model. Our animals, our other physical objects, our events we've\n\n04:23.360 --> 04:33.440\n done. In my mind, it's no special place for the memories of humans. Obviously, I know a lot about\n\n04:33.440 --> 04:41.120\n my wife and friends and so on, but it's not like a special place for humans or over here.\n\n04:41.920 --> 04:46.640\n We model everything, and we model other people's behaviors too. If I said there's a copy of your\n\n04:46.640 --> 04:53.280\n mind in my mind, it's just because I've learned how humans behave, and I've learned some things\n\n04:53.280 --> 05:00.560\n about you, and that's part of my world model. Well, I just also mean the collective intelligence\n\n05:00.560 --> 05:08.480\n of the human species. I wonder if there's something fundamental to the brain that enables that,\n\n05:08.480 --> 05:13.600\n so modeling other humans with their ideas. You're actually jumping into a lot of big\n\n05:13.600 --> 05:17.440\n topics. Collective intelligence is a separate topic that a lot of people like to talk about.\n\n05:17.440 --> 05:24.640\n We could talk about that. That's interesting. We're not just individuals. We live in society\n\n05:24.640 --> 05:30.960\n and so on. From our research point of view, again, let's just talk. We studied the neocortex.\n\n05:30.960 --> 05:37.040\n It's a sheet of neural tissue. It's about 75% of your brain. It runs on this very repetitive\n\n05:37.040 --> 05:44.000\n algorithm. It's a very repetitive circuit. You can apply that algorithm to lots of different\n\n05:44.000 --> 05:48.640\n problems, but underneath, it's the same thing. We're just building this model. From our point\n\n05:48.640 --> 05:52.720\n of view, we wouldn't look for these special circuits someplace buried in your brain that\n\n05:52.720 --> 05:58.640\n might be related to understanding other humans. It's more like, how do we build a model of\n\n05:58.640 --> 06:02.080\n anything? How do we understand anything in the world? Humans are just another part of\n\n06:02.080 --> 06:08.720\n the things we understand. There's nothing to the brain that knows the\n\n06:08.720 --> 06:13.120\n emergent phenomena of collective intelligence. Well, I certainly know about that. I've heard\n\n06:13.120 --> 06:16.800\n the terms, I've read. No, but that's as an idea.\n\n06:16.800 --> 06:21.920\n Well, I think we have language, which is built into our brains. That's a key part of collective\n\n06:21.920 --> 06:27.680\n intelligence. There are some prior assumptions about the world we're going to live in. When\n\n06:27.680 --> 06:35.520\n we're born, we're not just a blank slate. Did we evolve to take advantage of those situations?\n\n06:35.520 --> 06:39.040\n Yes. Again, we study only part of the brain, the neocortex. There's other parts of the\n\n06:39.040 --> 06:45.600\n brain that are very much involved in societal interactions and human emotions and how we\n\n06:45.600 --> 06:53.280\n interact and even societal issues about how we interact with other people, when we support\n\n06:53.280 --> 06:58.240\n them, when we're greedy and things like that. Certainly, the brain is a great place\n\n07:00.160 --> 07:06.720\n where to study intelligence. I wonder if it's the fundamental atom of intelligence.\n\n07:06.720 --> 07:12.000\n Well, I would say it's absolutely in a central component, even if you believe in collective\n\n07:12.000 --> 07:16.320\n intelligence as, hey, that's where it's all happening. That's what we need to study,\n\n07:16.320 --> 07:19.120\n which I don't believe that, by the way. I think it's really important, but I don't think that\n\n07:19.120 --> 07:26.080\n is the thing. Even if you do believe that, then you have to understand how the brain works in\n\n07:26.080 --> 07:32.880\n doing that. It's more like we are intelligent individuals and together, we are much more\n\n07:32.880 --> 07:37.200\n magnified, our intelligence. We can do things that we couldn't do individually, but even as\n\n07:37.200 --> 07:42.000\n individuals, we're pretty damn smart and we can model things and understand the world and interact\n\n07:42.000 --> 07:48.160\n with it. To me, if you're going to start someplace, you need to start with the brain. Then you could\n\n07:48.160 --> 07:53.760\n say, well, how do brains interact with each other? What is the nature of language? How do we share\n\n07:53.760 --> 07:56.960\n models that I've learned something about the world, how do I share it with you? Which is really\n\n07:56.960 --> 08:02.800\n what sort of communal intelligence is. I know something, you know something. We've had different\n\n08:02.800 --> 08:06.800\n experiences in the world. I've learned something about brains. Maybe I can impart that to you. You've\n\n08:06.800 --> 08:15.200\n learned something about physics and you can impart that to me. Even just the epistemological\n\n08:15.200 --> 08:20.880\n question of, well, what is knowledge and how do you represent it in the brain? That's where it's\n\n08:20.880 --> 08:27.280\n going to reside for in our writings. It's obvious that human collaboration, human interaction\n\n08:27.280 --> 08:33.280\n is how we build societies. But some of the things you talk about and work on,\n\n08:34.560 --> 08:40.560\n some of those elements of what makes up an intelligent entity is there with a single person.\n\n08:40.560 --> 08:47.040\n Absolutely. I mean, we can't deny that the brain is the core element here. At least I think it's\n\n08:47.040 --> 08:51.920\n obvious. The brain is the core element in all theories of intelligence. It's where knowledge\n\n08:51.920 --> 08:58.080\n is represented. It's where knowledge is created. We interact, we share, we build upon each other's\n\n08:58.080 --> 09:03.920\n work. But without a brain, you'd have nothing. There would be no intelligence without brains.\n\n09:03.920 --> 09:10.000\n And so that's where we start. I got into this field because I just was curious as to who I am.\n\n09:11.520 --> 09:15.840\n How do I think? What's going on in my head when I'm thinking? What does it mean to know something?\n\n09:16.560 --> 09:21.200\n I can ask what it means for me to know something independent of how I learned it from you or from\n\n09:21.200 --> 09:25.600\n someone else or from society. What does it mean for me to know that I have a model of you in my\n\n09:25.600 --> 09:28.880\n head? What does it mean to know I know what this microphone does and how it works physically,\n\n09:28.880 --> 09:34.480\n even when I can't see right now? How do I know that? What does it mean? How the neurons do that\n\n09:34.480 --> 09:39.680\n at the fundamental level of neurons and synapses and so on? Those are really fascinating questions.\n\n09:40.240 --> 09:44.400\n And I'm happy to be just happy to understand those if I could.\n\n09:44.400 --> 09:54.640\n So in your new book, you talk about our brain, our mind as being made up of many brains.\n\n09:55.920 --> 10:01.600\n So the book is called A Thousand Brain Theory of Intelligence. What is the key idea of this book?\n\n10:02.720 --> 10:09.360\n The book has three sections and it has sort of maybe three big ideas. So the first section is\n\n10:09.360 --> 10:13.760\n all about what we've learned about the neocortex and that's the thousand brains theory. Just to\n\n10:13.760 --> 10:16.960\n complete the picture, the second section is all about AI and the third section is about the future\n\n10:16.960 --> 10:26.720\n of humanity. So the thousand brains theory, the big idea there, if I had to summarize into one\n\n10:27.440 --> 10:33.440\n big idea, is that we think of the brain, the neocortex as learning this model of the world.\n\n10:33.440 --> 10:38.560\n But what we learned is actually there's tens of thousands of independent modeling systems going\n\n10:38.560 --> 10:44.560\n on. And so each, we call the column in the cortex is about 150,000 of them, is a complete modeling\n\n10:44.560 --> 10:50.320\n system. So it's a collective intelligence in your head in some sense. So the thousand brains theory\n\n10:50.320 --> 10:55.760\n says, well, where do I have knowledge about this coffee cup or where's the model of this cell phone?\n\n10:55.760 --> 10:59.360\n It's not in one place. It's in thousands of separate models that are complimentary and\n\n10:59.360 --> 11:04.240\n they communicate with each other through voting. So this idea that we feel like we're one person,\n\n11:04.240 --> 11:09.920\n that's our experience. We can explain that. But reality, there's lots of these, it's almost like\n\n11:09.920 --> 11:16.320\n little brains, but they're sophisticated modeling systems, about 150,000 of them in each human\n\n11:16.320 --> 11:21.280\n brain. And that's a total different way of thinking about how the neocortex is structured\n\n11:21.280 --> 11:26.400\n than we or anyone else thought of even just five years ago. So you mentioned you started\n\n11:27.280 --> 11:31.840\n this journey just looking in the mirror and trying to understand who you are.\n\n11:31.840 --> 11:38.080\n So if you have many brains, who are you then? So it's interesting. We have a singular perception,\n\n11:38.080 --> 11:42.560\n right? We think, oh, I'm just here. I'm looking at you. But it's composed of all these things,\n\n11:42.560 --> 11:48.080\n like there's sounds and there's vision and there's touch and all kinds of inputs. Yeah,\n\n11:48.080 --> 11:51.920\n we have the singular perception. And what the thousand brain theory says, we have these models\n\n11:51.920 --> 11:55.040\n that are visual models. We have a lot of models that are auditory models, models that talk to\n\n11:55.040 --> 12:01.200\n models and so on, but they vote. And so these things in the cortex, you can think about these\n\n12:01.200 --> 12:07.920\n columns as like little grains of rice, 150,000 stacked next to each other. And each one is its\n\n12:07.920 --> 12:11.840\n own little modeling system, but they have these long range connections that go between them.\n\n12:12.640 --> 12:20.080\n And we call those voting connections or voting neurons. And so the different columns try to\n\n12:20.080 --> 12:24.640\n reach a consensus. Like, what am I looking at? Okay. Each one has some ambiguity, but they come\n\n12:24.640 --> 12:30.640\n to a consensus. Oh, there's a water bottle I'm looking at. We are only consciously able to\n\n12:30.640 --> 12:35.680\n perceive the voting. We're not able to perceive anything that goes on under the hood. So the\n\n12:35.680 --> 12:39.920\n voting is what we're aware of. The results of the vote.\n\n12:39.920 --> 12:44.560\n Yeah. Well, you can imagine it this way. We were just talking about eye movements a moment ago. So\n\n12:44.560 --> 12:49.120\n as I'm looking at something, my eyes are moving about three times a second. And with each movement,\n\n12:49.120 --> 12:53.280\n a completely new input is coming into the brain. It's not repetitive. It's not shifting it around.\n\n12:54.480 --> 12:58.960\n I'm totally unaware of it. I can't perceive it. But yet if I looked at the neurons in your brain,\n\n12:58.960 --> 13:02.480\n they're going on and off, on and off, on and off, on and off. But the voting neurons are not.\n\n13:03.040 --> 13:06.240\n The voting neurons are saying, we all agree, even though I'm looking at different parts of this,\n\n13:06.240 --> 13:11.360\n this is a water bottle right now. And that's not changing. And it's in some position and\n\n13:11.360 --> 13:15.520\n pose relative to me. So I have this perception of the water bottle about two feet away from me\n\n13:15.520 --> 13:20.480\n at a certain pose to me. That is not changing. That's the only part I'm aware of. I can't be\n\n13:20.480 --> 13:25.040\n aware of the fact that the inputs from the eyes are moving and changing and all this other tapping.\n\n13:25.040 --> 13:31.200\n So these long range connections are the part we can be conscious of. The individual activity in\n\n13:31.200 --> 13:37.840\n each column doesn't go anywhere else. It doesn't get shared anywhere else. There's no way to extract\n\n13:37.840 --> 13:43.600\n it and talk about it or extract it and even remember it to say, oh, yes, I can recall that.\n\n13:45.200 --> 13:50.160\n But these long range connections are the things that are accessible to language and to our,\n\n13:50.160 --> 13:56.640\n like the hippocampus, our memories, our short term memory systems and so on. So we're not aware of\n\n13:56.640 --> 14:02.960\n 95% or maybe it's even 98% of what's going on in your brain. We're only aware of this sort of\n\n14:02.960 --> 14:09.280\n stable, somewhat stable voting outcome of all these things that are going on underneath the hood.\n\n14:09.920 --> 14:15.520\n So what would you say is the basic element in the thousand brains theory of intelligence\n\n14:15.520 --> 14:21.040\n of intelligence? Like what's the atom of intelligence when you think about it? Is it\n\n14:21.040 --> 14:25.920\n the individual brains and then what is a brain? Well, let's, let's, can we just talk about what\n\n14:25.920 --> 14:31.440\n intelligence is first and then, and then we can talk about the elements are. So in my, in my book,\n\n14:31.440 --> 14:37.600\n intelligence is the ability to learn a model of the world, to build internal to your head,\n\n14:38.560 --> 14:42.720\n a model that represents the structure of everything, you know, to know what this is a\n\n14:42.720 --> 14:47.200\n table and that's a coffee cup and this is a gooseneck lamp and all this to know these things.\n\n14:47.200 --> 14:50.240\n I have to have a model of it in my head. I just don't look at them and go, what is that?\n\n14:50.720 --> 14:55.680\n I already have internal representations of these things in my head and I had to learn them. I wasn't\n\n14:55.680 --> 15:00.320\n born of any of that knowledge. You were, you know, we have some lights in the room here. I, you know,\n\n15:00.320 --> 15:05.360\n that's not part of my evolutionary heritage, right? It's not in my genes. So, um, we have this\n\n15:05.360 --> 15:09.040\n incredible model and the model includes not only what things look like and feel like, but where\n\n15:09.040 --> 15:12.800\n they are relative to each other and how they behave. I've never picked up this water bottle\n\n15:12.800 --> 15:16.240\n before, but I know that if I took my hand on that blue thing and I turn it, it'll probably make a\n\n15:16.240 --> 15:20.720\n funny little sound as the little plastic things detach and then it'll rotate and it'll rotate a\n\n15:20.720 --> 15:23.680\n certain way and it'll come off. How do I know that? Because I have this model in my head.\n\n15:24.480 --> 15:29.360\n So the essence of intelligence as our ability to learn a model and the more sophisticated our\n\n15:29.360 --> 15:34.880\n model is, the smarter we are. Uh, not that there is a single intelligence, because you can know\n\n15:34.880 --> 15:37.680\n about, you know, a lot about things that I don't know. And I know about things you don't know.\n\n15:37.680 --> 15:42.080\n And we can both be very smart, but we both learned a model of the world through interacting with it.\n\n15:42.080 --> 15:46.320\n So that is the essence of intelligence. Then we can ask ourselves, what are the mechanisms in the\n\n15:46.320 --> 15:50.560\n brain that allow us to do that? And what are the mechanisms of learning, not just the neural\n\n15:50.560 --> 15:54.800\n mechanisms, what are the general process by how we learn a model? So that was a big insight for us.\n\n15:54.800 --> 15:59.840\n It's like, what are the, what is the actual things that, how do you learn this stuff? It turns out\n\n15:59.840 --> 16:04.000\n you have to learn it through movement. Um, you can't learn it just by that's how we learn. We\n\n16:04.000 --> 16:07.840\n learn through movement. We learn. Um, so you build up this model by observing things and\n\n16:07.840 --> 16:11.680\n touching them and moving them and walking around the world and so on. So either you move or the\n\n16:11.680 --> 16:16.960\n thing moves somehow. Yeah. You obviously can learn things just by reading a book, something like that.\n\n16:16.960 --> 16:21.120\n But think about if I were to say, oh, here's a new house. I want you to learn, you know,\n\n16:21.120 --> 16:25.440\n what do you do? You have to walk, you have to walk from room to the room. You have to open the doors,\n\n16:25.440 --> 16:29.680\n look around, see what's on the left, what's on the right. As you do this, you're building a model in\n\n16:29.680 --> 16:34.000\n your head. It's just, that's what you're doing. You can't just sit there and say, I'm going to grok\n\n16:34.000 --> 16:37.360\n the house. No. You know, or you can do it. You don't even want to just sit down and read some\n\n16:37.360 --> 16:41.600\n description of it, right? Yeah. You literally physically interact. The same with like a smartphone.\n\n16:41.600 --> 16:45.760\n If I'm going to learn a new app, I touch it and I move things around. I see what happens when I,\n\n16:45.760 --> 16:49.600\n when I do things with it. So that's the basic way we learn in the world. And by the way,\n\n16:49.600 --> 16:54.720\n when you say model, you mean something that can be used for prediction in the future.\n\n16:54.720 --> 17:01.200\n It's used for prediction and for behavior and planning. Right. And does a pretty good job\n\n17:02.000 --> 17:06.560\n doing so. Yeah. Here's the way to think about the model. A lot of people get hung up on this. So\n\n17:08.320 --> 17:13.360\n you can imagine an architect making a model of a house, right? So there's a physical model that's\n\n17:13.360 --> 17:17.520\n small. And why do they do that? Well, we do that because you can imagine what it would look like\n\n17:17.520 --> 17:21.200\n from different angles. Okay. Look from here, look from there. And you can also say, well,\n\n17:21.200 --> 17:25.760\n how, how far to get from the garage to the, to the swimming pool or something like that. Right. You\n\n17:25.760 --> 17:29.120\n can imagine looking at this and you can say, what would be the view from this location? So we build\n\n17:29.120 --> 17:34.720\n these physical models to let you imagine the future and imagine behaviors. Now we can take\n\n17:34.720 --> 17:39.840\n that same model and put it in a computer. So we now, today they'll build models of houses in a\n\n17:39.840 --> 17:45.840\n computer and they, and they do that using a set of, we'll come back to this term in a moment,\n\n17:45.840 --> 17:49.680\n reference frames, but basically you assign a reference frame for the palace and you assign\n\n17:49.680 --> 17:53.280\n different things for the house in different locations. And then the computer can generate\n\n17:53.280 --> 17:56.960\n an image and say, okay, this is what it looks like in this direction. The brain is doing something\n\n17:56.960 --> 18:02.160\n remarkably similar to this surprising. It's using reference frames. It's building these,\n\n18:02.160 --> 18:06.160\n it's similar to a model on a computer, which has the same benefits of building a physical model.\n\n18:06.160 --> 18:10.480\n It allows me to say, what would this thing look like if it was in this orientation? What would\n\n18:10.480 --> 18:15.120\n likely happen if I push this button? I've never pushed this button before, or how would I accomplish\n\n18:15.120 --> 18:21.520\n something? I want to, I want to convey a new idea I've learned. How would I do that? I can imagine\n\n18:21.520 --> 18:27.520\n in my head, well, I could talk about it. I could write a book. I could do some podcasts. I could,\n\n18:28.400 --> 18:32.720\n you know, maybe tell my neighbor, you know, and I can imagine the outcomes of all these things\n\n18:32.720 --> 18:36.880\n before I do any of them. That's what the model lets you do. It lets us plan the future and\n\n18:36.880 --> 18:42.720\n imagine the consequences of our actions. Prediction, you asked about prediction. Prediction\n\n18:42.720 --> 18:48.800\n is not the goal of the model. Prediction is an inherent property of it, and it's how the model\n\n18:48.800 --> 18:55.040\n corrects itself. So prediction is fundamental to intelligence. It's fundamental to building a model,\n\n18:55.600 --> 19:00.000\n and the model's intelligent. And let me go back and be very precise about this. Prediction,\n\n19:00.000 --> 19:03.520\n you can think of prediction two ways. One is like, hey, what would happen if I did this? That's a\n\n19:03.520 --> 19:07.920\n type of prediction. That's a key part of intelligence. But using prediction is like, oh,\n\n19:07.920 --> 19:13.120\n what's this water bottle going to feel like when I pick it up, you know? And that doesn't seem very\n\n19:13.120 --> 19:20.400\n intelligent. But one way to think about prediction is it's a way for us to learn where our model is\n\n19:20.400 --> 19:26.080\n wrong. So if I picked up this water bottle and it felt hot, I'd be very surprised. Or if I picked\n\n19:26.080 --> 19:32.720\n it up and it was very light, I'd be surprised. Or if I turned this top and I had to turn it the other\n\n19:32.720 --> 19:38.480\n way, I'd be surprised. And so all those might have a prediction like, okay, I'm going to do it. I'll\n\n19:38.480 --> 19:42.720\n drink some water. I'm okay. Okay, I do this. There it is. I feel opening, right? What if I had to turn\n\n19:42.720 --> 19:47.360\n it the other way? Or what if it's split in two? Then I say, oh my gosh, I misunderstood this. I\n\n19:47.360 --> 19:50.400\n didn't have the right model of this thing. My attention would be drawn to it. I'd be looking at\n\n19:50.400 --> 19:55.360\n it going, well, how the hell did that happen? Why did it open up that way? And I would update my\n\n19:55.360 --> 19:58.400\n model by doing it. Just by looking at it and playing around with that update and say, this is\n\n19:58.400 --> 20:05.040\n a new type of water bottle. So you're talking about sort of complicated things like a water bottle,\n\n20:05.040 --> 20:10.640\n but this also applies for just basic vision, just like seeing things. It's almost like a\n\n20:10.640 --> 20:17.200\n precondition of just perceiving the world is predicting it. So just everything that you see\n\n20:18.000 --> 20:23.600\n is first passed through your prediction. Everything you see and feel. In fact,\n\n20:23.600 --> 20:31.680\n this was the insight I had back in the early 80s. And I know that people have reached the same idea\n\n20:31.680 --> 20:37.600\n is that every sensory input you get, not just vision, but touch and hearing, you have an\n\n20:37.600 --> 20:43.440\n expectation about it and a prediction. Sometimes you can predict very accurately. Sometimes you\n\n20:43.440 --> 20:47.520\n can't. I can't predict what next word is going to come out of your mouth. But as you start talking,\n\n20:47.520 --> 20:51.440\n I'll get better and better predictions. And if you talk about some topics, I'd be very surprised.\n\n20:51.440 --> 20:57.040\n So I have this sort of background prediction that's going on all the time for all of my senses.\n\n20:58.160 --> 21:04.080\n Again, the way I think about that is this is how we learn. It's more about how we learn.\n\n21:04.960 --> 21:10.000\n It's a test of our understanding. Our predictions are a test. Is this really a water bottle? If it\n\n21:10.000 --> 21:14.960\n is, I shouldn't see a little finger sticking out the side. And if I saw a little finger sticking\n\n21:14.960 --> 21:20.480\n out, I was like, oh, what the hell's going on? That's not normal. I mean, that's fascinating\n\n21:20.480 --> 21:27.760\n that... Let me linger on this for a second. It really honestly feels that prediction is\n\n21:27.760 --> 21:35.760\n fundamental to everything, to the way our mind operates, to intelligence. So it's just a different\n\n21:35.760 --> 21:41.040\n way to see intelligence, which is like everything starts a prediction. And prediction requires a\n\n21:41.040 --> 21:46.880\n model. You can't predict something unless you have a model of it. Right. But the action is\n\n21:46.880 --> 21:53.280\n prediction. So the thing the model does is prediction. But it also... Yeah. But you can\n\n21:53.280 --> 21:59.600\n then extend it to things like, oh, what would happen if I took this today? I went and did this.\n\n21:59.600 --> 22:04.320\n What would be likely? Or how... You can extend prediction to like, oh, I want to get a promotion\n\n22:04.320 --> 22:09.280\n at work. What action should I take? And you can say, if I did this, I predict what might happen.\n\n22:09.280 --> 22:13.360\n If I spoke to someone, I predict what might happen. So it's not just low level predictions.\n\n22:13.360 --> 22:17.440\n Yeah. It's all predictions. It's all predictions. It's like this black box so you can ask basically\n\n22:17.440 --> 22:20.880\n any question, low level or high level. So we started off with that observation. It's\n\n22:21.920 --> 22:27.120\n this nonstop prediction. And I write about this in the book. And then we asked, how do neurons\n\n22:27.120 --> 22:30.960\n actually make predictions physically? Like what does the neuron do when it makes a prediction?\n\n22:32.400 --> 22:35.760\n Or the neural tissue does when it makes a prediction. And then we asked, what are the\n\n22:35.760 --> 22:40.400\n mechanisms by how we build a model that allows you to make predictions? So we started with prediction\n\n22:40.400 --> 22:47.520\n as sort of the fundamental research agenda, if in some sense. And say, well, we understand how\n\n22:47.520 --> 22:51.360\n the brain makes predictions. We'll understand how it builds these models and how it learns.\n\n22:51.360 --> 22:55.680\n And that's the core of intelligence. So it was the key that got us in the door\n\n22:55.680 --> 22:58.800\n to say, that is our research agenda. Understand predictions.\n\n22:59.360 --> 23:05.200\n So in this whole process, where does intelligence originate, would you say?\n\n23:05.200 --> 23:12.560\n So if we look at things that are much less intelligence to humans and you start to build\n\n23:12.560 --> 23:19.920\n up a human through the process of evolution, where's this magic thing that has a prediction\n\n23:19.920 --> 23:24.720\n model or a model that's able to predict that starts to look a lot more like intelligence?\n\n23:24.720 --> 23:30.960\n Is there a place where Richard Dawkins wrote an introduction to your book, an excellent\n\n23:30.960 --> 23:36.320\n introduction? I mean, it's, it puts a lot of things into context and it's funny just looking\n\n23:36.320 --> 23:42.080\n at parallels for your book and Darwin's Origin of Species. So Darwin wrote about the origin\n\n23:42.640 --> 23:47.760\n of species. So what is the origin of intelligence?\n\n23:47.760 --> 23:52.640\n Well, we have a theory about it and it's just that, it's a theory. The theory goes as follows.\n\n23:53.200 --> 23:58.720\n As soon as living things started to move, they're not just floating in sea, they're not just a\n\n23:58.720 --> 24:03.920\n plant, you know, grounded someplace. As soon as they started to move, there was an advantage to\n\n24:03.920 --> 24:08.960\n moving intelligently, to moving in certain ways. And there's some very simple things you can do,\n\n24:08.960 --> 24:14.480\n you know, bacteria or single cell organisms can move towards the source of gradient of\n\n24:14.480 --> 24:19.280\n food or something like that. But an animal that might know where it is and know where it's been\n\n24:19.280 --> 24:23.520\n and how to get back to that place, or an animal that might say, oh, there was a source of food\n\n24:23.520 --> 24:29.040\n someplace, how do I get to it? Or there was a danger, how do I get to it? There was a mate, how\n\n24:29.040 --> 24:34.480\n do I get to them? There was a big evolutionary advantage to that. So early on, there was a\n\n24:34.480 --> 24:40.640\n pressure to start understanding your environment, like where am I and where have I been? And what\n\n24:40.640 --> 24:49.600\n happened in those different places? So we still have this neural mechanism in our brains. In the\n\n24:49.600 --> 24:54.320\n mammals, it's in the hippocampus and entorhinal cortex, these are older parts of the brain.\n\n24:55.520 --> 25:02.240\n And these are very well studied. We build a map of the of our environment. So these neurons in\n\n25:02.240 --> 25:06.800\n these parts of the brain know where I am in this room, and where the door was and things like that.\n\n25:07.360 --> 25:09.360\n So a lot of other mammals have this?\n\n25:09.360 --> 25:15.600\n All mammals have this, right? And almost any animal that knows where it is, and get around\n\n25:15.600 --> 25:20.800\n must have some mapping system, must have some way of saying, I've learned a map of my environment,\n\n25:21.360 --> 25:26.640\n I have hummingbirds in my backyard. And they go to the same places all the time. They must know\n\n25:26.640 --> 25:30.000\n where they are. They just know where they are when they're not just randomly flying around. They\n\n25:30.000 --> 25:36.160\n know. They know particular flowers they come back to. So we all have this. And it turns out it's\n\n25:36.160 --> 25:42.320\n very tricky to get neurons to do this, to build a map of an environment. And so we now know,\n\n25:42.320 --> 25:47.440\n there's these famous studies that are still very active about place cells and grid cells and these\n\n25:47.440 --> 25:51.920\n other types of cells in the older parts of the brain, and how they build these maps of the world.\n\n25:51.920 --> 25:55.920\n It's really clever. It's obviously been under a lot of evolutionary pressure over a long period\n\n25:55.920 --> 26:01.040\n of time to get good at this. So animals now know where they are. What we think has happened,\n\n26:01.920 --> 26:06.080\n and there's a lot of evidence to suggest this, is that that mechanism we learned to map,\n\n26:06.080 --> 26:16.400\n like a space, was repackaged. The same type of neurons was repackaged into a more compact form.\n\n26:17.840 --> 26:23.760\n And that became the cortical column. And it was in some sense, genericized, if that's a word. It\n\n26:23.760 --> 26:28.800\n was turned into a very specific thing about learning maps of environments to learning maps\n\n26:28.800 --> 26:34.960\n of anything, learning a model of anything, not just your space, but coffee cups and so on. And\n\n26:34.960 --> 26:40.480\n it got sort of repackaged into a more compact version, a more universal version,\n\n26:41.280 --> 26:46.800\n and then replicated. So the reason we're so flexible is we have a very generic version of\n\n26:46.800 --> 26:52.800\n this mapping algorithm, and we have 150,000 copies of it. Sounds a lot like the progress\n\n26:52.800 --> 27:00.480\n of deep learning. How so? So take neural networks that seem to work well for a specific task,\n\n27:00.480 --> 27:07.680\n compress them, and multiply it by a lot. And then you just stack them on top of it. It's like the\n\n27:07.680 --> 27:12.640\n story of transformers in natural language processing. Yeah. But in deep learning networks,\n\n27:12.640 --> 27:18.160\n they end up, you're replicating an element, but you still need the entire network to do anything.\n\n27:18.160 --> 27:24.240\n Right. Here, what's going on, each individual element is a complete learning system. This is\n\n27:24.240 --> 27:29.680\n why I can take a human brain, cut it in half, and it still works. It's the same thing.\n\n27:29.680 --> 27:34.000\n It's pretty amazing. It's fundamentally distributed. It's fundamentally distributed,\n\n27:34.000 --> 27:42.560\n complete modeling systems. But that's our story we like to tell. I would guess it's likely largely\n\n27:42.560 --> 27:50.080\n right. But there's a lot of evidence supporting that story, this evolutionary story. The thing\n\n27:50.080 --> 27:58.720\n which brought me to this idea is that the human brain got big very quickly. So that led to the\n\n27:58.720 --> 28:02.640\n proposal a long time ago that, well, there's this common element just instead of creating\n\n28:02.640 --> 28:07.680\n new things, it just replicated something. We also are extremely flexible. We can learn things that\n\n28:07.680 --> 28:15.360\n we had no history about. And that tells it that the learning algorithm is very generic. It's very\n\n28:15.360 --> 28:20.080\n kind of universal because it doesn't assume any prior knowledge about what it's learning.\n\n28:20.960 --> 28:26.160\n And so you combine those things together and you say, okay, well, how did that come about? Where\n\n28:26.160 --> 28:29.760\n did that universal algorithm come from? It had to come from something that wasn't universal. It\n\n28:29.760 --> 28:34.000\n came from something that was more specific. So anyway, this led to our hypothesis that\n\n28:34.000 --> 28:38.960\n you would find grid cells and place cell equivalents in the neocortex. And when we\n\n28:38.960 --> 28:43.760\n first published our first papers on this theory, we didn't know of evidence for that. It turns out\n\n28:43.760 --> 28:48.960\n there was some, but we didn't know about it. So then we became aware of evidence for grid\n\n28:48.960 --> 28:53.200\n cells in parts of the neocortex. And then now there's been new evidence coming out. There's some\n\n28:53.200 --> 28:59.360\n interesting papers that came out just January of this year. So one of our predictions was if this\n\n28:59.360 --> 29:04.000\n evolutionary hypothesis is correct, we would see grid cell place cell equivalents, cells that work\n\n29:04.000 --> 29:08.640\n like them through every column in the neocortex. And that's starting to be seen. What does it mean\n\n29:08.640 --> 29:13.920\n that, why is it important that they're present? Because it tells us, well, we're asking about the\n\n29:13.920 --> 29:19.120\n evolutionary origin of intelligence, right? So our theory is that these columns in the cortex\n\n29:19.120 --> 29:25.120\n are working on the same principles, they're modeling systems. And it's hard to imagine how\n\n29:25.120 --> 29:30.240\n neurons do this. And so we said, hey, it's really hard to imagine how neurons could learn these\n\n29:30.240 --> 29:36.480\n models of things. We can talk about the details of that if you want. But there's this other part\n\n29:36.480 --> 29:41.840\n of the brain, we know that learns models of environments. So could that mechanism to learn\n\n29:41.840 --> 29:47.280\n to model this room be used to learn to model the water bottle? Is it the same mechanism? So we said\n\n29:47.280 --> 29:52.400\n it's much more likely the brain's using the same mechanism, which case it would have these equivalent\n\n29:52.400 --> 29:57.920\n cell types. So it's basically the whole theory is built on the idea that these columns have\n\n29:57.920 --> 30:02.640\n reference frames and they're learning these models and these grid cells create these reference frames.\n\n30:02.640 --> 30:09.200\n So it's basically the major, in some sense, the major predictive part of this theory is that we\n\n30:09.200 --> 30:14.560\n will find these equivalent mechanisms in each column in the neocortex, which tells us that\n\n30:14.560 --> 30:21.600\n that's what they're doing. They're learning these sensory motor models of the world. So we're pretty\n\n30:21.600 --> 30:26.000\n confident that would happen, but now we're seeing the evidence. So the evolutionary process, nature\n\n30:26.000 --> 30:31.920\n does a lot of copy and paste and see what happens. Yeah. Yeah. There's no direction to it. But it\n\n30:31.920 --> 30:37.600\n just found out like, hey, if I took these elements and made more of them, what happens? And let's hook\n\n30:37.600 --> 30:43.600\n them up to the eyes and let's hook them to ears. And that seems to work pretty well for us. Again,\n\n30:43.600 --> 30:48.240\n just to take a quick step back to our conversation of collective intelligence.\n\n30:48.960 --> 30:56.160\n Do you sometimes see that as just another copy and paste aspect is copying and pasting\n\n30:56.160 --> 31:04.080\n these brains and humans and making a lot of them and then creating social structures that then\n\n31:04.080 --> 31:08.320\n almost operate as a single brain? I wouldn't have said that, but you said it sounded pretty good.\n\n31:08.320 --> 31:15.440\n So to you, the brain is its own thing.\n\n31:15.440 --> 31:20.560\n I mean, our goal is to understand how the neocortex works. We can argue how essential\n\n31:20.560 --> 31:25.200\n that is to understand the human brain because it's not the entire human brain. You can argue\n\n31:25.200 --> 31:29.680\n how essential that is to understanding human intelligence. You can argue how essential this\n\n31:29.680 --> 31:38.640\n is to sort of communal intelligence. Our goal was to understand the neocortex.\n\n31:38.640 --> 31:41.680\n Yeah. So what is the neocortex and where does it fit\n\n31:41.680 --> 31:46.480\n in the various aspects of what the brain does? Like how important is it to you?\n\n31:46.480 --> 31:53.680\n Well, obviously, again, I mentioned again in the beginning, it's about 70 to 75% of the volume of\n\n31:53.680 --> 31:58.640\n the human brain. So it dominates our brain in terms of size. Not in terms of number of neurons,\n\n31:58.640 --> 32:00.640\n but in terms of size.\n\n32:00.640 --> 32:02.400\n Size isn't everything, Jeff.\n\n32:02.400 --> 32:09.040\n I know, but it's not that. We know that all high level vision,\n\n32:09.040 --> 32:13.920\n hearing, and touch happens in the neocortex. We know that all language occurs and is understood\n\n32:13.920 --> 32:17.280\n in the neocortex, whether that's spoken language, written language, sign language,\n\n32:17.280 --> 32:23.360\n whether it's language of mathematics, language of physics, music. We know that all high level\n\n32:23.360 --> 32:27.840\n planning and thinking occurs in the neocortex. If I were to say, what part of your brain designed\n\n32:27.840 --> 32:32.400\n a computer and understands programming and creates music? It's all the neocortex.\n\n32:33.040 --> 32:39.920\n So then that's an undeniable fact. But then there's other parts of our brain are important too,\n\n32:39.920 --> 32:46.720\n right? Our emotional states, our body regulating our body. So the way I like to look at it is,\n\n32:48.400 --> 32:53.200\n can you understand the neocortex without the rest of the brain? And some people say you can't,\n\n32:53.200 --> 32:58.480\n and I think absolutely you can. It's not that they're not interacting, but you can understand.\n\n32:58.480 --> 33:01.920\n Can you understand the neocortex without understanding the emotions of fear? Yes,\n\n33:01.920 --> 33:06.480\n you can. You can understand how the system works. It's just a modeling system. I make the analogy\n\n33:06.480 --> 33:12.720\n in the book that it's like a map of the world, and how that map is used depends on who's using it.\n\n33:12.720 --> 33:19.680\n So how our map of our world in our neocortex, how we manifest as a human depends on the rest of our\n\n33:19.680 --> 33:23.760\n brain. What are our motivations? What are my desires? Am I a nice guy or not a nice guy?\n\n33:23.760 --> 33:31.040\n Am I a cheater or not a cheater? How important different things are in my life?\n\n33:33.840 --> 33:39.760\n But the neocortex can be understood on its own. And I say that as a neuroscientist,\n\n33:39.760 --> 33:43.840\n I know there's all these interactions, and I don't want to say I don't know them and we\n\n33:43.840 --> 33:47.840\n don't think about them. But from a layperson's point of view, you can say it's a modeling system.\n\n33:47.840 --> 33:51.680\n I don't generally think too much about the communal aspect of intelligence, which you brought up a\n\n33:51.680 --> 33:55.040\n number of times already. So that's not really been my concern.\n\n33:55.040 --> 33:59.120\n I just wonder if there's a continuum from the origin of the universe, like\n\n34:00.320 --> 34:07.840\n this pockets of complexities that form living organisms. I wonder if we're just,\n\n34:08.480 --> 34:13.120\n if you look at humans, we feel like we're at the top. And I wonder if there's like just,\n\n34:13.120 --> 34:19.920\n I wonder if there's like just where everybody probably every living type pocket of complexity\n\n34:20.800 --> 34:26.240\n probably thinks they're the, pardon the French, they're the shit. They're at the top of the\n\n34:26.240 --> 34:32.240\n pyramid. Well, if they're thinking. Well, then what is thinking? In this sense,\n\n34:32.240 --> 34:40.560\n the whole point is in their sense of the world, their sense is that they're at the top of it.\n\n34:40.560 --> 34:44.320\n I think what is a turtle, but you're, you're, you're bringing up, you know,\n\n34:44.320 --> 34:48.320\n the problems of complexity and complexity theory are, you know, it's a huge,\n\n34:48.880 --> 34:55.280\n interesting problem in science. Um, and you know, I think we've made surprisingly little progress\n\n34:55.280 --> 35:01.120\n and understanding complex systems in general. Um, and so, you know, the Santa Fe Institute was\n\n35:01.120 --> 35:05.200\n founded to study this and even the scientists there will say, it's really hard. We haven't\n\n35:05.200 --> 35:10.560\n really been able to figure out exactly, you know, that science hasn't really congealed yet. We're\n\n35:10.560 --> 35:15.360\n still trying to figure out the basic elements of that science. Uh, what, you know, where does\n\n35:15.360 --> 35:20.000\n complexity come from and what is it and how you define it, whether it's DNA creating bodies or\n\n35:20.000 --> 35:26.480\n phenotypes or it's individuals creating societies or ants and, you know, markets and so on. It's,\n\n35:26.480 --> 35:32.800\n it's a very complex thing. I'm not a complexity theorist person, right? Um, and I, I think you\n\n35:32.800 --> 35:38.000\n should ask, well, the brain itself is a complex system. So can we understand that? Um, I think\n\n35:38.000 --> 35:42.640\n we've made a lot of progress understanding how the brain works. So, uh, but I haven't\n\n35:42.640 --> 35:46.800\n brought it out to like, oh, well, where are we on the complexity spectrum? You know, it's like,\n\n35:47.520 --> 35:55.680\n um, it's a great question. I'd prefer for that answer to be we're not special. It seems like\n\n35:55.680 --> 36:01.680\n if we're honest, most likely we're not special. So if there is a spectrum or probably not in some\n\n36:01.680 --> 36:06.480\n kind of significant place, there's one thing we could say that we are special. And again,\n\n36:06.480 --> 36:13.120\n only here on earth, I'm not saying is that if we think about knowledge, what we know,\n\n36:14.080 --> 36:21.040\n um, we clearly human brains have, um, the only brains that have a certain types of knowledge.\n\n36:21.040 --> 36:25.920\n We're the only brains on this earth to understand, uh, what the earth is, how old it is,\n\n36:25.920 --> 36:30.400\n that the universe is a picture as a whole with the only organisms understand DNA and\n\n36:30.400 --> 36:37.200\n the origins of, you know, of species. Uh, no other species on, on this planet has that knowledge.\n\n36:37.200 --> 36:43.440\n So if we think about, I like to think about, you know, one of the endeavors of humanity is to\n\n36:43.440 --> 36:49.920\n understand the universe as much as we can. Um, I think our species is further along in that\n\n36:49.920 --> 36:54.480\n undeniably, um, whether our theories are right or wrong, we can debate, but at least we have\n\n36:54.480 --> 36:59.760\n theories. You know, we, we know that what the sun is and how its fusion is and how what black holes\n\n36:59.760 --> 37:04.400\n are and, you know, we know general theory of relativity and no other animal has any of this\n\n37:04.400 --> 37:10.800\n knowledge. So in that sense that we're special, uh, are we special in terms of the hierarchy of\n\n37:10.800 --> 37:20.960\n complexity in the universe? Probably not. Can we look at a neuron? Yeah. You say that prediction\n\n37:20.960 --> 37:24.960\n happens in the neuron. What does that mean? So the neuron traditionally is seen as the\n\n37:24.960 --> 37:31.760\n basic element of the brain. So we, I mentioned this earlier that prediction was our research agenda.\n\n37:31.760 --> 37:37.840\n Yeah. We said, okay, um, how does the brain make a prediction? Like I I'm about to grab this water\n\n37:37.840 --> 37:42.720\n bottle and my brain is predicting what I'm going to feel on, on all my parts of my fingers. If I\n\n37:42.720 --> 37:46.560\n felt something really odd on any part here, I'd notice it. So my brain is predicting what it's\n\n37:46.560 --> 37:51.360\n going to feel as I grab this thing. So what does that, how does that manifest itself in neural\n\n37:51.360 --> 37:57.360\n tissue? Right. We got brains made of neurons and there's chemicals and there's neurons and there's\n\n37:57.360 --> 38:03.600\n spikes and the connect, you know, where, where is the prediction going on? And one argument could be\n\n38:03.600 --> 38:09.360\n that, well, when I'm predicting something, um, a neuron must be firing in advance. It's like, okay,\n\n38:09.360 --> 38:12.800\n this neuron represents what you're going to feel and it's firing. It's sending a spike.\n\n38:13.600 --> 38:17.760\n And certainly that happens to some extent, but our predictions are so ubiquitous\n\n38:17.760 --> 38:21.360\n that we're making so many of them, which we're totally unaware of just the vast majority of me\n\n38:21.360 --> 38:27.120\n have no idea that you're doing this. Um, that it, there wasn't really, we were trying to figure,\n\n38:27.120 --> 38:31.920\n how could this be? Where, where are these, where are these happening? Right. And I won't walk you\n\n38:31.920 --> 38:38.880\n through the whole story unless you insist upon it. But we came to the realization that most of your\n\n38:38.880 --> 38:43.440\n predictions are occurring inside individual neurons, especially these, the most common\n\n38:43.440 --> 38:49.120\n are in the parameter cells. And there are, there's a property of neurons. We, everyone knows,\n\n38:49.120 --> 38:53.280\n or most people know that a neuron is a cell and it has this spike called an action potential,\n\n38:53.280 --> 38:58.160\n and it sends information. But we now know that there's these spikes internal to the neuron,\n\n38:58.160 --> 39:03.200\n they're called dendritic spikes. They travel along the branches of the neuron and they don't leave\n\n39:03.200 --> 39:08.240\n the neuron. They're just internal only. There's far more dendritic spikes than there are action\n\n39:08.240 --> 39:14.240\n potentials, far more. They're happening all the time. And what we came to understand that those\n\n39:14.240 --> 39:18.880\n dendritic spikes, the ones that are occurring are actually a form of prediction. They're telling the\n\n39:18.880 --> 39:25.360\n neuron, the neuron is saying, I expect that I might become active shortly. And that internal,\n\n39:25.360 --> 39:30.240\n so the internal spike is a way of saying, you're going to, you might be generating external spikes\n\n39:30.240 --> 39:36.640\n soon. I predicted you're going to become active. And, and we've, we've, we wrote a paper in 2016\n\n39:36.640 --> 39:42.480\n which explained how this manifests itself in neural tissue and how it is that this all works\n\n39:42.480 --> 39:48.320\n together. But the vast majority, we think it's, there's a lot of evidence supporting it. So we,\n\n39:48.320 --> 39:51.360\n that's where we think that most of these predictions are internal. That's why you can't\n\n39:51.360 --> 39:54.160\n be, they're internal to the neuron, you can't perceive them.\n\n39:54.160 --> 40:00.080\n Well, from understanding the prediction mechanism of a single neuron, do you think there's deep\n\n40:00.080 --> 40:05.520\n insights to be gained about the prediction capabilities of the mini brains of the neural\n\n40:05.520 --> 40:08.160\n brain? Of the mini brains and then the bigger brain and the brain?\n\n40:08.160 --> 40:12.160\n Oh yeah. Yeah. Yeah. So having a prediction side of their individual neuron is not that useful.\n\n40:12.720 --> 40:22.320\n So what? The way it manifests itself in neural tissue is that when a neuron, a neuron emits these\n\n40:22.320 --> 40:27.440\n spikes are a very singular type event. If a neuron is predicting that it's going to be active, it\n\n40:27.440 --> 40:31.840\n emits its spike very, a little bit sooner, just a few milliseconds sooner than it would have\n\n40:31.840 --> 40:36.240\n been. It's like, I give the analogy of the book is like a sprinter on a, on a starting blocks in a,\n\n40:36.240 --> 40:42.480\n in a race. And if someone says, get ready, set, you get up and you're ready to go. And then when\n\n40:42.480 --> 40:46.320\n your race starts, you get a little bit earlier start. So that it's that, that ready set is like\n\n40:46.320 --> 40:50.800\n the prediction and the neurons like ready to go quicker. And what happens is when you have a whole\n\n40:50.800 --> 40:55.520\n bunch of neurons together and they're all getting these inputs, the ones that are in the predictive\n\n40:55.520 --> 40:59.920\n state, the ones that are anticipating to become active, if they do become active, they, they\n\n40:59.920 --> 41:03.680\n sooner, they disable everything else. And it leads to different representations in the brain. So\n\n41:04.240 --> 41:09.600\n you have to, it's not isolated just to the neuron, the prediction occurs with the neuron,\n\n41:09.600 --> 41:14.880\n but the network behavior changes. So what happens under different predictions, different inputs\n\n41:14.880 --> 41:20.800\n have different representations. So how I, what I predict is going to be different under different\n\n41:20.800 --> 41:24.960\n contexts, you know, what my input will be is different under different contexts. So this is,\n\n41:24.960 --> 41:30.000\n this is a key to the whole theory, how this works. So the theory of the thousand brains,\n\n41:30.560 --> 41:35.920\n if you were to count the number of brains, how would you do it? The thousand brain theory says\n\n41:35.920 --> 41:42.320\n that basically every cortical column in the, in your, in your cortex is a complete modeling system.\n\n41:42.320 --> 41:46.800\n And that when I ask, where do I have a model of something like a coffee cup? It's not in one of\n\n41:46.800 --> 41:51.040\n those models. It's in thousands of those models. There's thousands of models of coffee cups. That's\n\n41:51.040 --> 41:56.160\n what the thousand brains, then there's a voting mechanism, which you lead, which you're, which is\n\n41:56.160 --> 42:01.520\n the thing you're, which you're conscious of, which leads to your singular perception. That's why you,\n\n42:01.520 --> 42:07.200\n you perceive something. So that's the thousand brains theory. The details, how we got to that\n\n42:07.200 --> 42:13.440\n theory are complicated. It wasn't, we just thought of it one day. And one of those details that we\n\n42:13.440 --> 42:18.160\n had to ask, how does a model make predictions? And we've talked about just these predictive neurons.\n\n42:18.160 --> 42:22.320\n That's part of this theory. It's like saying, Oh, it's a detail, but it was like a crack in the\n\n42:22.320 --> 42:24.960\n door. It's like, how are we going to figure out how these neurons built through this? You know,\n\n42:24.960 --> 42:30.080\n what is going on here? So we just looked at prediction as like, well, we know that's ubiquitous.\n\n42:30.080 --> 42:34.400\n We know that every part of the cortex is making predictions. Therefore, whatever the predictive\n\n42:34.400 --> 42:39.040\n system is, it's going to be everywhere. We know there's a gazillion predictions happening at once.\n\n42:39.040 --> 42:44.000\n So this is where we can start teasing apart, you know, ask questions about, you know, how could\n\n42:44.000 --> 42:48.640\n neurons be making these predictions? And that sort of built up to now what we have this thousand\n\n42:48.640 --> 42:53.200\n brains theory, which is complex. You know, it's just, I can state it simply, but we just didn't\n\n42:53.200 --> 42:59.200\n think of it. We had to get there step by step, very, it took years to get there.\n\n42:59.200 --> 43:04.560\n And where does reference frames fit in? So, yeah.\n\n43:04.560 --> 43:11.200\n Okay. So again, a reference frame, I mentioned earlier about the model of a house. And I said,\n\n43:11.200 --> 43:14.560\n if you're going to build a model of a house in a computer, they have a reference frame. And you\n\n43:14.560 --> 43:19.600\n can think of reference frame like Cartesian coordinates, like X, Y, and Z axes. So I could\n\n43:19.600 --> 43:24.400\n say, oh, I'm going to design a house. I can say, well, the front door is at this location, X, Y,\n\n43:24.400 --> 43:28.560\n Z, and the roof is at this location, X, Y, Z, and so on. That's a type of reference frame.\n\n43:29.440 --> 43:33.600\n So it turns out for you to make a prediction, and I walk you through the thought experiment in the\n\n43:33.600 --> 43:37.360\n book where I was predicting what my finger was going to feel when I touched a coffee cup.\n\n43:37.360 --> 43:45.200\n It was a ceramic coffee cup, but this one will do. And what I realized is that to make a prediction\n\n43:45.200 --> 43:48.240\n of what my finger's going to feel, like it's going to feel different than this, what's it feel\n\n43:48.240 --> 43:53.280\n different if I touch the hole or this thing on the bottom, make that prediction. The cortex needs to\n\n43:53.280 --> 43:59.360\n know where the finger is, the tip of the finger, relative to the coffee cup. And exactly relative\n\n43:59.360 --> 44:03.440\n to the coffee cup. And to do that, I have to have a reference frame for the coffee cup. It has to\n\n44:03.440 --> 44:08.160\n have a way of representing the location of my finger to the coffee cup. And then we realized,\n\n44:08.160 --> 44:11.360\n of course, every part of your skin has to have a reference frame relative to things that touch.\n\n44:11.360 --> 44:16.240\n And then we did the same thing with vision. So the idea that a reference frame is necessary\n\n44:16.240 --> 44:19.520\n to make a prediction when you're touching something or when you're seeing something\n\n44:20.080 --> 44:22.800\n and you're moving your eyes or you're moving your fingers, it's just a requirement\n\n44:24.000 --> 44:29.200\n to predict. If I have a structure, I'm going to make a prediction. I have to know where it is I'm\n\n44:29.200 --> 44:34.960\n looking or touching it. So then we said, well, how do neurons make reference frames? It's not obvious.\n\n44:36.160 --> 44:40.480\n X, Y, Z coordinates don't exist in the brain. It's just not the way it works. So that's when we\n\n44:40.480 --> 44:45.120\n looked at the older part of the brain, the hippocampus and the anterior cortex, where we knew\n\n44:45.120 --> 44:49.920\n that in that part of the brain, there's a reference frame for a room or a reference frame for an\n\n44:49.920 --> 44:55.200\n environment. Remember, I talked earlier about how you could make a map of this room. So we said,\n\n44:55.200 --> 45:01.440\n oh, they are implementing reference frames there. So we knew that reference frames needed to exist\n\n45:01.440 --> 45:07.680\n in every quarter of a column. And so that was a deductive thing. We just deduced it. It has to\n\n45:07.680 --> 45:15.200\n exist. So you take the old mammalian ability to know where you are in a particular space\n\n45:15.920 --> 45:18.320\n and you start applying that to higher and higher levels.\n\n45:18.320 --> 45:22.560\n Yeah. First you apply it to like where your finger is. So here's what I think about it.\n\n45:22.560 --> 45:26.720\n The old part of the brain says, where's my body in this room? The new part of the brain says,\n\n45:26.720 --> 45:34.720\n where's my finger relative to this object? Where is a section of my retina relative to\n\n45:34.720 --> 45:39.120\n this object? I'm looking at one little corner. Where is that relative to this patch of my retina?\n\n45:40.800 --> 45:47.280\n And then we take the same thing and apply it to concepts, mathematics, physics, humanity,\n\n45:47.280 --> 45:50.240\n whatever you want to think about. And eventually you're pondering your own mortality.\n\n45:50.240 --> 45:55.520\n Well, whatever. But the point is when we think about the world, when we have knowledge about\n\n45:55.520 --> 46:00.560\n the world, how is that knowledge organized, Lex? Where is it in your head? The answer is it's in\n\n46:00.560 --> 46:05.920\n reference frames. So the way I learned the structure of this water bottle where the\n\n46:05.920 --> 46:11.200\n features are relative to each other, when I think about history or democracy or mathematics,\n\n46:11.200 --> 46:15.680\n the same basic underlying structure is happening. There's reference frames for where the knowledge\n\n46:15.680 --> 46:19.200\n that you're assigning things to. So in the book, I go through examples like mathematics\n\n46:19.200 --> 46:25.920\n and language and politics. But the evidence is very clear in the neuroscience. The same mechanism\n\n46:25.920 --> 46:29.600\n that we use to model this coffee cup, we're going to use to model high level thoughts.\n\n46:30.160 --> 46:33.600\n Your demise of humanity, whatever you want to think about.\n\n46:34.160 --> 46:38.960\n It's interesting to think about how different are the representations of those higher dimensional\n\n46:38.960 --> 46:45.680\n concepts, higher level concepts, how different the representation there is in terms of reference\n\n46:45.680 --> 46:52.080\n frames versus spatial. But the interesting thing, it's a different application, but it's the exact\n\n46:52.080 --> 46:59.680\n same mechanism. But isn't there some aspect to higher level concepts that they seem to be\n\n46:59.680 --> 47:05.600\n hierarchical? Like they just seem to integrate a lot of information into them. So is our physical\n\n47:05.600 --> 47:12.160\n objects. So take this water bottle. I'm not particular to this brand, but this is a Fiji\n\n47:12.160 --> 47:18.880\n water bottle and it has a logo on it. I use this example in my book, our company's coffee cup has\n\n47:18.880 --> 47:25.520\n a logo on it. But this object is hierarchical. It's got like a cylinder and a cap, but then it\n\n47:25.520 --> 47:29.360\n has this logo on it and the logo has a word, the word has letters, the letters have different\n\n47:29.360 --> 47:33.840\n features. And so I don't have to remember, I don't have to think about this. So I say,\n\n47:33.840 --> 47:37.920\n oh, there's a Fiji logo on this water bottle. I don't have to go through and say, oh, what is the\n\n47:37.920 --> 47:43.920\n Fiji logo? It's the F and I and the J and I, and there's a hibiscus flower. And, oh, it has the\n\n47:43.920 --> 47:47.760\n statement on it. I don't have to do that. I just incorporate all of that in some sort of hierarchical\n\n47:47.760 --> 47:55.040\n representation. I say, put this logo on this water bottle. And then the logo has a word\n\n47:55.040 --> 47:59.520\n and the word has letters, all hierarchical. All that stuff is big. It's amazing that the\n\n47:59.520 --> 48:04.960\n brain instantly just does all that. The idea that there's water, it's liquid and the idea that you\n\n48:04.960 --> 48:11.920\n can drink it when you're thirsty, the idea that there's brands and then there's like all of that\n\n48:11.920 --> 48:17.120\n information is instantly like built into the whole thing once you proceed. So I wanted to\n\n48:17.120 --> 48:21.040\n get back to your point about hierarchical representation. The world itself is hierarchical,\n\n48:21.680 --> 48:25.200\n right? And I can take this microphone in front of me. I know inside there's going to be some\n\n48:25.200 --> 48:28.080\n electronics. I know there's going to be some wires and I know there's going to be a little\n\n48:28.080 --> 48:33.920\n diaphragm that moves back and forth. I don't see that, but I know it. So everything in the world\n\n48:33.920 --> 48:37.840\n is hierarchical. You just go into a room. It's composed of other components. The kitchen has a\n\n48:37.840 --> 48:42.320\n refrigerator. The refrigerator has a door. The door has a hinge. The hinge has screws and pin.\n\n48:43.200 --> 48:49.360\n So anyway, the modeling system that exists in every cortical column learns the hierarchical\n\n48:49.360 --> 48:54.720\n structure of objects. So it's a very sophisticated modeling system in this grain of rice. It's hard\n\n48:54.720 --> 48:58.800\n to imagine, but this grain of rice can do really sophisticated things. It's got 100,000 neurons in\n\n48:58.800 --> 49:07.440\n it. It's very sophisticated. So that same mechanism that can model a water bottle or a coffee cup\n\n49:07.440 --> 49:13.600\n can model conceptual objects as well. That's the beauty of this discovery that this guy,\n\n49:13.600 --> 49:18.720\n Vernon Malmkastel, made many, many years ago, which is that there's a single cortical algorithm\n\n49:18.720 --> 49:23.840\n underlying everything we're doing. So common sense concepts and higher\n\n49:23.840 --> 49:26.720\n level concepts are all represented in the same way?\n\n49:26.720 --> 49:31.520\n They're set in the same mechanisms, yeah. It's a little bit like computers. All computers are\n\n49:31.520 --> 49:37.520\n universal Turing machines. Even the little teeny one that's in my toaster and the big one that's\n\n49:37.520 --> 49:41.680\n running some cloud server someplace. They're all running on the same principle. They can\n\n49:41.680 --> 49:46.080\n apply different things. So the brain is all built on the same principle. It's all about\n\n49:46.080 --> 49:53.120\n learning these structured models using movement and reference frames. And it can be applied to\n\n49:53.120 --> 49:56.400\n something as simple as a water bottle and a coffee cup. And it can be applied to thinking\n\n49:56.400 --> 50:02.800\n what's the future of humanity and why do you have a hedgehog on your desk? I don't know.\n\n50:02.800 --> 50:09.280\n Nobody knows. Well, I think it's a hedgehog. That's right. It's a hedgehog in the fog.\n\n50:09.280 --> 50:16.240\n It's a Russian reference. Does it give you any inclination or hope about how difficult\n\n50:16.240 --> 50:21.840\n it is to engineer common sense reasoning? So how complicated is this whole process?\n\n50:21.840 --> 50:28.640\n So looking at the brain, is this a marvel of engineering or is it pretty dumb stuff\n\n50:28.640 --> 50:34.640\n stuck on top of each other over? Can it be both? Can it be both, right?\n\n50:35.600 --> 50:42.080\n I don't know if it can be both because if it's an incredible engineering job, that means it's\n\n50:43.040 --> 50:48.320\n so evolution did a lot of work. Yeah, but then it just copied that.\n\n50:48.320 --> 50:55.760\n Yeah. Right. So as I said earlier, figuring out how to model something like a space is really hard\n\n50:55.760 --> 50:59.760\n and evolution had to go through a lot of trick. And these cells I was talking about,\n\n50:59.760 --> 51:03.040\n these grid cells and place cells, they're really complicated. This is not simple stuff.\n\n51:03.040 --> 51:07.120\n This neural tissue works on these really unexpected, weird mechanisms.\n\n51:08.720 --> 51:13.120\n But it did it. It figured it out. But now you could just make lots of copies of it.\n\n51:13.120 --> 51:18.320\n But then finding, yeah, so it's a very interesting idea that's a lot of copies\n\n51:18.320 --> 51:25.360\n of a basic mini brain. But the question is how difficult it is to find that mini brain\n\n51:25.360 --> 51:32.880\n that you can copy and paste effectively. Today, we know enough to build this.\n\n51:33.920 --> 51:37.920\n I'm sitting here with, I know the steps we have to go. There's still some engineering problems\n\n51:37.920 --> 51:43.760\n to solve, but we know enough. And this is not like, oh, this is an interesting idea. We have\n\n51:43.760 --> 51:47.520\n to go think about it for another few decades. No, we actually understand it pretty well in details.\n\n51:48.160 --> 51:54.800\n So not all the details, but most of them. So it's complicated, but it is an engineering problem.\n\n51:55.360 --> 52:00.080\n So in my company, we are working on that. We are basically a roadmap of how we do this.\n\n52:01.360 --> 52:06.880\n It's not going to take decades. It's a matter of a few years optimistically,\n\n52:06.880 --> 52:11.840\n but I think that's possible. It's, you know, complex things. If you understand them,\n\n52:11.840 --> 52:17.200\n you can build them. So in which domain do you think it's best to build them?\n\n52:17.200 --> 52:23.440\n Are we talking about robotics, like entities that operate in the physical world that are\n\n52:23.440 --> 52:27.920\n able to interact with that world? Are we talking about entities that operate in the digital world?\n\n52:27.920 --> 52:33.840\n Are we talking about something more like more specific, like it's done in the machine learning\n\n52:33.840 --> 52:40.240\n community where you look at natural language or computer vision? Where do you think is easiest?\n\n52:41.120 --> 52:43.920\n It's the first, it's the first two more than the third one, I would say.\n\n52:46.560 --> 52:52.320\n Again, let's just use computers as an analogy. The pioneers in computing, people like John\n\n52:52.320 --> 52:56.800\n Van Norman and Alan Turing, they created this thing, you know, we now call the universal\n\n52:56.800 --> 53:00.800\n Turing machine, which is a computer, right? Did they know how it was going to be applied?\n\n53:00.800 --> 53:04.960\n Where it was going to be used? Could they envision any of the future? No. They just said,\n\n53:04.960 --> 53:11.120\n this is like a really interesting computational idea about algorithms and how you can implement\n\n53:11.120 --> 53:18.400\n them in a machine. And we're doing something similar to that today. Like we are building this\n\n53:18.400 --> 53:23.680\n sort of universal learning principle that can be applied to many, many different things.\n\n53:24.480 --> 53:27.600\n But the robotics piece of that, the interactive...\n\n53:27.600 --> 53:31.360\n Okay. All right. Let's be just specific. You can think of this cortical column as\n\n53:31.360 --> 53:35.120\n what we call a sensory motor learning system. It has the idea that there's a sensor\n\n53:35.120 --> 53:39.520\n and then it's moving. That sensor can be physical. It could be like my finger\n\n53:39.520 --> 53:43.440\n and it's moving in the world. It could be like my eye and it's physically moving.\n\n53:43.440 --> 53:50.160\n It can also be virtual. So, it could be, an example would be, I could have a system that\n\n53:50.160 --> 53:55.360\n lives in the internet that actually samples information on the internet and moves by\n\n53:55.360 --> 54:02.240\n following links. That's a sensory motor system. Something that echoes the process of a finger\n\n54:02.240 --> 54:06.000\n moving along a cortical... But in a very, very loose sense. It's like,\n\n54:06.640 --> 54:10.720\n again, learning is inherently about discovering the structure of the world and discover the\n\n54:10.720 --> 54:14.720\n structure of the world, you have to move through the world. Even if it's a virtual world, even if\n\n54:14.720 --> 54:20.480\n it's a conceptual world, you have to move through it. It doesn't exist in one... It has some structure\n\n54:20.480 --> 54:25.840\n to it. So, here's a couple of predictions at getting what you're talking about.\n\n54:27.040 --> 54:31.840\n In humans, the same algorithm does robotics. It moves my arms, my eyes, my body.\n\n54:34.560 --> 54:40.000\n And so, in the future, to me, robotics and AI will merge. They're not going to be separate fields\n\n54:40.000 --> 54:45.200\n because the algorithms for really controlling robots are going to be the same algorithms we\n\n54:45.200 --> 54:50.000\n have in our brain, these sensory motor algorithms. Today, we're not there, but I think that's going\n\n54:50.000 --> 54:58.880\n to happen. But not all AI systems will have to be robotics. You can have systems that have very\n\n54:58.880 --> 55:02.480\n different types of embodiments. Some will have physical movements, some will not have physical\n\n55:02.480 --> 55:08.560\n movements. It's a very generic learning system. Again, it's like computers. The Turing machine,\n\n55:08.560 --> 55:11.680\n it doesn't say how it's supposed to be implemented, it doesn't tell you how big it is,\n\n55:11.680 --> 55:15.440\n it doesn't tell you what you can apply it to, but it's a computational principle.\n\n55:15.440 --> 55:20.640\n The cortical column equivalent is a computational principle about learning. It's about how you\n\n55:20.640 --> 55:27.440\n learn and it can be applied to a gazillion things. I think this impact of AI is going to be as large,\n\n55:27.440 --> 55:33.200\n if not larger, than computing has been in the last century, by far, because it's getting at\n\n55:33.200 --> 55:37.600\n a fundamental thing. It's not a vision system or a learning system. It's not a vision system or\n\n55:37.600 --> 55:41.600\n a hearing system. It is a learning system. It's a fundamental principle, how you learn the structure\n\n55:41.600 --> 55:46.400\n in the world, how you can gain knowledge and be intelligent. That's what the thousand brains says\n\n55:46.400 --> 55:49.680\n was going on. We have a particular implementation in our head, but it doesn't have to be like that\n\n55:49.680 --> 55:55.920\n at all. Do you think there's going to be some kind of impact? Okay, let me ask it another way.\n\n55:56.800 --> 56:05.360\n What do increasingly intelligent AI systems do with us humans in the following way? How hard is\n\n56:05.360 --> 56:13.040\n the human in the loop problem? How hard is it to interact? The finger on the coffee cup equivalent\n\n56:13.040 --> 56:19.120\n of having a conversation with a human being. How hard is it to fit into our little human world?\n\n56:20.880 --> 56:25.200\n I think it's a lot of engineering problems. I don't think it's a fundamental problem.\n\n56:25.200 --> 56:28.880\n I could ask you the same question. How hard is it for computers to fit into a human world?\n\n56:28.880 --> 56:39.680\n Right. That's essentially what I'm asking. How elitist are we as humans? We try to keep out\n\n56:40.720 --> 56:48.240\n systems. I don't know. I'm not sure that's the right question. Let's look at computers as an\n\n56:48.240 --> 56:52.480\n analogy. Computers are a million times faster than us. They do things we can't understand.\n\n56:52.480 --> 56:57.120\n Most people have no idea what's going on when they use computers. How do we integrate them\n\n56:57.120 --> 57:02.960\n in our society? Well, we don't think of them as their own entity. They're not living things.\n\n57:04.160 --> 57:12.800\n We don't afford them rights. We rely on them. Our survival as seven billion people or something\n\n57:12.800 --> 57:18.320\n like that is relying on computers now. Don't you think that's a fundamental problem\n\n57:18.320 --> 57:22.480\n that we see them as something we don't give rights to?\n\n57:22.480 --> 57:25.600\n Computers? Yeah, computers. Robots,\n\n57:25.600 --> 57:29.920\n computers, intelligence systems. It feels like for them to operate successfully,\n\n57:29.920 --> 57:36.720\n they would need to have a lot of the elements that we would start having to think about.\n\n57:37.760 --> 57:41.360\n Should this entity have rights? I don't think so. I think\n\n57:42.560 --> 57:47.680\n it's tempting to think that way. First of all, hardly anyone thinks that for computers today.\n\n57:47.680 --> 57:52.320\n No one says, oh, this thing needs a right. I shouldn't be able to turn it off. If I throw it\n\n57:52.320 --> 57:57.360\n in the trash can and hit it with a sledgehammer, it might form a criminal act. No one thinks that.\n\n57:59.360 --> 58:02.400\n Now we think about intelligent machines, which is where you're going.\n\n58:05.600 --> 58:10.080\n All of a sudden, you're like, well, now we can't do that. I think the basic problem we have here\n\n58:10.080 --> 58:14.000\n is that people think intelligent machines will be like us. They're going to have the same emotions\n\n58:14.000 --> 58:19.120\n as we do, the same feelings as we do. What if I can build an intelligent machine that absolutely\n\n58:19.120 --> 58:23.040\n could care less about whether it was on or off or destroyed or not? It just doesn't care. It's\n\n58:23.040 --> 58:28.400\n just like a map. It's just a modeling system. There's no desires to live. Nothing.\n\n58:28.400 --> 58:35.280\n Is it possible to create a system that can model the world deeply and not care\n\n58:35.280 --> 58:38.640\n about whether it lives or dies? Absolutely. No question about it.\n\n58:38.640 --> 58:43.920\n To me, that's not 100% obvious. It's obvious to me. We can debate it if we want.\n\n58:43.920 --> 58:52.560\n Where does your desire to live come from? It's an old evolutionary design. We could argue,\n\n58:52.560 --> 58:58.000\n does it really matter if we live or not? Objectively, no. We're all going to die eventually.\n\n59:00.720 --> 59:05.840\n Evolution makes us want to live. Evolution makes us want to fight to live. Evolution makes us want\n\n59:05.840 --> 59:11.840\n to care and love one another and to care for our children and our relatives and our family and so\n\n59:11.840 --> 59:18.880\n on. Those are all good things. They come about not because we're smart, because we're animals\n\n59:18.880 --> 59:25.280\n that grew up. The hummingbird in my backyard cares about its offspring. Every living thing\n\n59:25.280 --> 59:30.720\n in some sense cares about surviving. When we talk about creating intelligent machines,\n\n59:30.720 --> 59:35.360\n we're not creating life. We're not creating evolving creatures. We're not creating living\n\n59:35.360 --> 59:40.400\n things. We're just creating a machine that can learn really sophisticated stuff. That machine,\n\n59:40.400 --> 59:47.120\n it may even be able to talk to us. It's not going to have a desire to live unless somehow we put it\n\n59:47.120 --> 59:52.720\n into that system. Well, there's learning, right? The thing is... But you don't learn to want to\n\n59:52.720 --> 59:57.600\n live. It's built into you. It's part of your DNA. People like Ernest Becker argue,\n\n59:59.600 --> 1:00:06.000\n there's the fact of finiteness of life. The way we think about it is something we learned,\n\n1:00:06.000 --> 1:00:13.120\n perhaps. Okay. Yeah. Some people decide they don't want to live. Some people decide the desire to\n\n1:00:13.120 --> 1:00:18.880\n live is built in DNA, right? But I think what I'm trying to get to is in order to accomplish goals,\n\n1:00:18.880 --> 1:00:23.200\n it's useful to have the urgency of mortality. It's what the Stoics talked about,\n\n1:00:23.200 --> 1:00:31.600\n is meditating in your mortality. It might be a very useful thing to do to die and have the urgency\n\n1:00:31.600 --> 1:00:38.400\n of death and to realize that to conceive yourself as an entity that operates in this world that\n\n1:00:38.400 --> 1:00:43.280\n eventually will no longer be a part of this world and actually conceive of yourself as a conscious\n\n1:00:43.280 --> 1:00:49.760\n entity might be very useful for you to be a system that makes sense of the world. Otherwise,\n\n1:00:49.760 --> 1:00:55.360\n you might get lazy. Well, okay. We're going to build these machines, right? So we're talking\n\n1:00:55.360 --> 1:01:03.360\n about building AIs. But we're building the equivalent of the cortical columns.\n\n1:01:03.360 --> 1:01:11.120\n The neocortex. The neocortex. And the question is, where do they arrive at? Because we're not\n\n1:01:11.120 --> 1:01:16.560\n hard coding everything in. Well, in terms of if you build the neocortex equivalent,\n\n1:01:17.360 --> 1:01:22.640\n it will not have any of these desires or emotional states. Now, you can argue that\n\n1:01:22.640 --> 1:01:28.240\n that neocortex won't be useful unless I give it some agency, unless I give it some desire,\n\n1:01:28.240 --> 1:01:31.600\n unless I give it some motivation. Otherwise, you'll be just lazy and do nothing, right?\n\n1:01:31.600 --> 1:01:37.040\n You could argue that. But on its own, it's not going to do those things. It's just not going\n\n1:01:37.040 --> 1:01:41.120\n to sit there and say, I understand the world. Therefore, I care to live. No, it's not going\n\n1:01:41.120 --> 1:01:46.240\n to do that. It's just going to say, I understand the world. Why is that obvious to you? Do you think\n\n1:01:46.240 --> 1:01:52.960\n it's possible? Okay, let me ask it this way. Do you think it's possible it will at least assign to\n\n1:01:52.960 --> 1:02:04.240\n itself agency and perceive itself in this world as being a conscious entity as a useful way to\n\n1:02:04.240 --> 1:02:08.640\n operate in the world and to make sense of the world? I think an intelligent machine can be\n\n1:02:08.640 --> 1:02:16.400\n conscious, but that does not, again, imply any of these desires and goals that you're worried about.\n\n1:02:18.160 --> 1:02:20.560\n We can talk about what it means for a machine to be conscious.\n\n1:02:20.560 --> 1:02:24.640\n By the way, not worry about, but get excited about. It's not necessary that we should worry\n\n1:02:24.640 --> 1:02:29.200\n about it. I think there's a legitimate problem or not problem, a question asked,\n\n1:02:29.200 --> 1:02:35.600\n if you build this modeling system, what's it going to model? What's its desire? What's its\n\n1:02:35.600 --> 1:02:42.720\n goal? What are we applying it to? That's an interesting question. One thing, and it depends\n\n1:02:42.720 --> 1:02:46.800\n on the application, it's not something that inherent to the modeling system. It's something\n\n1:02:46.800 --> 1:02:51.360\n we apply to the modeling system in a particular way. If I wanted to make a really smart car,\n\n1:02:52.320 --> 1:02:57.760\n it would have to know about driving and cars and what's important in driving and cars.\n\n1:02:58.320 --> 1:03:01.760\n It's not going to figure that on its own. It's not going to sit there and say, I've understood\n\n1:03:01.760 --> 1:03:06.000\n the world and I've decided, no, no, no, no, we're going to have to tell it. We're going to have to\n\n1:03:06.000 --> 1:03:10.880\n say, so I imagine I make this car really smart. It learns about your driving habits. It learns\n\n1:03:10.880 --> 1:03:17.200\n about the world. Is it one day going to wake up and say, you know what? I'm tired of driving\n\n1:03:17.760 --> 1:03:21.280\n and doing what you want. I think I have better ideas about how to spend my time.\n\n1:03:22.080 --> 1:03:26.160\n Okay. No, it's not going to do that. Well, part of me is playing a little bit of devil's advocate,\n\n1:03:26.160 --> 1:03:32.560\n but part of me is also trying to think through this because I've studied cars quite a bit and\n\n1:03:32.560 --> 1:03:36.800\n I studied pedestrians and cyclists quite a bit. And there's part of me that thinks\n\n1:03:38.560 --> 1:03:45.120\n that there needs to be more intelligence than we realize in order to drive successfully.\n\n1:03:46.160 --> 1:03:54.720\n That game theory of human interaction seems to require some deep understanding of human nature\n\n1:03:54.720 --> 1:04:04.000\n that, okay. When a pedestrian crosses the street, there's some sense. They look at a car usually,\n\n1:04:04.880 --> 1:04:10.960\n and then they look away. There's some sense in which they say, I believe that you're not going\n\n1:04:10.960 --> 1:04:16.320\n to murder me. You don't have the guts to murder me. This is the little dance of pedestrian car\n\n1:04:16.320 --> 1:04:22.960\n interaction is saying, I'm going to look away and I'm going to put my life in your hands because\n\n1:04:22.960 --> 1:04:28.240\n I think you're human. You're not going to kill me. And then the car in order to successfully\n\n1:04:28.240 --> 1:04:34.400\n operate in like Manhattan streets has to say, no, no, no, no. I am going to kill you like a little\n\n1:04:34.400 --> 1:04:40.480\n bit. There's a little bit of this weird inkling of mutual murder. And that's a dance and somehow\n\n1:04:40.480 --> 1:04:44.160\n successfully operate through that. Do you think you were born of that? Did you learn that social\n\n1:04:44.160 --> 1:04:50.800\n interaction? I think it might have a lot of the same elements that you're talking about,\n\n1:04:50.800 --> 1:04:56.800\n which is we're leveraging things we were born with and applying them in the context that.\n\n1:04:57.600 --> 1:05:03.440\n All right. I would have said that that kind of interaction is learned because people in different\n\n1:05:03.440 --> 1:05:06.880\n cultures to have different interactions like that. If you cross the street in different cities and\n\n1:05:06.880 --> 1:05:10.400\n different parts of the world, they have different ways of interacting. I would say that's learned.\n\n1:05:10.400 --> 1:05:15.360\n And I would say an intelligent system can learn that too, but that does not lead. And the intelligent\n\n1:05:15.360 --> 1:05:24.320\n system can understand humans. It could understand that just like I can study an animal and learn\n\n1:05:24.320 --> 1:05:28.640\n something about that animal. I could study apes and learn something about their culture and so on.\n\n1:05:28.640 --> 1:05:34.160\n I don't have to be an ape to know that. I may not be completely, but I can understand something.\n\n1:05:34.160 --> 1:05:37.360\n So intelligent machine can model that. That's just part of the world. It's just part of the\n\n1:05:37.360 --> 1:05:42.640\n interactions. The question we're trying to get at, will the intelligent machine have its own personal\n\n1:05:42.640 --> 1:05:49.440\n agency that's beyond what we assign to it or its own personal goals or will it evolve and create\n\n1:05:49.440 --> 1:05:55.200\n these things? My confidence comes from understanding the mechanisms I'm talking about creating.\n\n1:05:55.920 --> 1:06:00.880\n This is not hand wavy stuff. It's down in the details. I'm going to build it. And I know what\n\n1:06:00.880 --> 1:06:03.760\n it's going to look like. And I know what it's going to behave. I know what the kind of things\n\n1:06:03.760 --> 1:06:08.000\n it could do and the kind of things it can't do. Just like when I build a computer, I know it's\n\n1:06:08.000 --> 1:06:13.440\n not going to, on its own, decide to put another register inside of it. It can't do that. No way.\n\n1:06:13.440 --> 1:06:16.080\n No matter what your software does, it can't add a register to the computer.\n\n1:06:17.440 --> 1:06:26.560\n So in this way, when we build AI systems, we have to make choices about how we embed them.\n\n1:06:26.560 --> 1:06:30.880\n So I talk about this in the book. I said intelligent system is not just the neocortex\n\n1:06:30.880 --> 1:06:36.800\n equivalent. You have to have that. But it has to have some kind of embodiment, physical or virtual.\n\n1:06:36.800 --> 1:06:41.040\n It has to have some sort of goals. It has to have some sort of ideas about dangers,\n\n1:06:41.040 --> 1:06:47.360\n about things it shouldn't do. We build in safeguards into systems. We have them in our\n\n1:06:47.360 --> 1:06:53.440\n bodies. We put them into cars. My car follows my directions until the day it sees I'm about to hit\n\n1:06:53.440 --> 1:06:58.240\n something and it ignores my directions and puts the brakes on. So we can build those things in.\n\n1:06:58.240 --> 1:07:06.480\n So that's a very interesting problem, how to build those in. I think my differing opinion about the\n\n1:07:06.480 --> 1:07:11.440\n risks of AI for most people is that people assume that somehow those things will disappear\n\n1:07:11.440 --> 1:07:17.600\n automatically and evolve. And intelligence itself begets that stuff or requires it.\n\n1:07:17.600 --> 1:07:21.120\n But it's not. Intelligence of the neocortex equipment doesn't require this. The neocortex\n\n1:07:21.120 --> 1:07:26.880\n equipment just says, I'm a learning system. Tell me what you want me to learn and ask me questions\n\n1:07:26.880 --> 1:07:33.920\n and I'll tell you the answers. And that, again, it's again like a map. A map has no intent about\n\n1:07:33.920 --> 1:07:40.800\n things, but you can use it to solve problems. Okay. So the building, engineering the neocortex\n\n1:07:41.920 --> 1:07:45.840\n in itself is just creating an intelligent prediction system.\n\n1:07:45.840 --> 1:07:50.960\n Modeling system. Sorry, modeling system. You can use it to then make predictions.\n\n1:07:52.480 --> 1:07:56.240\n But you can also put it inside a thing that's actually acting in this world.\n\n1:07:56.800 --> 1:08:02.160\n You have to put it inside something. Again, think of the map analogy, right? A map on its own doesn't\n\n1:08:02.160 --> 1:08:07.920\n do anything. It's just inert. It can learn, but it's just inert. So we have to embed it somehow\n\n1:08:07.920 --> 1:08:13.360\n in something to do something. So what's your intuition here? You had a conversation with\n\n1:08:13.360 --> 1:08:20.320\n Sam Harris recently that was sort of, you've had a bit of a disagreement and you're sticking on\n\n1:08:20.320 --> 1:08:29.520\n this point. Elon Musk, Stuart Russell kind of have us worry existential threats of AI.\n\n1:08:29.520 --> 1:08:36.400\n What's your intuition? Why, if we engineer increasingly intelligent neocortex type of system\n\n1:08:36.720 --> 1:08:40.240\n in the computer, why that shouldn't be a thing that we...\n\n1:08:40.240 --> 1:08:44.240\n It was interesting to use the word intuition and Sam Harris used the word intuition too.\n\n1:08:44.240 --> 1:08:47.840\n And we didn't use that intuition, that word. I immediately stopped and said,\n\n1:08:47.840 --> 1:08:52.960\n oh, that's the crux of the problem. He's using intuition. I'm not speaking about my intuition.\n\n1:08:52.960 --> 1:08:56.080\n I'm speaking about something I understand, something I'm going to build, something I am\n\n1:08:56.080 --> 1:09:01.840\n building, something I understand completely, or at least well enough to know what... I'm guessing,\n\n1:09:01.840 --> 1:09:08.160\n I know what this thing's going to do. And I think most people who are worried, they have trouble\n\n1:09:08.160 --> 1:09:13.280\n separating out... They don't have the knowledge or the understanding about what is intelligence,\n\n1:09:13.280 --> 1:09:16.720\n how's it manifest in the brain, how's it separate from these other functions in the brain.\n\n1:09:17.280 --> 1:09:21.680\n And so they imagine it's going to be human like or animal like. It's going to have the same sort of\n\n1:09:21.680 --> 1:09:27.680\n drives and emotions we have, but there's no reason for that. That's just because there's an unknown.\n\n1:09:27.680 --> 1:09:31.520\n If the unknown is like, oh my God, I don't know what this is going to do. We have to be careful.\n\n1:09:31.520 --> 1:09:35.680\n It could be like us, but really smarter. I'm saying, no, it won't be like us. It'll be really\n\n1:09:35.680 --> 1:09:42.080\n smarter, but it won't be like us at all. But I'm coming from that, not because I'm just guessing,\n\n1:09:42.080 --> 1:09:46.640\n I'm not using intuition. I'm basing it on like, okay, I understand this thing works. This is what\n\n1:09:46.640 --> 1:09:54.400\n it does. It makes money to you. Okay. But to push back, so I also disagree with the intuitions that\n\n1:09:54.400 --> 1:10:02.080\n Sam has, but I also disagree with what you just said, which, you know, what's a good analogy. So\n\n1:10:02.080 --> 1:10:08.720\n if you look at the Twitter algorithm in the early days, just recommender systems, you can understand\n\n1:10:08.720 --> 1:10:14.640\n how recommender systems work. What you can't understand in the early days is when you apply\n\n1:10:14.640 --> 1:10:20.400\n that recommender system at scale to thousands and millions of people, how that can change societies.\n\n1:10:20.400 --> 1:10:27.840\n Yeah. So the question is, yes, you're just saying this is how an engineer in your cortex works,\n\n1:10:27.840 --> 1:10:35.040\n but the, like when you have a very useful, uh, TikTok type of service that goes viral when your\n\n1:10:35.040 --> 1:10:40.160\n neural cortex goes viral and then millions of people start using it, can that destroy the world?\n\n1:10:40.160 --> 1:10:44.880\n No. Uh, well, first of all, this is back. One thing I want to say is that, um, AI is a dangerous\n\n1:10:44.880 --> 1:10:48.880\n technology. I don't, I'm not denying that. All technology is dangerous. Well, and AI,\n\n1:10:48.880 --> 1:10:54.400\n maybe particularly so. Okay. So, um, am I worried about it? Yeah, I'm totally worried about it.\n\n1:10:54.400 --> 1:11:00.320\n The thing where the narrow component we're talking about now is the existential risk of AI, right?\n\n1:11:00.320 --> 1:11:05.360\n Yeah. So I want to make that distinction because I think AI can be applied poorly. It can be applied\n\n1:11:05.360 --> 1:11:11.200\n in ways that, you know, people are going to understand the consequences of it. Um, these are\n\n1:11:11.200 --> 1:11:18.400\n all potentially very bad things, but they're not the AI system creating this existential risk on\n\n1:11:18.400 --> 1:11:23.440\n its own. And that's the only place that I disagree with other people. Right. So I, I think the\n\n1:11:23.440 --> 1:11:29.360\n existential risk thing is, um, humans are really damn good at surviving. So to kill off the human\n\n1:11:29.360 --> 1:11:36.000\n race, it'd be very, very difficult. Yes, but you can even, I'll go further. I don't think AI systems\n\n1:11:36.000 --> 1:11:40.720\n are ever going to try to, I don't think AI systems are ever going to like say, I'm going to ignore\n\n1:11:40.720 --> 1:11:46.480\n you. I'm going to do what I think is best. Um, I don't think that's going to happen, at least not\n\n1:11:46.480 --> 1:11:52.720\n in the way I'm talking about it. So you, the Twitter recommendation algorithm is an interesting\n\n1:11:52.720 --> 1:11:59.600\n example. Let's, let's use computers as an analogy again, right? I build a computer. It's a universal\n\n1:11:59.600 --> 1:12:03.440\n computing machine. I can't predict what people are going to use it for. They can build all kinds of\n\n1:12:03.440 --> 1:12:09.040\n things. They can, they can even create computer viruses. It's, you know, all kinds of stuff. So\n\n1:12:09.040 --> 1:12:13.360\n there's some unknown about its utility and about where it's going to go. But on the other hand,\n\n1:12:13.360 --> 1:12:18.960\n I pointed out that once I build a computer, it's not going to fundamentally change how it computes.\n\n1:12:18.960 --> 1:12:23.520\n It's like, I use the example of a register, which is a part, internal part of a computer. Um, you\n\n1:12:23.520 --> 1:12:27.600\n know, I say it can't just sit there because computers don't evolve. They don't replicate,\n\n1:12:27.600 --> 1:12:31.120\n they don't evolve. They don't, you know, the physical manifestation of the computer itself\n\n1:12:31.120 --> 1:12:36.320\n is not going to, there's certain things that can't do right. So we can break into things like things\n\n1:12:36.320 --> 1:12:40.400\n that are possible to happen. We can't predict and things that are just impossible to happen.\n\n1:12:40.400 --> 1:12:44.320\n Unless we go out of our way to make them happen, they're not going to happen unless somebody makes\n\n1:12:44.320 --> 1:12:49.120\n them happen. Yeah. So there's, there's a bunch of things to say. One is the physical aspect,\n\n1:12:49.120 --> 1:12:54.640\n which you're absolutely right. We have to build a thing for it to operate in the physical world\n\n1:12:54.640 --> 1:13:01.280\n and you can just stop building them. Uh, you know, the moment they're not doing the thing you want\n\n1:13:01.280 --> 1:13:05.760\n them to do or just change the design or change the design. The question is, I mean, there's,\n\n1:13:05.760 --> 1:13:10.640\n uh, it's possible in the physical world. This is probably longer term is you automate the building.\n\n1:13:10.640 --> 1:13:14.000\n It makes, it makes a lot of sense to automate the building. There's a lot of factories that\n\n1:13:14.000 --> 1:13:19.360\n are doing more and more and more automation to go from raw resources to the final product.\n\n1:13:19.360 --> 1:13:25.040\n It's possible to imagine that obviously much more efficient to keep, to create a factory that's\n\n1:13:25.040 --> 1:13:30.880\n creating robots that do something, uh, you know, that do something extremely useful for society.\n\n1:13:30.880 --> 1:13:35.840\n It could be a personal assistance. It could be, uh, it could, it could be your toaster, but a\n\n1:13:35.840 --> 1:13:41.680\n toaster as much as deeper knowledge of your culinary preferences. Yeah. And that could,\n\n1:13:41.680 --> 1:13:46.000\n uh, I think now you've hit on the right thing. The real thing we need to be worried about is\n\n1:13:46.000 --> 1:13:51.440\n self replication. Right. That is the thing that we're in the physical world or even the virtual\n\n1:13:51.440 --> 1:13:56.560\n world self replication because self replication is dangerous. It's probably more likely to be\n\n1:13:56.560 --> 1:14:01.760\n killed by a virus, you know, or a human hand veneered virus. Anybody can create a, you know,\n\n1:14:01.760 --> 1:14:05.680\n there's the technology is getting so almost anybody, but not anybody, but a lot of people\n\n1:14:05.680 --> 1:14:11.360\n could create a human engineered virus that could wipe out humanity. That is really dangerous. No\n\n1:14:11.360 --> 1:14:18.480\n intelligence required, just self replication. So, um, so we need to be careful about that.\n\n1:14:18.480 --> 1:14:24.240\n So when I think about, you know, AI, I'm not thinking about robots, building robots. Don't\n\n1:14:24.240 --> 1:14:28.320\n do that. Don't build a, you know, just, well, that's because you're interesting creating\n\n1:14:28.320 --> 1:14:35.360\n intelligence. It seems like self replication is a good way to make a lot of money. Well,\n\n1:14:35.360 --> 1:14:41.120\n fine. But so is, you know, maybe editing viruses is a good way too. I don't know. The point is,\n\n1:14:41.120 --> 1:14:46.880\n if as a society, when we want to look at existential risks, the existential risks we face\n\n1:14:46.880 --> 1:14:54.880\n that we can control almost all evolve around self replication. Yes. The question is, I don't see a\n\n1:14:54.880 --> 1:15:00.240\n good, uh, way to make a lot of money by engineering viruses and deploying them on the world. There\n\n1:15:00.240 --> 1:15:04.880\n could be, there could be applications that are useful, but let's separate out, let's separate out.\n\n1:15:04.880 --> 1:15:08.000\n I mean, you don't need to, you only need some, you know, terrorists who wants to do it. Cause\n\n1:15:08.000 --> 1:15:13.520\n it doesn't take a lot of money to make viruses. Um, let's just separate out what's risky and what's\n\n1:15:13.520 --> 1:15:18.560\n not risky. I'm arguing that the intelligence side of this equation is not risky. It's not risky at\n\n1:15:18.560 --> 1:15:23.520\n all. It's the self replication side of the equation that's risky. And I'm arguing that\n\n1:15:23.520 --> 1:15:28.880\n it's not risky. And I'm not dismissing that. I'm scared as hell. It's like the paperclip\n\n1:15:28.880 --> 1:15:34.400\n maximizer thing. Yeah. Those are often like talked about in the same conversation.\n\n1:15:35.200 --> 1:15:41.200\n Um, I think you're right. Like creating ultra intelligent, super intelligent systems\n\n1:15:42.000 --> 1:15:47.600\n is not necessarily coupled with a self replicating arbitrarily self replicating systems. Yeah. And\n\n1:15:47.600 --> 1:15:52.560\n you don't get evolution unless you're self replicating. Yeah. And so I think that's the gist\n\n1:15:52.560 --> 1:15:56.720\n of this argument that people have trouble separating those two out. They just think,\n\n1:15:56.720 --> 1:16:00.960\n Oh yeah, intelligence looks like us. And look how, look at the damage we've done to this planet,\n\n1:16:00.960 --> 1:16:04.640\n like how we've, you know, destroyed all these other species. Yeah. Well we replicate,\n\n1:16:04.640 --> 1:16:10.400\n which the 8 billion of us are 7 billion of us now. So, um, I think the idea is that the,\n\n1:16:10.400 --> 1:16:17.120\n the more intelligent we're able to build systems, the more tempting it becomes from a capitalist\n\n1:16:17.120 --> 1:16:21.920\n perspective of creating products, the more tempting it becomes to create self, uh, reproducing\n\n1:16:21.920 --> 1:16:26.720\n systems. All right. So let's say that's true. So does that mean we don't build intelligent systems?\n\n1:16:26.720 --> 1:16:33.760\n No, that means we regulate, we, we understand the risks. Uh, we regulate them. Uh, you know,\n\n1:16:33.760 --> 1:16:37.200\n look, there's a lot of things we could do as society, which have some sort of financial\n\n1:16:37.200 --> 1:16:42.560\n benefit to someone, which could do a lot of harm. And we have to learn how to regulate those things.\n\n1:16:42.560 --> 1:16:46.400\n We have to learn how to deal with those things. I will argue this. I would say the opposite. Like I\n\n1:16:46.400 --> 1:16:52.000\n would say having intelligent machines at our disposal will actually help us in the end more,\n\n1:16:52.000 --> 1:16:55.040\n because it'll help us understand these risks better. It'll help us mitigate these risks\n\n1:16:55.040 --> 1:16:59.040\n better. It might be ways of saying, oh, well, how do we solve climate change problems? You know,\n\n1:16:59.040 --> 1:17:05.600\n how do we do this? Or how do we do that? Um, that just like computers are dangerous in the hands of\n\n1:17:05.600 --> 1:17:09.840\n the wrong people, but they've been so great for so many other things. We live with those dangers.\n\n1:17:09.840 --> 1:17:13.520\n And I think we have to do the same with intelligent machines. We just, but we have to be\n\n1:17:13.520 --> 1:17:18.720\n constantly vigilant about this idea of a bad actors doing bad things with them and be,\n\n1:17:19.360 --> 1:17:25.440\n um, don't ever, ever create a self replicating system. Um, uh, and, and by the way, I don't even\n\n1:17:25.440 --> 1:17:30.320\n know if you could create a self replicating system that uses a factory. That's really dangerous.\n\n1:17:30.320 --> 1:17:36.000\n You know, nature's way of self replicating is so amazing. Um, you know, it doesn't require\n\n1:17:36.000 --> 1:17:41.680\n anything. It just, you know, the thing and resources and it goes right. Um, if I said to\n\n1:17:41.680 --> 1:17:46.880\n you, you know what we have to build, uh, our goal is to build a factory that can make that builds\n\n1:17:46.880 --> 1:17:54.000\n new factories and it has to end to end supply chain. It has to bind the resources, get the\n\n1:17:54.000 --> 1:18:00.000\n energy. I mean, that's really hard. It's, you know, no one's doing that in the next, you know,\n\n1:18:00.000 --> 1:18:06.400\n a hundred years. I've been extremely impressed by the efforts of Elon Musk and Tesla to try to do\n\n1:18:06.400 --> 1:18:12.720\n exactly that. Not, not from raw resource. Well, he actually, I think states the goal is to go from\n\n1:18:12.720 --> 1:18:19.440\n raw resource to the, uh, the final car in one factory. Yeah. That's the main goal. Of course,\n\n1:18:19.440 --> 1:18:23.600\n it's not currently possible, but they're taking huge leaps. Well, he's not the only one to do\n\n1:18:23.600 --> 1:18:28.720\n that. This has been a goal for many industries for a long, long time. Um, it's difficult to do.\n\n1:18:28.720 --> 1:18:34.480\n Well, a lot of people, what they do is instead they have like a million suppliers and then they\n\n1:18:34.480 --> 1:18:40.480\n like there's everybody's, they all co locate them and they, and they tie the systems together.\n\n1:18:40.480 --> 1:18:45.840\n It's a fundamental, I think that's, that also is not getting at the issue I was just talking about,\n\n1:18:45.840 --> 1:18:52.640\n um, which is self replication. It's, um, I mean, self replication means there's no\n\n1:18:53.840 --> 1:18:58.800\n entity involved other than the entity that's replicating. Um, right. And so if there are\n\n1:18:58.800 --> 1:19:04.400\n humans in this, in the loop, that's not really self replicating, right? It's unless somehow we're\n\n1:19:04.400 --> 1:19:09.440\n duped into doing it. But it's also, I don't necessarily\n\n1:19:11.920 --> 1:19:15.520\n agree with you because you've kind of mentioned that AI will not say no to us.\n\n1:19:16.480 --> 1:19:23.520\n I just think they will. Yeah. Yeah. So like, uh, I think it's a useful feature to build in. I'm\n\n1:19:23.520 --> 1:19:30.480\n just trying to like, uh, put myself in the mind of engineers to sometimes say no, you know, if you,\n\n1:19:32.480 --> 1:19:38.000\n I gave the example earlier, right? I gave the example of my car, right? My car turns the wheel\n\n1:19:38.000 --> 1:19:43.760\n and, and applies the accelerator and the brake as I say, until it decides there's something dangerous.\n\n1:19:43.760 --> 1:19:50.240\n Yes. And then it doesn't do that. Now that was something it didn't decide to do. It's something\n\n1:19:50.240 --> 1:19:57.600\n we programmed into the car. And so good. It was a good idea, right? The question again, isn't like\n\n1:19:57.600 --> 1:20:02.640\n if we create an intelligent system, will it ever ignore our commands? Of course it will. And\n\n1:20:02.640 --> 1:20:08.560\n sometimes is it going to do it because it came up, came up with its own goals that serve its purposes\n\n1:20:08.560 --> 1:20:11.680\n and it doesn't care about our purposes? No, I don't think that's going to happen.\n\n1:20:12.480 --> 1:20:16.960\n Okay. So let me ask you about these, uh, super intelligent cortical systems that we engineer\n\n1:20:16.960 --> 1:20:23.760\n and us humans, do you think, uh, with these entities operating out there in the world,\n\n1:20:24.320 --> 1:20:32.320\n what is the future most promising future look like? Is it us merging with them or is it us?\n\n1:20:33.040 --> 1:20:38.880\n Like, how do we keep us humans around when you have increasingly intelligent beings? Is it, uh,\n\n1:20:38.880 --> 1:20:42.960\n one of the dreams is to upload our minds in the digital space. So can we just\n\n1:20:42.960 --> 1:20:48.400\n give our minds to these, uh, systems so they can operate on them? Is there some kind of more\n\n1:20:48.400 --> 1:20:52.240\n interesting merger or is there more, more communication? I talked about all these\n\n1:20:52.240 --> 1:21:00.560\n scenarios and let me just walk through them. Sure. Um, the uploading the mind one. Yes. Extremely,\n\n1:21:00.560 --> 1:21:06.480\n really difficult to do. Like, like, we have no idea how to do this even remotely right now. Um,\n\n1:21:06.480 --> 1:21:11.280\n so it would be a very long way away, but I make the argument you wouldn't like the result.\n\n1:21:11.280 --> 1:21:16.080\n Um, and you wouldn't be pleased with the result. It's really not what you think it's going to be.\n\n1:21:16.080 --> 1:21:20.000\n Um, imagine I could upload your brain into a, into a computer right now. And now the computer\n\n1:21:20.000 --> 1:21:24.160\n sitting there going, Hey, I'm over here. Great. Get rid of that old bio person. I don't need them.\n\n1:21:24.160 --> 1:21:28.560\n You're still sitting here. Yeah. What are you going to do? No, no, that's not me. I'm here.\n\n1:21:28.560 --> 1:21:33.600\n Right. Are you going to feel satisfied then? Then you, but people imagine, look, I'm on my deathbed\n\n1:21:33.600 --> 1:21:38.240\n and I'm about to, you know, expire and I pushed the button and now I'm uploaded. But think about\n\n1:21:38.240 --> 1:21:42.640\n it a little differently. And, and so I don't think it's going to be a thing because people,\n\n1:21:42.640 --> 1:21:47.760\n by the time we're able to do this, if ever, because you have to replicate the entire body,\n\n1:21:47.760 --> 1:21:52.240\n not just the brain. It's, it's really, it's, I walked through the issues. It's really substantial.\n\n1:21:52.240 --> 1:21:59.520\n Um, do you have a sense of what makes us us? Is there, is there a shortcut to what can only save\n\n1:21:59.520 --> 1:22:04.720\n a certain part that makes us truly ours? No, but I think that machine would feel like it's you too.\n\n1:22:04.720 --> 1:22:08.400\n Right. Right. You have two people, just like I have a child, I have a child, right? I have two\n\n1:22:08.400 --> 1:22:16.160\n daughters. They're independent people. I created them. Well, partly. Yeah. And, um, uh, I don't,\n\n1:22:16.160 --> 1:22:20.400\n just because they're somewhat like me, I don't feel on them and they don't feel like I'm me. So\n\n1:22:20.400 --> 1:22:24.080\n if you split apart, you have two people. So we can tell them, come back to what, what makes,\n\n1:22:24.080 --> 1:22:28.400\n what consciousness do you want? We can talk about that, but we don't have like remote consciousness.\n\n1:22:28.400 --> 1:22:32.000\n I'm not sitting there going, Oh, I'm conscious of that. You know, I mean, that system of,\n\n1:22:32.000 --> 1:22:38.480\n so let's say, let's, let's stay on our topic. One was uploading a brand. Yep. It ain't gonna happen\n\n1:22:38.480 --> 1:22:43.280\n in a hundred years, maybe a thousand, but I don't think people are going to want to do it. The\n\n1:22:44.080 --> 1:22:50.240\n merging your mind with, uh, you know, the neural link thing, right? Like again, really, really\n\n1:22:50.240 --> 1:22:54.720\n difficult. It's, it's one thing to make progress, to control a prosthetic arm. It's another to have\n\n1:22:54.720 --> 1:22:58.960\n like a billion or several billion, you know, things and understanding what those signals\n\n1:22:58.960 --> 1:23:03.680\n mean. Like it's the one thing that like, okay, I can learn to think some patterns to make something\n\n1:23:03.680 --> 1:23:08.800\n happen. It's quite another thing to have a system, a computer, which actually knows exactly what\n\n1:23:08.800 --> 1:23:12.960\n cells it's talking to and how it's talking to them and interacting in a way like that. Very,\n\n1:23:12.960 --> 1:23:18.160\n very difficult. We're not getting anywhere closer to that. Um, interesting. Can I, can I, uh, can\n\n1:23:18.160 --> 1:23:24.880\n I ask a question here? What, so for me, what makes that merger very difficult practically in the next\n\n1:23:24.880 --> 1:23:32.000\n 10, 20, 50 years is like literally the biology side of it, which is like, it's just hard to do\n\n1:23:32.000 --> 1:23:38.640\n that kind of surgery in a safe way. But your intuition is even the machine learning part of it,\n\n1:23:38.640 --> 1:23:43.280\n where the machine has to learn what the heck it's talking to. That's even hard. I think it's even\n\n1:23:43.280 --> 1:23:49.200\n harder. And it's not, it's, it's easy to do when you're talking about hundreds of signals. It's,\n\n1:23:49.200 --> 1:23:53.840\n it's a totally different thing to say, talking about billions of years. It's, it's a totally\n\n1:23:53.840 --> 1:23:57.440\n different thing to say, talking about billions of signals. So you don't think it's the raw,\n\n1:23:57.440 --> 1:24:01.360\n the it's a machine learning problem. You don't think it could be learned? Well, I'm just saying,\n\n1:24:01.360 --> 1:24:05.440\n no, I think you'd have to have detailed knowledge. You'd have to know exactly what the types of\n\n1:24:05.440 --> 1:24:09.440\n neurons you're connecting to. I mean, in the brain, there's these, there are all different\n\n1:24:09.440 --> 1:24:13.520\n types of things. It's not like a neural network. It's a very complex organism system up here. We\n\n1:24:13.520 --> 1:24:16.640\n talked about the grid cells or the place cells, you know, you have to know what kind of cells\n\n1:24:16.640 --> 1:24:20.640\n you're talking to and what they're doing and how their timing works and all, all this stuff,\n\n1:24:20.640 --> 1:24:24.960\n which you can't today. There's no way of doing that. Right. But I think it's, I think it's a,\n\n1:24:24.960 --> 1:24:28.400\n I think the problem you're right. That the biological aspect of like who wants to have\n\n1:24:28.400 --> 1:24:32.640\n a surgery and have this stuff inserted in your brain. That's a problem. But this is when we\n\n1:24:32.640 --> 1:24:38.080\n solve that problem. I think the, the information coding aspect is much worse. I think that's much\n\n1:24:38.080 --> 1:24:41.600\n worse. It's not like what they're doing today. Today. It's simple machine learning stuff\n\n1:24:42.240 --> 1:24:46.720\n because you're doing simple things. But if you want to merge your brain, like I'm thinking on\n\n1:24:46.720 --> 1:24:51.440\n the internet, I'm merged my brain with the machine and we're both doing, that's a totally different\n\n1:24:51.440 --> 1:24:56.720\n issue. That's interesting. I tend to think if the, okay. If you have a super clean signal\n\n1:24:57.760 --> 1:25:04.400\n from a bunch of neurons at the start, you don't know what those neurons are. I think that's much\n\n1:25:04.400 --> 1:25:10.880\n easier than the getting of the clean signal. I think if you think about today's machine learning,\n\n1:25:10.880 --> 1:25:14.960\n that's what you would conclude. Right. I'm thinking about what's going on in the brain\n\n1:25:14.960 --> 1:25:19.520\n and I don't reach that conclusion. So we'll have to see. Sure. But I don't think even, even then,\n\n1:25:20.080 --> 1:25:26.240\n I think this kind of a sad future. Like, you know, do I, do I have to like plug my brain\n\n1:25:26.240 --> 1:25:29.440\n into a computer? I'm still a biological organism. I assume I'm still going to die.\n\n1:25:30.000 --> 1:25:36.640\n So what have I achieved? Right. You know, what have I achieved? Oh, I disagree that we don't\n\n1:25:36.640 --> 1:25:40.320\n know what those are, but it seems like there could be a lot of different applications. It's\n\n1:25:40.320 --> 1:25:47.280\n like virtual reality is to expand your brain's capability to, to like, to read Wikipedia.\n\n1:25:47.280 --> 1:25:50.080\n Yeah. But, but fine. But, but you're still a biological organism.\n\n1:25:50.080 --> 1:25:53.280\n Yes. Yes. You know, you're still, you're still mortal. All right. So,\n\n1:25:53.280 --> 1:25:57.360\n so what are you accomplishing? You're making your life in this short period of time better. Right.\n\n1:25:58.000 --> 1:26:03.760\n Just like having the internet made our life better. Yeah. Yeah. Okay. So I think that's of,\n\n1:26:03.760 --> 1:26:08.080\n of, if I think about all the possible gains we can have here, that's a marginal one.\n\n1:26:08.080 --> 1:26:15.280\n It's an individual, Hey, I'm better, you know, I'm smarter. But you know, fine. I'm not against it.\n\n1:26:15.280 --> 1:26:20.240\n I just don't think it's earth changing. I, but, but it w so this is the true of the internet.\n\n1:26:20.240 --> 1:26:24.800\n When each of us individuals are smarter, we get a chance to then share our smartness.\n\n1:26:24.800 --> 1:26:28.560\n We get smarter and smarter together as like, as a collective, this is kind of like this\n\n1:26:28.560 --> 1:26:32.480\n ant colony. Why don't I just create an intelligent machine that doesn't have any of this biological\n\n1:26:32.480 --> 1:26:39.360\n nonsense that has all the same. It's everything except don't burden it with my brain. Yeah.\n\n1:26:39.360 --> 1:26:43.680\n Right. It has a brain. It is smart. It's like my child, but it's much, much smarter than me.\n\n1:26:43.680 --> 1:26:48.320\n So I have a choice between doing some implant, doing some hybrid, weird, you know, biological\n\n1:26:48.320 --> 1:26:53.760\n thing that bleeding and all these problems and limited by my brain or creating a system,\n\n1:26:53.760 --> 1:26:58.240\n which is super smart that I can talk to. Um, that helps me understand the world that can\n\n1:26:58.240 --> 1:27:03.600\n read the internet, you know, read Wikipedia and talk to me. I guess my, the open questions there\n\n1:27:03.600 --> 1:27:10.000\n are what does the men manifestation of super intelligence look like? So like, what are we\n\n1:27:10.000 --> 1:27:14.880\n going to, you, you talked about why do I want to merge with AI? Like what, what's the actual\n\n1:27:14.880 --> 1:27:23.680\n marginal benefit here? If I, if we have a super intelligent system, how will it make our life\n\n1:27:23.680 --> 1:27:28.240\n better? So let's, let's, that's a great question, but let's break it down to little pieces. All\n\n1:27:28.240 --> 1:27:32.400\n right. On the one hand, it can make our life better in lots of simple ways. You mentioned\n\n1:27:32.400 --> 1:27:36.960\n like a care robot or something that helps me do things. It cooks. I don't know what it does. Right.\n\n1:27:36.960 --> 1:27:42.080\n Little things like that. We have super better, smarter cars. We can have, you know, better agents\n\n1:27:42.640 --> 1:27:46.800\n aids helping us in our work environment and things like that. To me, that's like the easy stuff, the\n\n1:27:47.360 --> 1:27:53.200\n simple stuff in the beginning. Um, um, and so in the same way that computers made our lives better\n\n1:27:53.200 --> 1:27:59.600\n in ways, many, many ways, I will have those kinds of things. To me, the really exciting thing about AI\n\n1:28:00.560 --> 1:28:05.760\n is the sort of it's transcendent, transcendent quality in terms of humanity. We're still\n\n1:28:05.760 --> 1:28:09.760\n biological organisms. We're still stuck here on earth. It's going to be hard for us to live\n\n1:28:09.760 --> 1:28:14.960\n anywhere else. Uh, I don't think you and I are going to want to live on Mars anytime soon. Um,\n\n1:28:14.960 --> 1:28:22.880\n um, and, um, and we're flawed, you know, we may end up destroying ourselves. It's totally possible.\n\n1:28:23.440 --> 1:28:28.320\n Uh, we, if not completely, we could destroy our civilizations. You know, it's this face the fact\n\n1:28:28.320 --> 1:28:33.680\n we have issues here, but we can create intelligent machines that can help us in various ways. For\n\n1:28:33.680 --> 1:28:38.160\n example, one example I gave, and that sounds a little sci fi, but I believe this. If we really\n\n1:28:38.160 --> 1:28:42.560\n wanted to live on Mars, we'd have to have intelligent systems that go there and build\n\n1:28:42.560 --> 1:28:48.240\n the habitat for us, not humans. Humans are never going to do this. It's just too hard. Um, but could\n\n1:28:48.240 --> 1:28:53.120\n we have a thousand or 10,000, you know, engineer workers up there doing this stuff, building things,\n\n1:28:53.120 --> 1:28:57.840\n terraforming Mars? Sure. Maybe we can move Mars. But then if we want to, if we want to go around\n\n1:28:57.840 --> 1:29:02.400\n the universe, should I send my children around the universe or should I send some intelligent machine,\n\n1:29:02.400 --> 1:29:07.520\n which is like a child that represents me and understands our needs here on earth that could\n\n1:29:07.520 --> 1:29:13.280\n travel through space. Um, so it's sort of, it, in some sense, intelligence allows us to transcend\n\n1:29:13.280 --> 1:29:19.920\n our, the limitations of our biology, uh, with, and, and don't think of it as a negative thing.\n\n1:29:19.920 --> 1:29:26.000\n It's in some sense, my children transcend my, the, my biology too, cause they, they live beyond me.\n\n1:29:26.000 --> 1:29:30.480\n Yeah. Um, and we impart, they represent me and they also have their own knowledge and I can\n\n1:29:30.480 --> 1:29:34.400\n impart knowledge to them. So intelligent machines will be like that too, but not limited like us.\n\n1:29:34.400 --> 1:29:40.320\n I mean, but the question is, um, there's so many ways that transcendence can happen\n\n1:29:40.320 --> 1:29:45.440\n and the merger with AI and humans is one of those ways. So you said intelligent,\n\n1:29:46.960 --> 1:29:52.000\n basically beings or systems propagating throughout the universe, representing us humans.\n\n1:29:53.280 --> 1:29:56.560\n They represent us humans in the sense they represent our knowledge and our history,\n\n1:29:56.560 --> 1:30:04.240\n not us individually. Right. Right. But I mean, the question is, is it just a database\n\n1:30:04.960 --> 1:30:09.600\n with, uh, with the really damn good, uh, model of the world?\n\n1:30:09.600 --> 1:30:12.800\n It's conscious, it's conscious just like us. Okay. But just different?\n\n1:30:12.800 --> 1:30:16.560\n They're different. Uh, just like my children are different. They're like me, but they're\n\n1:30:16.560 --> 1:30:22.560\n different. Um, these are more different. I guess maybe I've already, I kind of,\n\n1:30:22.560 --> 1:30:28.320\n I take a very broad view of our life here on earth. I say, you know, why are we living here?\n\n1:30:28.320 --> 1:30:32.960\n Are we just living because we live? Is it, are we surviving because we can survive? Are we fighting\n\n1:30:32.960 --> 1:30:38.880\n just because we want to just keep going? What's the point of it? Right. So to me, the point,\n\n1:30:38.880 --> 1:30:46.000\n if I asked myself, what's the point of life is what's transcends that ephemeral sort of biological\n\n1:30:46.000 --> 1:30:53.520\n experience is to me, this is my answer is the acquisition of knowledge to understand more about\n\n1:30:53.520 --> 1:31:01.040\n the universe, uh, and to explore. And that's partly to learn more. Right. Um, I don't view it as\n\n1:31:01.920 --> 1:31:09.040\n a terrible thing. If the ultimate outcome of humanity is we create systems that are intelligent\n\n1:31:09.040 --> 1:31:13.680\n that are offspring, but they're not like us at all. And we stay, we stay here and live on earth\n\n1:31:13.680 --> 1:31:20.960\n as long as we can, which won't be forever, but as long as we can and, but that would be a great\n\n1:31:20.960 --> 1:31:29.760\n thing to do. It's not a, it's not like a negative thing. Well, would, uh, you be okay then if, uh,\n\n1:31:29.760 --> 1:31:37.440\n the human species vanishes, but our knowledge is preserved and keeps being expanded by intelligence\n\n1:31:37.440 --> 1:31:44.960\n systems. I want our knowledge to be preserved and expanded. Yeah. Am I okay with humans dying? No,\n\n1:31:44.960 --> 1:31:50.400\n I don't want that to happen. But if it, if it does happen, what if we were sitting here and this is\n\n1:31:50.400 --> 1:31:53.920\n all the real, the last two people on earth and we're saying, Lex, we blew it. It's all over.\n\n1:31:53.920 --> 1:31:59.520\n Right. Wouldn't I feel better if I knew that our knowledge was preserved and that we had agents\n\n1:32:00.080 --> 1:32:04.800\n that knew about that, that were trans, you know, there were that left earth. I wouldn't want that.\n\n1:32:04.800 --> 1:32:08.240\n Mm. It's better than not having that, you know, I make the analogy of like, you know,\n\n1:32:08.240 --> 1:32:11.520\n the dinosaurs, the poor dinosaurs, they live for, you know, tens of millions of years.\n\n1:32:11.520 --> 1:32:15.840\n They raised their kids. They, you know, they, they fought to survive. They were hungry. They,\n\n1:32:15.840 --> 1:32:20.960\n they did everything we do. And then they're all gone. Yeah. Like, you know, and, and if we didn't\n\n1:32:20.960 --> 1:32:27.600\n discover their bones, nobody would ever know that they ever existed. Right. Do we want to be like\n\n1:32:27.600 --> 1:32:32.720\n that? I don't want to be like that. There's a sad aspect to it. And it's kind of, it's jarring to\n\n1:32:32.720 --> 1:32:39.600\n think about that. It's possible that a human like intelligence civilization has previously existed\n\n1:32:39.600 --> 1:32:46.640\n on earth. The reason I say this is like, it is jarring to think that we would not, if they went\n\n1:32:46.640 --> 1:32:53.040\n extinct, we wouldn't be able to find evidence of them after a sufficient amount of time. Of course,\n\n1:32:53.040 --> 1:32:58.800\n there's like, like basically humans, like if we destroy ourselves now, the human civilization\n\n1:32:58.800 --> 1:33:03.280\n destroyed ourselves. Now, after a sufficient amount of time, we would not be, we'd find evidence of\n\n1:33:03.280 --> 1:33:08.640\n the dinosaurs would not find evidence of humans. Yeah. That's kind of an odd thing to think about.\n\n1:33:08.640 --> 1:33:14.880\n Although I'm not sure if we have enough knowledge about species going back for billions of years,\n\n1:33:14.880 --> 1:33:18.960\n but we could, we could, we might be able to eliminate that possibility, but it's an interesting\n\n1:33:18.960 --> 1:33:23.200\n question. Of course, this is a similar question to, you know, there were lots of intelligent\n\n1:33:23.200 --> 1:33:29.680\n species throughout our galaxy that have all disappeared. That's super sad that they're,\n\n1:33:30.320 --> 1:33:36.000\n exactly that there may have been much more intelligent alien civilizations in our galaxy\n\n1:33:36.000 --> 1:33:42.480\n that are no longer there. Yeah. You actually talked about this, that humans might destroy\n\n1:33:42.480 --> 1:33:53.920\n ourselves and how we might preserve our knowledge and advertise that knowledge to other. Advertise\n\n1:33:53.920 --> 1:33:58.080\n is a funny word to use. From a PR perspective. There's no financial gain in this.\n\n1:34:00.720 --> 1:34:04.480\n You know, like make it like from a tourism perspective, make it interesting. Can you\n\n1:34:04.480 --> 1:34:07.600\n describe how you think about this problem? Well, there's a couple things. I broke it down\n\n1:34:07.600 --> 1:34:14.960\n into two parts, actually three parts. One is, you know, there's a lot of things we know that,\n\n1:34:14.960 --> 1:34:19.280\n what if, what if we were, what if we ended, what if our civilization collapsed? Yeah. I'm not\n\n1:34:19.280 --> 1:34:22.400\n talking tomorrow. Yeah. We could be a thousand years from now, like, so, you know, we don't\n\n1:34:22.400 --> 1:34:26.720\n really know, but, but historically it would be likely at some point. Time flies when you're\n\n1:34:26.720 --> 1:34:33.200\n having fun. Yeah. That's a good way to put it. You know, could we, and then intelligent life\n\n1:34:33.200 --> 1:34:37.680\n evolved again on this planet. Wouldn't they want to know a lot about us and what we knew? Wouldn't\n\n1:34:37.680 --> 1:34:42.000\n they wouldn't be able to ask us questions? So one very simple thing I said, how would we archive\n\n1:34:42.000 --> 1:34:46.080\n what we know? That was a very simple idea. I said, you know what, that wouldn't be that hard to put\n\n1:34:46.080 --> 1:34:51.200\n a few satellites, you know, going around the sun and we'd upload Wikipedia every day and that kind\n\n1:34:51.200 --> 1:34:55.600\n of thing. So, you know, if we end up killing ourselves, well, it's up there and the next intelligent\n\n1:34:55.600 --> 1:34:58.720\n species will find it and learn something. They would like that. They would appreciate that.\n\n1:34:58.720 --> 1:35:05.360\n Um, uh, so that's one thing. The next thing I said, well, what if, you know, how outside,\n\n1:35:05.360 --> 1:35:09.680\n outside of our solar system, we have the SETI program. We're looking for these intelligent\n\n1:35:09.680 --> 1:35:14.320\n signals from everybody. And if you do a little bit of math, which I did in the book, uh, and\n\n1:35:14.320 --> 1:35:18.800\n you say, well, what if intelligent species only live for 10,000 years before, you know,\n\n1:35:18.800 --> 1:35:22.560\n technologically intelligent species, like ones are really able to do the stuff we're just starting\n\n1:35:22.560 --> 1:35:26.800\n to be able to do. Um, well, the chances are we wouldn't be able to see any of them because they\n\n1:35:26.800 --> 1:35:31.040\n would have all been disappeared by now. Um, they would, they've lived for 10,000 years and now\n\n1:35:31.040 --> 1:35:36.080\n they're gone. And so we're not going to find these signals being sent from these people because, um,\n\n1:35:36.080 --> 1:35:40.560\n but I said, what kind of signal could you create that would last a million years or a billion years\n\n1:35:41.120 --> 1:35:46.080\n that someone would say, dammit, someone smart lived there that we know that that would be a\n\n1:35:46.080 --> 1:35:49.760\n life changing event for us to figure that out. Well, what we're looking for today in the study\n\n1:35:49.760 --> 1:35:54.560\n program, isn't that we're looking for very coded signals in some sense. Um, and so I asked myself,\n\n1:35:54.560 --> 1:35:58.160\n what would be a different type of signal one could create? Um, I've always thought about\n\n1:35:58.160 --> 1:36:04.480\n this throughout my life. And in the book, I gave one, one possible suggestion, which was, um, uh,\n\n1:36:04.480 --> 1:36:11.040\n we now detect planets going around other, other suns, uh, other stars, uh, excuse me. And we do\n\n1:36:11.040 --> 1:36:14.800\n that by seeing this, the, the slight dimming of the light as the planets move in front of them.\n\n1:36:14.800 --> 1:36:21.040\n That's how, uh, we detect, uh, planets elsewhere in our galaxy. Um, what if we created something\n\n1:36:21.040 --> 1:36:26.480\n like that, that just rotated around our, our, our, around the sun and it blocked out a little\n\n1:36:26.480 --> 1:36:31.760\n bit of light in a particular pattern that someone said, Hey, that's not a planet. That is a sign\n\n1:36:31.760 --> 1:36:36.000\n that someone was once there. You can say, what if it's beating up pie, you know, three point,\n\n1:36:36.000 --> 1:36:44.960\n whatever. Um, so I did it from a distance. Broadly broadcast takes no continue activation on our\n\n1:36:44.960 --> 1:36:48.320\n part. This is the key, right? No one has to be senior running a computer and supplying it with\n\n1:36:48.320 --> 1:36:55.200\n power. It just goes on. So we go, it's continuous. And, and I argued that part of the study program\n\n1:36:55.200 --> 1:36:58.880\n should be looking for signals like that. And to look for signals like that, you ought to figure\n\n1:36:58.880 --> 1:37:03.440\n out what the, how would we create a signal? Like what would we create that would be like that,\n\n1:37:03.440 --> 1:37:07.680\n that would persist for millions of years that would be broadcast broadly. You could see from\n\n1:37:07.680 --> 1:37:13.760\n a distance that was unequivocal, came from an intelligent species. And so I gave that one\n\n1:37:13.760 --> 1:37:18.480\n example. Um, cause they don't know what I know of actually. And then, and then finally, right.\n\n1:37:19.760 --> 1:37:26.640\n If, if our, ultimately our solar system will die at some point in time, you know, how do we go\n\n1:37:26.640 --> 1:37:31.600\n beyond that? And I think it's possible if it all possible, we'll have to create intelligent machines\n\n1:37:31.600 --> 1:37:36.880\n that travel throughout the, throughout the solar system or the galaxy. And I don't think that's\n\n1:37:36.880 --> 1:37:41.040\n going to be humans. I don't think it's going to be biological organisms. So these are just things to\n\n1:37:41.040 --> 1:37:44.560\n think about, you know, like, what's the old, you know, I don't want to be like the dinosaur. I\n\n1:37:44.560 --> 1:37:48.400\n don't want to just live in, okay, that was it. We're done. You know, well, there is a kind of\n\n1:37:48.400 --> 1:37:55.280\n presumption that we're going to live forever, which, uh, I think it is a bit sad to imagine\n\n1:37:55.280 --> 1:38:03.680\n that the message we send as, as you talk about is that we were once here instead of we are here.\n\n1:38:03.680 --> 1:38:09.520\n Well, it could be, we are still here. Uh, but it's more of a, it's more of an insurance policy\n\n1:38:09.520 --> 1:38:16.080\n in case we're not here, you know? Well, I don't know, but there is something I think about,\n\n1:38:16.080 --> 1:38:22.080\n we as humans don't often think about this, but it's like, like whenever I, um,\n\n1:38:23.680 --> 1:38:28.160\n record a video, I've done this a couple of times in my life. I've recorded a video for my future\n\n1:38:28.160 --> 1:38:34.400\n self, just for personal, just for fun. And it's always just fascinating to think about\n\n1:38:34.400 --> 1:38:41.600\n that preserving yourself for future civilizations. For me, it was preserving myself for a future me,\n\n1:38:41.600 --> 1:38:46.160\n but that's a little, that's a little fun example of archival.\n\n1:38:46.160 --> 1:38:50.720\n Well, these podcasts are, are, are preserving you and I in a way. Yeah. For future,\n\n1:38:51.280 --> 1:38:56.640\n hopefully well after we're gone. But you don't often, we're sitting here talking about this.\n\n1:38:56.640 --> 1:39:02.800\n You are not thinking about the fact that you and I are going to die and there'll be like 10 years\n\n1:39:02.800 --> 1:39:09.440\n after somebody watching this and we're still alive. You know, in some sense I do. I'm here\n\n1:39:09.440 --> 1:39:16.720\n cause I want to talk about ideas and these ideas transcend me and they transcend this time and, and\n\n1:39:16.720 --> 1:39:23.520\n on our planet. Um, we're talking here about ideas that could be around a thousand years from now.\n\n1:39:23.520 --> 1:39:29.360\n Or a million years from now. I, when I wrote my book, I had an audience in mind and one of the\n\n1:39:29.360 --> 1:39:35.200\n clearest audiences was aliens. No. Were people reading this a hundred years from now? Yes.\n\n1:39:35.200 --> 1:39:39.360\n I said to myself, how do I make this book relevant to someone reading this a hundred years from now?\n\n1:39:39.360 --> 1:39:44.160\n What would they want to know that we were thinking back then? What would make it like,\n\n1:39:44.160 --> 1:39:49.360\n that was an interesting, it's still an interesting book. I'm not sure I can achieve that, but that was\n\n1:39:49.360 --> 1:39:53.440\n how I thought about it because these ideas, like especially in the third part of the book, the ones\n\n1:39:53.440 --> 1:39:56.960\n we were just talking about, you know, these crazy, sounds like crazy ideas about, you know,\n\n1:39:56.960 --> 1:40:01.680\n storing our knowledge and, and, you know, merging our brains with computers and, and sending, you\n\n1:40:01.680 --> 1:40:07.360\n know, our machines out into space. It's not going to happen in my lifetime. Um, and they may not\n\n1:40:07.360 --> 1:40:10.640\n have been happening in the next hundred years. They may not happen for a thousand years. Who knows?\n\n1:40:10.640 --> 1:40:17.440\n Uh, but we have the unique opportunity right now. We, you, me, and other people in the world,\n\n1:40:17.440 --> 1:40:24.640\n right now, we, you, me, and other people like this, um, to sort of at least propose the agenda,\n\n1:40:24.640 --> 1:40:29.840\n um, that might impact the future like that. That's a fascinating way to think, uh, both like\n\n1:40:29.840 --> 1:40:37.680\n writing or creating, try to make, try to create ideas, try to create things that, uh, hold up\n\n1:40:38.400 --> 1:40:42.240\n in time. Yeah. You know, when understanding how the brain works, we're going to figure that out\n\n1:40:42.240 --> 1:40:46.720\n once. That's it. It's going to be figured out once. And after that, that's the answer. And\n\n1:40:46.720 --> 1:40:51.600\n people will, people will study that thousands of years now. We still, we still, you know,\n\n1:40:51.600 --> 1:40:59.040\n venerate Newton and, and Einstein and, um, and, you know, because, because ideas are exciting,\n\n1:40:59.040 --> 1:41:04.800\n even well into the future. Well, the interesting thing is like big ideas, even if they're wrong,\n\n1:41:05.520 --> 1:41:12.800\n are still useful. Like, yeah, especially if they're not completely wrong, right? Right.\n\n1:41:12.800 --> 1:41:19.840\n Newton's laws are not wrong. They're just Einstein's they're better. Um, so yeah, I mean,\n\n1:41:19.840 --> 1:41:23.440\n but we're talking with Newton and Einstein, we're talking about physics. I wonder if we'll ever\n\n1:41:23.440 --> 1:41:30.880\n achieve that kind of clarity, but understanding, um, like complex systems and the, this particular\n\n1:41:30.880 --> 1:41:36.160\n manifestation of complex systems, which is the human brain. I'm totally optimistic. We can do\n\n1:41:36.160 --> 1:41:41.440\n that. I mean, we're making progress at it. I don't see any reasons why we can't completely. I mean,\n\n1:41:41.440 --> 1:41:46.080\n completely understand in the sense, um, you know, we don't really completely understand what all\n\n1:41:46.080 --> 1:41:50.080\n the molecules in this water bottle are doing, but, you know, we have laws that sort of capture it\n\n1:41:50.080 --> 1:41:54.960\n pretty good. Um, and, uh, so we'll have that kind of understanding. I mean, it's not like you're\n\n1:41:54.960 --> 1:42:00.880\n gonna have to know what every neuron in your brain is doing. Um, but enough to, um, first of all,\n\n1:42:00.880 --> 1:42:06.400\n to build it. And second of all, to do, you know, do what physics does, which is like have, uh,\n\n1:42:06.400 --> 1:42:12.400\n concrete experiments where we can validate this is happening right now. Like it's not,\n\n1:42:12.400 --> 1:42:17.760\n this is not some future thing. Um, you know, I'm very optimistic about it because I know about our,\n\n1:42:17.760 --> 1:42:22.320\n our work and what we're doing. We'll have to prove it to people. Um, but, um,\n\n1:42:24.480 --> 1:42:30.640\n I, I consider myself a rational person and, um, you know, until fairly recently,\n\n1:42:30.640 --> 1:42:33.840\n I wouldn't have said that, but right now I'm, where I'm sitting right now, I'm saying, you know,\n\n1:42:33.840 --> 1:42:39.200\n we, we could, this is going to happen. There's no big obstacles to it. Um, we finally have a\n\n1:42:39.200 --> 1:42:44.960\n framework for understanding what's going on in the cortex and, um, and that's liberating. It's,\n\n1:42:44.960 --> 1:42:50.080\n it's like, Oh, it's happening. So I can't see why we wouldn't be able to understand it. I just can't.\n\n1:42:50.880 --> 1:42:54.560\n Okay. So, I mean, on that topic, let me ask you to play devil's advocate.\n\n1:42:54.560 --> 1:43:02.320\n Is it possible for you to imagine, look, look a hundred years from now and looking at your book,\n\n1:43:02.320 --> 1:43:09.840\n uh, in which ways might your ideas be wrong? Oh, I worry about this all the time. Um,\n\n1:43:11.840 --> 1:43:15.200\n yeah, it's still useful. Yeah. Yeah.\n\n1:43:15.200 --> 1:43:24.800\n Yeah. I think there's, you know, um, well I can, I can best relate it to like things I'm worried\n\n1:43:24.800 --> 1:43:29.920\n about right now. So we talked about this voting idea, right? It's happening. There's no question.\n\n1:43:29.920 --> 1:43:36.480\n It's happening, but it could be far more, um, um, there's, there's enough things I don't know about\n\n1:43:36.480 --> 1:43:41.520\n it that it might be working into ways differently than I'm thinking about the kind of what's voting,\n\n1:43:41.520 --> 1:43:45.680\n who's voting, you know, where are representations? I talked about, like, you have a thousand models\n\n1:43:45.680 --> 1:43:52.320\n of a coffee cup like that. That could turn out to be wrong. Um, because it may be, maybe there are a\n\n1:43:52.320 --> 1:43:56.400\n thousand models that are sub models, but not really a single model of the coffee cup. Um,\n\n1:43:57.120 --> 1:44:02.000\n I mean, there's things, these are all sort of on the edges, things that I present as like,\n\n1:44:02.000 --> 1:44:05.440\n Oh, it's so simple and clean. Well, it's not that it's always going to be more complex.\n\n1:44:05.440 --> 1:44:14.640\n And, um, and there's parts of the theory, which I don't understand the complexity well. So I think,\n\n1:44:14.640 --> 1:44:19.440\n I think the idea that this brain is a distributed modeling system is not controversial at all. Right.\n\n1:44:19.440 --> 1:44:22.720\n It's not, that's well understood by many people. The question then is,\n\n1:44:22.720 --> 1:44:29.040\n are each quarter of a column an independent modeling system? Um, I could be wrong about that.\n\n1:44:29.040 --> 1:44:35.600\n Um, I don't think so, but I worry about it. My intuition, not even thinking why you could\n\n1:44:35.600 --> 1:44:41.440\n be wrong is the same intuition I have about any sort of physicist, uh, like string theory\n\n1:44:42.480 --> 1:44:50.160\n that we as humans desire for a clean explanation. And, uh, a hundred years from now, uh,\n\n1:44:50.160 --> 1:44:56.560\n intelligent systems might look back at us and laugh at how we try to get rid of the whole mess\n\n1:44:56.560 --> 1:45:03.680\n by having simple explanation when the reality is it's way messier. And in fact, it's impossible\n\n1:45:03.680 --> 1:45:08.960\n to understand. You can only build it. It's like this idea of complex systems and cellular automata\n\n1:45:08.960 --> 1:45:13.840\n is you can only launch the thing. You cannot understand it. Yeah. I think that, you know,\n\n1:45:13.840 --> 1:45:19.520\n the history of science suggests that's not likely to occur. Um, the history of science suggests that\n\n1:45:20.240 --> 1:45:25.920\n as a theorist and we're theorists, you look for simple explanations, right? Fully knowing\n\n1:45:25.920 --> 1:45:30.640\n that whatever simple explanation you're going to come up with is not going to be completely correct.\n\n1:45:30.640 --> 1:45:35.840\n I mean, it can't be, I mean, it's just, it's just more complexity, but that's the role of theorists\n\n1:45:35.840 --> 1:45:41.600\n play. They, they sort of, they give you a framework on which you now can talk about a problem and\n\n1:45:41.600 --> 1:45:46.480\n figure out, okay, now we can start digging more details. The best frameworks stick around while\n\n1:45:46.480 --> 1:45:53.440\n the details change. You know, again, you know, the classic example is Newton and Einstein, right? You\n\n1:45:53.440 --> 1:46:00.000\n know, um, Newton's theories are still used. They're still valuable. They're still practical. They're\n\n1:46:00.000 --> 1:46:05.120\n not like wrong. It's just, they've been refined. Yeah. But that's in physics. It's not obvious,\n\n1:46:05.120 --> 1:46:10.400\n by the way, it's not obvious for physics either that the universe should be such that's amenable\n\n1:46:10.400 --> 1:46:17.920\n to these simple. But it's so far, it appears to be as far as we can tell. Um, yeah. I mean,\n\n1:46:17.920 --> 1:46:23.040\n but as far as we could tell, and, but it's also an open question whether the brain is amenable to\n\n1:46:23.040 --> 1:46:28.960\n such clean theories. That's the, uh, not the brain, but intelligence. Well, I, I, I don't know. I would\n\n1:46:28.960 --> 1:46:37.120\n take intelligence out of it. Just say, you know, um, well, okay. Um, the evidence we have suggests\n\n1:46:37.120 --> 1:46:42.960\n that the human brain is, is a, at the one time extremely messy and complex, but there's some\n\n1:46:42.960 --> 1:46:48.240\n parts that are very regular and structured. That's why we started the neocortex. It's extremely\n\n1:46:48.240 --> 1:46:53.440\n regular in its structure. Yeah. And unbelievably so. And then I mentioned earlier, the other thing is\n\n1:46:53.440 --> 1:47:00.560\n it's, it's universal abilities. It is so flexible to learn so many things. We don't, we haven't\n\n1:47:00.560 --> 1:47:03.440\n figured out what it can't learn yet. We don't know, but we haven't figured it out yet, but it\n\n1:47:03.440 --> 1:47:09.040\n can learn things that it never was evolved to learn. So those give us hope. Um, that's why I\n\n1:47:09.040 --> 1:47:14.880\n went into this field because I said, you know, this regular structure, it's doing this amazing\n\n1:47:14.880 --> 1:47:19.680\n number of things. There's gotta be some underlying principles that are, that are common and other,\n\n1:47:19.680 --> 1:47:25.600\n other scientists have come up with the same conclusions. Um, and so it's promising and,\n\n1:47:25.600 --> 1:47:32.400\n um, and that's, and whether the theories play out exactly this way or not, that is the role that\n\n1:47:32.400 --> 1:47:38.080\n theorists play. And so far it's worked out well, even though, you know, maybe, you know, we don't\n\n1:47:38.080 --> 1:47:42.000\n understand all the laws of physics, but so far it's been pretty damn useful. The ones we have\n\n1:47:42.000 --> 1:47:49.280\n are our theories are pretty useful. You mentioned that, uh, we should not necessarily be,\n\n1:47:49.840 --> 1:47:54.400\n at least to the degree that we are worried about the existential risks of artificial intelligence\n\n1:47:55.200 --> 1:48:02.080\n relative to, uh, human risks from human nature being existential risk.\n\n1:48:02.720 --> 1:48:07.600\n What aspect of human nature worries you the most in terms of the survival of the human species?\n\n1:48:07.600 --> 1:48:15.440\n I mean, I'm disappointed in humanity, humans. I mean, all of us, I'm one. So I'm disappointed\n\n1:48:15.440 --> 1:48:23.200\n myself too. Um, it's kind of a sad state. There's two things that disappoint me. One is\n\n1:48:24.880 --> 1:48:30.640\n how it's difficult for us to separate our rational component of ourselves from our evolutionary\n\n1:48:30.640 --> 1:48:38.800\n heritage, which is, you know, not always pretty, you know, um, uh, rape is a, is an evolutionary\n\n1:48:38.800 --> 1:48:45.760\n good strategy for reproduction. Murder can be at times too, you know, making other people miserable\n\n1:48:45.760 --> 1:48:50.640\n at times is a good strategy for reproduction. It's just, and it's just, and, and so now that\n\n1:48:50.640 --> 1:48:54.640\n we know that, and yet we have this sort of, you know, we, you and I can have this very rational\n\n1:48:54.640 --> 1:48:59.680\n discussion talking about, you know, intelligence and brains and life and so on. So many, it seems\n\n1:48:59.680 --> 1:49:05.520\n like it's so hard. It's just a big, big transition to get humans, all humans to, to make the\n\n1:49:05.520 --> 1:49:11.360\n transition from be like, let's pay no attention to all that ugly stuff over here. Let's just focus\n\n1:49:11.360 --> 1:49:16.720\n on the interesting. What's unique about humanity is our knowledge and our intellect. But the fact\n\n1:49:16.720 --> 1:49:22.480\n that we're striving is in itself amazing, right? The fact that we're able to overcome that part.\n\n1:49:22.480 --> 1:49:28.720\n And it seems like we are more and more becoming successful at overcoming that part. That is the\n\n1:49:28.720 --> 1:49:33.760\n optimistic view. And I agree with you, but I worry about it. I'm not saying I'm worrying about it. I\n\n1:49:33.760 --> 1:49:38.320\n think that was your question. I still worry about it. Yes. You know, we could be in tomorrow because\n\n1:49:38.320 --> 1:49:43.200\n some terrorists could get nuclear bombs and, you know, blow us all up. Who knows? Right. The other\n\n1:49:43.200 --> 1:49:47.760\n thing I think I'm disappointed is, and it's just, I understand it. It's, I guess you can't really\n\n1:49:47.760 --> 1:49:53.120\n be disappointed. It's just a fact is that we're so prone to false beliefs that we, you know, we have\n\n1:49:53.120 --> 1:50:00.080\n a model in our head, the things we can interact with directly, physical objects, people, that\n\n1:50:00.080 --> 1:50:04.800\n model is pretty good. And we can test it all the time, right? I touch something, I look at it,\n\n1:50:04.800 --> 1:50:09.760\n talk to you, see if my model is correct. But so much of what we know is stuff I can't directly\n\n1:50:09.760 --> 1:50:16.560\n interact with. I only know because someone told me about it. And so we're prone, inherently prone\n\n1:50:16.560 --> 1:50:20.560\n to having false beliefs because if I'm told something, how am I going to know it's right\n\n1:50:20.560 --> 1:50:26.800\n or wrong? Right. And so then we have the scientific process, which says we are inherently flawed.\n\n1:50:26.800 --> 1:50:34.800\n So the only way we can get closer to the truth is by looking for contrary evidence.\n\n1:50:34.800 --> 1:50:41.600\n Yeah. Like this conspiracy theory, this theory that scientists keep telling me about that the\n\n1:50:41.600 --> 1:50:46.960\n earth is round. As far as I can tell, when I look out, it looks pretty flat.\n\n1:50:46.960 --> 1:50:55.440\n Yeah. So, yeah, there is a tension, but it's also, I tend to believe that we haven't figured\n\n1:50:55.440 --> 1:51:02.240\n out most of this thing, right? Most of nature around us is a mystery. And so it...\n\n1:51:02.240 --> 1:51:06.080\n But that doesn't, does that worry you? I mean, it's like, oh, that's like a pleasure,\n\n1:51:06.080 --> 1:51:09.760\n more to figure out, right? Yeah. That's exciting. But I'm saying like\n\n1:51:09.760 --> 1:51:16.320\n there's going to be a lot of quote unquote, wrong ideas. I mean, I've been thinking a lot about\n\n1:51:16.320 --> 1:51:21.120\n engineering systems like social networks and so on. And I've been worried about censorship\n\n1:51:21.760 --> 1:51:25.520\n and thinking through all that kind of stuff, because there's a lot of wrong ideas. There's a\n\n1:51:25.520 --> 1:51:33.360\n lot of dangerous ideas, but then I also read a history, read history and see when you censor\n\n1:51:33.360 --> 1:51:39.760\n ideas that are wrong. Now this could be a small scale censorship, like a young grad student who\n\n1:51:39.760 --> 1:51:46.320\n comes up, who like raises their hand and says some crazy idea. A form of censorship could be,\n\n1:51:46.320 --> 1:51:52.000\n I shouldn't use the word censorship, but like de incentivize them from no, no, no, no,\n\n1:51:52.000 --> 1:51:54.800\n this is the way it's been done. Yeah. Yeah. You're a foolish kid. Don't\n\n1:51:54.800 --> 1:51:57.600\n think that's it. Yeah. You're foolish. So in some sense,\n\n1:51:59.760 --> 1:52:05.520\n those wrong ideas, most of the time end up being wrong, but sometimes end up being\n\n1:52:05.520 --> 1:52:11.280\n I agree with you. So I don't like the word censorship. Um, at the very end of the book, I,\n\n1:52:11.280 --> 1:52:20.000\n I ended up with a sort of a, um, a plea or a recommended force of action. Um, the best way I\n\n1:52:20.000 --> 1:52:26.240\n could, I know how to deal with this issue that you bring up is if everybody understood as part of\n\n1:52:26.240 --> 1:52:31.120\n your upbringing in life, something about how your brain works, that it builds a model of the world,\n\n1:52:31.120 --> 1:52:34.960\n uh, how it works, you know, how basically it builds that model of the world and that the model\n\n1:52:34.960 --> 1:52:39.760\n is not the real world. It's just a model and it's never going to reflect the entire world. And it\n\n1:52:39.760 --> 1:52:44.320\n can be wrong and it's easy to be wrong. And here's all the ways you can get a wrong model in your\n\n1:52:44.320 --> 1:52:50.960\n head. Right? It's not prescribed what's right or wrong. Just understand that process. If we all\n\n1:52:50.960 --> 1:52:54.720\n understood the processes and I got together and you say, I disagree with you, Jeff. And I said,\n\n1:52:54.720 --> 1:52:59.680\n Lex, I disagree with you that at least we understand that we're both trying to model\n\n1:52:59.680 --> 1:53:03.760\n something. We both have different information, which leads to our different models. And therefore\n\n1:53:03.760 --> 1:53:07.760\n I shouldn't hold it against you and you shouldn't hold it against me. And we can at least agree that,\n\n1:53:07.760 --> 1:53:13.600\n well, what can we look for in that's common ground to test our, our beliefs, as opposed to so much,\n\n1:53:13.600 --> 1:53:20.080\n uh, as we raise our kids on dogma, which is this is a fact, this is a fact, and these people are\n\n1:53:20.080 --> 1:53:31.120\n bad. And, and, and, you know, where every, if everyone knew just to, to be skeptical of every\n\n1:53:31.120 --> 1:53:35.440\n belief and why, and how their brains do that, I think we might have a better world.\n\n1:53:36.560 --> 1:53:45.600\n Do you think the human mind is able to comprehend reality? So you talk about this creating models\n\n1:53:45.600 --> 1:53:51.440\n how close do you think we get to, uh, to reality? There's so the wildest ideas is like Donald\n\n1:53:51.440 --> 1:53:56.560\n Hoffman saying, we're very far away from reality. Do you think we're getting close to reality?\n\n1:53:56.560 --> 1:54:02.000\n Well, it depends on what you define reality. Uh, we are getting, we have a model of the world\n\n1:54:02.000 --> 1:54:10.000\n that's very useful, right? For, for basic goals. Well, for our survival and our pleasure right\n\n1:54:10.000 --> 1:54:16.560\n now. Right. Um, so that's useful. Um, I mean, it's really useful. Oh, we can build planes. We can build computers. We can do these things. Right.\n\n1:54:17.200 --> 1:54:24.080\n Uh, I don't think, I don't know the answer to that question. Um, I think that's part of the\n\n1:54:24.080 --> 1:54:27.920\n question we're trying to figure out, right? Like, you know, obviously if you end up with a theory of\n\n1:54:27.920 --> 1:54:32.960\n everything that really is a theory of everything and all of a sudden everything comes into play\n\n1:54:32.960 --> 1:54:37.120\n and there's no room for something else, then you might feel like we have a good model of the world.\n\n1:54:37.120 --> 1:54:41.440\n Yeah. But if we have a theory of everything and somehow, first of all, you'll never be able to\n\n1:54:41.440 --> 1:54:46.480\n really conclusively say it's a theory of everything, but say somehow we are very damn sure it's a theory\n\n1:54:46.480 --> 1:54:51.680\n of everything. We understand what happened at the big bang and how just the entirety of the\n\n1:54:51.680 --> 1:54:57.120\n physical process. I'm still not sure that gives us an understanding of, uh, the next\n\n1:54:58.240 --> 1:55:03.600\n many layers of the hierarchy of abstractions that form. Well, also what if string theory\n\n1:55:03.600 --> 1:55:09.360\n turns out to be true? And then you say, well, we have no reality, no modeling what's going on in\n\n1:55:09.360 --> 1:55:14.320\n those other dimensions that are wrapped into it on each other. Right. Or, or the multiverse,\n\n1:55:14.880 --> 1:55:21.600\n you know, I honestly don't know how for us, for human interaction, for ideas of intelligence,\n\n1:55:21.600 --> 1:55:25.520\n how it helps us to understand that we're made up of vibrating strings that are\n\n1:55:26.800 --> 1:55:33.040\n like 10 to the whatever times smaller than us. I don't, you know, you could probably build better\n\n1:55:33.040 --> 1:55:37.200\n weapons, a better rockets, but you're not going to be able to understand intelligence. I guess,\n\n1:55:37.200 --> 1:55:41.680\n I guess maybe better computers. No, you won't be. I think it's just more purely knowledge.\n\n1:55:41.680 --> 1:55:45.440\n You might lead to a better understanding of the, of the beginning of the universe,\n\n1:55:46.240 --> 1:55:52.720\n right? It might lead to a better understanding of, uh, I don't know. I guess I think the acquisition\n\n1:55:52.720 --> 1:56:01.200\n of knowledge has always been one where you, you pursue it for its own pleasure. Um, and you don't\n\n1:56:01.200 --> 1:56:06.400\n always know what is going to make a difference. Yeah. Uh, you're pleasantly surprised by the,\n\n1:56:06.400 --> 1:56:11.760\n the weird things you find. Do you think, uh, for the, for the neocortex in general, do you,\n\n1:56:11.760 --> 1:56:16.320\n do you think there's a lot of innovation to be done on the machine side? You know,\n\n1:56:16.960 --> 1:56:21.600\n you use the computer as a metaphor quite a bit. Is there different types of computer that would\n\n1:56:21.600 --> 1:56:26.880\n help us build intelligence manifestations of intelligent machines? Yeah. Or is it, oh no,\n\n1:56:26.880 --> 1:56:31.920\n it's going to be totally crazy. Uh, we have no idea how this is going to look out yet.\n\n1:56:32.720 --> 1:56:37.760\n You can already see this. Um, today we've, of course, we model these things on traditional\n\n1:56:37.760 --> 1:56:43.840\n computers and now, now GPUs are really popular with, with, uh, you know, neural networks and so\n\n1:56:43.840 --> 1:56:50.640\n on. Um, but there are companies coming up with fundamentally new physical substrates, um, that\n\n1:56:50.640 --> 1:56:55.840\n are just really cool. I don't know if they're going to work or not. Um, but I think there'll\n\n1:56:55.840 --> 1:57:01.360\n be decades of innovation here. Yeah. Totally. Do you think the final thing will be messy,\n\n1:57:01.360 --> 1:57:07.360\n like our biology is messy? Or do you think, uh, it's, it's the, it's the old bird versus\n\n1:57:07.360 --> 1:57:16.320\n airplane question, or do you think we could just, um, build airplanes that, that fly way better\n\n1:57:16.320 --> 1:57:23.280\n than birds in the same way we could build, uh, uh, electrical neocortex? Yeah. You know,\n\n1:57:23.280 --> 1:57:26.400\n can I, can I, can I riff on the bird thing a bit? Because I think that's interesting.\n\n1:57:27.040 --> 1:57:33.120\n People really misunderstand this. The Wright brothers, um, the problem they were trying to\n\n1:57:33.120 --> 1:57:38.320\n solve was controlled flight, how to turn an airplane, not how to propel an airplane.\n\n1:57:38.320 --> 1:57:41.600\n They weren't worried about that. Interesting. Yeah. They already had, at that time,\n\n1:57:41.600 --> 1:57:45.520\n there was already wing shapes, which they had from studying birds. There was already gliders\n\n1:57:45.520 --> 1:57:49.440\n that carry people. The problem was if you put a rudder on the back of a glider and you turn it,\n\n1:57:49.440 --> 1:57:55.680\n the plane falls out of the sky. So the problem was how do you control flight? And they studied\n\n1:57:55.680 --> 1:58:00.240\n birds and they actually had birds in captivity. They watched birds in wind tunnels. They observed\n\n1:58:00.240 --> 1:58:05.200\n them in the wild and they discovered the secret was the birds twist their wings when they turn.\n\n1:58:05.200 --> 1:58:07.840\n And so that's what they did on the Wright brothers flyer. They had these sticks that\n\n1:58:07.840 --> 1:58:12.320\n you would twist the wing. And that was the, that was their innovation, not the propeller.\n\n1:58:12.320 --> 1:58:16.720\n And today airplanes still twist their wings. We don't twist the entire wing. We just twist\n\n1:58:16.720 --> 1:58:22.000\n the tail end of it, the flaps, which is the same thing. So today's airplanes fly on the\n\n1:58:22.000 --> 1:58:26.960\n same principles as birds would observe. So everyone get that analogy wrong, but let's\n\n1:58:26.960 --> 1:58:32.640\n step back from that. Once you understand the principles of flight, you can choose\n\n1:58:32.640 --> 1:58:39.120\n how to implement them. No one's going to use bones and feathers and muscles, but they do have wings\n\n1:58:39.120 --> 1:58:45.040\n and we don't flap them. We have propellers. So when we have the principles of computation that\n\n1:58:45.040 --> 1:58:49.440\n goes on to modeling the world in a brain, we understand those principles very clearly.\n\n1:58:50.160 --> 1:58:54.400\n We have choices on how to implement them. And some of them will be biological like and some won't.\n\n1:58:54.400 --> 1:58:59.600\n And, but I do think there's going to be a huge amount of innovation here.\n\n1:58:59.600 --> 1:59:03.920\n Just think about the innovation when in the computer, they had to invent the transistor,\n\n1:59:03.920 --> 1:59:09.200\n they invented the Silicon chip. They had to invent, you know, then this software. I mean,\n\n1:59:09.200 --> 1:59:13.360\n it's millions of things they had to do, memory systems. We're going to do, it's going to be\n\n1:59:13.360 --> 1:59:19.760\n similar. Well, it's interesting that the deep learning, the effectiveness of deep learning for\n\n1:59:19.760 --> 1:59:26.480\n specific tasks is driving a lot of innovation in the hardware, which may have effects for actually\n\n1:59:27.120 --> 1:59:31.760\n allowing us to discover intelligence systems that operate very differently or at least much\n\n1:59:31.760 --> 1:59:37.040\n bigger than deep learning. Yeah. Interesting. So ultimately it's good to have an application\n\n1:59:37.040 --> 1:59:42.960\n that's making our life better now because the capitalist process, if you can make money.\n\n1:59:42.960 --> 1:59:48.080\n Yeah. Yeah. That works. I mean, the other way, I mean, Neil deGrasse Tyson writes about this\n\n1:59:48.080 --> 1:59:53.360\n is the other way we fund science, of course, is through military. So like, yeah. Conquests.\n\n1:59:53.360 --> 1:59:57.920\n So here's an interesting thing we're doing on this regard. So we've decided, we used to have\n\n1:59:57.920 --> 2:00:01.360\n a series of these biological principles and we can see how to build these intelligent machines,\n\n2:00:01.920 --> 2:00:06.480\n but we've decided to apply some of these principles to today's machine learning techniques.\n\n2:00:07.280 --> 2:00:11.600\n So one of the, we didn't talk about this principle. One is a sparsity in the brain,\n\n2:00:11.600 --> 2:00:15.440\n um, most of the neurons are active at any point in time. It's sparse and the connectivity is sparse\n\n2:00:15.440 --> 2:00:20.800\n and that's different than deep learning networks. Um, so we've already shown that we can speed up\n\n2:00:20.800 --> 2:00:26.400\n existing deep learning networks, uh, anywhere from 10 to a factor of a hundred. I mean,\n\n2:00:26.400 --> 2:00:31.760\n literally a hundred, um, and make a more robust at the same time. So this is commercially very,\n\n2:00:31.760 --> 2:00:38.960\n very valuable. Um, and so, you know, if we can prove this actually in the largest systems that\n\n2:00:38.960 --> 2:00:44.240\n are commercially applied today, there's a big commercial desire to do this. Well,\n\n2:00:44.240 --> 2:00:50.640\n sparsity is something that doesn't run really well on existing hardware. It doesn't really run\n\n2:00:50.640 --> 2:00:58.800\n really well, um, on, um, GPUs, um, and on CPUs. And so that would be a way of sort of bringing more,\n\n2:00:59.520 --> 2:01:03.920\n more brain principles into the existing system on a, on a commercially valuable basis.\n\n2:01:03.920 --> 2:01:06.960\n Another thing we can think we can do is we're going to use these dendrites,\n\n2:01:06.960 --> 2:01:13.200\n um, models that we, uh, I talked earlier about the prediction occurring inside a neuron\n\n2:01:13.200 --> 2:01:18.000\n that that basic property can be applied to existing neural networks and allow them to\n\n2:01:18.000 --> 2:01:22.960\n learn continuously, which is something they don't do today. And so the dendritic spikes that you\n\n2:01:22.960 --> 2:01:26.080\n were talking about. Yeah. Well, we wouldn't model the spikes, but the idea that you have\n\n2:01:26.960 --> 2:01:30.640\n that neuron today's neural networks have this company called the point neurons is a very simple\n\n2:01:30.640 --> 2:01:36.000\n model of a neuron. And, uh, by adding dendrites to them at just one more level of complexity,\n\n2:01:36.000 --> 2:01:41.520\n uh, that's in biological systems, you can solve problems in continuous learning, um,\n\n2:01:41.520 --> 2:01:47.040\n and rapid learning. So we're trying to take, we're trying to bring the existing field,\n\n2:01:47.760 --> 2:01:50.640\n and we'll see if we can do it. We're trying to bring the existing field of machine learning,\n\n2:01:51.360 --> 2:01:55.040\n um, commercially along with us, you brought up this idea of keeping, you know,\n\n2:01:55.040 --> 2:01:59.680\n paying for it commercially along with us as we move towards the ultimate goal of a true AI system.\n\n2:02:00.320 --> 2:02:04.000\n Even small innovations on your own networks are really, really exciting.\n\n2:02:04.000 --> 2:02:04.480\n Yeah.\n\n2:02:04.480 --> 2:02:10.960\n Is it seems like such a trivial model of the brain and applying different insights\n\n2:02:11.920 --> 2:02:18.640\n that just even, like you said, continuous, uh, learning or, uh, making it more asynchronous\n\n2:02:19.360 --> 2:02:28.720\n or maybe making more dynamic or like, uh, incentivizing, making it robust and making it\n\n2:02:28.720 --> 2:02:35.840\n somehow much better incentivizing sparsity, uh, somehow. Yeah. Well, if you can make things a\n\n2:02:35.840 --> 2:02:40.480\n hundred times faster, then there's plenty of incentive. That's true. People, people are\n\n2:02:40.480 --> 2:02:44.400\n spending millions of dollars, you know, just training some of these networks. Now these, uh,\n\n2:02:44.400 --> 2:02:51.520\n these transforming networks, let me ask you the big question for young people listening to this\n\n2:02:51.520 --> 2:02:57.280\n today in high school and college, what advice would you give them in terms of, uh, which career\n\n2:02:57.280 --> 2:03:06.720\n path to take and, um, maybe just about life in general? Well, in my case, um, I didn't start\n\n2:03:06.720 --> 2:03:11.040\n life with any kind of goals. I was, when I was going to college, it's like, Oh, what do I study?\n\n2:03:11.040 --> 2:03:15.840\n Well, maybe I'll do this electrical engineering stuff, you know? Um, it wasn't like, you know,\n\n2:03:15.840 --> 2:03:18.720\n today you see some of these young kids are so motivated, like I'm changing the world. I was\n\n2:03:18.720 --> 2:03:25.920\n like, you know, whatever. And, um, but then I did fall in love with something besides my wife,\n\n2:03:25.920 --> 2:03:30.800\n but I fell in love with this, like, Oh my God, it would be so cool to understand how the brain works.\n\n2:03:30.800 --> 2:03:34.800\n And then I, I said to myself, that's the most important thing I could work on. I can't imagine\n\n2:03:34.800 --> 2:03:38.240\n anything more important because if we understand how the brains work, you build tells the machines\n\n2:03:38.240 --> 2:03:42.240\n and they could figure out all the other big questions of the world. Right. So, and then I\n\n2:03:42.240 --> 2:03:46.320\n said, but I want to understand how I work. So I fell in love with this idea and I became passionate\n\n2:03:46.320 --> 2:03:54.160\n about it. And this is a trope. People say this, but it was, it's true because I was passionate\n\n2:03:54.160 --> 2:04:01.040\n about it. I was able to put up almost so much crap, you know, you know, I was, I was in that,\n\n2:04:01.040 --> 2:04:05.200\n you know, I was like person said, you can't do this. I was, I was a graduate student at Berkeley\n\n2:04:05.200 --> 2:04:09.040\n when they said, you can't study this problem, you know, no one's can solve this or you can't get\n\n2:04:09.040 --> 2:04:13.120\n funded for it. You know, then I went into do mobile computing and it was like, people say,\n\n2:04:13.120 --> 2:04:18.880\n you can't do that. You can't build a cell phone, you know? So, but all along I kept being motivated\n\n2:04:18.880 --> 2:04:22.720\n because I wanted to work on this problem. I said, I want to understand the brain works. And I got\n\n2:04:22.720 --> 2:04:28.160\n myself, you know, I got one lifetime. I'm going to figure it out, do the best I can. So by having\n\n2:04:28.160 --> 2:04:33.440\n that, cause you know, it's really, as you pointed out, Lex, it's really hard to do these things.\n\n2:04:33.440 --> 2:04:38.320\n People, it just, there's so many downers along the way. So many ways, obstacles to get in your\n\n2:04:38.320 --> 2:04:42.000\n way. Yeah. I'm sitting here happy all the time, but trust me, it's not always like that.\n\n2:04:42.000 --> 2:04:47.520\n Well, that's, I guess the happiness, the passion is a prerequisite for surviving the whole thing.\n\n2:04:47.520 --> 2:04:53.120\n Yeah, I think so. I think that's right. And so I don't want to sit to someone and say, you know,\n\n2:04:53.120 --> 2:04:57.920\n you need to find a passion and do it. No, maybe you don't. But if you do find something you're\n\n2:04:57.920 --> 2:05:04.000\n passionate about, then you can follow it as far as your passion will let you put up with it.\n\n2:05:04.000 --> 2:05:07.360\n Do you remember how you found it? How the spark happened?\n\n2:05:09.200 --> 2:05:10.800\n Why specifically for me?\n\n2:05:10.800 --> 2:05:15.200\n Yeah. Cause you said it's such an interesting, so like almost like later in life, by later,\n\n2:05:15.200 --> 2:05:21.120\n I mean like not when you were five, you didn't really know. And then all of a sudden you fell\n\n2:05:21.120 --> 2:05:25.040\n in love with that idea. Yeah, yeah. There was two separate events that compounded one another.\n\n2:05:25.600 --> 2:05:31.520\n One, when I was probably a teenager, it might've been 17 or 18, I made a list of the most\n\n2:05:31.520 --> 2:05:36.960\n interesting problems I could think of. First was why does the universe exist? It seems like\n\n2:05:36.960 --> 2:05:41.120\n not existing is more likely. The second one was, well, given it exists, why does it behave the way\n\n2:05:41.120 --> 2:05:45.680\n it does? Laws of physics, why is it equal MC squared, not MC cubed? That's an interesting\n\n2:05:45.680 --> 2:05:51.680\n question. The third one was like, what's the origin of life? And the fourth one was, what's\n\n2:05:51.680 --> 2:05:56.240\n intelligence? And I stopped there. I said, well, that's probably the most interesting one. And I\n\n2:05:56.240 --> 2:06:05.680\n put that aside as a teenager. But then when I was 22 and I was reading the, no, excuse me, it was\n\n2:06:05.680 --> 2:06:13.520\n 1979, excuse me, 1979, I was reading, so I was, at that time I was 22, I was reading the September\n\n2:06:13.520 --> 2:06:19.440\n issue of Scientific American, which is all about the brain. And then the final essay was by Francis\n\n2:06:19.440 --> 2:06:25.920\n Crick, who of DNA fame, and he had taken his interest to studying the brain now. And he said,\n\n2:06:25.920 --> 2:06:33.600\n you know, there's something wrong here. He says, we got all this data, all this fact, this is 1979,\n\n2:06:33.600 --> 2:06:39.360\n all these facts about the brain, tons and tons of facts about the brain. Do we need more facts? Or do\n\n2:06:39.360 --> 2:06:42.800\n we just need to think about a way of rearranging the facts we have? Maybe we're just not thinking\n\n2:06:42.800 --> 2:06:51.440\n about the problem correctly. Cause he says, this shouldn't be like this. So I read that and I said,\n\n2:06:51.440 --> 2:06:57.360\n wow. I said, I don't have to become like an experimental neuroscientist. I could just\n\n2:06:57.360 --> 2:07:04.320\n take, look at all those facts and try and become a theoretician and try to figure it out. And I said\n\n2:07:04.320 --> 2:07:08.640\n that I felt like it was something I would be good at. I said, I wouldn't be a good experimentalist.\n\n2:07:08.640 --> 2:07:14.320\n I don't have the patience for it, but I'm a good thinker and I love puzzles. And this is like the\n\n2:07:14.320 --> 2:07:18.240\n biggest puzzle in the world. It's the biggest puzzle of all time. And I got all the puzzle\n\n2:07:18.240 --> 2:07:23.360\n pieces in front of me. Damn, that was exciting. And there's something obviously you can't\n\n2:07:23.360 --> 2:07:29.440\n convert into words that just kind of sparked this passion. And I have that a few times in my life,\n\n2:07:29.440 --> 2:07:37.680\n just something just like you, it grabs you. Yeah. I felt it was something that was both\n\n2:07:37.680 --> 2:07:41.680\n important and that I could make a contribution to. And so all of a sudden it felt like,\n\n2:07:41.680 --> 2:07:46.960\n oh, it gave me purpose in life. I honestly don't think it has to be as big as one of those four\n\n2:07:46.960 --> 2:07:54.160\n questions. I think you can find those things in the smallest. Oh, absolutely. David Foster Wallace\n\n2:07:54.160 --> 2:08:01.040\n said like the key to life is to be unboreable. I think it's very possible to find that intensity\n\n2:08:01.040 --> 2:08:06.000\n of joy in the smallest thing. Absolutely. I'm just, you asked me my story. Yeah. No, but I'm\n\n2:08:06.000 --> 2:08:10.800\n actually speaking to the audience. It doesn't have to be those four. You happen to get excited by one\n\n2:08:10.800 --> 2:08:18.320\n of the bigger questions of in the universe, but even the smallest things and watching the Olympics\n\n2:08:18.320 --> 2:08:25.920\n now, just giving yourself life, giving your life over to the study and the mastery of a particular\n\n2:08:25.920 --> 2:08:32.720\n sport is fascinating. And if it sparks joy and passion, you're able to, in the case of the\n\n2:08:32.720 --> 2:08:37.520\n Olympics, basically suffer for like a couple of decades to achieve. I mean, you can find joy and\n\n2:08:37.520 --> 2:08:43.600\n passion just being a parent. I mean, yeah, the parenting one is funny. So I was, not always,\n\n2:08:43.600 --> 2:08:48.720\n but for a long time, wanted kids and get married and stuff. And especially that has to do with the\n\n2:08:48.720 --> 2:08:57.440\n fact that I've seen a lot of people that I respect get a whole nother level of joy from kids. And\n\n2:08:58.880 --> 2:09:05.920\n at first is like, you're thinking is, well, like I don't have enough time in the day, right? If I\n\n2:09:05.920 --> 2:09:13.200\n have this passion to solve, but like, if I want to solve intelligence, how's this kid situation\n\n2:09:13.200 --> 2:09:22.000\n going to help me? But then you realize that, you know, like you said, the things that sparks joy,\n\n2:09:22.000 --> 2:09:28.640\n and it's very possible that kids can provide even a greater or deeper, more meaningful joy than\n\n2:09:28.640 --> 2:09:34.160\n those bigger questions when they enrich each other. And that seemed like, obviously when I\n\n2:09:34.160 --> 2:09:37.920\n was younger, it's probably a counterintuitive notion because there's only so many hours in the\n\n2:09:37.920 --> 2:09:44.160\n day, but then life is finite and you have to pick the things that give you joy.\n\n2:09:44.160 --> 2:09:50.800\n Yeah. But you also understand you can be patient too. I mean, it's finite, but we do have, you know,\n\n2:09:50.800 --> 2:09:58.480\n whatever, 50 years or something. So in my case, I had to give up on my dream of the neuroscience\n\n2:09:58.480 --> 2:10:02.240\n because I was a graduate student at Berkeley and they told me I couldn't do this and I couldn't\n\n2:10:02.240 --> 2:10:09.440\n get funded. And so I went back in the computing industry for a number of years. I thought it\n\n2:10:09.440 --> 2:10:14.880\n would be four, but it turned out to be more. But I said, I'll come back. I'm definitely going to\n\n2:10:14.880 --> 2:10:17.920\n come back. I know I'm going to do this computer stuff for a while, but I'm definitely coming back.\n\n2:10:17.920 --> 2:10:22.800\n Everyone knows that. And it's like raising kids. Well, yeah, you have to spend a lot of time with\n\n2:10:22.800 --> 2:10:28.240\n your kids. It's fun, enjoyable. But that doesn't mean you have to give up on other dreams. It just\n\n2:10:28.240 --> 2:10:34.720\n means that you may have to wait a week or two to work on that next idea. Well, you talk about the\n\n2:10:36.800 --> 2:10:42.240\n darker side of me, disappointing sides of human nature that we're hoping to overcome so that we\n\n2:10:42.240 --> 2:10:48.640\n don't destroy ourselves. I tend to put a lot of value in the broad general concept of love,\n\n2:10:48.640 --> 2:10:58.960\n of the human capacity of compassion towards each other, of just kindness, whatever that longing of\n\n2:10:58.960 --> 2:11:04.560\n like just the human to human connection. It connects back to our initial discussion. I tend to\n\n2:11:05.120 --> 2:11:09.360\n see a lot of value in this collective intelligence aspect. I think some of the magic of human\n\n2:11:09.360 --> 2:11:16.080\n civilization happens when there's a party is not as fun when you're alone. I totally agree with\n\n2:11:16.080 --> 2:11:24.080\n you on these issues. Do you think from a neocortex perspective, what role does love play in the human\n\n2:11:24.080 --> 2:11:29.600\n condition? Well, those are two separate things from a neocortex point of view. It doesn't impact\n\n2:11:29.600 --> 2:11:34.400\n our thinking about the neocortex. From a human condition point of view, I think it's core.\n\n2:11:34.400 --> 2:11:44.720\n I mean, we get so much pleasure out of loving people and helping people. I'll rack it up to\n\n2:11:44.720 --> 2:11:50.720\n old brain stuff and maybe we can throw it under the bus of evolution if you want. That's fine.\n\n2:11:52.800 --> 2:11:57.840\n It doesn't impact how I think about how we model the world, but from a humanity point of view,\n\n2:11:57.840 --> 2:12:03.680\n I think it's essential. Well, I tend to give it to the new brain and also I tend to give it to\n\n2:12:03.680 --> 2:12:09.120\n the old brain. Also, I tend to think that some aspects of that need to be engineered into AI\n\n2:12:09.120 --> 2:12:21.440\n systems, both in their ability to have compassion for other humans and their ability to maximize\n\n2:12:21.440 --> 2:12:27.760\n love in the world between humans. I'm more thinking about social networks. Whenever there's a deep\n\n2:12:27.760 --> 2:12:34.080\n AI systems in humans, specific applications where it's AI and humans, I think that's something that\n\n2:12:35.120 --> 2:12:42.880\n often not talked about in terms of metrics over which you try to maximize,\n\n2:12:44.480 --> 2:12:47.920\n like which metric to maximize in a system. It seems like one of the most\n\n2:12:48.960 --> 2:12:55.120\n powerful things in societies is the capacity to love.\n\n2:12:55.120 --> 2:13:01.120\n It's fascinating. I think it's a great way of thinking about it. I have been thinking more of\n\n2:13:01.120 --> 2:13:06.640\n these fundamental mechanisms in the brain as opposed to the social interaction between humans\n\n2:13:06.640 --> 2:13:13.680\n and AI systems in the future. If you think about that, you're absolutely right. That's a complex\n\n2:13:13.680 --> 2:13:17.360\n system. I can have intelligent systems that don't have that component, but they're not interacting\n\n2:13:17.360 --> 2:13:21.600\n with people. They're just running something or building some place or something. I don't know.\n\n2:13:21.600 --> 2:13:26.640\n But if you think about interacting with humans, yeah, but it has to be engineered in there. I\n\n2:13:26.640 --> 2:13:30.560\n don't think it's going to appear on its own. That's a good question.\n\n2:13:30.560 --> 2:13:38.000\n Yeah. Well, we could, we'll leave that open. In terms of, from a reinforcement learning\n\n2:13:38.000 --> 2:13:46.880\n perspective, whether the darker sides of human nature or the better angels of our nature win out,\n\n2:13:46.880 --> 2:13:51.680\n statistically speaking, I don't know. I tend to be optimistic and hope that love wins out in the end.\n\n2:13:52.960 --> 2:14:01.520\n You've done a lot of incredible stuff and your book is driving towards this fourth question that\n\n2:14:01.520 --> 2:14:08.880\n you started with on the nature of intelligence. What do you hope your legacy for people reading\n\n2:14:08.880 --> 2:14:14.560\n a hundred years from now? How do you hope they remember your work? How do you hope they remember\n\n2:14:14.560 --> 2:14:21.920\n this book? Well, I think as an entrepreneur or a scientist or any human who's trying to accomplish\n\n2:14:21.920 --> 2:14:30.960\n some things, I have a view that really all you can do is accelerate the inevitable. Yeah. It's like,\n\n2:14:30.960 --> 2:14:33.920\n you know, if we didn't figure out, if we didn't study the brain, someone else will study the\n\n2:14:33.920 --> 2:14:38.080\n brain. If, you know, if Elon didn't make electric cars, someone else would do it eventually.\n\n2:14:38.080 --> 2:14:42.400\n And if, you know, if Thomas Edison didn't invent a light bulb, we wouldn't be using candles today.\n\n2:14:42.400 --> 2:14:48.880\n So, what you can do as an individual is you can accelerate something that's beneficial\n\n2:14:48.880 --> 2:14:52.400\n and make it happen sooner than it would have. That's really it. That's all you can do.\n\n2:14:53.680 --> 2:15:00.080\n You can't create a new reality that it wasn't going to happen. So, from that perspective,\n\n2:15:01.280 --> 2:15:07.440\n I would hope that our work, not just me, but our work in general, people would look back and said,\n\n2:15:07.440 --> 2:15:14.160\n hey, they really helped make this better future happen sooner. They, you know, they helped us\n\n2:15:14.160 --> 2:15:18.640\n understand the nature of false beliefs sooner than they might have. Now we're so happy that\n\n2:15:18.640 --> 2:15:22.560\n we have these intelligent machines doing these things, helping us that maybe that solved the\n\n2:15:22.560 --> 2:15:28.320\n climate change problem and they made it happen sooner. So, I think that's the best I would hope\n\n2:15:28.320 --> 2:15:33.280\n for. Some would say those guys just moved the needle forward a little bit in time.\n\n2:15:33.280 --> 2:15:40.000\n Well, I do. It feels like the progress of human civilization is not, is there's a lot\n\n2:15:40.000 --> 2:15:48.480\n of trajectories. And if you have individuals that accelerate towards one direction that helps steer\n\n2:15:48.480 --> 2:15:55.200\n human civilization. So, I think in those long stretch of time, all trajectories will be traveled.\n\n2:15:55.200 --> 2:15:59.840\n But I think it's nice for this particular civilization on earth to travel down one that's\n\n2:15:59.840 --> 2:16:03.440\n not. Well, I think you're right. We have to take the whole period of, you know, World War II,\n\n2:16:03.440 --> 2:16:07.520\n Nazism or something like that. Well, that was a bad sidestep, right? We've been over there for a\n\n2:16:07.520 --> 2:16:13.680\n while. But, you know, there is the optimistic view about life that ultimately it does converge\n\n2:16:13.680 --> 2:16:21.920\n in a positive way. It progresses ultimately, even if we have years of darkness. So, yeah. So,\n\n2:16:21.920 --> 2:16:27.200\n I think you can perhaps that's accelerating the positive could also mean eliminating some bad\n\n2:16:27.200 --> 2:16:34.560\n missteps along the way, too. But I'm an optimistic in that way. Despite we talked about the end of\n\n2:16:34.560 --> 2:16:40.080\n civilization, you know, I think we're going to live for a long time. I hope we are. I think our\n\n2:16:40.080 --> 2:16:42.640\n society in the future is going to be better. We're going to have less discord. We're going to have\n\n2:16:42.640 --> 2:16:47.600\n less people killing each other. You know, we'll make them live in some sort of way that's compatible\n\n2:16:47.600 --> 2:16:53.520\n with the carrying capacity of the earth. I'm optimistic these things will happen. And all we\n\n2:16:53.520 --> 2:16:57.840\n can do is try to get there sooner. And at the very least, if we do destroy ourselves,\n\n2:16:57.840 --> 2:17:05.680\n we'll have a few satellites orbiting that will tell alien civilization that we were once here.\n\n2:17:05.680 --> 2:17:09.760\n Or maybe our future, you know, future inhabitants of earth. You know, imagine we,\n\n2:17:10.560 --> 2:17:13.600\n you know, the planet of the apes in here. You know, we kill ourselves, you know,\n\n2:17:13.600 --> 2:17:16.480\n a million years from now or a billion years from now. There's another species on the planet.\n\n2:17:16.480 --> 2:17:23.200\n Curious creatures were once here. Jeff, thank you so much for your work. And thank you so much for\n\n2:17:23.200 --> 2:17:27.040\n talking to me once again. Well, actually, it's great. I love what you do. I love your podcast.\n\n2:17:27.040 --> 2:17:34.640\n You have the most interesting people, me aside. So it's a real service, I think you do for,\n\n2:17:35.280 --> 2:17:39.040\n in a very broader sense for humanity, I think. Thanks, Jeff. All right. It's a pleasure.\n\n2:17:40.000 --> 2:17:43.360\n Thanks for listening to this conversation with Jeff Hawkins. And thank you to\n\n2:17:43.360 --> 2:17:50.960\n Codecademy, BioOptimizers, ExpressVPN, Asleep, and Blinkist. Check them out in the description\n\n2:17:50.960 --> 2:17:56.480\n to support this podcast. And now, let me leave you with some words from Albert Camus.\n\n2:17:57.600 --> 2:18:04.240\n An intellectual is someone whose mind watches itself. I like this, because I'm happy to be\n\n2:18:04.240 --> 2:18:10.880\n both halves, the watcher and the watched. Can they be brought together? This is the\n\n2:18:10.880 --> 2:18:17.200\n practical question we must try to answer. Thank you for listening. I hope to see you next time.\n\n"
}